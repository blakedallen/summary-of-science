Experiments with a Higher-Order Projective Dependency Parser
We present experiments with a dependency parsing model defined on rich factors.
Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children.
We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron.
Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption.
In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al., 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.
We extend the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model.
Our second-order models include head grandparent relations.
Our second order algorithm uses the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild.
We introduce the left-most and right-most grandchild as factors.
