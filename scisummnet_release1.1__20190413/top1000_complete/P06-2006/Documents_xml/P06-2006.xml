<PAPER>
  <S sid="0">Evaluating The Accuracy Of An Unlexicalized Statistical Parser On The PARC DepBank</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank.</S>
    <S sid="2" ssid="2">We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manuallyconstructed treebanks.</S>
    <S sid="3" ssid="3">This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.</S>
    <S sid="4" ssid="4">The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Considerable progress has been made in accurate statistical parsing of realistic texts, yielding rooted, hierarchical and/or relational representations of full sentences.</S>
    <S sid="6" ssid="2">However, much of this progress has been made with systems based on large lexicalized probabilistic contextfree like (PCFG-like) models trained on the Wall Street Journal (WSJ) subset of the Penn TreeBank (PTB).</S>
    <S sid="7" ssid="3">Evaluation of these systems has been mostly in terms of the PARSEVAL scheme using tree similarity measures of (labelled) precision and recall and crossing bracket rate applied to section 23 of the WSJ PTB.</S>
    <S sid="8" ssid="4">(See e.g.</S>
    <S sid="9" ssid="5">Collins (1999) for detailed exposition of one such very fruitful line of research.)</S>
    <S sid="10" ssid="6">We evaluate the comparative accuracy of an unlexicalized statistical parser trained on a smaller treebank and tested on a subset of section 23 of the WSJ using a relational evaluation scheme.</S>
    <S sid="11" ssid="7">We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly developed without reliance on large in-domain manually-constructed treebanks.</S>
    <S sid="12" ssid="8">This makes it more practical to use statistical parsers in diverse applications needing access to aspects of predicate-argument structure.</S>
    <S sid="13" ssid="9">We define a lexicalized statistical parser as one which utilizes probabilistic parameters concerning lexical subcategorization and/or bilexical relations over tree configurations.</S>
    <S sid="14" ssid="10">Current lexicalized statistical parsers developed, trained and tested on PTB achieve a labelled Fl-score &#8211; the harmonic mean of labelled precision and recall &#8211; of around 90%.</S>
    <S sid="15" ssid="11">Klein and Manning (2003) argue that such results represent about 4% absolute improvement over a carefully constructed unlexicalized PCFGlike model trained and tested in the same manner.1 Gildea (2001) shows that WSJ-derived bilexical parameters in Collins&#8217; (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus.</S>
    <S sid="16" ssid="12">Bikel (2004) shows that, in Collins&#8217; (1999) Model 2, bilexical parameters contribute less than 0.5% to accuracy on in-domain data while lexical subcategorization-like parameters contribute just over 1%.</S>
    <S sid="17" ssid="13">Several alternative relational evaluation schemes have been developed (e.g.</S>
    <S sid="18" ssid="14">Carroll et al., 1998; Lin, 1998).</S>
    <S sid="19" ssid="15">However, until recently, no WSJ data has been carefully annotated to support relational evaluation.</S>
    <S sid="20" ssid="16">King et al. (2003) describe the PARC 700 Dependency Bank (hereinafter DepBank), which consists of 700 WSJ sentences randomly drawn from section 23.</S>
    <S sid="21" ssid="17">These sentences have been annotated with syntactic features and with bilexical head-dependent relations derived from the F-structure representation of Lexical Functional Grammar (LFG).</S>
    <S sid="22" ssid="18">DepBank facilitates comparison of PCFG-like statistical parsers developed from the PTB with other parsers whose output is not designed to yield PTB-style trees, using an evaluation which is closer to the protypical parsing task of recovering predicate-argument structure.</S>
    <S sid="23" ssid="19">Kaplan et al. (2004) compare the accuracy and speed of the PARC XLE Parser to Collins&#8217; Model 3 parser.</S>
    <S sid="24" ssid="20">They develop transformation rules for both, designed to map native output to a subset of the features and relations in DepBank.</S>
    <S sid="25" ssid="21">They compare performance of a grammatically cut-down and complete version of the XLE parser to the publically available version of Collins&#8217; parser.</S>
    <S sid="26" ssid="22">One fifth of DepBank is held out to optimize the speed and accuracy of the three systems.</S>
    <S sid="27" ssid="23">They conclude from the results of these experiments that the cut-down XLE parser is two-thirds the speed of Collins&#8217; Model 3 but 12% more accurate, while the complete XLE system is 20% more accurate but five times slower.</S>
    <S sid="28" ssid="24">F1-score percentages range from the mid- to high-70s, suggesting that the relational evaluation is harder than PARSEVAL.</S>
    <S sid="29" ssid="25">Both Collins&#8217; Model 3 and the XLE Parser use lexicalized models for parse selection trained on the rest of the WSJ PTB.</S>
    <S sid="30" ssid="26">Therefore, although Kaplan et al. demonstrate an improvement in accuracy at some cost to speed, there remain questions concerning viability for applications, at some remove from the financial news domain, for which substantial treebanks are not available.</S>
    <S sid="31" ssid="27">The parser we deploy, like the XLE one, is based on a manually-defined feature-based unification grammar.</S>
    <S sid="32" ssid="28">However, the approach is somewhat different, making maximal use of more generic structural rather than lexical information, both within the grammar and the probabilistic parse selection model.</S>
    <S sid="33" ssid="29">Here we compare the accuracy of our parser with Kaplan et al.&#8217;s results, by repeating their experiment with our parser.</S>
    <S sid="34" ssid="30">This comparison is not straightforward, given both the systemspecific nature of some of the annotation in DepBank and the scoring reported.</S>
    <S sid="35" ssid="31">We, therefore, extend DepBank with a set of grammatical relations derived from our own system output and highlight how issues of representation and scoring can affect results and their interpretation.</S>
    <S sid="36" ssid="32">In &#167;2, we describe our development methodology and the resulting system in greater detail.</S>
    <S sid="37" ssid="33">&#167;3 describes the extended Depbank that we have developed and motivates our additions.</S>
    <S sid="38" ssid="34">&#167;2.4 discusses how we trained and tuned our current system and describes our limited use of information derived from WSJ text.</S>
    <S sid="39" ssid="35">&#167;4 details the various experiments undertaken with the extended DepBank and gives detailed results.</S>
    <S sid="40" ssid="36">&#167;5 discusses these results and proposes further lines of research.</S>
  </SECTION>
  <SECTION title="2 Unlexicalized Statistical Parsing" number="2">
    <S sid="41" ssid="1">Both the XLE system and Collins&#8217; Model 3 preprocess textual input before parsing.</S>
    <S sid="42" ssid="2">Similarly, our baseline system consists of a pipeline of modules.</S>
    <S sid="43" ssid="3">First, text is tokenized using a deterministic finite-state transducer.</S>
    <S sid="44" ssid="4">Second, tokens are part-ofspeech and punctuation (PoS) tagged using a 1storder Hidden Markov Model (HMM) utilizing a lexicon of just over 50K words and an unknown word handling module.</S>
    <S sid="45" ssid="5">Third, deterministic morphological analysis is performed on each tokentag pair with a finite-state transducer.</S>
    <S sid="46" ssid="6">Fourth, the lattice of lemma-affix-tags is parsed using a grammar over such tags.</S>
    <S sid="47" ssid="7">Finally, the n-best parses are computed from the parse forest using a probabilistic parse selection model conditioned on the structural parse context.</S>
    <S sid="48" ssid="8">The output of the parser can be displayed as syntactic trees, and/or factored into a sequence of bilexical grammatical relations (GRs) between lexical heads and their dependents.</S>
    <S sid="49" ssid="9">The full system can be extended in a variety of ways &#8211; for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth &#8211; broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006).</S>
    <S sid="50" ssid="10">However, in this paper we focus exclusively on the baseline unlexicalized system.</S>
    <S sid="51" ssid="11">The grammar is expressed in a feature-based, unification formalism.</S>
    <S sid="52" ssid="12">There are currently 676 phrase structure rule schemata, 15 feature propagation rules, 30 default feature value rules, 22 category expansion rules and 41 feature types which together define 1124 compiled phrase structure rules in which categories are represented as sets of features, that is, attribute-value pairs, possibly with variable values, possibly bound between mother and one or more daughter categories.</S>
    <S sid="53" ssid="13">142 of the phrase structure schemata are manually identified as peripheral rather than core rules of English grammar.</S>
    <S sid="54" ssid="14">Categories are matched using fixedarity term unification at parse time.</S>
    <S sid="55" ssid="15">The lexical categories of the grammar consist of feature-based descriptions of the 149 PoS tags and 13 punctuation tags (a subset of the CLAWS tagset, see e.g.</S>
    <S sid="56" ssid="16">Sampson, 1995) which constitute the preterminals of the grammar.</S>
    <S sid="57" ssid="17">The number of distinct lexical categories associated with each preterminal varies from 1 for some function words through to around 35 as, for instance, tags for main verbs are associated with a VSUBCAT attribute taking 33 possible values.</S>
    <S sid="58" ssid="18">The grammar is designed to enumerate possible valencies for predicates by including separate rules for each pattern of possible complementation in English.</S>
    <S sid="59" ssid="19">The distinction between arguments and adjuncts is expressed by adjunction of adjuncts to maximal projections (XP &#8212;* XP Adjunct) as opposed to government of arguments (i.e. arguments are sisters within X] projections; X] &#8212;* X0 Arg]... ArgN).</S>
    <S sid="60" ssid="20">Each phrase structure schema is associated with one or more GR specifications which can be conditioned on feature values instantiated at parse time and which yield a rule-to-rule mapping from local trees to GRs.</S>
    <S sid="61" ssid="21">The set of GRs associated with a given derivation define a connected, directed graph with individual nodes representing lemmaaffix-tags and arcs representing named grammatical relations.</S>
    <S sid="62" ssid="22">The encoding of this mapping within the grammar is similar to that of F-structure mapping in LFG.</S>
    <S sid="63" ssid="23">However, the connected graph is not constructed and completeness and coherence constraints are not used to filter the phrase structure derivation space.</S>
    <S sid="64" ssid="24">The grammar finds at least one parse rooted in the start category for 85% of the Susanne treebank, a 140K word balanced subset of the Brown Corpus, which we have used for development (Sampson, 1995).</S>
    <S sid="65" ssid="25">Much of the remaining data consists of phrasal fragments marked as independent text sentences, for example in dialogue.</S>
    <S sid="66" ssid="26">Grammatical coverage includes the majority of construction types of English, however the handling of some unbounded dependency constructions, particularly comparatives and equatives, is limited because of the lack of fine-grained subcategorization information in the PoS tags and by the need to balance depth of analysis against the size of the derivation space.</S>
    <S sid="67" ssid="27">On the Susanne corpus, the geometric mean of the number of analyses for a sentence of length n is 1.31'.</S>
    <S sid="68" ssid="28">The microaveraged F1-score for GR extraction on held-out data from Susanne is 76.5% (see section 4.2 for details of the evaluation scheme).</S>
    <S sid="69" ssid="29">The system has been used to analyse about 150 million words of English text drawn primarily from the PTB, TREC, BNC, and Reuters RCV1 datasets in connection with a variety of projects.</S>
    <S sid="70" ssid="30">The grammar and PoS tagger lexicon have been incrementally improved by manually examining cases of parse failure on these datasets.</S>
    <S sid="71" ssid="31">However, the effort invested amounts to a few days&#8217; effort for each new dataset as opposed to the main grammar development effort, centred on Susanne, which has extended over some years and now amounts to about 2 years&#8217; effort (see Briscoe, 2006 for further details).</S>
    <S sid="72" ssid="32">To build the parsing module, the unification grammar is automatically converted into an atomiccategoried context free &#8216;backbone&#8217;, and a nondeterministic LALR(1) table is constructed from this, which is used to drive the parser.</S>
    <S sid="73" ssid="33">The residue of features not incorporated into the backbone are unified on each rule application (reduce action).</S>
    <S sid="74" ssid="34">In practice, the parser takes average time roughly quadratic in the length of the input to create a packed parse forest represented as a graphstructured stack.</S>
    <S sid="75" ssid="35">The statistical disambiguation phase is trained on Susanne treebank bracketings, producing a probabilistic generalized LALR(1) parser (e.g.</S>
    <S sid="76" ssid="36">Inui et al., 1997) which associates probabilities with alternative actions in the LR table.</S>
    <S sid="77" ssid="37">The parser is passed as input the sequence of most probable lemma-affix-tags found by the tagger.</S>
    <S sid="78" ssid="38">During parsing, probabilities are assigned to subanalyses based on the the LR table actions that derived them.</S>
    <S sid="79" ssid="39">The n-best (i.e. most probable) parses are extracted by a dynamic programming procedure over subanalyses (represented by nodes in the parse forest).</S>
    <S sid="80" ssid="40">The search is efficient since probabilities are associated with single nodes in the parse forest and no weight function over ancestor or sibling nodes is needed.</S>
    <S sid="81" ssid="41">Probabilities capture structural context, since nodes in the parse forest partially encode a configuration of the graph-structured stack and lookahead symbol, so that, unlike a standard PCFG, the model discriminates between derivations which only differ in the order of application of the same rules and also conditions rule application on the PoS tag of the lookahead token.</S>
    <S sid="82" ssid="42">When there is no parse rooted in the start category, the parser returns a connected sequence of partial parses which covers the input based on subanalysis probability and a preference for longer and non-lexical subanalysis combinations (e.g.</S>
    <S sid="83" ssid="43">Kiefer et al., 1999).</S>
    <S sid="84" ssid="44">In these cases, the GR graph will not be fully connected.</S>
    <S sid="85" ssid="45">The HMM tagger has been trained on 3M words of balanced text drawn from the LOB, BNC and Susanne corpora, which are available with handcorrected CLAWS tags.</S>
    <S sid="86" ssid="46">The parser has been trained from 1.9K trees for sentences from Susanne that were interactively parsed to manually obtain the correct derivation, and also from 2.1K further sentences with unlabelled bracketings derived from the Susanne treebank.</S>
    <S sid="87" ssid="47">These bracketings guide the parser to one or possibly several closely-matching derivations and these are used to derive probabilities for the LR table using (weighted) Laplace estimation.</S>
    <S sid="88" ssid="48">Actions in the table involving rules marked as peripheral are assigned a uniform low prior probability to ensure that derivations involving such rules are consistently lower ranked than those involving only core rules.</S>
    <S sid="89" ssid="49">To improve performance on WSJ text, we examined some parse failures from sections other than section 23 to identify patterns of consistent failure.</S>
    <S sid="90" ssid="50">We then manually modified and extended the grammar with a further 6 rules, mostly to handle cases of indirect and direct quotation that are very common in this dataset.</S>
    <S sid="91" ssid="51">This involved 3 days&#8217; work.</S>
    <S sid="92" ssid="52">Once completed, the parser was retrained on the original data.</S>
    <S sid="93" ssid="53">A subsequent limited inspection of top-ranked parses led us to disable 6 existing rules which applied too freely to the WSJ text; these were designed to analyse auxiliary ellipsis which appears to be rare in this genre.</S>
    <S sid="94" ssid="54">We also catalogued incorrect PoS tags from WSJ parse failures and manually modified the tagger lexicon where appropriate.</S>
    <S sid="95" ssid="55">These modifications mostly consisted of adjusting lexical probabilities of extant entries with highly-skewed distributions.</S>
    <S sid="96" ssid="56">We also added some tags to extant entries for infrequent words.</S>
    <S sid="97" ssid="57">These modifications took a further day.</S>
    <S sid="98" ssid="58">The tag transition probabilities were not reestimated.</S>
    <S sid="99" ssid="59">Thus, we have made no use of the PTB itself and only limited use of WSJ text.</S>
    <S sid="100" ssid="60">This method of grammar and lexicon development incrementally improves the overall performance of the system averaged across all the datasets that it has been applied to.</S>
    <S sid="101" ssid="61">It is very likely that retraining the PoS tagger on the WSJ and retraining the parser using PTB would yield a system which would perform more effectively on DepBank.</S>
    <S sid="102" ssid="62">However, one of our goals is to demonstrate that an unlexicalized parser trained on a modest amount of annotated text from other sources, coupled to a tagger also trained on generic, balanced data, can perform competitively with systems which have been (almost) entirely developed and trained using PTB, whether or not these systems deploy hand-crafted grammars or ones derived automatically from treebanks.</S>
  </SECTION>
  <SECTION title="3 Extending and Validating DepBank" number="3">
    <S sid="103" ssid="1">DepBank was constructed by parsing the selected section 23 WSJ sentences with the XLE system and outputting syntactic features and bilexical relations from the F-structure found by the parser.</S>
    <S sid="104" ssid="2">These features and relations were subsequently checked, corrected and extended interactively with the aid of software tools (King et al., 2003).</S>
    <S sid="105" ssid="3">The choice of relations and features is based quite closely on LFG and, in fact, overlaps substantially with the GR output of our parser.</S>
    <S sid="106" ssid="4">Figure 1 illustrates some DepBank annotations used in the experiment reported by Kaplan et al. and our hand-corrected GR output for the example Ten of the nation&#8217;s governors meanwhile called on the justices to reject efforts to limit abortions.</S>
    <S sid="107" ssid="5">We have kept the GR representation simpler and more readable by suppressing lemmatization, token numbering and PoS tags, but have left the DepBank annotations unmodified.</S>
    <S sid="108" ssid="6">The example illustrates some differences between the schemes.</S>
    <S sid="109" ssid="7">For instance, the subj and ncsubj relations overlap as both annotations contain such a relation between call(ed) and Ten), but the GR annotation also includes this relation between limit and effort(s) and reject and justice(s), while DepBank links these two verbs to a variable pro.</S>
    <S sid="110" ssid="8">This reflects a difference of philosophy about resolution of such &#8216;understood&#8217; relations in different constructions.</S>
    <S sid="111" ssid="9">Viewed as output appropriate to specific applications, either approach is justifiable.</S>
    <S sid="112" ssid="10">However, for evaluation, these DepBank relations add little or no information not already specified by the xcomp relations in which these verbs also appear as dependents.</S>
    <S sid="113" ssid="11">On the other hand, DepBank includes an adjunct relation between meanwhile and call(ed), while the GR annotation treats meanwhile as a text adjunct (ta) of governors, delimited by balanced commas, following Nunberg&#8217;s (1990) text grammar but conveying less information here.</S>
    <S sid="114" ssid="12">There are also issues of incompatible tokenization and lemmatization between the systems and of differing syntactic annotation of similar information, which lead to problems mapping between our GR output and the current DepBank.</S>
    <S sid="115" ssid="13">Finally, differences in the linguistic intuitions of the annotators and errors of commission or omission on both sides can only be uncovered by manual comparison of output (e.g. xmod vs. xcomp for limit efforts above).</S>
    <S sid="116" ssid="14">Thus we reannotated the DepBank sentences with GRs using our current system, and then corrected and extended this annotation utilizing a software tool to highlight differences between the extant annotations and our own.2 This exercise, though time-consuming, uncovered problems in both annotations, and yields a doubly-annotated and potentially more valuable resource in which annotation disagreements over complex attachment decisions, for instance, can be inspected.</S>
    <S sid="117" ssid="15">The GR scheme includes one feature in DepBank (passive), several splits of relations in DepBank, such as adjunct, adds some of DepBank&#8217;s featural information, such as subord form, as a subtype slot of a relation (ccomp), merges DepBank&#8217;s oblique with iobj, and so forth.</S>
    <S sid="118" ssid="16">But it does not explicitly include all the features of DepBank or even of the reduced set of semanticallyrelevant features used in the experiments and evaluation reported in Kaplan et al..</S>
    <S sid="119" ssid="17">Most of these features can be computed from the full GR representation of bilexical relations between numbered lemma-affix-tags output by the parser.</S>
    <S sid="120" ssid="18">For instance, num features, such as the plurality of justices in the example, can be computed from the full det GR (det justice+s NN2:4 the AT:3) based on the CLAWS tag (NN2 indicating &#8216;plural&#8217;) selected for output.</S>
    <S sid="121" ssid="19">The few features that cannot be computed from GRs and CLAWS tags directly, such as stmt type, could be computed from the derivation tree.</S>
  </SECTION>
  <SECTION title="4 Experiments" number="4">
    <S sid="122" ssid="1">We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see &#167;2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.</S>
    <S sid="123" ssid="2">The results we report below are derived by choosing the most probable tag for each word returned by the PoS tagger and by choosing the unweighted GR set returned for the most probable parse with no lexical information guiding parse ranking.</S>
  </SECTION>
  <SECTION title="4.2 Results" number="5">
    <S sid="124" ssid="1">Our parser produced rooted sentential analyses for 84% of the test items; actual coverage is higher croaverages.</S>
    <S sid="125" ssid="2">The macroaverage is calculated by taking the average of each measure for each individual relation and feature; the microaverage measures are calculated from the counts for all relations and features.4 Indentation of GRs shows degree of specificity of the relation.</S>
    <S sid="126" ssid="3">Thus, mod scores are microaveraged over the counts for the five fully specified modifier relations listed immediately after it in Table 1.</S>
    <S sid="127" ssid="4">This allows comparison of overall accuracy on modifiers with, for instance overall accuracy on arguments.</S>
    <S sid="128" ssid="5">Figures in italics to the right are discussed in the next section.</S>
    <S sid="129" ssid="6">Kaplan et al.&#8217;s microaveraged scores for Collins&#8217; Model 3 and the cut-down and complete versions of the XLE parser are given in Table 2, along with the microaveraged scores for our parser from Table 1.</S>
    <S sid="130" ssid="7">Our system&#8217;s accuracy results (evaluated on the reannotated DepBank) are better than those for Collins and the cut-down XLE, and very similar overall to the complete XLE (evaluated on DepBank).</S>
    <S sid="131" ssid="8">Speed of processing is also very competitive.5 These results demonstrate that a statistical parser with roughly state-of-the-art accuracy can be constructed without the need for large in-domain treebanks.</S>
    <S sid="132" ssid="9">However, the performance of the system, as measured by microraveraged F1-score on GR extraction alone, has declined by 2.7% over the held-out Susanne data, so even the unlexicalized parser is by no means domain-independent. than this since some of the test sentences are elliptical or fragmentary, but in many cases are recognized as single complete constituents.</S>
    <S sid="133" ssid="10">Kaplan et al. report that the complete XLE system finds rooted analyses for 79% of section 23 of the WSJ but do not report coverage just for the test sentences.</S>
    <S sid="134" ssid="11">The XLE parser uses several performance optimizations which mean that processing of subanalyses in longer sentences can be curtailed or preempted, so that it is not clear what proportion of the remaining data is outside grammatical coverage.</S>
    <S sid="135" ssid="12">Table 1 shows accuracy results for each individual relation and feature, starting with the GR bilexical relations in the extended DepBank and followed by most DepBank features reported by Kaplan et al., and finally overall macro- and miThe DepBank num feature on nouns is evaluated by Kaplan et al. on the grounds that it is semantically-relevant for applications.</S>
    <S sid="136" ssid="13">There are over 5K num features in DepBank so the overall microaveraged scores for a system will be significantly affected by accuracy on num.</S>
    <S sid="137" ssid="14">We expected our system, which incorporates a tagger with good empirical (97.1%) accuracy on the test data, to recover this feature with 95% accuracy or better, as it will correlate with tags NNx1 and NNx2 (where &#8216;x&#8217; represents zero or more capitals in the CLAWS tagset).</S>
    <S sid="138" ssid="15">However, DepBank treats the majority of prenominal modifiers as adjectives rather than nouns and, therefore, associates them with an adegree rather than a num feature.</S>
    <S sid="139" ssid="16">The PoS tag selected depends primarily on the relative lexical probabilities of each tag for a given lexical item recorded in the tagger lexicon.</S>
    <S sid="140" ssid="17">But, regardless of this lexical decision, the correct GR is recovered, and neither adegree(positive) or num(sg) add anything semantically-relevant when the lexical item is a nominal premodifier.</S>
    <S sid="141" ssid="18">A strategy which only provided a num feature for nominal heads would be both more semantically-relevant and would also yield higher precision (95.2%).</S>
    <S sid="142" ssid="19">However, recall (48.4%) then suffers against DepBank as noun premodifiers have a num feature.</S>
    <S sid="143" ssid="20">Therefore, in the results presented in Table 1 we have not counted cases where either DepBank or our system assign a premodifier adegree(positive) or num(sg).</S>
    <S sid="144" ssid="21">There are similar issues with other DepBank features and relations.</S>
    <S sid="145" ssid="22">For instance, the form of a subordinator with clausal complements is annotated as a relation between verb and subordinator, while there is a separate comp relation between verb and complement head.</S>
    <S sid="146" ssid="23">The GR representation adds the subordinator as a subtype of ccomp recording essentially identical information in a single relation.</S>
    <S sid="147" ssid="24">So evaluation scores based on aggregated counts of correct decisions will be doubled for a system which structures this information as in DepBank.</S>
    <S sid="148" ssid="25">However, reproducing the exact DepBank subord form relation from the GR ccomp one is non-trivial because DepBank treats modal auxiliaries as syntactic heads while the GRscheme treats the main verb as head in all ccomp relations.</S>
    <S sid="149" ssid="26">We have not attempted to compensate for any further such discrepancies other than the one discussed in the previous paragraph.</S>
    <S sid="150" ssid="27">However, we do believe that they collectively damage scores for our system.</S>
    <S sid="151" ssid="28">As King et al. note, it is difficult to identify such informational redundancies to avoid doublecounting and to eradicate all system specific biases.</S>
    <S sid="152" ssid="29">However, reporting precision, recall and F1scores for each relation and feature separately and microaveraging these scores on the basis of a hierarchy, as in our GR scheme, ameliorates many of these problems and gives a better indication of the strengths and weaknesses of a particular parser, which may also be useful in a decision about its usefulness for a specific application.</S>
    <S sid="153" ssid="30">Unfortunately, Kaplan et al. do not report their results broken down by relation or feature so it is not possible, for example, on the basis of the arguments made above, to choose to compare the performance of our system on ccomp to theirs for comp, ignoring subord form.</S>
    <S sid="154" ssid="31">King et al. do report individual results for selected features and relations from an evaluation of the complete XLE parser on all 700 DepBank sentences with an almost identical overall microaveraged F1 score of 79.5%, suggesting that these results provide a reasonably accurate idea of the XLE parser&#8217;s relative performance on different features and relations.</S>
    <S sid="155" ssid="32">Where we believe that the information captured by a DepBank feature or relation is roughly comparable to that expressed by a GR in our extended DepBank, we have included King et al.&#8217;s scores in the rightmost column in Table 1 for comparison purposes.</S>
    <S sid="156" ssid="33">Even if these features and relations were drawn from the same experiment, however, they would still not be exactly comparable.</S>
    <S sid="157" ssid="34">For instance, as discussed in &#167;3 nearly half (just over 1K) the DepBank subj relations include pro as one element, mostly double counting a corresponding xcomp relation.</S>
    <S sid="158" ssid="35">On the other hand, our ta relation syntactically underspecifies many DepBank adjunct relations.</S>
    <S sid="159" ssid="36">Nevertheless, it is possible to see, for instance, that while both parsers perform badly on second objects ours is worse, presumably because of lack of lexical subcategorization information.</S>
  </SECTION>
  <SECTION title="5 Conclusions" number="6">
    <S sid="160" ssid="1">We have demonstrated that an unlexicalized parser with minimal manual modification for WSJ text &#8211; but no tuning of performance to optimize on this dataset alone, and no use of PTB &#8211; can achieve accuracy competitive with parsers employing lexicalized statistical models trained on PTB.</S>
    <S sid="161" ssid="2">We speculate that we achieve these results because our system is engineered to make minimal use of lexical information both in the grammar and in parse ranking, because the grammar has been developed to constrain ambiguity despite this lack of lexical information, and because we can compute the full packed parse forest for all the test sentences efficiently (without sacrificing speed of processing with respect to other statistical parsers).</S>
    <S sid="162" ssid="3">These advantages appear to effectively offset the disadvantage of relying on a coarser, purely structural model for probabilistic parse selection.</S>
    <S sid="163" ssid="4">In future work, we hope to improve the accuracy of the system by adding lexical information to the statistical parse selection component without exploiting in-domain treebanks.</S>
    <S sid="164" ssid="5">Clearly, more work is needed to enable more accurate, informative, objective and wider comparison of extant parsers.</S>
    <S sid="165" ssid="6">More recent PTB-based parsers show small improvements over Collins&#8217; Model 3 using PARSEVAL, while Clark and Curran (2004) and Miyao and Tsujii (2005) report 84% and 86.7% F1-scores respectively for their own relational evaluations on section 23 of WSJ.</S>
    <S sid="166" ssid="7">However, it is impossible to meaningfully compare these results to those reported here.</S>
    <S sid="167" ssid="8">The reannotated DepBank potentially supports evaluations which score according to the degree of agreement between this and the original annotation and/or development of future consensual versions through collaborative reannotation by the research community.</S>
    <S sid="168" ssid="9">We have also highlighted difficulties for relational evaluation schemes and argued that presenting individual scores for (classes of) relations and features is both more informative and facilitates system comparisons.</S>
  </SECTION>
</PAPER>
