<PAPER>
	<S sid="0">Concept Discovery From Text</S><ABSTRACT>
		<S sid="1" ssid="1">Broad-coverage lexical resources such as WordNet are extremely useful.</S>
		<S sid="2" ssid="2">However, they often include many rare senses while missing domain-specific senses.</S>
		<S sid="3" ssid="3">We present a clustering algorithm called CBC (Cluster ing By Committee) that automatically discovers concepts from text.</S>
		<S sid="4" ssid="4">It initially discovers a set of tight clusters called committees that are well scattered in the similarity space.</S>
		<S sid="5" ssid="5">The centroid of the members of a committee is used as the feature vector of the cluster.</S>
		<S sid="6" ssid="6">We proceed by assigning elements to their most similar cluster.</S>
		<S sid="7" ssid="7">Evaluating cluster quality has always been a difficult task.</S>
		<S sid="8" ssid="8">We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key).</S>
		<S sid="9" ssid="9">Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="10" ssid="10">Broad-coverage lexical resources such as WordNet are extremely useful in applications such as Word Sense Disambiguation (Leacock, Chodorow, Miller 1998) and Question Answering (Pasca and Harabagiu 2001).</S>
			<S sid="11" ssid="11">However, they often include many rare senses while missing domain-specific senses.</S>
			<S sid="12" ssid="12">For example, in WordNet, the words dog, computer and company all have a sense that is a hyponym of person.</S>
			<S sid="13" ssid="13">Such rare senses make it difficult for a coreference resolution system to use WordNet to enforce the constraint that personal pronouns (e.g. he or she) must refer to a person.</S>
			<S sid="14" ssid="14">On the other hand, WordNet misses the user-interface object sense of the word dialog (as often used in software manuals).</S>
			<S sid="15" ssid="15">One way to deal with these problems is to use a clustering algorithm to automatically induce semantic classes (Lin and Pantel 2001).</S>
			<S sid="16" ssid="16">Many clustering algorithms represent a cluster by the centroid of all of its members (e.g., K means) (McQueen 1967) or by a representative element (e.g., K-medoids) (Kaufmann and Rousseeuw 1987).</S>
			<S sid="17" ssid="17">When averaging over all elements in a cluster, the centroid of a cluster may be unduly influenced by elements that only marginally belong to the cluster or by elements that also belong to other clusters.</S>
			<S sid="18" ssid="18">For example, when clustering words, we can use the contexts of the words as features and group together the words that tend to appear in similar contexts.</S>
			<S sid="19" ssid="19">For instance, U.S. state names can be clustered this way because they tend to appear in the following contexts: (List A) ___ appellate court campaign in ___ ___ capital governor of ___ ___ driver's license illegal in ___ ___ outlaws sth.</S>
			<S sid="20" ssid="20">primary in ___ ___'s sales tax senator for ___ If we create a centroid of all the state names, the centroid will also contain features such as: (List B) ___'s airport archbishop of ___ ___'s business district fly to ___ ___'s mayor mayor of ___ ___'s subway outskirts of ___ because some of the state names (like New York and Washington) are also names of cities.</S>
			<S sid="21" ssid="21">Using a single representative from a cluster may be problematic too because each individual element has its own idiosyncrasies that may not be shared by other members of the cluster.</S>
			<S sid="22" ssid="22">In this paper, we propose a clustering algo rithm, CBC (Clustering By Committee), in which the centroid of a cluster is constructed by averaging the feature vectors of a subset of the cluster members.</S>
			<S sid="23" ssid="23">The subset is viewed as a committee that determines which other elements belong to the cluster.</S>
			<S sid="24" ssid="24">By carefully choosing committee members, the features of the centroid tend to be the more typical features of the target class.</S>
			<S sid="25" ssid="25">For example, our system chose the following committee members to compute the centroid of the state cluster: Illinois, Michigan, Minnesota, Iowa, Wisconsin, Indiana, Nebraska and Vermont.</S>
			<S sid="26" ssid="26">As a result, the centroid contains only features like those in List A. Evaluating clustering results is a very difficult task.</S>
			<S sid="27" ssid="27">We introduce a new evaluation methodol ogy that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key).</S>
	</SECTION>
	<SECTION title="Previous Work. " number="2">
			<S sid="28" ssid="1">Clustering algorithms are generally categorized as hierarchical and partitional.</S>
			<S sid="29" ssid="2">In hierarchical agglomerative algorithms, clusters are constructed by iteratively merging the most similar clusters.</S>
			<S sid="30" ssid="3">These algorithms differ in how they compute cluster similarity.</S>
			<S sid="31" ssid="4">In single-link clustering, the similarity between two clusters is the similarity between their most similar members while complete-link clustering uses the similarity between their least similar members.</S>
			<S sid="32" ssid="5">Average-link clustering computes this similarity as the average similarity between all pairs of elements across clusters.</S>
			<S sid="33" ssid="6">The complexity of these algorithms is O(n2logn), where n is the number of elements to be clustered (Jain, Murty, Flynn 1999).</S>
			<S sid="34" ssid="7">Chameleon is a hierarchical algorithm that employs dynamic modeling to improve clustering quality (Karypis, Han, Kumar 1999).</S>
			<S sid="35" ssid="8">When merging two clusters, one might consider the sum of the similarities between pairs of elements across the clusters (e.g. average-link clustering).</S>
			<S sid="36" ssid="9">A drawback of this approach is that the existence of a single pair of very similar elements might unduly cause the merger of two clusters.</S>
			<S sid="37" ssid="10">An alternative considers the number of pairs of elements whose similarity exceeds a certain threshold (Guha, Rastogi, Kyuseok 1998).</S>
			<S sid="38" ssid="11">However, this may cause undesirable mergers when there are a large number of pairs whose similarities barely exceed the threshold.</S>
			<S sid="39" ssid="12">Chameleon clustering combines the two approaches.</S>
			<S sid="40" ssid="13">K-means clustering is often used on large data sets since its complexity is linear in n, the number of elements to be clustered.</S>
			<S sid="41" ssid="14">K-means is a family of partitional clustering algorithms that iteratively assigns each element to one of K clusters according to the centroid closest to it and recomputes the centroid of each cluster as the average of the cluster?s elements.</S>
			<S sid="42" ssid="15">K-means has complexity O(K?T?n) and is efficient for many clustering tasks.</S>
			<S sid="43" ssid="16">Because the initial centroids are randomly selected, the resulting clusters vary in quality.</S>
			<S sid="44" ssid="17">Some sets of initial centroids lead to poor convergence rates or poor cluster quality.</S>
			<S sid="45" ssid="18">Bisecting K-means (Steinbach, Karypis, Kumar 2000), a variation of K-means, begins with a set containing one large cluster consisting of every element and iteratively picks the largest cluster in the set, splits it into two clusters and replaces it by the split clusters.</S>
			<S sid="46" ssid="19">Splitting a cluster consists of applying the basic K-means algorithm ? times with K=2 and keeping the split that has the highest average element centroid similarity.</S>
			<S sid="47" ssid="20">Hybrid clustering algorithms combine hierarchical and partitional algorithms in an attempt to have the high quality of hierarchical algorithms with the efficiency of partitional algorithms.</S>
			<S sid="48" ssid="21">Buckshot (Cutting, Karger, Pedersen, Tukey 1992) addresses the problem of randomly selecting initial centroids in K-means by combining it with average-link clustering.</S>
			<S sid="49" ssid="22">Buckshot first applies average-link to a random sample of n elements to generate K clusters.</S>
			<S sid="50" ssid="23">It then uses the centroids of the clusters as the initial K centroids of K-means clustering.</S>
			<S sid="51" ssid="24">The sample size counterbalances the quadratic running time of average-link to make Buckshot efficient: O(K?T?n + nlogn).</S>
			<S sid="52" ssid="25">The parameters K and T are usually considered to be small numbers.</S>
	</SECTION>
	<SECTION title="Word Similarity. " number="3">
			<S sid="53" ssid="1">Following (Lin 1998), we represent each word by a feature vector.</S>
			<S sid="54" ssid="2">Each feature corresponds to a context in which the word occurs.</S>
			<S sid="55" ssid="3">For example, ?threaten with __?</S>
			<S sid="56" ssid="4">is a context.</S>
			<S sid="57" ssid="5">If the word handgun occurred in this context, the context is a feature of handgun.</S>
			<S sid="58" ssid="6">The value of the feature is the pointwise mutual information (Manning and Sch?tze 1999 p.178) between the feature and the word.</S>
			<S sid="59" ssid="7">Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information between c and w is defined as: ( ) ( ) ( ) N jF N wF N wF cw j c i i c mi ???</S>
			<S sid="60" ssid="8">=, where N = ( )??</S>
			<S sid="61" ssid="9">i j i jF is the total frequency counts of all words and their contexts.</S>
			<S sid="62" ssid="10">A well known problem with mutual information is that it is biased towards infrequent words/features.</S>
			<S sid="63" ssid="11">We therefore multiplied miw,c with a discounting factor: ( ) ( ) ( ) ( ) ( ) ( ) 11 +???</S>
			<S sid="64" ssid="12">i j ci i j ci c c jF,wFmin jF,wFmin wF wF We compute the similarity between two words wi and wj using the cosine coefficient (Salton and McGill 1983) of their mutual information vectors: ( ) ??</S>
			<S sid="65" ssid="13">= c cw c cw c cwcw ji ji ji mimi mimi w,wsim 22</S>
	</SECTION>
	<SECTION title="CBC Algorithm. " number="4">
			<S sid="66" ssid="1">CBC consists of three phases.</S>
			<S sid="67" ssid="2">In Phase I, we compute each element?s top-k similar elements.</S>
			<S sid="68" ssid="3">In our experiments, we used k = 20.</S>
			<S sid="69" ssid="4">In Phase II, we construct a collection of tight clusters, where the elements of each cluster form a committee.</S>
			<S sid="70" ssid="5">The algorithm tries to form as many committees as possible on the condition that each newly formed committee is not very similar to any existing committee.</S>
			<S sid="71" ssid="6">If the condition is violated, the committee is simply discarded.</S>
			<S sid="72" ssid="7">In the final phase of the algorithm, each element is assigned to its most similar cluster.</S>
			<S sid="73" ssid="8">4.1.</S>
			<S sid="74" ssid="9">Phase I: Find top-similar elements.</S>
			<S sid="75" ssid="10">Computing the complete similarity matrix between pairs of elements is obviously quadratic.</S>
			<S sid="76" ssid="11">However, one can dramatically reduce the running time by taking advantage of the fact that the feature vector is sparse.</S>
			<S sid="77" ssid="12">By indexing the features, one can retrieve the set of elements that have a given feature.</S>
			<S sid="78" ssid="13">To compute the top similar words of a word w, we first sort w?s features according to their mutual information with w. We only compute pairwise similarities between w and the words that share a high mutual information feature with w. 4.2.</S>
			<S sid="79" ssid="14">Phase II: Find committees.</S>
			<S sid="80" ssid="15">The second phase of the clustering algorithm recursively finds tight clusters scattered in the similarity space.</S>
			<S sid="81" ssid="16">In each recursive step, the algorithm finds a set of tight clusters, called committees, and identifies residue elements that are not covered by any committee.</S>
			<S sid="82" ssid="17">We say a committee covers an element if the element?s similarity to the centroid of the committee exceeds some high similarity threshold.</S>
			<S sid="83" ssid="18">The algorithm then recursively attempts to find more committees among the residue elements.</S>
			<S sid="84" ssid="19">The output of the algorithm is the union of all committees found in each recursive step.</S>
			<S sid="85" ssid="20">The details of Phase II are presented in Figure 1.</S>
			<S sid="86" ssid="21">In Step 1, the score reflects a preference for bigger and tighter clusters.</S>
			<S sid="87" ssid="22">Step 2 gives preference to higher quality clusters in Step 3, where a cluster is only kept if its similarity to all previously kept clusters is below a fixed threshold.</S>
			<S sid="88" ssid="23">In our experiments, we set ?1 = 0.35.</S>
			<S sid="89" ssid="24">Input: A list of elements E to be clustered, a similarity database S from Phase I, thresh olds ?1 and ?2.</S>
			<S sid="90" ssid="25">Step 1: For each element e ? E Cluster the top similar elements of e from S using average-link clustering.</S>
			<S sid="91" ssid="26">For each cluster discovered c compute the following score: |c| ? avgsim(c), where |c| is the number of elements in c and avgsim(c) is the average pairwise simi larity between elements in c. Store the highest-scoring cluster in a list L. Step 2: Sort the clusters in L in descending order of their scores.</S>
			<S sid="92" ssid="27">Step 3: Let C be a list of committees, initially empty.</S>
			<S sid="93" ssid="28">For each cluster c ? L in sorted order Compute the centroid of c by averaging the frequency vectors of its elements and computing the mutual information vector of the centroid in the same way as we did for individual elements.</S>
			<S sid="94" ssid="29">If c?s similarity to the centroid of each committee previously added to C is be low a threshold ?1, add c to C. Step 4: If C is empty, we are done and return C. Step 5: For each element e ? E If e?s similarity to every committee in C is below threshold ?2, add e to a list of resi dues R. Step 6: If R is empty, we are done and return C. Otherwise, return the union of C and the output of a recursive call to Phase II us ing the same input except replacing E with R. Output: A list of committees.</S>
			<S sid="95" ssid="30">Figure 1.</S>
			<S sid="96" ssid="31">Phase II of CBC.</S>
			<S sid="97" ssid="32">Step 4 terminates the recursion if no committee is found in the previous step.</S>
			<S sid="98" ssid="33">The residue elements are identified in Step 5 and if no residues are found, the algorithm terminates; otherwise, we recursively apply the algorithm to the residue elements.</S>
			<S sid="99" ssid="34">Each committee that is discovered in this phase defines one of the final output clusters of the algorithm.</S>
			<S sid="100" ssid="35">4.3.</S>
			<S sid="101" ssid="36">Phase III: Assign elements to clusters.</S>
			<S sid="102" ssid="37">In Phase III, every element is assigned to the cluster containing the committee to which it is most similar.</S>
			<S sid="103" ssid="38">This phase resembles K-means in that every element is assigned to its closest centroid.</S>
			<S sid="104" ssid="39">Unlike K-means, the number of clusters is not fixed and the centroids do not change (i.e. when an element is added to a cluster, it is not added to the committee of the cluster).</S>
	</SECTION>
	<SECTION title="Evaluation Methodology. " number="5">
			<S sid="105" ssid="1">Many cluster evaluation schemes have been proposed.</S>
			<S sid="106" ssid="2">They generally fall under two categories: ? comparing cluster outputs with manually generated answer keys (hereon referred to as classes); or ? embedding the clusters in an application and using its evaluation measure.</S>
			<S sid="107" ssid="3">An example of the first approach considers the average entropy of the clusters, which measures the purity of the clusters (Steinbach, Karypis, and Kumar 2000).</S>
			<S sid="108" ssid="4">However, maximum purity is trivially achieved when each element forms its own cluster.</S>
			<S sid="109" ssid="5">An example of the second approach evaluates the clusters by using them to smooth probability distributions (Lee and Pereira 1999).</S>
			<S sid="110" ssid="6">Like the entropy scheme, we assume that there is an answer key that defines how the elements are supposed to be clustered.</S>
			<S sid="111" ssid="7">Let C be a set of clusters and A be the answer key.</S>
			<S sid="112" ssid="8">We define the editing distance, dist(C, A), as the number of operations required to make C consistent with A. We say that C is consistent with A if there is a one to one mapping between clusters in C and the classes in A such that for each cluster c in C, all elements of c belong to the same class in A. We allow two editing operations: ? merge two clusters; and ? move an element from one cluster to another.</S>
			<S sid="113" ssid="9">Let B be the baseline clustering where each element is its own cluster.</S>
			<S sid="114" ssid="10">We define the quality of a set of clusters C as follows: ( ) ( )ABdist ACdist , ,1?</S>
			<S sid="115" ssid="11">Suppose the goal is to construct a clustering consistent with the answer key.</S>
			<S sid="116" ssid="12">This measure can be interpreted as the percentage of operations saved by starting from C versus starting from the baseline.</S>
			<S sid="117" ssid="13">We aim to construct a clustering consistent with A as opposed to a clustering identical to A because some senses in A may not exist in the corpus used to generate C. In our experiments, we extract answer classes from WordNet.</S>
			<S sid="118" ssid="14">The word dog belongs to both the Person and Animal classes.</S>
			<S sid="119" ssid="15">However, in the newspaper corpus, the Person sense of dog is at best extremely rare.</S>
			<S sid="120" ssid="16">There is no reason to expect a clustering algorithm to discover this sense of dog.</S>
			<S sid="121" ssid="17">The baseline distance dist(B, A) is exactly the number of elements to be clustered.</S>
			<S sid="122" ssid="18">We made the assumption that each element belongs to exactly one cluster.</S>
			<S sid="123" ssid="19">The transforma tion procedure is as follows: 1.</S>
			<S sid="124" ssid="20">Suppose there are m classes in the answer.</S>
			<S sid="125" ssid="21">key.</S>
			<S sid="126" ssid="22">We start with a list of m empty sets, each of which is labeled with a class in the answer key.</S>
			<S sid="127" ssid="23">2.</S>
			<S sid="128" ssid="24">For each cluster, merge it with the set.</S>
			<S sid="129" ssid="25">whose class has the largest number of elements in the cluster (a tie is broken arbitrarily).</S>
			<S sid="130" ssid="26">3.</S>
			<S sid="131" ssid="27">If an element is in a set whose class is not.</S>
			<S sid="132" ssid="28">the same as one of the element?s classes, move the element to a set where it be longs.</S>
			<S sid="133" ssid="29">dist(C, A) is the number of operations performed using the above transformation rules on C. a b e c d e a c d b e b a c d e a b c d e A) B) C) D) E) Figure 2.</S>
			<S sid="134" ssid="30">An example of applying the transformation rules to three clusters.</S>
			<S sid="135" ssid="31">A) The classes in the answer key; B) the clusters to be transformed; C) the sets used to reconstruct the classes (Rule 1); D) the sets after three merge operations (Step 2); E) the sets after one move operation (Step 3).</S>
			<S sid="136" ssid="32">Figure 2 shows an example.</S>
			<S sid="137" ssid="33">In D) the cluster containing e could have been merged with either set (we arbitrarily chose the second).</S>
			<S sid="138" ssid="34">The total number of operations is 4.</S>
	</SECTION>
	<SECTION title="Experimental Results. " number="6">
			<S sid="139" ssid="1">We generated clusters from a news corpus using CBC and compared them with classes extracted from WordNet (Miller 1990).</S>
			<S sid="140" ssid="2">6.1.</S>
			<S sid="141" ssid="3">Test Data.</S>
			<S sid="142" ssid="4">To extract classes from WordNet, we first estimate the probability of a random word belonging to a subhierarchy (a synset and its hyponyms).</S>
			<S sid="143" ssid="5">We use the frequency counts of synsets in the SemCor corpus (Landes, Leacock, Tengi 1998) to estimate the probability of a subhierarchy.</S>
			<S sid="144" ssid="6">Since SemCor is a fairly small corpus, the frequency counts of the synsets in the lower part of the WordNet hierarchy are very sparse.</S>
			<S sid="145" ssid="7">We smooth the probabilities by assuming that all siblings are equally likely given the parent.</S>
			<S sid="146" ssid="8">A class is then defined as the maximal subhierarchy with probability less than a threshold (we used e-2).</S>
			<S sid="147" ssid="9">We used Minipar 1 (Lin 1994), a broad coverage English parser, to parse about 1GB (144M words) of newspaper text from the TREC collection (1988 AP Newswire, 1989-90 LA Times, and 1991 San Jose Mercury) at a speed of about 500 words/second on a PIII-750 with 512MB memory.</S>
			<S sid="148" ssid="10">We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information values from Section 3.</S>
			<S sid="149" ssid="11">The test set is constructed by intersecting the words in WordNet with the nouns in the corpus whose total mutual information with all of its contexts exceeds a threshold m. Since WordNet has a low coverage of proper names, we removed all capitalized nouns.</S>
			<S sid="150" ssid="12">We constructed two test sets: S13403 consisting of 13403 words (m = 250) and S3566 consisting of 3566 words (m = 3500).</S>
			<S sid="151" ssid="13">We then removed from the answer classes the words that did not occur in the test sets.</S>
			<S sid="152" ssid="14">Table 1 summa rizes the test sets.</S>
			<S sid="153" ssid="15">The sizes of the WordNet classes vary a lot.</S>
			<S sid="154" ssid="16">For S13403 there are 99 classes that contain three words or less and the largest class contains 3246 words.</S>
			<S sid="155" ssid="17">For S3566, 78 classes have three or less words and the largest class contains 1181 words.</S>
			<S sid="156" ssid="18">1Available at www.cs.ualberta.ca/~lindek/minipar.htm.</S>
			<S sid="157" ssid="19">6.2.</S>
			<S sid="158" ssid="20">Cluster Evaluation.</S>
			<S sid="159" ssid="21">We clustered the test sets using CBC and the clustering algorithms of Section 2 and applied the evaluation methodology from the previous section.</S>
			<S sid="160" ssid="22">Table 2 shows the results.</S>
			<S sid="161" ssid="23">The columns are our editing distance based evaluation measure.</S>
			<S sid="162" ssid="24">Test set S3566 has a higher score for all algorithms because it has a higher number of average features per word than S13403.</S>
			<S sid="163" ssid="25">For the K-means and Buckshot algorithms, we set the number of clusters to 250 and the maximum number of iterations to 8.</S>
			<S sid="164" ssid="26">We used a sample size of 2000 for Buckshot.</S>
			<S sid="165" ssid="27">For the Bisecting K-means algorithm, we applied the basic K-means algorithm twice (?</S>
			<S sid="166" ssid="28">= 2 in Section 2) with a maximum of 8 iterations per split.</S>
			<S sid="167" ssid="29">Our implementation of Chameleon was unable to complete clustering S13403 in reasonable time due to its time complexity.</S>
			<S sid="168" ssid="30">Table 2 shows that K-means, Buckshot and Average-link have very similar performance.</S>
			<S sid="169" ssid="31">CBC outperforms all other algorithms on both data sets.</S>
			<S sid="170" ssid="32">6.3.</S>
			<S sid="171" ssid="33">Manual Inspection.</S>
			<S sid="172" ssid="34">Let c be a cluster and wn(c) be the WordNet class that has the largest intersection with c. The precision of c is defined as: Table 1.</S>
			<S sid="173" ssid="35">A description of the test sets in our experiments.</S>
			<S sid="174" ssid="36">DATA SET TOTAL WORDS m Average # of Features TOTAL CLASSES S13403 13403 250 740.8 202 S3566 3566 3500 2218.3 150 DATA SET TOTAL WORDS M Avg.</S>
			<S sid="175" ssid="37">Features per Word 13403 250 740.8 3566 3500 2218.3 Table 2.</S>
			<S sid="176" ssid="38">Cluster quality (%) of several clustering algorithms on the test sets.</S>
			<S sid="177" ssid="39">ALGORITHM S13403 S3566 CBC 60.95 65.82 K-means (K=250) 56.70 62.48 Buckshot 56.26 63.15 Bisecting K-means 43.44 61.10 Chameleon n/a 60.82 Average-link 56.26 62.62 Complete-link 49.80 60.29 Single-link 20.00 31.74 ( ) ( ) c cwnc cprecision ?= CBC discovered 943 clusters.</S>
			<S sid="178" ssid="40">We sorted them according to their precision.</S>
			<S sid="179" ssid="41">Table 3 shows five of the clusters evenly distributed according to their precision ranking along with their Top-15 features with highest mutual-information.</S>
			<S sid="180" ssid="42">The words in the clusters are listed in descending order of their similarity to the cluster centroid.</S>
			<S sid="181" ssid="43">For each cluster c, we also include wn(c).</S>
			<S sid="182" ssid="44">The underlined words are in wn(c).</S>
			<S sid="183" ssid="45">The first cluster is clearly a cluster of firearms and the second one is of pests.</S>
			<S sid="184" ssid="46">In WordNet, the word pest is curiously only under the person hierarchy.</S>
			<S sid="185" ssid="47">The words stopwatch and houseplant do not belong to the clusters but they have low similarity to their cluster centroid.</S>
			<S sid="186" ssid="48">The third cluster represents some kind of control.</S>
			<S sid="187" ssid="49">In WordNet, the legal power sense of jurisdiction is not a hyponym of social control as are supervision, oversight and governance.</S>
			<S sid="188" ssid="50">The fourth cluster is about mixtures.</S>
			<S sid="189" ssid="51">The words blend and mix as the event of mixing are present in WordNet but not as the result of mixing.</S>
			<S sid="190" ssid="52">The last cluster is about consumers.</S>
			<S sid="191" ssid="53">Here is the consumer class in WordNet 1.5: addict, alcoholic, big spender, buyer, client, concert-goer, consumer, customer, cutter, diner, drinker, drug addict, drug user, drunk, eater, feeder, fungi, head, heroin addict, home buyer, junkie, junky, lush, nonsmoker, patron, policy holder, purchaser, reader, regular, shopper, smoker, spender, subscriber, sucker, taker, user, vegetarian, wearer In our cluster, only the word client belongs to WordNet?s consumer class.</S>
			<S sid="192" ssid="54">The cluster is ranked very low because WordNet failed to consider words like patient, tenant and renter as consumers.</S>
			<S sid="193" ssid="55">Table 3 shows that even the lowest ranking CBC clusters are fairly coherent.</S>
			<S sid="194" ssid="56">The features associated with each cluster can be used to classify previously unseen words into one or more existing clusters.</S>
			<S sid="195" ssid="57">Table 4 shows the clusters containing the word cell that are discovered by various clustering algorithms from S13403.</S>
			<S sid="196" ssid="58">The underlined words represent the words that belong to the cell class in WordNet.</S>
			<S sid="197" ssid="59">The CBC cluster corresponds almost exactly to WordNet?s cell class.</S>
			<S sid="198" ssid="60">K-means and Buckshot produced fairly coherent clusters.</S>
			<S sid="199" ssid="61">The cluster constructed by Bisecting K-means is obviously of inferior quality.</S>
			<S sid="200" ssid="62">This is consistent with the fact that Bisecting K-means has a much lower score on S13403 compared to CBC, K means and Buckshot.</S>
			<S sid="201" ssid="63">Table 3.</S>
			<S sid="202" ssid="64">Five of the 943 clusters discovered by CBC from S13403 along with their features with top-15 highest mutual information and the WordNet classes that have the largest intersection with each cluster.</S>
			<S sid="203" ssid="65">RANK MEMBERS TOP-15 FEATURES wn(c) 1 handgun, revolver, shotgun, pistol, rifle, machine gun, sawed-off shotgun, submachine gun, gun, automatic pistol, automatic rifle, firearm, carbine, ammunition, magnum, cartridge, automatic, stopwatch __ blast, barrel of __ , brandish __, fire __, point __, pull out __, __ discharge, __ fire, __ go off, arm with __, fire with __, kill with __, open fire with __, shoot with __, threaten with __ artifact / artifact 236 whitefly, pest, aphid, fruit fly, termite, mosquito, cockroach, flea, beetle, killer bee, maggot, predator, mite, houseplant, cricket __ control, __ infestation, __ larvae, __ population, infestation of __, specie of __, swarm of __ , attract __, breed __, eat __, eradicate __, feed on __, get rid of __, repel __, ward off __ animal / animate being / beast / brute / creature / fauna 471 supervision, discipline, oversight, control, governance, decision making, jurisdiction breakdown in __, lack of __ , loss of __, assume __, exercise __, exert __, maintain __, retain __, seize __, tighten __, bring under __, operate under __, place under __, put under __, remain under __ act / human action / human activity 706 blend, mix, mixture, combination, juxtaposition, combine, amalgam, sprinkle, synthesis, hybrid, melange dip in __, marinate in __, pour in __, stir in __, use in __, add to __, pour __, stir __, curious __, eclectic __, ethnic __, odd __, potent __, unique __, unusual __ group / grouping 941 employee, client, patient, applicant, tenant, individual, participant, renter, volunteer, recipient, caller, internee, enrollee, giver benefit for __, care for __, housing for __, benefit to __, service to __, filed by __, paid by __, use by __, provide for __, require for --, give to __, offer to __, provide to __, disgruntled __, indigent __ worker</S>
	</SECTION>
	<SECTION title="Conclusion. " number="7">
			<S sid="204" ssid="1">We presented a clustering algorithm, CBC, for automatically discovering concepts from text.</S>
			<S sid="205" ssid="2">It can handle a large number of elements, a large number of output clusters, and a large sparse feature space.</S>
			<S sid="206" ssid="3">It discovers clusters using well scattered tight clusters called committees.</S>
			<S sid="207" ssid="4">In our experiments, we showed that CBC outperforms several well known hierarchical, partitional, and hybrid clustering algorithms in cluster quality.</S>
			<S sid="208" ssid="5">For example, in one experiment, CBC outperforms K-means by 4.25%.</S>
			<S sid="209" ssid="6">By comparing the CBC clusters with WordNet classes, we not only find errors in CBC, but also oversights in WordNet.</S>
			<S sid="210" ssid="7">Evaluating cluster quality has always been a difficult task.</S>
			<S sid="211" ssid="8">We presented a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key).</S>
			<S sid="212" ssid="9">Acknowledgements The authors wish to thank the reviewers for their helpful comments.</S>
			<S sid="213" ssid="10">This research was partly supported by Natural Sciences and Engineering Research Council of Canada grant OGP121338 and scholarship PGSB207797.</S>
	</SECTION>
</PAPER>
