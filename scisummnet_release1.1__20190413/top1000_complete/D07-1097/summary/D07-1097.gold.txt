Single Malt or Blended? A Study in Multilingual Parser Optimization
We describe a two-stage optimization of the MaltParser system for the ten languages in the multilingual track of the CoNLL 2007 shared task on dependency parsing.
The first stage consists in tuning a single-parser system for each language by optimizing parameters of the parsing algorithm, the feature model, and the learning algorithm.
The second stage consists in building an ensemble system that combines six different parsing strategies, extrapolating from the optimal parameters settings for each language.
When evaluated on the official test sets, the ensemble system significantly outperforms the single-parser system and achieves the highest average labeled attachment score.
We extend the two-stage approach to a three-stage architecture where the parser and labeler generate an n-best list of parses which in turn is reranked.
We point out that the official results for Chinese contained a bug, and the true performance of our system is actually much higher.
We implement a left-to-right arc-eager parsing model in a way that the parser scan through an input sequence from left to right and the right dependents are attached to their heads as soon as possible.
