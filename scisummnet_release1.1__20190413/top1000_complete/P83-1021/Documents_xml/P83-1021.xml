<PAPER>
  <S sid="0">Parsing As Deduction</S>
  <ABSTRACT>
    <S sid="1" ssid="1">By exploring the relationship between parsing and deduction, a new and more general view of chart parsing is obtained, which encompasses parsing for grammar formalisms based on unification, and is the basis of the Earley Deduction proof procedure for definite clauses. efficiency of this approach for an interesting class grammars is discussed.</S>
  </ABSTRACT>
  <SECTION title="1." number="1">
    <S sid="2" ssid="1">The aim of this paper is to explore the relationship between parsing and deduction.</S>
    <S sid="3" ssid="2">The basic notion, which goes back to Kowalski (Kowalski, 1980) and Colmerauer (Colmerauer, 1978), has seen a very efficient, if limited, realization in the use of the logic programming language Prolog for parsing (Colmerauer, 1978; Pereira and Warren, 1980).</S>
    <S sid="4" ssid="3">The connection between parsing and deduction was developed further in the design of the Earley Deduction proof procedure (Warren, 1975), which will also be discussed at length here.</S>
    <S sid="5" ssid="4">Investigation of the connection between parsing and deduction yields several important benefits: (LFG).</S>
    <S sid="6" ssid="5">Our study of these topics is still far from complete; therefore, besides offering some initial results, we shall discuss various outstanding questions.</S>
    <S sid="7" ssid="6">The connection between parsing and deduction is based on the axiomatization of context-free grammars in definite clauses, a particularly simple subset of firstorder logic (Kowalski, 1980; van Emden and Kowalski, 1976).</S>
    <S sid="8" ssid="7">This axiomatization allows us to identify contextfree parsing algorithms with proof procedures for a restricted class of definite clauses, those derived from context-free rules.</S>
    <S sid="9" ssid="8">This identification can then be generalized to include larger classes of definite clauses to which the same algorithms can be applied, with simple modifications.</S>
    <S sid="10" ssid="9">Those larger classes of definite clauses can be seen as grammar formalisms in which the atomic grammar symbols of context-free grammars have been replaced by complex symbols that are matched by unification (Robinson, 1965; Colmerauer, 1978; Pereira and Warren, 1980).</S>
    <S sid="11" ssid="10">The simplest of these formalisms is definite-clause grammars (DCG) (Pereira and Warren, 1980).</S>
    <S sid="12" ssid="11">There is a close relationship between DCGs and other grammar formalisms based on unification. such as Unification Grammar (UG) (Kay, 1979), LFG, PATR-2 (Shieber.</S>
    <S sid="13" ssid="12">1983) and the more recent versions of GPSG (Gazdar and Pullum, 1982).</S>
    <S sid="14" ssid="13">The parsing algorithms we are concerned with are online algorithms, in the sense that they apply the constraints specified by the augmentation of a rule as soon as the rule is applied.</S>
    <S sid="15" ssid="14">In contrast, an online parsing algorithm will consist of two phases: a context-free parsing algorithm followed by application of the constraints to all the resulting analyses.</S>
    <S sid="16" ssid="15">The paper is organized as follows.</S>
    <S sid="17" ssid="16">Section 2 gives an overview of the concepts of definite clause logic, definite clause grammars, definite clause proof procedures, and chart parsing.</S>
    <S sid="18" ssid="17">Section 3 discusses the connection betwee DCGs and LFG.</S>
    <S sid="19" ssid="18">Section 4 describes the Earley Deduction definite-clause proof procedure.</S>
    <S sid="20" ssid="19">Section 5 then brings out the connection between Earley Deduction and chart parsing, and shows the added generality brought in by the proof procedure approach.</S>
    <S sid="21" ssid="20">Section 6 outlines some of the problems of implementing Earley Deduction and similar parsing procedures.</S>
    <S sid="22" ssid="21">Finally, Section 7 discusses questions of computational complexity and decidability.</S>
  </SECTION>
  <SECTION title="2." number="2">
    <S sid="23" ssid="1">A definite clause has the form P and Q1, , Qn are literals.</S>
    <S sid="24" ssid="2">P is the positive literal or head of the clause; Q1, , Qn are the negative literals, forming the body of the clause.</S>
    <S sid="25" ssid="3">Literals have the forth p(11,...4), where p is the predicate of arity k and the ti the arguments.</S>
    <S sid="26" ssid="4">The arguments are terms.</S>
    <S sid="27" ssid="5">A term may be: a variable (variable names start with capital letters); a constant; a compound term where f is a functor of arity m and the ti are terms.</S>
    <S sid="28" ssid="6">All the variables in a clause are implicitly universally quantified.</S>
    <S sid="29" ssid="7">A set of definite clauses forms a program, and the clauses in a program are called input clauses.</S>
    <S sid="30" ssid="8">A program defines the relations denoted by the predicates appearing in the heads of clauses.</S>
    <S sid="31" ssid="9">When using a definiteclause proof procedure, such as Prolog (Roussel.</S>
    <S sid="32" ssid="10">1975), a goal statement x-ai.&amp;quot;an can be translated into a definite clause x(So,Sn) 1(S0,S1) &amp; The variables Si are the string arguments, representing positions in the input string.</S>
    <S sid="33" ssid="11">For example, the context-free rule &amp;quot;S NP VP&amp;quot; is translated into &amp;quot;s(S0,S2) np(SO.S1) vp(S1,S2),&amp;quot; which can be paraphrased as -there is an S from SO to S2 in the input string if there is an NP from SO to Si and a VP from SI to S2.&amp;quot; Given the translation of a context-free grammar G with start symbol S into a set of definite clauses G' with corresponding predicate s, to say that a string w is in the grammar's language is equivalent to saying that the start goal s(po,p) is a consequence of G U W, where 1)0 and p represent the left. and right endpoints of w, and W is a set of unit clauses that represents w. It is easy to generalize the above notions to define DCGs.</S>
    <S sid="34" ssid="12">DCG nonterminals have arguments in the same way that predicates do.</S>
    <S sid="35" ssid="13">A DCG nonterminal with n arguments is translated into a predicate of n+2 arguments, the last two of which are the string points, as in the translation of context-free rules into definite clauses.</S>
    <S sid="36" ssid="14">The context-free grammar obtained from a DCG by dropping all nonterminal arguments is the contextfree skeleton of the DCG.</S>
    <S sid="37" ssid="15">The fundamental inference rule for definite clauses is the following resolution rule: From the clauses The proof procedure of Prolog is just a particular embedding of the resolution rule in a search procedure, in which a goal clause like (2) is successively rewritten by the resolution rule usii:g clauses from the program (I).</S>
    <S sid="38" ssid="16">The Prolog proof procedure can be implemented very efficiently. but it has the same theoretical problems of the top-down backtrack parsing algorithms after which it is modeled.</S>
    <S sid="39" ssid="17">These problems do not preclude its use for creating uniquely efficient parsers for suitably constructed grammars (Warren and Pereira, 1983: Pereira, 1982), but the broader questions of the relation between parsing and deduction and of the derivation of online parsing algorithms for unification formalisms require that we look at a more generally applicable class of proof procedures.</S>
    <S sid="40" ssid="18">Chart parsing is a general framework for constructing parsing algorithms for context-free grammars and related formalisms.</S>
    <S sid="41" ssid="19">The Earley context-free parsing algorithm. although independently developed, can be seen as a particular case of chart parsing.</S>
    <S sid="42" ssid="20">We will give here just the basic terminology of chart parsing and of the Earley algorithm.</S>
    <S sid="43" ssid="21">Full accounts can be found in the articles by Kay (Kay.</S>
    <S sid="44" ssid="22">1080) and Earley (Earley, 1970).</S>
    <S sid="45" ssid="23">The state of a chart parser is represented by the chart, which is a directed graph.</S>
    <S sid="46" ssid="24">The nodes of the chart represent positions in the string being analyzed.</S>
    <S sid="47" ssid="25">Each edge in (lie chart. is either active or passive.</S>
    <S sid="48" ssid="26">Both types of edges are labeled.</S>
    <S sid="49" ssid="27">A passive edge with label N links node r to node .9 if the string between r and s has been analyzed as a phrase of type N. Initially, the only edges are passive edges that link consecutive nodes and are labeled with I he words of the input string (see Figure 1).</S>
    <S sid="50" ssid="28">Active edges represent partially applied grammar rules.</S>
    <S sid="51" ssid="29">In the simplest. case, active edges are labeled by dotted rules.</S>
    <S sid="52" ssid="30">A dotted rule is a grammar rule with a dot inserted somewhere on its right-hand side (4) An edge with this label links node r to node s if the sentential form a/ ... is an analysis of the input string between r and s. An active edge that links a node to itself is called empty and acts like a top-down prediction.</S>
    <S sid="53" ssid="31">Chart-parsing procedures start with a chart containing the passive edges for the input string.</S>
    <S sid="54" ssid="32">New edges are added in two distinct ways.</S>
    <S sid="55" ssid="33">First, an active edge from r to .9 labeled with a dotted rule (41) combines with a passive edge from s to t with label cgi to produce a new edge from r to t, which will be a passive edge with label X if cri is the last symbol in the right-hand side of the dotted rule; otherwise it will be an active edge with the dot advanced over cri.</S>
    <S sid="56" ssid="34">Second, the parsing strategy must place into the chart, at appropriate points, new empty active edges that will be used to combine existing passive edges.</S>
    <S sid="57" ssid="35">The exact method used determines whether the parsing method is seen as top-down, bottom-up, or a combination of the two, The Earley parsing algorithm can be seen as a special case of chart parsing in which new empty active edges are introduced top-down and, for all k, the edge combinations involving only the first k nodes are done before any combinations that involve later nodes.</S>
    <S sid="58" ssid="36">This particular strategy allows certain simplifications to be made in the general algorithm.</S>
  </SECTION>
  <SECTION title="3." number="3">
    <S sid="59" ssid="1">We would like to make a few informal observations at this point to clarify the relationship between DCGs and other unification grammar formalisms &#8212; LFG in particular.</S>
    <S sid="60" ssid="2">A more detailed discussion would take us beyond the intended scope of this paper.</S>
    <S sid="61" ssid="3">The different noiational conventions of DCGs and LFG make the two formalisms less similar on the surface than they actually are from the computational point of view.</S>
    <S sid="62" ssid="4">The objects that appear as arguments in DCG rules are tree fragments every node of which has a number of children predetermined by the functor that labels the node.</S>
    <S sid="63" ssid="5">Explicit variables mark unspecified parts of the tree.</S>
    <S sid="64" ssid="6">In contrast, the functional structure nodes that are implicitly mentioned in LFG equations do not have a predefined number of children, and unspecified parts are either omitted or defined implicitly through equations.</S>
    <S sid="65" ssid="7">As a first approximation, a DCG rule such as The DCG rule can be read as &amp;quot;an s with structure / \ Subj Obj is an up with structure Subj followed by a vp with structure Obj.&#8226; The LFG rule can be read as 'an S is an NP followed by a VP, where the value of the subj attribute of the S is the functional structure of the NP and the value of the attribute obj of the S is the functional structure of the VP.'</S>
    <S sid="66" ssid="8">For those familiar with the details of the mapping from functional descriptions to functional structures in LFG, DCG variables are just &amp;quot;placeholder&amp;quot; symbols (Bresnan and Kaplan, 1982).</S>
    <S sid="67" ssid="9">As we noted above, an apparent difference between LFG and DCGs is that LFG functional structure nodes, unlike DCG function symbols, do not have a definite number of children.</S>
    <S sid="68" ssid="10">Although we must leave to a separate paper the details of the application to LFG of the unification algorithms from theorem proving, we will note here that the formal properties of logical and LFG or UG unification are similar, and there are adaptations to LFG and UG of the algorithms and data structures used in the logical case.</S>
  </SECTION>
  <SECTION title="4." number="4">
    <S sid="69" ssid="1">The Earley Deduction proof procedure schema is named after Earley's context-free parsing algorithm (Earley, 1970), on which it is based.</S>
    <S sid="70" ssid="2">Earley Deduction provides for definite clauses the same kind of mixed top-down bottom-up mechanism that the Earley parsing algorithm provides for context-free grammars.</S>
    <S sid="71" ssid="3">Earley Deduction operates on two sets of definite clauses called the program and the state.</S>
    <S sid="72" ssid="4">The program is just the set of input clauses and remains fixed.</S>
    <S sid="73" ssid="5">The state consists of a set of derived clauses, where each nonunit dause has one of its negative literals selected; the state is continually being added to.</S>
    <S sid="74" ssid="6">Whenever a nonunit clause is added to the state, one of its negative literals is selected.</S>
    <S sid="75" ssid="7">Initially the state contains just the goal statement (with one of its negative literals selected).</S>
    <S sid="76" ssid="8">There are two inference rules, called instantiation and reduction, which can map the current state into a new one by adding a new derived clause.</S>
    <S sid="77" ssid="9">For an instantiation step, there is some clause in the current state whose selected literal unifies with the positive literal of a nonunit clause C in the program.</S>
    <S sid="78" ssid="10">In this case, the derived clause is arj, where o. is a most general unifier (Robinson, 1965) of the two literals concerned.</S>
    <S sid="79" ssid="11">The selected literal is said to instantiate C to or].</S>
    <S sid="80" ssid="12">For a reduction step, there is some clause C in the current state whose selected literal unifies with a unit clause from either the program or the current state.</S>
    <S sid="81" ssid="13">In this case, the derived clause is a(C1, where a is a most general unifier of the two literals concerned, and C' is C' minus its selected literal.</S>
    <S sid="82" ssid="14">Thus, the deriyed clause is just the resolvent of C with the unit clause and the latter is said to reduce C to a(C.I.</S>
    <S sid="83" ssid="15">Before a derived clause is added to the state, a check is made to see whether the derived clause is subsumed by any clause already in the state.</S>
    <S sid="84" ssid="16">If the derived clause is subsumed, it is not added to the state, and that inference step is said to be blocked.</S>
    <S sid="85" ssid="17">In the examples that follow, we assume that the selected literal in a derived clause is always the leftmost literal in the body.</S>
    <S sid="86" ssid="18">This choice is not optimal (Kowalski, 1980), but it is sufficient for our purposes.</S>
    <S sid="87" ssid="19">For example, given the program At this point, all further steps are blocked, so the computation terminates.</S>
    <S sid="88" ssid="20">Earley Deduction generalizes Earley parsing in a direct and natural way.</S>
    <S sid="89" ssid="21">Instantiation is analogous to the &amp;quot;predictor&amp;quot; operation of Earley's algorithm, while reduction corresponds to the &amp;quot;scanner&amp;quot; and &amp;quot;completer&amp;quot; operations.</S>
    <S sid="90" ssid="22">The &amp;quot;scanner&amp;quot; operation amounts to reduction with an input unit clause representing a terminal symbol occurrence, while the &amp;quot;completer&amp;quot; operation amounts to reduction with a derived unit clause representing a nonterminal symbol occurrence.</S>
  </SECTION>
  <SECTION title="5." number="5">
    <S sid="91" ssid="1">Chart parsing (Kay, 1980) and other tabular parsing algorithms (Aho and Ullman, 1972; Graham et al., 1980) are usually presented in terms of certain (abstract) data structures that keep a record of the alternatives being explored by the parser.</S>
    <S sid="92" ssid="2">Looking at parsing procedures as proof procedures has the following advantages: (i) unification, gaps and unbounded dependencies are automatically handled; (ii) parsing strategies become possible that cannot be formulated in chart parsing.</S>
    <S sid="93" ssid="3">The chart represents completed nonterminals (passive edges) and partially applied rules (active edges).</S>
    <S sid="94" ssid="4">From the standpoint of Earley Deduction, both represent derived clauses that have been proved in the course of an attempt to deduce a goal statement whose meaning is that a string belongs to the language generated by the grammar.</S>
    <S sid="95" ssid="5">An active edge corresponds to a noaunit clause, a passive edge to a unit clause.</S>
    <S sid="96" ssid="6">Nowhere in this definition is there mention of k.he &amp;quot;endpoints&amp;quot; of the edges.</S>
    <S sid="97" ssid="7">The endpoints correspond to certain literal arguments, and are of no concern to the (abstract) proof procedure.</S>
    <S sid="98" ssid="8">Endpoints are just a convenient way .of indexing derived clauses in an implementation to reduce the number of nonproductive (nonunifying) attempts at applying the reduction rule.</S>
    <S sid="99" ssid="9">We shall give now an example of the application of Earley Deduction to parsing, corresponding to the chart of Figure 1. corresponds to the following definite-clause program: The lexical categories of the sentence 0Agatha1's2husband3hit4Ulrich5 can be represented by the unit clauses n(0,1). g-en(1,2). n(2,3). n(4,5).</S>
    <S sid="100" ssid="10">Thus, the task of determining whether (26) is a sentence can be represented by the goal statement ans s(0.5).</S>
    <S sid="101" ssid="11">(:32) If the sentence is in the language, the unit clause ans will he derived in the course of an Earley Deduction proof.</S>
    <S sid="102" ssid="12">Such a proof could proceed as follows: 5(0,5). ans.</S>
    <S sid="103" ssid="13">Note how subsumption is used to curtail the left recursion of rules (21) and (22), by stopping extraneous instantiation steps from the derived clauses (35) and (36).</S>
    <S sid="104" ssid="14">As we have seen in the example of the previous section, this mechanism is a general one, capable of handling complex grammar symbols within certain constraints that will be discussed later.</S>
    <S sid="105" ssid="15">The Earley Deduction derivation given above corresponds directly to the chart in Figure 1.</S>
    <S sid="106" ssid="16">In general, chart parsing cannot support strategies that would create active edges by reducing the symbols in the right-hand side of a rule in any arbitrary order.</S>
    <S sid="107" ssid="17">This is because an active edge must correspond to a contiguous sequence of analyzed symbols.</S>
    <S sid="108" ssid="18">Definite clause proof procedures do not have this limitation.</S>
    <S sid="109" ssid="19">For example, it is very simple to define a strategy, &amp;quot;head word narsing&amp;quot;2 (McCord, 1080), which would use the reduction rule to Each arc in the chart is labeled with the number of a clause in the proof.</S>
    <S sid="110" ssid="20">In each clause that corresponds to a chart arc, two literal arguments correspond to the two endpoints of the arc.</S>
    <S sid="111" ssid="21">These arguments have been underlined in the derivation.</S>
    <S sid="112" ssid="22">Notice how the endpoint arguments are the two string arguments in the head for unit clauses (passive edges) but, in the case of nonunit clauses (passive edges), are the first string argument in the head and the first in the leftmost literal in the body.</S>
    <S sid="113" ssid="23">As we noted before, our view of parsing as deduction makes it possible to derive general parsing mechanisms for augmented phrase-structure grammars with gaps and unbounded dependencies.</S>
    <S sid="114" ssid="24">It is difficult (especially in the case of pure bottom-up parsing strategies) to augment chart parsers to handle gaps and dependencies (Thompson, 1981).</S>
    <S sid="115" ssid="25">However, if gaps and dependencies are specified by extra predicate arguments in the .clauses that correspond to the rules, the general proof procedures will handle those phenomena without further change.</S>
    <S sid="116" ssid="26">This is the technique used in DCGs and is the basis of the specialized extraposition grammar formalism (Pereira, 1081).</S>
    <S sid="117" ssid="27">The increased generality of our approach in the area of parsing strategy stems from the fact that chart parsing strategies correspond to specialized proof procedures for definite clauses with string arguments.</S>
    <S sid="118" ssid="28">In other words, the origin of these proof procedures means that string arguments are treated differently from other arguments, as they correspond to the chart nodes. from the clauses np(SO,S) det(SO,SI) &amp; n(SI,S2) &amp; rel(S2,S).</S>
    <S sid="119" ssid="29">[NP Det N n(2,3).</S>
    <S sid="120" ssid="30">[There is an N between points 2 and 3 in the input) This example shows that the class of parsing strategies allowed in the deductive approach is broader than what is possible in the chart parsing approach.</S>
    <S sid="121" ssid="31">It remains to be shown which of those strategies will have practical importance as well.</S>
  </SECTION>
  <SECTION title="8." number="6">
    <S sid="122" ssid="1">To implement Earley Deduction with an efficiency comparable, say, to Prolog, presents some challenging problems.</S>
    <S sid="123" ssid="2">The main issues are needed and space can be recovered.</S>
    <S sid="124" ssid="3">There are two basic methods for representing derived clauses in resolution systems: the more direct copying method, in which substitutions are applied explicitly; the structure-sharing method of Boyer and Moore, which avoids copying by representing derived clauses implicitly with the aid of variable binding environments.</S>
    <S sid="125" ssid="4">A promising strategy for Earley Deduction might be to use copying for derived unit clauses, structure sharing for other derived clauses.</S>
    <S sid="126" ssid="5">When copying, care should be taken not to copy variable-free subterms, but to copy just pointers to those subterms instead.</S>
    <S sid="127" ssid="6">It is very costly to implement subsumption in its full generality.</S>
    <S sid="128" ssid="7">To keep the cost within reasonable bounds, it will be essential to index the derived clauses on at least the predicate symbols they contain &#8212; and probably also, on symbols in certain key argument positions.</S>
    <S sid="129" ssid="8">A simplification of full subsumption checking that would appear adequate to block most redundant steps is to keep track of selected literals that have been used exhaustively to generate instantiation steps.</S>
    <S sid="130" ssid="9">If another selected literal is an instance of one that has been exhaustively explored, there is no need to consider using it as a candidate for instantiation steps.</S>
    <S sid="131" ssid="10">Subsumption would then be only applied to derived unit clauses.</S>
    <S sid="132" ssid="11">A major efficiency problem with Earley deduction is that it is difficult to recognize situations in which derived clauses are no longer needed and space can be reclaimed.</S>
    <S sid="133" ssid="12">There is a marked contrast with purely top-down proof procedures, such as Prolog, to which highly effective spare recovery techniques can be applied relatively easily.</S>
    <S sid="134" ssid="13">The Earley algorithm pursues all possible parses in parallel, indexed by string position.</S>
    <S sid="135" ssid="14">In principle, this permits space to be recovered, as parsing progresses, by deleting information relating to earlier string positions.</S>
    <S sid="136" ssid="15">It may be possible to generalize this technique to Earley Deduction, by recognizing, either automatically or manually, certain special properties of the input clauses.</S>
  </SECTION>
  <SECTION title="7." number="7">
    <S sid="137" ssid="1">It is not at. all obvious that grammar formalisms based on unification can be parsed within reasonable bounds of time and space.</S>
    <S sid="138" ssid="2">In fact, unrestricted DCGs have Turing machine power, and LFG, although decidable, seems capable of encoding exponentially hard problems.</S>
    <S sid="139" ssid="3">However, we need not give up our interest in the complexity analysis of unification-based parsing.</S>
    <S sid="140" ssid="4">Whether for interesting subclasses of, grammars or specific grammars of interest, it is still important to determine how efficient parsing can be.</S>
    <S sid="141" ssid="5">A basic step in that direction is to estimate the cost added by unification to the operation of combining (reducing or expanding) a nonterminal in a derivation with a nonterminal in a grammar rule.</S>
    <S sid="142" ssid="6">Because definite clauses are only semidecidable, general proof procedures may not terminate for some sets of definite clauses.</S>
    <S sid="143" ssid="7">However, the specialized proof procedures we have derived from parsing algorithms are stable: if a set of definite clauses G is the translation of a context-free grammar, the procedure will always terminate (in success or failure) when to proving any start goal for G. More interesting in this context is the notion of strong stability, which depends on the following notion or offline parsability.</S>
    <S sid="144" ssid="8">A DCG is offline-parsable if its context-free skeleton is not infinitely ambiguous.</S>
    <S sid="145" ssid="9">Using different terminology, Bresnan and Kaplan (Bresnan and Kaplan, 1982) have shown that the parsing problem for LFG is decidable because LFGs are offline parsable.</S>
    <S sid="146" ssid="10">This result can be adapted easily to DCGs, showing that the parsing problem for offline-parsable DCGs is decidable.</S>
    <S sid="147" ssid="11">Strong stability can now be defined: a parsing algorithm is strongly stable if it always terminates for offline-parsable grammars.</S>
    <S sid="148" ssid="12">For example, a direct DCG version of the Earley parsing algorithm is stable but not strongly so.</S>
    <S sid="149" ssid="13">In the following complexity arguments, we restrict ourselves to offline-parsable grammars.</S>
    <S sid="150" ssid="14">This is a reasonable restriction for two reasons: (i) since general DCGs have Turing machine power, there is no useful notion of computational complexity for the parser on its own; (ii) there are good reasons to believe that linguistically relevant grammars must be offline-parsable (Bresnan and Kaplan, 1982).</S>
    <S sid="151" ssid="15">In estimating the added complexity of doing online unification, we start from the fact that the length of any derivation of a terminal string in a finitely ambiguous context-free grammar is linearly bounded by the length of the terminal string.</S>
    <S sid="152" ssid="16">The proof of this fact is omitted for lack of space, but can be found elsewhere (Pereira and Warren.</S>
    <S sid="153" ssid="17">1983).</S>
    <S sid="154" ssid="18">General definite-clause proof procedures need to access the values of variables (bindings) in derived clauses.</S>
    <S sid="155" ssid="19">The structure-sharing method of representation makes the time to access a variable binding at worst linear in the length of the derivation.</S>
    <S sid="156" ssid="20">Furthermore, the number of variables to be looked up in a derivation step is at worst linear in the size of the derivation.</S>
    <S sid="157" ssid="21">Finally, the time (and space) to finish a derivation step, once all the relevant bindings are known, does not depend on the size of the derivation.</S>
    <S sid="158" ssid="22">Therefore, using this method for parsing offline-parsable grammars makes the time complexity of each step at worst o(n2) in the length of the input.</S>
    <S sid="159" ssid="23">Some simplifications are possible that improve that time bound.</S>
    <S sid="160" ssid="24">First, it is possible to use a value array representation of bindings (Boyer and Moore.</S>
    <S sid="161" ssid="25">1972) while exploring any given derivation path. reducing to a constant the variable lookup time at the cost of having to save and restore o(n) variable bindings from the value array each time the parsing procedure moves to explore a different derivation path.</S>
    <S sid="162" ssid="26">Secondly, the unification cost can be made independent of the derivation length, if we forgo the occurs check that prevents a variable from being bound to a term containing it.</S>
    <S sid="163" ssid="27">Finally, the combination of structure sharing and copying suggested in the last section eliminates the overhead of switching to a different derivation path in the value array method at the cost of a uniform o(log n) time to look up or create a variahIP binding in a balanced binary tree.</S>
    <S sid="164" ssid="28">When adding a new edge to the chart, a chart parser must verify that no edge with the same label between the same nodes is already present.</S>
    <S sid="165" ssid="29">In general DCG parsing (and therefore in online parsing with any unificationbased formalism), we cannot check for the &amp;quot;same label&amp;quot; (same lemma), because lemmas in general will contain variables.</S>
    <S sid="166" ssid="30">We must instead check for subsumption of the new lemma by some old lemma.</S>
    <S sid="167" ssid="31">The obvious subsumption checking mechanism has an o(n3) worst case cost, but the improved binding representations described above, together with the other special techniques mentioned in the previous section, can be used to reduce this cost in practice.</S>
    <S sid="168" ssid="32">We do not yet have a full complexity comparison between online and offline parsing, but it is easy to envisage situations in which the number of edges created by an online algorithm is much smaller than that for the corresponding offline algorithm, whereas the cost of applying the unification constraints is the same for both algorithms.</S>
  </SECTION>
  <SECTION title="8." number="8">
    <S sid="169" ssid="1">We have outlined an approach to the problems of parsing unification-based grammar formalisms that builds on the relationship between parsing and definite-clause deduction.</S>
    <S sid="170" ssid="2">Several theoretical and practical problems remain.</S>
    <S sid="171" ssid="3">Among these are the question of recognizing derived clauses that are no longer useful in Earley-style parsing, the design of restricted formalisms with a polynomial bound on the number of distinct derived clauses, and independent, characterizations of the classes of offlinepa rsable grammars and languages.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="9">
    <S sid="172" ssid="1">We would like to thank Barbara Grosz and Stan Rosenschein for their comments on earlier versions of this paper.</S>
  </SECTION>
</PAPER>
