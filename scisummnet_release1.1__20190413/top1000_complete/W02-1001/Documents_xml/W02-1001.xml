<PAPER>
	<S sid="0">Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms</S><ABSTRACT>
		<S sid="1" ssid="1">We describe new algorithms for train ing tagging models, as an alternativeto maximum-entropy models or conditional random elds (CRFs).</S>
		<S sid="2" ssid="2">The al gorithms rely on Viterbi decoding oftraining examples, combined with simple additive updates.</S>
		<S sid="3" ssid="3">We describe the ory justifying the algorithms througha modi cation of the proof of conver gence of the perceptron algorithm forclassi cation problems.</S>
		<S sid="4" ssid="4">We give experimental results on part-of-speech tag ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">Maximum-entropy (ME) models are justi ably a very popular choice for tagging problems in Natural Language Processing: for example see (Ratnaparkhi 96) for their use on part-of-speech tagging, and (McCallum et al 2000) for their use on a FAQ segmentation task.</S>
			<S sid="6" ssid="6">ME models have the advantage of being quite exible in the features that can be incorporated in the model.However, recent theoretical and experimental re sults in (Laerty et al 2001) have highlighted problems with the parameter estimation method for ME models.</S>
			<S sid="7" ssid="7">In response to these problems, they describe alternative parameter estimation methods based on Conditional Markov RandomFields (CRFs).</S>
			<S sid="8" ssid="8">(Laerty et al 2001) give experimental results suggesting that CRFs can per form signi cantly better than ME models.In this paper we describe parameter estima tion algorithms which are natural alternatives toCRFs.</S>
			<S sid="9" ssid="9">The algorithms are based on the percep tron algorithm (Rosenblatt 58), and the voted or averaged versions of the perceptron described in (Freund  Schapire 99).</S>
			<S sid="10" ssid="10">These algorithms have been shown by (Freund  Schapire 99) to be competitive with modern learning algorithms such as support vector machines; however, theyhave previously been applied mainly to classi cation tasks, and it is not entirely clear how the algorithms can be carried across to NLP tasks such as tagging or parsing.This paper describes variants of the perceptron algorithm for tagging problems.</S>
			<S sid="11" ssid="11">The al gorithms rely on Viterbi decoding of trainingexamples, combined with simple additive updates.</S>
			<S sid="12" ssid="12">We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.</S>
			<S sid="13" ssid="13">We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger (a11.9% relative reduction in error for POS tag ging, a 5.1% relative reduction in error for NP chunking).</S>
			<S sid="14" ssid="14">Although we concentrate on taggingproblems in this paper, the theoretical frame work and algorithm described in section 3 ofthis paper should be applicable to a wide va riety of models where Viterbi-style algorithmscan be used for decoding: examples are Proba bilistic Context-Free Grammars, or ME models for parsing.</S>
			<S sid="15" ssid="15">See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.</S>
			<S sid="16" ssid="16">1</S>
	</SECTION>
	<SECTION title="Parameter Estimation. " number="2">
			<S sid="17" ssid="1">2.1 HMM Taggers.</S>
			<S sid="18" ssid="2">In this section, as a motivating example, we de scribe a special case of the algorithm in thispaper: the algorithm applied to a trigram tag ger.</S>
			<S sid="19" ssid="3">In a trigram HMM tagger, each trigram 1The theorems in section 3, and the proofs in sec tion 5, apply directly to the work in these other papers.</S>
			<S sid="20" ssid="4">Association for Computational Linguistics.</S>
			<S sid="21" ssid="5">Language Processing (EMNLP), Philadelphia, July 2002, pp.</S>
			<S sid="22" ssid="6">1-8.</S>
			<S sid="23" ssid="7">Proceedings of the Conference on Empirical Methods in Natural of tags and each tag/word pair have associated parameters.</S>
			<S sid="24" ssid="8">We write the parameter associated with a trigram hx; y; zi as  x;y;z, and the param eter associated with a tag/word pair (t; w) as  t;w. A common approach is to take the param eters to be estimates of conditional probabilities:  x;y;z = logP (z j x; y),  t;w = logP (w j t).</S>
			<S sid="25" ssid="9">For convenience we will use w [1:n]as short hand for a sequence of words [w 1 ; w 2 : : : w n ], and t [1:n] as shorthand for a taq sequence [t 1 ; t 2 : : : t n ].</S>
			<S sid="26" ssid="10">In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i</S></SECTION></PAPER>
