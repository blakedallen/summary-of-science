A Discriminative Global Training Algorithm For Statistical MT
This paper presents a novel training algorithm for a linearly-scored block sequence translation model.
The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder.
No translation, language, or distortion model probabilities are used as in earlier work on SMT.
Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches.
Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme.
The training algorithm is evaluated on a standard Arabic-English translation task.
We use a BLEU oracle decoder for discriminative training of a local reordering model.
We use a perceptron style algorithm for training a large number of features.
We compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple, local reordering models.
We present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations.
