[
  {
    "citance_No": 1, 
    "citing_paper_id": "N07-1008", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Abraham, Ittycheriah | Salim, Roukos", 
    "raw_text": "Recently, there have been several discriminative approaches at training large parameter sets including (Tillmann and Zhang, 2006) and (Liang et al, 2006)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "N07-1008", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Abraham, Ittycheriah | Salim, Roukos", 
    "raw_text": "In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU", 
    "clean_text": "In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P13-1110", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Vladimir, Eidelman | Yuval, Marton | Philip, Resnik", 
    "raw_text": "selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact", 
    "clean_text": "It might be the case that a larger k-best, or revisiting previous strategies for y+ and y\u2212 selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P12-1002", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Patrick, Simianer | Stefan, Riezler | Chris, Dyer", 
    "raw_text": "(2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training", 
    "clean_text": "Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W07-0414", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Markus, Dreyer | Keith, Hall | Sanjeev P., Khudanpur", 
    "raw_text": "Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model", 
    "clean_text": "Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P08-1010", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yonggang, Deng | Jia, Xu | Yuqing, Gao", 
    "raw_text": "Thetranslation probability can also be discriminatively trained such as in Tillmann and Zhang (2006) .The focus of this paper is the phrase pair extraction problem", 
    "clean_text": "The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D07-1055", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Richard, Zens | Sa&scaron;a, Hasan | Hermann, Ney", 
    "raw_text": "Here, we will present additional evidence that MBR decoding is preferable over MAP decoding. Tillmann and Zhang (2006) describe a percep tron style algorithm for training millions of features. Here, we focus on the comparison of different training criteria", 
    "clean_text": "Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D07-1025", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Dayne, Freitag | Shahram, Khadivi", 
    "raw_text": "Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights", 
    "clean_text": "Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "D08-1024", 
    "citing_paper_authority": 74, 
    "citing_paper_authors": "David, Chiang | Yuval, Marton | Philip, Resnik", 
    "raw_text": "translation (Tillmann and Zhang, 2006), which Arun and Koehn (2007) call max-B??? updating", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D07-1080", 
    "citing_paper_authority": 51, 
    "citing_paper_authors": "Taro, Watanabe | Jun, Suzuki | Hajime, Tsukada | Hideki, Isozaki", 
    "raw_text": "Tillmann and Zhang (2006), Liang et al (2006) and Bangalore et al (2006) introduced sparse binary features for statistical machine translation trained ona large training corpus", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D07-1080", 
    "citing_paper_authority": 51, 
    "citing_paper_authors": "Taro, Watanabe | Jun, Suzuki | Hajime, Tsukada | Hideki, Isozaki", 
    "raw_text": "Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm", 
    "clean_text": "Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D07-1080", 
    "citing_paper_authority": 51, 
    "citing_paper_authors": "Taro, Watanabe | Jun, Suzuki | Hajime, Tsukada | Hideki, Isozaki", 
    "raw_text": "Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance", 
    "clean_text": "Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "D07-1080", 
    "citing_paper_authority": 51, 
    "citing_paper_authors": "Taro, Watanabe | Jun, Suzuki | Hajime, Tsukada | Hideki, Isozaki", 
    "raw_text": "Tillmann and Zhang (2006) and Liang et al (2006) solved the problem by introducing a sentence-wise BLEU", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W07-0716", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Nitin, Madnani | Necip Fazil, Ayan | Philip, Resnik | Bonnie Jean, Dorr", 
    "raw_text": "For instance, some max-margin methods restrict their computations to a set of examples from a? feasible set,? where they are expected to be maximally disc rim inative (Tillmann and Zhang, 2006)", 
    "clean_text": "For instance, some max-margin methods restrict their computations to a set of examples from a feasible set, where they are expected to be maximally discriminative (Tillmann and Zhang, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W07-0716", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Nitin, Madnani | Necip Fazil, Ayan | Philip, Resnik | Bonnie Jean, Dorr", 
    "raw_text": "This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006) .This has important implications for data acquisition strategies For example, it suggests that rather than obtaining four reference translations per sentence for development sets, it may be more worth while to obtain fewer translations for a wider range of sentences ,e.g., expanding into new topics and genres", 
    "clean_text": "This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D10-1091", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Guillaume, Wisniewski | Alexandre, Allauzen | Fran&ccedil;ois, Yvon", 
    "raw_text": "This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypo the ses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering model", 
    "clean_text": "This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D09-1039", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Daniel, Galron | Sergio, Penkale | Andy, Way | I. Dan, Melamed", 
    "raw_text": "TillmannandZhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations", 
    "clean_text": "Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W12-3160", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Vladimir, Eidelman", 
    "raw_text": "This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB)", 
    "clean_text": "This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB).", 
    "keep_for_gold": 0
  }
]
