<PAPER>
  <S sid="0">A Decision Tree Of Bigrams Is An Accurate Predictor Of Word Sense</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.</S>
    <S sid="2" ssid="2">This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.</S>
    <S sid="3" ssid="3">It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs.</S>
    <S sid="5" ssid="2">For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined.</S>
    <S sid="6" ssid="3">For example, suppose bill has the following set of possible meanings: a piece of currency, pending legislation, or a bird jaw.</S>
    <S sid="7" ssid="4">When used in the context of The Senate bill is under consideration, a human reader immediately understands that bill is being used in the legislative sense.</S>
    <S sid="8" ssid="5">However, a computer program attempting to perform the same task faces a difficult problem since it does not have the benefit of innate common&#8212;sense or linguistic knowledge.</S>
    <S sid="9" ssid="6">Rather than attempting to provide computer programs with real&#8212;world knowledge comparable to that of humans, natural language processing has turned to corpus&#8212;based methods.</S>
    <S sid="10" ssid="7">These approaches use techniques from statistics and machine learning to induce models of language usage from large samples of text.</S>
    <S sid="11" ssid="8">These models are trained to perform particular tasks, usually via supervised learning.</S>
    <S sid="12" ssid="9">This paper describes an approach where a decision tree is learned from some number of sentences where each instance of an ambiguous word has been manually annotated with a sense&#8212;tag that denotes the most appropriate sense for that context.</S>
    <S sid="13" ssid="10">Prior to learning, the sense&#8212;tagged corpus must be converted into a more regular form suitable for automatic processing.</S>
    <S sid="14" ssid="11">Each sense&#8212;tagged occurrence of an ambiguous word is converted into a feature vector, where each feature represents some property of the surrounding text that is considered to be relevant to the disambiguation process.</S>
    <S sid="15" ssid="12">Given the flexibility and complexity of human language, there is potentially an infinite set of features that could be utilized.</S>
    <S sid="16" ssid="13">However, in corpus&#8212;based approaches features usually consist of information that can be readily identified in the text, without relying on extensive external knowledge sources.</S>
    <S sid="17" ssid="14">These typically include the part&#8212;of&#8212;speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word.</S>
    <S sid="18" ssid="15">The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.</S>
    <S sid="19" ssid="16">The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated.</S>
    <S sid="20" ssid="17">We take this approach since surface lexical features like bigrams, collocations, and co&#8212;occurrences often contribute a great deal to disambiguation accuracy.</S>
    <S sid="21" ssid="18">It is not clear how much disambiguation accuracy is improved through the use of features that are identified by more complex pre&#8212;processing such as part&#8212;of&#8212;speech tagging, parsing, or anaphora resolution.</S>
    <S sid="22" ssid="19">One of our objectives is to establish a clear upper bounds on the accuracy of disambiguation using feature sets that do not impose substantial pre&#8212; processing requirements.</S>
    <S sid="23" ssid="20">This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning.</S>
    <S sid="24" ssid="21">Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison.</S>
    <S sid="25" ssid="22">The experimental data is discussed, and then the empirical results are presented.</S>
    <S sid="26" ssid="23">We close with an analysis of our findings and a discussion of related work.</S>
    <S sid="27" ssid="24">2 Building a Feature Set of Bigrams We have developed an approach to word sense disambiguation that represents text entirely in terms of the occurrence of bigrams, which we define to be two cat &#8212;cat totals big n11= 10 n12= 20 n1+= 30 &#8212;big n21= 40 n22= 930 n2+= 970 totals n+1=50 n+2=950 n++=1000 consecutive words that occur in a text.</S>
    <S sid="28" ssid="25">The distributional characteristics of bigrams are fairly consistent across corpora; a majority of them only occur one time.</S>
    <S sid="29" ssid="26">Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen.</S>
    <S sid="30" ssid="27">We explore two alternatives, the power divergence family of goodness of fit statistics and the Dice Coefficient, an information theoretic measure related to pointwise Mutual Information.</S>
    <S sid="31" ssid="28">Figure 1 summarizes the notation for word and bigram counts used in this paper by way of a 2 x 2 contingency table.</S>
    <S sid="32" ssid="29">The value of n11 shows how many times the bigram big cat occurs in the corpus.</S>
    <S sid="33" ssid="30">The value of n12 shows how often bigrams occur where big is the first word and cat is not the second.</S>
    <S sid="34" ssid="31">The counts in n+1 and n1+ indicate how often words big and cat occur as the first and second words of any bigram in the corpus.</S>
    <S sid="35" ssid="32">The total number of bigrams in the corpus is represented by n++.</S>
    <S sid="36" ssid="33">(Cressie and Read, 1984) introduce the power divergence family of goodness of fit statistics.</S>
    <S sid="37" ssid="34">A number of well known statistics belong to this family, including the likelihood ratio statistic G2 and Pearson's X2 statistic.</S>
    <S sid="38" ssid="35">These measure the divergence of the observed (nij) and expected (mij) bigram counts, where mij is estimated based on the assumption that the component words in the bigram occur together strictly by chance: data distributions.</S>
    <S sid="39" ssid="36">However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other.</S>
    <S sid="40" ssid="37">In light of this, (Pedersen, 1996) presents Fisher's exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson's test and the likelihood ratio.</S>
    <S sid="41" ssid="38">Unfortunately it is usually not clear which test is most appropriate for a particular sample of data.</S>
    <S sid="42" ssid="39">We take the following approach, based on the observation that all tests should assign approximately the same measure of statistical significance when the bigram counts in the contingency table do not violate any of the distributional assumptions that underly the goodness of fit statistics.</S>
    <S sid="43" ssid="40">We perform tests using X2, G2, and Fisher's exact test for each bigram.</S>
    <S sid="44" ssid="41">If the resulting measures of statistical significance differ, then the distribution of the bigram counts is causing at least one of the tests to become unreliable.</S>
    <S sid="45" ssid="42">When this occurs we rely upon the value from Fisher's exact test since it makes fewer assumptions about the underlying distribution of data.</S>
    <S sid="46" ssid="43">For the experiments in this paper, we identified the top 100 ranked bigrams that occur more than 5 times in the training corpus associated with a word.</S>
    <S sid="47" ssid="44">There were no cases where rankings produced by G2, X2, and Fisher's exact test disagreed, which is not altogether surprising given that low frequency bigrams were excluded.</S>
    <S sid="48" ssid="45">Since all of these statistics produced the same rankings, hereafter we make no distinction among them and simply refer to them generically as the power divergence statistic.</S>
    <S sid="49" ssid="46">The Dice Coefficient is a descriptive statistic that provides a measure of association among two words in a corpus.</S>
    <S sid="50" ssid="47">It is similar to pointwise Mutual Information, a widely used measure that was first introduced for identifying lexical relationships in (Church and Hanks, 1990).</S>
    <S sid="51" ssid="48">Pointwise Mutual Information can be defined as follows: (Dunning, 1993) argues in favor of G2 over X2, especially when dealing with very sparse and skewed where w1 and w2 represent the two words that make up the bigram.</S>
    <S sid="52" ssid="49">Pointwise Mutual Information quantifies how often two words occur together in a bigram (the numerator) relative to how often they occur overall in the corpus (the denominator).</S>
    <S sid="53" ssid="50">However, there is a curious limitation to pointwise Mutual Information.</S>
    <S sid="54" ssid="51">A bigram w1w2 that occurs n11 times in the corpus, and whose component words w1 and w2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n11 decreases.</S>
    <S sid="55" ssid="52">Thus, the maximum pointwise Mutual Information in a given corpus will be assigned to bigrams that occur one time, and whose component words never occur outside that bigram.</S>
    <S sid="56" ssid="53">These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information.</S>
    <S sid="57" ssid="54">The Dice Coefficient overcomes this limitation, and can be defined as follows: When n11 = n1+ = n+1 the value of Dice(w1, w2) will be 1 for all values n11.</S>
    <S sid="58" ssid="55">When the value of n11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Coefficient are similar to those of Mutual Information.</S>
    <S sid="59" ssid="56">The relationship between pointwise Mutual Information and the Dice Coefficient is also discussed in (Smadja et al., 1996).</S>
    <S sid="60" ssid="57">We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests.</S>
    <S sid="61" ssid="58">This software is written in Perl and is freely available from www.d.umn.edu/~tpederse.</S>
  </SECTION>
  <SECTION title="3 Learning Decision Trees" number="2">
    <S sid="62" ssid="1">Decision trees are among the most widely used machine learning algorithms.</S>
    <S sid="63" ssid="2">They perform a general to specific search of a feature space, adding the most informative features to a tree structure as the search proceeds.</S>
    <S sid="64" ssid="3">The objective is to select a minimal set of features that efficiently partitions the feature space into classes of observations and assemble them into a tree.</S>
    <S sid="65" ssid="4">In our case, the observations are manually sense&#8212;tagged examples of an ambiguous word in context and the partitions correspond to the different possible senses.</S>
    <S sid="66" ssid="5">Each feature selected during the search process is represented by a node in the learned decision tree.</S>
    <S sid="67" ssid="6">Each node represents a choice point between a number of different possible values for a feature.</S>
    <S sid="68" ssid="7">Learning continues until all the training examples are accounted for by the decision tree.</S>
    <S sid="69" ssid="8">In general, such a tree will be overly specific to the training data and not generalize well to new examples.</S>
    <S sid="70" ssid="9">Therefore learning is followed by a pruning step where some nodes are eliminated or reorganized to produce a tree that can generalize to new circumstances.</S>
    <S sid="71" ssid="10">Test instances are disambiguated by finding a path through the learned decision tree from the root to a leaf node that corresponds with the observed features.</S>
    <S sid="72" ssid="11">An instance of an ambiguous word is disambiguated by passing it through a series of tests, where each test asks if a particular bigram occurs in the available window of context.</S>
    <S sid="73" ssid="12">We also include three benchmark learning algorithms in this study: the majority classifier, the decision stump, and the Naive Bayesian classifier.</S>
    <S sid="74" ssid="13">The majority classifier assigns the most common sense in the training data to every instance in the test data.</S>
    <S sid="75" ssid="14">A decision stump is a one node decision tree(Holte, 1993) that is created by stopping the decision tree learner after the single most informative feature is added to the tree.</S>
    <S sid="76" ssid="15">The Naive Bayesian classifier (Duda and Hart, 1973) is based on certain blanket assumptions about the interactions among features in a corpus.</S>
    <S sid="77" ssid="16">There is no search of the feature space performed to build a representative model as is the case with decision trees.</S>
    <S sid="78" ssid="17">Instead, all features are included in the classifier and assumed to be relevant to the task at hand.</S>
    <S sid="79" ssid="18">There is a further assumption that each feature is conditionally independent of all other features, given the sense of the ambiguous word.</S>
    <S sid="80" ssid="19">It is most often used with a bag of words feature set, where every word in the training sample is represented by a binary feature that indicates whether or not it occurs in the window of context surrounding the ambiguous word.</S>
    <S sid="81" ssid="20">We use the Weka (Witten and Frank, 2000) implementations of the C4.5 decision tree learner (known as J48), the decision stump, and the Naive Bayesian classifier.</S>
    <S sid="82" ssid="21">Weka is written in Java and is freely available from www.cs.waikato.ac.nz/~ml.</S>
  </SECTION>
  <SECTION title="4 Experimental Data" number="3">
    <S sid="83" ssid="1">Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems.</S>
    <S sid="84" ssid="2">Ten teams participated in the supervised learning portion of this event.</S>
    <S sid="85" ssid="3">Additional details about the exercise, including the data and results referred to in this paper, can be found at the SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) and in (Kilgarriff and Palmer, 2000).</S>
    <S sid="86" ssid="4">We included all 36 tasks from SENSEVAL for which training and test data were provided.</S>
    <S sid="87" ssid="5">Each task requires that the occurrences of a particular word in the test data be disambiguated based on a model learned from the sense&#8212;tagged instances in the training data.</S>
    <S sid="88" ssid="6">Some words were used in multiple tasks as different parts of speech.</S>
    <S sid="89" ssid="7">For example, there were two tasks associated with bet, one for its use as a noun and the other as a verb.</S>
    <S sid="90" ssid="8">Thus, there are 36 tasks involving the disambiguation of 29 different words.</S>
    <S sid="91" ssid="9">The words and part of speech associated with each task are shown in Table 1 in column 1.</S>
    <S sid="92" ssid="10">Note that the parts of speech are encoded as n for noun, a for adjective, v for verb, and p for words where the part of speech was not provided.</S>
    <S sid="93" ssid="11">The number of test and training instances for each task are shown in columns 2 and 4.</S>
    <S sid="94" ssid="12">Each instance consists of the sentence in which the ambiguous word occurs as well 2 * n11 as one or two surrounding sentences.</S>
    <S sid="95" ssid="13">In general the total context available for each ambiguous word is less than 100 surrounding words.</S>
    <S sid="96" ssid="14">The number of distinct senses in the test data for each task is shown in column 3.</S>
  </SECTION>
  <SECTION title="5 Experimental Method" number="4">
    <S sid="97" ssid="1">The following process is repeated for each task.</S>
    <S sid="98" ssid="2">Capitalization and punctuation are removed from the training and test data.</S>
    <S sid="99" ssid="3">Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coefficient.</S>
    <S sid="100" ssid="4">The bigram must have occurred 5 or more times to be included as a feature.</S>
    <S sid="101" ssid="5">This step filters out a large number of possible bigrams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process.</S>
    <S sid="102" ssid="6">The training and test data are converted to feature vectors where each feature represents the occurrence of one of the bigrams that belong in the feature set.</S>
    <S sid="103" ssid="7">This representation of the training data is the actual input to the learning algorithms.</S>
    <S sid="104" ssid="8">Decision tree and decision stump learning is performed twice, once using the feature set determined by the power divergence statistic and again using the feature set identified by the Dice Coefficient.</S>
    <S sid="105" ssid="9">The majority classifier simply determines the most frequent sense in the training data and assigns that to all instances in the test data.</S>
    <S sid="106" ssid="10">The Naive Bayesian classifier is based on a feature set where every word that occurs 5 or more times in the training data is included as a feature.</S>
    <S sid="107" ssid="11">All of these learned models are used to disambiguate the test data.</S>
    <S sid="108" ssid="12">The test data is kept separate until this stage.</S>
    <S sid="109" ssid="13">We employ a fine grained scoring method, where a word is counted as correctly disambiguated only when the assigned sense tag exactly matches the true sense tag.</S>
    <S sid="110" ssid="14">No partial credit is assigned for near misses.</S>
  </SECTION>
  <SECTION title="6 Experimental Results" number="5">
    <S sid="111" ssid="1">The accuracy attained by each of the learning algorithms is shown in Table 1.</S>
    <S sid="112" ssid="2">Column 5 reports the accuracy of the majority classifier, columns 6 and 7 show the best and average accuracy reported by the 10 participating SENSEVAL teams.</S>
    <S sid="113" ssid="3">The evaluation at SENSEVAL was based on precision and recall, so we converted those scores to accuracy by taking their product.</S>
    <S sid="114" ssid="4">However, the best precision and recall may have come from different teams, so the best accuracy shown in column 6 may actually be higher than that of any single participating SENSEVAL system.</S>
    <S sid="115" ssid="5">The average accuracy in column 7 is the product of the average precision and recall reported for the participating SENSEVAL teams.</S>
    <S sid="116" ssid="6">Column 8 shows the accuracy of the decision tree using the J48 learning algorithm and the features identified by a power divergence statistic.</S>
    <S sid="117" ssid="7">Column 10 shows the accuracy of the decision tree when the Dice Coefficient selects the features.</S>
    <S sid="118" ssid="8">Columns 9 and 11 show the accuracy of the decision stump based on the power divergence statistic and the Dice Coefficient respectively.</S>
    <S sid="119" ssid="9">Finally, column 13 shows the accuracy of the Naive Bayesian classifier based on a bag of words feature set.</S>
    <S sid="120" ssid="10">The most accurate method is the decision tree based on a feature set determined by the power divergence statistic.</S>
    <S sid="121" ssid="11">The last line of Table 1 shows the win-tie-loss score of the decision tree/power divergence method relative to every other method.</S>
    <S sid="122" ssid="12">A win shows it was more accurate than the method in the column, a loss means it was less accurate, and a tie means it was equally accurate.</S>
    <S sid="123" ssid="13">The decision tree/power divergence method was more accurate than the best reported SENSEVAL results for 19 of the 36 tasks, and more accurate for 30 of the 36 tasks when compared to the average reported accuracy.</S>
    <S sid="124" ssid="14">The decision stumps also fared well, proving to be more accurate than the best SENSEVAL results for 14 of the 36 tasks.</S>
    <S sid="125" ssid="15">In general the feature sets selected by the power divergence statistic result in more accurate decision trees than those selected by the Dice Coefficient.</S>
    <S sid="126" ssid="16">The power divergence tests prove to be more reliable since they account for all possible events surrounding two words w1 and w2; when they occur as bigram w1w2, when w1 or w2 occurs in a bigram without the other, and when a bigram consists of neither.</S>
    <S sid="127" ssid="17">The Dice Coefficient is based strictly on the event where w1 and w2 occur together in a bigram.</S>
    <S sid="128" ssid="18">There are 6 tasks where the decision tree / power divergence approach is less accurate than the SENSEVAL average; promise-n, scrap-n, shirt-n, amazev, bitter-p, and sanction-p.</S>
    <S sid="129" ssid="19">The most dramatic difference occurred with amaze-v, where the SENSEVAL average was 92.4% and the decision tree accuracy was 58.6%.</S>
    <S sid="130" ssid="20">However, this was an unusual task where every instance in the test data belonged to a single sense that was a minority sense in the training data.</S>
  </SECTION>
  <SECTION title="7 Analysis of Experimental Results" number="6">
    <S sid="131" ssid="1">The characteristics of the decision trees and decision stumps learned for each word are shown in Table 2.</S>
    <S sid="132" ssid="2">Column 1 shows the word and part of speech.</S>
    <S sid="133" ssid="3">Columns 2, 3, and 4 are based on the feature set selected by the power divergence statistic while columns 5, 6, and 7 are based on the Dice Coefficient.</S>
    <S sid="134" ssid="4">Columns 2 and 5 show the node selected to serve as the decision stump.</S>
    <S sid="135" ssid="5">Columns 3 and 6 show the number of leaf nodes in the learned decision tree relative to the number of total nodes.</S>
    <S sid="136" ssid="6">Columns 4 and 7 show the number of bigram features selected to represent the training data.</S>
    <S sid="137" ssid="7">This table shows that there is little difference in the decision stump nodes selected from feature sets determined by the power divergence statistics versus the Dice Coefficient.</S>
    <S sid="138" ssid="8">This is to be expected since the top ranked bigrams for each measure are consistent, and the decision stump node is generally chosen from among those.</S>
    <S sid="139" ssid="9">However, there are differences between the feature sets selected by the power divergence statistics and the Dice Coefficient.</S>
    <S sid="140" ssid="10">These are reflected in the different sized trees that are learned based on these feature sets.</S>
    <S sid="141" ssid="11">The number of leaf nodes and the total number of nodes for each learned tree is shown in columns 3 and 6.</S>
    <S sid="142" ssid="12">The number of internal nodes is simply the difference between the total nodes and the leaf nodes.</S>
    <S sid="143" ssid="13">Each leaf node represents the end of a path through the decision tree that makes a sense distinction.</S>
    <S sid="144" ssid="14">Since a bigram feature can only appear once in the decision tree, the number of internal nodes represents the number of bigram features selected by the decision tree learner.</S>
    <S sid="145" ssid="15">One of our original hypotheses was that accurate decision trees of bigrams will include a relatively small number of features.</S>
    <S sid="146" ssid="16">This was motivated by the success of decision stumps in performing disambiguation based on a single bigram feature.</S>
    <S sid="147" ssid="17">In these experiments, there were no decision trees that used all of the bigram features identified by the filtering step, and for many words the decision tree learner went on to eliminate most of the candidate features.</S>
    <S sid="148" ssid="18">This can be seen by comparing the number of internal nodes with the number of candidate features as shown in columns 4 or 7.1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice Coefficient.</S>
    <S sid="149" ssid="19">This is to be expected, since the selection of the bigrams from raw text is only mea'For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner.</S>
    <S sid="150" ssid="20">If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bigrams that occurred more than 5 times then all such bigrams are included in the feature set. suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses.</S>
    <S sid="151" ssid="21">In particular, the decision tree learner makes decisions as to what bigram to include as nodes in the tree using the gain ratio, a measure based on the overall Mutual Information between the bigram and a particular word sense.</S>
    <S sid="152" ssid="22">Finally, note that the smallest decision trees are functionally equivalent to our benchmark methods.</S>
    <S sid="153" ssid="23">A decision tree with 1 leaf node and no internal nodes (1/1) acts as a majority classifier.</S>
    <S sid="154" ssid="24">A decision tree with 2 leaf nodes and 1 internal node (2/3) has the structure of a decision stump.</S>
  </SECTION>
  <SECTION title="8 Discussion" number="7">
    <S sid="155" ssid="1">One of our long-term objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies.</S>
    <S sid="156" ssid="2">We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classifiers, each based on co&#8212; occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line.</S>
    <S sid="157" ssid="3">While the accuracy of this approach was as good as any previously published results, the learned models were complex and difficult to interpret, in effect acting as very accurate black boxes.</S>
    <S sid="158" ssid="4">Our experience has been that variations in learning algorithms are far less significant contributors to disambiguation accuracy than are variations in the feature set.</S>
    <S sid="159" ssid="5">In other words, an informative feature set will result in accurate disambiguation when used with a wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features.</S>
    <S sid="160" ssid="6">Therefore, our focus is on developing and discovering feature sets that make distinctions among word senses.</S>
    <S sid="161" ssid="7">Our learning algorithms must not only produce accurate models, but they should also shed new light on the relationships among features and allow us to continue refining and understanding our feature sets.</S>
    <S sid="162" ssid="8">We believe that decision trees meet these criteria.</S>
    <S sid="163" ssid="9">A wide range of implementations are available, and they are known to be robust and accurate across a range of domains.</S>
    <S sid="164" ssid="10">Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation.</S>
  </SECTION>
  <SECTION title="9 Related Work" number="8">
    <S sid="165" ssid="1">Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)).</S>
    <S sid="166" ssid="2">While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case.</S>
    <S sid="167" ssid="3">Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)).</S>
    <S sid="168" ssid="4">In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word.</S>
    <S sid="169" ssid="5">We believe that the approach in this paper is the first time that decision trees based strictly on bigram features have been employed.</S>
    <S sid="170" ssid="6">The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)).</S>
    <S sid="171" ssid="7">Rather than building and traversing a tree to perform disambiguation, a list is employed.</S>
    <S sid="172" ssid="8">In the general case a decision list may suffer from less fragmentation during learning than decision trees; as a practical matter this means that the decision list is less likely to be over&#8212;trained.</S>
    <S sid="173" ssid="9">However, we believe that fragmentation also reflects on the feature set used for learning.</S>
    <S sid="174" ssid="10">Ours consists of at most approximately 100 binary features.</S>
    <S sid="175" ssid="11">This results in a relatively small feature space that is not as likely to suffer from fragmentation as are larger spaces.</S>
  </SECTION>
  <SECTION title="10 Future Work" number="9">
    <S sid="176" ssid="1">There are a number of immediate extensions to this work.</S>
    <S sid="177" ssid="2">The first is to ease the requirement that bigrams be made up of two consecutive words.</S>
    <S sid="178" ssid="3">Rather, we will search for bigrams where the component words may be separated by other words in the text.</S>
    <S sid="179" ssid="4">The second is to eliminate the filtering step by which candidate bigrams are selected by a power divergence statistic.</S>
    <S sid="180" ssid="5">Instead, the decision tree learner would consider all possible bigrams.</S>
    <S sid="181" ssid="6">Despite increasing the danger of fragmentation, this is an interesting issue since the bigrams judged most informative by the decision tree learner are not always ranked highly in the filtering step.</S>
    <S sid="182" ssid="7">In particular, we will determine if the filtering process ever eliminates bigrams that could be significant sources of disambiguation information.</S>
    <S sid="183" ssid="8">In the longer term, we hope to adapt this approach to unsupervised learning, where disambiguation is performed without the benefit of sense tagged text.</S>
    <S sid="184" ssid="9">We are optimistic that this is viable, since bigram features are easy to identify in raw text.</S>
  </SECTION>
  <SECTION title="11 Conclusion" number="10">
    <S sid="185" ssid="1">This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.</S>
    <S sid="186" ssid="2">The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words.</S>
  </SECTION>
  <SECTION title="12 Acknowledgments" number="11">
    <S sid="187" ssid="1">The Bigram Statistics Package has been implemented by Satanjeev Banerjee, who is supported by a Grant-in-Aid of Research, Artistry and Scholarship from the Office of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota.</S>
    <S sid="188" ssid="2">We would like to thank the SENSEVAL organizers for making the data and results from the 1998 event freely available.</S>
    <S sid="189" ssid="3">The comments of three anonymous reviewers were very helpful in preparing the final version of this paper.</S>
    <S sid="190" ssid="4">A preliminary version of this paper appears in (Pedersen, 2001).</S>
  </SECTION>
</PAPER>
