<PAPER>
	<S sid="0">Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging</S><ABSTRACT>
		<S sid="1" ssid="1">We present a HMM part-of-speech tag ging method which is particularly suited for POS tagsets with a large number of fine-grained tags.</S>
		<S sid="2" ssid="2">It is based on three ideas: (1) splitting of the POS tags into attributevectors and decomposition of the contex tual POS probabilities of the HMM into aproduct of attribute probabilities, (2) esti mation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.</S>
		<S sid="3" ssid="3">In experiments on German andCzech data, our tagger outperformed state of-the-art POS taggers.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="4" ssid="4">A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence ? t N 1 = ? t 1 , ..., ? t N for a given word sequence w N 1 . ? t N 1 = argmax t N 1 p(t N 1 , w N 1 )The joint probability of the two sequences is de fined as the product of context probabilities and lexical probabilities over all POS tags: p(t N 1 , w N 1 ) = N ? i=1 p(t i |t i?1 i?k ) ? ??</S>
			<S sid="5" ssid="5">context prob.</S>
			<S sid="6" ssid="6">p(w i |t i ) ? ??</S>
			<S sid="7" ssid="7">lexical prob.</S>
			<S sid="8" ssid="8">(1)HMM taggers are fast and were successfully applied to a wide range of languages and training cor pora.</S>
			<S sid="9" ssid="9">c ? 2008.</S>
			<S sid="10" ssid="10">Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).</S>
			<S sid="11" ssid="11">Some rights reserved.</S>
			<S sid="12" ssid="12">POS taggers are usually trained on corpora with between 50 and 150 different POS tags.</S>
			<S sid="13" ssid="13">Tagsets of this size contain little or no information aboutnumber, gender, case and similar morphosyntactic features.</S>
			<S sid="14" ssid="14">For languages with a rich morphol ogy such as German or Czech, more fine-grained tagsets are often considered more appropriate.</S>
			<S sid="15" ssid="15">Theadditional information may also help to disam biguate the (base) part of speech.</S>
			<S sid="16" ssid="16">Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das Realit?at?</S>
			<S sid="17" ssid="17">(Is that reality?).</S>
			<S sid="18" ssid="18">The word das is ambiguous between an article and a demonstrative.</S>
			<S sid="19" ssid="19">Because of the lack of gender agreement between das (neuter) and the noun Realit?at (feminine), the article reading must be wrong.</S>
			<S sid="20" ssid="20">The German Tiger treebank (Brants et al, 2002) is an example of a corpus with a more fine-grainedtagset (over 700 tags overall).</S>
			<S sid="21" ssid="21">Large tagsets aggra vate sparse data problems.</S>
			<S sid="22" ssid="22">As an example, take the German sentence Das zu versteuernde Einkommen sinkt (?The to be taxed income decreases?; The taxable income decreases).</S>
			<S sid="23" ssid="23">This sentence should be tagged as shown in table 1.</S>
			<S sid="24" ssid="24">Das ART.Def.Nom.Sg.Neut zu PART.Zu versteuernde ADJA.Pos.Nom.Sg.Neut Einkommen N.Reg.Nom.Sg.Neut sinkt VFIN.Full.3.Sg.Pres.Ind . SYM.Pun.Sent.</S>
			<S sid="25" ssid="25">Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.</S>
			<S sid="26" ssid="26">Unfortunately, the POS trigram consisting of the tags of the first three words does not occurin the Tiger corpus.</S>
			<S sid="27" ssid="27">(Neither does the pair con sisting of the first two tags.)</S>
			<S sid="28" ssid="28">The unsmoothed 777context probability of the third POS tag is there fore 0.</S>
			<S sid="29" ssid="29">If the probability is smoothed with the backoff distribution p(?|PART.Zu), the most probable tag is ADJA.Pos.Acc.Sg.Fem rather thanADJA.Pos.Nom.Sg.Neut.</S>
			<S sid="30" ssid="30">Thus, the agreement be tween the article and the adjective is not checked anymore.</S>
			<S sid="31" ssid="31">A closer inspection of the Tiger corpus reveals that it actually contains all the information needed to completely disambiguate each component of the POS tag ADJA.Pos.Nom.Sg.Neut: ? All words appearing after an article (ART)and the infinitive particle zu (PART.zu) are at tributive adjectives (ADJA) (10 of 10 cases).</S>
			<S sid="32" ssid="32">All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases).</S>
			<S sid="33" ssid="33">All adjectives appearing after a nominative article and a particle have nominative case (11 of 11 cases).?</S>
			<S sid="34" ssid="34">All adjectives appearing after a singular arti cle and a particle are singular (32 of 32 cases).</S>
			<S sid="35" ssid="35">All adjectives appearing after a neuter article and a particle are neuter (4 of 4 cases).</S>
			<S sid="36" ssid="36">By (1) decomposing the context probability of ADJA.Pos.Nom.Sg.Neut into a product of attribute probabilities p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu) ? p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA) ? p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos) ? p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom) ? p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg) and (2) selecting the relevant context attributes for the prediction of each attribute, we obtain the following expression for the context probability: p(ADJA | ART, PART.Zu) ? p(Pos | 2:ART, 1:PART, 0:ADJA) ? p(Nom | 2:ART.Nom, 1:PART.Zu, 0:ADJA) ? p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA) ? p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA) The conditional probability of each attribute is 1.</S>
			<S sid="37" ssid="37">Hence the context probability of the whole tag is. also 1.</S>
			<S sid="38" ssid="38">Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context.</S>
			<S sid="39" ssid="39">These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees toestimate the probability of each attribute.</S>
			<S sid="40" ssid="40">Decision trees are ideal for this task because the iden tification of relevant attribute combinations is at the heart of this method.</S>
			<S sid="41" ssid="41">The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here.</S>
			<S sid="42" ssid="42">Discriminatively trained taggers, on the other hand, have difficulties to handle the huge number of features which are active at the same time if any possible combination of context attributes defines a separate feature.</S>
	</SECTION>
	<SECTION title="Decision Trees. " number="2">
			<S sid="43" ssid="1">Decision trees (Breiman et al, 1984; Quinlan,1993) are normally used as classifiers, i.e. they assign classes to objects which are represented as at tribute vectors.</S>
			<S sid="44" ssid="2">The non-terminal nodes are labeledwith attribute tests, the edges with the possible out comes of a test, and the terminal nodes are labeled with classes.</S>
			<S sid="45" ssid="3">An object is classified by evaluating the test of the top node on the object, following the respective edge to a daughter node, evaluating thetest of the daughter node, and so on until a termi nal node is reached whose class is assigned to the object.Decision Trees are turned into probability estimation trees by storing a probability for each pos sible class at the terminal nodes instead of a singleresult class.</S>
			<S sid="46" ssid="4">Figure 1 shows a probability estima tion tree for the prediction of the probability of the nominative attribute of nouns.</S>
			<S sid="47" ssid="5">2.1 Induction of Decision Trees.</S>
			<S sid="48" ssid="6">Decision trees are incrementally built by first selecting the test which splits the manually anno tated training sample into the most homogeneous subsets with respect to the class.</S>
			<S sid="49" ssid="7">This test, which maximizes the information gain 1 wrt.</S>
			<S sid="50" ssid="8">the class, is 1The information gain measures how much the test de creases the uncertainty about the class.</S>
			<S sid="51" ssid="9">It is the difference between the entropy of the empirical distribution of the class variable in the training set and the weighted average entropy 778 2:N.Reg p=0.571 p=0.938 p=0.999 0:N.Name 1:ART.Nom 0:N.Name 0:N.Name p=0.948 p=0.998 ....</S>
			<S sid="52" ssid="10">1:ADJA.Nom yes yes no noyes no yes no no yesFigure 1: Probability estimation tree for the nomi native case of nouns.</S>
			<S sid="53" ssid="11">The test 1:ART.Nom checks if the preceding word is a nominative article.</S>
			<S sid="54" ssid="12">assigned to the top node.</S>
			<S sid="55" ssid="13">The tree is recursivelyexpanded by selecting the best test for each sub set and so on, until all objects of the current subsetbelong to the same class.</S>
			<S sid="56" ssid="14">In a second step, the decision tree may be pruned in order to avoid overfit ting to the training data.</S>
			<S sid="57" ssid="15">Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.</S>
			<S sid="58" ssid="16">The motivation was that a tree which predicts a single value (say verb) does notfragment the data with tests which are only relevant for the distinction of two other values (e.g. ar ticle and possessive pronoun).</S>
			<S sid="59" ssid="17">2 Furthermore, we observed that such two-class decision trees require no optimization of the pruning threshold (see also section 2.2.)The tree induction algorithm only considers bi nary tests, which check whether some particular attribute is present or not.</S>
			<S sid="60" ssid="18">The best test for each node is selected with the standard information gaincriterion.</S>
			<S sid="61" ssid="19">The recursive tree building process ter minates if the information gain is 0.</S>
			<S sid="62" ssid="20">The decision tree is pruned with the pruning criterion described below.</S>
			<S sid="63" ssid="21">Since the tagger creates a separate tree for eachattribute, the probabilities of a set of competing at tributes such as masculine, feminine, and neuter will not exactly sum up to 1.</S>
			<S sid="64" ssid="22">To understand why,assume that there are three trees for the gender attributes.</S>
			<S sid="65" ssid="23">Two of them (say the trees for mascu line and feminine) consist of a single terminal node in the two subsets.</S>
			<S sid="66" ssid="24">The weight of each subset is proportional to its size.</S>
			<S sid="67" ssid="25">2We did not directly compare the two alternatives (two valued vs. multi-valued tests), because the implementational effort required would have been too large.</S>
			<S sid="68" ssid="26">which returns a probability of 0.3.</S>
			<S sid="69" ssid="27">The third tree for neuter has one non-terminal and two terminalnodes returning a probability of 0.3 and 0.5, re spectively.</S>
			<S sid="70" ssid="28">The sum of probabilities is therefore either 0.9 or 1.1, but never exactly 1.</S>
			<S sid="71" ssid="29">This problem is solved by renormalizing the probabilities.</S>
			<S sid="72" ssid="30">The probability of an attribute (such as ?Nom?)</S>
			<S sid="73" ssid="31">is always conditioned on the respective base POS (such as ?N?)</S>
			<S sid="74" ssid="32">(unless the predicted attribute is thebase POS) in order to make sure that the probabil ity of an attribute is 0 if it never appeared with the respective base POS.</S>
			<S sid="75" ssid="33">All context attributes other than the base POS are always used in combination with the base POS.</S>
			<S sid="76" ssid="34">A typical context attribute is ?1:ART.Nom?</S>
			<S sid="77" ssid="35">which states that the preceding tag is an article with the attribute ?Nom?.</S>
			<S sid="78" ssid="36">?1:ART?</S>
			<S sid="79" ssid="37">is also a valid attribute specification, but ?1:Nom?</S>
			<S sid="80" ssid="38">is not.</S>
			<S sid="81" ssid="39">The tagger further restricts the set of possible test attributes by requiring that some attribute ofthe POS tag at position i-k (i=position of the predicted POS tag, k ? 1) must have been used be fore an attribute of the POS tag at position i-(k+1) may be examined.</S>
			<S sid="82" ssid="40">This restriction improved the tagging accuracy for large contexts.</S>
			<S sid="83" ssid="41">2.2 Pruning Criterion.</S>
			<S sid="84" ssid="42">The tagger applies 3the critical-value pruning strat egy proposed by (Mingers, 1989).</S>
			<S sid="85" ssid="43">A node ispruned if the information gain of the best test mul tiplied by the size of the data subsample is below a given threshold.</S>
			<S sid="86" ssid="44">To illustrate the pruning, assume that D is the data of the current node with 50 positive and 25 negative elements, and that D 1 (with 20 positive and 20 negative elements) and D 2(with 30 posi tive and 5 negative elements) are the two subsets induced by the best test.</S>
			<S sid="87" ssid="45">The entropy of D is ?2/3 log 2 2/3 ? 1/3 log 2 1/3 = 0.92, the entropy ofD 1 is?1/2 log 2 1/2?1/2 log 2 1/2 = 1, and the entropy of D 2 is ?6/7 log 2 6/7 ? 1/7 log 2 1/7 = 0.59.</S>
			<S sid="88" ssid="46">The information gain is therefore 0.92 ?</S>
			<S sid="89" ssid="47">(8/15 ? 1 ? 7/15 ? 0.59) = 0.11.</S>
			<S sid="90" ssid="48">The resulting score is 75 ? 0.11 = 8.25.</S>
			<S sid="91" ssid="49">Given a threshold of 6, the node is therefore not pruned.</S>
			<S sid="92" ssid="50">We experimented with pre-pruning (where a node is always pruned if the gain is below the 3 We also experimented with a pruning criterion based on binomial tests, which returned smaller trees with a slightly lower accuracy, although the difference in accuracy was neverlarger than 0.1% for any context size.</S>
			<S sid="93" ssid="51">Thus, the simpler prun ing strategy presented here was chosen.</S>
			<S sid="94" ssid="52">779 threshold) as well as post-pruning (where a node is only pruned if its sub-nodes are terminal nodes or pruned nodes).</S>
			<S sid="95" ssid="53">The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold.</S>
			<S sid="96" ssid="54">A threshold of 6 consistently produced optimal or near optimal results for pre-pruning.</S>
			<S sid="97" ssid="55">Thus, pre-pruning with a threshold of 6 was used in the experiments.</S>
	</SECTION>
	<SECTION title="Splitting of the POS Tags. " number="3">
			<S sid="98" ssid="1">The tagger treats dots in POS tag labels as attribute separators.</S>
			<S sid="99" ssid="2">The first attribute of a POS tag is the main category.</S>
			<S sid="100" ssid="3">The number of additional attributes is fixed for each main category.</S>
			<S sid="101" ssid="4">The additionalattributes are category-specific.</S>
			<S sid="102" ssid="5">The singular at tribute of a noun and an adjective POS tag are therefore two different attributes.</S>
			<S sid="103" ssid="6">4Each position in the POS tags of a given category corresponds to a feature.</S>
			<S sid="104" ssid="7">The attributes oc curring at a certain position constitute the value set of the feature.</S>
	</SECTION>
	<SECTION title="Our Tagger. " number="4">
			<S sid="105" ssid="1">Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.</S>
			<S sid="106" ssid="2">The probability of an attribute given the attributes of the preceding POS tags as well as the preceding attributes of the predicted POS tagis estimated with a decision tree as described be fore.</S>
			<S sid="107" ssid="3">The probabilities at the terminal nodes of the decision trees are smoothed with the parent node probabilities (which themselves were smoothed in the same way).</S>
			<S sid="108" ssid="4">The smoothing is implemented by adding the weighted class probabilities p p (c) of theparent node to the frequencies f(c) before normal izing them to probabilities: p(c) = f(c) + ?p p (c) ? + ? c f(c)The weight ? was fixed to 1 after a few experiments on development data.</S>
			<S sid="109" ssid="5">This smoothing strat egy is closely related to Witten-Bell smoothing.</S>
			<S sid="110" ssid="6">The probabilities are normalized by dividing them by the total probability of all attribute values of the respective feature (see section 2.1).</S>
			<S sid="111" ssid="7">The best tag sequence is computed with theViterbi algorithm.</S>
			<S sid="112" ssid="8">The main differences of our tag ger to a standard trigram tagger are that the order of the Markov model (the k in equation 1) is not fixed 4 This is the reason why the attribute tests in figure 1 used complex attributes such as ART.Nom rather than Nom.</S>
			<S sid="113" ssid="9">and that the context probability p(t i |t i?1 i?k) is internally computed as a product of attribute probabili ties.</S>
			<S sid="114" ssid="10">In order to increase the speed, the tagger also applies a beam-search strategy which prunes allsearch paths whose probability is below the prob ability of the best path times a threshold.</S>
			<S sid="115" ssid="11">With a threshold of 10 ?3or lower, the influence of prun ing on the tagging accuracy was negligible.</S>
			<S sid="116" ssid="12">4.1 Supplementary Lexicon.</S>
			<S sid="117" ssid="13">The tagger may use an external lexicon which sup plies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.</S>
			<S sid="118" ssid="14">If anexternal lexicon is provided, the lexical probabili ties are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags.</S>
			<S sid="119" ssid="15">The Witten-Bellmethod is then applied to smooth the lexical prob abilities with the average probabilities.</S>
			<S sid="120" ssid="16">If the word w was observed with N different tags, and f(w, t) is the joint frequency of w and POS tag t, and p(t|[w]) is the average probability of t among words with the same set of possible tags as w, then the smoothed probability of t given w is defined as follows: p(t|w) = f(w, t) + Np(t|[w]) f(w) + NThe smoothed estimates of p(tag|word) are di vided by the prior probability p(tag) of the tag and used instead of p(word|tag).</S>
			<S sid="121" ssid="17">5 4.2 Unknown Words.</S>
			<S sid="122" ssid="18">The lexical probabilities of unknown words areobtained as follows: The unknown words are di vided into four disjoint classes 6with numeric ex pressions, words starting with an upper-case letter, words starting with a lower-case letter, and a fourthclass for the other words.</S>
			<S sid="123" ssid="19">The tagger builds a suf fix trie for each class of unknown words using the known word types from that class.</S>
			<S sid="124" ssid="20">The maximal length of the suffixes is 7.</S>
			<S sid="125" ssid="21">The suffix tries are pruned until (i) all suffixeshave a frequency of at least 5 and (ii) the information gain multiplied by the suffix frequency and di 5 p(word|tag) is equal to p(tag|word)p(word)/p(tag) and p(word) is a constant if the tokenization is unambiguous.</S>
			<S sid="126" ssid="22">Therefore dropping the factor p(word) has no influence on the ranking of the different tag sequences.</S>
			<S sid="127" ssid="23">6In earlier experiments, we had used a much larger num ber of word classes.</S>
			<S sid="128" ssid="24">Decreasing their number to 4 turned out to be better.</S>
			<S sid="129" ssid="25">780 vided by the number of different POS tags is above a threshold of 1.</S>
			<S sid="130" ssid="26">More precisely, if T ? is the set of POS tags that occurred with suffix ?, |T | is the size of the set T , f ? is the frequency of suffix ?, and p ?</S>
			<S sid="131" ssid="27">(t) is the probability of POS tag t among the words with suffix ?, then the following condition must hold: f a?</S>
			<S sid="132" ssid="28">|T a?</S>
			<S sid="133" ssid="29">| ? t?T a?</S>
			<S sid="134" ssid="30">p a?</S>
			<S sid="135" ssid="31">(t) log p a?</S>
			<S sid="136" ssid="32">(t) p ?</S>
			<S sid="137" ssid="33">(t) &lt; 1 The POS probabilities are recursively smoothedwith the POS probabilities of shorter suffixes us ing Witten-Bell smoothing.</S>
	</SECTION>
	<SECTION title="Evaluation. " number="5">
			<S sid="138" ssid="1">Our tagger was first evaluated on data from theGerman Tiger treebank.</S>
			<S sid="139" ssid="2">The results were com pared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gim?enez andM`arquez, 2004), which is based on support vec tor machines.</S>
			<S sid="140" ssid="3">7 The training of the SVMTool took more than a day.</S>
			<S sid="141" ssid="4">Therefore it was not possible to optimize the parameters systematically.</S>
			<S sid="142" ssid="5">We tookstandard features from a 5 word window and M4LRL training without optimization of the regular ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0(Hladk?a et al, 2007) and compared to the TnT tag ger.</S>
			<S sid="143" ssid="6">5.1 Tiger Corpus.</S>
			<S sid="144" ssid="7">The German Tiger treebank (Brants et al, 2002) contains over 888,000 tokens.</S>
			<S sid="145" ssid="8">It is annotated with POS tags from the coarse-grained STTS tagsetand with additional features encoding informa tion about number, gender, case, person, degree,tense, and mood.</S>
			<S sid="146" ssid="9">After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable er rors, 885,707 tokens were left.</S>
			<S sid="147" ssid="10">The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.</S>
			<S sid="148" ssid="11">Some of the 54 STTS labels were mapped to new labels with dots, which reduced the numberof main categories to 23.</S>
			<S sid="149" ssid="12">Examples are the nom inal POS tags NN and NE which were mapped toN.Reg and N.Name.</S>
			<S sid="150" ssid="13">Some lexically decidable dis tinctions missing in the Tiger corpus have been 7 It was planned to include also the Stanford tagger (Toutanova et al, 2003) in this comparison, but it was not possible to train it on the Tiger data.automatically added.</S>
			<S sid="151" ssid="14">Examples are the distinc tion between definite and indefinite articles, and the distinction between hyphens, slashes, left andright parentheses, quotation marks, and other sym bols which the Tiger treebank annotates with ?$(?.A supplementary lexicon was created by analyz ing a word list which included all words from the training, development, and test data with a Germancomputational morphology.</S>
			<S sid="152" ssid="15">The analyses gener ated by the morphology were mapped to the Tiger tagset.</S>
			<S sid="153" ssid="16">Note that only the words, but not the POS tags from the test and development data were used, here.</S>
			<S sid="154" ssid="17">Therefore, it is always possible to create asupplementary lexicon for the corpus to be pro cessed.In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tagpair was unknown, and with a frequency propor tional to the prior probability of the tag if the wordwas unknown.</S>
			<S sid="155" ssid="18">This strategy returned the best results on the development data.</S>
			<S sid="156" ssid="19">In case of the SVM Tool, we were not able to successfully integrate the supplementary lexicon.</S>
			<S sid="157" ssid="20">5.1.1 Refined Tagset Prepositions are not annotated with case in theTiger treebank, although this information is impor tant for the disambiguation of the case of the next noun phrase.</S>
			<S sid="158" ssid="21">In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with thiscase (APPR.Acc).</S>
			<S sid="159" ssid="22">Prepositions which select gen itive case, but also occur with dative case 8 , were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), werelexicalized (APPR.in).</S>
			<S sid="160" ssid="23">The refined tagset alo dis tinguished between the auxiliaries sein, haben, andwerden, and used lexicalized tags for the coor dinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from thedistribution of prototypical coordinating conjunc tions such as und (and) or oder (or).</S>
			<S sid="161" ssid="24">For evaluation purposes, the refined tags are mapped back to the original tags.</S>
			<S sid="162" ssid="25">This mapping is unambiguous.</S>
			<S sid="163" ssid="26">8 In German, the genitive case of arguments is more and more replaced by the dative.</S>
			<S sid="164" ssid="27">781 tagger default refined ref.+lexicon baseline 67.3 67.3 69.4 TnT 86.3 86.9 90.4 SVMTool 86.6 86.6 ? 2 tags 87.0 87.9 91.5 10 tags 87.6 88.5 92.2 Table 2: Tagging accuracies on development data in percent.</S>
			<S sid="165" ssid="28">Results for 2 and for 10 preceding POS tags as context are reported for our tagger.</S>
			<S sid="166" ssid="29">5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data.</S>
			<S sid="167" ssid="30">The accuracy of a baseline tagger which chooses the most probable tag 9ignoring the context is 67.3% without and 69.4% with the supple mentary lexicon.</S>
			<S sid="168" ssid="31">The TnT tagger achieves 86.3% accuracy on the default tagset.</S>
			<S sid="169" ssid="32">A tag is considered correct if allattributes are correct.</S>
			<S sid="170" ssid="33">The tagset refinement increases the accuracy by about 0.6%, and the ex ternal lexicon by another 3.5%.</S>
			<S sid="171" ssid="34">The SVMTool is slightly better than the TnTtagger on the default tagset, but shows little im provement from the tagset refinement.</S>
			<S sid="172" ssid="35">Apparently, the lexical features used by the SVMTool encode most of the information of the tagset refinement.With a context of two preceding POS tags (similar to the trigram tagger TnT), our tagger outper forms TnT by 0.7% on the default tagset, by 1% on the refined tagset, and by 1.1% on the refined tagset plus the additional lexicon.</S>
			<S sid="173" ssid="36">A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6, 0.6, and 0.7%, respectively.</S>
			<S sid="174" ssid="37">default refined ref.+lexicon TnT STTS 97.28 TnT Tiger 97.17 97.26 97.51 10 tags 97.39 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.</S>
			<S sid="175" ssid="38">These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants (2000) for the Negra treebank which is annotated with STTS tags without agreement features.</S>
			<S sid="176" ssid="39">This is to 9Unknown words are tagged by choosing the most fre quent tag of words with the same capitalization.</S>
			<S sid="177" ssid="40">be expected, however, because the STTS tagset ismuch smaller.</S>
			<S sid="178" ssid="41">Table 3 shows the results of an eval uation based on the plain STTS tagset.</S>
			<S sid="179" ssid="42">The first result was obtained with TnT trained on Tiger data which was mapped to STTS before.</S>
			<S sid="180" ssid="43">The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output ismapped to STTS.</S>
			<S sid="181" ssid="44">The third row gives the corre sponding figures for our tagger.</S>
			<S sid="182" ssid="45">91.491.5 91.691.7 91.891.9 9292.1 92.292.3 2 3 4 5 6 7 8 9 10 Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size.</S>
			<S sid="183" ssid="46">The best results are obtained with a context size of 10.</S>
			<S sid="184" ssid="47">What type of information is relevant across a distance of ten words?</S>
			<S sid="185" ssid="48">A good example is the decision tree for the attribute first person of finite verbs, which looks for a first person pronoun at positions -1 through -10 (relative to the position of the current word) in this order.</S>
			<S sid="186" ssid="49">Since German is a verb-final language, these tests clearly make sense.</S>
			<S sid="187" ssid="50">Table 4 shows the performance on the test data.</S>
			<S sid="188" ssid="51">Our tagger was used with a context size of 10.</S>
			<S sid="189" ssid="52">The suffix length parameter of the TnT tagger was set to 6 without lexicon and to 3 with lexicon.</S>
			<S sid="190" ssid="53">These values were optimal on the development data.</S>
			<S sid="191" ssid="54">Theaccuracy of our tagger is lower than on the devel opment data.</S>
			<S sid="192" ssid="55">This could be due to the higher rate of unknown words (10.0% vs. 7.7%).</S>
			<S sid="193" ssid="56">Relative tothe TnT tagger, however, the accuracy is quite sim ilar for test and development data.</S>
			<S sid="194" ssid="57">The differences between the two taggers are significant.</S>
			<S sid="195" ssid="58">10 tagger default refined ref.+lexicon TnT 83.45 84.11 89.14 our tagger 85.00 85.92 91.07 Table 4: Tagging accuracies on test data.</S>
			<S sid="196" ssid="59">By far the most frequent tagging error was the confusion of nominative and accusative case.</S>
			<S sid="197" ssid="60">If 10 726 sentences were better tagged by TnT (i.e. with few errors), 1450 sentences were better tagged by our tagger.</S>
			<S sid="198" ssid="61">The resulting score of a binomial test is below 0.001.</S>
			<S sid="199" ssid="62">782 this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%.</S>
			<S sid="200" ssid="63">Our tagger is quite fast, although not as fast asthe TnT tagger.</S>
			<S sid="201" ssid="64">With a context size of 3 (10), it annotates 7000 (2000) tokens per second on a com puter with an Athlon X2 4600 CPU.</S>
			<S sid="202" ssid="65">The training with a context size of 10 took about 4 minutes.</S>
			<S sid="203" ssid="66">5.2 Czech Academic Corpus.</S>
			<S sid="204" ssid="67">We also evaluated our tagger on the Czech Aca demic corpus (Hladk?a et al, 2007) which contains 652.131 tokens and about 1200 different POS tags.</S>
			<S sid="205" ssid="68">The data was divided into 80% training data, 10% development data and 10% test data.</S>
			<S sid="206" ssid="69">88.5 88.6 88.7 88.8 88.9 89 2 3 4 5 6 7 8 9 10 ?context-data2?Figure 3: Accuracy on development data depend ing on context sizeThe best accuracy of our tagger on the develop ment set was 88.9% obtained with a context of 4 preceding POS tags.</S>
			<S sid="207" ssid="70">The best accuracy of the TnT tagger was 88.2% with a maximal suffix length of 5.</S>
			<S sid="208" ssid="71">The corresponding figures for the test data are.</S>
			<S sid="209" ssid="72">89.53% for our tagger and 88.88% for the TnT tag ger.</S>
			<S sid="210" ssid="73">The difference is significant.</S>
	</SECTION>
	<SECTION title="Discussion. " number="6">
			<S sid="211" ssid="1">Our tagger combines two ideas, the decompositionof the probability of complex POS tags into a prod uct of feature probabilities, and the estimation of the conditional probabilities with decision trees.</S>
			<S sid="212" ssid="2">A similar idea was previously presented in Kempe (1994), but apparently never applied again.</S>
			<S sid="213" ssid="3">The tagging accuracy reported by Kempe was below that of a traditional trigram tagger.</S>
			<S sid="214" ssid="4">Unlike him, we found that our tagging method out-performed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.Schmid (1994) and M`arquez (1999) used decision trees for the estimation of contextual tag prob abilities, but without a decomposition of the tagprobability.</S>
			<S sid="215" ssid="5">Magerman (1994) applied probabilistic decision trees to parsing, but not with a genera tive model.Provost &amp; Domingos (2003) noted that well known decision tree induction algorithms such as C4.5 (Quinlan, 1993) or CART (Breiman et al,1984) fail to produce accurate probability esti mates.</S>
			<S sid="216" ssid="6">They proposed to grow the decision trees to their maximal size without pruning, and to smooth the probability estimates with add-1 smoothing (also known as the Laplace correction).</S>
			<S sid="217" ssid="7">Ferri et al (2003) describe a more complex backoffsmoothing method.</S>
			<S sid="218" ssid="8">Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0).</S>
			<S sid="219" ssid="9">Another difference is that we used N twoclass trees with normalization to predict the prob abilities of N classes.</S>
			<S sid="220" ssid="10">These two-class trees can be pruned with a fixed pruning threshold.</S>
			<S sid="221" ssid="11">Hence there is no need to put aside training data for parameter tuning.</S>
			<S sid="222" ssid="12">An open question is whether the SVMTool (orother discriminatively trained taggers) could outperform the presented tagger if the same decompo sition of POS tags and the same context size was used.</S>
			<S sid="223" ssid="13">We think that this might be the case if the SVM features are restricted to the set of relevant attribute combinations discovered by the decision tree, but we doubt that it is possible to train theSVMTool (or other discriminatively trained taggers) without such a restriction given the difficul ties to train it with the standard context size.Czech POS tagging has been extensively stud ied in the past (Haji?c and Vidov?a-Hladk?a, 1998; Haji?c et al, 2001; Votrubec, 2006).</S>
			<S sid="224" ssid="14">Spoustov etal.</S>
			<S sid="225" ssid="15">(2007) compared several POS taggers includ ing an n-gram tagger and a discriminatively trained tagger (Mor?ce), and evaluated them on the PragueDependency Treebank (PDT 2.0).</S>
			<S sid="226" ssid="16">Mor?ce?s tag ging accuracy was 95.12%, 0.3% better than the n-gram tagger.</S>
			<S sid="227" ssid="17">A hybrid system based on four different tagging methods reached an accuracy of 95.68%.</S>
			<S sid="228" ssid="18">Because of the different corpora used andthe different amounts of lexical information avail able, a direct comparison to our results is difficult.</S>
			<S sid="229" ssid="19">Furthermore, our tagger uses no corpus-specific heuristics, whereas Mor?ce e.g. is optimized for Czech POS tagging.</S>
			<S sid="230" ssid="20">The German tagging results are, to the best ofour knowledge, the first published results for fine grained POS tagging with the Tiger tagset.</S>
			<S sid="231" ssid="21">783</S>
	</SECTION>
	<SECTION title="Summary. " number="7">
			<S sid="232" ssid="1">We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attributevectors and estimates the conditional probabilities of the attributes with decision trees.</S>
			<S sid="233" ssid="2">In ex periments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).</S>
	</SECTION>
</PAPER>
