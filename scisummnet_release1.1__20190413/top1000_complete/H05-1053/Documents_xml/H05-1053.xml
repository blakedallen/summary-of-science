<PAPER>
	<S sid="0">Domain-Specific Sense Distributions And Predominant Sense Acquisition</S><ABSTRACT>
		<S sid="1" ssid="1">Distributions of the senses of words are often highly skewed.</S>
		<S sid="2" ssid="2">This fact is exploitedby word sense disambiguation (WSD) sys tems which back off to the predominant sense of a word when contextual clues arenot strong enough.</S>
		<S sid="3" ssid="3">The domain of a doc ument has a strong influence on the sensedistribution of words, but it is not feasi ble to produce large manually annotated corpora for every domain of interest.</S>
		<S sid="4" ssid="4">In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words.We apply an existing method for acquiring predominant sense information automatically from raw text, and for our sam ple demonstrate that (1) acquiring suchinformation automatically from a mixeddomain corpus is more accurate than de riving it from SemCor, and (2) acquiringit automatically from text in the same do main as the target domain performs best by a large margin.</S>
		<S sid="5" ssid="5">We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain, and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="6" ssid="6">From analysis of manually sense tagged corpora, Kilgarriff (2004) has demonstrated that distributions of the senses of words are often highly skewed.</S>
			<S sid="7" ssid="7">Most researchers working on word sense disambiguation(WSD) use manually sense tagged data such as SemCor (Miller et al, 1993) to train statistical classi fiers, but also use the information in SemCor on theoverall sense distribution for each word as a back off model.</S>
			<S sid="8" ssid="8">In WSD, the heuristic of just choosing themost frequent sense of a word is very powerful, especially for words with highly skewed sense distri butions (Yarowsky and Florian, 2002).</S>
			<S sid="9" ssid="9">Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004)outperformed the heuristic of choosing the most fre quent sense as derived from SemCor (which wouldgive 61.5% precision and recall1).</S>
			<S sid="10" ssid="10">Furthermore, sys tems that did outperform the first sense heuristic did so only by a small margin (the top score being 65% precision and recall).</S>
			<S sid="11" ssid="11">Over a decade ago, Gale et al (1992) observed the tendency for one sense of a word to prevail in a given discourse.</S>
			<S sid="12" ssid="12">To take advantage of this, a method for automatically determining the ?one sense?</S>
			<S sid="13" ssid="13">given a discourse or document is required.</S>
			<S sid="14" ssid="14">Magnini et al(2002) have shown that information about the do main of a document is very useful for WSD.</S>
			<S sid="15" ssid="15">This isbecause many concepts are specific to particular domains, and for many words their most likely mean ing in context is strongly correlated to the domain of the document they appear in.</S>
			<S sid="16" ssid="16">Thus, since word sense distributions are skewed and depend on the domain at hand we would like to know for each domain of application the most likely sense of a word.</S>
			<S sid="17" ssid="17">However, there are no extant domain-specificsense tagged corpora to derive such sense distribution information from.</S>
			<S sid="18" ssid="18">Producing them would be ex tremely costly, since a substantial corpus would have to be annotated by hand for every domain of interest.</S>
			<S sid="19" ssid="19">In response to this problem, McCarthy et al (2004) proposed a method for automatically inducing the1This figure is the mean of two different estimates (Sny der and Palmer, 2004), the difference being due to multiword handling.</S>
			<S sid="20" ssid="20">419 predominant sense of a word from raw text.</S>
			<S sid="21" ssid="21">They carried out a limited test of their method on text in two domains using subject field codes (Magnini andCavaglia`, 2000) to assess whether the acquired pre dominant sense information was broadly consistent with the domain of the text it was acquired from.But they did not evaluate their method on hand tagged domain-specific corpora since there was no such data publicly available.</S>
			<S sid="22" ssid="22">In this paper, we evaluate the method on domainspecific text by creating a sense-annotated gold standard2 for a sample of words.</S>
			<S sid="23" ssid="23">We used a lexical sam ple because the cost of hand tagging several corpora for an all-words task would be prohibitive.</S>
			<S sid="24" ssid="24">We show that the sense distributions of words in this lexical sample differ depending on domain.</S>
			<S sid="25" ssid="25">We also showthat sense distributions are more skewed in domain specific text.</S>
			<S sid="26" ssid="26">Using McCarthy et al?s method, weautomatically acquire predominant sense informa tion for the lexical sample from the (raw) corpora, and evaluate the accuracy of this and predominant sense information derived from SemCor.</S>
			<S sid="27" ssid="27">We show that in our domains and for these words, first sense information automatically acquired from a general corpus is more accurate than first senses derived from SemCor.</S>
			<S sid="28" ssid="28">We also show that deriving first senseinformation from text in the same domain as the tar get data performs best, particularly when focusing on words which are salient to that domain.</S>
			<S sid="29" ssid="29">The paper is structured as follows.</S>
			<S sid="30" ssid="30">In section 2 we summarise McCarthy et al?s predominant sense method.</S>
			<S sid="31" ssid="31">We then (section 3) describe the new gold standard corpora, and evaluate predominant sense accuracy (section 4).</S>
			<S sid="32" ssid="32">We discuss the results with a proposal for applying the method to an all-words task, and an analysis of our results in terms of this proposal before concluding with future directions.</S>
	</SECTION>
	<SECTION title="Finding Predominant Senses. " number="2">
			<S sid="33" ssid="1">We use the method described in McCarthy et al (2004) for finding predominant senses from raw text.</S>
			<S sid="34" ssid="2">The method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word (  ) with its top  nearest neighbours, where  is a constant.</S>
			<S sid="35" ssid="3">Like McCarthy 2This resource will be made publicly available for research purposes in the near future.</S>
			<S sid="36" ssid="4">et al (2004) we use   and obtain our thesaurus using the distributional similarity metric described by Lin (1998).</S>
			<S sid="37" ssid="5">We use WordNet (WN) as our sense inventory.</S>
			<S sid="38" ssid="6">The senses of a word  are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights eachneighbour?s score by a WN Similarity score (Pat wardhan and Pedersen, 2003) between the sense of  and the sense of the neighbour that maximises the WN Similarity score.</S>
			<S sid="39" ssid="7">This weight is normalised by the sum of such WN similarity scores between all senses of  and and the senses of the neighbour that maximises this score.</S>
			<S sid="40" ssid="8">We use the WN Similarity jcnscore (Jiang and Conrath, 1997) since this gave rea sonable results for McCarthy et al and it is efficientat run time given precompilation of frequency information.</S>
			<S sid="41" ssid="9">The jcn measure needs word frequency information, which we obtained from the British National Corpus (BNC) (Leech, 1992).</S>
			<S sid="42" ssid="10">The distributional thesaurus was constructed using subject, direct object adjective modifier and noun modifier re lations.</S>
	</SECTION>
	<SECTION title="Creating the Three Gold Standards. " number="3">
			<S sid="43" ssid="1">In our experiments, we compare for a sampleof nouns the sense rankings created from a bal anced corpus (the BNC) with rankings created from domain-specific corpora (FINANCE and SPORTS) extracted from the Reuters corpus (Rose et al, 2002).</S>
			<S sid="44" ssid="2">In more detail, the three corpora are: BNC: The ?written?</S>
			<S sid="45" ssid="3">documents, amounting to 3209 documents (around 89.7M words), and covering a wide range of topic domains.</S>
			<S sid="46" ssid="4">FINANCE: 117734 FINANCE documents (around 32.5M words) topic codes: ECAT and MCAT SPORTS: 35317 SPORTS documents (around 9.1M words) topic code: GSPO We computed thesauruses for each of these corpora using the procedure outlined in section 2.</S>
			<S sid="47" ssid="5">3.1 Word Selection.</S>
			<S sid="48" ssid="6">In our experiments we used FINANCE and SPORTS domains.</S>
			<S sid="49" ssid="7">To ensure that a significant number of the chosen words are relevant for these domains, we did not choose the words for our experiments completely randomly.</S>
			<S sid="50" ssid="8">The first selection criterionwe applied used the Subject Field Code (SFC) re 420 source (Magnini and Cavaglia`, 2000), which assignsdomain labels to synsets in WN version 1.6.</S>
			<S sid="51" ssid="9">We se lected all the polysemous nouns in WN 1.6 that have at least one synset labelled SPORT and one synset labelled FINANCE.</S>
			<S sid="52" ssid="10">This reduced the set of words to 38.</S>
			<S sid="53" ssid="11">However, some of these words were fairly obscure, did not occur frequently enough in one of the domain corpora or were simply too polysemous.We narrowed down the set of words using the crite ria: (1) frequency in the BNC 1000, (2) at most12 senses, and (3) at least 75 examples in each corpus.</S>
			<S sid="54" ssid="12">Finally a couple of words were removed because the domain-specific sense was particularly ob scure3.</S>
			<S sid="55" ssid="13">The resulting set consists of 17 words4: club,manager, record, right, bill, check, competition, conversion, crew, delivery, division, fishing, reserve, re turn, score, receiver, running We refer to this set of words as FS cds.</S>
			<S sid="56" ssid="14">The first four words occur in the BNC with high frequency ( 10000 occurrences), the last two with low frequency (  2000) and the rest are mid-frequency.</S>
			<S sid="57" ssid="15">Three further sets of words were selected on the basis of domain salience.</S>
			<S sid="58" ssid="16">We chose eight words that are particularly salient in the Sport corpus (referred to as S sal), eight in the Finance corpus (F sal), and seven that had equal (not necessarily high) salience in both, (eq sal).</S>
			<S sid="59" ssid="17">We computed salience as a ratio of normalised document frequencies, using the formula   fifffl   fffl where ffi !</S>
			<S sid="60" ssid="18">is the number of documents in domain  containing the noun (lemma)  , ffi#!</S>
			<S sid="61" ssid="19">is the number of documents in domain  , ffi  is the total number of documents containing the noun  and ffi is the total number of documents.To obtain the sets S sal, F sal and eq sal we gen erated the 50 most salient words for both domainsand 50 words that were equally salient for both do mains.</S>
			<S sid="62" ssid="20">These lists of 50 words were subjected to the same constraints as set FS cds, that is occurring in the BNC 1000, having at most 12 senses, and having at least 75 examples in each corpus.</S>
			<S sid="63" ssid="21">From the remaining words we randomly sampled 8 words 3For example the Finance sense of ?eagle?</S>
			<S sid="64" ssid="22">(a former gold coin in US worth 10 dollars) is very unlikely to be found.4One more word, ?pitch?, was in the original selection.</S>
			<S sid="65" ssid="23">However, we did not obtain enough usable annotated sentences (sec tion 3.2) for this particular word and therefore it was discarded.</S>
			<S sid="66" ssid="24">from the Sport salience list and Finance list and 7 from the salience list for words with equal salience in both domains.</S>
			<S sid="67" ssid="25">The resulting sets of words are: S sal: fan, star, transfer, striker, goal, title, tie, coach F sal: package, chip, bond, market, strike, bank, share, target eq sal: will, phase, half, top, performance, level, country The average degree of polysemy for this set of 40 nouns in WN (version 1.7.1) is 6.6.</S>
			<S sid="68" ssid="26">3.2 The Annotation Task.</S>
			<S sid="69" ssid="27">For the annotation task we recruited linguistics stu dents from two universities.</S>
			<S sid="70" ssid="28">All ten annotators are native speakers of English.We set up annotation as an Open Mind Word Ex pert task5.</S>
			<S sid="71" ssid="29">Open Mind is a web based system for annotating sentences.</S>
			<S sid="72" ssid="30">The user can choose a word from a pull down menu.</S>
			<S sid="73" ssid="31">When a word is selected, the user is presented with a list of sense definitions.</S>
			<S sid="74" ssid="32">The sense definitions were taken from WN1.7.1 andpresented in random order.</S>
			<S sid="75" ssid="33">Below the sense defini tions, sentences with the target word (highlighted) are given.</S>
			<S sid="76" ssid="34">Left of the sentence on the screen, there are as many tick-boxes as there are senses for the word plus boxes for ?unclear?</S>
			<S sid="77" ssid="35">and ?unlisted-sense?.The annotator is expected to first read the sense defi nitions carefully and then, after reading the sentence, decide which sense is best for the instance of the word in a particular sentence.</S>
			<S sid="78" ssid="36">Only the sentence inwhich the word appears is presented (not more sur rounding sentences).</S>
			<S sid="79" ssid="37">In case the sentence does notgive enough evidence to decide, the annotator is ex pected to check the ?unclear?</S>
			<S sid="80" ssid="38">box.</S>
			<S sid="81" ssid="39">When the correct sense is not listed, the annotator should check the ?unlisted-sense?</S>
			<S sid="82" ssid="40">box.</S>
			<S sid="83" ssid="41">The sentences to be annotated were randomly sampled from the corpora.</S>
			<S sid="84" ssid="42">The corpora were first part of speech tagged and lemmatised using RASP (Briscoe and Carroll, 2002).</S>
			<S sid="85" ssid="43">Up to 125 sentences were randomly selected for each word from eachcorpus.</S>
			<S sid="86" ssid="44">Sentences with clear problems (e.g. contain ing a begin or end of document marker, or mostly not text) were removed.</S>
			<S sid="87" ssid="45">The first 100 remaining sentences were selected for the task.</S>
			<S sid="88" ssid="46">For a few 5http://www.teach-computers.org/word-expert/english/ 421words there were not exactly 100 sentences per cor pus available.</S>
			<S sid="89" ssid="47">The Reuters corpus contains quite a few duplicate documents.</S>
			<S sid="90" ssid="48">No attempts were made to remove duplicates.</S>
			<S sid="91" ssid="49">3.3 Characterisation of the Annotated Data.</S>
			<S sid="92" ssid="50">Most of the sentences were annotated by at least three people.</S>
			<S sid="93" ssid="51">Some sentences were only done by two annotators.</S>
			<S sid="94" ssid="52">The complete set of data comprises 33225 tagging acts.</S>
			<S sid="95" ssid="53">The inter-annotator agreement on the complete set of data was 65%6.</S>
			<S sid="96" ssid="54">For the BNC data it was 60%, for the Sports data 65% and for the Finance data 69%.This is lower than reported for other sets of anno tated data (for example it was 75% for the nouns in the SENSEVAL-2 English all-words task), but quite close to the reported 62.8% agreement between the first two taggings for single noun tagging for the SENSEVAL-3 English lexical sample task (Mihalceaet al, 2004).</S>
			<S sid="97" ssid="55">The fairest comparison is probably be tween the latter and the inter-annotator agreement for the BNC data.</S>
			<S sid="98" ssid="56">Reasons why our agreement is relatively low include the fact that almost all of the sentences are annotated by three people, and also the high degree of polysemy of this set of words.</S>
			<S sid="99" ssid="57">Problematic cases The unlisted category was used as a miscellaneous category.</S>
			<S sid="100" ssid="58">In some cases a sense was truly missing from the inventory (e.g. the word ?tie?</S>
			<S sid="101" ssid="59">has a ?game?</S>
			<S sid="102" ssid="60">sense in British English which is not included in WN 1.7.1).</S>
			<S sid="103" ssid="61">In other cases we had not recognised thatthe word was really part of a multiword (e.g. a num ber of sentences for the word ?chip?</S>
			<S sid="104" ssid="62">contained themultiword ?blue chip?).</S>
			<S sid="105" ssid="63">Finally there were a num ber of cases where the word had been assigned the wrong part of speech tag (e.g. the verb ?will?</S>
			<S sid="106" ssid="64">had often been mistagged as a noun).</S>
			<S sid="107" ssid="65">We identified and removed all these systematic problem cases from theunlisted senses.</S>
			<S sid="108" ssid="66">After removing the problematic un listed cases, we had between 0.9% (FINANCE) and 4.5% (SPORTS) unlisted instances left.</S>
			<S sid="109" ssid="67">We also had between 1.8% (SPORTS) and 4.8% (BNC) unclearinstances.</S>
			<S sid="110" ssid="68">The percentage of unlisted instances re flects the fit of WN to the data whilst that of unclear cases reflects the generality of the corpus.6To compute inter-annotator agreement we used Amruta Pu randare and Ted Pedersen?s OMtoSVAL2 Package version 0.01.</S>
			<S sid="111" ssid="69">The sense distributions WSD accuracy is strongly related to the entropy of the sense distribution of the target word (Yarowskyand Florian, 2002).</S>
			<S sid="112" ssid="70">The more skewed the sense dis tribution is towards a small percentage of the senses, the lower the entropy.</S>
			<S sid="113" ssid="71">Accuracy is related to this because there is more data (both training and test) shared between fewer of the senses.</S>
			<S sid="114" ssid="72">When the first sense is very predominant (exceeding 80%) it is hard for any WSD system to beat the heuristic of always selecting that sense (Yarowsky and Florian, 2002).</S>
			<S sid="115" ssid="73">The sense distribution for a given word may varydepending on the domain of the text being processed.</S>
			<S sid="116" ssid="74">In some cases, this may result in a differ ent predominant sense; other characteristics of the sense distribution may also differ such as entropy ofthe sense distribution and the dominance of the pre dominant sense.</S>
			<S sid="117" ssid="75">In Table 1 we show the entropy per word in our sample and relative frequency (relfr) ofits first sense (fs), for each of our three gold stan dard annotated corpora.</S>
			<S sid="118" ssid="76">We compute the entropy ofa word?s sense distribution as a fraction of the pos sible entropy (Yarowsky and Florian, 2002) $#%  (*),+ .0/21fi3)5476982:;69869 where $  = @?BADC 6982:;6986FE G H2IKJFL E MG  . This measure reduces the impact of the number of senses of a word and focuses on the uncertainty within thedistribution.</S>
			<S sid="119" ssid="77">For each corpus, we also show the av erage entropy and average relative frequency of the first sense over all words.From Table 1 we can see that for the vast ma jority of words the entropy is highest in the BNC.However there are exceptions: return, fan and ti tle for FINANCE and return, half, level, running strike and share for SPORTS.</S>
			<S sid="120" ssid="78">Surprisingly, eq sal words, which are not particularly salient in eitherdomain, also typically have lower entropy in the domain specific corpora compared to the BNC.</S>
			<S sid="121" ssid="79">Pre sumably this is simply because of this small set ofwords, which seem particularly skewed to the fi nancial domain.</S>
			<S sid="122" ssid="80">Note that whilst the distributionsin the domain-specific corpora are more skewed to wards a predominant sense, only 7 of the 40 words in the FINANCE corpus and 5 of the 40 words in the SPORTS corpus have only one sense attested.</S>
			<S sid="123" ssid="81">Thus, even in domain-specific corpora ambiguity is 422 Training Testing BNC FINANCE SPORTS BNC 40.7 43.3 33.2 FINANCE 39.1 49.9 24.0 SPORTS 25.7 19.7 43.7 Random BL 19.8 19.6 19.4 SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8) Table 2: WSD using predominant senses, training and testing on all domain combinations.</S>
			<S sid="124" ssid="82">still present, even though it is less than for general text.</S>
			<S sid="125" ssid="83">We show the sense number of the first sense (fs) alongside the relative frequency of that sense.</S>
			<S sid="126" ssid="84">We use ?ucl?</S>
			<S sid="127" ssid="85">for unclear and ?unl?</S>
			<S sid="128" ssid="86">for unlisted senses where these are predominant in our annotated data.Although the predominant sense of a word is not al ways the domain-specific sense in a domain-specific corpus, the domain-specific senses typically occurmore than they do in non-relevant corpora.</S>
			<S sid="129" ssid="87">For ex ample, sense 11 of return (a tennis stroke) was notthe first sense in SPORTS, however it did have a rel ative frequency of 19% in that corpus and was absent from BNC and FINANCE.</S>
	</SECTION>
	<SECTION title="Predominant Sense Evaluation. " number="4">
			<S sid="130" ssid="1">We have run the predominant sense finding algo rithm on the raw text of each of the three corporain turn (the first step being to compute a distribu tional similarity thesaurus for each, as outlined in section 2).</S>
			<S sid="131" ssid="2">We evaluate the accuracy of performingWSD purely with the predominant sense heuristic us ing all 9 combinations of training and test corpora.</S>
			<S sid="132" ssid="3">The results are presented in Table 2.</S>
			<S sid="133" ssid="4">The random baseline is ? ADCON /MP 82:;6 Q 476982:;6986M) A . We also give the.</S>
			<S sid="134" ssid="5">accuracy using a first sense heuristic from SemCor (?SemCor FS?); the precision is given alongside inbrackets because a predominant sense is not sup plied by SemCor for every word.</S>
			<S sid="135" ssid="6">7 The automatic method proposes a predominant sense in every case.</S>
			<S sid="136" ssid="7">The best results are obtained when training on a domain relevant corpus.</S>
			<S sid="137" ssid="8">In all cases, when training on appropriate training data the automatic methodfor finding predominant senses beats both the ran dom baseline and the baseline provided by SemCor.Table 3 compares WSD accuracy using the auto matically acquired first sense on the 4 categories of 7There is one such word in our sample, striker.</S>
			<S sid="138" ssid="9">Test - Train FS cds F sal S sal eq sal BNC-APPR 33.3 51.5 39.7 48.0 BNC-SC 28.3 44.0 24.6 36.2 FINANCE-APPR 37.0 70.2 38.5 70.1 FINANCE-SC 30.3 51.1 22.9 33.5 SPORTS-APPR 42.6 18.1 65.7 46.9 SPORTS-SC 9.4 38.1 13.2 12.2Table 3: WSD using predominant senses, with train ing data from the same domain or from SemCor.</S>
			<S sid="139" ssid="10">words FS cds, F sal, S sal and eq sal separately.</S>
			<S sid="140" ssid="11">Results using the training data from the appropriate domain (e.g. SPORTS training data for SPORTS test data) are indicated with ?APPR?</S>
			<S sid="141" ssid="12">and contrasted with the results using SemCor data, indicated with ?SC?.</S>
			<S sid="142" ssid="13">8We see that for words which are pertinent to the do main of the test text, it pays to use domain specific training data.</S>
			<S sid="143" ssid="14">In some other cases, e.g. F sal tested on SPORTS, it is better to use SemCor data.</S>
			<S sid="144" ssid="15">For the eq sal words, accuracy is highest when FINANCEdata is used for training, reflecting their bias to fi nancial senses as noted in section 3.3.</S>
	</SECTION>
	<SECTION title="Discussion. " number="5">
			<S sid="145" ssid="1">We are not aware of any other domain-specific man ually sense tagged corpora.</S>
			<S sid="146" ssid="2">We have created sensetagged corpora from two specific domains for a sam ple of words, and a similar resource from a balanced corpus which covers a wide range of domains.</S>
			<S sid="147" ssid="3">Wehave used these resources to do a quantitative evaluation which demonstrates that automatic acquisi tion of predominant senses outperforms the SemCor baseline for this sample of words.The domain-specific manually sense tagged resource is an interesting source of information in it self.</S>
			<S sid="148" ssid="4">It shows for example that (at least for this particular lexical sample), the predominant sense is much more dominant in a specific domain than it is in the general case, even for words which are notparticularly salient in that domain.</S>
			<S sid="149" ssid="5">Similar obser vations can be made about the average number ofencountered senses and the skew of the sense distributions.</S>
			<S sid="150" ssid="6">It also shows that although the predom inant sense is more dominant and domain-specific 8For SemCor, precision figures for the S sal words are up to 4% higher than the accuracy figures given, however they are still lower than accuracy using the domain specific corpora; we leave them out due to lack of space.</S>
			<S sid="151" ssid="7">423 senses are used more within a specific domain,there is still a need for taking local context into account when disambiguating words.</S>
			<S sid="152" ssid="8">The predomi nant sense heuristic is hard to beat for some wordswithin a domain, but others remain highly ambiguous even within a specific domain.</S>
			<S sid="153" ssid="9">The return ex ample in section 3.3 illustrates this.</S>
			<S sid="154" ssid="10">Our results are for a lexical sample because we did not have the resources to produce manually tagged domain-specific corpora for an all words task.</S>
			<S sid="155" ssid="11">Although sense distribution data derived fromSemCor can be more accurate than such informa tion derived automatically (McCarthy et al, 2004), in a given domain there will be words for whichthe SemCor frequency distributions are inappropriate or unavailable.</S>
			<S sid="156" ssid="12">The work presented here demonstrates that the automatic method for finding pre dominant senses outperforms SemCor on a sampleof words, particularly on ones that are salient to a do main.</S>
			<S sid="157" ssid="13">As well as domain-salient words, there will be words which are not particularly salient but still havedifferent distributions than in SemCor.</S>
			<S sid="158" ssid="14">We therefore propose that automatic methods for determin ing the first sense should be used when either there is no manually tagged data, or the manually taggeddata seems to be inappropriate for the word and do main under consideration.</S>
			<S sid="159" ssid="15">While it is trivial to findthe words which are absent or infrequent in train ing data, such as SemCor, it is less obvious how to find words where the training data is not appropriate.</S>
			<S sid="160" ssid="16">One way of finding these words would be to look for differences in the automatic sense rankings of words in domain specific corpora compared to those of the same words in balanced corpora, such as the BNC.</S>
			<S sid="161" ssid="17">We assume that the sense rankings from a balancedtext will more or less correlate with a balanced resource such as SemCor.</S>
			<S sid="162" ssid="18">Of course there will be dif ferences in the corpus data, but these will be less radical than those between SemCor and a domain specific corpus.</S>
			<S sid="163" ssid="19">Then the automatic ranking methodshould be applied in cases where there is a clear deviation in the ranking induced from the domain specific corpus compared to that from the balanced cor pus.</S>
			<S sid="164" ssid="20">Otherwise, SemCor is probably more reliable if data for the given word is available.</S>
			<S sid="165" ssid="21">There are several possibilities for the definition of?clear deviation?</S>
			<S sid="166" ssid="22">above.</S>
			<S sid="167" ssid="23">One could look at differences in the ranking over all words, using a mea Training Testing FINANCE SPORTSFinance 35.5 Sports - 40.9 SemCor 14.2 (15.3) 10.0 Table 4: WSD accuracy for words with a different first sense to the BNC.</S>
			<S sid="168" ssid="24">sure such as pairwise agreement of rankings or a ranking correlation coefficient, such as Spearman?s.One could also use the rankings to estimate prob ability distributions and compare the distributions with measures such as alpha-skew divergence (Lee, 1999).</S>
			<S sid="169" ssid="25">A simple definition would be where the rankings assign different predominant senses to a word.</S>
			<S sid="170" ssid="26">Taking this simple definition of deviation, we demonstrate how this might be done for our corpora.</S>
			<S sid="171" ssid="27">We compared the automatic rankings from the BNC with those from each domain specific corpus (SPORTS and FINANCE) for all polysemous nouns in SemCor.</S>
			<S sid="172" ssid="28">Although the majority are assigned thesame first sense in the BNC as in the domain spe cific corpora, a significant proportion (31% SPORTS and 34% FINANCE) are not.</S>
			<S sid="173" ssid="29">For all words WSD in either of these domains, it would be these wordsfor which automatic ranking should be used.</S>
			<S sid="174" ssid="30">Ta ble 4 shows the WSD accuracy using this approach for the words in our lexical sample with a differentautomatically computed first sense in the BNC com pared to the target domain (SPORTS or FINANCE).</S>
			<S sid="175" ssid="31">We trained on the appropriate domain for each test corpus, and compared this with using SemCor first sense data.</S>
			<S sid="176" ssid="32">The results show clearly that using this approach to decide whether to use automatic sense rankings performs much better than always using SemCor rankings.</S>
	</SECTION>
	<SECTION title="Conclusions. " number="6">
			<S sid="177" ssid="1">The method for automatically finding the predominant sense beat SemCor consistently in our experiments.</S>
			<S sid="178" ssid="2">So for some words, it pays to obtain auto matic information on frequency distributions from appropriate corpora.</S>
			<S sid="179" ssid="3">Our sense annotated corpora exhibit higher entropy for word sense distributions for domain-specific text, even for words which are not specific to that domain.</S>
			<S sid="180" ssid="4">They also show that different senses predominate in different domains 424 and that dominance of the first sense varies to a great extent, depending on the word.</S>
			<S sid="181" ssid="5">Previous workin all words WSD has indicated that techniques us ing hand-tagged resources outperform unsupervisedmethods.</S>
			<S sid="182" ssid="6">However, we demonstrate that it is possi ble to apply a fully automatic method to a subset ofpertinent words to improve WSD accuracy.</S>
			<S sid="183" ssid="7">The au tomatic method seems to lead to better performance for words that are salient to a domain.</S>
			<S sid="184" ssid="8">There are alsoother words which though not particularly domainsalient, have a different sense distribution to that an ticipated for a balanced corpus.</S>
			<S sid="185" ssid="9">We propose that inorder to tackle an all words task, automatic methods should be applied to words which have a sub stantial difference in sense ranking compared to that obtained from a balanced corpus.</S>
			<S sid="186" ssid="10">We demonstrate that for a set of words which meet this condition,the performance of the automatic method is far bet ter than when using data from SemCor.</S>
			<S sid="187" ssid="11">We will dofurther work to ascertain the best method for quanti fying ?substantial change?.</S>
			<S sid="188" ssid="12">We also intend to exploit the automatic rankingto obtain information on sense frequency distribu tions (rather than just predominant senses) given the genre as well as the domain of the text.</S>
			<S sid="189" ssid="13">We plan to combine this with local context, using collocates of neighbours in the thesaurus, for contextual WSD.</S>
			<S sid="190" ssid="14">Acknowledgements We would like to thank Siddharth Patwardhan and Ted Pedersen for making the WN Similarity package available, Rada Mihalceaand Tim Chklovski for making the Open Mind software avail able to us and Julie Weeds for the thesaurus software.</S>
			<S sid="191" ssid="15">The work was funded by EU-2001-34460 project MEANING, UK EPSRC project ?Ranking Word Sense for Word Sense Disambiguation?</S>
			<S sid="192" ssid="16">and the UK Royal Society.</S>
	</SECTION>
</PAPER>
