<PAPER>
  <S sid="0">Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications.</S>
    <S sid="2" ssid="2">While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task.</S>
    <S sid="3" ssid="3">In particular, we present a corpus-based approach for finding base NPs by matching part-ofspeech tag sequences.</S>
    <S sid="4" ssid="4">The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a &amp;quot;treebank&amp;quot; corpus; then the grammar is improved by selecting rules with high &amp;quot;benefit&amp;quot; scores.</S>
    <S sid="5" ssid="5">Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Finding base noun phrases is a sensible first step for many natural language processing (NLP) tasks: Accurate identification of base noun phrases is arguably the most critical component of any partial parser; in addition, information retrieval systems rely on base noun phrases as the main source of multi-word indexing terms; furthermore, the psycholinguistic studies of Gee and Grosjean (1983) indicate that text chunks like base noun phrases play an important role in human language processing.</S>
    <S sid="7" ssid="2">In this work we define base NPs to be simple, nonrecursive noun phrases &#8212; noun phrases that do not contain other noun phrase descendants.</S>
    <S sid="8" ssid="3">The bracketed portions of Figure 1, for example, show the base NPs in one sentence from the Penn Treebank Wall Street Journal (WSJ) corpus (Marcus et al., 1993).</S>
    <S sid="9" ssid="4">Thus, the string the sunny confines of resort towns like Boca Raton and Hot Springs is too complex to be a base NP; instead, it contains four simpler noun phrases, each of which is considered a base NP: the sunny confines, resort towns, Boca Raton, and Hot Springs.</S>
    <S sid="10" ssid="5">Previous empirical research has addressed the problem of base NP identification.</S>
    <S sid="11" ssid="6">Several algorithms identify &amp;quot;terminological phrases&amp;quot; &#8212; certain When [it] is [time] for [their biannual powwow] , [the nation] 's [manufacturing titans] typically jet off to [the sunny confines] of [resort towns] like [Boca Raton] and [Hot Springs]. base noun phrases with initial determiners and modifiers removed: Justeson &amp; Katz (1995) look for repeated phrases; Bourigault (1992) uses a handcrafted noun phrase grammar in conjunction with heuristics for finding maximal length noun phrases; Voutilainen's NPTool (1993) uses a handcrafted lexicon and constraint grammar to find terminological noun phrases that include phrase-final prepositional phrases.</S>
    <S sid="12" ssid="7">Church's PARTS program (1988), on the other hand, uses a probabilistic model automatically trained on the Brown corpus to locate core noun phrases as well as to assign parts of speech.</S>
    <S sid="13" ssid="8">More recently, Ramshaw &amp; Marcus (In press) apply transformation-based learning (Brill, 1995) to the problem.</S>
    <S sid="14" ssid="9">Unfortunately, it is difficult to directly compare approaches.</S>
    <S sid="15" ssid="10">Each method uses a slightly different definition of base NP.</S>
    <S sid="16" ssid="11">Each is evaluated on a different corpus.</S>
    <S sid="17" ssid="12">Most approaches have been evaluated by hand on a small test set rather than by automatic comparison to a large test corpus annotated by an impartial third party.</S>
    <S sid="18" ssid="13">A notable exception is the Ramshaw &amp; Marcus work, which evaluates their transformation-based learning approach on a base NP corpus derived from the Penn Treebank WSJ, and achieves precision and recall levels of approximately 93%.</S>
    <S sid="19" ssid="14">This paper presents a new algorithm for identifying base NPs in an arbitrary text.</S>
    <S sid="20" ssid="15">Like some of the earlier work on base NP identification, ours is a trainable, corpus-based algorithm.</S>
    <S sid="21" ssid="16">In contrast to other corpus-based approaches, however, we hypothesized that the relatively simple nature of base NPs would permit their accurate identification using correspondingly simple methods.</S>
    <S sid="22" ssid="17">Assume, for example, that we use the annotated text of Figure 1 as our training corpus.</S>
    <S sid="23" ssid="18">To identify base NPs in an unseen text, we could simply search for all occurrences of the base NPs seen during training &#8212; it, time, their biannual powwow, .. .</S>
    <S sid="24" ssid="19">, Hot Springs &#8212; and mark them as base NPs in the new text.</S>
    <S sid="25" ssid="20">However, this method would certainly suffer from data sparseness.</S>
    <S sid="26" ssid="21">Instead, we use a similar approach, but back off from lexical items to parts of speech: we identify as a base NP any string having the same part-of-speech tag sequence as a base NP from the training corpus.</S>
    <S sid="27" ssid="22">The training phase of the algorithm employs two previously successful techniques: like Charniak's (1996) statistical parser, our initial base NP grammar is read from a &amp;quot;treebank&amp;quot; corpus; then the grammar is improved by selecting rules with high &amp;quot;benefit&amp;quot; scores.</S>
    <S sid="28" ssid="23">Our benefit measure is identical to that used in transformation-based learning to select an ordered set of useful transformations (Brill, 1995).</S>
    <S sid="29" ssid="24">Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on two base NP corpora of varying complexity, both derived from the Penn Treebank WSJ.</S>
    <S sid="30" ssid="25">The first base NP corpus is that used in the Ramshaw &amp; Marcus work.</S>
    <S sid="31" ssid="26">The second espouses a slightly simpler definition of base NP that conforms to the base NPs used in our Empire sentence analyzer.</S>
    <S sid="32" ssid="27">These simpler phrases appear to be a good starting point for partial parsers that purposely delay all complex attachment decisions to later phases of processing.</S>
    <S sid="33" ssid="28">Overall results for the approach are promising.</S>
    <S sid="34" ssid="29">For the Empire corpus, our base NP finder achieves 94% precision and recall; for the Ramshaw &amp; Marcus corpus, it obtains 91% precision and recall, which is 2% less than the best published results.</S>
    <S sid="35" ssid="30">Ramshaw &amp; Marcus, however, provide the learning algorithm with word-level information in addition to the partof-speech information used in our base NP finder.</S>
    <S sid="36" ssid="31">By controlling for this disparity in available knowledge sources, we find that our base NP algorithm performs comparably, achieving slightly worse precision (-1.1%) and slightly better recall (+0.2%) than the Ramshaw &amp; Marcus approach.</S>
    <S sid="37" ssid="32">Moreover, our approach offers many important advantages that make it appropriate for many NLP tasks: Note also that the treebank approach to base NP identification obtains good results in spite of a very simple algorithm for &amp;quot;parsing&amp;quot; base NPs.</S>
    <S sid="38" ssid="33">This is extremely encouraging, and our evaluation suggests at least two areas for immediate improvement.</S>
    <S sid="39" ssid="34">First, by replacing the naive match heuristic with a probabilistic base NP parser that incorporates lexical preferences, we would expect a nontrivial increase in recall and precision.</S>
    <S sid="40" ssid="35">Second, many of the remaining base NP errors tend to follow simple patterns; these might be corrected using localized, learnable repair rules.</S>
    <S sid="41" ssid="36">The remainder of the paper describes the specifics of the approach and its evaluation.</S>
    <S sid="42" ssid="37">The next section presents the training and application phases of the treebank approach to base NP identification in more detail.</S>
    <S sid="43" ssid="38">Section 3 describes our general approach for pruning the base NP grammar as well as two instantiations of that approach.</S>
    <S sid="44" ssid="39">The evaluation and a discussion of the results appear in Section 4, along with techniques for reducing training time and an initial investigation into the use of local repair heuristics.</S>
  </SECTION>
  <SECTION title="2 The Treebartk Approach" number="2">
    <S sid="45" ssid="1">Figure 2 depicts the treebank approach to base NP identification.</S>
    <S sid="46" ssid="2">For training, the algorithm requires a corpus that has been annotated with base NPs.</S>
    <S sid="47" ssid="3">More specifically, we assume that the training corpus is a sequence of words wi, w2,..., along with a set of base NP annotations b(i1,j1), ..., where b(,j) indicates that the NP brackets words i through j: [Nr, wi, , wj].</S>
    <S sid="48" ssid="4">The goal of the training phase is to create a base NP grammar from this training corpus: The resulting &amp;quot;grammar&amp;quot; can then be used to identify base NPs in a novel text.</S>
    <S sid="49" ssid="5">Not this year.</S>
    <S sid="50" ssid="6">National Association of Manufacturers settled on the Hoosier capital of Indianapolis for its next meeting.</S>
    <S sid="51" ssid="7">And the city decided to treat its guests more like royalty or rock stars than factory owners.</S>
    <S sid="52" ssid="8">Not [this year).</S>
    <S sid="53" ssid="9">/National Association) of /Manufacturers! settled on the Hoosier capital) of [Indianapolis) for I its next meeting).</S>
    <S sid="54" ssid="10">And the city) decided to treat its guests more like !royalty) or frock stars/ than [factory owners).</S>
    <S sid="55" ssid="11">Training Corpus When [it/ is [time/ for /their biannual powwow) . the nation] 's [manufacturing titans/ typically jet off to the sunny confines/ of /resort towns) like [Boca Raton) and [Hot Springs!.</S>
    <S sid="56" ssid="12">3.</S>
    <S sid="57" ssid="13">If there are multiple rules that match beginning at ti, use the longest matching rule R. Add the new base noun phrase t(i,i+IRi_i) to the set of base NPs.</S>
    <S sid="58" ssid="14">Continue matching at ti+tRi.</S>
    <S sid="59" ssid="15">With the rules stored in an appropriate data structure, this greedy &amp;quot;parsing&amp;quot; of base NPs is very fast.</S>
    <S sid="60" ssid="16">In our implementation, for example, we store the rules in a decision tree, which permits base NP identification in time linear in the length of the tagged input text when using the longest match heuristic.</S>
    <S sid="61" ssid="17">Unfortunately, there is an obvious problem with the algorithm described above.</S>
    <S sid="62" ssid="18">There will be many unhelpful rules in the rule set extracted from the training corpus.</S>
    <S sid="63" ssid="19">These &amp;quot;bad&amp;quot; rules arise from four sources: bracketing errors in the corpus; tagging errors; unusual or irregular linguistic constructs (such as parenthetical expressions); and inherent ambiguities in the base NPs &#8212; in spite of their simplicity.</S>
    <S sid="64" ssid="20">For example, the rule (VBG NNS), which was extracted from manufacturing/VBG titans/NNS in the example text, is ambiguous, and will cause erroneous bracketing in sentences such as The execs squeezed in a few meetings before [boarding/VBG buses/NNS] again.</S>
    <S sid="65" ssid="21">In order to have a viable mechanism for identifying base NPs using this algorithm, the grammar must be improved by removing problematic rules.</S>
    <S sid="66" ssid="22">The next section presents two such methods for automatically pruning the base NP grammar.</S>
  </SECTION>
  <SECTION title="3 Pruning the Base NP Grammar" number="3">
    <S sid="67" ssid="1">As described above, our goal is to use the base NP corpus to extract and select a set of noun phrase rules that can be used to accurately identify base NPs in novel text.</S>
    <S sid="68" ssid="2">Our general pruning procedure is shown in Figure 3.</S>
    <S sid="69" ssid="3">First, we divide the base NP corpus into two parts: a training corpus and a pruning corpus.</S>
    <S sid="70" ssid="4">The initial base NP grammar is extracted from the training corpus as described in Section 2.</S>
    <S sid="71" ssid="5">Next, the pruning corpus is used to evaluate the set of rules and produce a ranking of the rules in terms of their utility in identifying base NPs.</S>
    <S sid="72" ssid="6">More specifically, we use the rule set and the longest match heuristic to find all base NPs in the pruning corpus.</S>
    <S sid="73" ssid="7">Performance of the rule set is measured in terms of labeled precision (P): We then assign to each rule a score that denotes the &amp;quot;net benefit&amp;quot; achieved by using the rule during NP parsing of the improvement corpus.</S>
    <S sid="74" ssid="8">The benefit of rule r is given by Br = Cr &#8212; Er where Cr is the number of NPs correctly identified by r, and Er is the number of precision errors for which r is responsible.1 A rule is considered responsible for an error if it was the first rule to bracket part of a reference NP, i.e., an NP in the base NP training corpus.</S>
    <S sid="75" ssid="9">Thus, rules that form erroneous bracketings are not penalized if another rule previously bracketed part of the same reference NP.</S>
    <S sid="76" ssid="10">For example, suppose the fragment containing base NPs Boca Raton, Hot Springs, and Palm Beach is bracketed as shown below. resort towns like [NP, Boca/NNP Raton/NNP , Hot/NNP] [NP, Springs/NNP], and [NP, Palm/NNP Beach/NNP1 Rule (NNP NNP , NNP) brackets NPi; (NNP) brackets NP2; and (NNP NNP) brackets NP3.</S>
    <S sid="77" ssid="11">Rule (NNP NNP , NNP) incorrectly identifies Boca Raton , Hot as a noun phrase, so its score is &#8212;1.</S>
    <S sid="78" ssid="12">Rule (NNP) incorrectly identifies Springs, but it is not held responsible for the error because of the previous error by (NNP NNP , NNP) on the same original NP Hot Springs: so its score is 0.</S>
    <S sid="79" ssid="13">Finally, rule (NNP NNP) receives a score of 1 for correctly identifying Palm Beach as a base NP.</S>
    <S sid="80" ssid="14">The benefit scores from evaluation on the pruning corpus are used to rank the rules in the grammar.</S>
    <S sid="81" ssid="15">With such a ranking, we can improve the rule set by discarding the worst rules.</S>
    <S sid="82" ssid="16">Thus far, we have investigated two iterative approaches for discarding rules, a thresholding approach and an incremental approach.</S>
    <S sid="83" ssid="17">We describe each, in turn, in the subsections below.</S>
    <S sid="84" ssid="18">'This same benefit measure is also used in the R&amp;M study, but it is used to rank transformations rather than to rank NP rules.</S>
    <S sid="85" ssid="19">Given a ranking on the rule set, the threshold algorithm simply discards rules whose score is less than a predefined threshold R. For all of our experiments, we set R = 1 to select rules that propose more correct bracketings than incorrect.</S>
    <S sid="86" ssid="20">The process of evaluating, ranking, and discarding rules is repeated until no rules have a score less than R. For our evaluation on the WSJ corpus, this typically requires only four to five iterations.</S>
    <S sid="87" ssid="21">Thresholding provides a very coarse mechanism for pruning the NP grammar.</S>
    <S sid="88" ssid="22">In particular, because of interactions between the rules during bracketing, thresholding discards rules whose score might increase in the absence of other rules that are also being discarded.</S>
    <S sid="89" ssid="23">Consider, for example, the Boca Raton fragments given earlier.</S>
    <S sid="90" ssid="24">In the absence of (NNP NNP , NNP), the rule (NNP NNP) would have received a score of three for correctly identifying all three NPs.</S>
    <S sid="91" ssid="25">As a result, we explored a more fine-grained method of discarding rules: Each iteration of incremental pruning discards the N worst rules, rather than all rules whose rank is less than some threshold.</S>
    <S sid="92" ssid="26">In all of our experiments, we set N = 10.</S>
    <S sid="93" ssid="27">As with thresholding, the process of evaluating, ranking, and discarding rules is repeated, this time until precision of the current rule set on the pruning corpus begins to drop.</S>
    <S sid="94" ssid="28">The rule set that maximized precision becomes the final rule set.</S>
    <S sid="95" ssid="29">In the experiments below, we compare the thresholding and incremental methods for pruning the NP grammar to a rule set that was pruned by hand.</S>
    <S sid="96" ssid="30">When the training corpus is large, exhaustive review of the extracted rules is not practical.</S>
    <S sid="97" ssid="31">This is the case for our initial rule set, culled from the WSJ corpus, which contains approximately 4500 base NP rules.</S>
    <S sid="98" ssid="32">Rather than identifying and discarding individual problematic rules, our reviewer identified problematic classes of rules that could be removed from the grammar automatically.</S>
    <S sid="99" ssid="33">In particular, the goal of the human reviewer was to discard rules that introduced ambiguity or corresponded to overly complex base NPs.</S>
    <S sid="100" ssid="34">Within our partial parsing framework, these NPs are better identified by more informed components of the NLP system.</S>
    <S sid="101" ssid="35">Our reviewer identified the following classes of rules as possibly troublesome: rules that contain a preposition, period, or colon; rules that contain WH tags; rules that begin/end with a verb or adverb; rules that contain pronouns with any other tags; rules that contain misplaced commas or quotes; rules that end with adjectives.</S>
    <S sid="102" ssid="36">Rules covered under any of these classes were omitted from the human-pruned rule sets used in the experiments of Section 4.</S>
  </SECTION>
  <SECTION title="4 Evaluation" number="4">
    <S sid="103" ssid="1">To evaluate the treebank approach to base NP identification, we created two base NP corpora.</S>
    <S sid="104" ssid="2">Each is derived from the Penn Treebank WSJ.</S>
    <S sid="105" ssid="3">The first corpus attempts to duplicate the base NPs used the Ramshaw &amp; Marcus (R&amp;M) study.</S>
    <S sid="106" ssid="4">The second corpus contains slightly less complicated base NPs &#8212; base NPs that are better suited for use with our sentence analyzer, Empire.'</S>
    <S sid="107" ssid="5">By evaluating on both corpora, we can measure the effect of noun phrase complexity on the treebank approach to base NP identification.</S>
    <S sid="108" ssid="6">In particular, we hypothesize that the treebank approach will be most appropriate when the base NPs are sufficiently simple.</S>
    <S sid="109" ssid="7">For all experiments, we derived the training, pruning, and testing sets from the 25 sections of Wall Street Journal distributed with the Penn Treebank II.</S>
    <S sid="110" ssid="8">All experiments employ 5-fold cross validation.</S>
    <S sid="111" ssid="9">More specifically, in each of five runs, a different fold is used for testing the final, pruned rule set; three of the remaining folds comprise the training corpus (to create the initial rule set); and the final partition is the pruning corpus (to prune bad rules from the initial rule set).</S>
    <S sid="112" ssid="10">All results are averages across the five folds.</S>
    <S sid="113" ssid="11">Performance is measured in terms of precision and recall.</S>
    <S sid="114" ssid="12">Precision was described earlier &#8212; it is a standard measure of accuracy.</S>
    <S sid="115" ssid="13">Recall, on the other hand, is an attempt to measure coverage: Throughout the table, we see the effects of base NP complexity &#8212; the base NPs of the R&amp;M corpus are substantially more difficult for our approach to identify than the simpler NPs of the Empire corpus.</S>
    <S sid="116" ssid="14">For the R&amp;M corpus, we lag the best published results (93.1P/93.5R) by approximately 3%.</S>
    <S sid="117" ssid="15">This straightforward comparison, however, is not entirely appropriate.</S>
    <S sid="118" ssid="16">Ramshaw &amp; Marcus allow their learning algorithm to access word-level information in addition to part-of-speech tags.</S>
    <S sid="119" ssid="17">The treebank approach, on the other hand, makes use only of part-ofspeech tags.</S>
    <S sid="120" ssid="18">Table 2 compares Ramshaw &amp; Marcus' (In press) results with and without lexical knowledge.</S>
    <S sid="121" ssid="19">The first column reports their performance when using lexical templates; the second when lexical templates are not used; the third again shows the treebank approach using incremental pruning.</S>
    <S sid="122" ssid="20">The treebank approach and the R&amp;M approach without lecial templates are shown to perform comparably (-1.1P/+0.2R).</S>
    <S sid="123" ssid="21">Lexicalization of our base NP finder will be addressed in Section 4.1.</S>
    <S sid="124" ssid="22">Finally, note the relatively small difference between the threshold and incremental pruning methods in Table 1.</S>
    <S sid="125" ssid="23">For some applications, this minor drop in performance may be worth the decrease in training time.</S>
    <S sid="126" ssid="24">Another effective technique to speed up training is motivated by Charniak's (1996) observation that the benefit of using rules that only occurred once in training is marginal.</S>
    <S sid="127" ssid="25">By discarding these rules before pruning, we reduce the size of the initial grammar &#8212; and the time for incremental pruning &#8212; by 60%, with a performance drop of only -0.3P/-0.1R.</S>
    <S sid="128" ssid="26">It is informative to consider the kinds of errors made by the treebank approach to bracketing.</S>
    <S sid="129" ssid="27">In particular, the errors may indicate options for incorporating lexical information into the base NP finder.</S>
    <S sid="130" ssid="28">Given the increases in performance achieved by Ramshaw &amp; Marcus by including word-level cues, we would hope to see similar improvements by exploiting lexical information in the treebank approach.</S>
    <S sid="131" ssid="29">For each corpus we examined the first 100 or so errors and found that certain linguistic constructs consistently cause trouble.</S>
    <S sid="132" ssid="30">(In the examples that follow, the bracketing shown is the error.)</S>
    <S sid="133" ssid="31"># of NPs in the annotated text Table 1 summarizes the performance of the treebank approach to base NP identification on the R&amp;M and Empire corpora using the initial and pruned rule sets.</S>
    <S sid="134" ssid="32">The first column of results shows the performance of the initial, unpruned base NP grammar.</S>
    <S sid="135" ssid="33">The next two columns show the performance of the automatically pruned rule sets.</S>
    <S sid="136" ssid="34">The final column indicates the performance of rule sets that had been pruned using the handcrafted pruning heuristics.</S>
    <S sid="137" ssid="35">As expected, the initial rule set performs quite poorly.</S>
    <S sid="138" ssid="36">Both automated approaches provide significant increases in both recall and precision.</S>
    <S sid="139" ssid="37">In addition, they outperform the rule set pruned using handcrafted pruning heuristics.</S>
    <S sid="140" ssid="38">Many errors appear to stem from four underlying causes.</S>
    <S sid="141" ssid="39">First, close to 20% can be attributed to errors in the Treebank and in the Base NP corpus, bringing the effective performance of the algorithm to 94.2P/95.9R and 91.5P/92.7R for the Empire and R&amp;M corpora, respectively.</S>
    <S sid="142" ssid="40">For example, neither corpus includes WH-phrases as base NPs.</S>
    <S sid="143" ssid="41">When the bracketer correctly recognizes these NPs, they are counted as errors.</S>
    <S sid="144" ssid="42">Part-of-speech tagging errors are a second cause.</S>
    <S sid="145" ssid="43">Third, many NPs are missed by the bracketer because it lacks the appropriate rule.</S>
    <S sid="146" ssid="44">For example, household products business is bracketed as [household/NN products/NNS] [business/NM.</S>
    <S sid="147" ssid="45">Fourth, idiomatic and specialized expressions, especially time, date, money, and numeric phrases, also account for a substantial portion of the errors.</S>
    <S sid="148" ssid="46">These last two categories of errors can often be detected because they produce either recognizable patterns or unlikely linguistic constructs.</S>
    <S sid="149" ssid="47">Consecutive NPs, for example, usually denote bracketing errors, as in [household/NN products/NNS] [business/NM.</S>
    <S sid="150" ssid="48">Merging consecutive NPs in the correct contexts would fix many such errors.</S>
    <S sid="151" ssid="49">Idiomatic and specialized expressions might be corrected by similarly local repair heuristics.</S>
    <S sid="152" ssid="50">Typical examples might include changing [effective/JJ Monday/NNP] to effective [Monday]; changing [the/DT balance/NN due/JJ] to [the balance] due; and changing were/VBP In't/RB the/DT only/RB losers/NNS] to were n't [the only losers].</S>
    <S sid="153" ssid="51">Given these observations, we implemented three local repair heuristics.</S>
    <S sid="154" ssid="52">The first merges consecutive NPs unless either might be a time expression.</S>
    <S sid="155" ssid="53">The second identifies two simple date expressions.</S>
    <S sid="156" ssid="54">The third looks for quantifiers preceding of NP.</S>
    <S sid="157" ssid="55">The first heuristic, for example, merges [household products] [business] to form [household products business], but leaves increased [15 %) [last Friday] untouched.</S>
    <S sid="158" ssid="56">The second heuristic merges [June 5] , [1995] into [June 5, 1995]; and [June] , [1995] into [June, 1995].</S>
    <S sid="159" ssid="57">The third finds examples like some of [the companies] and produces [some] of [the companies].</S>
    <S sid="160" ssid="58">These heuristics represent an initial exploration into the effectiveness of employing lexical information in a post-processing phase rather than during grammar induction and bracketing.</S>
    <S sid="161" ssid="59">While we are investigating the latter in current work, local repair heuristics have the advantage of keeping the training and bracketing algorithms both simple and fast.</S>
    <S sid="162" ssid="60">The effect of these heuristics on recall and precision is shown in Table 3.</S>
    <S sid="163" ssid="61">We see consistent improvements for both corpora and both pruning methods, achieving approximately 94P/R for the Empire corpus and approximately 91P/R for the R&amp;M corpus.</S>
    <S sid="164" ssid="62">Note that these are the final results reported in the introduction and conclusion.</S>
    <S sid="165" ssid="63">Although these experiments represent only an initial investigation into the usefulness of local repair heuristics, we are very encouraged by the results.</S>
    <S sid="166" ssid="64">The heuristics uniformly boost precision without harming recall; they help the R&amp;M corpus even though they were designed in response to errors in the Empire corpus.</S>
    <S sid="167" ssid="65">In addition, these three heuristics alone recover 1/2 to 1/3 of the improvements we can expect to obtain from lexicalization based on the R&amp;M results.</S>
  </SECTION>
  <SECTION title="5 Conclusions" number="5">
    <S sid="168" ssid="1">This paper presented a new method for identifying base NPs.</S>
    <S sid="169" ssid="2">Our treebank approach uses the simple technique of matching part-of-speech tag sequences, with the intention of capturing the simplicity of the corresponding syntactic structure.</S>
    <S sid="170" ssid="3">It employs two existing corpus-based techniques: the initial noun phrase grammar is extracted directly from an annotated corpus; and a benefit score calculated from errors on an improvement corpus selects the best subset of rules via a coarse- or fine-grained pruning algorithm.</S>
    <S sid="171" ssid="4">The overall results are surprisingly good, especially considering the simplicity of the method.</S>
    <S sid="172" ssid="5">It achieves 94% precision and recall on simple base NPs.</S>
    <S sid="173" ssid="6">It achieves 91% precision and recall on the more complex NPs of the Rainshaw &amp; Marcus corpus.</S>
    <S sid="174" ssid="7">We believe, however, that the base NP finder can be improved further.</S>
    <S sid="175" ssid="8">First, the longest-match heuristic of the noun phrase bracketer could be replaced by more sophisticated parsing methods that account for lexical preferences.</S>
    <S sid="176" ssid="9">Rule application, for example, could be disambiguated statistically using distributions induced during training.</S>
    <S sid="177" ssid="10">We are currently investigating such extensions.</S>
    <S sid="178" ssid="11">One approach closely related to ours &#8212; weighted finite-state transducers (e.g.</S>
    <S sid="179" ssid="12">(Pereira and Riley, 1997)) &#8212; might provide a principled way to do this.</S>
    <S sid="180" ssid="13">We could then consider applying our error-driven pruning strategy to rules encoded as transducers.</S>
    <S sid="181" ssid="14">Second, we have only recently begun to explore the use of local repair heuristics.</S>
    <S sid="182" ssid="15">While initial results are promising, the full impact of such heuristics on overall performance can be determined only if they are systematically learned and tested using available training data.</S>
    <S sid="183" ssid="16">Future work will concentrate on the corpusbased acquisition of local repair heuristics.</S>
    <S sid="184" ssid="17">In conclusion, the treebank approach to base NPs provides an accurate and fast bracketing method, running in time linear in the length of the tagged text.</S>
    <S sid="185" ssid="18">The approach is simple to understand, implement, and train.</S>
    <S sid="186" ssid="19">The learned grammar is easily modified for use with new corpora, as rules can be added or deleted with minimal interaction problems.</S>
    <S sid="187" ssid="20">Finally, the approach provides a general framework for developing other treebank grammars (e.g., for subject/verb/object identification) in addition to these for base NPs.</S>
    <S sid="188" ssid="21">Acknowledgments.</S>
    <S sid="189" ssid="22">This work was supported in part by NSF Grants IRI-9624639 and GER-9454149.</S>
    <S sid="190" ssid="23">We thank Mitre for providing their part-of-speech tagger.</S>
  </SECTION>
</PAPER>
