<PAPER>
  <S sid="0">A Maximum Entropy Model For Part-Of-Speech Tagging</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper presents a statistical model which trains from a corpus annotated with Part-Of- Speech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%).</S>
    <S sid="2" ssid="2">The can be classified as a Entropy model and simultaneously uses many contextual &amp;quot;features&amp;quot; to predict the POS tag.</S>
    <S sid="3" ssid="3">Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.</S>
  </ABSTRACT>
  <SECTION title="Introduction" number="1">
    <S sid="4" ssid="1">Many natural language tasks require the accurate assignment of Part-Of-Speech (POS) tags to previously unseen text.</S>
    <S sid="5" ssid="2">Due to the availability of large corpora which have been manually annotated with POS information, many taggers use annotated text to &amp;quot;learn&amp;quot; either probability distributions or rules and use them to automatically assign POS tags to unseen text.</S>
    <S sid="6" ssid="3">The experiments in this paper were conducted on the Wall Street Journal corpus from the Penn Treebank project(Marcus et al., 1994), although the model can train from any large corpus annotated with POS tags.</S>
    <S sid="7" ssid="4">Since most realistic natural language applications must process words that were never seen before in training data, all experiments in this paper are conducted on test data that include unknown words.</S>
    <S sid="8" ssid="5">Several recent papers(Brill, 1994, Magerman, 1995) have reported 96.5% tagging accuracy on the Wall St. Journal corpus.</S>
    <S sid="9" ssid="6">The experiments in this paper test the hypothesis that better use of context will improve the accuracy.</S>
    <S sid="10" ssid="7">A Maximum Entropy model is well-suited for such experiments since it cornbines diverse forms of contextual information in a principled manner, and does not impose any distributional assumptions on the training data.</S>
    <S sid="11" ssid="8">Previous uses of this model include language modeling(Lau et al., 1993), machine translation(Berger et al., 1996), prepositional phrase attachment(Ratnaparkhi et al., 1994), and word morphology(Della Pietra et al., 1995).</S>
    <S sid="12" ssid="9">This paper briefly describes the maximum entropy and maximum likelihood properties of the model, features used for POS tagging, and the experiments on the Penn Treebank Wall St. Journal corpus.</S>
    <S sid="13" ssid="10">It then discusses the consistency problems discovered during an attempt to use specialized features on the word context.</S>
    <S sid="14" ssid="11">Lastly, the results in this paper are compared to those from previous work on POS tagging.</S>
    <S sid="15" ssid="12">The Probability Model The probability model is defined over It x T, where fl is the set of possible word and tag contexts, or &amp;quot;histories&amp;quot;, and T is the set of allowable tags.</S>
    <S sid="16" ssid="13">The model's probability of a history h together with a tag t is defined as: where ir is a normalization constant, fp, cu,.</S>
    <S sid="17" ssid="14">, a} are the positive model parameters and { , fk} are known as &amp;quot;features&amp;quot;, where fj (h, t) E {OM.</S>
    <S sid="18" ssid="15">Note that each parameter aj corresponds to a feature fj.</S>
    <S sid="19" ssid="16">Given a sequence of words {w1, , w} and tags {t1, .tn} as training data, define hi as the history available when predicting ti.</S>
    <S sid="20" ssid="17">The parameters {p, ai , &#8226; .. , } are then chosen to maximize the likelihood of the training data using p: This model also can be interpreted under the Maximum Entropy formalism, in which the goal is to maximize the entropy of a distribution subject to certain constraints.</S>
    <S sid="21" ssid="18">Here, the entropy of the distribution p is defined as: where the model's feature expectation is and the observed feature expectation is Ef =Epoi,ti) and where (h1, t1) denotes the observed probability of (hi , ti) in the training data.</S>
    <S sid="22" ssid="19">Thus the constraints force the model to match its feature expectations with those observed in the training data.</S>
    <S sid="23" ssid="20">In practice, I-1 is very large and the model's expectation E fi cannot be computed directly, so the following approximation(Lau et al., 1993) is used: where /3(h1) is the observed probability of the history hi in the training set.</S>
    <S sid="24" ssid="21">It can be shown (Darroch and Ratcliff, 1972) that if p has the form (1) and satisfies the k constraints (2), it uniquely maximizes the entropy H (p) over distributions that satisfy (2), and uniquely maximizes the likelihood L(p) over distributions of the form (1).</S>
    <S sid="25" ssid="22">The model parameters for the distribution p are obtained via Generalized Iterative Sca/ing(Darroch and Ratcliff, 1972).</S>
    <S sid="26" ssid="23">The joint probability of a history h and tag t is determined by those parameters whose corresponding features are active, i.e., those aj such that f (h,t) = 1.</S>
    <S sid="27" ssid="24">A feature, given (h,t), may activate on any word or tag in the history h, and must encode any information that might help predict t, such as the spelling of the current word, or the identity of the previous two tags.</S>
    <S sid="28" ssid="25">The specific word and tag context available to a feature is given in the following definition of a history hi: If the above feature exists in the feature set of the model, its corresponding model parameter will contribute towards the joint probability p(hi,ti) when wi ends with &amp;quot;ing&amp;quot; and when ti =VBG1.</S>
    <S sid="29" ssid="26">Thus a model parameter aj effectively serves as a &amp;quot;weight&amp;quot; for a certain contextual predictor, in this case the suffix &amp;quot;ing&amp;quot;, towards the probability of observing a certain tag, in this case a VBG.</S>
    <S sid="30" ssid="27">The model generates the space of features by scanning each pair (hi ,ti) in the training data with the feature &amp;quot;templates&amp;quot; given in Table 1.</S>
    <S sid="31" ssid="28">Given hi as the current history, a feature always asks some yes/no question about hi, and furthermore constrains ti to be a certain tag.</S>
    <S sid="32" ssid="29">The instantiations for the variables X, Y, and T in Table 1 are obtained automatically from the training data.</S>
    <S sid="33" ssid="30">The generation of features for tagging unknown words relies on the hypothesized distinction that &amp;quot;rare&amp;quot; words' in the training set are similar to unknown words in test data, with respect to how their spellings help predict their tags.</S>
    <S sid="34" ssid="31">The rare word features in Table 1, which look at the word spellings, will apply to both rare words and unknown words in test data.</S>
    <S sid="35" ssid="32">For example, Table 2 contains an excerpt from training data while Table 3 contains the features generated while scanning (h3, t3), in which the current word is about, and Table 4 contains features generated while scanning (h4, 14), in which the current word, well-heeled, occurs 3 times in training data and is therefore classified as &amp;quot;rare&amp;quot;.</S>
    <S sid="36" ssid="33">The behavior of a feature that occurs very sparsely in the training set is often difficult to predict, since its statistics may not be reliable.</S>
    <S sid="37" ssid="34">Therefore, the model uses the heuristic that any feature Condition Features wi is not rare wi = X wi is rare Xis prefix of wi, IXI &lt;4 &amp; ti = T X is suffix of wi, IXI &lt; 4 wi contains number &amp; ti = T wi contains uppercase character &amp; t&#8226; = T wi contains hyphen &amp; ti = T which occurs less than 10 times in the data is unreliable, and ignores features whose counts are less than 10.3 While there are many smoothing algorithms which use techniques more rigorous than a simple count cutoff, they have not yet been investigated in conjunction with this tagger.</S>
  </SECTION>
  <SECTION title="Testing the Model" number="2">
    <S sid="38" ssid="1">The test corpus is tagged one sentence at a time.</S>
    <S sid="39" ssid="2">The testing procedure requires a search to enumerate the candidate tag sequences for the sentence, and the tag sequence with the highest probability is chosen as the answer.</S>
    <S sid="40" ssid="3">The search algorithm, essentially a &amp;quot;beam search&amp;quot;, uses the conditional tag probability and maintains, as it sees a new word, the N highest probability tag sequence candidates up to that point in the sentence.</S>
    <S sid="41" ssid="4">Given a sentence {wl.</S>
    <S sid="42" ssid="5">&#8226; &#8226; &#8226; w,}, a tag sequence candidate {ti .</S>
    <S sid="43" ssid="6">&#8226; &#8226; tn} has conditional probability: In addition the search procedure optionally consults a Tag Dictionary, which, for each known word, lists the tags that it has appeared with in the training set.</S>
    <S sid="44" ssid="7">If the Tag Dictionary is in effect, the search procedure, for known words, generates only tags given by the dictionary entry, while for unknown words, generates all tags in the tag set.</S>
    <S sid="45" ssid="8">Without the Tag Dictionary, the search procedure generates all tags in the tag set for every word.</S>
    <S sid="46" ssid="9">Let W = {wi ...wn} be a test sentence, and let sij be the jth highest probability tag sequence up to and including word wi.</S>
    <S sid="47" ssid="10">The search is described below:</S>
  </SECTION>
  <SECTION title="Experiments" number="3">
    <S sid="48" ssid="1">In order to conduct tagging experiments, the Wall St. Journal data has been split into three contiguous sections, as shown in Table 5.</S>
    <S sid="49" ssid="2">The feature set and search algorithm were tested and debugged only on the Training and Development sets, and the official test result on the unseen Test set is presented in the conclusion of the paper.</S>
    <S sid="50" ssid="3">The performances of the &amp;quot;baseline&amp;quot; model on the Development Set, both with and without the Tag Dictionary, are shown in Table 6.</S>
    <S sid="51" ssid="4">All experiments use a beam size of N = 5; further increasing the beam size does not significantly increase performance on the Development Set but adversely affects the speed of the tagger.</S>
    <S sid="52" ssid="5">Even though use of the Tag Dictionary gave an apparently insignificant (.12%) improvement in accuracy, it is used in further experiments since it significantly reduces the number of hypotheses and thus speeds up the tagger.</S>
    <S sid="53" ssid="6">The running time of the parameter estimation algorithm is 0(NTA), where N is the training set size, T is the number of allowable tags, and A is the average number of features that are active for a given event (h, t).</S>
    <S sid="54" ssid="7">The running time of the search procedure on a sentence of length N is 0(NTAB), where T, A are defined above, and B is the beam size.</S>
    <S sid="55" ssid="8">In practice, the model for the experiment shown in Table 6 requires approximately 24 hours to train, and 1 hour to test' on an IBM RS/6000 Model 380 with 256MB of RAM.</S>
  </SECTION>
  <SECTION title="Specialized Features and Consistency" number="4">
    <S sid="56" ssid="1">The Maximum Entropy model allows arbitrary binary-valued features on the context, so it can use additional specialized, i.e., word-specific, features to correctly tag the &amp;quot;residue&amp;quot; that the baseline features cannot model.</S>
    <S sid="57" ssid="2">Since such features typically occur infrequently, the training set consistency must be good enough to yield reliable statistics.</S>
    <S sid="58" ssid="3">Otherwise the specialized features will model noise and perform poorly on test data.</S>
    <S sid="59" ssid="4">Such features can be designed for those words which are especially problematic for the model.</S>
    <S sid="60" ssid="5">The top errors of the model (over the training set) are shown in Table 7; clearly, the model has trouble with the words that and about, among others.</S>
    <S sid="61" ssid="6">As hypothesized in the introduction, better features on the context surrounding that and about should correct the tagging mistakes for these two words, assuming that the tagging errors are due to an impoverished feature set, and not inconsistent data.</S>
    <S sid="62" ssid="7">Specialized features for a given word are constructed by conjoining certain features in the baseline model with a question about the word itself.</S>
    <S sid="63" ssid="8">The features which ask about previous tags and surrounding words now additionally ask about the identity of the current word, e.g., a specialized feature for the word about in Table 3 could be: Table 8 shows the results of an experiment in which specialized features are constructed for &amp;quot;difficult&amp;quot; words, and are added to the baseline feature set.</S>
    <S sid="64" ssid="9">Here, &amp;quot;difficult&amp;quot; words are those that are mistagged a certain way at least 50 times when the training set is tagged with the baseline model.</S>
    <S sid="65" ssid="10">Using the set of 29 difficult words, the model performs at 96.49% accuracy on the Development Set, an insignificant improvement from the baseline accuracy of 96.43%.</S>
    <S sid="66" ssid="11">Table 9 shows the change in error rates on the Development Set for the frequently occurring &amp;quot;difficult&amp;quot; words.</S>
    <S sid="67" ssid="12">For most words, the specialized model yields little or no improvement, and for some, i.e., more and about, the specialized model performs worse.</S>
    <S sid="68" ssid="13">The lack of improvement implies that either the feature set is still impoverished, or that the training data is inconsistent.</S>
    <S sid="69" ssid="14">A simple consistency test is to graph the POS tag assignments for a given word as a function of the article in which it occurs.</S>
    <S sid="70" ssid="15">Consistently tagged words should have roughly the same tag distribution as the article numbers vary.</S>
    <S sid="71" ssid="16">Figure 1 represents each POS tag with a unique integer and graphs the POS annotation of about in the training set as a function of the article# (the points are &amp;quot;scattered&amp;quot; to show density).</S>
    <S sid="72" ssid="17">As seen in figure 1, about is usually annotated with tag#1, which denotes IN (preposition), or tag#9, which denotes RB (adverb), and the observed probability of either choice depends heavily on the current article#.</S>
    <S sid="73" ssid="18">Upon further examination', the tagging distribution for about changes precisely when the annotator changes.</S>
    <S sid="74" ssid="19">Figure 2, which again uses integers to denote POS tags, shows the tag distribution of about as a function of annotator, and implies that the tagging errors for this word are due mostly to inconsistent data.</S>
    <S sid="75" ssid="20">The words ago, chief, down, executive, off, out, up and yen also exhibit similar bias.</S>
    <S sid="76" ssid="21">Thus specialized features may be less effective for those words affected by inter-annotator bias.</S>
    <S sid="77" ssid="22">A simple solution to eliminate inter-annotator inconsistency is to train and test the model on data that has been created by the same annotator.</S>
    <S sid="78" ssid="23">The results of such an experiment' are shown in Table 10.</S>
    <S sid="79" ssid="24">The total accuracy is higher, implying that the singly-annotated training and test sets are more consistent, and the improvement due to the specialized features is higher than before (.1%) but still modest, implying that either the features need further improvement or that intra-annotator inconsistencies exist in the corpus.</S>
    <S sid="80" ssid="25">Most of the recent corpus-based POS taggers in the literature are either statistically based, and use Markov Model(Weischedel et al., 1993, Merialdo, 1994) or Statistical Decision Tree(Jelinek et al., 1994, Magerman, 1995)(SDT) techniques, or are primarily rule based, such as Brill's Transformation Based Learner(Brill, 1994)(TBL).</S>
    <S sid="81" ssid="26">The Maximum Entropy (MaxEnt) tagger presented in this paper combines the advantages of all these methods.</S>
    <S sid="82" ssid="27">It uses a rich feature representation, like TBL and SDT, and generates a tag probability distribution for each word, like Decision Tree and Markov Model techniques.</S>
    <S sid="83" ssid="28">(Weischedel et al., 1993) provide the results from a battery of &amp;quot;tri-tag&amp;quot; Markov Model experiments, in which the probability P(W,T) of observing a word sequence W = {wi, w2, &#8226; &#8226; &#8226; , wn} together with a tag sequence T = is given by: Furthermore, p(wiiti) for unknown words is computed by the following heuristic, which uses a set of 35 pre-determined endings: This approximation works as well as the MaxEnt model, giving 85% unknown word accuracy(Weischedel et al., 1993) on the Wall St. Journal, but cannot be generalized to handle more diverse information sources.</S>
    <S sid="84" ssid="29">Multiplying together all the probabilities becomes less convincing of an approximation as the information sources become less independent.</S>
    <S sid="85" ssid="30">In contrast, the MaxEnt model combines diverse and non-local information sources without making any independence assumptions.</S>
    <S sid="86" ssid="31">A POS tagger is one component in the SDT based statistical parsing system described in (Jelinek et al., 1994, Magerman, 1995).</S>
    <S sid="87" ssid="32">The total word accuracy on Wall St. Journal data, 96.5%(Magerman, 1995), is similar to that presented in this paper.</S>
    <S sid="88" ssid="33">However, the aforementioned SDT techniques require word classes(Brown et al., 1992) to help prevent data fragmentation, and a sophisticated smoothing algorithm to mitigate the effects of any fragmentation that occurs.</S>
    <S sid="89" ssid="34">Unlike SDT, the MaxEnt training procedure does not recursively split the data, and hence does not suffer from unreliable counts due to data fragmentation.</S>
    <S sid="90" ssid="35">As a result, no word classes are required and a trivial count cutoff suffices as a smoothing procedure in order to achieve roughly the same level of accuracy.</S>
    <S sid="91" ssid="36">TBL is a non-statistical approach to POS tagging which also uses a rich feature representation, and performs at a total word accuracy of 96.5% and an unknown word accuracy of 85%.</S>
    <S sid="92" ssid="37">(Brill, 1994).</S>
    <S sid="93" ssid="38">The TBL representation of the surrounding word context is almost the same7 and the TBL representation of unknown words is a superset8 of the unknown word representation in this paper.</S>
    <S sid="94" ssid="39">However, since TBL is non-statistical, it does not provide probability distributions and 7 (Brill, 1994) looks at words &#177;3 away from the current, whereas the feature set in this paper uses a window of &#177;2.</S>
    <S sid="95" ssid="40">8(Brill, 1994) uses prefix/suffix additions and deletions, which are not used in this paper. unlike MaxEnt, cannot be used as a probabilistic component in a larger model.</S>
    <S sid="96" ssid="41">MaxEnt can provide a probability for each tagging decision, which can be used in the probability calculation of any structure that is predicted over the POS tags, such as noun phrases, or entire parse trees, as in (Jelinek et al., 1994, Magerman, 1995).</S>
    <S sid="97" ssid="42">Thus MaxEnt has at least one advantage over each of the reviewed POS tagging techniques.</S>
    <S sid="98" ssid="43">It is better able to use diverse information than Markov Models, requires less supporting techniques than SDT, and unlike TBL, can be used in a probabilistic framework.</S>
    <S sid="99" ssid="44">However, the POS tagging accuracy on the Penn Wall St. Journal corpus is roughly the same for all these modelling techniques.</S>
    <S sid="100" ssid="45">The convergence of the accuracy rate implies that either all these techniques are missing the right predictors in their representation to get the &amp;quot;residue&amp;quot;, or more likely, that any corpus based algorithm on the Penn Treebank Wall St. Journal corpus will not perform much higher than 96.5% due to consistency problems.</S>
  </SECTION>
  <SECTION title="Conclusion" number="5">
    <S sid="101" ssid="1">The Maximum Entropy model is an extremely flexible technique for linguistic modelling, since it can use a virtually unrestricted and rich feature set in the framework of a probability model.</S>
    <S sid="102" ssid="2">The implementation in this paper is a state-of-the-art POS tagger, as evidenced by the 96.6% accuracy on the unseen Test set, shown in Table 11.</S>
    <S sid="103" ssid="3">The model with specialized features does not perform much better than the baseline model, and further discovery or refinement of word-based features is difficult given the inconsistencies in the training data.</S>
    <S sid="104" ssid="4">A model trained and tested on data from a single annotator performs at .5% higher accuracy than the baseline model and should produce more consistent input for applications that require tagged text.</S>
  </SECTION>
</PAPER>
