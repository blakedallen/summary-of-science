Evaluating Content Selection In Summarization: The Pyramid Method
We present an empirically grounded method for evaluating content selection in summarization.
It incorporates the idea that no single best model summary for a collection of documents exists.
Our method quantifies the relative importance of facts to be conveyed.
We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
We propose a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents.
