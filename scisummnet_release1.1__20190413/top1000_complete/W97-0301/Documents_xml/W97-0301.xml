<PAPER>
  <S sid="0">A Linear Observed Time Statistical Parser Based On Maximum Entropy Models</S>
  <ABSTRACT>
    <S sid="1" ssid="1">3.</S>
    <S sid="2" ssid="2">A search heuristic which attempts to find the highest scoring parse tree for a given input sentence.</S>
    <S sid="3" ssid="3">Abstract This paper presents a statistical parser for natural language that obtains a parsing accuracy&#8212;roughly 87% precision and 86% recall&#8212;which surpasses the best previously published results on the Wall St. Journal domain.</S>
    <S sid="4" ssid="4">The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. running time of the parser on test sentence linear with respect to the sentence length.</S>
    <S sid="5" ssid="5">Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring a dramatically higher accuracy of 93% precision and recall.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="6" ssid="1">This paper presents a statistical parser for natural language that obtains a parsing accuracy&#8212;roughly 87% precision and 86% recall&#8212;which surpasses the best previously published results on the Wall St. Journal domain.</S>
    <S sid="7" ssid="2">The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework.</S>
    <S sid="8" ssid="3">The observed running time of the parser on a test sentence is linear with respect to the sentence length.</S>
    <S sid="9" ssid="4">Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.</S>
  </SECTION>
  <SECTION title="1 Introduction" number="2">
    <S sid="10" ssid="1">This paper presents a statistical parser for natural language that finds one or more scored syntactic parse trees for a given input sentence.</S>
    <S sid="11" ssid="2">The parsing accuracy&#8212;roughly 87% precision and 86% recall&#8212; surpasses the best previously published results on the Wall St. Journal domain.</S>
    <S sid="12" ssid="3">The parser consists of the following three conceptually distinct parts: The maximum entropy models used here are similar in form to those in (Ratnaparkhi, 1996; Berger, Della Pietra, and Della Pietra, 1996; Lau, Rosenfeld, and Roukos, 1993).</S>
    <S sid="13" ssid="4">The models compute the probabilities of actions based on certain syntactic characteristics, or features, of the current context.</S>
    <S sid="14" ssid="5">The features used here are defined in a concise and simple manner, and their relative importance is determined automatically by applying a training procedure on a corpus of syntactically annotated sentences, such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz, 1994).</S>
    <S sid="15" ssid="6">Although creating the annotated corpus requires much linguistic expertise, creating the feature set for the parser itself requires very little linguistic effort.</S>
    <S sid="16" ssid="7">Also, the search heuristic is very simple, and its observed running time on a test sentence is linear with respect to the sentence length.</S>
    <S sid="17" ssid="8">Furthermore, the search heuristic returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.</S>
    <S sid="18" ssid="9">Sections 2, 3, and 4 describe the tree-building procedures, the maximum entropy models, and the search heuristic, respectively.</S>
    <S sid="19" ssid="10">Section 5 describes experiments with the Penn Treebank and section 6 compares this paper with previously published works.</S>
  </SECTION>
  <SECTION title="2 Procedures for Building Trees" number="3">
    <S sid="20" ssid="1">The parser uses four procedures, TAG, CHUNK, BUILD, and CHECK, that incrementally build parse trees with their actions.</S>
    <S sid="21" ssid="2">The procedures are applied in three left-to-right passes over the input sentence; the first pass applies TAG, the second pass applies CHUNK, and the third pass applies BUILD and CHECK.</S>
    <S sid="22" ssid="3">The passes, the procedures they apply, and the actions of the procedures are summarized in table 1 and described below.</S>
    <S sid="23" ssid="4">The actions of the procedures are designed so that any possible complete parse tree T for the input sentence corresponds to exactly one sequence of actions; call this sequence the derivation of T. Each procedure, when given a derivation d = {al an}, predicts some action an&#177;i to create a new derivation d' = {al an+i }.</S>
    <S sid="24" ssid="5">Typically, the procedures postulate many different values for a+1, which cause the parser to explore many different derivations when parsing an input sentence.</S>
    <S sid="25" ssid="6">But for demonstration purposes, figures 1-7 trace one possible derivation for the sentence &amp;quot;I saw the man with the telescope&amp;quot;, using the part-of-speech (POS) tag set and constituent label set of the Penn treebank.</S>
    <S sid="26" ssid="7">The first pass takes an input sentence, shown in figure 1, and uses TAG to assign each word a POS tag.</S>
    <S sid="27" ssid="8">The result of applying TAG to each word is shown in figure 2.</S>
    <S sid="28" ssid="9">The second pass takes the output of the first pass and uses CHUNK to determine the &amp;quot;flat&amp;quot; phrase chunks of the sentence, where a phrase is &amp;quot;flat&amp;quot; if and only if it is a constituent whose children consist solely of POS tags.</S>
    <S sid="29" ssid="10">Starting from the left, CHUNK assigns each (word,POS tag) pair a &amp;quot;chunk&amp;quot; tag, either Start X, Join X, or Other.</S>
    <S sid="30" ssid="11">Figure 3 shows the result after the second pass.</S>
    <S sid="31" ssid="12">The chunk tags are then used for chunk detection, in which any consecutive sequence of words vim wn (m &lt; n) are grouped into a &amp;quot;fiat&amp;quot; chunk X if tom has been assigned Start X and wn,fi wn have all been assigned Join X.</S>
    <S sid="32" ssid="13">The result of chunk detection, shown in figure 4, is a forest of trees and serves as the input to the third pass.</S>
    <S sid="33" ssid="14">The third pass always alternates between the use of BUILD and CHECK, and completes any remaining constituent structure.</S>
    <S sid="34" ssid="15">BUILD decides whether a tree will start a new constituent or join the incomplete constituent immediately to its left.</S>
    <S sid="35" ssid="16">Accordingly, it annotates the tree with either Start X, where X is any constituent label, or with Join X, where X matches the label of the incomplete constituent to the left.</S>
    <S sid="36" ssid="17">BUILD always processes the leftmost tree without any Start X or Join X annotation.</S>
    <S sid="37" ssid="18">Figure 5 shows an application of BUILD in which the action is Join VP.</S>
    <S sid="38" ssid="19">After BUILD, control passes to CHECK, which finds the most recently proposed constituent, and decides if it is complete.</S>
    <S sid="39" ssid="20">The most recently proposed constituent, shown in figure 6, is the rightmost sequence of trees tni .</S>
    <S sid="40" ssid="21">&#8226; &#8226; in (Tt.</S>
    <S sid="41" ssid="22">&lt; n) such that tin is annotated with Start X and t&#8222;,&#177;1 tn are annotated with Join X.</S>
    <S sid="42" ssid="23">If CHECK decides yes, then the proposed constituent takes its place in the forest as an actual constituent, on which BUILD does its work.</S>
    <S sid="43" ssid="24">Otherwise, the constituent is not finished and BUILD processes the next tree in the forest, tn+1.</S>
    <S sid="44" ssid="25">CHECK always answers no if the proposed constituent is a &amp;quot;flat&amp;quot; chunk, since such constituents must be formed in the second pass.</S>
    <S sid="45" ssid="26">Figure 7 shows the result when CHECK looks at the proposed constituent in figure 6 and decides No.</S>
    <S sid="46" ssid="27">The third pass terminates when CHECK is presented a constituent that spans the entire sentence.</S>
    <S sid="47" ssid="28">Table 2 compares the actions of BUILD and CHECK to the operations of a standard shift-reduce parser.</S>
    <S sid="48" ssid="29">The No and Yes actions of CHECK correspond to the shift and reduce actions, respectively.</S>
    <S sid="49" ssid="30">The important difference is that while a shift-reduce parser creates a constituent in one step (reduce a), the procedures BUILD and CHECK create it over several steps in smaller increments.</S>
  </SECTION>
  <SECTION title="3 Probability Model" number="4">
    <S sid="50" ssid="1">This paper takes a &amp;quot;history-based&amp;quot; approach (Black et al., 1993) where each tree-building procedure uses a probability model p(alb), derived from p(a, b), to weight any action a based on the available context, or history, b.</S>
    <S sid="51" ssid="2">First, we present a few simple categories of contextual predicates that capture any information in b that is useful for predicting a.</S>
    <S sid="52" ssid="3">Next, the predicates are used to extract a set of features from a corpus of manually parsed sentences.</S>
    <S sid="53" ssid="4">Finally, those features are combined under the maximum entropy framework, yielding p(a, b).</S>
    <S sid="54" ssid="5">Contextual predicates are functions that check for the presence or absence of useful information in a context b and return true or false accordingly.</S>
    <S sid="55" ssid="6">The comprehensive guidelines, or templates, for the contextual predicates of each tree building procedure are given in table 3.</S>
    <S sid="56" ssid="7">The templates use indices relative to the tree that is currently being modified.</S>
    <S sid="57" ssid="8">For example, if the current tree is the 5th tree, cons(-2) looks at the constituent label, head word, and start/join annotation of the 3rd tree in the forest.</S>
    <S sid="58" ssid="9">The actual contextual predicates are generated automatically by scanning the derivations of the trees in the manually parsed corpus with the templates.</S>
    <S sid="59" ssid="10">For example, an actual contextual predicate based on the template cons(0) might be &amp;quot;Does cons(0) ={ NP, he } ?&amp;quot; Constituent head words are found, when necessary, with the algorithm in (Magerman, 1995).</S>
    <S sid="60" ssid="11">Contextual predicates which look at head words, or especially pairs of head words, may not be reliable predictors for the procedure actions due to their sparseness in the training sample.</S>
    <S sid="61" ssid="12">Therefore, for each lexically based contextual predicate, there also exist one or more corresponding less specific, or &amp;quot;backed-off&amp;quot;, contextual predicates which look at the same context, but omit one or more words.</S>
    <S sid="62" ssid="13">For example, the contexts cons(0, 1*), cons(0*, 1), cons(0*, 1*) are the same as cons(0, 1) but omit references to the head word of the 1st tree, the 0th tree, and both the 0th and 1st tree, respectively.</S>
    <S sid="63" ssid="14">The backed-off contextual predicates should allow the model to provide reliable probability estimates when the words in the history are rare.</S>
    <S sid="64" ssid="15">Backed-off predicates are not enumerated in table 3, but their existence is indicated with a * and t. The contextual predicates derived from the templates of table 3 are used to create the features necessary for the maximum entropy models.</S>
    <S sid="65" ssid="16">The predicates for TAG, CHUNK, BUILD, and CHECK are used to scan the derivations of the trees in the corpus to form the training samples T 'TcHEci,, respectively.</S>
    <S sid="66" ssid="17">Each training sample has the form T = (ai b1), (a2, b2), , (as, bs)}, where ai is an action of the corresponding procedure and bi is the list of contextual predicates that were true in the context in which ai was decided.</S>
    <S sid="67" ssid="18">The training samples are respectively used to create the models n TAG, PCHUNK PBUILD and PCHECK all of which have the form: malization constant, aj are the model parameters, 0 &lt; aj &lt;00, and fj(a, b) E 10,1} are called features, j = {1... k}.</S>
    <S sid="68" ssid="19">Features encode an action a' as well as some contextual predicate cp that a tree-building procedure would find useful for predicting the action a'.</S>
    <S sid="69" ssid="20">Any contextual predicate cp derived from table 3 which occurs 5 or more times in a training sample with a particular action a' is used to construct a feature fj: for use in the corresponding model.</S>
    <S sid="70" ssid="21">Each feature fj corresponds to a parameter aj, which can be viewed as a &amp;quot;weight&amp;quot; that reflects the importance of the feature.</S>
    <S sid="71" ssid="22">The parameters {ai ... an} are found automatically with Generalized Iterative Scaling (Darroch and Ratcliff, 1972), or GIS.</S>
    <S sid="72" ssid="23">The GIS procedure, as well as the maximum entropy and maximum likelihood properties of the distribution of form (1), are described in detail in (Ratnaparkhi, 1997).</S>
    <S sid="73" ssid="24">In general, the maximum entropy framework puts no limitations on the kinds of features in the model; no special estimation technique is required to combine features that encode different kinds of contextual predicates, like punctuation and cons(0, 1, 2).</S>
    <S sid="74" ssid="25">As a result, experimenters need only worry about what features to use, and not how to use them.</S>
    <S sid="75" ssid="26">-AG PCHUNK PBUILD and We then use the models PT AG, to define a function score, which the search procedure uses to rank derivations of incomplete and complete parse trees.</S>
    <S sid="76" ssid="27">For each model, the corresponding conditional probability is defined as usual: if a is an action from CHECK Let deriv(T) = {al, ,a,,} be the derivation of a parse T, where T is not necessarily complete, and where each ai is an action of some tree-building procedure.</S>
    <S sid="77" ssid="28">By design, the tree-building procedures guarantee that {al, , an} is the only derivation for the parse T. Then the score of T is merely the product of the conditional probabilities of the individual actions in its derivation: where bi is the context in which ai was decided.</S>
  </SECTION>
  <SECTION title="4 Search" number="5">
    <S sid="78" ssid="1">The search heuristic attempts to find the best parse T*, defined as: where trees(S) are all the complete parses for an input sentence S. The heuristic employs a breadth-first search (BFS) which does not explore the entire frontier, but rather, explores only at most the top K scoring incomplete parses in the frontier, and terminates when it has found M complete parses, or when all the hypotheses have been exhausted.</S>
    <S sid="79" ssid="2">Furthermore, if {al ... an} are the possible actions for a given procedure on a derivation with context b, and they are sorted in decreasing order according to q(ailb), we only consider exploring those actions {al ... am} that hold most of the probability mass, where m is defined as follows: and where Q is a threshold less than 1.</S>
    <S sid="80" ssid="3">The search also uses a Tag Dictionary constructed from training data, described in (Ratnaparkhi, 1996), that reduces the number of actions explored by the tagging model.</S>
    <S sid="81" ssid="4">Thus there are three parameters for the search heuristic, namely K ,M, and Q and all experiments reported in this paper use K = 20, M = 20, and Q = .951 Table 4 describes the top K BFS and the semantics of the supporting functions.</S>
    <S sid="82" ssid="5">It should be emphasized that if K&gt; 1, the parser does not commit to a single POS or chunk assignment for the input sentence before building constituent structure.</S>
    <S sid="83" ssid="6">All three of the passes described in section 2 are integrated in the search, i.e., when parsing a test sentence, the input to the second pass consists of K of the best distinct POS tag assignments for the input sentence.</S>
    <S sid="84" ssid="7">Likewise, the input to the third pass consists of K of the best distinct chunk and POS tag assignments for the input sentence.</S>
    <S sid="85" ssid="8">The top K BFS described above exploits the observed property that the individual steps of correct derivations tend to have high probabilities, and thus avoids searching a large fraction of the search space.</S>
    <S sid="86" ssid="9">Since, in practice, it only does a constant amount of work to advance each step in a derivation, and since derivation lengths are roughly proportional to the 'The parameters K,M, and Q were optimized on &amp;quot;held out&amp;quot; data separate from the training and test sets. if cp(b) = true /kik a = a' advance: dxV&#8212;+di... /* Applies relevant tree building procedure to d and returns list of new derivations whose action probabilities pass the threshold Q */ insert: dxh&#8212;+ void /* inserts d in heap h */ extract: h d /* removes and returns derivation in h with highest score */ completed: d&#8212;&gt; {true,false} /* returns true if and only if d is a complete derivation */ sentence length, we would expect it to run in linear observed time with respect to sentence length.</S>
    <S sid="87" ssid="10">Figure 8 confirms our assumptions about the linear observed running time.</S>
  </SECTION>
  <SECTION title="5 Experiments" number="6">
    <S sid="88" ssid="1">The maximum entropy parser was trained on sections 2 through 21 (roughly 40000 sentences) of the Penn Treebank Wall St. Journal corpus, release 2 (Marcus, Santorini, and Marcinkiewicz, 1994), and tested on section 23 (2416 sentences) for comparison with other work.</S>
    <S sid="89" ssid="2">All trees were stripped of their semantic tags (e.g., -LOC, -BNF, etc.</S>
    <S sid="90" ssid="3">), coreference information(e.g., *-1), and quotation marks ( &amp;quot; and &amp;quot; ) for both training and testing.</S>
    <S sid="91" ssid="4">The PARSEVAL (Black and others, 1991) measures compare a proposed parse P with the corresponding correct treebank parse T as follows: A constituent in P is &amp;quot;correct&amp;quot; if there exists a constituent in T of the same label that spans the same words.</S>
    <S sid="92" ssid="5">Table 5 shows results using the PARSEVAL measures, as well as results using the slightly more forgiving measures of (Collins, 1996) and (Magerman, 1995).</S>
    <S sid="93" ssid="6">Table 5 shows that the maximum entropy parser performs better than the parsers presented in (Collins, 1996) and (Magerman, 1995)2, which have the best previously published parsing accuracies on the Wall St. Journal domain.</S>
    <S sid="94" ssid="7">It is often advantageous to produce the top N parses instead of just the top 1, since additional information can be used in a secondary model that reorders the top N and hopefully improves the quality of the top ranked parse.</S>
    <S sid="95" ssid="8">Suppose there exists a &amp;quot;perfect&amp;quot; reranking scheme that, for each sentence, magically picks the best parse from the top N parses produced by the maximum entropy parser, where the best parse has the highest average precision and recall when compared to the treebank parse.</S>
    <S sid="96" ssid="9">The performance of this &amp;quot;perfect&amp;quot; scheme is then an upper bound on the performance of any reranking scheme that might be used to reorder the top N parses.</S>
    <S sid="97" ssid="10">Figure 9 shows that the &amp;quot;perfect&amp;quot; scheme would achieve roughly 93% precision and recall, which is a dramatic increase over the top 1 accuracy of 87% precision and 86% recall.</S>
    <S sid="98" ssid="11">Figure 10 shows that the &amp;quot;Exact Match&amp;quot;, which counts the percentage of times the proposed parse P is identical (excluding POS tags) to the treebank parse T, rises substantially to about 53% from 30% when the &amp;quot;perfect&amp;quot; scheme is applied.</S>
    <S sid="99" ssid="12">For this reason, research into reranking schemes appears to be a promising step towards the goal of improving parsing accuracy.</S>
  </SECTION>
  <SECTION title="6 Comparison With Previous Work" number="7">
    <S sid="100" ssid="1">The two parsers which have previously reported the best accuracies on the Penn Treebank Wall St. Journal are the bigram parser described in (Collins, 1996) and the SPATTER parser described in (Jelinek et al., 1994; Magerman, 1995).</S>
    <S sid="101" ssid="2">The parser presented here outperforms both the bigram parser and the SPATTER parser, and uses different modelling technology and different information to drive its decisions.</S>
    <S sid="102" ssid="3">The bigram parser is a statistical CKY-style chart parser, which uses cooccurrence statistics of headmodifier pairs to find the best parse.</S>
    <S sid="103" ssid="4">The maximum entropy parser is a statistical shift-reduce style parser that cannot always access head-modifier pairs.</S>
    <S sid="104" ssid="5">For example, the checkcons(m, n) predicate of the maximum entropy parser may use two words such that neither is the intended head of the proposed consituent that the CHECK procedure must judge.</S>
    <S sid="105" ssid="6">And unlike the bigram parser, the maximum entropy parser cannot use head word information besides &amp;quot;flat&amp;quot; chunks in the right context.</S>
    <S sid="106" ssid="7">The bigram parser uses a backed-off estimation scheme that is customized for a particular task, whereas the maximum entropy parser uses a general purpose modelling technique.</S>
    <S sid="107" ssid="8">This allows the maximum entropy parser to easily integrate varying kinds of features, such as those for punctuation, whereas the bigram parser uses hand-crafted punctuation rules.</S>
    <S sid="108" ssid="9">Furthermore, the customized estimation framework of the bigram parser must use information that has been carefully selected for its value, whereas the maximum entropy framework ro</S>
  </SECTION>
  <SECTION title="% Accuracy 39" number="8">
    <S sid="109" ssid="1">bustly integrates any kind of information, obviating the need to screen it first.</S>
    <S sid="110" ssid="2">The SPATTER parser is a history-based parser that uses decision tree models to guide the operations of a few tree building procedures.</S>
    <S sid="111" ssid="3">It differs from the maximum entropy parser in how it builds trees and more critically, in how its decision trees use information.</S>
    <S sid="112" ssid="4">The SPATTER decision trees use predicates on word classes created with a statistical clustering technique, whereas the maximum entropy parser uses predicates that contain merely the words themselves, and thus lacks the need for a (typically expensive) word clustering procedure.</S>
    <S sid="113" ssid="5">Furthermore, the top K BFS search heuristic appears to be much simpler than the stack decoder algorithm outlined in (Magerman, 1995).</S>
  </SECTION>
  <SECTION title="7 Conclusion" number="9">
    <S sid="114" ssid="1">The maximum entropy parser presented here achieves a parsing accuracy which exceeds the best previously published results, and parses a test sentence in linear observed time, with respect to the sentence length.</S>
    <S sid="115" ssid="2">It uses simple and concisely specified predicates which can added or modified quickly with little human effort under the maximum entropy framework.</S>
    <S sid="116" ssid="3">Lastly, this paper clearly demonstrates that schemes for reranking the top 20 parses deserve research effort since they could yield vastly better accuracy results.</S>
  </SECTION>
  <SECTION title="8 Acknowledgements" number="10">
    <S sid="117" ssid="1">Many thanks to Mike Collins and Professor Mitch Marcus from the University of Pennsylvania for their helpful comments on this work.</S>
  </SECTION>
</PAPER>
