<PAPER>
	<S sid="0">Tree Kernel-Based Relation Extraction with Context-Sensitive Structured Parse Tree Information</S><ABSTRACT>
		<S sid="1" ssid="1">This paper proposes a tree kernel with contextsensitive structured parse tree information for re lation extraction.</S>
		<S sid="2" ssid="2">It resolves two critical problems in previous tree kernels for relation extraction in two ways.</S>
		<S sid="3" ssid="3">First, it automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the widely-used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.</S>
		<S sid="4" ssid="4">Second, it pro poses a context-sensitive convolution tree kernel, which enumerates both context-free and context sensitive sub-trees by considering their ancestor node paths as their contexts.</S>
		<S sid="5" ssid="5">Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel.</S>
		<S sid="6" ssid="6">Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy?s convolution tree kernel.</S>
		<S sid="7" ssid="7">It also shows that our tree kernel achieves much bet ter performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="8" ssid="8">Relation extraction is to find various predefined se mantic relations between pairs of entities in text.</S>
			<S sid="9" ssid="9">The research in relation extraction has been promoted by the Message Understanding Conferences (MUCs) (MUC, 1987-1998) and the NIST Automatic Content Extraction (ACE) program (ACE, 2002-2005).</S>
			<S sid="10" ssid="10">Ac cording to the ACE Program, an entity is an object or a set of objects in the world and a relation is an explicitly or implicitly stated relationship among enti ties.</S>
			<S sid="11" ssid="11">For example, the sentence ?Bill Gates is the chairman and chief software architect of Microsoft Corporation.?</S>
			<S sid="12" ssid="12">conveys the ACE-style relation ?EMPLOYMENT.exec?</S>
			<S sid="13" ssid="13">between the entities ?Bill Gates?</S>
			<S sid="14" ssid="14">(person name) and ?Microsoft Corporation?</S>
			<S sid="15" ssid="15">(organization name).</S>
			<S sid="16" ssid="16">Extraction of semantic relations between entities can be very useful in many applica tions such as question answering, e.g. to answer the query ?Who is the president of the United States??, and information retrieval, e.g. to expand the query ?George W. Bush?</S>
			<S sid="17" ssid="17">with ?the president of the United States?</S>
			<S sid="18" ssid="18">via his relationship with ?the United States?.</S>
			<S sid="19" ssid="19">Many researches have been done in relation extraction.</S>
			<S sid="20" ssid="20">Among them, feature-based methods (Kamb hatla 2004; Zhou et al, 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entityrelated information to syntactic parse trees, depend ency trees and semantic information.</S>
			<S sid="21" ssid="21">However, it is difficult for them to effectively capture structured parse tree information (Zhou et al2005), which is critical for further performance improvement in rela tion extraction.</S>
			<S sid="22" ssid="22">As an alternative to feature-based methods, tree kernel-based methods provide an elegant solution to explore implicitly structured features by directly computing the similarity between two trees.</S>
			<S sid="23" ssid="23">Although earlier researches (Zelenko et al2003; Culotta and Sorensen 2004; Bunescu and Mooney 2005a) only achieve success on simple tasks and fail on complex tasks, such as the ACE RDC task, tree kernel-based methods achieve much progress recently.</S>
			<S sid="24" ssid="24">As the state-of-the-art, Zhang et al(2006) applied the convo lution tree kernel (Collins and Duffy 2001) and achieved comparable performance with a state-of-the art linear kernel (Zhou et al2005) on the 5 relation types in the ACE RDC 2003 corpus.</S>
			<S sid="25" ssid="25">However, there are two problems in Collins and Duffy?s convolution tree kernel for relation extraction.</S>
			<S sid="26" ssid="26">The first is that the sub-trees enumerated in the tree kernel computation are context-free.</S>
			<S sid="27" ssid="27">That is, each sub-tree enumerated in the tree kernel computation 728 does not consider the context information outside the sub-tree.</S>
			<S sid="28" ssid="28">The second is to decide a proper tree span in relation extraction.</S>
			<S sid="29" ssid="29">Zhang et al(2006) explored five tree spans in relation extraction and it was a bit sur prising to find that the Shortest Path-enclosed Tree (SPT, i.e. the sub-tree enclosed by the shortest path linking two involved entities in the parse tree) performed best.</S>
			<S sid="30" ssid="30">This is contrast to our intuition.</S>
			<S sid="31" ssid="31">For ex ample, ?got married?</S>
			<S sid="32" ssid="32">is critical to determine the relationship between ?John?</S>
			<S sid="33" ssid="33">and ?Mary?</S>
			<S sid="34" ssid="34">in the sen tence ?John and Mary got married?</S>
			<S sid="35" ssid="35">as shown in Figure 1(e).</S>
			<S sid="36" ssid="36">It is obvious that the information con tained in SPT (?John and Marry?)</S>
			<S sid="37" ssid="37">is not enough to determine their relationship.</S>
			<S sid="38" ssid="38">This paper proposes a context-sensitive convolu tion tree kernel for relation extraction to resolve the above two problems.</S>
			<S sid="39" ssid="39">It first automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.</S>
			<S sid="40" ssid="40">Then it proposes a context-sensitive convolution tree kernel, whic h not only enumerates context free sub-trees but also context-sensitive sub-trees by considering their ancestor node paths as their contexts.</S>
			<S sid="41" ssid="41">Moreover, this paper evaluates the complementary nature of different linear kernels and tree kernels via a composite kernel.</S>
			<S sid="42" ssid="42">The layout of this paper is as follows.</S>
			<S sid="43" ssid="43">In Section 2, we review related work in more details.</S>
			<S sid="44" ssid="44">Then, the dynamic context-sensitive tree span and the contextsensitive convolution tree kernel are proposed in Sec tion 3 while Section 4 shows the experimental results.</S>
			<S sid="45" ssid="45">Finally, we conclude our work in Sec tion 5.</S>
	</SECTION>
	<SECTION title="Related Work. " number="2">
			<S sid="46" ssid="1">The relation extraction task was first introduced as part of the Template Element task in MUC6 and then formulated as the Template Relation task in MUC7.</S>
			<S sid="47" ssid="2">Since then, many methods, such as feature-based (Kambhatla 2004; Zhou et al2005, 2006), tree ker nel-based (Zelenko et al2003; Culotta and Sorensen 2004; Bunescu and Mooney 2005a; Zhang et al2006) and composite kernel-based (Zhao and Gris hman 2005; Zhang et al2006), have been proposed in lit erature.</S>
			<S sid="48" ssid="3">For the feature-based methods, Kambhatla (2004) employed Maximum Entropy models to combine diverse lexical, syntactic and semantic features in rela tion extraction, and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus.</S>
			<S sid="49" ssid="4">Zhou et al(2005) further systematically ex plored diverse features through a linear kernel and Support Vector Machines, and achieved the F measures of 68.0 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 cor pus respectively.</S>
			<S sid="50" ssid="5">One problem with the feature-based methods is that they need extensive feature engineering.</S>
			<S sid="51" ssid="6">Another problem is that, although they can ex plore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found dif ficult to well preserve structured information in the parse trees using the feature-based methods.</S>
			<S sid="52" ssid="7">Zhou et al (2006) further improved the performance by ex ploring the commonality among related classes in a class hierarchy using hierarchical learning strategy.</S>
			<S sid="53" ssid="8">As an alternative to the feature-based methods, the kernel-based methods (Haussler, 1999) have been proposed to implicitly explore various features in a high dimensional space by employing a kernel to cal culate the similarity between two objects directly.</S>
			<S sid="54" ssid="9">In particular, the kernel-based methods could be very effective at reducing the burden of feature engineer ing for structured objects in NLP researches, e.g. the tree structure in relation extraction.</S>
			<S sid="55" ssid="10">Zelenko et al (2003) proposed a kernel between two parse trees, which recursively matches nodes from roots to leaves in a top-down manner.</S>
			<S sid="56" ssid="11">For each pair of matched nodes, a subsequence kernel on their child nodes is invoked.</S>
			<S sid="57" ssid="12">They achieved quite success on two simple relation extraction tasks.</S>
			<S sid="58" ssid="13">Culotta and Sorensen (2004) extended this work to estimate simi larity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus.</S>
			<S sid="59" ssid="14">One problem with the above two tree kernels is that matched nodes must be at the same height and have the same path to the root node.</S>
			<S sid="60" ssid="15">Bunescu and Mooney (2005a) pro posed a shortest path dependency tree kernel, which just sums up the number of common word classes at each position in the two paths, and achieved the F-measure of 52.5 on the 5 relation types in the ACE RDC 2003 corpus.</S>
			<S sid="61" ssid="16">They argued that the information to model a relationship between two entities can be typically captured by the shortest path between them in the dependency graph.</S>
			<S sid="62" ssid="17">While the shortest path may not be able to well preserve structured de pendency tree information, another problem with their kernel is that the two paths should have same length.</S>
			<S sid="63" ssid="18">This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004): high preci sion but very low recall.</S>
			<S sid="64" ssid="19">As the state-of-the-art tree kernel-based method, Zhang et al(2006) explored various structured feature 729 spaces and used the convolution tree kernel over parse trees (Collins and Duffy 2001) to model syntac tic structured information for relation extraction.</S>
			<S sid="65" ssid="20">They achieved the F-measures of 61.9 and 63.6 on the 5 relation types of the ACE RDC 2003 corpus and the 7 relation types of the ACE RDC 2004 corpus respectively without entity-related information while the F measure on the 5 relation types in the ACE RDC 2003 corpus reached 68.7 when entity-related infor mation was included in the parse tree.</S>
			<S sid="66" ssid="21">One problem with Collins and Duffy?s convolution tree kernel is that the sub-trees involved in the tree kernel computa tion are context-free, that is, they do not consider the information outside the sub-trees.</S>
			<S sid="67" ssid="22">This is different from the tree kernel in Culota and Sorensen (2004), where the sub-trees involved in the tree kernel com putation are context-sensitive (that is, with the path from the tree root node to the sub-tree root node in consideration).</S>
			<S sid="68" ssid="23">Zhang et al(2006) also showed that the widely-used Shortest Path-enclosed Tree (SPT) performed best.</S>
			<S sid="69" ssid="24">One problem with SPT is that it fails to capture the contextual information outside the shortest path, which is important for relation extraction in many cases.</S>
			<S sid="70" ssid="25">Our random selection of 100 pos i tive training instances from the ACE RDC 2003 training corpus shows that ~25% of the cases need contextual information outside the shortest path.</S>
			<S sid="71" ssid="26">Among other kernels, Bunescu and Mooney (2005b) proposed a subsequence kernel and applied it in pro tein interaction and ACE relation extraction tasks.</S>
			<S sid="72" ssid="27">In order to integrate the advantages of featurebased and tree kernel-based methods, some research ers have turned to composite kernel-based methods.</S>
			<S sid="73" ssid="28">Zhao and Grishman (2005) defined several feature based composite kernels to integrate diverse features for relation extraction and achieved the F-measure of 70.4 on the 7 relation types of the ACE RDC 2004 corpus.</S>
			<S sid="74" ssid="29">Zhang et al(2006) proposed two composite kernels to integrate a linear kernel and Collins and Duffy?s convolution tree kernel.</S>
			<S sid="75" ssid="30">It achieved the Fmeasure of 70.9/57.2 on the 5 relation types/24 rela tion subtypes in the ACE RDC 2003 corpus and the F-measure of 72.1/63.6 on the 7 relation types/23 relation subtypes in the ACE RDC 2004 corpus.</S>
			<S sid="76" ssid="31">The above discussion suggests that structured in formation in the parse tree may not be fully utilized in the previous works, regardless of feature-based, tree kernel-based or composite kernel-based methods.</S>
			<S sid="77" ssid="32">Compared with the previous works, this paper pro poses a dynamic context-sensitive tree span trying to cover necessary structured information and a context sensitive convolution tree kernel considering both context-free and context-sensitive sub-trees.</S>
			<S sid="78" ssid="33">Further more, a composite kernel is applied to combine our tree kernel and a state-of-the-art linear kernel for in tegrating both flat and structured features in relation extraction as well as validating their complementary nature.</S>
	</SECTION>
	<SECTION title="Context Sensitive Convolution Tree. " number="3">
			<S sid="79" ssid="1">Kernel for Relation Extraction In this section, we first propose an algorithm to dy namically determine a proper context-sensitive tree span and then a context-sensitive convolution tree kernel for relation extraction.</S>
			<S sid="80" ssid="2">3.1 Dynamic Context-Sensitive Tree Span in.</S>
			<S sid="81" ssid="3">Relation Extraction A relation instance between two entities is encaps u lated by a parse tree.</S>
			<S sid="82" ssid="4">Thus, it is critical to understand which portion of a parse tree is important in the tree kernel calculation.</S>
			<S sid="83" ssid="5">Zhang et al(2006) systematically explored seven different tree spans, including the Shortest Path-enclosed Tree (SPT) and a Context Sensitive Path-enclosed Tree1 (CSPT), and found that SPT per formed best.</S>
			<S sid="84" ssid="6">That is, SPT even outperforms CSPT.</S>
			<S sid="85" ssid="7">This is contrary to our intuition.</S>
			<S sid="86" ssid="8">For example, ?got married?</S>
			<S sid="87" ssid="9">is critical to determine the relationship between ?John?</S>
			<S sid="88" ssid="10">and ?Mary?</S>
			<S sid="89" ssid="11">in the sentence ?John and Mary got married?</S>
			<S sid="90" ssid="12">as shown in Figure 1(e), and the information contained in SPT (?John and Mary?)</S>
			<S sid="91" ssid="13">is not enough to determine their relationship.</S>
			<S sid="92" ssid="14">Obviously, context-sensitive tree spans should have the potential for better performance.</S>
			<S sid="93" ssid="15">One problem with the context-sensitive tree span explored in Zhang et al(2006) is that it only considers the availability of entities?</S>
			<S sid="94" ssid="16">siblings and fails to consider following two factors: 1) Whether is the information contained in SPT enough to determine the relationship between two entities?</S>
			<S sid="95" ssid="17">It depends.</S>
			<S sid="96" ssid="18">In the embedded cases, SPT is enough.</S>
			<S sid="97" ssid="19">For example, ?John?s wife?</S>
			<S sid="98" ssid="20">is enough to determine the relationship between ?John?</S>
			<S sid="99" ssid="21">and ?John?s wife?</S>
			<S sid="100" ssid="22">in the sentence ?John?s wife got a good job?</S>
			<S sid="101" ssid="23">as shown in Figure 1(a) . However, SPT is not enough in the coordinated cases, e.g. to determine the relationship between ?John?</S>
			<S sid="102" ssid="24">and ?Mary?</S>
			<S sid="103" ssid="25">in the sentence ?John and Mary got married?</S>
			<S sid="104" ssid="26">as shown in Figure 1(e).</S>
			<S sid="105" ssid="27">1 CSPT means SPT extending with the 1st left sibling of.</S>
			<S sid="106" ssid="28">the node of entity 1 and the 1st right sibling of the node of entity 2.</S>
			<S sid="107" ssid="29">In the case of no available sibling, it moves to the parent of current node and repeat the same proc ess until a sibling is available or the root is reached.</S>
			<S sid="108" ssid="30">730 2) How can we extend SPT to include necessary context information if there is no enough infor mation in SPT for relation extraction?</S>
			<S sid="109" ssid="31">To answer the above two questions, we randomly chose 100 positive instances from the ACE RDC 2003 training data and studied their necessary tree spans.</S>
			<S sid="110" ssid="32">It was observed that we can classify them into 5 categories: 1) embedded (37 instances), where one entity is embedded in another entity, e.g. ?John?</S>
			<S sid="111" ssid="33">and ?John?s wife?</S>
			<S sid="112" ssid="34">as shown in Figure 1(a); 2) PP-linked (21 instances), where one entity is linked to another entity via PP attachment, e.g. ?CEO?</S>
			<S sid="113" ssid="35">and ?Microsoft?</S>
			<S sid="114" ssid="36">in the sentence ?CEO of Microsoft announced ? ?</S>
			<S sid="115" ssid="37">as shown in Figure 1(b); 3) semi-structured (15 in stances), where the sentence consists of a sequence of noun phrases (including the two given entities), e.g. ?Jane?</S>
			<S sid="116" ssid="38">and ?ABC news?</S>
			<S sid="117" ssid="39">in the sentence ?Jane, ABC news, California.?</S>
			<S sid="118" ssid="40">as shown in Figure 1(c); 4) de scriptive (7 instances), e.g. the citizenship between ?his mother?</S>
			<S sid="119" ssid="41">and ?Lebanese?</S>
			<S sid="120" ssid="42">in the sentence ?his mother Lebanese landed at ??</S>
			<S sid="121" ssid="43">as shown in Figure 1(d); 5) predicate-linked and others (19 instances, including coordinated cases), where the predicate information is necessary to determine the relationship between two entities, e.g. ?John?</S>
			<S sid="122" ssid="44">and ?Mary?</S>
			<S sid="123" ssid="45">in the sentence ?John and Mary got married??</S>
			<S sid="124" ssid="46">as shown in Figure 1(e); Based on the above observations, we implement an algorithm to determine the necessary tree span for the relation extract task.</S>
			<S sid="125" ssid="47">The idea behind the algorithm is that the necessary tree span for a relation should be determined dynamically according to its tree span category and context.</S>
			<S sid="126" ssid="48">Given a parsed tree and two entities in consideration, it first determin es the tree span category and then extends the tree span accordingly.</S>
			<S sid="127" ssid="49">By default, we adopt the Shortest Pathenclosed Tree (SPT) as our tree span.</S>
			<S sid="128" ssid="50">We only ex pand the tree span when the tree span belongs to the ?predicate-linked?</S>
			<S sid="129" ssid="51">category.</S>
			<S sid="130" ssid="52">This is based on our observation that the tree spans belonging to the ?predi cate-linked?</S>
			<S sid="131" ssid="53">category vary much syntactically and majority (~70%) of them need information outside SPT while it is quite safe (&gt;90%) to use SPT as the tree span for the remaining categories.</S>
			<S sid="132" ssid="54">In our algo rithm, the expansion is done by first moving up until a predicate-headed phrase is found and then moving down along the predicated-headed path to the predi cate terminal node.</S>
			<S sid="133" ssid="55">Figure 1(e) shows an example for the ?predicate-linked?</S>
			<S sid="134" ssid="56">category where the lines with arrows indicate the expansion path.</S>
			<S sid="135" ssid="57">e) predicate-linked: SPT and the dynamic context-sensitive tree span Figure 1: Different tree span categories with SPT (dotted circle) and an ex ample of the dynamic context-sensitive tree span (solid circle) Figure 2: Examples of contextfree and context-sensitive sub trees related with Figure 1(b).</S>
			<S sid="136" ssid="58">Note: the bold node is the root for a sub-tree.</S>
			<S sid="137" ssid="59">A problem with our algorithm is how to determine whether an entity pair belongs to the ?predi cate-linked?</S>
			<S sid="138" ssid="60">category.</S>
			<S sid="139" ssid="61">In this paper, a simple method is applied by regarding the ?predicate linked?</S>
			<S sid="140" ssid="62">category as the default category.</S>
			<S sid="141" ssid="63">That is, those entity pairs, which do not belong to the four well defined and easily detected categories (i.e. embedded, PP-liked, semi-structured and descriptive), are classified into the ?predicate-linked?</S>
			<S sid="142" ssid="64">cate gory.</S>
			<S sid="143" ssid="65">His mother Lebanese landed PRP$ NNP VBD IN NP-E1-PER NP-E2-GPE PP S d) descriptive NP NN at ? VP Jane ABC news , NNP , NNP NNS , NNP . NP NP-E1-PER NP-E2-ORG NP c) semi-structured California . . , , , NP(NN) of Microsoft IN NNP NP-E2-ORG PP(IN)-subroot b) context -sensitive NP(NN) of Microsoft IN NNP NP-E2-ORG S(VBD) PP(IN)-subroot c) context -sensitive PP(IN)-subtoot NP-E2-ORG of Microsoft IN NNP a) context -free ? NP John and Mary got NNP CC NNP VBD married NP-E1-PER NP-E2-PER VP S VP VBN ? John and Mary got NNP CC NNP VBD married NP-E1-PER NP-E2-PER VP NP VP ? NP CEO of Microsoft announced NN IN NNP VBD ? NP-E1-PER NP-E2-ORG VP S b) PP -linked PP ? John ?s wife found a job NNP POS NN VBD DT JJ NN NP NP-E1-PER NP-E2-PER VP S a) embedded good 731 Since ?predicate -linked?</S>
			<S sid="144" ssid="66">instances only occupy ~20% of cases, this explains why SPT performs better than the Context-Sensitive Path-enclosed Tree (CSPT) as described in Zhang et al(2006): consistently adopting CSPT may introduce too much noise/unnecessary information in the tree kernel.</S>
			<S sid="145" ssid="67">3.2 Context-Sensitive Convolution Tree Kernel.</S>
			<S sid="146" ssid="68">Given any tree span, e.g. the dynamic context sensitive tree span in the last subsection, we now study how to measure the similarity between two trees, using a convolution tree kernel.A convolution kernel (Haussler D., 1999) aims to capture structured information in terms of substructures . As a specialized convolution kernel, Collins and Duffy?s convolu tion tree kernel ),( 21 TTKC (?C? for convolution) counts the number of common sub-trees (substructures) as the syntactic structure similarity be tween two parse trees T1 and T2 (Collins and Duffy 2001): ? ??</S>
			<S sid="147" ssid="69">D= 2211 , 2121 ),(),( NnNn C nnTTK (1) where Nj is the set of nodes in tree Tj , and 1 2( , )n nD evaluates the common sub-trees rooted at n1 and n2 2 and is computed recursively as follows: 1) If the context-free productions (Context-Free Grammar(CFG) rules) at 1n and 2n are different, 1 2( , ) 0n nD = ; Otherwise go to 2.</S>
			<S sid="148" ssid="70">2) If both 1n and 2n are POS tags, 1 2( , ) 1n n lD = ? ; Otherwise go to 3.</S>
			<S sid="149" ssid="71">3) Calculate 1 2( , )n nD recursively as: ? = D+=D )(# 1 2121 1 )),(),,((1(),( nch k knchknchnn l (2) where )(# nch is the number of children of node n , ),( knch is the k th child of node n andl (0&lt; l &lt;1) is the decay factor in order to make the kernel value less variable with respect to different sub-tree sizes.</S>
			<S sid="150" ssid="72">This convolution tree kernel has been successfully applied by Zhang et al(2006) in relation extraction.</S>
			<S sid="151" ssid="73">However, there is one problem with this tree kernel: the sub-trees involved in the tree kernel computation are context-free (That is, they do not consider the information outside the sub-trees).</S>
			<S sid="152" ssid="74">This is contrast to 2 That is, each node n encodes the identity of a sub-.</S>
			<S sid="153" ssid="75">tree rooted at n and, if there are two nodes in the tree with the same label, the summation will go over both of them.</S>
			<S sid="154" ssid="76">the tree kernel proposed in Culota and Sorensen (2004) which is context-sensitive, that is, it considers the path from the tree root node to the sub-tree root node.</S>
			<S sid="155" ssid="77">In order to integrate the advantages of both tree kernels and resolve the problem in Collins and Duffy?s convolution tree kernel, this paper proposes a context-sensitive convolution tree kernel.</S>
			<S sid="156" ssid="78">It works by taking ancestral information (i.e. the root node path) of sub-trees into consideration: ? ?</S>
			<S sid="157" ssid="79">D= m i NnNn ii C iiii nnTTK 1 ]2[]2[],1[]1[ 11 1111 ])2[],1[(])2[],1[( (3) Where ? ][1 jN i is the set of root node paths with length i in tree T[j] while the maximal length of a root node path is defined by m. ? ])[...(][ 211 jnnnjn ii = is a root node path with length i in tree T[j] , which takes into account the i-1 ancestral nodes in2 [j] of 1n [j] in T[j].</S>
			<S sid="158" ssid="80">Here, ][1 jn k+ is the parent of ][ jn k and ][1 jn is the root node of a context-free sub-tree in T[j].</S>
			<S sid="159" ssid="81">For better differentiation, the label of each ancestral node in in1 [j] is augmented with the POS tag of its head word.</S>
			<S sid="160" ssid="82">])2[],1[( 11 ii nnD measures the common context sensitive sub-trees rooted at root node paths ]1[1in and ]2[1in 3.</S>
			<S sid="161" ssid="83">In our tree kernel, a sub-tree.</S>
			<S sid="162" ssid="84">becomes context-sensitive with its dependence on the root node path instead of the root node itself.</S>
			<S sid="163" ssid="85">Figure 2 shows a few examples of contextsensitive sub-trees with comparison to context free sub-trees.</S>
			<S sid="164" ssid="86">Similar to Collins and Duffy (2001), our tree ker nel computes ])2[],1[( 11 ii nnD recursively as follows: 1) If the context-sensitive productions (Context Sensitive Grammar (CSG) rules with root node paths as their left hand sides) rooted at ]1[1in and ]2[1 in are different, return ])2[],1[( 11 ii nnD =0; Otherwise go to Step 2.</S>
			<S sid="165" ssid="87">2) If both ]1[1n and ]2[1n are POS tags, l=D ])2[],1[( 11 ii nn ; Otherwise go to Step 3.</S>
			<S sid="166" ssid="88">3 That is, each root node path in1 encodes the identity.</S>
			<S sid="167" ssid="89">of a context-sensitive sub-tree rooted at in1 and, if there are two root node paths in the tree with the same label sequence, the summation will go over both of them.</S>
			<S sid="168" ssid="90">732 3) Calculate ])2[],1[( 11 ii nnD recursively as: ? = D+= D ])1[(# 1 11 11 1 ))],2[(),],1[((1( ])2[],1[( inch k ii ii knchknch nn l (4) where ])],[( 1 kjnch i is the k th context-sensitive child of the context-sensitive sub-tree rooted at ][1 jn i with ])[(# 1 jnch i the number of the con text-sensitive children.</S>
			<S sid="169" ssid="91">Here, l (0&lt; l &lt;1) is the decay factor in order to make the kernel value less variable with respect to different sizes of the context-sensitive sub-trees.</S>
			<S sid="170" ssid="92">It is worth comparing our tree kernel with previous tree kernels.</S>
			<S sid="171" ssid="93">Obviously, our tree kernel is an exten sion of Collins and Duffy?s convolution tree kernel, which is a special case of our tree kernel (if m=1 in Equation (3)).</S>
			<S sid="172" ssid="94">Our tree kernel not only counts the occurrence of each context-free sub-tree, which does not consider its ancestors, but also counts the occurrence of each context-sensitive sub-tree, which con siders its ancestors.</S>
			<S sid="173" ssid="95">As a result, our tree kernel is not limited by the constraints in previous tree kernels (as discussed in Section 2), such as Collins and Duffy (2001), Zhang et al(2006), Culotta and Sorensen (2004) and Bunescu and Mooney (2005a).</S>
			<S sid="174" ssid="96">Finally, let?s study the computational issue with our tree kernel.</S>
			<S sid="175" ssid="97">Although our tree kernel takes the context sensitive sub-trees into consideration, it only slightly increases the computational burden, compared with Collins and Duffy?s convolution tree kernel.</S>
			<S sid="176" ssid="98">This is due to that 0])2[],1[( 11 =D nn holds for the major ity of context-free sub-tree pairs (Collins and Duffy 2001) and that computation for context-sensitive sub tree pairs is necessary only when 0])2[],1[( 11 ?D nn and the context-sensitive sub tree pairs have the same root node path(i.e. ]2[]1[ 11 ii nn = in Equation (3)).</S>
	</SECTION>
	<SECTION title="Experimentation. " number="4">
			<S sid="177" ssid="1">This paper uses the ACE RDC 2003 and 2004 cor pora provided by LDC in all our experiments.</S>
			<S sid="178" ssid="2">4.1 Experimental Setting.</S>
			<S sid="179" ssid="3">The ACE RDC corpora are gathered from various newspapers, newswire and broadcasts.</S>
			<S sid="180" ssid="4">In the 2003 corpus , the training set consists of 674 documents and 9683 positive relation instances w hile the test set consists of 97 documents and 1386 positive relation in stances.</S>
			<S sid="181" ssid="5">The 2003 corpus defines 5 entity types, 5 major relation types and 24 relation subtypes.</S>
			<S sid="182" ssid="6">All the reported performances in this paper on the ACE RDC 2003 corpus are evaluated on the test data.</S>
			<S sid="183" ssid="7">The 2004 corpus contains 451 documents and 5702 positive relation instances.</S>
			<S sid="184" ssid="8">It redefines 7 entity types, 7 major relation types and 23 relation subtypes.</S>
			<S sid="185" ssid="9">For compari son, we use the same setting as Zhang et al(2006) by applying a 5-fold cross-validation on a subset of the 2004 data, containing 348 documents and 4400 rela tion instances.</S>
			<S sid="186" ssid="10">That is, all the reported performances in this paper on the ACE RDC 2004 corpus are evalu ated using 5-fold cross validation on the entire corpus . Both corpora are parsed using Charniak?s parser (Charniak, 2001) with the boundaries of all the entity mentions kept 4 . We iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instances5.</S>
			<S sid="187" ssid="11">In our experimentation, SVM (SVMLight, Joachims(1998)) is selected as our classifier.</S>
			<S sid="188" ssid="12">For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others.</S>
			<S sid="189" ssid="13">The training parameters are chosen using cross-validation on the ACE RDC 2003 training data.</S>
			<S sid="190" ssid="14">In particular, l in our tree kernel is fine-tuned to 0.5.</S>
			<S sid="191" ssid="15">This suggests that about 50% dis count is done as our tree kernel moves down one level in computing ])2[],1[( 11 ii nnD . 4.2 Experimental Results.</S>
			<S sid="192" ssid="16">First, we systematically evaluate the context-sensitive convolution tree kernel and the dynamic context sensitive tree span proposed in this paper.</S>
			<S sid="193" ssid="17">Then, we evaluate the complementary nature between our tree kernel and a state-of-the-art linear ker nel via a composite kernel.</S>
			<S sid="194" ssid="18">Generally different feature-based methods and tree kernel-based methods have their own merits.</S>
			<S sid="195" ssid="19">It is usually easy to build a system using a feature-based method and achieve the state-of-the-art performance, while tree kernel-based methods hold the potential for further performance improvement.</S>
			<S sid="196" ssid="20">Therefore, it is always a good idea to integrate them via a composite kernel.</S>
			<S sid="197" ssid="21">4 This can be done by first representing all entity men-.</S>
			<S sid="198" ssid="22">tions with their head words and then restoring all the entity mentions after parsing.</S>
			<S sid="199" ssid="23">Moreover, please note that the final performance of relation extraction may change much with different range of parsing errors.</S>
			<S sid="200" ssid="24">We will study this issue in the near future.</S>
			<S sid="201" ssid="25">tion extraction on ?true?</S>
			<S sid="202" ssid="26">mentions with ?true?</S>
			<S sid="203" ssid="27">chain ing of co-reference (i.e. as annotated by LDC annotators ).</S>
			<S sid="204" ssid="28">Moreover, we only model explicit relations and explicitly model the argument order of the two mentions in volved.</S>
			<S sid="205" ssid="29">733Finally, we compare our system with the state-of the-art systems in the literature.</S>
			<S sid="206" ssid="30">Context-Sensitive Convolution Tree Kernel In this paper, the m parameter of our context-sensitive convolution tree kernel as shown in Equation (3) indicates the maximal length of root node paths and is optimized to 3 using 5-fold cross validation on the ACE RDC 2003 training data.</S>
			<S sid="207" ssid="31">Table 1 compares the impact of different m in context-sensitive convolution tree kernels using the Shortest Path-enclosed Tree (SPT) (as described in Zhang et al(2006)) on the major relation types of the ACE RDC 2003 and 2004 corpora, in details.</S>
			<S sid="208" ssid="32">It also shows that our tree kernel achieves best performance on the test data using SPT with m = 3, which outperforms the one with m = 1 by ~2.3 in F-measure.</S>
			<S sid="209" ssid="33">This suggests the parent and grandparent nodes of a sub-tree contains much information for relation extraction while considering more ancestral nodes may not help.</S>
			<S sid="210" ssid="34">This may be due to that, although our experimentation on the training data indicates that more than 80% (on average) of subtrees has a root node path longer than 3 (since most of the subtrees are deep from the root node and more than 90% of the parsed trees in the training data are deeper than 6 levels), including a root node path longer than 3 may be vulnerable to the full parsing errors and have negative impact.</S>
			<S sid="211" ssid="35">Table 1 also evaluates the impact of entity-related information in our tree kernel by attaching entity type information (e.g. ?PER?</S>
			<S sid="212" ssid="36">in the entity node 1 of Figure 1(b)) into both entity nodes.</S>
			<S sid="213" ssid="37">It shows that such information can significantly improve the performance by ~6.0 in F-measure.</S>
			<S sid="214" ssid="38">In all the following experiments, we will apply our tree kernel with m=3 and entity-related information by default.</S>
			<S sid="215" ssid="39">Table 2 compares the dynamic context-sensitive tree span with SPT using our tree kernel.</S>
			<S sid="216" ssid="40">It shows that the dynamic tree span can futher improve the performance by ~1.2 in F-measure6.</S>
			<S sid="217" ssid="41">This suggests the usefulness of extending the tree span beyond SPT for the ?predicate-linked?</S>
			<S sid="218" ssid="42">tree span category.</S>
			<S sid="219" ssid="43">In the future work, we will further explore expanding the dynamic tree span beyond SPT for the remaining tree span categories.</S>
			<S sid="220" ssid="44">6 Significance test shows that the dynamic tree span per-.</S>
			<S sid="221" ssid="45">forms s tatistically significantly better than SPT with p values smaller than 0.05.</S>
			<S sid="222" ssid="46">m P(%) R(%) F 1 72.3(72.7) 56.6(53.8) 63.5(61.8) 2 74.9(75.2) 57.9(54.7) 65.3(63.5) 3 75.7(76.1) 58.3(55.1) 65.9(64.0) 4 76.0(75.9) 58.3(55.3) 66.0(63.9) a) without entity-related information m P(%) R(%) F 1 77.2(76.9) 63.5(60.8) 69.7(67.9) 2 79.1(78.6) 65.0(62.2) 71.3(69.4) 3 79.6(79.4) 65.6(62.5) 71.9(69.9) 4 79.4(79.1) 65.6(62.3) 71.8(69.7) b) with entity-related information Table 1: Evaluation of context-sensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora.</S>
			<S sid="223" ssid="47">Tree Span P(%) R(%) F Shortest Path- enclosed Tree 79.6 (79.4) 65.6 (62.5) 71.9 (69.9) Dynamic Context- Sensitive Tee 81.1 (80.1) 66.7 (63.8) 73.2 (71.0) Table 2: Comparison of dynamic context-sensitive tree span with SPT using our context-sensitive convolution tree kernel on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora.</S>
			<S sid="224" ssid="48">18% of positive instances in the ACE RDC 2003 test data belong to the predicate-linked category.</S>
			<S sid="225" ssid="49">Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al(2006), is ap plied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al2005) 7: ),()1(),(),(1 ???-+???=??</S>
			<S sid="226" ssid="50">CPL KKK aa (5) Here, ),( ??LK and ),( ??CK indicates the normal ized linear kernel and context-sensitive convolution tree kernel respectively while ( , )pK ? ?</S>
			<S sid="227" ssid="51">is the poly nomial expansion of ( , )K ? ?</S>
			<S sid="228" ssid="52">with degree d=2, i.e. 2( , ) ( ( , ) 1)pK K?</S>
			<S sid="229" ssid="53">?= + and a is the coefficient (a is set to 0.3 using cross-validation).</S>
			<S sid="230" ssid="54">7 Here, we use the same set of flat features (i.e. word,.</S>
			<S sid="231" ssid="55">entity type, mention level, overlap, base phrase chunking, dependency tree, parse tree and semantic informa tion) as Zhou et al(2005).</S>
			<S sid="232" ssid="56">734 Table 3 evaluates the performance of the composite kernel.</S>
			<S sid="233" ssid="57">It shows that the composite kernel much further improves the performance beyond that of either the state-of-the-art linear kernel or our tree kernel and achieves the F-measures of 74.1 and 75.8 on the major relation types of the ACE RDC 2003 and 2004 corpora respectively.</S>
			<S sid="234" ssid="58">This suggests that our tree kernel and the state-of-the-art linear kernel are quite complementary, and that our composite kernel can effectively integrate both flat and structured features.</S>
			<S sid="235" ssid="59">System P(%) R(%) F Linear Kernel 78.2 (77.2) 63.4 (60.7) 70.1 (68.0) Context-Sensitive Con volution Tree Kernel 81.1 (80.1) 66.7 (63.8) 73.2 (71.0) Composite Kernel 82.2 (80.8) 70.2 (68.4) 75.8 (74.1) Table 3: Performance of the compos ite kernel via polynomial interpolation on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora Comparison with Other Systems ACE RDC 2003 P(%) R(%) F Ours: composite kernel 80.8 (65.2) 68.4 (54.9) 74.1 (59.6) Zhang et al(2006): composite kernel 77.3 (64.9) 65.6 (51.2) 70.9 (57.2) Ours: context-sensitive convolution tree kernel 80.1 (63.4) 63.8 (51.9) 71.0 (57.1) Zhang et al(2006): convolution tree kernel 76.1 (62.4) 62.6 (48.5) 68.7 (54.6) Bunescu et al(2005): shortest path dependency kernel 65.5 (-) 43.8 (-) 52.5 (-) Culotta et al(2004): dependency kernel 67.1 (-) 35.0 (-) 45.8 (-) Zhou et al (2005): feature-based 77.2 (63.1) 60.7 (49.5) 68.0 (55.5) Kambhatla (2004): feature-based - (63.5) - (45.2) - (52.8) Table 4: Comparison of difference systems on the ACE RDC 2003 corpus over both 5 types (outside the parentheses) and 24 subtypes (inside the parentheses) ACE RDC 2004 P(%) R(%) F Ours: composite kernel 82.2 (70.3) 70.2 (62.2) 75.8 (66.0) Zhang et al(2006): composite kernel 76.1 (68.6) 68.4 (59.3) 72.1 (63.6) Zhao et al(2005):8 composite kernel 69.2 (-) 70.5 (-) 70.4 (-) Ours: context-sensitive convolution tree kernel 81.1 (68.8) 66.7 (60.3) 73.2 (64.3) Zhang et al(2006): convolution tree kernel 72.5 (-) 56.7 (-) 63.6 (-) Table 5: Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types (outside the parentheses) and 23 subtypes (inside the parentheses) Finally, Tables 4 and 5 compare our system with other state-of-the-art systems9 on the ACE RDC 2003 and 2004 corpora, respectively.</S>
			<S sid="236" ssid="60">They show that our tree kernel-based system outperforms previous tree kernel-based systems.</S>
			<S sid="237" ssid="61">This is largely due to the con text-sensitive nature of our tree kernel which resolves the limitations of the previous tree kernels.</S>
			<S sid="238" ssid="62">They also show that our tree kernel-based system outperforms the state-of-the-art feature-based system.</S>
			<S sid="239" ssid="63">This proves the great potential inherent in the parse tree structure for relation extraction and our tree kernel takes a big stride towards the right direction.</S>
			<S sid="240" ssid="64">Finally, they also show that our composite kernel-based system outper forms other composite kernel-based systems.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="5">
			<S sid="241" ssid="1">Structured parse tree information holds great potential for relation extraction.</S>
			<S sid="242" ssid="2">This paper proposes a contextsensitive convolution tree kernel to resolve two critical problems in previous tree kernels for relation ex traction by first automatically determining a dynamic context-sensitive tree span and then applying a con text-sensitive convolution tree kernel.</S>
			<S sid="243" ssid="3">Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel.</S>
			<S sid="244" ssid="4">Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than the widely -used Shortest Path-enclosed Tree and our tree kernel outperforms the state-of-the-art Collins and Duffy?s con volution tree kernel.</S>
			<S sid="245" ssid="5">It also shows that feature-based 8 There might be some typing errors for the performance.</S>
			<S sid="246" ssid="6">reported in Zhao and Grishman(2005) since P, R and F do not match.</S>
			<S sid="247" ssid="7">9 All the state-of-the-art systems apply the entity-related.</S>
			<S sid="248" ssid="8">information.</S>
			<S sid="249" ssid="9">It is not supervising: our experiments show that using the entity-related information gives a large performance improvement.</S>
			<S sid="250" ssid="10">735 and tree kernel-based methods well complement each other and the composite kernel can effectively inte grate both flat and structured features.</S>
			<S sid="251" ssid="11">To our knowledge, this is the first research to dem onstrate that, without extensive feature engineer ing, an individual tree kernel can achieve much better performance than the state-of-the-art linear kernel in re lation extraction.</S>
			<S sid="252" ssid="12">This shows the great potential of structured parse tree information for relation extrac tion and our tree kernel takes a big stride towards the right direction.</S>
			<S sid="253" ssid="13">For the future work, we will focus on improving the context-sensitive convolution tree kernel by ex ploring more useful context information.</S>
			<S sid="254" ssid="14">Moreover, we will explore more entity-related information in the parse tree.</S>
			<S sid="255" ssid="15">Our preliminary work of including the entity type information significantly improves the per formance.</S>
			<S sid="256" ssid="16">Finally, we will study how to resolve the data imbalance and sparseness issues from the learn ing algorithm viewpoint.</S>
			<S sid="257" ssid="17">Acknowledgement This research is supported by Project 60673041 under the National Natural Science Foundation of China and Project 2006AA01Z147 under the ?863?</S>
			<S sid="258" ssid="18">National High-Tech Research and Development of China.</S>
			<S sid="259" ssid="19">We would also like to thank the critical and insightful comments from the four anonymous reviewers.</S>
	</SECTION>
</PAPER>
