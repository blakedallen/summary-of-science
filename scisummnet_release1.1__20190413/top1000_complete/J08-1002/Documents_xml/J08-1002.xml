<PAPER>
  <S sid="0">Feature Forest Models for Probabilistic HPSG Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures.</S>
    <S sid="2" ssid="2">This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures.</S>
    <S sid="3" ssid="3">For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules.</S>
    <S sid="4" ssid="4">These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.</S>
    <S sid="5" ssid="5">This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures.</S>
    <S sid="6" ssid="6">The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests.</S>
    <S sid="7" ssid="7">Feature forests are generic data structures that represent ambiguous trees in a packed forest structure.</S>
    <S sid="8" ssid="8">Feature forest models are maximum entropy models defined over feature forests.</S>
    <S sid="9" ssid="9">A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests.</S>
    <S sid="10" ssid="10">Thus probabilistic modeling of any data structures is possible when they are represented by feature forests.</S>
    <S sid="11" ssid="11">This article also describes methods for representing HPSG syntactic structures and predicate&#8211;argument structures with feature forests.</S>
    <S sid="12" ssid="12">Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing.</S>
    <S sid="13" ssid="13">The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="14" ssid="1">Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures.</S>
    <S sid="15" ssid="2">This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures.</S>
    <S sid="16" ssid="3">For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules.</S>
    <S sid="17" ssid="4">These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.</S>
    <S sid="18" ssid="5">This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures.</S>
    <S sid="19" ssid="6">The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests.</S>
    <S sid="20" ssid="7">Feature forests are generic data structures that represent ambiguous trees in a packed forest structure.</S>
    <S sid="21" ssid="8">Feature forest models are maximum entropy models defined over feature forests.</S>
    <S sid="22" ssid="9">A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests.</S>
    <S sid="23" ssid="10">Thus probabilistic modeling of any data structures is possible when they are represented by feature forests.</S>
    <S sid="24" ssid="11">This article also describes methods for representing HPSG syntactic structures and predicate&#8211;argument structures with feature forests.</S>
    <S sid="25" ssid="12">Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing.</S>
    <S sid="26" ssid="13">The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="27" ssid="1">Following the successful development of wide-coverage lexicalized grammars (Riezler et al. 2000; Hockenmaier and Steedman 2002; Burke et al.</S>
    <S sid="28" ssid="2">2004; Miyao, Ninomiya, and Tsujii 2005), statistical modeling of these grammars is attracting considerable attention.</S>
    <S sid="29" ssid="3">This is because natural language processing applications usually require disambiguated or ranked parse results, and statistical modeling of syntactic/semantic preference is one of the most promising methods for disambiguation.</S>
    <S sid="30" ssid="4">The focus of this article is the problem of probabilistic modeling of wide-coverage HPSG parsing.</S>
    <S sid="31" ssid="5">Although previous studies have proposed maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen, Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing.</S>
    <S sid="32" ssid="6">In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities.</S>
    <S sid="33" ssid="7">This causes an exponential explosion when estimating the parameters of maximum entropy models.</S>
    <S sid="34" ssid="8">We therefore require solutions to make model estimation tractable.</S>
    <S sid="35" ssid="9">This article first proposes feature forest models, which are a general solution to the problem of maximum entropy modeling of tree structures (Miyao and Tsujii 2002).</S>
    <S sid="36" ssid="10">Our algorithm avoids exponential explosion by representing probabilistic events with feature forests, which are packed representations of tree structures.</S>
    <S sid="37" ssid="11">When complete structures are represented with feature forests of a tractable size, the parameters of maximum entropy models are efficiently estimated without unpacking the feature forests.</S>
    <S sid="38" ssid="12">This is due to dynamic programming similar to the algorithm for computing inside/outside probabilities in PCFG parsing.</S>
    <S sid="39" ssid="13">The latter half of this article (Section 4) is on the application of feature forest models to disambiguation in wide-coverage HPSG parsing.</S>
    <S sid="40" ssid="14">We describe methods for representing HPSG parse trees and predicate&#8211;argument structures using feature forests (Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005).</S>
    <S sid="41" ssid="15">Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing.</S>
    <S sid="42" ssid="16">The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004).</S>
    <S sid="43" ssid="17">We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994).</S>
    <S sid="44" ssid="18">The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003).</S>
    <S sid="45" ssid="19">The major contribution of this article is a strict mathematical definition of the feature forest model and the parameter estimation algorithm, which are substantially refined and extended from Miyao and Tsujii (2002).</S>
    <S sid="46" ssid="20">Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing.</S>
    <S sid="47" ssid="21">We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text.</S>
    <S sid="48" ssid="22">Section 2 discusses a problem of conventional probabilistic models for lexicalized grammars.</S>
    <S sid="49" ssid="23">Section 3 proposes feature forest models for solving this problem.</S>
    <S sid="50" ssid="24">Section 4 describes the application of feature forest models to probabilistic HPSG parsing.</S>
    <S sid="51" ssid="25">Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals.</S>
    <S sid="52" ssid="26">Section 7 concludes.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="53" ssid="1">Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or feature structure grammars (Johnson et al. 1999; Riezler et al.</S>
    <S sid="54" ssid="2">2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005).</S>
    <S sid="55" ssid="3">Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models.</S>
    <S sid="56" ssid="4">This is because these grammar formalisms exploit feature structures to represent linguistic constraints.</S>
    <S sid="57" ssid="5">Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997).</S>
    <S sid="58" ssid="6">The maximum entropy model is a reasonable choice for credible probabilistic models.</S>
    <S sid="59" ssid="7">It also allows various overlapping features to be incorporated, and we can expect higher accuracy in disambiguation.</S>
    <S sid="60" ssid="8">A maximum entropy model gives a probabilistic distribution that maximizes the likelihood of training data under given feature functions.</S>
    <S sid="61" ssid="9">Given training data E = {(x, y)}, a maximum entropy model gives conditional probability p(y|x) as follows.</S>
    <S sid="62" ssid="10">Definition 1(Maximum entropy model) A maximum entropy model is defined as the solution of the following optimization problem.</S>
    <S sid="63" ssid="11">In this definition, &#732;p(x, y) is the relative frequency of (x, y) in the training data. fi is a feature function, which represents a characteristic of probabilistic events by mapping an event into a real value. &#955;i is the model parameter of a corresponding feature function fi, and is determined so as to maximize the likelihood of the training data (i.e., the optimization in this definition).</S>
    <S sid="64" ssid="12">Y(x) is a set of y for given x; for example, in parsing, x is a given sentence and Y(x) is a parse forest for x.</S>
    <S sid="65" ssid="13">An advantage of maximum entropy models is that feature functions can represent any characteristics of events.</S>
    <S sid="66" ssid="14">That is, independence assumptions are unnecessary for the design of feature functions.</S>
    <S sid="67" ssid="15">Hence, this method provides a principled solution for the estimation of consistent probabilistic distributions over feature structure grammars.</S>
    <S sid="68" ssid="16">The remaining issue is how to estimate parameters.</S>
    <S sid="69" ssid="17">Several numerical algorithms, such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limitedmemory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright 1999), have been proposed for parameter estimation.</S>
    <S sid="70" ssid="18">Although the algorithm proposed in the present article is applicable to all of the above algorithms, we used L-BFGS for experiments.</S>
    <S sid="71" ssid="19">However, a computational problem arises in these parameter estimation algorithms.</S>
    <S sid="72" ssid="20">The size of Y(x) (i.e., the number of parse trees for a sentence) is generally very large.</S>
    <S sid="73" ssid="21">This is because local ambiguities in parse trees potentially cause exponential growth in the number of structures assigned to sub-sequences of words, resulting in billions of structures for whole sentences.</S>
    <S sid="74" ssid="22">For example, when we apply rewriting rule S &#8594; NP VP, and the left NP and the right VP, respectively, have n and m ambiguous subtrees, the result of the rule application generates n &#215; m trees.</S>
    <S sid="75" ssid="23">This is problematic because the complexity of parameter estimation is proportional to the size of Y(x).</S>
    <S sid="76" ssid="24">The cost of the parameter estimation algorithms is bound by the computation of model expectation, &#181;i, given as (Malouf 2002): As shown in this definition, the computation of model expectation requires the summation over Y(x) for every x in the training data.</S>
    <S sid="77" ssid="25">The complexity of the overall estimation algorithm is O( &#732;|Y |&#732;|F||E|), where &#732;|Y |and &#732;|F |are the average numbers of y and activated features for an event, respectively, and |E |is the number of events.</S>
    <S sid="78" ssid="26">When Y(x) grows exponentially, the parameter estimation becomes intractable.</S>
    <S sid="79" ssid="27">In PCFGs, the problem of computing probabilities of parse trees is avoided by using a dynamic programming algorithm for computing inside/outside probabilities (Baker 1979).</S>
    <S sid="80" ssid="28">With the algorithm, the computation becomes tractable.</S>
    <S sid="81" ssid="29">We can expect that the same approach would be effective for maximum entropy models as well.</S>
    <S sid="82" ssid="30">This notion yields a novel algorithm for parameter estimation for maximum entropy models, as described in the next section.</S>
  </SECTION>
  <SECTION title="3." number="4">
    <S sid="83" ssid="1">Our solution to the problem is a dynamic programming algorithm for computing inside/outside &#945;-products.</S>
    <S sid="84" ssid="2">Inside/outside &#945;-products roughly correspond to inside/ outside probabilities in PCFGs.</S>
    <S sid="85" ssid="3">In maximum entropy models, a probability is defined as a normalized product of &#945;fj j(= exp(&#955;jfj)).</S>
    <S sid="86" ssid="4">Hence, similar to the algorithm of computing (&#65533; ) inside/outside probabilities, we can compute exp j &#955;jfj , which we define as the &#945;-product, for each node in a tree structure.</S>
    <S sid="87" ssid="5">If we can compute &#945;-products at a tractable cost, the model expectation &#181;i is also computed at a tractable cost.</S>
    <S sid="88" ssid="6">We first define the notion of a feature forest, a packed representation of a set of an exponential number of tree structures.</S>
    <S sid="89" ssid="7">Feature forests correspond to packed charts in CFG parsing.</S>
    <S sid="90" ssid="8">Because feature forests are generalized representations of forest structures, the notion is not only applicable to syntactic parsing but also to sequence tagging, such as POS tagging and named entity recognition (which will be discussed in Section 6).</S>
    <S sid="91" ssid="9">We then define inside/outside &#945;-products that represent the &#945;-products of partial structures of a feature forest.</S>
    <S sid="92" ssid="10">Inside &#945;-products correspond to inside probabilities in PCFG, and represent the summation of &#945;-products of the daughter sub-trees.</S>
    <S sid="93" ssid="11">Outside &#945;-products correspond to outside probabilities in PCFG, and represent the summation of &#945;-products in the upper part of the feature forest.</S>
    <S sid="94" ssid="12">Both can be computed incrementally by a dynamic programming algorithm similar to the algorithm for computing inside/outside probabilities in PCFG.</S>
    <S sid="95" ssid="13">Given inside/outside o -products of all nodes in a feature forest, the model expectation &#181;i is easily computed by multiplying them for each node.</S>
    <S sid="96" ssid="14">To describe the algorithm, we first define the notion of a feature forest, the generalized representation of features in a packed forest structure.</S>
    <S sid="97" ssid="15">Feature forests are used for enumerating possible structures of events, that is, they correspond to Y(x) in Equation 1.</S>
    <S sid="98" ssid="16">A feature forest &#934; is a tuple (C, D, r, -y, b), where: We denote a feature forest for x as &#934;(x).</S>
    <S sid="99" ssid="17">For example, &#934;(x) can represent the set of all possible tag sequences of a given sentence x, or the set of all parse trees of x.</S>
    <S sid="100" ssid="18">A feature forest is an acyclic graph, and unpacked structures extracted from a feature forest are trees.</S>
    <S sid="101" ssid="19">We also assume that terminal nodes of feature forests are conjunctive nodes.</S>
    <S sid="102" ssid="20">That is, disjunctive nodes must have daughters (i.e., -y(d) =&#65533; 0 for all d E D).</S>
    <S sid="103" ssid="21">A feature forest represents a set of trees of conjunctive nodes in a packed structure.</S>
    <S sid="104" ssid="22">Conjunctive nodes correspond to entities such as states in Markov chains and nodes in CFG trees.</S>
    <S sid="105" ssid="23">Feature functions are assigned to conjunctive nodes and express their characteristics.</S>
    <S sid="106" ssid="24">Disjunctive nodes are for enumerating alternative choices.</S>
    <S sid="107" ssid="25">Conjunctive/ disjunctive daughter functions represent immediate relations of conjunctive and disjunctive nodes.</S>
    <S sid="108" ssid="26">By selecting a conjunctive node as a child of each disjunctive node, we can extract a tree consisting of conjunctive nodes from a feature forest.</S>
    <S sid="109" ssid="27">A feature forest. nodes as its daughters.</S>
    <S sid="110" ssid="28">The feature forest in Figure 1 represents a set of 2 x 2 x 2 = 8 unpacked trees shown in Figure 2.</S>
    <S sid="111" ssid="29">For example, by selecting the left-most conjunctive node at each disjunctive node, we extract an unpacked tree (c1, c2, c4, c6).</S>
    <S sid="112" ssid="30">An unpacked tree is represented as a set of conjunctive nodes.</S>
    <S sid="113" ssid="31">Generally, a feature forest represents an exponential number of trees with a polynomial number of nodes.</S>
    <S sid="114" ssid="32">Thus, complete structures, such as tag sequences and parse trees with ambiguities, can be represented in a tractable form.</S>
    <S sid="115" ssid="33">Feature functions are defined over conjunctive nodes.1 Definition 3 (Feature function for feature forests) A feature function for a feature forest is: Hence, together with feature functions, a feature forest represents a set of trees of features.</S>
    <S sid="116" ssid="34">Feature forests may be regarded as a packed chart in CFG parsing.</S>
    <S sid="117" ssid="35">Although feature forests have the same structure as PCFG parse forests, nodes in feature forests do not necessarily correspond to nodes in PCFG parse forests.</S>
    <S sid="118" ssid="36">In fact, in Sections 4.2 and 4.3, we will demonstrate that syntactic structures and predicate&#8211;argument structures in HPSG can be represented with tractable-size feature forests.</S>
    <S sid="119" ssid="37">The actual interpretation of a node in a feature forest may thus be ignored in the following discussion.</S>
    <S sid="120" ssid="38">Our algorithm is applicable whenever feature forests are of a tractable size.</S>
    <S sid="121" ssid="39">The descriptive power of feature forests will be discussed again in Section 6.</S>
    <S sid="122" ssid="40">As mentioned, a feature forest is a packed representation of trees of features.</S>
    <S sid="123" ssid="41">We first define model expectations, &#181;i, on a set of unpacked trees, and then show that they can be computed without unpacking feature forests.</S>
    <S sid="124" ssid="42">We denote an unpacked tree as a set, c &#8838; C, of conjunctive nodes.</S>
    <S sid="125" ssid="43">Our concern is only the set of features associated with each conjunctive node, and the shape of the tree structure is irrelevant to the computation of probabilities of unpacked trees.</S>
    <S sid="126" ssid="44">Hence, we do not distinguish an unpacked tree from a set of conjunctive nodes.</S>
    <S sid="127" ssid="45">The collection of unpacked trees represented by a feature forest is defined as a multiset of unpacked trees because we allow multiple occurrences of equivalent unpacked trees in a feature forest.2 Given multisets of unpacked trees, A, B, we define the union and the product as follows.</S>
    <S sid="128" ssid="46">Intuitively, the first operation is a collection of trees, and the second lists all combinations of trees in A and B.</S>
    <S sid="129" ssid="47">It is trivial that they satisfy commutative, associative, and distributive laws.</S>
    <S sid="130" ssid="48">We denote a set of unpacked trees rooted at node n E C U D as &#8486;(n).</S>
    <S sid="131" ssid="49">&#8486;(n) is defined recursively.</S>
    <S sid="132" ssid="50">For a terminal node c E C, obviously &#8486;(c) = {{c}}.</S>
    <S sid="133" ssid="51">For an internal conjunctive node c E C, an unpacked tree is a combination of trees, each of which is selected from a disjunctive daughter.</S>
    <S sid="134" ssid="52">Hence, a set of all unpacked trees is represented as a product of trees from disjunctive daughters.</S>
    <S sid="135" ssid="53">A disjunctive node d E D represents alternatives of packed trees, and obviously a set of its unpacked trees is represented as a union of the daughter trees, that is, &#8486;(d) = To summarize, a set of unpacked trees is defined formally as follows.</S>
    <S sid="136" ssid="54">Given a feature forest &#934; = (C, D, r,&#947;, 6), a set &#8486;(n) of unpacked trees rooted at node n E C U D is defined recursively as follows.</S>
    <S sid="137" ssid="55">Feature forests are directed acyclic graphs and, as such, this definition does not include a loop.</S>
    <S sid="138" ssid="56">Hence, &#8486;(n) is properly defined.</S>
    <S sid="139" ssid="57">A set of all unpacked trees is then represented by &#8486;(r); henceforth, we denote &#8486;(r) as &#8486;(&#934;), or just &#8486; when it is not confusing in context.</S>
    <S sid="140" ssid="58">Figure 3 shows &#8486;(&#934;) of the feature forest in Figure 1.</S>
    <S sid="141" ssid="59">Following Definition 4, the first element of each set is the root node, c1, and the rest are elements of the product of {c2, c3}, {c4, c5}, and {c6, c7}.</S>
    <S sid="142" ssid="60">Each set in Figure 3 corresponds to a tree in Figure 2.</S>
    <S sid="143" ssid="61">Given this formalization, the feature function for an unpacked tree is defined as follows.</S>
    <S sid="144" ssid="62">Definition 5 (Feature function for unpacked tree) The feature function fi for an unpacked tree, c E &#8486;(&#934;) is defined as: Because c E &#8486;(&#934;) corresponds to y of the conventional maximum entropy model, this function substitutes for fi(x,y) in the conventional model.</S>
    <S sid="145" ssid="63">Once a feature function for an unpacked tree is given, a model expectation is defined as in the traditional model.</S>
    <S sid="146" ssid="64">Definition 6 (Model expectation of feature forests) The model expectation &#181;i for a set of feature forests {&#934;(x)} is defined as: It is evident that the naive computation of model expectations requires exponential time complexity because the number of unpacked trees (i.e., |&#8486;(&#934;)|) is exponentially related to the number of nodes in the feature forest &#934;.</S>
    <S sid="147" ssid="65">We therefore need an algorithm for computing model expectations without unpacking a feature forest.</S>
    <S sid="148" ssid="66">Figure 3 Unpacked trees represented as sets of conjunctive nodes.</S>
    <S sid="149" ssid="67">Inside/outside at node c2 in a feature forest.</S>
    <S sid="150" ssid="68">To efficiently compute model expectations, we incorporate an approach similar to the dynamic programming algorithm for computing inside/outside probabilities in PCFGs.</S>
    <S sid="151" ssid="69">We first define the notion of inside/outside of a feature forest.</S>
    <S sid="152" ssid="70">Figure 4 illustrates this concept, which is similar to the analogous concept in PCFGs.3 Inside denotes a set of partial trees (sets of conjunctive nodes) derived from node c2.</S>
    <S sid="153" ssid="71">Outside denotes a set of partial trees that derive node c2.</S>
    <S sid="154" ssid="72">That is, outside trees are partial trees of complements of inside trees.</S>
    <S sid="155" ssid="73">We denote a set of inside trees at node n as &#953;(n), and that of outside trees as o(n).</S>
    <S sid="156" ssid="74">We define a set &#953;(n) of inside trees rooted at node n &#8712; C &#8746; D as a set of unpacked trees rooted at n. We define a set o(n) of outside trees rooted at node n &#8712; C &#8746; D as follows.</S>
    <S sid="157" ssid="75">In the definition, &#947;&#8722;1 and &#948;&#8722;1 denote mothers of conjunctive and disjunctive nodes, respectively.</S>
    <S sid="158" ssid="76">Formally, We can derive that the model expectations of a feature forest are computed as the product of the inside and outside &#945;-products.</S>
    <S sid="159" ssid="77">Theorem 1(Model expectation of feature forests) The model expectation &#181;i of a feature forest &#934;(x) = (Cx, Dx, rx, &#947;x, &#948;x) is computed as the product of inside and outside &#945;-products as follows: where Z(x) = &#981;rx This equation shows a method for efficiently computing model expectations by traversing conjunctive nodes without unpacking the forest, if the inside/outside &#945;-products are given.</S>
    <S sid="160" ssid="78">The remaining issue is how to efficiently compute inside/outside &#945;-products.</S>
    <S sid="161" ssid="79">Fortunately, inside/outside &#945;-products can be incrementally computed by dynamic programming without unpacking feature forests.</S>
    <S sid="162" ssid="80">Figure 5 shows the process of computing the inside &#945;-product at a conjunctive node from the inside &#945;-products of its daughter nodes.</S>
    <S sid="163" ssid="81">Because the inside of a conjunctive node is a set of the combinations of all of its descendants, the &#945;-product is computed by multiplying the &#945;-products of the daughter trees.</S>
    <S sid="164" ssid="82">The following equation is derived.</S>
    <S sid="165" ssid="83">The inside of a disjunctive node is the collection of the inside trees of its daughter nodes.</S>
    <S sid="166" ssid="84">Hence, the inside &#945;-product at disjunctive node d &#8712; D is computed as follows (Figure 6).</S>
    <S sid="167" ssid="85">The inside &#945;-product &#981;c at a conjunctive node c is computed by the following equation if &#981;d is given for all daughter disjunctive nodes d &#8712; &#948;(c).</S>
    <S sid="168" ssid="86">The outside of a disjunctive node is equivalent to the outside of its daughter nodes.</S>
    <S sid="169" ssid="87">Hence, the outside &#945;-product of a disjunctive node is propagated to its daughter conjunctive nodes (Figure 7).</S>
    <S sid="170" ssid="88">The computation of the outside &#945;-product of a disjunctive node is somewhat complicated.</S>
    <S sid="171" ssid="89">As shown in Figure 8, the outside trees of a disjunctive node are all combinations of Incremental computation of outside &#945;-products at conjunctive node c2.</S>
    <S sid="172" ssid="90">We finally find the following theorem for the computation of outside o -products.</S>
    <S sid="173" ssid="91">Theorem 3 (Outside o -product) The outside o -product *c at conjunctive node c is computed by the following equation if *d is given for all mother disjunctive nodes, that is, all d such that c &#8712; -y(d).</S>
    <S sid="174" ssid="92">The outside o -product *d at disjunctive node d is computed by the following equation if *c is given for all mother conjunctive nodes, that is, all c such that d &#8712; b(c), and yds for all sibling disjunctive nodes d'.</S>
    <S sid="175" ssid="93">Note that the order in which nodes are traversed is important for incremental computation, although it is not shown in Figure 9.</S>
    <S sid="176" ssid="94">The computation for the daughter nodes and mother nodes must be completed before computing the inside and outside &#945;-products, respectively.</S>
    <S sid="177" ssid="95">This constraint is easily solved using any topological sort algorithm.</S>
    <S sid="178" ssid="96">A topological sort is applied once at the beginning.</S>
    <S sid="179" ssid="97">The result of the sorting does not affect the cost and the result of estimation.</S>
    <S sid="180" ssid="98">In our implementation, we assume that conjunctive/disjunctive nodes are already ordered from the root node in input data.</S>
    <S sid="181" ssid="99">The complexity of this algorithm is O(( &#732;|C |+&#732;|D|) &#732;|F||E|), where &#732;|C |and &#732;|D |are the average numbers of conjunctive and disjunctive nodes, respectively.</S>
    <S sid="182" ssid="100">This is tractable when &#732;|C |and &#732;|D |are of a reasonable size.</S>
    <S sid="183" ssid="101">As noted in this section, the number of nodes in a feature forest is usually polynomial even when that of the unpacked trees is exponential.</S>
    <S sid="184" ssid="102">Thus we can efficiently compute model expectations with polynomial computational complexity.</S>
  </SECTION>
  <SECTION title="4." number="5">
    <S sid="185" ssid="1">Following previous studies on probabilistic models for HPSG (Oepen, Toutanova, et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), we apply a maximum entropy model to HPSG parse disambiguation.</S>
    <S sid="186" ssid="2">The probability, p(t|w), of producing parse result t of a given sentence w is defined as where where p0(t|w) is a reference distribution (usually assumed to be a uniform distribution) and T(w) is a set of parse candidates assigned to w. The feature function fi(t,w) represents the characteristics of t and w, and the corresponding model parameter &#955;i is its weight.</S>
    <S sid="187" ssid="3">Model parameters that maximize the log-likelihood of the training data are computed using a numerical optimization method (Malouf 2002).</S>
    <S sid="188" ssid="4">Estimation of the model requires a set of pairs (tw, T(w)), where tw is the correct parse for a sentence w. Whereas tw is provided by a treebank, T(w) has to be computed by parsing each w in the treebank.</S>
    <S sid="189" ssid="5">Previous studies assumed T(w) could be enumerated; however, this assumption is impractical because the size of T(w) is exponentially related to the length of w. Our solution here is to apply the feature forest model of Section 3 to the probabilistic modeling of HPSG parsing.</S>
    <S sid="190" ssid="6">Section 4.1 briefly introduces HPSG.</S>
    <S sid="191" ssid="7">Section 4.2 and 4.3 describe how to represent HPSG parse trees and predicate&#8211;argument structures by feature forests.</S>
    <S sid="192" ssid="8">Together with the parameter estimation algorithm in Section 3, these methods constitute a complete method for probabilistic disambiguation.</S>
    <S sid="193" ssid="9">We also address a method for accelerating the construction of feature forests for all treebank sentences in Section 4.4.</S>
    <S sid="194" ssid="10">The design of feature functions will be given in Section 4.5.</S>
    <S sid="195" ssid="11">HPSG (Pollard and Sag 1994; Sag, Wasow, and Bender 2003) is a syntactic theory that follows the lexicalist framework.</S>
    <S sid="196" ssid="12">In HPSG, linguistic entities, such as words and phrases, are denoted by signs, which are represented by typed feature structures (Carpenter 1992).</S>
    <S sid="197" ssid="13">Signs are a formal representation of combinations of phonological forms and syntactic/semantic structures, and express which phonological form signifies which syntactic/semantic structure.</S>
    <S sid="198" ssid="14">Figure 10 shows the lexical sign for loves.</S>
    <S sid="199" ssid="15">The geometry of signs follows Pollard and Sag: HEAD represents the part-of-speech of the head word, MOD denotes modifiee constraints, and SPR, SUBJ, and COMPS describe constraints of a specifier, a syntactic subject, and complements, respectively.</S>
    <S sid="200" ssid="16">CONT denotes the Lexical entry for the transitive verb loves.</S>
    <S sid="201" ssid="17">Simplified representation of the lexical entry in Figure 10. predicate&#8211;argument structure of a phrase/sentence.</S>
    <S sid="202" ssid="18">The notation of CONT in this article is borrowed from that of Minimal Recursion Semantics (Copestake et al. 2006): HOOK represents a structure accessed by other phrases, and RELS describes the remaining structure of the semantics.</S>
    <S sid="203" ssid="19">In what follows, we represent signs in a reduced form as shown in Figure 11, because of the large size of typical HPSG signs, which often include information not immediately relevant to the point being discussed.</S>
    <S sid="204" ssid="20">We will only show attributes that are relevant to an explanation, expecting that readers can fill in the values of suppressed attributes.</S>
    <S sid="205" ssid="21">In our actual implementation of the HPSG grammar, lexical/phrasal signs contain additional attributes that are not defined in the standard HPSG theory but are used by a disambiguation model.</S>
    <S sid="206" ssid="22">Examples include the surface form of lexical heads, and the type of lexical entry assigned to lexical heads, which are respectively used for computing the features WORD and LE introduced in Section 4.5.</S>
    <S sid="207" ssid="23">By incorporating additional attributes into signs, we can straightforwardly compute feature functions for each sign.</S>
    <S sid="208" ssid="24">This allows for a simple mapping between a parsing chart and a feature forest as described subsequently.</S>
    <S sid="209" ssid="25">However, this might increase the size of parse forests and therefore decrease parsing efficiency, because differences between additional attributes interfere with equivalence relations for ambiguity packing.</S>
    <S sid="210" ssid="26">We represent an HPSG parse tree with a set of tuples (m, l, r), where m,l, and r are the signs of the mother, left daughter, and right daughter, respectively.4 In chart parsing, partial parse candidates are stored in a chart, in which phrasal signs are identified and packed into equivalence classes if they are judged to be equivalent and dominate the same word sequences.</S>
    <S sid="211" ssid="27">A set of parse trees is then represented as a set of relations among equivalence classes.5 Figure 12 shows a chart for parsing he saw a girl with a telescope, where the modifiee of with is ambiguous (saw or girl).</S>
    <S sid="212" ssid="28">Each feature structure expresses an equivalence class, and the arrows represent immediate-dominance relations.</S>
    <S sid="213" ssid="29">The phrase, saw a girl with a telescope, has two trees (A in the figure).</S>
    <S sid="214" ssid="30">Because the signs of the top-most nodes are equivalent, they are packed into an equivalence class.</S>
    <S sid="215" ssid="31">The ambiguity is represented as the two pairs of arrows leaving the node A.</S>
    <S sid="216" ssid="32">A set of HPSG parse trees is represented in a chart as a tuple (E, Er, o ), where E is a set of equivalence classes, Er C E is a set of root nodes, and o : E -4 2E&#215;E is a function to represent immediate-dominance relations.</S>
    <S sid="217" ssid="33">Our representation of a chart can be interpreted as an instance of a feature forest.</S>
    <S sid="218" ssid="34">We map the tuple (em, el, er), which corresponds to (m, l, r), into a conjunctive node.</S>
    <S sid="219" ssid="35">Figure 13 shows (a part of) the HPSG parse trees in Figure 12 represented as a feature forest.</S>
    <S sid="220" ssid="36">Square boxes (ci) are conjunctive nodes, and di disjunctive nodes.</S>
    <S sid="221" ssid="37">A solid arrow represents a disjunctive daughter function, and a dotted line expresses a conjunctive daughter function.</S>
    <S sid="222" ssid="38">Formally, a chart (E, Er, o ) is mapped into a feature forest (C, D, R,-y, b) as follows.6 5 We assume that CONT and DTRS (a feature used to represent daughter signs) are restricted (Shieber 1985), and we will discuss a method for encoding CONT in a feature forest in Section 4.3.</S>
    <S sid="223" ssid="39">We also assume that parse trees are packed according to equivalence relations rather than subsumption relations (Oepen and Carroll 2000).</S>
    <S sid="224" ssid="40">We cannot simply map parse forests packed under subsumption into feature forests, because they over-generate possible unpacked trees.</S>
    <S sid="225" ssid="41">6 For ease of explanation, the definition of the root node is different from the original definition given in Section 3.</S>
    <S sid="226" ssid="42">In this section, we define R as a set of conjunctive nodes rather than a single node r. The definition here is translated into the original definition by introducing a dummy root node r' that has no features and only one disjunctive daughter whose daughters are R. Feature forest representation of HPSG parse trees in Figure 12. changing the model.</S>
    <S sid="227" ssid="43">Actually, we successfully developed a probabili stic model including features on nonlocalpredicate&#8211;argument dependencies, as described subsequently.</S>
    <S sid="228" ssid="44">Locality: In each step of composition of structure, only a limited depth of the structures are referred to.</S>
    <S sid="229" ssid="45">That is, local structures in the deep descendent phrases maybe ignored to construct larger phrases.</S>
    <S sid="230" ssid="46">This assumption mean apredicate&#8211;argument daughters&#8217;predicate&#8211;argument s that predicate&#8211;argument structures can be packed into conjunctive nodes by ignoring local structures.</S>
    <S sid="231" ssid="47">One may claim that restricting the domain of feature functions to (em, el, er) limits the flexibility of feature design.</S>
    <S sid="232" ssid="48">Although this is true to some extent, it does not necessarily mean the impossibility of incorporating features on nonlocal dependencies into the model.</S>
    <S sid="233" ssid="49">This is because a feature forest model does not assume probabilistic independence of conjunctive nodes.</S>
    <S sid="234" ssid="50">This means that we can unpack a part of the forest without With the method previously described, we can represent an HPSG parsing chart with a feature forest.</S>
    <S sid="235" ssid="51">However, equivalence classes in a chart might increase exponentially because predicate&#8211;argument structures in HPSG signs represent the semantic relations of all words that the phrase dominates.</S>
    <S sid="236" ssid="52">For example, Figure 14 shows phrasal signs with predicate&#8211;argument structures for saw a girl with a telescope.</S>
    <S sid="237" ssid="53">In the chart in Figure 12, these signs are packed into an equivalence class.</S>
    <S sid="238" ssid="54">However, Figure 14 shows that the values of CONT, that is, predicate&#8211;argument structures, have different values, and the signs as they are cannot be equivalent.</S>
    <S sid="239" ssid="55">As seen in this example, predicate&#8211;argument structures prevent us from packing signs into equivalence classes.</S>
    <S sid="240" ssid="56">In this section, we apply the feature forest model to predicate&#8211;argument structures, which may include reentrant structures and non-local dependencies.</S>
    <S sid="241" ssid="57">It is theoretically difficult to apply the feature forest model to predicate&#8211;argument structures; a feature forest cannot represent graph structures that include reentrant structures in a straightforward manner.</S>
    <S sid="242" ssid="58">However, if predicate&#8211;argument structures are constructed as in the manner described subsequently, they can be represented by feature forests of a tractable size.</S>
    <S sid="243" ssid="59">Feature forests can represent predicate&#8211;argument structures if we assume some locality and monotonicity in the composition of predicate&#8211;argument structures.</S>
    <S sid="244" ssid="60">Signs with predicate&#8211;argument structures.</S>
    <S sid="245" ssid="61">Computational Linguistics Volume 34, Number 1 Monotonicity: All relations in the daughters&#8217; predicate&#8211;argument structures are percolated to the mother.</S>
    <S sid="246" ssid="62">That is, none of the predicate&#8211;argument relations in the daughter phrases disappear in the mother.</S>
    <S sid="247" ssid="63">Thus predicate&#8211;argument structures of descendent phrases can be located at lower nodes in a feature forest.</S>
    <S sid="248" ssid="64">Predicate&#8211;argument structures usually satisfy the above conditions, even when they include non-local dependencies.</S>
    <S sid="249" ssid="65">For example, Figure 15 shows HPSG lexical entries for the wh-extraction of the object of love (left) and for the control construction of try (right).</S>
    <S sid="250" ssid="66">The first condition is satisfied because both lexical entries refer to CONT|HOOK of argument signs in SUBJ, COMPS, and SLASH.</S>
    <S sid="251" ssid="67">None of the lexical entries directly access ARGX of the arguments.</S>
    <S sid="252" ssid="68">The second condition is also satisfied because the values of CONT|HOOK of all of the argument signs are percolated to ARGX of the mother.</S>
    <S sid="253" ssid="69">In addition, the elements in CONT|RELS are percolated to the mother by the Semantic Principle.</S>
    <S sid="254" ssid="70">Compositional semantics usually satisfies the above conditions, including MRS (Copestake et al. 1995, 2006).</S>
    <S sid="255" ssid="71">The composition of MRS refers to HOOK, and no internal structures of daughters.</S>
    <S sid="256" ssid="72">The Semantic Principle of MRS also assures that all semantic relations in RELS are percolated to the mother.</S>
    <S sid="257" ssid="73">When these conditions are satisfied, semantics may include any constraints, such as selectional restrictions, although the grammar we used in the experiments does not include semantic restrictions to constrain parse forests.</S>
    <S sid="258" ssid="74">Under these conditions, local structures of predicate&#8211;argument structures are encoded into a conjunctive node when the values of all of its arguments have been instantiated.</S>
    <S sid="259" ssid="75">We introduce the notion of inactives to denote such local structures.</S>
    <S sid="260" ssid="76">An inactive is a subset of predicate&#8211;argument structures in which all arguments have been instantiated.</S>
    <S sid="261" ssid="77">Because inactive parts will not change during the rest of the parsing process, they can be placed in a conjunctive node.</S>
    <S sid="262" ssid="78">By placing newly generated inactives into corresponding conjunctive nodes, a set of predicate&#8211;argument structures can be represented in a feature forest by packing local ambiguities, and non-local dependencies are preserved.</S>
    <S sid="263" ssid="79">Lexical entries including non-local relations. and fact may optionally take a complementizer phrase.7 The predicate&#8211;argument structures for dispute1 and dispute2 are shown in Figure 17.</S>
    <S sid="264" ssid="80">Curly braces express the ambiguities of partially constructed predicate&#8211;argument structures.</S>
    <S sid="265" ssid="81">The resulting feature forest is shown in Figure 18.</S>
    <S sid="266" ssid="82">The boxes denote conjunctive nodes and dx represent disjunctive nodes.</S>
    <S sid="267" ssid="83">The clause I wanted to dispute has two possible predicate&#8211;argument structures: one corresponding to dispute1 (&#945; in Figure 16) and the other corresponding to dispute2 (&#946; in Figure 16).</S>
    <S sid="268" ssid="84">The nodes of the predicate&#8211;argument structure &#945; are all instantiated, that is, it contains only inactives.</S>
    <S sid="269" ssid="85">The corresponding conjunctive node (&#945;' in Figure 18) has two inactives, for want and dispute1.</S>
    <S sid="270" ssid="86">The other structure &#946; has an unfilled object in the argument (ARG28) of dispute2, which will be filled by the non-local dependency.</S>
    <S sid="271" ssid="87">Hence, the corresponding conjunctive node &#946;' has only one inactive corresponding to want, and the remaining part that corresponds to dispute2 is passed on for further processing.</S>
    <S sid="272" ssid="88">When we process the phrase the fact that I wanted to dispute, the object of dispute2 is filled by fact (&#947; in Figure 16), and the predicate&#8211;argument structure of dispute2 is then placed into a conjunctive node (&#947;' in Figure 18).</S>
    <S sid="273" ssid="89">A feature forest representation of predicate&#8211;argument structures.</S>
    <S sid="274" ssid="90">One of the beneficial characteristics of this packed representation is that the representation is isomorphic to the parsing process, that is, a chart.</S>
    <S sid="275" ssid="91">Hence, we can assign features of HPSG parse trees to a conjunctive node, together with features of predicate&#8211; argument structures.</S>
    <S sid="276" ssid="92">In Section 5, we will investigate the contribution of features on parse trees and predicate&#8211;argument structures to the disambiguation of HPSG parsing.</S>
    <S sid="277" ssid="93">The method just described is the essence of our solution for the tractable estimation of maximum entropy models on exponentially many HPSG parse trees.</S>
    <S sid="278" ssid="94">However, the problem of computational cost remains.</S>
    <S sid="279" ssid="95">Construction of feature forests requires parsing of all of the sentences in a treebank.</S>
    <S sid="280" ssid="96">Despite the development of methods to improve HPSG parsing efficiency (Oepen, Flickinger, et al. 2002), exhaustive parsing of all sentences is still expensive.</S>
    <S sid="281" ssid="97">We assume that computation of parse trees with low probabilities can be omitted in the estimation stage because T(w) can be approximated by parse trees with high probabilities.</S>
    <S sid="282" ssid="98">To achieve this, we first prepared a preliminary probabilistic model whose estimation did not require the parsing of a treebank.</S>
    <S sid="283" ssid="99">The preliminary model was used to reduce the search space for parsing a training treebank.</S>
    <S sid="284" ssid="100">The preliminary model in this study is a unigram model, &#175;p(t|w) _ fJw&#8712;w p(l|w), where w &#8712; w is a word in the sentence w, and l is a lexical entry assigned to w. This model is estimated by counting the relative frequencies of lexical entries used for w in the training data.</S>
    <S sid="285" ssid="101">Hence, the estimation does not require parsing of a treebank.</S>
    <S sid="286" ssid="102">Actually, we use a maximum entropy model to compute this probability as described in Section 5.</S>
    <S sid="287" ssid="103">The preliminary model is used for filtering lexical entries when we parse a treebank.</S>
    <S sid="288" ssid="104">Given this model, we restrict the number of lexical entries used to parse a treebank.</S>
    <S sid="289" ssid="105">With a threshold n for the number of lexical entries and a threshold c for the probability, lexical entries are assigned to a word in descending order of probability, until the number of assigned entries exceeds n, or the accumulated probability exceeds c. If this procedure does not assign a lexical entry necessary to produce a correct parse (i.e., an oracle lexical entry), it is added to the list of lexical entries.</S>
    <S sid="290" ssid="106">It should be noted that oracle lexical entries are given by the HPSG treebank.</S>
    <S sid="291" ssid="107">This assures that the filtering method does not exclude correct parse trees from parse forests.</S>
    <S sid="292" ssid="108">Figure 19 shows an example of filtering the lexical entries assigned to saw.</S>
    <S sid="293" ssid="109">With c = 0.95, four lexical entries are assigned.</S>
    <S sid="294" ssid="110">Although the lexicon includes other lexical entries, such as a verbal entry taking a sentential complement (p = 0.01 in the figure), they are filtered out.</S>
    <S sid="295" ssid="111">Although this method reduces the time required for parsing a treebank, this approximation causes bias in the training data and results in lower accuracy.</S>
    <S sid="296" ssid="112">The tradeoff between parsing cost and accuracy will be examined experimentally in Section 5.4.</S>
    <S sid="297" ssid="113">We have several ways to integrate p&#175; with the estimated model p(tjT(w)).</S>
    <S sid="298" ssid="114">In the experiments, we will empirically compare the following methods in terms of accuracy and estimation time.</S>
    <S sid="299" ssid="115">Filtering only: The unigram probability p&#175; is used only for filtering in training.</S>
    <S sid="300" ssid="116">Product: The probability is defined as the product of p&#175; and the estimated model p. Reference distribution: p&#175; is used as a reference distribution of p. Feature function: log p&#175; is used as a feature function of p. This method has been shown to be a generalization of the reference distribution method (Johnson and Riezler 2000).</S>
    <S sid="301" ssid="117">Feature functions in maximum entropy models are designed to capture the characteristics of (em, el, er).</S>
    <S sid="302" ssid="118">In this article, we investigate combinations of the atomic features listed Filtering of lexical entries for saw.</S>
    <S sid="303" ssid="119">SYM symbol of the phrasal category (e.g., NP, VP) WORD surface form of the head word POS part-of-speech of the head word LE lexical entry assigned to the head word ARG argument label of a predicate in Table 1.</S>
    <S sid="304" ssid="120">The following combinations are used for representing the characteristics of binary/unary schema applications.</S>
    <S sid="305" ssid="121">&#65533; RULE,DIST,COMMA, fbinary = SPANl, SYMl, WORDl, POSl, LEl, SPANr, SYMr, WORDr, POSr, LEr funary = (RULE,SYM,WORD,POS,LE) where subscripts l and r denote left and right daughters.</S>
    <S sid="306" ssid="122">In addition, the following is used for expressing the condition of the root node of the parse tree.</S>
    <S sid="307" ssid="123">Feature functions to capture predicate&#8211;argument dependencies are represented as follows: fpa = ARG, DIST, WORDp, POSp, LEp, WORDa, POSa, LEa) where subscripts p and a represent predicate and argument, respectively.</S>
    <S sid="308" ssid="124">Figure 20 shows examples: froot is for the root node, in which the phrase symbol is S and the surface form, part-of-speech, and lexical entry of the lexical head are saw, VBD, and a transitive verb, respectively. fbinary is for the binary rule application to saw a girl and with a telescope, in which the applied schema is the Head-Modifier Schema, the left daughter is VP headed by saw, and the right daughter is PP headed by with, whose part-of-speech is IN and whose lexical entry is a VP-modifying preposition.</S>
    <S sid="309" ssid="125">Figure 21 shows example features for predicate&#8211;argument structures.</S>
    <S sid="310" ssid="126">The figure shows features assigned to the conjunctive node denoted as &#945;' in Figure 18.</S>
    <S sid="311" ssid="127">Because inactive structures in the node have three predicate&#8211;argument relations, three features are activated.</S>
    <S sid="312" ssid="128">The first one is for the relation of want and I, where the label of the relation is ARG1, the distance between the head words is 1, the surface string and the POS of Example features for predicate&#8211;argument structures. the predicate are want and VBD, and those of the argument are I and PRP.</S>
    <S sid="313" ssid="129">The second and the third features are for the other two relations.</S>
    <S sid="314" ssid="130">We may include features on more than two relations, such as the dependencies among want, I, and dispute, although such features are not incorporated currently.</S>
    <S sid="315" ssid="131">In our implementation, some of the atomic features are abstracted (i.e., ignored) for smoothing.</S>
    <S sid="316" ssid="132">Tables 2, 3, and 4 show the full set of templates of combined features used in the experiments.</S>
    <S sid="317" ssid="133">Each row represents the template for a feature function.</S>
    <S sid="318" ssid="134">A check indicates the atomic feature is incorporated, and a hyphen indicates the feature is ignored.</S>
    <S sid="319" ssid="135">Feature templates for root condition.</S>
    <S sid="320" ssid="136">Feature templates for predicate&#8211;argument dependencies.</S>
  </SECTION>
  <SECTION title="5." number="6">
    <S sid="321" ssid="1">This section presents experimental results on the parsing accuracy attained by the feature forest models.</S>
    <S sid="322" ssid="2">In all of the following experiments, we use the HPSG grammar developed by the method of Miyao, Ninomiya, and Tsujii (2005).</S>
    <S sid="323" ssid="3">Section 5.1 describes how this grammar was developed.</S>
    <S sid="324" ssid="4">Section 5.2 explains other aspects of the experimental settings.</S>
    <S sid="325" ssid="5">In Sections 5.3 to 5.7, we report results of the experiments on HPSG parsing.</S>
    <S sid="326" ssid="6">In the following experiments, we use Enju 2.1 (Tsujii Laboratory 2004), which is a widecoverage HPSG grammar extracted from the Penn Treebank by the method of Miyao, Ninomiya, and Tsujii (2005).</S>
    <S sid="327" ssid="7">In this method, we convert the Penn Treebank into an HPSG treebank, and collect HPSG lexical entries from terminal nodes of the HPSG treebank.</S>
    <S sid="328" ssid="8">Figure 22 illustrates the process of treebank conversion and lexicon collection.</S>
    <S sid="329" ssid="9">We first convert and fertilize parse trees of the Penn Treebank.</S>
    <S sid="330" ssid="10">This step identifies syntactic constructions that require special treatment in HPSG, such as raising/control and long-distance dependencies.</S>
    <S sid="331" ssid="11">These constructions are then annotated with typed feature structures so that they conform to the HPSG analysis.</S>
    <S sid="332" ssid="12">Next, we apply HPSG schemas and principles, and obtain fully specified HPSG parse trees.</S>
    <S sid="333" ssid="13">This step solves feature structure constraints given in the previous step, and fills unspecified constraints.</S>
    <S sid="334" ssid="14">Failures of schema/principle applications indicate that the annotated constraints do not Extracting HPSG lexical entries from the Penn Treebank. conform to the HPSG analysis, and require revisions.</S>
    <S sid="335" ssid="15">Finally, we obtain lexical entries from the HPSG parse trees.</S>
    <S sid="336" ssid="16">The terminal nodes of HPSG parse trees are collected, and they are generalized by removing word-specific or context-specific constraints.</S>
    <S sid="337" ssid="17">An advantage of this method is that a wide-coverage HPSG lexicon is obtained because lexical entries are extracted from real-world sentences.</S>
    <S sid="338" ssid="18">Obtained lexical entries are guaranteed to construct well-formed HPSG parse trees because HPSG schemas and principles are successfully applied during the development of the HPSG treebank.</S>
    <S sid="339" ssid="19">Another notable feature is that we can additionally obtain an HPSG treebank, which can be used as training data for disambiguation models.</S>
    <S sid="340" ssid="20">In the following experiments, this HPSG treebank is used for the training of maximum entropy models.</S>
    <S sid="341" ssid="21">The lexicon used in the following experiments was extracted from Sections 02&#8211;21 of the Wall Street Journal portion of the Penn Treebank.</S>
    <S sid="342" ssid="22">This lexicon can assign correct lexical entries to 99.09% of words in the HPSG treebank converted from Penn Treebank Section 23.</S>
    <S sid="343" ssid="23">This number expresses &#8220;lexical coverage&#8221; in the strong sense defined by Hockenmaier and Steedman (2002).</S>
    <S sid="344" ssid="24">In this notion of &#8220;coverage,&#8221; this lexicon has 84.1% sentential coverage, where this means that the lexicon can assign correct lexical entries to all of the words in a sentence.</S>
    <S sid="345" ssid="25">Although the parser might produce parse results for uncovered sentences, these parse results cannot be completely correct.</S>
    <S sid="346" ssid="26">The data for the training of the disambiguation models was the HPSG treebank derived from Sections 02&#8211;21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction.</S>
    <S sid="347" ssid="27">For training of the disambiguation models, we eliminated sentences of 40 words or more and sentences for which the parser could not produce the correct parses.</S>
    <S sid="348" ssid="28">The resulting training set consists of 33,604 sentences (when n = 10 and c = 0.95; see Section 5.4 for details).</S>
    <S sid="349" ssid="29">The treebanks derived from Sections 22 and 23 were used as the development and final test sets, respectively.</S>
    <S sid="350" ssid="30">Following previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000), accuracy is measured for sentences of less than 40 words and for those with less than 100 words.</S>
    <S sid="351" ssid="31">Table 5 shows the specifications of the test data.</S>
    <S sid="352" ssid="32">The measure for evaluating parsing accuracy is precision/recall of predicate&#8211; argument dependencies output by the parser.</S>
    <S sid="353" ssid="33">A predicate&#8211;argument dependency is defined as a tuple (wh,wn,7t, p), where wh is the head word of the predicate, wn is the head word of the argument, 7t is the type of the predicate (e.g., adjective, intransitive verb), and p is an argument label (MODARG, ARG1, ..., ARG4).</S>
    <S sid="354" ssid="34">For example, He tried running has three dependencies as follows: Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the parser, and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identified regardless of &#960; and p. F-score is the harmonic mean of LP and LR.</S>
    <S sid="355" ssid="35">Sentence accuracy is the exact match accuracy of complete predicate&#8211;argument relations in a sentence.</S>
    <S sid="356" ssid="36">These measures correspond to those used in other studies measuring the accuracy of predicate&#8211;argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact figures cannot be compared directly because the definitions of dependencies are different.</S>
    <S sid="357" ssid="37">All predicate&#8211;argument dependencies in a sentence are the target of evaluation except quotation marks and periods.</S>
    <S sid="358" ssid="38">The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted.</S>
    <S sid="359" ssid="39">The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set.</S>
    <S sid="360" ssid="40">The algorithm for parameter estimation was the limited-memory BFGS method (Nocedal 1980; Nocedal and Wright 1999).</S>
    <S sid="361" ssid="41">The parser was implemented in C++ with the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and Tsujii 2004; Ninomiya et al.</S>
    <S sid="362" ssid="42">2005).</S>
    <S sid="363" ssid="43">Other efficient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were not used.</S>
    <S sid="364" ssid="44">The results obtained using these techniques are given in Ninomiya et al. A limit on the number of constituents was set for time-out; the parser stopped parsing when the number of constituents created during parsing exceeded 50,000.</S>
    <S sid="365" ssid="45">In such a case, the parser output nothing, and the recall was computed as zero.</S>
    <S sid="366" ssid="46">Features occurring more than twice were included in the probabilistic models.</S>
    <S sid="367" ssid="47">A method of filtering lexical entries was applied to the parsing of training data (Section 4.4).</S>
    <S sid="368" ssid="48">Unless otherwise noted, parameters for filtering were n = 10 and c = 0.95, and a reference distribution method was applied.</S>
    <S sid="369" ssid="49">The unigram model, p0(t|s), for filtering is a maximum entropy model with two feature templates, (WORD, POS, LE) and (POS, LE).</S>
    <S sid="370" ssid="50">The model includes 24,847 features.</S>
    <S sid="371" ssid="51">Tables 6 and 7 show parsing accuracy for the test set.</S>
    <S sid="372" ssid="52">In the tables, &#8220;Syntactic features&#8221; denotes a model with syntactic features, that is, fbinary, funary, and froot introduced in Section 4.5.</S>
    <S sid="373" ssid="53">&#8220;Semantic features&#8221; represents a model with features on predicate&#8211; argument structures, that is, fpa given in Table 4.</S>
    <S sid="374" ssid="54">&#8220;All&#8221; is a model with both syntactic and semantic features.</S>
    <S sid="375" ssid="55">The &#8220;Baseline&#8221; row shows the results for the reference model, p0(t|s), used for lexical entry filtering in the estimation of the other models.</S>
    <S sid="376" ssid="56">This model is considered as a simple application of a traditional PCFG-style model; that is, p(r) = 1 for any rule r in the construction rules of the HPSG grammar.</S>
    <S sid="377" ssid="57">The results demonstrate that feature forest models have significantly higher accuracy than a baseline model.</S>
    <S sid="378" ssid="58">Comparing &#8220;Syntactic features&#8221; with &#8220;Semantic features,&#8221; we see that the former model attained significantly higher accuracy than the latter.</S>
    <S sid="379" ssid="59">This indicates that syntactic features are more important for overall accuracy.</S>
    <S sid="380" ssid="60">We will examine the contributions of each atomic feature of the syntactic features in Section 5.5.</S>
    <S sid="381" ssid="61">Features on predicate&#8211;argument relations were generally considered as important for the accurate disambiguation of syntactic structures.</S>
    <S sid="382" ssid="62">For example, PP-attachment ambiguity cannot be resolved with only syntactic preferences.</S>
    <S sid="383" ssid="63">However, the results show that a model with only semantic features performs significantly worse than one with syntactic features.</S>
    <S sid="384" ssid="64">Even when combined with syntactic features, semantic features do not improve accuracy.</S>
    <S sid="385" ssid="65">Obviously, semantic preferences are necessary for accurate parsing, but the features used in this work were not sufficient to capture semantic preferences.</S>
    <S sid="386" ssid="66">A possible reason is that, as reported in Gildea (2001), bilexical dependencies may be too sparse to capture semantic preferences.</S>
    <S sid="387" ssid="67">For reference, our results are competitive with the best corresponding results reported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our results cannot be compared directly with other grammar formalisms because each formalism represents predicate&#8211;argument dependencies differently.</S>
    <S sid="388" ssid="68">In contrast with the results of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearly lower than precision.</S>
    <S sid="389" ssid="69">This may have resulted from the HPSG grammar having stricter feature constraints and the parser not being able to produce parse results for around 1% of the sentences.</S>
    <S sid="390" ssid="70">To improve recall, we need techniques to deal with these 1% of sentences.</S>
    <S sid="391" ssid="71">Table 8 gives the computation/space costs of model estimation.</S>
    <S sid="392" ssid="72">&#8220;Estimation time&#8221; indicates user times required for running the parameter estimation algorithm.</S>
    <S sid="393" ssid="73">&#8220;No. of feature occurrences&#8221; denotes the total number of occurrences of features in the training data, and &#8220;Data size&#8221; gives the sizes of the compressed files of training data.</S>
    <S sid="394" ssid="74">We can conclude that feature forest models are estimated at a tractable computational cost and a reasonable data size, even when a model includes semantic features including nonlocal dependencies.</S>
    <S sid="395" ssid="75">The results reveal that feature forest models essentially solve the problem of the estimation of probabilistic models of sentence structures.</S>
    <S sid="396" ssid="76">Table 9 compares the estimation methods introduced in Section 4.4.</S>
    <S sid="397" ssid="77">In all of the following experiments, we show the accuracy for the test set (&lt;40 words) only.</S>
    <S sid="398" ssid="78">Table 9 reveals that our method achieves significantly lower accuracy when it is used only for filtering in the training phrase.</S>
    <S sid="399" ssid="79">One reason is that the feature forest model prefers lexical entries that are filtered out in the training phase, because they are always oracle lexical entries in the training.</S>
    <S sid="400" ssid="80">This means that we must incorporate the preference of filtering into the final parse selection.</S>
    <S sid="401" ssid="81">As shown in Table 9, the models combined with a preliminary model achieved sufficient accuracy.</S>
    <S sid="402" ssid="82">The reference distribution method achieved higher accuracy and lower cost.</S>
    <S sid="403" ssid="83">The feature function method achieved lower accuracy in our experiments.</S>
    <S sid="404" ssid="84">A possible reason for this is that a hyper-parameter of the prior was set to the same value for all the features including the feature of the log-probability given by the preliminary distribution.</S>
    <S sid="405" ssid="85">Tables 10 and 11 show the results of changing the filtering threshold.</S>
    <S sid="406" ssid="86">We can determine the correlation between the estimation/parsing cost and accuracy.</S>
    <S sid="407" ssid="87">In our experiment, n &gt; 10 and c &gt; 0.90 seem necessary to preserve the F-score over 86.0.</S>
    <S sid="408" ssid="88">Table 12 shows the accuracy with different feature sets.</S>
    <S sid="409" ssid="89">Accuracy was measured for 15 models with some atomic features removed from the final model.</S>
    <S sid="410" ssid="90">The last row denotes the accuracy attained by the unigram model (i.e., the reference distribution).</S>
    <S sid="411" ssid="91">The numbers in bold type represent a significant difference from the final model according to stratified shuffling tests with the Bonferroni correction (Cohen 1995) with p-value &lt; .05 for 32 pairwise comparisons.</S>
    <S sid="412" ssid="92">The results indicate that DIST, COMMA, SPAN, WORD, and Filtering threshold vs. estimation cost. n, c Estimation time (sec.)</S>
    <S sid="413" ssid="93">Parsing time (sec.)</S>
    <S sid="414" ssid="94">Data size (MB) 5, 0.80 108 5,103 341 5, 0.90 150 6,242 407 5, 0.95 190 7,724 469 5, 0.98 259 9,604 549 10, 0.80 130 6,003 370 10, 0.90 268 8,855 511 10, 0.95 511 15,393 727 10, 0.98 1,395 36,009 1,230 15, 0.80 123 6,298 372 15, 0.90 259 9,543 526 15, 0.95 735 20,508 854 15, 0.98 3,777 86,844 2,031 POS features contributed to the final accuracy, although the differences were slight.</S>
    <S sid="415" ssid="95">In contrast, RULE, SYM, and LE features did not affect accuracy.</S>
    <S sid="416" ssid="96">However, when each was removed together with another feature, the accuracy decreased drastically.</S>
    <S sid="417" ssid="97">This implies that such features carry overlapping information.</S>
    <S sid="418" ssid="98">Table 13 shows parsing accuracy for covered and uncovered sentences.</S>
    <S sid="419" ssid="99">As defined in Section 5.1, &#8220;covered&#8221; indicates that the HPSG lexicon has all correct lexical entries for a sentence.</S>
    <S sid="420" ssid="100">In other words, for covered sentences, exactly correct parse trees are obtained if the disambiguation model worked perfectly.</S>
    <S sid="421" ssid="101">The result reveals clear differences in accuracy between covered and uncovered sentences.</S>
    <S sid="422" ssid="102">The F-score for covered sentences is around 2.5 points higher than the overall F-score, whereas the F-score is more than 10 points lower for uncovered sentences.</S>
    <S sid="423" ssid="103">This result indicates improvement of lexicon quality is an important factor for higher accuracy.</S>
    <S sid="424" ssid="104">Figure 23 shows the learning curve.</S>
    <S sid="425" ssid="105">A feature set was fixed, and the parameter of the Gaussian prior was optimized for each model.</S>
    <S sid="426" ssid="106">High accuracy is attained even with a small training set, and the accuracy seems to be saturated.</S>
    <S sid="427" ssid="107">This indicates that we cannot further improve the accuracy simply by increasing the size of the training data set.</S>
    <S sid="428" ssid="108">The exploration of new types of features is necessary for higher accuracy.</S>
    <S sid="429" ssid="109">It should also be noted that the upper bound of the accuracy is not 100%, because the grammar cannot produce completely correct parse results for uncovered sentences.</S>
    <S sid="430" ssid="110">Figure 24 shows the accuracy for each sentence length.</S>
    <S sid="431" ssid="111">It is apparent from this figure that the accuracy is significantly higher for sentences with less than 10 words.</S>
    <S sid="432" ssid="112">This implies that experiments with only short sentences overestimate the performance of parsers.</S>
    <S sid="433" ssid="113">Sentences with at least 10 words are necessary to properly evaluate the performance of parsing real-world texts.</S>
    <S sid="434" ssid="114">The accuracies for the sentences with more than 10 words are not very different, although data points for sentences with more than 50 words are not reliable.</S>
    <S sid="435" ssid="115">Table 14 shows the accuracies for predicate&#8211;argument relations when partsof-speech tags are assigned automatically by a maximum-entropy-based parts-ofspeech tagger (Tsuruoka and Tsujii 2005).</S>
    <S sid="436" ssid="116">The results indicate a drop of about three points in labeled precision/recall (a two-point drop in unlabeled precision/recall).</S>
    <S sid="437" ssid="117">A reason why we observed larger accuracy drops in labeled precision/recall is that Sentence length vs. accuracy. predicate&#8211;argument relations are fragile with respect to parts-of-speech errors because predicate types (e.g., adjective, intransitive verb) are determined depending on the parts-of-speech of predicate words.</S>
    <S sid="438" ssid="118">Although our current parsing strategy assumes that parts-of-speech are given beforehand, for higher accuracy in real application contexts, we will need a method for determining parts-of-speech and parse trees jointly.</S>
    <S sid="439" ssid="119">Table 15 shows a manual classification of the causes of disambiguation errors in 100 sentences randomly chosen from Section 00.</S>
    <S sid="440" ssid="120">In our evaluation, one error source may cause multiple dependency errors.</S>
    <S sid="441" ssid="121">For example, if an incorrect lexical entry is assigned to a verb, all of the argument dependencies of the verb are counted as errors.</S>
    <S sid="442" ssid="122">The numbers in the table include such double-counting.</S>
    <S sid="443" ssid="123">Figure 25 shows examples of disambiguation errors.</S>
    <S sid="444" ssid="124">The figure shows output from the parser.</S>
    <S sid="445" ssid="125">Major causes are classified into three types: attachment ambiguity, argument/ modifier distinction, and lexical ambiguity.</S>
    <S sid="446" ssid="126">As attachment ambiguities are well-known error sources, PP-attachment is the largest source of errors in our evaluation.</S>
    <S sid="447" ssid="127">Our disambiguation model cannot accurately resolve PP-attachment ambiguities because it does not include dependencies among a modifiee and the argument of the preposition.</S>
    <S sid="448" ssid="128">Because previous studies revealed that such dependencies are effective features for PP-attachment resolution, we should incorporate them into our model.</S>
    <S sid="449" ssid="129">Some of the attachment ambiguities, including adjective and adverb, should also be resolved with an extension of features.</S>
    <S sid="450" ssid="130">However, we cannot identify any effective features for the disambiguation of attachment of verbal phrases, including relative clauses, verb phrases, subordinate clauses, and to-infinitives.</S>
    <S sid="451" ssid="131">For example, Figure 25 shows an example error of the attachment of a relative clause.</S>
    <S sid="452" ssid="132">The correct answer is that the Examples of disambiguation errors. subject of yielded is acre, but this cannot be determined only by the relation among yield, grapes, and acre.</S>
    <S sid="453" ssid="133">The resolution of these errors requires a novel type of feature function.</S>
    <S sid="454" ssid="134">Errors of argument/modifier distinction are prominent in deep syntactic analysis, because arguments and modifiers are not explicitly distinguished in the evaluation of CFG parsers.</S>
    <S sid="455" ssid="135">Figure 25 shows an example of the argument/modifier distinction of a to-infinitive clause.</S>
    <S sid="456" ssid="136">In this case, the to-infinitive clause is a complement of tempts.</S>
    <S sid="457" ssid="137">The subcategorization frame of tempts seems responsible for this problem.</S>
    <S sid="458" ssid="138">However, the disambiguation model wrongly assigned a lexical entry for a transitive verb because of the sparseness of the training data (tempts occurred only once in the training data).</S>
    <S sid="459" ssid="139">The resolution of this sort of ambiguity requires the refinement of a probabilistic model of lexical entries.</S>
    <S sid="460" ssid="140">Errors of verb phrases and subordinate clauses are similar to this example.</S>
    <S sid="461" ssid="141">Errors of argument/modifier distinction of noun phrases are mainly caused by temporal nouns and cardinal numbers.</S>
    <S sid="462" ssid="142">The resolution of these errors seems to require the identification of temporal expressions and usage of cardinal numbers.</S>
    <S sid="463" ssid="143">Errors of lexical ambiguities were mainly caused by idioms.</S>
    <S sid="464" ssid="144">For example, in Figure 25, compared with is a compound preposition, but the parser recognized it as a verb phrase.</S>
    <S sid="465" ssid="145">This indicates that the grammar or the disambiguation model requires the special treatment of idioms.</S>
    <S sid="466" ssid="146">Errors of verb subcategorization frames were mainly caused by difficult constructions such as insertions.</S>
    <S sid="467" ssid="147">Figure 25 shows that the parser could not identify the inserted clause (says John Siegel...) and a lexical entry for a declarative transitive verb was chosen.</S>
    <S sid="468" ssid="148">Attachment errors of commas are also significant.</S>
    <S sid="469" ssid="149">It should be noted that commas were ignored in the evaluation of CFG parsers.</S>
    <S sid="470" ssid="150">We did not eliminate punctuation from the evaluation because punctuation sometimes contributes to semantics, as in coordination and insertion.</S>
    <S sid="471" ssid="151">In this error analysis, errors of commas representing coordination/insertion are classified into &#8220;coordination/insertion,&#8221; and &#8220;comma&#8221; indicates errors that do not contribute to the computation of semantics.</S>
    <S sid="472" ssid="152">Errors of noun phrase identification mean that a noun phrase was split into two phrases.</S>
    <S sid="473" ssid="153">These errors were mainly caused by the indirect effects of other errors.</S>
    <S sid="474" ssid="154">Errors of identifying coordination/insertion structures sometimes resulted in catastrophic analyses.</S>
    <S sid="475" ssid="155">While accurate analysis of such constructions is indispensable, it is also known to be difficult because disambiguation of coordination/insertion requires the computation of preferences over global structures, such as the similarity of syntactic/semantic structure of coordinates.</S>
    <S sid="476" ssid="156">Incorporating features for representing the similarity of global structures is difficult for feature forest models.</S>
    <S sid="477" ssid="157">Zero-pronoun resolution is also a difficult problem.</S>
    <S sid="478" ssid="158">However, we found that most were indirectly caused by errors of argument/modifier distinction in to-infinitive clauses.</S>
    <S sid="479" ssid="159">A significant portion of the errors discussed above cannot be resolved by the features we investigated in this study, and the design of other features will be necessary for improving parsing accuracy.</S>
  </SECTION>
  <SECTION title="6." number="7">
    <S sid="480" ssid="1">The model described in this article was first published in Miyao and Tsujii (2002), and has been applied to probabilistic models for parsing with lexicalized grammars.</S>
    <S sid="481" ssid="2">Applications to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al. 2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained higher accuracy than other models.</S>
    <S sid="482" ssid="3">These researchers applied feature forests to representations of the packed parse results of LFG and the dependency/derivation structures of CCG.</S>
    <S sid="483" ssid="4">Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars.</S>
    <S sid="484" ssid="5">Feature forest models were also shown to be effective for wide-coverage sentence realization (Nakanishi, Miyao, and Tsujii 2005).</S>
    <S sid="485" ssid="6">This work demonstrated that feature forest models are generic enough to be applied to natural language processing tasks other than parsing.</S>
    <S sid="486" ssid="7">The work of Geman and Johnson (2002) independently developed a dynamic programming algorithm for maximum entropy models.</S>
    <S sid="487" ssid="8">The solution was similar to our approach, although their method was designed to traverse LFG parse results represented with disjunctive feature structures as proposed by Maxwell and Kaplan (1995).</S>
    <S sid="488" ssid="9">The difference between the two approaches is that feature forests use a simpler generic data structure to represent packed forest structures.</S>
    <S sid="489" ssid="10">Therefore, without assuming what feature forests represent, our algorithm can be applied to various tasks, including theirs.</S>
    <S sid="490" ssid="11">Another approach to the probabilistic modeling of complete structures is a method of approximation.</S>
    <S sid="491" ssid="12">The work on whole sentence maximum entropy models (Rosenfeld 1997; Chen and Rosenfeld 1999b) proposed an approximation algorithm to estimate parameters of maximum entropy models on whole sentence structures.</S>
    <S sid="492" ssid="13">However, the algorithm suffered from slow convergence, and the model was basically a sequence model.</S>
    <S sid="493" ssid="14">It could not produce a solution for complex structures as our model can.</S>
    <S sid="494" ssid="15">We should also mention Conditional Random Fields (CRFs) (Lafferty, McCallum, and Pereira 2001) for solving a similar problem in the context of maximum entropy Markov models.</S>
    <S sid="495" ssid="16">Their solution was an algorithm similar to the computation of forward/backward probabilities of hidden Markov models (HMMs).</S>
    <S sid="496" ssid="17">Their algorithm is a special case of our algorithm in which each conjunctive node has only one daughter.</S>
    <S sid="497" ssid="18">This is obvious because feature forests can represent Markov chains.</S>
    <S sid="498" ssid="19">In an analogy, CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs.</S>
    <S sid="499" ssid="20">Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models.</S>
    <S sid="500" ssid="21">This fact implies that our algorithm is applicable to not only parsing but also to other tasks.</S>
    <S sid="501" ssid="22">CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al.</S>
    <S sid="502" ssid="23">2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004).</S>
    <S sid="503" ssid="24">These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures.</S>
    <S sid="504" ssid="25">Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models.</S>
    <S sid="505" ssid="26">The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical computation, to obtain approximate solutions.</S>
    <S sid="506" ssid="27">A similar method may be developed to overcome a bottleneck of feature forest models, that is, the fact that feature functions are localized to conjunctive nodes.</S>
    <S sid="507" ssid="28">The structure of feature forests is common in natural language processing and computational linguistics.</S>
    <S sid="508" ssid="29">As is easily seen, lattices, Markov chains, and CFG parse trees are represented by feature forests.</S>
    <S sid="509" ssid="30">Furthermore, because conjunctive nodes do not necessarily represent CFG nodes or rules and terminals of feature forests need not be words, feature forests can express any forest structures in which ambiguities are packed in local structures.</S>
    <S sid="510" ssid="31">Examples include the derivation trees of LTAG and CCG.</S>
    <S sid="511" ssid="32">Chiang (2003) proved that feature forests could be considered as the derivation forests of linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987; Weir 1988).</S>
    <S sid="512" ssid="33">LCFRSs define a wide variety of grammars, including LTAG and CCG, while preserving polynomial-time complexity of parsing.</S>
    <S sid="513" ssid="34">This demonstrates that feature forest models are applicable to probabilistic models far beyond PCFGs.</S>
    <S sid="514" ssid="35">Feature forests are also isomorphic to support graphs (or explanation graphs) used in the graphical EM algorithm (Kameya and Sato 2000).</S>
    <S sid="515" ssid="36">In their framework, a program in a logic programming language, PRISM (Sato and Kameya 1997), is converted into support graphs, and parameters of probabilistic models are automatically learned by an EM algorithm.</S>
    <S sid="516" ssid="37">Support graphs have been proved to represent various statistical structural models, including HMMs, PCFGs, Bayesian networks, and many other graphical structures (Sato and Kameya 2001; Sato 2005).</S>
    <S sid="517" ssid="38">Taken together, these results imply the high applicability of feature forest models to various real tasks.</S>
    <S sid="518" ssid="39">Because feature forests have a structure isomorphic to parse forests of PCFG, it might seem that they can represent only immediate dominance relations of CFG rules as in PCFG, resulting in only a slight, trivial extension of PCFG.</S>
    <S sid="519" ssid="40">As described herein, however, feature forests can represent structures beyond CFG parse trees.</S>
    <S sid="520" ssid="41">Furthermore, because feature forests are a generalized representation of ambiguous structures, each node in a feature forest need not correspond to a node in a PCFG parse forest.</S>
    <S sid="521" ssid="42">That is, a node in a feature forest may represent any linguistic entity, including a fragment of a syntactic structure, a semantic relation, or other sentence-level information.</S>
    <S sid="522" ssid="43">The idea of feature forest models could be applied to non-probabilistic machine learning methods.</S>
    <S sid="523" ssid="44">Taskar et al. (2004) proposed a dynamic programming algorithm for the learning of large-margin classifiers including support vector machines (Vapnik 1995), and presented its application to disambiguation in CFG parsing.</S>
    <S sid="524" ssid="45">Their algorithm resembles feature forest models; an optimization function is computed by a dynamic programing algorithm without unpacking packed forest structures.</S>
    <S sid="525" ssid="46">From the discussion in this article, it is evident that if the main part of an update formula is represented with (the exponential of) linear combinations, a method similar to feature forest models should be applicable.</S>
    <S sid="526" ssid="47">Before the advent of feature forest models, studies on probabilistic models of HPSG adopted conventional maximum entropy models to select the most probable parse from parse candidates given by HPSG grammars (Oepen, Toutanova, et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003).</S>
    <S sid="527" ssid="48">The difference between these studies and our work is that we used feature forests to avoid the exponential increase in the number of structures that results from unpacked parse results.</S>
    <S sid="528" ssid="49">These studies ignored the problem of exponential explosion; in fact, training sets in these studies were very small and consisted only of short sentences.</S>
    <S sid="529" ssid="50">A possible approach to avoid this problem is to develop a fully restrictive grammar that never causes an exponential explosion, although the development of such a grammar requires considerable effort and it cannot be acquired from treebanks using existing approaches.</S>
    <S sid="530" ssid="51">We think that exponential explosion is inevitable, particularly with the large-scale wide-coverage grammars required to analyze real-world texts.</S>
    <S sid="531" ssid="52">In such cases, these methods of model estimation are intractable.</S>
    <S sid="532" ssid="53">Another approach to estimating log-linear models for HPSG was to extract a small informative sample from the original set T(w) (Osborne 2000).</S>
    <S sid="533" ssid="54">The method was successfully applied to Dutch HPSG parsing (Malouf and van Noord 2004).</S>
    <S sid="534" ssid="55">A possible problem with this method is in the approximation of exponentially many parse trees by a polynomial-size sample.</S>
    <S sid="535" ssid="56">However, their method has an advantage in that any features on parse results can be incorporated into a model, whereas our method forces feature functions to be defined locally on conjunctive nodes.</S>
    <S sid="536" ssid="57">We will discuss the trade-off between the approximation solution and the locality of feature functions in Section 6.3.</S>
    <S sid="537" ssid="58">Non-probabilistic statistical classifiers have also been applied to disambiguation in HPSG parsing: voted perceptrons (Baldridge and Osborne 2003) and support vector machines (Toutanova, Markova, and Manning 2004).</S>
    <S sid="538" ssid="59">However, the problem of exponential explosion is also inevitable using their methods.</S>
    <S sid="539" ssid="60">As described in Section 6.1, an approach similar to ours may be applied, following the study of Taskar et al. (2004).</S>
    <S sid="540" ssid="61">A series of studies on parsing with LFG (Johnson et al. 1999; Riezler et al.</S>
    <S sid="541" ssid="62">2000, 2002) also proposed a maximum entropy model for probabilistic modeling of LFG parsing.</S>
    <S sid="542" ssid="63">However, similarly to the previous studies on HPSG parsing, these groups had no solution to the problem of exponential explosion of unpacked parse results.</S>
    <S sid="543" ssid="64">As discussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum entropy estimation for packed representations of LFG parses.</S>
    <S sid="544" ssid="65">Recent studies on CCG have proposed probabilistic models of dependency structures or predicate&#8211;argument dependencies, which are essentially the same as the predicate&#8211;argument structures described in the present article.</S>
    <S sid="545" ssid="66">Clark, Hockenmaier, and Steedman (2002) attempted the modeling of dependency structures, but the model was inconsistent because of the violation of the independence assumption.</S>
    <S sid="546" ssid="67">Hockenmaier (2003) proposed a consistent generative model of predicate&#8211;argument structures.</S>
    <S sid="547" ssid="68">The probability of a non-local dependency was conditioned on multiple words to preserve the consistency of the probability model; that is, probability p(Ilwant, dispute) in Section 4.3 was directly estimated.</S>
    <S sid="548" ssid="69">The problem was that such probabilities could not be estimated directly from the data due to data sparseness, and a heuristic method had to be employed.</S>
    <S sid="549" ssid="70">Probabilities were therefore estimated as the average of individual probabilities conditioned on a single word.</S>
    <S sid="550" ssid="71">Another problem is that the model is no longer consistent when unification constraints such as those in HPSG are introduced.</S>
    <S sid="551" ssid="72">Our solution is free of these problems, and is applicable to various grammars, not only HPSG and CCG.</S>
    <S sid="552" ssid="73">Most of the state-of-the-art studies on parsing with lexicalized grammars have adopted feature forest models (Clark and Curran 2003, 2004b; Kaplan et al. 2004; Riezler and Vasserman 2004).</S>
    <S sid="553" ssid="74">Their methods of translating parse results into feature forests are basically the same as our method described in Section 4, and details differ because different grammar theories represent syntactic structures differently.</S>
    <S sid="554" ssid="75">They reported higher accuracy in parsing the Penn Treebank than the previous methods introduced herein, and these results attest the effectiveness of feature forest models in practical deep parsing.</S>
    <S sid="555" ssid="76">A remaining problem is that no studies could provide empirical comparisons across grammar theories.</S>
    <S sid="556" ssid="77">The above studies and our research evaluated parsing accuracy on their own test sets.</S>
    <S sid="557" ssid="78">The construction of theory-independent standard test sets requires enormous effort because we must establish theory-independent criteria such as agreed definitions of phrases and headedness.</S>
    <S sid="558" ssid="79">Although this issue is beyond the scope of the present article, it is a fundamental obstacle to the transparency of these studies on parsing.</S>
    <S sid="559" ssid="80">Clark and Curran (2004a) described a method for reducing the cost of parsing a training treebank without sacrificing accuracy in the context of CCG parsing.</S>
    <S sid="560" ssid="81">They first assigned each word a small number of supertags, corresponding to lexical entries in our case, and parsed supertagged sentences.</S>
    <S sid="561" ssid="82">Because they did not use the probabilities of supertags in a parsing stage, their method corresponds to our &#8220;filtering only&#8221; method.</S>
    <S sid="562" ssid="83">The difference from our approach is that they also applied the supertagger in a parsing stage.</S>
    <S sid="563" ssid="84">We suppose that this was crucial for high accuracy in their approach, although empirical investigation is necessary.</S>
    <S sid="564" ssid="85">The proposed algorithm is an essential solution to the problem of estimating probabilistic models on exponentially many complete structures.</S>
    <S sid="565" ssid="86">However, the applicability of this algorithm relies on the constraint that features are defined locally in conjunctive nodes.</S>
    <S sid="566" ssid="87">As discussed in Section 6.1, this does not necessarily mean that features in our model can represent only the immediate-dominance relations of CFG rules, because conjunctive nodes may encode any fragments of complete structures.</S>
    <S sid="567" ssid="88">In fact, we demonstrated in Section 4.3 that certain assumptions allowed us to encode non-local predicate&#8211; argument dependencies in tractable-size feature forests.</S>
    <S sid="568" ssid="89">In addition, although in the experiments we used only features on bilexical dependencies, the method described in Section 4.3 allows us to define any features on a predicate and all of its arguments, such as a ternary relation among a subject, a verb, and a complement (e.g., the relation among I, want, and dispute1 in Figure 21), and a generalized relation among semantic classes of a predicate and its arguments.</S>
    <S sid="569" ssid="90">This is because a predicate and all of its arguments are included in a conjunctive node, and feature functions can represent any relations expressed within a conjunctive node.</S>
    <S sid="570" ssid="91">When we define more global features, such as co-occurrences of structures at distant places in a sentence, conjunctive nodes must be expanded so that they include all structures that are necessary to define these features.</S>
    <S sid="571" ssid="92">However, this obviously increases the number of conjunctive nodes, and consequently, the cost of parameter estimation increases.</S>
    <S sid="572" ssid="93">In an extreme case, for example, if we define features on any co-occurrences of partial parse trees, the full unpacking of parse forests would be necessary, and parameter estimation would be intractable.</S>
    <S sid="573" ssid="94">This indicates that there is a trade-off between the locality of features and the cost of estimation.</S>
    <S sid="574" ssid="95">That is, larger context features might contribute to higher accuracy, while they inflate the size of feature forests and increase the cost of parameter estimation.</S>
    <S sid="575" ssid="96">Sampling techniques (Rosenfeld 1997; Chen and Rosenfeld 1999b; Osborne 2000; Malouf and van Noord 2004) allow us to define any features on complete structures without any constraints.</S>
    <S sid="576" ssid="97">However, they force us to employ approximation methods for tractable computation.</S>
    <S sid="577" ssid="98">The effectiveness of those techniques therefore relies on convergence speed and approximation errors, which may vary depending on the characteristics of target problems and features.</S>
    <S sid="578" ssid="99">It is an open research question whether dynamic programming or sampling can deliver a better balance of estimation efficiency and accuracy.</S>
    <S sid="579" ssid="100">The answer will differ in different problems.</S>
    <S sid="580" ssid="101">When most effective features can be represented locally in tractablesize feature forests, dynamic programming methods including ours are suitable.</S>
    <S sid="581" ssid="102">However, when global context features are indispensable for high accuracy, sampling methods might be better.</S>
    <S sid="582" ssid="103">We should also investigate compromise solutions such as dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson 2005).</S>
    <S sid="583" ssid="104">There is no analytical way of predicting the best solution, and it must be investigated experimentally for each target task.</S>
  </SECTION>
  <SECTION title="7." number="8">
    <S sid="584" ssid="1">A dynamic programming algorithm was presented for maximum entropy modeling and shown to provide a solution to the parameter estimation of probabilistic models of complete structures without the independence assumption.</S>
    <S sid="585" ssid="2">We first defined the notion of a feature forest, which is a packed representation of an exponential number of trees of features.</S>
    <S sid="586" ssid="3">When training data is represented with feature forests, model parameters are estimated at a tractable cost without unpacking the forests.</S>
    <S sid="587" ssid="4">The method provides a more flexible modeling scheme than previous methods of application of maximum entropy models to natural language processing.</S>
    <S sid="588" ssid="5">Furthermore, it is applicable to complex data structures where an event is difficult to decompose into independent sub-events.</S>
    <S sid="589" ssid="6">We also demonstrated that feature forest models are applicable to probabilistic modeling of linguistic structures such as the syntactic structures of HPSG and predicate&#8211; argument structures including non-local dependencies.</S>
    <S sid="590" ssid="7">The presented approach can be regarded as a general solution to the probabilistic modeling of syntactic analysis with lexicalized grammars.</S>
    <S sid="591" ssid="8">Table 16 summarizes the best performance of the HPSG parser described in this article.</S>
    <S sid="592" ssid="9">The parser demonstrated impressively high coverage and accuracy for real-world texts.</S>
    <S sid="593" ssid="10">We therefore conclude that the HPSG parser for English is moving toward a practical level of use in real-world applications.</S>
    <S sid="594" ssid="11">Recently, the applicability of the HPSG parser to practical applications, such as information extraction and retrieval, has also been demonstrated (Miyao et al. 2006; Yakushiji et al.</S>
    <S sid="595" ssid="12">2006; Chun 2007).</S>
    <S sid="596" ssid="13">From our extensive investigation of HPSG parsing, we observed that exploration of new types of features is indispensable to further improvement of parsing accuracy.</S>
    <S sid="597" ssid="14">A possible research direction is to encode larger contexts of parse trees, which has been shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova, and Manning 2004).</S>
    <S sid="598" ssid="15">Future work includes not only the investigation of these features but also the abstraction of predicate&#8211;argument dependencies using semantic classes.</S>
    <S sid="599" ssid="16">Experimental results also suggest that an improvement in grammar coverage is crucial for higher accuracy.</S>
    <S sid="600" ssid="17">This indicates that an improvement in the quality of the grammar is a key factor for the improvement of parsing accuracy.</S>
    <S sid="601" ssid="18">The feature forest model provides new insight into the relationship between a linguistic structure and a unit of probability.</S>
    <S sid="602" ssid="19">Traditionally, a unit of probability was implicitly assumed to correspond to a meaningful linguistic structure; a tagging of a word or an application of a rewriting rule.</S>
    <S sid="603" ssid="20">One reason for the assumption is to enable dynamic programming algorithms, such as the Viterbi algorithm.</S>
    <S sid="604" ssid="21">The probability of a complete structure must be decomposed into atomic structures in which ambiguities are limited to a tractable size.</S>
    <S sid="605" ssid="22">Another reason is to estimate plausible probabilities.</S>
    <S sid="606" ssid="23">Because a probability is defined over atomic structures, they should also be meaningful so as to be assigned a probability.</S>
    <S sid="607" ssid="24">In feature forest models, however, conjunctive nodes are responsible for the former, whereas feature functions are responsible for the latter.</S>
    <S sid="608" ssid="25">Although feature functions must be defined locally in conjunctive nodes, they are not necessarily equivalent.</S>
    <S sid="609" ssid="26">Conjunctive nodes may represent any fragments of a complete structure, which are not necessarily linguistically meaningful.</S>
    <S sid="610" ssid="27">They should be designed to pack ambiguities and enable us to define useful features.</S>
    <S sid="611" ssid="28">Meanwhile, feature functions indicate an atomic unit of probability, and are designed to capture statistical regularity of the target problem.</S>
    <S sid="612" ssid="29">We expect the separation of a unit of probability from linguistic structures to open up a new framework for flexible probabilistic modeling.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="9">
    <S sid="613" ssid="1">The authors wish to thank the anonymous reviewers of Computational Linguistics for their helpful comments and discussions.</S>
    <S sid="614" ssid="2">We would also like to thank Takashi Ninomiya and Kenji Sagae for their precious support.</S>
  </SECTION>
</PAPER>
