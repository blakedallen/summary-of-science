<PAPER>
  <S sid="0">Generation Of Word Graphs In Statistical Machine Translation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">graphs: An efficient interface between continous speech recognition and language understanding.</S>
    <S sid="2" ssid="2">International Conference on Acoustics, and Signal Processing, 2, pages 119-122, Minneapolis, MN, April.</S>
    <S sid="3" ssid="3">Stefan Ortmanns, Hermann Ney, and Xavier Aubert.</S>
    <S sid="4" ssid="4">1997.</S>
    <S sid="5" ssid="5">A word graph algorithm for large vocabcontinuous speech recognition. and Language, January.</S>
    <S sid="6" ssid="6">Christoph Tillmann and Hermann Ney.</S>
    <S sid="7" ssid="7">2000.</S>
    <S sid="8" ssid="8">Word re-ordering and DP-based search in statistical matranslation.</S>
    <S sid="9" ssid="9">In '00: The 18th Int.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="10" ssid="1">whose probability is below this value multiplied with a threshold (lower than one) will not be regarded for further expansion.</S>
    <S sid="11" ssid="2">Histogram pruning means that all but the M best hypotheses are pruned for a fixed M. For finding the most likely partial hypotheses, first all hypotheses with the same set of covered source sentence positions are compared.</S>
    <S sid="12" ssid="3">After threshold and histogram pruning have been applied, we also compare all hypotheses with the same number of covered source sentence positions and apply both pruning types again.</S>
    <S sid="13" ssid="4">Those hypotheses that survive the pruning are called the active hypotheses.</S>
    <S sid="14" ssid="5">The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.</S>
  </SECTION>
  <SECTION title="3 Word Graphs" number="2">
    <S sid="15" ssid="1">It is widely accepted in the community that a significant improvement in translation quality will come from more sophisticated translation and language models.</S>
    <S sid="16" ssid="2">For example, a language model that goes beyond m-gram dependencies could be used, but this would be difficult to integrate into the search process.</S>
    <S sid="17" ssid="3">As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse.</S>
    <S sid="18" ssid="4">We can then apply rescoring with a refined model to those hypotheses.</S>
    <S sid="19" ssid="5">One efficient way to store the different alternatives is a word graph.</S>
    <S sid="20" ssid="6">Word graphs have been successfully applied in speech recognition, for the search process (Ortmanns et al., 1997) and as an interface to other systems (Oerder and Ney, 1993).</S>
    <S sid="21" ssid="7">(Knight and Hatzivassiloglou, 1995) and (Langkilde and Knight, 1998) propose the use of word graphs for natural language generation.</S>
    <S sid="22" ssid="8">In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.</S>
    <S sid="23" ssid="9">During search, we keep a bookkeeping tree.</S>
    <S sid="24" ssid="10">It is not necessary to keep all the information that we need for the expansion of hypotheses during search in this structure, thus we store only the following: After the search has finished, i.e. when all source sentence positions have been translated, we trace back the best sentence in the bookkeeping tree.</S>
    <S sid="25" ssid="11">To generate the N best hypotheses after search, it is not sufficient to simply trace back the complete hypotheses with the highest probabilities in the bookkeeping, because those hypotheses have been recombined.</S>
    <S sid="26" ssid="12">Thus, many hypotheses with a high probability have not been stored.</S>
    <S sid="27" ssid="13">To overcome this problem, we enhance the bookkeeping concept and generate a word graph as described in Section 3.3.</S>
    <S sid="28" ssid="14">If we want to generate a word graph, we have to store both alternatives in the bookkeeping when two hypotheses are recombined.</S>
    <S sid="29" ssid="15">Thus, an entry in the bookkeeping structure may have several backpointers to different preceding entries.</S>
    <S sid="30" ssid="16">The bookkeeping structure is no longer a tree but a network where the source is the bookkeeping entry with zero covered source sentence positions and the sink is a node accounting for complete hypotheses (see Figure 3).</S>
    <S sid="31" ssid="17">This leads us to the concept of word graph nodes and edges containing the following information: &#8212; the probabilities according to the different models: the language model and the translation submodels, &#8212; the backpointer to the preceding bookkeeping entry.</S>
    <S sid="32" ssid="18">After the pruning in beam search, all hypotheses that are no longer active do not have to be kept in the bookkeeping structure.</S>
    <S sid="33" ssid="19">Thus, we can perform garbage collection and remove all those bookkeeping entries that cannot be reached from the backpointers of the active hypotheses.</S>
    <S sid="34" ssid="20">This reduces the size of the bookkeeping structure significantly.</S>
    <S sid="35" ssid="21">An example of a word graph can be seen in Figure 3.</S>
    <S sid="36" ssid="22">To keep the presentation simple, we chose an example without reordering of sentence positions.</S>
    <S sid="37" ssid="23">The words on the edges are the produced target words, and the bitvectors in the nodes show the covered source sentence positions.</S>
    <S sid="38" ssid="24">If an edge is labeled with two words, this means that the first English word has no equivalence in the source sentence, like 'just' and 'have' in Figure 3.</S>
    <S sid="39" ssid="25">The reference translation 'what did you say ?' is contained in the graph, but it has a slightly lower probability than the sentence 'what do you say ?</S>
    <S sid="40" ssid="26">', which is then chosen by the single best search.</S>
    <S sid="41" ssid="27">The recombination of hypotheses can be seen in the nodes with two or more incoming edges: those hypotheses have been recombined, because they were indistinguishable by translation and language model state.</S>
  </SECTION>
  <SECTION title="4 Pruning and Rescoring" number="3">
    <S sid="42" ssid="1">To study the effect of the word graph size on the translation quality, we produce a conservatively large word graph.</S>
    <S sid="43" ssid="2">Then we apply word graph pruning with a threshold t &lt; 1 and study the change of graph error rate (see Section 5).</S>
    <S sid="44" ssid="3">The pruning is based on the beam search concept also used in the single best search: we determine the probability of the best sentence hypothesis in the word graph.</S>
    <S sid="45" ssid="4">All hypotheses in the graph which probability is lower than this maximum probability multiplied with the pruning threshold are discarded.</S>
    <S sid="46" ssid="5">If the pruning threshold t is zero, the word graph is not pruned at all, and if t = 1, we retain only the sentence with maximum probability.</S>
    <S sid="47" ssid="6">In single best search, a standard trigram language model is used.</S>
    <S sid="48" ssid="7">Search with a bigram language model is much faster, but it yields a lower translation quality.</S>
    <S sid="49" ssid="8">Therefore, we apply a twopass approach as it was widely used in speech recognition in the past (Ortmanns et al., 1997).</S>
    <S sid="50" ssid="9">This method combines both advantages in the following way: a word graph is constructed using a bigram language model and is then rescored with a trigram language model.</S>
    <S sid="51" ssid="10">The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).</S>
    <S sid="52" ssid="11">The results of the comparison of the one-pass and the two-pass search are given in Section 5.</S>
    <S sid="53" ssid="12">We use A* search for finding the N best sentences in a word graph: starting in the root of the graph, we successively expand the sentence hypotheses.</S>
    <S sid="54" ssid="13">The probability of the partial hypothesis is obtained by multiplying the probabilities of the edges expanded for this sentence.</S>
    <S sid="55" ssid="14">As rest cost estimation, we use the probabilities determined in a backward pass as follows: for each node in the graph, we calculate the probability of a best path from this node to the goal node, i.e. the highest probability for completing a partial hypothesis.</S>
    <S sid="56" ssid="15">This rest cost estimation is perfect because it takes the exact probability as heuristic, i.e. the probability of the partial hypothesis multiplied with the rest cost estimation yields the actual probability of the complete hypothesis.</S>
    <S sid="57" ssid="16">Thus, the N best hypothesis are extracted from the graph without additional overhead of finding sentences with a lower probability.</S>
    <S sid="58" ssid="17">Of course, the hypotheses must not be recombined during this search.</S>
    <S sid="59" ssid="18">We have to keep every partial hypothesis in the priority queue in order to determine the N best sentences.</S>
    <S sid="60" ssid="19">Otherwise, we might lose one of them by recombination.</S>
    <S sid="61" ssid="20">The graph error rate is computed by determining that sentence in the word graph that has the minimum Levenstein distance to a given reference.</S>
    <S sid="62" ssid="21">Thus, it is a lower bound for the word error rate and gives a measurement of what can be achieved by rescoring with more complex models.</S>
    <S sid="63" ssid="22">The calculation of the graph error rate is performed by a dynamic programming based algorithm.</S>
    <S sid="64" ssid="23">Its space complexity is the number of graph nodes times the length of the reference translation.</S>
    <S sid="65" ssid="24">In our experiments, we varied the word graph pruning threshold in order to obtain word graphs of different densities, i.e. different numbers of hypotheses.</S>
    <S sid="66" ssid="25">The word graph density is computed as the total number of word graph edges divided by the number of reference sentence words &#8212; analogously to the word graph density in speech recognition.</S>
    <S sid="67" ssid="26">The effect of pruning on the graph error rate is shown in Table 3.</S>
    <S sid="68" ssid="27">The value of the pruning threshold is given as the negative logarithm of the probability.</S>
    <S sid="69" ssid="28">Thus, t = 0 refers to pruning everything but the best hypothesis.</S>
    <S sid="70" ssid="29">Figure 4 shows the change in graph error rate in relation to the average graph density.</S>
    <S sid="71" ssid="30">We see that for graph densities up to 200, the graph error rate significantly changes if the graph is enlarged.</S>
    <S sid="72" ssid="31">The saturation point of the GER lies at 13% and is reached for an average graph density about 1000 which relates to a pruning threshold of 20.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="4">
    <S sid="73" ssid="1">We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm.</S>
    <S sid="74" ssid="2">Experiments have shown that the graph error rate significantly decreases for rising word graph densities.</S>
    <S sid="75" ssid="3">The quality of the hypotheses contained in a word graph is better than of those in an N-best list.</S>
    <S sid="76" ssid="4">This indicates that word graph rescoring can yield a significant gain in translation quality.</S>
    <S sid="77" ssid="5">For the future, we plan the application of refined translation and language models for rescoring on word graphs.</S>
  </SECTION>
</PAPER>
