<PAPER>
  <S sid="0">Finding Terminology Translations From Non-Parallel Corpora</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups.</S>
    <S sid="2" ssid="2">Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures.</S>
    <S sid="3" ssid="3">Word Relation Matrices are then mapped across the corpora to find translation pairs.</S>
    <S sid="4" ssid="4">Translation accuracies are around 30% when only the top candidate is counted.</S>
    <S sid="5" ssid="5">Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Despite a surge in research using parallel corpora for various machine translation tasks (Brown et al. 1993),(Brown et al.</S>
    <S sid="7" ssid="2">1991; Gale &amp; Church 1993; Church 1993; Dagan &amp; Church 1994; Simard et al. 1992; Chen 1993; Melamed 1995; Wu &amp; Xia 1994; Wu 1994; Smadja et al.</S>
    <S sid="8" ssid="3">1996), the amount of available bilingual parallel corpora is still relatively small in comparison to the large amount of available monolingual text.</S>
    <S sid="9" ssid="4">It is unlikely that one can find parallel corpora in any given domain in electronic form.</S>
    <S sid="10" ssid="5">This is a particularly acute problem in language pairs such as Chinese/English or Japanese/English where there are fewer translated texts than in European language pairs.</S>
    <S sid="11" ssid="6">While we should make use of any existing parallel corpora as lexical translation resources, we should not ignore the even larger amount of monolingual text.</S>
    <S sid="12" ssid="7">However, using non-parallel corpora for lexical translation has been a daunting task, considered much more difficult than that with parallel corpora.</S>
    <S sid="13" ssid="8">In this paper, we present an initial algorithm for translating technical terms using a pair of non-parallel corpora.</S>
    <S sid="14" ssid="9">Evaluation results show translation precisions at around 30% when only the top candidate is considered.</S>
    <S sid="15" ssid="10">While this precision is lower than that achieved with parallel corpora, we show that top 20 candidate output from our algorithm allows translators to increase their accuracy by 50.9%.</S>
    <S sid="16" ssid="11">In the following sections, we first describe a pair of non-parallel corpora we use for experiments, and then we introduce the Word Relation Matrix (WoRM), a statistical word feature representation for technical term translation from non-parallel corpora.</S>
    <S sid="17" ssid="12">We evaluate the effectiveness of this feature with two sets of experiments, using English/English, and English/Japanese non-parallel corpora.</S>
  </SECTION>
  <SECTION title="2 Related work" number="2">
    <S sid="18" ssid="1">Few attempts have been made to explore non-parallel corpora of monolingual texts in the same domain.</S>
    <S sid="19" ssid="2">Early work uses a pair of non-parallel texts for the task of lexical disambiguation between several senses of a word (Dagan 1990).</S>
    <S sid="20" ssid="3">This basic idea extends to choosing a translation among multiple candidates (Dagan &amp; Itai 1994) given collocation information.</S>
    <S sid="21" ssid="4">A similar idea is later applied by (Rapp 1995) to show the plausibility of correlations between words in non-parallel text.</S>
    <S sid="22" ssid="5">He proposed a matrix permutation method matching co-occurrence patterns in two non-parallel texts, but noted that computational limitations hamper further extension of this method.</S>
    <S sid="23" ssid="6">Using the same idea, (Tanaka &amp; Iwasaki 1996) demonstrated how to eliminate candidate words in a bilingual dictionary.</S>
    <S sid="24" ssid="7">All the above works point to a certain discriminatory feature in monolingual texts &#8212; context and word relations.</S>
    <S sid="25" ssid="8">However, these works remain in the realm of solving ambiguities or choosing the best candidate among a small set of possibilities.</S>
    <S sid="26" ssid="9">It is argued in (Gale &amp; Church 1994) that feature vectors of 100,000 dimensions are likely to be needed for high resolution discriminant analysis.</S>
    <S sid="27" ssid="10">It is so far questionable whether feature vectors of lower dimensions are discriminating enough for extracting bilingual lexical pairs from nonparallel corpora with a large number of candidates.</S>
    <S sid="28" ssid="11">Is it possible to achieve bilingual lexicon translation by looking at words in relation to other words?</S>
    <S sid="29" ssid="12">In this paper, we hope to shed some light on this question.</S>
  </SECTION>
  <SECTION title="3 Two pilot non-parallel corpora" number="3">
    <S sid="30" ssid="1">In our experiments, we use two sets of non-parallel corpora: (1) Wall Street Journal (WSJ) from 1993 and 1994, divided into two non-overlapping parts.</S>
    <S sid="31" ssid="2">Each resulting English corpus has 10.36M bytes of data.</S>
    <S sid="32" ssid="3">(2) Wall Street Journal in English and Nikkei Financial News in Japanese, from the same time period.</S>
    <S sid="33" ssid="4">The WSJ text contains 49M bytes of data, and the Nikkei 127M bytes.</S>
    <S sid="34" ssid="5">Since the Nikkei is encoded in two-byte Japanese character sets, the latter is equivalent to about 60M bytes of data in English.</S>
    <S sid="35" ssid="6">The English Wall Street Journal non-parallel corpus gives us an easier test set on which to start.</S>
    <S sid="36" ssid="7">The output of this corpus should consist of words matching to themselves as translations.</S>
    <S sid="37" ssid="8">It is useful as a baseline evaluation test set providing an estimate on performance.</S>
    <S sid="38" ssid="9">The WSJ/Nikkei corpus is the most non-parallel type of corpus.</S>
    <S sid="39" ssid="10">In addition to being written in languages across linguistic families by different journalists, WSJ/Nikkei also share only a limited amount of common topic.</S>
    <S sid="40" ssid="11">The Wall Street Journal tends to focus on 'U.S. domestic economic and political news, whereas the Nikkei Financial News focuses on economic and political events in Japan and in Asia.</S>
    <S sid="41" ssid="12">Due to the large difference in content, language, writing style, we consider this corpus more difficult than others.</S>
    <S sid="42" ssid="13">However, the result we obtain from this corpus gives us a lower-bound on the performance of our algorithm.</S>
  </SECTION>
  <SECTION title="4 An algorithm for finding terminology translations from non-parallel corpora" number="4">
    <S sid="43" ssid="1">Bilingual lexicon translation algorithms for parallel corpora in general make use of fixed correlations between a pair of bilingual terms, reflected in their frequent co-occurrences in translated texts, to find lexicon translations.</S>
    <S sid="44" ssid="2">We use correlations both between monolingual lexical units, and between bilingual or multilingual lexical units, to find a consistent pattern which is represented as statistical word features for translation.</S>
    <S sid="45" ssid="3">We illustrate the possible correlations using the word debentures in the two different parts of WSJ.</S>
    <S sid="46" ssid="4">Figure 1 shows segments from both texts containing the word debentures.</S>
    <S sid="47" ssid="5">Universal said its 15 3/4% debentures due Dec sold $75 million of 6% debentures priced at par and due Sept sold $40 million of 6 1/4% convertible debentures priced at par and due March 15 GTE offered a $250 million issue of 8 1/2% debentures due in 30 years $250 million of notes due 1997 and $250 million of debentures due 2017 sold $300 million of 7 1/2% convertible debentures due 2012 at par said it agreed to issue $125 million Canadian in convertible debentures senior subordinated debentures was offered through Drexel said it completed the redemption of all $16 million of its 9% subordinated debentures due 2003 Moody's assigned a Baa-3 rating to a proposed $100 million convertible subordinated debenture it and its 12 1/2% senior subordinated debentures at par $20 million of convertible debentures due June 1 issues of $110 million of senior notes due 1997 and $115 million of convertible debentures due said it reached an agreement with holders of $30 million of its convertible subordinated debentui downgraded the subordinated debentures of Bank of Montreal common shares and $35 million of convertible debentures due 2012 $35 million of convertible debentures due May 15 financed with $450 million of new Western Union senior secured debentures to be placed by Dre Commission to issue as much as $125 million of 30-year debentures packaged with common stoc to redeem its entire $55 million face amount of 8 3/4% convertible subordinated debentures dul Figure 1 shows that: We use online dictionaries to provide the it seed word lists.</S>
    <S sid="48" ssid="6">To avoid problems of polysemy and nonstandardization in dictionary entries, we choose a more reliable, less ambiguous subset of dictionary entries as the seed word list.</S>
    <S sid="49" ssid="7">This subset contains dictionary entries which occur at midrange frequency in the corpus so that they are more likely to be content words.</S>
    <S sid="50" ssid="8">They must occur in both sides of the non-parallel corpora, and have fewer number of candidate translations.</S>
    <S sid="51" ssid="9">Such seed words serve as the textual anchor points in non-parallel corpora.</S>
    <S sid="52" ssid="10">For example, we obtained 1,416 entries from the Japanese/English online dictionary EDICT using these criteria.</S>
  </SECTION>
  <SECTION title="5 A word in relation to seed words" number="5">
    <S sid="53" ssid="1">Word correlations are important statistical information which has been successfully employed to find bilingual word pairs from parallel corpora.</S>
    <S sid="54" ssid="2">Word correlations W(w, , wt) are computed from general likelihood scores based on the co-occurrence of words in common segments.</S>
    <S sid="55" ssid="3">Segments are either sentences, paragraphs, or string groups delimited by anchor points: where a = number of segments where both words occur number of segments where only w3 occur c = number of segments where only wt occur d = number of segments where neither words occur All correlation measures use the above likelihood scores in different formulations.</S>
    <S sid="56" ssid="4">In our Word Relation Matrix (WoRM) representation, we use the correlation measure W(w, , tat) between a seed word tv, and an unknown word tv,&#8222; a, b, c and d are computed from the segments in the monolingual text of the non-parallel corpus.</S>
    <S sid="57" ssid="5">W(wz, w5) is the weighted mutual information in our algorithm since it is most suitable for lexicon compilation of mid-frequency technical words or terms: As an initial step, all Pr(w, = 1) are pre-computed for the seed words in both languages.</S>
    <S sid="58" ssid="6">We have experimented with various segment sizes, ranging from phrases delimited by all punctuations, a sentence, to an entire paragraph.</S>
    <S sid="59" ssid="7">From our experiment results, we conclude that the right segment size is a function of the frequency of the seed words: segment size oc frequency(W,) If the seed words are frequent, and if the segment size is as large as a paragraph size, then these frequent seed words could occur in every single segment.</S>
    <S sid="60" ssid="8">In this case, the chances for co-occurrence between such seed words and all new words are very high, close to one.</S>
    <S sid="61" ssid="9">With large segments, such seed words are too biasing and thus, smaller segment size must be used.</S>
    <S sid="62" ssid="10">Conversely, we need a larger segment size if seed word frequency is low.</S>
    <S sid="63" ssid="11">Consequently, we use the paragraph as the segment size for our experiment on Wall Street Journal/Nikkei Corpus since all the seed words are mid-frequency content words.</S>
    <S sid="64" ssid="12">We computed all binary vectors of the 1,416 seed words w, where the i-th dimension of the vector is 1 if the seed word occurs in the i-th paragraph in the text, zero otherwise.</S>
    <S sid="65" ssid="13">We use a smaller segment size &#8212; between any two punctuations &#8212; for the segment size for the Wall Street Journal English/English corpus since many of the seed words are frequent.</S>
    <S sid="66" ssid="14">Next, Pr(wz = 1) is computed for all unknown words z in both texts.</S>
    <S sid="67" ssid="15">The WoRM vectors are then sorted according to W(wz, w,i).</S>
    <S sid="68" ssid="16">The most correlated seed word w&#8222;.</S>
    <S sid="69" ssid="17">; will have the top scoring W(wz, As an example, using 307 seed word pairs in the WSJ/WSJ corpus, we obtain the following most correlated seed words with debentures in two different years of Wall Street Journal as shown in Figure 2.</S>
    <S sid="70" ssid="18">In both texts, the same set of words correlate with debenture closely.</S>
    <S sid="71" ssid="19">WoRM plots of debentures and administration are shown in Figures 3 and 4 respectively.</S>
    <S sid="72" ssid="20">The horizontal axis has 307 points representing the seed words, the vertical axis has the value of the correlation scores between these 307 seed words and our example words.</S>
    <S sid="73" ssid="21">These figures show that WoRMs of the same words are similar to each other, but WoRMs are different between different words.</S>
  </SECTION>
  <SECTION title="6 Matching Word Relation Matrices" number="6">
    <S sid="74" ssid="1">When all unknown words are represented in WoRMs, a matching function is needed to find the best WoRM pairs as bilingual lexicon entries.</S>
    <S sid="75" ssid="2">There are many metrics we can use to measure the closeness of two WoRMs.</S>
    <S sid="76" ssid="3">When matching vectors are very similar such as those in the WSJ English/English corpus, a simple metric like the Euclidean Distance could be used to find those matching pairs: However, most word pairs in truly non-parallel bilingual corpus are less similar than those in Figure 3.</S>
    <S sid="77" ssid="4">The y value of a new word is high when there is a x-th seed word which co-occurs with it significantly often.</S>
    <S sid="78" ssid="5">If a pair of bilingual words are supposed to be translations of each other, they should share the most significant y values.</S>
    <S sid="79" ssid="6">In this case, the Cosine Measure would be more appropriate where: CEl&lt;i&lt;n(W.s, &amp;quot; = VEW82.</S>
    <S sid="80" ssid="7">- Ewq The Cosine Measure will give the highest value to vector pairs which share the most non-zero y values.</S>
    <S sid="81" ssid="8">Therefore, it favors word pairs which share the most number of closely related seed words.</S>
    <S sid="82" ssid="9">However, the Cosine Measure is also directly proportional to another parameter, namely the actual (w,, x wt,) values.</S>
    <S sid="83" ssid="10">Consequently, if w, has a high y value everywhere, then the Cosine Measure between any tot and this w, would be high.</S>
    <S sid="84" ssid="11">This violates our assumptions in that although w, and wt might not correlate closely with the same set of seed words, the matching score would be nevertheless high.</S>
    <S sid="85" ssid="12">This is another supporting reason for choosing mid-frequency content words as seed words.</S>
  </SECTION>
  <SECTION title="7 Evaluation 1: Matching English words to English" number="7">
    <S sid="86" ssid="1">The evaluation on the WSJ/WSJ English/English corpus is intended as a pilot test on the discriminative power of the Word Relation Matrix.</S>
    <S sid="87" ssid="2">This non-parallel corpus has minimal content and style differences.</S>
    <S sid="88" ssid="3">Furthermore, using such an English/English test set, the output can be evaluated automatically&#8212;a translated pair is considered correct if they are identical English words.</S>
    <S sid="89" ssid="4">307 seed words are chosen according to their occurrence frequency (400-3900) to minimize the number of function words.</S>
    <S sid="90" ssid="5">However, a frequency of 3900 in a corpus of 1.5M words is quite high.</S>
    <S sid="91" ssid="6">As a result, a segment delimited by two punctuations is used as the context window size.</S>
    <S sid="92" ssid="7">Furthermore, the frequent nature of the seed words led to our choice of the Euclidean Distance, instead of the Cosine Measure.</S>
    <S sid="93" ssid="8">The choices of segment size, seed words, and Euclidean Distance measure are all direct consequences of the atypical nature of the English/English pilot test set.</S>
    <S sid="94" ssid="9">We selected a test set of 582 (set A) by 687 (set B) single words with mid-range frequency from the WSJ texts.</S>
    <S sid="95" ssid="10">We computed the WoRM feature for each of these test words and computed the Euclidean Distance between every word in these sets.</S>
    <S sid="96" ssid="11">We then calculated the accuracy by counting the number of words whose top one candidate is identical to itself, obtaining a precision of 29%.</S>
    <S sid="97" ssid="12">By allowing N-top candidates, the accuracy improves as shown in the graphs for 582 words output in Figure 5 (i.e. a translation is correct if it appears among the first N candidates).</S>
    <S sid="98" ssid="13">If we find the correct translation among the top 100 candidates, we obtain a precision of around 58%.</S>
    <S sid="99" ssid="14">N-top candidates are useful as translator aids.</S>
    <S sid="100" ssid="15">Meanwhile, precisions for translating less polysemous content words are higher.</S>
    <S sid="101" ssid="16">If only the 445 content words (manually selected) are kept from the 582-word set, the precisions at different top N candidates for the 445-word set are higher as shown in Figure 5 by the dotted line.</S>
    <S sid="102" ssid="17">We believe the accuracy would be even higher if we only look at really unambiguous test words, such as an entire technical term.</S>
    <S sid="103" ssid="18">It is well known that polysemous words usually have only one sense when used as part of a collocation or technical term (Yarowsky 1993).</S>
  </SECTION>
  <SECTION title="8 Evaluation 2: Matching Japanese terms to English" number="8">
    <S sid="104" ssid="1">Evaluations are also carried out on the Wall Street Journal and Nikkei Financial News corpus, matching technical terms in Japanese to their counterpart in English.</S>
    <S sid="105" ssid="2">This evaluation is a difficult test case because (1) the two languages, English and Japanese, are across language groups; (2) the two texts, Wall Street Journal and Nikkei Financial News, do not focus on the same topics; and (3) the two texts are not written by the same authors.</S>
    <S sid="106" ssid="3">1,416 entries from the Japanese/English online dictionary EDICT with occurrence frequencies between 100 and 1000 are chosen as seed words.</S>
    <S sid="107" ssid="4">Since these seed words have relatively low frequencies compared to the corpus size of around 7 million words for the WSJ text, we chose the segment size to be that of an entire paragraph.</S>
    <S sid="108" ssid="5">For the same reason, the Cosine Measure is chosen as a matching function.</S>
    <S sid="109" ssid="6">For evaluation, we need to select a test set of known technical term translations.</S>
    <S sid="110" ssid="7">We handtranslated a selected set of technical terms from the Nikkei Financial News corpus and looked them up in the Wall Street Journal text.</S>
    <S sid="111" ssid="8">Among these, 19 terms, shown in Figure 6, have their counterparts in the WSJ text.</S>
    <S sid="112" ssid="9">Three evaluations were carried out.</S>
    <S sid="113" ssid="10">In all cases, a translation is counted as correct if the top candidate is the right one.</S>
    <S sid="114" ssid="11">Test I tries to find the correct translation for each of the nineteen Japanese terms among the nineteen English terms.</S>
    <S sid="115" ssid="12">To increase the candidate numbers, test II is carried out on 19 Japanese terms with their English counterparts plus 293 other English terms, giving a, total of 312 possible English candidates.</S>
    <S sid="116" ssid="13">The third test set III consists of the nineteen Japanese terms paired with their translations and 383 single English words in addition.</S>
    <S sid="117" ssid="14">The accuracies for the three test sets are shown in Figure 7; precision ranges from 21.1% to 52.6%.</S>
    <S sid="118" ssid="15">Figure 8 shows the ranking of the true translations among all the candidates for all 19 cases for the purpose of a translator-aid.</S>
    <S sid="119" ssid="16">Most of the correct translations can be found among the top 20 candidates.</S>
    <S sid="120" ssid="17">The previous two evaluations show that the precision of best-candidate translation using our algorithm is around 30% on average.</S>
    <S sid="121" ssid="18">While it is far from ideal, this is the first result of terminology translation from non-parallel corpora.</S>
    <S sid="122" ssid="19">Meanwhile, we have found that the correct translation is often among the top 20 candidates.</S>
    <S sid="123" ssid="20">This leads us to conjecture that the output from this algorithm can be used as a translator-aid.</S>
    <S sid="124" ssid="21">To evaluate this, we again chose the nineteen English/Japanese terms from the WSJ/Nikkei non-parallel corpus as a test set.</S>
    <S sid="125" ssid="22">We chose three evaluators who are all native Chinese speakers with bilingual knowledge in English and Chinese.</S>
    <S sid="126" ssid="23">Chinese speakers are able to recognize most Japanese technical terms since they are very similar to Chinese.</S>
    <S sid="127" ssid="24">We asked them to translate these nineteen Japanese terms into English without using dictionaries or other reference material.</S>
    <S sid="128" ssid="25">The translators have some general knowledge of international news.</S>
    <S sid="129" ssid="26">However, none of them specializes in economics or finance, which is the domain of the WSJ/Nikkei corpus.</S>
    <S sid="130" ssid="27">Their output is in SET A.</S>
    <S sid="131" ssid="28">Our system then proposes two sets of outputs: (1) for each Japanese term, our system proposes the top-20 candidates from the set of 312 noun phrases.</S>
    <S sid="132" ssid="29">Using this candidate list, the translators again translate the nineteen terms.</S>
    <S sid="133" ssid="30">Their output based on this information is in SETH; (2) for each Japanese term, our system proposes the top-20 candidates from the set containing 383 single words plus the nineteen terms.</S>
    <S sid="134" ssid="31">The result of human translation based on this candidate list is in SET C. Sets A, B and C are all compared to the original translation in the corpus.</S>
    <S sid="135" ssid="32">If the translation is the same as in the corpus, then it is judged as correct.</S>
    <S sid="136" ssid="33">The results are shown in Figure 9.</S>
    <S sid="137" ssid="34">Evaluators on average are able to translate 8 terms out of 19 by themselves, whereas they can translate 18 terms on average with the aid of our output.</S>
    <S sid="138" ssid="35">Translation precision increases on the average by 50.9%.</S>
  </SECTION>
  <SECTION title="9 Conclusion" number="9">
    <S sid="139" ssid="1">We have described a statistical word signature feature, the Word Relation Matrix, that can be used to find matching pairs of content words or terms in a pair of same-domain non-parallel bilingual texts.</S>
    <S sid="140" ssid="2">Evaluation shows a precision of about 30%.</S>
    <S sid="141" ssid="3">We showed that humans are able to translate more than twice as many Japanese technical terms into English when our system output is used, compared to translating a random set of 19 Japanese terms without aid.</S>
    <S sid="142" ssid="4">It is also a significant initial result for lexical translation from truly non-parallel corpora, particularly across language groups.</S>
    <S sid="143" ssid="5">For future work, the quality of seed words can be improved by using a training algorithm to select seed words according to their discriminative power.</S>
    <S sid="144" ssid="6">The dimensionality of WoRM vectors we have chosen is not optimal.</S>
    <S sid="145" ssid="7">A high dimensionality of vectors is usually favorable (Gale &amp; Church 1994).</S>
    <S sid="146" ssid="8">On the other hand, high dimensionality can also lead to noise, Therefore, dimensionality reduction methods such as the Singular Value Decomposition (Shiitze 1992) or clustering is often used.</S>
    <S sid="147" ssid="9">In our case, this means that we should choose a large subset of highly discriminative seed word pairs.</S>
    <S sid="148" ssid="10">Additionally, the Word Relation Matrix could be used in combination with other word signature features for non-parallel corpora.</S>
    <S sid="149" ssid="11">In addition to the evaluation results, we have also discovered that the content words in the same segment with a word or term all contribute to the occurrence of this word.</S>
    <S sid="150" ssid="12">This feature represents some of the long-distance relations between the word and multiple other words which are not its immediate neighbors.</S>
    <S sid="151" ssid="13">The information can be used in language modeling in addition to the currently popular N-gram models and word trigger pairs.</S>
  </SECTION>
</PAPER>
