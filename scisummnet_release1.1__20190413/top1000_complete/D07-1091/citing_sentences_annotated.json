[
  {
    "citance_No": 1, 
    "citing_paper_id": "P07-2045", 
    "citing_paper_authority": 746, 
    "citing_paper_authors": "Philipp, Koehn | Hieu, Hoang | Alexandra, Birch | Chris, Callison-Burch | Marcello, Federico | Nicola, Bertoldi | Brooke, Cowan | Wade, Shen | Christine, Moran | Richard, Zens | Chris, Dyer | Ond&#x159;ej, Bojar | Alexandra, Constantin | Evan, Herbst", 
    "raw_text": "Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007)", 
    "clean_text": "Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1010", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mohammad, Salameh | Colin, Cherry | Grzegorz, Kondrak", 
    "raw_text": "Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation", 
    "clean_text": "Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P10-1047", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Reyyan, Yeniterzi | Kemal, Oflazer", 
    "raw_text": "We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007)", 
    "clean_text": "We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W08-0305", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Marco, Turchi | Tijl, De Bie | Nello, Cristianini", 
    "raw_text": "Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007) .Obviously, it is always possible that the identification of radically different representations of language might introduce totally different constraints on both approximation and estimation error, and this might be worth considering", 
    "clean_text": "Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P11-1130", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Preslav, Nakov | Hwee Tou, Ng", 
    "raw_text": "Frameworks for the simultaneous use of different word-level representations have been pro posed as well (Koehn and Hoang, 2007)", 
    "clean_text": "Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W11-2129", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Sara, Stymne | Nicola, Cancedda", 
    "raw_text": "We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the 254 EU-Sv Auto-Sv Auto-Da Corpus Europarl Automotive Automotive Languages English? Swedish English? Swedish English? Danish Compounds split N, V, Adj N, V, Adj N POS tag-sets POS POS, RPOS RPOS Decoder Moses in-house in-house Training sentences SMT 1,520,549 329,090 168,047 Training words SMT (target) 34,282,247 3,061,282 1,553,382 Training sentences CRF 248,808 317,398 164,702 Extra training sentences CRF 3,000 3,000 163,201 Table 2: Overview of the experimental settings target side, with a sequence model on part-of speech", 
    "clean_text": "We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W09-0418", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Michael, Paul | Andrew, Finch | Eiichiro, Sumita", 
    "raw_text": "A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are trans lated separately, and this information is combined on the output side to generate the translation", 
    "clean_text": "A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W11-2126", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Philip, Williams | Philipp, Koehn", 
    "raw_text": "Koehn and Hoang (2007 )generalise the phrase-based model? s representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search", 
    "clean_text": "Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W10-1720", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Sergio, Penkale | Rejwanul, Haque | Sandipan, Dandapat | Pratyush, Banerjee | Ankit Kumar, Srivastava | Jinhua, Du | Pavel, Pecina | Sudip Kumar, Naskar | Mikel L., Forcada | Andy, Way", 
    "raw_text": "Factored models (KoehnandHoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002)", 
    "clean_text": "Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W09-0427", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Marine, Carpuat", 
    "raw_text": "Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are 151news-dev2009a representation OOV% METEOR BLEU NIST baseline surface form only 2.24 49.05 20.45 6.135 decoding lemma back off 2.13 49.12 20.44 6.143 word alignment lemma+POS for all 2.24 48.87 20.36 6.145lemma+POS for adj 2.25 48.94 20.46 6.131lemma+POS for verbs 2.21 49.05 20.47 6.137 decoding+ alignment back off+ all 2.10 48.97 20.36 6.147 back off+ adj 2.12 49.05 20.48 6.140 back off+ verbs 2.08 49.15 20.50 6.148news-dev2009b representation OOV% METEOR BLEU NIST baseline surface form only 2.52 49.60 21.10 6.211 decoding lemma back off 2.43 49.66 21.02 6.210 word alignment lemma+POS for all 2.53 49.56 21.03 6.199lemma+POS for adj 2.52 49.74 21.00 6.213lemma+POS for verbs 2.47 49.73 21.10 6.217decoding+alignment back off+ all 2.44 49.59 20.92 6.194 back off+ adj 2.43 49.80 21.03 6.217 back off+ verbs 2.39 49.80 21.03 6.217 Table 2: Evaluation of the decoding back off strategy, the modified word alignment strategy and their combination Input Me? me s ?ilde ?missionnait, la situation ne changerait pas", 
    "clean_text": "Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P12-1016", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Spence, Green | John, DeNero", 
    "raw_text": "In contrast, our class-based model does not require any manual rules and scores similar agreement phenomena as probabilistic sequences. Factored Translation Models Factoredtranslation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling", 
    "clean_text": "Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N09-2019", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Adri&agrave; de, Gispert | Sami, Virpioja | Mikko, Kurimo | William, Byrne", 
    "raw_text": "Factored models are introduced in (Koehnand Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding", 
    "clean_text": "Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W10-1604", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Helena M., Caseli | Bruno Akio, Sugiyama | Junia Coutinho, Anacleto", 
    "raw_text": "This is already being illustrated by the recent shift of researches towards linguistically enriched models as (Koehn and Hoang, 2007) and (Tinsley and Way, 2009) among others", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P13-2065", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Zhiyang, Wang | Yajuan, L&uuml; | Meng, Sun | Qun, Liu", 
    "raw_text": "Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007)", 
    "clean_text": "Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "D11-1080", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Christof, Monz", 
    "raw_text": "While they rarely seem to hurt translation quality, the redoes not seem to be a clear consensus that they significantly improve quality either. Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data", 
    "clean_text": "Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D10-1015", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Minh-Thang, Luong | Preslav, Nakov | Min-Yen, Kan", 
    "raw_text": "While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity ,e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. Note that scoring with twin LMs is conceptually superior to n-best re-scoring with a word-token LM ,e.g., (Oflazer and El-Kahlout, 2007), since it is tightly integrated into decoding: it scores partial hypotheses and influenced the search process directly", 
    "clean_text": "While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W12-0116", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Rui, Wang | Petya, Osenova | Kiril, Simov", 
    "raw_text": "Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework", 
    "clean_text": "Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D11-1034", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Gennadi, Lembersky | Noam, Ordan | Shuly, Wintner", 
    "raw_text": "factored translation model (Koehn and Hoang, 2007)", 
    "clean_text": "We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W12-3157", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Joern, Wuebker | Hermann, Ney", 
    "raw_text": "Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction", 
    "clean_text": "Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W11-2122", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Abby, Levenberg | Miles, Osborne | David, Matthews", 
    "raw_text": "We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder", 
    "clean_text": "We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder.", 
    "keep_for_gold": 0
  }
]
