<PAPER>
  <S sid="0">Feature-Rich Part-Of-Speech Tagging With A Cyclic Dependency Network</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features.</S>
    <S sid="2" ssid="2">Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="3" ssid="1">Almost all approaches to sequence problems such as partof-speech tagging take a unidirectional approach to conditioning inference along the sequence.</S>
    <S sid="4" ssid="2">Regardless of whether one is using HMMs, maximum entropy conditional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence (normally left to right, but occasionally right to left, e.g., Church (1988)).</S>
    <S sid="5" ssid="3">There are a few exceptions, such as Brill&#8217;s transformation-based learning (Brill, 1995), but most of the best known and most successful approaches of recent years have been unidirectional.</S>
    <S sid="6" ssid="4">Most sequence models can be seen as chaining together the scores or decisions from successive local models to form a global model for an entire sequence.</S>
    <S sid="7" ssid="5">Clearly the identity of a tag is correlated with both past and future tags&#8217; identities.</S>
    <S sid="8" ssid="6">However, in the unidirectional (causal) case, only one direction of influence is explicitly considered at each local point.</S>
    <S sid="9" ssid="7">For example, in a left-to-right first-order HMM, the current tag t0 is predicted based on the previous tag t_1 (and the current word).1 The backward interaction between t0 and the next tag t+1 shows up implicitly later, when t+1 is generated in turn.</S>
    <S sid="10" ssid="8">While unidirectional models are therefore able to capture both directions of influence, there are good reasons for suspecting that it would be advantageous to make information from both directions explicitly available for conditioning at each local point in the model: (i) because of smoothing and interactions with other modeled features, terms like P(t0|t+1, ...) might give a sharp estimate of t0 even when terms like P(t+1|t0, ...) do not, and (ii) jointly considering the left and right context together might be especially revealing.</S>
    <S sid="11" ssid="9">In this paper we exploit this idea, using dependency networks, with a series of local conditional loglinear (aka maximum entropy or multiclass logistic regression) models as one way of providing efficient bidirectional inference.</S>
    <S sid="12" ssid="10">Secondly, while all taggers use lexical information, and, indeed, it is well-known that lexical probabilities are much more revealing than tag sequence probabilities (Charniak et al., 1993), most taggers make quite limited use of lexical probabilities (compared with, for example, the bilexical probabilities commonly used in current statistical parsers).</S>
    <S sid="13" ssid="11">While modern taggers may be more principled than the classic CLAWS tagger (Marshall, 1987), they are in some respects inferior in their use of lexical information: CLAWS, through its IDIOMTAG module, categorically captured many important, correct taggings of frequent idiomatic word sequences.</S>
    <S sid="14" ssid="12">In this work, we incorporate appropriate multiword feature templates so that such facts can be learned and used automatically by 1Rather than subscripting all variables with a position index, we use a hopefully clearer relative notation, where t0 denotes the current position and t_&#8222; and t+&#8222; are left and right context tags, and similarly for words. the bidirectional dependency network. the model.</S>
    <S sid="15" ssid="13">Having expressive templates leads to a large number of features, but we show that by suitable use of a prior (i.e., regularization) in the conditional loglinear model &#8211; something not used by previous maximum entropy taggers &#8211; many such features can be added with an overall positive effect on the model.</S>
    <S sid="16" ssid="14">Indeed, as for the voted perceptron of Collins (2002), we can get performance gains by reducing the support threshold for features to be included in the model.</S>
    <S sid="17" ssid="15">Combining all these ideas, together with a few additional handcrafted unknown word features, gives us a part-of-speech tagger with a per-position tag accuracy of 97.24%, and a whole-sentence correct rate of 56.34% on Penn Treebank WSJ data.</S>
    <S sid="18" ssid="16">This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous loglinear model in Toutanova and Manning (2000).</S>
  </SECTION>
  <SECTION title="2 Bidirectional Dependency Networks" number="2">
    <S sid="19" ssid="1">When building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical model (e.g., an HMM (Brants, 2000) or a conditional Markov model (CMM) (Ratnaparkhi, 1996)).</S>
    <S sid="20" ssid="2">In such models, the probability assigned to a tagged sequence of words x = ht, wi is the product of a sequence of local portions of the graphical model, one from each time slice.</S>
    <S sid="21" ssid="3">For example, in the left-to-right CMM shown in figure 1(a), That is, the replicated structure is a local model P(t0|t&#8722;1, w0).2 Of course, if there are too many conditioned quantities, these local models may have to be estimated in some sophisticated way; it is typical in tagging to populate these models with little maximum entropy models.</S>
    <S sid="22" ssid="4">For example, we might populate a model for P(t0|t&#8722;1, w0) with a maxent model of the form: In this case, the w0 and t&#8722;1 can have joint effects on t0, but there are not joint features involving all three variables (though there could have been such features).</S>
    <S sid="23" ssid="5">We say that this model uses the feature templates ht0, t&#8722;1i (previous tag features) and ht0, w0i (current word features).</S>
    <S sid="24" ssid="6">Clearly, both the preceding tag t&#8722;1 and following tag t+1 carry useful information about a current tag t0.</S>
    <S sid="25" ssid="7">Unidirectional models do not ignore this influence; in the case of a left-to-right CMM, the influence of t&#8722;1 on t0 is explicit in the P(t0|t&#8722;1, w0) local model, while the influence of t+1 on t0 is implicit in the local model at the next position (via P(t+1|t0, w+1)).</S>
    <S sid="26" ssid="8">The situation is reversed for the right-to-left CMM in figure 1(b).</S>
    <S sid="27" ssid="9">From a seat-of-the-pants machine learning perspective, when building a classifier to label the tag at a certain position, the obvious thing to do is to explicitly include in the local model all predictive features, no matter on which side of the target position they lie.</S>
    <S sid="28" ssid="10">There are two good formal reasons to expect that a model explicitly conditioning on both sides at each position, like figure 1(c) could be advantageous.</S>
    <S sid="29" ssid="11">First, because of smoothing effects and interaction with other conditioning features (like the words), left-to-right factors like P(t0|t&#8722;1, w0) do not always suffice when t0 is implicitly needed to determine t&#8722;1.</S>
    <S sid="30" ssid="12">For example, consider a case of observation bias (Klein and Manning, 2002) for a first-order left-toright CMM.</S>
    <S sid="31" ssid="13">The word to has only one tag (TO) in the PTB tag set.</S>
    <S sid="32" ssid="14">The TO tag is often preceded by nouns, but rarely by modals (MD).</S>
    <S sid="33" ssid="15">In a sequence will to fight, that trend indicates that will should be a noun rather than a modal verb.</S>
    <S sid="34" ssid="16">However, that effect is completely lost in a CMM like (a): P(twill |will, hstarti) prefers the modal tagging, and P(TO|to, twill) is roughly 1 regardless of twill.</S>
    <S sid="35" ssid="17">While the model has an arrow between the two tag positions, that path of influence is severed.3 The same problem exists in the other direction.</S>
    <S sid="36" ssid="18">If we use the symmetric rightto-left model, fight will receive its more common noun tagging by symmetric reasoning.</S>
    <S sid="37" ssid="19">However, the bidirectional model (c) discussed in the next section makes both directions available for conditioning at all locations, using replicated models of P(t0|t&#8722;1, t+1, w0), and will be able to get this example correct.4 While the structures in figure 1(a) and (b) are wellunderstood graphical models with well-known semantics, figure 1(c) is not a standard Bayes&#8217; net, precisely because the graph has cycles.</S>
    <S sid="38" ssid="20">Rather, it is a more general dependency network (Heckerman et al., 2000).</S>
    <S sid="39" ssid="21">Each node represents a random variable along with a local conditional probability model of that variable, conditioned on the source variables of all incoming arcs.</S>
    <S sid="40" ssid="22">In this sense, the semantics are the same as for standard Bayes&#8217; nets.</S>
    <S sid="41" ssid="23">However, because the graph is cyclic, the net does not correspond to a proper factorization of a large joint probability estimate into local conditional factors.</S>
    <S sid="42" ssid="24">Consider the two-node cases shown in figure 2.</S>
    <S sid="43" ssid="25">Formally, for the net in (a), we can write P(a, b) = P(a)P(b|a).</S>
    <S sid="44" ssid="26">For (b) we write P(a, b) = P(b)P(a|b).</S>
    <S sid="45" ssid="27">However, in (c), the nodes A and B carry the information P(a|b) and P(b|a) respectively.</S>
    <S sid="46" ssid="28">The chain rule doesn&#8217;t allow us to reconstruct P(a, b) by multiplying these two quantities.</S>
    <S sid="47" ssid="29">Under appropriate conditions, we could reconstruct P(a, b) from these quantities using Gibbs sampling, and, in general, that is the best we can do.</S>
    <S sid="48" ssid="30">However, while reconstructing the joint probabilities from these local conditional probabilities may be difficult, estimating the local probabilities themselves is no harder than it is for acyclic models: we take observations of the local environments and use any maximum likelihood estimation method we desire.</S>
    <S sid="49" ssid="31">In our experiments, we used local maxent models, but if the event space allowed, (smoothed) relative counts would do.</S>
    <S sid="50" ssid="32">Cyclic or not, we can view the product of local probabilities from a dependency network as a score: where Pa(xi) are the nodes with arcs to the node xi.</S>
    <S sid="51" ssid="33">In the case of an acyclic model, this score will be the joint probability of the event x, P(x).</S>
    <S sid="52" ssid="34">In the general case, it will not be.</S>
    <S sid="53" ssid="35">However, we can still ask for the event, in this case the tag sequence, with the highest score.</S>
    <S sid="54" ssid="36">For dependency networks like those in figure 1, an adaptation of the Viterbi algorithm can be used to find the maximizing sequence in polynomial time.</S>
    <S sid="55" ssid="37">Figure 3 gives pseudocode for the concrete case of the network in figure 1(d); the general case is similar, and is in fact just a max-plus version of standard inference algorithms for Bayes&#8217; nets (Cowell et al., 1999, 97).</S>
    <S sid="56" ssid="38">In essence, there is no difference between inference on this network and a second-order left-to-right CMM or HMM.</S>
    <S sid="57" ssid="39">The only difference is that, when the Markov window is at a position i, rather than receiving the score for P(ti|ti&#8722;1, ti&#8722;2, wi), one receives the score for P(ti&#8722;1|ti, ti&#8722;2, wi&#8722;1).</S>
    <S sid="58" ssid="40">There are some foundational issues worth mentioning.</S>
    <S sid="59" ssid="41">As discussed previously, the maximum scoring sequence need not be the sequence with maximum likelihood according to the model.</S>
    <S sid="60" ssid="42">There is therefore a worry with these models about a kind of &#8220;collusion&#8221; where the model locks onto conditionally consistent but jointly unlikely sequences.</S>
    <S sid="61" ssid="43">Consider the two-node network in figure 2(c).</S>
    <S sid="62" ssid="44">If we have the following distribution of observations (in the form ab) h11, 11, 11, 12, 21, 33i, then clearly the most likely state of the network is 11.</S>
    <S sid="63" ssid="45">However, the score of 11 is P(a = 1|b = 1)P(b = 1|a = 1) = 3/4 &#215; 3/4 = 9/16, while the score of 33 is 1.</S>
    <S sid="64" ssid="46">An additional related problem is that the training set loss (sum of negative logarithms of the sequence scores) does not bound the training set error (0/1 loss on sequences) from above.</S>
    <S sid="65" ssid="47">Consider the following training set, for the same network, with each entire data point considered as a label: (11, 22).</S>
    <S sid="66" ssid="48">The relative-frequency model assigns loss 0 to both training examples, but cannot do better than 50% error in regenerating the training data labels.</S>
    <S sid="67" ssid="49">These issues are further discussed in Heckerman et al. (2000).</S>
    <S sid="68" ssid="50">Preliminary work of ours suggests that practical use of dependency networks is not in general immune to these theoretical concerns: a dependency network can choose a sequence model that is bidirectionally very consistent but does not match the data very well.</S>
    <S sid="69" ssid="51">However, this problem does not appear to have prevented the networks from performing well on the tagging problem, probably because features linking tags and observations are generally much sharper discriminators than tag sequence features.</S>
    <S sid="70" ssid="52">It is useful to contrast this framework with the conditional random fields of Lafferty et al. (2001).</S>
    <S sid="71" ssid="53">The CRF approach uses similar local features, but rather than chaining together local models, they construct a single, globally normalized model.</S>
    <S sid="72" ssid="54">The principal advantage of the dependency network approach is that advantageous bidirectional effects can be obtained without the extremely expensive global training required for CRFs.</S>
    <S sid="73" ssid="55">To summarize, we draw a dependency network in which each node has as neighbors all the other nodes that we would like to have influence it directly.</S>
    <S sid="74" ssid="56">Each node&#8217;s neighborhood is then considered in isolation and a local model is trained to maximize the conditional likelihood over the training data of that node.</S>
    <S sid="75" ssid="57">At test time, the sequence with the highest product of local conditional scores is calculated and returned.</S>
    <S sid="76" ssid="58">We can always find the exact maximizing sequence, but only in the case of an acyclic net is it guaranteed to be the maximum likelihood sequence.</S>
  </SECTION>
  <SECTION title="3 Experiments" number="3">
    <S sid="77" ssid="1">The part of speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III (Marcus et al., 1994).</S>
    <S sid="78" ssid="2">We extracted tagged sentences from the parse trees.5 We split the data into training, development, and test sets as in (Collins, 2002).</S>
    <S sid="79" ssid="3">Table 1 lists characteristics of the three splits.6 Except where indicated for the model BEST, all results are on the development set.</S>
    <S sid="80" ssid="4">One innovation in our reporting of results is that we present whole-sentence accuracy numbers as well as the traditional per-tag accuracy measure (over all tokens, even unambiguous ones).</S>
    <S sid="81" ssid="5">This is the quantity that most sequence models attempt to maximize (and has been motivated over doing per-state optimization as being more useful for subsequent linguistic processing: one wants to find a coherent sentence interpretation).</S>
    <S sid="82" ssid="6">Further, while some tag errors matter much more than others, to a first cut getting a single tag wrong in many of the more common ways (e.g., proper noun vs. common noun; noun vs. verb) would lead to errors in a subsequent processor such as an information extraction system or a parser that would greatly degrade results for the entire sentence.</S>
    <S sid="83" ssid="7">Finally, the fact that the measure has much more dynamic range has some appeal when reporting tagging results.</S>
    <S sid="84" ssid="8">The per-state models in this paper are log-linear models, building upon the models in (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000), though some models are in fact strictly simpler.</S>
    <S sid="85" ssid="9">The features in the models are defined using templates; there are different templates for rare words aimed at learning the correct tags for unknown words.7 We present the results of three classes of experiments: experiments with directionality, experiments with lexicalization, and experiments with smoothing.</S>
    <S sid="86" ssid="10">In this section, we report experiments using log-linear CMMs to populate nets with various structures, exploring the relative value of neighboring words&#8217; tags.</S>
    <S sid="87" ssid="11">Table 2 lists the discussed networks.</S>
    <S sid="88" ssid="12">All networks have the same vertical feature templates: (t0, w0) features for known words and various (t0, Q(w1n)) word signature features for all words, known or not, including spelling and capitalization features (see section 3.3).</S>
    <S sid="89" ssid="13">Just this vertical conditioning gives an accuracy of 93.69% (denoted as &#8220;Baseline&#8221; in table 2).8 Condition6Tagger results are only comparable when tested not only on the same data and tag set, but with the same amount of training data.</S>
    <S sid="90" ssid="14">Brants (2000) illustrates very clearly how tagging performance increases as training set size grows, largely because the percentage of unknown words decreases while system performance on them increases (they become increasingly restricted as to word class).</S>
    <S sid="91" ssid="15">7Except where otherwise stated, a count cutoff of 2 was used for common word features and 35 for rare word features (templates need a support set strictly greater in size than the cutoff before they are included in the model). ing on the previous tag as well (model L, ht0, t&#8722;1i features) gives 95.79%.</S>
    <S sid="92" ssid="16">The reverse, model R, using the next tag instead, is slightly inferior at 95.14%.</S>
    <S sid="93" ssid="17">Model L+R, using both tags simultaneously (but with only the individual-direction features) gives a much better accuracy of 96.57%.</S>
    <S sid="94" ssid="18">Since this model has roughly twice as many tag-tag features, the fact that it outperforms the unidirectional models is not by itself compelling evidence for using bidirectional networks.</S>
    <S sid="95" ssid="19">However, it also outperforms model L+L2 which adds the ht0, t&#8722;2i secondprevious word features instead of next word features, which gives only 96.05% (and R+R2 gives 95.25%).</S>
    <S sid="96" ssid="20">We conclude that, if one wishes to condition on two neighboring nodes (using two sets of 2-tag features), the symmetric bidirectional model is superior.</S>
    <S sid="97" ssid="21">High-performance taggers typically also include joint three-tag counts in some way, either as tag trigrams (Brants, 2000) or tag-triple features (Ratnaparkhi, 1996, Toutanova and Manning, 2000).</S>
    <S sid="98" ssid="22">Models LL, RR, and CR use only the vertical features and a single set of tag-triple features: the left tags (t&#8722;2, t&#8722;1 and t0), right tags (t0, t+1, t+2), or centered tags (t&#8722;1, t0, t+1) respectively.</S>
    <S sid="99" ssid="23">Again, with roughly equivalent feature sets, the left context is better than the right, and the centered context is better than either unidirectional context. line for this task high, while substantial annotator noise creates an unknown upper bound on the task.</S>
    <S sid="100" ssid="24">Lexicalization has been a key factor in the advance of statistical parsing models, but has been less exploited for tagging.</S>
    <S sid="101" ssid="25">Words surrounding the current word have been occasionally used in taggers, such as (Ratnaparkhi, 1996), Brill&#8217;s transformation based tagger (Brill, 1995), and the HMM model of Lee et al. (2000), but nevertheless, the only lexicalization consistently included in tagging models is the dependence of the part of speech tag of a word on the word itself.</S>
    <S sid="102" ssid="26">In maximum entropy models, joint features which look at surrounding words and their tags, as well as joint features of the current word and surrounding words are in principle straightforward additions, but have not been incorporated into previous models.</S>
    <S sid="103" ssid="27">We have found these features to be very useful.</S>
    <S sid="104" ssid="28">We explore here lexicalization both alone and in combination with preceding and following tag histories.</S>
    <S sid="105" ssid="29">Table 3 shows the development set accuracy of several models with various lexical features.</S>
    <S sid="106" ssid="30">All models use the same rare word features as the models in Table 2.</S>
    <S sid="107" ssid="31">The first two rows show a baseline model using the current word only.</S>
    <S sid="108" ssid="32">The count cutoff for this feature was 0 in the first model and 2 for the model in the second row.</S>
    <S sid="109" ssid="33">As there are no tag sequence features in these models, the accuracy drops significantly if a higher cutoff is used (from a per tag accuracy of about 93.7% to only 60.2%).</S>
    <S sid="110" ssid="34">The third row shows a model where a tag is decided solely by the three words centered at the tag position (3W).</S>
    <S sid="111" ssid="35">As far as we are aware, models of this sort have not been explored previously, but its accuracy is surprisingly high: despite having no sequence model at all, it is more accurate than a model which uses standard tag fourgram HMM features ((t0, w0), (t0, t&#8722;1), (t0, t&#8722;1, t&#8722;2), (t0, t&#8722;1, t&#8722;2, t&#8722;3), shown in Table 2, model L+LL+LLL).</S>
    <S sid="112" ssid="36">The fourth and fifth rows show models with bidirectional tagging features.</S>
    <S sid="113" ssid="37">The fourth model (3W+TAGS) uses the same tag sequence features as the last model in Table 2 ((t0, t&#8722;1), (t0, t&#8722;1, t&#8722;2), (t0, t&#8722;1, t+1), (t0, t+1), (t0, t+1, t+2)) and current, previous, and next word.</S>
    <S sid="114" ssid="38">The last model has in addition the feature templates (t0, w0, t&#8722;1), (t0, w0, t+1), (t0, w&#8722;1, w0), and (t0, w0, w+1), and includes the improvements in unknown word modeling discussed in section 3.3.9 We call this model BEST.</S>
    <S sid="115" ssid="39">BEST has a token accuracy on the final test set of 97.24% and a sentence accuracy of 56.34% (see Table 4).</S>
    <S sid="116" ssid="40">A 95% confidence interval for the accuracy (using a binomial model) is (97.15%,97.33%).</S>
    <S sid="117" ssid="41">In order to understand the gains from using right context tags and more lexicalization, let us look at an example of an error that the enriched models learn not to make.</S>
    <S sid="118" ssid="42">An interesting example of a common tagging error of the simpler models which could be corrected by a deterministic fixup rule of the kind used in the IDIOMTAG module of (Marshall, 1987) is the expression as X as (often, as far as).</S>
    <S sid="119" ssid="43">This should be tagged as/RB X/{RB,JJ} as/IN in the Penn Treebank.</S>
    <S sid="120" ssid="44">A model using only current word and two left tags (model L+L2 in Table 2), made 87 errors on this expression, tagging it as/INX as/IN &#8211; since the tag sequence probabilities do not give strong reasons to disprefer the most common tagging of as (it is tagged as IN over 80% of the time).</S>
    <S sid="121" ssid="45">However, the model 3W+TAGS, which uses two right tags and the two surrounding words in addition, made only 8 errors of this kind, and model BEST made only 6 errors.</S>
    <S sid="122" ssid="46">Most of the models presented here use a set of unknown word features basically inherited from (Ratnaparkhi, 1996), which include using character n-gram prefixes and suffixes (for n up to 4), and detectors for a few other prominent features of words, such as capitalization, hyphens, and numbers.</S>
    <S sid="123" ssid="47">Doing error analysis on unknown words on a simple tagging model (with (t0, t&#8722;1), (t0, t&#8722;1, t&#8722;2), and (w0, t0) features) suggested several additional specialized features that can usefully improve performance.</S>
    <S sid="124" ssid="48">By far the most significant is a crude company name detector which marks capitalized words followed within 3 words by a company name suffix like Co. or Inc.</S>
    <S sid="125" ssid="49">This suggests that further gains could be made by incorporating a good named entity recognizer as a preprocessor to the tagger (reversing the most common order of processing in pipelined systems!</S>
    <S sid="126" ssid="50">), and is a good example of something that can only be done when using a conditional model.</S>
    <S sid="127" ssid="51">Minor gains come from a few additional features: an allcaps feature, and a conjunction feature of words that are capitalized and have a digit and a dash in them (such words are normally common nouns, such as CFC-12 or F/A-18).</S>
    <S sid="128" ssid="52">We also found it advantageous to use prefixes and suffixes of length up to 10.</S>
    <S sid="129" ssid="53">Together with the larger templates, these features contribute to our unknown word accuracies being higher than those of previously reported taggers.</S>
    <S sid="130" ssid="54">With so many features in the model, overtraining is a distinct possibility when using pure maximum likelihood estimation.</S>
    <S sid="131" ssid="55">We avoid this by using a Gaussian prior (aka quadratic regularization or quadratic penalization) which resists high feature weights unless they produce great score gain.</S>
    <S sid="132" ssid="56">The regularized objective F is: Since we use a conjugate-gradientprocedure to maximize the data likelihood, the addition of a penalty term is easily incorporated.</S>
    <S sid="133" ssid="57">Both the total size of the penalty and the partial derivatives with repsect to each &#955;j are trivial to compute; these are added to the log-likelihood and log-likelihood derivatives, and the penalized optimization procedes without further modification.</S>
    <S sid="134" ssid="58">We have not extensively experimented with the value of U2 &#8211; which can even be set differently for different parameters or parameter classes.</S>
    <S sid="135" ssid="59">All the results in this paper use a constant U2 = 0.5, so that the denominator disappears in the above expression.</S>
    <S sid="136" ssid="60">Experiments on a simple model with U made an order of magnitude higher or lower both resulted in worse performance than with U2 = 0.5.</S>
    <S sid="137" ssid="61">Our experiments show that quadratic regularization is very effective in improving the generalization performance of tagging models, mostly by increasing the number of features which could usefully be incorporated.</S>
    <S sid="138" ssid="62">The number of features used in our complex models &#8211; in the several hundreds of thousands, is extremely high in comparison with the data set size and the number of features used in other machine learning domains.</S>
    <S sid="139" ssid="63">We describe two sets of experiments aimed at comparing models with and without regularization.</S>
    <S sid="140" ssid="64">One is for a simple model with a relatively small number of features, and the other is for a model with a large number of features.</S>
    <S sid="141" ssid="65">The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in Chen and Rosenfeld (2000), and used in all the stochastic LFG work (Johnson et al., 1999).</S>
    <S sid="142" ssid="66">However, until recently, its role and importance have not been widely understood.</S>
    <S sid="143" ssid="67">For example, Zhang and Oles (2001) attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization.</S>
    <S sid="144" ssid="68">At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: Ratnaparkhi (1996), Toutanova and Manning (2000), and Collins (2002) all present unregularized models.</S>
    <S sid="145" ssid="69">Indeed, the result of Collins (2002) that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized.</S>
    <S sid="146" ssid="70">Table 5 shows results on the development set from two pairs of experiments.</S>
    <S sid="147" ssid="71">The first pair of models use common word templates 40, w0), 40, t_1, t_2) and the same rare word templates as used in the models in table 2.</S>
    <S sid="148" ssid="72">The second pair of models use the same features as model BEST with a higher frequency cutoff of 5 for common word features.</S>
    <S sid="149" ssid="73">For the first pair of models, the error reduction from smoothing is 5.3% overall and 20.1% on unknown words.</S>
    <S sid="150" ssid="74">For the second pair of models, the error reduction is even bigger: 16.2% overall after convergence and 5.8% if looking at the best accuracy achieved by the unsmoothed model (by stopping training after 75 iterations; see below).</S>
    <S sid="151" ssid="75">The especially large reduction in unknown word error reflects the fact that, because penalties are effectively stronger for rare features than frequent ones, the presence of penalties increases the degree to which more general cross-word signature features (which apply to unknown words) are used, relative to word-specific sparse features (which do not apply to unknown words).</S>
    <S sid="152" ssid="76">Secondly, use of regularization allows us to incorporate features with low support into the model while improving performance.</S>
    <S sid="153" ssid="77">Whereas Ratnaparkhi (1996) used feature support cutoffs and early stopping to stop overfitting of the model, and Collins (2002) contends that including low support features harms a maximum entropy model, our results show that low support features are useful in a regularized maximum entropy model.</S>
    <S sid="154" ssid="78">Table 6 contrasts our results with those from Collins (2002).</S>
    <S sid="155" ssid="79">Since the models are not the same, the exact numbers are incomparable, but the difference in direction is important: in the regularized model, performance improves with the inclusion of low support features.</S>
    <S sid="156" ssid="80">Finally, in addition to being significantly more accurate, smoothed models train much faster than unsmoothed ones, and do not benefit from early stopping.</S>
    <S sid="157" ssid="81">For example, the first smoothed model in Table 5 required 80 conjugate gradient iterations to converge (somewhat arbitrarily defined as a maximum difference of 10_4 in feature weights between iterations), while its corresponding unsmoothed model required 335 iterations, thus training was roughly 4 times slower.10 The second pair of models required 134 and 370 iterations respectively.</S>
    <S sid="158" ssid="82">As might be expected, unsmoothed models reach their highest generalization capacity long before convergence and accuracy on an unseen test set drops considerably with further iterations.</S>
    <S sid="159" ssid="83">This is not the case for smoothed models, as their test set accuracy increases almost monotonically with training iterations.11 Figure 4 shows a graph of training iterations versus accuracy for the second pair of models on the development set.</S>
  </SECTION>
  <SECTION title="4 Conclusion" number="4">
    <S sid="160" ssid="1">We have shown how broad feature use, when combined with appropriate model regularization, produces a superior level of tagger performance.</S>
    <S sid="161" ssid="2">While experience suggests that the final accuracy number presented here could be slightly improved upon by classifier combination, it is worth noting that not only is this tagger better than any previous single tagger, but it also appears to outperform Brill and Wu (1998), the best-known combination tagger (they report an accuracy of 97.16% over the same WSJ data, but using a larger training set, which should favor them).</S>
    <S sid="162" ssid="3">While part-of-speech tagging is now a fairly well-worn road, and our ability to win performance increases in this domain is starting to be limited by the rate of errors and inconsistencies in the Penn Treebank training data, this work also has broader implications.</S>
    <S sid="163" ssid="4">Across the many NLP problems which involve sequence models over sparse multinomial distributions, it suggests that feature-rich models with extensive lexicalization, bidirectional inference, and effective regularization will be key elements in producing state-of-the-art results.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="5">
    <S sid="164" ssid="1">This work was supported in part by the Advanced Research and Development Activity (ARDA)&#8217;s Advanced Question Answering for Intelligence (AQUAINT) Program, by the National Science Foundation under Grant No.</S>
    <S sid="165" ssid="2">IIS-0085896, and by an IBM Faculty Partnership Award.</S>
  </SECTION>
</PAPER>
