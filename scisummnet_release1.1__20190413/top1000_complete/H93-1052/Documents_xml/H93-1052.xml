<PAPER>
  <S sid="0" ssid="0">ONE SENSE PER COLLOCATION David Yarowsky* Department  of  Computer  and In format ion Science Univers i ty of  Pennsy lvania Philadelphia, PA 19104 yarowsky@unagi .c is .upenn.edu ABSTRACT Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a polysemous word has one sense per discourse.</S>
  <S sid="1" ssid="1">In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation.</S>
  <S sid="2" ssid="2">We test his empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99% accuracy for binary ambiguities.</S>
  <S sid="3" ssid="3">We utilize this property in a disambiguation algorithm that achieves precision of 92% using combined models of very local context.</S>
  <S sid="4" ssid="4">INTRODUCTION The use of collocations to resolve lexical ambiguities i cer- tainly not a new idea.</S>
  <S sid="5" ssid="5">The first approaches to sense dis- ambiguation, such as [Kelly and Stone 1975], were based on simple hand-built decision tables consisting almost ex- clusively of questions about observed word associations in specific positions.</S>
  <S sid="6" ssid="6">Later work from the AI community relied heavily upon selectional restrictions for verbs, although pri- marily in terms of features exhibited by their arguments ( uch as +DRINKABLE) rather than in terms of individual words or word classes.</S>
  <S sid="7" ssid="7">More recent work [Brown et al.</S>
  <S sid="8" ssid="8">1991][Hearst 1991] has utilized a set of discrete local questions (such as word-to-the-right) in the development of statistical decision procedures.</S>
  <S sid="9" ssid="9">However, astrong trend in recent years is to treat a reasonably wide context window as an unordered bag of in- dependent evidence points.</S>
  <S sid="10" ssid="10">This technique from information retrieval has been used in neural networks, Bayesian discrim- inators, and dictionary definition matching.</S>
  <S sid="11" ssid="11">In a comparative paper in this volume [Leacock et al.</S>
  <S sid="12" ssid="12">1993], all three methods under investigation used words in wide context as a pool of evidence independent of relative position.</S>
  <S sid="13" ssid="13">It is perhaps not a coincidence that this work has focused almost exclusively on nouns, as will be shown in Section 6.2.</S>
  <S sid="14" ssid="14">In this study we will return again to extremely local sources of evidence, and show that models of discrete syntactic relationships have considerable advantages.</S>
  <S sid="15" ssid="15">*This research was supported by an NDSEG Fellowship and by DARPA grant N00014-90-J-1863.</S>
  <S sid="16" ssid="16">The author is also affiliated with the Linguistics Research Department ofAT&amp;T Bell Laboratories, and greatly appreciates the use of its resources in support of this work.</S>
  <S sid="17" ssid="17">He would also like to thank Eric Bfill, Bill Gale, Libby Levison, Mitch Marcus and Philip Resnik for their valuable feedback.</S>
  <S sid="18" ssid="18">DEF IN IT IONS OF  SENSE The traditional definition of word sense is "One of several meanings assigned to the same orthographic string".</S>
  <S sid="19" ssid="19">As meanings can always be partitioned into multiple refinements, senses are typically organized in a tree such as one finds in a dictionary.</S>
  <S sid="20" ssid="20">In the extreme case, one could continue making refinements until a word has a slightly different sense every time it is used.</S>
  <S sid="21" ssid="21">If so, the title of this paper is a tautology.</S>
  <S sid="22" ssid="22">However, the studies in this paper are focused on the sense distinctions atthe top of the tree.</S>
  <S sid="23" ssid="23">A good working definition of the distinctions considered are those meanings which are not typically translated to the same word in a foreign language.</S>
  <S sid="24" ssid="24">Therefore, one natural type of sense distinction to consider are those words in English which indeed have multiple trans- lations in a language such as French.</S>
  <S sid="25" ssid="25">As is now standard in the field, we use the Canadian Hansards, a parallel bilingual corpus, to provide sense tags in the form of French transla- tions.</S>
  <S sid="26" ssid="26">Unfortunately, the Hansards are highly skewed in their sense distributions, and it is difficult to find words for which there are adequate numbers of a second sense.</S>
  <S sid="27" ssid="27">More diverse large bilingual corpora re not yet readily available.</S>
  <S sid="28" ssid="28">We also use data sets which have been hand-tagged bynative English speakers.</S>
  <S sid="29" ssid="29">To make the selection of sense distinc- tions more objective, we use words such as bass where the sense distinctions (fish and musical instrument) correspond to pronunciation differences ([b~es] and [beIs]).</S>
  <S sid="30" ssid="30">Such data is often problematic, as the tagging is potentially subjective and error-filled, and sufficient quantities are difficult o obtain.</S>
  <S sid="31" ssid="31">As a solution to the data shortages for the above methods, [Gale, Church and Yarowsky 1992b] proposed the use of "pseudo-words," artificial sense ambiguities created by tak- ing two English words with the same part of speech (such as guerilla and reptile), and replacing each instance of both in a corpus with a new polysemous word guerrilla~reptile.</S>
  <S sid="32" ssid="32">As it is entirely possible that the concepts guerrilla nd reptile are represented by the same orthographic string in some foreign language, choosing between these two meanings based on context is a problem a word sense disambiguation algorithm could easily face.</S>
  <S sid="33" ssid="33">"Pseudo-words" are very useful for devel- oping and testing disambiguation methods because of their nearly unlimited availability and the known, fully reliable 266 ground truth they provide when grading performance.</S>
  <S sid="34" ssid="34">Finally, we consider sense disambiguation for mediums other than clean English text.</S>
  <S sid="35" ssid="35">For example, we look at word pairs such as terse/tense and cookie/rookie which may be plausi- bly confused in optical character recognition (OCR).</S>
  <S sid="36" ssid="36">Homo- phones, such as aid~aide, and censor/sensor, are ideal can- didates for such a study because large data sets with known ground truth are available in written text, yet they are true ambiguities which must be resolved routinely in oral commu- nication.</S>
  <S sid="37" ssid="37">We discover that the central claims of this paper hold for all of these potential definitions of sense.</S>
  <S sid="38" ssid="38">This corroborating evidence makes us much more confident in our results than if they were derived solely from a relatively small hand-tagged data set.</S>
  <S sid="39" ssid="39">DEF IN IT IONS OF  COLLOCATION Collocation means the co-occurrence of two words in some defined relationship.</S>
  <S sid="40" ssid="40">We look at several such relationships, in- cluding direct adjacency and first word to the left or right hav- ing a certain part-of-speech.</S>
  <S sid="41" ssid="41">We also consider certain direct syntactic relationships, uch as verb/object, subject/verb, and adjective/noun pairs.</S>
  <S sid="42" ssid="42">It appears that content words (nouns, verbs, adjectives, and adverbs) behave quite differently from function words (other parts of speech); we make use of this distinction in several definitions of collocation.</S>
  <S sid="43" ssid="43">We will attempt to quantify the validity of the one-sense-per- collocation hypothesis for these different collocation types.</S>
  <S sid="44" ssid="44">EXPERIMENTS In the experiments, we ask two central, related questions: For each definition of sense and collocation, ?</S>
  <S sid="45" ssid="45">What is the mean entropy of the distribution Pr(Sense[Collocation)?</S>
  <S sid="46" ssid="46">What is the performance of a disambiguation algorithm which uses only that collocation type as evidence?</S>
  <S sid="47" ssid="47">We examine several permutations for each, and are interested in how the results of these questions differ when applied to polysemous nouns, verbs, and adjectives.</S>
  <S sid="48" ssid="48">To limit the already very large number of parameters consid- ered, we study only binary sense distinctions.</S>
  <S sid="49" ssid="49">In all cases the senses being compared have the same part of speech.</S>
  <S sid="50" ssid="50">The selection between different possible parts of speech as been heavily studied and is not replicated here.</S>
  <S sid="51" ssid="51">Sample Collection All samples were extracted from a 380 million word cor- pus collection consisting of newswire text (AP Newswire and ?</S>
  <S sid="52" ssid="52">Hand Tagged (homographs): bass, axes, chi, bow, colon, lead, IV, sake, tear, ... ?</S>
  <S sid="53" ssid="53">French Translation Distinctions: sentence, duty, drug, language, position, paper, single .... ?</S>
  <S sid="54" ssid="54">Homophones: aid/aide, cellar/seller, censor/sensor, cue/queue, pedal/petal .... ?</S>
  <S sid="55" ssid="55">OCR Ambiguities: terse/tense, gum/gym, deaf/dear, cookie/rookie, beverage/leverage .... ?</S>
  <S sid="56" ssid="56">Pseudo-Words: covered/waved, kissed/slapped, abused/escorted, cute/compatible .... Table 1: A sample of the words used in the experiments Wall Street Journal), scientific abstracts (from NSF and the Department ofEnergy), the Canadian Hansards parliamentary debate records, Groliers Encyclopedia, a medical encyclo- pedia, over 100 Harper &amp; Row books, and several smaller corpora including the Brown Corpus, and ATIS and TIMIT sentences.1 The homophone pairs used were randomly selected from a list of words having the same pronunciation orwhich differed in only one phoneme.</S>
  <S sid="57" ssid="57">The OCR and pseudo-word pairs were randomly selected from corpus wordlists, with the former restricted to pairs which could plausibly be confused in a noisy FAX, typically words differing in only one character.</S>
  <S sid="58" ssid="58">Due to the difficulty of obtaining new data, the hand-tagged and French translation examples were borrowed from those used in our previous tudies in sense disambiguation.</S>
  <S sid="59" ssid="59">Measuring Entropies When computing the entropy of Pr(Sense[Collocation), we enumerate all collocations of a given type observed for the word or word pair being disambiguated.</S>
  <S sid="60" ssid="60">Table 2 shows the example of the homophone ambiguity aid~aide for the collo- cation type content-word-to-the-left.</S>
  <S sid="61" ssid="61">We list all words 2 ap- pearing in such a collocation with either of these two "senses" of the homograph, and calculate the raw distributional count for each.</S>
  <S sid="62" ssid="62">Note that the vast majority of the entries in Table 2 have zero as one of the frequency counts.</S>
  <S sid="63" ssid="63">It is not acceptable, however, t Training and test samples were not only extracted from different articles or discourses but also from entirely different blocks of the corpus.</S>
  <S sid="64" ssid="64">This was done to minimize long range discourse ffects such as one finds in the AP or Hansards.</S>
  <S sid="65" ssid="65">2Note: the entries in this table are lemmas (uninflected root forms), rather than raw words.</S>
  <S sid="66" ssid="66">By treating the verbal inflections squander, squanders, squandering, and squandered asthe same word, one can improve statistics and coverage at a slight cost of lost subtlety.</S>
  <S sid="67" ssid="67">Although we will refer to "words in collocation" throughout this paper for simplicity, this should always be interpreted as "lemmas in collocation."</S>
  <S sid="68" ssid="68">267 Frequency as Frequency as Collocation Aid Aide foreign federal western provide zovert appose future ~imilar presidential :hief longtime aids-infected deepy disaffected Lndispensable ~ractical ;quander 718 297 146 88 26 13 9 6 0 0 0 0 0 0 2 2 1 1 0 0 0 0 0 0 0 63 40 26 2 1 1 1 0 0 Table 2: A typical collocational distribution for the homo- phone ambiguity aid/aide.</S>
  <S sid="69" ssid="69">to treat these as having zero probability and hence a zero entropy for the distribution.</S>
  <S sid="70" ssid="70">It is quite possible, especially for the lower frequency distributions, that we would see a contrary example in a larger sample.</S>
  <S sid="71" ssid="71">By cross-validation, we discover for the aid~aide xample that for collocations with an observed 1/0 distribution, we would actually expect he minor sense to occur 6% of the time in an independent sample, on average.</S>
  <S sid="72" ssid="72">Thus a fairer distribution would be .94/.06, giving a cross-validated ntropy of .33 bits rather than 0 bits.</S>
  <S sid="73" ssid="73">For a more unbalanced observed istribution, such as 10/0, the probability of seeing the minor sense decreases to 2%, giving a cross-validated ntropy of H(.98,.02) = .14 bits.</S>
  <S sid="74" ssid="74">Repeating this process and taking the weighted mean yields the entropy of the full distribution, in this case .09 bits for the aid/aide ambiguity.</S>
  <S sid="75" ssid="75">For each type of collocation, we also compute how well an observed probability distribution predicts the correct classifi- cation for novel examples.</S>
  <S sid="76" ssid="76">In general, this is a more useful measure for most of the comparison purposes we will address.</S>
  <S sid="77" ssid="77">Not only does it reflect he underlying entropy of the distribu- tion, but it also has the practical advantage of showing how a working system would perform given this data.</S>
  <S sid="78" ssid="78">ALGORITHM The sense disambiguation algorithm used is quite straightfor- ward.</S>
  <S sid="79" ssid="79">When based on a single collocation type, such as the object of the verb or word immediately to the left, the pro- cedure is very simple.</S>
  <S sid="80" ssid="80">One identifies if this collocation type exists for the novel context and if the specific words found are listed in the table of probability distributions (as computed above).</S>
  <S sid="81" ssid="81">If so, we return the sense which was most frequent for that collocation in the training data.</S>
  <S sid="82" ssid="82">If not, we return the sense which is most frequent overall.</S>
  <S sid="83" ssid="83">When we consider more than one collocation type and com- bine evidence, the process is more complicated.</S>
  <S sid="84" ssid="84">The algo- rithm used is based on decision lists [Rivest, 1987], and was discussed in [Sproat, Hirschberg, and Yarowsky 1992].</S>
  <S sid="85" ssid="85">The goal is to base the decision on the single best piece of evi- dence available.</S>
  <S sid="86" ssid="86">Cross-validated probabilities are computed as in Section 4.2, and the different ypes of evidence are sorted by the absolute value of the log of these probabil- ?</S>
  <S sid="87" ssid="87">P r  Sense l  Co l loca~ion i )  ratios.</S>
  <S sid="88" ssid="88">Abs(Log(prls,n,~  Conocauo,~,)))" When a novel lty context is encountered, one steps through the decision list until the evidence at that point in the list (such as word-to- /eft="presidential") matches the current context under con- sideration.</S>
  <S sid="89" ssid="89">The sense with the greatest listed probability is returned, and this cross-validated probability represents he confidence in the answer.</S>
  <S sid="90" ssid="90">This approach is well-suited for the combination of multi- ple evidence types which are clearly not independent (such as those found in this study) as probabilities are never com- bined.</S>
  <S sid="91" ssid="91">Therefore this method offers advantages over Bayesian classifier techniques which assume independence of the fea- tures used.</S>
  <S sid="92" ssid="92">It also offers advantages over decision tree based techniques because the training pools are not split at each question.</S>
  <S sid="93" ssid="93">The interesting problems are how one should re- estimate probabilities conditional on questions asked earlier in the list, or how one should prune lower evidence which is categorically subsumed by higher evidence or is entirely conditional on higher evidence.</S>
  <S sid="94" ssid="94">1989] have dis- cussed some of these issues at length, and there is not space to consider them here.</S>
  <S sid="95" ssid="95">For simplicity, in this experiment no secondary smoothing or pruning is done.</S>
  <S sid="96" ssid="96">This does not ap- pear to be problematic when small numbers of independent evidence types are used, but performance should increase if this extra step is taken.</S>
  <S sid="97" ssid="97">RESULTS AND DISCUSSION 6.1.</S>
  <S sid="98" ssid="98">One  Sense Per  Co l locat ion For the collocations tudied, it appears that the hypothesis of one sense per collocation holds with high probability for binary ambiguities.</S>
  <S sid="99" ssid="99">The experimental results in the precision column of Table 3 quantify the validity of this claim.</S>
  <S sid="100" ssid="100">Accu- racy varies from 90% to 99% for different types of collocation and part of speech, with a mean of 95%.</S>
  <S sid="101" ssid="101">The significance of these differences will be discussed in Section 6.2.</S>
  <S sid="102" ssid="102">These precision values have several interpretations.</S>
  <S sid="103" ssid="103">First, they reflect the underlying probability distributions of sense 268 Collocation Part Ent Prec Rec No No Type of Sp.</S>
  <S sid="104" ssid="104">Coil Data Content ALL .18 .97 .29 .57 .14 word to Noun .98 .25 .66 .09 immediate Verb .95 .14 .71 .15 right [A] Adj .97 .51 .27 .22 Content ALL .24 .96 .26 .58 .16 word to Noun .99 .33 .56 .11 immediate Verb .91 .23 .47 .30 left [B] Adj .96 .15 .75 .10 First ALL .33 .94 .51 .09 .40 Content Noun .94 .49 .13 .38 Word to Verb .91 .44 .05 .51 Right Adj .96 .58 .04 .38 First ALL .40 .92 .50 .06 .44 Content Noun .96 .58 .06 .36 Word to Verb .87 .37 .05 .58 Left Adj .90 .45 .06 .49 Subject ~ Noun .33 .94 .13 .87 .06 Verb Pairs Verb .43 .91 .28 .33 .38 Verb ~ Noun .46 .90 .07 .81 .07 Object Pairs Verb .29 .95 .36 .32 .32 Adj ~-+ Noun Adj .14 .98 .54 .20 .26 A&amp;BAbove ALL - .97 .47 I .31 I .21 I All Above ALL - .92 .98 .00 .02 Table 3: IncludestheentropyofthePr(SennelGollocation ) distribution for several types of collocation, and the performance achieved when basing sense disambiguation solely on that evidence.</S>
  <S sid="105" ssid="105">Results are itemized by the part of speech of the ambiguous word (not of the collocate).</S>
  <S sid="106" ssid="106">Precision (Prec.)</S>
  <S sid="107" ssid="107">indicates percent correct and Recall (Rec.)</S>
  <S sid="108" ssid="108">refers to the percentage of samples for which an answer is returned.</S>
  <S sid="109" ssid="109">Precision is measured on this subset.</S>
  <S sid="110" ssid="110">No collocation (No Coil) indicates the failure to provide an answer because no collocation of that type was present in the test context, and "No Data" indicates the failure to return an answer because no data for-the observed collocation was present in the model.</S>
  <S sid="111" ssid="111">See Section 7.3 for a discussion of the "All Above" result.</S>
  <S sid="112" ssid="112">The results tated above are based on the average of the different types of sense considered, and have a mean prior probability of .69 and a mean sample size of 3944. conditional on collocation.</S>
  <S sid="113" ssid="113">For example, for the collocation type content-word-to-the-right, t evalue of .97 indicates that on average, given a Specific collocation we will expect o see the same sense 97% of the time.</S>
  <S sid="114" ssid="114">This mean distribution is also reflected in the entropy column.</S>
  <S sid="115" ssid="115">However, these numbers have much more practical interpre- tations.</S>
  <S sid="116" ssid="116">If we actually build a disambiguation procedure using exclusively the content word to the right as information, such a system performs with 97% precision on new data where a content word appears to the right and for which there is in- formation in the model .3 This is considerably higher than the 3The correlation between these numbers is not a coincidence.</S>
  <S sid="117" ssid="117">Because the probability distributions are based oncross-validated t sts on indepen- dent data and weighted by collocation frequency, if on average we find that Per formance  Us ing  Ev idence  a t  D i f fe rent  D is tances 8 " - ~ t  Verbs Adjectives 2o ,o 8o go ,no Distance Figure 1: Comparison of the performance ofnouns, verbs and adjectives based strictly on a 5 word window centered at the distance shown on the horizontal axis.</S>
  <S sid="118" ssid="118">performance of 69% one would expect simply by chance due to the unbalanced prior probability of the two senses.</S>
  <S sid="119" ssid="119">It should be noted that such precision is achieved at only partial recall.</S>
  <S sid="120" ssid="120">The three rightmost columns of Table 3 give the breakdown of the recall.</S>
  <S sid="121" ssid="121">On average, the model content- word-to-right could only be applied in 29% of the test samples.</S>
  <S sid="122" ssid="122">In 57% of the cases, no content word appeared to the right, so this collocational model did not hold.</S>
  <S sid="123" ssid="123">In 14% of the cases, a content word did appear to the right, but no instances of that word appeared in the training data, so the model had no information on which to base a decision.</S>
  <S sid="124" ssid="124">There are several solutions to both these deficiencies, and they are discussed in Section 7.</S>
  <S sid="125" ssid="125">Part of  Speech Differences It is interesting to note the difference in behavior between different parts of speech.</S>
  <S sid="126" ssid="126">Verbs, for example, derive more disambiguating information from their objects (.95) than from their subjects (.90).</S>
  <S sid="127" ssid="127">Adjectives derive almost all of their disambiguatinginformation fr m the nouns they modify (.98).</S>
  <S sid="128" ssid="128">Nouns are best disambiguated bydirectly adjacent adjectives or nouns, with the content word to the left indicating a single sense with 99% precision.</S>
  <S sid="129" ssid="129">Verbs appear to be less useful for noun sense disambiguation, although they are relatively better indicators when the noun is their object rather than their subject.</S>
  <S sid="130" ssid="130">97% of samples of a given collocation exhibit he same sense, this is the expected precision of a disambiguafion algorithm which assumes one sense per collocation, when applied to new samples of these collocations.</S>
  <S sid="131" ssid="131">269 Figure ] shows that nouns, verbs and adjectives also differ in their ability to be disambiguated by wider context.</S>
  <S sid="132" ssid="132">1993] previously showed that nouns can be disambiguated based strictly on distant context, and that useful information was present up to 10,000 words away.</S>
  <S sid="133" ssid="133">We replicated an exper- iment in which performance was calculated for disambigua- tions based strictly on 5 word windows centered at various distances (shown on the horizontal axis).</S>
  <S sid="134" ssid="134">Gales observation was tested only on nouns; our experiment also shows that reasonably accurate decisions may be made for nouns using exclusively remote context.</S>
  <S sid="135" ssid="135">Our results in this case are based on test sets with equal numbers of the two senses.</S>
  <S sid="136" ssid="136">Hence chance performance is at 50%.</S>
  <S sid="137" ssid="137">However, when tested on verbs and adjectives, precision drops off with a much steeper slope as the distance from the ambiguous word increases.</S>
  <S sid="138" ssid="138">This would indicate that approaches giving equal weight o all po- sitions in a broad window of context may be less well-suited for handling verbs and adjectives.</S>
  <S sid="139" ssid="139">Models which give greater weight o immediate context would seem more appropriate in these circumstances.</S>
  <S sid="140" ssid="140">A similar experiment was applied to function words, and the dropoff beyond strictly immediate context was precipitous, converging at near chance performance for distances greater than 5.</S>
  <S sid="141" ssid="141">However, function words did appear to have pre- dictive power of roughly 5% greater than chance in directly adjacent positions.</S>
  <S sid="142" ssid="142">The effect was greatest for verbs, where the function word to the right (typically a preposition or par- ticle) served to disambiguate at a precision of 13% above chance.</S>
  <S sid="143" ssid="143">This would indicate that methods which exclude function words from models to minimize noise should con- sider their inclusion, but only for restricted local positions.</S>
  <S sid="144" ssid="144">Comparison of Sense Definitions Results for the 5 different definitions of sense ambiguity stud- ied here are similar.</S>
  <S sid="145" ssid="145">However they tend to fluctuate relative to each other across experiments, and there appears to be no consistent ordering of the mean entropy of the different types of sense distributions.</S>
  <S sid="146" ssid="146">Because of the very large num- ber of permutations considered, it is not possible to give a full breakdown of the differences, and such a breakdown does not appear to be terribly informative.</S>
  <S sid="147" ssid="147">The important observa- tion, however, is that the basic conclusions drawn from this paper hold for each of the sense definitions considered, and hence corroborate and strengthen the conclusions which can be drawn from any one.</S>
  <S sid="148" ssid="148">Performance Given Little Evidence One of the most striking conclusions to emerge from this study is that for the local collocations considered, decisions based on a single data point are highly reliable.</S>
  <S sid="149" ssid="149">Normally one would consider a 1/0 sense distribution i  a 3944 sample training set to be noise, with performance based on this information ot Low Counts  a re  Re l iab le ; ~ ,o ~o ,;o o;o ,go., Training Frequency (f) Figure 2: Percentage correct for disambiguations based solely on a single content-word-to-the-rightcollocation seen ft imes in the training data without counter-examples.</S>
  <S sid="150" ssid="150">likely to much exceed the 69% prior probability expected by chance.</S>
  <S sid="151" ssid="151">But this is not what we observe.</S>
  <S sid="152" ssid="152">For example, when tested on the word-to-the-right collocation, disambiguations based solely on a single data point exceed 92% accuracy, and performance on 2/0 and 3/0 distributions climb rapidly from there, and reach nearly perfect accuracy for training samples as small as 15/0, as shown in Figure 2.</S>
  <S sid="153" ssid="153">In contrast, acollocation 30 words away which also exhibits a 1/0 sense distribution has a predictive value of only 3% greater than chance.</S>
  <S sid="154" ssid="154">This difference in the reliability of low frequency data from local and wide context will have implications for algorithm design.</S>
  <S sid="155" ssid="155">APPL ICAT IONS 7.1.</S>
  <S sid="156" ssid="156">Training Set Creation and Verification This last observation has relevance for new data set creation and correction.</S>
  <S sid="157" ssid="157">Collocations with an ambiguous content word which have frequency greater than 10-15 and which do not belong exclusively to one sense should be flagged for human reinspection, as they are most likely in error.</S>
  <S sid="158" ssid="158">One can speed the sense tagging process by computing the most frequent col- locates, and for each one assigning all examples to the same sense.</S>
  <S sid="159" ssid="159">For the data in Table 2, this will apparently fail for the foreignAid/Aide example in 1 out of 719 instances ( till 99.9% correct).</S>
  <S sid="160" ssid="160">However, in this example the models classification was actually correct; the given usage was a misspelling in the 1992 AP Newswire: "Bush accelerated foreign aide and weapons ales to Iraq.".</S>
  <S sid="161" ssid="161">It is quite likely that if were in- deed a foreign assistant being discussed, this example would also have another collocation (with the verb, for example), 270 which would indicate the correct sense.</S>
  <S sid="162" ssid="162">Such inconsisten- cies should also be flagged for human supervision.</S>
  <S sid="163" ssid="163">Working from the most to least frequent collocates in this manner, one can use previously tagged collocates to automatically suggest the classification of other words appearing in different collo- cation types for those tagged examples.</S>
  <S sid="164" ssid="164">The one sense per discourse constraint can be used to refine this process further.</S>
  <S sid="165" ssid="165">We are working on a similar use of these two constraints for unsupervised sense clustering.</S>
  <S sid="166" ssid="166">Algor i thm Design Our results also have implications for algorithm design.</S>
  <S sid="167" ssid="167">For the large number of current approaches which treat wide con- text as an unordered bag of words, it may be beneficial to model certain local collocations separately.</S>
  <S sid="168" ssid="168">We have shown that reliability of collocational evidence differs considerably between local and distant context, especially for verbs and adjectives.</S>
  <S sid="169" ssid="169">If one one is interested inproviding aprobability with an answer, modeling local collocations separately will improve the probability estimates and reduce cross entropy.</S>
  <S sid="170" ssid="170">Another eason for modeling local collocations separately is that his will allow the reliable inclusion of evidence with very low frequency counts.</S>
  <S sid="171" ssid="171">Evidence with observed frequency dis- tributions of 1/0 typically constitute on the order of 50% of all available vidence types, yet in a wide context window this low frequency evidence is effectively noise, with predic- tive power little better than chance.</S>
  <S sid="172" ssid="172">However, in very local collocations, ingle data points carry considerable informa- tion, and when used alone can achieve precision in excess of 92%.</S>
  <S sid="173" ssid="173">Their inclusion should improve system recall, with a much-reduced danger of overmodeling the data.</S>
  <S sid="174" ssid="174">Bui lding a Full  Disambiguation System Finally, one may ask to what extent can local collocational evidence alone support apractical sense disambiguation algo- rithm.</S>
  <S sid="175" ssid="175">As shown in Table 3, our models of single collocation types achieve high precision, but individually their applica- bility is limited.</S>
  <S sid="176" ssid="176">However, if we combine these models as described in Section 5, and use an additional function word collocation model when no other evidence is available, we achieve full coverage at a precision of 92%.</S>
  <S sid="177" ssid="177">This result is comparable to those previously reported in the literature us- ing wider context of up to 50 words away [5,6,7,12].</S>
  <S sid="178" ssid="178">Due to the large number of variables involved, we shall not at- tempt o compare these directly.</S>
  <S sid="179" ssid="179">Our results are encouraging, however, and and we plan to conduct amore formal compari- son of the "bag of words" approaches relative to our separate modeling of local collocation types.</S>
  <S sid="180" ssid="180">We will also consider ad- ditional collocation types covering awider range of syntactic relationships.</S>
  <S sid="181" ssid="181">In addition, we hope to incorporate class-based techniques, uch as the modeling of verb-argument selectional preferences [Resnik, 1992], as a mechanism for achieving im- proved performance on unfamiliar collocations.</S>
  <S sid="182" ssid="182">CONCLUSION This paper has examined some of the basic distributional prop- erties of lexical ambiguity in the English language.</S>
  <S sid="183" ssid="183">Our ex- periments have shown that for several definitions of sense and collocation, an ambiguous word has only one sense in a given collocation with a probability of 90-99%.</S>
  <S sid="184" ssid="184">We showed how this claim is influenced by part-of-speech, distance, and sample frequency.</S>
  <S sid="185" ssid="185">We discussed the implications of these results for data set creation and algorithm design, identifying potential weaknesses in the common "bag of words" approach to disambiguation.</S>
  <S sid="186" ssid="186">Finally, we showed that models of local collocation can be combined in a disambiguation algorithm that achieves overall precision of 92%.</S>
  <S sid="187" ssid="187">Bahl, L., P. Brown, P. de Souza, R. Mercer, "A Tree-Based Sta- tistical Language Model for Natural Language Speech Recog- nition," in IEEE Transactions on Acoustics, Speech, and Signal Processing, 37, 1989.</S>
  <S sid="188" ssid="188">Brown, Peter, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer, "Word Sense Disambiguation using Statisti- cal Methods," Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, 1991, pp 264-270.</S>
  <S sid="189" ssid="189">Gale, W., K. Church, and D. Yarowsky, "One Sense Per Dis- course," Proceedings of the 4th DARPA Speech and Natural Language Workshop, 1992.</S>
  <S sid="190" ssid="190">Gale, W., K. Church, and D. Yarowsky, "On Evaluation of Word-Sense Disambiguation Systems," in Proceedings, 30th Annual Meeting of the Association for Computational Linguis- tics, 1992b.</S>
  <S sid="191" ssid="191">Gale, W., K. Church, and D. Yarowsky, "A Method for Disam- biguating Word Senses in a Large Corpus," in Computers and the Humanities, 1993.</S>
  <S sid="192" ssid="192">Hearst, Marti, "Noun Homograph Disambiguation Using Local Context in Large Text Corpora," in Using Corpora, University of Waterloo, Waterloo, Ontario, 1991.</S>
  <S sid="193" ssid="193">Leacock, Claudia, Geoffrey Towell and Ellen Voorhees "Corpus-Based Statistical Sense Resolution," inProceedings, ARPA Human Language Technology Workshop, 1993.</S>
  <S sid="194" ssid="194">Kelly, Edward, and Phillip Stone, Computer Recognition of English Word Senses, North-Holland, Amsterdam, 1975.</S>
  <S sid="195" ssid="195">Resnik, Philip, "A Class-based Approach to Lexical Discov- ery," in Proceedings of 3Oth Annual Meeting of the Association for Computational Linguistics, 1992.</S>
  <S sid="196" ssid="196">Rivest, R. L., "Learning Decision Lists," in Machine Learning, 2, 1987, pp 229-246.</S>
  <S sid="197" ssid="197">Sproat, R., J. Hirschberg and D. Yarowsky "A Corpus-based Synthesizer," in Proceedings, International Conference on Spoken Language Processing, Banff, Alberta.</S>
  <S sid="198" ssid="198">Yarowsky, David "Word-Sense Disambiguation Using Statisti- cal Models of Rogets Categories Trained on Large Corpora," in Proceedings, COLING-92, Nantes, France, 1992.</S>
</PAPER>
