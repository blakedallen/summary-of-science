METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments
Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric.
It is one of several automatic metrics used in this yearâ€™s shared task within the ACL WMT-07 workshop.
This paper recaps the technical details underlying the metric and describes recent improvements in the metric.
The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English.
In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages.
