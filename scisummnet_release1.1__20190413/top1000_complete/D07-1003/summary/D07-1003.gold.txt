What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA
This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions.
Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations.
We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model.
Our model learns soft alignments as a hidden variable in discriminative training.
Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines.
We explore the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching.
We use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer.
