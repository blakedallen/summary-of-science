<PAPER>
  <S sid="0">Fully Automatic Lexicon Expansion For Domain-Oriented Sentiment Analysis</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper proposes an unsupervised lexicon building method for the detecof which convey positive or negative aspects in a specific domain.</S>
    <S sid="2" ssid="2">The lexical entries to be acare called the minimum human-understandable syntactic structures that specify the polarity of clauses.</S>
    <S sid="3" ssid="3">As a clue to obtain candidate atoms, we use the tendency for same polarities to appear successively in contexts.</S>
    <S sid="4" ssid="4">Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values.</S>
    <S sid="5" ssid="5">The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Sentiment Analysis (SA) (Nasukawa and Yi, 2003; Yi et al., 2003) is a task to recognize writers&#8217; feelings as expressed in positive or negative comments, by analyzing unreadably large numbers of documents.</S>
    <S sid="7" ssid="2">Extensive syntactic patterns enable us to detect sentiment expressions and to convert them into semantic structures with high precision, as reported by Kanayama et al. (2004).</S>
    <S sid="8" ssid="3">From the example Japanese sentence (1) in the digital camera domain, the SA system extracts a sentiment representation as (2), which consists of a predicate and an argument with positive (+) polarity.</S>
    <S sid="9" ssid="4">SA in general tends to focus on subjective sentiment expressions, which explicitly describe an author&#8217;s preference as in the above example (1).</S>
    <S sid="10" ssid="5">Objective (or factual) expressions such as in the following examples (3) and (4) may be out of scope even though they describe desirable aspects in a specific domain.</S>
    <S sid="11" ssid="6">However, when customers or corporate users use SA system for their commercial activities, such domain-specific expressions have a more important role, since they convey strong or weak points of the product more directly, and may influence their choice to purchase a specific product, as an example.</S>
    <S sid="12" ssid="7">This paper addresses the Japanese version of Domain-oriented Sentiment Analysis, which identifies polar clauses conveying goodness and badness in a specific domain, including rather objective expressions.</S>
    <S sid="13" ssid="8">Building domain-dependent lexicons for many domains is much harder work than preparing domainindependent lexicons and syntactic patterns, because the possible lexical entries are too numerous, and they may differ in each domain.</S>
    <S sid="14" ssid="9">To solve this problem, we have devised an unsupervised method to acquire domaindependent lexical knowledge where a user has only to collect unannotated domain corpora.</S>
    <S sid="15" ssid="10">The knowledge to be acquired is a domaindependent set of polar atoms.</S>
    <S sid="16" ssid="11">A polar atom is a minimum syntactic structure specifying polarity in a predicative expression.</S>
    <S sid="17" ssid="12">For example, to detect polar clauses in the sentences (3) and (4)i, the following polar atoms (5) and (6) should appear in the lexicon: The polar atom (5) specified the positive polarity of the verb kukkiri-suru.</S>
    <S sid="18" ssid="13">This atom can be generally used for this verb regardless of its arguments.</S>
    <S sid="19" ssid="14">In the polar atom (6), on the other hand, the nominative case of the verb tsuku (&#8216;have&#8217;) is limited to a specific noun zuumu (&#8216;zoom lens&#8217;), since the verb tsuku does not hold the polarity in itself.</S>
    <S sid="20" ssid="15">The automatic decision for the scopes of the atoms is one of the major issues.</S>
    <S sid="21" ssid="16">For lexical learning from unannotated corpora, our method uses context coherency in terms of polarity, an assumption that polar clauses with the same polarity appear successively unless the context is changed with adversative expressions.</S>
    <S sid="22" ssid="17">Exploiting this tendency, we can collect candidate polar atoms with their tentative polarities as those adjacent to the polar clauses which have been identified by their domain-independent polar atoms in the initial lexicon.</S>
    <S sid="23" ssid="18">We use both intrasentential and inter-sentential contexts to obtain more candidate polar atoms.</S>
    <S sid="24" ssid="19">Our assumption is intuitively reasonable, but there are many non-polar (neutral) clauses adjacent to polar clauses.</S>
    <S sid="25" ssid="20">Errors in sentence delimitation or syntactic parsing also result in false candidate atoms.</S>
    <S sid="26" ssid="21">Thus, to adopt a candidate polar atom for the new lexicon, some threshold values for the frequencies or ratios are required, but they depend on the type of the corpus, the size of the initial lexicon, etc.</S>
    <S sid="27" ssid="22">Our algorithm is fully automatic in the sense that the criteria for the adoption of polar atoms are set automatically by statistical estimation based on the distributions of coherency: coherent precision and coherent density.</S>
    <S sid="28" ssid="23">No manual tuning process is required, so the algorithm only needs unannotated domain corpora and the initial lexicon.</S>
    <S sid="29" ssid="24">Thus our learning method can be used not only by the developers of the system, but also by endusers.</S>
    <S sid="30" ssid="25">This feature is very helpful for users to 'The English translations are included only for convenience. analyze documents in new domains.</S>
    <S sid="31" ssid="26">In the next section, we review related work, and Section 3 describes our runtime SA system.</S>
    <S sid="32" ssid="27">In Section 4, our assumption for unsupervised learning, context coherency and its key metrics, coherent precision and coherent density are discussed.</S>
    <S sid="33" ssid="28">Section 5 describes our unsupervised learning method.</S>
    <S sid="34" ssid="29">Experimental results are shown in Section 6, and we conclude in Section 7.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="35" ssid="1">Sentiment analysis has been extensively studied in recent years.</S>
    <S sid="36" ssid="2">The target of SA in this paper is wider than in previous work.</S>
    <S sid="37" ssid="3">For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions.</S>
    <S sid="38" ssid="4">In contrast, our system detects factual polar clauses as well as sentiments.</S>
    <S sid="39" ssid="5">Unsupervised learning for sentiment analysis is also being studied.</S>
    <S sid="40" ssid="6">For example, Hatzivassiloglou and McKeown (1997) labeled adjectives as positive or negative, relying on semantic orientation.</S>
    <S sid="41" ssid="7">Turney (2002) used collocation with &#8220;excellent&#8221; or &#8220;poor&#8221; to obtain positive and negative clues for document classification.</S>
    <S sid="42" ssid="8">In this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring lexical entries from the noisy clues.</S>
    <S sid="43" ssid="9">Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis (Riloff and Wiebe, 2003; Pang and Lee, 2004), which is two-fold classification into subjective and objective sentences.</S>
    <S sid="44" ssid="10">Compared to it, this paper solves a more difficult problem: three-fold classification into positive, negative and non-polar expressions using imperfect coherency in terms of sentiment polarity.</S>
    <S sid="45" ssid="11">Learning methods for phrase-level sentiment analysis closely share an objective of our approach.</S>
    <S sid="46" ssid="12">Popescu and Etzioni (2005) achieved high-precision opinion phrases extraction by using relaxation labeling.</S>
    <S sid="47" ssid="13">Their method iteratively assigns a polarity to a phrase, relying on semantic orientation of co-occurring words in specific relations in a sentence, but the scope of semantic orientation is limited to within a sentence.</S>
    <S sid="48" ssid="14">Wilson et al. (2005) proposed supervised learning, dividing the resources into prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively.</S>
    <S sid="49" ssid="15">Wilson et al. prepared prior polarities from existing resources, and learned the context polarities by using prior polarities and annotated corpora.</S>
    <S sid="50" ssid="16">Therefore the prerequisite data and learned data are opposite from those in our approach.</S>
    <S sid="51" ssid="17">We took the approach used in this paper because we want to acquire more domain-dependent knowledge, and context polarity is easier to access in Japanese'.</S>
    <S sid="52" ssid="18">Our approach and their work can complement each other.</S>
  </SECTION>
  <SECTION title="3 Methodology of Clause-level SA" number="3">
    <S sid="53" ssid="1">As Figure 1 illustrates, the flow of our sentiment analysis system involves three steps.</S>
    <S sid="54" ssid="2">The first step is sentence delimitation: the input document is divided into sentences.</S>
    <S sid="55" ssid="3">The second step is proposition detection: propositions which can form polar clauses are identified in each sentence.</S>
    <S sid="56" ssid="4">The third step is polarity assignment: the polarity of each proposition is examined by considering the polar atoms.</S>
    <S sid="57" ssid="5">This section describes the last two processes, which are based on a deep sentiment analysis method analogous to machine translation (Kanayama et al., 2004) (hereafter &#8220;the MT method&#8221;).</S>
    <S sid="58" ssid="6">Our basic tactic for clause-level SA is the highprecision detection of polar clauses based on deep syntactic analysis.</S>
    <S sid="59" ssid="7">&#8216;Clause-level&#8217; means that only predicative verbs and adjectives such as in (7) are detected, and adnominal (attributive) usages of verbs and adjectives as in (8) are ignored, because utsukushii (&#8216;beautiful&#8217;) in (8) does not convey a positive polarity.</S>
    <S sid="60" ssid="8">Here we use the notion of a proposition as a clause without modality, led by a predicative verb or a predicative adjective.</S>
    <S sid="61" ssid="9">The propositions detected from a sentence are subject to the assignment of polarities.</S>
    <S sid="62" ssid="10">Basically, we detect a proposition only at the head of a syntactic tree3.</S>
    <S sid="63" ssid="11">However, this limitation reduces the recall of sentiment analysis to a very low level.</S>
    <S sid="64" ssid="12">In the example (7) above, utsukushii is the head of the tree, while those initial clauses in (9) to (11) below are not.</S>
    <S sid="65" ssid="13">In order to achieve higher recall while maintaining high precision, we apply two types of syntactic patterns, modality patterns and conjunctive patterns4, to the tree structures from the full-parsing.</S>
    <S sid="66" ssid="14">Modality patterns match some auxiliary verbs or corresponding sentence-final expressions, to allow for specific kinds of modality and negation.</S>
    <S sid="67" ssid="15">One of the typical patterns is [ v to omou] (`I think v ')5, which allows utsukushii in (9) to be a proposition.</S>
    <S sid="68" ssid="16">Also negation is handled with a modality pattern, such as [ v nai] (`not v ').</S>
    <S sid="69" ssid="17">In this case a neg feature is attached to the proposition to identify utsukushii in (10) as a negated proposition.</S>
    <S sid="70" ssid="18">On the other hand, no proposition is identified in (11) due to the deliberate absence of a pattern [ v to yoi] (`I hope v ').</S>
    <S sid="71" ssid="19">We used a total of 103 domain-independent modality patterns, most of which are derived from the coordinative (roughly `and') -te, -shi, -ueni, -dakedenaku, -nominarazu causal (roughly &#8216;because&#8217;) -tame, -kara, -node adversative (roughly `but') -ga, -kedo, -keredo, - monono, -nodaga Another type of pattern is conjunctive patterns, which allow multiple propositions in a sentence.</S>
    <S sid="72" ssid="20">We used a total of 22 conjunctive patterns also derived from the MT method, as exemplified in Table 1.</S>
    <S sid="73" ssid="21">In such cases of coordinative clauses and causal clauses, both clauses can be polar clauses.</S>
    <S sid="74" ssid="22">On the other hand, no proposition is identified in a conditional clause due to the absence of corresponding conjunctive patterns.</S>
    <S sid="75" ssid="23">To assign a polarity to each proposition, polar atoms in the lexicon are compared to the proposition.</S>
    <S sid="76" ssid="24">A polar atom consists of polarity, verb or adjective, and optionally, its arguments.</S>
    <S sid="77" ssid="25">Example (12) is a simple polar atom, where no argument is specified.</S>
    <S sid="78" ssid="26">This atom matches any proposition whose head is utsukushii.</S>
    <S sid="79" ssid="27">Example (13) is a complex polar atom, which assigns a negative polarity to any proposition whose head is the verb kaku and where the accusative case is miryoku.</S>
    <S sid="80" ssid="28">`to lack t&#8212; attraction-ACC&#8217; A polarity is assigned if there exists a polar atom for which verb/adjective and the arguments coincide with the proposition, and otherwise no polarity is assigned.</S>
    <S sid="81" ssid="29">The opposite polarity of the polar atom is assigned to a proposition which has the neg feature.</S>
    <S sid="82" ssid="30">We used a total of 3,275 polar atoms, most of which are derived from an English sentiment lexicon (Yi et al., 2003).</S>
    <S sid="83" ssid="31">According to the evaluation of the MT method (Kanayama et al., 2004), highprecision sentiment analysis had been achieved using the polar atoms and patterns, where the system never took positive sentiment for negative and vice versa, and judged positive or negative to neutral expressions in only about 10% cases.</S>
    <S sid="84" ssid="32">However, the recall is too low, and most of the lexicon is for domain-independent expressions, and thus we need more lexical entries to grasp the positive and negative aspects in a specific domain.</S>
  </SECTION>
  <SECTION title="4 Context Coherency" number="4">
    <S sid="85" ssid="1">This section introduces the intra- and intersentential contexts in which we assume context coherency for polarity, and describes some preliminary analysis of the assumption.</S>
  </SECTION>
  <SECTION title="4.1 Intra-sentential and Inter-sentential Context" number="5">
    <S sid="86" ssid="1">The identification of propositions described in Section 3.1 clarifies our viewpoint of the contexts.</S>
    <S sid="87" ssid="2">Here we consider two types of contexts: intra-sentential context and intersentential context.</S>
    <S sid="88" ssid="3">Figure 2 illustrates the context coherency in a sample discourse (14), where the polarities are perfectly coherent.</S>
    <S sid="89" ssid="4">`It's light and has a zoom lens.&#8217; &#8216;Though the LCD is small, I'm satisfied.'</S>
    <S sid="90" ssid="5">Tada, nedan-ga chotto takai.</S>
    <S sid="91" ssid="6">`But, the price is a little high.&#8217; The intra-sentential context is the link between propositions in a sentence, which are detected as coordinative or causal clauses.</S>
    <S sid="92" ssid="7">If there is an adversative conjunction such as -kedo (`but') in the third sentence in (14), a flag is attached to the relation, as denoted with `0' in Figure 2.</S>
    <S sid="93" ssid="8">Though there are differences in syntactic phenomena, this is simshikashi (&#8216;however&#8217;), demo (`but'), sorenanoni (&#8216;even though&#8217;), tadashi (`on condition that&#8217;), dakedo (`but'), gyakuni (`on the contrary&#8217;), tohaie (&#8216;although&#8217;), keredomo (&#8217;however&#8217;), ippou (`on the other hand&#8217;) used in this paper.</S>
    <S sid="94" ssid="9">The &#8220;Post.&#8221; and &#8220;Sent.&#8221; columns denote the numbers of postings and sentences, respectively.</S>
    <S sid="95" ssid="10">&#8220;Len.&amp;quot; is the average length of sentences (in Japanese characters). ilar to the semantic orientation proposed by Hatzivassiloglou and McKeown (1997).</S>
    <S sid="96" ssid="11">The inter-sentential context is the link between propositions in the main clauses of pairs of adjacent sentences in a discourse.</S>
    <S sid="97" ssid="12">The polarities are assumed to be the same in the inter-sentential context, unless there is an adversative expression as those listed in Table 2.</S>
    <S sid="98" ssid="13">If no proposition is detected as in a nominal sentence, the context is split.</S>
    <S sid="99" ssid="14">That is, there is no link between the proposition of the previous sentence and that of the next sentence.</S>
    <S sid="100" ssid="15">We claim these two types of context can be used for unsupervised learning as clues to assign a tentative polarity to unknown expressions.</S>
    <S sid="101" ssid="16">To validate our assumption, we conducted preliminary observations using various corpora.</S>
    <S sid="102" ssid="17">Throughout this paper we used Japanese corpora from discussion boards in four different domains, whose features are shown in Table 3.</S>
    <S sid="103" ssid="18">All of the corpora have clues to the boundaries of postings, so they were suitable to identify the discourses.</S>
    <S sid="104" ssid="19">How strong is the coherency in the context proposed in Section 4.1?</S>
    <S sid="105" ssid="20">Using the polar clauses detected by the SA system with the initial lexicon, we observed the coherent precision of domain d with lexicon L, defined as: where #(Coherent) and #(Conflict) are occurrence counts of the same and opposite polarities observed between two polar clauses as observed in the discourse.</S>
    <S sid="106" ssid="21">As the two polar clauses, we consider the following types: Window.</S>
    <S sid="107" ssid="22">A polar clause and the nearest polar clause which is found in the preceding n sentences in the discourse.</S>
    <S sid="108" ssid="23">Context.</S>
    <S sid="109" ssid="24">Two polar clauses in the intrasentential and/or inter-sentential context described in Section 4.1.</S>
    <S sid="110" ssid="25">This is the viewpoint of context in our method.</S>
    <S sid="111" ssid="26">Table 4 shows the frequencies of coherent pairs, conflicting pairs, and the coherent precision for half of the digital camera domain corpus.</S>
    <S sid="112" ssid="27">&#8220;Baseline&#8221; is the percentage of positive clauses among the polar clauses6.</S>
    <S sid="113" ssid="28">For the &#8220;Window&#8221; method, we tested for n=0, 1, 2, and oc.</S>
    <S sid="114" ssid="29">&#8220;0&amp;quot; means two propositions within a sentence.</S>
    <S sid="115" ssid="30">Apparently, the larger the window size, the smaller the cp value.</S>
    <S sid="116" ssid="31">When the window size is &#8220;oo&amp;quot;, implying anywhere within a discourse, the ratio is larger than the baseline by only 2.7%, and thus these types of coherency are not reliable even though the number of clues is relatively large.</S>
    <S sid="117" ssid="32">&#8220;Context&#8221; shows the coherency of the two types of context that we considered.</S>
    <S sid="118" ssid="33">The cp values are much higher than those in the &#8220;Window&#8221; methods, because the relationships between adjacent pairs of clauses are handled more appropriately by considering syntactic trees, adversative conjunctions, etc.</S>
    <S sid="119" ssid="34">The cp values for inter-sentential and intra-sentential contexts are almost the same, and thus both contexts can be used to obtain 2.5 times more clues for the intra-sentential context.</S>
    <S sid="120" ssid="35">In the rest of this paper we will use both contexts.</S>
    <S sid="121" ssid="36">We also observed the coherent precision for each domain corpus.</S>
    <S sid="122" ssid="37">The results in the center column of Table 5 indicate the number is slightly different among corpora, but all of them are far from perfect coherency.</S>
    <S sid="123" ssid="38">Besides the conflicting cases, there are many more cases where a polar clause does not appear in the polar context.</S>
    <S sid="124" ssid="39">We also observed the coherent density of the domain d with the lexicon L defined as: This indicates the ratio of polar clauses that appear in the coherent context, among all of the polar clauses detected by the system.</S>
    <S sid="125" ssid="40">The right column of Table 5 shows the coherent density in each domain.</S>
    <S sid="126" ssid="41">The movie domain has notably higher coherent density than the others.</S>
    <S sid="127" ssid="42">This indicates the sentiment expressions are more frequently used in the movie domain.</S>
    <S sid="128" ssid="43">The next section describes the method of our unsupervised learning using this imperfect context coherency.</S>
  </SECTION>
  <SECTION title="5 Unsupervised Learning for Acquisition of Polar Atoms" number="6">
    <S sid="129" ssid="1">Figure 3 shows the flow of our unsupervised learning method.</S>
    <S sid="130" ssid="2">First, the runtime SA system identifies the polar clauses, and the candidate polar atoms are collected.</S>
    <S sid="131" ssid="3">Then, each candidate atom is validated using the two metrics in the previous section, cp and cd, which are calculated from all of the polar clauses found in the domain corpus.</S>
    <S sid="132" ssid="4">Table 6: Examples of candidate polar atoms and their frequencies.</S>
    <S sid="133" ssid="5">`*' denotes that it should not be added to the lexicon. f(a), p(a), and n(a) denote the frequency of the atom and in positive and negative contexts, respectively.</S>
    <S sid="134" ssid="6">From each proposition which does not have a polarity, candidate polar atoms in the form of simple atoms (just a verb or adjective) or complex atoms (a verb or adjective and its rightmost argument consisting of a pair of a noun and a postpositional) are extracted.</S>
    <S sid="135" ssid="7">For each candidate polar atom a, the total appearances f(a), and the occurrences in positive contexts p(a) and negative contexts n(a) are counted, based on the context of the adjacent clauses (using the method described in Section 4.1).</S>
    <S sid="136" ssid="8">If the proposition has the neg feature, the polarity is inverted.</S>
    <S sid="137" ssid="9">Table 6 shows examples of candidate polar atoms with their frequencies.</S>
    <S sid="138" ssid="10">Among the located candidate polar atoms, how can we distinguish true polar atoms, which should be added to the lexicon, from fake polar atoms, which should be discarded?</S>
    <S sid="139" ssid="11">As shown in Section 4, both the coherent precision (72-77%) and the coherent density (7-19%) are so small that we cannot rely on each single appearance of the atom in the polar context.</S>
    <S sid="140" ssid="12">One possible approach is to set the threshold values for frequency in a polar context, max(p(a), n(a)) and for the ratio of appearances in polar contexts among the total appearances, max(p(a),n(a)) f(a) .</S>
    <S sid="141" ssid="13">However, the optimum threshold values should depend on the corpus and the initial lexicon.</S>
    <S sid="142" ssid="14">In order to set general criteria, here we assume that a true positive polar atom a should have higher p(a) f(a) than its average i.e. coherent density, cd(d, L+a), and also have higher p(a) than its average i.e. coherent precision, cp(d, L+a) and these criteria should be met with 90% confidence, where L+a is the initial lexicon with a added.</S>
    <S sid="143" ssid="15">Assuming the binomial distribution, a candidate polar atom is adopted as a positive polar atom7 if both (17) and (18) are satisfied$. where We can assume cd(d, L+a) ^_ cd(d, L), and cp(d, L+a) ^_ cp(d, L) when L is large.</S>
    <S sid="144" ssid="16">We compute the confidence interval using approximation with the F-distribution (Blyth, 1986).</S>
    <S sid="145" ssid="17">These criteria solve the problems in minimum frequency and scope of the polar atoms simultaneously.</S>
    <S sid="146" ssid="18">In the example of Table 6, the simple atom chiisai (ID=1) is discarded because it does not meet (18), while the complex atom chiisai +&#8212; bodii-ga (ID=3) is adopted as a positive atom. shikkari-suru (ID=2) is adopted as a positive simple atom, even though 10 cases out of 64 were observed in the negative context.</S>
    <S sid="147" ssid="19">On the other hand, todoku +&#8212; mokuyou-ni (ID=4) is discarded because it does not meet (17), even though n(a) i.e. always observed in negative contexts.</S>
  </SECTION>
  <SECTION title="6 Evaluation" number="7">
    <S sid="148" ssid="1">First we propose a method of evaluation of the lexical learning. ments of 200 polar atoms.</S>
    <S sid="149" ssid="2">K=0.83.</S>
    <S sid="150" ssid="3">It is costly to make consistent and large &#8216;gold standards&#8217; in multiple domains, especially in identification tasks such as clauselevel SA (cf. classification tasks).</S>
    <S sid="151" ssid="4">Therefore we evaluated the learning results by asking human annotators to classify the acquired polar atoms as positive, negative, and neutral, instead of the instances of polar clauses detected with the new lexicon.</S>
    <S sid="152" ssid="5">This can be done because the polar atoms themselves are informative enough to imply to humans whether the expressions hold positive or negative meanings in the domain.</S>
    <S sid="153" ssid="6">To justify the reliability of this evaluation method, two annotators9 evaluated 200 randomly selected candidate polar atoms in the digital camera domain.</S>
    <S sid="154" ssid="7">The agreement results are shown in Table 7.</S>
    <S sid="155" ssid="8">The manual classification was agreed upon in 89% of the cases and the Kappa value was 0.83, which is high enough to be considered consistent.</S>
    <S sid="156" ssid="9">Using manual judgment of the polar atoms, we evaluated the performance with the following three metrics.</S>
    <S sid="157" ssid="10">Type Precision.</S>
    <S sid="158" ssid="11">The coincidence rate of the polarity between the acquired polar atom and the human evaluators&#8217; judgments.</S>
    <S sid="159" ssid="12">It is always false if the evaluators judged it as &#8216;neutral.&#8217; Token Precision.</S>
    <S sid="160" ssid="13">The coincidence rate of the polarity, weighted by its frequency in the corpus.</S>
    <S sid="161" ssid="14">This metric emulates the precision of the detection of polar clauses with newly acquired poler atoms, in the runtime SA system.</S>
    <S sid="162" ssid="15">Relative Recall.</S>
    <S sid="163" ssid="16">The estimated ratio of the number of detected polar clauses with the expanded lexicon to the number of detected polar clauses with the initial lexicon.</S>
    <S sid="164" ssid="17">Relative recall will be 1 when no new polar atom is acquired.</S>
    <S sid="165" ssid="18">Since the precision was high enough, this metric can be used for approximation of the recall, which is hard to evaluate in extraction tasks such as clause-/phrase-level SA.</S>
  </SECTION>
  <SECTION title="6.2 Robustness for Different Conditions" number="8">
    <S sid="166" ssid="1">For each of the four domain corpora, the annotators evaluated 100 randomly selected polar atoms which were newly acquired by our method, to measure the precisions.</S>
    <S sid="167" ssid="2">Relative recall is estimated by comparing the numbers of detected polar clauses from randomly selected 2,000 sentences, with and without the acquired polar atoms.</S>
    <S sid="168" ssid="3">Table 8 shows the results.</S>
    <S sid="169" ssid="4">The token precision is higher than 90% in all of the corpora, including the movie domain, which is considered to be difficult for SA (Turney, 2002).</S>
    <S sid="170" ssid="5">This is extremely high precision for this task, because the correctness of both the extraction and polarity assignment was evaluated simultaneously.</S>
    <S sid="171" ssid="6">The relative recall 1.28 in the digital camera domain means the recall is increased from 43%10 to 55%.</S>
    <S sid="172" ssid="7">The difference was smaller in other domains, but the domain-dependent polar clauses are much informative than general ones, thus the highprecision detection significantly enhances the system.</S>
    <S sid="173" ssid="8">To see the effects of our method, we conducted a control experiment which used preset criteria.</S>
    <S sid="174" ssid="9">To adopt the candidate atom a, the frequency of polarity, max(p(a), n(a)) was required to be 3 or more, and the ratio of polarity, max(&#65533;(a)&#65533;n(a)) was required to be higher &#65533;(a) than the threshold 0.</S>
    <S sid="175" ssid="10">Varying 0 from 0.05 to with various preset threshold values 0 for the digital camera and movie domains.</S>
    <S sid="176" ssid="11">The rightmost star and circle denote the performance of our method.</S>
    <S sid="177" ssid="12">0.8, we evaluated the token precision and the relative recall in the domains of digital cameras and movies.</S>
    <S sid="178" ssid="13">Figure 4 shows the results.</S>
    <S sid="179" ssid="14">The results showed both relative recall and token precision were lower than in our method for every 0, in both corpora.</S>
    <S sid="180" ssid="15">The optimum 0 was 0.3 in the movie domain and 0.1 in the digital camera domain.</S>
    <S sid="181" ssid="16">Therefore, in this preset approach, a tuning process is necessary for each domain.</S>
    <S sid="182" ssid="17">Our method does not require this tuning, and thus fully automatic learning was possible.</S>
    <S sid="183" ssid="18">Unlike the normal precision-recall tradeoff, the token precision in the movie domain got lower when the 0 is strict.</S>
    <S sid="184" ssid="19">This is due to the frequent polar atoms which can be acquired at the low ratios of the polarity.</S>
    <S sid="185" ssid="20">Our method does not discard these important polar atoms.</S>
    <S sid="186" ssid="21">We also tested the performance while varying the size of the initial lexicon L. We prepared three subsets of the initial lexicon, L0.8, L0.5, and L0.2, removing polar atoms randomly.</S>
    <S sid="187" ssid="22">These lexicons had 0.8, 0.5, 0.2 times the polar atoms, respectively, compared to L. Table 9 shows the precisions and recalls using these lexicons for the learning process.</S>
    <S sid="188" ssid="23">Though the cd values vary, the precision was stable, which means that our method was robust even for different sizes of the lexicon.</S>
    <S sid="189" ssid="24">The smaller the initial lexicon, the higher the relative recall, because the polar atoms which were removed from L were recovered in the learning process.</S>
    <S sid="190" ssid="25">This result suggests the possibility of the initial lexicon (the digital camera domain). the bootstrapping method from a small initial lexicon.</S>
    <S sid="191" ssid="26">As seen in the agreement study, the polar atoms used in our study were intrinsically meaningful to humans.</S>
    <S sid="192" ssid="27">This is because the atoms are predicate-argument structures derived from predicative clauses, and thus humans could imagine the meaning of a polar atom by generating the corresponding sentence in its predicative form.</S>
    <S sid="193" ssid="28">In the evaluation process, some interesting results were observed.</S>
    <S sid="194" ssid="29">For example, a negative atom nai +- kerare-ga (`to be free from vignetting&#8217;) was acquired in the digital camera domain.</S>
    <S sid="195" ssid="30">Even the evaluator who was familiar with digital cameras did not know the term kerare (&#8216;vignetting&#8217;), but after looking up the dictionary she labeled it as negative.</S>
    <S sid="196" ssid="31">Our learning method could pick up such technical terms and labeled them appropriately.</S>
    <S sid="197" ssid="32">Also, there were discoveries in the error analysis.</S>
    <S sid="198" ssid="33">An evaluator assigned positive to aru +- kamera-ga (`to have camera&#8217;) in the mobile phone domain, but the acquired polar atom had the negative polarity.</S>
    <S sid="199" ssid="34">This was actually an insight from the recent opinions that many users want phones without camera functions11.</S>
  </SECTION>
  <SECTION title="7 Conclusion" number="9">
    <S sid="200" ssid="1">We proposed an unsupervised method to acquire polar atoms for domain-oriented SA, and demonstrated its high performance.</S>
    <S sid="201" ssid="2">The lexicon can be expanded automatically by using unannotated corpora, and tuning of the threshold values is not required.</S>
    <S sid="202" ssid="3">Therefore even end-users can use this approach to improve the sentiment analysis.</S>
    <S sid="203" ssid="4">These features allow them to do on-demand analysis of more narrow domains, such as the domain of digital &amp;quot;Perhaps because cameras tend to consume battery power and some users don&#8217;t need them. cameras of a specific manufacturer, or the domain of mobile phones from the female users&#8217; point of view.</S>
  </SECTION>
</PAPER>
