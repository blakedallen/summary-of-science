Sentence Compression Beyond Word Deletion
In this paper we generalise the sentence compression task.
Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion.
We present a new corpus that is suited to our task and a discriminative tree-to-tree transduction model that can naturally account for structural and lexical mismatches.
The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions.
Different from prior research, we achieve sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.
We present a model that can both compress and paraphrase individual sentences without however generating document-level summaries.
Our abstractive methods sheds more light on how people compress sentences, but do not always manage to outperform extractive methods.
We expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts.
We propose the first abstractive compression method.
