<PAPER>
  <S sid="0">Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation).</S>
    <S sid="2" ssid="2">However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word&#8217;s prior polarity.</S>
    <S sid="3" ssid="3">Positive words are used in phrases expressing negative sentiments, or vice versa.</S>
    <S sid="4" ssid="4">Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment.</S>
    <S sid="5" ssid="5">The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task.</S>
    <S sid="6" ssid="6">Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.</S>
    <S sid="7" ssid="7">The evaluation includes assessing the performance of features across multiple machine learning algorithms.</S>
    <S sid="8" ssid="8">For all learning algorithms except one, the combination of all features together gives the best performance.</S>
    <S sid="9" ssid="9">Another facet of the evaluation considers how the presence of neutral instances affects the performance offeatures for distinguishing between positive and negative polarity.</S>
    <S sid="10" ssid="10">These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system&#8217;s ability to identify when an instance is neutral.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="11" ssid="1">Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation).</S>
    <S sid="12" ssid="2">However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word&#8217;s prior polarity.</S>
    <S sid="13" ssid="3">Positive words are used in phrases expressing negative sentiments, or vice versa.</S>
    <S sid="14" ssid="4">Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment.</S>
    <S sid="15" ssid="5">The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task.</S>
    <S sid="16" ssid="6">Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.</S>
    <S sid="17" ssid="7">The evaluation includes assessing the performance of features across multiple machine learning algorithms.</S>
    <S sid="18" ssid="8">For all learning algorithms except one, the combination of all features together gives the best performance.</S>
    <S sid="19" ssid="9">Another facet of the evaluation considers how the presence of neutral instances affects the performance offeatures for distinguishing between positive and negative polarity.</S>
    <S sid="20" ssid="10">These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system&#8217;s ability to identify when an instance is neutral.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="21" ssid="1">Sentiment analysis is a type of subjectivity analysis (Wiebe 1994) that focuses on identifying positive and negative opinions, emotions, and evaluations expressed in natural language.</S>
    <S sid="22" ssid="2">It has been a central component in applications ranging from recognizing inflammatory messages (Spertus 1997), to tracking sentiments over time in online discussions (Tong 2001), to classifying positive and negative reviews (Pang, Lee, and Vaithyanathan 2002; Turney 2002).</S>
    <S sid="23" ssid="3">Although a great deal of work in sentiment analysis has targeted documents, applications such as opinion question answering (Yu and Hatzivassiloglou 2003; Maybury 2004; Stoyanov, Cardie, and Wiebe 2005) and review mining to extract opinions about companies and products (Morinaga et al. 2002; Nasukawa and Yi 2003) require sentence-level or even phrase-level analysis.</S>
    <S sid="24" ssid="4">For example, if a question answering system is to successfully answer questions about people&#8217;s opinions, it must be able not only to pinpoint expressions of positive and negative sentiments, such as we find in sentence (1), but also to determine when an opinion is not being expressed by a word or phrase that typically does evoke one, such as condemned in sentence (2).</S>
    <S sid="25" ssid="5">A common approach to sentiment analysis is to use a lexicon with information about which words and phrases are positive and which are negative.</S>
    <S sid="26" ssid="6">This lexicon may be manually compiled, as is the case with the General Inquirer (Stone et al. 1966), a resource often used in sentiment analysis.</S>
    <S sid="27" ssid="7">Alternatively, the information in the lexicon may be acquired automatically.</S>
    <S sid="28" ssid="8">Acquiring the polarity of words and phrases is itself an active line of research in the sentiment analysis community, pioneered by the work of Hatzivassiloglou and McKeown (1997) on predicting the polarity or semantic orientation of adjectives.</S>
    <S sid="29" ssid="9">Various techniques have been proposed for learning the polarity of words.</S>
    <S sid="30" ssid="10">They include corpus-based techniques, such as using constraints on the co-occurrence in conjunctions of words with similar or opposite polarity (Hatzivassiloglou and McKeown 1997) and statistical measures of word association (Turney and Littman 2003), as well as techniques that exploit information about lexical relationships (Kamps and Marx 2002; Kim and Hovy 2004) and glosses (Esuli and Sebastiani 2005; Andreevskaia and Bergler 2006) in resources such as WordNet.</S>
    <S sid="31" ssid="11">Acquiring the polarity of words and phrases is undeniably important, and there are still open research challenges, such as addressing the sentiments of different senses of words (Esuli and Sebastiani 2006b; Wiebe and Mihalcea 2006), and so on.</S>
    <S sid="32" ssid="12">However, what the polarity of a given word or phrase is when it is used in a particular context is another problem entirely.</S>
    <S sid="33" ssid="13">Consider, for example, the underlined positive and negative words in the following sentence.</S>
    <S sid="34" ssid="14">The first underlined word is Trust.</S>
    <S sid="35" ssid="15">Although many senses of the word trust express a positive sentiment, in this case, the word is not being used to express a sentiment at all.</S>
    <S sid="36" ssid="16">It is simply part of an expression referring to an organization that has taken on the charge of caring for the environment.</S>
    <S sid="37" ssid="17">The adjective well is considered positive, and indeed it is positive in this context.</S>
    <S sid="38" ssid="18">However, the same is not true for the words reason and reasonable.</S>
    <S sid="39" ssid="19">Out of context, we would consider both of these words to be positive.1 In context, the word reason is being negated, changing its polarity from positive to negative.</S>
    <S sid="40" ssid="20">The phrase no reason at all to believe changes the polarity of the proposition that follows; because reasonable falls within this proposition, its polarity becomes negative.</S>
    <S sid="41" ssid="21">The word polluters has a negative connotation, but here in the context of the discussion of the article and its position in the sentence, polluters is being used less to express a sentiment and more to objectively refer to companies that pollute.</S>
    <S sid="42" ssid="22">To clarify how the polarity of polluters is affected by its subject role, consider the purely negative sentiment that emerges when it is used as an object: They are polluters.</S>
    <S sid="43" ssid="23">We call the polarity that would be listed for a word in a lexicon the word&#8217;s prior polarity, and we call the polarity of the expression in which a word appears, considering the context of the sentence and document, the word&#8217;s contextual polarity.</S>
    <S sid="44" ssid="24">Although words often do have the same prior and contextual polarity, many times a word&#8217;s prior and contextual polarities differ.</S>
    <S sid="45" ssid="25">Words with a positive prior polarity may have a negative contextual polarity, or vice versa.</S>
    <S sid="46" ssid="26">Quite often words that are positive or negative out of context are neutral in context, meaning that they are not even being used to express a sentiment.</S>
    <S sid="47" ssid="27">Similarly, words that are neutral out of context, neither positive or negative, may combine to create a positive or negative expression in context.</S>
    <S sid="48" ssid="28">The focus of this work is on the recognition of contextual polarity&#8212;in particular, disambiguating the contextual polarity of words with positive or negative prior polarity.</S>
    <S sid="49" ssid="29">We begin by presenting an annotation scheme for marking sentiment expressions and their contextual polarity in the Multi-perspective Question Answering (MPQA) opinion corpus.</S>
    <S sid="50" ssid="30">We show that, given a set of subjective expressions (identified from the existing annotations in the MPQA corpus), contextual polarity can be annotated reliably.</S>
    <S sid="51" ssid="31">Using the contextual polarity annotations, we conduct experiments in automatically distinguishing between prior and contextual polarity.</S>
    <S sid="52" ssid="32">Beginning with a large lexicon of clues tagged with prior polarity, we identify the contextual polarity of the instances of those clues in the corpus.</S>
    <S sid="53" ssid="33">The process that we use has two steps, first classifying each clue as being in a neutral or polar phrase, and then disambiguating the contextual polarity of the clues marked as polar.</S>
    <S sid="54" ssid="34">For each step in the process, we experiment with a variety of features and evaluate the performance of the features using several different machine learning algorithms.</S>
    <S sid="55" ssid="35">Our experiments reveal a number of interesting findings.</S>
    <S sid="56" ssid="36">First, being able to accurately identify neutral contextual polarity&#8212;when a positive or negative clue is not being used to express a sentiment&#8212;is an important aspect of the problem.</S>
    <S sid="57" ssid="37">The importance of neutral examples has previously been noted for classifying the sentiment of documents (Koppel and Schler 2006), but ours is the first work to explore how neutral instances affect classifying the contextual polarity of words and phrases.</S>
    <S sid="58" ssid="38">In particular, we found that the performance of features for distinguishing between positive and negative polarity greatly degrades when neutral instances are included in the experiments.</S>
    <S sid="59" ssid="39">We also found that achieving the best performance for recognizing contextual polarity requires a wide variety of features.</S>
    <S sid="60" ssid="40">This is particularly true for distinguishing between neutral and polar instances.</S>
    <S sid="61" ssid="41">Although some features help to increase polar or neutral recall or precision, it is only the combination of features together that achieves significant improvements in accuracy over the baselines.</S>
    <S sid="62" ssid="42">Our experiments show that for distinguishing between positive and negative instances, features capturing negation are clearly the most important.</S>
    <S sid="63" ssid="43">However, there is more to the story than simple negation.</S>
    <S sid="64" ssid="44">Features that capture relationships between instances of clues also perform well, indicating that identifying features that represent more complex interdependencies between sentiment clues may be an important avenue for future research.</S>
    <S sid="65" ssid="45">The remainder of this article is organized as follows.</S>
    <S sid="66" ssid="46">Section 2 gives an overview of some of the things that can influence contextual polarity.</S>
    <S sid="67" ssid="47">In Section 3, we describe our corpus and present our annotation scheme and inter-annotator agreement study for marking contextual polarity.</S>
    <S sid="68" ssid="48">Sections 4 and 5 describe the lexicon used in our experiments and how the contextual polarity annotations are used to determine the gold-standard tags for instances from the lexicon.</S>
    <S sid="69" ssid="49">In Section 6, we consider what kind of performance can be expected from a simple, prior-polarity classifier.</S>
    <S sid="70" ssid="50">Section 7 describes the features that we use for recognizing contextual polarity, and our experiments and results are presented in Section 8.</S>
    <S sid="71" ssid="51">In Section 9 we discuss related work, and we conclude in Section 10.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="72" ssid="1">Phrase-level sentiment analysis is not a simple problem.</S>
    <S sid="73" ssid="2">Many things besides negation can influence contextual polarity, and even negation is not always straightforward.</S>
    <S sid="74" ssid="3">Negation may be local (e.g., not good), or involve longer-distance dependencies such as the negation of the proposition (e.g., does not look very good) or the negation of the subject (e.g., no one thinks that it&#8217;s good).</S>
    <S sid="75" ssid="4">In addition, certain phrases that contain negation words intensify rather than change polarity (e.g., not only good but amazing).</S>
    <S sid="76" ssid="5">Contextual polarity may also be influenced by modality: whether the proposition is asserted to be real (realis) or not real (irrealis) (no reason at all to believe is irrealis, for example); word sense (e.g., Environmental Trust vs.</S>
    <S sid="77" ssid="6">He has won the people&#8217;s trust); the syntactic role of a word in the sentence: whether the word is the subject or object of a copular verb (consider polluters are versus they are polluters); and diminishers such as little (e.g., little truth, little threat).</S>
    <S sid="78" ssid="7">Polanyi and Zaenen (2004) give a detailed discussion of many of these types of polarity influencers.</S>
    <S sid="79" ssid="8">Many of these contextual polarity influencers are represented as features in our experiments.</S>
    <S sid="80" ssid="9">Contextual polarity may also be influenced by the domain or topic.</S>
    <S sid="81" ssid="10">For example, the word cool is positive if used to describe a car, but it is negative if it is used to describe someone&#8217;s demeanor.</S>
    <S sid="82" ssid="11">Similarly, a word such as fever is unlikely to be expressing a sentiment when used in a medical context.</S>
    <S sid="83" ssid="12">We use one feature in our experiments to represent the topic of the document.</S>
    <S sid="84" ssid="13">Another important aspect of contextual polarity is the perspective of the person who is expressing the sentiment.</S>
    <S sid="85" ssid="14">For example, consider the phrase failed to defeat in the sentence Israel failed to defeat Hezbollah.</S>
    <S sid="86" ssid="15">From the perspective of Israel, failed to defeat is negative.</S>
    <S sid="87" ssid="16">From the perspective of Hezbollah, failed to defeat is positive.</S>
    <S sid="88" ssid="17">Therefore, the contextual polarity of this phrase ultimately depends on the perspective of who is expressing the sentiment.</S>
    <S sid="89" ssid="18">Although automatically detecting this kind of pragmatic influence on polarity is beyond the scope of this work, this as well as the other types of polarity influencers all are considered when annotating contextual polarity.</S>
  </SECTION>
  <SECTION title="3." number="4">
    <S sid="90" ssid="1">For the experiments in this work, we need a corpus that is annotated comprehensively for sentiment expressions and their contextual polarity.</S>
    <S sid="91" ssid="2">Rather than building a corpus from scratch, we chose to add contextual polarity annotations to the existing annotations in the Multi-perspective Question Answering (MPQA) opinion corpus2 (Wiebe, Wilson, and Cardie 2005).</S>
    <S sid="92" ssid="3">The MPQA corpus is a collection of English-language versions of news documents from the world press.</S>
    <S sid="93" ssid="4">The documents contain detailed, expression-level annotations of attributions and private states (Quirk et al. 1985).</S>
    <S sid="94" ssid="5">Private states are mental and emotional states; they include beliefs, speculations, intentions, and sentiments, among others.</S>
    <S sid="95" ssid="6">Although sentiments are not distinguished from other types of private states in the existing annotations, they are a subset of what already is annotated.</S>
    <S sid="96" ssid="7">This makes the annotations in the MPQA corpus a good starting point for annotating sentiment expressions and their contextual polarity.</S>
    <S sid="97" ssid="8">When developing our annotation scheme for sentiment expressions and contextual polarity, there were three main questions to address.</S>
    <S sid="98" ssid="9">First, which of the existing annotations in the MPQA corpus have the possibility of being sentiment expressions?</S>
    <S sid="99" ssid="10">Second, which of the possible sentiment expressions actually are expressing sentiments?</S>
    <S sid="100" ssid="11">Third, what coding scheme should be used for marking contextual polarity?</S>
    <S sid="101" ssid="12">The MPQA annotation scheme has four types of annotations: objective speech event frames, two types of private state frames, and agent frames that are used for marking speakers of speech events and experiencers of private states.</S>
    <S sid="102" ssid="13">A full description of the MPQA annotation scheme and an agreement study evaluating key aspects of the scheme are found in Wiebe, Wilson, and Cardie (2005).</S>
    <S sid="103" ssid="14">The two types of private state frames, direct subjective frames and expressive subjective element frames, are where we will find sentiment expressions.</S>
    <S sid="104" ssid="15">Direct subjective frames are used to mark direct references to private states as well as speech events in which private states are being expressed.</S>
    <S sid="105" ssid="16">For example, in the following sentences, fears, praised, and said are all marked as direct subjective annotations.</S>
    <S sid="106" ssid="17">The word fears directly refers to a private state; praised refers to a speech event in which a private state is being expressed; and said is marked as direct subjective because a private state is being expressed within the speech event referred to by said.</S>
    <S sid="107" ssid="18">Expressive subjective elements indirectly express private states through the way something is described or through a particular wording.</S>
    <S sid="108" ssid="19">In example (6), the phrase full of absurdities is an expressive subjective element.</S>
    <S sid="109" ssid="20">Subjectivity (Banfield 1982; Wiebe 1994) refers to the linguistic expression of private states, hence the names for the two types of private state annotations.</S>
    <S sid="110" ssid="21">All expressive subjective elements are included in the set of annotations that have the possibility of being sentiment expressions, but the direct subjective frames to include in this set can be pared down further.</S>
    <S sid="111" ssid="22">Direct subjective frames have an attribute, expression intensity, that captures the contribution of the annotated word or phrase to the overall intensity of the private state being expressed.</S>
    <S sid="112" ssid="23">Expression intensity ranges from neutral to high.</S>
    <S sid="113" ssid="24">In the given sentences, fears and praised have an expression intensity of medium, and said has an expression intensity of neutral.</S>
    <S sid="114" ssid="25">A neutral expression intensity indicates that the direct subjective phrase itself is not contributing to the expression of the private state.</S>
    <S sid="115" ssid="26">If this is the case, then the direct subjective phrase cannot be a sentiment expression.</S>
    <S sid="116" ssid="27">Thus, only direct subjective annotations with a non-neutral expression intensity are included in the set of annotations that have the possibility of being sentiment expressions.</S>
    <S sid="117" ssid="28">We call this set of annotations, the union of the expressive subjective elements and the direct subjective frames with a non-neutral intensity, the subjective expressions in the corpus; these are the annotations we will mark for contextual polarity.</S>
    <S sid="118" ssid="29">Table 1 gives a sample of subjective expressions marked in the MPQA corpus.</S>
    <S sid="119" ssid="30">Although many of the words and phrases express what we typically think of as sentiments, others do not, for example, believes, very definitely, and unconditionally and without delay.</S>
    <S sid="120" ssid="31">Now that we have identified which annotations have the possibility of being sentiment expressions, the next question is which of these annotated words and phrases are actually expressing sentiments.</S>
    <S sid="121" ssid="32">We define a sentiment as a positive or negative emotion, evaluation, or stance.</S>
    <S sid="122" ssid="33">On the left of Table 2 are examples of positive sentiments; examples of negative sentiments are on the right.</S>
    <S sid="123" ssid="34">Sample of subjective expressions from the MPQA corpus. victory of justice and freedom such a disadvantageous situation grown tremendously must such animosity not true at all throttling the voice imperative for harmonious society disdain and wrath glorious so exciting disastrous consequences could not have wished for a better situation believes freak show the embodiment of two-sided justice if you&#8217;re not with us, you&#8217;re against us appalling vehemently denied very definitely everything good and nice once and for all under no circumstances shameful mum most fraudulent, terrorist and extremist enthusiastically asked number one democracy hate seems to think gross misstatement indulging in blood-shed and their lunaticism surprised, to put it mildly take justice to pre-historic times unconditionally and without delay so conservative that it makes Pat Buchanan look vegetarian those digging graves for others, get engraved themselves lost the reputation of commitment to principles of human justice ultimately the demon they have reared will eat up their own vitals The final issue to address is the actual annotation scheme for marking contextual polarity.</S>
    <S sid="124" ssid="35">The scheme we developed has four tags: positive, negative, both, and neutral.</S>
    <S sid="125" ssid="36">The positive tag is used to mark positive sentiments.</S>
    <S sid="126" ssid="37">The negative tag is used to mark negative sentiments.</S>
    <S sid="127" ssid="38">The both tag is applied to expressions in which both a positive and negative sentiment are being expressed.</S>
    <S sid="128" ssid="39">Subjective expressions with positive, negative, or both tags are our sentiment expressions.</S>
    <S sid="129" ssid="40">The neutral tag is used for all other subjective expressions, including emotions, evaluations, and stances that are neither positive or negative.</S>
    <S sid="130" ssid="41">Instructions for the contextual-polarity annotation scheme are available at http://www.cs.pitt.edu/mpqa/databaserelease/polarityCodingInstructions.txt.</S>
    <S sid="131" ssid="42">Following are examples from the corpus of each of the different contextual-polarity annotations.</S>
    <S sid="132" ssid="43">Each underlined word or phrase is a subjective expression that was marked in the original MPQA annotations.3 In bold following each subjective expression is the contextual polarity with which it was annotated.</S>
    <S sid="133" ssid="44">To measure the reliability of the polarity annotation scheme, we conducted an agreement study with two annotators4 using 10 documents from the MPQA corpus.</S>
    <S sid="134" ssid="45">The 10 documents contain 447 subjective expressions.</S>
    <S sid="135" ssid="46">Table 3 shows the contingency table for the two annotators&#8217; judgments.</S>
    <S sid="136" ssid="47">Overall agreement is 82%, with a kappa value of 0.72.</S>
    <S sid="137" ssid="48">As part of the annotation scheme, annotators are asked to judge how certain they are in their polarity tags.</S>
    <S sid="138" ssid="49">For 18% of the subjective expressions, at least one annotator used the uncertain tag when marking polarity.</S>
    <S sid="139" ssid="50">If we consider these cases to be borderline and exclude them from the study, percent agreement increases to 90% and kappa rises to 0.84.</S>
    <S sid="140" ssid="51">Table 4 shows the revised contingency table with the uncertain cases removed.</S>
    <S sid="141" ssid="52">This shows that annotator agreement is especially high when both annotators are certain, and that annotators are certain for over 80% of their tags.</S>
    <S sid="142" ssid="53">Note that all annotations are included in the experiments.</S>
    <S sid="143" ssid="54">In total, all 19,962 subjective expressions in the 535 documents (11,112 sentences) of the MPQA corpus were annotated with their contextual polarity as just described.5 Three annotators carried out the task: the two who participated in the annotation study and a third who was trained later.6 Table 5 gives the distribution of the contextual polarity tags.</S>
    <S sid="144" ssid="55">Looking at this table, we see that a small majority of subjective expressions (54.6%) are expressing a positive, negative, or both (positive and negative) sentiment.</S>
    <S sid="145" ssid="56">We refer to these expressions as polar in context.</S>
    <S sid="146" ssid="57">Many of the subjective expressions are neutral and do not express a sentiment.</S>
    <S sid="147" ssid="58">This suggests that, although sentiment is a major type of subjectivity, distinguishing other prominent types of subjectivity will be important for future work in subjectivity analysis.</S>
    <S sid="148" ssid="59">As many NLP applications operate at the sentence level, one important issue to consider is the distribution of sentences with respect to the subjective expressions they contain.</S>
    <S sid="149" ssid="60">In the 11,112 sentences in the MPQA corpus, 28% contain no subjective expressions, 24% contain only one, and 48% contain two or more.</S>
    <S sid="150" ssid="61">Of the 5,304 sentences containing two or more subjective expressions, 17% contain mixtures of positive and negative expressions, and 61% contain mixtures of polar (positive/negative/both) and neutral subjective expressions.</S>
  </SECTION>
  <SECTION title="4." number="5">
    <S sid="151" ssid="1">For the experiments in this article, we use a lexicon of over 8,000 subjectivity clues.</S>
    <S sid="152" ssid="2">Subjectivity clues are words and phrases that may be used to express private states.</S>
    <S sid="153" ssid="3">In other words, subjectivity clues have subjective usages, though they may have objective usages as well.</S>
    <S sid="154" ssid="4">For this work, only single-word clues are used.</S>
    <S sid="155" ssid="5">To compile the lexicon, we began with the list of subjectivity clues from Riloff and Wiebe (2003), which includes the positive and negative adjectives from Hatzivassiloglou and McKeown (1997).</S>
    <S sid="156" ssid="6">The words in this list were grouped in previous work according to their reliability as subjectivity clues.</S>
    <S sid="157" ssid="7">Words that are subjective in most contexts are considered strong subjective clues, indicated by the strongsubj tag.</S>
    <S sid="158" ssid="8">Words that may only have certain subjective usages are considered weak subjective clues, indicated by the weaksubj tag.</S>
    <S sid="159" ssid="9">We expanded the list using a dictionary and a thesaurus, and added words from the General Inquirer positive and negative word lists (Stone et al. 1966) that we judged to be potentially subjective.7 We also gave the new words strongsubj and weaksubj reliability tags.</S>
    <S sid="160" ssid="10">The final lexicon has a coverage of 67% of subjective expressions in the MPQA corpus, where coverage is the percentage of subjective expressions containing one or more instances of clues from the lexicon.</S>
    <S sid="161" ssid="11">The coverage of just sentiment expressions is even higher: 75%.</S>
    <S sid="162" ssid="12">The next step was to tag the clues in the lexicon with their prior polarity: positive, negative, both, or neutral.</S>
    <S sid="163" ssid="13">A word in the lexicon is tagged as positive if out of context it seems to evoke something positive, and negative if it seems to evoke something negative.</S>
    <S sid="164" ssid="14">If a word has both positive and negative meanings, it is tagged with the polarity that seems the most common.</S>
    <S sid="165" ssid="15">A word is tagged as both if it is at the same time both positive and negative.</S>
    <S sid="166" ssid="16">For example, the word bittersweet evokes something both positive and negative.</S>
    <S sid="167" ssid="17">Words like brag are also tagged as both, because the one who is bragging is expressing something positive, yet at the same time describing someone as bragging is expressing a negative evaluation of that person.</S>
    <S sid="168" ssid="18">A word is tagged as neutral if it does not evoke anything positive or negative.</S>
    <S sid="169" ssid="19">For words that came from positive and negative word lists (Stone et al. 1966; Hatzivassiloglou and McKeown 1997), we largely retained their original polarity.</S>
    <S sid="170" ssid="20">However, we did change the polarity of a word if we strongly disagreed with its original class.8 For example, the word apocalypse is listed as positive in the General Inquirer; we changed its prior polarity to negative for our lexicon.</S>
    <S sid="171" ssid="21">By far, the majority of clues in the lexicon (92.8%) are marked as having either positive (33.1%) or negative (59.7%) prior polarity.</S>
    <S sid="172" ssid="22">Only a small number of clues (0.3%) are marked as having both positive and negative polarity.</S>
    <S sid="173" ssid="23">We refer to the set of clues marked as positive, negative, or both as sentiment clues.</S>
    <S sid="174" ssid="24">A total of 6.9% of the clues in the lexicon are marked as neutral.</S>
    <S sid="175" ssid="25">Examples of neutral clues are verbs such as feel, look, and think, and intensifiers such as deeply, entirely, and practically.</S>
    <S sid="176" ssid="26">Although the neutral clues make up a small proportion of the total words in the lexicon, we retain them for our later experiments in recognizing contextual polarity because many of them are good clues that a sentiment is being expressed (e.g., feels slighted, feels satisfied, look kindly on, look forward to).</S>
    <S sid="177" ssid="27">Including them increases the coverage of the system.</S>
    <S sid="178" ssid="28">At the end of the previous section, we considered the distribution of sentences in the MPQA corpus with respect to the subjective expressions they contain.</S>
    <S sid="179" ssid="29">It is interesting to compare that distribution with the distribution of sentences with respect to the instances they contain of clues from the lexicon.</S>
    <S sid="180" ssid="30">We find that there are more sentences with two or more clue instances (62%) than sentences with two or more subjective expressions (48%).</S>
    <S sid="181" ssid="31">More importantly, many more sentences have mixtures of positive and negative clue instances than actually have mixtures of positive and negative subjective expressions.</S>
    <S sid="182" ssid="32">Only 880 sentences have a mixture of both positive and negative subjective expressions, whereas 3,234 sentences have a mixture of positive and negative clue instances.</S>
    <S sid="183" ssid="33">Thus, a large number of positive and negative instances are either neutral in context, or they are combining to form more complex polarity expressions.</S>
    <S sid="184" ssid="34">Either way, this provides strong evidence of the need to be able to disambiguate the contextual polarity of subjectivity and sentiment clues.</S>
  </SECTION>
  <SECTION title="5." number="6">
    <S sid="185" ssid="1">In the experiments described in the following sections, the goal is to classify the contextual polarity of the expressions that contain instances of the subjectivity clues in our lexicon.</S>
    <S sid="186" ssid="2">However, determining which clue instances are part of the same expression and identifying expression boundaries are not the focus of this work.</S>
    <S sid="187" ssid="3">Thus, instead of trying to identify and label each expression, in the following experiments, each clue instance is labeled individually as to its contextual polarity.</S>
    <S sid="188" ssid="4">We define the gold-standard contextual polarity of a clue instance in terms of the manual annotations (Section 3) as follows.</S>
    <S sid="189" ssid="5">If a clue instance is not in a subjective expression (and therefore not in a sentiment expression), its gold class is neutral.</S>
    <S sid="190" ssid="6">If a clue instance appears in just one subjective expression or in multiple subjective expressions with the same contextual polarity, its gold class is the contextual polarity of the subjective expression(s).</S>
    <S sid="191" ssid="7">If a clue instance appears in a mixture of negative and neutral subjective expressions, its gold class is negative; if it is in a mixture of positive and neutral subjective expressions, its gold class is positive.</S>
    <S sid="192" ssid="8">Finally, if a clue instance appears in at least one positive and one negative subjective expression (or in a subjective expression marked as both), then its gold class is both.</S>
    <S sid="193" ssid="9">A clue instance can appear in more than one subjective expression because in the MPQA annotation scheme, it is possible for direct subjective frames and expressive subjective elements frames to overlap.</S>
  </SECTION>
  <SECTION title="6." number="7">
    <S sid="194" ssid="1">Before delving into the task of recognizing contextual polarity, an important question to address is how useful prior polarity alone is for identifying contextual polarity.</S>
    <S sid="195" ssid="2">To answer this question, we create a classifier that simply assumes the contextual polarity of a clue instance is the same as the clue&#8217;s prior polarity.</S>
    <S sid="196" ssid="3">We explore this classifier&#8217;s performance on a small amount of development data, which is not part of the data used in the subsequent experiments.</S>
    <S sid="197" ssid="4">This simple classifier has an accuracy of 48%.</S>
    <S sid="198" ssid="5">From the confusion matrix given in Table 6, we see that 76% of the errors result from words with non-neutral prior polarity appearing in phrases with neutral contextual polarity.</S>
    <S sid="199" ssid="6">Only 12% of the errors result from words with neutral prior polarity appearing in expressions with non-neutral contextual polarity, and only 11% of the errors come from words with a positive or negative prior polarity appearing in expressions with the opposite contextual polarity.</S>
    <S sid="200" ssid="7">Table 6 also shows that positive clues tend to be used in negative expressions far more often than negative clues tend to be used in positive expressions.</S>
    <S sid="201" ssid="8">Given that by far the largest number of errors come from clues with positive, negative, or both prior polarity appearing in neutral contexts, we were motivated to try a two-step approach to the problem of sentiment classification.</S>
    <S sid="202" ssid="9">The first step, Neutral&#8211; Polar Classification, tries to determine if an instance is neutral or polar in context.</S>
    <S sid="203" ssid="10">The second step, Polarity Classification, takes all instances that step one classified as polar, and tries to disambiguate their contextual polarity.</S>
    <S sid="204" ssid="11">This two-step approach is illustrated in Figure 1.</S>
  </SECTION>
  <SECTION title="7." number="8">
    <S sid="205" ssid="1">The features used in our experiments were motivated both by the literature and by exploration of the contextual-polarity annotations in our development data.</S>
    <S sid="206" ssid="2">A number Two-step approach to recognizing contextual polarity. of features were inspired by the paper on contextual-polarity influencers by Polanyi and Zaenan (2004).</S>
    <S sid="207" ssid="3">Other features are those that have been found useful in the past for recognizing subjective sentences (Wiebe, Bruce, and O&#8217;Hara 1999; Wiebe and Riloff 2005).</S>
    <S sid="208" ssid="4">For distinguishing between neutral and polar instances, we use the features listed in Table 7.</S>
    <S sid="209" ssid="5">For ease of description, we group the features into six sets: word features, general modification features, polarity modification features, structure features, sentence features, and one document feature.</S>
    <S sid="210" ssid="6">Word Features In addition to the word token (the token of the clue instance being classified), the word features include the parts of speech of the previous word, the word itself, and the next word.</S>
    <S sid="211" ssid="7">The prior polarity and reliability class features represent those pieces of information about the clue which are taken from the lexicon.</S>
    <S sid="212" ssid="8">General Modification Features These are binary features that capture different types of relationships involving the clue instance.</S>
    <S sid="213" ssid="9">The first four features involve relationships with the word immediately before or after the clue instance.</S>
    <S sid="214" ssid="10">The preceded by adjective feature is true if the clue instance is a noun preceded by an adjective.</S>
    <S sid="215" ssid="11">The preceded by adverb feature is true if the preceding word is an adverb other than not.</S>
    <S sid="216" ssid="12">The preceded by intensifier feature is true if the preceding word is an intensifier, and the self intensifier feature is true if the clue instance itself is an intensifier.</S>
    <S sid="217" ssid="13">A word is considered to be an intensifier if it appears in a list of intensifiers and if it precedes a word of the appropriate part of speech (e.g., an intensifier adjective must come before a noun).</S>
    <S sid="218" ssid="14">The list of intensifiers is a compilation of those listed in Quirk et al. (1985), intensifiers identified from existing entries in the subjectivity lexicon, and intensifiers identified during explorations of the development data.</S>
    <S sid="219" ssid="15">The modifies/modifed by features involve the dependency parse tree of the sentence, obtained by first parsing the sentence (Collins 1997) and then converting the tree into its dependency representation (Xia and Palmer 2001).</S>
    <S sid="220" ssid="16">In a dependency representation, every node in the tree structure is a surface word (i.e., there are no abstract nodes such as NP or VP).</S>
    <S sid="221" ssid="17">The parent word is called the head, and its children are its modifiers.</S>
    <S sid="222" ssid="18">The edge between a parent and a child specifies the grammatical relationship between the two words.</S>
    <S sid="223" ssid="19">Figure 2 shows an example of a dependency parse tree.</S>
    <S sid="224" ssid="20">Instances of clues in the tree are marked with the clue&#8217;s prior polarity and reliability class from the lexicon.</S>
    <S sid="225" ssid="21">For each clue instance, the modifies/modifed by features capture whether there are adj, mod, or vmod relationships between the clue instance and any other instances from the lexicon.</S>
    <S sid="226" ssid="22">Specifically, the modifies strongsubj feature is true if the clue instance and its parent share an adj, mod, or vmod relationship, and if its parent is an instance of a strongsubj clue from the lexicon.</S>
    <S sid="227" ssid="23">The modifies weaksubj feature is the same, except that it looks in the parent for an instance of a weaksubj clue.</S>
    <S sid="228" ssid="24">The modified by strongsubj The dependency tree for the sentence The human rights report poses a substantial challenge to the U.S. interpretation of good and evil.</S>
    <S sid="229" ssid="25">Prior polarity and reliability class are marked in parentheses for words that match clues from the lexicon. feature is true for a clue instance if one of its children is an instance of a strongsubj clue, and if the clue instance and its child share an adj, mod, or vmod relationship.</S>
    <S sid="230" ssid="26">The modified by weaksubj feature is the same, except that it looks for instances of weaksubj clues in the children.</S>
    <S sid="231" ssid="27">Although the adj and vmod relationships are typically local, the mod relationship involves longer-distance as well as local dependencies.</S>
    <S sid="232" ssid="28">Figure 2 helps to illustrate these features.</S>
    <S sid="233" ssid="29">The modifies weaksubj feature is true for substantial, because substantial modifies challenge, which is an instance of a weaksubj clue.</S>
    <S sid="234" ssid="30">For rights, the modifies weaksubj feature is false, because rights modifies report, which is not an instance of a weaksubj clue.</S>
    <S sid="235" ssid="31">The modified by weaksubj feature is false for substantial, because it has no modifiers that are instances of weaksubj clues.</S>
    <S sid="236" ssid="32">For challenge, the modified by weaksubj feature is true because it is being modified by substantial, which is an instance of a weaksubj clue.</S>
    <S sid="237" ssid="33">Polarity Modification Features The modifies polarity, modified by polarity, and conj polarity features capture specific relationships between the clue instance and other sentiment clues it may be related to.</S>
    <S sid="238" ssid="34">If the clue instance and its parent in the dependency tree share an obj, adj, mod, or vmod relationship, the modifies polarity feature is set to the prior polarity of the parent.</S>
    <S sid="239" ssid="35">If the parent is not in the prior-polarity lexicon, its prior polarity is considered neutral.</S>
    <S sid="240" ssid="36">If the clue instance is at the root of the tree and has no parent, the value of the feature is notmod.</S>
    <S sid="241" ssid="37">The modified by polarity feature is similar, looking for adj, mod, and vmod relationships and other sentiment clues in the children of the clue instance.</S>
    <S sid="242" ssid="38">The conj polarity feature determines if the clue instance is in a conjunction.</S>
    <S sid="243" ssid="39">If so, the value of this feature is its sibling&#8217;s prior polarity.</S>
    <S sid="244" ssid="40">As before, if the sibling is not in the Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity lexicon, its prior polarity is neutral.</S>
    <S sid="245" ssid="41">If the clue instance is not in a conjunction, the value for this feature is notmod.</S>
    <S sid="246" ssid="42">Figure 2 also helps to illustrate these modification features.</S>
    <S sid="247" ssid="43">The word substantial with positive prior polarity modifies the word challenge with negative prior polarity.</S>
    <S sid="248" ssid="44">Therefore the modifies polarity feature is negative for substantial, and the modified by polarity feature is positive for challenge.</S>
    <S sid="249" ssid="45">The words good and evil are in a conjunction together; thus the conj polarity feature is negative for good and positive for evil.</S>
    <S sid="250" ssid="46">Structure Features These are binary features that are determined by starting with the clue instance and climbing up the dependency parse tree toward the root, looking for particular relationships, words, or patterns.</S>
    <S sid="251" ssid="47">The in subject feature is true if we find a subj relationship on the path to the root.</S>
    <S sid="252" ssid="48">The in copular feature is true if in subject is false and if a node along the path is both a main verb and a copular verb.</S>
    <S sid="253" ssid="49">The in passive feature is true if a passive verb pattern is found on the climb.</S>
    <S sid="254" ssid="50">The in subject and in copular features were motivated by the intuition that the syntactic role of a word may influence whether a word is being used to express a sentiment.</S>
    <S sid="255" ssid="51">For example, consider the word polluters in each of the following two sentences.</S>
    <S sid="256" ssid="52">In the first sentence, polluters is simply being used as a referring expression.</S>
    <S sid="257" ssid="53">In the second sentence, polluters is clearly being used to express a negative evaluation of the farmers.</S>
    <S sid="258" ssid="54">The motivation for the in passive feature was previous work by Riloff and Wiebe (2003), who found that different words are more or less likely to be subjective depending on whether they are in the active or passive.</S>
    <S sid="259" ssid="55">Sentence Features These are features that previously were found useful for sentence-level subjectivity classification (Wiebe, Bruce, and O&#8217;Hara 1999; Wiebe and Riloff 2005).</S>
    <S sid="260" ssid="56">They include counts of strongsubj and weaksubj clue instances in the current, previous and next sentences, counts of adjectives and adverbs other than not in the current sentence, and binary features to indicate whether the sentence contains a pronoun, a cardinal number, and a modal other than will.</S>
    <S sid="261" ssid="57">Document Feature There is one document feature representing the topic or domain of the document.</S>
    <S sid="262" ssid="58">The motivation for this feature is that whether or not a word is expressing a sentiment or even a private state in general may depend on the subject of the discourse.</S>
    <S sid="263" ssid="59">For example, the words fever and sufferer may express a negative sentiment in certain contexts, but probably not in a health or medical context, as is the case in the following sentence.</S>
    <S sid="264" ssid="60">(14) The disease can be contracted if a person is bitten by a certain tick or if a person comes into contact with the blood of a congo fever sufferer.</S>
    <S sid="265" ssid="61">In the creation of the MPQA corpus, about two-thirds of the documents were selected to be on one of the 10 topics listed in Table 8.</S>
    <S sid="266" ssid="62">The documents for each topic were identified by human searches and by an information retrieval system.</S>
    <S sid="267" ssid="63">The remaining documents were semi-randomly selected from a very large pool of documents from the world press.</S>
    <S sid="268" ssid="64">In the corpus, these documents are listed with the topic miscellaneous.</S>
    <S sid="269" ssid="65">Rather than leaving these documents unlabeled, we chose to label them using the following general domain categories: economics, general politics, health, report events, and war and terrorism.</S>
    <S sid="270" ssid="66">Table 9 lists the features that we use for step two, polarity classification.</S>
    <S sid="271" ssid="67">Word token, word prior polarity, and the polarity-modification features are the same as described for neutral&#8211;polar classification.</S>
    <S sid="272" ssid="68">We use two features to capture two different types of negation.</S>
    <S sid="273" ssid="69">The negated feature is a binary feature that is used to capture more local negations: Its value is true if a negation word or phrase is found within the four words preceding the clue instance, and if the negation word is not also in a phrase that acts as an intensifier rather than a negator.</S>
    <S sid="274" ssid="70">Examples of phrases that intensify rather than negate are not only and nothing if not.</S>
    <S sid="275" ssid="71">The negated subject feature captures a longer-distance type of negation.</S>
    <S sid="276" ssid="72">This feature is true if the subject of the clause containing the clue instance is negated.</S>
    <S sid="277" ssid="73">For example, the negated subject feature is true for support in the following sentence.</S>
    <S sid="278" ssid="74">(15) No politically prudent Israeli could support either of them.</S>
    <S sid="279" ssid="75">The last three polarity features look in a window of four words before the clue instance, searching for the presence of particular types of polarity influencers.</S>
    <S sid="280" ssid="76">General polarity shifters reverse polarity (e.g., little truth, little threat).</S>
    <S sid="281" ssid="77">Negative polarity shifters typically make the polarity of an expression negative (e.g., lack of understanding).</S>
    <S sid="282" ssid="78">Positive polarity shifters typically make the polarity of an expression positive (e.g., abate the damage).</S>
    <S sid="283" ssid="79">The polarity influencers that we used were identified through explorations of the development data.</S>
  </SECTION>
  <SECTION title="8." number="9">
    <S sid="284" ssid="1">We have two primary goals with our experiments in recognizing contextual polarity.</S>
    <S sid="285" ssid="2">The first is to evaluate the features described in Section 7 as to their usefulness for this task.</S>
    <S sid="286" ssid="3">The second is to investigate the importance of recognizing neutral instances&#8212; recognizing when a sentiment clue is not being used to express a sentiment&#8212;for classifying contextual polarity.</S>
    <S sid="287" ssid="4">To evaluate features, we investigate their performance, both together and separately, across several different learning algorithms.</S>
    <S sid="288" ssid="5">Varying the learning algorithm allows us to verify that the features are robust and that their performance is not the artifact of a particular algorithm.</S>
    <S sid="289" ssid="6">We experiment with four different types of machine learning: boosting, memory-based learning, rule learning, and support vector learning.</S>
    <S sid="290" ssid="7">For boosting, we use BoosTexter (Schapire and Singer 2000) AdaBoost.MH.</S>
    <S sid="291" ssid="8">For rule learning, we use Ripper (Cohen 1996).</S>
    <S sid="292" ssid="9">For memory-based learning, we use TiMBL (Daelemans et al. 2003b) IB1 (k-nearest neighbor).</S>
    <S sid="293" ssid="10">For support vector learning, we use SVM-light and SVM-multiclass (Joachims 1999).</S>
    <S sid="294" ssid="11">SVM-light is used for the experiments involving binary classification (neutral&#8211;polar classification), and SVM-multiclass is used for experiments with more than two classes.</S>
    <S sid="295" ssid="12">These machine learning algorithms were chosen because they have been used successfully for a number of natural language processing tasks, and they represent several different types of learning.</S>
    <S sid="296" ssid="13">For all of the classification algorithms except for SVM, the features for a clue instance are represented as they are presented in Section 7.</S>
    <S sid="297" ssid="14">For SVM, the representations for numeric and discrete-valued features are changed.</S>
    <S sid="298" ssid="15">Numeric features, such as the count of strongsubj clue instances in a sentence, are scaled to range between 0 and 1.</S>
    <S sid="299" ssid="16">Discrete-valued features, such as the reliability class feature, are converted into multiple binary features.</S>
    <S sid="300" ssid="17">For example, the reliability class feature is represented by two binary features: one for whether the clue instance is a strongsubj clue and one for whether the clue instance is a weaksubj clue.</S>
    <S sid="301" ssid="18">To investigate the importance of recognizing neutral instances, we perform two sets of polarity classification (step two) experiments.</S>
    <S sid="302" ssid="19">First, we experiment with classifying the polarity of all gold-standard polar instances&#8212;the clue instances identified as polar in context by the manual polarity annotations.</S>
    <S sid="303" ssid="20">Second, we experiment with using the polar instances identified automatically by the neutral&#8211;polar classifiers.</S>
    <S sid="304" ssid="21">Because the second set of experiments includes the neutral instances misclassified in step one, we can compare results for the two sets of experiments to see how the noise of neutral instances affects the performance of the polarity features.</S>
    <S sid="305" ssid="22">All experiments are performed using 10-fold cross validation over a test set of 10,287 sentences from 494 MPQA corpus documents.</S>
    <S sid="306" ssid="23">We measure performance in terms of accuracy, recall, precision, and F-measure.</S>
    <S sid="307" ssid="24">Accuracy is simply the total number of instances correctly classified.</S>
    <S sid="308" ssid="25">Recall, precision, and F-measure for a given class C are defined as follows.</S>
    <S sid="309" ssid="26">Recall is the percentage of all instances of class C correctly identified.</S>
    <S sid="310" ssid="27">|all instances of C | Precision is the percentage of instances classified as class C that are class C in truth.</S>
    <S sid="311" ssid="28">Prec(C) = |instances of C correctly identified | |all instances identified as C | F-measure is the harmonic mean of recall and precision.</S>
    <S sid="312" ssid="29">In our two-step process for recognizing contextual polarity, the first step is neutral&#8211;polar classification, determining whether each instance of a clue from the lexicon is neutral or polar in context.</S>
    <S sid="313" ssid="30">In our test set, there are 26,729 instances of clues from the lexicon.</S>
    <S sid="314" ssid="31">The features we use for this step were listed above in Table 7 and described in Section 7.1.</S>
    <S sid="315" ssid="32">In this section, we perform two sets of experiments.</S>
    <S sid="316" ssid="33">In the first, we compare the results of neutral&#8211;polar classification using all the neutral&#8211;polar features against two baselines.</S>
    <S sid="317" ssid="34">The first baseline uses just the word token feature.</S>
    <S sid="318" ssid="35">The second baseline (word+priorpol) uses the word token and prior polarity features.</S>
    <S sid="319" ssid="36">In the second set of experiments, we explore the performance of different sets of features for neutral&#8211;polar classification.</S>
    <S sid="320" ssid="37">Research has shown that the performance of learning algorithms for NLP tasks can vary widely depending on their parameter settings, and that the optimal parameter settings can also vary depending on the set of features being evaluated (Daelemans et al. 2003a; Hoste 2005).</S>
    <S sid="321" ssid="38">Although the goal of this work is not to identify the optimal configuration for each algorithm and each set of features, we still want to make a reasonable attempt to find a good configuration for each algorithm.</S>
    <S sid="322" ssid="39">To do this, we perform 10-fold cross validation of the more challenging baseline classifier (word+priorpol) on the development data, varying select parameter settings.</S>
    <S sid="323" ssid="40">The results from those experiments are then used to select the parameter settings for each algorithm.</S>
    <S sid="324" ssid="41">For BoosTexter, we vary the number of rounds of boosting.</S>
    <S sid="325" ssid="42">For TiMBL, we vary the value for k (the number of neighbors) and the distance metric (overlap or modified value difference metric [MVDM]).</S>
    <S sid="326" ssid="43">For Ripper, we vary whether negative tests are disallowed for nominal (-!n) and set (-!s) valued attributes and how much to simplify the hypothesis (-S).</S>
    <S sid="327" ssid="44">For SVM, we experiment with linear, polynomial, and radial basis function kernels.</S>
    <S sid="328" ssid="45">Table 10 gives the settings selected for the neutral&#8211;polar classification experiments for the different learning algorithms.</S>
    <S sid="329" ssid="46">Table 11.</S>
    <S sid="330" ssid="47">For each algorithm, we give the results for the two baseline classifiers, followed by the results for the classifier trained using all the neutral&#8211;polar features.</S>
    <S sid="331" ssid="48">The results shown in bold are significantly better than both baselines (two-sided t-test, p &#8804; 0.05) for the given algorithm.</S>
    <S sid="332" ssid="49">Working together, how well do the neutral&#8211;polar features perform?</S>
    <S sid="333" ssid="50">For BoosTexter, TiMBL, and Ripper, the classifiers trained using all the features improve significantly over the two baselines in terms of accuracy, polar recall, polar F-measure, and neutral precision.</S>
    <S sid="334" ssid="51">Neutral F-measure is also higher, but not significantly so.</S>
    <S sid="335" ssid="52">These consistent results across three of the four algorithms show that the neutral&#8211;polar features are helpful for determining when a sentiment clue is actually being used to express a sentiment.</S>
    <S sid="336" ssid="53">Interestingly, Ripper is the only algorithm for which the word-token baseline performed better than the word+priorpol baseline.</S>
    <S sid="337" ssid="54">Nevertheless, the prior polarity feature is an important component in the performance of the Ripper classifier using all the features.</S>
    <S sid="338" ssid="55">Excluding prior polarity from this classifier results in a significant decrease in performance for every metric.</S>
    <S sid="339" ssid="56">Decreases range from 2.5% for neutral recall to 9.5% for polar recall.</S>
    <S sid="340" ssid="57">The best SVM classifier is the word+priorpol baseline.</S>
    <S sid="341" ssid="58">In terms of accuracy, this classifier does not perform much worse than the BoosTexter and TiMBL classifiers that use all the neutral&#8211;polar features: The SVM word+priorpol baseline classifier has an accuracy of 75.6%, and both the BoosTexter and TiMBL classifiers have an accuracy of 76.5%.</S>
    <S sid="342" ssid="59">However, the BoosTexter and TiMBL classifiers using all the features perform notably better in terms of polar recall and F-measure.</S>
    <S sid="343" ssid="60">The BoosTexter and TiMBL classifiers have polar recalls that are 7% and 9.2% higher than the SVM baseline.</S>
    <S sid="344" ssid="61">Polar F-measures for BoosTexter and TiMBL are 3.9% and 4.5% higher.</S>
    <S sid="345" ssid="62">These increases are significant for p &lt; 0.01.</S>
    <S sid="346" ssid="63">8.1.2 Feature Set Evaluation.</S>
    <S sid="347" ssid="64">To evaluate the contribution of the various features for neutral&#8211;polar classification, we perform a series of experiments in which different sets of neutral&#8211;polar features are added to the word+priorpol baseline and new classifiers are trained.</S>
    <S sid="348" ssid="65">We then compare the performance of these new classifiers to the word+priorpol baseline, with the exception of the Ripper classifiers, which we compare to the higher word baseline.</S>
    <S sid="349" ssid="66">Table 12 lists the sets of features tested in these experiments.</S>
    <S sid="350" ssid="67">The feature sets generally correspond to how the neutral&#8211;polar features are presented in Table 7, although some of the groups are broken down into more fine-grained sets that we believe capture meaningful distinctions.</S>
    <S sid="351" ssid="68">Table 13 gives the results for these experiments.</S>
    <S sid="352" ssid="69">Increases and decreases for a given metric as compared to the word+priorpol baseline (word baseline for Ripper) are indicated by + or &#8211;, respectively.</S>
    <S sid="353" ssid="70">Where changes are significant at the p &lt; 0.1 level, ++ or &#8211; &#8211; are used, and where changes are significant at the p &lt; 0.05 level, +++ or &#8211; &#8211; &#8211; are used.</S>
    <S sid="354" ssid="71">An &#8220;nc&#8221; indicates no change (a change of less than &#177; 0.05) compared to the baseline.</S>
    <S sid="355" ssid="72">What does Table 13 reveal about the performance of various feature sets for neutral&#8211; polar classification?</S>
    <S sid="356" ssid="73">Most noticeable is that no individual feature sets stand out as strong performers.</S>
    <S sid="357" ssid="74">The only significant improvements in accuracy come from the PARTSOF-SPEECH and RELIABILITY-CLASS feature sets for Ripper.</S>
    <S sid="358" ssid="75">These improvements are perhaps not surprising given that the Ripper baseline was much lower to begin with.</S>
    <S sid="359" ssid="76">Very few feature sets show any improvement for SVM.</S>
    <S sid="360" ssid="77">Again, this is not unexpected given that all the features together performed worse than the word+priorpol baseline Increases and decreases for a given metric as compared to the word+priorpol baseline (word baseline for Ripper) are indicated by + or &#8211;, respectively; ++ or &#8211; &#8211; indicates the change is significant at the p &lt; 0.1 level; +++ or &#8211; &#8211; &#8211; indicates significance at the p &lt; 0.05 level; nc indicates no change. for SVM.</S>
    <S sid="361" ssid="78">The performance of the feature sets for BoosTexter and TiMBL are perhaps the most revealing.</S>
    <S sid="362" ssid="79">In the previous experiments using all the features together, these algorithms produced classifiers with the same high performance.</S>
    <S sid="363" ssid="80">In these experiments, six different feature sets for each algorithm show improvements in accuracy over the baseline, yet none of those improvements are significant.</S>
    <S sid="364" ssid="81">This suggests that achieving the highest performance for neutral&#8211;polar classification requires a wide variety of features working together in combination.</S>
    <S sid="365" ssid="82">We further test this result by evaluating the effect of removing the features that produced either no change or a drop in accuracy from the respective all-feature classifiers.</S>
    <S sid="366" ssid="83">For example, we train a TiMBL neutral&#8211;polar classifier using all the features except for those in the PRECEDED-POS, INTENSIFY, STRUCTURE, CURSENT-COUNTS, and TOPIC feature sets, and then compare the performance of this new classifier to the TiMBL, allfeature classifier.</S>
    <S sid="367" ssid="84">Although removing the non-performing features has little effect for BoosTexter, performance does drop for both TiMBL and Ripper.</S>
    <S sid="368" ssid="85">The primary source of this performance drop is a decrease in polar recall: 2% for TiMBL and 3.2% for Ripper.</S>
    <S sid="369" ssid="86">Although no feature sets stand out in Table 13 as far as giving an overall high performance, there are some features that consistently improve performance across the different algorithms.</S>
    <S sid="370" ssid="87">The reliability class of the clue instance (RELIABILITY-CLASS) improves accuracy over the baseline for all four algorithms.</S>
    <S sid="371" ssid="88">It is the only feature that does so.</S>
    <S sid="372" ssid="89">The RELCLASS-MOD features give improvements for all metrics for BoosTexter, Ripper, and TiMBL, as well as improving polar F-measure for SVM.</S>
    <S sid="373" ssid="90">The PARTS-OFSPEECH features are also fairly consistent, improving performance for all the algorithms except for SVM.</S>
    <S sid="374" ssid="91">There are also a couple of feature sets that consistently do not improve performance for any of the algorithms: the INTENSIFY and PRECEDED-POS features.</S>
    <S sid="375" ssid="92">For the second step of recognizing contextual polarity, we classify the polarity of all clue instances identified as polar in step one.</S>
    <S sid="376" ssid="93">The features for polarity classification were listed in Table 9 and described in Section 7.2.</S>
    <S sid="377" ssid="94">We investigate the performance of the polarity features under two conditions: (1) perfect neutral&#8211;polar recognition and (2) automatic neutral&#8211;polar recognition.</S>
    <S sid="378" ssid="95">For condition 1, we identify the polar instances according to the gold-standard, manual contextual-polarity annotations.</S>
    <S sid="379" ssid="96">In the test data, 9,835 instances of the clues from the lexicon are polar in context according to the manual annotations.</S>
    <S sid="380" ssid="97">Experiments under condition 1 classify these instances as having positive, negative, or both (positive and negative) polarity.</S>
    <S sid="381" ssid="98">For condition 2, we take the best performing neutral&#8211;polar classifier for each algorithm and use the output from those algorithms to identify the polar instances.</S>
    <S sid="382" ssid="99">Because polar instances now are being identified automatically, there will be noise in the form of misclassified neutral instances.</S>
    <S sid="383" ssid="100">Therefore, for experiments under condition 2 we include the neutral class and perform four-way classification instead of three-way.</S>
    <S sid="384" ssid="101">Condition 1 allows us to investigate the performance of the different polarity features without the noise of misclassified neutral instances.</S>
    <S sid="385" ssid="102">Also, because the set of polar instances being classified is the same for all the algorithms, condition 1 allows us to compare the performance of the polarity features across the different algorithms.</S>
    <S sid="386" ssid="103">However, condition 2 is the more natural one.</S>
    <S sid="387" ssid="104">It allows us to see how the noise of neutral instances affects the performance of the polarity features.</S>
    <S sid="388" ssid="105">The following sections describe three sets of experiments.</S>
    <S sid="389" ssid="106">First, we investigate the performance of the polarity features used together for polarity classification under condition 1.</S>
    <S sid="390" ssid="107">As before, the word and word+priorpol classifiers provide our baselines.</S>
    <S sid="391" ssid="108">In the second set of experiments, we explore the performance of different sets of features for polarity classification, again assuming perfect recognition of the polar instances.</S>
    <S sid="392" ssid="109">Finally, we experiment with polarity classification using all the polarity features under condition 2, automatic recognition of the polar instances.</S>
    <S sid="393" ssid="110">As before, we use the development data to select the parameter settings for each algorithm.</S>
    <S sid="394" ssid="111">The settings for polarity classification are given in Table 14.</S>
    <S sid="395" ssid="112">They were selected based on the performance of the word+priorpol baseline classifier under condition 2.</S>
    <S sid="396" ssid="113">8.2.1 Classification Results: Condition 1.</S>
    <S sid="397" ssid="114">The results for polarity classification using all the polarity features, assuming perfect neutral&#8211;polar recognition for step one, are given in Table 15.</S>
    <S sid="398" ssid="115">For each algorithm, we give the results for the two baseline classifiers, followed by the results for the classifier trained using all the polarity features.</S>
    <S sid="399" ssid="116">For the metrics where the polarity features perform statistically better than both baselines (two-sided t-test, p &#8804; 0.05), the results are given in bold.</S>
    <S sid="400" ssid="117">How well do the polarity features perform working all together?</S>
    <S sid="401" ssid="118">For all algorithms, the polarity classifier using all the features significantly outperforms both baselines in terms of accuracy, positive F-measure, and negative F-measure.</S>
    <S sid="402" ssid="119">These consistent improvements in performance across all four algorithms show that these features are quite useful for polarity classification.</S>
    <S sid="403" ssid="120">One interesting thing that Table 15 reveals is that negative polarity words are much more straightforward to recognize than positive polarity words, at least in this corpus.</S>
    <S sid="404" ssid="121">For the negative class, precisions and recalls for the word+priorpol baseline range from 82.2 to 87.2.</S>
    <S sid="405" ssid="122">For the positive class, precisions and recalls for the word+priorpol baseline range from 63.7 to 76.7.</S>
    <S sid="406" ssid="123">However, it is with the positive class that polarity features seem to help the most.</S>
    <S sid="407" ssid="124">With the addition of the polarity features, positive F-measure improves by 5 points on average; improvements in negative F-measures average only 2.75 points.</S>
    <S sid="408" ssid="125">8.2.2 Feature Set Evaluation.</S>
    <S sid="409" ssid="126">To evaluate the performance of the various features for polarity classification, we again perform a series of ablation experiments.</S>
    <S sid="410" ssid="127">As before, we start with the word+priorpol baseline classifier, add different sets of polarity features, train new classifiers, and compare the results of the new classifiers to the baseline.</S>
    <S sid="411" ssid="128">Increases and decreases for a given metric as compared to the word+priorpol baseline are indicated by + or &#8211;, respectively; ++ or &#8211; &#8211; indicates the change is significant at the p &lt; 0.1 level; +++ or &#8211; &#8211; &#8211; indicates significance at the p &lt; 0.05 level.</S>
    <S sid="412" ssid="129">Table 16 lists the sets of features tested in each experiment, and Table 17 shows the results of the experiments.</S>
    <S sid="413" ssid="130">Results are reported as they were previously in Section 8.1.2, with increases and decreases compared to the baseline for a given metric indicated by + or &#8211;, respectively.</S>
    <S sid="414" ssid="131">Looking at Table 17, we see that all three sets of polarity features help to increase performance as measured by accuracy and positive and negative F-measures.</S>
    <S sid="415" ssid="132">This is true for all the classification algorithms.</S>
    <S sid="416" ssid="133">As we might expect, including the negation features has the most marked effect on the performance of polarity classification, with statistically significant improvements for most metrics across all the algorithms.9 The polarity-modification features also seem to be important for polarity classification, in particular for disambiguating the positive instances.</S>
    <S sid="417" ssid="134">For all the algorithms except TiMBL, including the polarity-modification features results in significant improvements for at least one of the positive metrics.</S>
    <S sid="418" ssid="135">The polarity shifters also help classification, but they seem to be the weakest of the features: Including them does not result in significant improvements for any algorithm.</S>
    <S sid="419" ssid="136">Another question that is interesting to consider is how much the word token feature contributes to polarity classification, given all the other polarity features.</S>
    <S sid="420" ssid="137">Is it enough to know the prior polarity of a word, whether it is being negated, and how it is related to other polarity influencers?</S>
    <S sid="421" ssid="138">To answer this question, we train classifiers using all the polarity features except for word token.</S>
    <S sid="422" ssid="139">Table 18 gives the results for these classifiers; for comparison, the results for the all-feature polarity classifiers are also given.</S>
    <S sid="423" ssid="140">Interestingly, excluding the word token feature produces only small changes in the overall results.</S>
    <S sid="424" ssid="141">The results for BoosTexter and Ripper are slightly lower, and the results for SVM are practically unchanged.</S>
    <S sid="425" ssid="142">TiMBL actually shows a slight improvement, with the exception of the both class.</S>
    <S sid="426" ssid="143">This provides further evidence of the strength of the polarity features.</S>
    <S sid="427" ssid="144">Also, a classifier not tied to actual word tokens may potentially be a more domain-independent classifier.</S>
    <S sid="428" ssid="145">8.2.3 Classification Results: Condition 2.</S>
    <S sid="429" ssid="146">The experiments in Section 8.2.1 show that the polarity features perform well under the ideal condition of perfect recognition of polar instances.</S>
    <S sid="430" ssid="147">The next question to consider is how well the polarity features perform under the more natural but less-than-perfect condition of automatic recognition of polar instances.</S>
    <S sid="431" ssid="148">To investigate this, the polarity classifiers (including the baselines) for each algorithm in these experiments start with the polar instances identified by the best performing neutral&#8211;polar classifier for that algorithm (from Section 8.1.1).</S>
    <S sid="432" ssid="149">The results for these experiments are given in Table 19.</S>
    <S sid="433" ssid="150">As before, statistically significant improvements over both baselines are given in bold.</S>
    <S sid="434" ssid="151">How well do the polarity features perform in the presence of noise from misclassified neutral instances?</S>
    <S sid="435" ssid="152">Our first observation comes from comparing Table 15 with Table 19: Polarity classification results are much lower for all classifiers with the noise of neutral instances.</S>
    <S sid="436" ssid="153">Yet in spite of this, the polarity features still produce classifiers that outperform the baselines.</S>
    <S sid="437" ssid="154">For three of the four algorithms, the classifier using all the polarity features has the highest accuracy.</S>
    <S sid="438" ssid="155">For BoosTexter and TiMBL, the improvements in accuracy over both baselines are significant.</S>
    <S sid="439" ssid="156">Also for all algorithms, using the polarity features gives the highest positive and negative F-measures.</S>
    <S sid="440" ssid="157">Because the set of polarity instances being classified by each algorithm is different, we cannot directly compare the results from one algorithm to the next.</S>
    <S sid="441" ssid="158">Although the two-step approach to recognizing contextual polarity allows us to focus our investigation on the performance of features for both neutral&#8211;polar classification and polarity classification, the question remains: How does the two-step approach compare to recognizing contextual polarity in a single classification step?</S>
    <S sid="442" ssid="159">The results shown in Table 20 help to answer this question.</S>
    <S sid="443" ssid="160">The first row in Table 20 for each algorithm shows the combined result for the two stages of classification.</S>
    <S sid="444" ssid="161">For BoosTexter, TiMBL, and Ripper, this is the combination of results from using all the neutral&#8211;polar features for step one, together with the results from using all of the polarity features for step two.10 For SVM, this is the combination of results from the word+priorpol baseline from step one, together with results for using all the polarity features for step two.</S>
    <S sid="445" ssid="162">Recall that the word+priorpol classifier was the best neutral&#8211;polar classifier for SVM (see Table 11).</S>
    <S sid="446" ssid="163">The second rows for BoosTexter, TiMBL, and Ripper show the results of a single classifier trained to recognize contextual polarity using all the neutral&#8211;polar and polarity features together.</S>
    <S sid="447" ssid="164">For SVM, the second row shows the results of classifying the contextual polarity using just the word token feature.</S>
    <S sid="448" ssid="165">This classifier outperformed all others for SVM.</S>
    <S sid="449" ssid="166">In the table, the best result for each metric for each algorithm is highlighted in bold.</S>
    <S sid="450" ssid="167">When comparing the two-step and one-step approaches, contrary to our expectations, we see that the one-step approach performs about as well or better than the two-step approach for recognizing contextual polarity.</S>
    <S sid="451" ssid="168">For SVM, the improvement in accuracy achieved by the two-step approach is significant, but this is not true for the other algorithms.</S>
    <S sid="452" ssid="169">One fairly consistent difference between the two approaches is that the two-step approach scores slightly higher for neutral F-measure, and the onestep approach achieves higher F-measures for the polarity classes.</S>
    <S sid="453" ssid="170">The difference in negative F-measure is significant for BoosTexter, TiMBL, and Ripper.</S>
    <S sid="454" ssid="171">The exception to this is SVM.</S>
    <S sid="455" ssid="172">For SVM, the two-step approach achieves significantly higher positive and negative F-measures.</S>
    <S sid="456" ssid="173">One last question we consider is how much the neutral&#8211;polar features contribute to the performance of the one-step classifiers.</S>
    <S sid="457" ssid="174">The third line in Table 20 for BoosTexter, TiMBL, and Ripper gives the results for a one-step classifier trained without the neutral&#8211; polar features.</S>
    <S sid="458" ssid="175">Although the differences are not always large, excluding the neutral&#8211; polar features consistently degrades performance in terms of accuracy and positive, negative, and neutral F-measures.</S>
    <S sid="459" ssid="176">The drop in negative F-measure is significant for all three algorithms, the drop in neutral F-measure is significant for BoosTexter and TiMBL, and the drop in accuracy is significant for TiMBL and Ripper (and for BoosTexter at the p &#8804; 0.1 level).</S>
    <S sid="460" ssid="177">The modest drop in performance that we see when excluding the neutral&#8211;polar features in the one-step approach seems to suggest that discriminating between neutral and polar instances is helpful but not necessarily crucial.</S>
    <S sid="461" ssid="178">However, consider Figure 3.</S>
    <S sid="462" ssid="179">In this figure, we show the F-measures for the positive, negative, and both classes for the BoosTexter polarity classifier that uses the gold-standard neutral/polar instances (from Table 15) and for the BoosTexter one-step polarity classifier that uses all features (from Table 20).</S>
    <S sid="463" ssid="180">Plotting the same sets of results for the other three algorithms produces very similar figures.</S>
    <S sid="464" ssid="181">The difference when the classifiers have to contend with the noise from neutral instances is dramatic.</S>
    <S sid="465" ssid="182">Although Table 20 shows that there is room for improvement across all the contextual polarity classes, Figure 3 shows us that perhaps the best way to achieve these improvements is to improve the ability to discriminate the neutral class from the others.</S>
  </SECTION>
  <SECTION title="9." number="10">
    <S sid="466" ssid="1">Other researchers who have worked on classifying the contextual polarity of sentiment expressions are Yi et al. (2003), Popescu and Etzioni (2005), and Suzuki, Takamura, and Okumura (2006).</S>
    <S sid="467" ssid="2">Yi et al. use a lexicon and manually developed patterns to classify contextual polarity.</S>
    <S sid="468" ssid="3">Their patterns are high-quality, yielding quite high precision over the set of expressions that they evaluate.</S>
    <S sid="469" ssid="4">Popescu and Etzioni use an unsupervised classification technique called relaxation labeling (Hummel and Zucker 1983) to recognize the contextual polarity of words that are at the heads of select opinion phrases.</S>
    <S sid="470" ssid="5">They take an iterative approach, using relaxation labeling first to determine the contextual polarities of the words, then again to label the polarities of the words with respect to their targets.</S>
    <S sid="471" ssid="6">A third stage of relaxation labeling then is used to assign final polarities to the words, taking into consideration the presence of other polarity terms and negation.</S>
    <S sid="472" ssid="7">As we do, Popescu and Etzioni use features that represent conjunctions and dependency relations between polarity words.</S>
    <S sid="473" ssid="8">Suzuki et al. use a bootstrapping approach to classify the polarity of tuples of adjectives and their target nouns in Japanese blogs.</S>
    <S sid="474" ssid="9">Included in the features that they use are the words that modify the adjectives and the word that the adjective modifies.</S>
    <S sid="475" ssid="10">They consider the effect of a single negation term, the Japanese equivalent of not.</S>
    <S sid="476" ssid="11">Our work in recognizing contextual polarity differs from this research on expression-level sentiment analysis in several ways.</S>
    <S sid="477" ssid="12">First, the set of expressions they evaluate is limited either to those that target specific items of interest, such as products and product features, or to tuples of adjectives and nouns.</S>
    <S sid="478" ssid="13">In contrast, we seek to classify the contextual polarity of all instances of words from a large lexicon of subjectivity clues that appear in the corpus.</S>
    <S sid="479" ssid="14">Included in the lexicon are not only adjectives, but nouns, verbs, adverbs, and even modals.</S>
    <S sid="480" ssid="15">Our work also differs from other research in the variety of features that we use.</S>
    <S sid="481" ssid="16">As other researchers do, we consider negation and the words that directly modify or are modified by the expression being classified.</S>
    <S sid="482" ssid="17">However, with negation, we have features for both local and longer-distance types of negation, and we take care to count negation terms only when they are actually being used to negate, excluding, for example, negation terms when they are used in phrases that intensify (e.g., not only).</S>
    <S sid="483" ssid="18">We also include contextual features to capture the presence of other clue instances in the surrounding sentences, and features that represent the reliability of clues from the lexicon.</S>
    <S sid="484" ssid="19">Finally, a unique aspect of the work presented in this article is the evaluation of different features for recognizing contextual polarity.</S>
    <S sid="485" ssid="20">We first presented the features explored in this research in Wilson, Wiebe, and Hoffman (2005), but this work significantly extends that initial evaluation.</S>
    <S sid="486" ssid="21">We explore the performance of features across different learning algorithms, and we evaluate not only features for discriminating between positive and negative polarity, but features for determining when a word is or is not expressing a sentiment in the first place (neutral in context).</S>
    <S sid="487" ssid="22">This is also the first work to evaluate the effect of neutral instances on the performance of features for discriminating between positive and negative contextual polarity.</S>
    <S sid="488" ssid="23">Recognizing contextual polarity is just one facet of the research in automatic sentiment analysis.</S>
    <S sid="489" ssid="24">Research ranges from work on learning the prior polarity (semantic orientation) of words and phrases (e.g., Hatzivassiloglou and McKeown 1997; Kamps and Marx 2002; Turney and Littman 2003; Hu and Liu 2004; Kim and Hovy 2004; Esuli and Sebastiani 2005; Takamura, Inui, and Okumura 2005; Popescu and Etzioni 2005; Andreevskaia and Bergler 2006; Esuli and Sebastiani 2006a; Kanayama and Nasukawa 2006) to characterizing the sentiment of documents, such as recognizing inflammatory messages (Spertus 1997), tracking sentiment over time in online discussions (Tong 2001), and classifying the sentiment of online messages (e.g., Das and Chen 2001; Koppel and Schler 2006), customer feedback data (Gamon 2004), or product and movie reviews (e.g., Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, and Pennock 2003; Beineke, Hastie, and Vaithyanathan 2004; Mullen and Collier 2004; Bai, Padman, and Airoldi 2005; Whitelaw, Garg, and Argamon 2005; Kennedy and Inkpen 2006; Koppel and Schler 2006).</S>
    <S sid="490" ssid="25">Identifying prior polarity is a different task than recognizing contextual polarity, although the two tasks are complementary.</S>
    <S sid="491" ssid="26">The goal of identifying prior polarity is to automatically acquire the polarity of words or phrases for listing in a lexicon.</S>
    <S sid="492" ssid="27">Our work on recognizing contextual polarity begins with a lexicon of words with established prior polarities and then disambiguates in the corpus the polarity being expressed by the phrases in which instances of those words appear.</S>
    <S sid="493" ssid="28">To make the relationship between that task and ours clearer, some word lists that are used to evaluate methods for recognizing prior polarity (positive and negative word lists from the General Inquirer [Stone et al. 1966] and lists of positive and negative adjectives created for evaluation by Hatzivassiloglou and McKeown [1997]) are included in the prior-polarity lexicon used in our experiments.</S>
    <S sid="494" ssid="29">For the most part, the features explored in this work differ from the ones used to identify prior polarity with just a few exceptions.</S>
    <S sid="495" ssid="30">Using a feature to capture conjunctions between clue instances was motivated in part by the work of Hatzivassiloglou and McKeown (1997).</S>
    <S sid="496" ssid="31">They use constraints on the co-occurrence in conjunctions of words with similar or opposite polarity to predict the prior polarity of adjectives.</S>
    <S sid="497" ssid="32">Esuli and Sebastiani (2005) consider negation in some of their experiments involving WordNet glosses.</S>
    <S sid="498" ssid="33">Takamura et al. (2005) use negation words and phrases, including phrases such as lack of that are members in our lists of polarity shifters, and conjunctive expressions that they collect from corpora.</S>
    <S sid="499" ssid="34">Esuli and Sebastiani (2006a) is the only work in prior-polarity identification to include a neutral (objective) category and to consider a three-way classification between positive, negative, and neutral words.</S>
    <S sid="500" ssid="35">Although identifying prior polarity is a different task, they report a finding similar to ours, namely, that accuracy is lower when neutral words are included.</S>
    <S sid="501" ssid="36">Some research in sentiment analysis classifies the sentiments of sentences.</S>
    <S sid="502" ssid="37">Morinaga et al. (2002), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004), and Grefenstette et al.</S>
    <S sid="503" ssid="38">(2004)11 all begin by first creating prior-polarity lexicons.</S>
    <S sid="504" ssid="39">Yu and Hatzivassiloglou then assign a sentiment to a sentence by averaging the prior semantic orientations of instances of lexicon words in the sentence.</S>
    <S sid="505" ssid="40">Thus, they do not identify the contextual polarity of individual phrases containing clue instances, which is the focus of this work.</S>
    <S sid="506" ssid="41">Morinaga et al. only consider the positive or negative clue instance in each sentence that is closest to some target reference; Kim and Hovy, Hu and Liu, and Grefenstette et al. multiply or count the prior polarities of clue instances in the sentence.</S>
    <S sid="507" ssid="42">These researchers also consider local negation to reverse polarity, with Morinaga et al. also taking into account the negating effect of words like insufficient.</S>
    <S sid="508" ssid="43">However, they do not use the other types of features that we consider in our experiments.</S>
    <S sid="509" ssid="44">Kaji and Kitsuregawa (2006) take a different approach to recognizing positive and negative sentences.</S>
    <S sid="510" ssid="45">They bootstrap from information easily obtained in &#8220;Pro&#8221; and &#8220;Con&#8221; HTML tables and lists, and from one high-precision linguistic pattern, to automatically construct a large corpus of positive and negative sentences.</S>
    <S sid="511" ssid="46">They then use this corpus to train a naive Bayes sentence classifier.</S>
    <S sid="512" ssid="47">In contrast to our work, sentiment classification in all of this research is restricted to identifying only positive and negative sentences (excluding our both and neutral categories).</S>
    <S sid="513" ssid="48">In addition, only one sentiment is assigned per sentence; our system assigns contextual polarity to individual expressions, which would allow for a sentence to be assigned to multiple sentiment categories.</S>
    <S sid="514" ssid="49">As we saw when exploring the contextual polarity annotations, it is not uncommon for sentences to contain more than one sentiment expression.</S>
    <S sid="515" ssid="50">Classifying the sentiment of documents is a very different task than recognizing the contextual polarity of words and phrases.</S>
    <S sid="516" ssid="51">However, some researchers have reported findings about document-level classification that are similar to our findings about phrase-level classification.</S>
    <S sid="517" ssid="52">Bai et al. (2005) argue that dependencies among key sentiment terms are important for classifying document sentiment.</S>
    <S sid="518" ssid="53">Similarly, we show that features for capturing when clue instances modify each other are important for phrase-level classification, in particular, for identifying positive expressions.</S>
    <S sid="519" ssid="54">Gamon (2004) achieves his best results for document classification using a wide variety of features, including rich linguistic features, such as features that capture constituent structure, features that combine part-of-speech and semantic relations (e.g., sentence subject or negated context), and features that capture tense information.</S>
    <S sid="520" ssid="55">We also achieve our best results for phrase-level classification using a wide variety of features, many of which are linguistically rich.</S>
    <S sid="521" ssid="56">Kennedy and Inkpen (2006) report consistently higher results for document sentiment classification when select polarity influencers, including negators and intensifiers, are included.12 Koppel and Schler (2006) demonstrate the importance of neutral examples for document-level classification.</S>
    <S sid="522" ssid="57">In this work, we show that being able to correctly identify neutral instances is also very important for phraselevel sentiment analysis.</S>
  </SECTION>
  <SECTION title="10." number="11">
    <S sid="523" ssid="1">Being able to determine automatically the contextual polarity of words and phrases is an important problem in sentiment analysis.</S>
    <S sid="524" ssid="2">In the research presented in this article, we tackle this problem and show that it is much more complex than simply determining whether a word or phrase is positive or negative.</S>
    <S sid="525" ssid="3">In our analysis of a corpus with annotations of subjective expressions and their contextual polarity, we find that positive and negative words from a lexicon are used in neutral contexts much more often than they are used in expressions of the opposite polarity.</S>
    <S sid="526" ssid="4">The importance of identifying when contextual polarity is neutral is further revealed in our classification experiments: When neutral instances are excluded, the performance of features for distinguishing between positive and negative polarity greatly improves.</S>
    <S sid="527" ssid="5">A focus of this research is on understanding which features are important for recognizing contextual polarity.</S>
    <S sid="528" ssid="6">We experiment with a wide variety of linguistically motivated features, and we evaluate the performance of these features using several different machine learning algorithms.</S>
    <S sid="529" ssid="7">Features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.</S>
    <S sid="530" ssid="8">For classifying neutral and polar instances, we find that, although some features produce significant improvements over the baseline in terms of polar or neutral recall or precision, it is the combination of features together that is needed to achieve significant improvements in accuracy.</S>
    <S sid="531" ssid="9">For classifying positive and negative contextual polarity, features for capturing negation prove to be the most important.</S>
    <S sid="532" ssid="10">However, we find that features that also perform well are those that capture when a word is (or is not) modifying or being modified by other polarity terms.</S>
    <S sid="533" ssid="11">This suggests that identifying features that represent more complex interdependencies between polarity clues will be an important avenue for future research.</S>
    <S sid="534" ssid="12">Another direction for future work will be to expand our lexicon using existing techniques for acquiring the prior polarity of words and phrases.</S>
    <S sid="535" ssid="13">It follows that a larger lexicon will have a greater coverage of sentiment expressions.</S>
    <S sid="536" ssid="14">However, expanding the lexicon with automatically acquired prior-polarity tags may result in an even greater proportion of neutral instances to contend with.</S>
    <S sid="537" ssid="15">Given the degradation in performance created by the neutral instances, whether expanding the lexicon automatically will result in improved performance for recognizing contextual polarity is an empirical question.</S>
    <S sid="538" ssid="16">Finally, the overall goal of our research is to use phrase-level sentiment analysis in higher-level NLP tasks, such as opinion question answering and summarization.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="12">
    <S sid="539" ssid="1">We would like to thank the anonymous reviewers for their valuable comments and suggestions.</S>
    <S sid="540" ssid="2">This work was supported in part by an Andrew Mellow Predoctoral Fellowship, by the NSF under grant IIS-0208798, by the Advanced Research and Development Activity (ARDA), and by the European IST Programme through the AMIDA Integrated Project FP6-0033812.</S>
  </SECTION>
</PAPER>
