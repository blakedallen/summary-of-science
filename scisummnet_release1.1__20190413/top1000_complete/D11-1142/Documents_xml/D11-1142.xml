<PAPER>
  <S sid="0">Identifying Relations for Open Information Extraction</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary.</S>
    <S sid="2" ssid="2">This paper shows that the output of state-ofthe-art Open IE systems is rife with uninformative and incoherent extractions.</S>
    <S sid="3" ssid="3">To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs.</S>
    <S sid="4" ssid="4">We imthe constraints in the Open IE system, which more than doubles the area under the precision-recall curve relative previous extractors such as More than are at precision higher&#8212; compared to virtually none for earlier systems.</S>
    <S sid="5" ssid="5">The paper concludes with a detailed analysis</S>
  </ABSTRACT>
  <SECTION title="1 Introduction and Motivation" number="1">
    <S sid="6" ssid="1">Typically, Information Extraction (IE) systems learn an extractor for each target relation from labeled training examples (Kim and Moldovan, 1993; Riloff, 1996; Soderland, 1999).</S>
    <S sid="7" ssid="2">This approach to IE does not scale to corpora where the number of target relations is very large, or where the target relations cannot be specified in advance.</S>
    <S sid="8" ssid="3">Open IE solves this problem by identifying relation phrases&#8212;phrases that denote relations in English sentences (Banko et al., 2007).</S>
    <S sid="9" ssid="4">The automatic identification of relation phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary.</S>
    <S sid="10" ssid="5">Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere.</S>
    <S sid="11" ssid="6">(Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009).</S>
    <S sid="12" ssid="7">The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011).</S>
    <S sid="13" ssid="8">In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010).</S>
    <S sid="14" ssid="9">We have observed that two types of errors are frequent in the output of Open IE systems such as TEXTRUNNER and WOE: incoherent extractions and uninformative extractions.</S>
    <S sid="15" ssid="10">Incoherent extractions are cases where the extracted relation phrase has no meaningful interpretation (see Table 1 for examples).</S>
    <S sid="16" ssid="11">Incoherent extractions arise because the learned extractor makes a sequence of decisions about whether to include each word in the relation phrase, often resulting in incomprehensible predictions.</S>
    <S sid="17" ssid="12">To solve this problem, we introduce a syntactic constraint: every multi-word relation phrase must begin with a verb, end with a preposition, and be a contiguous sequence of words in the sentence.</S>
    <S sid="18" ssid="13">Thus, the identification of a relation phrase is made in one fell swoop instead of on the basis of multiple, word-by-word decisions.</S>
    <S sid="19" ssid="14">Uninformative extractions are extractions that omit critical information.</S>
    <S sid="20" ssid="15">For example, consider the sentence &#8220;Faust made a deal with the devil.&#8221; Previous Open IE systems return the uninformative (Faust, made, a deal) instead of (Faust, made a deal with, the devil).</S>
    <S sid="21" ssid="16">This type of error is caused by improper handling of relation phrases that are expressed by a combination of a verb with a noun, such as light verb constructions (LVCs).</S>
    <S sid="22" ssid="17">An LVC is a multi-word expression composed of a verb and a noun, with the noun carrying the semantic content of the predicate (Grefenstette and Teufel, 1995; Stevenson et al., 2004; Allerton, 2002).</S>
    <S sid="23" ssid="18">Table 2 illustrates the wide range of relations expressed this way, which are not captured by existing open extractors.</S>
    <S sid="24" ssid="19">Our syntactic constraint leads the extractor to include nouns in the relation phrase, solving this problem.</S>
    <S sid="25" ssid="20">Although the syntactic constraint significantly reduces incoherent and uninformative extractions, it allows overly-specific relation phrases such as is offering only modest greenhouse gas reduction targets at.</S>
    <S sid="26" ssid="21">To avoid overly-specific relation phrases, we introduce an intuitive lexical constraint: a binary relation phrase ought to appear with at least a minimal number of distinct argument pairs in a large corpus.</S>
    <S sid="27" ssid="22">In summary, this paper articulates two simple but surprisingly powerful constraints on how binary relationships are expressed via verbs in English sentences, and implements them in the REVERB Open IE system.</S>
    <S sid="28" ssid="23">We release REVERB and the data used in our experiments to the research community.</S>
    <S sid="29" ssid="24">The rest of the paper is organized as follows.</S>
    <S sid="30" ssid="25">Section 2 analyzes previous work.</S>
    <S sid="31" ssid="26">Section 3 defines our constraints precisely.</S>
    <S sid="32" ssid="27">Section 4 describes REVERB, our implementation of the constraints.</S>
    <S sid="33" ssid="28">Section 5 reports on our experimental results.</S>
    <S sid="34" ssid="29">Section 6 concludes with a summary and discussion of future work.</S>
  </SECTION>
  <SECTION title="2 Previous Work" number="2">
    <S sid="35" ssid="1">Open IE systems like TEXTRUNNER (Banko et al., 2007), WOEPOs, and WOEParse (Wu and Weld, 2010) focus on extracting binary relations of the form (arg1, relation phrase, arg2) from text.</S>
    <S sid="36" ssid="2">These systems all use the following three-step method: put, identifies a candidate pair of NP arguments (arg1, arg2) from the sentence, and then uses the learned extractor to label each word between the two arguments as part of the relation phrase or not.</S>
    <S sid="37" ssid="3">The extractor is applied to the successive sentences in the corpus, and the resulting extractions are collected.</S>
    <S sid="38" ssid="4">This method faces several challenges.</S>
    <S sid="39" ssid="5">First, the training phase requires a large number of labeled training examples (e.g., 200, 000 heuristicallylabeled sentences for TEXTRUNNER and 300, 000 for WOE).</S>
    <S sid="40" ssid="6">Heuristic labeling of examples obviates hand labeling but results in noisy labels and distorts the distribution of examples.</S>
    <S sid="41" ssid="7">Second, the extraction step is posed as a sequence-labeling problem, where each word is assigned its own label.</S>
    <S sid="42" ssid="8">Because each assignment is uncertain, the likelihood that the extracted relation phrase is flawed increases with the length of the sequence.</S>
    <S sid="43" ssid="9">Finally, the extractor chooses an extraction&#8217;s arguments heuristically, and cannot backtrack over this choice.</S>
    <S sid="44" ssid="10">This is problematic when a word that belongs in the relation phrase is chosen as an argument (for example, deal from the &#8220;made a deal with&#8221; sentence).</S>
    <S sid="45" ssid="11">Because of the feature sets utilized in previous work, the learned extractors ignore both &#8220;holistic&#8221; aspects of the relation phrase (e.g., is it contiguous?) as well as lexical aspects (e.g., how many instances of this relation are there?).</S>
    <S sid="46" ssid="12">Thus, as we show in Section 5, systems such as TEXTRUNNER are unable to learn the constraints embedded in REVERB.</S>
    <S sid="47" ssid="13">Of course, a learning system, utilizing a different hypothesis space, and an appropriate set of training examples, could potentially learn and refine the constraints in REVERB.</S>
    <S sid="48" ssid="14">This is a topic for future work, which we consider in Section 6.</S>
    <S sid="49" ssid="15">The first Open IE system was TEXTRUNNER (Banko et al., 2007), which used a Naive Bayes model with unlexicalized POS and NP-chunk features, trained using examples heuristically generated from the Penn Treebank.</S>
    <S sid="50" ssid="16">Subsequent work showed that utilizing a linear-chain CRF (Banko and Etzioni, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction.</S>
    <S sid="51" ssid="17">The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over TEXTRUNNER (Wu and Weld, 2010).</S>
    <S sid="52" ssid="18">Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed.</S>
    <S sid="53" ssid="19">Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010).</S>
    <S sid="54" ssid="20">Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE.</S>
    <S sid="55" ssid="21">Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors.</S>
    <S sid="56" ssid="22">While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology.</S>
    <S sid="57" ssid="23">Many systems have used syntactic patterns based on verbs to extract relation phrases, usually relying on a full dependency parse of the input sentence (Lin and Pantel, 2001; Stevenson, 2004; Specia and Motta, 2006; Kathrin Eichler and Neumann, 2008).</S>
    <S sid="58" ssid="24">Our work differs from these approaches by focusing on relation phrase patterns expressed in terms of POS tags and NP chunks, instead of full parse trees.</S>
    <S sid="59" ssid="25">Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor.</S>
    <S sid="60" ssid="26">This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3).</S>
    <S sid="61" ssid="27">Further, while previous work in Open IE has mainly focused on syntactic patterns for relation extraction, we introduce a lexical constraint that boosts precision and recall.</S>
    <S sid="62" ssid="28">Finally, Open IE is closely related to semantic role labeling (SRL) (Punyakanok et al., 2008; Toutanova et al., 2008) in that both tasks extract relations and arguments from sentences.</S>
    <S sid="63" ssid="29">However, SRL systems traditionally rely on syntactic parsers, which makes them susceptible to parser errors and substantially slower than Open IE systems such as REVERB.</S>
    <S sid="64" ssid="30">This difference is particularly important when operating on the Web corpus due to its size and heterogeneity.</S>
    <S sid="65" ssid="31">Finally, SRL requires hand-constructed semantic resources like Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) as input.</S>
    <S sid="66" ssid="32">In contrast, Open IE systems require no relation-specific training data.</S>
    <S sid="67" ssid="33">ReVerb, in particular, relies on its explicit lexical and syntactic constraints, which have no correlate in SRL systems.</S>
    <S sid="68" ssid="34">For a more detailed comparison of SRL and Open IE, see (Christensen et al., 2010).</S>
  </SECTION>
  <SECTION title="3 Constraints on Relation Phrases" number="3">
    <S sid="69" ssid="1">In this section we introduce two constraints on relation phrases: a syntactic constraint and a lexical constraint.</S>
    <S sid="70" ssid="2">The syntactic constraint serves two purposes.</S>
    <S sid="71" ssid="3">First, it eliminates incoherent extractions, and second, it reduces uninformative extractions by capturing relation phrases expressed by a verb-noun combination, including light verb constructions. few possible instances, even in a Web-scale corpus.</S>
    <S sid="72" ssid="4">Consider the sentence: The Obama administration is offering only modest greenhouse gas reduction targets at the conference.</S>
    <S sid="73" ssid="5">The POS pattern will match the phrase: is offering only modest greenhouse gas reduction targets at (1) Thus, there are phrases that satisfy the syntactic constraint, but are not relational.</S>
    <S sid="74" ssid="6">To overcome this limitation, we introduce a lexical constraint that is used to separate valid relation phrases from overspecified relation phrases, like the example in (1).</S>
    <S sid="75" ssid="7">The constraint is based on the intuition that a valid relation phrase should take many distinct arguments in a large corpus.</S>
    <S sid="76" ssid="8">The phrase in (1) is specific to the argument pair (Obama administration, conference), so it is unlikely to represent a bona fide relation.</S>
    <S sid="77" ssid="9">We describe the implementation details of the lexical constraint in Section 4.</S>
    <S sid="78" ssid="10">3.3 Limitations Our constraints represent an idealized model of relation phrases in English.</S>
    <S sid="79" ssid="11">This raises the question: How much recall is lost due to the constraints?</S>
    <S sid="80" ssid="12">To address this question, we analyzed Wu and Weld&#8217;s set of 300 sentences from a set of random Web pages, manually identifying all verb-based relationships between noun phrase pairs.</S>
    <S sid="81" ssid="13">This resulted in a set of 327 relation phrases.</S>
    <S sid="82" ssid="14">For each relation phrase, we checked whether it satisfies our constraints.</S>
    <S sid="83" ssid="15">We found that 85% of the relation phrases do satisfy the constraints.</S>
    <S sid="84" ssid="16">Of the remaining 15%, we identified some of the common cases where the constraints were violated, summarized in Table 3.</S>
    <S sid="85" ssid="17">Many of the example relation phrases shown in Table 3 involve long-range dependencies between words in the sentence.</S>
    <S sid="86" ssid="18">These types of dependencies are not easily representable using a pattern over POS tags.</S>
    <S sid="87" ssid="19">A deeper syntactic analysis of the input sentence would provide a much more general language for modeling relation phrases.</S>
    <S sid="88" ssid="20">For example, one could create a model of relations expressed in Figure 1: A simple part-of-speech-based regular expression reduces the number of incoherent extractions like was central torpedo and covers relations expressed via light verb constructions like gave a talk at.</S>
    <S sid="89" ssid="21">The syntactic constraint requires the relation phrase to match the POS tag pattern shown in Figure 1.</S>
    <S sid="90" ssid="22">The pattern limits relation phrases to be either a verb (e.g., invented), a verb followed immediately by a preposition (e.g., located in), or a verb followed by nouns, adjectives, or adverbs ending in a preposition (e.g., has atomic weight of).</S>
    <S sid="91" ssid="23">If there are multiple possible matches in a sentence for a single verb, the longest possible match is chosen.</S>
    <S sid="92" ssid="24">Finally, if the pattern matches multiple adjacent sequences, we merge them into a single relation phrase (e.g., wants to extend).</S>
    <S sid="93" ssid="25">This refinement enables the model to readily handle relation phrases containing multiple verbs.</S>
    <S sid="94" ssid="26">A consequence of this pattern is that the relation phrase must be a contiguous span of words in the sentence.</S>
    <S sid="95" ssid="27">The syntactic constraint eliminates the incoherent relation phrases returned by existing systems.</S>
    <S sid="96" ssid="28">For example, given the sentence Extendicare agreed to buy Arbor Health Care for about US $432 million in cash and assumed debt.</S>
    <S sid="97" ssid="29">TEXTRUNNER returns the extraction (Arbor Health Care, for assumed, debt).</S>
    <S sid="98" ssid="30">The phrase for assumed is clearly not a valid relation phrase: it begins with a preposition and splices together two distant words in the sentence.</S>
    <S sid="99" ssid="31">The syntactic constraint prevents this type of error by simply restricting relation phrases to match the pattern in Figure 1.</S>
    <S sid="100" ssid="32">The syntactic constraint reduces uninformative extractions by capturing relation phrases expressed via LVCs.</S>
    <S sid="101" ssid="33">For example, the POS pattern matched against the sentence &#8220;Faust made a deal with the Devil,&#8221; would result in the relation phrase made a deal with, instead of the uninformative made.</S>
    <S sid="102" ssid="34">Finally, we require the relation phrase to appear between its two arguments in the sentence.</S>
    <S sid="103" ssid="35">This is a common constraint that has been implicitly enforced in other open extractors. terms of dependency parse features that would capture the non-contiguous relation phrases in Table 3.</S>
    <S sid="104" ssid="36">Previous work has shown that dependency paths do indeed boost the recall of relation extraction systems (Wu and Weld, 2010; Mintz et al., 2009).</S>
    <S sid="105" ssid="37">While using dependency path features allows for a more flexible model of relations, it significantly increases processing time, which is problematic for Web-scale extraction.</S>
    <S sid="106" ssid="38">Further, we have found that this increased recall comes at the cost of lower precision on Web text (see Section 5).</S>
    <S sid="107" ssid="39">The results in Table 3 are similar to Banko and Etzioni&#8217;s findings that a set of eight POS patterns cover a large fraction of binary verbal relation phrases.</S>
    <S sid="108" ssid="40">However, their analysis was based on a set of sentences known to contain either a company acquisition or birthplace relationship, while our results are on a random sample of Web sentences.</S>
    <S sid="109" ssid="41">We applied Banko and Etzioni&#8217;s verbal patterns to our random sample of 300 Web sentences, and found that they cover approximately 69% of the relation phrases in the corpus.</S>
    <S sid="110" ssid="42">The gap in recall between this and the 85% shown in Table 3 is largely due to LVC relation phrases (made a deal with) and phrases containing multiple verbs (refuses to return to), which their patterns do not cover.</S>
    <S sid="111" ssid="43">In sum, our model is by no means complete.</S>
    <S sid="112" ssid="44">However, we have empirically shown that the majority of binary verbal relation phrases in a sample of Web sentences are captured by our model.</S>
    <S sid="113" ssid="45">By focusing on this subset of language, our model can be used to perform Open IE at significantly higher precision than before.</S>
  </SECTION>
  <SECTION title="4 REVERB" number="4">
    <S sid="114" ssid="1">This section introduces REVERB, a novel open extractor based on the constraints defined in the previous section.</S>
    <S sid="115" ssid="2">REVERB first identifies relation phrases that satisfy the syntactic and lexical constraints, and then finds a pair of NP arguments for each identified relation phrase.</S>
    <S sid="116" ssid="3">The resulting extractions are then assigned a confidence score using a logistic regression classifier.</S>
    <S sid="117" ssid="4">This algorithm differs in three important ways from previous methods (Section 2).</S>
    <S sid="118" ssid="5">First, the relation phrase is identified &#8220;holistically&#8221; rather than word-by-word.</S>
    <S sid="119" ssid="6">Second, potential phrases are filtered based on statistics over a large corpus (the implementation of our lexical constraint).</S>
    <S sid="120" ssid="7">Finally, REVERB is &#8220;relation first&#8221; rather than &#8220;arguments first&#8221;, which enables it to avoid a common error made by previous methods&#8212;confusing a noun in the relation phrase for an argument, e.g. the noun deal in made a deal with.</S>
    <S sid="121" ssid="8">REVERB takes as input a POS-tagged and NPchunked sentence and returns a set of (x, r, y) extraction triples.2 Given an input sentence s, REVERB uses the following extraction algorithm: We check whether a candidate relation phrase r&#8222; satisfies the syntactic constraint by matching it against the regular expression in Figure 1.</S>
    <S sid="122" ssid="9">To determine whether r&#8222; satisfies the lexical constraint, we use a large dictionary D of relation phrases that are known to take many distinct arguments.</S>
    <S sid="123" ssid="10">In an offline step, we construct D by finding all matches of the POS pattern in a corpus of 500 million Web sentences.</S>
    <S sid="124" ssid="11">For each matching relation phrase, we heuristically identify its arguments (as in Step 2 above).</S>
    <S sid="125" ssid="12">We set D to be the set of all relation phrases that take at least k distinct argument pairs in the set of extractions.</S>
    <S sid="126" ssid="13">In order to allow for minor variations in relation phrases, we normalize each relation phrase by removing inflection, auxiliary verbs, adjectives, and adverbs.</S>
    <S sid="127" ssid="14">Based on experiments on a held-out set of sentences, we found that a value of k = 20 works well for filtering out overspecified relations.</S>
    <S sid="128" ssid="15">This results in a set of approximately 1.7 million distinct normalized relation phrases, which are stored in memory at extraction time.</S>
    <S sid="129" ssid="16">As an example of the extraction algorithm in action, consider the following input sentence: Hudson was born in Hampstead, which is a suburb of London.</S>
    <S sid="130" ssid="17">Step 1 of the algorithm identifies three relation phrases that satisfy the syntactic and lexical constraints: was, born in, and is a suburb of.</S>
    <S sid="131" ssid="18">The first two phrases are adjacent in the sentence, so they are merged into the single relation phrase was born in.</S>
    <S sid="132" ssid="19">Step 2 then finds an argument pair for each relation phrase.</S>
    <S sid="133" ssid="20">For was born in, the nearest NPs are (Hudson, Hampstead).</S>
    <S sid="134" ssid="21">For is a suburb of, the extractor skips over the NP which and chooses the argument pair (Hampstead, London).</S>
    <S sid="135" ssid="22">The final output is The extraction algorithm in the previous section has high recall, but low precision.</S>
    <S sid="136" ssid="23">Like with previous open extractors, we want way to trade recall for precision by tuning a confidence threshold.</S>
    <S sid="137" ssid="24">We use a logistic regression classifier to assign a confidence score to each extraction, which uses the features shown in Table 4.</S>
    <S sid="138" ssid="25">All of these features are efficiently computable and relation independent.</S>
    <S sid="139" ssid="26">We trained the confidence function by manually labeling the extractions from a set of 1, 000 sentences from the Web and Wikipedia as correct or incorrect.</S>
    <S sid="140" ssid="27">Previous open extractors require labeled training data to learn a model of relations, which is then used to extract relation phrases from text.</S>
    <S sid="141" ssid="28">In contrast, REVERB uses a specified model of relations for extraction, and requires labeled data only for assigning confidence scores to its extractions.</S>
    <S sid="142" ssid="29">Learning a confidence function is a much simpler task than learning a full model of relations, using two orders of magnitude fewer training examples than TEXTRUNNER or WOE.</S>
    <S sid="143" ssid="30">The model of relation phrases used by REVERB is specified, but could a TEXTRUNNER-like system learn this model from training data?</S>
    <S sid="144" ssid="31">While it is difficult to answer such a question for all possible permutations of features sets, training examples, and learning biases, we demonstrate that TEXTRUNNER itself cannot learn REVERB&#8217;s model even when re-trained using the output of REVERB as labeled training data.</S>
    <S sid="145" ssid="32">The resulting system, TEXTRUNNER-R, uses the same feature representation as TEXTRUNNER, but different parameters, and a different set of training examples.</S>
    <S sid="146" ssid="33">To generate positive instances, we ran REVERB on the Penn Treebank, which is the same dataset that TEXTRUNNER is trained on.</S>
    <S sid="147" ssid="34">To generate negative instances from a sentence, we took each noun phrase pair in the sentence that does not appear as arguments in a REVERB extraction.</S>
    <S sid="148" ssid="35">This process resulted in a set of 67, 562 positive instances, and 356,834 negative instances.</S>
    <S sid="149" ssid="36">We then passed these labeled examples to TEXTRUNNER&#8217;s training procedure, which learns a linear-chain CRF using closedclass features like POS tags, capitalization, punctuation, etc.TEXTRUNNER-R uses the argument-first extraction algorithm described in Section 2.</S>
  </SECTION>
  <SECTION title="5 Experiments" number="5">
    <S sid="150" ssid="1">We compare REVERB to the following systems: Each system is given a set of sentences as input, and returns a set of binary extractions as output.</S>
    <S sid="151" ssid="2">We created a test set of 500 sentences sampled from the Web, using Yahoo&#8217;s random link service.3 After running each extractor over the input sentences, two human judges independently evaluated each extraction as correct or incorrect.</S>
    <S sid="152" ssid="3">The judges reached agreement on 86% of the extractions, with an agreement score of n = 0.68.</S>
    <S sid="153" ssid="4">We report results on the subset of the data where the two judges concur.</S>
    <S sid="154" ssid="5">The judges labeled uninformative extractions conservatively.</S>
    <S sid="155" ssid="6">That is, if critical information was dropped from the relation phrase but included in the second argument, it is labeled correct.</S>
    <S sid="156" ssid="7">For example, both the extractions (Ackerman, is a professor of, biology) and (Ackerman, is, a professor of biology) are considered correct.</S>
    <S sid="157" ssid="8">Each system returns confidence scores for its extractions.</S>
    <S sid="158" ssid="9">For a given threshold, we can measure the precision and recall of the output.</S>
    <S sid="159" ssid="10">Precision is the fraction of returned extractions that are correct.</S>
    <S sid="160" ssid="11">Recall is the fraction of correct extractions in the corpus that are returned.</S>
    <S sid="161" ssid="12">We use the total number of extractions labeled as correct by the judges as our measure of recall for the corpus.</S>
    <S sid="162" ssid="13">In order to avoid double-counting, we treat extractions that differ superficially (e.g., different punctuation or dropping inessential modifiers) as a single extraction.</S>
    <S sid="163" ssid="14">We compute a precision-recall curve by varying the confidence threshold, and then compute the area under the curve (AUC).</S>
    <S sid="164" ssid="15">Figure 2 shows the AUC of each system.</S>
    <S sid="165" ssid="16">REVERB achieves an AUC that is 30% higher than WOEparse and is more than double the AUC of WOEpOs or TEXTRUNNER.</S>
    <S sid="166" ssid="17">The lexical constraint provides a significant boost in performance, with REVERB achieving an AUC 23% higher than REVERB&#8212;Lex.</S>
    <S sid="167" ssid="18">REVERB proves to be a useful source of training data, with TEXTRUNNER-R having an AUC 71% higher than TEXTRUNNER and performing on par with WOEpOs.</S>
    <S sid="168" ssid="19">From the training data, TEXTRUNNER-R was able to learn a model that predicts contiguous relation phrases, but still returned incoherent relation phrases (e.g., starting with a preposition) and overspecified relation phrases.</S>
    <S sid="169" ssid="20">These errors are due to TEXTRUNNER-R overfitting the training data and not having access to the lexical constraint.</S>
    <S sid="170" ssid="21">Figure 3 shows the precision-recall curves of the systems introduced in this paper.</S>
    <S sid="171" ssid="22">TEXTRUNNER-R has much lower precision than REVERB and REVERB&#8212;Lex at all levels of recall.</S>
    <S sid="172" ssid="23">The lexical constraint gives REVERB a boost in precision over REVERB&#8212;Lex, reducing overspecified extractions from 20% of REVERB&#8212;Lex&#8217;s output to 1% of REVERB&#8217;s.</S>
    <S sid="173" ssid="24">The lexical constraint also boosts recall over REVERB&#8212;Lex, since REVERB is able to find a correct relation phrase where REVERB&#8212;Lex finds an overspecified one.</S>
    <S sid="174" ssid="25">Figure 4 shows the precision-recall curves of REVERB and the external systems.</S>
    <S sid="175" ssid="26">REVERB has much higher precision than the other systems at nearly all levels of recall.</S>
    <S sid="176" ssid="27">In particular, more than 30% of REVERB&#8217;s extractions are at precision 0.8 or higher, compared to virtually none for the other systems.</S>
    <S sid="177" ssid="28">WOEparse achieves a slightly higher recall than REVERB (0.62 versus 0.64), but at the cost of lower precision.</S>
    <S sid="178" ssid="29">In order to highlight the role of the relational model of each system, we also evaluate their performance on the subtask of extracting just the relation phrases from the input text.</S>
    <S sid="179" ssid="30">Figure 5 shows the precision-recall curves for each system on the relation phrase-only evaluation.</S>
    <S sid="180" ssid="31">In this case, REVERB has both higher precision and recall than the other systems.</S>
    <S sid="181" ssid="32">REVERB&#8217;s biggest improvement came from the elimination of incoherent extractions.</S>
    <S sid="182" ssid="33">Incoherent extractions were a large fraction of the errors made by previous systems, accounting for approximately 13% of TEXTRUNNER&#8217;s extractions, 15% of WOEpOs&#8217;s, and 30% of WOEparse&#8217;s.</S>
    <S sid="183" ssid="34">Uninformative tion. extractions had a smaller effect on other systems&#8217; precision, accounting for 4% of WOEp&amp;quot;3&#65533;&#8217;s extractions, 5% of WOEpo3&#8217;s, and 7% of TEXTRUNNER&#8217;s, while only appearing in 1% of REVERB&#8217;s extractions.</S>
    <S sid="184" ssid="35">REVERB&#8217;s reduction in uninformative extractions resulted in a boost in recall, capturing many LVC relation phrases missed by other systems (like those shown in Table 2).</S>
    <S sid="185" ssid="36">To test the systems&#8217; speed, we ran each extractor on a set of 100, 000 sentences using a Pentium 4 machine with 4GB of RAM.</S>
    <S sid="186" ssid="37">The processing times were 16 minutes for REVERB, 21 minutes for TEXTRUNNER, 21 minutes for WOEpo3, and 11 hours for WOEp&#65533;&amp;quot;'.</S>
    <S sid="187" ssid="38">The times for REVERB, TEXTRUNNER, and WOEpo3 are all approximately the same, since they all use the same POS-tagging and NP-chunking software.</S>
    <S sid="188" ssid="39">WOEp&#65533;.3. processes each sentence with a dependency parser, resulting in much longer processing time.</S>
    <S sid="189" ssid="40">5.2 REVERB Error Analysis To better understand the limitations of REVERB, we performed a detailed analysis of its errors in precision (incorrect extractions returned by REVERB) and its errors in recall (correct extractions that REVERB missed).</S>
    <S sid="190" ssid="41">Table 5 summarizes the types of incorrect extractions that REVERB returns.</S>
    <S sid="191" ssid="42">We found that 65% of the incorrect extractions returned by REVERB were cases where a relation phrase was correctly identified, but the argument-finding heuristics failed.</S>
    <S sid="192" ssid="43">The remaining errors were cases where REVERB extracted an incorrect relation phrase.</S>
    <S sid="193" ssid="44">One common mistake that REVERB made was extracting a relation phrase that expresses an n-ary relationship via a ditransitive verb.</S>
    <S sid="194" ssid="45">For example, given the sentence Table 6: The majority of extractions that were missed by REVERB were cases where the correct relation phrase was found, but the arguments were not correctly identified.</S>
    <S sid="195" ssid="46">&#8220;I gave him 15 photographs,&#8221; REVERB extracts (I, gave, him).</S>
    <S sid="196" ssid="47">These errors are due to the fact that REVERB only models binary relations.</S>
    <S sid="197" ssid="48">Table 6 summarizes the correct extractions that were extracted by other systems and were not extracted by REVERB.</S>
    <S sid="198" ssid="49">As with the false positive extractions, the majority of false negatives (52%) were due to the argument-finding heuristics choosing the wrong arguments, or failing to extract all possible arguments (in the case of coordinating conjunctions).</S>
    <S sid="199" ssid="50">Other sources of failure were due to the lexical constraint either failing to filter out an overspecified relation phrase or filtering out a valid relation phrase.</S>
    <S sid="200" ssid="51">These errors hurt both precision and recall, since each case results in the extractor overlooking a correct relation phrase and choosing another.</S>
    <S sid="201" ssid="52">5.3 Evaluation At Scale Section 5.1 shows that REVERB outperforms existing Open IE systems when evaluated on a sample of sentences.</S>
    <S sid="202" ssid="53">Previous work has shown that the frequency of an extraction in a large corpus is useful for assessing the correctness of extractions (Downey et al., 2005).</S>
    <S sid="203" ssid="54">Thus, it is possible a priori that REVERB&#8217;s gains over previous systems will diminish when extraction frequency is taken into account.</S>
    <S sid="204" ssid="55">In fact, we found that REVERB&#8217;s advantage over TEXTRUNNER when run at scale is qualitatively similar to its advantage on single sentences.</S>
    <S sid="205" ssid="56">We ran both REVERB and TEXTRUNNER on Banko and Etzioni&#8217;s corpus of 500 million Web sentences and examined the effect of redundancy on precision.</S>
    <S sid="206" ssid="57">As Downey&#8217;s work predicts, precision increased in both systems for extractions found multiple times, compared with extractions found only once.</S>
    <S sid="207" ssid="58">However, REVERB had higher precision than 1543 TEXTRUNNER at all frequency thresholds.</S>
    <S sid="208" ssid="59">In fact, REVERB&#8217;s frequency 1 extractions had a precision of 0.75, which TEXTRUNNER could not approach even with frequency 10 extractions, which had a precision of 0.34.</S>
    <S sid="209" ssid="60">Thus, REVERB is able to return more correct extractions at a higher precision than TEXTRUNNER, even when redundancy is taken into account.</S>
  </SECTION>
  <SECTION title="6 Conclusions and Future Work" number="6">
    <S sid="210" ssid="1">The paper&#8217;s contributions are as follows: &#8226; We have identified and analyzed the problems of incoherent and uninformative extractions for Open IE systems, and shown their prevalence for systems such as TEXTRUNNER and WOE.</S>
    <S sid="211" ssid="2">&#8226; We articulated general, easy-to-enforce constraints on binary, verb-based relation phrases in English that ameliorate these problems and yield richer and more informative relations (see, for example, Table 2).</S>
    <S sid="212" ssid="3">&#8226; Based on these constraints, we designed, implemented, and evaluated the REVERB extractor, which substantially outperforms previous Open IE systems in both recall and precision.</S>
    <S sid="213" ssid="4">&#8226; We make REVERB and the data used in our experiments available to the research community.4 In future work, we plan to explore utilizing our constraints to improve the performance of learned CRF models.</S>
    <S sid="214" ssid="5">Roth et al. have shown how to incorporate constraints into CRF learners (Roth and Yih, 2005).</S>
    <S sid="215" ssid="6">It is natural, then, to consider whether the combination of heuristically labeled training examples, CRF learning, and our constraints will result in superior performance.</S>
    <S sid="216" ssid="7">The error analysis in Section 5.2 also suggests natural directions for future work.</S>
    <S sid="217" ssid="8">For instance, since many of REVERB&#8217;s errors are due to incorrect arguments, improved methods for argument extraction are in order.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="218" ssid="1">We would like to thank Mausam, Dan Weld, Yoav Artzi, Luke Zettlemoyer, members of the KnowItAll group, and the anonymous reviewers for their helpful comments.</S>
    <S sid="219" ssid="2">This research was supported in part by NSF grant IIS-0803481, ONR grant N00014-081-0431, and DARPA contract FA8750-09-C-0179, and carried out at the University of Washington&#8217;s Turing Center.</S>
  </SECTION>
</PAPER>
