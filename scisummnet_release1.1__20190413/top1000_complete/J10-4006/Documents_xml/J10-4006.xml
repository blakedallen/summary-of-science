<PAPER>
  <S sid="0">Distributional Memory: A General Framework for Corpus-Based Semantics</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus.</S>
    <S sid="2" ssid="2">As an alternative to this &#8220;one task, one model&#8221; approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.</S>
    <S sid="3" ssid="3">Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.</S>
    <S sid="4" ssid="4">In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.</S>
    <S sid="5" ssid="5">Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.</S>
    <S sid="6" ssid="6">The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="7" ssid="1">single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus.</S>
    <S sid="8" ssid="2">As an alternative to this &#8220;one task, one model&#8221; approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.</S>
    <S sid="9" ssid="3">Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.</S>
    <S sid="10" ssid="4">In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.</S>
    <S sid="11" ssid="5">Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.</S>
    <S sid="12" ssid="6">The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="13" ssid="1">The last two decades have seen a rising wave of interest among computational linguists and cognitive scientists in corpus-based models of semantic representation (Grefenstette 1994; Lund and Burgess 1996; Landauer and Dumais 1997; Sch&#168;utze 1997; Sahlgren 2006; Bullinaria and Levy 2007; Griffiths, Steyvers, and Tenenbaum 2007; Pad&#180;o and Lapata 2007; Lenci 2008; Turney and Pantel 2010).</S>
    <S sid="14" ssid="2">These models, variously known as vector spaces, semantic spaces, word spaces, corpus-based semantic models, or, using the term we will adopt, distributional semantic models (DSMs), all rely on some version of the distributional hypothesis (Harris 1954; Miller and Charles 1991), stating that the degree of semantic similarity between two words (or other linguistic units) can be modeled as a function of the degree of overlap among their linguistic contexts.</S>
    <S sid="15" ssid="3">Conversely, the format of distributional representations greatly varies depending on the specific aspects of meaning they are designed to model.</S>
    <S sid="16" ssid="4">The most straightforward phenomenon tackled by DSMs is what Turney (2006b) calls attributional similarity, which encompasses standard taxonomic semantic relations such as synonymy, co-hyponymy, and hypernymy.</S>
    <S sid="17" ssid="5">Words like dog and puppy, for example, are attributionally similar in the sense that their meanings share a large number of attributes: They are animals, they bark, and so on.</S>
    <S sid="18" ssid="6">Attributional similarity is typically addressed by DSMs based on word collocates (Grefenstette 1994; Lund and Burgess 1996; Sch&#168;utze 1997; Bullinaria and Levy 2007; Pad&#180;o and Lapata 2007).</S>
    <S sid="19" ssid="7">These collocates are seen as proxies for various attributes of the concepts that the words denote.</S>
    <S sid="20" ssid="8">Words that share many collocates denote concepts that share many attributes.</S>
    <S sid="21" ssid="9">Both dog and puppy may occur near owner, leash, and bark, because these words denote properties that are shared by dogs and puppies.</S>
    <S sid="22" ssid="10">The attributional similarity between dog and puppy, as approximated by their contextual similarity, will be very high.</S>
    <S sid="23" ssid="11">DSMs succeed in tasks like synonym detection (Landauer and Dumais 1997) or concept categorization (Almuhareb and Poesio 2004) because such tasks require a measure of attributional similarity that favors concepts that share many properties, such as synonyms and co-hyponyms.</S>
    <S sid="24" ssid="12">However, many other tasks require detecting different kinds of semantic similarity.</S>
    <S sid="25" ssid="13">Turney (2006b) defines relational similarity as the property shared by pairs of words (e.g, dog&#8211;animal and car&#8211;vehicle) linked by similar semantic relations (e.g., hypernymy), despite the fact that the words in one pair might not be attributionally similar to those in the other pair (e.g., dog is not attributionally similar to car, nor is animal to vehicle).</S>
    <S sid="26" ssid="14">Turney generalizes DSMs to tackle relational similarity and represents pairs of words in the space of the patterns that connect them in the corpus.</S>
    <S sid="27" ssid="15">Pairs of words that are connected by similar patterns probably hold similar relations, that is, they are relationally similar.</S>
    <S sid="28" ssid="16">For example, we can hypothesize that dog&#8211;tail is more similar to car&#8211;wheel than to dog&#8211;animal, because the patterns connecting dog and tail (of, have, etc.) are more like those of car&#8211;wheel than like those of dog&#8211;animal (is a, such as, etc.).</S>
    <S sid="29" ssid="17">Turney uses the relational space to implement tasks such as solving analogies and harvesting instances of relations.</S>
    <S sid="30" ssid="18">Although they are not explicitly expressed in these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and focus on learning one relation type at a time (e.g., finding parts).</S>
    <S sid="31" ssid="19">Although semantic similarity, either attributional or relational, has the lion&#8217;s share in DSMs, similarity is not the only aspect of meaning that is addressed by distributional approaches.</S>
    <S sid="32" ssid="20">For instance, the notion of property plays a key role in cognitive science and linguistics, which both typically represent concepts as clusters of properties (Jackendoff 1990; Murphy 2002).</S>
    <S sid="33" ssid="21">In this case, the task is not to find out that dog is similar to puppy or cat, but that it has a tail, it is used for hunting, and so on.</S>
    <S sid="34" ssid="22">Almuhareb (2006), Baroni and Lenci (2008), and Baroni et al. (2010) use the words co-occurring with a noun to approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects.</S>
    <S sid="35" ssid="23">Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles (Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction).</S>
    <S sid="36" ssid="24">The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth.</S>
    <S sid="37" ssid="25">Finally, some DSMs capture a sort of &#8220;topical&#8221; relatedness between words: They might find, for example, a relation between dog and fidelity.</S>
    <S sid="38" ssid="26">Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Griffiths, Steyvers, and Tenenbaum 2007), is not further discussed in this article.</S>
    <S sid="39" ssid="27">DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004).</S>
    <S sid="40" ssid="28">Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008).</S>
    <S sid="41" ssid="29">Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010).</S>
    <S sid="42" ssid="30">Innovative applications of corpus-based semantics are also being explored in linguistics, for instance in the study of semantic change (Sagi, Kaufmann, and Clark 2009), lexical variation (Peirsman and Speelman 2009), and for the analysis of multiword expressions (Alishahi and Stevenson 2008).</S>
    <S sid="43" ssid="31">The wealth and variety of semantic issues that DSMs are able to tackle confirms the importance of looking at distributional data to explore meaning, as well as the maturity of this research field.</S>
    <S sid="44" ssid="32">However, if we looked from a distance at the whole field of DSMs we would see that, besides the general assumption shared by all models that information about the context of a word is an important key in grasping its meaning, the elements of difference overcome the commonalities.</S>
    <S sid="45" ssid="33">For instance, DSMs geared towards attributional similarity represent words in the contexts of other (content) words, thereby looking very different from models that represent word pairs in terms of patterns linking them.</S>
    <S sid="46" ssid="34">In turn, both these models differ from those used to explore concept properties or argument alternations.</S>
    <S sid="47" ssid="35">The typical approach in the field has been a local one, in which each semantic task (or set of closely related tasks) is treated as a separate problem, that requires its own corpus-derived model and algorithm, both optimized to achieve the best performance in a given task, but lacking generality, since they resort to task-specific distributional representations, often complemented by additional taskspecific resources.</S>
    <S sid="48" ssid="36">As a consequence, the landscape of DSMs looks more like a jigsaw puzzle in which different parts have been completed and the whole figure starts to emerge from the fragments, but it is not clear yet how to put everything together and compose a coherent picture.</S>
    <S sid="49" ssid="37">We argue that the &#8220;one semantic task, one distributional model&#8221; approach represents a great limit of the current state of the art.</S>
    <S sid="50" ssid="38">From a theoretical perspective, corpusbased models hold promise as large-scale simulations of how humans acquire and use conceptual and linguistic information from their environment (Landauer and Dumais 1997).</S>
    <S sid="51" ssid="39">However, existing DSMs lack exactly the multi-purpose nature that is a hallmark of human semantic competence.</S>
    <S sid="52" ssid="40">The common view in cognitive (neuro)science is that humans resort to a single semantic memory, a relatively stable long-term knowledge database, adapting the information stored there to the various tasks at hand (Murphy 2002; Rogers and McClelland 2004).</S>
    <S sid="53" ssid="41">The fact that DSMs need to go back to their environment (the corpus) to collect ad hoc statistics for each semantic task, and the fact that different aspects of meaning require highly different distributional representations, cast many shadows on the plausibility of DSMs as general models of semantic memory.</S>
    <S sid="54" ssid="42">From a practical perspective, going back to the corpus to train a different model for each application is inefficient, and it runs the risk of overfitting the model to a specific task, while losing sight of its adaptivity&#8212;a highly desirable feature for any intelligent system.</S>
    <S sid="55" ssid="43">Think, by contrast, of WordNet (Fellbaum 1998), a single, general purpose network of semantic information that has been adapted to all sorts of tasks, many of them certainly not envisaged by the resource creators.</S>
    <S sid="56" ssid="44">We think that it is not by chance that no comparable resource has emerged from DSM development.</S>
    <S sid="57" ssid="45">In this article, we want to show that a unified approach is not only a desirable goal, but it is also a feasible one.</S>
    <S sid="58" ssid="46">With this aim in mind, we introduce Distributional Memory (DM), a generalized framework for distributional semantics.</S>
    <S sid="59" ssid="47">Differently from other current proposals that share similar aims, we believe that the lack of generalization in corpus-based semantics stems from the choice of representing co-occurrence statistics directly as matrices&#8212;geometrical objects that model distributional data in terms of binary relations between target items (the matrix rows) and their contexts (the matrix columns).</S>
    <S sid="60" ssid="48">This results in the development of ad hoc models that lose sight of the fact that different semantic spaces actually rely on the same kind of underlying distributional information.</S>
    <S sid="61" ssid="49">DM instead represents corpus-extracted co-occurrences as a third-order tensor, a ternary geometrical object that models distributional data in terms of word&#8211; link&#8211;word tuples.</S>
    <S sid="62" ssid="50">Matrices are then generated from the tensor in order to perform semantic tasks in the spaces they define.</S>
    <S sid="63" ssid="51">Crucially, these on-demand matrices are derived from the same underlying resource (the tensor) and correspond to different &#8220;views&#8221; of the same data, extracted once and for all from a corpus.</S>
    <S sid="64" ssid="52">DM is tested here on what we believe to be the most varied array of semantic tasks ever addressed by a single distributional model.</S>
    <S sid="65" ssid="53">In all cases, we compare the performance of several DM implementations to state-of-the-art results.</S>
    <S sid="66" ssid="54">While some of the ad hoc models that were developed to tackle specific tasks do outperform our most successful DM implementation, the latter is never too far from the top, without any task-specific tuning.</S>
    <S sid="67" ssid="55">We think that the advantage of having a general model that does not need to be retrained for each new task outweighs the (often minor) performance advantage of the task-specific models.</S>
    <S sid="68" ssid="56">The article is structured as follows.</S>
    <S sid="69" ssid="57">After framing our proposal within the general debate on co-occurrence modeling in distributional semantics (Section 2), we introduce the DM framework in Section 3 and compare it to other unified approaches in Section 4.</S>
    <S sid="70" ssid="58">Section 5 pertains to the specific implementations of the DM framework we will test experimentally.</S>
    <S sid="71" ssid="59">The experiments are reported in Section 6.</S>
    <S sid="72" ssid="60">Section 7 concludes by summarizing what we have achieved, and discussing the implications of these results for corpus-based distributional semantics.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="73" ssid="1">Corpus-based semantics aims at characterizing the meaning of linguistic expressions in terms of their distributional properties.</S>
    <S sid="74" ssid="2">The standard view models such properties in terms of two-way structures, that is, matrices coupling target elements (either single words or whatever other linguistic constructions we try to capture distributionally) and contexts.</S>
    <S sid="75" ssid="3">In fact, the formal definition of semantic space provided by Pad&#180;o and Lapata (2007) is built around the notion of a matrix M|B|&#215;|T|, with B the set of basis elements representing the contexts used to compare the distributional similarity of the target elements T. This binary structure is inherently suitable for approaches that represent distributional data in terms of unstructured co-occurrence relations between an element and a context.</S>
    <S sid="76" ssid="4">The latter can be either documents (Landauer and Dumais 1997; Griffiths, Steyvers, and Tenenbaum 2007) or lexical collocates within a certain distance from the target (Lund and Burgess 1996; Sch&#168;utze 1997; Rapp 2003; Bullinaria and Levy 2007).</S>
    <S sid="77" ssid="5">We will refer to such models as unstructured DSMs, because they do not use the linguistic structure of texts to compute co-occurrences, and only record whether the target occurs in or close to the context element, without considering the type of this relation.</S>
    <S sid="78" ssid="6">For instance, an unstructured DSM might derive from a sentence like The teacher eats a red apple that eat is a feature shared by apple and red, just because they appear in the same context window, without considering the fact that there is no real linguistic relation linking eat and red, besides that of linear proximity.</S>
    <S sid="79" ssid="7">In structured DSMs, co-occurrence statistics are collected instead in the form of corpus-derived triples: typically, word pairs and the parser-extracted syntactic relation or lexico-syntactic pattern that links them, under the assumption that the surface connection between two words should cue their semantic relation (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Almuhareb and Poesio 2004; Turney 2006b; Pad&#180;o and Lapata 2007; Erk and Pad&#180;o 2008; Rothenh&#168;ausler and Sch&#168;utze 2009).</S>
    <S sid="80" ssid="8">Distributional triples are also used in computational lexicography to identify the grammatical and collocational behavior of a word and to define its semantic similarity spaces.</S>
    <S sid="81" ssid="9">For instance, the Sketch Engine1 builds &#8220;word sketches&#8221; consisting of triples extracted from parsed corpora and formed by two words linked by a grammatical relation (Kilgarriff et al. 2004).</S>
    <S sid="82" ssid="10">The number of shared triples is then used to measure the attributional similarity between word pairs.</S>
    <S sid="83" ssid="11">Structured models take into account the crucial role played by syntactic structures in shaping the distributional properties of words.</S>
    <S sid="84" ssid="12">To qualify as context of a target item, a word must be linked to it by some (interesting) lexico-syntactic relation, which is also typically used to distinguish the type of this co-occurrence.</S>
    <S sid="85" ssid="13">Given the sentence The teacher eats a red apple, structured DSMs would not consider eat as a legitimate context for red and would distinguish the object relation connecting eat and apple as a different type of co-occurrence from the modifier relation linking red and apple.</S>
    <S sid="86" ssid="14">On the other hand, structured models require more preliminary corpus processing (parsing or extraction of lexico-syntactic patterns), and tend to be more sparse (because there are more triples than pairs).</S>
    <S sid="87" ssid="15">What little systematic comparison of the two approaches has been carried out (Pad&#180;o and Lapata 2007; Rothenh&#168;ausler and Sch&#168;utze 2009) suggests that structured models have a slight edge.</S>
    <S sid="88" ssid="16">In our experiments in Section 6.1 herein, the performance of unstructured and structured models trained on the same corpus is in general comparable.</S>
    <S sid="89" ssid="17">It seems safe to conclude that structured models are at least not worse than unstructured models&#8212;an important conclusion for us, as DM is built upon the structured DSM idea.</S>
    <S sid="90" ssid="18">Structured DSMs extract a much richer array of distributional information from linguistic input, but they still represent it in the same way as unstructured models.</S>
    <S sid="91" ssid="19">The corpus-derived ternary data are mapped directly onto a two-way matrix, either by dropping one element from the tuple (Pad&#180;o and Lapata 2007) or, more commonly, by concatenating two elements.</S>
    <S sid="92" ssid="20">The two words can be concatenated, treating the links as basis elements, in order to model relational similarity (Pantel and Pennacchiotti 2006; Turney 2006b).</S>
    <S sid="93" ssid="21">Alternatively, pairs formed by the link and one word are concatenated as basis elements to measure attributional similarity among the other words, treated as target elements (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Almuhareb and Poesio 2004; Rothenh&#168;ausler and Sch&#168;utze 2009).</S>
    <S sid="94" ssid="22">In this way, typed DSMs obtain finer-grained features to compute distributional similarity, but, by couching distributional information as two-way matrices, they lose the high expressive power of corpusderived triples.</S>
    <S sid="95" ssid="23">We believe that falling short of fully exploiting the potential of ternary distributional structures is the major reason for the lack of unification in corpus-based semantics.</S>
    <S sid="96" ssid="24">The debate in DSMs has so far mostly focused on the context choice&#8212;for example, lexical collocates vs. documents (Sahlgren 2006; Turney and Pantel 2010)&#8212;or on the costs and benefits of having structured contexts (Pad&#180;o and Lapata 2007; Rothenh&#168;ausler and Sch&#168;utze 2009).</S>
    <S sid="97" ssid="25">Although we see the importance of these issues, we believe that a real breakthrough in DSMs can only be achieved by overcoming the limits of current two-way models of distributional data.</S>
    <S sid="98" ssid="26">We propose here the alternative DM approach, in which the core geometrical structure of a distributional model is a three-way object, namely a third-order tensor.</S>
    <S sid="99" ssid="27">As in structured DSMs, we adopt word&#8211;link&#8211;word tuples as the most suitable way to capture distributional facts.</S>
    <S sid="100" ssid="28">However, we extend and generalize this assumption, by proposing that, once they are formalized as a threeway tensor, tuples can become the backbone of a unified model for distributional semantics.</S>
    <S sid="101" ssid="29">Different semantic spaces are then generated on demand through the independently motivated operation of tensor matricization, mapping the third-order tensor onto two-way matrices.</S>
    <S sid="102" ssid="30">The matricization of the tuple tensor produces both familiar spaces, similar to those commonly used for attributional or relational similarity, and other less known distributional spaces, which will yet prove useful for capturing some interesting semantic phenomena.</S>
    <S sid="103" ssid="31">The crucial fact is that all these different semantic spaces are now alternative views of the same underlying distributional object.</S>
    <S sid="104" ssid="32">Apparently unrelated semantic tasks can be addressed in terms of the same distributional memory, harvested only once from the source corpus.</S>
    <S sid="105" ssid="33">Thus, thanks to the tensor-based representation, distributional data can be turned into a general purpose resource for semantic modeling.</S>
    <S sid="106" ssid="34">As a further advantage, the third-order tensor formalization of corpus-based tuples allows distributional information to be represented in a similar way to other types of knowledge.</S>
    <S sid="107" ssid="35">In linguistics, cognitive science, and AI, semantic and conceptual knowledge is represented in terms of symbolic structures built around typed relations between elements, such as synsets, concepts, properties, and so forth.</S>
    <S sid="108" ssid="36">This is customary in lexical networks like WordNet (Fellbaum 1998), commonsense resources like ConceptNet (Liu and Singh 2004), and cognitive models of semantic memory (Rogers and McClelland 2004).</S>
    <S sid="109" ssid="37">The tensor representation of corpus-based distributional data promises to build new bridges between existing approaches to semantic representation that still appear distant in many respects.</S>
    <S sid="110" ssid="38">This may indeed contribute to the ongoing efforts to combine distributional and symbolic approaches to meaning (Clark and Pulman 2007).</S>
  </SECTION>
  <SECTION title="3." number="4">
    <S sid="111" ssid="1">We first introduce the notion of a weighted tuple structure, the format in which DM expects the distributional data extracted from the corpus to be arranged (and that it shares with traditional structured DSMs).</S>
    <S sid="112" ssid="2">We then show how a weighted tuple structure can be represented, in linear algebraic terms, as a labeled third-order tensor.</S>
    <S sid="113" ssid="3">Finally, we derive different semantic vector spaces from the tensor by the operation of labeled tensor matricization.</S>
    <S sid="114" ssid="4">Relations among entities can be represented by ternary tuples, or triples.</S>
    <S sid="115" ssid="5">Let O1 and O2 be two sets of objects, and R &#8838; O1 x O2 a set of relations between these objects.</S>
    <S sid="116" ssid="6">A triple (o1, r, o2) expresses the fact that o1 is linked to o2 through the relation r. DM (like previous structured DSMs) includes tuples of a particular type, namely, weighted distributional tuples that encode distributional facts in terms of typed co-occurrence relations among words.</S>
    <S sid="117" ssid="7">Let W1 and W2 be sets of strings representing content words, and L a set of strings representing syntagmatic co-occurrence links between words in a text.</S>
    <S sid="118" ssid="8">T C_ W1 x L x W2 is a set of corpus-derived tuples t = (w1, l, w2), such that w1 co-occurs with w2 and l represents the type of this co-occurrence relation.</S>
    <S sid="119" ssid="9">For instance, the tuple (marine, use, bomb) in the toy example reported in Table 1 encodes the piece of distributional information that marine co-occurs with bomb in the corpus, and use specifies the type of the syntagmatic link between these words.</S>
    <S sid="120" ssid="10">Each tuple t has a weight, a real-valued score vt, assigned by a scoring function 6 : W1 x L x W2 - 4R.</S>
    <S sid="121" ssid="11">A weighted tuple structure consists of the set TW of weighted distributional tuples tw = (t,vt) for all t E T and 6(t) = vt.</S>
    <S sid="122" ssid="12">The 6 function encapsulates all the operations performed to score the tuples, for example, by processing an input corpus with a dependency parser, counting the occurrences of tuples, and weighting the raw counts by mutual information.</S>
    <S sid="123" ssid="13">Because our focus is on how tuples, once they are harvested, should be represented geometrically, we gloss over the important challenges of choosing the appropriate W1, L and W2 string sets, as well as specifying 6.</S>
    <S sid="124" ssid="14">In this article, we make the further assumption that W1 = W2.</S>
    <S sid="125" ssid="15">This is a natural assumption when the tuples represent (link-mediated) co-occurrences of word pairs.</S>
    <S sid="126" ssid="16">Moreover, we enforce an inverse link constraint such that for any link l in L, there is a k in L such that for each tuple tw = ((wi, l, wj), vt) in the weighted tuple structure TW, the tuple t&#8722;1 w = ((wj, k, wi), vt) is also in TW (we call k the inverse link of l).</S>
    <S sid="127" ssid="17">Again, this seems reasonable in our context: If we extract a tuple (marine, use, gun) and assign it a certain score, we might as well add the tuple (gun, use&#8722;1, marine) with the same score.</S>
    <S sid="128" ssid="18">The two assumptions, combined, lead the matricization process described in Section 3.3 to generate exactly four distinct vector spaces that, as we discuss there, are needed for the semantic analyses we conduct.</S>
    <S sid="129" ssid="19">See Section 6.6 of Turney (2006b) for a discussion of similar assumptions.</S>
    <S sid="130" ssid="20">Still, it is worth emphasizing that the general formalism we are proposing, where corpus-extracted weighted tuple structures are represented as labeled tensors, does not strictly require these assumptions.</S>
    <S sid="131" ssid="21">For example, W2 could be a larger set of &#8220;relata&#8221; including not only words, but also documents, morphological features, or even visual features (with appropriate links, such as, for word-document relations, occurs-at-the-beginning-of).</S>
    <S sid="132" ssid="22">The inverse link constraint might not be appropriate, for example, if we use an asymmetric association measure, or if we are only interested in one direction of certain grammatical relations.</S>
    <S sid="133" ssid="23">We leave the investigation of all these possibilities to further studies.</S>
    <S sid="134" ssid="24">A toy weighted tuple structure. word link word weight word link word weight marine own bomb 40.0 sergeant use gun 51.9 marine use bomb 82.1 sergeant own book 8.0 marine own gun 85.3 sergeant use book 10.1 marine use gun 44.8 teacher own bomb 5.2 marine own book 3.2 teacher use bomb 7.0 marine use book 3.3 teacher own gun 9.3 sergeant own bomb 16.7 teacher use gun 4.7 sergeant use bomb 69.5 teacher own book 48.4 sergeant own gun 73.4 teacher use book 53.6 DSMs adopting a binary model of distributional information (either unstructured models or structured models reduced to binary structures) are represented by matrices containing corpus-derived co-occurrence statistics, with rows and columns labeled by the target elements and their contexts.</S>
    <S sid="135" ssid="25">In DM, we formalize the weighted tuple structure as a labeled third-order tensor, from which semantic spaces are then derived through the operation of labeled matricization.</S>
    <S sid="136" ssid="26">Tensors are multi-way arrays, conventionally denoted by boldface Euler script letters: X (Turney 2007; Kolda and Bader 2009).</S>
    <S sid="137" ssid="27">The order (or n-way) of a tensor is the number of indices needed to identify its elements.</S>
    <S sid="138" ssid="28">Tensors are a generalization of vectors and matrices.</S>
    <S sid="139" ssid="29">The entries in a vector can be denoted by a single index.</S>
    <S sid="140" ssid="30">Vectors are thus first-order tensors, often indicated by a bold lowercase letter: v. The i-th element of a vector v is indicated by vi.</S>
    <S sid="141" ssid="31">Matrices are secondorder tensors, and are indicated with bold capital letters: A.</S>
    <S sid="142" ssid="32">The entry (i, j) in the i-th row and j-th column of a matrix A is denoted by aij.</S>
    <S sid="143" ssid="33">An array with three indices is a thirdorder (or three-way) tensor.</S>
    <S sid="144" ssid="34">The element (i, j, k) of a third-order tensor X is denoted by xijk.</S>
    <S sid="145" ssid="35">A convenient way to display third-order tensors is via nested tables such as Table 2, where the first index is in the header column, the second index in the first header row, and the third index in the second header row.</S>
    <S sid="146" ssid="36">The entry x321 of the tensor in the table is 7.0 and the entry x112 is 85.3.</S>
    <S sid="147" ssid="37">An index has dimensionality I if it ranges over the integers from 1 to I.</S>
    <S sid="148" ssid="38">The dimensionality of a third-order tensor is the product of the dimensionalities of its indices I x J x K. For example, the third-order tensor in Table 2 has dimensionality 3 x 2 x 3.</S>
    <S sid="149" ssid="39">If we fix the integer i as the value of the first index of a matrix A and take the entries corresponding to the full range of values of the other index j, we obtain a row vector (that we denote ai&#8727;).</S>
    <S sid="150" ssid="40">Similarly, by fixing the second index to j, we obtain the column vector a&#8727;j.</S>
    <S sid="151" ssid="41">Generalizing, a fiber is equivalent to rows and columns in higher order tensors, and it is obtained by fixing the values of all indices but one.</S>
    <S sid="152" ssid="42">A mode-n fiber is a fiber where only the n-th index has not been fixed.</S>
    <S sid="153" ssid="43">For example, in the tensor X of Table 2, x&#8727;11 = (40.0,16.7, 5.2) is a mode-1 fiber, x2&#8727;3 = (8.0, 10.1) is a mode-2 fiber, and x32&#8727; = (7.0, 4.7, 53.6) is a mode-3 fiber.</S>
    <S sid="154" ssid="44">A weighted tuple structure can be represented as a third-order tensor whose entries contain the tuple scores.</S>
    <S sid="155" ssid="45">As for the two-way matrices of classic DSMs, in order to make tensors linguistically meaningful we need to assign linguistic labels to the elements of the tensor indices.</S>
    <S sid="156" ssid="46">We define a labeled tensor X&#955; as a tensor such that for each of its indices there is a one-to-one mapping of the integers from 1 to I (the dimensionality of the index) to I distinct strings, that we call the labels of the index.</S>
    <S sid="157" ssid="47">We will refer herein to the string A uniquely associated to index element i as the label of i, their correspondence A labeled third-order tensor of dimensionality 3 x 2 x 3 representing the weighted tuple structure of Table 1. being indicated by i : A.</S>
    <S sid="158" ssid="48">A simple way to perform the mapping&#8212;the one we apply in the running example of this section&#8212;is by sorting the I items in the string set alphabetically, and mapping increasing integers from 1 to I to the sorted strings.</S>
    <S sid="159" ssid="49">A weighted tuple structure TW built from W1, L, and W2 can be represented by a labeled third-order tensor X&#955; with its three indices labeled by W1, L, and W2, respectively, and such that for each weighted tuple t E TW = ((w1,l,w2),vt) there is a tensor entry (i : w1, j : l,k : w2)= vt.</S>
    <S sid="160" ssid="50">In other terms, a weighted tuple structure corresponds to a tensor whose indices are labeled with the string sets forming the triples, and whose entries are the tuple weights.</S>
    <S sid="161" ssid="51">Given the toy weighted tuple structure in Table 1, the object in Table 2 is the corresponding labeled third-order tensor.</S>
    <S sid="162" ssid="52">Matricization rearranges a higher order tensor into a matrix (Kolda 2006; Kolda and Bader 2009).</S>
    <S sid="163" ssid="53">The simplest case is mode-n matricization, which arranges the mode-n fibers to be the columns of the resulting Dn x Dj matrix (where Dn is the dimensionality of the n-th index, Dj is the product of the dimensionalities of the other indices).</S>
    <S sid="164" ssid="54">Mode-n matricization of a third-order tensor can be intuitively understood as the process of making vertical, horizontal, or depth-wise slices of a three-way object like the tensor in Table 2, and arranging these slices sequentially to obtain a matrix (a two-way object).</S>
    <S sid="165" ssid="55">Matricization unfolds the tensor into a matrix with the n-th index indexing the rows of the matrix and a column for each pair of elements from the other two tensor indices.</S>
    <S sid="166" ssid="56">For example, the mode-1 matricization of the tensor in Table 2 results in a matrix with the entries vertically arranged as they are in the table, but replacing the second and third indices with a single index ranging from 1 to 6 (cf. matrix A of Table 3).</S>
    <S sid="167" ssid="57">More explicitly, in mode-n matricization we map each tensor entry (i1, i2, ..., iN) to matrix entry (in, j), where j is computed as in Equation (1), adapted from Kolda and Bader (2009).</S>
    <S sid="168" ssid="58">For example, if we apply mode-1 matricization to the tensor of dimensionality 3 x 2 x 3 in Table 2, we obtain the matrix A3x6 in Table 3 (ignore the labels for now).</S>
    <S sid="169" ssid="59">The tensor entry x3,1,1 is mapped to the matrix cell a3,1; x3,2,3 is mapped to a3,6; and x1,2,2 is mapped to a1,4.</S>
    <S sid="170" ssid="60">Observe that each column of the matrix is a mode-1 fiber of the tensor: The first column is the x*11 fiber; the second column is the x*21 fiber, and so on.</S>
    <S sid="171" ssid="61">Matricization has various mathematically interesting properties and practical applications in computations involving tensors (Kolda 2006).</S>
    <S sid="172" ssid="62">In DM, matricization is applied to labeled tensors and it is the fundamental operation for turning the third-order tensor representing the weighted tuple structure into matrices whose row and column vector spaces correspond to the linguistic objects we want to study; that is, the outcome of matricization must be labeled matrices.</S>
    <S sid="173" ssid="63">Therefore, we must define an operation of labeled mode-n matricization.</S>
    <S sid="174" ssid="64">Recall from earlier discussion that when mode-n matricization is applied, the n-th index becomes the row index of the resulting matrix, and the corresponding labels do not need to be updated.</S>
    <S sid="175" ssid="65">The problem is to determine the labels of the column index of the resulting matrix.</S>
    <S sid="176" ssid="66">We saw that the columns of the matrix produced by mode-n matricization are the mode-n fibers of the original tensor.</S>
    <S sid="177" ssid="67">We must therefore assign a proper label to mode-n tensor fibers.</S>
    <S sid="178" ssid="68">A mode-n fiber is obtained by fixing the values of two indices, and by taking the tensor entries corresponding to the full range of values of the third index.</S>
    <S sid="179" ssid="69">Thus, the natural choice for labeling a mode-n fiber is to use the pair formed by the labels of the two index elements that are fixed.</S>
    <S sid="180" ssid="70">Specifically, each mode-n fiber of a tensor X&#955; is labeled with the binary tuple whose elements are the labels of the corresponding fixed index elements.</S>
    <S sid="181" ssid="71">For instance, given the labeled tensor in Table 2, the mode-1 fiber x&#8222;11 = (40,16.7, 5.2) is labeled with the pair (own, bomb), the mode-2 fiber x2&#8222;1 = (16.7, 69.5) is labeled with the pair (sergeant, bomb), and the mode-3 fiber x32&#8222; = (7.0, 4.7, 53.6) is labeled with the pair (teacher, use).</S>
    <S sid="182" ssid="72">Because mode-n fibers are the columns of the matrices obtained through mode-n matricization, we define the operation of labeled mode-n matricization that, given a labeled third-order tensor X&#955;, maps each entry (i1 : &#955;1, i2 : &#955;2, i3 : &#955;3) to the labeled entry (in : &#955;n, j : &#955;j) such that j is obtained according to Equation (1), and &#955;j is the binary tuple obtained from the triple (&#955;1, &#955;2, &#955;3) by removing &#955;n.</S>
    <S sid="183" ssid="73">For instance, in mode-1 matricization, the entry (1:marine, 1:own, 2:gun) in the tensor in Table 2 is mapped onto the entry (1:marine, 3:(own, gun)).</S>
    <S sid="184" ssid="74">Table 3 reports the matrices A, B, and C, respectively, obtained by applying labeled mode-1, mode-2, and mode-3 matricization to the labeled tensor in Table 2.</S>
    <S sid="185" ssid="75">The columns of each matrix are labeled with pairs, according to the definition of labeled matricization we just gave.</S>
    <S sid="186" ssid="76">From now on, when we refer to mode-n matricization we always assume we are performing labeled mode-n matricization.</S>
    <S sid="187" ssid="77">The rows and columns of the three matrices resulting from n-mode matricization of a third-order tensor are vectors in spaces whose dimensions are the corresponding column and row elements.</S>
    <S sid="188" ssid="78">Such vectors can be used to perform all standard linear algebra operations applied in vector-based semantics: Measuring the cosine of the angle between vectors, applying singular value decomposition (SVD) to the whole matrix, and so on.</S>
    <S sid="189" ssid="79">Under the assumption that W1 = W2 and the inverse link constraint (see Section 3.1), it follows that for each column of the matrix resulting from mode-1 matricization and labeled by (l, w2), there will be a column in the matrix resulting from mode-3 matricization that is labeled by (w1, k) (with k being the inverse link of l and w1 = w2) and that is identical to the former, except possibly for the order of the dimensions (which is irrelevant to all operations we perform on matrices and vectors, however).</S>
    <S sid="190" ssid="80">Similarly, for any row w2 in the matrix resulting from mode-3 matricization, there will be an identical row w1 in the mode-1 matricization.</S>
    <S sid="191" ssid="81">Therefore, given a weighted tuple structure TW extracted from a corpus and subject to the constraints we just mentioned, by matricizing the corresponding labeled third-order tensor X&#955; we obtain the following four distinct semantic vector spaces: word by link&#8211;word (W1xLW2): vectors are labeled with words w1, and vector dimensions are labeled with tuples of type (l, w2); word&#8211;word by link (W1W2xL): vectors are labeled with tuples of type (w1, w2), and vector dimensions are labeled with links l; word&#8211;link by word (W1LxW2): vectors are labeled with tuples of type (w1,l), and vector dimensions are labeled with words w2; link by word&#8211;word (LxW1W2): vectors are labeled with links l and vector dimensions are labeled with tuples of type (w1,w2).</S>
    <S sid="192" ssid="82">Words like marine and teacher are represented in the W1xLW2 space by vectors whose dimensions correspond to features such as (use, gun) or (own, book).</S>
    <S sid="193" ssid="83">In this space, we can measure the similarity of words to each other, in order to tackle attributional similarity tasks such as synonym detection or concept categorization.</S>
    <S sid="194" ssid="84">The W1W2xL vectors represent instead word pairs in a space whose dimensions are links, and it can be used to measure relational similarity among different pairs.</S>
    <S sid="195" ssid="85">For example, one could notice that the link vector of (sergeant, gun) is highly similar to that of (teacher, book).</S>
    <S sid="196" ssid="86">Crucially, as can be seen in Table 3, the corpus-derived scores that populate the vectors in these two spaces are exactly the same, just arranged in different ways.</S>
    <S sid="197" ssid="87">In DM, attributional and relational similarity spaces are different views of the same underlying tuple structure.</S>
    <S sid="198" ssid="88">The other two distinct spaces generated by tensor matricization look less familiar, and yet we argue that they allow us to subsume under the same general DM framework other interesting semantic phenomena.</S>
    <S sid="199" ssid="89">We will show in Section 6.3 how the W1LxW2 space can be used to capture different verb classes based on the argument alternations they display.</S>
    <S sid="200" ssid="90">For instance, this space can be used to find out that the object slot of kill is more similar to the subject slot of die than to the subject slot of kill (and, generalizing from similar observations, that the subject slot of die is a theme rather than an agent).</S>
    <S sid="201" ssid="91">The LxW1W2 space displays similarities among links.</S>
    <S sid="202" ssid="92">The usefulness of this will of course depend on what the links are.</S>
    <S sid="203" ssid="93">We will illustrate in Section 6.4 one function of this space, namely, to perform feature selection, picking links that can then be used to determine a meaningful subspace of the W1W2xL space.</S>
    <S sid="204" ssid="94">Direct matricization is just one of the possible uses we can make of the labeled tensor.</S>
    <S sid="205" ssid="95">In Section 6.5 we illustrate another use of the tensor formalism by performing smoothing through tensor decomposition.</S>
    <S sid="206" ssid="96">Other possibilities, such as graph-based algorithms operating directly on the graph defined by the tensor (Baroni and Lenci 2009), or deriving unstructured semantic spaces from the tensor by removing one of the indices, are left to future work.</S>
    <S sid="207" ssid="97">Before we move on, it is worth emphasizing that, from a computational point of view, there is virtually no additional cost in the tensor approach, with respect to traditional structured DSMs.</S>
    <S sid="208" ssid="98">The labeled tensor is nothing other than a formalization of distributional data extracted in the word&#8211;link&#8211;word&#8211;score format, which is customary in many structured DSMs.</S>
    <S sid="209" ssid="99">Labeled matricization can then simply be obtained by concatenating two elements in the original triple to build the corresponding matrix&#8212;again, a common step in building a structured DSM.</S>
    <S sid="210" ssid="100">In spite of being cost-free in terms of implementation, the mathematical formalism of labeled tensors highlights the common core shared by different views of the semantic space, thereby making distributional semantics more general.</S>
  </SECTION>
  <SECTION title="4." number="5">
    <S sid="211" ssid="1">As will be clear in the next sections, the ways in which we tackle specific tasks are, by themselves, mostly not original.</S>
    <S sid="212" ssid="2">The main element of novelty is the fact that methods originally developed to resort to ad hoc distributional spaces are now adapted to fit into the unified DM framework.</S>
    <S sid="213" ssid="3">We will point out connections to related research specific to the various tasks in the sections devoted to describing their reinterpretation in DM.</S>
    <S sid="214" ssid="4">We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).</S>
    <S sid="215" ssid="5">Instead, we briefly discuss two other studies that explicitly advocate a uniform approach to corpus-based semantic tasks, and one article that, like us, proposes a tensor-based formalization of corpus-extracted triples.</S>
    <S sid="216" ssid="6">See Turney and Pantel (2010) for a very recent general survey of DSMs.</S>
    <S sid="217" ssid="7">Pad&#180;o and Lapata (2007), partly inspired by Lowe (2001), have proposed an interesting general formalization of DSMs.</S>
    <S sid="218" ssid="8">In their approach, a corpus-based semantic model is characterized by (1) a set of functions to extract statistics from the corpus, (2) construction of the basis-by-target-elements co-occurrence matrix, and (3) a similarity function operating on the matrix.</S>
    <S sid="219" ssid="9">Our focus is entirely on the second aspect.</S>
    <S sid="220" ssid="10">A DM, according to the characterization in Section 3, is a labeled tensor based on a source weighted tuple structure and coupled with matricization operations.</S>
    <S sid="221" ssid="11">How the tuple structure was built (corpus extraction methods, association measures, etc.) is not part of the DM formalization.</S>
    <S sid="222" ssid="12">At the other end, DM provides sets of vectors in different vector spaces, but it is agnostic about how they are used (measuring similarity via cosines or other measures, reducing the matrices with SVD, etc.).</S>
    <S sid="223" ssid="13">Of course, much of the interesting progress in distributional semantics will occur at the two ends of our tensor, with better tuple extraction and weighting techniques on one side, and better matrix manipulation and similarity measurement on the other.</S>
    <S sid="224" ssid="14">As long as the former operations result in data that can be arranged into a weighted tuple structure, and the latter procedures act on vectors, such innovations fit into the DM framework and can be used to improve performance on tasks defined on any space derivable from the DM tensor.</S>
    <S sid="225" ssid="15">Whereas the model proposed by Pad&#180;o and Lapata (2007) is designed only to address tasks involving the measurement of the attributional similarity between words, Turney (2008) shares with DM the goal of unifying attributional and relational similarity under the same distributional model.</S>
    <S sid="226" ssid="16">He observes that tasks that are traditionally solved with an attributional similarity approach can be recast as relational similarity tasks.</S>
    <S sid="227" ssid="17">Instead of determining whether two words are, for example, synonymous by looking at the features they share, we can learn what the typical patterns are that connect synonym pairs when they co-occur (also known as, sometimes called, etc.</S>
    <S sid="228" ssid="18">), and make a decision about a potential synonym pair based on their occurrence in similar contexts.</S>
    <S sid="229" ssid="19">Given a list of pairs instantiating an arbitrary relation, Turney&#8217;s PairClass algorithm extracts patterns that are correlated with the relation, and can be used to discover new pairs instantiating it.</S>
    <S sid="230" ssid="20">Turney tests his system in a variety of tasks (TOEFL synonyms; SAT analogies; distinguishing synonyms and antonyms; distinguishing pairs that are semantically similar, associated, or both), obtaining good results across the board.</S>
    <S sid="231" ssid="21">In the DM approach, we collect one set of statistics from the corpus, and then exploit different views of the extracted data and different algorithms to tackle different tasks.</S>
    <S sid="232" ssid="22">Turney, on the contrary, uses a single generic algorithm, but must go back to the corpus to obtain new training data for each new task.</S>
    <S sid="233" ssid="23">We compare DM with some of Turney&#8217;s results in Section 6 but, independently of performance, we find the DM approach more appealing.</S>
    <S sid="234" ssid="24">As corpora grow in size and are enriched with further levels of annotation, extracting ad hoc data from them becomes a very time-consuming operation.</S>
    <S sid="235" ssid="25">Although we did not carry out any systematic experiments, we observe that the extraction of tuple counts from (already POS-tagged and parsed) corpora in order to train our sample DM models took days, whereas even the most time-consuming operations to adapt DM to a task took on the order of 1 to 2 hours on the same machines (task-specific training is also needed in PairClass, anyway).</S>
    <S sid="236" ssid="26">Similar considerations apply to space: Compressed, our source corpora take about 21 GB, our best DM tensor (TypeDM) 1.1 GB (and optimized sparse tensor representations could bring this quantity down drastically, if the need arises).</S>
    <S sid="237" ssid="27">Perhaps more importantly, extracting features from the corpus requires a considerable amount of NLP know-how (to pre-process the corpus appropriately, to navigate a dependency tree, etc.</S>
    <S sid="238" ssid="28">), whereas the DM representation of distributional data as weighted triples is more akin to other standard knowledge representation formats based on typed relations, which are familiar to most computer and cognitive scientists.</S>
    <S sid="239" ssid="29">Thus, a trained DM can become a general-purpose resource and be used by researchers beyond the realms of the NLP community, whereas applying PairClass requires a good understanding of various aspects of computational linguistics.</S>
    <S sid="240" ssid="30">This severely limits its interdisciplinary appeal.</S>
    <S sid="241" ssid="31">At a more abstract level, DM and PairClass differ in the basic strategy with which unification in distributional semantics is pursued.</S>
    <S sid="242" ssid="32">Turney&#8217;s approach amounts to picking a task (identifying pairs expressing the same relation) and reinterpreting other tasks as its particular instances.</S>
    <S sid="243" ssid="33">Thus, attributional and relational similarity are unified by considering the former as a subtype of the latter.</S>
    <S sid="244" ssid="34">Conversely, DM assumes that each semantic task may keep its specificity, and unification is achieved by designing a sufficiently general distributional structure, populating a specific instance of the structure, and generating semantic spaces on demand from the latter.</S>
    <S sid="245" ssid="35">This way, DM is able to address a wider range of semantic tasks than Turney&#8217;s model.</S>
    <S sid="246" ssid="36">For instance, language is full of productive semantic phenomena, such as the selectional preferences of verbs with respect to unseen arguments (eating topinambur vs. eating sympathy).</S>
    <S sid="247" ssid="37">Predicting the plausibility of unseen pairs cannot, by definition, be tackled by the current version of PairClass, which will have to be expanded to deal with such cases, perhaps adopting ideas similar to those we present (that are, in turn, inspired by Turney&#8217;s own work on attributional and relational similarity).</S>
    <S sid="248" ssid="38">A first step in this direction, within a framework similar to Turney&#8217;s, was taken by Herda&#711;gdelen and Baroni (2009).</S>
    <S sid="249" ssid="39">Turney (2007) explicitly formalizes the set of corpus-extracted word&#8211;link&#8211;word triples as a tensor, and was our primary source of inspiration in formalizing DM in these terms.</S>
    <S sid="250" ssid="40">The focus of Turney&#8217;s article, however, is on dimensionality reduction techniques applied to tensors, and the application to corpora is only briefly discussed.</S>
    <S sid="251" ssid="41">Moreover, Turney only derives the W1&#215;LW2 space from the tensor, and does not discuss the possibility of using the tensor-based formalization to unify different views of semantic data, which is instead our main point.</S>
    <S sid="252" ssid="42">The higher-order tensor dimensionality reduction techniques tested on language data by Turney (2007) and Van de Cruys (2009) can be applied to the DM tensors before matricization.</S>
    <S sid="253" ssid="43">We present a pilot study in this direction in Section 6.5.</S>
  </SECTION>
  <SECTION title="5." number="6">
    <S sid="254" ssid="1">In order to make our proposal concrete, we experiment with three different DM models, corresponding to different ways to construct the underlying weighted tuple structure (Section 3.1).</S>
    <S sid="255" ssid="2">All models are based on the natural idea of extracting word&#8211;link&#8211;word tuples from a dependency parse of a corpus, but this is not a requirement for DM: The links could for example be based on frequent n-grams as in Turney (2006b) and Baroni et al. (2010), or even on very different kinds of relation, such as co-occurring within the same document.</S>
    <S sid="256" ssid="3">The current models are trained on the concatenation of (1) the Web-derived ukWaC corpus,2 about 1.915 billion tokens (here and subsequently, counting only strings that are entirely made of alphabetic characters); (2) a mid-2009 dump of the English Wikipedia,3 about 820 million tokens; and (3) the British National Corpus,4 about 95 million tokens.</S>
    <S sid="257" ssid="4">The resulting concatenated corpus was tokenized, POS-tagged, and lemmatized with the TreeTagger5 and dependency-parsed with the MaltParser.6 It contains about 2.83 billion tokens.</S>
    <S sid="258" ssid="5">The ukWaC and Wikipedia sections can be freely downloaded, with full annotation, from the ukWaC corpus site.</S>
    <S sid="259" ssid="6">For all our models, the label sets W1 = W2 contain 30,693 lemmas (20,410 nouns, 5,026 verbs, and 5,257 adjectives).</S>
    <S sid="260" ssid="7">These terms were selected based on their frequency in the corpus (they are approximately the top 20,000 most frequent nouns and top 5,000 most frequent verbs and adjectives), augmenting the list with lemmas that we found in various standard test sets, such as the TOEFL and SAT lists.</S>
    <S sid="261" ssid="8">In all models, the words are stored in POS-suffixed lemma form.</S>
    <S sid="262" ssid="9">The weighted tuple structures differ for the choice of links in L and/or for the scoring function &#963;. DepDM.</S>
    <S sid="263" ssid="10">Our first DM model relies on the classic intuition that dependency paths are a good approximation to semantic relations between words (Grefenstette 1994; Curran and Moens 2002; Pad&#180;o and Lapata 2007; Rothenh&#168;ausler and Sch&#168;utze 2009).</S>
    <S sid="264" ssid="11">DepDM is also the model with the least degree of link lexicalization among the three DM instances we have built (its only lexicalized links are prepositions).</S>
    <S sid="265" ssid="12">LDepDM includes the following noun&#8211;verb, noun&#8211;noun, and adjective&#8211;noun links (in order to select more reliable dependencies and filter out possible parsing errors, dependencies between words with more than five intervening items were discarded): sbj intr: subject of a verb that has no direct object: The teacher is singing -4 (teacher, sbj intr, sing); The soldier talked with his sergeant -4 (soldier, sbj intr, talk); sbj tr: subject of a verb that occurs with a direct object: The soldier is reading a book -4 (soldier, sbj tr, read); obj: direct object: The soldier is reading a book -4 (book, obj, read); iobj: indirect object in a double object construction: The soldier gave the woman a book -4 (woman, iobj, give); nmod: noun modifier: good teacher -4 (good, nmod, teacher); school teacher -4 (school, nmod, teacher); coord: noun coordination: teachers and soldiers -4 (teacher, coord, soldier); prd: predicate noun: The soldier became sergeant -4 (sergeant, prd, become); verb: an underspecified link between a subject noun and a complement noun of the same verb: The soldier talked with his sergeant -4 (soldier, verb, sergeant); The soldier is reading a book -4 (soldier, verb, book); preposition: every preposition linking the noun head of a prepositional phrase to its noun or verb head (a different link for each preposition): I saw a soldier with the gun -4 (gun, with, soldier); The soldier talked with his sergeant -4 (sergeant, with, talk).</S>
    <S sid="266" ssid="13">For each link, we also extract its inverse (this holds for all our DM models).</S>
    <S sid="267" ssid="14">For example, there is a sbj intr&#8722;1 link between an intransitive verb and its subject: (talk, sbj intr&#8722;1, soldier).</S>
    <S sid="268" ssid="15">The cardinality of LDepDM is 796, including direct and inverse links.</S>
    <S sid="269" ssid="16">The weights assigned to the tuples by the scoring function &#963; are given by Local Mutual Information (LMI) computed on the raw corpus-derived word&#8211;link&#8211;word cooccurrence counts.</S>
    <S sid="270" ssid="17">Given the co-occurrence count Oijk of three elements of interest (in our case, the first word, the link, and the second word), and the corresponding expected count under independence Eijk, LMI = Oijk log Oijk Eijk .</S>
    <S sid="271" ssid="18">LMI is an approximation to the log-likelihood ratio measure that has been shown to be a very effective weighting scheme for sparse frequency counts (Dunning 1993; Pad&#180;o and Lapata 2007).</S>
    <S sid="272" ssid="19">The measure can also be interpreted as the dominant term of average MI or as a heuristic variant of pointwise MI to avoid its bias towards overestimating the significance of low frequency events, and it is nearly identical to the Poisson&#8211;Stirling measure (Evert 2005).</S>
    <S sid="273" ssid="20">LMI has considerable computational advantages in cases like ours, in which we measure the association of three elements, because it does not require keeping track of the full 2 x 2 x 2 contingency table, which is the case for the log-likelihood ratio.</S>
    <S sid="274" ssid="21">Following standard practice (Bullinaria and Levy 2007), negative weights (cases where the observed value is lower than the expected value) are raised to 0.</S>
    <S sid="275" ssid="22">The number of non-zero tuples in the DepDM tensor is about 110M, including tuples with direct links and their inverses.</S>
    <S sid="276" ssid="23">DepDM is a 30,693 x 796 x 30,693 tensor with density 0.0149% (the proportion of non-zero entries in the tensor).</S>
    <S sid="277" ssid="24">LexDM.</S>
    <S sid="278" ssid="25">The second model is inspired by the idea that the lexical material connecting two words is very informative about their relation (Hearst 1992; Pantel and Pennacchiotti 2006; Turney 2006b).</S>
    <S sid="279" ssid="26">LLexDM contains complex links, each with the structure pattern+suffix.</S>
    <S sid="280" ssid="27">The suffix is in turn formed by two substrings separated by a +, each respectively encoding the following features of w1 and w2: their POS and morphological features (number for nouns, number and tense for verbs); the presence of an article (further specified with its definiteness value) and of adjectives for nouns; the presence of adverbs for adjectives; and the presence of adverbs, modals, and auxiliaries for verbs, together with their diatheses (for passive only).</S>
    <S sid="281" ssid="28">If the adjective (adverb) modifying w1 or w2 belongs to a list of 10 (250) high frequency adjectives (adverbs), the suffix string contains the adjective (adverb) itself, otherwise only its POS.</S>
    <S sid="282" ssid="29">For instance, from the sentence The tall soldier has already shot we extract the tuple (soldier, sbj intr+n-thej+vn-aux-already, shoot).</S>
    <S sid="283" ssid="30">Its complex link contains the pattern sbj intr and the suffix n-the-j+vn-aux-already.</S>
    <S sid="284" ssid="31">The suffix substring n-the-j encodes the information that w1 is a singular noun (n), is definite (the), and has an adjective (j) that does not belong to the list of high frequency adjectives.</S>
    <S sid="285" ssid="32">The substring vn-aux-already specifies that w2 is a past-participle (vn), has an auxiliary (aux), and is modified by already, belonging to the pre-selected list of high frequency adverbs.</S>
    <S sid="286" ssid="33">The patterns in the LexDM links include: verb: if the verb link between a subject noun and a complement noun belongs to a list of 52 high frequency verbs, the underspecified verb link of DepDM is replaced by the verb itself: The soldier used a gun -4 (soldier, use+n-the+n-a, gun); The soldier read the yellow book -4 (soldier, verb+n-the+n-the-j, book); is: copulative structures with an adjectival predicate: The soldier is tall -4 (tall, is+j+n-the, soldier); preposition&#8211;link noun&#8211;preposition: this schema captures connecting expressions such as of a number of, in a kind of; link noun is one of 48 semi-manually selected nouns such as number, variety, or kind; the arrival of a number of soldiers -4 (soldier, of-number-of+ns+n-the, arrival); attribute noun: one of 127 nouns extracted from WordNet and expressing attributes of concepts, such as size, color, or height.</S>
    <S sid="287" ssid="34">This pattern connects adjectives and nouns that occur in the templates (the) attribute noun of (a|the) NOUN is ADJ (Almuhareb and Poesio 2004) and (a|the) ADJ attribute noun of NOUN (Veale and Hao 2008): the color of strawberries is red -4 (red, color+j+ns, strawberry); the autumnal color of the forest -4 (autumnal, color+j+n-the, forest); as adj as: this pattern links an adjective and a noun that match the template as ADJ as (a|the) NOUN (Veale and Hao 2008): as sharp as a knife -4 (sharp, as adj as+j+n-a, knife); such as: links two nouns occurring in the templates NOUN such as NOUN and such NOUN as NOUN (Hearst 1992, 1998): animals such as cats-4 (animal, such as+ns+ns, cat); such vehicles as cars -4 (vehicle, such as+ns+ns, car).</S>
    <S sid="288" ssid="35">LexDM links have a double degree of lexicalization.</S>
    <S sid="289" ssid="36">First, the suffixes encode a wide array of surface features of the tuple elements.</S>
    <S sid="290" ssid="37">Secondly, the link patterns themselves, besides including standard syntactic relations (such as direct object or coordination), extend to lexicalized dependency relations (specific verbs) and lexico-syntactic shallow templates.</S>
    <S sid="291" ssid="38">The latter include patterns adopted in the literature to extract specific pieces of semantic knowledge.</S>
    <S sid="292" ssid="39">For instance, NOUN such as NOUN and such NOUN as NOUN were first proposed by Hearst (1992) as highly reliable patterns for hypernym identification, whereas (the) attribute noun of (a|the) NOUN is ADJ and (a|the) ADJ attribute noun of NOUN were successfully used to identify typical values of concept attributes (Almuhareb and Poesio 2004; Veale and Hao 2008).</S>
    <S sid="293" ssid="40">Therefore, the LexDM distributional memory is a repository of partially heterogeneous types of corpus-derived information, differing in their level of abstractness, which ranges from fairly abstract syntactic relations to shallow lexicalized patterns.</S>
    <S sid="294" ssid="41">LLexDM contains 3,352,148 links, including inverses.</S>
    <S sid="295" ssid="42">The scoring function 6 is the same as that in DepDM, and the number of nonzero tuples is about 355M, including direct and inverse links.</S>
    <S sid="296" ssid="43">LexDM is a 30,693 x 3,352,148 x 30,693 tensor with density 0.00001%.</S>
    <S sid="297" ssid="44">TypeDM.</S>
    <S sid="298" ssid="45">This model is based on the idea, motivated and tested by Baroni et al. (2010)&#8212; but see also Davidov and Rappoport (2008a, 2008b) for a related method&#8212;that what matters is not so much the frequency of a link, but the variety of surface forms that express it.</S>
    <S sid="299" ssid="46">For example, if we just look at frequency of co-occurrence (or strength of association), the triple (fat, of&#8722;1, land) (a figurative expression) is much more common than the semantically more informative (fat, of&#8722;1, animal).</S>
    <S sid="300" ssid="47">However, if we count the different surface realizations of the former pattern in our corpus, we find that there are only three of them (the fat of the land, the fat of the ADJ land, and the ADJ fat of the land), whereas (fat, of&#8722;1, animal) has nine distinct realizations (a fat of the animal, the fat of the animal, fats of animal, fats of the animal, fats of the animals, ADJ fats of the animal, and the fats of the animal).</S>
    <S sid="301" ssid="48">TypeDM formalizes this intuition by adopting as links the patterns inside the LexDM links, while the suffixes of these patterns are used to count their number of distinct surface realizations.</S>
    <S sid="302" ssid="49">We call the model TypeDM because it counts types of realizations, not tokens.</S>
    <S sid="303" ssid="50">For instance, the two LexDM links of&#8722;1+n-a+n-the and of&#8722;1+nsj+n-the are counted as two occurrences of the same TypeDM link of&#8722;1, corresponding to the pattern in the two original links.</S>
    <S sid="304" ssid="51">The scoring function 6 computes LMI not on the raw word&#8211;link&#8211;word cooccurrence counts, but on the number of distinct suffix types displayed by a link when it co-occurs with the relevant words.</S>
    <S sid="305" ssid="52">For instance, a TypeDM link derived from a LexDM pattern that occurs with nine different suffix types in the corpus is assigned a frequency of 9 for the purpose of the computation of LMI.</S>
    <S sid="306" ssid="53">The distinct TypeDM links are 25,336.</S>
    <S sid="307" ssid="54">The number of non-zero tuples in the TypeDM tensor is about 130M, including direct and inverse links.</S>
    <S sid="308" ssid="55">TypeDM is a 30,693 x 25,336 x 30,693 tensor with density 0.0005%.</S>
    <S sid="309" ssid="56">To sum up, the three DM instance models herein differ in the degree of lexicalization of the link set, and/or in the scoring function.</S>
    <S sid="310" ssid="57">LexDM is a heavily lexicalized model, contrasting with DepDM, which has a minimum degree of lexicalization, and consequently the smallest set of links.</S>
    <S sid="311" ssid="58">TypeDM represents a sort of middle level both for the kind and the number of links.</S>
    <S sid="312" ssid="59">These consist of syntactic and lexicalized patterns, as in LexDM.</S>
    <S sid="313" ssid="60">The lexical information encoded in the LexDM suffixes, however, is not used to generate different links, but to implement a different counting scheme as part of a different scoring function.</S>
    <S sid="314" ssid="61">A weighted tuple structure (equivalently: a labeled DM tensor) is intended as a long-term semantic resource that can be used in different projects for different tasks, analogously to traditional hand-coded resources such as WordNet.</S>
    <S sid="315" ssid="62">Coherent with this approach, we make our best DM model (TypeDM) publicly available from http://clic.cimec.unitn.it/dm.</S>
    <S sid="316" ssid="63">The site also contains a set of Perl scripts that perform the basic operations on the tensor and its derived vectors we are about to describe.</S>
    <S sid="317" ssid="64">The DM framework provides, via matricization, a set of matrices with associated labeled row and column vectors.</S>
    <S sid="318" ssid="65">These labeled matrices can simply be derived from the tuple tensor by concatenating two elements in the original triples.</S>
    <S sid="319" ssid="66">Any operation that can be performed on the resulting matrices and that might help in tackling a semantic task is fair game.</S>
    <S sid="320" ssid="67">However, in the experiments reported in this article we will work with a limited number of simple operations that are well-motivated in terms of the geometric framework we adopt, and suffice to face all the tasks we will deal with (the decomposition techniques explored in Section 6.5 are briefly introduced there).</S>
    <S sid="321" ssid="68">Vector length and normalization.</S>
    <S sid="322" ssid="69">The length of a vector v with dimensions v1, v2,.</S>
    <S sid="323" ssid="70">.</S>
    <S sid="324" ssid="71">.</S>
    <S sid="325" ssid="72">, vn is: A vector is normalized to have length 1 by dividing each dimension by the original vector length.</S>
    <S sid="326" ssid="73">Cosine.</S>
    <S sid="327" ssid="74">We measure the similarity of two vectors x and y by the cosine of the angle they form: The cosine ranges from 11 for vectors pointing in the same direction to 0 for orthogonal vectors.</S>
    <S sid="328" ssid="75">Other similarity measures, such as Lin&#8217;s measure (Lin 1998b), work better than the cosine in some tasks (Curran and Moens 2002; Pad&#180;o and Lapata 2007).</S>
    <S sid="329" ssid="76">However, the cosine is the most natural similarity measure in the geometric formalism we are adopting, and we stick to it as the default approach to measuring similarity.</S>
    <S sid="330" ssid="77">Vector sum.</S>
    <S sid="331" ssid="78">Two or more vectors are summed in the obvious way, by adding their values on each dimension.</S>
    <S sid="332" ssid="79">We always normalize the vectors before summing.</S>
    <S sid="333" ssid="80">The resulting vector points in the same direction as the average of the summed normalized vectors.</S>
    <S sid="334" ssid="81">We refer to it as the centroid of the vectors.</S>
    <S sid="335" ssid="82">Projection onto a subspace.</S>
    <S sid="336" ssid="83">It is sometimes useful to measure length or compare vectors by taking only some of their dimensions into account.</S>
    <S sid="337" ssid="84">For example, one way to find nouns that are typical objects of the verb to sing is to measure the length of nouns in a W1xLW2 subspace in which only dimensions such as (obj, sing) have non-0 values.</S>
    <S sid="338" ssid="85">We project a vector onto a subspace of this kind through multiplication of the vector by a square diagonal matrix with 1s in the diagonal cells corresponding to the dimensions we want to preserve and 0s elsewhere.</S>
    <S sid="339" ssid="86">A matrix of this sort performs an orthogonal projection of the vector it multiplies (Meyer 2000, chapter 5).</S>
  </SECTION>
  <SECTION title="6." number="7">
    <S sid="340" ssid="1">As we saw in Section 3, labeled matricization generates four distinct semantic spaces from the third-order tensor.</S>
    <S sid="341" ssid="2">For each space, we have selected a set of semantic experiments that we model by applying some combination of the vector manipulation operations of Section 5.2.</S>
    <S sid="342" ssid="3">The experiments correspond to key semantic tasks in computational linguistics and/or cognitive science, typically addressed by distinct DSMs so far.</S>
    <S sid="343" ssid="4">We have also aimed at maximizing the variety of aspects of meaning covered by the experiments, ranging from synonymy detection to argument structure and concept properties, and encompassing all the major lexical classes.</S>
    <S sid="344" ssid="5">Both these facts support the view of DM as a generalized model that is able to overtake state-of-the-art DSMs in the number and types of semantic issues addressed, while being competitive in each specific task.</S>
    <S sid="345" ssid="6">The choice of the DM semantic space to tackle a particular task is essentially based on the &#8220;naturalness&#8221; with which the task can be modeled in that space.</S>
    <S sid="346" ssid="7">However, alternatives are conceivable, both with respect to space selection, and to the operations performed on the space.</S>
    <S sid="347" ssid="8">For instance, Turney (2008) models synonymy detection with a DSM that closely resembles our W1W2&#215;L space, whereas we tackle this task under the more standard W1&#215;LW2 view.</S>
    <S sid="348" ssid="9">It is an open question whether there are principled ways to select the optimal space configuration for a given semantic task.</S>
    <S sid="349" ssid="10">In this article, we limit ourselves to proving that each space derived through tensor matricization is semantically interesting in the sense that it provides the proper ground to address some semantic task.</S>
    <S sid="350" ssid="11">Feature selection/reweighting and dimensionality reduction have been shown to improve DSM performance.</S>
    <S sid="351" ssid="12">For instance, the feature bootstrapping method proposed by Zhitomirsky-Geffet and Dagan (2009) boosts the precision of a DSM in lexical entailment recognition.</S>
    <S sid="352" ssid="13">Even if these methods can be applied to DM as well, we did not use them in our experiments.</S>
    <S sid="353" ssid="14">The results presented subsequently should be regarded as a &#8220;baseline&#8221; performance that could be enhanced in future work by exploring various task-specific parameters (we will come back in the conclusion to the role of parameter tuning in DM).</S>
    <S sid="354" ssid="15">This is consistent with our current aim of focusing on the generality and adaptivity of DM, rather than on task-specific optimization.</S>
    <S sid="355" ssid="16">As a first, important step in this latter direction, however, we conclude the empirical evaluation in Section 6.5 by replicating one experiment using tensor-decomposition-based smoothing, a form of optimization that can only be performed within the tensor-based approach to DSMs.</S>
    <S sid="356" ssid="17">In order to maximize coverage of the experimental test sets, they are pre-processed with a mixture of manual and heuristic procedures to assign a POS to the words they contain, lemmatize, convert some multiword forms to single words, and turn some adverbs into adjectives (our models do not contain multiwords or adverbs).</S>
    <S sid="357" ssid="18">Nevertheless, some words (or word pairs) are unrecoverable, and in such cases we make a random guess (in cases where we do not have full coverage of a data set, the reported results are averages across repeated experiments, to account for the variability in random guesses).</S>
    <S sid="358" ssid="19">In many of the experiments herein, DM is not only compared to the results available in the literature, but also to our implementation of state-of-the-art DSMs.</S>
    <S sid="359" ssid="20">These alternative models have been trained on the same corpus (with the same linguistic preprocessing) used to build the DM tuple tensors.</S>
    <S sid="360" ssid="21">This way, we aim at achieving a fairer comparison with alternative approaches in distributional semantics, abstracting away from the effects induced by differences in the training data.</S>
    <S sid="361" ssid="22">Most experiments report global (micro-averaged) test set accuracy (alone, or combined with other measures) to assess the performance of the algorithms.</S>
    <S sid="362" ssid="23">The number of correctly classified items among all test elements can be seen as a binomially distributed random variable, and we follow the ACL Wiki state-of-the-art site7 in reporting also Clopper&#8211;Pearson binomial 95% confidence intervals around the accuracies (binomial intervals and other statistical quantities were computed using the R package;8 where no further references are given, we used the standard R functions for the relevant analysis).</S>
    <S sid="363" ssid="24">The binomial confidence intervals give a sense of the spread of plausible population values around the test-set-based point estimates of accuracy.</S>
    <S sid="364" ssid="25">Where appropriate and interesting, we compare the accuracy of two specific models statistically with an exact Fisher test on the contingency table of correct and wrong responses given by the two models.</S>
    <S sid="365" ssid="26">This approach to significance testing is problematic in many respects, the most important being that we ignore dependencies in correct and wrong counts due to the fact that the algorithms are evaluated on the same test set (Dietterich 1998).</S>
    <S sid="366" ssid="27">More appropriate tests, however, would require access to the fully itemized results from the compared algorithms, whereas in most cases we only know the point estimate reported in the earlier literature.</S>
    <S sid="367" ssid="28">For similar reasons, we do not make significance claims regarding other performance measures, such as macro-averaged F. Other forms of statistical analysis of the results are introduced herein when they are used; they are mostly limited to the models for which we have full access to the results.</S>
    <S sid="368" ssid="29">Note that we are interested in whether DM performance is overall within state-of-the-art range, and not on making precise claims about the models it outperforms.</S>
    <S sid="369" ssid="30">In this respect, we think that our general results are clear even where they are not supported by statistical inference, or interpretation of the latter is problematic.</S>
    <S sid="370" ssid="31">The vectors of this space are labeled with words w1 (rows of matrix Amode-1 in Table 3), and their dimensions are labeled with binary tuples of type (l, w2) (columns of the same matrix).</S>
    <S sid="371" ssid="32">The dimensions represent the attributes of words in terms of lexico-syntactic relations with lexical collocates, such as (sbj intr, read), or (use, gun).</S>
    <S sid="372" ssid="33">Consistently, all the semantic tasks that we address with this space involve the measurement of the attributional similarity between words.</S>
    <S sid="373" ssid="34">The W1xLW2 matrix is a structured semantic space similar to those used by Curran and Moens (2002), Grefenstette (1994), and Lin (1998a), among others.</S>
    <S sid="374" ssid="35">To test if the use of links detracts from performance on attributional similarity tasks, we trained on our concatenated corpus two alternative models&#8212;Win and DV&#8212;whose features only include lexical collocates of the target.</S>
    <S sid="375" ssid="36">Win is an unstructured DSM that does not rely on syntactic structure to select the collocates, but just on their linear proximity to the targets (Lund and Burgess 1996; Sch&#168;utze 1997; Bullinaria and Levy 2007, and many others).</S>
    <S sid="376" ssid="37">Its matrix is based on co-occurrences of the same 30K words we used for the other models within a window of maximally five content words before or after the target.</S>
    <S sid="377" ssid="38">DV is an implementation of the Dependency Vectors approach of Pad&#180;o and Lapata (2007).</S>
    <S sid="378" ssid="39">It is a structured DSM, but dependency paths are used to pick collocates, without being part of the attributes.</S>
    <S sid="379" ssid="40">The DV model is obtained from the same co-occurrence data as DepDM (thus, relying on the dependency paths we picked, not the ones originally selected by Pad&#180;o and Lapata for their tests).</S>
    <S sid="380" ssid="41">Frequencies are summed across dependency path links for word&#8211;link&#8211;word triples with the same first and second words.</S>
    <S sid="381" ssid="42">Suppose that soldier and gun occur in the tuples (soldier, have, gun) (frequency 3) and (soldier, use, gun) (frequency 37).</S>
    <S sid="382" ssid="43">In DepDM, this results in two features for soldier: (have, gun) and (use, gun).</S>
    <S sid="383" ssid="44">In DV, we would derive a single gun feature with frequency 40.</S>
    <S sid="384" ssid="45">As for the DM models, the Win and DV counts are converted to LMI weights, and negative LMI values are raised to 0.</S>
    <S sid="385" ssid="46">Win is a 30,693 x 30,693 matrix with about 110 million non-zero entries (density: 11.5%).</S>
    <S sid="386" ssid="47">DV is a 30,693 x 30,693 matrix with about 38 million non-zero values (density: 4%).</S>
    <S sid="387" ssid="48">6.1.1 Similarity Judgments.</S>
    <S sid="388" ssid="49">Our first challenge comes from the classic data set of Rubenstein and Goodenough (1965), consisting of 65 noun pairs rated by 51 subjects on a 0&#8211;4 similarity scale.</S>
    <S sid="389" ssid="50">The average rating for each pair is taken as an estimate of the perceived similarity between the two words (e.g., car&#8211;automobile: 3.9, cord&#8211;smile: 0.0).</S>
    <S sid="390" ssid="51">Following the earlier literature, we use Pearson&#8217;s r to evaluate how well the cosines in the W1&#215;LW2 space between the nouns in each pair correlate with the ratings.</S>
    <S sid="391" ssid="52">The results (expressed in terms of percentage correlations) are presented in Table 4, which also reports state-of-the-art performance levels of corpus-based systems from the literature (the correlation of all systems with the ratings is very significantly above chance, according to a two-tailed t-test for Pearson correlation coefficients; df = 63, p &lt; 0.0001 for all systems).</S>
    <S sid="392" ssid="53">One of the DM models, namely TypeDM, does very well on this task, outperformed only by DoubleCheck, an unstructured system that relies on Web queries (and thus on a much larger corpus) and for which we report the best result across parameter settings.</S>
    <S sid="393" ssid="54">We also report the best results from a range of experiments with different models and parameter settings from Herda&#711;gdelen, Erk, and Baroni (2009) (whose corpus is about half the size of ours) and Pad&#180;o and Lapata (2007) (who use a much smaller corpus).</S>
    <S sid="394" ssid="55">For the latter, we also report the best result they obtain when using cosine as the similarity measure (cosDV-07).</S>
    <S sid="395" ssid="56">Overall, the TypeDM result is in line with the state of the art, given the size of the input corpus, and the fact that we did not perform any tuning.</S>
    <S sid="396" ssid="57">Following Pad&#180;o, Pad&#180;o, and Erk (2007) we used the approximate test proposed by Raghunathan (2003) to compare the correlations with the human ratings of sets of models (this is only possible for the models we developed, as the test requires computation of correlation coefficients across models).</S>
    <S sid="397" ssid="58">The test suggests that the difference in correlation with human ratings between TypeDM and our second best model, Win, is significant (Q = 4.55, df = 0.23, p &lt; 0.01).</S>
    <S sid="398" ssid="59">On the other hand, there is no significant difference across Win, DepDM, DV and LexDM (Q = 1.02, df = 1.80, p = 0.55). late quantitative similarity ratings.</S>
    <S sid="399" ssid="60">The classic TOEFL synonym detection task focuses on the high end of the similarity scale, asking the models to make a discrete decision about which word is the synonym from a set of candidates.</S>
    <S sid="400" ssid="61">The data set, introduced to computational linguistics by Landauer and Dumais (1997), consists of 80 multiplechoice questions, each made of a target word (a noun, verb, adjective, or adverb) and four candidates.</S>
    <S sid="401" ssid="62">For example, given the target levied, the candidates are imposed, believed, requested, correlated, the first one being the correct choice.</S>
    <S sid="402" ssid="63">Our algorithms pick the candidate with the highest cosine to the target item as their guess of the right synonym.</S>
    <S sid="403" ssid="64">Table 5 reports results (percentage accuracies) on the TOEFL set for our models as well as the best model of Herda&#711;gdelen and Baroni (2009) and the corpus-based models from the ACL Wiki TOEFL state-of-the-art table (we do not include those models from the Wiki that resort to other knowledge sources, such as WordNet or a thesaurus).</S>
    <S sid="404" ssid="65">The claims to follow about the relative performance of the models must be interpreted cautiously, in light of the spread of the confidence intervals: It suffices to note that, according to a Fisher test, the difference between the second-best model, GLSA, and the twelfth model, PMI-IR-01, is not significant at the &#945; = .05 level (p = 0.07).</S>
    <S sid="405" ssid="66">The difference between the bottom model, LSA-97, and random guessing is, on the other hand, highly significant (p &lt; .00001).</S>
    <S sid="406" ssid="67">The best DM model is again TypeDM, which also outperforms Turney&#8217;s unified PairClass approach (supervised, and relying on a much larger corpus), as well as his Web-statistics based PMI-IR-01 model.</S>
    <S sid="407" ssid="68">TypeDM does better than the best Pad&#180;o and Lapata model (DV-07), and comparably to our DV implementation.</S>
    <S sid="408" ssid="69">Its accuracy is more than 10% higher than the average human test taker and the classic LSA model (LSA-97).</S>
    <S sid="409" ssid="70">Among the approaches that outperform TypeDM, BagPack is supervised, and CWO and PMI-IR-03 rely on much larger corpora.</S>
    <S sid="410" ssid="71">This leaves us with three unsupervised (and unstructured) models from the literature that outperform TypeDM while being trained on comparable or smaller corpora: LSA-03, GLSA, and PPMIC.</S>
    <S sid="411" ssid="72">In all three cases, the authors show that parameter tuning is beneficial in attaining the reported best performance.</S>
    <S sid="412" ssid="73">Further work should investigate how we could improve TypeDM by exploring various parameter settings (many of which do not require going back to the corpus: feature selection and reweighting, SVD, etc.).</S>
    <S sid="413" ssid="74">6.1.3 Noun Categorization.</S>
    <S sid="414" ssid="75">Humans are able to group words into classes or categories depending on their meaning similarities.</S>
    <S sid="415" ssid="76">Categorization tasks play a prominent role in cognitive research on concepts and meaning, as a probe into the semantic organization of the lexicon and the ability to arrange concepts hierarchically into taxonomies (Murphy 2002).</S>
    <S sid="416" ssid="77">Research in corpus-based semantics has always been interested in investigating whether distributional (attributional) similarity could be used to group words into semantically coherent categories.</S>
    <S sid="417" ssid="78">From the computational point of view, this is a particularly crucial issue because it concerns the possibility of using distributional information to assign a semantic class or type to words.</S>
    <S sid="418" ssid="79">Categorization requires (at least in current settings) a discrete decision, as in the TOEFL task, but it is based on detecting not only synonyms but also less strictly related words that stand in a coordinate/cohyponym relation.</S>
    <S sid="419" ssid="80">We focus here on noun categorization, which we operationalize as a clustering task.</S>
    <S sid="420" ssid="81">Distributional categorization has been investigated for other POS as well, most notably verbs (Merlo and Stevenson 2001; Schulte im Walde 2006).</S>
    <S sid="421" ssid="82">However, verb classifications are notoriously more controversial than nominal ones, and deeply interact with argument structure properties.</S>
    <S sid="422" ssid="83">Some experiments on verb classification will be carried out in the W1L&#215;W2 space in Section 6.3.</S>
    <S sid="423" ssid="84">Because the task of clustering concepts/words into superordinates has recently attracted much attention, we have three relevant data sets from the literature available for our tests.</S>
    <S sid="424" ssid="85">The Almuhareb&#8211;Poesio (AP) set includes 402 concepts from WordNet, balanced in terms of frequency and ambiguity.</S>
    <S sid="425" ssid="86">The concepts must be clustered into 21 classes, each selected from one of the 21 unique WordNet beginners, and represented by between 13 and 21 nouns.</S>
    <S sid="426" ssid="87">Examples include the vehicle class (helicopter, motorcycle... ), the motivation class (ethics, incitement, ... ), and the social unit class (platoon, branch).</S>
    <S sid="427" ssid="88">See Almuhareb (2006) for the full set.</S>
    <S sid="428" ssid="89">The Battig test set introduced by Baroni et al. (2010) is based on the expanded Battig and Montague norms of Van Overschelde, Rawson, and Dunlosky (2004).</S>
    <S sid="429" ssid="90">The set comprises 83 concepts from 10 common concrete categories (up to 10 concepts per class), with the concepts selected so that they are rated as highly prototypical of the class.</S>
    <S sid="430" ssid="91">Class examples include land mammals (dog, elephant... ), tools (screwdriver, hammer) and fruit (orange, plum).</S>
    <S sid="431" ssid="92">See Baroni et al. (2010) for the full list.</S>
    <S sid="432" ssid="93">Finally, the ESSLLI 2008 set was used for one of the Distributional Semantic Workshop shared tasks (Baroni, Evert, and Lenci 2008).</S>
    <S sid="433" ssid="94">It is also based on concrete nouns, but it includes fewer prototypical members of categories (rocket as vehicle or snail as land animal).</S>
    <S sid="434" ssid="95">The 44 target concepts are organized into a hierarchy of classes of increasing abstraction.</S>
    <S sid="435" ssid="96">There are 6 lower level classes, with maximally 13 concepts per class (birds, land animals, fruit, greens, tools, vehicles).</S>
    <S sid="436" ssid="97">At a middle level, concepts are grouped into three classes (animals, vegetables, and artifacts).</S>
    <S sid="437" ssid="98">At the most abstract level, there is a two-way distinction between living beings and objects.</S>
    <S sid="438" ssid="99">See http://wordspace. collocations.de for the full set.</S>
    <S sid="439" ssid="100">We cluster the nouns in each set by computing their similarity matrix based on pairwise cosines, and feeding it to the widely used CLUTO toolkit (Karypis 2003).</S>
    <S sid="440" ssid="101">We use CLUTO&#8217;s built-in repeated bisections with global optimization method, accepting all of CLUTO&#8217;s default values for this method.</S>
    <S sid="441" ssid="102">Cluster quality is evaluated by percentage purity, one of the standard clustering quality measures returned by CLUTO (Zhao and Karypis 2003).</S>
    <S sid="442" ssid="103">If nir is the number of items from the i-th true (gold standard) class that were assigned to the r-th cluster, n the total number of items, and k the number of clusters, then Expressed in words, for each cluster we count the number of items that belong to the true class that is most represented in the cluster, and then we sum these counts across clusters.</S>
    <S sid="443" ssid="104">The resulting sum is divided by the total number of items so that, in the best case (perfect clusters), purity will be 1 (in percentage terms, 100%).</S>
    <S sid="444" ssid="105">As cluster quality deteriorates, purity approaches 0.</S>
    <S sid="445" ssid="106">For the models where we have full access to the results, we use a heuristic bootstrap procedure to obtain confidence intervals around the purities (Efron and Tibshirani 1994).</S>
    <S sid="446" ssid="107">We resample with replacement 10K data sets (clusterassignment+true-label pairs) of the original size.</S>
    <S sid="447" ssid="108">Empirical 95% confidence intervals are then computed from the distribution of the purities in the bootstrapped data sets (for the ESSLLI results, we only perform the procedure for 6-way clustering).</S>
    <S sid="448" ssid="109">The confidence intervals give a rough idea of how stable purity estimates are across small variations of the items in the data sets.</S>
    <S sid="449" ssid="110">The Random models for this task are baselines assigning the concepts randomly to the target clusters, with the constraint that each cluster must contain at least one concept.</S>
    <S sid="450" ssid="111">Random assignment is repeated 10K times, and we obtain means and confidence intervals from the distribution of these simulations.</S>
    <S sid="451" ssid="112">Table 6 reports purity results for the three data sets, comparing our models to those in the literature.</S>
    <S sid="452" ssid="113">Again, the TypeDM model has an excellent performance.</S>
    <S sid="453" ssid="114">On the ESSLLI 2008 set, it outperforms the best configuration of the best shared task system among those that did three-level categorization (Katrenko&#8217;s), despite the fact that the latter uses the full Web as a corpus and manually crafted patterns to improve feature extraction.</S>
    <S sid="454" ssid="115">TypeDM&#8217;s performance is equally impressive on the AP set, where it outperforms AttrValue-05, the best unsupervised model by the data set proponents, trained on the full Web.</S>
    <S sid="455" ssid="116">Interestingly, the DepPath model of Rothenh&#168;ausler and Sch&#168;utze (2009), which is the only one outperforming TypeDM on the AP set, is another structured model with dependency-based link-mediated features, which would fit well into the DM framework.</S>
    <S sid="456" ssid="117">TypeDM&#8217;s purity is extremely high with the Battig set as well, although here it is outperformed by the unstructured Win model.</S>
    <S sid="457" ssid="118">Our top two performances are higher than Strudel, the best model by the proponents of the task.</S>
    <S sid="458" ssid="119">The latter was trained on about half of the data we used, however (moreover, the confidence intervals of these models largely overlap, suggesting that their difference is not significant).</S>
    <S sid="459" ssid="120">6.1.4 Selectional Preferences.</S>
    <S sid="460" ssid="121">Our last pair of data sets for the W1xLW2 space illustrate how the space can be used not only to measure similarity among words, but also to work with more abstract notions, such as that of a typical filler of an argument slot of a verb (such as the typical killer and the typical killee).</S>
    <S sid="461" ssid="122">We think that these are especially important experiments, because they show how the same matrix that has been used for tasks that were entirely bound to lexical items can also be used to generalize to structures that go beyond what is directly observed in the corpus.</S>
    <S sid="462" ssid="123">In particular, we model here selectional preferences (how plausible a noun is as subject/object of a verb), but our method is generalizable to many other semantic tasks that pertain to composition constraints; that is, they require measuring the goodness of fit of a word/concept as argument filler of another word/concept, including assigning semantic roles, logical metonymy, coercion (Pustejovsky 1995), and many other challenges.</S>
    <S sid="463" ssid="124">The selectional preference test sets are based on averages of human judgments on a seven-point scale about the plausibility of nouns as arguments (either subjects or objects) of verbs.</S>
    <S sid="464" ssid="125">The McRae data set (McRae, Spivey-Knowlton, and Tanenhaus 1998) consists of 100 noun&#8211;verb pairs rated by 36 subjects.</S>
    <S sid="465" ssid="126">The Pad&#180;o set (Pad&#180;o 2007) has 211 pairs rated by 20 subjects.</S>
    <S sid="466" ssid="127">For each verb, we first use the W1xLW2 space to select a set of nouns that are highly associated with the verb via a subject or an object link.</S>
    <S sid="467" ssid="128">In this space, nouns are represented as vectors with dimensions that are labeled with (link, word) tuples, where the word might be a verb, and the link might stand for, among other things, syntactic relations such as obj (or, in the LexDM model, an expansion thereof, such as obj+the-j).</S>
    <S sid="468" ssid="129">To find nouns that are highly associated with a verb v when linked by the subject relation, we project the W1xLW2 vectors onto a subspace where all dimensions are mapped to 0 except the dimensions that are labeled with (lsbj, v), where lsbj is a link containing either the string sbj intr or the string sbj tr, and v is the verb.</S>
    <S sid="469" ssid="130">We then measure the length of the noun vectors in this subspace, and pick the top n longest ones as prototypical subjects of the verb.</S>
    <S sid="470" ssid="131">The same operation is performed for the object relation.</S>
    <S sid="471" ssid="132">In our experiments, we set n to 20, but this is of course a parameter that should be explored.</S>
    <S sid="472" ssid="133">We normalize and sum the vectors (in the full W1xLW2 space) of the picked nouns, to obtain a centroid that represents an abstract &#8220;subject prototype&#8221; for the verb (and analogously for objects).</S>
    <S sid="473" ssid="134">The plausibility of an arbitrary noun as the subject (object) of a verb is then measured by the cosine of the noun vector to the subject (object) centroid in W1xLW2 space.</S>
    <S sid="474" ssid="135">Crucially, the algorithm can provide plausibility scores for nouns that do not co-occur with the target verb in the corpus, by looking at how close they are to the centroid of nouns that do often co-occur with the verb.</S>
    <S sid="475" ssid="136">The corpus may contain neither eat topinambur nor eat sympathy, but the topinambur vector will likely be closer to the prototypical eat object vector than the one of sympathy would be.</S>
    <S sid="476" ssid="137">It is worth stressing that the whole process relies on a single W1xLW2 matrix: This space is first used to identify typical subjects (or objects) of a verb via subspacing, then to construct centroid vectors for the verb subject (object) prototypes, and finally to measure the distance of nouns to these centroids.</S>
    <S sid="477" ssid="138">Our method is essentially the same, save for implementation and parameter choice details, as the one proposed by Pad&#180;o, Pad&#180;o, and Erk (2007), in turn inspired by Erk (2007).</S>
    <S sid="478" ssid="139">However, they treat the identification of typical argument fillers of a verb as an operation to be carried out using different resources, whereas we reinterpret it as a different way to use the same W1&#215;LW2 space in which we measure plausibility.</S>
    <S sid="479" ssid="140">Following Pad&#180;o and colleagues, we measure performance by the Spearman p correlation coefficient between the average human ratings and the model predictions, considering only verb&#8211;noun pairs that are present in the model.</S>
    <S sid="480" ssid="141">Table 7 reports percentage coverage and correlations for the DM models (the task requires the links to extract typical subjects and objects, so we cannot use DV nor Win), results from Pad&#180;o, Pad&#180;o, and Erk (2007) (ParCos is the best among their purely corpus-based systems), and the performance on the Pad&#180;o data set of the supervised system of Herda&#711;gdelen and Baroni (2009).</S>
    <S sid="481" ssid="142">Testing for significance of the correlation coefficients with two-tailed tests based on a Spearman-coefficient derived t statistic, we find that the Resnik&#8217;s model correlation for the McRae data is not significantly different from 0 (t = 0.29, df = 92, p = 0.39), ParCos on McRae is significant at &#945; = .05 (t = 2.134, df = 89, p = 0.018), and all other models on either data set are significant at &#945; = .01 and below.</S>
    <S sid="482" ssid="143">TypeDM emerges as an excellent model to tackle selectional preferences, and as the overall winner on this task.</S>
    <S sid="483" ssid="144">On the Pad&#180;o data set, it is as good as Pad&#180;o&#8217;s (2007) FrameNet based model, and it is outperformed only by the supervised BagPack approach.</S>
    <S sid="484" ssid="145">On the McRae data set, all three DM models do very well, and TypeDM is slightly worse than the other two models.</S>
    <S sid="485" ssid="146">On this data set, the DM models are outperformed by Pad&#180;o&#8217;s FrameNet model in terms of correlation, but the latter has a much lower coverage, suggesting that for practical purposes the DM models are a better choice.</S>
    <S sid="486" ssid="147">According to Raghunathan&#8217;s test (see Section 6.1.1), the difference in correlation with human ratings among the three DM models is not significant on the McRae data, where TypeDM is below the other models (Q = 0.19, df = 0.67, p = 0.50).</S>
    <S sid="487" ssid="148">On the Pad&#180;o data set, on the other hand, where TypeDM outperforms the other DM models, the same difference is highly significant (Q = 12.70, df = 1.00, p &lt; 0.001).</S>
    <S sid="488" ssid="149">As a final remark on the W1&#215;LW2 space, we can notice that DM models perform very well in tasks involving attributional similarity.</S>
    <S sid="489" ssid="150">The performance of unstructured DSMs (including Win, our own implementation of this type of model) is also high, sometimes even better than that of structured DSMs.</S>
    <S sid="490" ssid="151">However, our best DM model also achieves brilliant results in capturing selectional preferences, a task that is not directly addressable by unstructured DSMs.</S>
    <S sid="491" ssid="152">This fact suggests that the real advantage provided by structured DSMs&#8212;particularly when linguistic structure is suitably exploited, as with the DM third-order tensor&#8212;actually resides in their versatility in addressing a much larger and various range of semantic tasks.</S>
    <S sid="492" ssid="153">This preliminary conclusion will also be confirmed by the experiments modeled with the other DM spaces.</S>
    <S sid="493" ssid="154">The vectors of this space are labeled with word pair tuples (w1, w2) (columns of matrix Bmode-2 in Table 3) and their dimensions are labeled with links l (rows of the same matrix).</S>
    <S sid="494" ssid="155">This arrangement of our tensor reproduces the &#8220;relational similarity&#8221; space of Turney (2006b), also implicitly assumed in much relation extraction work, where word pairs are compared based on the patterns that link them in the corpus, in order to measure the similarity of their relations (Pantel and Pennacchiotti 2006).</S>
    <S sid="495" ssid="156">The links that in W1xLW2 space provide a form of shallow typing of lexical features ((use, gun)) associated with single words (soldier) constitute under the W1W2xL view full features (use) associated with word pairs ((soldier, gun)).</S>
    <S sid="496" ssid="157">Besides exploiting this view of the tensor to solve classic relational tasks, we will also show how problems that have not been traditionally defined in terms of a word-pair-by-link matrix, such as qualia harvesting with patterns or generating lists of characteristic properties, can be elegantly recast in the W1W2xL space by measuring the length of (w1,w2) vectors in a link (sub)space, thus bringing a wider range of semantic operations under the umbrella of the natural DM spaces.</S>
    <S sid="497" ssid="158">The W1W2xL space represents pairs of words that co-occur in the corpus within the maximum span determined by the scope of the links connecting them (for our models, this maximum span is never larger than a single sentence).</S>
    <S sid="498" ssid="159">When words do not co-occur or only co-occur very rarely (and even in large corpora this will often be the case), attributional similarity can come to the rescue.</S>
    <S sid="499" ssid="160">Given a target pair, we can construct other, probably similar pairs by replacing one of the words with an attributional neighbor.</S>
    <S sid="500" ssid="161">For example, given the pair (automobile, wheel), we might discover in W1xLW2 space that car is a close neighbor of automobile.</S>
    <S sid="501" ssid="162">We can then look for the pair (car, wheel), and use relational evidence about this pair as if it pertained to (automobile, wheel).</S>
    <S sid="502" ssid="163">This is essentially the way to deal with W1W2xL data sparseness proposed by Turney (2006b), except that he relies on independently harvested attributional and relational spaces, whereas we derive both from the same tensor.</S>
    <S sid="503" ssid="164">More precisely, in the W1W2xL tasks where we know the set of target pairs in advance (Sections 6.2.1 and 6.2.2), we smooth the DM models by combining in turn one of the words of each target pair with the top 20 nearest W1xLW2 neighbors of the other word, obtaining a total of 41 pairs (including the original).</S>
    <S sid="504" ssid="165">The centroid of the W1W2xL vectors of these pairs is then taken to represent a target pair (the smoothed (automobile, wheel) vector is an average of the (automobile, wheel), (car, wheel), (automobile, circle), etc., vectors).</S>
    <S sid="505" ssid="166">The nearest neighbors are efficiently searched in the W1xLW2 matrix by compressing it to 5,000 dimensions via random indexing, using the parameters suggested by Sahlgren (2005).</S>
    <S sid="506" ssid="167">Smoothing consistently improved performance, and we only report the relevant results for the smoothed versions of the models (including our implementation of LRA, to be discussed next).</S>
    <S sid="507" ssid="168">We reimplemented Turney&#8217;s Latent Relational Analysis (LRA) model, training it on our source corpus (LRA is trained separately for each test set, because it relies on a given list of word pairs to find the patterns that link them).</S>
    <S sid="508" ssid="169">We chose the parameter values of Turney&#8217;s main model (his &#8220;baseline LRA system&#8221;).</S>
    <S sid="509" ssid="170">In short (see Turney&#8217;s article for details), for a given set of target pairs we count all the patterns that connect them, in either order, in the corpus.</S>
    <S sid="510" ssid="171">Patterns are sequences of one to three words occurring between the targets, with all, none, or any subset of the elements replaced by wildcards (with the, with *, * the, * *).</S>
    <S sid="511" ssid="172">Only the top 4,000 most frequent patterns are preserved, and a target-pairby-pattern matrix is constructed (with 8,000 dimensions, to account for directionality).</S>
    <S sid="512" ssid="173">Values in the matrix are log- and entropy-transformed using Turney&#8217;s formula.</S>
    <S sid="513" ssid="174">Finally, SVD is applied, reducing the columns to the top 300 latent dimensions (here and subsequently, we use SVDLIBC9 to perform SVD).</S>
    <S sid="514" ssid="175">For simplicity and to make LRA more directly comparable to the DM models, we applied our attributional-neighbor-based smoothing technique (the neighbors for target pair expansion are taken from the best attributional DM model, namely, TypeDM) instead of the more sophisticated one used by Turney.</S>
    <S sid="515" ssid="176">Thus, our LRA implementation differs from Turney&#8217;s original in two aspects: the smoothing method and the source corpus (Turney uses a corpus of more than 50 billion words).</S>
    <S sid="516" ssid="177">Neither variation pertains to inherent differences between LRA and DM.</S>
    <S sid="517" ssid="178">Given the appropriate resources, a DM model could be trained on Turney&#8217;s gigantic corpus, and smoothed with his technique.</S>
    <S sid="518" ssid="179">6.2.1 Solving Analogy Problems.</S>
    <S sid="519" ssid="180">The SAT test set introduced by Turney and collaborators contains 374 multiple-choice questions from the SAT college entrance exam.</S>
    <S sid="520" ssid="181">Each question includes one target (ostrich&#8211;bird) and five candidate analogies (lion&#8211;cat, goose&#8211;flock, ewe&#8211;sheep, cub&#8211;bear, primate&#8211;monkey).</S>
    <S sid="521" ssid="182">The data set is dominated by noun&#8211;noun pairs, but all other combinations are also attested (noun&#8211;verb, verb&#8211;adjective, verb&#8211;verb, etc.)</S>
    <S sid="522" ssid="183">The task is to choose the candidate pair most analogous to the target (lion&#8211;cat in the previous example).</S>
    <S sid="523" ssid="184">This is essentially the same task as the TOEFL, but applied to word pairs instead of words.</S>
    <S sid="524" ssid="185">As in the TOEFL, we pick the candidate with the highest cosine with the target as the right analogy.</S>
    <S sid="525" ssid="186">Table 8 reports our SAT results together with those of other corpus-based methods from the ACL Wiki and other systems.</S>
    <S sid="526" ssid="187">TypeDM is again emerging as the best among our models.</S>
    <S sid="527" ssid="188">To put its performance in context statistically, according to a Fisher test its accuracy is not significantly different from that of VSM (p = 0.239), whereas it is better than that of PMI-IR-06 (p = 0.043; even the bottom model, LexDM, is significantly better than the random guesser, p = 0.004).</S>
    <S sid="528" ssid="189">TypeDM is at least as good as LRA when the latter is trained on the same data and smoothed with our method, suggesting that the excellent performance of Turney&#8217;s version of LRA (LRA-06) is due to the fact that he used a much larger corpus, and/or to his more sophisticated smoothing technique, and not to the specific way in which LRA collects corpus-based statistics.</S>
    <S sid="529" ssid="190">All the algorithms with higher accuracy than TypeDM are based on much larger input corpora, except BagPack, which is, however, supervised.</S>
    <S sid="530" ssid="191">The LSA system of Quesada, Mangalath, and Kintsch (2004), which performs similarly to TypeDM, is based on a smaller corpus, but it relies on hand-coded &#8220;analogy domains&#8221; that are represented by lists of manually selected characteristic words.</S>
    <S sid="531" ssid="192">6.2.2 Relation Classification.</S>
    <S sid="532" ssid="193">Just as the SAT is the relational equivalent of the TOEFL task, the test sets we tackle next are a relational analog to attributional concept clustering, in that they require grouping pairs of words into classes that instantiate the same relations.</S>
    <S sid="533" ssid="194">Whereas we cast attributional categorization as an unsupervised clustering problem (following much of the earlier literature), the common approach to classifying word pairs by relation is supervised, and relies on labeled examples for training.</S>
    <S sid="534" ssid="195">In this article, we exploit training data in a very simple way, via a nearest centroid method.</S>
    <S sid="535" ssid="196">In the SEMEVAL task we are about to introduce, where both positive and negative examples are available for each class, we use the positive examples to construct a centroid that represents a target class, and negative examples to construct a centroid representing items outside the class.</S>
    <S sid="536" ssid="197">We then decide if a test pair belongs to the target class by measuring its distance from the positive and negative centroids, picking the nearest one.</S>
    <S sid="537" ssid="198">For example, the Cause&#8211;Effect relation has positive training examples such as cycling&#8211;happiness and massage&#8211;relief and negative examples such as customer&#8211;satisfaction and exposure&#8211;protection.</S>
    <S sid="538" ssid="199">We create a positive centroid by summing the W1W2&#215;L vectors of the first set of pairs, and a negative centroid by summing the latter.</S>
    <S sid="539" ssid="200">We then measure the cosine of a test item such as smile&#8211;wrinkle with the centroids, and decide if it instantiates the Cause&#8211;Effect relation based on whether it is closer to the positive or negative centroid.</S>
    <S sid="540" ssid="201">For the other tasks (as well as the transitive alternation task of Section 6.3), we do not have negative examples, but positive examples for different classes.</S>
    <S sid="541" ssid="202">We create a centroid for each class, and classify test items based on the centroid they are nearest to.</S>
    <S sid="542" ssid="203">Our first test pertains to the seven relations between nominals in Task 4 of SEMEVAL 2007 (Girju et al. 2007): Cause&#8211;Effect, Instrument&#8211;Agency, Product&#8211; Producer, Origin&#8211;Entity, Theme&#8211;Tool, Part&#8211;Whole, Content&#8211;Container.</S>
    <S sid="543" ssid="204">For each relation, the data set includes 140 training and about 80 test items.</S>
    <S sid="544" ssid="205">Each item consists of a Web snippet, containing word pairs connected by a certain pattern (e.g., &#8220;* causes *&#8221;).</S>
    <S sid="545" ssid="206">The retrieved snippets are manually classified by the SEMEVAL organizers as positive or negative instances of a certain relation (see the earlier Cause&#8211;Effect examples).</S>
    <S sid="546" ssid="207">About 50% training and test cases are positive instances.</S>
    <S sid="547" ssid="208">In our experiments we do not make use of the contexts of the target word pairs that are provided with the test set.</S>
    <S sid="548" ssid="209">The second data set (NS) comes from Nastase and Szpakowicz (2003).</S>
    <S sid="549" ssid="210">It pertains to the classification of 600 modifier&#8211;noun pairs and it is of interest because it proposes a very fine-grained categorization into 30 semantic classes, such as Cause (cloud&#8211;storm), Purpose (album&#8211;picture), Location-At (pain&#8211;chest), Location-From (visitor&#8211;country), Frequency (superstition&#8211;occasional), Time-At (snack&#8211;midnight), and so on.</S>
    <S sid="550" ssid="211">The modifiers can be nouns, adjectives, or adverbs.</S>
    <S sid="551" ssid="212">Because the data set is not split into training and test data we follow Turney (2006b) and perform leave-one-out cross-validation.</S>
    <S sid="552" ssid="213">The data set also comes with a coarser five-way classification.</S>
    <S sid="553" ssid="214">Our unreported results on it are comparable, in terms of relative performance, to the ones for the 30-way classification.</S>
    <S sid="554" ssid="215">The last data set (OC) contains 1,443 noun&#8211;noun compounds classified by O&#180; S&#180;eaghdha and Copestake (2009) into 6 relations: Be (celebrity&#8211;winner), Have (door&#8211;latch), In (air&#8211;disaster), Actor (university&#8211;scholarship), Instrument (freight&#8211;train), and About (bank&#8211;panic); see O&#180; S&#180;eaghdha and Copestake (2009) and references there.</S>
    <S sid="555" ssid="216">We use the same five-way cross-validation splits as the data set proponents.</S>
    <S sid="556" ssid="217">Table 9 reports performance of models from our experiments and from the literature on the three supervised relation classification tasks.</S>
    <S sid="557" ssid="218">Following the relevant earlier studies, for SEMEVAL we report macro-averaged accuracy, whereas for the other two data sets we report global accuracy (with binomial confidence intervals).</S>
    <S sid="558" ssid="219">All other measures are macro-averaged.</S>
    <S sid="559" ssid="220">Majority is the performance of a classifier that always guesses the majority class in the test set (in SEMEVAL, for each class, it guesses that all or no items belong to it depending on whether there are more positive or negative examples in the test data; in the other tasks, it labels all items with the majority class).</S>
    <S sid="560" ssid="221">AllTrue always assigns an item to the target class (being inherently binary, it does not provide a welldefined multi-class global accuracy).</S>
    <S sid="561" ssid="222">ProbMatch randomly guesses classes matching their distribution in the test data (in SEMEVAL, it matches the proportion of positive and negative examples within each class).</S>
    <S sid="562" ssid="223">For SEMEVAL, the table reports the results of those models that took part in the shared task and, like ours, did not use the organizer-provided WordNet sense labels nor information about the query used to retrieve the examples.</S>
    <S sid="563" ssid="224">All these models are outperformed by TypeDM, despite the fact that they exploit the training contexts and/or specific additional resources: an annotated compound database (UCD-FC), more sophisticated machine learning algorithms to train the relation classifiers (ILK, UCD-FC), Web counts (UCB), and so on.</S>
    <S sid="564" ssid="225">For the NS data set, none of the DM models do well, although TypeDM is once more the best among them.</S>
    <S sid="565" ssid="226">The DM models are outperformed by other models from the literature, all trained on much larger corpora, and also by our implementation of LRA.</S>
    <S sid="566" ssid="227">The difference in global accuracy between LRA and TypeDM is significant (Fisher test, p = 0.00002).</S>
    <S sid="567" ssid="228">TypeDM&#8217;s accuracy is nevertheless well above the best (Majority) baseline accuracy (p = 0.0001).</S>
    <S sid="568" ssid="229">The OC results confirm that TypeDM is the best of our models, again (slightly) outperforming our LRA implementation.</S>
    <S sid="569" ssid="230">Still, our best performance is well below that of OC-Comb, the absolute best, and OC-Rel, the best purely relational model of O&#180; S&#180;eaghdha and Copestake (2009) (the difference in global accuracy between the latter and TypeDM is highly significant, p &lt; 0.000001).</S>
    <S sid="570" ssid="231">O&#180; S&#180;eaghdha and Copestake use sophisticated kernel-based methods and extensive parameter tuning to achieve these results.</S>
    <S sid="571" ssid="232">We hope that the TypeDM performance would also improve by improving the machine learning aspects of the procedure.</S>
    <S sid="572" ssid="233">As an ad interim summary, we observe that TypeDM achieves competitive results in semantic tasks involving relational similarity.</S>
    <S sid="573" ssid="234">In particular, in both analogy solving and two out of three relation classification experiments, TypeDM is at least as good as our LRA implementation.</S>
    <S sid="574" ssid="235">We now move on to show how this same view of the DM tensor can be successfully applied to aspects of meaning that are not normally addressed by relational DSMs.</S>
    <S sid="575" ssid="236">6.2.3 Qualia Extraction.</S>
    <S sid="576" ssid="237">A popular alternative to the supervised approach to relation extraction is to pick a set of lexico-syntactic patterns that should capture the relation of interest and to harvest pairs they connect in text, as famously illustrated by Hearst (1992) for the hyponymy relation.</S>
    <S sid="577" ssid="238">In the DM approach, instead of going back to the corpus to harvest the patterns, we exploit the information already available in the W1W2&#215;L space.</S>
    <S sid="578" ssid="239">We select promising links as our equivalent of patterns and we measure the length of word pair vectors in the W1W2&#215;L subspace defined by these links.</S>
    <S sid="579" ssid="240">We illustrate this with the data set of Cimiano and Wenderoth (2007), which contains qualia structures (Pustejovsky 1995) for 30 nominal concepts, both concrete (door) and abstract (imagination).</S>
    <S sid="580" ssid="241">Cimiano and Wenderoth asked 30 subjects to produce qualia for these words (each word was rated by at least three subjects), obtaining a total of 1,487 word&#8211; quale pairs, instantiating the four roles postulated by Pustejovsky: Formal (the category of the object: door&#8211;barrier), Constitutive (constitutive parts, materials the object is made of: food&#8211;fat), Agentive (what brings the object about: letter&#8211;write), and Telic (the function of the object: novel&#8211;entertain).</S>
    <S sid="581" ssid="242">We approximate the patterns proposed by Cimiano and Wenderoth by manually selecting links that are already in our DM models, as reported in Table 10 (here and subsequently when discussing qualia-harvesting links, we use n and q to indicate the linear position of the noun and the potential quale with respect to the link).</S>
    <S sid="582" ssid="243">All qualia roles have links pertaining to noun&#8211;noun pairs.</S>
    <S sid="583" ssid="244">The Agentive and Telic patterns also harvest noun&#8211;verb pairs.</S>
    <S sid="584" ssid="245">For LexDM, we pick all links that begin with one of the strings in Table 10.</S>
    <S sid="585" ssid="246">For the DepDM model, the only attested links are n with q (Constitutive), n sbj intr q, n sbj tr q (Telic), and q obj n (Agentive).</S>
    <S sid="586" ssid="247">Consequently, DepDM does not harvest Formal qualia, and is penalized accordingly in the evaluation.</S>
    <S sid="587" ssid="248">We project all W1W2xL vectors that contain a target noun onto each of the four subspaces determined by the quale-specific link sets, and we compute their subspace lengths.</S>
    <S sid="588" ssid="249">Given a target noun n and a potential quale q, the length of the (n, q) vector in the subspace characterized by the links that represent role r is our measure of how good q is as a quale of type r for n (for example, the length of (book, read) in the subspace defined by the Telic links is our measure of fitness of read as Telic role of book).</S>
    <S sid="589" ssid="250">We use length in the subspace associated to the qualia role r to rank all (n, q) pairs relevant to r. Following Cimiano and Wenderoth&#8217;s evaluation method, for each noun we first compute, separately for each role, the ranked list precision (with respect to the manually constructed qualia structure) at 11 equally spaced recall levels from 0% to 100%.</S>
    <S sid="590" ssid="251">We select the precision, recall, and F values at the recall level that results in the highest F score (i.e., in the best precision&#8211;recall trade-off).</S>
    <S sid="591" ssid="252">We then average across the roles, and then across target nouns.</S>
    <S sid="592" ssid="253">The task, as framed here, cannot be run with the LRA model, and, because of its open-ended nature (we do not start from a predefined list of pairs), we do not smooth the models.</S>
    <S sid="593" ssid="254">Table 11 reports the performance of our models, as well as the F scores reported by Cimiano and Wenderoth.</S>
    <S sid="594" ssid="255">For our models, where we have access to the itemized data, we also report the standard deviation of F across the target nouns.</S>
    <S sid="595" ssid="256">All the DM models perform well (including DepDM, which is disfavored by the lack of Formal links), and once more TypeDM emerges as the best among them, with an F value that is also (slightly) above the best Cimiano and Wenderoth models (that are based on co-occurrence counts from the whole Web).</S>
    <S sid="596" ssid="257">Despite the large standard deviations, the difference in F across concepts between TypeDM and the second-best DM model (DepDM) is highly significant (paired t-test, t = 4.02, df = 29, p &lt; 0.001), suggesting that the large variance is due to different degrees of difficulty of the concepts, affecting the models in similar ways.</S>
    <S sid="597" ssid="258">Links approximating the patterns proposed in Cimiano and Wenderoth (2007).</S>
    <S sid="598" ssid="259">automated generation of commonsense concept descriptions in terms of intuitively salient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb 2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al. 2010).</S>
    <S sid="599" ssid="260">Similar property lists, collected from subjects in elicitation tasks, are widely used in cognitive science as surrogates of mental features (Garrard et al. 2001; McRae et al.</S>
    <S sid="600" ssid="261">2005; Vinson and Vigliocco 2008).</S>
    <S sid="601" ssid="262">Large-scale collections of property-based concept descriptions are also carried out in AI, where they are important for commonsense reasoning (Liu and Singh 2004).</S>
    <S sid="602" ssid="263">In the qualia task, given a concept we had to extract properties of certain kinds (corresponding to the qualia roles).</S>
    <S sid="603" ssid="264">The property-based description task is less constrained, because the most salient relations of a nominal concept might be in all sorts of relations with it (parts, typical behaviors, location, etc.).</S>
    <S sid="604" ssid="265">Still, we couch the task of unconstrained property extraction as a challenge in the W1W2xL space.</S>
    <S sid="605" ssid="266">The approach is similar to the method adopted for qualia roles, but now the whole W1W2xL space is used, instead of selected subspaces.</S>
    <S sid="606" ssid="267">Given all the (n, w2) pairs that have the target nominal concept as first element, we rank them by length in the W1W2xL space.</S>
    <S sid="607" ssid="268">The longest (n, w2) vectors in this space should correspond to salient properties of the target concept, as we expect a concept to often co-occur in texts with its important properties (because in the current DM implementations links are disjoint across POS, we map properties with different POS onto the same scale by dividing the length of the vector representing a pair by the length of the longest vector in the harvested concept&#8211;property set that has the same POS pair).</S>
    <S sid="608" ssid="269">For example, among the longest W1W2xL vectors with car as first item we find (car, drive), (car, park), and (car, engine).</S>
    <S sid="609" ssid="270">The first two pairs are normalized by dividing by the longest (noun, verb) vector in the harvested set, the third by dividing by the longest (noun, noun) vector.</S>
    <S sid="610" ssid="271">We test this approach in the ESSLLI 2008 Distributional Semantic Workshop unconstrained property generation challenge (Baroni, Evert, and Lenci 2008).</S>
    <S sid="611" ssid="272">The data set contains, for each of 44 concrete concepts, 10 properties that are those that were most frequently produced by subjects in the elicitation experiment of McRae et al. (2005) (the &#8220;gold standard lists&#8221;).</S>
    <S sid="612" ssid="273">Algorithms must generate lists of 10 properties per concept, and performance is measured by overlap with the subject-produced properties, that is, by the cross-concept average proportions of properties in the generated lists that are also in the corresponding gold standard lists.</S>
    <S sid="613" ssid="274">Smoothing would be very costly (we would need to smooth all pairs that contain a target concept) and probably counterproductive (as the most typical properties of a concept should be highly specific to it, rather than shared with neighbors).</S>
    <S sid="614" ssid="275">Because LRA (at least in a reasonably efficient implementation) requires a priori specification of the target pairs, it is not well suited to this task.</S>
    <S sid="615" ssid="276">Table 12 reports the percentage overlap with the gold standard properties (averaged across the 44 concepts) for our models as well as the only ESSLLI 2008 participant that tried this task, and for the models of Baroni et al. (2010).</S>
    <S sid="616" ssid="277">TypeDM is the best DM model, and it also does quite well compared to the state of the art.</S>
    <S sid="617" ssid="278">The difference between Strudel, the best model from the earlier literature, and TypeDM is not statistically significant, according to a paired t-test across the target concepts (t = 1.1, df = 43, p = 0.27).</S>
    <S sid="618" ssid="279">The difference between TypeDM and DV-10, the second best model from the literature, is highly significant (t = 2.9, df = 43, p &lt; 0.01).</S>
    <S sid="619" ssid="280">If we consider how difficult this sort of open-ended task is (see the very low performance of the respectable models at the bottom of the list), matching on average two out of ten speaker-generated properties, as TypeDM does, is an impressive feat.</S>
    <S sid="620" ssid="281">The vectors of this space are labeled with binary tuples of type (w1, l) (columns of matrix Cmode-3 in Table 3), and their dimensions are labeled with words w2 (rows of the same matrix).</S>
    <S sid="621" ssid="282">We illustrate this space in the task of discriminating verbs participating in different argument alternations.</S>
    <S sid="622" ssid="283">However, other uses of the space can also be foreseen.</S>
    <S sid="623" ssid="284">For example, the rows of W1LxW2 correspond to the columns of the W1xLW2 space (given the constraints on the tuple structure we adopted in Section 3.1).</S>
    <S sid="624" ssid="285">We could use the former space for feature smoothing or selection in the latter space, for example, by merging the features of W1xLW2 whose corresponding vectors in W1LxW2 have a cosine similarity over a given threshold.</S>
    <S sid="625" ssid="286">We leave this possibility to further work.</S>
    <S sid="626" ssid="287">Among the linguistic objects represented by the W1LxW2 vectors, we find the syntactic slots of verb frames.</S>
    <S sid="627" ssid="288">For instance, the vector labeled with the tuple (read, sbj&#8722;1) represents the subject slot of the verb read in terms of the distribution of its noun fillers, which label the dimensions of the space.</S>
    <S sid="628" ssid="289">We can use the W1LxW2 space to explore the semantic properties of syntactic frames, and to extract generalizations about the inner structure of lexico-semantic representations of the sort formal semanticists have traditionally been interested in.</S>
    <S sid="629" ssid="290">For instance, the high similarity between the object slot of kill and the subject slot of die might provide a distributional correlate to the classic cause(subj,die(obj)) analysis of killing by Dowty (1977) and many others.</S>
    <S sid="630" ssid="291">Measuring the cosine between the vectors of different syntactic slots of the same verb corresponds to estimating the amount of fillers they share.</S>
    <S sid="631" ssid="292">Measures of &#8220;slot overlap&#8221; have been used by Joanis, Stevenson, and James (2008) as features to classify verbs on the basis of their argument alternations.</S>
    <S sid="632" ssid="293">Levin and Rappaport-Hovav (2005) define argument alternations as the possibility for verbs to have multiple syntactic realizations of their semantic argument structure.</S>
    <S sid="633" ssid="294">Alternations involve the expression of the same semantic argument in two different syntactic slots.</S>
    <S sid="634" ssid="295">We expect that, if a verb undergoes a particular alternation, then the set of nouns that appear in the two alternating slots should overlap to a certain degree.</S>
    <S sid="635" ssid="296">Argument alternations represent a key aspect of the complex constraints that shape the syntax&#8211;semantics interface.</S>
    <S sid="636" ssid="297">Verbs differ with respect to the possible alternations they can undergo, and this variation is strongly dependent on their semantic properties (semantic roles, event type, etc.).</S>
    <S sid="637" ssid="298">Levin (1993) has in fact proposed a well-known classification of verbs based on their range of syntactic alternations.</S>
    <S sid="638" ssid="299">Recognizing the alternations licensed by a verb is extremely important in capturing its argument structure properties, and consequently in describing its semantic behavior.</S>
    <S sid="639" ssid="300">We focus here on a particular class of alternations, namely transitivity alternations, whose verbs allow both for a transitive NP V NP variant and for an intransitive NP V (PP) variant (Levin 1993).</S>
    <S sid="640" ssid="301">We use the W1LxW2 space to carry out the automatic classification of verbs that participate in different types of transitivity alternations.</S>
    <S sid="641" ssid="302">In the causative/inchoative alternation, the object argument (e.g., John broke the vase) can also be realized as an intransitive subject (e.g., The vase broke).</S>
    <S sid="642" ssid="303">In a first experiment, we use the W1LxW2 space to discriminate between transitive verbs undergoing the causative/inchoative alternation (C/I) (e.g., break) and non-alternating ones (e.g., mince; cf.</S>
    <S sid="643" ssid="304">John minced the meat vs. *The meat minced).</S>
    <S sid="644" ssid="305">The C/I data set was introduced by Baroni and Lenci (2009), but not tested in a classification task there.</S>
    <S sid="645" ssid="306">It consists of 232 causative/inchoative verbs and 170 non-alternating transitive verbs from Levin (1993).</S>
    <S sid="646" ssid="307">In a second experiment, we apply the W1LxW2 space to discriminate verbs that belong to three different classes, each corresponding to a different type of transitive alternation.</S>
    <S sid="647" ssid="308">We use the MS data set (Merlo and Stevenson 2001), which includes 19 unergative verbs undergoing the induced action alternation (e.g., race), 19 unaccusative verbs that undergo the causative/inchoative alternation (e.g., break), and 20 object-drop verbs participating in the unexpressed object alternation (e.g., play).</S>
    <S sid="648" ssid="309">See Levin (1993) for details about each of these transitive alternations.</S>
    <S sid="649" ssid="310">The complexity of this task is due to the fact that the verbs in the three classes have both transitive and intransitive variants, but with very different semantic roles.</S>
    <S sid="650" ssid="311">For instance, the transitive subject of unaccusative (The man broke the vase) and unergative verbs (The jockey raced the horse past the barn) is an agent of causation, whereas the subject of the intransitive variant of unaccusative verbs has a theme role (i.e., undergoes a change of state: The vase broke), and the intransitive subject of unergative verbs has instead an agent role (The horse raced past the barn).</S>
    <S sid="651" ssid="312">Thus, their surface identity notwithstanding, the semantic properties of the syntactic slots of the verbs in each class are very different.</S>
    <S sid="652" ssid="313">By testing the W1LxW2 space on such a task we can therefore evaluate its ability to capture non-trivial properties of the verb&#8217;s thematic structure.</S>
    <S sid="653" ssid="314">We address these tasks by measuring the similarities between the W1LxW2 vectors of the transitive subject, intransitive subject, and direct object slots of a verb, and using these inter-slot similarities to classify the verb.</S>
    <S sid="654" ssid="315">For instance, given the definition of the C/I alternation, we can predict that with alternating verbs the intransitive subject slot should be similar to the direct object slot (the things that are broken also break), while this should not hold for non-alternating verbs (mincees are very different from mincers).</S>
    <S sid="655" ssid="316">For each verb v in a data set, we extract the corresponding W1LxW2 slot vectors (v, l) whose links are sbj intr, sbj tr, and obj (for LexDM, we sum the vectors with links beginning with one of these three patterns).</S>
    <S sid="656" ssid="317">Then, for each v we build a three-dimensional vector with the cosines between the three slot vectors.</S>
    <S sid="657" ssid="318">These second order vectors encode the profile of similarity across the slots of a verb, and can be used to spot verbs that have comparable profiles (e.g., verbs that have a high similarity between their subj intr and obj slots).</S>
    <S sid="658" ssid="319">We model both experiments as classification tasks using the nearest centroid method on the three-dimensional vectors, with leave-one-out cross-validation.</S>
    <S sid="659" ssid="320">We perform binary classification of the C/I data set (treating non-alternating verbs as negative examples), and three-way classification of the MS data.</S>
    <S sid="660" ssid="321">Table 13 reports the results, with the baselines computed similarly to the ones in Section 6.2.2 (for C/I, Majority is equivalent to AllTrue).</S>
    <S sid="661" ssid="322">The DM performance is also compared with the results of Merlo and Stevenson (2001) for their classifiers tested with the leave-one-out methodology (macroaveraged F has been computed on the class-by-class scores reported in that article).</S>
    <S sid="662" ssid="323">All the DM models discriminate the verb classes much more reliably than the baselines.</S>
    <S sid="663" ssid="324">The accuracy of DepDM, the worst DM model, is significantly higher than that of the best baselines, AllTrue in C/I (Fisher test, p = 0.024) and Majority on MS (p = 0.039).</S>
    <S sid="664" ssid="325">TypeDM is again our best model.</S>
    <S sid="665" ssid="326">Its performance is comparable to the lower range of the Merlo and Stevenson classifiers (considering the large confidence intervals due to the small sample size, the accuracy of TypeDM is not significantly below even that of the top model NoPass; p = 0.43).</S>
    <S sid="666" ssid="327">The TypeDM results were obtained simply by measuring the verb inter-slot similarities in the W1L&#215;W2 space.</S>
    <S sid="667" ssid="328">Conversely, the classifiers in Merlo and Stevenson (2001) rely on a much larger range of knowledge-intensive features selected in an ad hoc fashion for this task (on the other hand, their training corpus is not parsed and it is much smaller than ours).</S>
    <S sid="668" ssid="329">Finally, we can notice that in both experiments the mildly (TypeDM) and heavily (LexDM) lexicalized DM models score better than their non-lexicalized counterpart (DepDM), although the difference between the best DM model and DepDM is not significant on either data set (p = 0.23 for the LexDM/DepDM difference in C/I; p = 0.57 for the TypeDM/DepDM difference in MS).</S>
    <S sid="669" ssid="330">Verb alternations do not typically appear among the standard tasks on which DSMs are tested.</S>
    <S sid="670" ssid="331">Moreover, they involve non-trivial properties of argument structure.</S>
    <S sid="671" ssid="332">The good performance of DM in these experiments is therefore particularly significant in supporting its vocation as a general model for distributional semantics.</S>
    <S sid="672" ssid="333">The vectors of this space are labeled with links l (rows of matrix Bmode-2 in Table 3) and their dimensions are labeled with word pair tuples (w1, w2) (columns of the same matrix).</S>
    <S sid="673" ssid="334">Links are represented in terms of the word pairs they connect.</S>
    <S sid="674" ssid="335">The LxW1W2 space supports tasks where we are directly interested in the links as an object of study&#8212;for example, characterizing prepositions (Baldwin, Kordoni, and Villavicencio 2009) or measuring the relative similarity of different kinds of verb&#8211;noun relations.</S>
    <S sid="675" ssid="336">We focus here instead on a potentially more common use of LxW1W2 vectors as a &#8220;feature selection and labeling&#8221; space for W1W2xL tasks.</S>
    <S sid="676" ssid="337">Specifically, we go back to the qualia extraction task of Section 6.2.3.</S>
    <S sid="677" ssid="338">There, we started with manually identified links.</S>
    <S sid="678" ssid="339">Here, we start with examples of noun&#8211;quale pairs (n, qr) that instantiate a role r. We project all LxW1W2 vectors in a subspace where only dimensions corresponding to one of the example pairs are non-zero.</S>
    <S sid="679" ssid="340">We then pick the most characteristic links in this subspace to represent the target role r, and look for new pairs (n, qr) in the W1W2xL subspace defined by these automatically picked links, instead of the manual ones.</S>
    <S sid="680" ssid="341">Although we stop at this point, the procedure can be seen as a DM version of popular iterative bootstrapping algorithms such as Espresso (Pantel and Pennacchiotti 2006): Start with some examples of the target relation, find links that are typical of these examples, use the links to find new examples, and so on.</S>
    <S sid="681" ssid="342">In DM, the process does not go back to a corpus to harvest new links and example pairs, but it iterates between the column and row spaces of a pre-compiled matrix (i.e, the mode-2 matricization in Table 3).</S>
    <S sid="682" ssid="343">For each of the 30 noun concepts in the Cimiano and Wenderoth gold standard, we use the noun&#8211;quale pairs pertaining to the remaining 29 concepts as training examples to select a set of 20 links that we then use in the same way as the manually selected links of Section 6.2.3.</S>
    <S sid="683" ssid="344">Simply picking the longest links in the LxW1W2 subspace defined by the example (n, qr) dimensions does not work, because we harvest links that are frequent in general, rather than characteristic of the qualia roles (noun modification, of, etc.).</S>
    <S sid="684" ssid="345">For each role r, we construct instead two LxW1W2 subspaces, one positive subspace with the example pairs (n, qr) as unique non-zero dimensions, and a negative subspace with nonzero dimensions corresponding to all (w1,w2) pairs such that w1 is one of the training nominal concepts, and w2 is not a quale qr in the example pairs.</S>
    <S sid="685" ssid="346">We then measure the length of each link in both subspaces.</S>
    <S sid="686" ssid="347">For example, we measure the length of the obj link in a subspace characterized by (n, qtelic) example pairs, and the length of obj in a subspace characterized by (n, w2) pairs that are probably not Telic examples.</S>
    <S sid="687" ssid="348">We compute the pointwise mutual information (PMI) statistic (Church and Hanks 1990) on these lengths to find the links that are most typical of the positive subspace corresponding to each qualia role.</S>
    <S sid="688" ssid="349">PMI, with respect to other association measures, finds more specific links, which is good for our purposes.</S>
    <S sid="689" ssid="350">However, it is also notoriously prone to over-estimating the importance of rare items (Manning and Sch&#168;utze 1999, Chapter 5).</S>
    <S sid="690" ssid="351">Thus, before selecting the top 20 links ranked by PMI, we filter out those links that do not have at least 10 non-zero dimensions in the positive subspace.</S>
    <S sid="691" ssid="352">Many parameters here should be tuned more systematically (top n links, association measure, minimum non-zero dimensions), but the current results will nevertheless illustrate our methodology.</S>
    <S sid="692" ssid="353">Table 14 reports, for each quale, the TypeDM links that were selected in each of the 30 leave-one-concept-out folds.</S>
    <S sid="693" ssid="354">The links n is q, n in q, and q such as n are a good sketch of the Formal relation, which essentially subsumes various taxonomic relations.</S>
    <S sid="694" ssid="355">The other Formal links are less conspicuous.</S>
    <S sid="695" ssid="356">However, note the presence of noun coordination (n coord q and q coord n), consistently with the common claim that coordinated terms tend to be related taxonomically (Widdows and Dorow 2002).</S>
    <S sid="696" ssid="357">Constitutive is mostly a whole&#8211;part relation, and the harvested links do a good job at illustrating such a relation.</S>
    <S sid="697" ssid="358">For the Telic, q by n, q through n, and q via n capture cases in which the quale stands in an action&#8211;instrument relation to the target noun (murder by knife).</S>
    <S sid="698" ssid="359">These links thus encode the subtype of Telic role that Pustejovsky (1995) calls &#8220;indirect.&#8221; The two verb&#8211;noun links (q obj n and n sbj intr q) instead capture &#8220;direct&#8221; Telic roles, which are typically expressed by the theme of a verb (read a book, the book reads well).</S>
    <S sid="699" ssid="360">The least convincing results are those for the Agentive role, where only q obj n and perhaps q out n are intuitively plausible canonical links.</S>
    <S sid="700" ssid="361">Interestingly, the manual selections we carried out in Section 6.2.3 also gave very poor results for the Agentive role, as shown by the fact that Table 10 reports just one link for such a role.</S>
    <S sid="701" ssid="362">This suggests that the problems with this qualia role might be due to the number and type of lexicalized links used to build the DM tensors, rather than to the selection algorithm presented here.</S>
    <S sid="702" ssid="363">Coming now to the quantitative evaluation of the harvested patterns, the results in Table 15 (to be compared to Table 11 in Section 6.2.3) are based on W1W2&#215;L subspaces where the non-zero dimensions correspond to the links that we picked automatically with the method we just described (different links for each concept, because of the leave-one-concept-out procedure).</S>
    <S sid="703" ssid="364">TypeDM is the best model in this setting as well.</S>
    <S sid="704" ssid="365">Its performance is even better than the one (reported in Table 11) obtained with the manually picked patterns (although the difference is not statistically significant; paired t-test, t = 0.75, df = 29, p = 0.46), and the automated approach has more room for improvement via parameter optimization.</S>
    <S sid="705" ssid="366">We did not get as deeply into L&#215;W1W2 space as we did with the other views, but our preliminary results on qualia harvesting suggest at least that looking at links as Links selected in all folds of the leave-one-out procedure to extract links typical of each qualia role.</S>
  </SECTION>
  <SECTION title="FORMAL CONSTITUTIVE" number="8">
    <S sid="706" ssid="1">n is q, q is n, q become n, n coord q, n have q, n use q, n with q, n without q q coord n, q have n, n in q, n provide q, q such as n</S>
  </SECTION>
  <SECTION title="AGENTIVE TELIC" number="9">
    <S sid="707" ssid="1">q after n, q alongside n, q as n, q before n, q behind n, q by n, q like n, q obj n, q besides n, q during n, q in n, q obj n, n sbj intr q, q through n, q via n q out n, q over n, q since n, q unlike n LxW1W2 vectors might be useful for feature selection in W1W2xL or for tasks in which we are given a set of pairs, and we have to find links that can function as verbal labels for the relation between the word pairs (Turney 2006a).</S>
    <S sid="708" ssid="2">Dimensionality reduction techniques such as the (truncated) SVD approximate a sparse co-occurrence matrix with a denser lower-rank matrix of the same size, and they have been shown to be effective in many semantic tasks, probably because they provide a beneficial form of smoothing of the dimensions.</S>
    <S sid="709" ssid="3">See Turney and Pantel (2010) for references and discussion.</S>
    <S sid="710" ssid="4">We can apply SVD (or similar methods) to any of the tensorderived matrices we used for the tasks herein.</S>
    <S sid="711" ssid="5">An interesting alternative is to smooth the source tensor directly by a tensor decomposition technique.</S>
    <S sid="712" ssid="6">In this section, we present (very preliminary) evidence that tensor decomposition can improve performance, and it is at least as good in this respect as matrix-based SVD.</S>
    <S sid="713" ssid="7">This is the only experiment in which we operate on the tensor directly, rather than on the matrices derived from it, paving the way to a more active role for the underlying tensor in the DM approach to semantics.</S>
    <S sid="714" ssid="8">The (truncated) Tucker decomposition of a tensor can be seen as a higher-order generalization of SVD.</S>
    <S sid="715" ssid="9">Given a tensor X of dimensionality I1 x I2 x I3, its n-rank Rn is the rank of the vector space spanned by its mode-n fibers (obviously, for each mode n of the tensor, Rn &lt; In).</S>
    <S sid="716" ssid="10">Tucker decomposition approximates the tensor X having n-ranks R1, ... , Rn with X&#732;, a tensor with n-ranks Qn &lt; Rn for all modes n. Unlike the case of SVD, there is no analytical procedure to find the best lower-rank approximation to a tensor, and Tucker decomposition algorithms search for the reduced rank tensor with the best fit (as measured by least square error) iteratively.</S>
    <S sid="717" ssid="11">Specifically, we use the memory-efficient MET(1) algorithm of Kolda and Sun (2008) as implemented in the Matlab Tensor Toolbox.10 Kolda and Bader (2009) provide details on Tucker decomposition, its general properties, as well as applications and alternatives.</S>
    <S sid="718" ssid="12">SVD is believed to exploit patterns of higher order co-occurrence between the rows and columns of a matrix (Manning and Sch&#168;utze 1999; Turney and Pantel 2010), making row elements that co-occur with two synonymic columns more similar than in the original space.</S>
    <S sid="719" ssid="13">Tucker decomposition applied to the mode-3 tuple tensor could capture patterns of higher order co-occurrence for each of the modes.</S>
    <S sid="720" ssid="14">For example, it might capture at the same time similarities between links such as use and hold and w2 elements such as gun and knife.</S>
    <S sid="721" ssid="15">SVD applied after construction of the W1xLW2 matrix, on the other hand, would miss the composite nature of columns such as (use, gun), (use, knife) and (hold, gun).</S>
    <S sid="722" ssid="16">Another attractive feature of Tucker decomposition is that it could be applied once to smooth the source tensor, whereas with SVD each matricization must be smoothed separately.</S>
    <S sid="723" ssid="17">However, Tucker decomposition and SVD are computationally intensive procedures, and, at least with our current computational resources, we are not able to decompose even the smallest DM tensor (similarly, we cannot apply SVD to a full matricization).</S>
    <S sid="724" ssid="18">Given the continuous growth in computational power and the fact that efficient tensor decomposition is a very active area of research (Turney 2007; Kolda and Sun 2008) full tensor decomposition is nevertheless a realistic near future task.</S>
    <S sid="725" ssid="19">For the current pilot study, we replicated the AP concept clustering experiment described in Section 6.1.3.</S>
    <S sid="726" ssid="20">Because for efficiency reasons we must work with just a portion of the original tensor, we thought that the AP data set, consisting of a relatively large and balanced collection of nominal concepts, would offer a sensible starting point to extract the subset.</S>
    <S sid="727" ssid="21">Specifically, we extract from our best tensor TypeDM the values labeled by tuples (wAP, l, w2) where wAP is in the AP set, l is one of the 100 most common links occurring in tuples with a wAP, and w2 is one of the 1,000 most common words occurring in tuples with a wAP and a l. The resulting (sub-)tensor, APTypeDM, has dimensionality 402 x 100 x 1, 000 with 1,318,214 non-zero entries (density: 3%).</S>
    <S sid="728" ssid="22">The W1xLW2 matricization of APTypeDM results in a 402 x 1, 000, 000 matrix with 66,026 non-zero columns and the same number of non-zero entries and density as the tensor.</S>
    <S sid="729" ssid="23">The possible combinations of target lower n-ranks constitute a large tridimensional parameter space, and we leave its systematic exploration to further work.</S>
    <S sid="730" ssid="24">Instead, we pick 300, 50, and 500 as (intuitively reasonable) initial target n-ranks for the three modes, and we explore their neighborhood in parameter space by changing one target n-rank at a time, by a relatively small value (300 1 50, 50 1 10, and 500 1 50, respectively).</S>
    <S sid="731" ssid="25">For the parameters concerning the reduced tensor fitting process, we accept the default values of the Tensor Toolbox.</S>
    <S sid="732" ssid="26">For comparison purposes, we also apply SVD to the W1xLW2 matrix derived from APTypeDM.</S>
    <S sid="733" ssid="27">We systematically explore the SVD target lower rank parameter from 50 to 350 in increments of 50 units.</S>
    <S sid="734" ssid="28">The results are reported in Table 16.</S>
    <S sid="735" ssid="29">The rank column reports the n-ranks when reduction is performed on the tensor, and matrix ranks in the other cases.</S>
    <S sid="736" ssid="30">Bootstrapped confidence intervals are obtained as described in Section 6.1.3.</S>
    <S sid="737" ssid="31">In general, the results confirm that smoothing by rank reduction is beneficial to semantic performance, although not spectacularly so, with an improvement of about 4% for the best reduced model with respect to the raw APTypeDM tensor (consider however also the relatively wide confidence intervals).</S>
    <S sid="738" ssid="32">As a general trend, tensor-based smoothing (Tucker) does better than matrix-based smoothing (SVD).</S>
    <S sid="739" ssid="33">As we said, for Tucker we only report results from a small region of the tridimensional parameter space, whereas the SVD rank parameter range is explored coarsely but exhaustively.</S>
    <S sid="740" ssid="34">Thus, although other parameter combinations might lead to dramatic changes in Tucker performance, the best SVD performance in the table is probably close to the SVD performance upper bound.</S>
    <S sid="741" ssid="35">The present pilot study suggests an attitude of cautious optimism towards tensor decomposition as a smoothing technique.</S>
    <S sid="742" ssid="36">At least in the AP task, it helps as compared to no smoothing at all.</S>
    <S sid="743" ssid="37">The same conclusion is reached by Turney (2007), who uses essentially the same method (with some differences in implementation) to tackle the TOEFL task, and obtains more than 10% improvement in accuracy with respect to the corresponding raw tensor.</S>
    <S sid="744" ssid="38">At least as a trend, tensor decomposition appears to be better than matrix decomposition, but only marginally so (Turney does not perform this comparison).</S>
    <S sid="745" ssid="39">Still, even if the tensor- and matrix-based decompositions turned out to have comparable effects, tensor-based smoothing is more attractive in the DM framework because we could perform the decomposition once, and use the smoothed tensor as our stable underlying DM (modulo, of course, memory problems with computing such a large tensor decomposition).</S>
    <S sid="746" ssid="40">Beyond smoothing, tensor decomposition might provide some novel avenues for distributional semantics, while keeping to the DM program of a single model for many tasks.</S>
    <S sid="747" ssid="41">Van de Cruys (2009) used tensor decomposition to find commonalities in latent dimensions across the fiber labels (in the DM formalism, this would amount to finding commonalities across w1, l, and w2 elements).</S>
    <S sid="748" ssid="42">Another possible use for smoothing would be to propagate &#8220;link mass&#8221; across parts of speech.</S>
    <S sid="749" ssid="43">Our tensors, being based on POS tagging and dependency parsing, have 0 values for noun-link-noun tuples such as (city, obj, destruction) and (city, subj tr, destruction).</S>
    <S sid="750" ssid="44">In a smoothed tensor, by the influence of tuples such as (city, obj, destroy) and (city, sbj tr, destroy), these tuples will get some non-0 weight that, hopefully, will make the object relation between city and destruction emerge.</S>
    <S sid="751" ssid="45">This is at the moment just a conjecture, but it constitutes an exciting direction for further work focusing on tensor decomposition within the DM framework.</S>
  </SECTION>
  <SECTION title="7." number="10">
    <S sid="752" ssid="1">A general framework for distributional semantics should satisfy the following two requirements: (1) representing corpus-derived data in such a way as to capture aspects of meaning that have so far been modeled with different, prima facie incompatible data structures; (2) using this common representation to address a large battery of semantic experiments, achieving a performance at least comparable to that of state-of-art, taskspecific DSMs.</S>
    <S sid="753" ssid="2">We can now safely claim that DM satisfies both these desiderata, and thereby represents a genuine step forward in the quest for a general purpose approach to distributional semantics.</S>
    <S sid="754" ssid="3">DM addresses point (1) by modeling distributional data as a structure of weighted tuples that is formalized as a labeled third-order tensor.</S>
    <S sid="755" ssid="4">This is a generalization with respect to the common approach of many corpus-based semantic models (the structured DSMs) that rely on distributional information encoded into word&#8211;link&#8211;word tuples, associated with weights that are functions of their frequency of co-occurrence in the corpus.</S>
    <S sid="756" ssid="5">Existing structured DSMs still couch this information directly in binary structures, namely, co-occurrence matrices, thereby giving rise to different semantic spaces and losing sight of the fact that such spaces share the same kind of distributional information.</S>
    <S sid="757" ssid="6">The third-order tensor formalization of distributional data allows DM to fully exploit the potential of corpus-derived tuples.</S>
    <S sid="758" ssid="7">The four semantic spaces we analyzed and tested in Section 6 are generated from the same underlying third-order tensor, by the standard operation of tensor matricization.</S>
    <S sid="759" ssid="8">This way, we derive a set of semantic spaces that can be used for measuring attributional similarity (finding synonyms, categorizing concepts into superordinates, etc.) and relational similarity (finding analogies, grouping concept pairs into relation classes, etc.).</S>
    <S sid="760" ssid="9">Moreover, the distributional information encoded in the tensor and unfolded via matricization leads to further arrangements of the data useful in addressing semantic problems that do not fall straightforwardly into the attributional or the relational paradigm (grouping verbs by alternations, harvesting patterns that represent a relation).</S>
    <S sid="761" ssid="10">In some cases, it is obvious how to reformulate a semantic problem in the new framework.</S>
    <S sid="762" ssid="11">Other tasks can be reframed in terms of our four semantic spaces using geometric operations such as centroid computations and projection onto a subspace.</S>
    <S sid="763" ssid="12">This was the case for selectional preferences, pattern- and example-based relation extraction (illustrated by qualia harvesting), and the task of generating typical properties of concepts.</S>
    <S sid="764" ssid="13">We consider a further strength of the DM approach that it naturally encourages us to think, as we did in these cases, of ways to tackle apparently unrelated tasks with the existing resources, rather than devising unrelated approaches to deal with them.</S>
    <S sid="765" ssid="14">Regarding point (2), that is, addressing a large battery of semantic experiments with good performance, in nearly all test sets our best implementation of DM (TypeDM) is at least as good as other algorithms reported in recently published papers (typically developed or tuned for the task at hand), often towards (or at) the top of the state-ofthe-art ranking.</S>
    <S sid="766" ssid="15">Where other models outperform TypeDM by a large margin, there are typically obvious reasons for this: The rivals have been trained on much larger corpora, or they rely on special knowledge resources, or on sophisticated machine learning algorithms.</S>
    <S sid="767" ssid="16">Importantly, TypeDM is consistently at least as good (or better than) those models we reimplemented to be fully comparable to our DMs (i.e., Win, DV, LRA).</S>
    <S sid="768" ssid="17">Moreover, the best DM implementation does not depend on the semantic space: TypeDM outperforms (at least in terms of average performance across tasks) the other two models in all four spaces.</S>
    <S sid="769" ssid="18">This is not surprising (better distributional tuples should still be better when seen from different views), but it is good to have an empirical confirmation of the a priori intuition.</S>
    <S sid="770" ssid="19">The current results suggest that one could, for example, compare alternative DMs on a few attributional tasks, and expect the best DM in these tasks to also be the best in relational tasks and other semantic challenges.</S>
    <S sid="771" ssid="20">The final experiment of Section 6 briefly explored an interesting aspect of the tensor-based formalism, namely, the possibility of improving performance on some tasks by working directly on the tensor (in this case, applying tensor rank reduction for smoothing purposes) rather than on the matrices derived from it.</S>
    <S sid="772" ssid="21">Besides this pilot study, we did not carry out any task-specific optimization of TypeDM, which achieves its very good performance using exactly the same underlying parameter configuration (e.g., dependency paths, weighting function) across the different spaces and tasks.</S>
    <S sid="773" ssid="22">Parameter tuning is an important aspect in DSM development, with an often dramatic impact of parameter variation (Bullinaria and Levy 2007; Erk and Pad&#180;o 2009).</S>
    <S sid="774" ssid="23">We leave the exploration of parameter space in DM for future research.</S>
    <S sid="775" ssid="24">Its importance notwithstanding, however, we regard this as a rather secondary aspect, if compared with the good performance of a DM model (even in its current implementation) in the large and multifarious set of tasks we presented.</S>
    <S sid="776" ssid="25">Of course, many issues are still open.</S>
    <S sid="777" ssid="26">It is one thing to claim that the models that outperform TypeDM do so because they rely on larger corpora; it is another to show that TypeDM trained on more data does reach the top of the current heap.</S>
    <S sid="778" ssid="27">The differences between TypeDM and the other, generally worse-performing DM models remind us that the idea of a shared distributional memory per se is not enough to obtain good results, and the extraction of an ideal DM from the corpus certainly demands further attention.</S>
    <S sid="779" ssid="28">We need to reach a better understanding of which pieces of distributional information to extract, and whether different semantic tasks require focusing on specific subsets of distributional data.</S>
    <S sid="780" ssid="29">Another issue we completely ignored but which will be of fundamental importance in applications is how a DM-based system can deal with outof-vocabulary items.</S>
    <S sid="781" ssid="30">Ideally, we would like a seamless way to integrate new terms in the model incrementally, based on just a few extra data points, but we leave it to further research to study how this could be accomplished, together with the undoubtedly many further practical and theoretical problems that will emerge.</S>
    <S sid="782" ssid="31">We will conclude, instead, by discussing some general advantages that follow from the DM approach of separating corpus-based model building, the multi-purpose long term distributional memory, and different views of the memory data to accomplish different semantic tasks, without resorting to the source corpus again.</S>
    <S sid="783" ssid="32">First of all, we would like to make a more general point regarding parameter tuning and task-specific optimization, by going back to the analogy with WordNet as a semantic multi-purpose resource.</S>
    <S sid="784" ssid="33">If you want to improve performance of a WordNetbased system, you will probably not wait for its next release, but rather improve the algorithms that work on the existing WordNet graph.</S>
    <S sid="785" ssid="34">Similarly, in the DM approach we propose that corpus-based resources for distributional semantics should be relatively stable, multi-purpose, large-scale databases (in the form of weighted tuple structures), only occasionally updated (because a better or larger corpus becomes available, a better parser, etc.).</S>
    <S sid="786" ssid="35">Still, given the same underlying DM and a certain task, much work can be done to exploit the DM optimally in the task, with no need to go back to corpus-based resource construction.</S>
    <S sid="787" ssid="36">For example, performance on attributional tasks could be raised by dimension reweighting techniques such as recently proposed by Zhitomirsky-Geffet and Dagan (2009).</S>
    <S sid="788" ssid="37">For the problem of data sparseness in the W1W2&#215;L space, we could treat the tensor as a graph and explore random walks and other graphical approaches that have been shown to &#8220;scale down&#8221; gracefully to capture relations in sparser data sets (Minkov and Cohen 2007, 2008).</S>
    <S sid="789" ssid="38">As in our simple example of smoothing relational pairs with attributional neighbors, more complex tasks may be tackled by combining different views of DM, and/or resorting to different (sub)spaces within the same view, as in our approach to selectional preferences.</S>
    <S sid="790" ssid="39">One might even foresee an algorithmic way to mix and match the spaces as most appropriate to a certain task.</S>
    <S sid="791" ssid="40">We propose a similar split for the role of supervision in DSMs.</S>
    <S sid="792" ssid="41">Construction of the DM tensor from the corpus is most naturally framed as an unsupervised task, because the model will serve many different purposes.</S>
    <S sid="793" ssid="42">On the other hand, supervision can be of great help in tuning the DM data to specific tasks (as we did, in a rather naive way, with the nearest centroid approach to most non-attributional tasks).</S>
    <S sid="794" ssid="43">A crucial challenge for DSMs is whether and how corpus-derived vectors can also be used in the construction of meaning for constituents larger than words.</S>
    <S sid="795" ssid="44">These are the traditional domains of formal semantics, which is most interested in how the logical representation of a sentence or a discourse is built compositionally by combining the meanings of its constituents.</S>
    <S sid="796" ssid="45">DSMs have so far focused on representing lexical meaning, and compositional and logical issues have either remained out of the picture, or have received still unsatisfactory accounts.</S>
    <S sid="797" ssid="46">A general consensus exists on the need to overcome this limitation, and to build new bridges between corpus-based semantics and symbolic models of meanings (Clark and Pulman 2007; Widdows 2008).</S>
    <S sid="798" ssid="47">Most problems encountered by DSMs in tackling this challenge are specific instances of more general issues concerning the possibility of representing symbolic operations with distributed, vector-based data structures (Markman 1999).</S>
    <S sid="799" ssid="48">Many avenues are currently being explored in corpus-based semantics, and interesting synergies are emerging with research areas such as neural systems (Smolensky 1990; Smolensky and Legendre 2006), quantum information (Widdows and Peters 2003; Aerts and Czachor 2004; Widdows 2004; Van Rijsbergen 2004; Bruza and Cole 2005; Hou and Song 2009), holographic models of memory (Jones and Mewhort 2007), and so on.</S>
    <S sid="800" ssid="49">A core problem in dealing with compositionality with DSMs is to account for the role of syntactic information in determining the way semantic representations are built from lexical items.</S>
    <S sid="801" ssid="50">For instance, the semantic representation assigned to The dog bites the man must be different from the one assigned to The man bites the dog, even if they contain exactly the same lexical items.</S>
    <S sid="802" ssid="51">Although it is still unclear which is the best way to compose the representation of content words in vector spaces, it is nowadays widely assumed that structured representations like those adopted by DM are in the right direction towards a solution to this issue, exactly because they allow distributional representations to become sensitive to syntactic structures (Erk and Pad&#180;o 2008).</S>
    <S sid="803" ssid="52">Compositionality and similar issues in DSMs lie beyond the scope of this paper.</S>
    <S sid="804" ssid="53">However, there is nothing in DM that prevents it from interacting with any of the research directions we have mentioned here.</S>
    <S sid="805" ssid="54">Indeed, we believe that the generalized nature of DM represents a precondition for distributional semantics to be able to satisfactorily address these more advanced challenges.</S>
    <S sid="806" ssid="55">A multi-purpose, distributional semantic resource like DM can allow researchers to focus on the next steps of semantic modeling.</S>
    <S sid="807" ssid="56">These include compositionality, but also modulating word meaning in context (Erk and Pad&#180;o 2008; Mitchell and Lapata 2008) and finding ways to embed the distributional memory in complex NLP systems (e.g., for question answering or textual entailment) or even embodied agents and robots.</S>
    <S sid="808" ssid="57">DM-style triples predicating a relation between two entities are common currency in many semantic representation models (e.g., semantic networks) and knowledgeexchange formalisms such as RDF.</S>
    <S sid="809" ssid="58">This might also pave the way to the integration of corpus-based information with other knowledge sources.</S>
    <S sid="810" ssid="59">It is hard to see how such integration could be pursued within generalized systems, such as PairClass (Turney 2008), that require keeping a full corpus around and corpus-processing know-how on behalf of interested researchers from outside the NLP community (see discussion in Section 4 above).</S>
    <S sid="811" ssid="60">Similarly, the DM triples might help in fostering the dialogue between computational linguists and the computational neuro-cognitive community, where it is common to adopt triple-based representations of knowledge, and to use the same set of tuples to simulate various aspects of cognition.</S>
    <S sid="812" ssid="61">For a recent extended example of this approach, see Rogers and McClelland (2004).</S>
    <S sid="813" ssid="62">It would be relatively easy to use a DM model in lieu of their neural network, and use it to simulate the conceptual processes they reproduce.</S>
    <S sid="814" ssid="63">DM, unlike classic DSM models that go directly from the corpus data to solving specific semantic tasks, introduces a clear distinction between an acquisition phase (corpus-based tuple extraction and weighting), the declarative structure at the core of semantic modeling (the distributional memory), and the procedural problem-solving components (possibly supervised procedures to perform different semantic tasks).</S>
    <S sid="815" ssid="64">This separation is in line with what is commonly assumed in cognitive science and formal linguistics, and we hope it will contribute to make corpus-based modeling a core part of the ongoing study of semantic knowledge in humans and machines.</S>
  </SECTION>
</PAPER>
