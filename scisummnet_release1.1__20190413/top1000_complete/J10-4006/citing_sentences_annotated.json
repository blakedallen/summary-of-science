[
  {
    "citance_No": 1, 
    "citing_paper_id": "P14-1045", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Philippe, Muller | C\u00c3\u00a9cile, Fabre | Cl\u00c3\u00a9mentine, Adam", 
    "raw_text": "We use a distributional resource for French, built on a 200M word corpus extracted from the FrenchWikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroniand Lenci, 2010) ,i.e. using syntactic contexts", 
    "clean_text": "We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W11-2505", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Pierpaolo, Basile | Annalina, Caputo | Giovanni, Semeraro", 
    "raw_text": "Several approaches have investigated the above mentioned problem: (Baroni and Lenci, 2010) use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure", 
    "clean_text": "Several approaches have investigated the above mentioned problem: (Baroni and Lenci, 2010) use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D12-1050", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "William, Blacoe | Mirella, Lapata", 
    "raw_text": "DistributionalMemory Tensor Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived", 
    "clean_text": "Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D12-1050", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "William, Blacoe | Mirella, Lapata", 
    "raw_text": "word w link l co-word v value c 1950s-n of essence-n 2.4880 1950s-n during bring-v 16.4636 Anyone-nnmod reaction-n 1.2161 American-ncoord-1 athlete-n 5.6485 American-jnmod wasp-n 3.4945 American-n such as-1 country-n 14.4269 American-nsbj tr build-v 23.1014 Table 1: Example entries in Baroni and Lenci (2010)? s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged intoa third-order tensor", 
    "clean_text": "Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D12-1050", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "William, Blacoe | Mirella, Lapata", 
    "raw_text": "In this way, the same distributional information can be shared across tasks such as word similarity or analogical learning. More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. This representation operates over adependency-parsed corpus and the scores c are obtained via counting the occurrences of tuples, and weighting the raw counts by mutual information", 
    "clean_text": "More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D12-1050", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "William, Blacoe | Mirella, Lapata", 
    "raw_text": "549 frequency link l co-word v 17059 obj include-v 16713 obj use-v 16573 obj call-v 16475 obj see-v 15962 obj make-v 15707nmod-1 other-j 15554nmod-1 new-j 15224 obj find-v 15221nmod-1 more-j 14715nmod-1 first-j 14348 obj give-v Table 2: The 11 most frequent contexts in Baroni and Lenci (2010)? s tensor (v and j represent verbs and adjectives, respectively)", 
    "clean_text": "The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D12-1050", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "William, Blacoe | Mirella, Lapata", 
    "raw_text": "Extracting a 3-dimensional tensor from the BNC alone would create very sparse representations. We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC)", 
    "clean_text": "We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D12-1050", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "William, Blacoe | Mirella, Lapata", 
    "raw_text": "In the case of vectors obtained from Baroni and Lenci (2010)? s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1)", 
    "clean_text": "In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "D12-1050", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "William, Blacoe | Mirella, Lapata", 
    "raw_text": "F1 Baseline 66.5 79.9 Mihalcea et al2006) 70.3 81.3 Rus et al2008) 70.6 80.5 Qiu et al2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) () 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D12-1050", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "William, Blacoe | Mirella, Lapata", 
    "raw_text": "Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010)", 
    "clean_text": "Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P12-1015", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Nam Khanh, Tran | Elia, Bruni | Gemma, Boleda | Marco, Baroni", 
    "raw_text": "We add to the models we constructed the freely available Distributional Memory (DM )model,3 that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010)", 
    "clean_text": "We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W11-2508", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Kristina, Gulordava | Marco, Baroni", 
    "raw_text": "2LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010", 
    "clean_text": "LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W12-1707", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Alessandra, Zarcone | Jason, Utt | Sebastian, Pad&oacute;", 
    "raw_text": "3.1.1 Distributional MemoryA recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010))", 
    "clean_text": "A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W12-1707", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Alessandra, Zarcone | Jason, Utt | Sebastian, Pad&oacute;", 
    "raw_text": "Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review)", 
    "clean_text": "Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W12-1707", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Alessandra, Zarcone | Jason, Utt | Sebastian, Pad&oacute;", 
    "raw_text": "As for?, we used local mutual information (LMI) as proposed by Baroni and Lenci (2010)", 
    "clean_text": "we used local mutual information (LMI) as proposed by Baroni and Lenci (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W11-2503", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Elia, Bruni | Giang Binh, Tran | Marco, Baroni", 
    "raw_text": "The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM", 
    "clean_text": "The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W11-2503", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Elia, Bruni | Giang Binh, Tran | Marco, Baroni", 
    "raw_text": "Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined", 
    "clean_text": "Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P14-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marco, Baroni | Georgiana, Dinu | Germ\u00c3\u00a1n, Kruszewski", 
    "raw_text": "Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available", 
    "clean_text": "Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P14-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marco, Baroni | Georgiana, Dinu | Germ\u00c3\u00a1n, Kruszewski", 
    "raw_text": "Themcrae set (McRae et al, 1998) consists of 100 noun? verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on 241 syntactic information. Analogy While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduce din Mikolov et al (2013a) specifically to test predict models", 
    "clean_text": "The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P14-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marco, Baroni | Georgiana, Dinu | Germ\u00c3\u00a1n, Kruszewski", 
    "raw_text": "Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010) .Interestingly, count vectors achieve performance comparable to that of predict vectors only on the selectional preference tasks", 
    "clean_text": "Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010).", 
    "keep_for_gold": 0
  }
]
