<PAPER>
  <S sid="0">A Comparison Of Algorithms For Maximum Entropy Parameter Estimation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing.</S>
    <S sid="2" ssid="2">However, the flexibility of ME models is not without cost.</S>
    <S sid="3" ssid="3">While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters.</S>
    <S sid="4" ssid="4">In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods.</S>
    <S sid="5" ssid="5">Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Maximum entropy (ME) models, variously known as log-linear, Gibbs, exponential, and multinomial logit models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to fields as diverse as computer vision and econometrics.</S>
    <S sid="7" ssid="2">In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications (Abney, 1997; Berger et al., 1996; Ratnaparkhi, 1998; Johnson et al., 1999).</S>
    <S sid="8" ssid="3">A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features.</S>
    <S sid="9" ssid="4">However, the richness of the representations is not without cost.</S>
    <S sid="10" ssid="5">Even modest ME models can require considerable computational resources and very large quantities of annotated training data in order to accurately estimate the model&#8217;s parameters.</S>
    <S sid="11" ssid="6">While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are usually quite large, and frequently contain hundreds of thousands of free parameters.</S>
    <S sid="12" ssid="7">Estimation of such large models is not only expensive, but also, due to sparsely distributed features, sensitive to round-off errors.</S>
    <S sid="13" ssid="8">Thus, highly efficient, accurate, scalable methods are required for estimating the parameters of practical models.</S>
    <S sid="14" ssid="9">In this paper, we consider a number of algorithms for estimating the parameters of ME models, including Generalized Iterative Scaling and Improved Iterative Scaling, as well as general purpose optimization techniques such as gradient ascent, conjugate gradient, and variable metric methods.</S>
    <S sid="15" ssid="10">Surprisingly, the widely used iterative scaling algorithms perform quite poorly, and for all of the test problems, a limited memory variable metric algorithm outperformed the other choices.</S>
  </SECTION>
  <SECTION title="2 Maximum likelihood estimation" number="2">
    <S sid="16" ssid="1">Suppose we are given a probability distribution p over a set of events X which are characterized by a d dimensional feature vector function f : X &#8594; Rd.</S>
    <S sid="17" ssid="2">In addition, we have also a set of contexts W and a function Y which partitions the members of X.</S>
    <S sid="18" ssid="3">In the case of a stochastic context-free grammar, for example, X might be the set of possible trees, the feature vectors might represent the number of times each rule applied in the derivation of each tree, W might be the set of possible strings of words, and Y(w) the set of trees whose yield is w &#8712; W. A conditional maximum entropy model q&#952;(x|w) for p has the parametric form (Berger et al., 1996; Chi, 1998; where &#952; is a d-dimensional parameter vector and &#952;T f (x) is the inner product of the parameter vector and a feature vector.</S>
    <S sid="19" ssid="4">Given the parametric form of an ME model in (1), fitting an ME model to a collection of training data entails finding values for the parameter vector &#952; which minimize the Kullback-Leibler divergence between the model q0 and the empirical distribution p: ratio of Ep[f] to Eq(k)[f], with the restriction that &#8721;j fj(x) = C for each event x in the training data (a condition which can be easily satisfied by the addition of a correction feature).</S>
    <S sid="20" ssid="5">We can adapt GIS to estimate the model parameters &#952; rather than the model probabilities q, yielding the update rule: The step size, and thus the rate of convergence, depends on the constant C: the larger the value of C, the smaller the step size.</S>
    <S sid="21" ssid="6">In case not all rows of the training data sum to a constant, the addition of a correction feature effectively slows convergence to match the most difficult case.</S>
    <S sid="22" ssid="7">To avoid this slowed convergence and the need for a correction feature, Della Pietra et al. (1997) propose an Improved Iterative Scaling (IIS) algorithm, whose update rule is the solution to the equation: The gradient of the log likelihood function, or the Ep[f] = &#8721;w,x p(w)q(k)(x|w)f(x)exp(M(x)&#948;(k)) vector of its first derivatives with respect to the parameter &#952; is: Since the likelihood function (2) is concave over the parameter space, it has a global maximum where the gradient is zero.</S>
    <S sid="23" ssid="8">Unfortunately, simply setting G(&#952;) = 0 and solving for &#952; does not yield a closed form solution, so we proceed iteratively.</S>
    <S sid="24" ssid="9">At each step, we adjust an estimate of the parameters &#952;(k) to a new estimate &#952;(k+1) based on the divergence between the estimated probability distribution q(k) and the empirical distribution p. We continue until successive improvements fail to yield a sufficiently large decrease in the divergence.</S>
    <S sid="25" ssid="10">While all parameter estimation algorithms we will consider take the same general form, the method for computing the updates &#948;(k) at each search step differs substantially.</S>
    <S sid="26" ssid="11">As we shall see, this difference can have a dramatic impact on the number of updates required to reach convergence.</S>
    <S sid="27" ssid="12">One popular method for iteratively refining the model parameters is Generalized Iterative Scaling (GIS), due to Darroch and Ratcliff (1972).</S>
    <S sid="28" ssid="13">An extension of Iterative Proportional Fitting (Deming and Stephan, 1940), GIS scales the probability distribution q(k) by a factor proportional to the where M(x) is the sum of the feature values for an event x in the training data.</S>
    <S sid="29" ssid="14">This is a polynomial in exp(&#948;(k)), and the solution can be found straightforwardly using, for example, the Newton-Raphson method.</S>
    <S sid="30" ssid="15">Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables.</S>
    <S sid="31" ssid="16">Their primary strength is that on each iteration they only require computation of the expected values Eq(k).</S>
    <S sid="32" ssid="17">They do not depend on evaluation of the gradient of the log-likelihood function, which, depending on the distribution, could be prohibitively expensive.</S>
    <S sid="33" ssid="18">In the case of ME models, however, the vector of expected values required by iterative scaling essentially is the gradient G. Thus, it makes sense to consider methods which use the gradient directly.</S>
    <S sid="34" ssid="19">The most obvious way of making explicit use of the gradient is by Cauchy&#8217;s method, or the method of steepest ascent.</S>
    <S sid="35" ssid="20">The gradient of a function is a vector which points in the direction in which the function&#8217;s value increases most rapidly.</S>
    <S sid="36" ssid="21">Since our goal is to maximize the log-likelihood function, a natural strategy is to shift our current estimate of the parameters in the direction of the gradient via the update rule: where the step size &#945;(k) is chosen to maximize L(&#952;(k) +&#948;(k)).</S>
    <S sid="37" ssid="22">Finding the optimal step size is itself an optimization problem, though only in one dimension and, in practice, only an approximate solution is required to guarantee global convergence.</S>
    <S sid="38" ssid="23">Since the log-likelihood function is concave, the method of steepest ascent is guaranteed to find the global maximum.</S>
    <S sid="39" ssid="24">However, while the steps taken on each iteration are in a very narrow sense locally optimal, the global convergence rate of steepest ascent is very poor.</S>
    <S sid="40" ssid="25">Each new search direction is orthogonal (or, if an approximate line search is used, nearly so) to the previous direction.</S>
    <S sid="41" ssid="26">This leads to a characteristic &#8220;zig-zag&#8221; ascent, with convergence slowing as the maximum is approached.</S>
    <S sid="42" ssid="27">One way of looking at the problem with steepest ascent is that it considers the same search directions many times.</S>
    <S sid="43" ssid="28">We would prefer an algorithm which considered each possible search direction only once, in each iteration taking a step of exactly the right length in a direction orthogonal to all previous search directions.</S>
    <S sid="44" ssid="29">This intuition underlies conjugate gradient methods, which choose a search direction which is a linear combination of the steepest ascent direction and the previous search direction.</S>
    <S sid="45" ssid="30">The step size is selected by an approximate line search, as in the steepest ascent method.</S>
    <S sid="46" ssid="31">Several non-linear conjugate gradient methods, such as the Fletcher-Reeves (cg-fr) and the Polak-Ribi`erePositive (cf-prp) algorithms, have been proposed.</S>
    <S sid="47" ssid="32">While theoretically equivalent, they use slighly different update rules and thus show different numeric properties.</S>
    <S sid="48" ssid="33">Another way of looking at the problem with steepest ascent is that while it takes into account the gradient of the log-likelihood function, it fails to take into account its curvature, or the gradient of the gradient.</S>
    <S sid="49" ssid="34">The usefulness of the curvature is made clear if we consider a second-order Taylor series approximation of L(&#952; +&#948;): where H is Hessian matrix of the log-likelihood function, the d &#215; d matrix of its second partial derivatives with respect to &#952;.</S>
    <S sid="50" ssid="35">If we set the derivative of (4) to zero and solve for &#948;, we get the update rule for Newton&#8217;s method: Newton&#8217;s method converges very quickly (for quadratic objective functions, in one step), but it requires the computation of the inverse of the Hessian matrix on each iteration.</S>
    <S sid="51" ssid="36">While the log-likelihood function for ME models in (2) is twice differentiable, for large scale problems the evaluation of the Hessian matrix is computationally impractical, and Newton&#8217;s method is not competitive with iterative scaling or first order methods.</S>
    <S sid="52" ssid="37">Variable metric or quasi-Newton methods avoid explicit evaluation of the Hessian by building up an approximation of it using successive evaluations of the gradient.</S>
    <S sid="53" ssid="38">That is, we replace H&#8722;1(&#952;(k)) in (5) with a local approximation of the inverse Hessian B(k): with B(k) a symmatric, positive definite matrix which satisfies the equation: where y(k) = G(&#952;(k)) &#8722; G(&#952;(k&#8722;1)).</S>
    <S sid="54" ssid="39">Variable metric methods also show excellent convergence properties and can be much more efficient than using true Newton updates, but for large scale problems with hundreds of thousands of parameters, even storing the approximate Hessian is prohibitively expensive.</S>
    <S sid="55" ssid="40">For such cases, we can apply limited memory variable metric methods, which implicitly approximate the Hessian matrix in the vicinity of the current estimate of &#952;(k) using the previous m values of y(k) and &#948;(k).</S>
    <S sid="56" ssid="41">Since in practical applications values of m between 3 and 10 suffice, this can offer a substantial savings in storage requirements over variable metric methods, while still giving favorable convergence properties.1</S>
  </SECTION>
  <SECTION title="3 Comparing estimation techniques" number="3">
    <S sid="57" ssid="1">The performance of optimization algorithms is highly dependent on the specific properties of the problem to be solved.</S>
    <S sid="58" ssid="2">Worst-case analysis typically 'Space constraints preclude a more detailed discussion of these methods here.</S>
    <S sid="59" ssid="3">For algorithmic details and theoretical analysis of first and second order methods, see, e.g., Nocedal (1997) or Nocedal and Wright (1999). does not reflect the actual behavior on actual problems.</S>
    <S sid="60" ssid="4">Therefore, in order to evaluate the performance of the optimization techniques sketched in previous section when applied to the problem of parameter estimation, we need to compare the performance of actual implementations on realistic data sets (Dolan and Mor&#180;e, 2002).</S>
    <S sid="61" ssid="5">Minka (2001) offers a comparison of iterative scaling with other algorithms for parameter estimation in logistic regression, a problem similar to the one considered here, but it is difficult to transfer Minka&#8217;s results to ME models.</S>
    <S sid="62" ssid="6">For one, he evaluates the algorithms with randomly generated training data.</S>
    <S sid="63" ssid="7">However, the performance and accuracy of optimization algorithms can be sensitive to the specific numerical properties of the function being optimized; results based on random data may or may not carry over to more realistic problems.</S>
    <S sid="64" ssid="8">And, the test problems Minka considers are relatively small (100&#8211;500 dimensions).</S>
    <S sid="65" ssid="9">As we have seen, though, algorithms which perform well for small and medium scale problems may not always be applicable to problems with many thousands of dimensions.</S>
    <S sid="66" ssid="10">As a basis for the implementation, we have used PETSc (the &#8220;Portable, Extensible Toolkit for Scientific Computation&#8221;), a software library designed to ease development of programs which solve large systems of partial differential equations (Balay et al., 2001; Balay et al., 1997; Balay et al., 2002).</S>
    <S sid="67" ssid="11">PETSc offers data structures and routines for parallel and sequential storage, manipulation, and visualization of very large sparse matrices.</S>
    <S sid="68" ssid="12">For any of the estimation techniques, the most expensive operation is computing the probability distribution q and the expectations Eq[f] for each iteration.</S>
    <S sid="69" ssid="13">In order to make use of the facilities provided by PETSc, we can store the training data as a (sparse) matrix F, with rows corresponding to events and columns to features.</S>
    <S sid="70" ssid="14">Then given a parameter vector &#952;, the unnormalized probabilities &#729;q0 are the matrix-vector product: and the feature expectations are the transposed matrix-vector product: By expressing these computations as matrix-vector operations, we can take advantage of the high performance sparse matrix primitives of PETSc.</S>
    <S sid="71" ssid="15">For the comparison, we implemented both Generalized and Improved Iterative Scaling in C++ using the primitives provided by PETSc.</S>
    <S sid="72" ssid="16">For the other optimization techniques, we used TAO (the &#8220;Toolkit for Advanced Optimization&#8221;), a library layered on top of the foundation of PETSc for solving nonlinear optimization problems (Benson et al., 2002).</S>
    <S sid="73" ssid="17">TAO offers the building blocks for writing optimization programs (such as line searches and convergence tests) as well as high-quality implementations of standard optimization algorithms (including conjugate gradient and variable metric methods).</S>
    <S sid="74" ssid="18">Before turning to the results of the comparison, two additional points need to be made.</S>
    <S sid="75" ssid="19">First, in order to assure a consistent comparison, we need to use the same stopping rule for each algorithm.</S>
    <S sid="76" ssid="20">For these experiments, we judged that convergence was reached when the relative change in the loglikelihood between iterations fell below a predetermined threshold.</S>
    <S sid="77" ssid="21">That is, each run was stopped when: where the relative tolerance &#949; = 10&#8722;7.</S>
    <S sid="78" ssid="22">For any particular application, this may or may not be an appropriate stopping rule, but is only used here for purposes of comparison.</S>
    <S sid="79" ssid="23">Finally, it should be noted that in the current implementation, we have not applied any of the possible optimizations that appear in the literature (Lafferty and Suhm, 1996; Wu and Khudanpur, 2000; Lafferty et al., 2001) to speed up normalization of the probability distribution q.</S>
    <S sid="80" ssid="24">These improvements take advantage of a model&#8217;s structure to simplify the evaluation of the denominator in (1).</S>
    <S sid="81" ssid="25">The particular data sets examined here are unstructured, and such optimizations are unlikely to give any improvement.</S>
    <S sid="82" ssid="26">However, when these optimizations are appropriate, they will give a proportional speed-up to all of the algorithms.</S>
    <S sid="83" ssid="27">Thus, the use of such optimizations is independent of the choice of parameter estimation method.</S>
    <S sid="84" ssid="28">To compare the algorithms described in &#167;2, we applied the implementation outlined in the previous section to four training data sets (described in Table 1) drawn from the domain of natural language processing.</S>
    <S sid="85" ssid="29">The &#8216;rules&#8217; and &#8216;lex&#8217; datasets are examples of stochastic attribute value grammars, one with a small set of SCFG-like features, and with a very large set of fine-grained lexical features (Bouma et al., 2001).</S>
    <S sid="86" ssid="30">The &#8216;summary&#8217; dataset is part of a sentence extraction task (Osborne, to appear), and the &#8216;shallow&#8217; dataset is drawn from a text chunking application (Osborne, 2002).</S>
    <S sid="87" ssid="31">These datasets vary widely in their size and composition, and are representative of the kinds of datasets typically encountered in applying ME models to NLP classification tasks.</S>
    <S sid="88" ssid="32">The results of applying each of the parameter estimation algorithms to each of the datasets is summarized in Table 2.</S>
    <S sid="89" ssid="33">For each run, we report the KL divergence between the fitted model and the training data at convergence, the prediction accuracy of fitted model on a held-out test set (the fraction of contexts for which the event with the highest probability under the model also had the highest probability under the reference distribution), the number of iterations required, the number of log-likelihood and gradient evaluations required (algorithms which use a line search may require several function evaluations per iteration), and the total elapsed time (in seconds).2 There are a few things to observe about these results.</S>
    <S sid="90" ssid="34">First, while IIS converges in fewer steps the GIS, it takes substantially more time.</S>
    <S sid="91" ssid="35">At least for this implementation, the additional bookkeeping overhead required by IIS more than cancels any improvements in speed offered by accelerated convergence.</S>
    <S sid="92" ssid="36">This may be a misleading conclusion, however, since a more finely tuned implementation of IIS may well take much less time per iteration than the one used for these experiments.</S>
    <S sid="93" ssid="37">However, even if each iteration of IIS could be made as fast as an iteration of GIS (which seems unlikely), the benefits of IIS over GIS would in these cases be quite modest.</S>
    <S sid="94" ssid="38">Second, note that for three of the four datasets, the KL divergence at convergence is roughly the same for all of the algorithms.</S>
    <S sid="95" ssid="39">For the &#8216;summary&#8217; dataset, however, they differ by up to two orders of magnitude.</S>
    <S sid="96" ssid="40">This is an indication that the convergence test in (6) is sensitive to the rate of convergence and thus to the choice of algorithm.</S>
    <S sid="97" ssid="41">Any degree of precision desired could be reached by any of the algorithms, with the appropriate value of &#949;.</S>
    <S sid="98" ssid="42">However, GIS, say, would require many more iterations than reported in Table 2 to reach the precision achieved by the limited memory variable metric algorithm.</S>
    <S sid="99" ssid="43">Third, the prediction accuracy is, in most cases, more or less the same for all of the algorithms.</S>
    <S sid="100" ssid="44">Some variability is to be expected&#8212;all of the data sets being considered here are badly ill-conditioned, and many different models will yield the same likelihood.</S>
    <S sid="101" ssid="45">In a few cases, however, the prediction accuracy differs more substantially.</S>
    <S sid="102" ssid="46">For the two SAVG data sets (&#8216;rules&#8217; and &#8216;lex&#8217;), GIS has a small advantage over the other methods.</S>
    <S sid="103" ssid="47">More dramatically, both iterative scaling methods perform very poorly on the &#8216;shallow&#8217; dataset.</S>
    <S sid="104" ssid="48">In this case, the training data is very sparse.</S>
    <S sid="105" ssid="49">Many features are nearly &#8216;pseudo-minimal&#8217; in the sense of Johnson et al. (1999), and so receive weights approaching &#8722;&#8734;.</S>
    <S sid="106" ssid="50">Smoothing the reference probabilities would likely improve the results for all of the methods and reduce the observed differences.</S>
    <S sid="107" ssid="51">However, this does suggest that gradient-based methods are robust to certain problems with the training data.</S>
    <S sid="108" ssid="52">Finally, the most significant lesson to be drawn from these results is that, with the exception of steepest ascent, gradient-based methods outperform iterative scaling by a wide margin for almost all the datasets, as measured by both number of function evaluations and by the total elapsed time.</S>
    <S sid="109" ssid="53">And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.</S>
  </SECTION>
  <SECTION title="4 Conclusions" number="4">
    <S sid="110" ssid="1">In this paper, we have described experiments comparing the performance of a number of different algorithms for estimating the parameters of a conditional ME model.</S>
    <S sid="111" ssid="2">The results show that variants of iterative scaling, the algorithms which are most widely used in the literature, perform quite poorly when compared to general function optimization algorithms such as conjugate gradient and variable metric methods.</S>
    <S sid="112" ssid="3">And, more specifically, for the NLP classification tasks considered, the limited memory variable metric algorithm of Benson and Mor&#180;e (2001) outperforms the other choices by a substantial margin.</S>
    <S sid="113" ssid="4">This conclusion has obvious consequences for the field.</S>
    <S sid="114" ssid="5">ME modeling is a commonly used machine learning technique, and the application of improved parameter estimation algorithms will it practical to construct larger, more complex models.</S>
    <S sid="115" ssid="6">And, since the parameters of individual models can be estimated quite quickly, this will further open up the possibility for more sophisticated model and feature selection techniques which compare large numbers of alternative model specifications.</S>
    <S sid="116" ssid="7">This suggests that more comprehensive experiments to compare the convergence rate and accuracy of various algorithms on a wider range of problems is called for.</S>
    <S sid="117" ssid="8">In addition, there is a larger lesson to be drawn from these results.</S>
    <S sid="118" ssid="9">We typically think of computational linguistics as being primarily a symbolic discipline.</S>
    <S sid="119" ssid="10">However, statistical natural language processing involves non-trivial numeric computations.</S>
    <S sid="120" ssid="11">As these results show, natural language processing can take great advantage of the algorithms and software libraries developed by and for more quantitatively oriented engineering and computational sciences.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="5">
    <S sid="121" ssid="1">The research of Dr. Malouf has been made possible by a fellowship of the Royal Netherlands Academy of Arts and Sciences and by the NWO PIONIER project Algorithms for Linguistic Processing.</S>
    <S sid="122" ssid="2">Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.</S>
  </SECTION>
</PAPER>
