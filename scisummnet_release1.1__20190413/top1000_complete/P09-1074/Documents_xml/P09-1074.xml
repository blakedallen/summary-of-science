<PAPER>
  <S sid="0">Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora.</S>
    <S sid="2" ssid="2">First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.</S>
    <S sid="3" ssid="3">We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task.</S>
    <S sid="4" ssid="4">Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">As is common for many natural language processing problems, the state-of-the-art in noun phrase (NP) coreference resolution is typically quantified based on system performance on manually annotated text corpora.</S>
    <S sid="6" ssid="2">In spite of the availability of several benchmark data sets (e.g.</S>
    <S sid="7" ssid="3">MUC-6 (1995), ACE NIST (2004)) and their use in many formal evaluations, as a field we can make surprisingly few conclusive statements about the state-of-theart in NP coreference resolution.</S>
    <S sid="8" ssid="4">In particular, it remains difficult to assess the effectiveness of different coreference resolution approaches, even in relative terms.</S>
    <S sid="9" ssid="5">For example, the 91.5 F-measure reported by McCallum and Wellner (2004) was produced by a system using perfect information for several linguistic subproblems.</S>
    <S sid="10" ssid="6">In contrast, the 71.3 F-measure reported by Yang et al. (2003) represents a fully automatic end-to-end resolver.</S>
    <S sid="11" ssid="7">It is impossible to assess which approach truly performs best because of the dramatically different assumptions of each evaluation.</S>
    <S sid="12" ssid="8">Results vary widely across data sets.</S>
    <S sid="13" ssid="9">Coreference resolution scores range from 85-90% on the ACE 2004 and 2005 data sets to a much lower 6070% on the MUC 6 and 7 data sets (e.g.</S>
    <S sid="14" ssid="10">Soon et al. (2001) and Yang et al.</S>
    <S sid="15" ssid="11">(2003)).</S>
    <S sid="16" ssid="12">What accounts for these differences?</S>
    <S sid="17" ssid="13">Are they due to properties of the documents or domains?</S>
    <S sid="18" ssid="14">Or do differences in the coreference task definitions account for the differences in performance?</S>
    <S sid="19" ssid="15">Given a new text collection and domain, what level of performance should we expect?</S>
    <S sid="20" ssid="16">We have little understanding of which aspects of the coreference resolution problem are handled well or poorly by state-of-the-art systems.</S>
    <S sid="21" ssid="17">Except for some fairly general statements, for example that proper names are easier to resolve than pronouns, which are easier than common nouns, there has been little analysis of which aspects of the problem have achieved success and which remain elusive.</S>
    <S sid="22" ssid="18">The goal of this paper is to take initial steps toward making sense of the disparate performance results reported for NP coreference resolution.</S>
    <S sid="23" ssid="19">For our investigations, we employ a state-of-the-art classification-based NP coreference resolver and focus on the widely used MUC and ACE coreference resolution data sets.</S>
    <S sid="24" ssid="20">We hypothesize that performance variation within and across coreference resolvers is, at least in part, a function of (1) the (sometimes unstated) assumptions in evaluation methodologies, and (2) the relative difficulty of the benchmark text corpora.</S>
    <S sid="25" ssid="21">With these in mind, Section 3 first examines three subproblems that play an important role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.</S>
    <S sid="26" ssid="22">We quantitatively measure the impact of each of these subproblems on coreference resolution performance as a whole.</S>
    <S sid="27" ssid="23">Our results suggest that the availability of accurate detectors for anaphoricity or coreference elements could substantially improve the performance of state-ofthe-art resolvers, while improvements to named entity recognition likely offer little gains.</S>
    <S sid="28" ssid="24">Our results also confirm that the assumptions adopted in some evaluations dramatically simplify the resolution task, rendering it an unrealistic surrogate for the original problem.</S>
    <S sid="29" ssid="25">In Section 4, we quantify the difficulty of a text corpus with respect to coreference resolution by analyzing performance on different resolution classes.</S>
    <S sid="30" ssid="26">Our goals are twofold: to measure the level of performance of state-of-the-art coreference resolvers on different types of anaphora, and to develop a quantitative measure for estimating coreference resolution performance on new data sets.</S>
    <S sid="31" ssid="27">We introduce a coreference performance prediction (CPP) measure and show that it accurately predicts the performance of our coreference resolver.</S>
    <S sid="32" ssid="28">As a side effect of our research, we provide a new set of much-needed benchmark results for coreference resolution under common sets of fully-specified evaluation assumptions.</S>
  </SECTION>
  <SECTION title="2 Coreference Task Definitions" number="2">
    <S sid="33" ssid="1">This paper studies the six most commonly used coreference resolution data sets.</S>
    <S sid="34" ssid="2">Two of those are from the MUC conferences (MUC-6, 1995; MUC7, 1997) and four are from the Automatic Content Evaluation (ACE) Program (NIST, 2004).</S>
    <S sid="35" ssid="3">In this section, we outline the differences between the MUC and ACE coreference resolution tasks, and define terminology for the rest of the paper.</S>
    <S sid="36" ssid="4">Noun phrase coreference resolution is the process of determining whether two noun phrases (NPs) refer to the same real-world entity or concept.</S>
    <S sid="37" ssid="5">It is related to anaphora resolution: a NP is said to be anaphoric if it depends on another NP for interpretation.</S>
    <S sid="38" ssid="6">Consider the following: John Hall is the new CEO.</S>
    <S sid="39" ssid="7">He starts on Monday.</S>
    <S sid="40" ssid="8">Here, he is anaphoric because it depends on its antecedent, John Hall, for interpretation.</S>
    <S sid="41" ssid="9">The two NPs also corefer because each refers to the same person, JOHN HALL.</S>
    <S sid="42" ssid="10">As discussed in depth elsewhere (e.g. van Deemter and Kibble (2000)), the notions of coreference and anaphora are difficult to define precisely and to operationalize consistently.</S>
    <S sid="43" ssid="11">Furthermore, the connections between them are extremely complex and go beyond the scope of this paper.</S>
    <S sid="44" ssid="12">Given these complexities, it is not surprising that the annotation instructions for the MUC and ACE data sets reflect different interpretations and simplifications of the general coreference relation.</S>
    <S sid="45" ssid="13">We outline some of these differences below.</S>
    <S sid="46" ssid="14">Syntactic Types.</S>
    <S sid="47" ssid="15">To avoid ambiguity, we will use the term coreference element (CE) to refer to the set of linguistic expressions that participate in the coreference relation, as defined for each of the MUC and ACE tasks.1 At times, it will be important to distinguish between the CEs that are included in the gold standard &#8212; the annotated CEs &#8212; from those that are generated by the coreference resolution system &#8212; the extracted CEs.</S>
    <S sid="48" ssid="16">At a high level, both the MUC and ACE evaluations define CEs as nouns, pronouns, and noun phrases.</S>
    <S sid="49" ssid="17">However, the MUC definition excludes (1) &#8220;nested&#8221; named entities (NEs) (e.g.</S>
    <S sid="50" ssid="18">&#8220;America&#8221; in &#8220;Bank of America&#8221;), (2) relative pronouns, and (3) gerunds, but allows (4) nested nouns (e.g.</S>
    <S sid="51" ssid="19">&#8220;union&#8221; in &#8220;union members&#8221;).</S>
    <S sid="52" ssid="20">The ACE definition, on the other hand, includes relative pronouns and gerunds, excludes all nested nouns that are not themselves NPs, and allows premodifier NE mentions of geo-political entities and locations, such as &#8220;Russian&#8221; in &#8220;Russian politicians&#8221;.</S>
    <S sid="53" ssid="21">Semantic Types.</S>
    <S sid="54" ssid="22">ACE restricts CEs to entities that belong to one of seven semantic classes: person, organization, geo-political entity, location, facility, vehicle, and weapon.</S>
    <S sid="55" ssid="23">MUC has no semantic restrictions.</S>
    <S sid="56" ssid="24">Singletons.</S>
    <S sid="57" ssid="25">The MUC data sets include annotations only for CEs that are coreferent with at least one other CE.</S>
    <S sid="58" ssid="26">ACE, on the other hand, permits &#8220;singleton&#8221; CEs, which are not coreferent with any other CE in the document.</S>
    <S sid="59" ssid="27">These substantial differences in the task definitions (summarized in Table 1) make it extremely difficult to compare performance across the MUC and ACE data sets.</S>
    <S sid="60" ssid="28">In the next section, we take a closer look at the coreference resolution task, analyzing the impact of various subtasks irrespective of the data set differences.</S>
  </SECTION>
  <SECTION title="3 Coreference Subtask Analysis" number="3">
    <S sid="61" ssid="1">Coreference resolution is a complex task that requires solving numerous non-trivial subtasks such as syntactic analysis, semantic class tagging, pleonastic pronoun identification and antecedent identification to name a few.</S>
    <S sid="62" ssid="2">This section examines the role of three such subtasks &#8212; named entity recognition, anaphoricity determination, and coreference element detection &#8212; in the performance of an end-to-end coreference resolution system.</S>
    <S sid="63" ssid="3">First, however, we describe the coreference resolver that we use for our study.</S>
    <S sid="64" ssid="4">We use the RECONCILE coreference resolution platform (Stoyanov et al., 2009) to configure a coreference resolver that performs comparably to state-of-the-art systems (when evaluated on the MUC and ACE data sets under comparable assumptions).</S>
    <S sid="65" ssid="5">This system is a classification-based coreference resolver, modeled after the systems of Ng and Cardie (2002b) and Bengtson and Roth (2008).</S>
    <S sid="66" ssid="6">First it classifies pairs of CEs as coreferent or not coreferent, pairing each identified CE with all preceding CEs.</S>
    <S sid="67" ssid="7">The CEs are then clustered into coreference chains2 based on the pairwise decisions.</S>
    <S sid="68" ssid="8">RECONCILE has a pipeline architecture with four main steps: preprocessing, feature extraction, classification, and clustering.</S>
    <S sid="69" ssid="9">We will refer to the specific configuration of RECONCILE used for this paper as RECONCILEACL09.</S>
    <S sid="70" ssid="10">Preprocessing.</S>
    <S sid="71" ssid="11">The RECONCILEACL09 preprocessor applies a series of language analysis tools (mostly publicly available software packages) to the source texts.</S>
    <S sid="72" ssid="12">The OpenNLP toolkit (Baldridge, J., 2005) performs tokenization, sentence splitting, and part-of-speech tagging.</S>
    <S sid="73" ssid="13">The Berkeley parser (Petrov and Klein, 2007) generates phrase structure parse trees, and the de Marneffe et al. (2006) system produces dependency relations.</S>
    <S sid="74" ssid="14">We employ the Stanford CRF-based Named Entity Recognizer (Finkel et al., 2004) for named entity tagging.</S>
    <S sid="75" ssid="15">With these preprocessing components, RECONCILEACL09 uses heuristics to correctly extract approximately 90% of the annotated CEs for the MUC and ACE data sets.</S>
    <S sid="76" ssid="16">Feature Set.</S>
    <S sid="77" ssid="17">To achieve roughly state-of-theart performance, RECONCILEACL09 employs a fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)).</S>
    <S sid="78" ssid="18">We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details.</S>
    <S sid="79" ssid="19">Lexical (9): String-based comparisons of the two CEs, such as exact string matching and head noun matching.</S>
    <S sid="80" ssid="20">Proximity (5): Sentence and paragraph-based measures of the distance between two CEs.</S>
    <S sid="81" ssid="21">Grammatical (28): A wide variety of syntactic properties of the CEs, either individually or as a pair.</S>
    <S sid="82" ssid="22">These features are based on part-of-speech tags, parse trees, or dependency relations.</S>
    <S sid="83" ssid="23">For example: one feature indicates whether both CEs are syntactic subjects; another indicates whether the CEs are in an appositive construction.</S>
    <S sid="84" ssid="24">Semantic (19): Capture semantic information about one or both NPs such as tests for gender and animacy, semantic compatibility based on WordNet, and semantic comparisons of NE types.</S>
    <S sid="85" ssid="25">Classification and Clustering.</S>
    <S sid="86" ssid="26">We configure RECONCILEACL09 to use the Averaged Perceptron learning algorithm (Freund and Schapire, 1999) and to employ single-link clustering (i.e. transitive closure) to generate the final partitioning.3 Our experiments rely on the MUC and ACE corpora.</S>
    <S sid="87" ssid="27">For ACE, we use only the newswire portion because it is closest in composition to the MUC corpora.</S>
    <S sid="88" ssid="28">Statistics for each of the data sets are shown in Table 2.</S>
    <S sid="89" ssid="29">When available, we use the standard test/train split.</S>
    <S sid="90" ssid="30">Otherwise, we randomly split the data into a training and test set following a 70/30 ratio.</S>
    <S sid="91" ssid="31">Scoring Algorithms.</S>
    <S sid="92" ssid="32">We evaluate using two common scoring algorithms4 &#8212; MUC and B3.</S>
    <S sid="93" ssid="33">The MUC scoring algorithm (Vilain et al., 1995) computes the F1 score (harmonic mean) of precision and recall based on the identifcation of unique coreference links.</S>
    <S sid="94" ssid="34">We use the official MUC scorer implementation for the two MUC corpora and an equivalent implementation for ACE.</S>
    <S sid="95" ssid="35">The B3 algorithm (Bagga and Baldwin, 1998) computes a precision and recall score for each CE: where RCe is the coreference chain to which ce is assigned in the response (i.e. the system-generated output) and KCe is the coreference chain that contains ce in the key (i.e. the gold standard).</S>
    <S sid="96" ssid="36">Precision and recall for a set of documents are computed as the mean over all CEs in the documents and the F1 score of precision and recall is reported.</S>
    <S sid="97" ssid="37">B3 Complications.</S>
    <S sid="98" ssid="38">Unlike the MUC score, which counts links between CEs, B3 presumes that the gold standard and the system response are clusterings over the same set of CEs.</S>
    <S sid="99" ssid="39">This, of course, is not the case when the system automatically identifies the CEs, so the scoring algorithm requires a mapping between extracted and annotated CEs.</S>
    <S sid="100" ssid="40">We will use the term twin(ce) to refer to the unique annotated/extracted CE to which the extracted/annotated CE is matched.</S>
    <S sid="101" ssid="41">We say that a CE is twinless (has no twin) if no corresponding CE is identified.</S>
    <S sid="102" ssid="42">A twinless extracted CE signals that the resolver extracted a spurious CE, while an annotated CE is twinless when the resolver fails to extract it.</S>
    <S sid="103" ssid="43">Unfortunately, it is unclear how the B3 score should be computed for twinless CEs.</S>
    <S sid="104" ssid="44">Bengtson and Roth (2008) simply discard twinless CEs, but this solution is likely too lenient &#8212; it doles no punishment for mistakes on twinless annotated or extracted CEs and it would be tricked, for example, by a system that extracts only the CEs about which it is most confident.</S>
    <S sid="105" ssid="45">We propose two different ways to deal with twinless CEs for B3.</S>
    <S sid="106" ssid="46">One option, B3all, retains all twinless extracted CEs.</S>
    <S sid="107" ssid="47">It computes the preci4We also experimented with the CEAF score (Luo, 2005), but excluded it due to difficulties dealing with the extracted, rather than annotated, CEs.</S>
    <S sid="108" ssid="48">CEAF assigns a zero score to each twinless extracted CE and weights all coreference chains equally, irrespective of their size.</S>
    <S sid="109" ssid="49">As a result, runs with extracted CEs exhibit very low CEAF precision, leading to unreliable scores. sion as above when ce has a twin, and computes the precision as 1/|RCe |if ce is twinless.</S>
    <S sid="110" ssid="50">(Similarly, recall(ce) = 1/|KC  |if ce is twinless.)</S>
    <S sid="111" ssid="51">The second option, B30, discards twinless extracted CEs, but penalizes recall by setting recall(ce) = 0 for all twinless annotated CEs.</S>
    <S sid="112" ssid="52">Thus, B30 presumes that all twinless extracted CEs are spurious.</S>
    <S sid="113" ssid="53">Results.</S>
    <S sid="114" ssid="54">Table 3, box 1 shows the performance of RECONCILEACL09 using a default (0.5) coreference classifier threshold.</S>
    <S sid="115" ssid="55">The MUC score is highest for the MUC6 data set, while the four ACE data sets show much higher B3 scores as compared to the two MUC data sets.</S>
    <S sid="116" ssid="56">The latter occurs because the ACE data sets include singletons.</S>
    <S sid="117" ssid="57">The classification threshold, however, can be gainfully employed to control the trade-off between precision and recall.</S>
    <S sid="118" ssid="58">This has not traditionally been done in learning-based coreference resolution research &#8212; possibly because there is not much training data available to sacrifice as a validation set.</S>
    <S sid="119" ssid="59">Nonetheless, we hypothesized that estimating a threshold from just the training data might be effective.</S>
    <S sid="120" ssid="60">Our results (BASELINE box in Table 3) indicate that this indeed works well.5 With the exception of MUC6, results on all data sets and for all scoring algorithms improve; moreover, the scores approach those for runs using an optimal threshold (box 3) for the experiment as determined by using the test set.</S>
    <S sid="121" ssid="61">In all remaining experiments, we learn the threshold from the training set as in the BASELINE system.</S>
    <S sid="122" ssid="62">Below, we resume our investigation of the role of three coreference resolution subtasks and measure the impact of each on overall performance.</S>
    <S sid="123" ssid="63">Previous work has shown that resolving coreference between proper names is relatively easy (e.g.</S>
    <S sid="124" ssid="64">Kameyama (1997)) because string matching functions specialized to the type of proper name (e.g. person vs. location) are quite accurate.</S>
    <S sid="125" ssid="65">Thus, we would expect a coreference resolution system to depend critically on its Named Entity (NE) extractor.</S>
    <S sid="126" ssid="66">On the other hand, state-of-the-art NE taggers are already quite good, so improving this component may not provide much additional gain.</S>
    <S sid="127" ssid="67">To study the influence of NE recognition, we replace the system-generated NEs of RECONCILEACL09 with gold-standard NEs and retrain the coreference classifier.</S>
    <S sid="128" ssid="68">Results for each of the data sets are shown in box 4 of Table 3.</S>
    <S sid="129" ssid="69">(No gold standard NEs are available for MUC7.)</S>
    <S sid="130" ssid="70">Comparison to the BASELINE system (box 2) shows that using gold standard NEs leads to improvements on all data sets with the exception of ACE2 and ACE05, on which performance is virtually unchanged.</S>
    <S sid="131" ssid="71">The improvements tend to be small, however, between 0.5 to 3 performance points.</S>
    <S sid="132" ssid="72">We attribute this to two factors.</S>
    <S sid="133" ssid="73">First, as noted above, although far from perfect, NE taggers generally perform reasonably well.</S>
    <S sid="134" ssid="74">Second, only 20 to 25% of the coreference element resolutions required for these data sets involve a proper name (see Section 4).</S>
    <S sid="135" ssid="75">Conclusion #1: Improving the performance of NE taggers is not likely to have a large impact on the performance of state-of-the-art coreference resolution systems.</S>
    <S sid="136" ssid="76">We expect CE detection to be an important subproblem for an end-to-end coreference system.</S>
    <S sid="137" ssid="77">Results for a system that assumes perfect CEs are shown in box 5 of Table 3.</S>
    <S sid="138" ssid="78">For these runs, RECONCILEACL09 uses only the annotated CEs for both training and testing.</S>
    <S sid="139" ssid="79">Using perfect CEs solves a large part of the coreference resolution task: the annotated CEs divulge anaphoricity information, perfect NP boundaries, and perfect information regarding the coreference relation defined for the data set.</S>
    <S sid="140" ssid="80">We see that focusing attention on all and only the annotated CEs leads to (often substantial) improvements in performance on all metrics over all data sets, especially when measured using the MUC score.</S>
    <S sid="141" ssid="81">Conclusion #2: Improving the ability of coreference resolvers to identify coreference elements would likely improve the state-of-the-art immensely &#8212; by 10-20 points in MUC F1 score and from 2-12 F1 points for B3.</S>
    <S sid="142" ssid="82">This finding explains previously published results that exhibit striking variability when run with annotated CEs vs. system-extracted CEs.</S>
    <S sid="143" ssid="83">On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al., 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g.</S>
    <S sid="144" ssid="84">Luo et al. (2004), McCallum and Wellner (2004)).</S>
    <S sid="145" ssid="85">We argue that providing a resolver with the annotated CEs is a rather unrealistic evaluation: determining whether an NP is part of an annotated coreference chain is precisely the job of a coreference resolver!</S>
    <S sid="146" ssid="86">Conclusion #3: Assuming the availability of CEs unrealistically simplifies the coreference resolution task.</S>
    <S sid="147" ssid="87">Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g.</S>
    <S sid="148" ssid="88">Ng and Cardie (2002a) and Bean and Riloff (2004)).</S>
    <S sid="149" ssid="89">The goal of the module is to determine whether or not an NP is anaphoric.</S>
    <S sid="150" ssid="90">For example, pleonastic pronouns (e.g. it is raining) are special cases that do not require coreference resolution.</S>
    <S sid="151" ssid="91">Unfortunately, neither the MUC nor the ACE data sets include anaphoricity information for all NPs.</S>
    <S sid="152" ssid="92">Rather, they encode anaphoricity information implicitly for annotated CEs: a CE is considered anaphoric if is not a singleton.6 To study the utility of anaphoricity information, we train and test only on the &#8220;anaphoric&#8221; extracted CEs, i.e. the extracted CEs that have an annotated twin that is not a singleton.</S>
    <S sid="153" ssid="93">Note that for the MUC datasets all extracted CEs that have twins are considered anaphoric.</S>
    <S sid="154" ssid="94">Results for this experiment (box 6 in Table 3) are similar to the previous experiment using perfect CEs: we observe big improvements across the board.</S>
    <S sid="155" ssid="95">This should not be surprising since the experimental setting is quite close to that for perfect CEs: this experiment also presumes knowledge of when a CE is part of an annotated coreference chain.</S>
    <S sid="156" ssid="96">Nevertheless, we see that anaphoricity infomation is important.</S>
    <S sid="157" ssid="97">First, good anaphoricity identification should reduce the set of extracted CEs making it closer to the set of annotated CEs.</S>
    <S sid="158" ssid="98">Second, further improvements in MUC score for the ACE data sets over the runs using perfect CEs (box 5) reveal that accurately determining anaphoricity can lead to substantial improvements in MUC score.</S>
    <S sid="159" ssid="99">ACE data includes annotations for singleton CEs, so knowling whether an annotated CE is anaphoric divulges additional information.</S>
    <S sid="160" ssid="100">Conclusion #4: An accurate anaphoricity determination component can lead to substantial improvement in coreference resolution performance.</S>
  </SECTION>
  <SECTION title="4 Resolution Complexity" number="4">
    <S sid="161" ssid="1">Different types of anaphora that have to be handled by coreference resolution systems exhibit different properties.</S>
    <S sid="162" ssid="2">In linguistic theory, binding mechanisms vary for different kinds of syntactic constituents and structures.</S>
    <S sid="163" ssid="3">And in practice, empirical results have confirmed intuitions that different types of anaphora benefit from different classifier features and exhibit varying degrees of difficulty (Kameyama, 1997).</S>
    <S sid="164" ssid="4">However, performance 6Also, the first element of a coreference chain is usually non-anaphoric, but we do not consider that issue here. evaluations rarely include analysis of where stateof-the-art coreference resolvers perform best and worst, aside from general conclusions.</S>
    <S sid="165" ssid="5">In this section, we analyze the behavior of our coreference resolver on different types of anaphoric expressions with two goals in mind.</S>
    <S sid="166" ssid="6">First, we want to deduce the strengths and weaknesses of state-of-the-art systems to help direct future research.</S>
    <S sid="167" ssid="7">Second, we aim to understand why current coreference resolvers behave so inconsistently across data sets.</S>
    <S sid="168" ssid="8">Our hypothesis is that the distribution of different types of anaphoric expressions in a corpus is a major factor for coreference resolution performance.</S>
    <S sid="169" ssid="9">Our experiments confirm this hypothesis and we use our empirical results to create a coreference performance prediction (CPP) measure that successfully estimates the expected level of performance on novel data sets.</S>
    <S sid="170" ssid="10">We study the resolution complexity of a text corpus by defining resolution classes.</S>
    <S sid="171" ssid="11">Resolution classes partition the set of anaphoric CEs according to properties of the anaphor and (in some cases) the antecedent.</S>
    <S sid="172" ssid="12">Previous work has studied performance differences between pronominal anaphora, proper names, and common nouns, but we aim to dig deeper into subclasses of each of these groups.</S>
    <S sid="173" ssid="13">In particular, we distinguish between proper and common nouns that can be resolved via string matching, versus those that have no antecedent with a matching string.</S>
    <S sid="174" ssid="14">Intuitively, we expect that it is easier to resolve the cases that involve string matching.</S>
    <S sid="175" ssid="15">Similarly, we partition pronominal anaphora into several subcategories that we expect may behave differently.</S>
    <S sid="176" ssid="16">We define the following nine resolution classes: Proper Names: Three resolution classes cover CEs that are named entities (e.g. the PERSON, LOCATION, ORGANIZATION and DATE classes for MUC and ACE) and have a prior referent7 in the text.</S>
    <S sid="177" ssid="17">These three classes are distinguished by the type of antecedent that can be resolved against the proper name.</S>
    <S sid="178" ssid="18">Common NPs: Three analogous string match classes cover CEs that have a common noun as a head: (4) CN-e (5) CN-p (6) CN-n. As noted above, resolution classes are defined for annotated CEs.</S>
    <S sid="179" ssid="19">We use the twin relationship to match extracted CEs to annotated CEs and to evaluate performance on each resolution class.</S>
    <S sid="180" ssid="20">To score each resolution class separately, we define a new variant of the MUC scorer.</S>
    <S sid="181" ssid="21">We compute a MUC-RC score (for MUC Resolution Class) for class C as follows: we assume that all CEs that do not belong to class C are resolved correctly by taking the correct clustering for them from the gold standard.</S>
    <S sid="182" ssid="22">Starting with this correct partial clustering, we run our classifier on all ordered pairs of CEs for which the second CE is of class C, essentially asking our coreference resolver to determine whether each member of class C is coreferent with each of its preceding CEs.</S>
    <S sid="183" ssid="23">We then count the number of unique correct/incorrect links that the system introduced on top of the correct partial clustering and compute precision, recall, and F1 score.</S>
    <S sid="184" ssid="24">This scoring function directly measures the impact of each resolution class on the overall MUC score.</S>
    <S sid="185" ssid="25">Table 4 shows the results of our resolution class analysis on the test portions of the six data sets.</S>
    <S sid="186" ssid="26">The # columns show the frequency counts for each resolution class, and the % columns show the distributions of the classes in each corpus (i.e.</S>
    <S sid="187" ssid="27">17% of all resolutions in the MUC6 corpus were in the PN-e class).</S>
    <S sid="188" ssid="28">The scr columns show the MUCRC score for each resolution class.</S>
    <S sid="189" ssid="29">The right-hand side of Table 4 shows the average distribution and scores across all data sets.</S>
    <S sid="190" ssid="30">These scores confirm our expectations about the relative difficulty of different types of resolutions.</S>
    <S sid="191" ssid="31">For example, it appears that proper names are easier to resolve than common nouns; gendered pronouns are easier than 1st and 2nd person pronouns, which, in turn, are easier than ungendered 3rd person pronouns.</S>
    <S sid="192" ssid="32">Similarly, our intuition is confirmed that many CEs can be accurately resolved based on exact string matching, whereas resolving against antecedents that do not have overlapping strings is much more difficult.</S>
    <S sid="193" ssid="33">The average scores in Table 4 show that performance varies dramatically across the resolution classes, but, on the surface, appears to be relatively consistent across data sets.</S>
    <S sid="194" ssid="34">None of the data sets performs exactly the same, of course, so we statistically analyze whether the behavior of each resolution class is similar across the data sets.</S>
    <S sid="195" ssid="35">For each data set, we compute the correlation between the vector of MUC-RC scores over the resolution classes and the average vector of MUC-RC scores for the remaining five data sets.</S>
    <S sid="196" ssid="36">Table 5 contains the results, which show high correlations (over .90) for all six data sets.</S>
    <S sid="197" ssid="37">These results indicate that the relative performance of the resolution classes is consistent across corpora.</S>
    <S sid="198" ssid="38">Next, we hypothesize that the distribution of resolution classes in a corpus explains (at least partially) why performance varies so much from corpus to corpus.</S>
    <S sid="199" ssid="39">To explore this issue, we create a sure to predict the performance on new data sets.</S>
    <S sid="200" ssid="40">The CPP measure uses the empirical performance of each resolution class observed on previous data sets and forms a predicton based on the make-up of resolution classes in a new corpus.</S>
    <S sid="201" ssid="41">The distribution of resolution classes for a new corpus can be easily determined because the classes can be recognized superficially by looking only at the strings that represent each NP.</S>
    <S sid="202" ssid="42">We compute the CPP score for each of our six data sets based on the average resolution class performance measured on the other five data sets.</S>
    <S sid="203" ssid="43">The predicted score for each class is computed as a weighted sum of the observed scores for each resolution class (i.e. the mean for the class measured on the other five data sets) weighted by the proportion of CEs that belong to the class.</S>
    <S sid="204" ssid="44">The predicted scores are shown in Table 6 and compared with the MUC scores that are produced by RECONCILEACL09.8 Our results show that the CPP measure is a good predictor of coreference resolution performance on unseen data sets, with the exception of one outlier &#8211; the MUC6 data set.</S>
    <S sid="205" ssid="45">In fact, the correlation between predicted and observed scores is 0.731 for all data sets and 0.913 excluding MUC6.</S>
    <S sid="206" ssid="46">RECONCILEACL09&#8217;s performance on MUC6 is better than predicted due to the higher than average scores for the common noun classes.</S>
    <S sid="207" ssid="47">We attribute this to the fact that MUC6 includes annotations for nested nouns, which almost always fall in the CN-e and CN-p classes.</S>
    <S sid="208" ssid="48">In addition, many of the features were first created for the MUC6 data set, so the feature extractors are likely more accurate than for other data sets.</S>
    <S sid="209" ssid="49">Overall, results indicate that coreference performance is substantially influenced by the mix of resolution classes found in the data set.</S>
    <S sid="210" ssid="50">Our CPP measure can be used to produce a good estimate of the level of performance on a new corpus.</S>
    <S sid="211" ssid="51">8Observed scores for MUC6 and 7 differ slightly from Table 3 because this part of the work did not use the OPTIONAL field of the key, employed by the official MUC scorer.</S>
  </SECTION>
  <SECTION title="5 Related Work" number="5">
    <S sid="212" ssid="1">The bulk of the relevant related work is described in earlier sections, as appropriate.</S>
    <S sid="213" ssid="2">This paper studies complexity issues for NP coreference resolution using a &#8220;good&#8221;, i.e. near state-of-the-art, system.</S>
    <S sid="214" ssid="3">For state-of-the-art performance on the MUC data sets see, e.g.</S>
    <S sid="215" ssid="4">Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g.</S>
    <S sid="216" ssid="5">Bengtson and Roth (2008) and Luo (2007).</S>
    <S sid="217" ssid="6">While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance.</S>
    <S sid="218" ssid="7">In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g.</S>
    <S sid="219" ssid="8">Mitkov (2002) and Byron (2001)).</S>
    <S sid="220" ssid="9">However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task.</S>
    <S sid="221" ssid="10">Previous work has developed methods to predict system performance on NLP tasks given data set characteristics, e.g.</S>
    <S sid="222" ssid="11">Birch et al. (2008) does this for machine translation.</S>
    <S sid="223" ssid="12">Our work looks for the first time at predicting the performance of NP coreference resolvers.</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="6">
    <S sid="224" ssid="1">We examine the state-of-the-art in NP coreference resolution.</S>
    <S sid="225" ssid="2">We show the relative impact of perfect NE recognition, perfect anaphoricity information for coreference elements, and knowledge of all and only the annotated CEs.</S>
    <S sid="226" ssid="3">We also measure the performance of state-of-the-art resolvers on several classes of anaphora and use these results to develop a measure that can accurately estimate a resolver&#8217;s performance on new data sets.</S>
    <S sid="227" ssid="4">Acknowledgments.</S>
    <S sid="228" ssid="5">We gratefully acknowledge technical contributions from David Buttler and David Hysom in creating the Reconcile coreference resolution platform.</S>
    <S sid="229" ssid="6">This research was supported in part by the Department of Homeland Security under ONR Grant N0014-07-1-0152 and Lawrence Livermore National Laboratory subcontract B573245.</S>
    <S sid="230" ssid="7">Xiaoqiang Luo.</S>
    <S sid="231" ssid="8">2007.</S>
    <S sid="232" ssid="9">Coreference or Not: A Twin Model for Coreference Resolution.</S>
    <S sid="233" ssid="10">In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2007).</S>
  </SECTION>
</PAPER>
