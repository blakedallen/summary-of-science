<PAPER>
  <S sid="0">Bayesian Word Sense Induction</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Sense induction seeks to automatically identify word senses directly from a corpus.</S>
    <S sid="2" ssid="2">A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning.</S>
    <S sid="3" ssid="3">Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word&#8217;s contexts into different classes, each representing a word sense.</S>
    <S sid="4" ssid="4">Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.</S>
    <S sid="5" ssid="5">The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task.</S>
    <S sid="6" ssid="6">The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">Sense induction is the task of discovering automatically all possible senses of an ambiguous word.</S>
    <S sid="8" ssid="2">It is related to, but distinct from, word sense disambiguation (WSD) where the senses are assumed to be known and the aim is to identify the intended meaning of the ambiguous word in context.</S>
    <S sid="9" ssid="3">Although the bulk of previous work has been devoted to the disambiguation problem1, there are good reasons to believe that sense induction may be able to overcome some of the issues associated with WSD.</S>
    <S sid="10" ssid="4">Since most disambiguation methods assign senses according to, and with the aid of, dictionaries or other lexical resources, it is difficult to adapt them to new domains or to languages where such resources are scarce.</S>
    <S sid="11" ssid="5">A related problem concerns the granularity of the sense distinctions which is fixed, and may not be entirely suitable for different applications.</S>
    <S sid="12" ssid="6">In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand.</S>
    <S sid="13" ssid="7">There is little risk that an important sense will be left out, or that irrelevant senses will influence the results.</S>
    <S sid="14" ssid="8">Furthermore, recent work in machine translation (Vickrey et al., 2005) and information retrieval (V&#180;eronis, 2004) indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed (Carpuat and Wu, 2005; Voorhees, 1993).</S>
    <S sid="15" ssid="9">Sense induction is typically treated as an unsupervised clustering problem.</S>
    <S sid="16" ssid="10">The input to the clustering algorithm are instances of the ambiguous word with their accompanying contexts (represented by co-occurrence vectors) and the output is a grouping of these instances into classes corresponding to the induced senses.</S>
    <S sid="17" ssid="11">In other words, contexts that are grouped together in the same class represent a specific word sense.</S>
    <S sid="18" ssid="12">In this paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model.</S>
    <S sid="19" ssid="13">For each ambiguous word we first draw a distribution over senses, and then generate context words according to this distribution.</S>
    <S sid="20" ssid="14">It is thus assumed that different senses will correspond to distinct lexical distributions.</S>
    <S sid="21" ssid="15">In this framework, sense distinctions arise naturally through the generative process: our model postulates that the observed data (word contexts) are explicitly intended to communicate a latent structure (their meaning).</S>
    <S sid="22" ssid="16">Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.</S>
    <S sid="23" ssid="17">LDA models each document using a mixture over K topics, which are in turn characterized as distributions over words.</S>
    <S sid="24" ssid="18">The words in the document are generated by repeatedly sampling a topic according to the topic distribution, and selecting a word given the chosen topic.</S>
    <S sid="25" ssid="19">Whereas LDA generates words from global topics corresponding to the whole document, our model generates words from local topics chosen based on a context window around the ambiguous word.</S>
    <S sid="26" ssid="20">Document-level topics resemble general domain labels (e.g., finance, education) and cannot faithfully model more fine-grained meaning distinctions.</S>
    <S sid="27" ssid="21">In our work, therefore, we create an individual model for every (ambiguous) word rather than a global model for an entire document collection.</S>
    <S sid="28" ssid="22">We also show how multiple information sources can be straightforwardly integrated without changing the underlying probabilistic model.</S>
    <S sid="29" ssid="23">For instance, besides lexical information we may want to consider parts of speech or dependencies in our sense induction problem.</S>
    <S sid="30" ssid="24">This is in marked contrast with previous LDA-based models which mostly take only word-based information into account.</S>
    <S sid="31" ssid="25">We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art.</S>
    <S sid="32" ssid="26">The remainder of this paper is structured as follows.</S>
    <S sid="33" ssid="27">We first present an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4).</S>
    <S sid="34" ssid="28">Section 5 describes the resources and evaluation methodology used in our experiments.</S>
    <S sid="35" ssid="29">We discuss our results in Section 6, and conclude in Section 7.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="36" ssid="1">Sense induction is typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts.</S>
    <S sid="37" ssid="2">Considerable latitude is allowed in selecting and representing the cooccurring contexts.</S>
    <S sid="38" ssid="3">Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch&#168;utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003).</S>
    <S sid="39" ssid="4">The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004).</S>
    <S sid="40" ssid="5">In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method.</S>
    <S sid="41" ssid="6">A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch&#168;utze, 1998), and the Information Bottleneck (Niu et al., 2007).</S>
    <S sid="42" ssid="7">Graph-based methods have also been applied to the sense induction task.</S>
    <S sid="43" ssid="8">In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences.</S>
    <S sid="44" ssid="9">Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V&#180;eronis, 2004; Dorow and Widdows, 2003).</S>
    <S sid="45" ssid="10">Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature.</S>
    <S sid="46" ssid="11">The inferred document-level topics can help determine coarsegrained sense distinctions.</S>
    <S sid="47" ssid="12">Cai et al. (2007) propose to use LDA&#8217;s word-topic distributions as features for training a supervised WSD system.</S>
    <S sid="48" ssid="13">In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD.</S>
    <S sid="49" ssid="14">Here, LDA topics are integrated with McCarthy et al.&#8217;s (2004) algorithm.</S>
    <S sid="50" ssid="15">For each target word, a topic is sampled from the document&#8217;s topic distribution, and a word is generated from that topic.</S>
    <S sid="51" ssid="16">Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word.</S>
    <S sid="52" ssid="17">Then, the word sense is selected based on the word, neighbor, and topic.</S>
    <S sid="53" ssid="18">Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process.</S>
    <S sid="54" ssid="19">In this case the model discovers both the topics of the corpus and the senses assigned to each of its words.</S>
    <S sid="55" ssid="20">Our own model is also inspired by LDA but crucially performs word sense induction, not disambiguation.</S>
    <S sid="56" ssid="21">Unlike the work mentioned above, we do not rely on a pre-existing list of senses, and do not assume a correspondence between our automatically derived sense-clusters and those of any given inventory.2 A key element in these previous attempts at adapting LDA for WSD is the tendency to remain at a high level, document-like, setting.</S>
    <S sid="57" ssid="22">In contrast, we make use of much smaller units of text (a few sentences, rather than a full document), and create an individual model for each (ambiguous) word type.</S>
    <S sid="58" ssid="23">Our induced senses are few in number (typically less than ten).</S>
    <S sid="59" ssid="24">This is in marked contrast to tens, and sometimes hundreds, of topics commonly used in document-modeling tasks.</S>
    <S sid="60" ssid="25">Unlike many conventional clustering methods (e.g., Purandare and Pedersen 2004; Sch&#168;utze 1998), our model is probabilistic; it specifies a probability distribution over possible values, which makes it easy to integrate and combine with other systems via mixture or product models.</S>
    <S sid="61" ssid="26">Furthermore, the Bayesian framework allows the incorporation of several information sources in a principled manner.</S>
    <S sid="62" ssid="27">Our model can easily handle an arbitrary number of feature classes (e.g., parts of speech, dependencies).</S>
    <S sid="63" ssid="28">This functionality in turn enables us to evaluate which linguistic information matters for the sense induction task.</S>
    <S sid="64" ssid="29">Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al.</S>
    <S sid="65" ssid="30">2003) have been task-specific and limited to only two layers of information.</S>
    <S sid="66" ssid="31">Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction.</S>
  </SECTION>
  <SECTION title="3 The Sense Induction Model" number="3">
    <S sid="67" ssid="1">The core idea behind sense induction is that contextual information provides important cues regarding a word&#8217;s meaning.</S>
    <S sid="68" ssid="2">The idea dates back to (at least) Firth (1957) (&#8220;You shall know a word by the company it keeps&#8221;), and underlies most WSD and lexicon acquisition work to date.</S>
    <S sid="69" ssid="3">Under this premise, we should expect different senses to be signaled by different lexical distributions.</S>
    <S sid="70" ssid="4">We can place sense induction in a probabilistic setting by modeling the context words around the ambiguous target as samples from a multinomial sense distribution.</S>
    <S sid="71" ssid="5">More formally, we will write P(s) for the distribution over senses s of an ambiguous target in a specific context window and P(w1s) for the probability distribution over context words w given sense s. Each word wi in the context window is generated by first sampling a sense from the sense distribution, then choosing a word from the sense-context distribution.</S>
    <S sid="72" ssid="6">P(si = j) denotes the probability that the jth sense was sampled for the ith word token and P(wi|si = j) the probability of context word wi under sense j.</S>
    <S sid="73" ssid="7">The model thus specifies a distribution over words within a context window: where S is the number of senses.</S>
    <S sid="74" ssid="8">We assume that each target word has C contexts and each context c cate conditional dependencies between variables, whereas plates (the rectangles in the figure) refer to repetitions of sampling steps.</S>
    <S sid="75" ssid="9">The variables in the lower right corner refer to the number of samples. consists of Nc word tokens.</S>
    <S sid="76" ssid="10">We shall write &#65533;(j) as a shorthand for P(wi|si = j), the multinomial distribution over words for sense j, and 0(c) as a shorthand for the distribution of senses in context c. Following Blei et al. (2003) we will assume that the mixing proportion over senses 0 is drawn from a Dirichlet prior with parameters a.</S>
    <S sid="77" ssid="11">The role of the hyperparameter a is to create a smoothed sense distribution.</S>
    <S sid="78" ssid="12">We also place a symmetric Dirichlet R on &#65533; (Griffiths and Steyvers, 2002).</S>
    <S sid="79" ssid="13">The hyperparmeter R can be interpreted as the prior observation count on the number of times context words are sampled from a sense before any word from the corpus is observed.</S>
    <S sid="80" ssid="14">Our model is represented in graphical notation in Figure 1.</S>
    <S sid="81" ssid="15">The model sketched above only takes word information into account.</S>
    <S sid="82" ssid="16">Methods developed for supervised WSD often use a variety of information sources based not only on words but also on lemmas, parts of speech, collocations and syntactic relationships (Lee and Ng, 2002).</S>
    <S sid="83" ssid="17">The first idea that comes to mind, is to use the same model while treating various features as word-like elements.</S>
    <S sid="84" ssid="18">In other words, we could simply assume that the contexts we wish to model are the union of all our features.</S>
    <S sid="85" ssid="19">Although straightforward, this solution is undesirable.</S>
    <S sid="86" ssid="20">It merges the distributions of distinct feature categories into a single one, and is therefore conceptually incorrect, and can affect the performance of the model.</S>
    <S sid="87" ssid="21">For instance, parts-ofspeech (which have few values, and therefore high probability), would share a distribution with words (which are much sparser).</S>
    <S sid="88" ssid="22">Layers containing more elements (e.g.</S>
    <S sid="89" ssid="23">10 word window) would overwhelm rectangles represent different sources (layers) of information.</S>
    <S sid="90" ssid="24">All layers share the same, instancespecific, sense distribution (0), but each have their own (multinomial) sense-feature distribution (&#65533;).</S>
    <S sid="91" ssid="25">Shaded nodes represent observed features f; these can be words, parts of speech, collocations or dependencies. unconditional joint distribution P(s) of the unobserved variables (provided certain criteria are fulfilled).</S>
    <S sid="92" ssid="26">In our model, each element in each layer is a variable, and is assigned a sense label (see Figure 2, where distinct layers correspond to different representations of the context around the target word).</S>
    <S sid="93" ssid="27">From these assignments, we must determine the sense distribution of the instance as a whole.</S>
    <S sid="94" ssid="28">This is the purpose of the Gibbs sampling procedure.</S>
    <S sid="95" ssid="29">Specifically, in order to derive the update function used in the Gibbs sampler, we must provide the conditional probability of the i-th variable being assigned sense si in layer l, given the feature value fi of the context variable and the current sense assignments of all the other variables in the data (s&#8722;i): p(si|s&#8722;i, f) &#8212; p(fi|s, f &#8722;i,R) &#183; p(si|s&#8722;i,a) (2) The probability of a single sense assignment, si, is proportional to the product of the likelihood (of feature fi, given the rest of the data) and the prior probability of the assignment. smaller ones (e.g.</S>
    <S sid="96" ssid="30">1 word window).</S>
    <S sid="97" ssid="31">Our solution is to treat each information source (or feature type) individually and then combine all of them together in a unified model.</S>
    <S sid="98" ssid="32">Our underlying assumption is that the context window around the target word can have multiple representations, all of which share the same sense distribution.</S>
    <S sid="99" ssid="33">We illustrate this in Figure 2 where each inner rectangle (layer) corresponds to a distinct feature type.</S>
    <S sid="100" ssid="34">We will naively assume independence between multiple layers, even though this is clearly not the case in our task.</S>
    <S sid="101" ssid="35">The idea here is to model each layer as faithfully as possible to the empirical data while at the same time combining information from all layers in estimating the sense distribution of each target instance.</S>
  </SECTION>
  <SECTION title="4 Inference" number="4">
    <S sid="102" ssid="1">Our inference procedure is based on Gibbs sampling (Geman and Geman, 1984).</S>
    <S sid="103" ssid="2">The procedure begins by randomly initializing all unobserved random variables.</S>
    <S sid="104" ssid="3">At each iteration, each random variable si is sampled from the conditional distribution P(si|s&#8722;i) where s&#8722;i refers to all variables other than si.</S>
    <S sid="105" ssid="4">Eventually, the distribution over samples drawn from this process will converge to the f p(fi |l, S, 0) &#183; p(O |f-i, Rt)dO _ #(fi, si) + Rl For the likelihood term p(fi|s, f &#8722;i,R), integrating over all possible values of the multinomial featuresense distribution &#65533; gives us the rightmost term in Equation 3, which has an intuitive interpretation.</S>
    <S sid="106" ssid="5">The term #(fi,si) indicates the number of times the feature-value fi was assigned sense si in the rest of the data.</S>
    <S sid="107" ssid="6">Similarly, #(si) indicates the number of times the sense assignment si was observed in the data.</S>
    <S sid="108" ssid="7">Rl is the Dirichlet prior for the featuresense distribution &#65533; in the current layer l, and Vl is the size of the vocabulary of that layer, i.e., the number of possible feature values in the layer.</S>
    <S sid="109" ssid="8">Intuitively, the probability of a feature-value given a sense is directly proportional to the number of times we have seen that value and that senseassignment together in the data, taking into account a pseudo-count prior, expressed through R. This can also be viewed as a form of smoothing.</S>
    <S sid="110" ssid="9">A similar approach is taken with regards to the prior probability p(si|s&#8722;i,a).</S>
    <S sid="111" ssid="10">In this case, however, all layers must be considered: Here &#955;l is the weight for the contribution of layer l, and &#945;l is the portion of the Dirichlet prior for the sense distribution &#952; in the current layer.</S>
    <S sid="112" ssid="11">Treating each layer individually, we integrate over the possible values of &#952;, obtaining a similar count-based term: where #l(si) indicates the number of elements in layer l assigned the sense si, #l indicates the number of elements in layer l, i.e., the size of the layer and S the number of senses.</S>
    <S sid="113" ssid="12">To distribute the pseudo counts represented by &#945; in a reasonable fashion among the layers, we define &#945;l = #l #m &#183; &#945; where #m = &#8721;l #l, i.e., the total size of the instance.</S>
    <S sid="114" ssid="13">This distributes &#945; according to the relative size of each layer in the instance.</S>
    <S sid="115" ssid="14">Placing these values in Equation 4 we obtain the following: #m+S&#183;&#945; Putting it all together, we arrive at the final update equation for the Gibbs sampling: Note that when dealing with a single layer, Equation 8 collapses to: where #m(si) indicates the number of elements (e.g., words) in the context window assigned to sense si.</S>
    <S sid="116" ssid="15">This is identical to the update equation in the original, word-based LDA model.</S>
    <S sid="117" ssid="16">The sampling algorithm gives direct estimates of s for every context element.</S>
    <S sid="118" ssid="17">However, in view of our task, we are more interested in estimating &#952;, the sense-context distribution which can be obtained as in Equation 7, but taking into account all sense assignments, without removing assignment i.</S>
    <S sid="119" ssid="18">Our system labels each instance with the single, most probable sense.</S>
  </SECTION>
  <SECTION title="5 Evaluation Setup" number="5">
    <S sid="120" ssid="1">In this section we discuss our experimental set-up for assessing the performance of the model presented above.</S>
    <S sid="121" ssid="2">We give details on our training procedure, describe our features, and explain how our system output was evaluated.</S>
    <S sid="122" ssid="3">Data In this work, we focus solely on inducing senses for nouns, since they constitute the largest portion of content words.</S>
    <S sid="123" ssid="4">For example, nouns represent 45% of the content words in the British National Corpus.</S>
    <S sid="124" ssid="5">Moreover, for many tasks and applications (e.g., web queries, Jansen et al. 2000) nouns are the most frequent and most important part-of-speech.</S>
    <S sid="125" ssid="6">For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007).</S>
    <S sid="126" ssid="7">The dataset contains texts from the Penn Treebank II corpus, a collection of articles from the first half of the 1989 Wall Street Journal (WSJ).</S>
    <S sid="127" ssid="8">It is hand-annotated with OntoNotes senses (Hovy et al., 2006) and has 35 nouns.</S>
    <S sid="128" ssid="9">The average noun ambiguity is 3.9, with a high (almost 80%) skew towards the predominant sense.</S>
    <S sid="129" ssid="10">This is not entirely surprising since OntoNotes senses are less fine-grained than WordNet senses.</S>
    <S sid="130" ssid="11">We used two corpora for training as we wanted to evaluate our model&#8217;s performance across different domains.</S>
    <S sid="131" ssid="12">The British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations.</S>
    <S sid="132" ssid="13">This served as our out-of-domain corpus, and contained approximately 730 thousand instances of the 35 target nouns in the Semeval lexical sample.</S>
    <S sid="133" ssid="14">The second, in-domain, corpus was built from selected portions of the Wall Street Journal.</S>
    <S sid="134" ssid="15">We used all articles (excluding the Penn Treebank II portion used in the Semeval dataset) from the years 1987-89 and 1994 to create a corpus of similar size to the BNC, containing approximately 740 thousand instances of the target words.</S>
    <S sid="135" ssid="16">Additionally, we used the Senseval 2 and 3 lexical sample data (Preiss and Yarowsky, 2001; Mihalcea and Edmonds, 2004) as development sets, for experimenting with the hyper-parameters of our model (see Section 6).</S>
    <S sid="136" ssid="17">Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods.</S>
    <S sid="137" ssid="18">Under the first scheme, the system output is compared to the gold standard using standard clustering evaluation metrics (e.g., purity, entropy).</S>
    <S sid="138" ssid="19">Here, no attempt is made to match the induced senses against the labels of the gold standard.</S>
    <S sid="139" ssid="20">Under the second scheme, the gold standard is partitioned into a test and training corpus.</S>
    <S sid="140" ssid="21">The latter is used to derive a mapping of the induced senses to the gold standard labels.</S>
    <S sid="141" ssid="22">The mapping is then used to calculate the system&#8217;s F-Score on the test corpus.</S>
    <S sid="142" ssid="23">Unfortunately, the first scheme failed to discriminate among participating systems.</S>
    <S sid="143" ssid="24">The onecluster-per-word baseline outperformed all systems, except one, which was only marginally better.</S>
    <S sid="144" ssid="25">The scheme ignores the actual labeling and due to the dominance of the first sense in the data, encourages a single-sense approach which is further amplified by the use of a coarse-grained sense inventory.</S>
    <S sid="145" ssid="26">For the purposes of this work, therefore, we focused on the second evaluation scheme.</S>
    <S sid="146" ssid="27">Here, most of the participating systems outperformed the most-frequent-sense baseline, and the rest obtained only slightly lower scores.</S>
    <S sid="147" ssid="28">Feature Space Our experiments used a feature set designed to capture both immediate local context, wider context and syntactic context.</S>
    <S sid="148" ssid="29">Specifically, we experimented with six feature categories: &#177;10-word window (10w), &#177;5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp).</S>
    <S sid="149" ssid="30">These features have been widely adopted in various WSD algorithms (see Lee and Ng 2002 for a detailed evaluation).</S>
    <S sid="150" ssid="31">In all cases, we use the lemmatized version of the word(s).</S>
    <S sid="151" ssid="32">The Semeval workshop organizers provided a small amount of context for each instance (usually a sentence or two surrounding the sentence containing the target word).</S>
    <S sid="152" ssid="33">This context, as well as the text in the training corpora, was parsed using RASP (Briscoe and Carroll, 2002), to extract part-of-speech tags, lemmas, and dependency information.</S>
    <S sid="153" ssid="34">For instances containing more than one occurrence of the target word, we disambiguate the first occurrence.</S>
    <S sid="154" ssid="35">Instances which were not correctly recognized by the parser (e.g., a target word labeled with the wrong lemma or part-of-speech), were automatically assigned to the largest sensecluster.3</S>
  </SECTION>
  <SECTION title="6 Experiments" number="6">
    <S sid="155" ssid="1">Model Selection The framework presented in Section 3 affords great flexibility in modeling the empirical data.</S>
    <S sid="156" ssid="2">This however entails that several parameters must be instantiated.</S>
    <S sid="157" ssid="3">More precisely, our model is conditioned on the Dirichlet hyperparameters &#945; and &#946; and the number of senses S. Additional parameters include the number of iterations for the Gibbs sampler and whether or not the layers are assigned different weights.</S>
    <S sid="158" ssid="4">Our strategy in this paper is to fix &#945; and &#946; and explore the consequences of varying S. The value for the &#945; hyperparameter was set to 0.02.</S>
    <S sid="159" ssid="5">This was optimized in an independent tuning experiment which used the Senseval 2 (Preiss and Yarowsky, 2001) and Senseval 3 (Mihalcea and Edmonds, 2004) datasets.</S>
    <S sid="160" ssid="6">We experimented with &#945; values ranging from 0.005 to 1.</S>
    <S sid="161" ssid="7">The &#946; parameter was set to 0.1 (in all layers).</S>
    <S sid="162" ssid="8">This value is often considered optimal in LDA-related models (Griffiths and Steyvers, 2002).</S>
    <S sid="163" ssid="9">For simplicity, we used uniform weights for the layers.</S>
    <S sid="164" ssid="10">The Gibbs sampler was run for 2,000 iterations.</S>
    <S sid="165" ssid="11">Due to the randomized nature of the inference procedure, all reported results are average scores over ten runs.</S>
    <S sid="166" ssid="12">Our experiments used the same number of senses for all the words, since tuning this number individually for each word would be prohibitive.</S>
    <S sid="167" ssid="13">We experimented with values ranging from three to nine senses.</S>
    <S sid="168" ssid="14">Figure 3 shows the results obtained for different numbers of senses when the model is trained on the WSJ (in-domain) and BNC (out-ofdomain) corpora, respectively.</S>
    <S sid="169" ssid="15">Here, we are using the optimal combination of layers for each system (which we discuss in the following section in detail).</S>
    <S sid="170" ssid="16">For the model trained on WSJ, performance peaks at four senses, which is similar to the average ambiguity in the test data.</S>
    <S sid="171" ssid="17">For the model trained on the BNC, however, the best results are obtained using twice as many senses.</S>
    <S sid="172" ssid="18">Using fewer senses with the BNC-trained system can result in a drop in accuracy of almost 2%.</S>
    <S sid="173" ssid="19">This is due to the shift in domain.</S>
    <S sid="174" ssid="20">As the sense-divisions of the learning domain do not match those of the target domain, finer granularity is required in order to encompass all the relevant distinctions.</S>
    <S sid="175" ssid="21">Table 1 illustrates the senses inferred for the word drug when using the in-domain and out-ofdomain corpora, respectively.</S>
    <S sid="176" ssid="22">The most probable words for each sense are also shown.</S>
    <S sid="177" ssid="23">Firstly, note that the model infers some plausible senses for drug on the WSJ corpus (top half of Table 1).</S>
    <S sid="178" ssid="24">Sense 1 corresponds to the &#8220;enforcement&#8221; sense of drug, Sense 2 refers to &#8220;medication&#8221;, Sense 3 to the &#8220;drug industry&#8221; and Sense 4 to &#8220;drugs research&#8221;.</S>
    <S sid="179" ssid="25">The inferred senses for drug on the BNC (bottom half of Table 1) are more fine grained.</S>
    <S sid="180" ssid="26">For example, the model finds distinct senses for &#8220;medication&#8221; (Sense 1 and 7) and &#8220;illegal substance&#8221; (Senses 2, 4, 6, 7).</S>
    <S sid="181" ssid="27">It also finds a separate sense for &#8220;drug dealing&#8221; (Sense 5) and &#8220;enforcement&#8221; (Sense 8).</S>
    <S sid="182" ssid="28">Because the BNC has a broader focus, finer distinctions are needed to cover as many senses as possible that are relevant to the target domain (WSJ).</S>
    <S sid="183" ssid="29">Layer Analysis We next examine which individual feature categories are most informative in our sense induction task.</S>
    <S sid="184" ssid="30">We also investigate whether their combination, through our layered model (see Figure 2), yields performance improvements.</S>
    <S sid="185" ssid="31">We used 4 senses for the system trained on WSJ and 8 for the system trained on the BNC (a was set to 0.02 and b to 0.1) Table 2 (left side) shows the performance of our model when using only one layer.</S>
    <S sid="186" ssid="32">The layer composed of words co-occurring within a &#177;10-word window (10w), and representing wider, topical, information gives the highest scores on its own.</S>
    <S sid="187" ssid="33">It is followed by the &#177;5 (5w) and &#177;1 (1w) word windows, which represent more immediate, local context.</S>
    <S sid="188" ssid="34">Part-of-speech n-grams (pg) and word ngrams (ng), on their own, achieve lower scores, largely due to over-generalization and data sparseness, respectively.</S>
    <S sid="189" ssid="35">The lowest-scoring single layer is the dependency layer (dp), with performance only slightly above the most-frequent-sense baseline (MFS).</S>
    <S sid="190" ssid="36">Dependency information is very informative when present, but extremely sparse.</S>
    <S sid="191" ssid="37">Table 2 (middle) also shows the results obtained when running the layered model with all but one of the layers as input.</S>
    <S sid="192" ssid="38">We can use this information to determine the contribution of each layer by comparing to the combined model with all layers (all).</S>
    <S sid="193" ssid="39">Because we are dealing with multiple layers, there is an element of overlap involved.</S>
    <S sid="194" ssid="40">Therefore, each of the word-window layers, despite relatively high informativeness on its own, does not cause as much damage when it is absent, since the other layers compensate for the topical and local information.</S>
    <S sid="195" ssid="41">The absence of the word n-gram layer, which provides specific local information, does not make a great impact when the 1w and pg layers are present.</S>
    <S sid="196" ssid="42">Finally, we can see that the extremely sparse dependency layer is detrimental to the multi-layer model as a whole, and its removal increases performance.</S>
    <S sid="197" ssid="43">The sparsity of the data in this layer means that there is often little information on which to base a decision.</S>
    <S sid="198" ssid="44">In these cases, the layer contributes a close-to-uniform estimation of the sense distribution, which confuses the combined model.</S>
    <S sid="199" ssid="45">Other layer combinations obtained similar results.</S>
    <S sid="200" ssid="46">Table 2 (right side) shows the most informative two and three layer combinations.</S>
    <S sid="201" ssid="47">Again, dependencies tend to decrease performance.</S>
    <S sid="202" ssid="48">On the other hand, combining features that have similar performance on their own is beneficial.</S>
    <S sid="203" ssid="49">We obtain the best performance overall with a two layered model combining topical (+10w) and local (+5w) contexts.</S>
    <S sid="204" ssid="50">Table 3 replicates the same suite of experiments on the BNC corpus.</S>
    <S sid="205" ssid="51">The general trends are similar.</S>
    <S sid="206" ssid="52">Some interesting differences are apparent, however.</S>
    <S sid="207" ssid="53">The sparser layers, notably word n-grams and dependencies, fare comparatively worse.</S>
    <S sid="208" ssid="54">This is expected, since the more precise, local, information is likely to vary strongly across domains.</S>
    <S sid="209" ssid="55">Even when both domains refer to the same sense of a word, it is likely to be used in a different immediate context, and local contextual information learned in one domain will be less effective in the other.</S>
    <S sid="210" ssid="56">Another observable difference is that the combined model without the dependency layer does slightly better than each of the single layers.</S>
    <S sid="211" ssid="57">The 1w+pg combination improves over its components, which have similar individual performance.</S>
    <S sid="212" ssid="58">Finally, the best performing model on the BNC also combines two layers capturing wider (10w) and more local (5w) contextual information (see Table 3, right side).</S>
    <S sid="213" ssid="59">Comparison to State-of-the-Art Table 4 compares our model against the two best performing sense induction systems that participated in the Semeval-2007 competition.</S>
    <S sid="214" ssid="60">IR2 (Niu et al., 2007) performed sense induction using the Information Bottleneck algorithm, whereas UMND2 (Pedersen, 2007) used k-means to cluster second order co-occurrence vectors associated with the target word.</S>
    <S sid="215" ssid="61">These models and our own model significantly outperform the most-frequent-sense baseline (p &lt; 0.01 using a x2 test).</S>
    <S sid="216" ssid="62">Our best system (10w+5w on WSJ) is significantly better than UMND2 (p &lt; 0.01) and quantitatively better than IR2, although the difference is not statistically significant.</S>
  </SECTION>
  <SECTION title="7 Discussion" number="7">
    <S sid="217" ssid="1">This paper presents a novel Bayesian approach to sense induction.</S>
    <S sid="218" ssid="2">We formulated sense induction in a generative framework that describes how the contexts surrounding an ambiguous word might be generated on the basis of latent variables.</S>
    <S sid="219" ssid="3">Our model incorporates features based on lexical information, parts of speech, and dependencies in a principled manner, and outperforms state-of-theart systems.</S>
    <S sid="220" ssid="4">Crucially, the approach is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account.</S>
    <S sid="221" ssid="5">For example, in document classification, one could consider an accompanying image and its caption as possible additional layers to the main text.</S>
    <S sid="222" ssid="6">In the future, we hope to explore more rigorous parameter estimation techniques.</S>
    <S sid="223" ssid="7">Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.</S>
    <S sid="224" ssid="8">Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model&#8217;s performance.</S>
    <S sid="225" ssid="9">In addition, we could allow an infinite number of senses and use an infinite Dirichlet model (Teh et al., 2006) to automatically determine how many senses are optimal.</S>
    <S sid="226" ssid="10">This provides an elegant solution to the model-order problem, and eliminates the need for external cluster-validation methods.</S>
    <S sid="227" ssid="11">Acknowledgments The authors acknowledge the support of EPSRC (grant EP/C538447/1).</S>
    <S sid="228" ssid="12">We are grateful to Sharon Goldwater for her feedback on earlier versions of this work.</S>
  </SECTION>
</PAPER>
