Models Of Translational Equivalence Among Words
Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data.
First, most words translate to only one other word.
Second, bitext correspondence is typically only partial - many words in each text have no clear equivalent in the other text.
This article presents methods for biasing statistical translation models to reflect these properties.
Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model.
This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs.
Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks.
Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms.
We measure the orthographic similarity using longest common subsequence ratio (LCSR).
We define a direct association as an association between two words where the two words are indeed mutual translations.
We propose Competitive Linking Algorithm (CLA) to align the words to construct confusion network.
We use competitive linking to greedily construct matchings where the pair score is a measure of word-to-word association.
We argue that there are ways to determine the boundaries of some multi-words phrases, allowing to treat several words as a single token.
