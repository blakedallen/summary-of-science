[
  {
    "citance_No": 1, 
    "citing_paper_id": "W08-0308", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and build 62ing a syntax-based word alignment model Mayand Knight (2007) with TTS templates", 
    "clean_text": "Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P10-1147", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "John, DeNero | Dan, Klein", 
    "raw_text": "Wetrained and combined two HMM alignment mod els (Ney and Vogel, 1996) using the BerkeleyAligner.7 We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-toword posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. Supervised Baseline: Block ITG", 
    "clean_text": "We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P09-1104", 
    "citing_paper_authority": 41, 
    "citing_paper_authors": "Aria, Haghighi | John, Blitzer | John, DeNero | Dan, Klein", 
    "raw_text": "When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. For likelihood based models, we set the L2 regularization parameter,? 2, to 100 and the threshold for posterior decoding to 0.33", 
    "clean_text": "When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P09-1104", 
    "citing_paper_authority": 41, 
    "citing_paper_authors": "Aria, Haghighi | John, Blitzer | John, DeNero | Dan, Klein", 
    "raw_text": "We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features", 
    "clean_text": "We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P09-1104", 
    "citing_paper_authority": 41, 
    "citing_paper_authors": "Aria, Haghighi | John, Blitzer | John, DeNero | Dan, Klein", 
    "raw_text": "thresholding (DeNero and Klein, 2007)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P10-1017", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Jason, Riesa | Daniel, Marcu", 
    "raw_text": "This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007)", 
    "clean_text": "This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P10-1017", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Jason, Riesa | Daniel, Marcu", 
    "raw_text": "And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments.DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance", 
    "clean_text": "DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D10-1052", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Hendra, Setiawan | Chris, Dyer | Philip, Resnik", 
    "raw_text": "This gap be tween alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007)", 
    "clean_text": "This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N10-1083", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Alexandre, Bouchard-C&ocirc;t&eacute; | John, DeNero | Dan, Klein", 
    "raw_text": "The final alignments, in both the base line and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006)", 
    "clean_text": "The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D09-1136", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments", 
    "clean_text": "DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D09-1136", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "The word alignment used in GHKM is usually computed independent of the syntactic structure, and as DeNero and Klein (2007) and May and Knight (2007) have noted, Ch-En En-Ch Union Heuristic 28.6% 33.0% 45.9% 20.1% Table 1: Percentage of corpus used to generate big templates, based on different word alignments 9-12 13-20? 21 Ch-En 18.2% 17.4% 64.4% En-Ch 15.9% 20.7% 63.4% Union 9.8% 15.1% 75.1% Heuristic 24.6% 27.9% 47.5% Table 2: In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N12-1040", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jeremy, Nicholson | Trevor, Cohn | Timothy, Baldwin", 
    "raw_text": "We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus", 
    "clean_text": "We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N12-1040", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jeremy, Nicholson | Trevor, Cohn | Timothy, Baldwin", 
    "raw_text": "The default implementation of the system involves two jointly trained HMMs (one for each source-target direction) over five iterations,2 with so-called competitive thresholding in the decoding step; these are more fully described in DeNero and Klein (2007) and Liang et al (2006) .Our approach examines morphological preprocessing of the Inuktitut training and test sets, with the idea of leveraging the morphological in formation into a corpus which is more amenable to alignment", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]
