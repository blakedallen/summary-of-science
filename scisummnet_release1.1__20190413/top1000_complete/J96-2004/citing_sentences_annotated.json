[
  {
    "citance_No": 1, 
    "citing_paper_id": "W96-0402", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Keith, Vander Linden | Barbara Di, Eugenio", 
    "raw_text": "The percentage agreement for each of the features is shown in the following table: feature percent agreement form 100% intentionality 74.9% awareness 93.5% safety 90.7% As advocated by Carletta (1996), we have used the Kappa coefficient (Siegel and Castellan, 1988) as a measure of coder agreement", 
    "clean_text": "As advocated by Carletta (1996), we have used the Kappa coefficient (Siegel and Castellan, 1988) as a measure of coder agreement.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1088", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Arne, Skj\u00c3\u00a6rholt", 
    "raw_text": "Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used", 
    "clean_text": "Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1088", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Arne, Skj\u00c3\u00a6rholt", 
    "raw_text": "(Cohen, 1960, introduced to computational linguistics by Carletta, 1996) and pi (Scott, 1955)", 
    "clean_text": "(Cohen, 1960, introduced to computational linguistics by Carletta, 1996) and pi (Scott, 1955).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W04-0216", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Annie, Zaenen | Jean, Carletta | Gregory, Garretson | Joan, Bresnan | Andrew, Koontz-Garboden | Tatiana, Nikitina | M. Catherine, O'Connor | Thomas, Wasow", 
    "raw_text": "The reliability of the annotation was evaluated using the kappa statistic (Carletta, 1996)", 
    "clean_text": "The reliability of the annotation was evaluated using the kappa statistic (Carletta, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P09-1101", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "T. Daniel, Midgley", 
    "raw_text": "(Carletta 1996) is another method of comparing inter-annotator agreement 0 30 60 90 120 150 1 2 3 4 5 6 7 8 9 10 11& gt; 11 120 25 10 32 3 4 3 1 2 0 17 2 Nu m be r o f a nn ot at or s Number of dialogues completed Figure 2", 
    "clean_text": "(Carletta 1996) is another method of comparing inter-annotator agreement.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D08-1021", 
    "citing_paper_authority": 44, 
    "citing_paper_authors": "Chris, Callison-Burch", 
    "raw_text": "We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common", 
    "clean_text": "We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P09-1095", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Isaac, Persing | Vincent, Ng", 
    "raw_text": "To measure inter annotator agreement, we compute Cohen? s Kappa (Carletta, 1996) from the two sets of annotations, obtaining a Kappa value of only 0.43", 
    "clean_text": "To measure inter-annotator agreement, we compute Cohen's Kappa (Carletta, 1996) from the two sets of annotations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W07-1707", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Dominik, Flejter | Karol, Wieloch | Witold, Abramowicz", 
    "raw_text": "Obtained percent agreement of 0.988 and? coefficient (Carletta, 1996) of 0.975suggest high convergence of both annotations", 
    "clean_text": "Obtained percent agreement of 0.988 and coefficient (Carletta, 1996) of 0.975 suggest high convergence of both annotations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "D10-1055", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Dominic, Espinosa | Rajakrishnan, Rajkumar | Michael, White | Shoshana, Berleant", 
    "raw_text": "is defined as P (A)? P (E) 1? P (E), where P (A) is the observed agreement between annotators and P (E) is the probability of agreement due to chance (Carletta, 1996)", 
    "clean_text": "P (A) is the observed agreement between annotators and P (E) is the probability of agreement due to chance (Carletta, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W07-2007", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Katja, Markert | Malvina, Nissim", 
    "raw_text": "Annotation was highly reliable with a kappa (Carletta, 1996) of 3https: //www.cia.gov/cia/publications/factbook/index.html 4Given that the task is not about standard Named Entity Recognition, we assume that the general semantic class of the name is already known", 
    "clean_text": "Annotation was highly reliable with a kappa (Carletta, 1996) of 3.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W03-2802", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Laurence, Devillers | H&eacute;l&egrave;ne, Maynard | Patrick, Paroubek | Sophie, Rosset", 
    "raw_text": "With the help of the kappa coefficient (Carletta, 1996) proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation", 
    "clean_text": "With the help of the kappa coefficient (Carletta, 1996) proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W04-2703", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Eleni, Miltsakaki | Aravind K., Joshi | Rashmi, Prasad | Bonnie Lynn, Webber", 
    "raw_text": "To test the reliability of the annotation, we first con side red the kappa statistic (Siegel and Castellan, 1988) which is used extensively in empirical studies of dis course (Carletta, 1996)", 
    "clean_text": "To test the reliability of the annotation, we first considered the kappa statistic (Siegel and Castellan, 1988) which is used extensively in empirical studies of dis course (Carletta, 1996).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N10-1113", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Florian, Schwarck | Alexander, Fraser | Hinrich, Sch&uuml;tze", 
    "raw_text": "= 0.77 on average (Carletta, 1996)) .Evaluation Measures", 
    "clean_text": "Inter-annotator agreement was sufficient (\u03ba = 0.77 on average (Carletta, 1996)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W04-0807", 
    "citing_paper_authority": 40, 
    "citing_paper_authors": "Rada, Mihalcea | Timothy, Chklovski | Adam, Kilgarriff", 
    "raw_text": "In addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance (Carletta, 1996), was also determined", 
    "clean_text": "In addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance (Carletta, 1996), was also determined.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W06-1318", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Jeroen, Geertzen | Harry C., Bunt", 
    "raw_text": "Following the suggestions in (Carletta, 1996), Coreetal", 
    "clean_text": "Following the suggestions in (Carletta, 1996), Core et al. consider kappa scores above 0.67 to indicate significant agreement and scores above 0.8 reliable agreement.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W11-2035", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Lin, Chen | Anruo, Wang | Barbara Di, Eugenio", 
    "raw_text": "Subjects Tasks Utterances Gestures Pronouns 12 114 1920 896 1635 Table 2: Annotated Corpus Size In order to test the reliability of our annotation, we double coded about 18% of the data, namely 21 sub-dialogues comprising 213 pronouns, on which we computed the Kappa coefficient (Carletta, 1996)", 
    "clean_text": "In order to test the reliability of our annotation, we double coded about 18% of the data, namely 21 sub-dialogues comprising 213 pronouns, on which we computed the Kappa coefficient (Carletta, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W03-2115", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Robert, Porzel | Iryna, Gurevych | Christof E., M&uuml;ller", 
    "raw_text": "Themost important criteria was how well the SRH captures the intentional content of the user? s utterance.2As reported elsewhere the resulting Kappa statistics (Carletta, 1996) over the annotated data yields?= 0.7, which indicates that human annotators can reliably distinguish between coherent samples and incoherent ones. If none of the SRHs captured the user? s intention adequately, the decision had to be made by looking at the actual word error rate", 
    "clean_text": "As reported elsewhere the resulting Kappa statistics (Carletta, 1996) over the annotated data yields 0.7.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P10-1142", 
    "citing_paper_authority": 27, 
    "citing_paper_authors": "Vincent, Ng", 
    "raw_text": "Since co reference is a clustering task, any general-purpose method for evaluating a response partition against a key partition (e.g., Kappa (Carletta, 1996)) can be used for co reference scor 1403ing (see Popescu-Belis et al (2004))", 
    "clean_text": "Since co-reference is a clustering task, any general-purpose method for evaluating a response partition against a key partition (e.g., Kappa (Carletta, 1996)) can be used for coreference scoring (see Popescu-Belis et al (2004)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "C04-1034", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Costanza, Navarretta", 
    "raw_text": "Thereliability for the two annotation tasks (? -statistics (Carletta, 1996)) was of 0.94 and 0.90 respectively", 
    "clean_text": "The reliability for the two annotation tasks (statistics (Carletta, 1996)) was of 0.94 and 0.90 respectively.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "S12-1048", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "parisa, kordjamshidi | Steven, Bethard | Marie-Francine, Moens", 
    "raw_text": "From the first effort an inter-annotator agreement (Carletta, 1996) of 0.89 for Cohen? s kappa was obtained", 
    "clean_text": "From the first effort an inter-annotator agreement (Carletta, 1996) of 0.89 for Cohen's kappa was obtained.", 
    "keep_for_gold": 0
  }
]
