Further Meta-Evaluation of Machine Translation
This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish.
We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort.
We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level.
We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information.
Thus, the human an notation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make.
Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns.
