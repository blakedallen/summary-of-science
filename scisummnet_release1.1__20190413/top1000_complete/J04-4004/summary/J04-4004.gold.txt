Intricacies Of Collins Parsing Model
This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results.
Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model.
We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser.
We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought.
Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech.
The results suggest that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words.
We show that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance.
