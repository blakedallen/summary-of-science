<PAPER>
  <S sid="0">An Algorithm For Finding Noun Phrase Correspondences In Bilingual Corpora</S>
  <ABSTRACT>
    <S sid="1" ssid="1">The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus.</S>
    <S sid="2" ssid="2">The taggers provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages.</S>
    <S sid="3" ssid="3">Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers.</S>
    <S sid="4" ssid="4">The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated.</S>
    <S sid="5" ssid="5">Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings.</S>
  </ABSTRACT>
  <SECTION title="INTRODUCTION" number="1">
    <S sid="6" ssid="1">Areas of investigation using bilingual corpora have included the following: The work described here makes use of the aligned Canadian Hansards [Gale and Church, 1991b] to obtain noun phrase correspondences between the English and French text.</S>
    <S sid="7" ssid="2">The term &amp;quot;correspondence&amp;quot; is used here to signify a mapping between words in two aligned sentences.</S>
    <S sid="8" ssid="3">Consider an English sentence Ei and a French sentence Fi which are assumed to be approximate translations of each other.</S>
    <S sid="9" ssid="4">The subscript i denotes the i'th alignment of sentences in both languages.</S>
    <S sid="10" ssid="5">A word sequence in Ei is defined here as the correspondence of another sequence in Fi if the words of one sequence are considered to represent the words in the other.</S>
    <S sid="11" ssid="6">Single word correspondences have been investigated [Gale and Church, 1991a] using a statistic operating on contingency tables.</S>
    <S sid="12" ssid="7">An algorithm for producing collocational correspondences has also been described [Smadja, 1992].</S>
    <S sid="13" ssid="8">The algorithm involves several steps.</S>
    <S sid="14" ssid="9">English collocations are first extracted from the English side of the corpus.</S>
    <S sid="15" ssid="10">Instances of the English collocation are found and the mutual information is calculated between the instances and various single word candidates in aligned French sentences.</S>
    <S sid="16" ssid="11">The highest ranking candidates are then extended by another word and the procedure is repeated until a corresponding French collocation having the highest mutual information is found.</S>
    <S sid="17" ssid="12">An alternative approach is described here, which employs simple iterative re-estimation.</S>
    <S sid="18" ssid="13">It is used to make correspondences between simple noun phrases that have been isolated in corresponding sentences of each language using finitestate recognizers.</S>
    <S sid="19" ssid="14">The algorithm is applicable for finding single or multiple word correspondences and can accommodate additional kinds of phrases.</S>
    <S sid="20" ssid="15">In contrast to the other methods that have been mentioned, the algorithm can be extended in a straightforward way to enable correct correspondences to be made in circumstances where numerous low frequency phrases are involved.</S>
    <S sid="21" ssid="16">This is important consideration because in large text corpora roughly a third of the word types only occur once.</S>
    <S sid="22" ssid="17">Several applications for bilingual correspondence information have been suggested.</S>
    <S sid="23" ssid="18">They can be used in bilingual concordances, for automatically constructing bilingual lexicons, and probabilistically quantified correspondences may be useful for statistical translation methods.</S>
  </SECTION>
  <SECTION title="COMPONENTS" number="2">
    <S sid="24" ssid="1">Figure 1 illustrates how the corpus is analyzed.</S>
    <S sid="25" ssid="2">The words in sentences are first tagged with their corresponding part-of-speech categories.</S>
    <S sid="26" ssid="3">Each tagger contains a hidden Markov model (HMM), which is trained using samples of raw text from the Hansards for each language.</S>
    <S sid="27" ssid="4">The taggers are robust and operate with a low error rate [Kupiec, 19921.</S>
    <S sid="28" ssid="5">Simple noun phrases (excluding pronouns and digits) are then extracted from the sentences by finite-state recognizers that are specified by regular expressions defined in terms of part-ofspeech categories.</S>
    <S sid="29" ssid="6">Simple noun phrases are identified because they are most reliably recognized; it is also assumed that they can be identified unambiguously.</S>
    <S sid="30" ssid="7">The only embedding that is allowed is by prepositional phrases involving &amp;quot;of' in English and &amp;quot;de&amp;quot; in French, as noun phrases involving them can be identified with relatively low error (revisions to this restriction are considered later).</S>
    <S sid="31" ssid="8">Noun phrases are placed in an index to associate a unique identifier with each one.</S>
    <S sid="32" ssid="9">A noun phrase is defined by its word sequence, excluding any leading determiners.</S>
    <S sid="33" ssid="10">Singular and plural forms of common nouns are thus distinct and assigned different positions in the index.</S>
    <S sid="34" ssid="11">For each sentence corresponding to an alignment, the index positions of all noun phrases in the sentence are recorded in a separate data structure, providing a compact representation of the corpus.</S>
    <S sid="35" ssid="12">So far it has been assumed (for the sake of simplicity) that there is always a one-to-one mapping between English and French sentences.</S>
    <S sid="36" ssid="13">In practice, if an alignment program produces blocks of several sentences in one or both languages, this can be accommodated by treating the block instead as a single bigger &amp;quot;compound sentence&amp;quot; in which noun phrases have a higher number of possible correspondences.</S>
  </SECTION>
  <SECTION title="THE MAPPING ALGORITHM" number="3">
    <S sid="37" ssid="1">Some terminology is necessary to describe the algorithm concisely.</S>
    <S sid="38" ssid="2">Let there be L total alignments in the corpus; then Ei is the English sentence for alignment i.</S>
    <S sid="39" ssid="3">Let the function 0(E2) be the number of noun phrases identified in the sentence.</S>
    <S sid="40" ssid="4">If there are k of them, k = cb(Ei), and they can be referenced by j = 1...k. Considering the j'th noun phrase in sentence Ei, the function p(Ei, j) produces an identifier for the phrase, which is the position of the phrase in the English index.</S>
    <S sid="41" ssid="5">If this phrase is at position s, then p(Ei, j) = s. In turn, the French sentence Fi will contain 0(Fi) noun phrases and given the p'th one, its position in the French index will be given by p(Fi,p).</S>
    <S sid="42" ssid="6">It will also be assumed that there are a total of VE and VE phrases in the English and French indexes respectively.</S>
    <S sid="43" ssid="7">Finally, the indicator function /0 has the value unity if its argument is true, and zero otherwise.</S>
    <S sid="44" ssid="8">Assuming these definitions, the algorithm is stated in Figure 2.</S>
    <S sid="45" ssid="9">The equations assume a directionality: finding French &amp;quot;target&amp;quot; correspondences for English &amp;quot;source&amp;quot; phrases.</S>
    <S sid="46" ssid="10">The algorithm is reversible, by swapping E with F. The model for correspondence is that a source noun phrase in Ei is responsible for producing the various different target noun phrases in Fi with correspondingly different probabilities.</S>
    <S sid="47" ssid="11">Two quantities are calculated; C,.</S>
    <S sid="48" ssid="12">(s, t) and Pr(s,t).</S>
    <S sid="49" ssid="13">Computation proceeds by evaluating Equation (1), Equation (2) and then iteratively applying Equations (3) and (2); r increasing with each successive iteration.</S>
    <S sid="50" ssid="14">The argument s refers to the English noun phrase npE(s) having position s in the English index, and the argument t refers to the French noun phrase npF(t) at position t in the French index.</S>
    <S sid="51" ssid="15">Equation (1) assumes that each English noun phrase in Ei is initially equally likely to correspond to each French noun phrase in Fi.</S>
    <S sid="52" ssid="16">All correspondences are thus equally weighted, reflecting a state of ignorance.</S>
    <S sid="53" ssid="17">Weights are summed over the corpus, so noun phrases that co-occur in several sentences will have larger sums.</S>
    <S sid="54" ssid="18">The weights Co(s, t) can be interpreted as the mean number of times that npF(t) corresponds to npE(s) given the corpus and the initial assumption of equiprobable correspondences.</S>
    <S sid="55" ssid="19">These weights can be used to form a new estimate of the probability that npF(t) corresponds to npE(s), by considering the mean number of times npF(t) corresponds to npE(s) as a fraction of the total mean number of correspondences for npE(s), as in Equation (2).</S>
    <S sid="56" ssid="20">The procedure is then iterated using Equations (3), and (2) to obtain successively refined, convergent estimates of the probability that npF(t) corresponds to npE(s).</S>
    <S sid="57" ssid="21">The probability of correspondences can be used as a method of ranking them (occurrence counts can be taken into account as an indication of the reliability of a correspondence).</S>
    <S sid="58" ssid="22">Although Figure 2 defines the coefficients simply, the algorithm is not implemented literally from it.</S>
    <S sid="59" ssid="23">The algorithm employs a compact representation of the correspondences for efficient operation.</S>
    <S sid="60" ssid="24">An arbitrarily large corpus can be accommodated by segmenting it appropriately.</S>
    <S sid="61" ssid="25">The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm [Dempster et al., 1977].</S>
    <S sid="62" ssid="26">In contrast to reservations that have been expressed [Gale and Church, 1991a] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness.</S>
    <S sid="63" ssid="27">Unlike the other methods that have been mentioned, the approach has the capability to accommodate more context to improve performance.</S>
  </SECTION>
  <SECTION title="RESULTS" number="4">
    <S sid="64" ssid="1">A sample of the aligned corpus comprising 2,600 alignments was used for testing the algorithm (not all of the alignments contained sentences).</S>
    <S sid="65" ssid="2">4,900 distinct English noun phrases and 5,100 distinct French noun phrases were extracted from the sample.</S>
    <S sid="66" ssid="3">When forming correspondences involving long sentences with many clauses, it was observed that the position at which a noun phrase occurred in Ei was very roughly proportional to the corresponding noun phrase in F. .</S>
    <S sid="67" ssid="4">In such cases it was not necessary to form correspondences with all noun phrases in Fi for each noun phrase in E. Instead, the location of a phrase in Ei was mapped linearly to a position in Fi and correspondences were formed for noun phrases occurring in a window around that position.</S>
    <S sid="68" ssid="5">This resulted in a total of 34,000 correspondences.</S>
    <S sid="69" ssid="6">The mappings are stable within a few (2-4) iterations.</S>
    <S sid="70" ssid="7">In discussing results, a selection of examples will be presented that demonstrates the strengths and weaknesses of the algorithm.</S>
    <S sid="71" ssid="8">To give an indication of noun phrase frequency counts in the sample, the highest ranking correspondences are shown in Table 1.</S>
    <S sid="72" ssid="9">The figures in columns (1) and (3) indicate the number of instances of the noun phrase to their right.</S>
    <S sid="73" ssid="10">To give an informal impression of overall performance, the hundred highest ranking correspondences were inspected and of these, ninety were completely correct.</S>
    <S sid="74" ssid="11">Less frequently occurring noun phrases are also of interest for purposes of evaluation; some of these are shown in Table 2.</S>
    <S sid="75" ssid="12">The table also illustrates an unembedded English noun phrase having multiple prepositional phrases in its French correspondent.</S>
    <S sid="76" ssid="13">Organizational acronyms (which may be not be available in general-purpose dictionaries) are also extracted, as the taggers are robust.</S>
    <S sid="77" ssid="14">Even when a noun phrase only occurs once, a correct correspondence can be found if there are only single noun phrases in each sentence of the alignment.</S>
    <S sid="78" ssid="15">This is demonstrated in the last row of Table 2, which is the result of the following alignment: Ei: &amp;quot;The whole issue of free trade has been mentioned.&amp;quot; &amp;quot;On a mentionne la question du libreechange.&amp;quot; Table 3 shows some incorrect correspondences produced by the algorithm (in the table, &amp;quot;usine&amp;quot; means &amp;quot;factory&amp;quot;).</S>
    <S sid="79" ssid="16">The sentences that are responsible for these correspondences illustrate some of the problems associated with the correspondence model: Ei: &amp;quot;They use what is known as the dual system in which there is a mix of on-the-job and offthe-job training.&amp;quot; Fi: &amp;quot;Ils ont recours a une formation mixte, partie en usine et partie hors usine.&amp;quot; The first problem is that the conjunctive modifiers in the English sentence cannot be accommodated by the noun phrase recognizer.</S>
    <S sid="80" ssid="17">The tagger also assigned &amp;quot;on-the-job&amp;quot; as a noun when adjectival use would be preferred.</S>
    <S sid="81" ssid="18">If verb correspondences were included, there is a mismatch between the three that exist in the English sentence and the single one in the French.</S>
    <S sid="82" ssid="19">If the English were to reflect the French for the correspondence model to be appropriate, the noun phrases would perhaps be &amp;quot;part in the factory&amp;quot; and &amp;quot;part out of the factory&amp;quot;.</S>
    <S sid="83" ssid="20">Considered as a translation, this is lame.</S>
    <S sid="84" ssid="21">The majority of errors that occur are not the result of incorrect tagging or noun phrase recognition, but are the result of the approximate nature of the correspondence model.</S>
    <S sid="85" ssid="22">The correspondences in Table 4 are likewise flawed (in the table, &amp;quot;souris&amp;quot; means &amp;quot;mouse&amp;quot; and &amp;quot;tigre de papier&amp;quot; means &amp;quot;paper tiger&amp;quot;): These correspondences are the result of the following sentences: Ei: &amp;quot;It is a roaring rabbit, a toothless tiger.&amp;quot; Fi: &amp;quot;C' est un tigre de papier, un souris qui rugit.&amp;quot; In the case of the alliterative English phrase &amp;quot;roaring rabbit&amp;quot;, the (presumably) rhetorical aspect is preserved as a rhyme in &amp;quot;souris qui rugit&amp;quot;; the result being that &amp;quot;rabbit&amp;quot; corresponds to &amp;quot;souris&amp;quot; (mouse).</S>
    <S sid="86" ssid="23">Here again, even if the best correspondence were made the result would be wrong because of the relatively sophisticated considerations involved in the translation.</S>
  </SECTION>
  <SECTION title="EXTENSIONS" number="5">
    <S sid="87" ssid="1">As regards future possibilities, the algorithm lends itself to a range of improvements and applications, which are outlined next.</S>
    <S sid="88" ssid="2">Finding Word Correspondences: The algorithm finds corresponding noun phrases but provides no information about word-level correspondences within them.</S>
    <S sid="89" ssid="3">One possibility is simply to eliminate the tagger and noun phrase recognizer (treating all words as individual phrases of length unity and having a larger number of correspondences).</S>
    <S sid="90" ssid="4">Alternatively, the following strategy can be adopted, which involves fewer total correspondences.</S>
    <S sid="91" ssid="5">First, the algorithm is used to build noun phrase correspondences, then the phrase pairs that are produced are themselves treated as a bilingual noun phrase corpus.</S>
    <S sid="92" ssid="6">The algorithm is then employed again on this corpus, treating all words as individual phrases.</S>
    <S sid="93" ssid="7">This results in a set of single word correspondences for the internal words in noun phrases.</S>
    <S sid="94" ssid="8">Reducing Ambiguity: The basic algorithm assumes that noun phrases can be uniquely identified in both languages, which is only true for simple noun phrases.</S>
    <S sid="95" ssid="9">The problem of prepositional phrase attachment is exemplified by the following correspondences: The correct English and French noun phrases are &amp;quot;Secretary of State for External Affairs&amp;quot; and &amp;quot;secretaire d' Etat aux Affaires exterieures&amp;quot;.</S>
    <S sid="96" ssid="10">If prepositional phrases involving &amp;quot;for&amp;quot; and &amp;quot;a&amp;quot; were also permitted, these phrases would be correctly identified; however many other adverbial prepositional phrases would also be incorrectly attached to noun phrases.</S>
    <S sid="97" ssid="11">If all embedded prepositional phrases were permitted by the noun phrase recognizer, the algorithm could be used to reduce the degree of ambiguity between alternatives.</S>
    <S sid="98" ssid="12">Consider a sequence npePPe of an unembedded English noun phrase npe followed by a prepositional phrase ppe, and likewise a corresponding French sequence npf ppf.</S>
    <S sid="99" ssid="13">Possible interpretations of this are: 1.</S>
    <S sid="100" ssid="14">The prepositional phrase attaches to the noun phrase in both languages.</S>
    <S sid="101" ssid="15">2.</S>
    <S sid="102" ssid="16">The prepositional phrase attaches to the noun phrase in one language and does not in the other.</S>
    <S sid="103" ssid="17">3.</S>
    <S sid="104" ssid="18">The prepositional phrase does not attach to the noun phrase in either language.</S>
    <S sid="105" ssid="19">If the prepositional phrases attach to the noun phrases in both languages, they are likely to be repeated in most instances of the noun phrase; it is less likely that the same prepositional phrase will be used adverbially with each instance of the noun phrase.</S>
    <S sid="106" ssid="20">This provides a heuristic method for reducing ambiguity in noun phrases that occur several times.</S>
    <S sid="107" ssid="21">The only modifications required to the algorithm are that the additional possible noun phrases and correspondences between them must be included.</S>
    <S sid="108" ssid="22">Given thresholds on the number of occurrences and the probability of the correspondence, the most likely correspondence can be predicted.</S>
    <S sid="109" ssid="23">Including Context: In the algorithm, correspondences between source and target noun phrases are considered irrespectively of other correspondences in an alignment.</S>
    <S sid="110" ssid="24">This does not make the best use of the information available, and can be improved upon.</S>
    <S sid="111" ssid="25">For example, consider the following alignment: &amp;quot;The Bill was introduced just before Christmas.&amp;quot; Fi: &amp;quot;Le projet de loi a ete presente juste avant le conge des Fetes.&amp;quot; Here it is assumed that there are many instances of the correspondence &amp;quot;Bill&amp;quot; and &amp;quot;projet de loi&amp;quot;, but only one instance of &amp;quot;Christmas&amp;quot; and &amp;quot;conge des Fetes&amp;quot;.</S>
    <S sid="112" ssid="26">This suggests that &amp;quot;Bill&amp;quot; corresponds to &amp;quot;projet de loi&amp;quot; with a high probability and that &amp;quot;Christmas&amp;quot; likewise corresponds strongly to &amp;quot;conge des Fetes&amp;quot;.</S>
    <S sid="113" ssid="27">However, the model will assert that &amp;quot;Christmas&amp;quot; corresponds to &amp;quot;projet de loi&amp;quot; and to &amp;quot;conge des Fetes&amp;quot; with equal probability, no matter how likely the correspondence between &amp;quot;Bill&amp;quot; and &amp;quot;projet de loi&amp;quot;.</S>
    <S sid="114" ssid="28">The model can be refined to reflect this situation by considering the joint probability that a target npF(t) corresponds to a source npE(s) and all the other possible correspondences in the alignment are produced.</S>
    <S sid="115" ssid="29">This situation is very similar to that involved in training HMM text taggers, where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech, and the rest of the words in the sentence are also generated (e.g.</S>
    <S sid="116" ssid="30">[Cutting et al., 1992]).</S>
  </SECTION>
  <SECTION title="CONCLUSION" number="6">
    <S sid="117" ssid="1">The algorithm described in this paper provides a practical means for obtaining correspondences between noun phrases in a bilingual corpus.</S>
    <S sid="118" ssid="2">Linguistic structure is used in the form of noun phrase recognizers to select phrases for a stochastic model which serves as a means of minimizing errors due to the approximations inherent in the correspondence model.</S>
    <S sid="119" ssid="3">The algorithm is robust, and extensible in several ways.</S>
  </SECTION>
</PAPER>
