Parsing The WSJ Using CCG And Log-Linear Models
This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar (CCG).
A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation.
We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies.
We compare models which use all CCG derivations, including non-standard derivations, with normal-form models.
The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers.
Our CCG parser is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second.
Our parsing peformance relies on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word).
Our parsing performance provides an indication of how super tagging accuracy corresponds to overall dependency recovery.
