<PAPER>
  <S sid="0">Learning Dependency Translation Models As Collections Of Finite-State Head Transducers</S>
  <ABSTRACT>
    <S sid="1" ssid="1">The paper defines weighted head transducers, finite-state machines that perform middle-out string transduction.</S>
    <S sid="2" ssid="2">These transducers are strictly more expressive than the special case of standard leftto-right finite-state transducers.</S>
    <S sid="3" ssid="3">Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically.</S>
    <S sid="4" ssid="4">A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model.</S>
    <S sid="5" ssid="5">A method for automatically training a dependency transduction model from a set of input-output example strings is presented.</S>
    <S sid="6" ssid="6">The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments.</S>
    <S sid="7" ssid="7">Experimental results are given for applying the training method to translation from English to Spanish and Japanese.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="8" ssid="1">The paper defines weighted head transducers, finite-state machines that perform middle-out string transduction.</S>
    <S sid="9" ssid="2">These transducers are strictly more expressive than the special case of standard leftto-right finite-state transducers.</S>
    <S sid="10" ssid="3">Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically.</S>
    <S sid="11" ssid="4">A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model.</S>
    <S sid="12" ssid="5">A method for automatically training a dependency transduction model from a set of input-output example strings is presented.</S>
    <S sid="13" ssid="6">The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments.</S>
    <S sid="14" ssid="7">Experimental results are given for applying the training method to translation from English to Spanish and Japanese.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="15" ssid="1">We will define a dependency transduction model in terms of a collection of weighted head transducers.</S>
    <S sid="16" ssid="2">Each head transducer is a finite-state machine that differs from &amp;quot;standard&amp;quot; finite-state transducers in that, instead of consuming the input string left to right, it consumes it &amp;quot;middle out&amp;quot; from a symbol in the string.</S>
    <S sid="17" ssid="3">Similarly, the output of a head transducer is built up middle out at positions relative to a symbol in the output string.</S>
    <S sid="18" ssid="4">The resulting finite-state machines are more expressive than standard left-to-right transducers.</S>
    <S sid="19" ssid="5">In particular, they allow long-distance movement with fewer states than a traditional finite-state transducer, a useful property for the translation task to which we apply them in this paper.</S>
    <S sid="20" ssid="6">(In fact, finite-state head transducers are capable of unbounded movement with a finite number of states.)</S>
    <S sid="21" ssid="7">In Section 2, we introduce head transducers and explain how input-output positions on state transitions result in middle-out transduction.</S>
    <S sid="22" ssid="8">When applied to the problem of translation, the head transducers forming the dependency transduction model operate on input and output strings that are sequences of dependents of corresponding headwords in the source and target languages.</S>
    <S sid="23" ssid="9">The dependency transduction model produces synchronized dependency trees in which each local tree is produced by a head transducer.</S>
    <S sid="24" ssid="10">In other words, the dependency model applies the head transducers recursively, imposing a recursive decomposition of the source and target strings.</S>
    <S sid="25" ssid="11">A dynamic programming search algorithm finds optimal (lowest total weight) derivations of target strings from input strings or word lattices produced by a speech recognizer.</S>
    <S sid="26" ssid="12">Section 3 defines dependency transduction models and describes the search algorithm.</S>
    <S sid="27" ssid="13">We construct the dependency transduction models for translation automatically from a set of unannotated examples, each example comprising a source string and a corresponding target string.</S>
    <S sid="28" ssid="14">The recursive decomposition of the training examples results from an algorithm for computing hierarchical alignments of the examples, described in Section 4.2.</S>
    <S sid="29" ssid="15">This alignment algorithm uses dynamic programming search guided by source-target word correlation statistics as described in Section 4.1.</S>
    <S sid="30" ssid="16">Having constructed a hierarchical alignment for the training examples, a set of head transducer transitions are constructed from each example as described in Section 4.3.</S>
    <S sid="31" ssid="17">Finally, the dependency transduction model is constructed by aggregating the resulting head transducers and assigning transition weights, which are log probabilities computed from the training counts by simple maximum likelihood estimation.</S>
    <S sid="32" ssid="18">We have applied this method of training statistical dependency transduction models in experiments on English-to-Spanish and English-to-Japanese translations of transcribed spoken utterances.</S>
    <S sid="33" ssid="19">The results of these experiments are described in Section 5; our concluding remarks are in Section 6.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="34" ssid="1">In this section we describe the basic structure and operation of a weighted head transducer.</S>
    <S sid="35" ssid="2">In some respects, this description is simpler than earlier presentations (e.g., Alshawi 1996); for example, here final states are simply a subset of the transducer states whereas in other work we have described the more general case in which final states are specified by a probability distribution.</S>
    <S sid="36" ssid="3">The simplified description is adequate for the purposes of this paper.</S>
    <S sid="37" ssid="4">Formally, a weighted head transducer is a 5-tuple: an alphabet W of input symbols; an alphabet V of output symbols; a finite set Q of states go, &#8226; &#8226; &#8226; , qs; a set of final states F C Q; and a finite set T of state transitions.</S>
    <S sid="38" ssid="5">A transition from state q to state q' has the form (q, q' ,w, v, a, 0, c) where w is a member of W or is the empty string e; v is a member of V or 6; the integer a is the input position; the integer 0 is the output position; and the real number c is the weight or cost of the transition.</S>
    <S sid="39" ssid="6">A transition in which a = 0 and f3 = 0 is called a head transition.</S>
    <S sid="40" ssid="7">The interpretation of q, q', w, and v in transitions is similar to left-to-right transducers, i.e., in transitioning from state q to state q', the transducer &amp;quot;reads&amp;quot; input symbol w and &amp;quot;writes&amp;quot; output symbol v, and as usual if w (or v) is &#8364; then no read (respectively write) takes place for the transition.</S>
    <S sid="41" ssid="8">The difference lies in the interpretation of the read position a and the write position 0.</S>
    <S sid="42" ssid="9">To interpret the transition positions as transducer actions, we consider notional input and output tapes divided into squares.</S>
    <S sid="43" ssid="10">On such a tape, one square is numbered 0, and the other squares are numbered 1, 2, ... rightwards from square 0, and &#8212;1, &#8212;2, leftwards from square 0 (Figure 1).</S>
    <S sid="44" ssid="11">A transition with input position a and output position 0 is interpreted as reading w from square a on the input tape and writing v to square 0 of the output tape; if square 0 is already occupied, then v is written to the next empty square to the left of 0 if &lt; 0, or to the right of 3 if )3 &gt; 0, and similarly, if input was already read from position a, w is taken from the next unread square to the left of a if a &lt; 0 or to the right of a if a &gt; 0.</S>
    <S sid="45" ssid="12">The operation of a head transducer is nondeterministic.</S>
    <S sid="46" ssid="13">It starts by taking a head transition Kg, 9', wo, vo, 0, 0, c) where wo is one of the symbols (not necessarily the leftmost) in the input string.</S>
    <S sid="47" ssid="14">(The valid initial states are therefore implicitly defined as those with an outgoing head transition.) wo is considered to be at square 0 of the input tape and vo is output at square 0 of the output tape.</S>
    <S sid="48" ssid="15">Further state transitions may then be taken until a final state in F is reached.</S>
    <S sid="49" ssid="16">For a derivation to be valid, it must read each symbol in the input string exactly once.</S>
    <S sid="50" ssid="17">At the end of a derivation, the output string is formed by taking the sequence of symbols on the target tape, ignoring any empty squares on this tape.</S>
    <S sid="51" ssid="18">The cost of a derivation of an input string to an output string by a weighted head transducer is the sum of the costs of transitions taken in the derivation.</S>
    <S sid="52" ssid="19">We can now define the string-to-string transduction function for a head transducer to be the function that maps an input string to the output string produced by the lowest-cost valid derivation taken over all initial states and initial symbols.</S>
    <S sid="53" ssid="20">(Formally, the function is partial in that it is not defined on an input when there are no derivations or when there are multiple outputs with the same minimal cost.)</S>
    <S sid="54" ssid="21">In the transducers produced by the training method described in this paper, the source and target positions are in the set {-1, 0,1}, though we have also used handcoded transducers (Alshawi and Xia 1997) and automatically trained transducers (Alshawi and Douglas 2000) with a larger range of positions.</S>
    <S sid="55" ssid="22">The operation of a traditional left-to-right transducer can be simulated by a head transducer by starting at the leftmost input symbol and setting the positions of the first transition taken to a = 0 and /3 = 0, and the positions for subsequent transitions to a = 1 and )3 = 1.</S>
    <S sid="56" ssid="23">However, we can illustrate the fact that head transducers are more Head transducer to reverse an input string of arbitrary length in the alphabet {a, b}. expressive than left-to-right transducers by the case of a finite-state head transducer that reverses a string of arbitrary length.</S>
    <S sid="57" ssid="24">(This cannot be performed by a traditional transducer with a finite number of states.)</S>
    <S sid="58" ssid="25">For example, the head transducer described below (and shown in Figure 2) with input alphabet {a, b} will reverse an input string of arbitrary length in that alphabet.</S>
    <S sid="59" ssid="26">The states of the example transducer are Q = {qi, q2} and F .--- {q2}, and it has the following transitions (costs are ignored here): The only possible complete derivations of the transducer read the input string right to left, but write it left to right, thus reversing the string.</S>
    <S sid="60" ssid="27">Another similar example is using a finite-state head transducer to convert a palindrome of arbitrary length into one of its component halves.</S>
    <S sid="61" ssid="28">This clearly requires the use of an empty string on some of the output transitions.</S>
  </SECTION>
  <SECTION title="3." number="4">
    <S sid="62" ssid="1">In this section we describe dependency transduction models, which can be used for machine translation and other transduction tasks.</S>
    <S sid="63" ssid="2">These models consist of a collection of head transducers that are applied hierarchically.</S>
    <S sid="64" ssid="3">Applying the machines hierarchically means that a nonhead transition is interpreted not simply as reading an inputoutput pair (w, v), but instead as reading and writing a pair of strings headed by (w, v) according to the derivation of a subnetwork.</S>
    <S sid="65" ssid="4">For example, the head transducer shown in Figure 3 can be applied recursively in order to convert an arithmetic expression from infix to prefix (Polish) notation (as noted by Lewis and Stearns [1968], this transduction cannot be performed by a pushdown transducer).</S>
    <S sid="66" ssid="5">In the case of machine translation, the transducers derive pairs of dependency trees, a source language dependency tree and a target dependency tree.</S>
    <S sid="67" ssid="6">A dependency tree for a sentence, in the sense of dependency grammar (for example Hays [1964] and Hudson [1984]), is a tree in which the words of the sentence appear as nodes (we do not have terminal symbols of the kind used in phrase structure grammar).</S>
    <S sid="68" ssid="7">In such a tree, the parent of a node is its head and the child of a node is the node's dependent.</S>
    <S sid="69" ssid="8">The source and target dependency trees derived by a dependency transduction model are ordered, i.e., there is an ordering on the nodes of each local tree.</S>
    <S sid="70" ssid="9">This Synchronized dependency trees derived for transducing I want to make a collect call into quiero hacer una llamada de cobrar. means, in particular, that the target sentence can be constructed directly by a simple recursive traversal of the target dependency tree.</S>
    <S sid="71" ssid="10">Each pair of source and target trees generated is synchronized in the sense to be formalized in Section 4.2.</S>
    <S sid="72" ssid="11">An example is given in Figure 4.</S>
    <S sid="73" ssid="12">Head transducers and dependency transduction models are thus related as follows: Each pair of local trees produced by a dependency transduction derivation is the result of a head transducer derivation.</S>
    <S sid="74" ssid="13">Specifically, the input to such a head transducer is the string corresponding to the flattened local source dependency tree.</S>
    <S sid="75" ssid="14">Similarly, the output of the head transducer derivation is the string corresponding to the flattened local target dependency tree.</S>
    <S sid="76" ssid="15">In other words, the head transducer is used to convert a sequence consisting of a headword w and its left and right dependent words to a sequence consisting of a target word v and its left and right dependent words (Figure 5).</S>
    <S sid="77" ssid="16">Since the empty string may appear in a transition in place of a source or target symbol, the number of source and target dependents can be different.</S>
    <S sid="78" ssid="17">The cost of a derivation produced by a dependency transduction model is the sum of all the weights of the head transducer derivations involved.</S>
    <S sid="79" ssid="18">When applying a dependency transduction model to language translation, we choose the target string obtained by flattening the target tree of the lowest-cost dependency derivation that also generates the source string.</S>
    <S sid="80" ssid="19">We have not yet indicated what weights to use for head transducer transitions.</S>
    <S sid="81" ssid="20">The definition of head transducers as such does not constrain these.</S>
    <S sid="82" ssid="21">However, for a dependency transduction model to be a statistical model for generating pairs of strings, we assign transition weights that are derived from conditional probabilities.</S>
    <S sid="83" ssid="22">Several Head transducer converts the sequences of left and right dependents (wi wk_1) and (wk+i w,i) of w into left and right dependents (vi vj_i) and (v,&#177;1 vp) of V. probabilistic parameterizations can be used for this purpose including the following for a transition with headwords w and v and dependent words w' and v': P(qcw' , , a, 131w,v,q).</S>
    <S sid="84" ssid="23">Here q and q' are the from-state and to-state for the transition and a and 13 are the source and target positions, as before.</S>
    <S sid="85" ssid="24">We also need parameters P(q0, 911w v) for the probability of choosing a head transition given this pair of headwords.</S>
    <S sid="86" ssid="25">To start the derivation, we need parameters P(roots(wo, vo)) for the probability of choosing wo,vo as the root nodes of the two trees.</S>
    <S sid="87" ssid="26">These model parameters can be used to generate pairs of synchronized dependency trees starting with the topmost nodes of the two trees and proceeding recursively to the leaves.</S>
    <S sid="88" ssid="27">The probability of such a derivation can be expressed as: for a derivation in which the dependents of w and v are generated by n transitions.</S>
    <S sid="89" ssid="28">To carry out translation with a dependency transduction model, we apply a dynamic programming search to find the optimal derivation.</S>
    <S sid="90" ssid="29">This algorithm can take as input either word strings, or word lattices produced by a speech recognizer.</S>
    <S sid="91" ssid="30">The algorithm is similar to those for context-free parsing such as chart parsing (Earley 1970) and the CKY algorithm (Younger 1967).</S>
    <S sid="92" ssid="31">Since word string input is a special case of word lattice input, we need only describe the case of lattices.</S>
    <S sid="93" ssid="32">We now present a sketch of the transduction algorithm.</S>
    <S sid="94" ssid="33">The algorithm works bottom-up, maintaining a set of configurations.</S>
    <S sid="95" ssid="34">A configuration has the form [fli, n2, w, v, q, c, t] corresponding to a bottom-up partial derivation currently in state q covering an input sequence between nodes n1 and n2 of the input lattice. w and v are the topmost Alshawi, Bangalore, and Douglas Learning Dependency Translation Models nodes in the source and target derivation trees.</S>
    <S sid="96" ssid="35">Only the target tree t is stored in the configuration.</S>
    <S sid="97" ssid="36">The algorithm first initializes configurations for the input words, and then performs transitions and optimizations to develop the set of configurations bottom-up: Such an initial configuration has the form: [n,n' , wo, vo, q',c,vo] (q, 17', w1, vi, &#8212;1,1, c') It is applicable when there are the following head and dependent configurations: where the dependent configuration is in a final state qf.</S>
    <S sid="98" ssid="37">.</S>
    <S sid="99" ssid="38">The result of applying the transition is to add the following to the set of configurations: [ni , n3, w, v, q', c + ci + c', g] where t' is the target dependency tree formed by adding t1 as the rightmost dependent of t. [n,n',w,v,q,ci, ti] [n,n' , w, v, q,c2,t2] and c2 &gt; cl, the second configuration is removed from the set of configurations.</S>
    <S sid="100" ssid="39">If, after all applicable transitions have been taken, there are configurations spanning the entire input lattice, then the one with the lowest cost is the optimal derivation.</S>
    <S sid="101" ssid="40">When there are no such configurations, we take a pragmatic approach in the translation application and simply concatenate the lowest costing of the minimal length sequences of partial derivations that span the entire lattice.</S>
    <S sid="102" ssid="41">A Viterbi-like search of the graph formed by configurations is used to find the optimal sequence of derivations.</S>
    <S sid="103" ssid="42">One of the advantages of middle-out transduction is that robustness is improved through such use of partial derivations when no complete derivations are available.</S>
  </SECTION>
  <SECTION title="4." number="5">
    <S sid="104" ssid="1">Our training method for head transducer models only requires a set of training examples.</S>
    <S sid="105" ssid="2">Each example, or bitext, consists of a source language string paired with a target language string.</S>
    <S sid="106" ssid="3">In our experiments, the bitexts are transcriptions of spoken English utterances paired with their translations into Spanish or Japanese.</S>
    <S sid="107" ssid="4">It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages.</S>
    <S sid="108" ssid="5">Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus.</S>
    <S sid="109" ssid="6">For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.</S>
    <S sid="110" ssid="7">This contrasts with one of the traditional approaches (e.g., Dorr 1994; Watanabe 1995) to posing the translation problem, i.e., the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language.</S>
    <S sid="111" ssid="8">The training method has four stages: (i) Compute co-occurrence statistics from the training data.</S>
    <S sid="112" ssid="9">(ii) Search for an optimal synchronized hierarchical alignment for each bitext.</S>
    <S sid="113" ssid="10">(iii) Construct a set of head transducers that can generate these alignments with transition weights derived from maximum likelihood estimation.</S>
    <S sid="114" ssid="11">For each source word w in the data set, assign a cost, the translation pairing cost c(w, v) for all possible translations v into the target language.</S>
    <S sid="115" ssid="12">These translations of the source word may be zero, one, or several target language words (see Section 4.4 for discussion of the multiword case).</S>
    <S sid="116" ssid="13">The assignment of translation pairing costs (effectively a statistical bilingual dictionary) may be done using various statistical measures.</S>
    <S sid="117" ssid="14">For this purpose, a suitable statistical function needs to indicate the strength of cooccurrence correlation between source and target words, which we assume is indicative of carrying the same semantic content.</S>
    <S sid="118" ssid="15">Our preferred choice of statistical measure for assigning the costs is the 0 correlation measure (Gale and Church 1991).</S>
    <S sid="119" ssid="16">We apply this statistic to co-occurrence of the source word with all its possible translations in the data set examples.</S>
    <S sid="120" ssid="17">We have found that, at least for our data, this measure leads to better performance than the use of the log probabilities of target words given source words (cf.</S>
    <S sid="121" ssid="18">Brown et al. 1993).</S>
    <S sid="122" ssid="19">In addition to the correlation measure, the cost for a pairing includes a distance measure component that penalizes pairings proportionately to the difference between the (normalized) positions of the source and target words in their respective sentences.</S>
    <S sid="123" ssid="20">As noted earlier, dependency transduction models are generative probabilistic models; each derivation generates a pair of dependency trees.</S>
    <S sid="124" ssid="21">Such a pair can be represented as a synchronized hierarchical alignment of two strings.</S>
    <S sid="125" ssid="22">A hierarchical alignment consists of four functions.</S>
    <S sid="126" ssid="23">The first two functions are an alignment mapping f from source words w to target words f(w) (which may be the empty string &#8364;), and an inverse alignment mapping from target words v to source words f' (v).</S>
    <S sid="127" ssid="24">The inverse mapping is needed to handle mapping of target words to c; it coincides with f for pairs without source &#8364;.</S>
    <S sid="128" ssid="25">The other two functions are a source head-map g mapping source dependent words w to their heads g(w) in the source string, and a target head-map h mapping target dependent words v to their headwords h(v) in the target string.</S>
    <S sid="129" ssid="26">An A hierarchical alignment: alignment mappings f and f', and head-maps g and h. example hierarchical alignment is shown in Figure 6 (f and f' are shown separately for clarity).</S>
    <S sid="130" ssid="27">A hierarchical alignment is synchronized (i.e., it corresponds to synchronized dependency trees) if these conditions hold: Nonoverlap: If w1 w2, then f (wi) f(w2), and similarly, if 01 0 02, then/ (vi) f' (02).</S>
    <S sid="131" ssid="28">Synchronization: if f(w) =- v and v &#8364;, then f(g(w)) = h(o), and f' (v) = w. Similarly, if f' (v) w and w e, then f'(h(v)) = g(w), and f(w) = v. Phrase contiguity: The image under f of the maximal substring dominated by a headword w is a contiguous segment of the target string.</S>
    <S sid="132" ssid="29">(Here w and v refer to word tokens not symbols (types).</S>
    <S sid="133" ssid="30">We hope that the context of discussion will make the type-token distinction clear in the rest of this article.)</S>
    <S sid="134" ssid="31">The hierarchical alignment in Figure 6 is synchronized.</S>
    <S sid="135" ssid="32">Of course, translations of phrases are not always transparently related by a hierarchical alignment.</S>
    <S sid="136" ssid="33">In cases where the mapping between a source and target phrase is unclear (for example, one of the phrases might be an idiom), then the most reasonable choice of hierarchical alignment may be for f and f to link the heads of the phrases only, all the other words being mapped to e, with no constraints on the monolingual head mappings h and g. (This is the approach we take to compound lexical pairings, discussed in Section 4.4.)</S>
    <S sid="137" ssid="34">In the hierarchical alignments produced by the training method described here, the source and target strings of a bitext are decomposed into three aligned regions, as shown in Figure 7: a head region consisting of headword w in the source and its corresponding target f(w) in the target string, a left substring region consisting of the source substring to the left of w and its projection under f on the target string, and a right substring region consisting of the source substring to the right of w and its projection under f on the target string.</S>
    <S sid="138" ssid="35">The decomposition is recursive in that the left substring region is decomposed around a left headword w1, and the right substring region is decomposed around a right headword w&#8222; This process of decomposition continues for each left and right substring until it only contains a single word.</S>
    <S sid="139" ssid="36">For each bitext there are, in general, multiple such recursive decompositions that satisfy the synchronization constraints for hierarchical alignments.</S>
    <S sid="140" ssid="37">We wish to find such an alignment that respects the co-occurrence statistics of bitexts as well as the phrasal structure implicit in the source and target strings.</S>
    <S sid="141" ssid="38">For this purpose we define a cost function on hierarchical alignments.</S>
    <S sid="142" ssid="39">The cost function is the sum of three terms.</S>
    <S sid="143" ssid="40">The first term is the total of all the translation pairing costs c(w,f (w)) of each source word w and its translation f (w) in the alignment; the second term is proportional to the distance in the source string between dependents wd and their heads g(wd); and the third term is proportional to the distance in the target string between target dependent words vd and their heads h(vd).</S>
    <S sid="144" ssid="41">The hierarchical alignment that minimizes this cost function is computed using a dynamic programming procedure.</S>
    <S sid="145" ssid="42">In this procedure, the pairing costs are first retrieved for each possible source-target pair allowed by the example.</S>
    <S sid="146" ssid="43">Adjacent source substrings are then combined to determine the lowest-cost subalignments for successively larger substrings of the bitext satisfying the constraints stated above.</S>
    <S sid="147" ssid="44">The successively larger substrings eventually span the entire source string, yielding the optimal hierarchical alignment for the bitext.</S>
    <S sid="148" ssid="45">This procedure has 0(n6) complexity in the number of words in the source (or target) sentence.</S>
    <S sid="149" ssid="46">In Alshawi and Douglas (2000) we describe a version of the alignment algorithm in which heads may have an arbitrary number of dependents, and in which the hierarchical alignments for the training corpus are refined by iterative reestimation.</S>
    <S sid="150" ssid="47">Building a head transducer involves creating appropriate head transducer states and tracing hypothesized head transducer transitions between them that are consistent with the hierarchical alignment of a bitext.</S>
    <S sid="151" ssid="48">The main transitions that are traced in our construction are those that map heads, w1 and w&#8222; of the right and left dependent phrases of w to their translations as indicated by the alignment function f in the hierarchical alignment.</S>
    <S sid="152" ssid="49">The positions of the dependents in the target string are computed by comparing the positions of f (wi) and f (wr) to the position of v = f (w).</S>
    <S sid="153" ssid="50">In order to generalize from instances in the training data, some model states arising from different training instances are shared.</S>
    <S sid="154" ssid="51">In particular, in the construction described here, for a given pair (w, v) there is only one final state.</S>
    <S sid="155" ssid="52">(We have also tried using automatic word-clustering techniques to merge states further, but for the limited domain corpora we have used so far, the results are inconclusive.)</S>
    <S sid="156" ssid="53">To specify States and transitions constructed for the &amp;quot;swapping&amp;quot; decomposition shown in Figure 7. the sharing of states we make use of a one-to-one state-naming function a- from sequences of strings to transducer states.</S>
    <S sid="157" ssid="54">The same state-naming function is used for all examples in the data set, ensuring that the transducer fragments recorded for the entire data set will form a complete collection of head transducer transition networks.</S>
    <S sid="158" ssid="55">Figure 7 shows a decomposition in which w has a dependent to either side, v has both dependents to the right, and the alignment is &amp;quot;swapping&amp;quot; (f(wi) is to the right of f (wr)).</S>
    <S sid="159" ssid="56">The construction for this decomposition case is illustrated in Figure 8 as part of a finite-state transition diagram, and described in more detail below.</S>
    <S sid="160" ssid="57">(The other transition arrows shown in the diagram will arise from other bitext alignments containing (w, f (w)) pairings.)</S>
    <S sid="161" ssid="58">Other cases covered by our algorithm (e.g., a single left source dependent but no right source dependent, or target dependents on either side of the target head) are simple variants.</S>
    <S sid="162" ssid="59">The detailed construction is as follows: If instead the alignment had been as in Figure 9, in which the source dependents are mapped to target dependents in a parallel rather than swapping configuration (the configuration of sin escalas and Boston around flights:los vuelos in Figure 6), the construction is the same, except for the following differences: Other states are the same as for the first case.</S>
    <S sid="163" ssid="60">The resulting states and transitions are shown in Figure 10.</S>
    <S sid="164" ssid="61">After the construction described above is applied to the entire set of aligned hitexts in the training set, the counts for transitions are treated as event observation counts of a statistical dependency transduction model with the parameters described in Section 3.1.</S>
    <S sid="165" ssid="62">More specifically, the negated logs of these parameters are used as the weights for transducer transitions.</S>
    <S sid="166" ssid="63">In the translation application, source word w and target word v are generalized so they can be short substrings (compounds) of the source and target strings.</S>
    <S sid="167" ssid="64">Examples of such multiword pairs are show me:muestreme and nonstop:sin escalas in Figure 6.</S>
    <S sid="168" ssid="65">The cost for such pairings still uses the same 0 statistic, now taking the observations to be the co-occurrences of the substrings in the training bitexts.</S>
    <S sid="169" ssid="66">However, in order that these costs can be comparable to the costs for simple pairings, they are multiplied by the number of words in the source substring of the pairing.</S>
    <S sid="170" ssid="67">The use of compounds in pairings does not require any fundamental changes to the hierarchical alignment dynamic programming algorithm, which simply produces dependency trees with nodes that may be compounds.</S>
    <S sid="171" ssid="68">In the transducer construction phase of the training method, one of the words of a compound is taken to be the primary or &amp;quot;real&amp;quot; headword.</S>
    <S sid="172" ssid="69">(In fact, we take the least common word of a compound to be its head.)</S>
    <S sid="173" ssid="70">An extra chain of transitions is constructed to transduce the other words of compounds, if necessary using transitions with epsilon strings.</S>
    <S sid="174" ssid="71">This compilation means that the transduction algorithm is unaffected by the use of compounds when aligning training data, and there is no need for a separate compound identification phase when the transduction algorithm is applied to test data.</S>
    <S sid="175" ssid="72">Some results for different choices of substring lengths can be found in Alshawi, Bangalore, and Douglas (1998).</S>
  </SECTION>
  <SECTION title="5." number="6">
    <S sid="176" ssid="1">In order to reduce the time required to carry out training evaluation experiments, we have chosen two simple, string-based evaluation metrics that can be calculated automatically.</S>
    <S sid="177" ssid="2">These metrics, simple accuracy and translation accuracy, are used to compare the target string produced by the system against a reference human translation from held-out data.</S>
    <S sid="178" ssid="3">Simple accuracy is computed by first finding a transformation of one string into another that minimizes the total weight of insertions, deletions, and substitutions.</S>
    <S sid="179" ssid="4">(We use the same weights for these operations as in the NIST ASR evaluation software [National Institute of Standards and Technology 1997].)</S>
    <S sid="180" ssid="5">Translation accuracy includes transpositions (i.e., movement) of words as well as insertions, deletions, and substitutions.</S>
    <S sid="181" ssid="6">We regard the latter metric as more appropriate for evaluation of translation systems because the simple metric would count a transposition as two errors: an insertion plus a deletion.</S>
    <S sid="182" ssid="7">(This issue does not arise for speech recognizers because these systems do not normally make transposition errors.)</S>
    <S sid="183" ssid="8">For the lowest edit-distance transformation between the reference translation and system output, if we write I for the number of insertions, D for deletions, S for substitutions, and R for number of words in the reference translation string, we can express simple accuracy as simple accuracy = 1 &#8212; (/ + D + S)/R.</S>
    <S sid="184" ssid="9">Similarly, if T is the number of transpositions in the lowest weight transformation including transpositions, we can express translation accuracy as translation accuracy = 1 &#8212; (/' + D' + S + T)/R.</S>
    <S sid="185" ssid="10">Since a transposition corresponds to an insertion and a deletion, the values of I' and D' for translation accuracy will, in general, be different from I and D in the computation of simple accuracy.</S>
    <S sid="186" ssid="11">For Spanish, the units for string operations in the evaluation metrics are words, whereas for Japanese they are Japanese characters.</S>
    <S sid="187" ssid="12">The training and test data for the English-to-Spanish experiments were taken from a set of transcribed utterances from the Air Travel Information System (ATIS) corpus together with a translation of each utterance to Spanish.</S>
    <S sid="188" ssid="13">An utterance is typically a single sentence but is sometimes more than one sentence spoken in sequence.</S>
    <S sid="189" ssid="14">Alignment search and transduction training was carried out only on bitexts with sentences up to length 20, a total of 13,966 training bitexts.</S>
    <S sid="190" ssid="15">The test set consisted of 1,185 held-out bitexts at all lengths.</S>
    <S sid="191" ssid="16">Table 1 shows the word accuracy percentages (see Section 5.1) for the trained model, e2s, against the original held-out translations at various source sentence lengths.</S>
    <S sid="192" ssid="17">Scores are also given for a &amp;quot;word-for-word&amp;quot; baseline, sww, in which each English word is translated by the most highly correlated Spanish word.</S>
    <S sid="193" ssid="18">The training and test data for the English-to-Japanese experiments was a set of transcribed utterances of telephone service customers talking to AT&amp;T operators.</S>
    <S sid="194" ssid="19">These utterances, collected from real customer-operator interactions, tend to include fragmented language, restarts, etc.</S>
    <S sid="195" ssid="20">Both training and test partitions were restricted to bitexts with at most 20 English words, giving 12,226 training bitexts and 3,253 held-out test bitexts.</S>
    <S sid="196" ssid="21">In the Japanese text, we introduce &amp;quot;word&amp;quot; boundaries that are convenient Length &lt; 5 &lt; 10 &lt; 15 &lt; 20 All jww 75.8/78.0 45.2/50.4 40.0/45.4 37.2/42.8 37.2/42.8 e2j 89.2/89.7 74.0/76.6 68.6/72.2 66.4/70.1 66.4/70.1 for the training process.</S>
    <S sid="197" ssid="22">These word boundaries are parasitic on the word boundaries in the English transcriptions: the translators are asked to insert such a word boundary between any two Japanese characters that are taken to have arisen from the translation of distinct English words.</S>
    <S sid="198" ssid="23">This results in bitexts in which the number of multicharacter Japanese &amp;quot;words&amp;quot; is at most the number of English words.</S>
    <S sid="199" ssid="24">However, as noted above, evaluation of the Japanese output is done with Japanese characters, i.e., with the Japanese text in its natural format.</S>
    <S sid="200" ssid="25">Table 2 shows the Japanese character accuracy percentages for the trained English-to-Japanese model, e2j, and a baseline model, jww, which gives each English word its most highly correlated translation.</S>
    <S sid="201" ssid="26">The vocabularies in these English-Spanish and English-Japanese experiments are only a few thousand words; the utterances are fairly short (an average of 7.3 words per utterance) and often contain errors typical of spoken language.</S>
    <S sid="202" ssid="27">So while the domains may be representative of task-oriented dialogue settings, further experimentation would be needed to assess the effectiveness of our method in situations such as translating newspaper articles.</S>
    <S sid="203" ssid="28">In terms of the training data required, Tsukada et al. (1999) provide indirect empirical evidence suggesting accuracy can be further improved by increasing the size of our training sets, though also suggesting that the learning curve is relatively shallow beyond the current size of corpus.</S>
  </SECTION>
  <SECTION title="6." number="7">
    <S sid="204" ssid="1">Formalisms for finite-state and context-free transduction have a long history (e.g., Lewis and Stearns 1968; Aho and Ullman 1972), and such formalisms have been applied to the machine translation problem, both in the finite-state case (e.g., Vilar et al. 1996) and the context-free case (e.g., Wu 1997).</S>
    <S sid="205" ssid="2">In this paper we have added to this line of research by providing a method for automatically constructing fully lexicalized statistical dependency transduction models from training examples.</S>
    <S sid="206" ssid="3">Automatically training a translation system brings important benefits in terms of maintainability, robustness, and reducing expert coding effort as compared with traditional rule-based translation systems (a number of which are described in Hutchins and Somers [1992]).</S>
    <S sid="207" ssid="4">The reduction of effort results, in large part, from being able to do without artificial intermediate representations of meaning; we do not require the development of semantic mapping rules (or indeed any rules) or the creation of a corpus including semantic annotations.</S>
    <S sid="208" ssid="5">Compared with left-to-right transduction, middle-out transduction also aids robustness because, when complete derivations are not available, partial derivations tend to have meaningful headwords.</S>
    <S sid="209" ssid="6">At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al.</S>
    <S sid="210" ssid="7">1993) for training translation systems automatically.</S>
    <S sid="211" ssid="8">One advantage is that our method attempts to model the natural decomposition of sentences into phrases.</S>
    <S sid="212" ssid="9">Another is that the compilation of this decomposition into lexically anchored finite-state head transducers produces implementations that are much more efficient than those for the IBM model.</S>
    <S sid="213" ssid="10">In particular, our search algorithm finds optimal transductions of test sentences in less than &amp;quot;real time&amp;quot; on a 300MHz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application.</S>
  </SECTION>
</PAPER>
