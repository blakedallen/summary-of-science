Manual And Automatic Evaluation Of Machine Translation Between European Languages
We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back.
Evaluation was done automatically using the BLEU score and manually on fluency and adequacy.
The results of the workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems.
We report and analyze several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric.
