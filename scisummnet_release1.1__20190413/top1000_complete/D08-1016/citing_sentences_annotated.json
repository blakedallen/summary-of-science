[
  {
    "citance_No": 1, 
    "citing_paper_id": "P14-1098", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mohit, Bansal | David, Burkett | Gerard de, Melo | Dan, Klein", 
    "raw_text": "Note that finding taxonomy trees isa structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008)", 
    "clean_text": "Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1098", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mohit, Bansal | David, Burkett | Gerard de, Melo | Dan, Klein", 
    "raw_text": "5 The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008)", 
    "clean_text": "The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D10-1004", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Eric P., Xing | Pedro, Aguiar | Mario, Figueiredo", 
    "raw_text": "We present a unified view of two state-of-the art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009)", 
    "clean_text": "We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D10-1004", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Eric P., Xing | Pedro, Aguiar | Mario, Figueiredo", 
    "raw_text": "Often, inference with such models becomes computationally in tractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection be tween two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al,2009)", 
    "clean_text": "In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al,2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D10-1004", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Eric P., Xing | Pedro, Aguiar | Mario, Figueiredo", 
    "raw_text": "The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009)", 
    "clean_text": "The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D10-1004", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Eric P., Xing | Pedro, Aguiar | Mario, Figueiredo", 
    "raw_text": "Throughtout, we call these turbo parsers.1 Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints (? 2), which extends some combinatorial factors considered by Smith and Eisner (2008)", 
    "clean_text": "Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints, which extends some combinatorial factors considered by Smith and Eisner (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D10-1004", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Eric P., Xing | Pedro, Aguiar | Mario, Figueiredo", 
    "raw_text": "Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig", 
    "clean_text": "Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D10-1004", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Eric P., Xing | Pedro, Aguiar | Mario, Figueiredo", 
    "raw_text": "Fortunately, for all the hard constraint factors in rows 3? 5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor)? this ex tends results presented in Smith and Eisner (2008) .4 4The insight behind these speed-ups is that messages on binary-valued potentials can be expressed as MC? i (yi)? 36 TREE 1 ARC (T, RE) SIB (T1RE1RE) 1 2 SIB (T1RE1RE) 1 3 SIB (T1RE1RE) 2 3GRAND (A, T1RE) 1 2 ARC (T, RE) 3 ARC (T, RE) ARC (A, T) Figure 1: Factor graph corresponding to the dependency parsing model of Smith and Eisner (2008) with sibling and grandparent features", 
    "clean_text": "Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "D10-1004", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Eric P., Xing | Pedro, Aguiar | Mario, Figueiredo", 
    "raw_text": "Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig", 
    "clean_text": "Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D10-1004", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Eric P., Xing | Pedro, Aguiar | Mario, Figueiredo", 
    "raw_text": "? a, b?? P Ia; b (za ,zb ,zab), (13) where we introduced the mutual information associated with each pairwise factor, Ia; b (za ,zb ,zab)= 6Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity", 
    "clean_text": "Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D10-1004", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Eric P., Xing | Pedro, Aguiar | Mario, Figueiredo", 
    "raw_text": "min (h, m) &lt; j &lt; min (h, m) ?phj. Details are omitted for space. In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D12-1074", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Jason, Naradowsky | David A., Smith | Sebastian, Riedel", 
    "raw_text": "A dependency graph canbe modeled with the following nodes, as first pro posed by Smith and Eisner (2008):? Let{ Link (i, j): 0? i? j? n, n 6= j} be O (n2) boolean variables corresponding to the possible links in a dependency parse", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "D12-1074", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Jason, Naradowsky | David A., Smith | Sebastian, Riedel", 
    "raw_text": "811 Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally con strain variable configurations. Since the outgoing messages from such factors to a variable can be computed from the factor? s posterior beliefs about that variable, there is no difficulty in exchanging beliefs between these special-purpose factors and the rest of the graph, and inference can proceed using the standard sum-product or max-product belief propagation", 
    "clean_text": "However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D12-1074", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Jason, Naradowsky | David A., Smith | Sebastian, Riedel", 
    "raw_text": "Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1iff the configuration of Link variables forms a valid projective dependency graph", 
    "clean_text": "Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "D11-1031", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Michael, Auli | Adam, Lopez", 
    "raw_text": "To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008)", 
    "clean_text": "To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P13-2109", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Andre, Martins | Miguel, Almeida | Noah A., Smith", 
    "raw_text": "Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011)", 
    "clean_text": "Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D12-1067", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "David A., Smith | Sebastian, Riedel | Andrew, McCallum", 
    "raw_text": "For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), be lief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider", 
    "clean_text": "For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D12-1067", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "David A., Smith | Sebastian, Riedel | Andrew, McCallum", 
    "raw_text": "In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients", 
    "clean_text": "In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P11-1048", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Michael, Auli | Adam, Lopez", 
    "raw_text": "This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009)", 
    "clean_text": "This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P11-1048", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Michael, Auli | Adam, Lopez", 
    "raw_text": "One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008)", 
    "clean_text": "One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008).", 
    "keep_for_gold": 0
  }
]
