Domain Adaptation for Statistical Machine Translation with Monolingual Resources
Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions.
The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system.
Previous work showed small performance gains by adapting from limited in-domain bilingual data.
Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language.
We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language.
Investigations were conducted on a state-of-the-art phrase-based system trained on the Spanishâ€“English part of the UN corpus, and adapted on the corresponding Europarl data.
Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline.
By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set.
In order to use source-side monolingual data, we employ the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence.
We adapt an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses.
