<PAPER>
  <S sid="0">Domain Adaptation for Statistical Machine Translation with Monolingual Resources</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions.</S>
    <S sid="2" ssid="2">The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system.</S>
    <S sid="3" ssid="3">Previous work showed small performance gains by adapting from limited in-domain bilingual data.</S>
    <S sid="4" ssid="4">Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language.</S>
    <S sid="5" ssid="5">We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language.</S>
    <S sid="6" ssid="6">Investigations were conducted on a stateof-the-art phrase-based system trained on the Spanish&#8211;English part of the UN corpus, and adapted on the corresponding Europarl data.</S>
    <S sid="7" ssid="7">Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline.</S>
    <S sid="8" ssid="8">By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="9" ssid="1">A well-known problem of Statistical Machine Translation (SMT) is that performance quickly degrades as soon as testing conditions deviate from training conditions.</S>
    <S sid="10" ssid="2">The very simple reason is that the underlying statistical models always tend to closely approximate the empirical distributions of the training data, which typically consist of bilingual texts and monolingual target-language texts.</S>
    <S sid="11" ssid="3">The former provide a means to learn likely translations pairs, the latter to form correct sentences with translated words.</S>
    <S sid="12" ssid="4">Besides the general difficulties of language translation, which we do not consider here, there are two aspects that make machine learning of this task particularly hard.</S>
    <S sid="13" ssid="5">First, human language has intrinsically very sparse statistics at the surface level, hence gaining complete knowledge on translation phrase pairs or target language n-grams is almost impractical.</S>
    <S sid="14" ssid="6">Second, language is highly variable with respect to several dimensions, style, genre, domain, topics, etc.</S>
    <S sid="15" ssid="7">Even apparently small differences in domain might result in significant deviations in the underlying statistical models.</S>
    <S sid="16" ssid="8">While data sparseness corroborates the need of large language samples in SMT, linguistic variability would indeed suggest to consider many alternative data sources as well.</S>
    <S sid="17" ssid="9">By rephrasing a famous saying we could say that &#8220;no data is better than more and assorted data&#8221;.</S>
    <S sid="18" ssid="10">The availability of language resources for SMT has dramatically increased over the last decade, at least for a subset of relevant languages and especially for what concerns monolingual corpora.</S>
    <S sid="19" ssid="11">Unfortunately, the increase in quantity has not gone in parallel with an increase in assortment, especially for what concerns the most valuable resource, that is bilingual corpora.</S>
    <S sid="20" ssid="12">Large parallel data available to the research community are for the moment limited to texts produced by international organizations (European Parliament, United Nations, Canadian Hansard), press agencies, and technical manuals.</S>
    <S sid="21" ssid="13">The limited availability of parallel data poses challenging questions regarding the portability of SMT across different application domains and language pairs, and its adaptability with respect to language variability within the same application domain.</S>
    <S sid="22" ssid="14">This work focused on the second issue, namely the adaptation of a Spanish-to-English phrasebased SMT system across two apparently close domains: the United Nation corpus and the European Parliament corpus.</S>
    <S sid="23" ssid="15">Cross-domain adaptation is faced under the assumption that only monolingual texts are available, either in the source language or in the target language.</S>
    <S sid="24" ssid="16">The paper is organized as follows.</S>
    <S sid="25" ssid="17">Section 2 presents previous work on the problem of adaptation in SMT; Section 3 introduces the exemplar task and research questions we addressed; Section 4 describes the SMT system and the adaptation techniques that were investigated; Section 5 presents and discusses experimental results; and Section 6 provides conclusions.</S>
  </SECTION>
  <SECTION title="2 Previous Work" number="2">
    <S sid="26" ssid="1">Domain adaptation in SMT has been investigated only recently.</S>
    <S sid="27" ssid="2">In (Eck et al., 2004) adaptation is limited to the target language model (LM).</S>
    <S sid="28" ssid="3">The background LM is combined with one estimated on documents retrieved from the WEB by using the input sentence as query and applying crosslanguage information retrieval techniques.</S>
    <S sid="29" ssid="4">Refinements of this approach are described in (Zhao et al., 2004).</S>
    <S sid="30" ssid="5">In (Hildebrand et al., 2005) information retrieval techniques are applied to retrieve sentence pairs from the training corpus that are relevant to the test sentences.</S>
    <S sid="31" ssid="6">Both the language and the translation models are retrained on the extracted data.</S>
    <S sid="32" ssid="7">In (Foster and Kuhn, 2007) two basic settings are investigated: cross-domain adaptation, in which a small sample of parallel in-domain text is assumed, and dynamic adaptation, in which only the current input source text is considered.</S>
    <S sid="33" ssid="8">Adaptation relies on mixture models estimated on the training data through some unsupervised clustering method.</S>
    <S sid="34" ssid="9">Given available adaptation data, mixture weights are re-estimated ad-hoc.</S>
    <S sid="35" ssid="10">A variation of this approach was also recently proposed in (Finch and Sumita, 2008).</S>
    <S sid="36" ssid="11">In (Civera and Juan, 2007) mixture models are instead employed to adapt a word alignment model to in-domain parallel data.</S>
    <S sid="37" ssid="12">In (Koehn and Schroeder, 2007) cross-domain adaptation techniques were applied on a phrasebased SMT trained on the Europarl task, in order to translate news commentaries, from French to English.</S>
    <S sid="38" ssid="13">In particular, a small portion of indomain bilingual data was exploited to adapt the Europarl language model and translation models by means of linear interpolation techniques.</S>
    <S sid="39" ssid="14">Ueffing et al. (2007) proposed several elaborate adaptation methods relying on additional bilingual data synthesized from the development or test set.</S>
    <S sid="40" ssid="15">Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data: i.e. only monolingual in-domain texts are available.</S>
    <S sid="41" ssid="16">The adaptation of the translation and re-ordering models is performed by generating synthetic bilingual data from monolingual texts, similarly to what proposed in (Schwenk, 2008).</S>
    <S sid="42" ssid="17">Interpolation of multiple phrase tables is applied in a more principled way than in (Koehn and Schroeder, 2007): all entries are merged into one single table, corresponding feature functions are concatenated and smoothing is applied when observations are missing.</S>
    <S sid="43" ssid="18">The approach proposed in this paper has many similarities with the simplest technique in (Ueffing et al., 2007), but it is applied to a much larger monolingual corpus.</S>
    <S sid="44" ssid="19">Finally, with respect to previous work we also investigate the behavior of the minimum error training procedure to optimize the combination of feature functions on a small in-domain bilingual sample.</S>
  </SECTION>
  <SECTION title="3 Task description" number="3">
    <S sid="45" ssid="1">This paper addresses the issue of adapting an already developed phrase-based translation system in order to work properly on a different domain, for which almost no parallel data are available but only monolingual texts.1 The main components of the SMT system are the translation model, which aims at porting the content from the source to the target language, and the language model, which aims at building fluent sentences in the target language.</S>
    <S sid="46" ssid="2">While the former is trained with bilingual data, the latter just needs monolingual target texts.</S>
    <S sid="47" ssid="3">In this work, a lexicalized re-ordering model is also exploited to control re-ordering of target words.</S>
    <S sid="48" ssid="4">This model is also learnable from parallel data.</S>
    <S sid="49" ssid="5">Assuming some large monolingual in-domain texts are available, two basic adaptation approaches are pursued here: (i) generating synthetic bilingual data with an available SMT system and use this data to adapt its translation and re-ordering models; (ii) using synthetic or provided target texts to also, or only, adapt its language model.</S>
    <S sid="50" ssid="6">The following research questions summarize our basic interest in this work:</S>
  </SECTION>
  <SECTION title="4 System description" number="4">
    <S sid="51" ssid="1">The investigation presented in this paper was carried out with the Moses toolkit (Koehn et al., 2007), a state-of-the-art open-source phrase-based SMT system.</S>
    <S sid="52" ssid="2">We trained Moses in a standard configuration, including a 4-feature translation model, a 7-feature lexicalized re-ordering model, one LM, word and phrase penalties.</S>
    <S sid="53" ssid="3">The translation and the re-ordering model relied on &#8220;grow-diag-final&#8221; symmetrized word-toword alignments built using GIZA++ (Och and Ney, 2003) and the training script of Moses.</S>
    <S sid="54" ssid="4">A 5-gram language model was trained on the target side of the training parallel corpus using the IRSTLM toolkit (Federico et al., 2008), exploiting Modified Kneser-Ney smoothing, and quantizing both probabilities and backoff weights.</S>
    <S sid="55" ssid="5">Decoding was performed applying cube-pruning with a poplimit of 6000 hypotheses.</S>
    <S sid="56" ssid="6">Log-linear interpolations of feature functions were estimated with the parallel version of minimum error rate training procedure distributed with Moses.</S>
    <S sid="57" ssid="7">The standard procedure of Moses for the estimation of the translation and re-ordering models from a bilingual corpus consists in three main steps: Recently, we enhanced Moses decoder to also output the word-to-word alignment between the input sentence and its translation, given that they have been added to the phrase table at training time.</S>
    <S sid="58" ssid="8">Notice that the additional information introduces an overhead in disk usage of about 70%, but practically no overhead at decoding time.</S>
    <S sid="59" ssid="9">However, when training translation and re-ordering models from synthetic data generated by the decoder, this feature allows to completely skip the time-expensive step 1.2 We tested the efficiency of this solution for training a translation model on a synthesized corpus of about 300K Spanish sentences and 8.8M running words, extracted from the EuroParl corpus.</S>
    <S sid="60" ssid="10">With respect to the standard procedure, the total training time was reduced by almost 50%, phrase extraction produced 10% more phrase pairs, and the final translation system showed a loss in translation performance (BLEU score) below 1% relative.</S>
    <S sid="61" ssid="11">Given this outcome we decided to apply the faster procedure in all experiments.</S>
    <S sid="62" ssid="12">Once monolingual adaptation data is automatically translated, we can use the synthetic parallel corpus to estimate new language, translation, and re-ordering models.</S>
    <S sid="63" ssid="13">Such models can either replace or be combined with the original models of the SMT system.</S>
    <S sid="64" ssid="14">There is another simple option which is to concatenate the synthetic parallel data with the original training data and re-build the system.</S>
    <S sid="65" ssid="15">We did not investigate this approach because it does not allow to properly balance the contribution of different data sources, and also showed to underperform in preliminary work.</S>
    <S sid="66" ssid="16">Concerning the combination of models, in the following we explain how Moses was extended to manage multiple translation models (TMs) and multiple re-ordering models (RMs).</S>
    <S sid="67" ssid="17">In Moses, a TM is provided as a phrase table, which is a set S = {(&#65533;f, e)} of phrase pairs associated with a given number of features values h(&#65533;f, E; S).</S>
    <S sid="68" ssid="18">In our configuration, 5 features for the TM (the phrase penalty is included) are taken into account.</S>
    <S sid="69" ssid="19">In the first phase of the decoding process, Moses generates translation options for all possible input phrases f through a lookup into S; it simply extracts alternative phrase pairs ( f, E) for a specific f and optionally applies pruning (based on the feature values and weights) to limit the number of such pairs.</S>
    <S sid="70" ssid="20">In the second phase of decoding, it creates translation hypotheses of the full input sentence by combining in all possible ways (satisfying given re-ordering constraints) the prefetched translation options.</S>
    <S sid="71" ssid="21">In this phase the hypotheses are scored, according to all features functions, ranked, and possibly pruned.</S>
    <S sid="72" ssid="22">When more TMs Sj are available, Moses can behave in two different ways in pre-fetching the translation options.</S>
    <S sid="73" ssid="23">It searches a given f in all sets and keeps a phrase pair ( f, E) if it belongs to either i) their intersection or ii) their union.</S>
    <S sid="74" ssid="24">The former method corresponds to building one new TM SI, whose set is the intersection of all given sets: phrase-based and lexical-based direct features are defined as follows: Here, &#966;(ek  |fh) is the probability of ek given fh provided by the word-to-word lexicon computed on Sj.</S>
    <S sid="75" ssid="25">The inverted features are defined similarly.</S>
    <S sid="76" ssid="26">The phrase penalty is trivially set to 1.</S>
    <S sid="77" ssid="27">The same approach has been applied to build the union of re-ordering models.</S>
    <S sid="78" ssid="28">In this case, however, the smoothing value is constant and set to 0.001.</S>
    <S sid="79" ssid="29">As concerns as the use of multiple LMs, Moses has a very easy policy, consisting of querying each of them to get the likelihood of a translation hypotheses, and uses all these scores as features.</S>
    <S sid="80" ssid="30">It is worth noting that the exploitation of multiple models increases the number of features of the whole system, because each model adds its set of features.</S>
    <S sid="81" ssid="31">Furthermore, the first approach of Moses for model combination shrinks the size of the phrase table, while the second one enlarges it.</S>
    <S sid="82" ssid="32">The set of features of the new TM is the union of the features of all single TMs.</S>
    <S sid="83" ssid="33">Straightforwardly, all feature values are well-defined.</S>
    <S sid="84" ssid="34">The second method corresponds to building one new TM SU, whose set is the union of all given sets: SU = {( &#65533;f, &#65533;e)  |&#65533;j ( f, E) E Sj} Again, the set of features of the new TM is the union of the features of all single TMs; but for a phrase pair (&#65533;f, E) belonging to SU \Sj, the feature values h(&#65533;f, E; Sj) are undefined.</S>
    <S sid="85" ssid="35">In these undefined situations, Moses provides a default value of 0, which is the highest available score, as the feature values come from probabilistic distributions and are expressed as logarithms.</S>
    <S sid="86" ssid="36">Henceforth, a phrase pair belonging to all original sets is penalized with respect to phrase pairs belonging to few of them only.</S>
    <S sid="87" ssid="37">To address this drawback, we proposed a new method3 to compute a more reliable and smoothed score in the undefined case, based on the IBM model 1 (Brown et al., 1993).</S>
    <S sid="88" ssid="38">If ( f&#65533; = f1, ... , fl, e&#65533; = e1, ... , el) E SU \ Sj for any j the In this work, the background domain is given by the Spanish-English portion of the UN parallel corpus,4 composed by documents coming from the Office of Conference Services at the UN in New York spanning the period between 1988 and 1993.</S>
    <S sid="89" ssid="39">The adaptation data come from the European Parliament corpus (Koehn, 2002) (EP) as provided for the shared translation task of the 2008 Workshop on Statistical Machine Translation.5 Development and test sets for this task, namely dev2006 and test2008, are supplied as well, and belong to the European Parliament domain.</S>
    <S sid="90" ssid="40">We use the symbol S&#175; (&#175;E) to denote synthetic Spanish (English) data.</S>
    <S sid="91" ssid="41">Spanish-to-English and English-to-Spanish systems trained on UN data were exploited to generate English and Spanish synthetic portions of the original EP corpus, respectively.</S>
    <S sid="92" ssid="42">In this way, we created two synthetic versions of the EP corpus, named S&#175;E-EP and &#175;SEEP, respectively.</S>
    <S sid="93" ssid="43">All presented translation systems were optimized on the dev2006 set with respect to the BLEU score (Papineni et al., 2002), and tested on test2008.</S>
    <S sid="94" ssid="44">(Notice that one reference translation is available for both sets.)</S>
    <S sid="95" ssid="45">Table 1 reports statistics of original and synthetic parallel corpora, as well of the employed development and evaluation data sets.</S>
    <S sid="96" ssid="46">All the texts were just tokenized and mixed case was kept.</S>
    <S sid="97" ssid="47">Hence, all systems were developed to produce case-sensitive translations.</S>
    <S sid="98" ssid="48">Three Spanish-to-English baseline systems were trained by exploiting different parallel or monolingual corpora summarized in the first three lines in Table 2.</S>
    <S sid="99" ssid="49">For each system, the table reports the perplexity and out-of-vocabulary (OOV) percentage of their LM, and its translation performance achieved on the test set in terms of BLEU score, NIST score, WER (word error rate) and PER (position independent error rate).</S>
    <S sid="100" ssid="50">The distance in style, genre, jargon, etc. between the UN and the EP corpora is made evident by the gap in perplexity (Federico and De Mori, 1998) and OOV percentage between their English LMs: 286 vs 74 and 1.12% vs 0.15%, respectively.</S>
    <S sid="101" ssid="51">Performance of the system trained on the EP corpus (third row) can be taken as an upper bound for any adaptation strategy trying to exploit parts of the EP corpus, while those of the first line clearly provide the corresponding lower-bound.</S>
    <S sid="102" ssid="52">The system in the second row can instead be consider as the lower bound when only monolingual English adaptation data are assumed.</S>
    <S sid="103" ssid="53">The synthesis of the S&#175;E-EP corpus was performed with the system trained just on the UN training data (first row of Table 2), because we had assumed that the in-domain data were only monolingual Spanish and thus not useful for neither the TM, RM nor target LM estimation.</S>
    <S sid="104" ssid="54">Similarly, the system in the last row of Table 2 was developed on the UN corpus to translate the English part of the EP data to generate the synthetic &#175;SE-EP corpus.</S>
    <S sid="105" ssid="55">Again, any in-domain data were exploited to train this sytem.</S>
    <S sid="106" ssid="56">Of course, this system cannot be compared with any other because of the different translation direction.</S>
    <S sid="107" ssid="57">In order to compare reported performance with the state-of-the-art, Table 2 also reports results of the best system published in the EuroMatrix project website6 and of the Google online translation engine.7 It is well-known that tuning the SMT system is fundamental to achieve good performance.</S>
    <S sid="108" ssid="58">The standard tuning procedure consists of a minimum error rate training (mert) (Och and Ney, 2003) which relies on the availability of a development data set.</S>
    <S sid="109" ssid="59">On the other hand, the most important assumption we make is that almost no parallel indomain data are available. the tuning process and BLEU score achieved on the test set by the uniform interpolation weights (first row), and by the optimal weights with different configurations of the tuning parameters.</S>
    <S sid="110" ssid="60">In a preliminary phase, we investigated different settings of the tuning process in order to understand how much development data is required to perform a reliable weight optimization.</S>
    <S sid="111" ssid="61">Our models were trained on the S&#175;E-EP parallel corpus and by using uniform interpolation weights the system achieved a BLEU score of 22.28% on the test set (see Table 3).</S>
    <S sid="112" ssid="62">We assumed to dispose of either a regular in-domain development set of 2,000 sentences (dev2006), or a small portion of it of just 200 sentences.</S>
    <S sid="113" ssid="63">Moreover, we tried to employ either 1,000best or 200-best translation candidates during the mert process.</S>
    <S sid="114" ssid="64">From a theoretical point of view, computational effort of the tuning process is proportional to the square of the number of translation alternatives generated at each iteration times the number of iterations until convergence.</S>
    <S sid="115" ssid="65">Figure 1 reports incremental tuning time and translation performance on the test set at each iteration.</S>
    <S sid="116" ssid="66">Notice that the four tuning configurations are ranked in order of complexity.</S>
    <S sid="117" ssid="67">Table 3 summaries the final performance of each tuning process, after convergence was reached.</S>
    <S sid="118" ssid="68">Notice that decoding time is not included in this plot, as Moses allows to perform this step in parallel on a computer cluster.</S>
    <S sid="119" ssid="69">Hence, to our view the real bottleneck of the tuning process is actually related to the strictly serial part of the mert implementation of Moses.</S>
    <S sid="120" ssid="70">As already observed in previous literature (Macherey et al., 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository.</S>
    <S sid="121" ssid="71">Configurations exploiting the small development set (c,d) show a slower and more unstable convergence; however, their final performance in Table 3 result only slightly lower than that obtained with the standard dev sets (a, b).</S>
    <S sid="122" ssid="72">Due to the larger number of iterations they needed, both configurations are indeed more time consuming than the intermediate configuration (b), which seems the best one.</S>
    <S sid="123" ssid="73">In conclusion, we found that the size of the n-best list has essentially no effect on the quality of the final weights, but it impacts significantly on the computational time.</S>
    <S sid="124" ssid="74">Moreover, using the regular development set with few translation alternatives ends up to be the most efficient configuration in terms of computational effort, robustness, and performance.</S>
    <S sid="125" ssid="75">Our analysis suggests that it is important to dispose of a sufficiently large development set although reasonably good weights can be obtained even if such data are very few.</S>
    <S sid="126" ssid="76">A set of experiments was devoted to the adaptation of the LM only.</S>
    <S sid="127" ssid="77">We trained three different LMs on increasing portions of the EP and we employed them either alone or in combination with the background LM trained on the UN corpus.</S>
    <S sid="128" ssid="78">Percentage of monolingual English adaptation data systems.</S>
    <S sid="129" ssid="79">The absolute gain with respect to the baseline is fairly high, even with the smallest amount of adaptation data (+4.02).</S>
    <S sid="130" ssid="80">The benefit of using the background data together with indomain data is very small, and rapidly vanishes as the amount of such data increases.</S>
    <S sid="131" ssid="81">If English synthetic texts are employed to adapt the LM component, the increase in performance is significantly lower but still remarkable (see Table 2).</S>
    <S sid="132" ssid="82">By employing all the available data, the gain in BLEU% score was of 4% relative, that is from 22.60 to 23.52. opment set as usual.</S>
    <S sid="133" ssid="83">Results of these experiments are reported in Figure 3.</S>
    <S sid="134" ssid="84">Results suggest that regardless of the used bilingual corpora the in-domain TMs and RMs work better alone than combined with the original models.</S>
    <S sid="135" ssid="85">We think that this behavior can be explained by a limited disciminative power of the resulting combined model.</S>
    <S sid="136" ssid="86">The background translation model could contain phrases which either do or do not fit the adaptation domain.</S>
    <S sid="137" ssid="87">As the weights are optimized to balance the contribution of all phrases, the system is not able to well separate the positive examples from the negative ones.</S>
    <S sid="138" ssid="88">In addition to it, system tuning is much more complex because the number of features increases from 14 to 26.</S>
    <S sid="139" ssid="89">Finally, TMs and RMs estimated from synthetic data show to provide smaller, but consistent, contributions than the corresponding LMs.</S>
    <S sid="140" ssid="90">When English in-domain data is provided, BLEU% score increases from 22.60 to 28.10; TM and RM contribute by about 5% relative, by covering the gap from 27.83 to 28.10.</S>
    <S sid="141" ssid="91">When Spanish in-domain data is provided BLEU% score increases from 22.60 to 23.68; TM and RM contribute by about 15% relative, by covering the gap from 23.52 to 23.68 .</S>
    <S sid="142" ssid="92">Summarizing, the most important role in the domain adaptation is played by the LM; nevertheless the adaptation of the TM and RM gives a small further improvement.. Another set of experiments relates to the adaptation of the TM and the RM.</S>
    <S sid="143" ssid="93">In-domain TMs and RMs were estimated on three different versions of the full parallel EP corpus, namely EP, S&#175;E-EP, and &#175;SE-EP.</S>
    <S sid="144" ssid="94">In-domain LMs were trained on the corresponding English side.</S>
    <S sid="145" ssid="95">All in-domain models were either used alone or combined with the baseline models according to multiple-model paradigm explained in Section 4.3.</S>
    <S sid="146" ssid="96">Tuning of the interpolation weights was performed on the standard devel</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="5">
    <S sid="147" ssid="1">This paper investigated cross-domain adaptation of a state-of-the-art SMT system (Moses), by exploiting large but cheap monolingual data.</S>
    <S sid="148" ssid="2">We proposed to generate synthetic parallel data by translating monolingual adaptation data with a background system and to train statistical models from the synthetic corpus.</S>
    <S sid="149" ssid="3">We found that the largest gain (25% relative) is achieved when in-domain data are available for the target language.</S>
    <S sid="150" ssid="4">A smaller performance improvement is still observed (5% relative) if source adaptation data are available.</S>
    <S sid="151" ssid="5">We also observed that the most important role is played by the LM adaptation, while the adaptation of the TM and RM gives consistent but small improvement.</S>
    <S sid="152" ssid="6">We also showed that a very tiny development set of only 200 parallel sentences is adequate enough to get comparable performance as a 2000-sentence set.</S>
    <S sid="153" ssid="7">Finally, we described how to reduce the time for training models from a synthetic corpus generated through Moses by 50% at least, by exploiting word-alignment information provided during decoding.</S>
  </SECTION>
</PAPER>
