<PAPER>
	<S sid="0">Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries</S><ABSTRACT>
		<S sid="1" ssid="1">In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonslrate he use of these networks for word sense disambiguation.</S>
		<S sid="2" ssid="2">Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models.</S>
		<S sid="3" ssid="3">The automatic construction of VLNNs enables real-size xperiments with neural networks for natural language processing, which in turn provides insight into their behavior and design and can lead to possible improvements.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="4" ssid="4">Automated language understanding requires the determination f the concept which a given use of a word represents, a process referred to as word sense disambiguation (WSD).</S>
			<S sid="5" ssid="5">WSD is typically effected in natural llanguage processing systems by utilizing semantic teature lists for each word in the system's lexicon, together with restriction mechanisms such as case role selection.</S>
			<S sid="6" ssid="6">However, it is often impractical to manually encode such information, especially for generalized text where the variety and meaning of words is potentially unrestricted.</S>
			<S sid="7" ssid="7">Furthermore, restriction mechanisms usually operate within a single sentence~ and thus the broader context cannot assist in the disambiguation process.</S>
			<S sid="8" ssid="8">in this paper, we describe a means tor automatically building Very Large Neural Networks (VLNNs) from definition texts in machine-readable dictionaries, and denmnstrate he use of these networks for WSD.</S>
			<S sid="9" ssid="9">Our method brings together two earlier, independent approaches to WSD: the use of machine-readable dictionaries and spreading and activation models.</S>
			<S sid="10" ssid="10">The automatic onstruction of VLNNs enables real-size experiments with neural networks, which in turn The authors would like to acknowledge the contributions of St~phanc tlari6 and Gavin Huntlcy to the work presented in this paper.</S>
			<S sid="11" ssid="11">provides insight into their behavior and design and can lead to possible improvements.</S>
	</SECTION>
	<SECTION title="Previous work. " number="2">
			<S sid="12" ssid="1">2.1.</S>
			<S sid="13" ssid="2">Machine-readable dictionaries Jbr WSD.</S>
			<S sid="14" ssid="3">There have been several attempts to exploit the information in maclfine-readable versions of everyday dictionaries ( ee, tor instance, Amsler, 1980; Calzolari, 1984; Chodorow, Byrd and Heidorn, 1985; Markowitz, Ahlswede and Evens, 1986; Byrd et al, 1987; V&amp;onis, Ide and Wurbel, 1989), in which an enormous amount of lexical and semantic knowledge is already "encoded".</S>
			<S sid="15" ssid="4">Such information is not systematic or even complete, and its extraction from machine- readable dictionaries is not always straightforward.</S>
			<S sid="16" ssid="5">However, it has been shown that even in its base form, information from machine-readable dictionaries can be used, for example, to assist in the disambiguation f prepositional phrase attachment (Jensen and Bluet, 1987), or to find subject domains in texts (Walker and Amsler, 1986).</S>
			<S sid="17" ssid="6">The most general and well-known attempt to utilize information i machine-readable dictionaries for WSD is that of Lesk (1986), which computes the degree of overlap--that is, number of shared words--in definition texts of words that appear in a ten-word window of 1 389 context.</S>
			<S sid="18" ssid="7">The sense of a word with the greatest number of overlaps with senses of other words in the window is chosen as the correct one.</S>
			<S sid="19" ssid="8">For example, consider the definitions of pen and sheep from the Collins English Dictionary, the dictionary used in our experiments, in figure 1.</S>
			<S sid="20" ssid="9">Figure 1: Definitions of PEN, SHEEP, GOAT and PAGE in the Collins English Dictionary pen 1 1.</S>
			<S sid="21" ssid="10">an implement for writing or drawing using ink, formerly consisting of a sharpened and split quill, and now of a metal nib attached to a holder.</S>
			<S sid="22" ssid="11">2.</S>
			<S sid="23" ssid="12">the writing end of such an implement; nib.</S>
			<S sid="24" ssid="13">3.</S>
			<S sid="25" ssid="14">style of writing.</S>
			<S sid="26" ssid="15">4.</S>
			<S sid="27" ssid="16">the pen.</S>
			<S sid="28" ssid="17">a. writing as an occupation, b. the written word.</S>
			<S sid="29" ssid="18">5, the long horny internal shell of a squid.</S>
			<S sid="30" ssid="19">6.</S>
			<S sid="31" ssid="20">to write or compose.</S>
			<S sid="32" ssid="21">pen 2 1.</S>
			<S sid="33" ssid="22">an enclosure in which domestic animals are kept.</S>
			<S sid="34" ssid="23">2.any place of confinement.</S>
			<S sid="35" ssid="24">3.</S>
			<S sid="36" ssid="25">a dock for servicing submarines.</S>
			<S sid="37" ssid="26">4.</S>
			<S sid="38" ssid="27">to enclose or keep in a pen.</S>
			<S sid="39" ssid="28">pen 3 short for penitentiary.</S>
			<S sid="40" ssid="29">pen 4 a female swan.</S>
			<S sid="41" ssid="30">sheep L any of various bovid mammals of the genus O~is and related genera having transversely ribbed horns and a narrow face, There are many breeds of domestic sheep, raised for their wool and for meat.</S>
			<S sid="42" ssid="31">2.</S>
			<S sid="43" ssid="32">:Barbary sheep.</S>
			<S sid="44" ssid="33">3.</S>
			<S sid="45" ssid="34">a meek or timid person.</S>
			<S sid="46" ssid="35">4.</S>
			<S sid="47" ssid="36">separate the sheep from the goats, to pick out the members of any group who are superior in some respects.</S>
			<S sid="48" ssid="37">goat 1.</S>
			<S sid="49" ssid="38">any sure-footed agile bovid mammal of the genus Capra, naturally inhabiting rough stony ground in Europe, Asia, and N Africa, typically having a brown-grey colouring and a beard.</S>
			<S sid="50" ssid="39">Domesticated varieties (C. hircus) are reared for milk, meat, and wool.</S>
			<S sid="51" ssid="40">3.</S>
			<S sid="52" ssid="41">a lecherous man. 4.</S>
			<S sid="53" ssid="42">a bad or inferior member of any group 6.</S>
			<S sid="54" ssid="43">act (or play) the (giddy) goat.</S>
			<S sid="55" ssid="44">to fool around.</S>
			<S sid="56" ssid="45">7.</S>
			<S sid="57" ssid="46">get (someone's) goat.</S>
			<S sid="58" ssid="47">to cause annoyance to (someone) page I 1.</S>
			<S sid="59" ssid="48">one side of one of the leaves of a book, newspaper, letter, etc. or the written or printed matter it bears.</S>
			<S sid="60" ssid="49">2.</S>
			<S sid="61" ssid="50">such a leaf considered as a unit 3.</S>
			<S sid="62" ssid="51">an episode, phase, or period 4.</S>
			<S sid="63" ssid="52">Printing.</S>
			<S sid="64" ssid="53">the type as set up for printing a page.</S>
			<S sid="65" ssid="54">6.</S>
			<S sid="66" ssid="55">to look through (a book, report, etc.); leaf through.</S>
			<S sid="67" ssid="56">page 2 1.</S>
			<S sid="68" ssid="57">a boy employed to run errands, carry messages, etc., for the guests in a hotel, club, etc. 2.</S>
			<S sid="69" ssid="58">a youth in attendance at official functions or ceremonies.</S>
			<S sid="70" ssid="59">3.</S>
			<S sid="71" ssid="60">a. a boy in training for knighthood in personal attendance on a knight, b. a youth in the personal service of a person of rank.</S>
			<S sid="72" ssid="61">4.</S>
			<S sid="73" ssid="62">an attendant at Congress or other legislative body.</S>
			<S sid="74" ssid="63">5.</S>
			<S sid="75" ssid="64">a boy or girl employed in the debating chamber of the house of Commons, the Senate, or a legislative assembly to carry messages for members.</S>
			<S sid="76" ssid="65">6.</S>
			<S sid="77" ssid="66">to call out the name of (a person).</S>
			<S sid="78" ssid="67">7.</S>
			<S sid="79" ssid="68">to call (a person) by an electronic device, such as bleep, g. to act as a page to or attend as a page.</S>
			<S sid="80" ssid="69">If these two words appear together in context, the appropriate senses of pen (2.1: "enclosure") and sheep (1: "mammal") will be chosen because the definitions of these two senses have the word domestic in common.</S>
			<S sid="81" ssid="70">However, with one word as a basis, the relation is tenuous and wholly dependent upon a particular dictionary's wording.</S>
			<S sid="82" ssid="71">The method also fails to take into account less immediate r lationships between words.</S>
			<S sid="83" ssid="72">As a result, it will not determine the correct sense of pen in the context of goat.</S>
			<S sid="84" ssid="73">The correct sense of pen (2.1: enclosure ) and the correct sense of goat (1: mammal ) do not share any words in common in their definitions in the Collins English Dictionary; however, a strategy 390 which takes into account a longer path through definitions will find that animal is in the definition of pen 2.1, each of mammal and animal appear in the definition of the other, and mammal is in the definition of goat 1.</S>
			<S sid="85" ssid="74">Similarly, Lesk's method would also be unable to determine the correct sense of pen (1.1: writing utensil ) in the context of page, because seven of the thirteen senses of pen have the same number of overlaps with senses of page.</S>
			<S sid="86" ssid="75">Six of the senses of pen share only the word write with the correct sense of page (1.1: "leaf of a book").</S>
			<S sid="87" ssid="76">However, pen 1.1 also contains words such as draw and ink, and page 1.1 contains book, newspaper, letter, and print.</S>
			<S sid="88" ssid="77">These other words are heavily interconnected in a complex network which cannot be discovered by simply counting overlaps.</S>
			<S sid="89" ssid="78">Wilks et al (forthcoming) build on Lesk's method by computing the degree of overlap for related word-sets constructed using co-occurrence data from definition texts, but their method suffers from the same problems, in addition to combinatorial problems thai prevent disambiguating more than one word at a time.</S>
			<S sid="90" ssid="79">2.2.</S>
			<S sid="91" ssid="80">Neural networks for WSD.</S>
			<S sid="92" ssid="81">Neural network approaches to WSD have been suggested (Cottrell and Small, 1983; Waltz and Pollack, 1985).</S>
			<S sid="93" ssid="82">These models consist of networks in which the nodes ("neurons") represent words or concepts, connected by "activatory" links: the words activate the concepts to which they are semantically related, and vice versa.</S>
			<S sid="94" ssid="83">In addition, "lateral" inhibitory links usually interconnect competing senses of a given word.</S>
			<S sid="95" ssid="84">Initially, the nodes corresponding tothe words in the sentence to be analyzed are activated.</S>
			<S sid="96" ssid="85">These words activate their neighbors in the next cycle in turn, these neighbors activate their immediate neighbors, and so on.</S>
			<S sid="97" ssid="86">After a number of cycles, the network stabilizes in a state in which one sense for each input word is more activated than the others, using a parallel, analog, relaxation process.</S>
			<S sid="98" ssid="87">Neural network approaches to WSD seem able to capture most of what cannot be handled by overlap strategies such as Lesk's.</S>
			<S sid="99" ssid="88">However, the networks used in experiments o far are hand-coded and thus necessarily very small (at most, a few dozen words and concepts).</S>
			<S sid="100" ssid="89">Due to a lack of real-size data, it is not clear that he same neural net models will scale up for realistic application.</S>
			<S sid="101" ssid="90">Further, some approaches rely on "context- setting" nodes to prime particular word senses in order to force 1the correct interpretation?</S>
			<S sid="102" ssid="91">But as Waltz and Pollack point out, it is possible that such words (e.g., writing in the context of pen ) are not explicitly present in the text under analysis, but may be inferred by the reader from the presence of other, related words (e.g., page, book, inkwell, etc.).</S>
			<S sid="103" ssid="92">To solve this problem, words in such networks have been represented by sets of semantic "microfeatures" (Waltz and Pollack, 1985; Bookman, 1987) which correspond to fundamental semantic distinctions (animate/inanimate, edible/ inedible, threatening/safe, etc.), characteristic duration of events (second, minute, hour, day, etc.), locations (city, country, continent, etc.), and other similar distinctions that humans typically make about situations in the world.</S>
			<S sid="104" ssid="93">To be comprehensive, the authors uggest that these features must number in the thousands.</S>
			<S sid="105" ssid="94">Each concept iin the network is linked, via bidirectional activatory or inhibitory links, to only a subset of the complete microfeature s t. A given concept theoretically shares everal microfeatures with concepts to which it is closely related, and will therefore activate the nodes corresponding to closely related concepts when it is activated :itself.</S>
			<S sid="106" ssid="95">ttowever, such schemes are problematic due to the difficulties of designing an appropriate set of microfeatures, which in essence consists of designing semantic primitives.</S>
			<S sid="107" ssid="96">This becomes clear when one exmnines the sample microfeatures given by Waltz ~md Pollack: they specify micro.f carfares uch as CASINO and CANYON, but it is obviously questionable whether such concepts constitute fundamental semantic distinctions.</S>
			<S sid="108" ssid="97">More practically, it is simply difficult to imagine how vectors of several thousands of microfeamrcs for each one of the lens of thousands of words and hundreds of thousands of senses can be realistically encoded by hand.</S>
	</SECTION>
	<SECTION title="Word sense disambiguation with VLNNs. " number="3">
			<S sid="109" ssid="1">Our approach to WSD takes advantage of both strategies outlined above, but enables us to address solutions to their shortcomings.</S>
			<S sid="110" ssid="2">This work has been carried out in tile context of a joint project of Vassar College and the Groupe Reprdsentation et Traitement des Connaissances of the Centre National de la Recherche Scientifique (CNRS), which is concerned with the construction and exploitation of a large lexical data base of English and French.</S>
			<S sid="111" ssid="3">At present, the Vassar/CNRS data base includes, through the courtesy of several editors and research institutions, several English and French dictionaries (the Collins English Dictionary, the Oxford Advanced Learner's Dictionary, the COBUILD Dictionary, the Longman) Dictionary of Contemporary English, theWebster's 9th Dictionary, and the ZYZOMYS CD-ROM dictionary from Hachette Publishers) as well as several other lexical and textual materials (the Brown Corpus of American English, the CNRS BDLex data base, the MRC Psycholinguistic Data Base, etc.).</S>
			<S sid="112" ssid="4">We build VLNNs utilizing definitions in the Collins English Dictionary.</S>
			<S sid="113" ssid="5">Like Lesk and Wilks, we assume that there are significant semantic relations between a word and the words used to define it.</S>
			<S sid="114" ssid="6">The connections in the network reflect these relations.</S>
			<S sid="115" ssid="7">All of the knowledge represented in the network is automatically generated from a machine-readable dictionary, and therefore no hand coding is required.</S>
			<S sid="116" ssid="8">Further, the lexicon m~d the knowledge it contains potentially cover all of English (90,000 words), and as a result this information cml potentially be used to help dismnbiguate unrestricted text.</S>
			<S sid="117" ssid="9">3.1.</S>
			<S sid="118" ssid="10">Topology of the network.</S>
			<S sid="119" ssid="11">In our model, words are complex units.</S>
			<S sid="120" ssid="12">Each word in the input is represented by a word node connected by excitatory links to sense nodes (figure 2) representing the different possible senses tbr that word in the Collins English Dictionary.</S>
			<S sid="121" ssid="13">Each sense node is in turn connected by excitatory links to word nodes rcpreseming the words in tile definition of that sense.</S>
			<S sid="122" ssid="14">This process is repeated a number of times, creating an increasingly complex and interconnected network.</S>
			<S sid="123" ssid="15">Ideally, the network would include the entire dictionary, but for practical reasons we limit the number of repetitions and thus restrict tile size of the network to a few thousand nodes and 10 to 20 thousand transitions.</S>
			<S sid="124" ssid="16">All words in the network are reduced to their lemmas, and grammatical words are excluded.</S>
			<S sid="125" ssid="17">The different sense nodes tor a given word are interconnected by lateral inhibitory links.</S>
			<S sid="126" ssid="18">3 391 Figure 2.</S>
			<S sid="127" ssid="19">Topology of the network ~.</S>
			<S sid="128" ssid="20">, : ' .i \ [ ~ Word Node Sense Node ~ . Excitatory Link ..........................</S>
			<S sid="129" ssid="21">Inhibitory Link When the network is run, the input word nodes are activated first.</S>
			<S sid="130" ssid="22">Then each input word node sends activation to its sense nodes, which in turn send activation to the word nodes to which they are connected, and so on throughout he network for a number of cycles.</S>
			<S sid="131" ssid="23">At each cycle, word and sense nodes receive feedback from connected nodes.</S>
			<S sid="132" ssid="24">Competing sense nodes send inhibition to one another.</S>
			<S sid="133" ssid="25">Feedback and inhibition cooperate in a "winner-take-all" strategy to activate increasingly related word and sense nodes and deactivate the unrelated or weakly related nodes.</S>
			<S sid="134" ssid="26">Eventually, after a few dozen cycles, the network stabilizes in a configuration where only the sense nodes with the strongest relations to other nodes in the network are activated.</S>
			<S sid="135" ssid="27">Because of the "winner-take-all" strategy, at most one sense node per word will ultimately be activated.</S>
			<S sid="136" ssid="28">Our model does not use microfeatures, because, as we will show below, the context is taken into account by the number of nodes in the network and the extent to which they are heavily interconnected.</S>
			<S sid="137" ssid="29">So far, we do not consider the syntax of the input sentence, in order to locus on the semantic properties of the model.</S>
			<S sid="138" ssid="30">However, it is clear that syntactic information can assist in the disambiguation process in certain cases, and a network including a syntactic layer, such as that proposed by Waltz and Pol lack, would undoubtedly enhance the model's behavior.</S>
			<S sid="139" ssid="31">3.2.</S>
			<S sid="140" ssid="32">Results.</S>
			<S sid="141" ssid="33">The network finds the correct sense in cases where Lesk's strategy succeeds.</S>
			<S sid="142" ssid="34">For example, if the input consists of pen and sheep, pen 2.1 and sheep 1 are correct ly act ivated.</S>
			<S sid="143" ssid="35">More interestingly, the network selects " the appropriate senses in cases where Lesk's strategy fails.</S>
			<S sid="144" ssid="36">Figures 3 and 4 show the state of the network after being run with pen and goat, and pen and page, respectively.</S>
			<S sid="145" ssid="37">The figures represent only the most activated part of each network after 100 cycles.</S>
			<S sid="146" ssid="38">Over the course of the run, the network reinforces only a small cluster of the most semantically relevant words and senses, and filters out tile rest of the thousands of nodes.</S>
			<S sid="147" ssid="39">The correct sense for each word in each context (pen 2.1 with goat 1, and pen 1.1 withpage 1.1) is the only one activated at the end of the run.</S>
			<S sid="148" ssid="40">This model solves the context-setting problem mentioned above without any use of microfeatures.</S>
			<S sid="149" ssid="41">Sense 1.1 of pen would also be activated if it appeared in the context of a large number of other words--e.g., book, ink, inkwell, pencil, paper, write, draw, sketch, etc.--which ave a similar semantic relationship to pen.</S>
			<S sid="150" ssid="42">For example, figure 5 shows the state of the network after being run with pen and book.</S>
			<S sid="151" ssid="43">It is apparent that the subset of nodes activated is similar to those which were activated by page.</S>
			<S sid="152" ssid="44">392 4 Figure 3.</S>
			<S sid="153" ssid="45">State of the network after being run with "pen" and "goat" \[ are the most activated } Figure 4.</S>
			<S sid="154" ssid="46">State of the network after being run with "pen" and "page" ~ \[ The darker nodes \] Figure 5.</S>
			<S sid="155" ssid="47">State of the network after being run with "pen" and "book" r The darker nodes \] ~ ~ , ook 393 The examples given here utilize only two words as input, in order to show clearly the behavior of the network.</S>
			<S sid="156" ssid="48">In fact, the performance of the network improves with additional input, since additional context can only contribute more to the disambiguation process.</S>
			<S sid="157" ssid="49">For example, given the sentence The young page put the sheep in the pen, the network correctly chooses the correct senses of page (2.3: "a youth in personal service"), sheep (1), and pen (2.1).</S>
			<S sid="158" ssid="50">This example is particularly difficult, because page and sheep compete against each other to activate different senses of pen, as demonstrated in the examples above.</S>
			<S sid="159" ssid="51">However, the word young reinforces sense 2.3 of page, which enables sheep to win the struggle.</S>
			<S sid="160" ssid="52">Inter-sentential context could be used as well, by retaining the most activated nodes within the network during subsequent runs.</S>
			<S sid="161" ssid="53">By running various experiments on VLNNs, we have discovered that when the simple models proposed so far are scaled up, several improvements are necessary.</S>
			<S sid="162" ssid="54">We have, for instance, discovered that "gang effects" appear due to extreme imbalance among words having few senses and hence few connections, and words containing up to 80 senses and several hundred connections, and that therefore dampening is required.</S>
			<S sid="163" ssid="55">tn addition, we have found that is is necessary to treat a word node and its sense nodes as a complex, ecological unit rather than as separate ntities.</S>
			<S sid="164" ssid="56">In our model, word nodes corttrol the behavior of sense nodes by means of a differential neuron that prevents, for example, a sense node from becoming more activated than its master word node.</S>
			<S sid="165" ssid="57">Our experimentation with VLNNs has also shed light on the role of and need for various other parameters, uch as thresholds, decay, etc.</S>
	</SECTION>
	<SECTION title="Conclus ion. " number="4">
			<S sid="166" ssid="1">The use of word relations implicitly encoded in machine-readable dictionaries, coupled with the neural network strategy, seems to offer a promising approach to WSD.</S>
			<S sid="167" ssid="2">This approach succeeds where the Lesk strategy fails, and it does not require determining and encoding microfeatures or other semantic information.</S>
			<S sid="168" ssid="3">The model is also more robust than the Lesk strategy, since it does not rely on the presence or absence of a particular word or words and can filter out some degree of "noise" (such as inclusion of some wrong lemmas due to lack of information about part-of-speech or occasional activation of misleading homographs).</S>
			<S sid="169" ssid="4">How- ever, there are clearly several improvements which can be made: for instance, the part-of-speech for input words and words in definitions can be used to extract only the correct lemmas from the dictionary, the frequency of use for particular senses of each word can be used to help choose among competing senses, and additional knowledge can be extracted from other dictionaries and thesauri.</S>
			<S sid="170" ssid="5">It is also conceivable that the network could "learn" by giving more weight to links which have been heavily activated over numerous runs on large samples of text.</S>
			<S sid="171" ssid="6">The model we describe here is only a first step toward a fuller understanding and refinement of the use of VLNNs for language processing, and it opens several interesting avenues for further application and research.</S>
	</SECTION>
</PAPER>
