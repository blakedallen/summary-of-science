<PAPER>
  <S sid="0">Scaling Web-Based Acquisition Of Entailment Relations</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Paraphrase recognition is a critical step for natural language interpretation.</S>
    <S sid="2" ssid="2">Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases.</S>
    <S sid="3" ssid="3">However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited.</S>
    <S sid="4" ssid="4">We present a fully unsupervised learning algorithm for Web-based extraction an extended model of paraphrases.</S>
    <S sid="5" ssid="5">We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base.</S>
    <S sid="6" ssid="6">Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates.</S>
    <S sid="7" ssid="7">Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="8" ssid="1">Modeling semantic variability in language has drawn a lot of attention in recent years.</S>
    <S sid="9" ssid="2">Many applications like QA, IR, IE and Machine Translation (Moldovan and Rus, 2001; Hermjakob et al., 2003; Jacquemin, 1999) have to recognize that the same meaning can be expressed in the text in a huge variety of surface forms.</S>
    <S sid="10" ssid="3">Substantial research has been dedicated to acquiring paraphrase patterns, which represent various forms in which a certain meaning can be expressed.</S>
    <S sid="11" ssid="4">Following (Dagan and Glickman, 2004) we observe that a somewhat more general notion needed for applications is that of entailment relations (e.g.</S>
    <S sid="12" ssid="5">(Moldovan and Rus, 2001)).</S>
    <S sid="13" ssid="6">These are directional relations between two expressions, where the meaning of one can be entailed from the meaning of the other.</S>
    <S sid="14" ssid="7">For example &#8220;X acquired Y&#8221; entails &#8220;X owns Y&#8221;.</S>
    <S sid="15" ssid="8">These relations provide a broad framework for representing and recognizing semantic variability, as proposed in (Dagan and Glickman, 2004).</S>
    <S sid="16" ssid="9">For example, if a QA system has to answer the question &#8220;Who owns Overture?&#8221; and the corpus includes the phrase &#8220;Yahoo acquired Overture&#8221;, the system can use the known entailment relation to conclude that this phrase really indicates the desired answer.</S>
    <S sid="17" ssid="10">More examples of entailment relations, acquired by our method, can be found in Table 1 (section 4).</S>
    <S sid="18" ssid="11">To perform such inferences at a broad scale, applications need to possess a large knowledge base (KB) of entailment patterns.</S>
    <S sid="19" ssid="12">We estimate such a KB should contain from between a handful to a few dozens of relations per meaning, which may sum to a few hundred thousands of relations for a broad domain, given that a typical lexicon includes tens of thousands of words.</S>
    <S sid="20" ssid="13">Our research goal is to approach unsupervised acquisition of such a full scale KB.</S>
    <S sid="21" ssid="14">We focus on developing methods that acquire entailment relations from the Web, the largest available resource.</S>
    <S sid="22" ssid="15">To this end substantial improvements are needed in order to promote scalability relative to current Webbased approaches.</S>
    <S sid="23" ssid="16">In particular, we address two major goals: reducing dramatically the complexity of required auxiliary inputs, thus enabling to apply the methods at larger scales, and generalizing the types of structures that can be acquired.</S>
    <S sid="24" ssid="17">The algorithms described in this paper were applied for acquiring entailment relations for verb-based expressions.</S>
    <S sid="25" ssid="18">They successfully discovered several relations on average per each randomly selected expression.</S>
  </SECTION>
  <SECTION title="2 Background and Motivations" number="2">
    <S sid="26" ssid="1">This section provides a qualitative view of prior work, emphasizing the perspective of aiming at a full-scale paraphrase resource.</S>
    <S sid="27" ssid="2">As there are still no standard benchmarks, current quantitative results are not comparable in a consistent way.</S>
    <S sid="28" ssid="3">The major idea in paraphrase acquisition is often to find linguistic structures, here termed templates, that share the same anchors.</S>
    <S sid="29" ssid="4">Anchors are lexical elements describing the context of a sentence.</S>
    <S sid="30" ssid="5">Templates that are extracted from different sentences and connect the same anchors in these sentences, are assumed to paraphrase each other.</S>
    <S sid="31" ssid="6">For example, the sentences &#8220;Yahoo bought Overture&#8221; and &#8220;Yahoo acquired Overture&#8221; share the anchors {X=Yahoo, Y=Overture}, suggesting that the templates &#8216;X buy Y&#8217; and &#8216;X acquire Y&#8217; paraphrase each other.</S>
    <S sid="32" ssid="7">Algorithms for paraphrase acquisition address two problems: (a) finding matching anchors and (b) identifying template structure, as reviewed in the next two subsections.</S>
    <S sid="33" ssid="8">The prominent approach for paraphrase learning searches sentences that share common sets of multiple anchors, assuming they describe roughly the same fact or event.</S>
    <S sid="34" ssid="9">To facilitate finding many matching sentences, highly redundant comparable corpora have been used.</S>
    <S sid="35" ssid="10">These include multiple translations of the same text (Barzilay and McKeown, 2001) and corresponding articles from multiple news sources (Shinyama et al., 2002; Pang et al., 2003; Barzilay and Lee, 2003).</S>
    <S sid="36" ssid="11">While facilitating accuracy, we assume that comparable corpora cannot be a sole resource due to their limited availability.</S>
    <S sid="37" ssid="12">Avoiding a comparable corpus, (Glickman and Dagan, 2003) developed statistical methods that match verb paraphrases within a regular corpus.</S>
    <S sid="38" ssid="13">Their limited scale results, obtaining several hundred verb paraphrases from a 15 million word corpus, suggest that much larger corpora are required.</S>
    <S sid="39" ssid="14">Naturally, the largest available corpus is the Web.</S>
    <S sid="40" ssid="15">Since exhaustive processing of the Web is not feasible, (Duclaye et al., 2002) and (Ravichandran and Hovy, 2002) attempted bootstrapping approaches, which resemble the mutual bootstrapping method for Information Extraction of (Riloff and Jones, 1999).</S>
    <S sid="41" ssid="16">These methods start with a provided known set of anchors for a target meaning.</S>
    <S sid="42" ssid="17">For example, the known anchor set {Mozart, 1756} is given as input in order to find paraphrases for the template &#8216;X born in Y&#8217;.</S>
    <S sid="43" ssid="18">Web searching is then used to find occurrences of the input anchor set, resulting in new templates that are supposed to specify the same relation as the original one (&#8220;born in&#8221;).</S>
    <S sid="44" ssid="19">These new templates are then exploited to get new anchor sets, which are subsequently processed as the initial {Mozart, 1756}.</S>
    <S sid="45" ssid="20">Eventually, the overall procedure results in an iterative process able to induce templates from anchor sets and vice versa.</S>
    <S sid="46" ssid="21">The limitation of this approach is the requirement for one input anchor set per target meaning.</S>
    <S sid="47" ssid="22">Preparing such input for all possible meanings in broad domains would be a huge task.</S>
    <S sid="48" ssid="23">As will be explained below, our method avoids this limitation by finding all anchor sets automatically in an unsupervised manner.</S>
    <S sid="49" ssid="24">Finally, (Lin and Pantel, 2001) present a notably different approach that relies on matching separately single anchors.</S>
    <S sid="50" ssid="25">They limit the allowed structure of templates only to paths in dependency parses connecting two anchors.</S>
    <S sid="51" ssid="26">The algorithm constructs for each possible template two feature vectors, representing its co-occurrence statistics with the two anchors.</S>
    <S sid="52" ssid="27">Two templates with similar vectors are suggested as paraphrases (termed inference rule).</S>
    <S sid="53" ssid="28">Matching of single anchors relies on the general distributional similarity principle and unlike the other methods does not require redundancy of sets of multiple anchors.</S>
    <S sid="54" ssid="29">Consequently, a much larger number of paraphrases can be found in a regular corpus.</S>
    <S sid="55" ssid="30">Lin and Pantel report experiments for 9 templates, in which their system extracted 10 correct inference rules on average per input template, from 1GB of news data.</S>
    <S sid="56" ssid="31">Yet, this method also suffers from certain limitations: (a) it identifies only templates with pre-specified structures; (b) accuracy seems more limited, due to the weaker notion of similarity; and (c) coverage is limited to the scope of an available corpus.</S>
    <S sid="57" ssid="32">To conclude, several approaches exhaustively process different types of corpora, obtaining varying scales of output.</S>
    <S sid="58" ssid="33">On the other hand, the Web is a huge promising resource, but current Web-based methods suffer serious scalability constraints.</S>
    <S sid="59" ssid="34">Paraphrasing approaches learn different kinds of template structures.</S>
    <S sid="60" ssid="35">Interesting algorithms are presented in (Pang et al., 2003; Barzilay and Lee, 2003).</S>
    <S sid="61" ssid="36">They learn linear patterns within similar contexts represented as finite state automata.</S>
    <S sid="62" ssid="37">Three classes of syntactic template learning approaches are presented in the literature: learning ofpredicate argument templates (Yangarber et al., 2000), learning of syntactic chains (Lin and Pantel, 2001) and learning of sub-trees (Sudo et al., 2003).</S>
    <S sid="63" ssid="38">The last approach is the most general with respect to the template form.</S>
    <S sid="64" ssid="39">However, its processing time increases exponentially with the size of the templates.</S>
    <S sid="65" ssid="40">As a conclusion, state of the art approaches still learn templates of limited form and size, thus restricting generality of the learning process.</S>
  </SECTION>
  <SECTION title="3 The TE/ASE Acquisition Method" number="3">
    <S sid="66" ssid="1">Motivated by prior experience, we identify two major goals for scaling Web-based acquisition of entailment relations: (a) Covering the broadest possible range of meanings, while requiring minimal input and (b) Keeping template structures as general as possible.</S>
    <S sid="67" ssid="2">To address the first goal we require as input only a phrasal lexicon of the relevant domain (including single words and multiword expressions).</S>
    <S sid="68" ssid="3">Broad coverage lexicons are widely available or may be constructed using known term acquisition techniques, making it a feasible and scalable input requirement.</S>
    <S sid="69" ssid="4">We then aim to acquire entailment relations that include any of the lexicon&#8217;s entries.</S>
    <S sid="70" ssid="5">The second goal is addressed by a novel algorithm for extracting the most general templates being justified by the data.</S>
    <S sid="71" ssid="6">For each lexicon entry, denoted a pivot, our extraction method performs two phases: (a) extract promising anchor sets for that pivot (ASE, Section 3.1), and (b) from sentences containing the anchor sets, extract templates for which an entailment relation holds with the pivot (TE, Section 3.2).</S>
    <S sid="72" ssid="7">Examples for verb pivots are: &#8216;acquire&#8217;, &#8216;fall to&#8217;, &#8216;prevent&#8217;.</S>
    <S sid="73" ssid="8">We will use the pivot &#8216;prevent&#8217; for examples through this section.</S>
    <S sid="74" ssid="9">Before presenting the acquisition method we first define its output.</S>
    <S sid="75" ssid="10">A template is a dependency parsetree fragment, with variable slots at some tree nodes (e.g.</S>
    <S sid="76" ssid="11">&#8216;X s+_ prevent &#65533; Y&#8217;).</S>
    <S sid="77" ssid="12">An entailment relation between two templates T1 and T2 holds if the meaning of T2 can be inferred from the meaning of T1 (or vice versa) in some contexts, but not necessarily all, under the same variable instantiation.</S>
    <S sid="78" ssid="13">For example, &#8216;X s+ prevent 0* Y&#8217; entails &#8216;X s+_ reduce -* Y risk&#8217; because the sentence &#8220;aspirin reduces heart attack risk&#8221; can be inferred from &#8220;aspirin prevents a first heart attack&#8221;.</S>
    <S sid="79" ssid="14">Our output consists of pairs of templates for which an entailment relation holds.</S>
    <S sid="80" ssid="15">The goal of this phase is to find a substantial number of promising anchor sets for each pivot.</S>
    <S sid="81" ssid="16">A good anchor-set should satisfy a proper balance between specificity and generality.</S>
    <S sid="82" ssid="17">On one hand, an anchor set should correspond to a sufficiently specific setting, so that entailment would hold between its different occurrences.</S>
    <S sid="83" ssid="18">On the other hand, it should be sufficiently frequent to appear with different entailing templates.</S>
    <S sid="84" ssid="19">Finding good anchor sets based on just the input pivot is a hard task.</S>
    <S sid="85" ssid="20">Most methods identify good repeated anchors &#8220;in retrospect&#8221;, that is after processing a full corpus, while previous Web-based methods require at least one good anchor set as input.</S>
    <S sid="86" ssid="21">Given our minimal input, we needed refined criteria that identify a priori the relatively few promising anchor sets within a sample of pivot occurrences.</S>
    <S sid="87" ssid="22">The ASE algorithm (presented in Figure 1) performs 4 main steps.</S>
    <S sid="88" ssid="23">STEP (1) creates a complete template, called the pivot template and denoted Tp, for the input pivot, denoted P. Variable slots are added for the major types of syntactic relations that interact with P, based on its syntactic type.</S>
    <S sid="89" ssid="24">These slots enable us to later match Tp with other templates.</S>
    <S sid="90" ssid="25">For verbs, we add slots for a subject and for an object or a modifier (e.g.</S>
    <S sid="91" ssid="26">&#8216;X s+_ prevent &#65533; Y&#8217;).</S>
    <S sid="92" ssid="27">STEP (2) constructs asample corpus, denoted S, for the pivot template.</S>
    <S sid="93" ssid="28">STEP (2.A) utilizes a Web search engine to initialize S by retrieving sentences containing P. The sentences are parsed by the MINIPAR dependency parser (Lin, 1998), keeping only sentences that contain the complete syntactic template Tp (with all the variables instantiated).</S>
    <S sid="94" ssid="29">STEP (2.B) identifies phrases that are statistically associated with Tp in S. We test all noun-phrases in S , discarding phrases that are too common on the Web (absolute frequency higher than a threshold MAXPHRASEF), such as &#8220;desire&#8221;.</S>
    <S sid="95" ssid="30">Then we select the N phrases with highest tf &#183;idf score1.</S>
    <S sid="96" ssid="31">These phrases have a strong collocation relationship with the pivot P and are likely to indicate topical (rather than anecdotal) occurrences of P. For example, the phrases &#8220;patient&#8221; and &#8220;American Dental Association&#8221;, which indicate contexts of preventing health problems, were selected for the pivot &#8216;prevent&#8217;.</S>
    <S sid="97" ssid="32">Fi1Here, tf &#183;idf = freqS(X) &#183; log (freqN (X) ) where freqS(X) is the number of occurrences in S containing X, N is the total number of Web documents, and freqW (X) is the number of Web documents containing X. nally, STEP (2.C) expands S by querying the Web with the both P and each of the associated phrases, adding the retrieved sentences to S as in step (2.a).</S>
    <S sid="98" ssid="33">STEP (3) extracts candidate anchor sets for Tp.</S>
    <S sid="99" ssid="34">From each sentence in S we try to generate one candidate set, containing noun phrases whose Web frequency is lower than MAXPHRASEF.</S>
    <S sid="100" ssid="35">STEP (3.A) extracts slot anchors &#8211; phrases that instantiate the slot variables of Tp.</S>
    <S sid="101" ssid="36">Each anchor is marked with the corresponding slot.</S>
    <S sid="102" ssid="37">For example, the anchors {antibioticssubj&#8592; , miscarriage obj&#8592;} were extracted from the sentence &#8220;antibiotics in pregnancy prevent miscarriage&#8221;.</S>
    <S sid="103" ssid="38">STEP (3.B) tries to extend each candidate set with one additional context anchor, in order to improve its specificity.</S>
    <S sid="104" ssid="39">This anchor is chosen as the highest tf &#183;idf scoring phrase in the sentence, if it exists.</S>
    <S sid="105" ssid="40">In the previous example, &#8216;pregnancy&#8217; is selected.</S>
    <S sid="106" ssid="41">STEP (4) filters out bad candidate anchor sets by two different criteria.</S>
    <S sid="107" ssid="42">STEP (4.A) maintains only candidates with absolute Web frequency within a threshold range [MINSETF, MAXSETF], to guarantee an appropriate specificity-generality level.</S>
    <S sid="108" ssid="43">STEP (4.B) guarantees sufficient (directional) association between the candidate anchor set c and Tp, by estimating where freqW is Web frequency and P is the pivot.</S>
    <S sid="109" ssid="44">We maintain only candidates for which this probability falls within a threshold range [SETMINP, SETMAXP].</S>
    <S sid="110" ssid="45">Higher probability often corresponds to a strong linguistic collocation between the candidate and Tp, without any semantic entailment.</S>
    <S sid="111" ssid="46">Lower probability indicates coincidental cooccurrence, without a consistent semantic relation.</S>
    <S sid="112" ssid="47">The remaining candidates in S become the input anchor-sets for the template extraction phase, for example, {Aspirinsubj&#8592; , heart attackobj&#8592;} for &#8216;prevent&#8217;.</S>
    <S sid="113" ssid="48">The Template Extraction algorithm accepts as its input a list of anchor sets extracted from ASE for each pivot template.</S>
    <S sid="114" ssid="49">Then, TE generates a set of syntactic templates which are supposed to maintain an entailment relationship with the initial pivot template.</S>
    <S sid="115" ssid="50">TE performs three main steps, described in the following subsections: For each input anchor set, TE acquires from the Web a sample corpus of sentences containing it.</S>
    <S sid="116" ssid="51">For example, a sentence from the sample corpus for {aspirin, heart attack} is: &#8220;Aspirin stops heart attack?&#8221;.</S>
    <S sid="117" ssid="52">All of the sample sentences are then parsed with MINIPAR (Lin, 1998), which generates from each sentence a syntactic directed acyclic graph (DAG) representing the dependency structure of the sentence.</S>
    <S sid="118" ssid="53">Each vertex in this graph is labeled with a word and some morphological information; each graph edge is labeled with the syntactic relation between the words it connects.</S>
    <S sid="119" ssid="54">TE then substitutes each slot anchor (see section 3.1) in the parse graphs with its corresponding slot variable.</S>
    <S sid="120" ssid="55">Therefore, &#8220;Aspirin stops heart attack?&#8221; will be transformed into &#8216;X stop Y&#8217;.</S>
    <S sid="121" ssid="56">This way all the anchors for a certain slot are unified under the same variable name in all sentences.</S>
    <S sid="122" ssid="57">The parsed sentences related to all of the anchor sets are subsequently merged into a single set of parse graphs S = {P1, P2, ... , Pn} (see P1 and P2 in Figure 2).</S>
    <S sid="123" ssid="58">The core of TE is a General Structure Learning algorithm (GSL) that is applied to the set of parse graphs S resulting from the previous step.</S>
    <S sid="124" ssid="59">GSL extracts single-rooted syntactic DAGs, which are named spanning templates since they must span at least over Na slot variables, and should also appear in at least Nr sentences from S (In our experiments we set Na=2 and Nr=2).</S>
    <S sid="125" ssid="60">GSL learns maximal most general templates: they are spanning templates which, at the same time, (a) cannot be generalized by further reduction and (b) cannot be further extended keeping the same generality level.</S>
    <S sid="126" ssid="61">In order to properly define the notion of maximal most general templates, we introduce some formal definitions and notations.</S>
    <S sid="127" ssid="62">DEFINITION: For a spanning template t we define a sentence set, denoted with &#963;(t), as the set of all parsed sentences in S containing t. For each pair of templates t1 and t2, we use the notation t1 :&#65533; t2 to denote that t1 is included as a subgraph or is equal to t2.</S>
    <S sid="128" ssid="63">We use the notation t1 &#8826; t2 when such inclusion holds strictly.</S>
    <S sid="129" ssid="64">We define T(S) as the set of all spanning templates in the sample S. DEFINITION: A spanning template t E T (S) is maximal most general if and only if both of the following conditions hold: Condition A ensures that the extracted templates do not contain spanning sub-structures that are more &#8221;general&#8221; (i.e. having a larger sentence set); condition B ensures that the template cannot be further enlarged without reducing its sentence set.</S>
    <S sid="130" ssid="65">GSL performs template extraction in two main steps: (1) build a compact graph representation of all the parse graphs from S; (2) extract templates from the compact representation.</S>
    <S sid="131" ssid="66">A compact graph representation is an aggregate graph which joins all the sentence graphs from S ensuring that all identical spanning sub-structures from different sentences are merged into a single one.</S>
    <S sid="132" ssid="67">Therefore, each vertex v (respectively, edge e) in the aggregate graph is either a copy of a corresponding vertex (edge) from a sentence graph Pi or it represents the merging of several identically labeled vertices (edges) from different sentences in S. The set of such sentences is defined as the sentence set of v (e), and is represented through the set of index numbers of related sentences (e.g.</S>
    <S sid="133" ssid="68">&#8220;(1,2)&#8221; in the third tree of Figure 2).</S>
    <S sid="134" ssid="69">We will denote with Gi the compact graph representation of the first i sentences in S. The parse trees P1 and P2 of two sentences and their related compact representation G2 are shown in Figure 2.</S>
    <S sid="135" ssid="70">Building the compact graph representation The compact graph representation is built incrementally.</S>
    <S sid="136" ssid="71">The algorithm starts with an empty aggregate graph G0 and then merges the sentence graphs from S one at a time into the aggregate structure.</S>
    <S sid="137" ssid="72">Let&#8217;s denote the current aggregate graph with Gi_1(Vg, Eg) and let Pi(Vp, Ep) be the parse graph which will be merged next.</S>
    <S sid="138" ssid="73">Note that the sentence set of Pi is a single element set W. During each iteration a new graph is created as the union of both input graphs: Gi = Gi_1 U Pi.</S>
    <S sid="139" ssid="74">Then, the following merging procedure is performed on the elements of Gi ated and added to Gi.</S>
    <S sid="140" ssid="75">The new vertex takes the same label and holds a sentence set which is formed from the sentence set of vg by adding i to it.</S>
    <S sid="141" ssid="76">Still with reference to Figure 2, the generalized vertices in G2 are &#8216;X&#8217;, &#8216;Y&#8217; and &#8216;stop&#8217;.</S>
    <S sid="142" ssid="77">The algorithm connects the generalized vertex vnew g with all the vertices which are connected with vg and vp.</S>
    <S sid="143" ssid="78">As an optimization step, we merge only vertices and edges that are included in equal spanning templates.</S>
    <S sid="144" ssid="79">Extracting the templates GSL extracts all maximal most general templates from the final compact representation Gn using the following sub-algorithm: In Figure 2 the maximal most general template in obj As a last step, names and numbers are filtered out from the templates.</S>
    <S sid="145" ssid="80">Moreover, TE removes those templates which are very long or which appear with just one anchor set and in less than four sentences.</S>
    <S sid="146" ssid="81">Finally, the templates are sorted first by the number of anchor sets with which each template appeared, and then by the number of sentences in which they appeared.</S>
  </SECTION>
  <SECTION title="4 Evaluation" number="4">
    <S sid="147" ssid="1">We evaluated the results of the TE/ASE algorithm on a random lexicon of verbal forms and then assessed its performance on the extracted data through human-based judgments.</S>
    <S sid="148" ssid="2">The test set for human evaluation was generated by picking out 53 random verbs from the 1000 most frequent ones found in a subset of the Reuters corpus2.</S>
    <S sid="149" ssid="3">For each verb entry in the lexicon, we provided the judges with the corresponding pivot template and the list of related candidate entailment templates found by the system.</S>
    <S sid="150" ssid="4">The judges were asked to evaluate entailment for a total of 752 templates, extracted for 53 pivot lexicon entries; Table 1 shows a sample of the evaluated templates; all of them are clearly good and were judged as correct ones. included in the evaluation test set.</S>
    <S sid="151" ssid="5">Concerning the ASE algorithm, threshold parameters3 were set as PHRASEMAXF=107, SETMINF=102, SETMAXF=105, SETMINP=0.066, and SETMAXP=0.666.</S>
    <S sid="152" ssid="6">An upper limit of 30 was imposed on the number of possible anchor sets used for each pivot.</S>
    <S sid="153" ssid="7">Since this last value turned out to be very conservative with respect to system coverage, we subsequently attempted to relax it to 50 (see Discussion in Section 4.3).</S>
    <S sid="154" ssid="8">Further post-processing was necessary over extracted data in order to remove syntactic variations referring to the same candidate template (typically passive/active variations).</S>
    <S sid="155" ssid="9">Three possible judgment categories have been considered: Correct if an entailment relationship in at least one direction holds between the judged template and the pivot template in some non-bizarre context; Incorrect if there is no reasonable context and variable instantiation in which entailment holds; No Evaluation if the judge cannot come to a definite conclusion.</S>
    <S sid="156" ssid="10">Each of the three assessors (referred to as J#1, J#2, and J#3) issued judgments for the 752 different templates.</S>
    <S sid="157" ssid="11">Correct templates resulted to be 283, 313, and 295 with respect to the three judges.</S>
    <S sid="158" ssid="12">No evaluation&#8217;s were 2, 0, and 16, while the remaining templates were judged Incorrect.</S>
    <S sid="159" ssid="13">For each verb, we calculate Yield as the absolute number of Correct templates found and Precision as the percentage of good templates out of all extracted templates.</S>
    <S sid="160" ssid="14">Obtained Precision is 44.15%, averaged over the 53 verbs and the 3 judges.</S>
    <S sid="161" ssid="15">Considering Low Majority on judges, the precision value is 42.39%.</S>
    <S sid="162" ssid="16">Average Yield was 5.5 templates per verb.</S>
    <S sid="163" ssid="17">These figures may be compared (informally, as data is incomparable) with average yield of 10.1 and average precision of 50.3% for the 9 &#8220;pivot&#8221; templates of (Lin and Pantel, 2001).</S>
    <S sid="164" ssid="18">The comparison suggests that it is possible to obtain from the (very noisy) web a similar range of precision as was obtained from a clean news corpus.</S>
    <S sid="165" ssid="19">It also indicates that there is potential for acquiring additional templates per pivot, which would require further research on broadening efficiently the search for additional web data per pivot.</S>
    <S sid="166" ssid="20">Agreement among judges is measured by the Kappa value, which is 0.55 between J#1 and J#2, 0.57 between J#2 and J#3, and 0.63 between J#1 and J#3.</S>
    <S sid="167" ssid="21">Such Kappa values correspond to moderate agreement for the first two pairs and substantial agreement for the third one.</S>
    <S sid="168" ssid="22">In general, unanimous agreement among all of the three judges has been reported on 519 out of 752 templates, which corresponds to 69%.</S>
    <S sid="169" ssid="23">Our algorithm obtained encouraging results, extracting a considerable amount of interesting templates and showing inherent capability of discovering complex semantic relations.</S>
    <S sid="170" ssid="24">Concerning overall coverage, we managed to find correct templates for 86% of the verbs (46 out of 53).</S>
    <S sid="171" ssid="25">Nonetheless, presented results show a substantial margin of possible improvement.</S>
    <S sid="172" ssid="26">In fact yield values (5.5 Low Majority, up to 24 in best cases), which are our first concern, are inherently dependent on the breadth of Web search performed by the ASE algorithm.</S>
    <S sid="173" ssid="27">Due to computational time, the maximal number of anchor sets processed for each verb was held back to 30, significantly reducing the amount of retrieved data.</S>
    <S sid="174" ssid="28">In order to further investigate ASE potential, we subsequently performed some extended experiment trials raising the number of anchor sets per pivot to 50.</S>
    <S sid="175" ssid="29">This time we randomly chose a subset of 10 verbs out of the less frequent ones in the original main experiment.</S>
    <S sid="176" ssid="30">Results for these verbs in the main experiment were an average Yield of 3 and an average Precision of 45.19%.</S>
    <S sid="177" ssid="31">In contrast, the extended experiments on these verbs achieved a 6.5 Yield and 59.95% Precision (average values).</S>
    <S sid="178" ssid="32">These results are indeed promising, and the substantial growth in Yield clearly indicates that the TE/ASE algorithms can be further improved.</S>
    <S sid="179" ssid="33">We thus suggest that the feasibility of our approach displays the inherent scalability of the TE/ASE process, and its potential to acquire a large entailment relation KB using a full scale lexicon.</S>
    <S sid="180" ssid="34">A further improvement direction relates to template ranking and filtering.</S>
    <S sid="181" ssid="35">While in this paper we considered anchor sets to have equal weights, we are also carrying out experiments with weights based on cross-correlation between anchor sets.</S>
  </SECTION>
  <SECTION title="5 Conclusions" number="5">
    <S sid="182" ssid="1">We have described a scalable Web-based approach for entailment relation acquisition which requires only a standard phrasal lexicon as input.</S>
    <S sid="183" ssid="2">This minimal level of input is much simpler than required by earlier web-based approaches, while succeeding to maintain good performance.</S>
    <S sid="184" ssid="3">This result shows that it is possible to identify useful anchor sets in a fully unsupervised manner.</S>
    <S sid="185" ssid="4">The acquired templates demonstrate a broad range of semantic relations varying from synonymy to more complicated entailment.</S>
    <S sid="186" ssid="5">These templates go beyond trivial paraphrases, demonstrating the generality and viability of the presented approach.</S>
    <S sid="187" ssid="6">From our current experiments we can expect to learn about 5 relations per lexicon entry, at least for the more frequent entries.</S>
    <S sid="188" ssid="7">Moreover, looking at the extended test, we can extrapolate a notably larger yield by broadening the search space.</S>
    <S sid="189" ssid="8">Together with the fact that we expect to find entailment relations for about 85% of a lexicon, it is a significant step towards scalability, indicating that we will be able to extract a large scale KB for a large scale lexicon.</S>
    <S sid="190" ssid="9">In future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way, as well as precision, using statistical methods such as supervised learning for better anchor set identification and cross-correlation between different pivots.</S>
    <S sid="191" ssid="10">We also plan to support noun phrases as input, in addition to verb phrases.</S>
    <S sid="192" ssid="11">Finally, we would like to extend the learning task to discover the correct entailment direction between acquired templates, completing the knowledge required by practical applications.</S>
    <S sid="193" ssid="12">Like (Lin and Pantel, 2001), learning the context for which entailment relations are valid is beyond the scope of this paper.</S>
    <S sid="194" ssid="13">As stated, we learn entailment relations holding for some, but not necessarily all, contexts.</S>
    <S sid="195" ssid="14">In future work we also plan to find the valid contexts for entailment relations.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="6">
    <S sid="196" ssid="1">The authors would like to thank Oren Glickman (Bar Ilan University) for helpful discussions and assistance in the evaluation, Bernardo Magnini for his scientific supervision at ITC-irst, Alessandro Vallin and Danilo Giampiccolo (ITC-irst) for their help in developing the human based evaluation, and Prof. Yossi Matias (Tel-Aviv University) for supervising the first author.</S>
    <S sid="197" ssid="2">This work was partially supported by the MOREWEB project, financed by Provincia Autonoma di Trento.</S>
    <S sid="198" ssid="3">It was also partly carried out within the framework of the ITC-IRST (TRENTO, ITALY) &#8211; UNIVERSITY OF HAIFA (ISRAEL) collaboration project.</S>
    <S sid="199" ssid="4">For data visualization and analysis the authors intensively used the CLARK system (www.bultreebank.org) developed at the Bulgarian Academy of Sciences.</S>
  </SECTION>
</PAPER>
