<PAPER>
  <S sid="0" ssid="0">PART-OF-SPEECH TAGGING WITH NEURAL NETWORKS Hehnut Schmid Institute for Computational Linguistics, Azenbergstr.12, 70174 Stuttgart, Germany, schmid@ims.uni-stuttgart.de Topic area: large text corpora, part-of-speech tag- ging, neural networks 1 ABSTRACT Text corpora which are tagged with part-of-speech in- formation are useful in many areas of linguistic re- search.</S>
  <S sid="1" ssid="1">In this paper, a new part-of-speech tagging method hased on neural networks (Net-Tagger) is pre- sented and its performance is compared to that of a llMM-tagger (Cutting et al., 1992) and a trigram- based tagger (Kempe, 1993).</S>
  <S sid="2" ssid="2">It is shown that the Net-Tagger performs as well as the trigram-based tag- ger and better than the iIMM-tagger.</S>
  <S sid="3" ssid="3">2 INTRODUCTION Words are often ambiguous in their part of speech.</S>
  <S sid="4" ssid="4">The English word store for example can be either a noun, a finite verb or an infinitive.</S>
  <S sid="5" ssid="5">In an utterance, this ambiguity is normally resolved by the context of a word: e.g.</S>
  <S sid="6" ssid="6">in the seutence "The 1977 P6s could store two pages of data.</S>
  <S sid="7" ssid="7">", store can only be an intluitive.</S>
  <S sid="8" ssid="8">A part-of-speech tagger is a system which automat- ically assigns the part of speech to words using con- textual information.</S>
  <S sid="9" ssid="9">Potential applications for part- of-speech taggers exist in many areas inclnding speech recognition, speech synthesis, machine translation and information retrieval.</S>
  <S sid="10" ssid="10">l)ifiereut methods have been used for the im plemen- ration of part-of-speech taggers.</S>
  <S sid="11" ssid="11">TAGGIT (Greene, Rnbin, 1971), an early system, which was used for the initial tagging of the Brown corpus was rule-based.</S>
  <S sid="12" ssid="12">It was able to assign the correct part-of-speech to about 77 % of the words in the Brown corpus.</S>
  <S sid="13" ssid="13">In another approach contextual dependencies are modelled statistically.</S>
  <S sid="14" ssid="14">Churcb (1988) and Kempe (1993) use second order Markov Models and train their systems on large handtagged corpora.</S>
  <S sid="15" ssid="15">Using this metbod, they are able to tag more than 96 % of their test words with the correct part-of-speech.</S>
  <S sid="16" ssid="16">The need for reliably tagged training data, however, is a prob- lem for languages, where such data is not available in sufficient quantities.</S>
  <S sid="17" ssid="17">Jelinek (1985) and Cutting et al.</S>
  <S sid="18" ssid="18">(1992) circumvent his problem by training their taggers on untagged ata using tile Itaum-Welch algo- rithm (also know as the forward-backward algorithm).</S>
  <S sid="19" ssid="19">They report rates of correctly tagged words which are comparable to that presented by Church (1988) and Kempe (1993).</S>
  <S sid="20" ssid="20">A third and rather new approach is tagging with artificial neural networks.</S>
  <S sid="21" ssid="21">In the area of speech recog- nition neural networks have been used for a decade r, ow.</S>
  <S sid="22" ssid="22">They have shown performances comparable to that of IIidden Ivlarkov model systems or even better (Lippmann, 1989).</S>
  <S sid="23" ssid="23">Part-of-speech prediction is an- other area, closer to POS tagging, where neural net- works have been applied successfidly.</S>
  <S sid="24" ssid="24">Nakamura el; al.</S>
  <S sid="25" ssid="25">(1990) trained a d-layer feed-forward network with up to three preceding part-of-speech tags ,as input to predict the word category of the next word.</S>
  <S sid="26" ssid="26">The pre- diction accuracy was similar to that of a trigram-b,~sed predictor.</S>
  <S sid="27" ssid="27">Using tile predictor, Nakamura et al.</S>
  <S sid="28" ssid="28">were able to improve the recognition rate of their speech recognition system from 81.0 % to 86.9 %.</S>
  <S sid="29" ssid="29">Federici and Pirrelli (199a) developed a part-of- speech tagger which is based on a special type of neural network.</S>
  <S sid="30" ssid="30">It disambiguates between alternative morphosyntactic tags which are generated by a roof phological analyzer.</S>
  <S sid="31" ssid="31">The tagger is trained with an analogy-driven learning procedure.</S>
  <S sid="32" ssid="32">Only preliminary results are presented, so that a comparison with other methods is difficult.</S>
  <S sid="33" ssid="33">Ill this paper, a part-of-speech tagger based on a multilayer perceptrou network is presented.</S>
  <S sid="34" ssid="34">It is simi- lar to tile network of Nakamura et al.</S>
  <S sid="35" ssid="35">(1990) in so far as the same training procedure (Backpropagation) is used; but it differs in the structure of tile network and also in its purpose (disambignation vs. prediction).</S>
  <S sid="36" ssid="36">The performance of tl,e presented tagger is measured and compared to that of two other taggers (Cutting et al., 1992; Kempe, 1993).</S>
  <S sid="37" ssid="37">3 NEURAL NETWORKS Artificial neural networks consist of a large number of simple processing units.</S>
  <S sid="38" ssid="38">These units are highly inter- connected by directed weighted links.</S>
  <S sid="39" ssid="39">Associated with each unit is an activation value.</S>
  <S sid="40" ssid="40">Through tile connec- tions, this activation is propagated to other units.</S>
  <S sid="41" ssid="41">In mnltilayer perceptron etworks (MLP-networks), tile most popular network type, the processing units are arranged vertically in several ayers (fig.</S>
  <S sid="42" ssid="42">Con- nections exist only between units in adjacent layers.</S>
  <S sid="43" ssid="43">The bottom layer is called input layer, because the activations of the units in this layer represent the in- put of tile network.</S>
  <S sid="44" ssid="44">Correspondingly, the top layer is called output layer.</S>
  <S sid="45" ssid="45">Any layers between input layer 772 Figure 1: A 3-layer perceptron etwork ( output units hidden units b input units and outlmt layer are called hidden layers.</S>
  <S sid="46" ssid="46">Their acti- wttions are not visible externally.</S>
  <S sid="47" ssid="47">During the processing in a MLP-network, actiwt- tions are propagated from inlmt units through hidden units to output units.</S>
  <S sid="48" ssid="48">At each unit j ,  the weighted inlmt activations aiwij are summed and a bias pa- rameter Oj is added.</S>
  <S sid="49" ssid="49">net i = ~ aiwlj + Oj (1) t The resulting network input ,telj is then l)~uqsed through a sigmoid fimction (the logistic funclion) in order to restrict the value range of the resulting acti- vation aj to the interval [0,i].</S>
  <S sid="50" ssid="50">~, (:~) The network learns by adapting the weights of the connections between units, tmtil the correct output is t~rocluced.</S>
  <S sid="51" ssid="51">One widely used method is the backl.o p- ~gation algorithm which performs a gradient descent search on the error surface, The weight update ~XlOij , i.e.</S>
  <S sid="52" ssid="52">the difference between the old and the new value of weight wij, is here defined ,~s: AWij - -  rlapi6pj, where { ,,pj(1 --,,,)(t,,j - "p J ) , if j is an output unit a,,~ = ,,vj(l _avs)~vk,oik, (a) k if j is a hidden unit Ilere, Zp is the target output vector which the network lnnst learn t .</S>
  <S sid="53" ssid="53">"Daining the MLP-network with the backpropagao tion rule guarantees that a local minimum of the er- ror surface is found, thougl, this is not necessarily the global one.</S>
  <S sid="54" ssid="54">In order to speed up the trahfiug process, a momentum term is often introduced into the update rormula: ?kWij(t -~" 1) "~ Oapi~pj "+ ( :~ l t ) i j ( l )  (4) 1We assume here that the hia.s parameter Oj is realized ms a weight o an additional unit whidt has always the activation va}.ue 1 (cp.</S>
  <S sid="55" ssid="55">(B.umelhart, McChdland, t98,1)).</S>
  <S sid="56" ssid="56">For a de.tailed introduction to MLP networks see e.g.</S>
  <S sid="57" ssid="57">(l{unaelhart, McClellan(l, 1984).</S>
  <S sid="58" ssid="58">r 4 Tt IG  I~_AGGER NI i  ,TWO1{I ( The Net-Tagger consists of a Ml, P-network and a lex- icon (see tlg.</S>
  <S sid="59" ssid="59">l;igu,e 2: Structure.</S>
  <S sid="60" ssid="60">of I.he Net-Tagger without hidden layer; tile arrow symbolizes the connections between the layers.</S>
  <S sid="61" ssid="61">11 f  @ @ ?...?</S>
  <S sid="62" ssid="62">@ @ @...@ @...?</S>
  <S sid="63" ssid="63">@...@ p f In the output layer of the MLP network, each unit corresponds to one of the tags in the tagset.</S>
  <S sid="64" ssid="64">The net- work learns during the training to activate that output unit which represents the correct ag and to deactivate all other output units, llence, in the trained network, the output unit with the higlu.st activation indicates, which tag shouhl be attached to the word that is cur- rently processed.</S>
  <S sid="65" ssid="65">The input of the network comprises all the informa- tion whicii the systeni ti;Ls about the parts of speech of the current word, the p precedhig words al,d the f fol- lowing words.</S>
  <S sid="66" ssid="66">More precisely, for each part-of-speech tag posj and each of the p-t- 1-kf words in the context, there is an input unit whose activation in U represents the probability that wordl h~Ls part of speech posj.</S>
  <S sid="67" ssid="67">For the word which is being tagged and the fol- lowing words, the lezical part-of-speech probability l(posj]wordi) is all we know about the part of speech ~, This probability does not take into account arty contextual influences.</S>
  <S sid="68" ssid="68">So, we get the following in- put representation for the currently tagged word and the following words: i , , , j  : v(vo.,v I,,,o,.d,), ir i &gt; o (s) 2 Lexical probabilities are estimated hy dividing, the number of times a word occurs with a giw:n tag by the ownall numher of times the word occurs.</S>
  <S sid="69" ssid="69">This method is known as the Ma.vimum Likelihood Principle.</S>
  <S sid="70" ssid="70">IZ ~ For tile preceding words, there is more information available, because they have already bccn tagged.</S>
  <S sid="71" ssid="71">The activation values of the output units at the time of processing are here used instead of the lexieal part-of- speech probabilitiesa: i , , ; /t)  = o, , t / t  + O, if ; &lt; 0 (6) Copying output activations of tile network into the input units introduces recurrence into the network.</S>
  <S sid="72" ssid="72">This complicates the training process, because the out- put of the network is not correct, when the training starts and therefore, it cannot be fed back directly, when the training starts.</S>
  <S sid="73" ssid="73">Instead a weighted average of the actual output and the target output is used.</S>
  <S sid="74" ssid="74">It resembles more the output of the trained network which is similar (or at least shouhl be similar) to the target output.</S>
  <S sid="75" ssid="75">At tile beginning of the training, the weighting of the target output is high.</S>
  <S sid="76" ssid="76">It fails to zero during the training.</S>
  <S sid="77" ssid="77">The network is trained on a tagged corpus.</S>
  <S sid="78" ssid="78">Target activations are 0 for all output units, excepted for the unit which corresponds to the correct tag, for which it is 1.</S>
  <S sid="79" ssid="79">A slightly modified version of the backpropaga- tion algorithm with momentum term which has been presented in the last section is used: if the difference between the activation of an output unit j and the cor- responding target output is below a predefined thresh- old (we used 0.1), the error signal ~pJ is set to zero.</S>
  <S sid="80" ssid="80">In this way the network is forced to pay more attention to larger error signals.</S>
  <S sid="81" ssid="81">This resulted in an improvement of the tagging accuracy by more than 1 percent.</S>
  <S sid="82" ssid="82">Network architectures with and without hidden lay- ers have been trained and tested.</S>
  <S sid="83" ssid="83">In general, MLP- networks with hidden layers are more powerful than networks without one, but they also need more train- ing and there is a higher risk of overlearning 4.</S>
  <S sid="84" ssid="84">As will be shown in the next section, the Net-Tagger did not profit from a hidden layer.</S>
  <S sid="85" ssid="85">In both network types, the tagging of a single word is performed by copying the tag probabilities of the current word and its neighbours into the input units, propagating the activations through the network to the output units and determining the output unit which has the highest activation.</S>
  <S sid="86" ssid="86">The tag correspond- ing to this unit is then attached to the current word.</S>
  <S sid="87" ssid="87">If the second strongest activation in the output layer is close to the strongest one, tile tag corresponding to the second strongest activation may be given as an alternative output.</S>
  <S sid="88" ssid="88">No additional computation is required for this.</S>
  <S sid="89" ssid="89">Further, it is possible to give a scored list of all tags as output.</S>
  <S sid="90" ssid="90">aThe output  act ivat ions  of the network do not necessar- ily sum to 1.</S>
  <S sid="91" ssid="91">Therefore,  they should not he interpreted as probabi l i t ies.</S>
  <S sid="92" ssid="92">40verlearning means  that  irrelevant features of the t ra in ing set are learned.</S>
  <S sid="93" ssid="93">As a result ,  the uetwork is unable to generalize.</S>
  <S sid="94" ssid="94">5 TIIE LEX ICON The lexicon which contains the a priori tag probabili- ties for each word is similar to the lexicon which was used by Cutting et al.</S>
  <S sid="95" ssid="95">it has three parts: a fullform lexicon, a suffix lexicon and a default enlry.</S>
  <S sid="96" ssid="96">No documentation f tile construction algorithm of the su[lix lexicon in (Cutting et al., 1992) was available.</S>
  <S sid="97" ssid="97">Thus, a new method based on information theoretic principles was developed.</S>
  <S sid="98" ssid="98">During the lookup of a word in the lexicon of the Net-Tagger, the fifllform lexicon is searched first.</S>
  <S sid="99" ssid="99">If the word is found there, the corresponding tag prob- ability vector is returned.</S>
  <S sid="100" ssid="100">Otherwise, the uppercase letters of the word are turned to lowercase, and the search in the fullform lexicon is repeated.</S>
  <S sid="101" ssid="101">If it fails again, the suIfix lexicon is searched next.</S>
  <S sid="102" ssid="102">If none of the previous teps has been snccessfull, tile default en- try of the lexicon is returned.</S>
  <S sid="103" ssid="103">The fullform lexicon was created from a tagged training corpus (some 2 million words of the Penn Treebank Corpus).</S>
  <S sid="104" ssid="104">First, the number of occurrences of each word/tag pair was counted.</S>
  <S sid="105" ssid="105">Afterwards, those tags of each word with an estimated probability of less than 1 percent were removed, because they were in most eases the result of tagging errors in the original corpus.</S>
  <S sid="106" ssid="106">Figure 3: A sample suffix tree of length 3 i es Oils ous scd old ble lie nee ive ing ion SOIl ton man ity The second part of the lexicon, the suflix lexicon, forms a tree.</S>
  <S sid="107" ssid="107">Each node of tile tree (excepted tile root node) is labeled with a character.</S>
  <S sid="108" ssid="108">At tile leaves, tag probability vectors are attached.</S>
  <S sid="109" ssid="109">During a lookup, tile suffix tree is searched from the root.</S>
  <S sid="110" ssid="110">In each step, tile branch which is labeled with the next character from tile end of the word suffix, is followed.</S>
  <S sid="111" ssid="111">Assume e.g., wc want to look for tile word taggiu 9 in the suflqx lexicon which is shown in fig.</S>
  <S sid="112" ssid="112">We start at the root (labeled #) and follow the branch which leads to the node labeled g. From there, we move to the node labeled n, and finally we end up in tile node 174 Table 1: Sample frequencies at a tree node and its two child nodes.</S>
  <S sid="113" ssid="113">suffix ess 10 I gp /  &lt;iS l_5 __1 2 t~a~ 143 sufllx ness suffix less 1 85 2 8 45 0 0 2 48 95 labeled i.</S>
  <S sid="114" ssid="114">This node is a leaf and the attached tag probability vector (which is not shown in lib.</S>
  <S sid="115" ssid="115">3) is returned.</S>
  <S sid="116" ssid="116">The suffix lexicon was automatically built from the training corpus.</S>
  <S sid="117" ssid="117">First, a sujJiz tree wits constructed from the suffices of length 5 of sill words wliich were annotated with an open class l)art-of-speecli s. Then tag frequencies were cotlnted for all suffices and stored at the corresponding tree nodes.</S>
  <S sid="118" ssid="118">In the next step, an information measure I(S) was calculated for each node of the tree: I(S) = - ~ P(posiS ) tomd(p,&gt;,qS) (7) po* IIere, S is the suffix which corresponds to the current node and P(poslS ) is the probability of tag pos given a word with suff ix S. Using this information measure, the suffix tree has been pruned.</S>
  <S sid="119" ssid="119">For each leaf, the weighted information gain G(aS) was calculated: a(aS) = V(aS) (S(S) - S(&lt;,S)), (8) where S is the suffix of the parent node, aS is the suffix of the current node and F(aS) is the frequency of suffix nS.</S>
  <S sid="120" ssid="120">If the information gain at some leaf of the suffix tree is below a given threshoht ~, it is removed.</S>
  <S sid="121" ssid="121">The tag frequencies of all deleted subnodes of a parent node are collected at the defi, ult node of the parent node.</S>
  <S sid="122" ssid="122">If the default node is the only renlaining subnodc, it is deleted too.</S>
  <S sid="123" ssid="123">In this case, the parent node becomes a leaf and is also checked, whether it is deletable.</S>
  <S sid="124" ssid="124">To illustrate this process consider the following ex- ample, where ess is the suffix of the parent node, less is tim suffix of one child node and hess is the suffix of the other child node.</S>
  <S sid="125" ssid="125">The tag frequencies of these nodes are given in table 1.</S>
  <S sid="126" ssid="126">Tim information measure for the parent node is: 86 86 10 10 S(ess) .</S>
  <S sid="127" ssid="127">l o ,a~- -  ... ~ 1.32 (9) 143 143 143 143 ].lie corresponding values for the chihl nodes are 0.39 for hess and 0.56 for less.</S>
  <S sid="128" ssid="128">Now, we can determine the welghted information gain at each of the ehihl nodes.</S>
  <S sid="129" ssid="129">We get: G(ness) = 48(1.32 - 0.39) = 44.64 (10) 5Opell class parts-of-speech are those, width allow for the production of new words {e.g.</S>
  <S sid="130" ssid="130">noun, verb, adjective).</S>
  <S sid="131" ssid="131">6We used a gain threshohl of 10.</S>
  <S sid="132" ssid="132">Table 2: Comparison of recognition rates method accuracy t Net-Tagger 96.22 % trigrarn tagger 96.06 % IIMM tagger 94.24 % G(less) = 95(1.32-  0.56) = 72,20 (11) Both wdues are well above a threshohl of 10, and there- fore none of them should be deleted.</S>
  <S sid="133" ssid="133">As explained before, the suflix tree is walked during a lookup along the l)ath, where the nodes are anno- tated with the letters of the word snflix in reversed or- der.</S>
  <S sid="134" ssid="134">If at some node on the path, no matching subnode can be found, and there is a default subitode, then the default node is followed.</S>
  <S sid="135" ssid="135">If a leaf is reached at the end of the path, the corresponding tag probability vector is returned.</S>
  <S sid="136" ssid="136">Otherwise, the search fails and the default entry is returned.</S>
  <S sid="137" ssid="137">The defaull entry is constructed by subtracting the tag frequencies at all leaves of the pruned suffix tree from the tag frequencies of the root node and nor- malizing the resulting frequencies.</S>
  <S sid="138" ssid="138">Thereby, relative frequencies are obtained which sum to one.</S>
  <S sid="139" ssid="139">6 Rl,~suurs The 2-layer version of the Net-Tagger w,~s trained on a 2 million word subpart of the Pe.nn-Treebank corpus.</S>
  <S sid="140" ssid="140">Its performance was tested on a 100,000 word subpart which was not part of the training corlms.</S>
  <S sid="141" ssid="141">The set- tings of the network parameters were as follows: the number of preceding words in the context p w,~s 3, the number of following words f was 2 and the number of training cycles was 4 millions.</S>
  <S sid="142" ssid="142">The training of the tagger took one day on a Sparcl0 workstation and the tagging of 100,000 words took 12 minutes on the same machine.</S>
  <S sid="143" ssid="143">In tabh; 2, the accuracy rate of the Net-Tagger is cOrolLated to that of a trigram l)msed tagger (Kempe, 1993) and a lIidden Markov Model tagger (Cutting et al., 1992) which were.</S>
  <S sid="144" ssid="144">trained and tested on the same data.</S>
  <S sid="145" ssid="145">In order to determine the influence of tim size of the training sample, the taggers were also trained on corpora of different sizes and tested again r. The resulting percentages of correctly tagged words are shown in figure 4.</S>
  <S sid="146" ssid="146">These experiments demonstrate that the perfor- mance of the Net-Tagger is comparable to that of the trigram tagger and better than that of the IIMM tag- ger.</S>
  <S sid="147" ssid="147">They further show tl,at the performance of the Net-Tagger is less affected by a small amount of train- ing data than that of tim trigram tagger.</S>
  <S sid="148" ssid="148">This may be due to a much smaller number of paraineters in the Net-Tagger: while the trigram tagger must accurately ~l:or this test, a slightly simpler netwmk structure with two preceding and one following word in the input context was used.</S>
  <S sid="149" ssid="149">775 Figure 4: Recognition rates for varying sizes of the training corpus.</S>
  <S sid="150" ssid="150">, 95 .=_ 90 ~ ?</S>
  <S sid="151" ssid="151">:~ 85 Net-Tagger - " - ~o= Xer0x-Tagger .... o J 80 -~ Trigram Tagger -=- 75 ..,I .</S>
  <S sid="152" ssid="152">I 10000 100000 1 e+06 size of training corpus estimate 110,592 trigrams, the Net-Tagger only has to train 13,824 network parameters.</S>
  <S sid="153" ssid="153">It was fitrther tested, whether an additional hid- den layer in the network with 50 units would improve the accuracy of the tagging.</S>
  <S sid="154" ssid="154">It turned out that the accuracy actually deteriorated slightly, although the number of training cycles had been increased to 50 millions .</S>
  <S sid="155" ssid="155">Also, tire influence of the size of the input context was determined.</S>
  <S sid="156" ssid="156">Shrinking the context from three preceding and two following words to two preceding and one following word reduced the accuracy only by 0.1%.</S>
  <S sid="157" ssid="157">Enlarging the context gave no improvement.</S>
  <S sid="158" ssid="158">A context of three preceding and two following words seems to he optimal.</S>
  <S sid="159" ssid="159">As mentioned previously, the tagger can produce an alternative tag, if the decision between two tags is difficult.</S>
  <S sid="160" ssid="160">In that way, the accuracy can be raised to 97.79 % at the expense of 4.6 % ambiguously tagged words.</S>
  <S sid="161" ssid="161">An analysis of tire errors of the Net-Tagger and the trigram tagger shows that both have problems with the same words, althot, gh the individual errors are of- ten different 9 .</S>
  <S sid="162" ssid="162">7 CONCLUSIONS In this paper, the Net-Tagger was presented, a part- of-speech tagger which is based on a MLP-network.</S>
  <S sid="163" ssid="163">A comparison of the tagging results with those of a trigram tagger and a IIMM tagger showed that the accuracy is as high as that of the trigram tagger and the robustness on small training corpora is as good as that of the HMM tagger.</S>
  <S sid="164" ssid="164">Thus, the Net-Tagger combines advantages of both of these methods.</S>
  <S sid="165" ssid="165">The Net-Tagger has the additional advantage that problematic decisions between tags are easy to detect, aDue to the large training times needed to train the 3-layer- network, no further tests have been conducted.</S>
  <S sid="166" ssid="166">o Less than 60 % of the tagging errors were made in common by both taggers.</S>
  <S sid="167" ssid="167">so that in these cases an additional tag can be given in the output.</S>
  <S sid="168" ssid="168">In this way, the final decision can be delayed to a later processing stage, e.g.</S>
  <S sid="169" ssid="169">A disadvantage of the presented method may be its lower processing speed compared to statistical meth- ods.</S>
  <S sid="170" ssid="170">In the light of the high speed of present computer hardware, however, this does not seem to be a serious drawback.</S>
  <S sid="171" ssid="171">8 REFERENCES Church, K. W. (1985).</S>
  <S sid="172" ssid="172">A stochastic parts program and noun phrase parser for unrestricted text.</S>
  <S sid="173" ssid="173">Pro- ceedings of the Second Conference on Applied Natvral Language Processing, p. 136-143.</S>
  <S sid="174" ssid="174">Cutting, D., a. Kupiec, a. Pedersen and P. Sibun (1992).</S>
  <S sid="175" ssid="175">A practical part-of-speech tagger.</S>
  <S sid="176" ssid="176">Proceedings of the Third Conference on Applied Nalural Laguage Processing, r1?ento, Italy (ACL), pages 133-140, 1992.</S>
  <S sid="177" ssid="177">Also awtilable as Xerox technical report SSL-92-01.</S>
  <S sid="178" ssid="178">Federici, S. and V. Pirrelli (1993).</S>
  <S sid="179" ssid="179">Analogical mod- elling of text tagging, unpublished report, Istituto di Linguistica Computazionale, Pisa, Italy.</S>
  <S sid="180" ssid="180">B and G. M. R.ubin (1971).</S>
  <S sid="181" ssid="181">Auto- matic grammatical tagging of English.</S>
  <S sid="182" ssid="182">technical re- port, Department of Linguistics, Brown University, Providence, Rhode Island.</S>
  <S sid="183" ssid="183">aelinek, F. (1985).</S>
  <S sid="184" ssid="184">Markov Source modeling of text generation".</S>
  <S sid="185" ssid="185">In J.K. Skwirzinski Ed., Impact of Pro- cessing Techniques on Communication, Nijhoff, Dor- drecht.</S>
  <S sid="186" ssid="186">A stochastic Tagger and an Analysis of Tagging Errors.</S>
  <S sid="187" ssid="187">Internal paper.</S>
  <S sid="188" ssid="188">In- stitute for Computational Linguistics, University of Stuttgart.</S>
  <S sid="189" ssid="189">Lippmann, R.. P. (1989).</S>
  <S sid="190" ssid="190">Review of Neural Networks for Speech Recognition.</S>
  <S sid="191" ssid="191">Neural Computation, Vol.</S>
  <S sid="192" ssid="192">Nakamura, M., I(.</S>
  <S sid="193" ssid="193">Marnyama, T. Kawabata and K. Shikano (1990).</S>
  <S sid="194" ssid="194">Neural network approach to word cat- egory prediction for Englis}i texts.</S>
  <S sid="195" ssid="195">l(arlgren Ed., COLING-90, lIelslnki University, p. 213-218.</S>
  <S sid="196" ssid="196">Rumelhart, D. E. and J. L. McClelland (1984).</S>
  <S sid="197" ssid="197">Par- allel Distributed Processing.</S>
  <S sid="198" ssid="198">MIT-Press, Cambridge, MA.</S>
</PAPER>
