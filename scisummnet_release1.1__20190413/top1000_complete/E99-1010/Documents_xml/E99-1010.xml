<PAPER>
  <S sid="0">An Efficient Method For Determining Bilingual Word Classes</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In statistical natural language processing we always face the problem of sparse data.</S>
    <S sid="2" ssid="2">One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.</S>
    <S sid="3" ssid="3">In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation.</S>
    <S sid="4" ssid="4">We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm.</S>
    <S sid="5" ssid="5">We will show that the usage of the bilingual word classes we get can improve statistical machine translation.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Word classes are often used in language modelling to solve the problem of sparse data.</S>
    <S sid="7" ssid="2">Various clustering techniques have been proposed (Brown et al., 1992; Jardino and Adda, 1993; Martin et al., 1998) which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms.</S>
    <S sid="8" ssid="3">In the field of statistical machine translation we also face the problem of sparse data.</S>
    <S sid="9" ssid="4">Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models.</S>
    <S sid="10" ssid="5">A naive approach for doing this would be the use of mono-lingually optimized word classes in source and target language.</S>
    <S sid="11" ssid="6">Unfortunately we can not expect these independently optimized classes to be correspondent.</S>
    <S sid="12" ssid="7">Therefore mono-lingually optimized word classes do not seem to be useful for machine translation (see also (Fung and Wu, 1995)).</S>
    <S sid="13" ssid="8">We define bilingual word clustering as the process of forming corresponding word classes suitable for machine translation purposes for a pair of languages using a parallel training corpus.</S>
    <S sid="14" ssid="9">The described method to determine bilingual word classes is an extension and improvement of the method mentioned in (Och and Weber, 1998).</S>
    <S sid="15" ssid="10">Our approach is simpler and computationally more efficient than (Wang et al., 1996).</S>
  </SECTION>
  <SECTION title="2 Monolingual Word Clustering" number="2">
    <S sid="16" ssid="1">The task of a statistical language model is to estimate the probability Pr(wiev) of a sequence of words wiv = wi 'WN.</S>
    <S sid="17" ssid="2">A simple approximation of Pr(41) is to model it as a product of bigram probabilities: Pr (wPI) = HiN_, p(wi *2_1).</S>
    <S sid="18" ssid="3">If we want to estimate the bigram probabilities p(wlw') using a realistic natural language corpus we are faced with the problem that most of the bigrams are rarely seen.</S>
    <S sid="19" ssid="4">One possibility to solve this problem is to partition the set of all words into equivalence classes.</S>
    <S sid="20" ssid="5">The function C maps words w to their classes C(w).</S>
    <S sid="21" ssid="6">Rewriting the corpus probability using classes we arrive at the following probability model p(wiv IC): In this model we have two types of probabilities: the transition probability p(C1C1) for class C given its predecessor class C' and the membership probability p(wIC) for word w given class C. To determine the optimal classes C for a given number of classes M we perform a maximumlikelihood approach: = arg mrc p(wiv IC) (2) We estimate the probabilities of Eq.</S>
    <S sid="22" ssid="7">(1) by relative frequencies: p(CIC&amp;quot;) := n(C1C1')In(C'), p(wIC) = n(w)In(C).</S>
    <S sid="23" ssid="8">The function n(-) provides the frequency of a uni- or bigram in the training corpus.</S>
    <S sid="24" ssid="9">If we insert this into Eq.</S>
    <S sid="25" ssid="10">(2) and apply the negative logarithm and change the summation order we arrive at the following optimization Proceedings of EACL '99 criterion LP,.</S>
    <S sid="26" ssid="11">(Kneser and Ney, 1991): The function h(n) is a shortcut for n &#8226; log(n).</S>
    <S sid="27" ssid="12">It is necessary to fix the number of classes in C in advance as the optimum is reached if every word is a class of its own.</S>
    <S sid="28" ssid="13">Because of this it is necessary to perform an additional optimization process which determines the number of classes.</S>
    <S sid="29" ssid="14">The use of leaving-one-out in a modified optimization criterion as in (Kneser and Ney, 1993) could in principle solve this problem.</S>
    <S sid="30" ssid="15">An efficient optimization algorithm for LPI is described in section 4.</S>
  </SECTION>
  <SECTION title="3 Bilingual Word Clustering" number="3">
    <S sid="31" ssid="1">In bilingual word clustering we are interested in classes F and E which form partitions of the vocabulary of two languages.</S>
    <S sid="32" ssid="2">To perform bilingual word clustering we use a maximum-likelihood approach as in the monolingual case.</S>
    <S sid="33" ssid="3">We maximize the joint probability of a bilingual training corpus To perform the maximization of Eq.</S>
    <S sid="34" ssid="4">(6) we have to model the monolingual a priori probability p(ef1E) and the translation probability p(f lel; e, F).</S>
    <S sid="35" ssid="5">For the first we use the class-based bigram probability from Eq.</S>
    <S sid="36" ssid="6">(1).</S>
    <S sid="37" ssid="7">To model p(fillef; E, .7) we assume the existence of an alignment af.</S>
    <S sid="38" ssid="8">We assume that every word fj is produced by the word ea, at position a3 in the training corpus with the probability P(filea,): The word alignment ail is trained automatically using statistical translation models as described in (Brown et al., 1993; Vogel et al., 1996).</S>
    <S sid="39" ssid="9">The idea is to introduce the unknown alignment a as hidden variable into a statistical model of the translation probability p(glef).</S>
    <S sid="40" ssid="10">By applying the EMalgorithm we obtain the model parameters.</S>
    <S sid="41" ssid="11">The alignment cif that we use is the Viterbi-Alignment of an HMM alignment model similar to (Vogel et al., 1996).</S>
    <S sid="42" ssid="12">By rewriting the translation probability using word classes, we obtain (corresponding to Eq.</S>
    <S sid="43" ssid="13">(1)): (8) The variables F and E denote special classes in and E. We use relative frequencies to estimate p(FIE) and p(fIF): The function nt(FIE) counts how often the words in class F are aligned to words in class E. If we insert these relative frequencies into Eq.</S>
    <S sid="44" ssid="14">(8) and apply the same transformations as in the monolingual case we obtain a similar optimization criterion for the translation probability part of Eq.</S>
    <S sid="45" ssid="15">(6).</S>
    <S sid="46" ssid="16">Thus the full optimization criterion for bilingual word classes is: The two count functions n(E1E) and nt(FIE) can be combined into one count function ng (X1Y) n(X1Y) + nt (X1Y) as for all words f and all words e and e' holds n(f le) = 0 and nt(ele1) = 0.</S>
    <S sid="47" ssid="17">Using the function n9 we arrive at the following optimization criterion: Here we defined ng,i (X) = Ex, ng (XIX') and n9,2(X) = Ex, n9(X11X).</S>
    <S sid="48" ssid="18">The variable X runs over the classes in &#163; and F. In the optimization process it cannot be allowed that words of different languages occur in one class.</S>
    <S sid="49" ssid="19">It can be seen that Eq.</S>
    <S sid="50" ssid="20">(3) is a special case of Eq.</S>
    <S sid="51" ssid="21">(9) with ng,1 = n9,2.</S>
    <S sid="52" ssid="22">Another possibility to perform bilingual word clustering is to apply a two-step approach.</S>
    <S sid="53" ssid="23">In a first step we determine classes S optimizing only the monolingual part of Eq.</S>
    <S sid="54" ssid="24">(6) and secondly we determine classes F optimizing the bilingual part (without changing 6): By using these two optimization processes we enforce that the classes E are mono-lingually 'good' classes and that the classes .7- correspond to 6.</S>
    <S sid="55" ssid="25">Interestingly enough this results in a higher translation quality (see section 5).</S>
  </SECTION>
  <SECTION title="4 Implementation" number="4">
    <S sid="56" ssid="1">An efficient optimization algorithm for LPI is the exchange algorithm (Martin et al., 1998).</S>
    <S sid="57" ssid="2">For the optimization of LP2 we can use the same algorithm with small modifications.</S>
    <S sid="58" ssid="3">Our starting point is a random partition of the training corpus vocabulary.</S>
    <S sid="59" ssid="4">This initial partition is improved iteratively by moving a single word from one class to another.</S>
    <S sid="60" ssid="5">The algorithm to determine bilingual classes is depicted in Figure 1.</S>
    <S sid="61" ssid="6">If only one word w is moved between the partitions C and C' the change LP(C,n9)&#8212; LP(C',n9) can be computed efficiently looking only at classes C for which ng (w, C) &gt; 0 or ng(C,w) &gt;0.</S>
    <S sid="62" ssid="7">We define Mc, to be the average number of seen predecessor and successor word classes.</S>
    <S sid="63" ssid="8">With the notation I for the number of iterations needed for convergence, B for the number of word bigrams, M for the number of classes and V for the vocabulary size the computational complexity of this algorithm is roughly I (B &#8226; log2 (B IV) +V M &#8226; Mo).</S>
    <S sid="64" ssid="9">A detailed analysis of the complexity can be found in (Martin et al., 1998).</S>
    <S sid="65" ssid="10">The algorithm described above provides only a local optimum.</S>
    <S sid="66" ssid="11">The quality of the resulting local optima can be improved if we accept a short-term degradation of the optimization criterion during the optimization process.</S>
    <S sid="67" ssid="12">We do this in our implementation by applying the optimization method threshold accepting (Dueck and Scheuer, 1990) which is an efficient simplification of simulated annealing.</S>
  </SECTION>
  <SECTION title="5 Results" number="5">
    <S sid="68" ssid="1">The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.</S>
    <S sid="69" ssid="2">The key element of this approach are the alignment templates (originally referred to as translation rules) which are pairs of phrases together with an alignment between the words of the phrases.</S>
    <S sid="70" ssid="3">Examples of alignment templates are shown in Figure 2.</S>
    <S sid="71" ssid="4">The advantage of the alignment template approach against word-based statistical translation models is that word context and local re-orderings are explicitly taken into account.</S>
    <S sid="72" ssid="5">The alignment templates are automatically trained using a parallel training corpus.</S>
    <S sid="73" ssid="6">The translation of a sentence is done by a search process which determines the set of alignment templates which optimally cover the source sentence.</S>
    <S sid="74" ssid="7">The bilingual word classes are used to generalize the applicability of the alignment templates in search.</S>
    <S sid="75" ssid="8">If there exists a class which contains all cities in source and target language it is possible that an alignment template containing a special city can be generalized to all cities.</S>
    <S sid="76" ssid="9">More details are given in (Och and Weber, 1998; Och and Ney, 1999).</S>
    <S sid="77" ssid="10">We demonstrate results of our bilingual clustering method for two different bilingual corpora (see Tables 1 and 2).</S>
    <S sid="78" ssid="11">The EuTRANs-I corpus is a subtask of the &amp;quot;Traveller Task&amp;quot; (Vidal, 1997) which is an artificially generated Spanish-English corpus.</S>
    <S sid="79" ssid="12">The domain of the corpus is a humanto-human communication situation at a reception Table 3: Example of bilingual word classes (corpus EuTRANs-I, method BIL-2).</S>
    <S sid="80" ssid="13">El: how it pardon what when where which&#8226; who why E2: my our E3: today tomorrow E4: ask call make E5: carrying changing giving looking moving putting sending showing waking E6: full half quarter Si: c'omo cu'al cu'ando cu'anta d'onde dice dicho hace qu'e qui'en tiene desk of a hotel.</S>
    <S sid="81" ssid="14">The EuTRANs-II corpus is a natural German-English corpus consisting of different text types belonging to the domain of tourism: bilingual Web pages of hotels, bilingual touristic brochures and business correspondence.</S>
    <S sid="82" ssid="15">The target language of our experiments is English.</S>
    <S sid="83" ssid="16">We compare the three described methods to generate bilingual word classes.</S>
    <S sid="84" ssid="17">The classes MONO are determined by monolingually optimizing source and target language classes with Eq.</S>
    <S sid="85" ssid="18">(4).</S>
    <S sid="86" ssid="19">The classes BIL are determined by bilingually optimizing classes with Eq.</S>
    <S sid="87" ssid="20">(10).</S>
    <S sid="88" ssid="21">The classes BIL-2 are determined by first optimizing mono-lingually classes for the target language (English) and afterwards optimizing classes for the source language (Eq.</S>
    <S sid="89" ssid="22">(11) and Eq.</S>
    <S sid="90" ssid="23">(12)).</S>
    <S sid="91" ssid="24">For EuTRANs-I we used 60 classes and for EuTRANs-II we used 500 classes.</S>
    <S sid="92" ssid="25">We chose the number of classes in such a way that the final performance of the translation system was optimal.</S>
    <S sid="93" ssid="26">The CPU time for optimization of bilingual word classes on an Alpha workstation was under 20 seconds for EuTRANs-I and less than two hours for EuTRANs-II.</S>
    <S sid="94" ssid="27">Table 3 provides examples of bilingual word classes for the EuTRANs-I corpus.</S>
    <S sid="95" ssid="28">It can be seen that the resulting classes often contain words that are similar in their syntactic and semantic functions.</S>
    <S sid="96" ssid="29">The grouping of words with a different meaning like today and tomorrow does not imply that these words should be translated by the same Spanish word, but it does imply that the translations of these words are likely to be in the same Spanish word class.</S>
    <S sid="97" ssid="30">To measure the quality of our bilingual word classes we applied two different evaluation measures: exp (J-1 E maxi log (p (C ( f j) (ei)))) 3=1 Both measures determine the extent to which the translation probability is spread out.</S>
    <S sid="98" ssid="31">A small value means that the translation probability is very focused and that the knowledge of the source language class provides much information about the target language class. sertions/deletions/substitutions relative to a reference translation.</S>
    <S sid="99" ssid="32">As expected the translation quality improves using classes.</S>
    <S sid="100" ssid="33">For the small EuTRANs-I task the word error rates reduce significantly.</S>
    <S sid="101" ssid="34">The word error rates for the EuTRANs-II task are much larger because the task has a very large vocabulary and is more complex.</S>
    <S sid="102" ssid="35">The bilingual classes show better results than the monolingual classes MONO.</S>
    <S sid="103" ssid="36">One explanation for the improvement in translation quality is that the bilingually optimized classes result in an increased average size of used alignment templates.</S>
    <S sid="104" ssid="37">For example the average length of alignment templates with the EuTRANs-I corpus using WORD is 2.85 and using BIL-2 it is 5.19.</S>
    <S sid="105" ssid="38">The longer the average alignment template length, the more context is used in the translation and therefore the translation quality is higher.</S>
    <S sid="106" ssid="39">An explanation for the superiority of BIL-2 over BIL is that by first optimizing the English classes mono-lingually, it is much more probable that longer sequences of classes occur more often thereby increasing the average alignment template size.</S>
  </SECTION>
  <SECTION title="6 Summary and future works" number="6">
    <S sid="107" ssid="1">By applying a maximum-likelihood approach to the joint probability of a parallel corpus we obtained an optimization criterion for bilingual word classes which is very similar to the one used in monolingual maximum-likelihood word clustering.</S>
    <S sid="108" ssid="2">For optimization we used the exchange algorithm.</S>
    <S sid="109" ssid="3">The obtained word classes give a low translation lexicon perplexity and improve the quality of staProceedings of EACL '99 tistical machine translation.</S>
    <S sid="110" ssid="4">We expect improvements in translation quality by allowing that words occur in more than one class and by performing a hierarchical clustering.</S>
    <S sid="111" ssid="5">Acknowledgements This work has been partially supported by the European Community under the ESPRIT project number 30268 (EuTrans).</S>
  </SECTION>
</PAPER>
