<PAPER>
  <S sid="0">Discriminative Reordering Models For Statistical Machine Translation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present discriminative reordering models for phrase-based statistical machine translation.</S>
    <S sid="2" ssid="2">The models are trained using the maximum entropy principle.</S>
    <S sid="3" ssid="3">We use several types of features: based on words, based on word classes, based on the local context.</S>
    <S sid="4" ssid="4">We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus.</S>
    <S sid="5" ssid="5">Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">In recent evaluations, phrase-based statistical machine translation systems have achieved good performance.</S>
    <S sid="7" ssid="2">Still the fluency of the machine translation output leaves much to desire.</S>
    <S sid="8" ssid="3">One reason is that most phrase-based systems use a very simple reordering model.</S>
    <S sid="9" ssid="4">Usually, the costs for phrase movements are linear in the distance, e.g. see (Och et al., 1999; Koehn, 2004; Zens et al., 2005).</S>
    <S sid="10" ssid="5">Recently, in (Tillmann and Zhang, 2005) and in (Koehn et al., 2005), a reordering model has been described that tries to predict the orientation of a phrase, i.e. it answers the question &#8217;should the next phrase be to the left or to the right of the current phrase?&#8217; This phrase orientation probability is conditioned on the current source and target phrase and relative frequencies are used to estimate the probabilities.</S>
    <S sid="11" ssid="6">We adopt the idea of predicting the orientation, but we propose to use a maximum-entropy based model.</S>
    <S sid="12" ssid="7">The relative-frequency based approach may suffer from the data sparseness problem, because most of the phrases occur only once in the training corpus.</S>
    <S sid="13" ssid="8">Our approach circumvents this problem by using a combination of phrase-level and word-level features and by using word-classes or part-of-speech information.</S>
    <S sid="14" ssid="9">Maximum entropy is a suitable framework for combining these different features with a well-defined training criterion.</S>
    <S sid="15" ssid="10">In (Koehn et al., 2005) several variants of the orientation model have been tried.</S>
    <S sid="16" ssid="11">It turned out that for different tasks, different models show the best performance.</S>
    <S sid="17" ssid="12">Here, we let the maximum entropy training decide which features are important and which features can be neglected.</S>
    <S sid="18" ssid="13">We will see that additional features do not hurt performance and can be safely added to the model.</S>
    <S sid="19" ssid="14">The remaining part is structured as follows: first we will describe the related work in Section 2 and give a brief description of the baseline system in Section 3.</S>
    <S sid="20" ssid="15">Then, we will present the discriminative reordering model in Section 4.</S>
    <S sid="21" ssid="16">Afterwards, we will evaluate the performance of this new model in Section 5.</S>
    <S sid="22" ssid="17">This evaluation consists of two parts: first we will evaluate the prediction capabilities of the model on a word-aligned corpus and second we will show improved translation quality compared to the baseline system.</S>
    <S sid="23" ssid="18">Finally, we will conclude in Section 6.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="24" ssid="1">As already mentioned in Section 1, many current phrase-based statistical machine translation systems use a very simple reordering model: the costs for phrase movements are linear in the distance.</S>
    <S sid="25" ssid="2">This approach is also used in the publicly available Pharaoh decoder (Koehn, 2004).</S>
    <S sid="26" ssid="3">The idea of predicting the orientation is adopted from (Tillmann and Zhang, 2005) and (Koehn et al., 2005).</S>
    <S sid="27" ssid="4">Here, we use the maximum entropy principle to combine a variety of different features.</S>
    <S sid="28" ssid="5">A reordering model in the framework of weighted finite state transducers is described in (Kumar and Byrne, 2005).</S>
    <S sid="29" ssid="6">There, the movements are defined at the phrase level, but the window for reordering is very limited.</S>
    <S sid="30" ssid="7">The parameters are estimated using an EM-style method.</S>
    <S sid="31" ssid="8">None of these methods try to generalize from the words or phrases by using word classes or part-ofspeech information.</S>
    <S sid="32" ssid="9">The approach presented here has some resemblance to the bracketing transduction grammars (BTG) of (Wu, 1997), which have been applied to a phrase-based machine translation system in (Zens et al., 2004).</S>
    <S sid="33" ssid="10">The difference is that, here, we do not constrain the phrase reordering.</S>
    <S sid="34" ssid="11">Nevertheless the inverted/monotone concatenation of phrases in the BTG framework is similar to the left/right phrase orientation used here.</S>
  </SECTION>
  <SECTION title="3 Baseline System" number="3">
    <S sid="35" ssid="1">In statistical machine translation, we are given a source language sentence fJ1 = f1 .</S>
    <S sid="36" ssid="2">.</S>
    <S sid="37" ssid="3">. fj ... fJ, which is to be translated into a target language sentence eI1 = e1 ... ei ... eI.</S>
    <S sid="38" ssid="4">Among all possible target language sentences, we will choose the sentence with the highest probability: The posterior probability Pr(eI1|fJ1 ) is modeled directly using a log-linear combination of several models (Och and Ney, 2002): (2) The denominator represents a normalization factor that depends only on the source sentence fJ1 .</S>
    <S sid="39" ssid="5">Therefore, we can omit it during the search process.</S>
    <S sid="40" ssid="6">As a decision rule, we obtain: This approach is a generalization of the sourcechannel approach (Brown et al., 1990).</S>
    <S sid="41" ssid="7">It has the advantage that additional models h(&#183;) can be easily integrated into the overall system.</S>
    <S sid="42" ssid="8">The model scaling factors &#955;M1 are trained with respect to the final translation quality measured by an error criterion (Och, 2003).</S>
    <S sid="43" ssid="9">We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon model.</S>
    <S sid="44" ssid="10">The latter two models are used for both directions: p(f|e) and p(e|f).</S>
    <S sid="45" ssid="11">Additionally, we use a word penalty and a phrase penalty.</S>
    <S sid="46" ssid="12">The reordering model of the baseline system is distancebased, i.e. it assigns costs based on the distance from the end position of a phrase to the start position of the next phrase.</S>
    <S sid="47" ssid="13">This very simple reordering model is widely used, for instance in (Och et al., 1999; Koehn, 2004; Zens et al., 2005).</S>
  </SECTION>
  <SECTION title="4 The Reordering Model" number="4">
    <S sid="48" ssid="1">In this section, we will describe the proposed discriminative reordering model.</S>
    <S sid="49" ssid="2">To make use of word level information, we need the word alignment within the phrase pairs.</S>
    <S sid="50" ssid="3">This can be easily stored during the extraction of the phrase pairs from the bilingual training corpus.</S>
    <S sid="51" ssid="4">If there are multiple possible alignments for a phrase pair, we use the most frequent one.</S>
    <S sid="52" ssid="5">The notation is introduced using the illustration in Figure 1.</S>
    <S sid="53" ssid="6">There is an example of a left and a right phrase orientation.</S>
    <S sid="54" ssid="7">We assume that we have already produced the three-word phrase in the lower part.</S>
    <S sid="55" ssid="8">Now, the model has to predict if the start position of the next phrase j&#8242; is to the left or to the right of the current phrase.</S>
    <S sid="56" ssid="9">The reordering model is applied only at the phrase boundaries.</S>
    <S sid="57" ssid="10">We assume that the reordering within the phrases is correct.</S>
    <S sid="58" ssid="11">In the remaining part of this section, we will describe the details of this reordering model.</S>
    <S sid="59" ssid="12">The classes our model predicts will be defined in Section 4.2.</S>
    <S sid="60" ssid="13">Then, the feature functions will be defined in Section 4.3.</S>
    <S sid="61" ssid="14">The training criterion and the training events of the maximum entropy model will be described in Section 4.4.</S>
    <S sid="62" ssid="15">Ideally, this model predicts the start position of the next phrase.</S>
    <S sid="63" ssid="16">But as predicting the exact position is rather difficult, we group the possible start positions into classes.</S>
    <S sid="64" ssid="17">In the simplest case, we use only two classes.</S>
    <S sid="65" ssid="18">One class for the positions to the left and one class for the positions to the right.</S>
    <S sid="66" ssid="19">As a refinement, we can use four classes instead of two: 1) one position to the left, 2) more than one positions to the left, 3) one position to the right, 4) more than one positions to the right.</S>
    <S sid="67" ssid="20">In general, we use a parameter D to specify 2 &#183; D classes of the types: Let cj,j&#8242; denote the orientation class for a movement from source position j to source position j&#8242; as illustrated in Figure 1.</S>
    <S sid="68" ssid="21">In the case of two orientation classes, cj,j&#8242; is defined as: r left, if j&#8242; &lt; j cj,j&#8242; =right, if j&#8242; &gt; j Then, the reordering model has the form p(cj,j&#8242;|fJ1 , eI1, i, j) A well-founded framework for directly modeling the probability p(cj,j&#8242;|fJ1 , eI1, i, j) is maximum entropy (Berger et al., 1996).</S>
    <S sid="69" ssid="22">In this framework, we have a set of N feature functions hn(fJ1 , eI1, i, j, cj,j&#8242;), n = 1, ... , N. Each feature function hn is weighted with a factor &#955;n.</S>
    <S sid="70" ssid="23">The resulting model is: The functional form is identical to Equation 2, but here we will use a large number of binary features, whereas in Equation 2 usually only a very small number of real-valued features is used.</S>
    <S sid="71" ssid="24">More precisely, the resulting reordering model p&#955;N1 (cj,j&#8242;|fJ1 , eI1, i, j) is used as an additional component in the log-linear combination of Equation 2.</S>
    <S sid="72" ssid="25">The feature functions of the reordering model depend on the last alignment link (j, i) of a phrase.</S>
    <S sid="73" ssid="26">Note that the source position j is not necessarily the end position of the source phrase.</S>
    <S sid="74" ssid="27">We use the source position j which is aligned to the last word of the target phrase in target position i.</S>
    <S sid="75" ssid="28">The illustration in Figure 1 contains such an example.</S>
    <S sid="76" ssid="29">To introduce generalization capabilities, some of the features will depend on word classes or partof-speech information.</S>
    <S sid="77" ssid="30">Let F1J denote the word class sequence that corresponds to the source language sentence fJ1 and let EI1 denote the target word class sequence that corresponds to the target language sentence eI1.</S>
    <S sid="78" ssid="31">Then, the feature functions are of the form hn(fJ1 , eI1, F1J , EI1, i, j, j&#8242;).</S>
    <S sid="79" ssid="32">We consider the following binary features: Here, &#948;(', ') denotes the Kronecker-function.</S>
    <S sid="80" ssid="33">In the experiments, we will use d E {&#8722;1, 0, 11.</S>
    <S sid="81" ssid="34">Many other feature functions are imaginable, e.g. combinations of the described feature functions, n-gram or multi-word features, joint source and target language feature functions.</S>
    <S sid="82" ssid="35">As training criterion, we use the maximum class posterior probability.</S>
    <S sid="83" ssid="36">This corresponds to maximizing the likelihood of the maximum entropy model.</S>
    <S sid="84" ssid="37">Since the optimization criterion is convex, there is only a single optimum and no convergence problems occur.</S>
    <S sid="85" ssid="38">To train the model parameters &#955;N1 , we use the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972).</S>
    <S sid="86" ssid="39">In practice, the training procedure tends to result in an overfitted model.</S>
    <S sid="87" ssid="40">To avoid overfitting, (Chen and Rosenfeld, 1999) have suggested a smoothing method where a Gaussian prior distribution of the parameters is assumed.</S>
    <S sid="88" ssid="41">This method tried to avoid very large lambda values and prevents features that occur only once for a specific class from getting a value of infinity.</S>
    <S sid="89" ssid="42">We train IBM Model 4 with GIZA++ (Och and Ney, 2003) in both translation directions.</S>
    <S sid="90" ssid="43">Then the alignments are symmetrized using a refined heuristic as described in (Och and Ney, 2003).</S>
    <S sid="91" ssid="44">This wordaligned bilingual corpus is used to train the reordering model parameters, i.e. the feature weights &#955;N1 .</S>
    <S sid="92" ssid="45">Each alignment link defines an event for the maximum entropy training.</S>
    <S sid="93" ssid="46">An exception are the oneto-many alignments, i.e. one source word is aligned to multiple target words.</S>
    <S sid="94" ssid="47">In this case, only the topmost alignment link is considered because the other ones cannot occur at a phrase boundary.</S>
    <S sid="95" ssid="48">Many-toone and many-to-many alignments are handled in a similar way.</S>
  </SECTION>
  <SECTION title="5 Experimental Results" number="5">
    <S sid="96" ssid="1">The experiments were carried out on the Basic Travel Expression Corpus (BTEC) task (Takezawa et al., 2002).</S>
    <S sid="97" ssid="2">This is a multilingual speech corpus which contains tourism-related sentences similar to those that are found in phrase books.</S>
    <S sid="98" ssid="3">We use the Arabic-English, the Chinese-English and the Japanese-English data.</S>
    <S sid="99" ssid="4">The corpus statistics are shown in Table 1.</S>
    <S sid="100" ssid="5">As the BTEC is a rather clean corpus, the preprocessing consisted mainly of tokenization, i.e., separating punctuation marks from words.</S>
    <S sid="101" ssid="6">Additionally, we replaced contractions such as it&#8217;s or I&#8217;m in the English corpus and we removed the case information.</S>
    <S sid="102" ssid="7">For Arabic, we removed the diacritics and we split common prefixes: Al, w, f, b, l. There was no special preprocessing for the Chinese and the Japanese training corpora.</S>
    <S sid="103" ssid="8">To train and evaluate the reordering model, we use the word aligned bilingual training corpus.</S>
    <S sid="104" ssid="9">For evaluating the classification power of the reordering model, we partition the corpus into a training part and a test part.</S>
    <S sid="105" ssid="10">In our experiments, we use about 10% of the corpus for testing and the remaining part for training the feature weights of the reordering model with the GIS algorithm using YASMET (Och, 2001).</S>
    <S sid="106" ssid="11">The statistics of the training and test alignment links is shown in Table 2.</S>
    <S sid="107" ssid="12">The number of training events ranges from 119K for JapaneseEnglish to 144K for Arabic-English.</S>
    <S sid="108" ssid="13">The word classes for the class-based features are trained using the mkcls tool (Och, 1999).</S>
    <S sid="109" ssid="14">In the experiments, we use 50 word classes.</S>
    <S sid="110" ssid="15">Alternatively, one could use part-of-speech information for this purpose.</S>
    <S sid="111" ssid="16">Additional experiments were carried out on the large data track of the Chinese-English NIST task.</S>
    <S sid="112" ssid="17">The corpus statistics of the bilingual training corpus are shown in Table 3.</S>
    <S sid="113" ssid="18">The language model was trained on the English part of the bilingual training corpus and additional monolingual English data from the GigaWord corpus.</S>
    <S sid="114" ssid="19">The total amount of language model training data was about 600M running words.</S>
    <S sid="115" ssid="20">We use a fourgram language model with modified Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002).</S>
    <S sid="116" ssid="21">For the four English reference translations of the evaluation sets, the accumulated statistics are presented.</S>
    <S sid="117" ssid="22">In this section, we present the classification results for the three language pairs.</S>
    <S sid="118" ssid="23">In Table 4, we present the classification results for two orientation classes.</S>
    <S sid="119" ssid="24">As baseline we always choose the most frequent orientation class.</S>
    <S sid="120" ssid="25">For Arabic-English, the baseline is with 6.3% already very low.</S>
    <S sid="121" ssid="26">This means that the word order in Arabic is very similar to the word order in English.</S>
    <S sid="122" ssid="27">For Chinese-English, the baseline is with 12.7% about twice as large.</S>
    <S sid="123" ssid="28">The most differences in word order occur for Japanese-English.</S>
    <S sid="124" ssid="29">This seems to be reasonable as Japanese has usually a different sentence structure, subject-objectverb compared to subject-verb-object in English.</S>
    <S sid="125" ssid="30">For each language pair, we present results for several combination of features.</S>
    <S sid="126" ssid="31">The three columns per language pair indicate if the features are based on the words (column label &#8217;Words&#8217;), on the word classes (column label &#8217;Classes&#8217;) or on both (column label &#8217;W+C&#8217;).</S>
    <S sid="127" ssid="32">We also distinguish if the features depend on the target sentence (&#8217;Tgt&#8217;), on the source sentence (&#8217;Src&#8217;) or on both (&#8217;Src+Tgt&#8217;).</S>
    <S sid="128" ssid="33">For Arabic-English, using features based only on words of the target sentence the classification error rate can be reduced to 4.5%.</S>
    <S sid="129" ssid="34">If the features are based only on the source sentence words, a classification error rate of 2.9% is reached.</S>
    <S sid="130" ssid="35">Combining the features based on source and target sentence words, a classification error rate of 2.8% can be achieved.</S>
    <S sid="131" ssid="36">Adding the features based on word classes, the classification error rate can be further improved to 2.1%.</S>
    <S sid="132" ssid="37">For the other language pairs, the results are similar except that the absolute values of the classification error rates are higher.</S>
    <S sid="133" ssid="38">We observe the following: These are desirable properties of an appropriate reordering model.</S>
    <S sid="134" ssid="39">The main point is that these are fulfilled not only on the training data, but on unseen test data.</S>
    <S sid="135" ssid="40">There seems to be no overfitting problem.</S>
    <S sid="136" ssid="41">In Table 5, we present the results for four orientation classes.</S>
    <S sid="137" ssid="42">The final error rates are a factor 2-4 larger than for two orientation classes.</S>
    <S sid="138" ssid="43">Despite that we observe the same tendencies as for two orientation classes.</S>
    <S sid="139" ssid="44">Again, using more features always helps to improve the performance.</S>
    <S sid="140" ssid="45">For the translation experiments on the BTEC task, we report the two accuracy measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) as well as the two error rates: word error rate (WER) and position-independent word error rate (PER).</S>
    <S sid="141" ssid="46">These criteria are computed with respect to 16 references.</S>
    <S sid="142" ssid="47">In Table 6, we show the translation results for the BTEC task.</S>
    <S sid="143" ssid="48">In these experiments, the reordering model uses two orientation classes, i.e. it predicts either a left or a right orientation.</S>
    <S sid="144" ssid="49">The features for the maximum-entropy based reordering model are based on the source and target language words within a window of one.</S>
    <S sid="145" ssid="50">The word-class based features are not used for the translation experiments.</S>
    <S sid="146" ssid="51">The maximum-entropy based reordering model achieves small but consistent improvement for all the evaluation criteria.</S>
    <S sid="147" ssid="52">Note that the baseline system, i.e. using the distance-based reordering, was among the best systems in the IWSLT 2005 evaluation campaign (Eck and Hori, 2005).</S>
    <S sid="148" ssid="53">Some translation examples are presented in Table 7.</S>
    <S sid="149" ssid="54">We observe that the system using the maximum-entropy based reordering model produces more fluent translations.</S>
    <S sid="150" ssid="55">Additional translation experiments were carried out on the large data track of the Chinese-English NIST task.</S>
    <S sid="151" ssid="56">For this task, we use only the BLEU and NIST scores.</S>
    <S sid="152" ssid="57">Both scores are computed caseinsensitive with respect to four reference translations using the mteval-v11b tool1.</S>
    <S sid="153" ssid="58">For the NIST task, we use the BLEU score as primary criterion which is optimized on the NIST 2002 evaluation set using the Downhill Simplex algorithm (Press et al., 2002).</S>
    <S sid="154" ssid="59">Note that only the eight or nine model scaling factors of Equation 2 are optimized using the Downhill Simplex algorithm.</S>
    <S sid="155" ssid="60">The feature weights of the reordering model are trained using the GIS algorithm as described in Section 4.4.</S>
    <S sid="156" ssid="61">We use a state-of-the-art baseline system which would have obtained a good rank in the last NIST evaluation (NIST, 2005).</S>
    <S sid="157" ssid="62">The translation results for the NIST task are presented in Table 8.</S>
    <S sid="158" ssid="63">We observe consistent improvements of the BLEU score on all evaluation sets.</S>
    <S sid="159" ssid="64">The overall improvement due to reordering ranges from 1.2% to 2.0% absolute.</S>
    <S sid="160" ssid="65">The contribution of the maximum-entropy based reordering model to this improvement is in the range of 25% to 58%, e.g. for the NIST 2003 evaluation set about 58% of the improvement using reordering can be attributed to the maximum-entropy based reordering model.</S>
    <S sid="161" ssid="66">We also measured the classification performance for the NIST task.</S>
    <S sid="162" ssid="67">The general tendencies are identical to the BTEC task.</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="6">
    <S sid="163" ssid="1">We have presented a novel discriminative reordering model for statistical machine translation.</S>
    <S sid="164" ssid="2">This model is trained on the word aligned bilingual corpus using the maximum entropy principle.</S>
    <S sid="165" ssid="3">Several types of features have been used: We have evaluated the performance of the reordering model on a held-out word-aligned corpus.</S>
    <S sid="166" ssid="4">We have shown that the model is able to predict the orientation very well, e.g. for Arabic-English the classification error rate is only 2.1%.</S>
    <S sid="167" ssid="5">We presented improved translation results for three language pairs on the BTEC task and for the large data track of the Chinese-English NIST task.</S>
    <S sid="168" ssid="6">In none of the cases additional features have hurt the classification performance on the held-out test corpus.</S>
    <S sid="169" ssid="7">This is a strong evidence that the maximum entropy framework is suitable for this task.</S>
    <S sid="170" ssid="8">Another advantage of our approach is the generalization capability via the use of word classes or part-of-speech information.</S>
    <S sid="171" ssid="9">Furthermore, additional features can be easily integrated into the maximum entropy framework.</S>
    <S sid="172" ssid="10">So far, the word classes were not used for the translation experiments.</S>
    <S sid="173" ssid="11">As the word classes help for the classification task, we might expect further improvements of the translation results.</S>
    <S sid="174" ssid="12">Using partof-speech information instead (or in addition) to the automatically computed word classes might also be beneficial.</S>
    <S sid="175" ssid="13">More fine-tuning of the reordering model toward translation quality might also result in improvements.</S>
    <S sid="176" ssid="14">As already mentioned in Section 4.3, a richer feature set could be helpful.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="177" ssid="1">This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.</S>
    <S sid="178" ssid="2">HR0011-06-C-0023, and was partly funded by the European Union under the integrated project TC-STAR (Technology and Corpora for Speech to Speech Translation, IST2002-FP6-506738, http://www.tc-star.org).</S>
  </SECTION>
</PAPER>
