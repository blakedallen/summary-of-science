<PAPER>
	<S sid="0">Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles</S><ABSTRACT>
		<S sid="1" ssid="1">We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabil istic generalized LR dependency parsing.</S>
		<S sid="2" ssid="2">Parser actions are determined by a classifier, based on features that represent the current state of the parser.</S>
		<S sid="3" ssid="3">We apply this pars ing framework to both tracks of the CoNLL 2007 shared task, in each case taking ad vantage of multiple models trained with different learners.</S>
		<S sid="4" ssid="4">In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme.</S>
		<S sid="5" ssid="5">In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="6" ssid="6">There are now several approaches for multilingual dependency parsing, as demonstrated in the CoNLL 2006 shared task (Buchholz and Marsi, 2006).</S>
			<S sid="7" ssid="7">The dependency parsing approach pre sented here extends the existing body of work mainly in four ways: 1.</S>
			<S sid="8" ssid="8">Although stepwise 1 dependency parsing has.</S>
			<S sid="9" ssid="9">commonly been performed using parsing algo 1 Stepwise parsing considers each step in a parsing algo rithm separately, while all-pairs parsing considers entire rithms designed specifically for this task, such as those described by Nivre (2003) and Yamada and Matsumoto (2003), we show that this can also be done using the well known LR parsing algorithm (Knuth, 1965), providing a connec tion between current research on shift-reduce dependency parsing and previous parsing work using LR and GLR models; wise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Rat naparkhi (1997) and later by Sagae and Lavie (2006); 3.</S>
			<S sid="10" ssid="10">We provide additional evidence that the parser.</S>
			<S sid="11" ssid="11">ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be ob tained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algo rithms are used, although this is not pursued here); and, finally, 4.</S>
			<S sid="12" ssid="12">We present a straightforward way to perform.</S>
			<S sid="13" ssid="13">parser domain adaptation using unlabeled data in the target domain.</S>
			<S sid="14" ssid="14">We entered a system based on the approach de scribed in this paper in the CoNLL 2007 shared trees.</S>
			<S sid="15" ssid="15">For a more complete definition, see the CoNLL X shared task description paper (Buchholz and Marsi, 2006).</S>
			<S sid="16" ssid="16">1044 task (Nivre et al, 2007), which differed from the 2006 edition by featuring two separate tracks, one in multilingual parsing, and a new track on domain adaptation for dependency parsers.</S>
			<S sid="17" ssid="17">In the multi lingual parsing track, participants train dependency parsers using treebanks provided for ten languages: Arabic (Hajic et al, 2004), Basque (Aduriz et al 2003), Catalan (Mart?</S>
			<S sid="18" ssid="18">et al, 2007), Chinese (Chen et al, 2003), Czech (B?hmova et al, 2003), Eng lish (Marcus et al, 1993; Johansson and Nugues, 2007), Greek (Prokopidis et al, 2005), Hungarian (Czendes et al, 2005), Italian (Montemagni et al, 2003), and Turkish (Oflazer et al, 2003).</S>
			<S sid="19" ssid="19">In the domain adaptation track, participants were pro vided with English training data from the Wall Street Journal portion of the Penn Treebank (Marcus et al, 1993) converted to dependencies (Jo hansson and Nugues, 2007) to train parsers to be evaluated on material in the biological (develop ment set) and chemical (test set) domains (Kulick et al, 2004), and optionally on text from the CHILDES database (MacWhinney, 2000; Brown, 1973).</S>
			<S sid="20" ssid="20">Our system?s accuracy was the highest in the domain adaptation track (with labeled attachment score of 81.06%), and only 0.43% below the top scoring system in the multilingual parsing track (our average labeled attachment score over the ten languages was 79.89%).</S>
			<S sid="21" ssid="21">We first describe our approach to multilingual dependency parsing, fol lowed by our approach for domain adaptation.</S>
			<S sid="22" ssid="22">We then provide an analysis of the results obtained with our system, and discuss possible improve ments.</S>
	</SECTION>
	<SECTION title="A Probabilistic LR Approach for De-" number="2">
			<S sid="23" ssid="1">pendency Parsing Our overall parsing approach uses a best-first probabilistic shift-reduce algorithm based on the LR algorithm (Knuth, 1965).</S>
			<S sid="24" ssid="2">As such, it follows a bottom-up strategy, or bottom-up-trees, as defined in Buchholz and Marsi (2006), in contrast to the shift-reduce dependency parsing algorithm described by Nivre (2003), which is a bottom-up/top down hybrid, or bottom-up-spans.</S>
			<S sid="25" ssid="3">It is unclear whether the use of a bottom-up-trees algorithm has any advantage over the use of a bottom-up-spans algorithm (or vice-versa) in practice, but the avail ability of different algorithms that perform the same parsing task could be advantageous in parser ensembles.</S>
			<S sid="26" ssid="4">The main difference between our pars er and a traditional LR parser is that we do not use an LR table derived from an explicit grammar to determine shift/reduce actions.</S>
			<S sid="27" ssid="5">Instead, we use a classifier with features derived from much of the same information contained in an LR table: the top few items on the stack, and the next few items of lookahead in the remaining input string.</S>
			<S sid="28" ssid="6">Addition ally, following Sagae and Lavie (2006), we extend the basic deterministic LR algorithm with a bestfirst search, which results in a parsing strategy sim ilar to generalized LR parsing (Tomita, 1987; 1990), except that we do not perform Tomita?s stack-merging operations.</S>
			<S sid="29" ssid="7">The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective trans formations as described in (Nivre and Nilsson, 2005).</S>
			<S sid="30" ssid="8">We use Nivre and Nilsson?s PATH scheme2.</S>
			<S sid="31" ssid="9">For clarity, we first describe the basic variant of the LR algorithm for dependency parsing, which is a deterministic stepwise algorithm.</S>
			<S sid="32" ssid="10">We then show how we extend the deterministic parser into a best first probabilistic parser.</S>
			<S sid="33" ssid="11">2.1 Dependency Parsing with a Data-Driven.</S>
			<S sid="34" ssid="12">Variant of the LR Algorithm The two main data structures in the algorithm are a stack S and a queue Q. S holds subtrees of the fi nal dependency tree for an input sentence, and Q holds the words in an input sentence.</S>
			<S sid="35" ssid="13">S is initia lized to be empty, and Q is initialized to hold every word in the input in order, so that the first word in the input is in the front of the queue.3 The parser performs two main types of actions: shift and reduce.</S>
			<S sid="36" ssid="14">When a shift action is taken, a word is shifted from the front of Q, and placed on the top of S (as a tree containing only one node, the word itself).</S>
			<S sid="37" ssid="15">When a reduce action is taken, the 2 The PATH scheme was chosen (even though Nivre and Nilsson report slightly better results with the HEAD scheme) because it does not result in a potentially qua dratic increase in the number of dependency label types, as observed with the HEAD and HEAD+PATH schemes.</S>
			<S sid="38" ssid="16">Unfortunately, experiments comparing the use of the different pseudo-projectivity schemes were not performed due to time constraints.</S>
			<S sid="39" ssid="17">3 We append a ?virtual root?</S>
			<S sid="40" ssid="18">word to the beginning of every sentence, which is used as the head of every word in the dependency structure that does not have a head in the sentence.</S>
			<S sid="41" ssid="19">1045 two top items in S (s1 and s2) are popped, and a new item is pushed onto S. This new item is a tree formed by making the root s1 of a dependent of the root of s2, or the root of s2 a dependent of the root of s1.</S>
			<S sid="42" ssid="20">Depending on which of these two cases oc cur, we call the action reduce-left or reduce-right, according to whether the head of the new tree is to the left or to the right its new dependent.</S>
			<S sid="43" ssid="21">In addi tion to deciding the direction of a reduce action, the label of the newly formed dependency arc must also be decided.</S>
			<S sid="44" ssid="22">Parsing terminates successfully when Q is emp ty (all words in the input have been processed) and S contains only a single tree (the final dependency tree for the input sentence).</S>
			<S sid="45" ssid="23">If Q is empty, S contains two or more items, and no further reduce ac tions can be taken, parsing terminates and the input is rejected.</S>
			<S sid="46" ssid="24">In such cases, the remaining items in S contain partial analyses for contiguous segments of the input.</S>
			<S sid="47" ssid="25">2.2 A Probabilistic LR Model for Dependen cy Parsing In the traditional LR algorithm, parser states are placed onto the stack, and an LR table is consulted to determine the next parser action.</S>
			<S sid="48" ssid="26">In our case, the parser state is encoded as a set of features de rived from the contents of the stack S and queue Q, and the next parser action is determined according to that set of features.</S>
			<S sid="49" ssid="27">In the deterministic case described above, the procedure used for determin ing parser actions (a classifier, in our case) returns a single action.</S>
			<S sid="50" ssid="28">If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model simi lar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation.</S>
			<S sid="51" ssid="29">To find the most probable parse tree according to the probabilistic LR model, we use a best-first strategy.</S>
			<S sid="52" ssid="30">This involves an extension of the deter ministic shift-reduce into a best-first shift-reduce algorithm.</S>
			<S sid="53" ssid="31">To describe this extension, we first in troduce a new data structure Ti that represents a parser state, which includes a stack Si, a queue Qi, and a probability Pi.</S>
			<S sid="54" ssid="32">The deterministic algorithm is a special case of the probabilistic algorithm where we have a single parser state T0 that contains S0 and Q0, and the probability of the parser state is 1.</S>
			<S sid="55" ssid="33">The best-first algorithm, on the other hand,.</S>
			<S sid="56" ssid="34">keeps a heap H containing multiple parser states T0...</S>
			<S sid="57" ssid="35">Tm.</S>
			<S sid="58" ssid="36">These states are ordered in the heap ac cording to their probabilities, which are determined by multiplying the probabilities of each of the parser actions that resulted in that parser state.</S>
			<S sid="59" ssid="37">The heap H is initialized to contain a single parser state T0, which contains a stack S0, a queue Q0 and prob ability P0 = 1.0.</S>
			<S sid="60" ssid="38">S0 and Q0 are initialized in the same way as S and Q in the deterministic algo rithm.</S>
			<S sid="61" ssid="39">The best-first algorithm then loops while H is non-empty.</S>
			<S sid="62" ssid="40">At each iteration, first a state Tcurrent is popped from the top of H. If Tcurrent corresponds to a final state (Qcurrent is empty and Scurrent contains a single item), we return the single item in Scurrent as the dependency structure corresponding to the input sentence.</S>
			<S sid="63" ssid="41">Otherwise, we get a list of parser actions act0...actn (with associated probabilities Pact0...Pactn) corresponding to state Tcurrent.</S>
			<S sid="64" ssid="42">For each of these parser actions actj, we create a new parser state Tnew by applying actj to Tcurrent, and set the probability Tnew to be Pnew = Pcurrnet * Pactj.</S>
			<S sid="65" ssid="43">Then, Tnew is inserted into the heap H. Once new states have been inserted onto H for each of the n parser actions, we move on to the next iteration of the algorithm.</S>
	</SECTION>
	<SECTION title="Multilingual Parsing Experiments. " number="3">
			<S sid="66" ssid="1">For each of the ten languages for which training data was provided in the multilingual track of the CoNLL 2007 shared task, we trained three LR models as follows.</S>
			<S sid="67" ssid="2">The first LR model for each language uses maximum entropy classification (Berger et al, 1996) to determine possible parser actions and their probabilities4.</S>
			<S sid="68" ssid="3">To control overfitting in the MaxEnt models, we used box-type in equality constraints (Kazama and Tsujii, 2003).</S>
			<S sid="69" ssid="4">The second LR model for each language also uses MaxEnt classification, but parsing is performed backwards, which is accomplished simply by reversing the input string before parsing starts.</S>
			<S sid="70" ssid="5">Sa gae and Lavie (2006a) and Zeman and ?abokrtsk?</S>
			<S sid="71" ssid="6">(2005) have observed that reversing the direction of stepwise parsers can be beneficial in parser combinations.</S>
			<S sid="72" ssid="7">The third model uses support vector machines 5 (Vapnik, 1995) using the polynomial 4 Implementation by Yoshimasa Tsuruoka, available at http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/maxent/ 5 Implementation by Taku Kudo, available at http://chasen.org/~taku/software/TinySVM/ and all vs. all was used for multi-class classification.</S>
			<S sid="73" ssid="8">1046 kernel with degree 2.</S>
			<S sid="74" ssid="9">Probabilities were estimated for SVM outputs using the method described in (Platt, 1999), but accuracy improvements were not observed during development when these esti mated probabilities were used instead of simply the single best action given by the classifier (with probability 1.0), so in practice the SVM parsing models we used were deterministic.</S>
			<S sid="75" ssid="10">At test time, each input sentence is parsed using each of the three LR models, and the three result ing dependency structures are combined according to the maximum-spanning-tree parser combination scheme6 (Sagae and Lavie, 2006a) where each de pendency proposed by each of the models has the same weight (it is possible that one of the more sophisticated weighting schemes proposed by Sa gae and Lavie may be more effective, but these were not attempted).</S>
			<S sid="76" ssid="11">The combined dependency tree is the final analysis for the input sentence.</S>
			<S sid="77" ssid="12">Although it is clear that fine-tuning could provide accuracy improvements for each of the models in each language, the same set of meta parameters and features were used for all of the ten languages, due to time constraints during system development.</S>
			<S sid="78" ssid="13">The features used were7: ? For the subtrees in S(1) and S(2) ? the number of children of the root word of the subtrees; ? the number of children of the root word of the subtree to the right of the root word; ? the number of children of the root word of the subtree to the left of the root word; ? the POS tag and DEPREL of the rightmost and leftmost children; ? The POS tag of the word immediately to the right of the root word of S(2); ? The POS tag of the word immediately to the left of S(1); 6 Each dependency tree is deprojectivized before the combination occurs.</S>
			<S sid="79" ssid="14">7 S(n) denotes the nth item from the top of the stack (where S(1) is the item on top of the stack), and Q(n) denotes the nth item in the queue.</S>
			<S sid="80" ssid="15">For a description of the features names in capital letters, see the shared task description (Nivre et al, 2007).</S>
			<S sid="81" ssid="16">The previous parser action; ? The features listed for the root words of the subtrees in table 1.</S>
			<S sid="82" ssid="17">In addition, the MaxEnt models also used selected combinations of these features.</S>
			<S sid="83" ssid="18">The classes used to represent parser actions were designed to encode all aspects of an action (shift vs. reduce, right vs. left, and dependency label) simultaneously.</S>
			<S sid="84" ssid="19">Results for each of the ten languages are shown in table 2 as labeled and unlabeled attachment scores, along with the average labeled attachment score and highest labeled attachment score for all participants in the shared task.</S>
			<S sid="85" ssid="20">Our results shown in boldface were among the top three scores for those particular languages (five out of the ten lan guages).</S>
			<S sid="86" ssid="21">S(1) S(2) S(3) Q(0) Q(1) Q(3) WORD x x x x x LEMMA x x x POS x x x x x x CPOS x x x FEATS x x x Table 1: Additional features.</S>
			<S sid="87" ssid="22">Language LAS UAS Avg LAS Top LAS Arabic 74.71 84.04 68.34 76.52 Basque 74.64 81.19 68.06 76.94 Catalan 88.16 93.34 79.85 88.70 Chinese 84.69 88.94 76.59 84.69 Czech 74.83 81.27 70.12 80.19 English 89.01 89.87 80.95 89.61 Greek 73.58 80.37 70.22 76.31 Hungarian 79.53 83.51 71.49 80.27 Italian 83.91 87.68 78.06 84.40 Turkish 75.91 82.72 70.06 79.81 ALL 79.90 85.29 65.50 80.32 Table 2: Multilingual results.</S>
	</SECTION>
	<SECTION title="Domain Adaptation Experiments. " number="4">
			<S sid="88" ssid="1">In a similar way as we used multiple LR models in the multilingual track, in the domain adaptation track we first trained two LR models on the out-of 1047domain labeled training data.</S>
			<S sid="89" ssid="2">The first was a forward MaxEnt model, and the second was a back ward SVM model.</S>
			<S sid="90" ssid="3">We used these two models to perform a procedure similar to a single iteration of co-training, except that selection of the newly (au tomatically) produced training instances was done by selecting sentences for which the two models produced identical analyses.</S>
			<S sid="91" ssid="4">On the development data we verified that sentences for which there was perfect agreement between the two models had labeled attachment score just above 90 on average, even though each of the models had accuracy be tween 78 and 79 over the entire development set.</S>
			<S sid="92" ssid="5">Our approach was as follows: 1.</S>
			<S sid="93" ssid="6">We trained the forward MaxEnt and backward.</S>
			<S sid="94" ssid="7">SVM models using the out-of-domain labeled training data; 2.</S>
			<S sid="95" ssid="8">We then used each of the models to parse the.</S>
			<S sid="96" ssid="9">first two of the three sets of domain-specific unlabeled data that were provided (we did not use the larger third set) 3.</S>
			<S sid="97" ssid="10">We compared the output for the two models,.</S>
			<S sid="98" ssid="11">and selected only identical analyses that were produced by each of the two separate models; 4.</S>
			<S sid="99" ssid="12">We added those analyses (about 200k words in.</S>
			<S sid="100" ssid="13">the test domain) to the original (out-of domain) labeled training set; the new larger training set; and finally 6.</S>
			<S sid="101" ssid="14">We used this model to parse the test data..</S>
			<S sid="102" ssid="15">Following this procedure we obtained a labeled attachment score of 81.06, and unlabeled attach ment score of 83.42, both the highest scores for this track.</S>
			<S sid="103" ssid="16">This was done without the use of any additional resources (closed track), but these re sults are also higher than the top score for the open track, where the use of certain additional resources was allowed.</S>
			<S sid="104" ssid="17">See (Nivre et al, 2007).</S>
	</SECTION>
	<SECTION title="Analysis and Discussion. " number="5">
			<S sid="105" ssid="1">One of the main assumptions in our use of differ ent models based on the same algorithm is that while the output generated by those models may often differ, agreement between the models is an indication of correctness.</S>
			<S sid="106" ssid="2">In our domain adapta tion approach, this was clearly true.</S>
			<S sid="107" ssid="3">In fact, the approach would not have worked if this assump tion was false.</S>
			<S sid="108" ssid="4">Experiments on the development set were encouraging.</S>
			<S sid="109" ssid="5">As stated before, when the parsers agreed, labeled attachment score was over 90, even though the score of each model alone was lower than 79.</S>
			<S sid="110" ssid="6">The domain-adapted parser had a score of 82.1, a significant improvement.</S>
			<S sid="111" ssid="7">Interes tingly, the ensemble used in the multilingual track also produced good results on the development set for the domain adaptation data, without the use of the unlabeled data at all, with a score of 81.9 (al though the ensemble is more expensive to run).</S>
			<S sid="112" ssid="8">The different models used in each track were distinct in a few ways: (1) direction (forward or backward); (2) learner (MaxEnt or SVM); and (3) search strategy (best-first or deterministic).</S>
			<S sid="113" ssid="9">Of those differences, the first one is particularly inter esting in single-stack shift-reduce models, as ours.</S>
			<S sid="114" ssid="10">In these models, the context to each side of a (po tential) dependency differs in a fundamental way.</S>
			<S sid="115" ssid="11">To one side, we have tokens that have already been processed and are already in subtrees, and to the other side we simply have a look-ahead of the re maining input sentence.</S>
			<S sid="116" ssid="12">This way, the context of the same dependency in a forward parser may differ significantly from the context of the same de pendency in a backward parser.</S>
			<S sid="117" ssid="13">Interestingly, the accuracy scores of the MaxEnt backward models were found to be generally just below the accuracy of their corresponding forward models when tested on development data, with two exceptions: Hunga rian and Turkish.</S>
			<S sid="118" ssid="14">In Hungarian, the accuracy scores produced by the forward and backward MaxEnt LR models were not significantly differ ent, with both labeled attachment scores at about 77.3 (the SVM model score was 76.1, and the final combination score on development data was 79.3).</S>
			<S sid="119" ssid="15">In Turkish, however, the backward score was sig nificantly higher than the forward score, 75.0 and 72.3, respectively.</S>
			<S sid="120" ssid="16">The forward SVM score was 73.1, and the combined score was 75.8.</S>
			<S sid="121" ssid="17">In expe riments performed after the official submission of results, we evaluated a backward SVM model (which was trained after submission) on the same development set, and found it to be significantly more accurate than the forward model, with a score of 75.7.</S>
			<S sid="122" ssid="18">Adding that score to the combination raised the combination score to 77.9 (a large im provement from 75.8).</S>
			<S sid="123" ssid="19">The likely reason for this difference is that over 80% of the dependencies in the Turkish data set have the head to the right of 1048 the dependent, while only less than 4% have the head to the left.</S>
			<S sid="124" ssid="20">This means that the backward model builds much more partial structure in the stack as it consumes input tokens, while the for ward model must consume most tokens before it starts making attachments.</S>
			<S sid="125" ssid="21">In other words, context in general in the backward model has more struc ture, and attachments are made while there are still look-ahead tokens, while the opposite is generally true in the forward model.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="6">
			<S sid="126" ssid="1">Our results demonstrate the effectiveness of even small ensembles of parsers that are relatively similar (using the same features and the same algorithm).</S>
			<S sid="127" ssid="2">There are several possible extensions and improvements to the approach we have described.</S>
			<S sid="128" ssid="3">For example, in section 3 we mention the use of different weighting schemes in dependency voting.</S>
			<S sid="129" ssid="4">We list additional ideas that were not attempted due to time constraints, but that are likely to produce improved results.</S>
			<S sid="130" ssid="5">One of the simplest improvements to our approach is simply to train more models with no oth er changes to our set-up.</S>
			<S sid="131" ssid="6">As mentioned in section 5, the addition of a backward SVM model did im prove accuracy on the Turkish set significantly, and it is likely that improvements would also be obtained in other languages.</S>
			<S sid="132" ssid="7">In addition, other learning approaches, such as memory-based lan guage processing (Daelemans and Van den Bosch, 2005), could be used.</S>
			<S sid="133" ssid="8">A drawback of adding more models that became obvious in our experiments was the increased cost of both training (for example, the SVM parsers we used required significant ly longer to train than the MaxEnt parsers) and run-time (parsing with MBL models can be several times slower than with MaxEnt, or even SVM).</S>
			<S sid="134" ssid="9">A similar idea that may be more effective, but requires more effort, is to add parsers based on dif ferent approaches.</S>
			<S sid="135" ssid="10">For example, using MSTParser (McDonald and Pereira, 2005), a large-margin all pairs parser, in our domain adaptation procedure results in significantly improved accuracy (83.2 LAS).</S>
			<S sid="136" ssid="11">Of course, the use of different approaches used by different groups in the CoNLL 2006 and 2007 shared tasks represents great opportunity for parser ensembles.</S>
			<S sid="137" ssid="12">Acknowledgements We thank the shared task organizers and treebank providers.</S>
			<S sid="138" ssid="13">We also thank the reviewers for their comments and suggestions, and Yusuke Miyao for insightful discussions.</S>
			<S sid="139" ssid="14">This work was supported in part by Grant-in-Aid for Specially Promoted Re search 18002007.</S>
	</SECTION>
</PAPER>
