Fast Cheap and Creative: Evaluating Translation Quality Using Amazon&rsquo;s Mechanical Turk
Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive.
We explore a fast and inexpensive way of doing it using Amazonâ€™s Mechanical Turk to pay small sums to a large number of non-expert annotators.
For $10 we redundantly recreate judgments from a WMT08 translation task.
We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does.
We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.
We find that lazy annotators tended to stay longer and do more annotations.
We treat evaluation as a weighted voting problem where each annotator's contribution is weighted by agreement with either a gold standard or with other annotators.
We show the effectiveness of crowd sourcing as a method of accomplishing labor intensive natural language processing tasks.
