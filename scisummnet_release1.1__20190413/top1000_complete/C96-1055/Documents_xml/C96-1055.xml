<PAPER>
  <S sid="0" ssid="0">Role of Word Sense Disambiguation i  Lexical Acquisition: Predicting Semantics from Syntactic Cues Bonn ie  J. Dor r  and Doug Jones Depar tment  of Computer  Sc ience and Ins t i tu te  for Advanced Computer  Stud ies Un ivers i ty  of Mary land A.V.</S>
  <S sid="1" ssid="1">Wi l l i ams Bu i ld ing Col lege Park ,  MD 20742 {bonnie, j ones}~umiacs, umd.</S>
  <S sid="2" ssid="2">edu ABSTRACT This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the con- struction of large-scale knowledge sources.</S>
  <S sid="3" ssid="3">We describe two experiments: one which ignored word-sense distinctions, re- sulting in 6.3% accuracy for semantic lassification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy.</S>
  <S sid="4" ssid="4">These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate hat a 15-fold improvement can be achieved in deriving semantic information from syntactic ues if we first divide the syntac- tic cues into distinct groupings that correlate with different word senses.</S>
  <S sid="5" ssid="5">Finally, we show that we can provide effective acquisition techniques for novel word senses using a combi- nation of online sources.</S>
  <S sid="6" ssid="6">1 In t roduct ion This paper addresses the issue of word-sense ambigu- ity in extraction from machine-readable resources for the construction of large-scale knowledge sources.</S>
  <S sid="7" ssid="7">We describe two experiments: one which ignored word- sense distinctions, resulting in 6.3% accuracy for seman- tic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy.</S>
  <S sid="8" ssid="8">These experiments were dual pur- pose: (l) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic be- havior are predictably related; (2) to demonstrate hat a 15-fold improvement can be achieved in deriving se- mantic information from syntactic ues if we first divide the syntactic ues into distinct groupings that correlate with different word senses.</S>
  <S sid="9" ssid="9">Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources, in particular, Longmans Dictionary of Contemporary En- glish (LDOCE) (Procter, 1978), Levins verb classifica- tion scheme (Levin, 1993), and WordNet (Miller, 1985).</S>
  <S sid="10" ssid="10">We have used these techniques to build a database of 10,000 English verb entries containing semantic infor- mation that we are currently porting into languages such as Arabic, Spanish, and Korean for multilingual NLP tasks such as foreign language tutoring and ma- chine translation.</S>
  <S sid="11" ssid="11">322 2 Automat ic  Lex ica l  Acqu is i t ion  fo r NLP  Tasks As machine-readable resources (i.e., online dictionaries, thesauri, and other knowledge sources) become read- ily available to NLP researchers, automated acquisition has become increasingly more attractive.</S>
  <S sid="12" ssid="12">Several re- searchers have noted that the average time needed to construct a lexical entry can be as much as 30 min- utes (see, e.g., (Neff and McCord, 1990; Copestakc et al., 1995; Walker and Amsler, 1986)).</S>
  <S sid="13" ssid="13">Given that we are aiming for large-scale lexicons of 20-60,000 words, automation of the acquisition process has become a ne- cessity.</S>
  <S sid="14" ssid="14">Previous research in automatic acquisition focuscs primarily on the use of statistical techniques, such as bilingual alignment (Church and Hanks, 1990; Kla- vans and Tzoukermann, 1996; Wu and Xia, 1995), or extraction of syntactic constructions from online dic- tionaries and corpora (Brant, 1993; Dorr, Garman, and Weinberg, 1995).</S>
  <S sid="15" ssid="15">Others who have taken a more knowledge-based (interlingual) approach (Lonsdale, Mi- tamura, and Nyberg, 1996) do not provide a means for systematically deriving the relation between sur- face syntactic structures and their underlying semantic representations.</S>
  <S sid="16" ssid="16">Those who have taken more argument structures into account, e.g., (Copestake t al., 1995), do not take full advantage of the systematic relation be- tween syntax and semantics during lexical acquisition.</S>
  <S sid="17" ssid="17">We adopt the central thesis of Levin (1993), i.e., that the semantic class of a verb and its syntactic behav- ior are predictably related.</S>
  <S sid="18" ssid="18">We base our work on a correlation between semantic classes and patterns of grammar codes in the Longmans Dictionary of Con- temporary English (LDOCE) (Procter, 1978).</S>
  <S sid="19" ssid="19">While the LDOCE has been used previously in automatic x- traction tasks (Alshawi, 1989; Farwell, Guthrie, and Wilks, 1993; Boguraev and Briscoe, 1989; ,Wilks et al., 1989; Wilks et al., 1990) these tasks are primarily con- cerned with the extraction of other types of informa- tion including syntactic phrase structure and broad ar- gument restrictions or with the derivation of semantic structures from definition analyses.</S>
  <S sid="20" ssid="20">The work of San- filippo and Poznanski (1992) is more closely related to our approach in that it attempts to recover a syntactic- semantic relation from machine-readable dictionaries.</S>
  <S sid="21" ssid="21">Itowever, they claim that the semantic lassification of verbs based on standard machine-readable dictionaries (e.g., the LDOCE) is % hopeless pursuit [since] stan- dard dictionaries are simply not equipped to offer this kind of information with consistency and exhaustive- ness."</S>
  <S sid="22" ssid="22">Others have also argued that the task of simplify- in K lexical entries on the basis of broad semantic lass membership is complex and, perhaps, infeasible (see, e.g., Boguraev and llriscoe (1989)).</S>
  <S sid="23" ssid="23">tlowever, a number of researchers (l,ilhnore, 1968; Grimshaw, 1990; Gru- ber, 1965; Guthrie et al., 1991; Hearst, 1991; Jackend- otr, 1983; Jackendoff, 1990; l,evin, 1993; Pinker, t989; Yarowsky, 1992) have demonstrated conclusively that there is a clear relationship between syntactic context and word senses; it is our aim to exploit this relationship for the acquisition of semantic lexicons.</S>
  <S sid="24" ssid="24">3 Syntax-Semantics Relation: -Verb Classification Based on Syntactic Behavior The central thesis of (Levin, 1993) is that the seman- tics of a verb and its syntactic behavior are predictably related.</S>
  <S sid="25" ssid="25">As a demonstration that such predictable rela- tionships are not confined to an insignificant portion of the vocabulary, Levin surveys 4183 verbs, grouped into 191 semantic lasses in Part Two of her book.</S>
  <S sid="26" ssid="26">The syn- tactic behavior of these classes is illustrated with 1668 example sentences, an average of 8 sentences per (:lass.</S>
  <S sid="27" ssid="27">Given the scope of bevins work, it is not easy to verify the central thesis.</S>
  <S sid="28" ssid="28">lb this end, we created a database of Levins verb classes and example sentences from each class, and wrote a parser to extract, basic syn- tactic patterns from tire sentences.1 We then character- ized each semantic lass by a set of syntactic patterns, which we call a syntactic signature, and used the re- suiting database as the basis of two experiments, both designed to to discover whether the syntactic signatures tell us anything about the meaning of the verbs.</S>
  <S sid="29" ssid="29">2 [he first experiment, which we label Verb-Based, ignores word-sense distinctions by assigning one syntactic sig- nature to each verb, regardless of whether it occurred in multiple classes.</S>
  <S sid="30" ssid="30">The second experiment, which we label Class-Based, implicitly takes word-sense distinc- tions into account by considering each occurrence of a verb individually and assigning it a single syntactic sig- nature according to class membership.</S>
  <S sid="31" ssid="31">The remainder of this section describes the assign- rnent of signatures to semantic busses and the two ex- periments for determining the relation of syntactic in- formation to semantic btsses.</S>
  <S sid="32" ssid="32">We will see that our clas- sitication technique shows a 15-fold improvement in the experiment where we implicitly account for word-sense distinctions.</S>
  <S sid="33" ssid="33">1Both the database and the parser are encoded in Quin- tus Prolog.</S>
  <S sid="34" ssid="34">2The design of this experiment is inspired by the work of (Dubois and Saint-Dizier, 1995).</S>
  <S sid="35" ssid="35">In particular, we depart from the alternation-based data in (Levin, 1993), which is primarily binary in that sentences are presented in pairs which constitute an alternation.</S>
  <S sid="36" ssid="36">Following Saint-Diziers work, we construct N-ary syntactic haracterizations.</S>
  <S sid="37" ssid="37">The choice is of no empirieM consequence, but it simplifms the experiment by eliminating the problem of naming the syn- tactic patterns.</S>
  <S sid="38" ssid="38">Verbs: break, chip, crack, crash, crush, fracture, rip, shatter, slnash, snap, sl)linter, split, tear Example Sentences: Crystal vases break easily.</S>
  <S sid="39" ssid="39">The hammer broke the window.</S>
  <S sid="40" ssid="40">The window broke.</S>
  <S sid="41" ssid="41">qony broke her arm.</S>
  <S sid="42" ssid="42">l?ony broke his finger.</S>
  <S sid="43" ssid="43">"lbny broke the crystal vase.</S>
  <S sid="44" ssid="44">qbny broke the cup against he wall.</S>
  <S sid="45" ssid="45">qony broke the glass to 1)ieces.</S>
  <S sid="46" ssid="46">Tony broke the piggy bank open.</S>
  <S sid="47" ssid="47">Tony broke the window with a hanuner.</S>
  <S sid="48" ssid="48">Tony broke the window.</S>
  <S sid="49" ssid="49">*Tony broke at tit(; window.</S>
  <S sid="50" ssid="50">*qbny broke herself on the arm.</S>
  <S sid="51" ssid="51">*Tony broke himself.</S>
  <S sid="52" ssid="52">*qbny broke the wall with the cup.</S>
  <S sid="53" ssid="53">Derived Syntactic Signature: 1-[np,v] 1-[np,v,np] 1 -  [np ,v ,np ,ad ject iw ] 1- [np, v, np ,pp(against) ] l-[np,v,np,pp(to)] 1- [np, v, np,pp (with) ] 1- [np, v, pess, np] 1-  [np,v,adv(easi ly)  ]  l - in ] O-[np,v,np,pp(with)] 0-  [np,v ,se l f  ] O-[np,v,seH,pp(on)] 0-  [np,v,pp(at)  ] Table 1: Syntactic Signatm:e for Change of State break subclass 3.1 Ass lgntnent  of  S ignatures For tile first experiment below, we construct a verb- based syntactic signature, while for the second exl)eri- ment, we constructed a class-based signature.</S>
  <S sid="54" ssid="54">The first step for constructing a signature is to decide what syntactic information to extract for ttre t)asic syntactic patterns that make up the signature.</S>
  <S sid="55" ssid="55">It turns out that a very simple strategy works well, namely, flat parses that contain lists of the major cat- egories in the sentence, the verb, and a handfifl of other elements.</S>
  <S sid="56" ssid="56">The "parse", then, for the sentence Tony broke the c rys ta l  vase is simply the syntac- tic pattern [np,v,np].</S>
  <S sid="57" ssid="57">For Tony broke the vase to pieces we get [np,v,np,pp(to)].</S>
  <S sid="58" ssid="58">Note that the pp node is marked with its head preposition.</S>
  <S sid="59" ssid="59">Table l shows an example class, the break subclass of the Change of State verbs (45.1), along with example sentences and the derived syntactic signature based on sentence pat- terns.</S>
  <S sid="60" ssid="60">Positive example sentences are denoted by the number 1 in the sentence patterns and negative xample sentences are denoted by the number 0 (corresponding to sentences marked with a *).</S>
  <S sid="61" ssid="61">3.2 Exper iment  1: Verb -based  Approach In the first experiment, we ignored word sense distinc- tions and considered each verb only once, regardless of whether it occurred in multiple classes.</S>
  <S sid="62" ssid="62">In fact;, 46% of the verbs appear more than once.</S>
  <S sid="63" ssid="63">In some cases, the verb appears to have a related sense even though it appears in different classes.</S>
  <S sid="64" ssid="64">For example, the verb roll appears in two subclasses of Manner of Motion Verbs that are distinguished on the basis of whether the gram- matical subject is animate or inanimate.</S>
  <S sid="65" ssid="65">In other cases, tile verb may have (largely) unrelated senses.</S>
  <S sid="66" ssid="66">For ex- ample, the verb move is both a Manner of Motion verb 323 and verb of Psychological State.</S>
  <S sid="67" ssid="67">To compose the syntactic signatures for each verb, we collect all of the syntactic patterns associated with every class a particular verb appears in, regardless of the different classes are semantically related.</S>
  <S sid="68" ssid="68">A syntactic signature for a verb, by definition, is the union of the frames extracted from every example sentence for each verb.</S>
  <S sid="69" ssid="69">The outline of the verb-based experiment is as follows: 1.</S>
  <S sid="70" ssid="70">Automatically extract syntactic information from the example sentences.</S>
  <S sid="71" ssid="71">Group the verbs according to their syntactic signature.</S>
  <S sid="72" ssid="72">Determine where the two ways of grouping verbs over- lap: (a) the semantic lassification given by Levin.</S>
  <S sid="73" ssid="73">(1)) the syntactic classification based on the derived syntactic signatures.</S>
  <S sid="74" ssid="74">To return to the Change of State verbs, we now con- sider the syntactic signature of the verb break, rather than the signature of the semantic lass as a unit.</S>
  <S sid="75" ssid="75">The verb break belongs not only to the Change of State class, but also four other classes: 10.6 Cheat, 23.2 Split, 40.8.3 Hurl, and 48.1.1 Appear.</S>
  <S sid="76" ssid="76">Each of these classes is characterized syntactically with a set of sentences.</S>
  <S sid="77" ssid="77">The union of the syntactic patterns corresponding to these sentences forms the syntactic signature for the verb.</S>
  <S sid="78" ssid="78">So although the signature for the Change of State class has 13 frames, the verb break has 39 frames from the other classes it appears in.</S>
  <S sid="79" ssid="79">Conceptually, it is helpful to consider the difference between the intension of a function versus its exten- sion.</S>
  <S sid="80" ssid="80">In this case, we are interested in the functions that group the verbs syntactically and semantically.</S>
  <S sid="81" ssid="81">In- tensionally speaking, the definition of the function that groups verbs semantically would have something to do with the actual meaning of the verbs.</S>
  <S sid="82" ssid="82">~ Likewise, the in- tension of the function that groups verbs syntactically would be defined in terms of something strictly syntac- tic, such as subcategorization frames.</S>
  <S sid="83" ssid="83">But the inten- sions of these functions are matters of significant he- oretical investigation, and although much has been ac- complished in this ~rea, the question of mapping syntax to semantics and vice versa is an open research topic.</S>
  <S sid="84" ssid="84">Therefore, we can turn to the extensions of the func- tions: the actual groupings of verbs, based on these two separate criteria.</S>
  <S sid="85" ssid="85">The semantic extensions are sets of verb tokens, and likewise, the syntactic extensions are sets of verb tokens.</S>
  <S sid="86" ssid="86">To the extent that these functions map between syntax and semantics intensionally, they will pick out the same verbs extensionally.</S>
  <S sid="87" ssid="87">So for the verb-based experiment, our technique for establishing the relatedness between the syntactic signa- tures and the semantic lasses, is mediated by the verbs themselves.</S>
  <S sid="88" ssid="88">We compare the two orthogonal groupings of the inventory of verbs: the semantic lasses defined by Levin and the sets of verbs that correspond to each of the derived syntactic signatures.</S>
  <S sid="89" ssid="89">When these two groupings overlap, we have discovered a mapping from the syntax of the verbs to their semantics, via the verb tokens.</S>
  <S sid="90" ssid="90">More specifically, we define the overlap index 3An example of the intensional characterization of the Levin classes are the definitions of Lexical Conceptual Struc- tures which correspond to each of Levins semantic lasses.</S>
  <S sid="91" ssid="91">See (Dorr and Voss, to appear).</S>
  <S sid="92" ssid="92">as the number of overlapping verbs divided by the av- erage of the number of verbs in the semantic lass and the number of verbs in the syntactic signature.</S>
  <S sid="93" ssid="93">Thus an overlap index of 1.00 is a complete overlap and an over- lap of 0 is completely disjoint.</S>
  <S sid="94" ssid="94">In this experiment, he sets of verbs with a high overlap index are of interest.</S>
  <S sid="95" ssid="95">When we parsed the 1668 example sentences in Part Two of Levins book (including the negative xamples), these sentences reduce to 282 unique patterns.</S>
  <S sid="96" ssid="96">The 191 sets of sentences listed with each of the 191 semantic classes in turn reduces to 748 distinct syntactic signa- tures.</S>
  <S sid="97" ssid="97">Since there are far more syntactic signatures than the 191 semantic lasses, it is clear that the mapping between signatures and semantic classes is not direct,.</S>
  <S sid="98" ssid="98">Only 12 mappings have complete overlaps.</S>
  <S sid="99" ssid="99">That means 6.3% of the 191 semantic lasses have a complete over- lap with a syntactic signature.</S>
  <S sid="100" ssid="100">The results of this experiment are shown in Table 2.</S>
  <S sid="101" ssid="101">Three values are shown for each of the six variations in the experiment: the mean overlap, the median overlap, and the percentage of perfect overlaps (overlaps of value 1.00).</S>
  <S sid="102" ssid="102">In every case, the median is higher than the mean.</S>
  <S sid="103" ssid="103">Put another way, there is always a cluster of good overlaps, but the general tendency is to have fairly poor overlaps.</S>
  <S sid="104" ssid="104">The six variations of the experiment are as follows.</S>
  <S sid="105" ssid="105">The first distinction is whether or not to count the neg- ative evidence.</S>
  <S sid="106" ssid="106">We note that the use of negative xam- ples, i.e., plausible uses of the verb in contexts which are disallowed, was a key component of this experi- ment.</S>
  <S sid="107" ssid="107">There are 1082 positive examples and 586 nega- tive examples.</S>
  <S sid="108" ssid="108">Although this evidence is useful, it is not available in dictionaries, corpora, or other convenient resources that could be used to extend Levins classi- fication.</S>
  <S sid="109" ssid="109">Thus, to extend our approach to novel word senses (i.e., words not occurring in Levin), we would not be able to use negative evidence.</S>
  <S sid="110" ssid="110">For this reason, we felt it necessary to determine the importance of nega- tive evidence for building uniquely identifying syntactic signatures.</S>
  <S sid="111" ssid="111">As one might expect, throwing out the neg- ative evidence degrades the usefulness of the signatures across the board.</S>
  <S sid="112" ssid="112">The results which had the negative evidence are shown in the left-hand column of numbers in Table 2, and the results which had only positive evi- dence are shown in the right-hand side.</S>
  <S sid="113" ssid="113">The second, three-way, distinction involves preposi- tions, and breaks the two previous distinctions involv- ing negative vidence into three sub-cases.</S>
  <S sid="114" ssid="114">Because we were interested in the role of prepositions in the sig- natures, we also ran the experiment with two different parse types: ones that ignored the actual prepositions in the pps, and ones that ignored all information except for the values of the prepositions.</S>
  <S sid="115" ssid="115">Interestingly, we still got useful results with these impoverished parses, al- though fewer semantic lasses had uniquely-identifying syntactic signatures under these conditions.</S>
  <S sid="116" ssid="116">These re- sults are shown in the three major rows of Table 2.</S>
  <S sid="117" ssid="117">The best result, using both positive and negative v- idence to identify semantic classes, gives 6.3% of the verbs having perfect overlaps relating semantic lasses to syntactic signatures.</S>
  <S sid="118" ssid="118">See Table 2 for the full results.</S>
  <S sid="119" ssid="119">3.3 Experiment 2" Class-based Approach In this experiment, we attempt o discover whether each class-based syntactic signature uniquely identifies a sin- 324 Verb-based Exper iment  (No Disami)iguation) :ed ,~sitions ~i~ ed )sitions qYgfy )sitions Overlap Median Mean Perfect Median Mean Perfect Median Mean Perfect With No Negative Negative Evidence Evidence O.lO 0.09 0.17 0.17 6.3% 5.2% 0.t0 0.09 0.17 O.</S>
  <S sid="120" ssid="120">16 6.3% 4.2% 0.10 0.09 0.16 0.715 3.1% 3.1% Table 2: Verb-Ba~sed Results Class-based Exper iment  (Disambiguated Verbs) - With No Negative Negative Overlap Evidence Evidence Marked Prepositions ~lgnored Pret)ositions ~3nly Prepositions -TVledian Mean Perfect Median Mean Perfect Median Mean Perfect 1.00 1.00 0.99 0.93 97.9% 88.0% -1.00 1.00 0.96 0.69 87.4% 52.4% 1.00 0.54 0.82 0.57 66.5% 42.9% fable 3: Cla~ss-Based l{esnlts gle semantic lass.</S>
  <S sid="121" ssid="121">By h)cnsing on the classes, the verbs are implicitly disambiguated: the word sense is by def- inition the sense of the verb as a member of a given class.</S>
  <S sid="122" ssid="122">To compare these signatures with the previous verb-based signatures, it may be helpfnl to note that a verb-based signature is the union of all of the class~ based signatures of the semantic lasses that the verb appears m. Fhe outline for this class-based exl)eriment is as fol- lows: 1.</S>
  <S sid="123" ssid="123">Automatically extract syntactic information from tile example sentences to yMd the syntactic signatnre for the class.</S>
  <S sid="124" ssid="124">Determine which semantic classes have uniquely- identifying syntactic signatures.</S>
  <S sid="125" ssid="125">If we use the class-based syntactic signatures contain- ing t)rcposition-marked pps and both positive and neg- ative evidence, the 1668 example sentences reduce to 282 syntactic patterns, just as before.</S>
  <S sid="126" ssid="126">But now there are 189 class-based syntactic signatures, as compared with 748 verb-based signatures from before.</S>
  <S sid="127" ssid="127">187 of them mriquely identify a semantic (:lass, meaning that 97.9% of the classes have uniquely identifying syntactic signa- tures.</S>
  <S sid="128" ssid="128">Four of the semantic lasses do not have enough syntactic information to distinguish them uniquely.</S>
  <S sid="129" ssid="129">4 Although the effects of the various distinctions were present in the verb-based experiment, these effects are much clearer in the class-based experiments.</S>
  <S sid="130" ssid="130">The effects of negative and positive evidence, as well as the three ways of handling prepositions how up much clearer here, as is clear in Table 4.</S>
  <S sid="131" ssid="131">In the class-based experiment, we counted the per- centage of semantic classes that had uniquely ide.nti- fying signatures.</S>
  <S sid="132" ssid="132">In the verb-based experiment, we counted the number of perfect overlaps (i.e., index of 1.00) between the verbs as grouped in the semantic classes and grouped by syntactic signature.</S>
  <S sid="133" ssid="133">The over- all results of the suite of experiments, illustrating tile role of disambiguation, egative vidence, and preposi- tions, is shown in Table 4.</S>
  <S sid="134" ssid="134">There were three ways of treating prepositions: (i) mark the pp with the prepo- sition, (ii) ignore the preposition, and (iii) keel) only the prepositions.</S>
  <S sid="135" ssid="135">For these different strategies, we see the percentage of perfect overlaps, as well as both tire 4Two of these classes correspond to one of the two non- unique signatures, and two (:orrespond to the other non- unique signature.</S>
  <S sid="136" ssid="136">median and mean overlap ratios for each experiment.</S>
  <S sid="137" ssid="137">Fhese data show that the most important factor in the experiments i word-sense disambiguation.</S>
  <S sid="138" ssid="138">Marked Prepositions ignored Prepositions Only Prepositions ~W~{h Dismnl)lguation Marked Prepositions Ignored Prepositions Only Prepositions With No Negative Negative Evidence Evidence 6.3% 5.2% 6.3% 4.2% 3.1% 3.1% 97.9% 88.</S>
  <S sid="139" ssid="139">{)% 87.4% 52.4% 66.5% 42.9% Table 4: Overall Results 4 Semant ic  C lass i f i ca t ion  o f  Nove l Words As we saw above, word sense disambiguation is critical to tile success of any [exical acquisition algorithm.</S>
  <S sid="140" ssid="140">The Levin-based verbs are already disambiguated by virtue of their membership in different classes.</S>
  <S sid="141" ssid="141">The difficulty, then, is to disambiguate and classify verbs that do not occur in Levin.</S>
  <S sid="142" ssid="142">Our current direction is to make use of the results of tire first two experiments, i.e., the re- lation t)etween syntactic patterns and semantic lasses, but to use two additional techniques for disambiguation and classification of non-Levin verbs: (1) extraction of synonym sets provided in WordNet (Miller, 1985), an online lexical database containing thesaurus-like rela- tions such as synonymy; and (2) selection of appropri- ate synonyms based on correlations between syntactic information in l ,ongmans Dictionary of Contemporary English (LDOCF,) (Procter, 1978) and semantic lasses in Levin.</S>
  <S sid="143" ssid="143">Phe basic idea is to first determine tire most likely candidates for semantic lassification of a verb by examining the verbs synonym sets, many of which in- tersect directly with the verbs classified by Leviu.</S>
  <S sid="144" ssid="144">The "closest" synonyms are then selected flom these sets by comparing the LDOCE grammar codes of tire unknown word with those associated with each synonym candi- date.</S>
  <S sid="145" ssid="145">The use of LDOCE as a syntactic filter on tire semantics derived from WordNet is tire key to resolv- ing word-sense ambiguity during the acquisition pro- cess.</S>
  <S sid="146" ssid="146">The fldl acquisition algorithm is as follows: 325 Given a verb, check I,evin (:lass.</S>
  <S sid="147" ssid="147">If in Levitt, classify directly.</S>
  <S sid="148" ssid="148">2. if not in Levin, find synonym set from WordNet.</S>
  <S sid="149" ssid="149">(a) if synonym in Levin, select, the class that has the closest match with canonical LDOCE codes.</S>
  <S sid="150" ssid="150">(b) If no synonyms in Levin or canonical LDOCE codes are completely mismatched, hypothesize new class.</S>
  <S sid="151" ssid="151">Note that this algorithm assmnes that there is a "canonicM" set of LDOCE codes tbr each of Levins semantic lasses.</S>
  <S sid="152" ssid="152">Table 5 describes the significance of a subset of the syntactic codes in LDOCE.</S>
  <S sid="153" ssid="153">(The total nmnber of codes is 174.)</S>
  <S sid="154" ssid="154">We have developed a relation between LDOCE codes and Levin classes, in mnch the same way that we associated syntactic signatures with the semantic lasses in the earlier experiments.</S>
  <S sid="155" ssid="155">These canonical codes are for syntactic filtering (checking for the closest match) in the classification algorithm.</S>
  <S sid="156" ssid="156">As an example of how the word-sense disambigua- tion process and classifcation, consider the non-Levin verb attempt.</S>
  <S sid="157" ssid="157">The LDOCE specification for this verb is: T1 T3 T4 WV5 N. Using the synonymy feature of WordNet, the algorithm automatically extracts tire candidate classes associated with the synonyms of this word: (1) Class 29.6 "Masquerade Verbs" (ace), (2) Class 29.8 "Captain Verbs" (pioneer), (3) Class 31.1 "Amuse Verbs" (try), (4) Class 35.6 "Ferret Verbs" (seek), and (5) Class 55.2 "Complete Verbs" (initiate).</S>
  <S sid="158" ssid="158">The synonyms for each of these classes have the follow- ing LDOCE encodiugs, respectively: (1) I I -FOIl I-ON I-UPON LI L9 T1 N; (2) L9 T1 N; (3) I T1 T3 T4 WV4 N; (4) ~ bAF[EI~ I-FOR T1 T3; and (5) T1 T I - INTO N. The largest intersection with the syntactic odes for attempt occurs with the verb try (TI T3 T4 N).</S>
  <S sid="159" ssid="159">How- ever, Levins class 31.1 is not the correct class for at- tempt since this sense of try has a "negative amuse" meaning (e.g., Johns behavior tried my patience.</S>
  <S sid="160" ssid="160">In fact, the (:odes T1 l3 14 are not part of the canonical class-code mapping associated with class 31.1.</S>
  <S sid="161" ssid="161">Thus, at- tempt falls under case 2(b) of the algorithm, and a new class is hypothesized.</S>
  <S sid="162" ssid="162">This is a case where word-sense disambiguation has allowed us to classify a new word and to enhance Levins verb classification by adding a new class to the word try as well.</S>
  <S sid="163" ssid="163">In our experiment;s, our algorithm found severM additional non-Levin verbs that fell into this newly hypothesized (;lass, including aspire, attempt, dare, decide, desire, elect, need, and swear.</S>
  <S sid="164" ssid="164">We have automatically classified 10,000 "unknown" verbs, i.e., those not occurring in the Levin classifica- tion, using this technique.</S>
  <S sid="165" ssid="165">These verbs are taken from i e , translations provided in bilin- English "glosses" ( .</S>
  <S sid="166" ssid="166">gual dictionaries for Spanish and Arabic)  As a pre- liminary measure of success, we picked out 84 L1)OCE control vocabulary verbs, (i.e., primitive words used for defning dictionary entries) and hand-checked our re- sults.</S>
  <S sid="167" ssid="167">We found that 69 verbs were classifed correctly, SThe Spanish-English dictionary was built at the Univer- sity of Maryland; The Arabic-English dictionary was pro- duced by Alpnet, a company in Utah that develops transla- tion aids.</S>
  <S sid="168" ssid="168">We are Mso in the process of developing bilingual dictionaries for Korean and French, and we will be porting our LCS acquisition technology to these languages in the near future.</S>
  <S sid="169" ssid="169">i.e., 82% accuracy.</S>
  <S sid="170" ssid="170">5 Summary We have conducted two experiments with the intent of addressing the issue of word-sense ambiguity in extrac- tion from machine-readable resources for the construe tion of large-scale knowledge sources.</S>
  <S sid="171" ssid="171">In the first exper- iment, verbs that appeared in different classes collected the syntactic information flom each class it appeared in.</S>
  <S sid="172" ssid="172">Therefore, the syntactic signature was coml)osed from all of the example sentences fiom every (:lass the verb appeared in.</S>
  <S sid="173" ssid="173">In some cases, the verbs were seanan- tically unrelated and consequently the mat)ping from syntax to semantics was muddied.</S>
  <S sid="174" ssid="174">[he second exper- iment attelnpted to determine a relationship between a semantic lass and the syntactic information associ- ated with each class.</S>
  <S sid="175" ssid="175">Not surprisingly, but not insignif- icantly, this relationship was very clear, since this ex- periment avoided the problem of word sense ambiguity.</S>
  <S sid="176" ssid="176">These experiments served to validate Levins claim that verb semantics and syntactic behavior are predictably related and also demonstrated that a significant con&gt; ponent of any lexical acquisition program is the ability to perform word-sense disambiguation.</S>
  <S sid="177" ssid="177">We have used the results of our first two experiments to help in constructing and augmenting online dictio- naries for novel verb senses.</S>
  <S sid="178" ssid="178">We have used the same syntactic signatures to categorize new verbs into Lcvins classes on the basis of WordNet and 1,1)O(?1!3.</S>
  <S sid="179" ssid="179">We are currently porting these results to new languages using online bilingual lexicons.</S>
  <S sid="180" ssid="180">Acknowledgements The research reported herein was supported, in part, by Army l{,esearch Office contract I)AAL03-91-C-0034 through Battelle Corporation, NSF NYI IRl-9357731, Alfred P. Sloan Research Fellow Award BR3336, and a General Research Board Semester Award.</S>
  <S sid="181" ssid="181">References Alshawi, H. 1989.</S>
  <S sid="182" ssid="182">Analysing the Dictionary l)efini- tions.</S>
  <S sid="183" ssid="183">In B. Boguraev and T. Briscoe, editor, Compuo rational Lexicography for Natural Language Prvcess- ing.</S>
  <S sid="184" ssid="184">Longman, London, pages 153 169.</S>
  <S sid="185" ssid="185">Boguraev, B. and T. Briscoe.</S>
  <S sid="186" ssid="186">Utilising the LDOCE Grammar Codes.</S>
  <S sid="187" ssid="187">In B. Boguraev and T. Briscoe, editor, Computational Lexicography for" Nat- ural Language Processing.</S>
  <S sid="188" ssid="188">Longman, London, pages 85-116.</S>
  <S sid="189" ssid="189">Brent, M. 1993.</S>
  <S sid="190" ssid="190">Unsupervised Learning of Lexical Syn- tax.</S>
  <S sid="191" ssid="191">Computational Linguistics, 19:243-262.</S>
  <S sid="192" ssid="192">Chmch, K. and P. Hanks.</S>
  <S sid="193" ssid="193">Word Association Norlns, Mutual Information and Lexicography.</S>
  <S sid="194" ssid="194">Com- pntational Linguistics, 1(5:22 29.</S>
  <S sid="195" ssid="195">Copestake, A., T. Briscoe, 1 ).</S>
  <S sid="196" ssid="196">Vossen, A. Ageno, I. Castellon, F. l{ibas, (J. t{igau, 1t.</S>
  <S sid="197" ssid="197">l{odr{guez, and A. Samiotou.</S>
  <S sid="198" ssid="198">Acquisition of LexicM Transla- tion Relations from MRDS.</S>
  <S sid="199" ssid="199">Machine Translation, 9. l)orr, B., J. Garlnan, and A. Weinberg.</S>
  <S sid="200" ssid="200">1995. l,rom Syntactic Encodings to [hematic Roles: Building Lexieal Entries lbr Interlingual MT.</S>
  <S sid="201" ssid="201">Machine 7rans- lation, 9.</S>
  <S sid="202" ssid="202">326 LDOCE Code Argmnentg Adjuncts ExmnI)le I I-AI,TER I- I,O 1/.</S>
  <S sid="203" ssid="203">I-ON n I-UPON ], 1 L9 -q,] .</S>
  <S sid="204" ssid="204">T1-IN~IO T3 q4 NP ADV/PP NP NP t P~af~ T q , ~ .  ]</S>
  <S sid="205" ssid="205">-  - - PP[on] Olivier is acting t, onight She sought ;ffter the truth They sought for the right otto lie acted on our suggestion [he drug acted upon the pa.in 11(.</S>
  <S sid="206" ssid="206">a(ts the exl)erienced man lhe ])by a(.ts well - I i ) loneered tit(: new bum -WeT~it~,eTd-him into tit(: group lie.</S>
  <S sid="207" ssid="207">tr led to do it 7qh(7 Grh.~dl eating the new food WV4 ing adjectiwd Ive ha.d a, trying day -ed adjectiwd WV5 lie was convicted for attelnpte(1 murder Tabh.</S>
  <S sid="208" ssid="208">5: Sample Syntactic Codes used in IA)OCI; l)orr, B. and C. Voss.</S>
  <S sid="209" ssid="209">A Multiq,evel Ap proach to lnterlingual MT: I)etining the Interface between l{epresentt~tional l, nguages, lnlcrnational Journal of l,,,pert Systcms.</S>
  <S sid="210" ssid="210">Construction el, repres6ntation de classes sdmantiques de verbes: une coop&amp;ation entre syntaxe et cognition, manuscript, IRIT- CNRS, Toulouse, lirance.</S>
  <S sid="211" ssid="211">Farwell, D., 1,.</S>
  <S sid="212" ssid="212">Guthrie, trod Y. Wilks.</S>
  <S sid="213" ssid="213">Automat- ically Creating Lexical Entries for UI,FRA, a Multi- lingual MT System.</S>
  <S sid="214" ssid="214">Machine 7Yanslaliou, 8(3).</S>
  <S sid="215" ssid="215">Fillmore, C.,I.</S>
  <S sid="216" ssid="216">The Case lbr (?ase.</S>
  <S sid="217" ssid="217">Bach and t~.1./larms, editor, Universals in Linguistic Th, cory.</S>
  <S sid="218" ssid="218">[lolt, l{,inehart, a11d Winston, p~ges 1 88.</S>
  <S sid="219" ssid="219">Argument blruclure.</S>
  <S sid="220" ssid="220">MIT Press, Cambridge, MA.</S>
  <S sid="221" ssid="221">Studies in Lcxical Rela?ions.</S>
  <S sid="222" ssid="222">thesis, MIT, Cambridge, MA.</S>
  <S sid="223" ssid="223">Guthrie, J., L. Guthrie, Y. Wilks, and 11.</S>
  <S sid="224" ssid="224">Subjeet-l)ependent Co occurrence and Word Sense l)isambiguation.</S>
  <S sid="225" ssid="225">In l~roceedings oJlhc 29th An- nual Meeting of lhe Associalion for Compulalional Linguistics, pages 146 152, (Jniversity of California, Berkeley, CA.</S>
  <S sid="226" ssid="226">Ilearst, M. 1991.</S>
  <S sid="227" ssid="227">Noun llomograph l)isambiguation Using 1,oeal (Jontext in Large Pext Corl)ora.</S>
  <S sid="228" ssid="228">In Using Corpora, University of Waterloo, Waterloo, Ontario.</S>
  <S sid="229" ssid="229">,l~u;kendoff, R,.</S>
  <S sid="230" ssid="230">Semantics aud Cognition.</S>
  <S sid="231" ssid="231">MtT Press, Cambridge, MA.</S>
  <S sid="232" ssid="232">Jackendoff, R. 1990. fiemanlic Structures.</S>
  <S sid="233" ssid="233">MIT Press, Cambridge, MA.</S>
  <S sid="234" ssid="234">Dictionar- ies and (k)rpora: (Jolnbiniug (~ort)us aud Machine,- readable Dictionary l)ata lbr Building 13ilingual I,ex icons.</S>
  <S sid="235" ssid="235">Machine lYanslatiou, 10.</S>
  <S sid="236" ssid="236">1993. lh~.glid~, Verb Classes and Alternation.s: A Preliminary Investigation.</S>
  <S sid="237" ssid="237">l,onsdale, I)., T. Mitamura, and F,.</S>
  <S sid="238" ssid="238">quisition of l,arge Lexicous for Practical Knowledge- Based MT.</S>
  <S sid="239" ssid="239">Machine Translation,9.</S>
  <S sid="240" ssid="240">WOIU)NF, T: A /)ictiouary lh:owser.</S>
  <S sid="241" ssid="241">In Proceedings of the First lnlcrualional Conference on htJbrmalion, in l)ala, University of Waterloo (,en- tre for the New ()li)l), Waterloo, Ontario.</S>
  <S sid="242" ssid="242">Neff, M. and M. Mc(;ord.</S>
  <S sid="243" ssid="243">Acquiring I,exh:at I)ata fronl Machinc-II.eadable )ictionary I~esourccs for Machine Translation.</S>
  <S sid="244" ssid="244">]ii Third lnternalional Con- ference on Theoretical and Methodological issues in Machine Translation oJNahwal Languages (7MI-90), A us t in, lexas.</S>
  <S sid="245" ssid="245">Pinker, S. 1989. hearnability and Cognilion: The Ac- quisitiou of Argument Structure.</S>
  <S sid="246" ssid="246">MIq Press, Cam- bridge, MA.</S>
  <S sid="247" ssid="247">Procter, P. 1978.</S>
  <S sid="248" ssid="248">Lou, gman Dictionary of Conlcmpoo ra W I5uglish.</S>
  <S sid="249" ssid="249">l,onginan, l,ondon.</S>
  <S sid="250" ssid="250">Sanfilippo, A. and V. Iozmmsk[.</S>
  <S sid="251" ssid="251">The Acquisi- tion of [,exieal Knowledge from (~olnbine(l Machine- Readable Dictionary Ih-.som(es.</S>
  <S sid="252" ssid="252">In Proceedings of th.e Applied Natural Languaqc Processing Confcrencc, pages 80 87, Tren{;o, Italy.</S>
  <S sid="253" ssid="253">The Use of Machine-- readable Dictionaries in Sublanguage Atmlysis.</S>
  <S sid="254" ssid="254">In 1L Grishman and IL Kittredge, editors, Analyzing Language in t{cslricled Domains.</S>
  <S sid="255" ssid="255">l,awrence Erlbaum Associates, llillsdale, New Jersey, pages 69 83.</S>
  <S sid="256" ssid="256">Wilks, Y., 71).</S>
  <S sid="257" ssid="257">M(Domdd, and T. l|ate.</S>
  <S sid="258" ssid="258">Providing Machine tractable [)ic- ~ionary lbols.</S>
  <S sid="259" ssid="259">Machine 7ranslalion, 5(2):99 154.</S>
  <S sid="260" ssid="260">P]al,e, and B.M.</S>
  <S sid="261" ssid="261">A Tractable Machine Dicl;io- nary as a. I{.esource for Computational Semanl, ics.</S>
  <S sid="262" ssid="262">In B. Bogura.ev and T. Briscoe, editor, Computaiional Lexicograph, y for Natural Language Processing.</S>
  <S sid="263" ssid="263">[,oug-- man, l,ondon, pages 85 116.</S>
  <S sid="264" ssid="264">I,mge-Seale Automatic Ex- traction o[ an l,]nglish-Chinese, Translation l,exieon.</S>
  <S sid="265" ssid="265">Machine Translalion, 9.</S>
  <S sid="266" ssid="266">Word-Sense l)isambigual, io|~:, Us ing Statistical Models of Rogets Categories Trainc.,d on l,arge Corpora.</S>
  <S sid="267" ssid="267">In Proceedings of the l/ourlce~lh lntcrnatioual UonJi:renc~ on Compulalional Linguis- lie.s, pages 454 460, Nantes, l~rancc.</S>
</PAPER>
