<PAPER>
  <S sid="0">Grammatical Category Disambiguation By Statistical Optimization</S>
  <ABSTRACT>
    <S sid="1" ssid="1">three previous efforts directed specifically to this problem.</S>
    <S sid="2" ssid="2">The first published effort is that of Klein and Simmons (1963), a simple system using suffix lists and limited frame rules.</S>
    <S sid="3" ssid="3">The second approach to lexical disambiguation is and Rubin (1971)), a system of several thousand context-frame rules.</S>
    <S sid="4" ssid="4">This algorithm was used to assign initial tags to the Brown Corpus.</S>
    <S sid="5" ssid="5">Third is the CLAWS system develto tag the (or LOB) Coris a corpus of British written English, parallel to the Brown Corpus.</S>
    <S sid="6" ssid="6">Parsing systems always encounter the problem of category ambiguity; but usually the focus of such systems is at other levels, making their responses less relevant for our purposes here.</S>
    <S sid="7" ssid="7">1.1 KLEIN AND SIMMONS Klein and Simmons (1963) describe a method directed primarily towards the task of initial categorial tagging rather than disambiguation.</S>
    <S sid="8" ssid="8">Its primary goal is avoiding &amp;quot;the labor of constructing a very large dictionary&amp;quot; (p. 335); a consideration of greater import then than now.</S>
    <S sid="9" ssid="9">The Klein and Simmons algorithm uses a palette of 30 categories, and claims an accuracy of 90% in tagging.</S>
    <S sid="10" ssid="10">The algorithm first seeks each word in dictionaries of about 400 function words, and of about 1500 words which &amp;quot;are exceptions to the computational rules used&amp;quot; (p. 339).</S>
    <S sid="11" ssid="11">The program then checks for suffixes and special characters as clues. of all, frame tests applied.</S>
    <S sid="12" ssid="12">These work on scopes bounded by unambiguous words, as do later algorithms.</S>
    <S sid="13" ssid="13">However, Klein and Simmons impose an explicit limit of three ambiguous words in a row.</S>
    <S sid="14" ssid="14">For such ambiguous words, the pair of unambiguous categories bounding it is mapped into a list.</S>
    <S sid="15" ssid="15">The list includes all known sequences of tags occurring between the particular bounding tags; all such sequences of the correct length become candidates.</S>
    <S sid="16" ssid="16">The program then matches the candidate sequences against the ambiguities remaining from earlier steps of the algorithm.</S>
    <S sid="17" ssid="17">When only one sequence is possible, disambiguation is successful.</S>
    <S sid="18" ssid="18">The samples used for calibration and testing were limited.</S>
    <S sid="19" ssid="19">First, Klein and Simmons (1963) performed &amp;quot;hand analysis of a sample [size unspecified] of Golden Grammatical Category Disambiguation by Statistical Optimization Book Encyclopedia text&amp;quot; (p. 342).</S>
    <S sid="20" ssid="20">Later, &amp;quot;[w]hen it was run on several pages from that encyclopedia, it correctly and unambiguously tagged slightly over 90% of the words&amp;quot; (p. 344).</S>
    <S sid="21" ssid="21">Further tests were run on small from the Americana from Scientific American.</S>
    <S sid="22" ssid="22">Klein and Simmons (1963) assert that &amp;quot;[o]riginal fears that sequences of four or more unidentified parts of speech would occur with great frequency were not substantiated in fact&amp;quot; (p. 3).</S>
    <S sid="23" ssid="23">This felicity, however, is an artifact.</S>
    <S sid="24" ssid="24">First, the relatively small set of categories reduces ambiguity.</S>
    <S sid="25" ssid="25">Second, a larger sample would reveal both (a) low-frequency ambiguities and (b) many long spans, as discussed below.</S>
    <S sid="26" ssid="26">1.2 GREENE AND RUBIN (TAGGIT) Greene and Rubin (1971) developed TAGGIT for tagging the Brown Corpus.</S>
    <S sid="27" ssid="27">The palette of 86 tags that TAGGIT uses has, with some modifications, also been used in both CLAWS and VOLSUNGA.</S>
    <S sid="28" ssid="28">The rationale underlying the choice of tags is described on pages 3-21 of Greene and Rubin (1971).</S>
    <S sid="29" ssid="29">Francis and Kucera (1982) report that this algorithm correctly tagged approxithe million words in the Brown Corpus (the tagging was then completed by human post-editors).</S>
    <S sid="30" ssid="30">Although this accuracy is substantially lower than that reported by Klein and Simmons, it should be remembered that Greene and Rubin were the first to attempt so large and varied a sample.</S>
    <S sid="31" ssid="31">TAGGIT divides the task of category assignment into initial (potentially ambiguous) tagging, and disambiguation.</S>
    <S sid="32" ssid="32">Tagging is carried out as follows: first, the program consults an exception dictionary of about 3,000 words.</S>
    <S sid="33" ssid="33">Among other items, this contains all known closed-class words.</S>
    <S sid="34" ssid="34">It then handles various special cases, such as words with initial &amp;quot;$&amp;quot;, contractions, special symbols, and capitalized words.</S>
    <S sid="35" ssid="35">The word's ending is then checked against a suffix list of about 450 strings.</S>
    <S sid="36" ssid="36">The lists were derived from lexicostatistics of the Brown Corpus.</S>
    <S sid="37" ssid="37">If TAGGIT has not assigned some tag(s) after these several steps, &amp;quot;the word is tagged NN, VB, or JJ [that is, as being three-ways ambiguous], in order that the disambiguation routine may have something to work with&amp;quot; (Greene and Rubin (1971), p. 25).</S>
    <S sid="38" ssid="38">After tagging, TAGGIT applies a set of 3300 context frame rules.</S>
    <S sid="39" ssid="39">Each rule, when its context is satisfied, has the effect of deleting one or more candidates from the list of possible tags for one word.</S>
    <S sid="40" ssid="40">If the number of candidates is reduced to one, disambiguation is considered successful subject to human post-editing.</S>
    <S sid="41" ssid="41">Each rule can include a scope of up to two unambiguous words on each side of the ambiguous word to which the rule is being applied.</S>
    <S sid="42" ssid="42">This constraint was determined as follows: In order to create the original inventory of Context Frame Tests, a 900-sentence subset of the Brown University Corpus was tagged.</S>
    <S sid="43" ssid="43">.</S>
    <S sid="44" ssid="44">. and its ambiguities were resolved manually; then a program was run 32 Computational Linguistics, Volume 14, Number 1, Winter 1988 Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization which produced and sorted all possible Context Frame Rules which would have been necessary to perform this disambiguation automatically.</S>
    <S sid="45" ssid="45">The rules generated were able to handle up to three consecutive ambiguous words preceded and followed by two non-ambiguous words [a constraint similar to Klein and Simmons'].</S>
    <S sid="46" ssid="46">However, upon examination of these rules, it was found that a sequence of two or three ambiguities rarely occurred more than once in a given context.</S>
    <S sid="47" ssid="47">Consequently, a decision was made to examine only one ambiguity at a time with up to two unambiguously tagged words on either side.</S>
    <S sid="48" ssid="48">The first rules created were the results of informed intuition (Greene and Rubin (1972), p. 32).</S>
    <S sid="49" ssid="49">1.3 CLAWS Marshall (1983, p. 139) describes the LOB Corpus tagging algorithm, later named CLAWS (Booth (1985)), as &amp;quot;similar to those employed in the TAGGIT program&amp;quot;.</S>
    <S sid="50" ssid="50">The tag set used is very similar, but somewhat larger, at about 130 tags.</S>
    <S sid="51" ssid="51">The dictionary used is derived from the tagged Brown Corpus, rather than from the untagged.</S>
    <S sid="52" ssid="52">It contains 7000 rather than 3000 entries, and 700 rather than 450 suffixes.</S>
    <S sid="53" ssid="53">CLAWS treats plural, possessive, and hyphenated words as special cases for purposes of initial tagging.</S>
    <S sid="54" ssid="54">The LOB researchers began by using TAGGIT on parts of the LOB Corpus.</S>
    <S sid="55" ssid="55">They noticed that While less than 25% of TAGGIT's context frame rules are concerned with only the immediately preceding or succeeding word.</S>
    <S sid="56" ssid="56">.</S>
    <S sid="57" ssid="57">. these rules were applied in about 80% of all attempts to apply rules.</S>
    <S sid="58" ssid="58">This relative overuse of minimally specified contexts indicated that exploitation of the relationship between successive tags, coupled with a mechanism that would be applied throughout a sequence of ambiguous words, would produce a more accurate and effective method of word disambiguation (Marshall (1983), p. 141).</S>
    <S sid="59" ssid="59">The main innovation of CLAWS is the use of a matrix probabilities, the relative likelihood of co-occurrence of all ordered pairs of tags.</S>
    <S sid="60" ssid="60">This matrix can be mechanically derived from any pre-tagged corpus.</S>
    <S sid="61" ssid="61">CLAWS used &amp;quot;[a] large proportion of the Brown Corpus&amp;quot;, 200,000 words (Marshall (1983), pp.</S>
    <S sid="62" ssid="62">141, 150).</S>
    <S sid="63" ssid="63">The ambiguities contained within a span of ambiguous words define a precise number of complete sets of mappings from words to individual tags.</S>
    <S sid="64" ssid="64">Each such of tags is called a path is composed of a number of tag collocations, and each such collocation has a probability which may be obtained from the collocation matrix.</S>
    <S sid="65" ssid="65">One may thus approximate each path's probability by the product of the probabilities of all its collocations.</S>
    <S sid="66" ssid="66">Each path corresponds to a unique assignment of tags to all words within a span. paths constitute a network, the path of maximal probability may be taken to contain the &amp;quot;best&amp;quot; tags.</S>
    <S sid="67" ssid="67">(1983) states that CLAWS the most probable sequence of tags, and in the majority of cases the correct tag for each individual word corresponds to the associated tag in the most probable sequence of tags&amp;quot; (p. 142).</S>
    <S sid="68" ssid="68">But a more detailed examination of the Pascal code for CLAWS revealed that CLAWS has a more complex definition of &amp;quot;most probable sequence&amp;quot; than one might expect.</S>
    <S sid="69" ssid="69">A probability called &amp;quot;SUMSUCCPROBS&amp;quot; is predicated of each word.</S>
    <S sid="70" ssid="70">SUMSUCCPROBS is calculated by looping through all tags for the words immediately preceding, at, and following a word; for each tag triple, an increment is added, defined by: DownGrade(GetSucc(Tag2, Tag3), TagMark) * Get3SeqFactor(Tag1, Tag2, Tag3) the collocational probability of a tag either 1, or a special value the tag-triple list described below. the value of accordance with RTPs as described below.</S>
    <S sid="71" ssid="71">The CLAWS documentation describes SUMSUCC- PROBS as &amp;quot;the total value of all relationships between the tags associated with this word and the tags associated with the next word.</S>
    <S sid="72" ssid="72">.</S>
    <S sid="73" ssid="73">.</S>
    <S sid="74" ssid="74">[found by] simulating all accesses to SUCCESSORS and ORDER2VALS which will be made.</S>
    <S sid="75" ssid="75">.</S>
    <S sid="76" ssid="76">.</S>
    <S sid="77" ssid="77">.&amp;quot; The probability of each node of the span network (or rather, tree) is then calculated in the following way as a tree representing all paths through which the span network is built: = currenttag), TagMark) * Get3SeqFactor(.</S>
    <S sid="78" ssid="78">.</S>
    <S sid="79" ssid="79">.))</S>
    <S sid="80" ssid="80">= PROB * (predecessor's It appears that the goal is to make each tag's probabe the summed probability of passing through it.</S>
    <S sid="81" ssid="81">At the final word of a span, pointers are followed back up the chosen path, and tags are chosen en route.</S>
    <S sid="82" ssid="82">We will see below that a simpler definition of optimal path is possible; nevertheless, there are several advantages of this general approach over previous ones.</S>
    <S sid="83" ssid="83">First, spans of unlimited length can be handled (subject to machine resources).</S>
    <S sid="84" ssid="84">Although earlier researchers (Klein and Simmons, Greene and Rubin) have suggested that spans of length over 5 are rare enough to be of little concern, this is not the case.</S>
    <S sid="85" ssid="85">The number of spans of a given length is a function of that length and the corpus size; so long spans may be obtained merely by examining more text.</S>
    <S sid="86" ssid="86">The total numbers of spans in the Brown Corpus, for each length from 3 to 19, are: 397111, 143447, 60224, 26515, 11409,5128, 2161, 903, 382, 161, 58, 29, 14, 6, 1, 0, 1.</S>
    <S sid="87" ssid="87">Graphing the logarithms Computational Linguistics, Volume 14, Number 1, Winter 1988 33 Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization of these quantities versus the span length for each, produces a near-perfect straight line.</S>
    <S sid="88" ssid="88">Second, a precise mathematical definition is possible for the fundamental idea of CLAWS.</S>
    <S sid="89" ssid="89">Whereas earlier efforts were based primarily on ad hoc or subjectively determined sets of rules and descriptions, and employed substantial exception dictionaries, this algorithm requires no human intervention for set-up; it is a systematic process.</S>
    <S sid="90" ssid="90">Third, the algorithm is quantitative and analog, rather than artificially discrete.</S>
    <S sid="91" ssid="91">The various tests and employed by earlier algorithms enforced absolute constraints on particular tags or collocations of tags.</S>
    <S sid="92" ssid="92">Here relative probabilities are weighed, and a series of very likely assignments can make possible a particular, a priori unlikely assignment with which they are associated.</S>
    <S sid="93" ssid="93">In addition to collocational probabilities, CLAWS also takes into account one other empirical quantity: Tags associated with words.</S>
    <S sid="94" ssid="94">.</S>
    <S sid="95" ssid="95">. can be with a marker @ or %; @ indicates that the tag is infrequently the correct tag for the associated word(s) (less than 1 in 10 occasions), % indicates is highly improbable.</S>
    <S sid="96" ssid="96">.</S>
    <S sid="97" ssid="97">.</S>
    <S sid="98" ssid="98">(less than 1 in 100 oc- .</S>
    <S sid="99" ssid="99">.</S>
    <S sid="100" ssid="100">.</S>
    <S sid="101" ssid="101">The word disambiguation program currently uses these markers top devalue values when retrieving a value from the matrix, @ results in the value being halved, % in the value being divided by eight (Marshall (1983), p. 149).</S>
    <S sid="102" ssid="102">Thus, the independent probability of each possible tag for a given word influences the choice of an optimal Such probabilities will be referred to as Probabilities, Other features have been added to the basic algorithm.</S>
    <S sid="103" ssid="103">For example, a good deal of suffix analysis is used in initial tagging.</S>
    <S sid="104" ssid="104">Also, the program filters its output, considering itself to have failed if the optimal tag assignment for a span is not &amp;quot;more than 90% probable&amp;quot;. cases it reorders tags rather than actually disambiguating.</S>
    <S sid="105" ssid="105">On long spans this criterion is effectively more stringent than on short spans.</S>
    <S sid="106" ssid="106">A more significant addition to the algorithm is that a number of tag triples associated with a have been introduced which may either upgrade or downgrade values in the tree computed from the one-step matrix.</S>
    <S sid="107" ssid="107">For example, the triple [1] [2] adverb [3] past-tense-verb has been assigned a factor which downgrades a sequence containing this triple compared with a competing of [1] 'be' [2] adverb [3]-past-participle/adjective, on the basis that after a form of 'be', past participles and adjectives are more likely than a past tense verb (Marshall (1983), p. 146).</S>
    <S sid="108" ssid="108">A similar move was used near conjunctions, for which the words on either side, though separated, are more closely correlated to each other than either is to the conjunction itself (Marshall (1983), pp.</S>
    <S sid="109" ssid="109">146-147).</S>
    <S sid="110" ssid="110">For example, a verb/noun ambiguity conjoined to a verb should probably be taken as a verb.</S>
    <S sid="111" ssid="111">Leech, Garside, and Atwell (1983, p. 23) describe &amp;quot;IDIOMTAG&amp;quot;, which is applied after initial tag assignment and before disambiguation.</S>
    <S sid="112" ssid="112">It was developed as a means of dealing with sequences which would otherwise cause diffifor the automatic tagging.</S>
    <S sid="113" ssid="113">.</S>
    <S sid="114" ssid="114">.</S>
    <S sid="115" ssid="115">. for example, that tagged as a single conjunction.</S>
    <S sid="116" ssid="116">.</S>
    <S sid="117" ssid="117">.</S>
    <S sid="118" ssid="118">.</S>
    <S sid="119" ssid="119">Tagging Program.</S>
    <S sid="120" ssid="120">.</S>
    <S sid="121" ssid="121">. can look at any combination of words and tags, with or without intervening words.</S>
    <S sid="122" ssid="122">It can delete tags, add tags, or change the probability of tags.</S>
    <S sid="123" ssid="123">Although this program might to be an hoc it is worth bearing in that any fully automatic language analysis syshas to come to with problems of lexical idiosyncrasy.</S>
    <S sid="124" ssid="124">IDIOMTAG also accounts for the fact that the probability of a verb being a past participle, and not simply past, is greater when the following word is &amp;quot;by&amp;quot;, as opposed to other prepositions.</S>
    <S sid="125" ssid="125">Certain cases of this sort may be soluble by making the collocational matrix distinguish classes of ambiguities&#8212;this question is being pursued.</S>
    <S sid="126" ssid="126">Approximately 1% of running text is tagged by IDIOMTAG (letter, G. N. Leech to Henry Kucera, June 7, 1985; letter, E. S. Atwell to Henry Kucera, June 20, 1985).</S>
    <S sid="127" ssid="127">Marshall notes the possibility of consulting a complete three-dimensional matrix of collocational probabilities.</S>
    <S sid="128" ssid="128">Such a matrix would map ordered triples of tags into the relative probability of occurrence of each such triple.</S>
    <S sid="129" ssid="129">Marshall points out that such a table would be too large for its probable usefulness.</S>
    <S sid="130" ssid="130">The author has proa table based upon more 85% of the Brown Corpus; it occupies about 2 megabytes (uncompressed).</S>
    <S sid="131" ssid="131">Also, the mean number of examples per triple is very low, thus decreasing accuracy.</S>
    <S sid="132" ssid="132">CLAWS has been applied to the entire LOB Corpus with an accuracy of &amp;quot;between 96% and 97%&amp;quot; (Booth (1985), p. 29).</S>
    <S sid="133" ssid="133">Without the idiom list, the algorithm was 94% accurate on a sample of 15,000 words (Marshall (1983)).</S>
    <S sid="134" ssid="134">Thus, the pre-processor tagging of 1% of all tokens resulted in a 3% change in accuracy; those particular assignments must therefore have had a substantial effect upon their context, resulting in changes of two other words for every one explicitly tagged.</S>
    <S sid="135" ssid="135">But CLAWS is timeand storage-inefficient in the extreme, and in some cases a fallback algorithm is employed to prevent running out of memory, as was discovered by examining the Pascal program code.</S>
    <S sid="136" ssid="136">How often the fallback is employed is not known, nor is it known what effect its use has on overall accuracy.</S>
    <S sid="137" ssid="137">Since CLAWS calculates the probability of every path, it operates in time and space proportional to the product of all the degrees of ambiguity of the words in the span.</S>
    <S sid="138" ssid="138">Thus, the time is exponential (and hence Non-Polynomial) in the span length.</S>
    <S sid="139" ssid="139">For the longest span in the Brown Corpus, of length 18, the number of paths examined would be 1,492,992.</S>
    <S sid="140" ssid="140">34 Computational Linguistics, Volume 14, Number 1, Winter 1988 Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization LINEAR-TIME ALGORITHM The algorithm described here depends on a similar empirically-derived transitional probability matrix to that of CLAWS, and has a similar definition of &amp;quot;optimal path&amp;quot;.</S>
    <S sid="141" ssid="141">The tagset is larger than TAGGIT's, though smaller than CLAWS', containing 97 tags.</S>
    <S sid="142" ssid="142">The ultimate assignments of tags are much like those of CLAWS.</S>
    <S sid="143" ssid="143">However, it embodies several substantive changes.</S>
    <S sid="144" ssid="144">Those features that can be algorithmically defined have been used to the fullest extent.</S>
    <S sid="145" ssid="145">Other add-ons have been minimized.</S>
    <S sid="146" ssid="146">The major differences are outlined below.</S>
    <S sid="147" ssid="147">First, the optimal path is defined to be the one whose component collocations multiply out to the highest probability.</S>
    <S sid="148" ssid="148">The more complex definition applied by using the sum of all paths at of the network, is not used.</S>
    <S sid="149" ssid="149">Second, VOLSUNGA overcomes the Non-Polynomial complexity of CLAWS.</S>
    <S sid="150" ssid="150">Because of this change, it is never necessary to resort to a fallback algorithm, and the program is far smaller.</S>
    <S sid="151" ssid="151">Furthermore, testing the algorithm on extensive texts is not prohibitively costly.</S>
    <S sid="152" ssid="152">Third, VOLSUNGA implements Relative Tag Probabilities (RTPs) in a more quantitative manner, based upon counts from the Brown Corpus.</S>
    <S sid="153" ssid="153">Where CLAWS scales probabilities by 1/2 for RTP &lt; 0.1 (i.e., where less than 10% of the tokens for an ambiguous word are in the category in question), and by 1/8 for p &lt; 0.01, VOLSUNGA uses the RTP value itself as a factor in the equation which defines probability.</S>
    <S sid="154" ssid="154">Fourth, VOLSUNGA uses no tag triples and no idioms.</S>
    <S sid="155" ssid="155">Because of this, manually constructing specialcase lists is not necessary.</S>
    <S sid="156" ssid="156">These methods are useful in certain cases, as the accuracy figures for CLAWS show; but the goal here was to measure the accuracy of a wholly algorithmic tagger on a standard corpus.</S>
  </ABSTRACT>
  <SECTION title="GRAMMATICAL CATEGORY DISAMBIGUATION BY STATISTICAL OPTIMIZATION Steven J. DeRose" number="1">
    <S sid="157" ssid="1">Brown University and the Summer Institute of Linguistics, 7500 W. Camp Wisdom Road, Dallas, TX 75236 Several algorithms have been developed in the past that attempt to resolve categorial ambiguities in natural language text without recourse to syntactic or semantic level information.</S>
    <S sid="158" ssid="2">An innovative method (called &amp;quot;CLAWS&amp;quot;) was recently developed by those working with the Lancaster &#8212;Oslo/Bergen Corpus of British English.</S>
    <S sid="159" ssid="3">This algorithm uses a systematic calculation based upon the probabilities of co-occurrence of particular tags.</S>
    <S sid="160" ssid="4">Its accuracy is high, but it is very slow, and it has been manually augmented in a number of ways.</S>
    <S sid="161" ssid="5">The effects upon accuracy of this manual augmentation are not individually known.</S>
    <S sid="162" ssid="6">The current paper presents an algorithm for disambiguation that is similar to CLAWS but that operates in linear rather than in exponential time and space, and which minimizes the unsystematic augments.</S>
    <S sid="163" ssid="7">Tests of the algorithm using the million words of the Brown Standard Corpus of English are reported; the overall accuracy is 96%.</S>
    <S sid="164" ssid="8">This algorithm can provide a fast and accurate front end to any parsing or natural language processing system for English.</S>
    <S sid="165" ssid="9">Every computer system that accepts natural language input must, if it is to derive adequate representations, decide upon the grammatical category of each input word.</S>
    <S sid="166" ssid="10">In English and many other languages, tokens are frequently ambiguous.</S>
    <S sid="167" ssid="11">They may represent lexical items of different categories, depending upon their syntactic and semantic context.</S>
    <S sid="168" ssid="12">Several algorithms have been developed that examine a prose text and decide upon one of the several possible categories for a given word.</S>
    <S sid="169" ssid="13">Our focus will be on algorithms which specifically address this task of disambiguation, and particularly on a new algorithm called VOLSUNGA, which avoids syntactic-level analysis, yields about 96% accuracy, and runs in far less time and space than previous attempts.</S>
    <S sid="170" ssid="14">The most recent previous algorithm runs in NP (Non-Polynomial) time, while VOLSUNGA runs in linear time.</S>
    <S sid="171" ssid="15">This is provably optimal; no improvements in the order of its execution time and space are possible.</S>
    <S sid="172" ssid="16">VOLSUNGA is also robust in cases of ungrammaticality.</S>
    <S sid="173" ssid="17">Improvements to this accuracy may be made, perhaps the most potentially significant being to include some higher-level information.</S>
    <S sid="174" ssid="18">With such additions, the accuracy of statistically-based algorithms will approach 100%; and the few remaining cases may be largely those with which humans also find difficulty.</S>
    <S sid="175" ssid="19">In subsequent sections we examine several disambiguation algorithms.</S>
    <S sid="176" ssid="20">Their techniques, accuracies, and efficiencies are analyzed.</S>
    <S sid="177" ssid="21">After presenting the research carried out to date, a discussion of VOLSUNGA' s application to the Brown Corpus will follow.</S>
    <S sid="178" ssid="22">The Brown Corpus, described in Kucera and Francis (1967), is a collection of 500 carefully distributed samples of English text, totalling just over one million words.</S>
    <S sid="179" ssid="23">It has been used as a standard sample in many studies of English.</S>
    <S sid="180" ssid="24">Generous advice, encouragement, and assistance from Henry Kucera and W. Nelson Francis in this research is gratefully acknowledged.</S>
  </SECTION>
  <SECTION title="1 PREVIOUS DISAMBIGUATION ALGORITHMS" number="2">
    <S sid="181" ssid="1">The problem of lexical category ambiguity has been little examined in the literature of computational linguistics and artificial intelligence, though it pervades English to an astonishing degree.</S>
    <S sid="182" ssid="2">About 11.5% of types (vocabulary), and over 40% of tokens (running words) in English prose are categorically ambiguous (as measured via the Brown Corpus).</S>
    <S sid="183" ssid="3">The vocabulary breaks down as shown in Table 1 (derived from Francis and Kucera (1982)).</S>
    <S sid="184" ssid="4">A search of the relevant literature has revealed only three previous efforts directed specifically to this problem.</S>
    <S sid="185" ssid="5">The first published effort is that of Klein and Simmons (1963), a simple system using suffix lists and limited frame rules.</S>
    <S sid="186" ssid="6">The second approach to lexical category disambiguation is TAGGIT (Greene and Rubin (1971)), a system of several thousand context-frame rules.</S>
    <S sid="187" ssid="7">This algorithm was used to assign initial tags to the Brown Corpus.</S>
    <S sid="188" ssid="8">Third is the CLAWS system developed to tag the Lancaster &#8212;Oslo/Bergen (or LOB) Corpus.</S>
    <S sid="189" ssid="9">This is a corpus of British written English, parallel to the Brown Corpus.</S>
    <S sid="190" ssid="10">Parsing systems always encounter the problem of category ambiguity; but usually the focus of such systems is at other levels, making their responses less relevant for our purposes here.</S>
    <S sid="191" ssid="11">Klein and Simmons (1963) describe a method directed primarily towards the task of initial categorial tagging rather than disambiguation.</S>
    <S sid="192" ssid="12">Its primary goal is avoiding &amp;quot;the labor of constructing a very large dictionary&amp;quot; (p. 335); a consideration of greater import then than now.</S>
    <S sid="193" ssid="13">The Klein and Simmons algorithm uses a palette of 30 categories, and claims an accuracy of 90% in tagging.</S>
    <S sid="194" ssid="14">The algorithm first seeks each word in dictionaries of about 400 function words, and of about 1500 words which &amp;quot;are exceptions to the computational rules used&amp;quot; (p. 339).</S>
    <S sid="195" ssid="15">The program then checks for suffixes and special characters as clues.</S>
    <S sid="196" ssid="16">Last of all, context frame tests are applied.</S>
    <S sid="197" ssid="17">These work on scopes bounded by unambiguous words, as do later algorithms.</S>
    <S sid="198" ssid="18">However, Klein and Simmons impose an explicit limit of three ambiguous words in a row.</S>
    <S sid="199" ssid="19">For each such span of ambiguous words, the pair of unambiguous categories bounding it is mapped into a list.</S>
    <S sid="200" ssid="20">The list includes all known sequences of tags occurring between the particular bounding tags; all such sequences of the correct length become candidates.</S>
    <S sid="201" ssid="21">The program then matches the candidate sequences against the ambiguities remaining from earlier steps of the algorithm.</S>
    <S sid="202" ssid="22">When only one sequence is possible, disambiguation is successful.</S>
    <S sid="203" ssid="23">The samples used for calibration and testing were limited.</S>
    <S sid="204" ssid="24">First, Klein and Simmons (1963) performed &amp;quot;hand analysis of a sample [size unspecified] of Golden Book Encyclopedia text&amp;quot; (p. 342).</S>
    <S sid="205" ssid="25">Later, &amp;quot;[w]hen it was run on several pages from that encyclopedia, it correctly and unambiguously tagged slightly over 90% of the words&amp;quot; (p. 344).</S>
    <S sid="206" ssid="26">Further tests were run on small samples from the Encyclopedia Americana and from Scientific American.</S>
    <S sid="207" ssid="27">Klein and Simmons (1963) assert that &amp;quot;[o]riginal fears that sequences of four or more unidentified parts of speech would occur with great frequency were not substantiated in fact&amp;quot; (p. 3).</S>
    <S sid="208" ssid="28">This felicity, however, is an artifact.</S>
    <S sid="209" ssid="29">First, the relatively small set of categories reduces ambiguity.</S>
    <S sid="210" ssid="30">Second, a larger sample would reveal both (a) low-frequency ambiguities and (b) many long spans, as discussed below.</S>
    <S sid="211" ssid="31">Greene and Rubin (1971) developed TAGGIT for tagging the Brown Corpus.</S>
    <S sid="212" ssid="32">The palette of 86 tags that TAGGIT uses has, with some modifications, also been used in both CLAWS and VOLSUNGA.</S>
    <S sid="213" ssid="33">The rationale underlying the choice of tags is described on pages 3-21 of Greene and Rubin (1971).</S>
    <S sid="214" ssid="34">Francis and Kucera (1982) report that this algorithm correctly tagged approximately 77% of the million words in the Brown Corpus (the tagging was then completed by human post-editors).</S>
    <S sid="215" ssid="35">Although this accuracy is substantially lower than that reported by Klein and Simmons, it should be remembered that Greene and Rubin were the first to attempt so large and varied a sample.</S>
    <S sid="216" ssid="36">TAGGIT divides the task of category assignment into initial (potentially ambiguous) tagging, and disambiguation.</S>
    <S sid="217" ssid="37">Tagging is carried out as follows: first, the program consults an exception dictionary of about 3,000 words.</S>
    <S sid="218" ssid="38">Among other items, this contains all known closed-class words.</S>
    <S sid="219" ssid="39">It then handles various special cases, such as words with initial &amp;quot;$&amp;quot;, contractions, special symbols, and capitalized words.</S>
    <S sid="220" ssid="40">The word's ending is then checked against a suffix list of about 450 strings.</S>
    <S sid="221" ssid="41">The lists were derived from lexicostatistics of the Brown Corpus.</S>
    <S sid="222" ssid="42">If TAGGIT has not assigned some tag(s) after these several steps, &amp;quot;the word is tagged NN, VB, or JJ [that is, as being three-ways ambiguous], in order that the disambiguation routine may have something to work with&amp;quot; (Greene and Rubin (1971), p. 25).</S>
    <S sid="223" ssid="43">After tagging, TAGGIT applies a set of 3300 context frame rules.</S>
    <S sid="224" ssid="44">Each rule, when its context is satisfied, has the effect of deleting one or more candidates from the list of possible tags for one word.</S>
    <S sid="225" ssid="45">If the number of candidates is reduced to one, disambiguation is considered successful subject to human post-editing.</S>
    <S sid="226" ssid="46">Each rule can include a scope of up to two unambiguous words on each side of the ambiguous word to which the rule is being applied.</S>
    <S sid="227" ssid="47">This constraint was determined as follows: In order to create the original inventory of Context Frame Tests, a 900-sentence subset of the Brown University Corpus was tagged.</S>
    <S sid="228" ssid="48">.</S>
    <S sid="229" ssid="49">. and its ambiguities were resolved manually; then a program was run which produced and sorted all possible Context Frame Rules which would have been necessary to perform this disambiguation automatically.</S>
    <S sid="230" ssid="50">The rules generated were able to handle up to three consecutive ambiguous words preceded and followed by two non-ambiguous words [a constraint similar to Klein and Simmons'].</S>
    <S sid="231" ssid="51">However, upon examination of these rules, it was found that a sequence of two or three ambiguities rarely occurred more than once in a given context.</S>
    <S sid="232" ssid="52">Consequently, a decision was made to examine only one ambiguity at a time with up to two unambiguously tagged words on either side.</S>
    <S sid="233" ssid="53">The first rules created were the results of informed intuition (Greene and Rubin (1972), p. 32).</S>
    <S sid="234" ssid="54">Marshall (1983, p. 139) describes the LOB Corpus tagging algorithm, later named CLAWS (Booth (1985)), as &amp;quot;similar to those employed in the TAGGIT program&amp;quot;.</S>
    <S sid="235" ssid="55">The tag set used is very similar, but somewhat larger, at about 130 tags.</S>
    <S sid="236" ssid="56">The dictionary used is derived from the tagged Brown Corpus, rather than from the untagged.</S>
    <S sid="237" ssid="57">It contains 7000 rather than 3000 entries, and 700 rather than 450 suffixes.</S>
    <S sid="238" ssid="58">CLAWS treats plural, possessive, and hyphenated words as special cases for purposes of initial tagging.</S>
    <S sid="239" ssid="59">The LOB researchers began by using TAGGIT on parts of the LOB Corpus.</S>
    <S sid="240" ssid="60">They noticed that While less than 25% of TAGGIT's context frame rules are concerned with only the immediately preceding or succeeding word.</S>
    <S sid="241" ssid="61">.</S>
    <S sid="242" ssid="62">. these rules were applied in about 80% of all attempts to apply rules.</S>
    <S sid="243" ssid="63">This relative overuse of minimally specified contexts indicated that exploitation of the relationship between successive tags, coupled with a mechanism that would be applied throughout a sequence of ambiguous words, would produce a more accurate and effective method of word disambiguation (Marshall (1983), p. 141).</S>
    <S sid="244" ssid="64">The main innovation of CLAWS is the use of a matrix of collocational probabilities, indicating the relative likelihood of co-occurrence of all ordered pairs of tags.</S>
    <S sid="245" ssid="65">This matrix can be mechanically derived from any pre-tagged corpus.</S>
    <S sid="246" ssid="66">CLAWS used &amp;quot;[a] large proportion of the Brown Corpus&amp;quot;, 200,000 words (Marshall (1983), pp.</S>
    <S sid="247" ssid="67">141, 150).</S>
    <S sid="248" ssid="68">The ambiguities contained within a span of ambiguous words define a precise number of complete sets of mappings from words to individual tags.</S>
    <S sid="249" ssid="69">Each such assignment of tags is called a path.</S>
    <S sid="250" ssid="70">Each path is composed of a number of tag collocations, and each such collocation has a probability which may be obtained from the collocation matrix.</S>
    <S sid="251" ssid="71">One may thus approximate each path's probability by the product of the probabilities of all its collocations.</S>
    <S sid="252" ssid="72">Each path corresponds to a unique assignment of tags to all words within a span.</S>
    <S sid="253" ssid="73">The paths constitute a span network, and the path of maximal probability may be taken to contain the &amp;quot;best&amp;quot; tags.</S>
    <S sid="254" ssid="74">Marshall (1983) states that CLAWS -calculates the most probable sequence of tags, and in the majority of cases the correct tag for each individual word corresponds to the associated tag in the most probable sequence of tags&amp;quot; (p. 142).</S>
    <S sid="255" ssid="75">But a more detailed examination of the Pascal code for CLAWS revealed that CLAWS has a more complex definition of &amp;quot;most probable sequence&amp;quot; than one might expect.</S>
    <S sid="256" ssid="76">A probability called &amp;quot;SUMSUCCPROBS&amp;quot; is predicated of each word.</S>
    <S sid="257" ssid="77">SUMSUCCPROBS is calculated by looping through all tags for the words immediately preceding, at, and following a word; for each tag triple, an increment is added, defined by: GetSucc returns the collocational probability of a tag pair; Get3SeqFactor returns either 1, or a special value from the tag-triple list described below.</S>
    <S sid="258" ssid="78">DownGrade modifies the value of GetSucc in accordance with RTPs as described below.</S>
    <S sid="259" ssid="79">The CLAWS documentation describes SUMSUCCPROBS as &amp;quot;the total value of all relationships between the tags associated with this word and the tags associated with the next word.</S>
    <S sid="260" ssid="80">.</S>
    <S sid="261" ssid="81">.</S>
    <S sid="262" ssid="82">[found by] simulating all accesses to SUCCESSORS and ORDER2VALS which will be made.</S>
    <S sid="263" ssid="83">.</S>
    <S sid="264" ssid="84">.</S>
    <S sid="265" ssid="85">.&amp;quot; The probability of each node of the span network (or rather, tree) is then calculated in the following way as a tree representing all paths through which the span network is built: It appears that the goal is to make each tag's probability be the summed probability of all paths passing through it.</S>
    <S sid="266" ssid="86">At the final word of a span, pointers are followed back up the chosen path, and tags are chosen en route.</S>
    <S sid="267" ssid="87">We will see below that a simpler definition of optimal path is possible; nevertheless, there are several advantages of this general approach over previous ones.</S>
    <S sid="268" ssid="88">First, spans of unlimited length can be handled (subject to machine resources).</S>
    <S sid="269" ssid="89">Although earlier researchers (Klein and Simmons, Greene and Rubin) have suggested that spans of length over 5 are rare enough to be of little concern, this is not the case.</S>
    <S sid="270" ssid="90">The number of spans of a given length is a function of that length and the corpus size; so long spans may be obtained merely by examining more text.</S>
    <S sid="271" ssid="91">The total numbers of spans in the Brown Corpus, for each length from 3 to 19, are: 397111, 143447, 60224, 26515, 11409,5128, 2161, 903, 382, 161, 58, 29, 14, 6, 1, 0, 1.</S>
    <S sid="272" ssid="92">Graphing the logarithms Computational Linguistics, Volume 14, Number 1, Winter 1988 33 Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization of these quantities versus the span length for each, produces a near-perfect straight line.</S>
    <S sid="273" ssid="93">Second, a precise mathematical definition is possible for the fundamental idea of CLAWS.</S>
    <S sid="274" ssid="94">Whereas earlier efforts were based primarily on ad hoc or subjectively determined sets of rules and descriptions, and employed substantial exception dictionaries, this algorithm requires no human intervention for set-up; it is a systematic process.</S>
    <S sid="275" ssid="95">Third, the algorithm is quantitative and analog, rather than artificially discrete.</S>
    <S sid="276" ssid="96">The various tests and frames employed by earlier algorithms enforced absolute constraints on particular tags or collocations of tags.</S>
    <S sid="277" ssid="97">Here relative probabilities are weighed, and a series of very likely assignments can make possible a particular, a priori unlikely assignment with which they are associated.</S>
    <S sid="278" ssid="98">In addition to collocational probabilities, CLAWS also takes into account one other empirical quantity: Tags associated with words.</S>
    <S sid="279" ssid="99">.</S>
    <S sid="280" ssid="100">. can be associated with a marker @ or %; @ indicates that the tag is infrequently the correct tag for the associated word(s) (less than 1 in 10 occasions), % indicates that it is highly improbable.</S>
    <S sid="281" ssid="101">.</S>
    <S sid="282" ssid="102">.</S>
    <S sid="283" ssid="103">(less than 1 in 100 occasions).</S>
    <S sid="284" ssid="104">.</S>
    <S sid="285" ssid="105">.</S>
    <S sid="286" ssid="106">.</S>
    <S sid="287" ssid="107">The word disambiguation program currently uses these markers top devalue transition matrix values when retrieving a value from the matrix, @ results in the value being halved, % in the value being divided by eight (Marshall (1983), p. 149).</S>
    <S sid="288" ssid="108">Thus, the independent probability of each possible tag for a given word influences the choice of an optimal path.</S>
    <S sid="289" ssid="109">Such probabilities will be referred to as Relative Tag Probabilities, or RTPs.</S>
    <S sid="290" ssid="110">Other features have been added to the basic algorithm.</S>
    <S sid="291" ssid="111">For example, a good deal of suffix analysis is used in initial tagging.</S>
    <S sid="292" ssid="112">Also, the program filters its output, considering itself to have failed if the optimal tag assignment for a span is not &amp;quot;more than 90% probable&amp;quot;.</S>
    <S sid="293" ssid="113">In such cases it reorders tags rather than actually disambiguating.</S>
    <S sid="294" ssid="114">On long spans this criterion is effectively more stringent than on short spans.</S>
    <S sid="295" ssid="115">A more significant addition to the algorithm is that a number of tag triples associated with a scaling factor have been introduced which may either upgrade or downgrade values in the tree computed from the one-step matrix.</S>
    <S sid="296" ssid="116">For example, the triple [1] 'be' [2] adverb [3] past-tense-verb has been assigned a scaling factor which downgrades a sequence containing this triple compared with a competing sequence of [1] 'be' [2] adverb [3]-past-participle/adjective, on the basis that after a form of 'be', past participles and adjectives are more likely than a past tense verb (Marshall (1983), p. 146).</S>
    <S sid="297" ssid="117">A similar move was used near conjunctions, for which the words on either side, though separated, are more closely correlated to each other than either is to the conjunction itself (Marshall (1983), pp.</S>
    <S sid="298" ssid="118">146-147).</S>
    <S sid="299" ssid="119">For example, a verb/noun ambiguity conjoined to a verb should probably be taken as a verb.</S>
    <S sid="300" ssid="120">Leech, Garside, and Atwell (1983, p. 23) describe &amp;quot;IDIOMTAG&amp;quot;, which is applied after initial tag assignment and before disambiguation.</S>
    <S sid="301" ssid="121">It was developed as a means of dealing with idiosyncratic word sequences which would otherwise cause difficulty for the automatic tagging.</S>
    <S sid="302" ssid="122">.</S>
    <S sid="303" ssid="123">.</S>
    <S sid="304" ssid="124">. for example, in order that is tagged as a single conjunction.</S>
    <S sid="305" ssid="125">.</S>
    <S sid="306" ssid="126">.</S>
    <S sid="307" ssid="127">.</S>
    <S sid="308" ssid="128">The Idiom Tagging Program.</S>
    <S sid="309" ssid="129">.</S>
    <S sid="310" ssid="130">. can look at any combination of words and tags, with or without intervening words.</S>
    <S sid="311" ssid="131">It can delete tags, add tags, or change the probability of tags.</S>
    <S sid="312" ssid="132">Although this program might seem to be an ad hoc device, it is worth bearing in mind that any fully automatic language analysis system has to come to terms with problems of lexical idiosyncrasy.</S>
    <S sid="313" ssid="133">IDIOMTAG also accounts for the fact that the probability of a verb being a past participle, and not simply past, is greater when the following word is &amp;quot;by&amp;quot;, as opposed to other prepositions.</S>
    <S sid="314" ssid="134">Certain cases of this sort may be soluble by making the collocational matrix distinguish classes of ambiguities&#8212;this question is being pursued.</S>
    <S sid="315" ssid="135">Approximately 1% of running text is tagged by IDIOMTAG (letter, G. N. Leech to Henry Kucera, June 7, 1985; letter, E. S. Atwell to Henry Kucera, June 20, 1985).</S>
    <S sid="316" ssid="136">Marshall notes the possibility of consulting a complete three-dimensional matrix of collocational probabilities.</S>
    <S sid="317" ssid="137">Such a matrix would map ordered triples of tags into the relative probability of occurrence of each such triple.</S>
    <S sid="318" ssid="138">Marshall points out that such a table would be too large for its probable usefulness.</S>
    <S sid="319" ssid="139">The author has produced a table based upon more than 85% of the Brown Corpus; it occupies about 2 megabytes (uncompressed).</S>
    <S sid="320" ssid="140">Also, the mean number of examples per triple is very low, thus decreasing accuracy.</S>
    <S sid="321" ssid="141">CLAWS has been applied to the entire LOB Corpus with an accuracy of &amp;quot;between 96% and 97%&amp;quot; (Booth (1985), p. 29).</S>
    <S sid="322" ssid="142">Without the idiom list, the algorithm was 94% accurate on a sample of 15,000 words (Marshall (1983)).</S>
    <S sid="323" ssid="143">Thus, the pre-processor tagging of 1% of all tokens resulted in a 3% change in accuracy; those particular assignments must therefore have had a substantial effect upon their context, resulting in changes of two other words for every one explicitly tagged.</S>
    <S sid="324" ssid="144">But CLAWS is time- and storage-inefficient in the extreme, and in some cases a fallback algorithm is employed to prevent running out of memory, as was discovered by examining the Pascal program code.</S>
    <S sid="325" ssid="145">How often the fallback is employed is not known, nor is it known what effect its use has on overall accuracy.</S>
    <S sid="326" ssid="146">Since CLAWS calculates the probability of every path, it operates in time and space proportional to the product of all the degrees of ambiguity of the words in the span.</S>
    <S sid="327" ssid="147">Thus, the time is exponential (and hence Non-Polynomial) in the span length.</S>
    <S sid="328" ssid="148">For the longest span in the Brown Corpus, of length 18, the number of paths examined would be 1,492,992.</S>
  </SECTION>
  <SECTION title="2 THE LINEAR-TIME ALGORITHM (VOLSUNGA)" number="3">
    <S sid="329" ssid="1">The algorithm described here depends on a similar empirically-derived transitional probability matrix to that of CLAWS, and has a similar definition of &amp;quot;optimal path&amp;quot;.</S>
    <S sid="330" ssid="2">The tagset is larger than TAGGIT's, though smaller than CLAWS', containing 97 tags.</S>
    <S sid="331" ssid="3">The ultimate assignments of tags are much like those of CLAWS.</S>
    <S sid="332" ssid="4">However, it embodies several substantive changes.</S>
    <S sid="333" ssid="5">Those features that can be algorithmically defined have been used to the fullest extent.</S>
    <S sid="334" ssid="6">Other add-ons have been minimized.</S>
    <S sid="335" ssid="7">The major differences are outlined below.</S>
    <S sid="336" ssid="8">First, the optimal path is defined to be the one whose component collocations multiply out to the highest probability.</S>
    <S sid="337" ssid="9">The more complex definition applied by CLAWS, using the sum of all paths at each node of the network, is not used.</S>
    <S sid="338" ssid="10">Second, VOLSUNGA overcomes the Non-Polynomial complexity of CLAWS.</S>
    <S sid="339" ssid="11">Because of this change, it is never necessary to resort to a fallback algorithm, and the program is far smaller.</S>
    <S sid="340" ssid="12">Furthermore, testing the algorithm on extensive texts is not prohibitively costly.</S>
    <S sid="341" ssid="13">Third, VOLSUNGA implements Relative Tag Probabilities (RTPs) in a more quantitative manner, based upon counts from the Brown Corpus.</S>
    <S sid="342" ssid="14">Where CLAWS scales probabilities by 1/2 for RTP &lt; 0.1 (i.e., where less than 10% of the tokens for an ambiguous word are in the category in question), and by 1/8 for p &lt; 0.01, VOLSUNGA uses the RTP value itself as a factor in the equation which defines probability.</S>
    <S sid="343" ssid="15">Fourth, VOLSUNGA uses no tag triples and no idioms.</S>
    <S sid="344" ssid="16">Because of this, manually constructing specialcase lists is not necessary.</S>
    <S sid="345" ssid="17">These methods are useful in certain cases, as the accuracy figures for CLAWS show; but the goal here was to measure the accuracy of a wholly algorithmic tagger on a standard corpus.</S>
    <S sid="346" ssid="18">Interestingly, if the introduction of idiom tagging were to make as much difference for VOLSUNGA as for CLAWS, we would have an accuracy of 99%.</S>
    <S sid="347" ssid="19">This would be an interesting extension.</S>
    <S sid="348" ssid="20">I believe that the reasons for VOLSUNGA's 96% accuracy without idiom tagging are (a) the change in definition of &amp;quot;optimal path&amp;quot;, and (b) the increased precision of RTPs.</S>
    <S sid="349" ssid="21">The difference in tag-set size may also be a factor; but most of the difficult cases are major class differences, such as noun versus verb, rather than the fine distinction which the CLAWS tag-set adds, such as several subtypes of proper noun.</S>
    <S sid="350" ssid="22">Ongoing research with VOLSUNGA may shed more light on the interaction of these factors.</S>
    <S sid="351" ssid="23">Last, the current version of VOLSUNGA is designed for use with a complete dictionary (as is the case when working with a known corpus).</S>
    <S sid="352" ssid="24">Thus, unknown words are handled in a rudimentary fashion.</S>
    <S sid="353" ssid="25">This problem has been repeatedly solved via affix analysis, as mentioned above, and is not of substantial interest here.</S>
    <S sid="354" ssid="26">Since the number of paths over a span is an exponential function of the span length, it may not be obvious how one can guarantee finding the best path, without examining an exponential number of paths (namely all of them).</S>
    <S sid="355" ssid="27">The insight making fast discovery of the optimal path possible is the use of a Dynamic Programming solution (Dano (1975), Dreyfus and Law (1977)).</S>
    <S sid="356" ssid="28">The two key ideas of Dynamic Programming have been characterized as &amp;quot;first, the recognition that a given 'whole problem' can be solved if the values of the best solutions of certain subproblems can be determined.</S>
    <S sid="357" ssid="29">.</S>
    <S sid="358" ssid="30">.</S>
    <S sid="359" ssid="31">; and secondly, the realization that if one starts at or near the end of the 'whole problem,' the subproblems are so simple as to have trivial solutions&amp;quot; (Dreyfus and Law (1977), p. 5).</S>
    <S sid="360" ssid="32">Dynamic Programming is closely related to the study of Graph Theory and of Network Optimization, and can lead to rapid solutions for otherwise intractable problems, given that those problems obey certain structural constraints.</S>
    <S sid="361" ssid="33">In this case, the constraints are indeed obeyed, and a linear-time solution is available.</S>
    <S sid="362" ssid="34">Consider a span of length n = 5, with the words in the path denoted by v, w, x, y, z.</S>
    <S sid="363" ssid="35">Assume that v and z are the unambiguous bounding words, and that the other three words are each three ways ambiguous.</S>
    <S sid="364" ssid="36">Subscripts will index the various tags for each word: w1 will denote the first tag in the set of possible tags for word w. Every path must contain v1 and z1, since v and z are unambiguous.</S>
    <S sid="365" ssid="37">Now consider the partial spans beginning at v, and ending (respectively) at each of the four remaining words.</S>
    <S sid="366" ssid="38">The partial span network ending at w contains exactly three paths.</S>
    <S sid="367" ssid="39">One of these must be a portion of the optimal path for the entire span.</S>
    <S sid="368" ssid="40">So we save all three: one path to each tag under w. The probability of each path is the value found in the collocation matrix entry for its tag-pair, namely p(v,wi) for i ranging from one to three.</S>
    <S sid="369" ssid="41">Next, consider the three tags under word x.</S>
    <S sid="370" ssid="42">One of these tags must lie on the optimal path.</S>
    <S sid="371" ssid="43">Assume it is xl.</S>
    <S sid="372" ssid="44">Under this assumption, we have a complete span of length 3, for x is unambiguous.</S>
    <S sid="373" ssid="45">Only one of the paths to xi can be optimal.</S>
    <S sid="374" ssid="46">Therefore we can disambiguate v. .</S>
    <S sid="375" ssid="47">. w. .</S>
    <S sid="376" ssid="48">. xi under this assumption, namely, as MAX (p(v,wir p(wi,x I)) for all wi.</S>
    <S sid="377" ssid="49">Now, of course, the assumption that x1 is on the optimal path is unacceptable.</S>
    <S sid="378" ssid="50">However, the key to VOLSUNGA is to notice that by making three such independent assumptions, namely for xl, x2, and x3, we exhaust all possible optimal paths.</S>
    <S sid="379" ssid="51">Only a path which optimally leads to one of x's tags can be part of the optimal path.</S>
    <S sid="380" ssid="52">Thus, when examining the partial span network ending at word y, we need only consider three possibly optimal paths, namely those leading to x1, x2, and x3, and how those three combine with the tags of y.</S>
    <S sid="381" ssid="53">At most one of those three paths can lie along the optimal path to each tag of y; so we have 32, or 9, comparisons.</S>
    <S sid="382" ssid="54">But only three paths will survive, namely, the optimal path to each of the three tags under y.</S>
    <S sid="383" ssid="55">Each of those three is then considered as a potential path to z, and one is chosen.</S>
    <S sid="384" ssid="56">This reduces the algorithm from exponential complexity to linear.</S>
    <S sid="385" ssid="57">The number of paths retained at any stage is the same as the degree of ambiguity at that stage; and this value is bounded by a very small value established by independent facts about the English lexicon.</S>
    <S sid="386" ssid="58">No faster order of speed is possible if each word is to be considered at all.</S>
    <S sid="387" ssid="59">As an example, we will consider the process by which VOLSUNGA would tag &amp;quot;The man still saw her&amp;quot;.</S>
    <S sid="388" ssid="60">We will omit a few ambiguities, reducing the number of paths to 24 for ease of exposition.</S>
    <S sid="389" ssid="61">The tags for each word are shown in Table 2.</S>
    <S sid="390" ssid="62">The notation is fairly mnemonic, but it is worth clarifying that PPO indicates an objective personal pronoun, and PP$ the possessive thereof, while VBD is a past-tense verb.</S>
    <S sid="391" ssid="63">Examples of the various collocational probabilities are illustrated in Table 3 (VOLSUNGA does not actually consider any collocation truly impossible, so zeros are raised to a minimal non-zero value when loaded).</S>
    <S sid="392" ssid="64">The product of 1*2*3*2*2*1 ambiguities gives 24 paths through this span.</S>
    <S sid="393" ssid="65">In this case, a simple process of choosing the best successor for each word in order would produce the correct tagging (AT NN RB VBD PPO).</S>
    <S sid="394" ssid="66">But of course this is often not the case.</S>
    <S sid="395" ssid="67">Using VOLSUNGA's method we would first stack &amp;quot;the&amp;quot;, with certainty for the tag AT (we will denote this by &amp;quot;p(the-AT) = CERTAIN)&amp;quot;).</S>
    <S sid="396" ssid="68">Next we stack &amp;quot;man&amp;quot;, and look up the collocational probabilities of all tag pairs between the two words at the top of the stack.</S>
    <S sid="397" ssid="69">In this case they will be p(AT, NN) = 186, and p(AT, VB) = 1.</S>
    <S sid="398" ssid="70">We save the best (in this case only) path to each of man-NN and man-VB.</S>
    <S sid="399" ssid="71">It is sufficient to save a pointer to the tag of &amp;quot;the&amp;quot; which ends each of these paths, making backward-linked lists (which, in this case, converge).</S>
    <S sid="400" ssid="72">Now we stack &amp;quot;still&amp;quot;.</S>
    <S sid="401" ssid="73">For each of its tags (NN, VB, and RB), we choose either the NN or the VB tag of &amp;quot;man&amp;quot; as better. p(still-NN) is the best of: p(man-NN) *p(NN,NN) = 186 *40 = 744 p(man-VB) *p(VB,NN) = 1 *22 = 22 Thus, the best path to still-NN is AT NN NN.</S>
    <S sid="402" ssid="74">Similarly, we find that the best path to still-RB is AT NN RB, and the best path to still-VB is AT NN RB.</S>
    <S sid="403" ssid="75">This shows the (realistically) overwhelming effect of an article on disambiguating an immediately following noun/verb ambiguity.</S>
    <S sid="404" ssid="76">At this point, only the optimal path to each of the tags for &amp;quot;still&amp;quot; is saved.</S>
    <S sid="405" ssid="77">We then go on to match each of those paths with each of the tags for &amp;quot;saw&amp;quot;, discovering the optimal paths to saw-NN and to saw-VB.</S>
    <S sid="406" ssid="78">The next iteration reveals the optimal paths to her-PPO and her-PP$, and the final one picks the optimal path to the period, which this example treats as unambiguous.</S>
    <S sid="407" ssid="79">Now we have the best path between two certain tags (AT and .</S>
    <S sid="408" ssid="80">), and can merely pop the stack, following pointers to optimal predecessors to disambiguate the sequence.</S>
    <S sid="409" ssid="81">The period becomes the start of the next span.</S>
    <S sid="410" ssid="82">Initial testing of the algorithm used only transitional probability information.</S>
    <S sid="411" ssid="83">RTPs had no effect upon choosing an optimal path.</S>
    <S sid="412" ssid="84">For example, in deciding whether to consider the word &amp;quot;time&amp;quot; to be a noun or a verb, environments such as a preceding article or proper noun, or a following verb or pronoun, were the sole criteria.</S>
    <S sid="413" ssid="85">The fact that &amp;quot;time&amp;quot; is almost always a noun (1901 instances in the Brown Corpus) rather than a verb (16 instances) was not considered.</S>
    <S sid="414" ssid="86">Accuracy averaged 92-93%, with a peak of 93.7%.</S>
    <S sid="415" ssid="87">There are clear examples for which the use of RTPs is important.</S>
    <S sid="416" ssid="88">One such case which arises in the Brown Corpus is &amp;quot;so that&amp;quot;.</S>
    <S sid="417" ssid="89">&amp;quot;So&amp;quot; occurs 932 times as a qualifier (QL), 479 times as a subordinating conjunction (CS), and once as an interjection (UH).</S>
    <S sid="418" ssid="90">The standard tagging for &amp;quot;so that&amp;quot; is &amp;quot;CS CS&amp;quot;, but this is an extremely low-frequency collocation, lower than the alternative &amp;quot;UH CS&amp;quot; (which is mainly limited to fiction).</S>
    <S sid="419" ssid="91">Barring strong contextual counter-evidence, &amp;quot;UH CS&amp;quot; is the preferred assignment if RTP information is not used.</S>
    <S sid="420" ssid="92">By weighing the RTPs for &amp;quot;so&amp;quot;, however, the &amp;quot;UH&amp;quot; assignment can be avoided.</S>
    <S sid="421" ssid="93">The LOB Corpus would (via idiom tagging) use &amp;quot;CS CS&amp;quot; in this case, employing a special &amp;quot;ditto tag&amp;quot; to indicate that two separate orthographic words constitute (at least for tagging purposes) a single syntactic word.</S>
    <S sid="422" ssid="94">Another example would be &amp;quot;so as to&amp;quot;, tagged 'TO TO TO&amp;quot;.</S>
    <S sid="423" ssid="95">Blackwell comments that &amp;quot;it was difficult to know where to draw the line in defining what constituted an idiom, and some such decisions seemed to have been influenced by semantic factors.</S>
    <S sid="424" ssid="96">Nonetheless, IDIOMTAG had played a significant part in increasing the accuracy of the Tagging Suite [i.e., CLAWS].</S>
    <S sid="425" ssid="97">.</S>
    <S sid="426" ssid="98">.&amp;quot; (Blackwell (1985), p. 7).</S>
    <S sid="427" ssid="99">It may be better to treat this class of &amp;quot;idioms&amp;quot; as lexical items which happen to contain blanks; but RTPs permit correct tagging in some of these cases.</S>
    <S sid="428" ssid="100">The main difficulty in using RTPs is determining how heavily to weigh them relative to collocational information.</S>
    <S sid="429" ssid="101">At first, VOLSUNGA multiplied raw relative frequencies into the path probability calculations; but the ratios were so high in some cases as to totally swamp collocational data.</S>
    <S sid="430" ssid="102">Thus, normalization is required.</S>
    <S sid="431" ssid="103">The present solution is a simple one; all ratios over a fixed limit are truncated to that limit.</S>
    <S sid="432" ssid="104">Implementing RTPs increased accuracy by approximately 4%, to the range 95-97%, with a peak of 97.5% on one small sample.</S>
    <S sid="433" ssid="105">Thus, about half of the residual errors were eliminated.</S>
    <S sid="434" ssid="106">It is likely that tuning the normalization would improve this figure slightly more.</S>
    <S sid="435" ssid="107">VOLSUNGA was not designed with psychological reality as a goal, though it has some plausible characteristics.</S>
    <S sid="436" ssid="108">We will consider a few of these briefly.</S>
    <S sid="437" ssid="109">This section should not be interpreted as more than suggestive.</S>
    <S sid="438" ssid="110">First, consider dictionary learning; the program currently assumes that a full dictionary is available.</S>
    <S sid="439" ssid="111">This assumption is nearly true for mature language users, but humans have little trouble even with novel lexical items, and generally speak of &amp;quot;context&amp;quot; when asked to describe how they figure out such words.</S>
    <S sid="440" ssid="112">As Ryder and Walker (1982) note, the use of structural analysis based on contextual clues allows speakers to compute syntactic structures even for a text such as Jabberwocky, where lexical information is clearly insufficient.</S>
    <S sid="441" ssid="113">The immediate syntactic context severely restricts the likely choices for the grammatical category of each neologism.</S>
    <S sid="442" ssid="114">VOLSUNGA can perform much the same task via a minor modification, even if a suffix analysis fails.</S>
    <S sid="443" ssid="115">The most obvious solution is simply to assign all tags to the unknown word and find the optimal path through the containing span as usual.</S>
    <S sid="444" ssid="116">Since the algorithm is fast, this is not prohibitive.</S>
    <S sid="445" ssid="117">Better, one can assign only those tags with a non-minimal probability of being adjacent to the possible tags of neighboring words.</S>
    <S sid="446" ssid="118">Precisely calculating the mean number of tags remaining under this approach is left as a question for further research, but the number is certainly very low.</S>
    <S sid="447" ssid="119">About 3900 of the 9409 theoretically possible tag pairs occur in the Brown Corpus.</S>
    <S sid="448" ssid="120">Also, all tags marking closed classes (about two-thirds of all tags) may be eliminated from consideration.</S>
    <S sid="449" ssid="121">Also, since VOLSUNGA operates from left to right, it can always decide upon an optimum partial result, and can predict a set of probable successors.</S>
    <S sid="450" ssid="122">For these reasons, it is largely robust against ungrammaticality.</S>
    <S sid="451" ssid="123">Shannon (1951) performed experiments of a similar sort, asking human subjects to predict the next character of a partially presented sentence.</S>
    <S sid="452" ssid="124">The accuracy of their predictions increased with the length of the sentence fragment presented.</S>
    <S sid="453" ssid="125">The fact that VOLSUNGA requires a great deal of persistent memory for its dictionary, yet very little temporary space for processing, is appropriate.</S>
    <S sid="454" ssid="126">By contrast, the space requirements of CLAWS would overtax the short-term memory of any language user.</S>
    <S sid="455" ssid="127">Another advantage of VOLSUNGA is that it requires little inherent linguistic knowledge.</S>
    <S sid="456" ssid="128">Probabilities may be acquired simply through counting instances of collocation.</S>
    <S sid="457" ssid="129">The results will increase in accuracy as more input text is seen.</S>
    <S sid="458" ssid="130">Previous algorithms, on the other hand, have included extensive manually generated lists of rules or exceptions.</S>
    <S sid="459" ssid="131">An obvious difference between VOLSUNGA and humans is that VOLSUNGA makes no use whatsoever of semantic information.</S>
    <S sid="460" ssid="132">No account is taken of the high probability that in a text about carpentry, &amp;quot;saw&amp;quot; is more likely a noun than in other types of text.</S>
    <S sid="461" ssid="133">There may also be genre and topic-dependent influences upon the frequencies of various syntactic, and hence categorial, structures.</S>
    <S sid="462" ssid="134">Before such factors can be incorporated into VOLSUNGA, however, more complete dictionaries, including semantic information of at least a rudimentary kind, must be available.</S>
  </SECTION>
  <SECTION title="3 ACCURACY ANALYSIS:" number="4">
    <S sid="463" ssid="1">VOLSUNGA requires a tagged corpus upon which to base its tables of probabilities.</S>
    <S sid="464" ssid="2">The calculation of transitional probabilities is described by Marshall (1983).</S>
    <S sid="465" ssid="3">The entire Brown Corpus (modified by the expansion of contracted forms) was analyzed in order to produce the tables used in VOLSUNGA.</S>
    <S sid="466" ssid="4">A complete dictionary was therefore available when running the program on that same corpus.</S>
    <S sid="467" ssid="5">Since the statistics comprising the dictionary and probability matrix used by the program were derived from the same corpus analyzed, the results may be considered optimal.</S>
    <S sid="468" ssid="6">On the other hand, the Corpus is comprehensive enough so that use of other input text is unlikely to introduce statistically significant changes in the program's performance.</S>
    <S sid="469" ssid="7">This is especially true because many of the unknown words would be (a) capitalized proper names, for which tag assignment is trivial modulo a small percentage at sentence boundaries, or (b) regular formations from existing words, which are readily identified by suffixes.</S>
    <S sid="470" ssid="8">Greene and Rubin (1971) note that their suffix list &amp;quot;consists mainly of Romance endings which are the source of continuing additions to the language&amp;quot; (p. 41).</S>
    <S sid="471" ssid="9">A natural relationship exists between the size of a dictionary, and the percentage of words in an average text which it accounts for.</S>
    <S sid="472" ssid="10">A complete table showing the relationship appears in Kucera and Francis (1967) pp.</S>
    <S sid="473" ssid="11">300-307.</S>
    <S sid="474" ssid="12">A few representative entries are shown in Table 4.</S>
    <S sid="475" ssid="13">The &amp;quot;#Types&amp;quot; column indicates how many vocabulary items occur at least &amp;quot;Freq Limit&amp;quot; times in the Corpus.</S>
    <S sid="476" ssid="14">The &amp;quot;#Tokens&amp;quot; column shows how many tokens are accounted for by those types, and the &amp;quot;%Tokens&amp;quot; column converts this number to a percentage.</S>
    <S sid="477" ssid="15">(See also pp.</S>
    <S sid="478" ssid="16">358-362 in the same volume for several related graphs.)</S>
    <S sid="479" ssid="17">Table 5 lists the accuracy for each genre from the Brown Corpus.</S>
    <S sid="480" ssid="18">The total token count differs from Table 4 due to inclusion of non-lexical tokens, such as punctuation.</S>
    <S sid="481" ssid="19">The figure shown deducts from the error count those particular instances in which the Corpus tag indicates by an affix that the word is part of a headline, title, etc.</S>
    <S sid="482" ssid="20">Since the syntax of such structures is often deviant, such errors are less significant.</S>
    <S sid="483" ssid="21">The difference this makes ranges from 0.09% (Genre L), up to 0.64% (Genre A), with an unweighted mean of 0.31%.</S>
    <S sid="484" ssid="22">Detailed breakdowns of the particular errors made for each genre exist in machine-readable form.</S>
  </SECTION>
  <SECTION title="4 CONCLUSION" number="5">
    <S sid="485" ssid="1">The high degree of lexical category ambiguity in languages such as English poses problems for parsing.</S>
    <S sid="486" ssid="2">Specifically, until the categories of individual words have been established, it is difficult to construct a unique and accurate syntactic structure.</S>
    <S sid="487" ssid="3">Therefore, a method for locally disambiguating lexical items has been developed.</S>
    <S sid="488" ssid="4">Early efforts to solve this problem relied upon large libraries of manually chosen context frame rules.</S>
    <S sid="489" ssid="5">More recently, however, work on the LOB Corpus of British English led to a more systematic algorithm based upon combinatorial statistics.</S>
    <S sid="490" ssid="6">This algorithm operates entirely from left to right, and has no inherent limit upon the number of consecutive ambiguities which may be processed.</S>
    <S sid="491" ssid="7">Its authors report an accuracy of 96-97%.</S>
    <S sid="492" ssid="8">However, CLAWS falls prey to other problems.</S>
    <S sid="493" ssid="9">First, the probabilistic system has been augmented in several ways, such as by pre-tagging of categorially troublesome &amp;quot;idioms&amp;quot; (this feature contributes 3% towards the total accuracy).</S>
    <S sid="494" ssid="10">Second, it was not based upon the most complete statistics available.</S>
    <S sid="495" ssid="11">Third, and perhaps most significant, it requires non-polynomially large time and space.</S>
    <S sid="496" ssid="12">The algorithm developed here, called VOLSUNGA, addresses these problems.</S>
    <S sid="497" ssid="13">First, the various additions to CLAWS (i.e., beyond the use of two-place probabilities and RTPs) have been deleted.</S>
    <S sid="498" ssid="14">Second, the program has been calibrated by reference to 100% instead of 20% of the Brown Corpus, and has been applied to the entire Corpus for testing.</S>
    <S sid="499" ssid="15">This is a particularly important test because the Brown Corpus provides a long-established standard against which accuracy can be measured.</S>
    <S sid="500" ssid="16">Third, the algorithm has been completely redesigned so that it establishes the optimal tag assignments in linear time, as opposed to exponential.</S>
    <S sid="501" ssid="17">Tests on the one million words of the Brown Corpus show an overall accuracy of approximately 96%, despite the non-use of auxiliary algorithms.</S>
    <S sid="502" ssid="18">Suggestions have been given for several possible modifications which might yield even higher accuracies.</S>
    <S sid="503" ssid="19">The accuracy and speed of VOLSUNGA make it suitable for use in pre-processing natural language input to parsers and other language understanding systems.</S>
    <S sid="504" ssid="20">Its systematicity makes it suitable also for work in computational studies of language learning.</S>
  </SECTION>
</PAPER>
