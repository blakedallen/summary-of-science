[
  {
    "citance_No": 1, 
    "citing_paper_id": "P13-1125", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Karthik, Visweswariah | Mitesh M., Khapra | Ananthakrishnan, Ramanathan", 
    "raw_text": "This model was significantly better than the MaxEnt aligner (Ittycheriah and Roukos, 2005) and is also flexible in the sense that it allows for arbitrary features to be introduced while still keeping training and decoding tractable by using a greedy decoding algorithm that explores potential alignments in a small neighborhood of the current alignment", 
    "clean_text": "This model was significantly better than the MaxEnt aligner (Ittycheriah and Roukos, 2005) and is also flexible in the sense that it allows for arbitrary features to be introduced while still keeping training and decoding tractable by using a greedy decoding algorithm that explores potential alignments in a small neighborhood of the current alignment.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P13-1125", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Karthik, Visweswariah | Mitesh M., Khapra | Ananthakrishnan, Ramanathan", 
    "raw_text": "The model thus needs a reasonably good initial alignment to start with for which we use the MaxEnt aligner (Ittycheriah and Roukos, 2005) as in McCarley et al (2011)", 
    "clean_text": "The model thus needs a reasonably good initial alignment to start with for which we use the MaxEnt aligner (Ittycheriah and Roukos, 2005) as in McCarley et al (2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P13-1125", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Karthik, Visweswariah | Mitesh M., Khapra | Ananthakrishnan, Ramanathan", 
    "raw_text": "We experimented with two different supervised aligners: a maximum entropy aligner (Ittycheriah and Roukos, 2005) and an improved correction model that corrects the maximum entropy alignments (McCarley et al, 2011)", 
    "clean_text": "We experimented with two different supervised aligners: a maximum entropy aligner (Ittycheriah and Roukos, 2005) and an improved correction model that corrects the maximum entropy alignments (McCarley et al, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "C10-1126", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Karthik, Visweswariah | Jiri, Navratil | Jeffrey S., Sorensen | Vijil, Chenthamarakshan | Nanda, Kambhatla", 
    "raw_text": "In each language, the rule extraction was performed using approximately 1.2M sentence pairs aligned using a maxent aligner (Ittycheriah and Roukos, 2005) trained using a variety of domains (Europarl, computer manuals) 1122 and a maximum entropy parser for English (Ratnaparkhi, 1999)", 
    "clean_text": "In each language, the rule extraction was performed using approximately 1.2M sentence pairs aligned using a maxent aligner (Ittycheriah and Roukos, 2005) trained using a variety of domains (Europarl, computer manuals) and a maximum entropy parser for English (Ratnaparkhi, 1999).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "C10-1126", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Karthik, Visweswariah | Jiri, Navratil | Jeffrey S., Sorensen | Vijil, Chenthamarakshan | Nanda, Kambhatla", 
    "raw_text": "To quantify this effect, we learn reordering rules using three sets of alignments: HMM alignments, alignments from a supervised MaxEnt aligner (Ittycheriah and Roukos, 2005), and hand alignments", 
    "clean_text": "To quantify this effect, we learn reordering rules using three sets of alignments: HMM alignments, alignments from a supervised MaxEnt aligner (Ittycheriah and Roukos, 2005), and hand alignments.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D08-1060", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Bing, Zhao | Yaser, Al-Onaizan", 
    "raw_text": "Parallel sentences were first word-aligned using a MaxEnt aligner (Ittycheriah and Roukos, 2005) .Then, phrase-pairs that overlap with our development and test set were extracted from the word alignments (from both hand alignments and automatically aligned GALE corpora) based on the projection principle (Tillmann, 2003)", 
    "clean_text": "Parallel sentences were first word-aligned using a MaxEnt aligner (Ittycheriah and Roukos, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D08-1060", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Bing, Zhao | Yaser, Al-Onaizan", 
    "raw_text": "As our word aligner (Ittycheriah and Roukos, 2005) can introduce errors in extracting Tree-to-String rules, we use a small hand-aligned data set? CE16K?, which consists of 16K sentence pairs, to get relatively clean rules, free from alignment errors", 
    "clean_text": "As our word aligner (Ittycheriah and Roukos, 2005) can introduce errors in extracting Tree-to-String rules, we use a small hand-aligned data set CE16K, which consists of 16K sentence pairs, to get relatively clean rules, free from alignment errors.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D07-1006", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Alexander, Fraser | Daniel, Marcu", 
    "raw_text": "Adiscriminativelytrained1-to-N model with feature functions specifically designed for Arabic was presented in (Ittycheriah and Roukos, 2005)", 
    "clean_text": "A discriminatively trained 1-to-N model with feature functions specifically designed for Arabic was presented in (Ittycheriah and Roukos, 2005).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P06-1009", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "They formulated a global model, without making a Markovian assumption, leading to the need for a sub-optimal heuristic search strategies. Ittycheriah and Roukos (2005) trained a dis 71criminative model on a corpus of ten thousand word aligned Arabic-English sentence pairs that outperformed a GIZA++ baseline", 
    "clean_text": "Ittycheriah and Roukos (2005) trained a discriminative model on a corpus of ten thousand word aligned Arabic-English sentence pairs that outperformed a GIZA++ baseline.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N07-2007", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jakob, Elming | Nizar, Habash", 
    "raw_text": "The gold standard alignments we use here are part of the IBM Arabic-English aligned corpus (IBMAC) 4 (Ittycheriah and Roukos, 2005)", 
    "clean_text": "The gold standard alignments we use here are part of the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "N07-2007", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jakob, Elming | Nizar, Habash", 
    "raw_text": "Since the IBMAC gold alignments we use are not marked as such, AER reduces to 1 F-score (Ittycheriah and Roukos, 2005): Pr= |A? S||A| Rc= |A? S| |S| AER= 1? 2PrRc Pr+Rc where A links are proposed and S links are gold", 
    "clean_text": "Since the IBMAC gold alignments we use are not marked as such, AER reduces to 1 F-score (Ittycheriah and Roukos, 2005): Pr= |A? S||A| Rc= |A? S| |S| AER= 1? 2PrRc Pr+Rc where A links are proposed and S links are gold.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N07-2007", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jakob, Elming | Nizar, Habash", 
    "raw_text": "Ittycheriah and Roukos (2005) used only the top50 sentences in IBMAC test data", 
    "clean_text": "Ittycheriah and Roukos (2005) used only the top 50 sentences in IBMAC test data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N07-2007", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jakob, Elming | Nizar, Habash", 
    "raw_text": "The two results are not comparable because: (a) Ittycheriah and Roukos (2005) used additional gold aligned data that was not released and (b) they use an additional 500K sentences from the LDC UN corpus for Giza training that was created by adapting to the source side of the test set? the details of such adaptation were not provided and thus it is not clear how to replicate them to compare fairly", 
    "clean_text": "The two results are not comparable because: (a) Ittycheriah and Roukos (2005) used additional gold aligned data that was not released and (b) they use an additional 500K sentences from the LDC UN corpus for Giza training that was created by adapting to the source side of the test set - the details of such adaptation were not provided and thus it is not clear how to replicate them to compare fairly.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "C10-2147", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Karthik, Visweswariah | Vijil, Chenthamarakshan | Nanda, Kambhatla", 
    "raw_text": "For word alignments we use the Maximum Entropy aligner described in (Ittycheriah and Roukos, 2005) that is trained using hand aligned training data", 
    "clean_text": "For word alignments we use the Maximum Entropy aligner described in (Ittycheriah and Roukos, 2005) that is trained using hand aligned training data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "C10-2147", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Karthik, Visweswariah | Vijil, Chenthamarakshan | Nanda, Kambhatla", 
    "raw_text": "To find the most likely alignment we use the same algorithm as in (Ittycheriah and Roukos, 2005) since the structure of the model is unchanged", 
    "clean_text": "To find the most likely alignment we use the same algorithm as in (Ittycheriah and Roukos, 2005) since the structure of the model is unchanged.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D10-1097", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Yassine, Benajiba | Imed, Zitouni", 
    "raw_text": "Thedatain this corpus is automatically aligned using a technique presented in (Ittycheriah and Roukos, 2005)", 
    "clean_text": "The data in this corpus is automatically aligned using a technique presented in (Ittycheriah and Roukos, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D11-1125", 
    "citing_paper_authority": 66, 
    "citing_paper_authors": "Mark, Hopkins | Jonathan, May", 
    "raw_text": "Ittycheriah and Roukos (2005) used a maximum entropy classifier to train an alignment model using hand-labeled data", 
    "clean_text": "Ittycheriah and Roukos (2005) used a maximum entropy classifier to train an alignment model using hand-labeled data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P06-1091", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Christoph, Tillmann | Tong, Zhang", 
    "raw_text": "Additional phrase-based features include block orientation, target and source phrase bigram features. Word-based features are used as well ,e.g .feature K &apos ;cNc\u0012 captures word-to-word translation de 4On our test set, (Tillmann and Zhang, 2005) reports able U score of d? e ?f+g and (Ittycheriah and Roukos, 2005) re ports a BLEU score of hYg? f i .pendencies similar to the use of Model b probabilities in (Koehn et al, 2003)", 
    "clean_text": "On our test set, (Tillmann and Zhang, 2005) reports a BLEU score of 37.8 and (Ittycheriah and Roukos, 2005) reports a BLEU score of 48.0.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "N07-1008", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Abraham, Ittycheriah | Salim, Roukos", 
    "raw_text": "We assume that each sentence pair in the training corpus is word-aligned (e.g. using a MaxEnt aligner (Ittycheriah and Roukos, 2005) or an HMM aligner (Ge, 2004))", 
    "clean_text": "We assume that each sentence pair in the training corpus is word-aligned (e.g. using a MaxEnt aligner (Ittycheriah and Roukos, 2005) or an HMM aligner (Ge, 2004)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P09-1105", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Fei, Huang", 
    "raw_text": "and (Ittycheriah and Roukos, 2005)), which also introduce many word alignment errors. The example in Figure 1 shows the word alignment of the given Chinese and English sentence pair, where the English words following each Chinese word is its literal translation", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]
