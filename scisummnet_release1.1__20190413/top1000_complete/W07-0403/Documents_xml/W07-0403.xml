<PAPER>
  <S sid="0">Inversion Transduction Grammar for Joint Phrasal Translation Modeling</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models.</S>
    <S sid="2" ssid="2">This syntactic model is similar to its flatstring phrasal predecessors, but admits polynomial-time algorithms for Viterbi alignment and EM training.</S>
    <S sid="3" ssid="3">We demonstrate that the consistency constraints that allow flat phrasal models to scale also help ITG algorithms, producing an 80-times faster inside-outside algorithm.</S>
    <S sid="4" ssid="4">We also show that the phrasal translation tables produced by the ITG are superior to those of the flat joint phrasal model, producing up to a 2.5 point improvement in BLEU score.</S>
    <S sid="5" ssid="5">Finally, we explore, for the first time, the utility of a joint phrasal translation model as a word alignment method.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Statistical machine translation benefits greatly from considering more than one word at a time.</S>
    <S sid="7" ssid="2">One can put forward any number of non-compositional translations to support this point, such as the colloquial Canadian French-English pair, (Wo les moteurs, Hold your horses), where no clear word-toword connection can be drawn.</S>
    <S sid="8" ssid="3">Nearly all current decoding methods have shifted to phrasal representations, gaining the ability to handle noncompositional translations, but also allowing the decoder to memorize phenomena such as monolingual agreement and short-range movement, taking pressure off of language and distortion models.</S>
    <S sid="9" ssid="4">Despite the success of phrasal decoders, knowledge acquisition for translation generally begins with a word-level analysis of the training text, taking the form of a word alignment.</S>
    <S sid="10" ssid="5">Attempts to apply the same statistical analysis used at the word level in a phrasal setting have met with limited success, held back by the sheer size of phrasal alignment space.</S>
    <S sid="11" ssid="6">Hybrid methods that combine well-founded statistical analysis with high-confidence word-level alignments have made some headway (Birch et al., 2006), but suffer from the daunting task of heuristically exploring a still very large alignment space.</S>
    <S sid="12" ssid="7">In the meantime, synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents, but continue to be employed primarily for word-to-word analysis (Wu, 1997).</S>
    <S sid="13" ssid="8">In this paper we unify the probability models for phrasal translation with the algorithms for synchronous parsing, harnessing the benefits of both to create a statistically and algorithmically wellfounded method for phrasal analysis of bitext.</S>
    <S sid="14" ssid="9">Section 2 begins by outlining the phrase extraction system we intend to replace and the two methods we combine to do so: the joint phrasal translation model (JPTM) and inversion transduction grammar (ITG).</S>
    <S sid="15" ssid="10">Section 3 describes our proposed solution, a phrasal ITG.</S>
    <S sid="16" ssid="11">Section 4 describes how to apply our phrasal ITG, both as a translation model and as a phrasal word-aligner.</S>
    <S sid="17" ssid="12">Section 5 tests our system in both these capacities, while Section 6 concludes.</S>
  </SECTION>
  <SECTION title="2 Background" number="2">
    <S sid="18" ssid="1">Phrasal decoders require a phrase table (Koehn et al., 2003), which contains bilingual phrase pairs and scores indicating their utility.</S>
    <S sid="19" ssid="2">The surface heuristic is the most popular method for phrase-table construction.</S>
    <S sid="20" ssid="3">It extracts all consistent phrase pairs from word-aligned bitext (Koehn et al., 2003).</S>
    <S sid="21" ssid="4">The word alignment provides bilingual links, indicating translation relationships between words.</S>
    <S sid="22" ssid="5">Consistency is defined so that alignment links are never broken by phrase boundaries.</S>
    <S sid="23" ssid="6">For each token w in a consistent phrase pair p, all tokens linked to w by the alignment must also be included in p. Each consistent phrase pair is counted as occurring once per sentence pair.</S>
    <S sid="24" ssid="7">The scores for the extracted phrase pairs are provided by normalizing these flat counts according to common English or Foreign components, producing the conditional distributions p( f|e) and p(e |f).</S>
    <S sid="25" ssid="8">The surface heuristic can define consistency according to any word alignment; but most often, the alignment is provided by GIZA++ (Och and Ney, 2003).</S>
    <S sid="26" ssid="9">This alignment system is powered by the IBM translation models (Brown et al., 1993), in which one sentence generates the other.</S>
    <S sid="27" ssid="10">These models produce only one-to-many alignments: each generated token can participate in at most one link.</S>
    <S sid="28" ssid="11">Many-to-many alignments can be created by combining two GIZA++ alignments, one where English generates Foreign and another with those roles reversed (Och and Ney, 2003).</S>
    <S sid="29" ssid="12">Combination approaches begin with the intersection of the two alignments, and add links from the union heuristically.</S>
    <S sid="30" ssid="13">The grow-diag-final (GDF) combination heuristic (Koehn et al., 2003) adds links so that each new link connects a previously unlinked token.</S>
    <S sid="31" ssid="14">The IBM models that power GIZA++ are trained with Expectation Maximization (Dempster et al., 1977), or EM, on sentence-aligned bitext.</S>
    <S sid="32" ssid="15">A translation model assigns probabilities to alignments; these alignment distributions are used to count translation events, which are then used to estimate new parameters for the translation model.</S>
    <S sid="33" ssid="16">Sampling is employed when the alignment distributions cannot be calculated efficiently.</S>
    <S sid="34" ssid="17">This statistically-motivated process is much more appealing than the flat counting described in Section 2.1, but it does not directly include phrases.</S>
    <S sid="35" ssid="18">The joint phrasal translation model (Marcu and Wong, 2002), or JPTM, applies the same statistical techniques from the IBM models in a phrasal setting.</S>
    <S sid="36" ssid="19">The JPTM is designed according to a generative process where both languages are generated simultaneously.</S>
    <S sid="37" ssid="20">First, a bag of concepts, or cepts, C is generated.</S>
    <S sid="38" ssid="21">Each ci E C corresponds to a bilingual phrase pair, ci = (ei, &#65533;fi).</S>
    <S sid="39" ssid="22">These contiguous phrases are permuted in each language to create two sequences of phrases.</S>
    <S sid="40" ssid="23">Initially, Marcu and Wong assume that the number of cepts, as well as the phrase orderings, are drawn from uniform distributions.</S>
    <S sid="41" ssid="24">That leaves a joint translation distribution p(ei, &#65533;fi) to determine which phrase pairs are selected.</S>
    <S sid="42" ssid="25">Given a lexicon of possible cepts and a predicate L(E, F, C) that determines if a bag of cepts C can be bilingually permuted to create the sentence pair (E, F), the probability of a sentence pair is: If left unconstrained, (1) will consider every phrasal segmentation of E and F, and every alignment between those phrases.</S>
    <S sid="43" ssid="26">Later, a distortion model based on absolute token positions is added to (1).</S>
    <S sid="44" ssid="27">The JPTM faces several problems when scaling up to large training sets: all co-occurring phrases observed in the bitext.</S>
    <S sid="45" ssid="28">This is far too large to fit in main memory, and can be unwieldly for storage on disk.</S>
    <S sid="46" ssid="29">Marcu and Wong (2002) address point 2 with a lexicon constraint; monolingual phrases that are above a length threshold or below a frequency threshold are excluded from the lexicon.</S>
    <S sid="47" ssid="30">Point 3 is handled by hill-climbing to a likely phrasal alignment and sampling around it.</S>
    <S sid="48" ssid="31">However, point 1 remains unaddressed, which prevents the model from scaling to large data sets.</S>
    <S sid="49" ssid="32">Birch et al. (2006) handle point 1 directly by reducing the size of the alignment space.</S>
    <S sid="50" ssid="33">This is f) will cover accomplished by constraining the JPTM to only use phrase pairs that are consistent with a highconfidence word alignment, which is provided by GIZA++ intersection.</S>
    <S sid="51" ssid="34">We refer to this constrained JPTM as a C-JPTM.</S>
    <S sid="52" ssid="35">This strikes an interesting middle ground between the surface heuristic described in Section 2.1 and the JPTM.</S>
    <S sid="53" ssid="36">Like the surface heuristic, a word alignment is used to limit the phrase pairs considered, but the C-JPTM reasons about distributions over phrasal alignments, instead of taking flat counts.</S>
    <S sid="54" ssid="37">The consistency constraint allows them to scale their C-JPTM up to 700,000 sentence pairs.</S>
    <S sid="55" ssid="38">With this constraint in place, the use of hill-climbing and sampling during EM training becomes one of the largest remaining weaknesses of the C-JPTM.</S>
    <S sid="56" ssid="39">Like the JPTM, stochastic synchronous grammars provide a generative process to produce a sentence and its translation simultaneously.</S>
    <S sid="57" ssid="40">Inversion transduction grammar (Wu, 1997), or ITG, is a wellstudied synchronous grammar formalism.</S>
    <S sid="58" ssid="41">Terminal productions of the form A &#8212;* e/f produce a token in each stream, or a token in one stream with the null symbol 0 in the other.</S>
    <S sid="59" ssid="42">To allow for movement during translation, non-terminal productions can be either straight or inverted.</S>
    <S sid="60" ssid="43">Straight productions, with their non-terminals inside square brackets [...], produce their symbols in the given order in both streams.</S>
    <S sid="61" ssid="44">Inverted productions, indicated by angled brackets (...), are output in reverse order in the Foreign stream only.</S>
    <S sid="62" ssid="45">The work described here uses the binary bracketing ITG, which has a single non-terminal: This grammar admits an efficient bitext parsing algorithm, and holds no language-specific biases.</S>
    <S sid="63" ssid="46">(2) cannot represent all possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (Melamed, 2003).</S>
    <S sid="64" ssid="47">This ITG constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997).</S>
    <S sid="65" ssid="48">Empirical studies suggest that only a small percentage of human translations violate these constraints (Cherry and Lin, 2006).</S>
    <S sid="66" ssid="49">Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A &#8212;* X are assigned probability Pr(X |A).</S>
    <S sid="67" ssid="50">These parameters can be learned from sentence-aligned bitext using the EM algorithm.</S>
    <S sid="68" ssid="51">The expectation task of counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended to bitext (Zhang and Gildea, 2004).</S>
  </SECTION>
  <SECTION title="3 ITG as a Phrasal Translation Model" number="3">
    <S sid="69" ssid="1">This paper introduces a phrasal ITG; in doing so, we combine ITG with the JPTM.</S>
    <S sid="70" ssid="2">ITG parsing algorithms consider every possible two-dimensional span of bitext, each corresponding to a bilingual phrase pair.</S>
    <S sid="71" ssid="3">Each multi-token span is analyzed in terms of how it could be built from smaller spans using a straight or inverted production, as is illustrated in Figures 2 (a) and (b).</S>
    <S sid="72" ssid="4">To extend ITG to a phrasal setting, we add a third option for span analysis: that the span under consideration might have been drawn directly from the lexicon.</S>
    <S sid="73" ssid="5">This option can be added to our grammar by altering the definition of a terminal production to include phrases: A &#8212;* 6/ 1.</S>
    <S sid="74" ssid="6">This third option is shown in Figure 2 (c).</S>
    <S sid="75" ssid="7">The model implied by this extended grammar is trained using inside-outside and EM.</S>
    <S sid="76" ssid="8">Our approach differs from previous attempts to use ITGs for phrasal bitext analysis.</S>
    <S sid="77" ssid="9">Wu (1997) used a binary bracketing ITG to segment a sentence while simultaneously word-aligning it to its translation, but the model was trained heuristically with a fixed segmentation.</S>
    <S sid="78" ssid="10">Vilar and Vidal (2005) used ITG-like dynamic programming to drive both training and alignment for their recursive translation model, but they employed a conditional model that did not maintain a phrasal lexicon.</S>
    <S sid="79" ssid="11">Instead, they scored phrase pairs using IBM Model 1.</S>
    <S sid="80" ssid="12">Our phrasal ITG is quite similar to the JPTM.</S>
    <S sid="81" ssid="13">Both models are trained with EM, and both employ generative stories that create a sentence and its translation simultaneously.</S>
    <S sid="82" ssid="14">The similarities become more apparent when we consider the canonical-form binary-bracketing ITG (Wu, 1997) shown here: (3) is employed in place of (2) to reduce redundant alignments and clean up EM expectations.1 More importantly for our purposes, it introduces a preterminal C, which generates all phrase pairs or cepts.</S>
    <S sid="83" ssid="15">When (3) is parameterized as a stochastic ITG, the conditional distribution p(6/ 1|C) is equivalent to the JPTM&#8217;s p(e, 1); both are joint distributions over all possible phrase pairs.</S>
    <S sid="84" ssid="16">The distributions conditioned on the remaining three non-terminals assign probability to concept movement by tracking inversions.</S>
    <S sid="85" ssid="17">Like the JPTM&#8217;s distortion model, these parameters grade each movement decision independently.</S>
    <S sid="86" ssid="18">With terminal productions producing cepts, and inversions measuring distortion, our phrasal ITG is essentially a variation on the JPTM with an alternate distortion model.</S>
    <S sid="87" ssid="19">Our phrasal ITG has two main advantages over the JPTM.</S>
    <S sid="88" ssid="20">Most significantly, we gain polynomialtime algorithms for both Viterbi alignment and EM expectation, through the use of ITG parsing and inside-outside algorithms.</S>
    <S sid="89" ssid="21">These phrasal ITG algorithms are no more expensive asymptotically than their word-to-word counterparts, since each potential phrase needs to be analyzed anyway during constituent construction.</S>
    <S sid="90" ssid="22">We hypothesize that using these methods in place of heuristic search and sampling will improve the phrasal translation model learned by EM.</S>
    <S sid="91" ssid="23">Also, we can easily incorporate links to 0 by including the symbol among our terminals.</S>
    <S sid="92" ssid="24">To minimize redundancy, we allow only single tokens, not phrases, to align to 0.</S>
    <S sid="93" ssid="25">The JPTM does not allow links to 0.</S>
    <S sid="94" ssid="26">The phrasal ITG also introduces two new complications.</S>
    <S sid="95" ssid="27">ITG Viterbi and inside-outside algorithms have polynomial complexity, but that polynomial is O(n6), where n is the length of the longer sentence in the pair.</S>
    <S sid="96" ssid="28">This is too slow to train on large data sets without massive parallelization.</S>
    <S sid="97" ssid="29">Also, ITG algorithms explore their alignment space perfectly, but that space has been reduced by the ITG constraint described in Section 2.3.</S>
    <S sid="98" ssid="30">We will address each of these issues in the following two subsections.</S>
    <S sid="99" ssid="31">First, we address the problem of scaling ITG to large data.</S>
    <S sid="100" ssid="32">ITG dynamic programming algorithms work by analyzing each bitext span only once, storing its value in a table for future use.</S>
    <S sid="101" ssid="33">There are O(n4) of these spans, and each analysis takes O(n2) time.</S>
    <S sid="102" ssid="34">An effective approach to speeding up ITG algorithms is to eliminate unlikely spans as a preprocessing step, assigning them 0 probability and saving the time spent processing them.</S>
    <S sid="103" ssid="35">Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006).</S>
    <S sid="104" ssid="36">The former is referred to as tic-tac-toe pruning because it uses both inside and outside estimates.</S>
    <S sid="105" ssid="37">We propose a new ITG pruning method that leverages high-confidence links by pruning all spans that are inconsistent with a provided alignment.</S>
    <S sid="106" ssid="38">This is similar to the constraint used in the C-JPTM, but we do not just eliminate those spans as potential phrase-to-phrase links: we never consider any ITG parse that builds a non-terminal over a pruned span.2 This fixed-link pruning will speed up both Viterbi alignment and EM training by reducing the number of analyzed spans, and so long as we trust our high-confidence links, it will do so harmlessly.</S>
    <S sid="107" ssid="39">We demonstrate the effectiveness of this pruning method experimentally in Section 5.1.</S>
    <S sid="108" ssid="40">Our remaining concern is the ITG constraint.</S>
    <S sid="109" ssid="41">There are some alignments that we just cannot build, and sentence pairs requiring those alignments will occur.</S>
    <S sid="110" ssid="42">These could potentially pollute our training data; if the system is unable to build the right alignment, the counts it will collect from that pair must be wrong.</S>
    <S sid="111" ssid="43">Furthermore, if our high-confidence links are not ITG-compatible, our fixed-link pruning will prevent the aligner from forming any alignments at all.</S>
    <S sid="112" ssid="44">However, these two potential problems cancel each other out.</S>
    <S sid="113" ssid="45">Sentence pairs containing non-ITG translations will tend to have high-confidence links that are also not ITG-compatible.</S>
    <S sid="114" ssid="46">Our EM learner will simply skip these sentence pairs during training, avoiding pollution of our training data.</S>
    <S sid="115" ssid="47">We can use a linear-time algorithm (Zhang et al., 2006) to detect non-ITG movement in our high-confidence links, and remove the offending sentence pairs from our training corpus.</S>
    <S sid="116" ssid="48">This results in only a minor reduction in training data; in our French-English training set, we lose less than 1%.</S>
    <S sid="117" ssid="49">In the experiments described in Section 5, all systems that do not use ITG will take advantage of the complete training set.</S>
  </SECTION>
  <SECTION title="4 Applying the model" number="4">
    <S sid="118" ssid="1">Any phrasal translation model can be used for two tasks: translation modeling and phrasal word alignment.</S>
    <S sid="119" ssid="2">Previous work on JPTM has focused on only the first task.</S>
    <S sid="120" ssid="3">We are interested in phrasal alignment because it may be better suited to heuristic phraseextraction than word-based models.</S>
    <S sid="121" ssid="4">This section describes how to use our phrasal ITG first as a translation model, and then as a phrasal aligner.</S>
    <S sid="122" ssid="5">We can test our model&#8217;s utility for translation by transforming its parameters into a phrase table for the phrasal decoder Pharaoh (Koehn et al., 2003).</S>
    <S sid="123" ssid="6">Any joint model can produce the necessary conditional probabilities by conditionalizing the joint table in both directions.</S>
    <S sid="124" ssid="7">We use our p(&#175;e/ &#175;f|C) distribution from our stochastic grammar to produce p(&#175;e |&#175;f) and p(&#175;f|&#175;e) values for its phrasal lexicon.</S>
    <S sid="125" ssid="8">Pharaoh also includes lexical weighting parameters that are derived from the alignments used to induce its phrase pairs (Koehn et al., 2003).</S>
    <S sid="126" ssid="9">Using the phrasal ITG as a direct translation model, we do not produce alignments for individual sentence pairs.</S>
    <S sid="127" ssid="10">Instead, we provide a lexical preference with an IBM Model 1 feature pM1 that penalizes unmatched words (Vogel et al., 2003).</S>
    <S sid="128" ssid="11">We include both pM1(&#175;e |f) and pM1( f|&#175;e).</S>
    <S sid="129" ssid="12">We can produce a translation model using insideoutside, without ever creating a Viterbi parse.</S>
    <S sid="130" ssid="13">However, we can also examine the maximum likelihood phrasal alignments predicted by the trained model.</S>
    <S sid="131" ssid="14">Despite its strengths derived from using phrases throughout training, the alignments predicted by our phrasal ITG are usually unsatisfying.</S>
    <S sid="132" ssid="15">For example, the fragment pair (order of business, ordre des travaux) is aligned as a phrase pair by our system, linking every English word to every French word.</S>
    <S sid="133" ssid="16">This is frustrating, since there is a clear compositional relationship between the fragment&#8217;s component words.</S>
    <S sid="134" ssid="17">This happens because the system seeks only to maximize the likelihood of its training corpus, and phrases are far more efficient than word-to-word connections.</S>
    <S sid="135" ssid="18">When aligning text, annotators are told to resort to many-to-many links only when no clear compositional relationship exists (Melamed, 1998).</S>
    <S sid="136" ssid="19">If we could tell our phrasal aligner the same thing, we could greatly improve the intuitive appeal of our alignments.</S>
    <S sid="137" ssid="20">Again, we can leverage high-confidence links for help.</S>
    <S sid="138" ssid="21">In the high-confidence alignments provided by GIZA++ intersection, each token participates in at most one link.</S>
    <S sid="139" ssid="22">Links only appear when two wordbased IBM translation models can agree.</S>
    <S sid="140" ssid="23">Therefore, they occur at points of high compositionality: the two words clearly account for one another.</S>
    <S sid="141" ssid="24">We adopt an alignment-driven definition of compositionality: any phrase pair containing two or more highconfidence links is compositional, and can be separated into at least two non-compositional phrases.</S>
    <S sid="142" ssid="25">By removing any phrase pairs that are compositional by this definition from our terminal productions, we can ensure that our aligner never creates such phrases during training or alignment.</S>
    <S sid="143" ssid="26">Doing so produces far more intuitive alignments.</S>
    <S sid="144" ssid="27">Aligned with a model trained using this non-compositional constraint (NCC), our example now forms three wordto-word connections, rather than a single phrasal one.</S>
    <S sid="145" ssid="28">The phrases produced with this constraint are very small, and include only non-compositional context.</S>
    <S sid="146" ssid="29">Therefore, we use the constraint only to train models intended for Viterbi alignment, and not when generating phrase tables directly as in Section 4.1.</S>
  </SECTION>
  <SECTION title="5 Experiments and Results" number="5">
    <S sid="147" ssid="1">In this section, we first verify the effectiveness of fixed-link pruning, and then test our phrasal ITG, both as an aligner and as a translation model.</S>
    <S sid="148" ssid="2">We train all translation models with a French-English Europarl corpus obtained by applying a 25 token sentence-length limit to the training set provided for the HLT-NAACL SMT Workshop Shared Task (Koehn and Monz, 2006).</S>
    <S sid="149" ssid="3">The resulting corpus has 393,132 sentence pairs.</S>
    <S sid="150" ssid="4">3,376 of these are omitted for ITG methods because their highconfidence alignments have ITG-incompatible constructions.</S>
    <S sid="151" ssid="5">Like our predecessors (Marcu and Wong, 2002; Birch et al., 2006), we apply a lexicon constraint: no monolingual phrase can be used by any phrasal model unless it occurs at least five times.</S>
    <S sid="152" ssid="6">High-confidence alignments are provided by intersecting GIZA++ alignments trained in each direction with 5 iterations each of Model 1, HMM, and Model 4.</S>
    <S sid="153" ssid="7">All GIZA++ alignments are trained with no sentence-length limit, using the full 688K corpus.</S>
    <S sid="154" ssid="8">To measure the speed-up provided by fixed-link pruning, we timed our phrasal inside-outside algorithm on the first 100 sentence pairs in our training set, with and without pruning.</S>
    <S sid="155" ssid="9">The results are shown in Table 1.</S>
    <S sid="156" ssid="10">Tic-tac-toe pruning is included for comparison.</S>
    <S sid="157" ssid="11">With fixed-link pruning, on average 95% of the possible spans are pruned, reducing running time by two orders of magnitude.</S>
    <S sid="158" ssid="12">This improvement makes ITG training feasible, even with large bitexts.</S>
    <S sid="159" ssid="13">The goal of this experiment is to compare the Viterbi alignments from the phrasal ITG to gold standard human alignments.</S>
    <S sid="160" ssid="14">We do this to validate our noncompositional constraint and to select good alignments for use with the surface heuristic.</S>
    <S sid="161" ssid="15">Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998).</S>
    <S sid="162" ssid="16">We did not differentiate between sure and possible links.</S>
    <S sid="163" ssid="17">We report precision, recall and balanced F-measure (Och and Ney, 2003).</S>
    <S sid="164" ssid="18">For comparison purposes, we include the results of three types of GIZA++ combination, including the grow-diag-final heuristic (GDF).</S>
    <S sid="165" ssid="19">We tested our phrasal ITG with fixed link pruning, and then added the non-compositional constraint (NCC).</S>
    <S sid="166" ssid="20">During development we determined that performance levels off for both of the ITG models after 3 EM iterations.</S>
    <S sid="167" ssid="21">The results are shown in Table 2.</S>
    <S sid="168" ssid="22">The first thing to note is that GIZA++ Intersection is indeed very high precision.</S>
    <S sid="169" ssid="23">Our confidence in it as a constraint is not misplaced.</S>
    <S sid="170" ssid="24">We also see that both phrasal models have significantly higher recall than any of the GIZA++ alignments, even higher than the permissive GIZA++ union.</S>
    <S sid="171" ssid="25">One factor contributing to this is the phrasal model&#8217;s use of cepts: it completely interconnects any phrase pair, while GIZA++ union and GDF may not.</S>
    <S sid="172" ssid="26">Its global view of phrases also helps in this regard: evidence for a phrase can be built up over multiple sentences.</S>
    <S sid="173" ssid="27">Finally, we note that in terms of alignment quality, the non-compositional constraint is an unqualified success for the phrasal ITG.</S>
    <S sid="174" ssid="28">It produces a 25 point improvement in precision, at the cost of 2 points of recall.</S>
    <S sid="175" ssid="29">This produces the highest balanced Fmeasure observed on our test set, but the utility of its alignments will depend largely on one&#8217;s desired precision-recall trade-off.</S>
    <S sid="176" ssid="30">In this section, we compare a number of different methods for phrase table generation in a French to English translation task.</S>
    <S sid="177" ssid="31">We are interested in answering three questions: With this in mind, we test five phrase tables.</S>
    <S sid="178" ssid="32">Two are conditionalized phrasal models, each EM trained until performance degrades: We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006).</S>
    <S sid="179" ssid="33">Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005).</S>
    <S sid="180" ssid="34">Results on the provided 2000sentence development set are reported using the BLEU metric (Papineni et al., 2002).</S>
    <S sid="181" ssid="35">For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs.</S>
    <S sid="182" ssid="36">The results of all experiments are shown in Table 3.</S>
    <S sid="183" ssid="37">We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points.</S>
    <S sid="184" ssid="38">A large component of this improvement is due to the ITG&#8217;s use of inside-outside for expectation calculation, though there are other differences between the two systems.4 This improvement over search and sampling is demonstrated by the ITG&#8217;s larger table size; by exploring more thoroughly, it is extracting more phrase pairs from the same amount of data.</S>
    <S sid="185" ssid="39">Both systems improve drastically with the addition of IBM Model 1 features for lexical preference.</S>
    <S sid="186" ssid="40">These features also narrow the gap between the two systems.</S>
    <S sid="187" ssid="41">To help calibrate the contribution of these features, we parameterized the ITG&#8217;s phrase table using only Model 1 features, which scores 27.17.</S>
    <S sid="188" ssid="42">Although ITG+M1 comes close, neither phrasal model matches the performance of the surface heuristic.</S>
    <S sid="189" ssid="43">Whatever the surface heuristic lacks in sophistication, it makes up for in sheer coverage, as demonstrated by its huge table sizes.</S>
    <S sid="190" ssid="44">Even the Phrasal ITG Viterbi alignments, which over-commit wildly and have horrible precision, score slightly higher than the best phrasal model.</S>
    <S sid="191" ssid="45">The surface heuristic benefits from capturing as much context as possible, while still covering smaller translation events with its flat counts.</S>
    <S sid="192" ssid="46">It is not held back by any lexicon constraints.</S>
    <S sid="193" ssid="47">When GIZA++ GDF+M1 is forced to conform to a lexicon constraint by dropping any phrase with a frequency lower than 5 from its table, it scores only 29.26, for a reduction of 1.35 BLEU points.</S>
    <S sid="194" ssid="48">Phrases extracted from our non-compositional Viterbi alignments receive the highest BLEU score, but they are not significantly better than GIZA++ GDF.</S>
    <S sid="195" ssid="49">The two methods also produce similarly-sized tables, despite the ITG&#8217;s higher recall.</S>
    <S sid="196" ssid="50">4Unlike our system, the Birch implementation does table smoothing and internal lexical weighting, both of which should help improve their results.</S>
    <S sid="197" ssid="51">The systems also differ in distortion modeling and 0 handling, as described in Section 3.</S>
    <S sid="198" ssid="52">We have presented a phrasal ITG as an alternative to the joint phrasal translation model.</S>
    <S sid="199" ssid="53">This syntactic solution to phrase modeling admits polynomial-time training and alignment algorithms.</S>
    <S sid="200" ssid="54">We demonstrate that the same consistency constraints that allow joint phrasal models to scale also dramatically speed up ITGs, producing an 80-times faster inside-outside algorithm.</S>
    <S sid="201" ssid="55">We show that when used to learn phrase tables for the Pharaoh decoder, the phrasal ITG is superior to the constrained joint phrasal model, producing tables that result in a 2.5 point improvement in BLEU when used alone, and a 1 point improvement when used with IBM Model 1 features.</S>
    <S sid="202" ssid="56">This suggests that ITG&#8217;s perfect expectation counting does matter; other phrasal models could benefit from either adopting the ITG formalism, or improving their sampling heuristics.</S>
    <S sid="203" ssid="57">We have explored, for the first time, the utility of a joint phrasal model as a word alignment method.</S>
    <S sid="204" ssid="58">We present a non-compositional constraint that turns the phrasal ITG into a high-recall phrasal aligner with an F-measure that is comparable to GIZA++.</S>
    <S sid="205" ssid="59">With search and sampling no longer a concern, the remaining weaknesses of the system seem to lie with the model itself.</S>
    <S sid="206" ssid="60">Phrases are just too efficient probabilistically: were we to remove all lexicon constraints, EM would always align entire sentences to entire sentences.</S>
    <S sid="207" ssid="61">This pressure to always build the longest phrase possible may be overwhelming otherwise strong correlations in our training data.</S>
    <S sid="208" ssid="62">A promising next step would be to develop a prior over lexicon size or phrase size, allowing EM to introduce large phrases at a penalty, and removing the need for artificial constraints on the lexicon.</S>
    <S sid="209" ssid="63">Acknowledgments Special thanks to Alexandra Birch for the use of her code, and to our reviewers for their comments.</S>
    <S sid="210" ssid="64">The first author is funded by Alberta Ingenuity and iCORE studentships.</S>
  </SECTION>
</PAPER>
