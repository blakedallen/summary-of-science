<PAPER>
  <S sid="0">Indexing With WordNet Synsets Can Improve Text Retrieval</S>
  <ABSTRACT>
    <S sid="1" ssid="1">tem: Experiments in Automatic Document Pro- M. Sanderson.</S>
    <S sid="2" ssid="2">1994.</S>
    <S sid="3" ssid="3">Word sense disambiguation information retrieval.</S>
    <S sid="4" ssid="4">In of 17th International Conference on Research and Development in Information Retrieval.</S>
    <S sid="5" ssid="5">A.F.</S>
    <S sid="6" ssid="6">Smeaton and A. Quigley.</S>
    <S sid="7" ssid="7">1996.</S>
    <S sid="8" ssid="8">Experiments on using semantic distances between words in imcaption retrieval.</S>
    <S sid="9" ssid="9">Proceedings of the International Conference on Research and Development in IR.</S>
    <S sid="10" ssid="10">A. Smeaton, F. Kelledy, and R. O'Donnell.</S>
    <S sid="11" ssid="11">1995.</S>
    <S sid="12" ssid="12">TREC-4 experiments at dublin city university: Thresolding posting lists, query expansion with and POS tagging of spanish.</S>
    <S sid="13" ssid="13">In Proceedings of TREC-4.</S>
    <S sid="14" ssid="14">M. Voorhees.</S>
    <S sid="15" ssid="15">1994.</S>
    <S sid="16" ssid="16">Query relations.</S>
    <S sid="17" ssid="17">In of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="18" ssid="1">Text retrieval deals with the problem of finding all the relevant documents in a text collection for a given user's query.</S>
    <S sid="19" ssid="2">A large-scale semantic database such as WordNet (Miller, 1990) seems to have a great potential for this task.</S>
    <S sid="20" ssid="3">There are, at least, two obvious reasons: However, the general feeling within the information retrieval community is that dealing explicitly with semantic information does not improve significantly the performance of text retrieval systems.</S>
    <S sid="21" ssid="4">This impression is founded on the results of some experiments measuring the role of Word Sense Disambiguation (WSD) for text retrieval, on one hand, and some attempts to exploit the features of WordNet and other lexical databases, on the other hand.</S>
    <S sid="22" ssid="5">In (Sanderson, 1994), word sense ambiguity is shown to produce only minor effects on retrieval accuracy, apparently confirming that query/document matching strategies already perform an implicit disambiguation.</S>
    <S sid="23" ssid="6">Sanderson also estimates that if explicit WSD is performed with less than 90% accuracy, the results are worse than non disambiguating at all.</S>
    <S sid="24" ssid="7">In his experimental setup, ambiguity is introduced artificially in the documents, substituting randomly chosen pairs of words (for instance, banana and kalashmkov) with artificially ambiguous terms (banana/kalashnikov).</S>
    <S sid="25" ssid="8">While his results are very interesting, it remains unclear, in our opinion, whether they would be corroborated with real occurrences of ambiguous words.</S>
    <S sid="26" ssid="9">There is also other minor weakness in Sanderson's experiments.</S>
    <S sid="27" ssid="10">When he &amp;quot;disambiguates&amp;quot; a term such as spring/bank to get, for instance, bank, he has done only a partial disambiguation, as bank can be used in more than one sense in the text collection.</S>
    <S sid="28" ssid="11">Besides disambiguation. many attempts have been done to exploit WordNet for text retrieval purposes.</S>
    <S sid="29" ssid="12">Mainly two aspects have been addressed: the enrichment of queries with semantically-related terms, on one hand, and the comparison of queries and documents via conceptual distance measures, on the other.</S>
    <S sid="30" ssid="13">Query expansion with WordNet has shown to be potentially relevant to enhance recall, as it permits matching relevant documents that could not contain any of the query terms (Smeaton et al., 1995).</S>
    <S sid="31" ssid="14">However, it has produced few successful experiments.</S>
    <S sid="32" ssid="15">For instance, (Voorhees, 1994) manually expanded 50 queries over a TREC-1 collection (Harman, 1993) using synonymy and other semantic relations from WordNet 1.3.</S>
    <S sid="33" ssid="16">Voorhees found that the expansion was useful with short, incomplete queries, and rather useless for complete topic statements -where other expansion techniques worked better-.</S>
    <S sid="34" ssid="17">For short queries, it remained the problem of selecting the expansions automatically: doing it badly could degrade retrieval performance rather than enhancing it.</S>
    <S sid="35" ssid="18">In (Richardson and Smeaton, 1995), a combination of rather sophisticated techniques based on WordNet, including automatic disambiguation and measures of semantic relatedness between query/document concepts resulted in a drop of effectiveness.</S>
    <S sid="36" ssid="19">Unfortunately, the effects of WSD errors could not be discerned from the accuracy of the retrieval strategy.</S>
    <S sid="37" ssid="20">However, in (Smeaton and Quigley, 1996), retrieval on a small collection of image captions - that is, on very short documents - is reasonably improved using measures of conceptual distance between words based on WordNet 1.4.</S>
    <S sid="38" ssid="21">Previously, captions and queries had been manually disambiguated against WordNet.</S>
    <S sid="39" ssid="22">The reason for such success is that with very short documents (e.g. boys playing in the sand) the chance of finding the original terms of the query (e.g. of children running on a beach) are much lower than for average-size documents (that typically include many phrasings for the same concepts).</S>
    <S sid="40" ssid="23">These results are in agreement with (Voorhees, 1994), but it remains the question of whether the conceptual distance matching would scale up to longer documents and queries.</S>
    <S sid="41" ssid="24">In addition, the experiments in _ (Smeaton and Quigley, 1996) only consider nouns, while WordNet offers the chance to use all open-class words (nouns, verbs, adjectives and adverbs).</S>
    <S sid="42" ssid="25">Our essential retrieval strategy in the experiments reported here is to adapt a classical vector model based system, using WordNet synsets as indexing space instead of word forms.</S>
    <S sid="43" ssid="26">This approach combines two benefits for retrieval: one, that terms are fully disambiguated (this should improve precision); and two, that equivalent terms can be identified (this should improve recall).</S>
    <S sid="44" ssid="27">Note that query expansion does not satisfy the first condition, as the terms used to expand are words and, therefore, are in turn ambiguous.</S>
    <S sid="45" ssid="28">On the other hand, plain word sense disambiguation does not satisfy the second condition. as equivalent senses of two different words are not matched.</S>
    <S sid="46" ssid="29">Thus, indexing by synsets gets maximum matching and minimum spurious matching, seeming a good starting point to study text retrieval with WordNet.</S>
    <S sid="47" ssid="30">Given this approach, our goal is to test two main issues which are not clearly answered -to our knowledge- by the experiments mentioned above: WSD.</S>
    <S sid="48" ssid="31">This paper reports on our first results answering these questions.</S>
    <S sid="49" ssid="32">The next section describes the test collection that we have produced.</S>
    <S sid="50" ssid="33">The experiments are described in Section 3, and the last Section discusses the results obtained.</S>
  </SECTION>
  <SECTION title="2 The test collection" number="2">
    <S sid="51" ssid="1">The best-known publicly available corpus handtagged with WordNet senses is SEMCOR (Miller et al., 1993), a subset of the Brown Corpus of about 100 documents that occupies about 11 Mb.</S>
    <S sid="52" ssid="2">(including tags) The collection is rather heterogeneous, covering politics, sports, music, cinema, philosophy, excerpts from fiction novels, scientific texts... A new, bigger version has been made available recently (Landes et al., 1998), but we have not still adapted it for our collection.</S>
    <S sid="53" ssid="3">We have adapted SEMCOR in order to build a test collection -that we call IR-SEMCOR- in four manual steps: ments, with lengths varying between 4 and 50 words and an average of 22 words per summary.</S>
    <S sid="54" ssid="4">Each summary is a human explanation of the text contents, not a mere bag of related keywords.</S>
    <S sid="55" ssid="5">These summaries serve as queries on the text collection, and then there is exactly one relevant document per query.</S>
    <S sid="56" ssid="6">We also generated a list of &amp;quot;stop-senses&amp;quot; and a list of &amp;quot;stop-synsets&amp;quot;, automatically translating a standard list of stop words for English.</S>
    <S sid="57" ssid="7">Such a test collection offers the chance to measure the adequacy of WordNet-based approaches to IR independently from the disambiguator being used, but also offers the chance to measure the role of automatic disambiguation by introducing different rates of &amp;quot;disambiguation errors&amp;quot; in the collection.</S>
    <S sid="58" ssid="8">The only disadvantage is the small size of the collection, which does not allow fine-grained distinctions in the results.</S>
    <S sid="59" ssid="9">However, it has proved large enough to give meaningful statistics for the experiments reported here.</S>
    <S sid="60" ssid="10">Although designed for our concrete text retrieval testing purposes, the resulting database could also be useful for many other tasks.</S>
    <S sid="61" ssid="11">For instance, it could be used to evaluate automatic summarization systems (measuring the semantic relation between the manually written and hand-tagged summaries of IRSEMCOR and the output of text summarization systems) and other related tasks.</S>
  </SECTION>
  <SECTION title="3 The experiments" number="3">
    <S sid="62" ssid="1">We have performed a number of experiments using a standard vector-model based text retrieval system, SmAFrr (Salton, 1971), and three different indexing spaces: the original terms in the documents (for standard SMART runs), the word-senses corresponding to the document terms (in other words, a manually disambiguated version of the documents) and the WordNet synsets corresponding to the document terms (roughly equivalent to concepts occurring in the documents).</S>
    <S sid="63" ssid="2">These are all the experiments considered here: the file.</S>
    <S sid="64" ssid="3">In this case, it is a noun belonging to the noun.communication file.</S>
    <S sid="65" ssid="4">With this collection we can see if plain disambiguation is helpful for retrieval, because word senses are distinguished but synonymous word senses are not identified.</S>
    <S sid="66" ssid="5">&amp;quot;{argument, debatel}&amp;quot; (a discussion in which reasons are advanced for and against some proposition or proposal; &amp;quot;the argument over foreign aid goes on and on&amp;quot;) This collection represents conceptual indexing, as equivalent word senses are represented with a unique identifier.</S>
    <S sid="67" ssid="6">4.</S>
    <S sid="68" ssid="7">We produced different versions of the synset indexed collection. introducing fixed percentages of erroneous synsets.</S>
    <S sid="69" ssid="8">Thus we simulated a word-sense disambiguation process with 5%, 10%, 20%, 30% and 60% error rates.</S>
    <S sid="70" ssid="9">The errors were introduced randomly in the ambiguous words of each document.</S>
    <S sid="71" ssid="10">With this set of experiments we can measure the sensitivity of the retrieval process to disambiguation errors.</S>
    <S sid="72" ssid="11">In all cases, we compared at c and nnn standard weighting schemes, and they produced very similar results.</S>
    <S sid="73" ssid="12">Thus we only report here on the results for nnn weighting scheme.</S>
  </SECTION>
  <SECTION title="4 Discussion of results" number="4">
    <S sid="74" ssid="1">In Figure 1 we compare different indexing approaches: indexing by synsets, indexing by words (basic SMART) and indexing by word senses (experiments 1, 2 and 3).</S>
    <S sid="75" ssid="2">The leftmost point in each curve represents the percentage of documents that were successfully ranked as the most relevant for its summary/query.</S>
    <S sid="76" ssid="3">The next point represents the documents retrieved as the first or the second most relevant to its summary/query, and so on.</S>
    <S sid="77" ssid="4">Note that, as there is only one relevant document per query, the leftmost point is the most representative of each curve.</S>
    <S sid="78" ssid="5">Therefore, we have included this results separately in Table 1.</S>
    <S sid="79" ssid="6">The results are encouraging: documents, a 29% improvement with respect to SMART.</S>
    <S sid="80" ssid="7">This is an excellent result, although we should keep in mind that is obtained with manually disambiguated queries and documents.</S>
    <S sid="81" ssid="8">Nevertheless, it shows that WordNet can greatly enhance text retrieval: the problem resides in achieving accurate automatic Word Sense Disambiguation.</S>
    <S sid="82" ssid="9">&#8226; Indexing by word senses improves performance when considering up to four documents retrieved for each query/summary, although it is worse than indexing by synsets.</S>
    <S sid="83" ssid="10">This confirms our intuition that synset indexing has advantages over plain word sense disambiguation, because it permits matching semantically similar terms.</S>
    <S sid="84" ssid="11">Taking only the first document retrieved for each summary, the disambiguated collection gives a 53.2% success against a 48% of the plain SN1ART query, which represents a 11% improvement.</S>
    <S sid="85" ssid="12">For recall levels higher than 0.85, however, the disambiguated collection performs slightly worse.</S>
    <S sid="86" ssid="13">This may seem surprising, as word sense disambiguation should only increase our knowledge about queries and documents.</S>
    <S sid="87" ssid="14">But we should bear in mind that WordNet 1.5 is not the perfect database for text retrieval, and indexing by word senses prevents some matchings that can be useful for retrieval.</S>
    <S sid="88" ssid="15">For instance, design is used as a noun repeatedly in one of the documents, while its summary uses design as a verb.</S>
    <S sid="89" ssid="16">WordNet 1.5 does not include cross-part-of-speech semantic relations, so this relation cannot be used with word senses, while term indexing simply (and successfully!) does not distinguish them.</S>
    <S sid="90" ssid="17">Other problems of WordNet for text retrieval include too much finegrained sense-distinctions and lack of domain information; see (Gonzalo et al., In press) for a more detailed discussion on the adequacy of WordNet structure for text retrieval.</S>
    <S sid="91" ssid="18">Figure 2 shows the sensitivity of the synset indexing system to degradation of disambiguation accuracy (corresponding to the experiments 4 and 5 described above).</S>
    <S sid="92" ssid="19">From the plot, it can be seen that: differs from (Sanderson, 1994) result (namely, that it is better not to disambiguate below a 90% accuracy).</S>
    <S sid="93" ssid="20">The main difference is that we are using concepts rather than word senses.</S>
    <S sid="94" ssid="21">But, in addition, it must be noted that Sanderson's setup used artificially created ambiguous pseudo words (such as 'bank/spring) which are not guaranteed to behave as real ambiguous words.</S>
    <S sid="95" ssid="22">Moreover, what he understands as disambiguating is selecting -in the example- bank or spring which remain to be ambiguous words themselves.</S>
    <S sid="96" ssid="23">It is too soon to say if state-of-the-art WSD techniques can perform with less than 30% errors, because each technique is evaluated in fairly different settings.</S>
    <S sid="97" ssid="24">Some of the best results on a comparable setting (namely, disambiguating against WordNet, evaluating on a subset of the Brown Corpus, and treating the 191 most frequently occurring and ambiguous words of English) are reported reported in (Ng, 1997).</S>
    <S sid="98" ssid="25">They reach a 58.7% accuracy on a Brown Corpus subset and a 75.2% on a subset of the Wall Street Journal Corpus.</S>
    <S sid="99" ssid="26">A more careful evaluation of the role of WSD is needed to know if this is good enough for our purposes.</S>
    <S sid="100" ssid="27">Anyway, we have only emulated a WSD algorithm that just picks up one sense and discards the rest.</S>
    <S sid="101" ssid="28">A more reasonable approach here could be giving different probabilities for each sense of a word, and use them to weight synsets in the vectorial representation of documents and queries.</S>
    <S sid="102" ssid="29">In Figure 3 we have plot the results of runs with a non-disambiguated version of the queries, both for word sense indexing and synset indexing, against the manually disambiguated collection (experiment 6).</S>
    <S sid="103" ssid="30">The synset run performs approximately as the basic SMART run.</S>
    <S sid="104" ssid="31">It seems therefore useless to apply conceptual indexing if no disambiguation of the query is feasible.</S>
    <S sid="105" ssid="32">This is not a major problem in an interactive system that may help the user to disambiguate his query, but it must be taken into account if the process is not interactive and the query is too short to do reliable disambiguation.</S>
  </SECTION>
  <SECTION title="5 Conclusions" number="5">
    <S sid="106" ssid="1">We have experimented with a retrieval approach based on indexing in terms of WordNet synsets instead of word forms, trying to address two questions: 1) what potential does WordNet offer for text retrieval, abstracting from the problem of sense disambiguation, and 2) what is the sensitivity of retrieval performance to disambiguation errors.</S>
    <S sid="107" ssid="2">The answer to the first question is that indexing by synsets can be very helpful for text retrieval; our experiments give up to a 29% improvement over a standard SMART run indexing with words.</S>
    <S sid="108" ssid="3">We believe that these results have to be further contrasted, but they strongly suggest that WordNet can be more useful to Text Retrieval than it was previously thought.</S>
    <S sid="109" ssid="4">The second question needs further, more finegrained, experiences to be clearly answered.</S>
    <S sid="110" ssid="5">However, for our test collection, we find that error rates below 30% still produce better results than standard word indexing, and that from 30% to 60% error rates, it does not behave worse than the standard SMART run.</S>
    <S sid="111" ssid="6">We also find that the queries have to be disambiguated to take advantage of the approach; otherwise, the best possible results with synset indexing does not improve the performance of standard word indexing.</S>
    <S sid="112" ssid="7">Our first goal now is to improve our retrieval system in many ways. studying how to enrich the query with semantically related synsets, how to cornpare documents and queries using semantic information beyond the cosine measure, and how to obtain weights for synsets according to their position in the WordNet hierarchy, among other issues.</S>
    <S sid="113" ssid="8">A second goal is to apply synset indexing in a Cross-Language environment, using the Euro WordNet multilingual database (Gonzalo et al., In press).</S>
    <S sid="114" ssid="9">Indexing by synsets offers a neat way of performing language-independent retrieval, by mapping synsets into the EuroWordNet InterLingual Index that links monolingual wordnets for all the languages covered by EuroWordNet.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="6">
    <S sid="115" ssid="1">This research is being supported by the European Community, project LE #4003 and also partially by the Spanish government, project TIC-96-1243-0O3-01.</S>
    <S sid="116" ssid="2">We are indebted to Renee Pohlmann for giving us good pointers at an early stage of this work, and to AnseImo Peilas and David Fernandez for their help finishing up the test collection.</S>
  </SECTION>
</PAPER>
