Online Large-Margin Training Of Dependency Parsers
We present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996).
The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements.
We have achieved parsers with O(n3) time complexity without the grammar constant.
We use the prefix of each word form instead of word form itself as features.
Our dependency parser achieves accuracy as good as Charniak (2000) with speed ten times faster than Collins (1997) and four times faster than Charniak (2000).
