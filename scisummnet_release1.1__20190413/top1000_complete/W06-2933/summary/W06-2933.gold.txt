Labeled Pseudo-Projective Dependency Parsing With Support Vector Machines
We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion.
Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.
We present evaluation results and an error analysis focusing on Swedish and Turkish.
Our pseudo-projective approach transforms non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the DEPREL, so that this inverse transformation can also be carried out on the test trees (Nivre et al, 2006).
