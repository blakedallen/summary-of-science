<PAPER>
  <S sid="0">Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions.</S>
    <S sid="2" ssid="2">Our method learns vector space representations for multi-word phrases.</S>
    <S sid="3" ssid="3">In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules.</S>
    <S sid="4" ssid="4">We also evaluate the model&#8217;s ability to predict sentiment distributions on a new dataset based on confessions from the experience project.</S>
    <S sid="5" ssid="5">The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions.</S>
    <S sid="6" ssid="6">Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">The ability to identify sentiments about personal experiences, products, movies etc. is crucial to understand user generated content in social networks, blogs or product reviews.</S>
    <S sid="8" ssid="2">Detecting sentiment in these data is a challenging task which has recently spawned a lot of interest (Pang and Lee, 2008).</S>
    <S sid="9" ssid="3">Current baseline methods often use bag-of-words representations which cannot properly capture more complex linguistic phenomena in sentiment analysis (Pang et al., 2002).</S>
    <S sid="10" ssid="4">For instance, while the two phrases &#8220;white blood cells destroying an infection&#8221; and &#8220;an infection destroying white blood cells&#8221; have the same bag-of-words representation, the former is a positive reaction while the later is very negative.</S>
    <S sid="11" ssid="5">More advanced methods such as (Nakagawa et al., tecture which learns semantic vector representations of phrases.</S>
    <S sid="12" ssid="6">Word indices (orange) are first mapped into a semantic vector space (blue).</S>
    <S sid="13" ssid="7">Then they are recursively merged by the same autoencoder network into a fixed length sentence representation.</S>
    <S sid="14" ssid="8">The vectors at each node are used as features to predict a distribution over sentiment labels.</S>
    <S sid="15" ssid="9">2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules).</S>
    <S sid="16" ssid="10">This limits the applicability of these methods to a broader range of tasks and languages.</S>
    <S sid="17" ssid="11">Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings.</S>
    <S sid="18" ssid="12">Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al., 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007).</S>
    <S sid="19" ssid="13">Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments.</S>
    <S sid="20" ssid="14">In this work, we seek to address three issues.</S>
    <S sid="21" ssid="15">(i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment.</S>
    <S sid="22" ssid="16">(ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, Sorry, Hugs You Rock Teehee I Understand Wow, Just Wow i walked into a parked car parsers, etc.</S>
    <S sid="23" ssid="17">(iii) Rather than limiting sentiment to a positive/negative scale, we predict a multidimensional distribution over several complex, interconnected sentiments.</S>
    <S sid="24" ssid="18">We introduce an approach based on semisupervised, recursive autoencoders (RAE) which use as input continuous word vectors.</S>
    <S sid="25" ssid="19">Fig.</S>
    <S sid="26" ssid="20">1 shows an illustration of the model which learns vector representations of phrases and full sentences as well as their hierarchical structure from unsupervised text.</S>
    <S sid="27" ssid="21">We extend our model to also learn a distribution over sentiment labels at each node of the hierarchy.</S>
    <S sid="28" ssid="22">We evaluate our approach on several standard datasets where we achieve state-of-the art performance.</S>
    <S sid="29" ssid="23">Furthermore, we show results on the recently introduced experience project (EP) dataset (Potts, 2010) that captures a broader spectrum of human sentiments and emotions.</S>
    <S sid="30" ssid="24">The dataset consists of very personal confessions anonymously made by people on the experience project website www.experienceproject.com.</S>
    <S sid="31" ssid="25">Confessions are labeled with a set of five reactions by other users.</S>
    <S sid="32" ssid="26">Reaction labels are you rock (expressing approvement), tehee (amusement), I understand, Sorry, hugs and Wow, just wow (displaying shock).</S>
    <S sid="33" ssid="27">For evaluation on this dataset we predict both the label with the most votes as well as the full distribution over the sentiment categories.</S>
    <S sid="34" ssid="28">On both tasks our model outperforms competitive baselines.</S>
    <S sid="35" ssid="29">A set of over 31,000 confessions as well as the code of our model are available at www.socher.org.</S>
    <S sid="36" ssid="30">After describing the model in detail, we evaluate it qualitatively by analyzing the learned n-gram vector representations and compare quantitatively against other methods on standard datasets and the EP dataset.</S>
  </SECTION>
  <SECTION title="2 Semi-Supervised Recursive Autoencoders" number="2">
    <S sid="37" ssid="1">Our model aims to find vector representations for variable-sized phrases in either unsupervised or semi-supervised training regimes.</S>
    <S sid="38" ssid="2">These representations can then be used for subsequent tasks.</S>
    <S sid="39" ssid="3">We first describe neural word representations and then proceed to review a related recursive model based on autoencoders, introduce our recursive autoencoder (RAE) and describe how it can be modified to jointly learn phrase representations, phrase structure and sentiment distributions.</S>
    <S sid="40" ssid="4">We represent words as continuous vectors of parameters.</S>
    <S sid="41" ssid="5">We explore two settings.</S>
    <S sid="42" ssid="6">In the first setting we simply initialize each word vector x E Rn by sampling it from a zero mean Gaussian distribution: x &#8212; N(0, U2).</S>
    <S sid="43" ssid="7">These word vectors are then stacked into a word embedding matrix L E Rn&#215;|V |, where |V  |is the size of the vocabulary.</S>
    <S sid="44" ssid="8">This initialization works well in supervised settings where a network can subsequently modify these vectors to capture certain label distributions.</S>
    <S sid="45" ssid="9">In the second setting, we pre-train the word vectors with an unsupervised neural language model (Bengio et al., 2003; Collobert and Weston, 2008).</S>
    <S sid="46" ssid="10">These models jointly learn an embedding of words into a vector space and use these vectors to predict how likely a word occurs given its context.</S>
    <S sid="47" ssid="11">After learning via gradient ascent the word vectors capture syntactic and semantic information from their co-occurrence statistics.</S>
    <S sid="48" ssid="12">In both cases we can use the resulting matrix of word vectors L for subsequent tasks as follows.</S>
    <S sid="49" ssid="13">Assume we are given a sentence as an ordered list of m words.</S>
    <S sid="50" ssid="14">Each word has an associated vocabulary index k into the embedding matrix which we use to retrieve the word&#8217;s vector representation.</S>
    <S sid="51" ssid="15">Mathematically, this look-up operation can be seen as a simple projection layer where we use a binary vector b which is zero in all positions except at the kth index, In the remainder of this paper, we represent a sentence (or any n-gram) as an ordered list of these vectors (x1, ... , xm).</S>
    <S sid="52" ssid="16">This word representation is better suited to autoencoders than the binary number representations used in previous related autoencoder models such as the recursive autoassociative memory (RAAM) model (Pollack, 1990; Voegtlin and Dominey, 2005) or recurrent neural networks (Elman, 1991) since sigmoid units are inherently continuous.</S>
    <S sid="53" ssid="17">Pollack circumvented this problem by having vocabularies with only a handful of words and by manually defining a threshold to binarize the resulting vectors.</S>
    <S sid="54" ssid="18">The goal of autoencoders is to learn a representation of their inputs.</S>
    <S sid="55" ssid="19">In this section we describe how to obtain a reduced dimensional vector representation for sentences.</S>
    <S sid="56" ssid="20">In the past autoencoders have only been used in setting where the tree structure was given a-priori.</S>
    <S sid="57" ssid="21">We review this setting before continuing with our model which does not require a given tree structure.</S>
    <S sid="58" ssid="22">Fig.</S>
    <S sid="59" ssid="23">2 shows an instance of a recursive autoencoder (RAE) applied to a given tree.</S>
    <S sid="60" ssid="24">Assume we are given a list of word vectors x = (x1,... , xm) as described in the previous section as well as a binary tree structure for this input in the form of branching triplets of parents with children: (p -+ c1c2).</S>
    <S sid="61" ssid="25">Each child can be either an input word vector xi or a nonterminal node in the tree.</S>
    <S sid="62" ssid="26">For the example in Fig.</S>
    <S sid="63" ssid="27">2, we have the following triplets: ((y1 -+ x3x4), (y2 -+ x2y1), (y1 -+ x1y2)).</S>
    <S sid="64" ssid="28">In order to be able to apply the same neural network to each pair of children, the hidden representations yi have to have the same dimensionality as the xi&#8217;s.</S>
    <S sid="65" ssid="29">Given this tree structure, we can now compute the parent representations.</S>
    <S sid="66" ssid="30">The first parent vector y1 is computed from the children (c1, c2) = (x3, x4): where we multiplied a matrix of parameters W (1) E Rnx2n by the concatenation of the two children.</S>
    <S sid="67" ssid="31">After adding a bias term we applied an elementwise activation function such as tanh to the resulting vector.</S>
    <S sid="68" ssid="32">One way of assessing how well this ndimensional vector represents its children is to try to reconstruct the children in a reconstruction layer: During training, the goal is to minimize the reconstruction errors of this input pair.</S>
    <S sid="69" ssid="33">For each pair, we compute the Euclidean distance between the original input and its reconstruction: This model of a standard autoencoder is boxed in Fig.</S>
    <S sid="70" ssid="34">2.</S>
    <S sid="71" ssid="35">Now that we have defined how an autoencoder can be used to compute an n-dimensional vector representation (p) of two n-dimensional children (c1, c2), we can describe how such a network can be used for the rest of the tree.</S>
    <S sid="72" ssid="36">Essentially, the same steps repeat.</S>
    <S sid="73" ssid="37">Now that y1 is given, we can use Eq.</S>
    <S sid="74" ssid="38">2 to compute y2 by setting the children to be (c1, c2) = (x2, y1).</S>
    <S sid="75" ssid="39">Again, after computing the intermediate parent vector y2, we can assess how well this vector capture the content of the children by computing the reconstruction error as in Eq.</S>
    <S sid="76" ssid="40">4.</S>
    <S sid="77" ssid="41">The process repeat until the full tree is constructed and we have a reconstruction error at each nonterminal node.</S>
    <S sid="78" ssid="42">This model is similar to the RAAM model (Pollack, 1990) which also requires a fixed tree structure.</S>
    <S sid="79" ssid="43">Now, assume there is no tree structure given for the input vectors in x.</S>
    <S sid="80" ssid="44">The goal of our structureprediction RAE is to minimize the reconstruction error of all vector pairs of children in a tree.</S>
    <S sid="81" ssid="45">We define A(x) as the set of all possible trees that can be built from an input sentence x.</S>
    <S sid="82" ssid="46">Further, let T (y) be a function that returns the triplets of a tree indexed by s of all the non-terminal nodes in a tree.</S>
    <S sid="83" ssid="47">Using the reconstruction error of Eq.</S>
    <S sid="84" ssid="48">4, we compute We now describe a greedy approximation that constructs such a tree.</S>
    <S sid="85" ssid="49">Greedy Unsupervised RAE.</S>
    <S sid="86" ssid="50">For a sentence with m words, we apply the autoencoder recursively.</S>
    <S sid="87" ssid="51">It takes the first pair of neighboring vectors, defines them as potential children of a phrase (c1; c2) = (x1; x2), concatenates them and gives them as input to the autoencoder.</S>
    <S sid="88" ssid="52">For each word pair, we save the potential parent node p and the resulting reconstruction error.</S>
    <S sid="89" ssid="53">After computing the score for the first pair, the network is shifted by one position and takes as input vectors (c1, c2) = (x2, x3) and again computes a potential parent node and a score.</S>
    <S sid="90" ssid="54">This process repeats until it hits the last pair of words in the sentence: (c1, c2) = (xm&#8722;1, xm).</S>
    <S sid="91" ssid="55">Next, it selects the pair which had the lowest reconstruction error (ETec) and its parent representation p will represent this phrase and replace both children in the sentence word list.</S>
    <S sid="92" ssid="56">For instance, consider the sequence (x1, x2, x3, x4) and assume the lowest ETec was obtained by the pair (x3, x4).</S>
    <S sid="93" ssid="57">After the first pass, the new sequence then consists of (x1, x2, p(3,4)).</S>
    <S sid="94" ssid="58">The process repeats and treats the new vector p(3,4) like any other input vector.</S>
    <S sid="95" ssid="59">For instance, subsequent states could be either: (x1,p(2,(3,4))) or (p(1,2),p(3,4)).</S>
    <S sid="96" ssid="60">Both states would then finish with a deterministic choice of collapsing the remaining two states into one parent to obtain (p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively.</S>
    <S sid="97" ssid="61">The tree is then recovered by unfolding the collapsing decisions.</S>
    <S sid="98" ssid="62">The resulting tree structure captures as much of the single-word information as possible (in order to allow reconstructing the word vectors) but does not necessarily follow standard syntactic constraints.</S>
    <S sid="99" ssid="63">We also experimented with a method that finds better solutions to Eq.</S>
    <S sid="100" ssid="64">5 based on CKY-like beam search algorithms (Socher et al., 2010; Socher et al., 2011) but the performance is similar and the greedy version is much faster.</S>
    <S sid="101" ssid="65">Weighted Reconstruction.</S>
    <S sid="102" ssid="66">One problem with simply using the reconstruction error of both children equally as describe in Eq.</S>
    <S sid="103" ssid="67">4 is that each child could represent a different number of previously collapsed words and is hence of bigger importance for the overall meaning reconstruction of the sentence.</S>
    <S sid="104" ssid="68">For instance in the case of (x1,p(2,(3,4))) one would like to give more importance to reconstructing p than x1.</S>
    <S sid="105" ssid="69">We capture this desideratum by adjusting the reconstruction error.</S>
    <S sid="106" ssid="70">Let n1, n2 be the number of words underneath a current potential child, we re-define the reconstruction error to be Length Normalization.</S>
    <S sid="107" ssid="71">One of the goals of RAEs is to induce semantic vector representations that allow us to compare n-grams of different lengths.</S>
    <S sid="108" ssid="72">The RAE tries to lower reconstruction error of not only the bigrams but also of nodes higher in the tree.</S>
    <S sid="109" ssid="73">Unfortunately, since the RAE computes the hidden representations it then tries to reconstruct, it can just lower reconstruction error by making the hidden layer very small in magnitude.</S>
    <S sid="110" ssid="74">To prevent such undesirable behavior, we modify the hidden layer such that the resulting parent representation always has length one, after computing p as in Eq.</S>
    <S sid="111" ssid="75">2, we simply set: p = p ||p||.</S>
    <S sid="112" ssid="76">So far, the RAE was completely unsupervised and induced general representations that capture the semantics of multi-word phrases.In this section, we extend RAEs to a semi-supervised setting in order to predict a sentence- or phrase-level target distribution t.1 One of the main advantages of the RAE is that each node of the tree built by the RAE has associated with it a distributed vector representation (the parent vector p) which could also be seen as features describing that phrase.</S>
    <S sid="113" ssid="77">We can leverage this representation by adding on top of each parent node a simple softmax layer to predict class distributions: Assuming there are K labels, d E RK is a K-dimensional multinomial distribution and P k=1 dk = 1.</S>
    <S sid="114" ssid="78">Fig.</S>
    <S sid="115" ssid="79">3 shows such a semi-supervised RAE unit.</S>
    <S sid="116" ssid="80">Let tk be the kth element of the multinomial target label distribution t for one entry.</S>
    <S sid="117" ssid="81">The softmax layer&#8217;s outputs are interpreted as conditional probabilities dk = p(kJ[c1; c2]), hence the cross-entropy error is 1For the binary label classification case, the distribution is of the form [1, 0] for class 1 and [0, 1] for class 2.</S>
    <S sid="118" ssid="82">Using this cross-entropy error for the label and the reconstruction error from Eq.</S>
    <S sid="119" ssid="83">6, the final semisupervised RAE objective over (sentences,label) pairs (x, t) in a corpus becomes where we have an error for each entry in the training set that is the sum over the error at the nodes of the tree that is constructed by the greedy RAE: Let &#952; = (W (1), b(1), W(2), b(1), Wlabel, L) be the set of our model parameters, then the gradient becomes: To compute this gradient, we first greedily construct all trees and then derivatives for these trees are computed efficiently via backpropagation through structure (Goller and K&#168;uchler, 1996).</S>
    <S sid="120" ssid="84">Because the algorithm is greedy and the derivatives of the supervised cross-entropy error also modify the matrix W(1), this objective is not necessarily continuous and a step in the gradient descent direction may not necessarily decrease the objective.</S>
    <S sid="121" ssid="85">However, we found that L-BFGS run over the complete training data (batch mode) to minimize the objective works well in practice, and that convergence is smooth, with the algorithm typically finding a good solution quickly.</S>
    <S sid="122" ssid="86">The error at each nonterminal node is the weighted sum of reconstruction and cross-entropy errors, The hyperparameter &#945; weighs reconstruction and cross-entropy error.</S>
    <S sid="123" ssid="87">When minimizing the crossentropy error of this softmax layer, the error will backpropagate and influence both the RAE parameters and the word representations.</S>
    <S sid="124" ssid="88">Initially, words such as good and bad have very similar representations.</S>
    <S sid="125" ssid="89">This is also the case for Brown clusters and other methods that use only cooccurrence statistics in a small window around each word.</S>
    <S sid="126" ssid="90">When learning with positive/negative sentiment, the word embeddings get modified and capture less syntactic and more sentiment information.</S>
    <S sid="127" ssid="91">In order to predict the sentiment distribution of a sentence with this model, we use the learned vector representation of the top tree node and train a simple logistic regression classifier.</S>
    <S sid="128" ssid="92">We first describe the new experience project (EP) dataset, results of standard classification tasks on this dataset and how to predict its sentiment label distributions.</S>
    <S sid="129" ssid="93">We then show results on other commonly used datasets and conclude with an analysis of the important parameters of the model.</S>
    <S sid="130" ssid="94">In all experiments involving our model, we represent words using 100-dimensional word vectors.</S>
    <S sid="131" ssid="95">We explore the two settings mentioned in Sec.</S>
    <S sid="132" ssid="96">2.1.</S>
    <S sid="133" ssid="97">We compare performance on standard datasets when using randomly initialized word vectors (random word init.)</S>
    <S sid="134" ssid="98">or word vectors trained by the model of Collobert and Weston (2008) and provided by Turian et al. (2010).2 These vectors were trained on an unlabeled corpus of the English Wikipedia.</S>
    <S sid="135" ssid="99">Note that alternatives such as Brown clusters are not suitable since they do not capture sentiment information (good and bad are usually in the same cluster) and cannot be modified via backpropagation.</S>
    <S sid="136" ssid="100">The confessions section of the experience project website3 lets people anonymously write short personal stories or &#8220;confessions&#8221;.</S>
    <S sid="137" ssid="101">Once a story is on the site, each user can give a single vote to one of five label categories (with our interpretation): The EP dataset has 31,676 confession entries, a total number of 74,859 votes for the 5 labels above, the average number of votes per entry is 2.4 (with a variance of 33).</S>
    <S sid="138" ssid="102">For the five categories, the numbers of votes are [14, 816;13, 325;10, 073; 30, 844; 5, 801].</S>
    <S sid="139" ssid="103">Since an entry with less than 4 votes is not very well identified, we train and test only on entries with at least 4 total votes.</S>
    <S sid="140" ssid="104">There are 6,129 total such entries.</S>
    <S sid="141" ssid="105">The distribution over total votes in the 5 classes is similar: [0.22; 0.2; 0.11; 0.37; 0.1].</S>
    <S sid="142" ssid="106">The average length of entries is 129 words.</S>
    <S sid="143" ssid="107">Some entries contain multiple sentences.</S>
    <S sid="144" ssid="108">In these cases, we average the predicted label distributions from the sentences.</S>
    <S sid="145" ssid="109">Table 1 shows statistics of this and other commonly used sentiment datasets (which we compare on in later experiments).</S>
    <S sid="146" ssid="110">Table 2 shows example entries as well as gold and predicted label distributions as described in the next sections.</S>
    <S sid="147" ssid="111">Compared to other datasets, the EP dataset contains a wider range of human emotions that goes far beyond positive/negative product or movie reviews.</S>
    <S sid="148" ssid="112">Each item is labeled with a multinomial distribution over interconnected response categories.</S>
    <S sid="149" ssid="113">This is in contrast to most other datasets (including multiaspect rating) where several distinct aspects are rated independently but on the same scale.</S>
    <S sid="150" ssid="114">The topics range from generic happy statements, daily clumsiness reports, love, loneliness, to relationship abuse and suicidal notes.</S>
    <S sid="151" ssid="115">As is evident from the total number of label votes, the most common user reaction is one of empathy and an ability to relate to the authors experience.</S>
    <S sid="152" ssid="116">However, some stories describe horrible scenarios that are not common and hence receive more offers of condolence.</S>
    <S sid="153" ssid="117">In the following sections we show some examples of stories with predicted and true distributions but refrain from listing the most horrible experiences.</S>
    <S sid="154" ssid="118">For all experiments on the EP dataset, we split the data into train (49%), development (21%) and test data (30%).</S>
    <S sid="155" ssid="119">The first task for our evaluation on the EP dataset is to simply predict the single class that receives the most votes.</S>
    <S sid="156" ssid="120">In order to compare our novel joint phrase representation and classifier learning framework to traditional methods, we use the following baselines: Random Since there are five classes, this gives 20% accuracy.</S>
    <S sid="157" ssid="121">Most Frequent Selecting the class which most frequently has the most votes (the class I understand).</S>
    <S sid="158" ssid="122">Baseline 1: Binary BoW This baseline uses logistic regression on binary bag-of-word representations that are 1 if a word is present and 0 otherwise.</S>
    <S sid="159" ssid="123">Baseline 2: Features This model is similar to traditional approaches to sentiment classification in that it uses many hand-engineered resources.</S>
    <S sid="160" ssid="124">We first used a spell-checker and Wordnet to map words and their misspellings to synsets to reduce the total number of words.</S>
    <S sid="161" ssid="125">We then replaced sentiment words with a sentiment category identifier using the sentiment lexica of the Harvard Inquirer (Stone, 1966) and LIWC (Pennebaker et al., 2007).</S>
    <S sid="162" ssid="126">Lastly, we used tf-idf weighting on the bag-of-word representations and trained an SVM.</S>
    <S sid="163" ssid="127">KL Predicted&amp;Gold V. Entry (Shortened if it ends with ...) .03 .16 .16 .16 .33 .16 6 I reguarly shoplift.</S>
    <S sid="164" ssid="128">I got caught once and went to jail, but I&#8217;ve found that this was not a deterrent.</S>
    <S sid="165" ssid="129">I don&#8217;t buy groceries, I don&#8217;t buy school supplies for my kids, I don&#8217;t buy gifts for my kids, we don&#8217;t pay for movies, and I dont buy most incidentals for the house (cleaning supplies, toothpaste, etc.)...</S>
    <S sid="166" ssid="130">.03 .38 .04 .06 .35 .14 165 i am a very succesfull buissnes man.i make good money but i have been addicted to crack for 13 years.i moved 1 hour away from my dealers 10 years ago to stop using now i dont use daily but once a week usally friday nights. i used to use 1 or 2 hundred a day now i use 4 or 5 hundred on a friday.my problem is i am a funcational addict... .05 .14 .28 .14 .28 .14 7 Hi there, Im a guy that loves a girl, the same old bloody story...</S>
    <S sid="167" ssid="131">I met her a while ago, while studying, she Is so perfect, so mature and yet so lonely, I get to know her and she get ahold of me, by opening her life to me and so did I with her, she has been the first person, male or female that has ever made that bond with me,... .07 .27 .18 .00 .45 .09 11 be kissing you right now. i should be wrapped in your arms in the dark, but instead i&#8217;ve ruined everything. i&#8217;ve piled bricks to make a wall where there never should have been one. i feel an ache that i shouldn&#8217;t feel because i&#8217;ve never had you close enough. we&#8217;ve never touched, but i still feel as though a part of me is missing.... .05 23 Dear Love, I just want to say that I am looking for you.</S>
    <S sid="168" ssid="132">Tonight I felt the urge to write, and I am becoming more and more frustrated that I have not found you yet.</S>
    <S sid="169" ssid="133">I&#8217;m also tired of spending so much heart on an old dream.... .05 5 I wish I knew somone to talk to here.</S>
    <S sid="170" ssid="134">.06 24 I loved her but I screwed it up.</S>
    <S sid="171" ssid="135">Now she&#8217;s moved on.</S>
    <S sid="172" ssid="136">I&#8217;ll never have her again.</S>
    <S sid="173" ssid="137">I don&#8217;t know if I&#8217;ll ever stop thinking about her.</S>
    <S sid="174" ssid="138">.06 5 i am 13 years old and i hate my father he is alwas geting drunk and do&#8217;s not care about how it affects me or my sisters i want to care but the truthis i dont care if he dies .13 6 well i think hairy women are attractive .35 5 As soon as I put clothings on I will go down to DQ and get a thin mint blizzard.</S>
    <S sid="175" ssid="139">I need it.</S>
    <S sid="176" ssid="140">It&#8217;ll make my soul feel a bit better :) .36 6 I am a 45 year old divoced woman, and I havent been on a date or had any significant relationship in 12 years...yes, 12 yrs. the sad thing is, Im not some dried up old granny who is no longer interested in men, I just can&#8217;t meet men.</S>
    <S sid="177" ssid="141">(before you judge, no Im not terribly picky!)</S>
    <S sid="178" ssid="142">What is wrong with me?</S>
    <S sid="179" ssid="143">.63 6 When i was in kindergarden i used to lock myself in the closet and eat all the candy.</S>
    <S sid="180" ssid="144">Then the teacher found out it was one of us and made us go two days without freetime.</S>
    <S sid="181" ssid="145">It might be a little late now, but sorry guys it was me haha .92 4 My paper is due in less than 24 hours and I&#8217;m still dancing round my room!</S>
    <S sid="182" ssid="146">Baseline 3: Word Vectors We can ignore the RAE tree structure and only train softmax layers directly on the pre-trained words in order to influence the word vectors.</S>
    <S sid="183" ssid="147">This is followed by an SVM trained on the average of the word vectors.</S>
    <S sid="184" ssid="148">We also experimented with latent Dirichlet allocation (Blei et al., 2003) but performance was very low.</S>
    <S sid="185" ssid="149">Table 3 shows the results for predicting the class with the most votes.</S>
    <S sid="186" ssid="150">Even the approach that is based on sentiment lexica and other resources is outperformed by our model by almost 3%, showing that for tasks involving complex broad-range human sentiment, the often used sentiment lexica lack in coverage and traditional bag-of-words representations are not powerful enough.</S>
    <S sid="187" ssid="151">We now turn to evaluating our distributionprediction approach.</S>
    <S sid="188" ssid="152">In both this and the previous maximum label task, we backprop using the gold multinomial distribution as a target.</S>
    <S sid="189" ssid="153">Since we maximize likelihood and because we want to predict a distribution that is closest to the distribution of labels that people would assign to a story, we evaluate using KL divergence: KL(g||p) = Ei gi log(gi/pi), where g is the gold distribution and p is the predicted one.</S>
    <S sid="190" ssid="154">We report the average KL divergence, where a smaller value indicates better predictive power.</S>
    <S sid="191" ssid="155">To get an idea of the values of KL divergence, predicting random distributions gives a an average of 1.2 in KL divergence, predicting simply the average distribution in the training data give 0.83.</S>
    <S sid="192" ssid="156">Fig.</S>
    <S sid="193" ssid="157">4 shows that our RAE-based model outperforms the other baselines.</S>
    <S sid="194" ssid="158">Table 2 shows EP example entries with predicted and gold distributions, as well as numbers of votes.</S>
    <S sid="195" ssid="159">In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews4 (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al., 2005).We give statistical information on these and the EP corpus in Table 1.</S>
    <S sid="196" ssid="160">We compare to the state-of-the-art system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables.</S>
    <S sid="197" ssid="161">We use the same training and testing regimen (10-fold cross validation) as well as their baselines: majority phrase voting using sentiment and reversal lexica; rule-based reversal using a dependency tree; Bag-of-Features and their full Tree-CRF model.</S>
    <S sid="198" ssid="162">As shown in Table 4, our algorithm outperforms their approach on both datasets.</S>
    <S sid="199" ssid="163">For the movie review (MR) data set, we do not use any handdesigned lexica.</S>
    <S sid="200" ssid="164">An error analysis on the MPQA dataset showed several cases of single words which never occurred in the training set.</S>
    <S sid="201" ssid="165">Correctly classifying these instances can only be the result of having them in the original sentiment lexicon.</S>
    <S sid="202" ssid="166">Hence, for the experiment on MPQA we added the same sentiment lexicon that (Nakagawa et al., 2010) used in their system to our training set.</S>
    <S sid="203" ssid="167">This improved accuracy from 86.0 to 86.4.Using the pre-trained word vectors boosts performance by less than 1% compared to randomly initialized word vectors (setting: random word init).</S>
    <S sid="204" ssid="168">This shows that our method can work well even in settings with little training data.</S>
    <S sid="205" ssid="169">We visualize the semantic vectors that the recursive autoencoder learns by listing n-grams that give the highest probability for each polarity.</S>
    <S sid="206" ssid="170">Table 5 shows such n-grams for different lengths when the RAE is trained on the movie review polarity dataset.</S>
    <S sid="207" ssid="171">On a 4-core machine, training time for the smaller corpora such as the movie reviews takes around 3 hours and for the larger EP corpus around 12 hours until convergence.</S>
    <S sid="208" ssid="172">Testing of hundreds of movie reviews takes only a few seconds.</S>
    <S sid="209" ssid="173">In this experiment, we show how the hyperparameter &#945; influences accuracy on the development set of one of the cross-validation splits of the MR dataset.</S>
    <S sid="210" ssid="174">This parameter essentially trade-off the supervised and unsupervised parts of the objective.</S>
    <S sid="211" ssid="175">Fig.</S>
    <S sid="212" ssid="176">5 shows that a larger focus on the supervised objective is important but that a weight of &#945; = 0.2 for the reconstruction error prevents overfitting and achieves the highest performance.</S>
  </SECTION>
  <SECTION title="5 Related Work" number="3">
    <S sid="213" ssid="1">Autoencoders are neural networks that learn a reduced dimensional representation of fixed-size inputs such as image patches or bag-of-word representations of text documents.</S>
    <S sid="214" ssid="2">They can be used to efficiently learn feature encodings which are useful for classification.</S>
    <S sid="215" ssid="3">Recently, Mirowski et al. (2010) learn dynamic autoencoders for documents in a bagof-words format which, like ours, combine supervised and reconstruction objectives.</S>
    <S sid="216" ssid="4">The idea of applying an autoencoder in a recursive setting was introduced by Pollack (1990).</S>
    <S sid="217" ssid="5">Pollack&#8217;s recursive auto-associative memories (RAAMs) are similar to ours in that they are a connectionst, feedforward model.</S>
    <S sid="218" ssid="6">However, RAAMs learn vector representations only for fixed recursive data structures, whereas our RAE builds this recursive data structure.</S>
    <S sid="219" ssid="7">More recently, (Voegtlin and Dominey, 2005) introduced a linear modification to RAAMs that is able to better generalize to novel combinations of previously seen constituents.</S>
    <S sid="220" ssid="8">One of the major shortcomings of previous applications of recursive autoencoders to natural language sentences was their binary word representation as discussed in Sec.</S>
    <S sid="221" ssid="9">2.1.</S>
    <S sid="222" ssid="10">Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.</S>
    <S sid="223" ssid="11">Their models are applicable to natural language and computer vision tasks such as parsing or object detection.</S>
    <S sid="224" ssid="12">The current work is related in that it uses a recursive deep learning model.</S>
    <S sid="225" ssid="13">However, RNNs require labeled tree structures and use a supervised score at each node.</S>
    <S sid="226" ssid="14">Instead, RAEs learn hierarchical structures that are trying to capture as much of the the original word vectors as possible.</S>
    <S sid="227" ssid="15">The learned structures are not necessarily syntactically plausible but can capture more of the semantic content of the word vectors.</S>
    <S sid="228" ssid="16">Other recent deep learning methods for sentiment analysis include (Maas et al., 2011).</S>
    <S sid="229" ssid="17">Pang et al. (2002) were one of the first to experiment with sentiment classification.</S>
    <S sid="230" ssid="18">They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification.</S>
    <S sid="231" ssid="19">Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements.</S>
    <S sid="232" ssid="20">Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004).</S>
    <S sid="233" ssid="21">For further references, see (Pang and Lee, 2008).</S>
    <S sid="234" ssid="22">Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees.</S>
    <S sid="235" ssid="23">They also show improvements by first distinguishing between neutral and polar sentences.</S>
    <S sid="236" ssid="24">Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions.</S>
    <S sid="237" ssid="25">Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008).</S>
    <S sid="238" ssid="26">Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002).</S>
    <S sid="239" ssid="27">In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words.</S>
    <S sid="240" ssid="28">In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5).</S>
    <S sid="241" ssid="29">(Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment.</S>
    <S sid="242" ssid="30">The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as &#8220;prevent cancer&#8221;, where prevent reverses the negative polarity of cancer.</S>
    <S sid="243" ssid="31">Like our approach they capture compositional semantics.</S>
    <S sid="244" ssid="32">However, our model does so without manually constructing any rules or lexica.</S>
    <S sid="245" ssid="33">Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as &#8220;once in a life time&#8221;.</S>
    <S sid="246" ssid="34">While our method can also learn multiword phrases it does not require a seed set or a large web graph.</S>
    <S sid="247" ssid="35">(Nakagawa et al., 2010) introduced an approach based on CRFs with hidden variables with very good performance.</S>
    <S sid="248" ssid="36">We compare to their stateof-the-art system.</S>
    <S sid="249" ssid="37">We outperform them on the standard corpora that we tested on without requiring external systems such as POS taggers, dependency parsers and sentiment lexica.</S>
    <S sid="250" ssid="38">Our approach jointly learns the necessary features and tree structure.</S>
    <S sid="251" ssid="39">In multi-aspect rating (Snyder and Barzilay, 2007) one finds several distinct aspects such as food or service in a restaurant and then rates them on a fixed linear scale such as 1-5 stars, where all aspects could obtain just 1 star or all aspects could obtain 5 stars independently.</S>
    <S sid="252" ssid="40">In contrast, in our method a single aspect (a complex reaction to a human experience) is predicted not in terms of a fixed scale but in terms of a multinomial distribution over several interconnected, sometimes mutually exclusive emotions.</S>
    <S sid="253" ssid="41">A single story cannot simultaneously obtain a strong reaction in different emotional responses (by virtue of having to sum to one).</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="4">
    <S sid="254" ssid="1">We presented a novel algorithm that can accurately predict sentence-level sentiment distributions.</S>
    <S sid="255" ssid="2">Without using any hand-engineered resources such as sentiment lexica, parsers or sentiment shifting rules, our model achieves state-of-the-art performance on commonly used sentiment datasets.</S>
    <S sid="256" ssid="3">Furthermore, we introduce a new dataset that contains distributions over a broad range of human emotions.</S>
    <S sid="257" ssid="4">Our evaluation shows that our model can more accurately predict these distributions than other models.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="5">
    <S sid="258" ssid="1">We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.</S>
    <S sid="259" ssid="2">FA8750-09-C-0181.</S>
    <S sid="260" ssid="3">Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of DARPA, AFRL, or the US government.</S>
    <S sid="261" ssid="4">This work was also supported in part by the DARPA Deep Learning program under contract number FA8650-10-C-7020.</S>
    <S sid="262" ssid="5">We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.</S>
  </SECTION>
</PAPER>
