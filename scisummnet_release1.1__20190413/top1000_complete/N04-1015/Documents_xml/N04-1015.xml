<PAPER>
	<S sid="0">Catching The Drift: Probabilistic Content Models With Applications To Generation And Summarization</S><ABSTRACT>
		<S sid="1" ssid="1">We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.</S>
		<S sid="2" ssid="2">We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.</S>
		<S sid="3" ssid="3">We then apply our method to two complementary tasks: information ordering and ex tractive summarization.</S>
		<S sid="4" ssid="4">Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">The development and application of computational models of text structure is a central concern in natural language processing.</S>
			<S sid="6" ssid="6">Document-level analysis of text struc ture is an important instance of such work.</S>
			<S sid="7" ssid="7">Previous research has sought to characterize texts in terms of domain-independent rhetorical elements, such as schema items (McKeown, 1985) or rhetorical relations (Mann and Thompson, 1988; Marcu, 1997).</S>
			<S sid="8" ssid="8">The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text: content.</S>
			<S sid="9" ssid="9">Our use of the term ?content?</S>
			<S sid="10" ssid="10">corresponds roughly to the notions of topic and topic change.</S>
			<S sid="11" ssid="11">We desire models that can specify, for example, that articles about earthquakes typically contain information about quake strength, location, and casualties, and that descriptions of casualties usually precede those of rescue efforts.</S>
			<S sid="12" ssid="12">But rather than manually determine the topics for a given domain, we take a distributional view, learning them directly from un-annotated texts via analysis of word distribution patterns.</S>
			<S sid="13" ssid="13">This idea dates back at least to Harris (1982), who claimed that ?various types of [word] recurrence patterns seem to characterize various types ofdiscourse?.</S>
			<S sid="14" ssid="14">Advantages of a distributional perspective include both drastic reduction in human effort and recogni tion of ?topics?</S>
			<S sid="15" ssid="15">that might not occur to a human expert and yet, when explicitly modeled, aid in applications.</S>
			<S sid="16" ssid="16">Of course, the success of the distributional approachdepends on the existence of recurrent patterns.</S>
			<S sid="17" ssid="17">In arbi trary document collections, such patterns might be toovariable to be easily detected by statistical means.</S>
			<S sid="18" ssid="18">How ever, research has shown that texts from the same domain tend to exhibit high similarity (Wray, 2002).</S>
			<S sid="19" ssid="19">Cognitive psychologists have long posited that this similarity is not accidental, arguing that formulaic text structure facilitates readers?</S>
			<S sid="20" ssid="20">comprehension and recall (Bartlett, 1932).1In this paper, we investigate the utility of domain specific content models for representing topics and topic shifts.</S>
			<S sid="21" ssid="21">Content models are Hidden Markov Models (HMMs) wherein states correspond to typesof information characteristic to the domain of interest (e.g., earthquake magnitude or previous earth quake occurrences), and state transitions capture possible information-presentation orderings within that domain.</S>
			<S sid="22" ssid="22">We first describe an efficient, knowledge-lean methodfor learning both a set of topics and the relations be tween topics directly from un-annotated documents.</S>
			<S sid="23" ssid="23">Our technique incorporates a novel adaptation of the standard HMM induction algorithm that is tailored to the task of modeling content.</S>
			<S sid="24" ssid="24">Then, we apply techniques based on content models totwo complex text-processing tasks.</S>
			<S sid="25" ssid="25">First, we consider in formation ordering, that is, choosing a sequence in whichto present a pre-selected set of items; this is an essen tial step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.</S>
			<S sid="26" ssid="26">In our experiments, content models outperform Lapata?s (2003) state-of-the-art ordering method by a wide margin ? forone domain and performance metric, the gap was 78 percentage points.</S>
			<S sid="27" ssid="27">Second, we consider extractive summa 1But ?formulaic?</S>
			<S sid="28" ssid="28">is not necessarily equivalent to ?simple?, so automated approaches still offer advantages over manual techniques, especially if one needs to model several domains.</S>
			<S sid="29" ssid="29">rization: the compression of a document by choosinga subsequence of its sentences.</S>
			<S sid="30" ssid="30">For this task, we de velop a new content-model-based learning algorithm for sentence selection.</S>
			<S sid="31" ssid="31">The resulting summaries yield 88%match with human-written output, which compares fa vorably to the 69% achieved by the standard ?leading </S></SECTION></PAPER>
