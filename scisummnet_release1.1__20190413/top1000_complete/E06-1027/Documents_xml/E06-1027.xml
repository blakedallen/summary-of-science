<PAPER>
	<S sid="0">Mining WordNet For A Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses</S><ABSTRACT>
		<S sid="1" ssid="1">Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semanticfeatures.</S>
		<S sid="2" ssid="2">We present a method for ex tracting sentiment-bearing adjectives fromWordNet using the Sentiment Tag Extrac tion Program (STEP).</S>
		<S sid="3" ssid="3">We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list ofpositive and negative adjectives and evaluated the results against other manually annotated lists.</S>
		<S sid="4" ssid="4">The 58 runs were then col lapsed into a single set of 7, 813 unique words.</S>
		<S sid="5" ssid="5">For each word we computed a Net Overlap Score by subtracting the totalnumber of runs assigning this word a neg ative sentiment from the total of the runs that consider it positive.</S>
		<S sid="6" ssid="6">We demonstrate that Net Overlap Score can be used as ameasure of the words degree of member ship in the fuzzy category of sentiment:the core adjectives, which had the high est Net Overlap scores, were identifiedmost accurately both by STEP and by hu man annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="7" ssid="7">Many of the tasks required for effective seman tic tagging of phrases and texts rely on a list ofwords annotated with some lexical semantic fea tures.</S>
			<S sid="8" ssid="8">Traditional approaches to the development of such lists are based on the implicit assumption of classical truth-conditional theories of meaningrepresentation, which regard all members of a category as equal: no element is more of a member than any other (Edmonds, 1999).</S>
			<S sid="9" ssid="9">In this paper, we challenge the applicability of this assump tion to the semantic category of sentiment, whichconsists of positive, negative and neutral subcate gories, and present a dictionary-based Sentiment Tag Extraction Program (STEP) that we use to generate a fuzzy set of English sentiment-bearing words for the use in sentiment tagging systems 1.</S>
			<S sid="10" ssid="10">The proposed approach based on the fuzzy logic(Zadeh, 1987) is used here to assign fuzzy sen timent tags to all words in WordNet (Fellbaum, 1998), that is it assigns sentiment tags and a degreeof centrality of the annotated words to the sentiment category.</S>
			<S sid="11" ssid="11">This assignment is based on Word Net glosses.</S>
			<S sid="12" ssid="12">The implications of this approach for NLP and linguistic research are discussed.</S>
	</SECTION>
	<SECTION title="The Category of Sentiment as a Fuzzy. " number="2">
			<S sid="13" ssid="1">Set Some semantic categories have clear membership (e.g., lexical fields (Lehrer, 1974) of color, body parts or professions), while others are much more difficult to define.</S>
			<S sid="14" ssid="2">This prompted the developmentof approaches that regard the transition frommem bership to non-membership in a semantic category as gradual rather than abrupt (Zadeh, 1987; Rosch, 1978).</S>
			<S sid="15" ssid="3">In this paper we approach the category of sentiment as one of such fuzzy categories wheresome words ? such as good, bad ? are very central, prototypical members, while other, less central words may be interpreted differently by differ ent people.</S>
			<S sid="16" ssid="4">Thus, as annotators proceed from thecore of the category to its periphery, word mem 1Sentiment tagging is defined here as assigning positive,negative and neutral labels to words according to the senti ment they express.</S>
			<S sid="17" ssid="5">209 bership in this category becomes more ambiguous, and hence, lower inter-annotator agreement can be expected for more peripheral words.</S>
			<S sid="18" ssid="6">Under theclassical truth-conditional approach the disagree ment between annotators is invariably viewed as a sign of poor reliability of coding and is eliminatedby ?training?</S>
			<S sid="19" ssid="7">annotators to code difficult and am biguous cases in some standard way.</S>
			<S sid="20" ssid="8">While this procedure leads to high levels of inter-annotator agreement on a list created by a coordinated team of researchers, the naturally occurring differencesin the interpretation of words located on the pe riphery of the category can clearly be seen whenannotations by two independent teams are compared.</S>
			<S sid="21" ssid="9">The Table 1 presents the comparison of GI H4 (General Inquirer Harvard IV-4 list, (Stone et al., 1966)) 2 and HM (from (Hatzivassiloglou and McKeown, 1997) study) lists of words manuallyannotated with sentiment tags by two different re search teams.</S>
			<S sid="22" ssid="10">GI-H4 HM List composition nouns, verbs, adj., adv.</S>
			<S sid="23" ssid="11">adj.</S>
			<S sid="24" ssid="12">only Total list size 8, 211 1, 336 Total adjectives 1, 904 1, 336Tags assigned Positiv, Nega tiv or no tag Positiveor Nega tive Adj.</S>
			<S sid="25" ssid="13">with 1, 268 1, 336 non-neutral tags Intersection 774 (55% 774 (58% (% intersection) of GI-H4 adj) of HM) Agreement on tags 78.7%Table 1: Agreement between GI-H4 and HM an notations on sentiment tags.</S>
			<S sid="26" ssid="14">The approach to sentiment as a category withfuzzy boundaries suggests that the 21.3% dis agreement between the two manually annotatedlists reflects a natural variability in human annotators?</S>
			<S sid="27" ssid="15">judgment and that this variability is related to the degree of centrality and/or relative importance of certain words to the category of sen timent.</S>
			<S sid="28" ssid="16">The attempts to address this difference 2The General Inquirer (GI) list used in this study was manually cleaned to remove duplicate entries for words with same part of speech and sentiment.</S>
			<S sid="29" ssid="17">Only the Harvard IV-4 list component of the whole GI was used in this study, sinceother lists included in GI lack the sentiment annotation.</S>
			<S sid="30" ssid="18">Un less otherwise specified, we used the full GI-H4 list including the Neutral words that were not assigned Positiv or Negativ annotations.</S>
			<S sid="31" ssid="19">in importance of various sentiment markers have crystallized in two main approaches: automatic assignment of weights based on some statistical criterion ((Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002; Kim and Hovy, 2004), and others) or manual annotation (Subasic andHuettner, 2001).</S>
			<S sid="32" ssid="20">The statistical approaches usually employ some quantitative criterion (e.g., mag nitude of pointwise mutual information in (Turney and Littman, 2002), ?goodness-for-fit?</S>
			<S sid="33" ssid="21">measure in(Hatzivassiloglou and McKeown, 1997), probabil ity of word?s sentiment given the sentiment if itssynonyms in (Kim and Hovy, 2004), etc.) to de fine the strength of the sentiment expressed by aword or to establish a threshold for the member ship in the crisp sets 3 of positive, negative andneutral words.</S>
			<S sid="34" ssid="22">Both approaches have their limi tations: the first approach produces coarse results and requires large amounts of data to be reliable,while the second approach is prohibitively expen sive in terms of annotator time and runs the risk ofintroducing a substantial subjective bias in anno tations.</S>
			<S sid="35" ssid="23">In this paper we seek to develop an approachfor semantic annotation of a fuzzy lexical cate gory and apply it to sentiment annotation of allWordNet words.</S>
			<S sid="36" ssid="24">The sections that follow (1) describe the proposed approach used to extract sen timent information from WordNet entries usingSTEP (Semantic Tag Extraction Program) algo rithm, (2) discuss the overall performance of STEP on WordNet glosses, (3) outline the method fordefining centrality of a word to the sentiment cate gory, and (4) compare the results of both automatic (STEP) and manual (HM) sentiment annotations to the manually-annotated GI-H4 list, which was used as a gold standard in this experiment.</S>
			<S sid="37" ssid="25">The comparisons are performed separately for each of the subsets of GI-H4 that are characterized by adifferent distance from the core of the lexical cat egory of sentiment.</S>
	</SECTION>
	<SECTION title="Sentiment Tag Extraction from. " number="3">
			<S sid="38" ssid="1">WordNet Entries Word lists for sentiment tagging applications can be compiled using different methods.</S>
			<S sid="39" ssid="2">Automatic methods of sentiment annotation at the word level can be grouped into two major categories: (1) corpus-based approaches and (2) dictionary-based3We use the term crisp set to refer to traditional, non fuzzy sets 210 approaches.</S>
			<S sid="40" ssid="3">The first group includes methods that rely on syntactic or co-occurrence patternsof words in large texts to determine their sentiment (e.g., (Turney and Littman, 2002; Hatzivassiloglou and McKeown, 1997; Yu and Hatzivassiloglou, 2003; Grefenstette et al, 2004) and oth ers).</S>
			<S sid="41" ssid="4">The majority of dictionary-based approaches use WordNet information, especially, synsets and hierarchies, to acquire sentiment-marked words (Hu and Liu, 2004; Valitutti et al, 2004; Kim and Hovy, 2004) or to measure the similarity between candidate words and sentiment-bearing words such as good and bad (Kamps et al, 2004).In this paper, we propose an approach to sentiment annotation of WordNet entries that was implemented and tested in the Semantic Tag Extrac tion Program (STEP).</S>
			<S sid="42" ssid="5">This approach relies bothon lexical relations (synonymy, antonymy and hyponymy) provided in WordNet and on the WordNet glosses.</S>
			<S sid="43" ssid="6">It builds upon the properties of dic tionary entries as a special kind of structured text:such lexicographical texts are built to establish se mantic equivalence between the left-hand and theright-hand parts of the dictionary entry, and there fore are designed to match as close as possible the components of meaning of the word.</S>
			<S sid="44" ssid="7">They have relatively standard style, grammar and syntactic structures, which removes a substantial source of noise common to other types of text, and finally, they have extensive coverage spanning the entire lexicon of a natural language.</S>
			<S sid="45" ssid="8">The STEP algorithm starts with a small set of seed words of known sentiment value (positive or negative).</S>
			<S sid="46" ssid="9">This list is augmented during thefirst pass by adding synonyms, antonyms and hy ponyms of the seed words supplied in WordNet.</S>
			<S sid="47" ssid="10">This step brings on average a 5-fold increase in the size of the original list with the accuracy of the resulting list comparable to manual annotations (78%, similar to HM vs. GI-H4 accuracy).</S>
			<S sid="48" ssid="11">At the second pass, the system goes through all WordNet glosses and identifies the entries that contain in their definitions the sentiment-bearing words from the extended seed list and adds these head words (or rather, lexemes) to the corresponding category ? positive, negative or neutral (the remainder).</S>
			<S sid="49" ssid="12">A third, clean-up pass is then performed to partially disambiguate the identified WordNet glosses with Brill?s part-of-speech tagger (Brill, 1995), which performs with up to 95% accuracy, and eliminates errors introduced into the list by part-of-speech ambiguity of some words acquired in pass 1 and from the seed list.</S>
			<S sid="50" ssid="13">At this step, we also filter outall those words that have been assigned contradict ing, positive and negative, sentiment values within the same run.</S>
			<S sid="51" ssid="14">The performance of STEP was evaluated using GI-H4 as a gold standard, while the HM list wasused as a source of seed words fed into the system.</S>
			<S sid="52" ssid="15">We evaluated the performance of our sys tem against the complete list of 1904 adjectives in GI-H4 that included not only the words that were marked as Positiv, Negativ, but also those that werenot considered sentiment-laden by GI-H4 annota tors, and hence were by default considered neutralin our evaluation.</S>
			<S sid="53" ssid="16">For the purposes of the evalua tion we have partitioned the entire HM list into 58non-intersecting seed lists of adjectives.</S>
			<S sid="54" ssid="17">The re sults of the 58 runs on these non-intersecting seed lists are presented in Table 2.</S>
			<S sid="55" ssid="18">The Table 2 showsthat the performance of the system exhibits sub stantial variability depending on the composition of the seed list, with accuracy ranging from 47.6%to 87.5% percent (Mean = 71.2%, Standard Devi ation (St.Dev) = 11.0%).</S>
			<S sid="56" ssid="19">Average Average run size % correct # of adj StDev % StDev PASS 1 103 29 78.0% 10.5% (WN Relations) PASS 2 630 377 64.5% 10.8% (WN Glosses) PASS 3 435 291 71.2% 11.0% (POS clean-up) Table 2: Performance statistics on STEP runs.</S>
			<S sid="57" ssid="20">The significant variability in accuracy of the runs (Standard Deviation over 10%) is attributable to the variability in the properties of the seed list words in these runs.</S>
			<S sid="58" ssid="21">The HM list includes some sentiment-marked words where not all meanings are laden with sentiment, but also the words where some meanings are neutral and even the wordswhere such neutral meanings are much more fre quent than the sentiment-laden ones.</S>
			<S sid="59" ssid="22">The runswhere seed lists included such ambiguous adjectives were labeling a lot of neutral words as sen timent marked since such seed words were more likely to be found in the WordNet glosses in their more frequent neutral meaning.</S>
			<S sid="60" ssid="23">For example, run # 53 had in its seed list two ambiguous adjectives 1 dim and plush, which are neutral in most of the contexts.</S>
			<S sid="61" ssid="24">This resulted in only 52.6% accuracy (18.6% below the average).</S>
			<S sid="62" ssid="25">Run # 48, on theother hand, by a sheer chance, had only unam biguous sentiment-bearing words in its seed list, and, thus, performed with a fairly high accuracy (87.5%, 16.3% above the average).In order to generate a comprehensive list cov ering the entire set of WordNet adjectives, the 58 runs were then collapsed into a single set of unique words.</S>
			<S sid="63" ssid="26">Since many of the clearly sentiment-laden adjectives that form the core of the category of sentiment were identified by STEP in multiple runs and had, therefore, multiple duplicates in thelist that were counted as one entry in the com bined list, the collapsing procedure resulted in a lower-accuracy (66.5% - when GI-H4 neutralswere included) but much larger list of English adjectives marked as positive (n = 3, 908) or neg ative (n = 3, 905).</S>
			<S sid="64" ssid="27">The remainder of WordNet?s 22, 141 adjectives was not found in any STEP run and hence was deemed neutral (n = 14, 328).</S>
			<S sid="65" ssid="28">Overall, the system?s 66.5% accuracy on thecollapsed runs is comparable to the accuracy re ported in the literature for other systems run onlarge corpora (Turney and Littman, 2002; Hatzi vassiloglou and McKeown, 1997).</S>
			<S sid="66" ssid="29">In order to make a meaningful comparison with the results reported in (Turney and Littman, 2002), we also did an evaluation of STEP results on positives andnegatives only (i.e., the neutral adjectives from GI H4 list were excluded) and compared our labels tothe remaining 1266 GI-H4 adjectives.</S>
			<S sid="67" ssid="30">The accuracy on this subset was 73.4%, which is compara ble to the numbers reported by Turney and Littman(2002) for experimental runs on 3, 596 sentiment marked GI words from different parts of speechusing a 2x109 corpus to compute point-wise mu tual information between the GI words and 14 manually selected positive and negative paradigm words (76.06%).</S>
			<S sid="68" ssid="31">The analysis of STEP system performancevs.</S>
			<S sid="69" ssid="32">GI-H4 and of the disagreements between man ually annotated HM and GI-H4 showed that the greatest challenge with sentiment tagging ofwords lies at the boundary between sentimentmarked (positive or negative) and sentiment neutral words.</S>
			<S sid="70" ssid="33">The 7% performance gain (from 66.5% to 73.4%) associated with the removal of neutrals from the evaluation set emphasizes the importance of neutral words as a major source of sentiment extraction system errors 4.</S>
			<S sid="71" ssid="34">Moreover, the boundary between sentiment-bearing (positive or negative) and neutral words in GI-H4 accountsfor 93% of disagreements between the labels assigned to adjectives in GI-H4 and HM by two in dependent teams of human annotators.</S>
			<S sid="72" ssid="35">The viewtaken here is that the vast majority of such inter annotator disagreements are not really errors but a reflection of the natural ambiguity of the words that are located on the periphery of the sentiment category.</S>
	</SECTION>
	<SECTION title="Establishing the degree of word?s. " number="4">
			<S sid="73" ssid="1">centrality to the semantic category The approach to sentiment category as a fuzzyset ascribes the category of sentiment some spe cific structural properties.</S>
			<S sid="74" ssid="2">First, as opposed to thewords located on the periphery, more central ele ments of the set usually have stronger and more numerous semantic relations with other categorymembers 5.</S>
			<S sid="75" ssid="3">Second, the membership of these cen tral words in the category is less ambiguous than the membership of more peripheral words.</S>
			<S sid="76" ssid="4">Thus, we can estimate the centrality of a word in a given category in two ways: 1.</S>
			<S sid="77" ssid="5">Through the density of the word?s relation-.</S>
			<S sid="78" ssid="6">ships with other words ? by enumerating its semantic ties to other words within the field, and calculating membership scores based on the number of these ties; and 2.</S>
			<S sid="79" ssid="7">Through the degree of word membership am-.</S>
			<S sid="80" ssid="8">biguity ? by assessing the inter-annotator agreement on the word membership in this category.</S>
			<S sid="81" ssid="9">Lexicographical entries in the dictionaries, suchas WordNet, seek to establish semantic equivalence between the word and its definition and provide a rich source of human-annotated relationships between the words.</S>
			<S sid="82" ssid="10">By using a bootstrap ping system, such as STEP, that follows the links between the words in WordNet to find similarwords, we can identify the paths connecting mem bers of a given semantic category in the dictionary.</S>
			<S sid="83" ssid="11">With multiple bootstrapping runs on different seed 4It is consistent with the observation by Kim and Hovy (2004) who noticed that, when positives and neutrals were collapsed into the same category opposed to negatives, the agreement between human annotators rose by 12%.</S>
			<S sid="84" ssid="12">5The operationalizations of centrality derived from thenumber of connections between elements can be found in so cial network theory (Burt, 1980) 212lists, we can then produce a measure of the density of such ties.</S>
			<S sid="85" ssid="13">The ambiguity measure de rived from inter-annotator disagreement can then be used to validate the results obtained from the density-based method of determining centrality.</S>
			<S sid="86" ssid="14">In order to produce a centrality measure, we conducted multiple runs with non-intersecting seed lists drawn from HM.</S>
			<S sid="87" ssid="15">The lists of wordsfetched by STEP on different runs partially over lapped, suggesting that the words identified by the system many times as bearing positive or negativesentiment are more central to the respective cate gories.</S>
			<S sid="88" ssid="16">The number of times the word has been fetched by STEP runs is reflected in the Gross Overlap Measure produced by the system.</S>
			<S sid="89" ssid="17">Insome cases, there was a disagreement between dif ferent runs on the sentiment assigned to the word.Such disagreements were addressed by comput ing the Net Overlap Scores for each of the found words: the total number of runs assigning the worda negative sentiment was subtracted from the to tal of the runs that consider it positive.</S>
			<S sid="90" ssid="18">Thus, the greater the number of runs fetching the word (i.e.,Gross Overlap) and the greater the agreement be tween these runs on the assigned sentiment, the higher the Net Overlap Score of this word.The Net Overlap scores obtained for each iden tified word were then used to stratify these wordsinto groups that reflect positive or negative dis tance of these words from the zero score.</S>
			<S sid="91" ssid="19">The zero score was assigned to (a) the WordNet adjectivesthat were not identified by STEP as bearing posi tive or negative sentiment 6 and to (b) the words with equal number of positive and negative hits on several STEP runs.</S>
			<S sid="92" ssid="20">The performance measuresfor each of the groups were then computed to al low the comparison of STEP and human annotator performance on the words from the core and from the periphery of the sentiment category.</S>
			<S sid="93" ssid="21">Thus, foreach of the Net Overlap Score groups, both automatic (STEP) and manual (HM) sentiment annota tions were compared to human-annotated GI-H4,which was used as a gold standard in this experi ment.</S>
			<S sid="94" ssid="22">On 58 runs, the system has identified 3, 908English adjectives as positive, 3, 905 as nega tive, while the remainder (14, 428) of WordNet?s 22, 141 adjectives was deemed neutral.</S>
			<S sid="95" ssid="23">Of these 14, 328 adjectives that STEP runs deemed neutral,6The seed lists fed into STEP contained positive or neg ative, but no neutral words, since HM, which was used as a source for these seed lists, does not include any neutrals.</S>
			<S sid="96" ssid="24">Figure 1: Accuracy of word sentiment tagging.</S>
			<S sid="97" ssid="25">884 were also found in GI-H4 and/or HM lists, which allowed us to evaluate STEP performance and HM-GI agreement on the subset of neutrals as well.</S>
			<S sid="98" ssid="26">The graph in Figure 1 shows the distributionof adjectives by Net Overlap scores and the aver age accuracy/agreement rate for each group.Figure 1 shows that the greater the Net Over lap Score, and hence, the greater the distance of the word from the neutral subcategory (i.e., from zero), the more accurate are STEP results and thegreater is the agreement between two teams of hu man annotators (HM and GI-H4).</S>
			<S sid="99" ssid="27">On average, for all categories, including neutrals, the accuracy of STEP vs. GI-H4 was 66.5%, human-annotated HM had 78.7% accuracy vs. GI-H4.</S>
			<S sid="100" ssid="28">For the words with Net Overlap of ?7 and greater, both STEPand HM had accuracy around 90%.</S>
			<S sid="101" ssid="29">The accu racy declined dramatically as Net Overlap scores approached zero (= Neutrals).</S>
			<S sid="102" ssid="30">In this category,human-annotated HM showed only 20% agree ment with GI-H4, while STEP, which deemedthese words neutral, rather than positive or neg ative, performed with 57% accuracy.</S>
			<S sid="103" ssid="31">These results suggest that the two measures ofword centrality, Net Overlap Score based on mul tiple STEP runs and the inter-annotator agreement (HM vs. GI-H4), are directly related 7.</S>
			<S sid="104" ssid="32">Thus, the Net Overlap Score can serve as a useful tool in the identification of core and peripheral membersof a fuzzy lexical category, as well as in predic 7In our sample, the coefficient of correlation between thetwo was 0.68.</S>
			<S sid="105" ssid="33">The Absolute Net Overlap Score on the sub groups 0 to 10 was used in calculation of the coefficient of correlation.</S>
			<S sid="106" ssid="34">213tion of inter-annotator agreement and system per formance on a subgroup of words characterized by a given Net Overlap Score value.</S>
			<S sid="107" ssid="35">In order to make the Net Overlap Score measure usable in sentiment tagging of texts and phrases,the absolute values of this score should be normalized and mapped onto a standard [0, 1] inter val.</S>
			<S sid="108" ssid="36">Since the values of the Net Overlap Score may vary depending on the number of runs used inthe experiment, such mapping eliminates the vari ability in the score values introduced with changesin the number of runs performed.</S>
			<S sid="109" ssid="37">In order to ac complish this normalization, we used the value ofthe Net Overlap Score as a parameter in the stan dard fuzzy membership S-function (Zadeh, 1975; Zadeh, 1987).</S>
			<S sid="110" ssid="38">This function maps the absolute values of the Net Overlap Score onto the interval from 0 to 1, where 0 corresponds to the absence of membership in the category of sentiment (in our case, these will be the neutral words) and 1 reflects the highest degree of membership in this category.</S>
			<S sid="111" ssid="39">The function can be defined as follows: S(u;?, ?, ?) = ? ?</S>
			<S sid="112" ssid="40">0 for u ? ?</S>
			<S sid="113" ssid="41">2(u??</S>
			<S sid="114" ssid="42">)2 for?</S>
			<S sid="115" ssid="43">u ? ?</S>
			<S sid="116" ssid="44">1?</S>
			<S sid="117" ssid="45">2(u??</S>
			<S sid="118" ssid="46">)2 for ? ?</S>
			<S sid="119" ssid="47">u ? ?</S>
			<S sid="120" ssid="48">1 for u ? ?</S>
			<S sid="121" ssid="49">where u is the Net Overlap Score for the word and ?, ?, ? are the three adjustable parameters: ? is set to 1, ? is set to 15 and ?, which represents a crossover point, is defined as ? = (?</S>
			<S sid="122" ssid="50">+ ?)/2 = 8.</S>
			<S sid="123" ssid="51">Defined this way, the S-function assigns highest degree of membership (=1) to words that have the the Net Overlap Score u ? 15.</S>
			<S sid="124" ssid="52">The accuracy vs. GI-H4 on this subset is 100%.</S>
			<S sid="125" ssid="53">The accuracy goes down as the degree of membership decreases and reaches 59% for values with the lowest degrees of membership.</S>
	</SECTION>
	<SECTION title="Discussion and conclusions. " number="5">
			<S sid="126" ssid="1">This paper contributes to the development of NLP and semantic tagging systems in several respects.</S>
			<S sid="127" ssid="2">The structure of the semantic category of sentiment.</S>
			<S sid="128" ssid="3">The analysis of the category of sentiment of English adjectives presented here suggests that this category is structured as a fuzzy set: the distance from the coreof the category, as measured by Net Over lap scores derived from multiple STEP runs,is shown to affect both the level of interannotator agreement and the system perfor mance vs. human-annotated gold standard.</S>
			<S sid="129" ssid="4">The list of sentiment-bearing adjectives.</S>
			<S sid="130" ssid="5">The list produced and cross-validated by multipleSTEP runs contains 7, 814 positive and negative English adjectives, with an average ac curacy of 66.5%, while the human-annotated list HM performed at 78.7% accuracy vs. the gold standard (GI-H4) 8.</S>
			<S sid="131" ssid="6">The remaining14, 328 adjectives were not identified as sen timent marked and therefore were considered neutral.</S>
			<S sid="132" ssid="7">The stratification of adjectives by their Net Overlap Score can serve as an indicatorof their degree of membership in the cate gory of (positive/negative) sentiment.</S>
			<S sid="133" ssid="8">Since low degrees of membership are associated with greater ambiguity and inter-annotator disagreement, the Net Overlap Score valuecan provide researchers with a set of vol ume/accuracy trade-offs.</S>
			<S sid="134" ssid="9">For example, by including only the adjectives with the Net Overlap Score of 4 and more, the researchercan obtain a list of 1, 828 positive and negative adjectives with accuracy of 81% vs. GI H4, or 3, 124 adjectives with 75% accuracy if the threshold is set at 3.</S>
			<S sid="135" ssid="10">The normalization of the Net Overlap Score values for the use inphrase and text-level sentiment tagging systems was achieved using the fuzzy member ship function that we proposed here for the category of sentiment of English adjectives.</S>
			<S sid="136" ssid="11">Future work in the direction laid out by thisstudy will concentrate on two aspects of sys tem development.</S>
			<S sid="137" ssid="12">First further incremental improvements to the precision of the STEPalgorithm will be made to increase the ac curacy of sentiment annotation through the use of adjective-noun combinatorial patterns within glosses.</S>
			<S sid="138" ssid="13">Second, the resulting list of adjectives annotated with sentiment and withthe degree of word membership in the cate gory (as measured by the Net Overlap Score) will be used in sentiment tagging of phrases and texts.</S>
			<S sid="139" ssid="14">This will enable us to compute the degree of importance of sentiment markers found in phrases and texts.</S>
			<S sid="140" ssid="15">The availability 8GI-H4 contains 1268 and HM list has 1336 positive andnegative adjectives.</S>
			<S sid="141" ssid="16">The accuracy figures reported here in clude the errors produced at the boundary with neutrals.</S>
			<S sid="142" ssid="17">214of the information on the degree of central ity of words to the category of sentiment mayimprove the performance of sentiment determination systems built to identify the senti ment of entire phrases or texts.?</S>
			<S sid="143" ssid="18">System evaluation considerations.</S>
			<S sid="144" ssid="19">The con tribution of this paper to the developmentof methodology of system evaluation is twofold.</S>
			<S sid="145" ssid="20">First, this research emphasizes the im portance of multiple runs on different seedlists for a more accurate evaluation of senti ment tag extraction system performance.</S>
			<S sid="146" ssid="21">Wehave shown how significantly the system re sults vary, depending on the composition of the seed list.Second, due to the high cost of manual an notation and other practical considerations, most bootstrapping and other NLP systems are evaluated on relatively small manually annotated gold standards developed for agiven semantic category.</S>
			<S sid="147" ssid="22">The implied assumption is that such a gold standard represents a random sample drawn from the pop ulation of all category members and hence, system performance observed on this goldstandard can be projected to the whole se mantic category.</S>
			<S sid="148" ssid="23">Such extrapolation is notjustified if the category is structured as a lex ical field with fuzzy boundaries: in this casethe precision of both machine and human annotation is expected to fall when more peripheral members of the category are pro cessed.</S>
			<S sid="149" ssid="24">In this paper, the sentiment-bearing words identified by the system were stratifiedbased on their Net Overlap Score and evaluated in terms of accuracy of sentiment an notation within each stratum.</S>
			<S sid="150" ssid="25">These strata, derived from Net Overlap scores, reflect the degree of centrality of a given word to the semantic category, and, thus, provide greater assurance that system performance on other words with the same Net Overlap Score will be similar to the performance observed on the intersection of system results with the gold standard.?</S>
			<S sid="151" ssid="26">The role of the inter-annotator disagree ment.</S>
			<S sid="152" ssid="27">The results of the study presented in this paper call for reconsideration of the roleof inter-annotator disagreement in the devel opment of lists of words manually annotated with semantic tags.</S>
			<S sid="153" ssid="28">It has been shown here that the inter-annotator agreement tends to fall as we proceed from the core of a fuzzysemantic category to its periphery.</S>
			<S sid="154" ssid="29">Therefore, the disagreement between the annota tors does not necessarily reflect a quality problem in human annotation, but rather a structural property of the semantic category.This suggests that inter-annotator disagree ment rates can serve as an important source of empirical information about the structural properties of the semantic category and canhelp define and validate fuzzy sets of seman tic category members for a number of NLP tasks and applications.</S>
	</SECTION>
</PAPER>
