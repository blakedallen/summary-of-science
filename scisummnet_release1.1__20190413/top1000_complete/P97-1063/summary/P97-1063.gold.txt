A Word-To-Word Model Of Translational Equivalence
Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model.
For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level. The model's precision/recall trade-off can be directly controlled via one threshold parameter.
This feature makes the model more suitable for applications that are not fully statistical.
The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as part-of-speech, dictionaries, word order, etc..
Our model can link word tokens in parallel texts as well as other translation models in the literature.
Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy.
We propose the Competitive Linking Algorithm for linking word pairs and a method which calculates the optimized correspondence level between the word pairs by hill climbing.
One problem that arises in word-to-word alignment is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation.
