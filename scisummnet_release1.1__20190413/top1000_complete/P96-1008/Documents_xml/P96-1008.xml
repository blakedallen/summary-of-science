<PAPER>
  <S sid="0">A Fully Statistical Approach To Natural Language Interfaces</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present a natural language interface system which is based entirely on trained statistical models.</S>
    <S sid="2" ssid="2">The system consists of three stages of processing: parsing, semantic interpretation, and discourse.</S>
    <S sid="3" ssid="3">Each of these stages is modeled as a statistical process.</S>
    <S sid="4" ssid="4">The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.</S>
  </ABSTRACT>
  <SECTION title="1." number="1">
    <S sid="5" ssid="1">A recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs (Church 1988), and continuing with other work using statistical part-of-speech tagging programs, such as BBN PLUM (Weischedel et al. 1993) and NYU Proteus (Grishman and Sterling 1993).</S>
    <S sid="6" ssid="2">More recently, statistical methods have been applied to domain-specific semantic parsing (Miller et al. 1994), and to the more difficult problem of wide-coverage syntactic parsing (Magerman 1995).</S>
    <S sid="7" ssid="3">Nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as AT&amp;T Chronus (Levin and Pieraccini 1995), continue to require a significant rule based component.</S>
    <S sid="8" ssid="4">Development of a complete end-to-end statistical understanding system has been the focus of several ongoing research efforts, including (Miller et al. 1995) and (Koppelman et al.</S>
    <S sid="9" ssid="5">1995).</S>
    <S sid="10" ssid="6">In this paper, we present such a system.</S>
    <S sid="11" ssid="7">The overall structure of our approach is conventional, consisting of a parser, a semantic interpreter, and a discourse module.</S>
    <S sid="12" ssid="8">The implementation and integration of these elements is far less conventional.</S>
    <S sid="13" ssid="9">Within each module, every processing step is assigned a probability value, and very large numbers of alternative theories are pursued in parallel.</S>
    <S sid="14" ssid="10">The individual modules are integrated through an n-best paradigm, in which many theories are passed from one stage to the next, together with their associated probability scores.</S>
    <S sid="15" ssid="11">The meaning of a sentence is determined by taking the highest scoring theory from among the n-best possibilities produced by the final stage in the model.</S>
    <S sid="16" ssid="12">Some key advantages to statistical modeling techniques are: sufficient to provide the system with examples specifying the correct parses for a set of training examples.</S>
    <S sid="17" ssid="13">There is no need to specify an exact set of rules or a detailed procedure for producing such parses. are principled techniques for estimating the gradations.</S>
    <S sid="18" ssid="14">The system is thus free to pursue unusual theories, while remaining aware of the fact that they are unlikely.</S>
    <S sid="19" ssid="15">In the event that a more likely theory exists, then the more likely theory is selected, but if no more likely interpretation can be found, the unlikely interpretation is accepted.</S>
    <S sid="20" ssid="16">The focus of this work is primarily to extract sufficient information from each utterance to give an appropriate response to a user's request.</S>
    <S sid="21" ssid="17">A variety of problems regarded as standard in computational linguistics, such as quantification, reference and the like, are thus ignored.</S>
    <S sid="22" ssid="18">To evaluate our approach, we trained an experimental system using data from the Air Travel Information (ATIS) domain (Bates et al. 1990; Price 1990).</S>
    <S sid="23" ssid="19">The selection of ATIS was motivated by three concerns.</S>
    <S sid="24" ssid="20">First, a large corpus of ATIS sentences already exists and is readily available.</S>
    <S sid="25" ssid="21">Second, ATIS provides an existing evaluation methodology, complete with independent training and test corpora, and scoring programs.</S>
    <S sid="26" ssid="22">Finally, evaluating on a common corpus makes it easy to compare the performance of the system with those based on different approaches.</S>
    <S sid="27" ssid="23">We have evaluated our system on the same blind test sets used in the ARPA evaluations (Pallett et al. 1995), and present a preliminary result at the conclusion of this paper.</S>
    <S sid="28" ssid="24">The remainder of the paper is divided into four sections, one describing the overall structure of our models, and one for each of the three major components of parsing, semantic interpretation and discourse.</S>
  </SECTION>
  <SECTION title="2." number="2">
    <S sid="29" ssid="1">Given a string of input words W and a discourse history H, the task of a statistical language understanding system is to search among the many possible discourse-dependent meanings MD for the most likely meaning Mo: Mo = arg max P(MD I W, H).</S>
    <S sid="30" ssid="2">Directly modeling P(MD I W, H) is difficult because the gap that the model must span is large.</S>
    <S sid="31" ssid="3">A common approach in non-statistical natural language systems is to bridge this gap by introducing intermediate representations such as parse structure and pre-discourse sentence meaning.</S>
    <S sid="32" ssid="4">Introducing these intermediate levels into the statistical framework gives: where T denotes a semantic parse tree, and Ms denotes prediscourse sentence meaning.</S>
    <S sid="33" ssid="5">This expression can be simplified by introducing two independence assumptions: Now, since P(W) is constant for any given word string, the problem of finding meaning MD that maximizes We now introduce a third independence assumption: 3.</S>
    <S sid="34" ssid="6">The probability of words W does not depend on meaning Ms, given that parse T is known.</S>
    <S sid="35" ssid="7">This assumption is justified because the word tags in our parse representation specify both semantic and syntactic class information.</S>
    <S sid="36" ssid="8">Under this assumption: Finally, we assume that most of the probability mass for each discourse-dependent meaning is focused on a single parse tree and on a single pre-discourse meaning.</S>
    <S sid="37" ssid="9">Under this (Viterbi) assumption, the summation operator can be replaced by the maximization operator, yielding: This expression corresponds to the computation actually performed by our system which is shown in Figure 1.</S>
    <S sid="38" ssid="10">Processing proceeds in three stages: These parses, together with their probability scores, are passed to the semantic interpretation model.</S>
    <S sid="39" ssid="11">2.</S>
    <S sid="40" ssid="12">The constrained space of candidate parses T (received from the parsing model), combined with the full space of possible pre-discourse meanings Ms, is searched for n-best candidates according to the measure P(Ms,T) P(W IT).</S>
    <S sid="41" ssid="13">These pre-discourse meanings, together with their associated probability scores, are passed to the discourse model.</S>
    <S sid="42" ssid="14">3.</S>
    <S sid="43" ssid="15">The constrained space of candidate pre-discourse meanings Ms (received from the semantic interpretation model), combined with the full space of possible postdiscourse meanings MD, is searched for the single candidate that maximizes P( MD I H,M s) P(Ms,T) P(W IT) ,conditioned on the current history H. The discourse history is then updated and the post-discourse meaning is returned.</S>
    <S sid="44" ssid="16">We now proceed to a detailed discussion of each of these three stages, beginning with parsing.</S>
  </SECTION>
  <SECTION title="3." number="3">
    <S sid="45" ssid="1">Our parse representation is essentially syntactic in form, patterned on a simplified head-centered theory of phrase structure.</S>
    <S sid="46" ssid="2">In content, however, the parse trees are as much semantic as syntactic.</S>
    <S sid="47" ssid="3">Specifically, each parse node indicates both a semantic and a syntactic class (excepting a few types that serve purely syntactic functions).</S>
    <S sid="48" ssid="4">Figure 2 shows a sample parse of a typical ATIS sentence.</S>
    <S sid="49" ssid="5">The semantic,/syntactic character of this representation offers several advantages: /top&#176; /wh-question process: semantic labels identify the basic units of meaning, while syntactic structures help identify relationships between those units.</S>
    <S sid="50" ssid="6">The parsing model is a probabilistic recursive transition network similar to those described in (Miller et al. 1994) and (Seneff 1992).</S>
    <S sid="51" ssid="7">The probability of a parse tree T given a word string W is rewritten using Bayes rule as: Since P(W) is constant for any given word string, candidate parses can be ranked by considering only the product P(T) P(W I 7).</S>
    <S sid="52" ssid="8">The probability P(7) is modeled by state transition probabilities in the recursive transition network, and P(W I 7) is modeled by word transition probabilities.</S>
    <S sid="53" ssid="9">P(location/pp I arrivaL/vp-head, arrival/vp) is the probability of a location/pp following an arrivaMvphead within an arrivallvp constituent. probability along the path corresponding to T. Transition probabilities are estimated directly by observing occurrence and transition frequencies in a training corpus of annotated parse trees.</S>
    <S sid="54" ssid="10">These estimates are then smoothed to overcome sparse data limitations.</S>
    <S sid="55" ssid="11">The semantic/syntactic parse labels, described above, provide a further advantage in terms of smoothing: for cases of undertrained probability estimates, the model backs off to independent syntactic and semantic probabilities as follows: where A. is estimated as in (Placeway et al. 1993).</S>
    <S sid="56" ssid="12">Backing off to independent semantic and syntactic probabilities potentially provides more precise estimates than the usual strategy of backing off directly form bigram to unigram models.</S>
    <S sid="57" ssid="13">In order to explore the space of possible parses efficiently, the parsing model is searched using a decoder based on an adaptation of the Earley parsing algorithm (Earley 1970).</S>
    <S sid="58" ssid="14">This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules.</S>
    <S sid="59" ssid="15">For details of the decoder, see (Miller 1996).</S>
  </SECTION>
  <SECTION title="4." number="4">
    <S sid="60" ssid="1">Both pre-discourse and post-discourse meanings in our current system are represented using a simple frame representation.</S>
    <S sid="61" ssid="2">Figure 3 shows a sample semantic frame corresponding to the parse in Figure 2.</S>
    <S sid="62" ssid="3">Recall that the semantic interpreter is required to compute P(M s ,T) P(W IT) .</S>
    <S sid="63" ssid="4">The conditional word probability P(W IT) has already been computed during the parsing phase and need not be recomputed.</S>
    <S sid="64" ssid="5">The current problem, then, is to compute the prior probability of meaning Ms and parse T occurring together.</S>
    <S sid="65" ssid="6">Our strategy is to embed the instructions for constructing Ms directly into parse T , resulting in an augmented tree structure.</S>
    <S sid="66" ssid="7">For example, the instructions needed to create the frame shown in Figure 3 are: These instructions are attached to the parse tree at the points indicated by the circled numbers (see Meanings Ms are decomposed into two parts: the frame type FT, and the slot fillers S. The frame type is always attached to the topmost node in the augmented parse tree, while the slot filling instructions are attached to nodes lower down in the tree.</S>
    <S sid="67" ssid="8">Except for the topmost node, all parse nodes are required to have some slot filling operation.</S>
    <S sid="68" ssid="9">For nodes that do not directly trigger any slot fill operation, the special operation null is attached.</S>
    <S sid="69" ssid="10">The probability P(Ms, 7) is then: Obviously, the prior probabilities P(FT) can be obtained directly from the training data.</S>
    <S sid="70" ssid="11">To compute P(T I FT), each of the state transitions from the previous parsing model are simply rescored conditioned on the frame type.</S>
    <S sid="71" ssid="12">The new state transition probabilities are: To compute P(S I FT, 7) , we make the independence assumption that slot filling operations depend only on the frame type, the slot operations already performed, and on the local parse structure around the operation.</S>
    <S sid="72" ssid="13">This local neighborhood consists of the parse node itself, its two left siblings, its two right siblings, and its four immediate ancestors.</S>
    <S sid="73" ssid="14">Further, the syntactic and semantic components of these nodes are considered independently.</S>
    <S sid="74" ssid="15">Under these assumptions, the probability of a slot fill operation is: and the probability P(S I FT, 7) is simply the product of all such slot fill operations in the augmented tree.</S>
    <S sid="75" ssid="16">Transition probabilities are estimated from a training corpus of augmented trees.</S>
    <S sid="76" ssid="17">Unlike probabilities in the parsing model, there obviously is not sufficient training data to estimate slot fill probabilities directly.</S>
    <S sid="77" ssid="18">Instead, these probabilities are estimated by statistical decision trees similar to those used in the Spatter parser (Magerman 1995).</S>
    <S sid="78" ssid="19">Unlike more common decision tree classifiers, which simply classify sets of conditions, statistical decision trees give a probability distribution over all possible outcomes.</S>
    <S sid="79" ssid="20">Statistical decision trees are constructed in a two phase process.</S>
    <S sid="80" ssid="21">In the first phase, a decision tree is constructed in the standard fashion using entropy reduction to guide the construction process.</S>
    <S sid="81" ssid="22">This phase is the same as for classifier models, and the distributions at the leaves are often extremely sharp, sometimes consisting of one outcome with probability 1, and all others with probability 0.</S>
    <S sid="82" ssid="23">In the second phase, these distributions are smoothed by mixing together distributions of various nodes in the decision tree.</S>
    <S sid="83" ssid="24">As in (Magerman 1995), mixture weights are determined by deleted interpolation on a separate block of training data.</S>
    <S sid="84" ssid="25">Searching the interpretation model proceeds in two phases.</S>
    <S sid="85" ssid="26">In the first phase, every parse T received from the parsing model is rescored for every possible frame type, computing P(T I F7) (our current model includes only a half dozen different types, so this computation is tractable).</S>
    <S sid="86" ssid="27">Each of these theories is combined with the corresponding prior probability P(FT) yielding P(FT) P(T I PT).</S>
    <S sid="87" ssid="28">The n-best of these theories are then passed to the second phase of the interpretation process.</S>
    <S sid="88" ssid="29">This phase searches the space of slot filling operations using a simple beam search procedure.</S>
    <S sid="89" ssid="30">For each combination of FT and T, the beam search procedure considers all possible combinations of fill operations, while pruning partial theories that fall beneath the threshold imposed by the beam limit.</S>
    <S sid="90" ssid="31">The surviving theories are then combined with the conditional word probabilities P(W I 7), computed during the parsing model.</S>
    <S sid="91" ssid="32">The final result of these steps is the n-best set of candidate pre-discourse meanings, scored according to the measure P(Ms,T) P(W IT) .</S>
  </SECTION>
  <SECTION title="5." number="5">
    <S sid="92" ssid="1">The discourse module computes the most probable postdiscourse meaning of an utterance from its pre-discourse meaning and the discourse history, according to the measure: Because pronouns can usually be ignored in the ATIS domain, our work does not treat the problem of pronominal reference.</S>
    <S sid="93" ssid="2">Our probability model is instead shaped by the key discourse problem of the ATIS domain, which is the inheritance of constraints from context.</S>
    <S sid="94" ssid="3">This inheritance phenomenon, similar in spirit to one-anaphora, is illustrated by the following dialog:: SYSTEM2: &lt;displays Boston to Denver flights for Tuesday&gt; In USER2, it is obvious from context that the user is asking about flights whose ORIGIN is BOSTON and whose DESTINATION is DENVER, and not all flights between any two cities.</S>
    <S sid="95" ssid="4">Constraints are not always inherited, however.</S>
    <S sid="96" ssid="5">For example, in the following continuation of this dialogue: USER3: Show me return flights from Denver to Boston, it is intuitively much less likely that the user means the &amp;quot;on Tuesday&amp;quot; constraint to continue to apply.</S>
    <S sid="97" ssid="6">The discourse history H simply consists of the list of all postdiscourse frame representations for all previous utterances in the current session with the system.</S>
    <S sid="98" ssid="7">These frames are the source of candidate constraints to be inherited.</S>
    <S sid="99" ssid="8">For most utterances, we make the simplifying assumption that we need only look at the last (i.e. most recent) frame in this list, which we call M. The statistical discourse model maps a 23 element input vector X onto a 23 element output vector Y.</S>
    <S sid="100" ssid="9">These vectors have the following interpretations: The 23 elements in vectors X and Y correspond to the 23 possible slots in the frame schema.</S>
    <S sid="101" ssid="10">Each element in X can have one of five values, specifying the relationship between the filler of the corresponding slot in Mp and Ms: INITIAL - slot filled in Ms but not in Mp TACIT - slot filled in Mp but not in Ms REITERATE - slot filled in both MA and Ms; value the same CHANGE - slot filled in both Mp and Ms; value different IRRELEVANT - slot not filled in either Mp or Ms Output vector Y is constructed by directly copying all fields from input vector X except those labeled TACIT.</S>
    <S sid="102" ssid="11">These direct copying operations are assigned probability 1.</S>
    <S sid="103" ssid="12">For fields labeled TACIT, the corresponding field in Y is filled with either INHERITED or NOT-INHERITED.</S>
    <S sid="104" ssid="13">The probability of each of these operations is determined by a statistical decision tree model.</S>
    <S sid="105" ssid="14">The discourse model contains 23 such statistical decision trees, one for each slot position.</S>
    <S sid="106" ssid="15">An ordering is imposed on the set of frame slots, such that inheritance decisions for slots higher in the order are conditioned on the decisions for slots lower in the order.</S>
    <S sid="107" ssid="16">The probability P(Y I X) is then the product of all 23 statistical models to additional linguistic phenomena such as quantification and anaphora resolution. decision probabilities: The discourse model is trained from a corpus annotated with both pre-discourse and post-discourse semantic frames.</S>
    <S sid="108" ssid="17">Corresponding pairs of input and output (X, Y) vectors are computed from these annotations, which are then used to train the 23 statistical decision trees.</S>
    <S sid="109" ssid="18">The training procedure for estimating these decision tree models is similar to that used for training the semantic interpretation model.</S>
    <S sid="110" ssid="19">Searching the discourse model begins by selecting a meaning frame Mp from the history stack H, and combining it with each pre-discourse meaning Ms received from the semantic interpretation model.</S>
    <S sid="111" ssid="20">This process yields a set of candidate input vectors X.</S>
    <S sid="112" ssid="21">Then, for each vector X, a search process exhaustively constructs and scores all possible output vectors Y according to the measure P(Y I X) (this computation is feasible because the number of TACIT fields is normally small).</S>
    <S sid="113" ssid="22">These scores are combined with the pre-discourse scores P(Ms,T) P(W IT) , already computed by the semantic interpretation process.</S>
    <S sid="114" ssid="23">This computation yields: The highest scoring theory is then selected, and a straightforward computation derives the final meaning frame MD from output vector Y.</S>
  </SECTION>
  <SECTION title="6." number="6">
    <S sid="115" ssid="1">We have trained and evaluated the system on a common corpus of utterances collected from naive users in the ATIS domain.</S>
    <S sid="116" ssid="2">In this test, the system was trained on approximately 4000 ATIS 2 and ATIS 3 sentences, and then evaluated on the December 1994 test material (which was held aside as a blind test set).</S>
    <S sid="117" ssid="3">The combined system produced an error rate of 21.6%.</S>
    <S sid="118" ssid="4">Work on the system is ongoing, however, and interested parties are encouraged to contact the authors for more recent results.</S>
  </SECTION>
  <SECTION title="7." number="7">
    <S sid="119" ssid="1">We have presented a fully trained statistical natural language interface system, with separate models corresponding to the classical processing steps of parsing, semantic interpretation and discourse.</S>
    <S sid="120" ssid="2">Much work remains to be done in order to refine the statistical modeling techniques, and to extend the</S>
  </SECTION>
  <SECTION title="8." number="8">
    <S sid="121" ssid="1">We wish to thank Robert Ingria for his effort in supervising the annotation of the training corpus, and for his helpful technical suggestions.</S>
    <S sid="122" ssid="2">This work was supported by the Advanced Research Projects Agency and monitored by the Office of Naval Research under Contract No.</S>
    <S sid="123" ssid="3">NO0014-91-C-0115, and by Ft. Huachuca under Contract Nos.</S>
    <S sid="124" ssid="4">DABT63-94-C-0061 and DABT63-94C-0063 .</S>
    <S sid="125" ssid="5">The content of the information does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred.</S>
  </SECTION>
</PAPER>
