First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests
Many statistical translation models can be regarded as weighted logical deduction.
Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs).
We then introduce a novel second-order expectation semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy).
This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk.
We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 BLEU point.
We consider minimum risk training using a linearly decomposable approximation of BLEU.
The sufficient statistics for graph expected BLEU can be computed using expectation semirings.
We extend the work of Smith and Eisner and obtain much better estimates of feature expectations by using a packed chart instead of an n-best list.
We perform expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007).
