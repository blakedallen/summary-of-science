<PAPER>
  <S sid="0">Name Tagging With Word Clusters And Discriminative Training</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model.</S>
    <S sid="2" ssid="2">Active learning is used to select training examples.</S>
    <S sid="3" ssid="3">We evaluate the technique for named-entity tagging.</S>
    <S sid="4" ssid="4">Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance.</S>
    <S sid="5" ssid="5">Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">At a recent meeting, we presented name-tagging technology to a potential user.</S>
    <S sid="7" ssid="2">The technology had performed well in formal evaluations, had been applied successfully by several research groups, and required only annotated training examples to configure for new name classes.</S>
    <S sid="8" ssid="3">Nevertheless, it did not meet the user's needs.</S>
    <S sid="9" ssid="4">To achieve reasonable performance, the HMM-based technology we presented required roughly 150,000 words of annotated examples, and over a million words to achieve peak accuracy.</S>
    <S sid="10" ssid="5">Given a typical annotation rate of 5,000 words per hour, we estimated that setting up a name finder for a new problem would take four person days of annotation work &#8211; a period we considered reasonable.</S>
    <S sid="11" ssid="6">However, this user's problems were too dynamic for that much setup time.</S>
    <S sid="12" ssid="7">To be useful, the system would have to be trainable in minutes or hours, not days or weeks.</S>
    <S sid="13" ssid="8">We left the meeting thinking about ways to reduce training requirements to no more than a few hours.</S>
    <S sid="14" ssid="9">It seemed that three existing ideas could be combined in a way that might reduce training requirements sufficiently to achieve the objective.</S>
    <S sid="15" ssid="10">First were techniques for producing word clusters from large unannotated corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999).</S>
    <S sid="16" ssid="11">The resulting clusters appeared to contain a great deal of implicit semantic information.</S>
    <S sid="17" ssid="12">This implicit information, we believed, could serve to augment a small amount of annotated data.</S>
    <S sid="18" ssid="13">Particularly promising were techniques for producing hierarchical clusters at various scales, from small and highly specific to large and more general.</S>
    <S sid="19" ssid="14">To benefit from such information, however, we would need an automatic learning mechanism that could effectively exploit it.</S>
    <S sid="20" ssid="15">Fortunately, a second line of recent research provided a potential solution.</S>
    <S sid="21" ssid="16">Recent work in discriminative methods (Lafferty et al., 2001; Sha and Pereira, 2003, Collins 2002) suggested a framework for exploiting large numbers of arbitrary input features.</S>
    <S sid="22" ssid="17">These methods seemed to have exactly the right characteristics for incorporating the statistically-correlated hierarchical word clusters we wished to exploit.</S>
    <S sid="23" ssid="18">Combining these two methods, we suspected, would be sufficient to drastically reduce the number of annotated examples required.</S>
    <S sid="24" ssid="19">However, we also hoped that a third technique, active learning (Cohn et al., 1996; McCallum and Nigam, 1998), would be particularly effective when used in conjunction with hierarchical word clusters.</S>
    <S sid="25" ssid="20">Specifically, active learning attempts to select examples for annotation by estimating the system's certainty about the answer, requesting a human judgment only for those cases where it is most uncertain.</S>
    <S sid="26" ssid="21">Unfortunately, the issue often comes down to whether a specific word has previously been observed in training: if the system has seen the word, it is certain, if not, it is uncertain.</S>
    <S sid="27" ssid="22">Word clusters at various scales, we hoped, would permit more subtle distinctions to influence the system's certainty, increasing the method&#8217;s effectiveness earlier in the process when fewer training examples have been annotated.</S>
  </SECTION>
  <SECTION title="2 Word Clustering" number="2">
    <S sid="28" ssid="1">We view clustering here as a method for estimating the probabilities of low frequency events, particularly events that are likely to go unobserved in a small annotated training corpus.</S>
    <S sid="29" ssid="2">For example, a clustering mechanism may choose to place AT&amp;T in the same cluster as other company names based on contextual similarity.</S>
    <S sid="30" ssid="3">Then, even if the word AT&amp;T was not previously annotated as a company, it may nonetheless be possible to infer that AT&amp;T indeed is a company because it occupies a cluster that is populated mostly by other company names.</S>
    <S sid="31" ssid="4">Likewise, cluster membership can be used to exploit information from neighboring words.</S>
    <S sid="32" ssid="5">For example, if the word reported has previously been observed to follow person names, but the word announced has not yet been seen, it may be possible to guess that the word preceding announced is a person based on the fact that reported and announced occupy the same cluster.</S>
    <S sid="33" ssid="6">A practical obstacle to using clusters for this purpose is selecting an appropriate level of granularity: too small, and the clusters provide insufficient generalization; too large, and they inappropriately overgeneralize.</S>
    <S sid="34" ssid="7">Hierarchical clusters provide one way around the problem by avoiding commitment to any particular granularity in advance.</S>
    <S sid="35" ssid="8">However, the dominant trend during the past decade toward generative models has made integration of such hierarchical clusters difficult.</S>
    <S sid="36" ssid="9">Because the nested clusters surrounding each word are highly correlated, it is unreasonable to treat them as independent.</S>
    <S sid="37" ssid="10">Unfortunately, any treatment in a generative framework other than independent requires considerable ingenuity.</S>
    <S sid="38" ssid="11">Interestingly, before generative models began to dominate parsing, the Spatter parser (Magerman, 1995) achieved extremely promising results using a nongenerative statistical model.</S>
    <S sid="39" ssid="12">Of particular interest is the fact that Spatter used hierarchical word clusters for estimating its lexical attachment probabilities.</S>
    <S sid="40" ssid="13">However, the statistical decision trees underlying Spatter&#8217;s probability model never gained widespread acceptance, and indeed, our own limited experience with them yielded mixed results.</S>
    <S sid="41" ssid="14">In the past few years, researchers have begun to view generative models as instances of a broader class of linear (or log-linear) models, and have introduced discriminative methods (e.g. conditional random fields) to estimate the model parameters.</S>
    <S sid="42" ssid="15">These estimation methods do not impose the same strict independence conditions as generative models.</S>
    <S sid="43" ssid="16">Armed with modern discriminative training methods, it seemed reasonable to us to revisit hierarchical clustering.</S>
    <S sid="44" ssid="17">Specifically, we picked up where Spatter left off, with the clustering algorithm of (Brown et al., 1990).</S>
    <S sid="45" ssid="18">We implemented this algorithm twice as part of our work.</S>
    <S sid="46" ssid="19">The first implementation derived directly from the description given in the Brown paper.</S>
    <S sid="47" ssid="20">Then, in the hope of achieving greater efficiency, we reverseengineered the clustering software in Spatter.</S>
    <S sid="48" ssid="21">While the mathematical details differ slightly between the two algorithms, both aim to cluster together words so as to minimize the bigram language-model perplexity of the unsupervised corpus.</S>
    <S sid="49" ssid="22">In practice, we observed no significant differences in accuracy when using one or the other in our experiments.</S>
    <S sid="50" ssid="23">All experimental results given in this paper are with the Spatter clustering algorithm.</S>
    <S sid="51" ssid="24">The result of running the clustering algorithm is a binary tree, where each word occupies a single leaf node, and where each leaf node contains a single word.</S>
    <S sid="52" ssid="25">The root node defines a cluster containing the entire vocabulary.</S>
    <S sid="53" ssid="26">Interior nodes represent intermediate size clusters containing all of the words that they dominate.</S>
    <S sid="54" ssid="27">Thus, nodes higher in the tree correspond to larger word clusters, while lower nodes correspond to smaller clusters.</S>
    <S sid="55" ssid="28">A particular word can be assigned a binary string by following the traversal path from the root to its leaf, assigning a 0 for each left branch, and a 1 for each right branch.</S>
    <S sid="56" ssid="29">The following are example bit strings from the Spatter clustering algorithm:</S>
  </SECTION>
  <SECTION title="3 Discriminative Name Tagger" number="3">
    <S sid="57" ssid="1">To implement discriminative training, we followed the averaged perceptron approach of (Collins, 2002).</S>
    <S sid="58" ssid="2">Our decision was based on three criteria.</S>
    <S sid="59" ssid="3">First, the method performed nearly as well as the currently best global discriminative model (Sha and Pereira, 2003), as evaluated on one of the few tasks for which there are any published results (noun phrase chunking).</S>
    <S sid="60" ssid="4">Second, convergence rates appeared favorable, which would facilitate multiple experiments.</S>
    <S sid="61" ssid="5">Finally, and most important, the method appeared far simpler to implement than any of the alternatives. algorithm exactly as described by Collins.</S>
    <S sid="62" ssid="6">However, we did not implement cross-validation to determine when to stop training.</S>
    <S sid="63" ssid="7">Instead, we simply iterated for 5 epochs in all cases, regardless of the training set size or number of features used.</S>
    <S sid="64" ssid="8">Furthermore, we did not implement features that occurred in no training instances, as was done in (Sha and Pereira, 2003).</S>
    <S sid="65" ssid="9">We suspect that these simplifications may have cost several tenths of a point in performance.</S>
    <S sid="66" ssid="10">A set of 16 tags was used to tag 8 name classes (the seven MUC classes plus the additional null class).</S>
    <S sid="67" ssid="11">Two tags were required per class to account for adjacent elements of the same type.</S>
    <S sid="68" ssid="12">For example, the string Betty Mary and Bobby Lou would be tagged as PERSON-START PERSON-START NULL-START PERSON-START PERSON-CONTINUE.</S>
    <S sid="69" ssid="13">Our model uses a total of 19 classes of features.</S>
    <S sid="70" ssid="14">The first seven of these correspond closely to features used in a typical HMM name tagger.</S>
    <S sid="71" ssid="15">The remaining twelve encode cluster membership.</S>
    <S sid="72" ssid="16">Clusters of various granularity are specified by prefixes of the bit strings.</S>
    <S sid="73" ssid="17">Short prefixes specify short paths from the root node and therefore large clusters.</S>
    <S sid="74" ssid="18">Long prefixes specify long paths and small clusters.</S>
    <S sid="75" ssid="19">We used 4 different prefix lengths: 8 bit, 12 bit, 16 bit, and 20 bit.</S>
    <S sid="76" ssid="20">Thus, the clusters decrease in size by about a factor of 16 at each level.</S>
    <S sid="77" ssid="21">The complete set of features is given in Table 2.</S>
  </SECTION>
  <SECTION title="4 Active Learning" number="4">
    <S sid="78" ssid="1">We implemented the averaged perceptron training We used only a rudimentary confidence measure to perform active learning, introducing no additional features beyond those used in training and decoding.</S>
    <S sid="79" ssid="2">The confidence score we assign to a sentence is just the un-normalized difference in perceptron scores between the highest scoring theory and the second highest scoring alternative.</S>
    <S sid="80" ssid="3">To apply active learning, we simply To compute the confidence scores efficiently, we use a combination of the forward Viterbi and backward Viterbi scores at each word.</S>
    <S sid="81" ssid="4">We define the confidence at a word to be the difference between the summed forward and backward scores of the best and second best tags for that word.</S>
    <S sid="82" ssid="5">The confidence for the entire sentence is then just the minimum of the scores at each word position.</S>
  </SECTION>
  <SECTION title="5 Experimental Results" number="5">
    <S sid="83" ssid="1">We performed our experiments using the seven MUC-6 name categories: person, organization, location, date, time, percent, and monetary amount.</S>
    <S sid="84" ssid="2">For annotated data, we used text from Sections 02-23 of the Wall Street Journal Treebank corpus that had previously been annotated with the MUC name classes.</S>
    <S sid="85" ssid="3">Sections 02-21 were used as training material, and Section 23 was used as test (note that the syntactic trees were not used in any way).</S>
    <S sid="86" ssid="4">Scoring was performed using the MUC scorer.</S>
    <S sid="87" ssid="5">For unsupervised clustering data, we used the Wall Street Journal subset of the Continuous Speech Recognition (CSR-III) collection (LDC catalog # LDC95T6).</S>
    <S sid="88" ssid="6">This portion of the collection contains approximately 100 million words.</S>
    <S sid="89" ssid="7">Active learning experiments were performed by permitting the system to choose examples from among the pool of annotated data, rather than presenting the examples in their natural chronological order.</S>
    <S sid="90" ssid="8">This approach, previously used in [Boschee et al, 2002], permits simulation of human-in-the-loop experiments that are inexpensive to run and repeatable because they don&#8217;t actually involve a human annotator.</S>
    <S sid="91" ssid="9">However, because the pool of pre-annotated examples is limited, the results are most meaningful for small training sets.</S>
    <S sid="92" ssid="10">Once the system has selected the most useful examples from the pool, it is forced to choose among the remainder that it previously rejected as less useful.</S>
    <S sid="93" ssid="11">At the extreme where all available examples are used, our experimental framework prevents active learning from exhibiting any benefit whatsoever since the system is left no choice in selecting examples.</S>
    <S sid="94" ssid="12">Before considering the impact of word clustering on system performance, we first evaluate the discriminative tagger relative to the baseline HMM.</S>
    <S sid="95" ssid="13">For this experiment, we used all of the features described in Section 3 except word cluster features.</S>
    <S sid="96" ssid="14">The remaining features encode essentially the same information used in the HMM, although in a slightly different form.</S>
    <S sid="97" ssid="15">Results are shown in Figure 1.</S>
    <S sid="98" ssid="16">For very small and very large training sets, the systems perform about the same.</S>
    <S sid="99" ssid="17">Between these extremes, the discriminative tagger exhibits somewhat, though not distressingly, worse performance.</S>
    <S sid="100" ssid="18">We conjecture that lack of smoothing in the discriminative tagger may account for the difference.</S>
    <S sid="101" ssid="19">Second, we consider the impact of word clusters.</S>
    <S sid="102" ssid="20">Figure 2 compares performance of the discriminative tagger, now with cluster features included, to the baseline HMM.</S>
    <S sid="103" ssid="21">Immediately, with only 5,000 words of training, the discriminative model significantly outperforms the HMM.</S>
    <S sid="104" ssid="22">With 50,000 words of training, performance for the discriminative model exceeds 90F, a level not reached by the HMM until it has observed 150,000 words of training.</S>
    <S sid="105" ssid="23">Somewhat surprisingly, the clusters continue to provide some benefit even with 1,000,000 words of training.</S>
    <S sid="106" ssid="24">At this operating point, the discriminative tagger achieves an F-score of 96.08 compared to 94.72 for the HMM, a 25% reduction in error.</S>
    <S sid="107" ssid="25">Third, we consider the impact of active learning.</S>
    <S sid="108" ssid="26">Figure 3 shows (a) discriminative tagger performance without cluster features, (b) the same tagger using active learning, (c) the discriminative tagger with cluster features, and (d) the discriminative tagger with cluster features using active learning.</S>
    <S sid="109" ssid="27">Both with and without clusters, active learning exhibits a noticeable increase in learning rates.</S>
    <S sid="110" ssid="28">However, the increase in learning rate is significantly more pronounced when cluster features are introduced.</S>
    <S sid="111" ssid="29">We attribute this increase to better confidence measures provided by word clusters &#8211; the system is no longer restricted to whether or not it knows a word; it now can know something about the clusters to which a word belongs, even if it does not know the word.</S>
    <S sid="112" ssid="30">Finally, Figure 4 shows the impact of consolidating the gains from both cluster features and active learning compared to the baseline HMM.</S>
    <S sid="113" ssid="31">This final combination achieves an F-score of 90 with less than 20,000 words of training &#8211; a quantity that can be annotated in about 4 person hours &#8211; compared to 150,000 words for the HMM &#8211; a quantity requiring nearly 4 person days to annotate.</S>
    <S sid="114" ssid="32">At 1,000,000 word of training, the final combination continues to exhibit a 25% reduction in error over the baseline system (because of limitations in the experimental framework discussed earlier, active learning can provide no additional gain at this operating point).</S>
  </SECTION>
  <SECTION title="6 Discussion" number="6">
    <S sid="115" ssid="1">The work presented here extends a substantial body of previous work (Blum and Mitchell, 1998; Riloff and Jones, 1999; Lin et al., 2003; Boschee et al, 2002; Collins and Singer, 1999; Yarowsky, 1995) that all focuses on reducing annotation requirements through a combination of (a) seed examples, (b) large unannotated corpora, and (c) training example selection.</S>
    <S sid="116" ssid="2">Moreover, our work is based largely on existing techniques for word clustering (Brown et al., 1990), discriminative training (Collins 2002), and active learning.</S>
    <S sid="117" ssid="3">The synthesis of these techniques, nevertheless, proved highly effective in achieving our primary objective of reducing the need for annotated data.</S>
    <S sid="118" ssid="4">Much work remains to be done.</S>
    <S sid="119" ssid="5">In an effort to move rapidly toward our primary objective, we investigated only one type of discriminative training (averaged perceptron), only one type of clustering (bigram mutual information), and only one simple confidence measure for active learning.</S>
    <S sid="120" ssid="6">It seems likely that some additional gains could be realized by alternative discriminative methods (e.g. conditional random fields estimated with conjugate-gradient training).</S>
    <S sid="121" ssid="7">Similarly, alternative clustering techniques, perhaps based on different contextual features or different distance measures, could further improve performance.</S>
    <S sid="122" ssid="8">On the application side, it would be interesting to apply the technique to other language problems.</S>
    <S sid="123" ssid="9">Applying it to parsing would yield a rare sense of closure, knitting together the word clustering of Magerman&#8217;s (1995) Spatter parser &#8211; arguably the first successful broadcoverage statistical parser &#8211; with structural elements of the now-dominant Collins (1997) style parsers.</S>
    <S sid="124" ssid="10">Because our combined method promises to require substantially less training data, it may also prove useful for so-called low-density languages, where limited resources &#8211; and even more limited numbers of native speakers &#8211; are available.</S>
    <S sid="125" ssid="11">For the moment, we find the initial results encouraging.</S>
    <S sid="126" ssid="12">We achieved a 25% reduction in error on a standard named-entity problem, compared to a state-of-the-art HMM.</S>
    <S sid="127" ssid="13">Our main objective, though, was not reducing error rates but rather reducing the amount of annotation required.</S>
    <S sid="128" ssid="14">At least for the named-entity task we studied, using the method described, a single annotator could begin work after breakfast and, by lunchtime, have enough data annotated to achieve an F-score of 90.</S>
  </SECTION>
</PAPER>
