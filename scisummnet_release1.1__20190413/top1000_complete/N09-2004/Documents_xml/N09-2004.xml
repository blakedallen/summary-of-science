<PAPER>
  <S sid="0">Semantic Roles for SMT: A Hybrid Two-Pass Model</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present results on a novel hybrid semantic SMT model that incorporates the strengths of both semantic role labeling and phrase-based statistical machine translation.</S>
    <S sid="2" ssid="2">The approach avoids major complexity limitations via a two-pass architecture.</S>
    <S sid="3" ssid="3">The first pass is performed using a conventional phrase-based SMT model.</S>
    <S sid="4" ssid="4">The second pass is performed by a re-ordering strategy guided by shallow semantic parsers that produce both semantic frame and role labels.</S>
    <S sid="5" ssid="5">Evaluation on a Wall Street Journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in BLEU score over a strong pure phrase-based SMT baseline &#8211; to our knowledge, the first successful application of semantic role labeling to SMT.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Many of the most glaring errors made by today&#8217;s statistical machine translation systems are those resulting from confusion of semantic roles.</S>
    <S sid="7" ssid="2">Translation errors of this type frequently result in critical misunderstandings of the essential meaning of the original input language sentences &#8211; who did what to whom, for whom or what, how, where, when, and why.</S>
    <S sid="8" ssid="3">Semantic role confusions are errors of adequacy rather than fluency.</S>
    <S sid="9" ssid="4">It has often been noted that the dominance of lexically-oriented, precisionbased metrics such as BLEU (Papineni et al. 2002) tend to reward fluency more than adequacy.</S>
    <S sid="10" ssid="5">The length penalty in the BLEU metric, in particular, is only an indirect and weak indicator of adequacy.</S>
    <S sid="11" ssid="6">As a result, SMT work has been driven to optimize systems such that they often produce translations that contain significant role confusion errors despite reading fluently.</S>
    <S sid="12" ssid="7">The present work is inspired by the question of whether we can improve translation utility via a strategy of favoring semantic adequacy slightly more &#8211; possibly at the expense of slight degradations in lexical fluency.</S>
    <S sid="13" ssid="8">Shallow semantic parsing models have attained increasing levels of accuracy in recent years (Gildea and Jurafsky 2000; Sun and Jurafsky 2004; Pradhan et al. 2004, 2005; Pradhan 2005; Fung et al.</S>
    <S sid="14" ssid="9">2006, 2007; Gim6nez and M&#710;rquez 2007a, 2008).</S>
    <S sid="15" ssid="10">Such models, which identify semantic frames within input sentences by marking its predicates, and labeling their arguments with the semantic roles that they fill.</S>
    <S sid="16" ssid="11">Evidence has begun to accumulate that semantic frames &#8211; predicates and semantic roles &#8211; tend to preserve consistency across translations better than syntactic roles do.</S>
    <S sid="17" ssid="12">This is, of course, by design; it follows from the definition of semantic roles, which are less language-dependent than syntactic roles.</S>
    <S sid="18" ssid="13">Across Chinese and English, for example, it has been reported that approximately 84% of semantic roles are preserved consistently (Fung et al. 2006).</S>
    <S sid="19" ssid="14">Of these, roughly 15% do not preserve syntactic roles consistently.</S>
    <S sid="20" ssid="15">Since this directly targets the task of determining semantic correctness, we believe that the adequacy of MT output could be improved by leveraging the predictions of semantic parsers.</S>
    <S sid="21" ssid="16">We would like to exploit automatic semantic parsers to identify inconsistent semantic frame and role mappings between the input source sentences and their output translations.</S>
    <S sid="22" ssid="17">However, we take note of the difficult experience in making syntactic and semantic models conBoulder, Colorado, June 2009. c&#65533;2009 Association for Computational Linguistics tribute to improving SMT accuracy.</S>
    <S sid="23" ssid="18">On the one hand, there is reason to be optimistic.</S>
    <S sid="24" ssid="19">Over the past decade, we have seen an accumulation of evidence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy, in the form of techniques adapted from word sense disambiguation models (Chan et al. 2007; Gimenez and M&#710;rquez 2007b; Carpuat and Wu 2007).</S>
    <S sid="25" ssid="20">On the other hand, both directions saw unexpected disappointments along the way (e.g., Och et al. 2003; Carpuat and Wu 2005).</S>
    <S sid="26" ssid="21">We are therefore forewarned that it is likely to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling to the translation task.</S>
    <S sid="27" ssid="22">In this paper, we present a novel hybrid model that, for the first time to our knowledge, successfully applies semantic parsing technology to the challenge of improving the quality of ChineseEnglish statistical machine translation.</S>
    <S sid="28" ssid="23">The model makes use of a typical representative SMT system based on Moses, plus shallow semantic parsers for both English and Chinese.</S>
  </SECTION>
  <SECTION title="2 Hybrid two-pass semantic SMT" number="2">
    <S sid="29" ssid="1">While the accuracy of shallow semantic parsers has been approaching reasonably high levels in recent years for well-studied languages like English, and to a lesser extent, Chinese, the problem of excessive computational complexity is one of the primary challenges in adapting semantic parsing technology to the translation task.</S>
    <S sid="30" ssid="2">Semantic parses, by definition, are less likely than syntactic parses to obey clearly nested hierarchical composition rules.</S>
    <S sid="31" ssid="3">Moreover, the semantic parses are less likely to share an exactly isomorphic structure across the input and output languages, since the raison d&#8217;&#234;tre of semantic parsing is to capture semantic frame and role regularities independent of syntactic variation &#8211; monolingually and cross-lingually.</S>
    <S sid="32" ssid="4">This makes it difficult to incorporate semantic parsing into SMT merely by applying the sort of dynamic programming techniques found in current syntactic and tree-structured SMT models, most of which rely on being able to factor the computation into independent computations on the subtrees.</S>
    <S sid="33" ssid="5">In other words, the key computational obstacle is that the semantic parse of a larger string (or string pair, in the case of translation) is not in general strictly mechanically composable from the semantic parses of its smaller substrings (or substring pairs).</S>
    <S sid="34" ssid="6">In fact, the lack of easy compositionality is the reason that today&#8217;s most accurate shallow semantic parsers rely not primarily on compositional parsing techniques, but rather on ensembles of predictors that independently rate/rank a wide variety of factors supporting the role assignments given a broad sentence-wide range of context features.</S>
    <S sid="35" ssid="7">But while this improves semantic parsing accuracy, it poses a major obstacle for efficient tight integration into the sub-hypothesis construction and maintenance loops within SMT decoders.</S>
    <S sid="36" ssid="8">To circumvent this computational obstacle, the hybrid two-pass model defers application of the non-compositional semantic parsing information until a second error-correcting pass.</S>
    <S sid="37" ssid="9">This imposes a division of labor between the two passes.</S>
    <S sid="38" ssid="10">The first pass is performed using a conventional phrase-based SMT model.</S>
    <S sid="39" ssid="11">The phrase-based SMT model is assigned to the tasks of (a) providing an initial baseline hypothesis translation, and (b) fixing the lexical choice decisions.</S>
    <S sid="40" ssid="12">Note that the lexical choice decisions are not only at the single-word level, but are in general at the phrasal level.</S>
    <S sid="41" ssid="13">The second pass takes the output of the first pass, and re-orders constituent phrases corresponding to semantic predicates and arguments, seeking to maximize the cross-lingual match of the semantic parse of the re-ordered translation to that of the original input sentence.</S>
    <S sid="42" ssid="14">The second pass algorithm performs the error correction shown in Figure 1.</S>
    <S sid="43" ssid="15">The design decision to allow the first pass to fix all lexical choices follows an insight inspired by an empirical observation from our error analyses: the lexical choice decisions being made by today&#8217;s SMT models have attained fairly reasonable levels, and are not where the major problems of adequacy lie.</S>
    <S sid="44" ssid="16">Rather, the ordering of arguments in relation to their predicates is often where the main failures of adequacy occur.</S>
    <S sid="45" ssid="17">By avoiding lexical choice variations while considering re-ordering hypotheses, a significantly larger amount of re-ordering can be done without further increasing computational complexity.</S>
    <S sid="46" ssid="18">So we sacrifice a small amount of fluency by allowing re-ordering without compensating lexical choice &#8211; in exchange for gaining potentially a larger amount of fluency by getting the predicate-argument structure right.</S>
    <S sid="47" ssid="19">The model has a similar rationale for employing a re-ordering pass instead of re-ranking n-best lists or lattices.</S>
    <S sid="48" ssid="20">Oracle analysis of n-best lists and lattices show that they often focus on lexical choice alternatives rather than re-ordering / role variations which are more important to semantic adequacy.</S>
  </SECTION>
  <SECTION title="3 Experiment" number="3">
    <S sid="49" ssid="1">A Chinese-English experiment was conducted on the two-pass hybrid model.</S>
    <S sid="50" ssid="2">A phrase-based SMT baseline model was built by augmenting the open source statistical machine translation decoder Moses (Koehn et al. 2007) with additional preprocessors.</S>
    <S sid="51" ssid="3">English and Chinese shallow semantic parsers followed those discussed in Section 1.</S>
    <S sid="52" ssid="4">The model was trained on LDC newswire parallel text consisting of 3.42 million sentence pairs, containing 64.1 million English words and 56.9 million Chinese words.</S>
    <S sid="53" ssid="5">The English was tokenized and case-normalized; the Chinese was tokenized via a maximum-entropy model (Fung et al. 2004).</S>
    <S sid="54" ssid="6">Phrase translations were extracted via the growdiag-final heuristic.</S>
    <S sid="55" ssid="7">The language model is a 6-gram model trained with Kneser-Ney smoothing using the SRI language modeling toolkit (Stolcke 2002).</S>
    <S sid="56" ssid="8">The test set of Wall Street Journal newswire sentences was randomly extracted from the Chinese-English Bilingual Propbank.</S>
    <S sid="57" ssid="9">Although we did not make use of the Propbank annotations, this would potentially allow other types of analyses in the future.</S>
    <S sid="58" ssid="10">The phrase-based SMT model used for the first pass achieves a BLEU score of 42.99, establishing a fairly strong baseline to begin with.</S>
    <S sid="59" ssid="11">In comparison, the automatically errorcorrected translations that are output by the second pass achieve a BLEU score of 43.51.</S>
    <S sid="60" ssid="12">This represents approximately half a point improvement over the strong baseline.</S>
    <S sid="61" ssid="13">An example is seen in Figure 2.</S>
    <S sid="62" ssid="14">The SMT first pass translation has an ARG0 National Development Bank of Japan in the capital market which is badly mismatched to both the input sentence&#8217;s &#36164;* r J &#22330;.</S>
    <S sid="63" ssid="15">The second pass ends up re-ordering the constituent phrase corresponding to the mismatched ARGM-LOC, of Japan in the capital market, to follow the PRED issued, where the new English semantic parse now assigns most of its words the correctly matched ARGM-LOC semantic role label.</S>
    <S sid="64" ssid="16">Similarly, samurai bonds 30 billion yen is re-ordered to 30 billion yen samurai bonds.</S>
  </SECTION>
  <SECTION title="4 Discussion and conclusion" number="4">
    <S sid="65" ssid="1">To our knowledge, this is a first result demonstrating that shallow semantic parsing can improve translation accuracy of SMT models.</S>
    <S sid="66" ssid="2">We note that accuracy here was measured via BLEU, and it has been widely observed that the negative impacts of semantic predicate-argument errors on the utility of the translation are underestimated by evaluation metrics based on lexical criteria such as BLEU.</S>
    <S sid="67" ssid="3">We conjecture that more expensive manual evaluation techniques which directly measure translation utility could even more strongly reveal improvement in role confusion errors.</S>
    <S sid="68" ssid="4">The hybrid two-pass approach can be compared with the greedy re-ordering based strategy of the ReWrite decoder (Germann et al. 2001), although our search is breadth-first rather than purely greedy.</S>
    <S sid="69" ssid="5">Whereas ReWrite was based on wordlevel re-ordering, however, our approach is based on constituent phrase re-ordering, and the phrases to be re-ordered are more selectively chosen via the semantic parse labels.</S>
    <S sid="70" ssid="6">Moreover, the objective function being maximized by ReWrite is still the SMT model score; whereas in our case the new objective function is cross-lingual semantic predicate-argument match (plus an implicit search bias toward fewer re-orderings).</S>
    <S sid="71" ssid="7">The hybrid two-pass approach can also be compared with serial combination architectures for hybrid MT (e.g., Ueffing et al. 2008).</S>
    <S sid="72" ssid="8">But whereas Ueffing et al. take the output from a first-pass rulebased MT system, and then correct it using a second-pass SMT system, our two-pass semantic SMT model does the reverse: it takes the output from a first-pass SMT system, and then corrects it with the aid of semantic analyzers.</S>
  </SECTION>
</PAPER>
