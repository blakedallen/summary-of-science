Improving Accuracy In Word Class Tagging Through The Combination Of Machine Learning Systems
We examine how differences in language models, learned by different data-driven systems performing the same NLP task, can be exploited to yield a higher accuracy than the best individual system.
We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora.
Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data.
After comparison, their outputs are combined using several voting strategies and second-stage classifiers.
All combination taggers outperform their best component.
The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.
We report on accuracy of arounf 97% with in-domain training data for POS tagging using the Penn Treebank.
