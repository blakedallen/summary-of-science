<PAPER>
  <S sid="0">A Phrase-Based Statistical Model For SMS Text Normalization</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Jelinek.</S>
    <S sid="2" ssid="2">1991. language modeling speech In A. Waibel and K.F.</S>
    <S sid="3" ssid="3">Lee, editors, Readings in Speech Recognition, pages 450-506.</S>
    <S sid="4" ssid="4">Morgan Kaufmann, 1991 D. Kernighan, K Church and W. Gale.</S>
    <S sid="5" ssid="5">1990. spelling correction program based on a noisy model.</S>
    <S sid="6" ssid="6">Kukich.</S>
    <S sid="7" ssid="7">1992. for automatically corwords in ACM Computing Surveys,</S>
  </ABSTRACT>
  <SECTION title="1 Motivation" number="1">
    <S sid="8" ssid="1">SMS translation is a mobile Machine Translation (MT) application that translates a message from one language to another.</S>
    <S sid="9" ssid="2">Though there exists many commercial MT systems, direct use of such systems fails to work well due to the special phenomena in SMS texts, e.g. the unique relaxed and creative writing style and the frequent use of unconventional and not yet standardized shortforms.</S>
    <S sid="10" ssid="3">Direct modeling of these special phenomena in MT requires tremendous effort.</S>
    <S sid="11" ssid="4">Alternatively, we can normalize SMS texts into grammatical texts before MT.</S>
    <S sid="12" ssid="5">In this way, the traditional MT is treated as a &#8220;black-box&#8221; with little or minimal adaptation.</S>
    <S sid="13" ssid="6">One advantage of this pre-translation normalization is that the diversity in different user groups and domains can be modeled separately without accessing and adapting the language model of the MT system for each SMS application.</S>
    <S sid="14" ssid="7">Another advantage is that the normalization module can be easily utilized by other applications, such as SMS to voicemail and SMS-based information query.</S>
    <S sid="15" ssid="8">In this paper, we present a phrase-based statistical model for SMS text normalization.</S>
    <S sid="16" ssid="9">The normalization is visualized as a translation problem where messages in the SMS language are to be translated to normal English using a similar phrase-based statistical MT method (Koehn et al., 2003).</S>
    <S sid="17" ssid="10">We use IBM&#8217;s BLEU score (Papineni et al., 2002) to measure the performance of SMS text normalization.</S>
    <S sid="18" ssid="11">BLEU score computes the similarity between two sentences using n-gram statistics, which is widely-used in MT evaluation.</S>
    <S sid="19" ssid="12">A set of parallel SMS messages, consisting of 5000 raw (un-normalized) SMS messages and their manually normalized references, is constructed for training and testing.</S>
    <S sid="20" ssid="13">Evaluation by 5fold cross validation on this corpus shows that our method can achieve accuracy of 0.80702 in BLEU score compared to the baseline system of 0.6985.</S>
    <S sid="21" ssid="14">We also study the impact of our SMS text normalization on the task of SMS translation.</S>
    <S sid="22" ssid="15">The experiment of translating SMS texts from English to Chinese on a corpus comprising 402 SMS texts shows that, SMS normalization as a preprocessing step of MT can boost the translation performance from 0.1926 to 0.3770 in BLEU score.</S>
    <S sid="23" ssid="16">The rest of the paper is organized as follows.</S>
    <S sid="24" ssid="17">Section 2 reviews the related work.</S>
    <S sid="25" ssid="18">Section 3 summarizes the characteristics of English SMS texts.</S>
    <S sid="26" ssid="19">Section 4 discusses our method and Section 5 reports our experiments.</S>
    <S sid="27" ssid="20">Section 6 concludes the paper.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="28" ssid="1">There is little work reported on SMS normalization and translation.</S>
    <S sid="29" ssid="2">Bangalore et al. (2002) used a consensus translation technique to bootstrap parallel data using off-the-shelf translation systems for training a hierarchical statistical translation model for general domain instant messaging used in Internet chat rooms.</S>
    <S sid="30" ssid="3">Their method deals with the special phenomena of the instant messaging language (rather than the SMS language) in each individual MT system.</S>
    <S sid="31" ssid="4">Clark (2003) proposed to unify the process of tokenization, segmentation and spelling correction for normalization of general noisy text (rather than SMS or instant messaging texts) based on a noisy channel model at the character level.</S>
    <S sid="32" ssid="5">However, results of the normalization are not reported.</S>
    <S sid="33" ssid="6">Aw et al. (2005) gave a brief description on their input pre-processing work for an English-toChinese SMS translation system using a wordgroup model.</S>
    <S sid="34" ssid="7">In addition, in most of the commercial SMS translation applications 2 , SMS lingo (i.e., SMS short form) dictionary is provided to replace SMS short-forms with normal English words.</S>
    <S sid="35" ssid="8">Most of the systems do not handle OOV (out-of-vocabulary) items and ambiguous inputs.</S>
    <S sid="36" ssid="9">Following compares SMS text normalization with other similar or related applications.</S>
    <S sid="37" ssid="10">General text normalization deals with NonStandard Words (NSWs) and has been wellstudied in text-to-speech (Sproat et al., 2001) while SMS normalization deals with Non-Words (NSs) or lingoes and has seldom been studied before.</S>
    <S sid="38" ssid="11">NSWs, such as digit sequences, acronyms, mixed case words (WinNT, SunOS), abbreviations and so on, are grammatically correct in linguistics.</S>
    <S sid="39" ssid="12">However lingoes, such as &#8220;b4&#8221; (before) and &#8220;bf&#8221; (boyfriend), which are usually selfcreated and only accepted by young SMS users, are not yet formalized in linguistics.</S>
    <S sid="40" ssid="13">Therefore, the special phenomena in SMS texts impose a big challenge to SMS normalization.</S>
    <S sid="41" ssid="14">Intuitively, many would regard SMS normalization as a spelling correction problem where the lingoes are erroneous words or non-words to be replaced by English words.</S>
    <S sid="42" ssid="15">Researches on spelling correction centralize on typographic and cognitive/orthographic errors (Kukich, 1992) and use approaches (M.D.</S>
    <S sid="43" ssid="16">Kernighan, Church and Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002).</S>
    <S sid="44" ssid="17">These models are mostly character-based or string-based without considering the context.</S>
    <S sid="45" ssid="18">In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as &#8220;poor&#8221; or &#8220;pour&#8221;.</S>
    <S sid="46" ssid="19">In SMS, errors are not isolated within word and are usually not surrounded by clean context.</S>
    <S sid="47" ssid="20">Words are altered deliberately to reflect sender&#8217;s distinct creation and idiosyncrasies.</S>
    <S sid="48" ssid="21">A character can be deleted on purpose, such as &#8220;wat&#8221; (what) and &#8220;hv&#8221; (have).</S>
    <S sid="49" ssid="22">It also consists of short-forms such as &#8220;b4&#8221; (before), &#8220;bf&#8221; (boyfriend).</S>
    <S sid="50" ssid="23">In addition, normalizing SMS text might require the context to be spanned over more than one lexical unit such as &#8220;lemme&#8221; (let me), &#8220;ur&#8221; (you are) etc.</S>
    <S sid="51" ssid="24">Therefore, the models used in spelling correction are inadequate for providing a complete solution for SMS normalization.</S>
    <S sid="52" ssid="25">Others may regard SMS normalization as a paraphrasing problem.</S>
    <S sid="53" ssid="26">Broadly speaking, paraphrases capture core aspects of variability in language, by representing equivalencies between different expressions that correspond to the same meaning.</S>
    <S sid="54" ssid="27">In most of the recent works (Barzilay and McKeown, 2001; Shimohata, 2002), they are acquired (semi-) automatically from large comparable or parallel corpora using lexical and morpho-syntactic information.</S>
    <S sid="55" ssid="28">Text paraphrasing works on clean texts in which contextual and lexical-syntactic features can be extracted and used to find &#8220;approximate conceptual equivalence&#8221;.</S>
    <S sid="56" ssid="29">In SMS normalization, we are dealing with non-words and &#8220;ungrammatically&#8221; sentences with the purpose to normalize or standardize these words and form better sentences.</S>
    <S sid="57" ssid="30">The SMS normalization problem is thus different from text paraphrasing.</S>
    <S sid="58" ssid="31">On the other hand, it bears some similarities with MT as we are trying to &#8220;convert&#8221; text from one language to another.</S>
    <S sid="59" ssid="32">However, it is a simpler problem as most of the time; we can find the same word in both the source and target text, making alignment easier.</S>
  </SECTION>
  <SECTION title="3 Characteristics of English SMS" number="3">
    <S sid="60" ssid="1">Our corpus consists of 55,000 messages collected from two sources, a SMS chat room and correspondences between university students.</S>
    <S sid="61" ssid="2">The content is mostly related to football matches, making friends and casual conversations on &#8220;how, what and where about&#8221;.</S>
    <S sid="62" ssid="3">We summarize the text behaviors into two categories as below.</S>
    <S sid="63" ssid="4">The most significant orthographic variant in SMS texts is in the use of non-standard, selfcreated short-forms.</S>
    <S sid="64" ssid="5">Usually, sender takes advantage of phonetic spellings, initial letters or number homophones to mimic spoken conversation or shorten words or phrases (hw vs. homework or how, b4 vs. before, cu vs. see you, 2u vs. to you, oic vs. oh I see, etc.) in the attempt to minimize key strokes.</S>
    <S sid="65" ssid="6">In addition, senders create a new form of written representation to express their oral utterances.</S>
    <S sid="66" ssid="7">Emotions, such as &#8220;:(&#8220; symbolizing sad, &#8220;:)&#8221; symbolizing smiling, &#8220;:()&#8221; symbolizing shocked, are representations of body language.</S>
    <S sid="67" ssid="8">Verbal effects such as &#8220;hehe&#8221; for laughter and emphatic discourse particles such as &#8220;lor&#8221;, &#8220;lah&#8221;, &#8220;meh&#8221; for colloquial English are prevalent in the text collection.</S>
    <S sid="68" ssid="9">The loss of &#8220;alpha-case&#8221; information posts another challenge in lexical disambiguation and introduces difficulty in identifying sentence boundaries, proper nouns, and acronyms.</S>
    <S sid="69" ssid="10">With the flexible use of punctuation or not using punctuation at all, translation of SMS messages without prior processing is even more difficult.</S>
    <S sid="70" ssid="11">SMS messages are short, concise and convey much information within the limited space quota (160 letters for English), thus they tend to be implicit and influenced by pragmatic and situation reasons.</S>
    <S sid="71" ssid="12">These inadequacies of language expression such as deletion of articles and subject pronoun, as well as problems in number agreements or tenses make SMS normalization more challenging.</S>
    <S sid="72" ssid="13">Table 1 illustrates some orthographic and grammar variations of SMS texts.</S>
    <S sid="73" ssid="14">We investigate the corpus to assess the feasibility of replacing the lingoes with normal English words and performing limited adjustment to the text structure.</S>
    <S sid="74" ssid="15">Similarly to Aw et al. (2005), we focus on the three major cases of transformation as shown in the corpus: (1) replacement of OOV words and non-standard SMS lingoes; (2) removal of slang and (3) insertion of auxiliary or copula verb and subject pronoun.</S>
    <S sid="75" ssid="16">Substitution Deletion Insertion u -&gt; you m are 2 &#8594; to lah am n &#8594; and t is r &#8594; are ah you ur &#8594;your leh to dun &#8594; don&#8217;t 1 do man &#8594; manches- huh a ter no &#8594; number one in intro &#8594; introduce lor yourself wat &#8594; what ahh will Table 3.</S>
    <S sid="76" ssid="17">Top 10 Most Common Substitution, Deletion and Insertion Table 2 shows the statistics of these transformations based on 700 messages randomly selected, where 621 (88.71%) messages required normalization with a total of 2300 transformations.</S>
    <S sid="77" ssid="18">Substitution accounts for almost 86% of all transformations.</S>
    <S sid="78" ssid="19">Deletion and substitution make up the rest.</S>
    <S sid="79" ssid="20">Table 3 shows the top 10 most common transformations.</S>
  </SECTION>
  <SECTION title="4 SMS Normalization" number="4">
    <S sid="80" ssid="1">We view the SMS language as a variant of English language with some derivations in vocabulary and grammar.</S>
    <S sid="81" ssid="2">Therefore, we can treat SMS normalization as a MT problem where the SMS language is to be translated to normal English.</S>
    <S sid="82" ssid="3">We thus propose to adapt the statistical machine translation model (Brown et al., 1993; Zens and Ney, 2004) for SMS text normalization.</S>
    <S sid="83" ssid="4">In this section, we discuss the three components of our method: modeling, training and decoding for SMS text normalization.</S>
    <S sid="84" ssid="5">The SMS normalization model is based on the source channel model (Shannon, 1948).</S>
    <S sid="85" ssid="6">Assuming that an English sentence e, of length N is &#8220;corrupted&#8221; by a noisy channel to produce a SMS message s, of length M, the English sentence e, could be recovered through a posteriori distribution for a channel target text given the source text P s , and a prior distribution for (  |e) the channel source text .</S>
    <S sid="86" ssid="7">Assuming that one SMS word is mapped exactly to one English word in the channel model under an alignment A , we need to conP(sm|ea m ) (Brown et al. 1993).</S>
    <S sid="87" ssid="8">The channel en as in the following equation If we include the word &#8220;null&#8221; in the English vocabulary, the above model can fully address the deletion and substitution transformations, but inadequate to address the insertion transformation.</S>
    <S sid="88" ssid="9">For example, the lingoes &#8220;duno&#8221;, &#8220;ysnite&#8221; have to be normalized using an insertion transformation to become &#8220;don&#8217;t know&#8221; and &#8220;yesterday night&#8221;.</S>
    <S sid="89" ssid="10">Moreover, we also want the normalization to have better lexical affinity and linguistic equivalent, thus we extend the model to allow many words to many words alignment, allowing a sequence of SMS words to be normalized to a sequence of contiguous English words.</S>
    <S sid="90" ssid="11">We call this updated model a phrase-based normalization model.</S>
  </SECTION>
  <SECTION title="4.2 Phrase-based Model" number="5">
    <S sid="91" ssid="1">Given an English sentence e and SMS sentence s , if we assume that e can be decomposed into K phrases with a segmentation T , such that each phrase e in can be corresponded with m is the position of a word in san d its am ider only two types of probabilities: the alignment probabilities denoted by This is the basic function of the channel model for the phrase-based SMS normalization model, where we used the maximum approximation for the sum over all segmentations.</S>
    <S sid="92" ssid="2">Then we further We are now able to model the three tran sformations through the normalization pair ( , ) with the mapping probability P s &#65533; k &#65533; e~ a k lowings show the scenarios in which the three transformations occur.</S>
    <S sid="93" ssid="3">The statistics in our training corpus shows that by selecting appropriate phrase segmentation, the position re-ordering at the phrase level occurs rarely.</S>
    <S sid="94" ssid="4">It is not surprising since most of the English words or phrases in normal English text are replaced with lingoes in SMS messages without position change to make SMS text short and concise and to retain the meaning.</S>
    <S sid="95" ssid="5">Thus we need to consider only monotone alignment at phrase level, i.e., k , as in equation (4).</S>
    <S sid="96" ssid="6">In addition, = &#65533; ak the word-level reordering within phrase is learned during training.</S>
    <S sid="97" ssid="7">Now we can further derive equation (4) as follows: The mapping probability P(s&#65533;k  |e&#65533;k) is estimated via relative frequencies as follows: Here, N(s&#65533;k, e&#65533;k) denotes the frequency of the normalization pair ( s &#65533; k , e &#65533; k ) .</S>
    <S sid="98" ssid="8">Using a bigram language model and assuming Bayes decision rule, we finally obtain the following search criterion for equation (1).</S>
    <S sid="99" ssid="9">For the above equation, we assume the segFinally, the SMS normalization model consists of two sub-models: a word-based language model (LM), characterized by P(en  |en&#8722;1 ) and a phrasebased lexical mapping model (channel model), characterized by P ( s k  |e For the phrase-based model training, the sentence-aligned SMS corpus needs to be aligned first at the phrase level.</S>
    <S sid="100" ssid="10">The maximum likelihood approach, through EM algorithm and Viterbi search (Dempster et al., 1977) is employed to infer such an alignment.</S>
    <S sid="101" ssid="11">Here, we make a reasonable assumption on the alignment unit that a single SMS word can be mapped to a sequence of contiguous English words, but not vice verse.</S>
    <S sid="102" ssid="12">The EM algorithm for phrase alignment is illustrated in Figure 1 and is formulated by equation (8).</S>
    <S sid="103" ssid="13">The Expectation-Maximization Algorithm The alignment process given in equation (8) is different from that of normalization given in equation (7) in that, here we have an aligned input sentence pair, s and .</S>
    <S sid="104" ssid="14">The alignment process is just to find the alignment segmentation between the two sentences that maximizes the joint probability.</S>
    <S sid="105" ssid="15">Therefore, in step (2) of the EM algorithm given at Figure 1, only the joint probabilities P(s&#65533;k, e&#65533;k ) are involved and updated.</S>
    <S sid="106" ssid="16">Since EM may fall into local optimization, in order to speed up convergence and find a nearly global optimization, a string matching technique is exploited at the initialization step to identify the most probable normalization pairs.</S>
    <S sid="107" ssid="17">The orthographic similarities captured by edit distance and a SMS lingo dictionary3 which contains the commonly used short-forms are first used to establish phrase mapping boundary candidates.</S>
    <S sid="108" ssid="18">Heuristics are then exploited to match tokens within the pairs of boundary candidates by trying to combine consecutive tokens within the boundary candidates if the numbers of tokens do not agree.</S>
    <S sid="109" ssid="19">Finally, a filtering process is carried out to manually remove the low-frequency noisy alignment pairs.</S>
    <S sid="110" ssid="20">Table 4 shows some of the extracted normalization pairs.</S>
    <S sid="111" ssid="21">As can be seen from the table, our algorithm discovers ambiguous mappings automatically that are otherwise missing from most of the lingo dictionary.</S>
    <S sid="112" ssid="22">Given the phrase-aligned SMS corpus, the lexical mapping model, characterized by P(s&#65533;k  |ek) , is easily to be trained using equation (6).</S>
    <S sid="113" ssid="23">Our n-gram LM P(en  |en&#8722;1) is trained on English Gigaword provided by LDC using SRILM language modeling toolkit (Stolcke, 2002).</S>
    <S sid="114" ssid="24">Backoff smoothing (Jelinek, 1991) is used to adjust and assign a non-zero probability to the unseen words to address data sparseness.</S>
    <S sid="115" ssid="25">Given an input , the search, characterized in model.</S>
    <S sid="116" ssid="26">In this paper, the maximization problem in equation (7) is solved using a monotone search, implemented as a Viterbi search through dynamic programming.</S>
  </SECTION>
  <SECTION title="5 Experiments" number="6">
    <S sid="117" ssid="1">The aim of our experiment is to verify the effectiveness of the proposed statistical model for SMS normalization and the impact of SMS normalization on MT.</S>
    <S sid="118" ssid="2">A set of 5000 parallel SMS messages, which consists of raw (un-normalized) SMS messages and reference messages manually prepared by two project members with inter-normalization agreement checked, was prepared for training and testing.</S>
    <S sid="119" ssid="3">For evaluation, we use IBM&#8217;s BLEU score (Papineni et al., 2002) to measure the performance of the SMS normalization.</S>
    <S sid="120" ssid="4">BLEU score measures the similarity between two sentences using n-gram statistics with a penalty for too short sentences, which is already widely-used in MT evaluation.</S>
    <S sid="121" ssid="5">The baseline experiment is to moderate the texts using a lingo dictionary comprises 142 normalization pairs, which is also used in bootstrapping the phrase alignment learning process.</S>
    <S sid="122" ssid="6">Table 5 compares the performance of the different setups of the baseline experiments.</S>
    <S sid="123" ssid="7">We first measure the complexity of the SMS normalization task by directly computing the similarity between the raw SMS text and the normalized English text.</S>
    <S sid="124" ssid="8">The 1st row of Table 5 reports the similarity as 0.5784 in BLEU score, which implies that there are quite a number of English word 3-gram that are common in the raw and normalized messages.</S>
    <S sid="125" ssid="9">The 2nd experiment is carried out using only simple dictionary look-up.</S>
    <S sid="126" ssid="10">Lexical ambiguity is addressed by selecting the highest-frequency normalization candidate, i.e., only unigram LM is used.</S>
    <S sid="127" ssid="11">The performance of the 2nd experiment is 0.6958 in BLEU score.</S>
    <S sid="128" ssid="12">It suggests that the lingo dictionary plus the unigram LM is very useful for SMS normalization.</S>
    <S sid="129" ssid="13">Finally we carry out the 3rd experiment using dictionary look-up plus bi-gram LM.</S>
    <S sid="130" ssid="14">Only a slight improvement of 0.0128 (0.7086-0.6958) is obtained.</S>
    <S sid="131" ssid="15">This is largely because the English words in the lingo dictionary are mostly highfrequency and commonly-used.</S>
    <S sid="132" ssid="16">Thus bi-gram does not show much more discriminative ability than unigram without the help of the phrasebased lexical mapping model.</S>
    <S sid="133" ssid="17">We then conducted the experiment using the proposed method (Bi-gram LM plus a phrase-based lexical mapping model) through a five-fold cross validation on the 5000 parallel SMS messages.</S>
    <S sid="134" ssid="18">Table 6 shows the results.</S>
    <S sid="135" ssid="19">An average score of 0.8070 is obtained.</S>
    <S sid="136" ssid="20">Compared with the baseline performance in Table 5, the improvement is very significant.</S>
    <S sid="137" ssid="21">It suggests that the phrase-based lexical mapping model is very useful and our method is effective for SMS text normalization.</S>
    <S sid="138" ssid="22">Figure 2 is the learning curve.</S>
    <S sid="139" ssid="23">It shows that our algorithm converges when training data is increased to 3000 SMS parallel messages.</S>
    <S sid="140" ssid="24">This suggests that our collected corpus is representative and enough for training our model.</S>
    <S sid="141" ssid="25">Table 7 illustrates some examples of the normalization results.</S>
    <S sid="142" ssid="26">Experimental result analysis reveals that the strength of our model is in its ability to disambiguate mapping as in &#8220;2&#8221; to &#8220;two&#8221; or &#8220;to&#8221; and &#8220;w&#8221; to &#8220;with&#8221; or &#8220;who&#8221;.</S>
    <S sid="143" ssid="27">Error analysis shows that the challenge of the model lies in the proper insertion of subject pronoun and auxiliary or copula verb, which serves to give further semantic information about the main verb, however this requires significant context understanding.</S>
    <S sid="144" ssid="28">For example, a message such as &#8220;u smart&#8221; gives little clues on whether it should be normalized to &#8220;Are you smart?&#8221; or &#8220;You are smart.&#8221; unless the full conversation is studied.</S>
    <S sid="145" ssid="29">Takako w r u?</S>
    <S sid="146" ssid="30">Takako who are you?</S>
    <S sid="147" ssid="31">Im in ns, lik soccer, clubbin hangin w frenz!</S>
    <S sid="148" ssid="32">Wat bout u mee?</S>
    <S sid="149" ssid="33">I'm in ns, like soccer, clubbing hanging with friends!</S>
    <S sid="150" ssid="34">What about you? fancy getting excited w others' boredom Fancy getting excited with others' boredom If u ask me b4 he ask me then i'll go out w u all lor.</S>
    <S sid="151" ssid="35">N u still can act so real.</S>
    <S sid="152" ssid="36">If you ask me before he asked me then I'll go out with you all.</S>
    <S sid="153" ssid="37">And you still can act so real.</S>
    <S sid="154" ssid="38">Doing nothing, then u not having dinner w us?</S>
    <S sid="155" ssid="39">Doing nothing, then you do not having dinner with us?</S>
    <S sid="156" ssid="40">Aiyar sorry lor forgot 2 tell u... Mtg at 2 pm.</S>
    <S sid="157" ssid="41">Sorry forgot to tell you... Meeting at two pm. tat's y I said it's bad dat all e gals know u... Wat u doing now?</S>
    <S sid="158" ssid="42">That's why I said it's bad that all the girls know you... What you doing now?</S>
    <S sid="159" ssid="43">An experiment was also conducted to study the effect of normalization on MT using 402 messages randomly selected from the text corpus.</S>
    <S sid="160" ssid="44">We compare three types of SMS message: raw SMS messages, normalized messages using simple dictionary look-up and normalized messages using our method.</S>
    <S sid="161" ssid="45">The messages are passed to two different English-to-Chinese translation systems provided by Systran4 and Institute for Infocomm Research5(I2R) separately to produce three sets of translation output.</S>
    <S sid="162" ssid="46">The translation quality is measured using 3-gram cumulative BLEU score against two reference messages.</S>
    <S sid="163" ssid="47">3-gram is</S>
  </SECTION>
  <SECTION title="BLEU" number="7">
    <S sid="164" ssid="1">used as most of the messages are short with average length of seven words.</S>
    <S sid="165" ssid="2">Table 8 shows the details of the BLEU scores.</S>
    <S sid="166" ssid="3">We obtain an average of 0.3770 BLEU score for normalized messages against 0.1926 for raw messages.</S>
    <S sid="167" ssid="4">The significant performance improvement suggests that preprocessing of normalizing SMS text using our method before MT is an effective way to adapt a general MT system to SMS domain.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="8">
    <S sid="168" ssid="1">In this paper, we study the differences among SMS normalization, general text normalization, spelling check and text paraphrasing, and investigate the different phenomena of SMS messages.</S>
    <S sid="169" ssid="2">We propose a phrase-based statistical method to normalize SMS messages.</S>
    <S sid="170" ssid="3">The method produces messages that collate well with manually normalized messages, achieving 0.8070 BLEU score against 0.6958 baseline score.</S>
    <S sid="171" ssid="4">It also significantly improves SMS translation accuracy from 0.1926 to 0.3770 in BLEU score without adjusting the MT model.</S>
    <S sid="172" ssid="5">This experiment results provide us with a good indication on the feasibility of using this method in performing the normalization task.</S>
    <S sid="173" ssid="6">We plan to extend the model to incorporate mechanism to handle missing punctuation (which potentially affect MT output and are not being taken care at the moment), and making use of pronunciation information to handle OOV caused by the use of phonetic spelling.</S>
    <S sid="174" ssid="7">A bigger data set will also be used to test the robustness of the system leading to a more accurate alignment and normalization.</S>
  </SECTION>
</PAPER>
