[
  {
    "citance_No": 1, 
    "citing_paper_id": "D11-1065", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Keith, Vertanen | Per Ola, Kristensson", 
    "raw_text": "We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010)", 
    "clean_text": "We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D11-1065", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Keith, Vertanen | Per Ola, Kristensson", 
    "raw_text": "In cross-entropy difference selection, a sentence ?sscore is the in-domain cross-entropy minus the back ground cross-entropy (Moore and Lewis, 2010) .This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010)", 
    "clean_text": "In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D11-1065", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Keith, Vertanen | Per Ola, Kristensson", 
    "raw_text": "We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010)", 
    "clean_text": "We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P14-1072", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Hui, Zhang | David, Chiang", 
    "raw_text": "One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010)", 
    "clean_text": "One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P14-1072", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Hui, Zhang | David, Chiang", 
    "raw_text": "Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data", 
    "clean_text": "Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W12-3148", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Milo&scaron;, Stanojevi&#263; | Amir, Kamran | Petra, Galu&scaron;&ccaron;&aacute;kov&aacute; | Ale&scaron;, Tamchyna | Ond&#x159;ej, Bojar", 
    "raw_text": "In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. Naturally, we only used the source side of the test set", 
    "clean_text": "In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W11-2149", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Matthias, Huck | Joern, Wuebker | Christoph, Schmidt | Markus, Freitag | Stephan, Peitz | Daniel, Stein | Arnaud, Dagnelies | Saab, Mansour | Gregor, Leusch | Hermann, Ney", 
    "raw_text": "For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010)", 
    "clean_text": "For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W12-3149", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "David, Vilar", 
    "raw_text": "2.1.2 Moore LM Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy 382of an in-domain language model and an out-of domain language model trained on a random sampling of the data", 
    "clean_text": "Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "D11-1033", 
    "citing_paper_authority": 36, 
    "citing_paper_authors": "Amittai, Axelrod | Xiaodong, He | Jianfeng, Gao", 
    "raw_text": "This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010)", 
    "clean_text": "This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D11-1033", 
    "citing_paper_authority": 36, 
    "citing_paper_authors": "Amittai, Axelrod | Xiaodong, He | Jianfeng, Gao", 
    "raw_text": "Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy", 
    "clean_text": "Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D11-1033", 
    "citing_paper_authority": 36, 
    "citing_paper_authors": "Amittai, Axelrod | Xiaodong, He | Jianfeng, Gao", 
    "raw_text": "Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language modelLMO of similar size over the general-domain corpus", 
    "clean_text": "Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D11-1033", 
    "citing_paper_authority": 36, 
    "citing_paper_authors": "Amittai, Axelrod | Xiaodong, He | Jianfeng, Gao", 
    "raw_text": "Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010)", 
    "clean_text": "Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "D11-1033", 
    "citing_paper_authority": 36, 
    "citing_paper_authors": "Amittai, Axelrod | Xiaodong, He | Jianfeng, Gao", 
    "raw_text": "We used the three methods from Section 4 to identify the best-scoring sentences in the general domain corpus. We consider three methods for extracting domain targeted parallel data from a general corpus: source side cross-entropy (Cross-Ent), source-side cross entropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (b Ml), which is novel", 
    "clean_text": "We consider three methods for extracting domain targeted parallel data from a general corpus: source side cross-entropy (Cross-Ent), source-side cross entropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (b Ml), which is novel.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W12-3140", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Stephan, Peitz | Hai-Son, Le | Markus, Freitag | Matthias, Huck | Thomas, Lavergne | Teresa, Herrmann | Alex, Waibel | Jean, Senellart | Jan, Niehues | Bianka, Buschbeck-Wolf | Alexandre, Allauzen | H., Ney | Josep M., Crego", 
    "raw_text": "For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010)", 
    "clean_text": "For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P11-1157", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Barbara, Plank | Gertjan, van Noord", 
    "raw_text": "However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010)", 
    "clean_text": "However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W12-3137", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Malte, Nuhn | Matthias, Huck | Hermann, Ney | Stephan, Peitz | Markus, Freitag", 
    "raw_text": "For the 109 French-English, UN and LDC Gigawordcorpora we apply the data selection technique described in (Moore and Lewis, 2010)", 
    "clean_text": "For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P13-2119", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Kevin, Duh | Graham, Neubig | Katsuhito, Sudoh | Hajime, Tsukada", 
    "raw_text": "We employ the data selection method of (Ax el rod et al, 2011), which builds upon (Moore and Lewis, 2010)", 
    "clean_text": "We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P13-1157", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Yuki, Arase | Ming, Zhou", 
    "raw_text": "These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010)", 
    "clean_text": "These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P13-1157", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Yuki, Arase | Ming, Zhou", 
    "raw_text": "We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy)", 
    "clean_text": "We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D12-1047", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Shixiang, Lu | Xiaoyin, Fu | Wei, Wei | Bo, Xu", 
    "raw_text": "It seems to be a universal truth that LM performance can always be improved by using more training data (Brantsetal., 2007), but only if the training data is reason ably well-matched with the desired output (Moore and Lewis, 2010)", 
    "clean_text": "It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010).", 
    "keep_for_gold": 0
  }
]
