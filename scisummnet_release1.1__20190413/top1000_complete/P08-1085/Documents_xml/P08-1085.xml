<PAPER>
  <S sid="0">EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start)</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We address the task of unsupervised POS tagging.</S>
    <S sid="2" ssid="2">We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries.</S>
    <S sid="3" ssid="3">We present a family of algorithms to compute effective estimations We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline.</S>
    <S sid="4" ssid="4">We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">The task of unsupervised (or semi-supervised) partof-speech (POS) tagging is the following: given a dictionary mapping words in a language to their possible POS, and large quantities of unlabeled text data, learn to predict the correct part of speech for a given word in context.</S>
    <S sid="6" ssid="2">The only supervision given to the learning process is the dictionary, which in a realistic scenario, contains only part of the word types observed in the corpus to be tagged.</S>
    <S sid="7" ssid="3">Unsupervised POS tagging has been traditionally approached with relative success (Merialdo, 1994; Kupiec, 1992) by HMM-based generative models, employing EM parameters estimation using the Baum-Welch algorithm.</S>
    <S sid="8" ssid="4">However, as recently noted 'This work is supported in part by the Lynn and William Frankel Center for Computer Science. by Banko and Moore (2004), these works made use of filtered dictionaries: dictionaries in which only relatively probable analyses of a given word are preserved.</S>
    <S sid="9" ssid="5">This kind of filtering requires serious supervision: in theory, an expert is needed to go over the dictionary elements and filter out unlikely analyses.</S>
    <S sid="10" ssid="6">In practice, counts from an annotated corpus have been traditionally used to perform the filtering.</S>
    <S sid="11" ssid="7">Furthermore, these methods require rather comprehensive dictionaries in order to perform well.</S>
    <S sid="12" ssid="8">In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on &#8220;diluted dictionaries&#8221; &#8211; in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ).</S>
    <S sid="13" ssid="9">All the work mentioned above focuses on unsupervised English POS tagging.</S>
    <S sid="14" ssid="10">The dictionaries are all derived from tagged English corpora (all recent work uses the WSJ corpus).</S>
    <S sid="15" ssid="11">As such, the setting of the research is artificial: there is no reason to perform unsupervised learning when an annotated corpus is available.</S>
    <S sid="16" ssid="12">The problem is rather approached as a workbench for exploring new learning methods.</S>
    <S sid="17" ssid="13">The result is a series of creative algorithms, that have steadily improved results on the same dataset: unsupervised CRF training using contrastive estimation (SE), a fully-bayesian HMM model that jointly performs clustering and sequence learning (GG), and a Bayesian LDA-based model using only observed context features to predict tag words (TJ).</S>
    <S sid="18" ssid="14">These sophisticated learning algorithms all outperform the traditional baseline of EM-HMM based methods, while relying on similar knowledge: the lexical context of the words to be tagged and their letter structure (e.g., presence of suffixes, capitalization and hyphenation).1 Our motivation for tackling unsupervised POS tagging is different: we are interested in developing a Hebrew POS tagger.</S>
    <S sid="19" ssid="15">We have access to a good Hebrew lexicon (and a morphological analyzer), and a fair amount of unlabeled training data, but hardly any annotated corpora.</S>
    <S sid="20" ssid="16">We actually report results on full morphological disambiguation for Hebrew, a task similar but more challenging than POS tagging: we deal with a tagset much larger than English (over 3,561 distinct tags) and an ambiguity level of about 2.7 per token as opposed to 1.4 for English.</S>
    <S sid="21" ssid="17">Instead of inventing a new learning framework, we go back to the traditional EM trained HMMs.</S>
    <S sid="22" ssid="18">We argue that the key challenge to learning an effective model is to define good enough initial conditions.</S>
    <S sid="23" ssid="19">Given sufficiently good initial conditions, EM trained models can yield highly competitive results.</S>
    <S sid="24" ssid="20">Such models have other benefits as well: they are simple, robust, and computationally more attractive.</S>
    <S sid="25" ssid="21">In this paper, we concentrate on methods for deriving sufficiently good initial conditions for EMHMM learning.</S>
    <S sid="26" ssid="22">Our method for learning initial conditions for the p(tjw) distributions relies on a mixture of language specific models: a paradigmatic model of similar words (where similar words are words with similar inflection patterns), simple syntagmatic constraints (e.g., the sequence V-V is extremely rare in English).</S>
    <S sid="27" ssid="23">These are complemented by a linear lexical context model.</S>
    <S sid="28" ssid="24">Such models are simple to build and test.</S>
    <S sid="29" ssid="25">We present results for unsupervised PoS tagging of Hebrew text and for the common WSJ English test sets.</S>
    <S sid="30" ssid="26">We show that our method achieves state-ofthe-art results for the English setting, even with a relatively small dictionary.</S>
    <S sid="31" ssid="27">Furthermore, while recent work report results on a reduced English tagset of 17 PoS tags, we also present results for the complete 45 tags tagset of the WSJ corpus.</S>
    <S sid="32" ssid="28">This considerably raises the bar of the EM-HMM baseline.</S>
    <S sid="33" ssid="29">We also report state-of-the-art results for Hebrew full morphological disambiguation.</S>
    <S sid="34" ssid="30">Our primary conclusion is that the problem of learning effective stochastic classifiers remains primarily a search task.</S>
    <S sid="35" ssid="31">Initial conditions play a dominant role in solving this task and can rely on linguistically motivated approximations.</S>
    <S sid="36" ssid="32">A robust learning method (EM-HMM) combined with good initial conditions based on a robust feature set can go a long way (as opposed to a more complex learning method).</S>
    <S sid="37" ssid="33">It seems that computing initial conditions is also the right place to capture complex linguistic intuition without fear that over-generalization could lead a learner to diverge.</S>
  </SECTION>
  <SECTION title="2 Previous Work" number="2">
    <S sid="38" ssid="1">The tagging accuracy of supervised stochastic taggers is around 96%&#8211;97% (Manning and Schutze, 1999).</S>
    <S sid="39" ssid="2">Merialdo (1994) reports an accuracy of 86.6% for an unsupervised token-based EMestimated HMM, trained on a corpus of about 1M words, over a tagset of 159 tags.</S>
    <S sid="40" ssid="3">Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tagset of 134 tags.</S>
    <S sid="41" ssid="4">With (artificially created) good initial conditions, such as a good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27%, and 94.51% on the same data sets.</S>
    <S sid="42" ssid="5">Merialdo, on the other hand, reports an improvement to 92.6% and 94.4% for the case where 100 and 2,000 sentences of the training corpus are manually tagged.</S>
    <S sid="43" ssid="6">Later, Banko and Moore (2004) observed that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept.</S>
    <S sid="44" ssid="7">Brill (1995b) proposed an unsupervised tagger based on transformationbased learning (Brill, 1995a), achieving accuracies of above 95%.</S>
    <S sid="45" ssid="8">This unsupervised tagger relied on an initial step in which the most probable tag for each word is chosen.</S>
    <S sid="46" ssid="9">Optimized lexicons and Brill&#8217;s most-probable-tag Oracle are not available in realistic unsupervised settings, yet, they show that good initial conditions greatly facilitate learning.</S>
    <S sid="47" ssid="10">Recent work on unsupervised POS tagging for English has significantly improved the results on this task: GG, SE and most recently TJ report the best results so far on the task of unsupervised POS tagging of the WSJ with diluted dictionaries.</S>
    <S sid="48" ssid="11">With dictionaries as small as 1249 lexical entries the LDA-based method with a strong ambiguity-class model reaches POS accuracy as high as 89.7% on a reduced tagset of 17 tags.</S>
    <S sid="49" ssid="12">While these 3 methods rely on the same feature set (lexical context, spelling features) for the learning stage, the LDA approach bases its predictions entirely on observable features, and excludes the traditional hidden states sequence.</S>
    <S sid="50" ssid="13">In Hebrew, Levinger et al. (1995) introduced the similar-words algorithm for estimating p(t|w) from unlabeled data, which we describe below.</S>
    <S sid="51" ssid="14">Our method uses this algorithm as a first step, and refines the approximation by introducing additional linguistic constraints and an iterative refinement step.</S>
  </SECTION>
  <SECTION title="3 Initial Conditions For EM-HMM" number="3">
    <S sid="52" ssid="1">The most common model for unsupervised learning of stochastic processes is Hidden Markov Models (HMM).</S>
    <S sid="53" ssid="2">For the case of tagging, the states correspond to the tags ti, and words wi are emitted each time a state is visited.</S>
    <S sid="54" ssid="3">The parameters of the model can be estimated by applying the Baum-Welch EM algorithm (Baum, 1972), on a large-scale corpus of unlabeled text.</S>
    <S sid="55" ssid="4">The estimated parameters are then used in conjunction with Viterbi search, to find the most probable sequence of tags for a given sentence.</S>
    <S sid="56" ssid="5">In this work, we follow Adler (2007) and use a variation of second-order HMM in which the probability of a tag is conditioned by the tag that precedes it and by the one that follows it, and the probability of an emitted word is conditioned by its tag and the tag that follows it2.</S>
    <S sid="57" ssid="6">In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexical probabilities.</S>
    <S sid="58" ssid="7">We investigate methods to approximate the initial parameters of the p(t|w) distribution, from which we obtain p(w|t) by marginalization and Bayesian inversion.</S>
    <S sid="59" ssid="8">We also experiment with constraining the p(t|t_1, t+1) distribution.</S>
    <S sid="60" ssid="9">General syntagmatic constraints We set linguistically motivated constraints on the p(t|t_1, t+1) distribution.</S>
    <S sid="61" ssid="10">In our setting, these are used to force the probability of some events to 0 (e.g., &#8220;Hebrew verbs can not be followed by the of preposition&#8221;).</S>
    <S sid="62" ssid="11">Morphology-based p(t|w) approximation Levinger et al. (1995) developed a context-free method for acquiring morpho-lexical probabilities (p(t|w)) from an untagged corpus.</S>
    <S sid="63" ssid="12">The method is based on language-specific rules for constructing a similar words (SW) set for each analysis of a word.</S>
    <S sid="64" ssid="13">This set is composed of morphological variations of the word under the given analysis.</S>
    <S sid="65" ssid="14">For example, the Hebrew token &#1491;&#1500;&#1497; can be analyzed as either a noun (boy) or a verb (gave birth).</S>
    <S sid="66" ssid="15">The noun SW set for this token is composed of the definiteness and number inflections &#1501;&#1497;&#1491;&#1500;&#1497;&#1492;,&#1501;&#1497;&#1491;&#1500;&#1497;,&#1491;&#1500;&#1497;&#1492; (the boy, boys, the boys), while the verb SW set is composed of gender and tense inflections &#1493;&#1491;&#1500;&#1497;,&#1492;&#1491;&#1500;&#1497; (she/they gave birth).</S>
    <S sid="67" ssid="16">The approximated probability of each analysis is based on the corpus frequency of its SW set.</S>
    <S sid="68" ssid="17">For the complete details, refer to the original paper.</S>
    <S sid="69" ssid="18">Cucerzan and Yarowsky (2000) proposed a similar method for the unsupervised estimation of p(t|w) in English, relying on simple spelling features to characterize similar word classes.</S>
    <S sid="70" ssid="19">Linear-Context-based p(t|w) approximation The method of Levinger et al. makes use of Hebrew inflection patterns in order to estimate context free approximation of p(t|w) by relating a word to its different inflections.</S>
    <S sid="71" ssid="20">However, the context in which a word occurs can also be very informative with respect to its POS-analysis (Sch&#168;utze, 1995).</S>
    <S sid="72" ssid="21">We propose a novel algorithm for estimating p(t|w) based on the contexts in which a word occurs.3 The algorithm starts with an initial p(t|w) estimate, and iteratively re-estimates: 3While we rely on the same intuition, our use of context differs from earlier works on distributional POS-tagging like (Sch&#168;utze, 1995), in which the purpose is to directly assign the possible POS for an unknown word.</S>
    <S sid="73" ssid="22">In contrast, our algorithm aims to improve the estimate for the whole distribution p(tIw), to be further disambiguated by the EM-HMM learner. where Z is a normalization factor, W is the set of all words in the corpus, C is the set of all contexts, and RELc &#8838; C is a set of reliable contexts, defined below. allow(t, w) is a binary function indicating whether t is a valid tag for w. p(c|w) and p(w|c) are estimated via raw corpus counts.</S>
    <S sid="74" ssid="23">Intuitively, we estimate the probability of a tag given a context as the average probability of a tag given any of the words appearing in that context, and similarly the probability of a tag given a word is the averaged probability of that tag in all the (reliable) contexts in which the word appears.</S>
    <S sid="75" ssid="24">At each round, we define RELc, the set of reliable contexts, to be the set of all contexts in which p(t|c) &gt; 0 for at most X different ts.</S>
    <S sid="76" ssid="25">The method is general, and can be applied to different languages.</S>
    <S sid="77" ssid="26">The parameters to specify for each language are: the initial estimation p(t|w), the estimation of the allow relation for known and OOV words, and the types of contexts to consider.</S>
  </SECTION>
  <SECTION title="4 Application to Hebrew" number="4">
    <S sid="78" ssid="1">In Hebrew, several words combine into a single token in both agglutinative and fusional ways.</S>
    <S sid="79" ssid="2">This results in a potentially high number of tags for each token.</S>
    <S sid="80" ssid="3">On average, in our corpus, the number of possible analyses per known word reached 2.7, with the ambiguity level of the extended POS tagset in corpus for English (1.41) (Dermatas and Kokkinakis, 1995).</S>
    <S sid="81" ssid="4">In this work, we use the morphological analyzer of MILA &#8211; Knowledge Center for Processing Hebrew (KC analyzer).</S>
    <S sid="82" ssid="5">In contrast to English tagsets, the number of tags for Hebrew, based on all combinations of the morphological attributes, can grow theoretically to about 300,000 tags.</S>
    <S sid="83" ssid="6">In practice, we found &#8216;only&#8217; about 3,560 tags in a corpus of 40M tokens training corpus taken from Hebrew news material and Knesset transcripts.</S>
    <S sid="84" ssid="7">For testing, we manually tagged the text which is used in the Hebrew Treebank (Sima&#8217;an et al., 2001) (about 90K tokens), according to our tagging guidelines.</S>
    <S sid="85" ssid="8">General syntagmatic constraints We define 4 syntagmatic constraints over p(t|t_1, t+1): (1) a construct state form cannot be followed by a verb, preposition, punctuation, existential, modal, or copula; (2) a verb cannot be followed by the preposition * &#711;sel (of), (3) copula and existential cannot be followed by a verb, and (4) a verb cannot be followed by another verb, unless one of them has a prefix, or the second verb is an infinitive, or the first verb is imperative and the second verb is in future tense.4 Morphology-Based p(t|w) approximation We extended the set of rules used in Levinger et al. , in order to support the wider tagset used by the KC analyzer: (1) The SW set for adjectives, copulas, existentials, personal pronouns, verbs and participles, is composed of all gender-number inflections; (2) The SW set for common nouns is composed of all number inflections, with definite article variation for absolute noun; (3) Prefix variations for proper nouns; (4) Gender variation for numerals; and (5) Gendernumber variation for all suffixes (possessive, nominative and accusative).</S>
    <S sid="86" ssid="9">Linear-Context-based p(t|w) approximation For the initial p(t|w) we use either a uniform distribution based on the tags allowed in the dictionary, or the estimate obtained by using the modified Levinger et al. algorithm.</S>
    <S sid="87" ssid="10">We use contexts of the form LR=w_1, w+1 (the neighbouring words).</S>
    <S sid="88" ssid="11">We estimate p(w|c) and p(c|w) via relative frequency over all the events w1, w2, w3 occurring at least 10 times in the corpus. allow(t, w) follows the dictionary.</S>
    <S sid="89" ssid="12">Because of the wide coverage of the Hebrew lexicon, we take RELc to be C (all available contexts).</S>
    <S sid="90" ssid="13">We run a series of experiments with 8 distinct initial conditions, as shown in Table 1: our baseline (Uniform) is the uniform distribution over all tags provided by the KC analyzer for each word.</S>
    <S sid="91" ssid="14">The Syntagmatic initial conditions add the p(t|t_1, t+1) constraints described above to the uniform baseline.</S>
    <S sid="92" ssid="15">The Morphology-Based and Linear-Context initial conditions are computed as described above, while the Morph+Linear is the result of applying the linear-context algorithm over initial values computed by the Morphology-based method.</S>
    <S sid="93" ssid="16">We repeat these last 3 models with the addition of the syntagmatic constraints (Synt+Morph).</S>
    <S sid="94" ssid="17">For each of these, we first compare the computed p(tjw) against a gold standard distribution, taken from the test corpus (90K tokens), according to the measure used by (Levinger et al., 1995) (Dist).</S>
    <S sid="95" ssid="18">On this measure, we confirm that our improved morpholexical approximation improves the results reported by Levinger et al. from 74% to about 80% on a richer tagset, and on a much larger test set (90K vs. 3,400 tokens).</S>
    <S sid="96" ssid="19">We then report on the effectiveness of p(tjw) as a context-free tagger that assigns to each word the most likely tag, both for full morphological analysis (3,561 tags) (Full) and for the simpler task of token segmentation and POS tag selection (36 tags) (Seg+Pos).</S>
    <S sid="97" ssid="20">The best results on this task are 80.8% and 87.5% resp. achieved on the Morph+Linear initial conditions.</S>
    <S sid="98" ssid="21">Finally, we test effectiveness of the initial conditions with EM-HMM learning.</S>
    <S sid="99" ssid="22">We reach 88% accuracy on full morphological and 92% accuracy for POS tagging and word segmentation, for the Morph+Linear initial conditions.</S>
    <S sid="100" ssid="23">As expected, EM-HMM improves results (from 80% to 88%).</S>
    <S sid="101" ssid="24">Strikingly, EM-HMM improves the uniform initial conditions from 64% to above 85%.</S>
    <S sid="102" ssid="25">However, better initial conditions bring us much over this particular local maximum &#8211; with an error reduction of 20%.</S>
    <S sid="103" ssid="26">In all cases, the main improvement over the uniform baseline is brought by the morphology-based initial conditions.</S>
    <S sid="104" ssid="27">When applied on its own, the linear context brings modest improvement.</S>
    <S sid="105" ssid="28">But the combination of the paradigmatic morphology-based method with the linear context improves all measures.</S>
    <S sid="106" ssid="29">A most interesting observation is the detrimental contribution of the syntagmatic constraints we introduced.</S>
    <S sid="107" ssid="30">We found that 113,453 sentences of the corpus (about 5%) contradict these basic and apparently simple constraints.</S>
    <S sid="108" ssid="31">As an alternative to these common-sense constraints, we tried to use a small seed of randomly selected sentences (10K annotated tokens) in order to skew the initial uniform distribution of the state transitions.</S>
    <S sid="109" ssid="32">We initialize the p(tjt_1, t+1) distribution with smoothed ML estimates based on tag trigram and bigram counts (ignoring the tag-word annotations).</S>
    <S sid="110" ssid="33">This small seed initialization (InitTrans) has a great impact on accuracy.</S>
    <S sid="111" ssid="34">Overall, we reach 89.4% accuracy on full morphological and 92.4% accuracy for POS tagging and word segmentation, for the Morph+Linear conditions &#8211; an error reduction of more than 25% from the uniform distribution baseline.</S>
  </SECTION>
  <SECTION title="5 Application to English" number="5">
    <S sid="112" ssid="1">We now apply the same technique to English semisupervised POS tagging.</S>
    <S sid="113" ssid="2">Recent investigations of this task use dictionaries derived from the Penn WSJ corpus, with a reduced tag set of 17 tags5 instead of the original 45-tags tagset.</S>
    <S sid="114" ssid="3">They experiment with full dictionaries (containing complete POS information for all the words in the text) as well as &#8220;diluted&#8221; dictionaries, from which large portions of the vocabulary are missing.</S>
    <S sid="115" ssid="4">These settings are very different from those used for Hebrew: the tagset is much smaller (17 vs. 3,560) and the dictionaries are either complete or extremely crippled.</S>
    <S sid="116" ssid="5">However, for the sake of comparison, we have reproduced the same experimental settings.</S>
    <S sid="117" ssid="6">We derive dictionaries from the complete WSJ corpus6, and the exact same diluted dictionaries used in SE, TJ and GG. many of the stop words get wrong analyses stemming from tagging mistakes (for instance, the word the has 6 possible analyses in the data-derived dictionary, which we checked manually and found all but DT erroneous).</S>
    <S sid="118" ssid="7">Such noise is not expected in a real world dictionary, and our algorithm is not designed to accomodate it.</S>
    <S sid="119" ssid="8">We corrected the entries for the 20 most frequent words in the corpus.</S>
    <S sid="120" ssid="9">This step could probably be done automatically, but we consider it to be a non-issue in any realistic setting.</S>
    <S sid="121" ssid="10">Syntagmatic Constraints We indirectly incorporated syntagmatic constraints through a small change to the tagset.</S>
    <S sid="122" ssid="11">The 17-tags English tagset allows for V-V transitions.</S>
    <S sid="123" ssid="12">Such a construction is generally unlikely in English.</S>
    <S sid="124" ssid="13">By separating modals from the rest of the verbs, and creating an additional class for the 5 be verbs (am,is,are,was,were), we made such transition much less probable.</S>
    <S sid="125" ssid="14">The new 19-tags tagset reflects the &#8220;verb can not follow a verb&#8221; constraint.</S>
    <S sid="126" ssid="15">Morphology-Based p(t|w) approximation English morphology is much simpler compared to that of Hebrew, making direct use of the Levinger context free approximation impossible.</S>
    <S sid="127" ssid="16">However, some morphological cues exist in English as well, in particular common suffixation patterns.</S>
    <S sid="128" ssid="17">We implemented our morphology-based context-free p(t|w) approximation for English as a special case of the linear context-based algorithm described in Sect.3.</S>
    <S sid="129" ssid="18">Instead of generating contexts based on neighboring words, we generate them using the following 5 morphological templates: suff=S The word has suffix 5 (suff=ing).</S>
    <S sid="130" ssid="19">L+suff=W,S The word appears just after word W, with suffix 5 (L+suff=have,ed).</S>
    <S sid="131" ssid="20">R+suff=S,W The word appears just before word W, with suffix 5 (R+suff=ing,to) wsuf=S1,S2 The word suffix is 51, the same stem is seen with suffix 52 (wsuf=E,s). suffs=SG The word stem appears with the 5G group of suffixes (suffs=ed,ing,s).</S>
    <S sid="132" ssid="21">We consider a word to have a suffix only if the word stem appears with a different suffix somewhere in the text.</S>
    <S sid="133" ssid="22">We implemented a primitive stemmer for extracting the suffixes while preserving a usable stem by taking care of few English orthography rules (handling, e.g., , bigger &#8594; big er, nicer &#8594; nice er, happily &#8594; happy ly, picnicking &#8594; picnic ing).</S>
    <S sid="134" ssid="23">For the immediate context W in the templates L+suff,R+suff, we consider only the 20 most frequent tokens in the corpus.</S>
    <S sid="135" ssid="24">Linear-Context-based p(t|w) approximation We expect the context based approximation to be particularly useful in English.</S>
    <S sid="136" ssid="25">We use the following 3 context templates: LL=w_2,w_1, LR=w_1,w+1 and RR=w+1,w+2.</S>
    <S sid="137" ssid="26">We estimate p(w|c) and p(c|w) by relative frequency over word triplets occurring at least twice in the unannotated training corpus.</S>
    <S sid="138" ssid="27">Combined p(t|w) approximation This approximation combines the morphological and linear context approximations by using all the abovementioned context templates together in the iterative process.</S>
    <S sid="139" ssid="28">For all three p(t|w) approximations, we take RELC to be contexts containing at most 4 tags. allow(t, w) follows the dictionary for known words, and is the set of all open-class POS for unknown words.</S>
    <S sid="140" ssid="29">We take the initial p(t|w) for each w to be uniform over all the dictionary specified tags for w. Accordingly, the initial p(t|w) = 0 for w not in the dictionary.</S>
    <S sid="141" ssid="30">We run the process for 8 iterations.7 Diluted Dictionaries and Unknown Words Some of the missing dictionary elements are assigned a set of possible POS-tags and corresponding probabilities in the p(t|w) estimation process.</S>
    <S sid="142" ssid="31">Other unknown tokens remain with no analysis at the end of the initial process computation.</S>
    <S sid="143" ssid="32">For these missing elements, we assign an ambiguity class by a simple ambiguity-class guesser, and set p(t|w) to be uniform over all the tags in the ambiguity class.</S>
    <S sid="144" ssid="33">Our ambiguity-class guesser assigns for each word the set of all open-class tags that appeared with the word suffix in the dictionary.</S>
    <S sid="145" ssid="34">The word suffix is the longest (up to 3 characters) suffix of the word that also appears in the top-100 suffixes in the dictionary.</S>
    <S sid="146" ssid="35">Taggers We test the resulting p(t|w) approximation by training 2 taggers: CF-Tag, a context-free tagger assigning for each word its most probable POS according to p(t|w), with a fallback to the most probable tag in case the word does not appear in the dictionary or if &#8704;t, p(t|w) = 0.</S>
    <S sid="147" ssid="36">EM-HMM, a second-order EM-HMM initialized with the estimated p(t|w).</S>
    <S sid="148" ssid="37">Baselines As baseline, we use two EM-trained HMM taggers, initialized with a uniform p(t|w) for every word, based on the allowed tags in the dictionary.</S>
    <S sid="149" ssid="38">For words not in the dictionary, we take the allowed tags to be either all the open-class POS 7This is the first value we tried, and it seems to work fine.</S>
    <S sid="150" ssid="39">We haven&#8217;t experimented with other values.</S>
    <S sid="151" ssid="40">The same applies for the choice of 4 as the RELC threshold.</S>
    <S sid="152" ssid="41">(uniform(oc)) or the allowed tags according to our simple ambiguity-class guesser (uniform(suf)).</S>
    <S sid="153" ssid="42">All the p(tjw) estimates and HMM models are trained on the entire WSJ corpus.</S>
    <S sid="154" ssid="43">We use the same 24K word test-set as used in SE, TJ and GG, as well as the same diluted dictionaries.</S>
    <S sid="155" ssid="44">We report the results on the same reduced tagsets for comparison, but also include the results on the full 46 tags tagset.</S>
    <S sid="156" ssid="45">Table 2 summarizes the results of our experiments.</S>
    <S sid="157" ssid="46">Uniform initialization based on the simple suffixbased ambiguity class guesser yields big improvements over the uniform all-open-class initialization.</S>
    <S sid="158" ssid="47">However, our refined initial conditions always improve the results (by as much as 40% error reduction).</S>
    <S sid="159" ssid="48">As expected, the linear context is much more effective than the morphological one, especially with richer dictionaries.</S>
    <S sid="160" ssid="49">This seem to indicate that in English the linear context is better at refining the estimations when the ambiguity classes are known, while the morphological context is in charge of adding possible tags when the ambiguity classes are not known.</S>
    <S sid="161" ssid="50">Furthermore, the benefit of the morphology-context is bigger for the complete tagset setting, indicating that, while the coarsegrained POS-tags are indicated by word distribution, the finer distinctions are indicated by inflections and orthography.</S>
    <S sid="162" ssid="51">The combination of linear and morphology contexts is always beneficial.</S>
    <S sid="163" ssid="52">Syntagmatic constraints (e.g., separating be verbs and modals from the rest of the verbs) constantly improve results by about 1%.</S>
    <S sid="164" ssid="53">Note that the context-free tagger based on our p(tjw) estimates is quite accurate.</S>
    <S sid="165" ssid="54">As with the EM trained models, combining linear and morphological contexts is always beneficial.</S>
    <S sid="166" ssid="55">To put these numbers in context, Table 3 lists current state-of-the art results for the same task.</S>
    <S sid="167" ssid="56">CE+spl is the Contrastive-Estimation CRF method of SE.</S>
    <S sid="168" ssid="57">BHMM is the completely Bayesian-HMM of GG.</S>
    <S sid="169" ssid="58">PLSA+AC, LDA, LDA+AC are the models presented in TJ, LDA+AC is a Bayesian model with a strong ambiguity class (AC) component, and is the current state-of-the-art of this task.</S>
    <S sid="170" ssid="59">The other models are variations excluding the Bayesian components (PLSA+AC) or the ambiguity class.</S>
    <S sid="171" ssid="60">While our models are trained on the unannotated text of the entire WSJ Treebank, CE and BHMM use much less training data (only the 24k words of the test-set).</S>
    <S sid="172" ssid="61">However, as noted by TJ, there is no reason one should limit the amount of unlabeled data used, and in addition other results reported in GG,SE show that accuracy does not seem to improve as more unlabeled data are used with the models.</S>
    <S sid="173" ssid="62">We also report results for training our EM-HMM tagger on the smaller dataset (the p(tjw) estimation is still based on the entire unlabeled WSJ).</S>
    <S sid="174" ssid="63">All the abovementioned models follow the assumption that all 17 tags are valid for the unknown words.</S>
    <S sid="175" ssid="64">In contrast, we restrict the set of allowed tags for an unknown word to open-class tags.</S>
    <S sid="176" ssid="65">Closed class words are expected to be included in a dictionary, even a small one.</S>
    <S sid="177" ssid="66">The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al., 1993), and proved highly beneficial also in our case.</S>
    <S sid="178" ssid="67">Notice that even our simplest models, in which the initial p(tjw) distribution for each w is uniform, already outperform most of the other models, and, in the case of the diluted dictionaries, by a wide margin.</S>
    <S sid="179" ssid="68">Similarly, given the p(tjw) estimate, EMHMM training on the smaller dataset (24k) is still very competitive (yet results improve with more unlabeled data).</S>
    <S sid="180" ssid="69">When we use our refined p(tjw) distribution as the basis of EM-HMM training, we get the best results for the complete dictionary case.</S>
    <S sid="181" ssid="70">With the diluted dictionaries, we are outperformed only by LDA+AC.</S>
    <S sid="182" ssid="71">As we outperform this model in the complete dictionary case, it seems that the advantage of this model is due to its much stronger ambiguity class model, and not its Bayesian components.</S>
    <S sid="183" ssid="72">Also note that while we outperform this model when using the 19-tags tagset, it is slightly better in the original 17-tags setting.</S>
    <S sid="184" ssid="73">It could be that the reliance of the LDA models on observed surface features instead of hidden state features is beneficial avoiding the misleading V-V transitions.</S>
    <S sid="185" ssid="74">We also list the performance of our best models with a slightly more realistic dictionary setting: we take our dictionary to include information for all words occurring in section 0-18 of the WSJ corpus (43208 words).</S>
    <S sid="186" ssid="75">We then train on the entire unannotated corpus, and test on sections 22-24 &#8211; the standard train/test split for supervised English POS tagging.</S>
    <S sid="187" ssid="76">We achieve accuracy of 92.85% for the 19tags set, and 91.3% for the complete 46-tags tagset.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="6">
    <S sid="188" ssid="1">We have demonstrated that unsupervised POS tagging can reach good results using the robust EMHMM learner when provided with good initial conditions, even with incomplete dictionaries.</S>
    <S sid="189" ssid="2">We presented a general family of algorithms to compute effective initial conditions: estimation of p(t|w) relying on an iterative process shifting probabilities between words and their contexts.</S>
    <S sid="190" ssid="3">The parameters of this process (definition of the contexts and initial estimations of p(t|w) can safely encapsulate rich linguistic intuitions.</S>
    <S sid="191" ssid="4">While recent work, such as GG, aim to use the Bayesian framework and incorporate &#8220;linguistically motivated priors&#8221;, in practice such priors currently only account for the fact that language related distributions are sparse - a very general kind of knowledge.</S>
    <S sid="192" ssid="5">In contrast, our method allow the incorporation of much more fine-grained intuitions.</S>
    <S sid="193" ssid="6">We tested the method on the challenging task of full morphological disambiguation in Hebrew (which was our original motivation) and on the standard WSJ unsupervised POS tagging task.</S>
    <S sid="194" ssid="7">In Hebrew, our model includes an improved version of the similar words algorithm of (Levinger et al., 1995), a model of lexical context, and a small set of tag ngrams.</S>
    <S sid="195" ssid="8">The combination of these knowledge sources in the initial conditions brings an error reduction of more than 25% over a strong uniform distribution baseline.</S>
    <S sid="196" ssid="9">In English, our model is competitive with recent state-of-the-art results, while using simple and efficient learning methods.</S>
    <S sid="197" ssid="10">The comparison with other algorithms indicates directions of potential improvement: (1) our initialconditions method might benefit the other, more sophisticated learning algorithms as well.</S>
    <S sid="198" ssid="11">(2) Our models were designed under the assumption of a relatively complete dictionary.</S>
    <S sid="199" ssid="12">As such, they are not very good at assigning ambiguity-classes to OOV tokens when starting with a very small dictionary.</S>
    <S sid="200" ssid="13">While we demonstrate competitive results using a simple suffix-based ambiguity-class guesser which ignores capitalization and hyphenation information, we believe there is much room for improvement in this respect.</S>
    <S sid="201" ssid="14">In particular, (Haghighi and Klein, 2006) presents very strong results using a distributional-similarity module and achieve impressive tagging accuracy while starting with a mere 116 prototypical words.</S>
    <S sid="202" ssid="15">Experimenting with combining similar models (as well as TJ&#8217;s ambiguity class model) with our p(t|w) distribution estimation method is an interesting research direction.</S>
  </SECTION>
</PAPER>
