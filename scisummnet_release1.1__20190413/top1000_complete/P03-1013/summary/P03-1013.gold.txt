Probabilistic Parsing For German Using Sister-Head Dependencies
We present a probabilistic parsing model for German trained on the Negra treebank.
We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German.
Learning curves show that this effect is not due to lack of training data.
We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
This model outperforms the baseline, achieving a labeled precision and recall of up to 74%.
This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra.
We show that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains.
We show that the task of assigning correct grammatical functions is harder than mere constituent-based parsing.
