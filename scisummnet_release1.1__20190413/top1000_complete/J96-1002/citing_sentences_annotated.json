[
  {
    "citance_No": 1, 
    "citing_paper_id": "W97-1005", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mehmet, Kayaalp | Ted, Pedersen | Rebecca F., Bruce", 
    "raw_text": "The approach made use of a maximum entropy model (Berger et al, 1996) formulated from frequency information for various combinations of the observed features", 
    "clean_text": "The approach made use of a maximum entropy model (Berger et al, 1996) formulated from frequency information for various combinations of the observed features.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1137", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Deyi, Xiong | Min, Zhang", 
    "raw_text": "This is not a issue for the MaxEnt classifier as it can deal with arbitrary overlapping features (Berger et al, 1996)", 
    "clean_text": "This is not a issue for the MaxEnt classifier as it can deal with arbitrary overlapping features (Berger et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W06-1643", 
    "citing_paper_authority": 23, 
    "citing_paper_authors": "Michel, Galley", 
    "raw_text": "We performed feature selection by incrementally growing a log-linear model with order0 features f (x ,yt) using a forward feature selection procedure similar to (Berger et al, 1996)", 
    "clean_text": "We performed feature selection by incrementally growing a log-linear model with order0 features f (x,yt) using a forward feature selection procedure similar to (Berger et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D11-1055", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Dani, Yogatama | Michael, Heilman | Brendan, O'Connor | Chris, Dyer | Bryan R., Routledge | Noah A., Smith", 
    "raw_text": "model (Berger et al, 1996) or a log-linear model", 
    "clean_text": "For the ACL data, where response is the binary cited-or-not variable we use logistic regression, often referred to as a \"maximum entropy\" model (Berger et al., 1996) or a log-linear model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P99-1069", 
    "citing_paper_authority": 65, 
    "citing_paper_authors": "Mark, Johnson | Stuart, Geman | Stephen, Canon | Zhiyi, Chi | Stefan, Riezler", 
    "raw_text": "Berger et al (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument", 
    "clean_text": "Berger et al (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "H05-1059", 
    "citing_paper_authority": 39, 
    "citing_paper_authors": "Yoshimasa, Tsuruoka | Jun'ichi, Tsujii", 
    "raw_text": "A common choice for the local probabilistic classifier is maximum entropy classifiers (Berger et al, 1996)", 
    "clean_text": "A common choice for the local probabilistic classifier is maximum entropy classifiers (Berger et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "H05-1059", 
    "citing_paper_authority": 39, 
    "citing_paper_authors": "Yoshimasa, Tsuruoka | Jun'ichi, Tsujii", 
    "raw_text": "For local classifiers, we used a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing (Berger et al, 1996)", 
    "clean_text": "For local classifiers, we used a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing (Berger et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D09-1160", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Naoki, Yoshinaga | Masaru, Kitsuregawa", 
    "raw_text": "The log-linear model (LLM), or also known as maximum-entropy model (Berger et al, 1996), is a linear classifier widely used in the NLP literature", 
    "clean_text": "The log-linear model (LLM), or also known as maximum-entropy model (Berger et al, 1996), is a linear classifier widely used in the NLP literature.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P05-1020", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Vincent, Ng", 
    "raw_text": "We consider three learning algorithms, namely, the C4.5 decision tree induction system (Quinlan, 1993), the RIPPER rule learning algorithm (Cohen, 1995), and maximum entropy classification (Berger et al, 1996)", 
    "clean_text": "We consider three learning algorithms, namely, the C4.5 decision tree induction system (Quinlan, 1993), the RIPPER rule learning algorithm (Cohen, 1995), and maximum entropy classification (Berger et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "S10-1064", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Bin, Lu | Benjamin K., T'sou", 
    "raw_text": "Maximum entropy classification (MaxEnt) is a technique which has proven effective in a number of natural language processing applications (Berger et al, 1996)", 
    "clean_text": "Maximum entropy classification (MaxEnt) is a technique which has proven effective in a number of natural language processing applications (Berger et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W11-2708", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Dani&euml;l, de Kok", 
    "raw_text": "Since it is computationally intractable to do this for every fi in Z, Berger et al (1996) propose to estimate the weight wi of the candidate feature fi, while assuming that the weights of features in S stay constant", 
    "clean_text": "Berger et al (1996) propose to estimate the weight wi of the candidate feature fi, while assuming that the weights of features in S stay constant.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W11-2708", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Dani&euml;l, de Kok", 
    "raw_text": "Under this assumption ,wi can be estimated using a simple line search method. However, Zhou et al (2003) observe that ,de spite this simplification, the gain-informed selection method proposed by Berger et al (1996) still recalculates the weights of all the candidate features during every cycle", 
    "clean_text": "the gain-informed selection method proposed by Berger et al (1996) still recalculates the weights of all the candidate features during every cycle.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "C08-1041", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Zhongjun, He | Qun, Liu | Shouxun, Lin", 
    "raw_text": "The maximum entropy approach (Berger et al, 1996) is known to be well suited to solve the classification problem", 
    "clean_text": "The maximum entropy approach (Berger et al, 1996) is known to be well suited to solve the classification problem.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W12-1637", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Anton, Leuski | David, DeVault", 
    "raw_text": "The NLU uses a maximum-entropy model (Berger et al, 1996) to classify utterances as one of the user SAs using shallow text features", 
    "clean_text": "The NLU uses a maximum-entropy model (Berger et al, 1996) to classify utterances as one of the user SAs using shallow text features.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N07-1001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Vivek Kumar, Rangarajan Sridhar | Srinivas, Bangalore | Shrikanth S., Narayanan", 
    "raw_text": "we use the general technique of choosing the maximum entropy (maxent) distribution that estimates the average of each feature over the training data (Berger et al, 1996)", 
    "clean_text": "we use the general technique of choosing the maximum entropy (maxent) distribution that estimates the average of each feature over the training data (Berger et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "I08-2122", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Yoshinobu, Kano | Ngan, Nguyen | Rune, S&aelig;tre | Kazuhiro, Yoshida | Keiichiro, Fukamachi | Yusuke, Miyao | Yoshimasa, Tsuruoka | Sophia, Ananiadou | Jun'ichi, Tsujii", 
    "raw_text": "Uses Maximum Entropy (Berger et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER)", 
    "clean_text": "Uses Maximum Entropy (Berger et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P02-1038", 
    "citing_paper_authority": 218, 
    "citing_paper_authors": "Franz Josef, Och | Hermann, Ney", 
    "raw_text": "An especially well-founded framework for doing this is maximum entropy (Berger et al, 1996)", 
    "clean_text": "An especially well-founded framework for doing this is maximum entropy (Berger et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W05-0509", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Felice, Dell'Orletta | Alessandro, Lenci | Simonetta, Montemagni | Vito, Pirrelli", 
    "raw_text": "It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy, is unique and has the following exponential form (Berger et al 1996): (1)?== k j cajf jcZ cap 1), () (1)| (a where Z (c) is a normalization factor, f j (a, c) are the values of k features of the pair (a, c) and correspond to the linguistic cues of c that are relevant to predict the outcome a. Features are extracted from the training data and define the constraints that the probabilistic model p must satisfy", 
    "clean_text": "It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy, is unique and has the following exponential form (Berger et al 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W06-1617", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Zheng Ping, Jiang | Hwee Tou, Ng", 
    "raw_text": "Since its introduction to the Natural Language Processing (NLP) community (Berger et al, 1996), ME-based classifiers have been shown to be effective in various NLP tasks", 
    "clean_text": "Since its introduction to the Natural Language Processing (NLP) community (Berger et al, 1996), ME-based classifiers have been shown to be effective in various NLP tasks.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "N06-1013", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Necip Fazil, Ayan | Bonnie Jean, Dorr", 
    "raw_text": "Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al, 1996), parsing, POS tagging and PP attachment (Ratna park hi, 1998), machine translation (Och and Ney, 2002), and FrameNet classification (Fleischman et al., 2003)", 
    "clean_text": "Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al, 1996),.", 
    "keep_for_gold": 0
  }
]
