<PAPER>
  <S sid="0">Extracting Opinions Opinion Holders And Topics Expressed In Online News Media Text</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts.</S>
    <S sid="2" ssid="2">We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.</S>
    <S sid="3" ssid="3">This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet.</S>
    <S sid="4" ssid="4">We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.</S>
    <S sid="5" ssid="5">For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.</S>
    <S sid="6" ssid="6">Our experimental results show that our system performs significantly better than the baseline.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">The challenge of automatically identifying opinions in text automatically has been the focus of attention in recent years in many different domains such as news articles and product reviews.</S>
    <S sid="8" ssid="2">Various approaches have been adopted in subjectivity detection, semantic orientation detection, review classification and review mining.</S>
    <S sid="9" ssid="3">Despite the successes in identifying opinion expressions and subjective words/phrases, there has been less achievement on the factors closely related to subjectivity and polarity, such as opinion holder, topic of opinion, and inter-topic/inter-opinion relationships.</S>
    <S sid="10" ssid="4">This paper addresses the problem of identifying not only opinions in text but also holders and topics of opinions from online news articles.</S>
    <S sid="11" ssid="5">Identifying opinion holders is important especially in news articles.</S>
    <S sid="12" ssid="6">Unlike product reviews in which most opinions expressed in a review are likely to be opinions of the author of the review, news articles contain different opinions of different opinion holders (e.g. people, organizations, and countries).</S>
    <S sid="13" ssid="7">By grouping opinion holders of different stance on diverse social and political issues, we can have a better understanding of the relationships among countries or among organizations.</S>
    <S sid="14" ssid="8">An opinion topic can be considered as an object an opinion is about.</S>
    <S sid="15" ssid="9">In product reviews, for example, opinion topics are often the product itself or its specific features, such as design and quality (e.g.</S>
    <S sid="16" ssid="10">&#8220;I like the design of iPod video&#8221;, &#8220;The sound quality is amazing&#8221;).</S>
    <S sid="17" ssid="11">In news articles, opinion topics can be social issues, government&#8217;s acts, new events, or someone&#8217;s opinions.</S>
    <S sid="18" ssid="12">(e.g., &#8220;Democrats in Congress accused vice president Dick Cheney&#8217;s shooting accident.&#8221;, &#8220;Shiite leaders accused Sunnis of a mass killing of Shiites in Madaen, south of Baghdad.&#8221;) As for opinion topic identification, little research has been conducted, and only in a very limited domain, product reviews.</S>
    <S sid="19" ssid="13">In most approaches in product review mining, given a product (e.g. mp3 player), its frequently mentioned features (e.g. sound, screen, and design) are first collected and then used as anchor points.</S>
    <S sid="20" ssid="14">In this study, we extract opinion topics from news articles.</S>
    <S sid="21" ssid="15">Also, we do not pre-limit topics in advance.</S>
    <S sid="22" ssid="16">We first identify an opinion and then find its holder and topic.</S>
    <S sid="23" ssid="17">We define holder as an entity who holds an opinion, and topic, as what the opinion is about.</S>
    <S sid="24" ssid="18">In this paper, we propose a novel method that employs Semantic Role Labeling, a task of identifying semantic roles given a sentence.</S>
    <S sid="25" ssid="19">We deProceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1&#8211;8, Sydney, July 2006. c&#65533;2006 Association for Computational Linguistics compose the overall task into the following steps: In this paper, we focus on the first three subtasks.</S>
    <S sid="26" ssid="20">The main contribution of this paper is to present a method that identifies not only opinion holders but also opinion topics.</S>
    <S sid="27" ssid="21">To achieve this goal, we utilize FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics, and then use them for system training.</S>
    <S sid="28" ssid="22">We demonstrate that investigating semantic relations between an opinion and its holder and topic is crucial in opinion holder and topic identification.</S>
    <S sid="29" ssid="23">This paper is organized as follows: Section 2 briefly introduces related work both in sentiment analysis and semantic role labeling.</S>
    <S sid="30" ssid="24">Section 3 describes our approach for identifying opinions and labeling holders and topics by utilizing FrameNet1 data for our task.</S>
    <S sid="31" ssid="25">Section 4 reports our experiments and results with discussions and finally Section 5 concludes.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="32" ssid="1">This section reviews previous works in both sentiment detection and semantic role labeling.</S>
    <S sid="33" ssid="2">Subjectivity detection is the task of identifying subjective words, expressions, and sentences (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003).</S>
    <S sid="34" ssid="3">Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc.</S>
    <S sid="35" ssid="4">Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005), phrases and sentences (Kim and Hovy, 2004; Wilson et al., 2005), or documents (Pang et al., 2002; Turney, 2002).</S>
    <S sid="36" ssid="5">Building on this work, more sophisticated problems such as opinion holder identification have also been studied.</S>
    <S sid="37" ssid="6">(Bethard et al., 2004) identify opinion propositions and holders.</S>
    <S sid="38" ssid="7">Their work is similar to ours but different because their opinion is restricted to propositional opinion and mostly to verbs.</S>
    <S sid="39" ssid="8">Another related works are (Choi et al., 2005; Kim and Hovy, 2005).</S>
    <S sid="40" ssid="9">Both of them use the MPQA corpus2 but they only identify opinion holders, not topics.</S>
    <S sid="41" ssid="10">As for opinion topic identification, little research has been conducted, and only in a very limited domain, product reviews.</S>
    <S sid="42" ssid="11">(Hu and Liu, 2004; Popescu and Etzioni, 2005) present product mining algorithms with extracting certain product features given specific product types.</S>
    <S sid="43" ssid="12">Our paper aims at extracting topics of opinion in general news media text.</S>
    <S sid="44" ssid="13">Semantic role labeling is the task of identifying semantic roles such as Agent, Patient, Speaker, or Topic, in a sentence.</S>
    <S sid="45" ssid="14">A statistical approach for semantic role labeling was introduced by (Gildea and Jurafsky, 2002).</S>
    <S sid="46" ssid="15">Their system learned semantic relationship among constituents in a sentence from FrameNet, a large corpus of semantically hand-annotated data.</S>
    <S sid="47" ssid="16">The FrameNet annotation scheme is based on Frame Semantics (Fillmore, 1976).</S>
    <S sid="48" ssid="17">Frames are defined as &#8220;schematic representations of situations involving various frame elements such as participants, props, and other conceptual roles.&#8221; For example, given a sentence &#8220;Jack built a new house out of bricks&#8221;, a semantic role labeling system should identify the roles for the verb built such as &#8220;[Agent Jack] built [Created_entity a new house] [Component out of bricks]&#8221;3.</S>
    <S sid="49" ssid="18">In our study, we build a semantic role labeling system as an intermediate step to label opinion holders and topics by training it on opinion-bearing frames and their frame elements in FrameNet.</S>
    <S sid="50" ssid="19">For the goal of this study, extracting opinions from news media texts with their holders and topics, we utilize FrameNet data.</S>
    <S sid="51" ssid="20">The basic idea of our approach is to explore how an opinion holder and a topic are semantically related to an opinion bearing word in a sentence.</S>
    <S sid="52" ssid="21">Given a sentence and an opinion bearing word, our method identifies frame elements in the sentence and searches which frame element corresponds to the opinion holder and which to the topic.</S>
    <S sid="53" ssid="22">The example in Figure 1 shows the intuition of our algorithm.</S>
    <S sid="54" ssid="23">We decompose our task in 3 subtasks: (1) collect opinion words and opinion-related frames, We describe the subtask of collecting opinion words and related frames in 3 phases.</S>
    <S sid="55" ssid="24">In this study, we consider an opinion-bearing (positive/negative) word is a key indicator of an opinion.</S>
    <S sid="56" ssid="25">Therefore, we first identify opinionbearing word from a given sentence and extract its holder and topic.</S>
    <S sid="57" ssid="26">Since previous studies indicate that opinion-bearing verbs and adjectives are especially efficient for opinion identification, we focus on creating a set of opinion-bearing verbs and adjectives.</S>
    <S sid="58" ssid="27">We annotated 1860 adjectives and 2011 verbs4 by classifying them into positive, negative, and neutral classes.</S>
    <S sid="59" ssid="28">Words in the positive class carry positive valence whereas 4 These were randomly selected from 8011 English verbs and 19748 English adjectives. those in negative class carry negative valence.</S>
    <S sid="60" ssid="29">Words that are not opinion-bearing are classified as neutral.</S>
    <S sid="61" ssid="30">Note that in our study we treat word sentiment classification as a three-way classification problem instead of a two-way classification problem (i.e. positive and negative).</S>
    <S sid="62" ssid="31">By adding the third class, neutral, we can prevent the classifier assigning either positive or negative sentiment to weak opinion-bearing word.</S>
    <S sid="63" ssid="32">For example, the word &#8220;central&#8221; that Hatzivassiloglou and McKeown (1997) marked as a positive adjective is not classified as positive by our system.</S>
    <S sid="64" ssid="33">Instead we mark it as &#8220;neutral&#8221;, since it is a weak clue for an opinion.</S>
    <S sid="65" ssid="34">For the same reason, we did not consider &#8220;able&#8221; classified as a positive word by General Inquirer5, a sentiment word lexicon, as a positive opinion indicator.</S>
    <S sid="66" ssid="35">Finally, we collected 69 positive and 151 negative verbs and 199 positive and 304 negative adjectives.</S>
    <S sid="67" ssid="36">We collected frames related to opinion words from the FrameNet corpus.</S>
    <S sid="68" ssid="37">We used FrameNet II (Baker et al., 2003) which contains 450 semantic frames and more than 3000 frame elements (FE).</S>
    <S sid="69" ssid="38">A frame consists of lexical items, called Lexical Unit (LU), and related frame elements.</S>
    <S sid="70" ssid="39">For instance, LUs in ATTACK frame are verbs such as assail, assault, and attack, and nouns such as invasion, raid, and strike.</S>
    <S sid="71" ssid="40">FrameNet II contains approximately 7500 lexical units and over 100,000 annotated sentences.</S>
    <S sid="72" ssid="41">For each word in our opinion word set described in Phase 1, we find a frame to which the word belongs.</S>
    <S sid="73" ssid="42">49 frames for verbs and 43 frames for adjectives are collected.</S>
    <S sid="74" ssid="43">Table 1 shows examples of selected frames with some of the lexical units those frames cover.</S>
    <S sid="75" ssid="44">For example, our system found the frame Desiring from opinionbearing words want, wish, hope, etc.</S>
    <S sid="76" ssid="45">Finally, we collected 8256 and 11877 sentences related to selected opinion bearing frames for verbs and adjectives respectively.</S>
    <S sid="77" ssid="46">Phase 3: FrameNet expansion Even though Phase 2 searches for a correlated frame for each verb and adjective in our opinionbearing word list, not all of them are defined in FrameNet data.</S>
    <S sid="78" ssid="47">Some words such as criticize and harass in our list have associated frames (Case 1), whereas others such as vilify and maltreat do not have those (Case 2).</S>
    <S sid="79" ssid="48">For a word in Case 2, we use a clustering algorithms CBC (Clustering By Committee) to predict the closest (most reasonable) frame of undefined word from existing frames.</S>
    <S sid="80" ssid="49">CBC (Pantel and Lin, 2002) was developed based on the distributional hypothesis (Harris, 1954) that words which occur in the same contexts tend to be similar.</S>
    <S sid="81" ssid="50">Using CBC, for example, our clustering module computes lexical similarity between the word vilify in Case 2 and all words in Case 1.</S>
    <S sid="82" ssid="51">Then it picks criticize as a similar word, so that we can use for vilify the frame Judgment_communication to which criticize belongs and all frame elements defined under Judgment_ communication.</S>
    <S sid="83" ssid="52">To find a potential holder and topic of an opinion word in a sentence, we first label semantic roles in a sentence.</S>
    <S sid="84" ssid="53">Modeling: We follow the statistical approaches for semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et. al, 2003) which separate the task into two steps: identify candidates of frame elements (Step 1) and assign semantic roles for those candidates (Step 2).</S>
    <S sid="85" ssid="54">Like their intuition, we treated both steps as classification problems.</S>
    <S sid="86" ssid="55">We first collected all constituents of the given sentence by parsing it using the Charniak parser.</S>
    <S sid="87" ssid="56">Then, in Step 1, we classified candidate constituents of frame elements from non-candidates.</S>
    <S sid="88" ssid="57">In Step 2, each selected candidate was thus classified into one of frame element types (e.g.</S>
    <S sid="89" ssid="58">Stimulus, Degree, Experiencer, etc.).</S>
    <S sid="90" ssid="59">As a learning algorithm for our classification model, we used Maximum Entropy (Berger et al., 1996).</S>
    <S sid="91" ssid="60">For system development, we used MEGA model optimization package6, an implementation of ME models.</S>
    <S sid="92" ssid="61">Data: We collected 8256 and 11877 sentences which were associated to opinion bearing frames for verbs and adjectives from FrameNet annotation data.</S>
    <S sid="93" ssid="62">Each sentence in our dataset contained a frame name, a target predicate (a word whose meaning represents aspects of the frame), and frame elements labeled with element types.</S>
    <S sid="94" ssid="63">We divided the data into 90% for training and 10% for test.</S>
    <S sid="95" ssid="64">Features used: Table 2 describes features that we used for our classification model.</S>
    <S sid="96" ssid="65">The target word is an opinion-bearing verb or adjective which is associated to a frame.</S>
    <S sid="97" ssid="66">We used the Charniak parser to get a phrase type feature of a frame element and the parse tree path feature.</S>
    <S sid="98" ssid="67">We determined a head word of a phrase by an algorithm using a tree head table7, position feature by the order of surface words of a frame element and the target word, and the voice feature by a simple pattern.</S>
    <S sid="99" ssid="68">Frame name for a target word was selected by methods described in Phase 2 and Phase 3 in Subsection 3.1.</S>
    <S sid="100" ssid="69">After identifying frame elements in a sentence, our system finally selects holder and topic from those frame elements.</S>
    <S sid="101" ssid="70">In the example in Table 1, the frame &#8220;Desiring&#8221; has frame elements such as Event (&#8220;The change that the Experiencer would like to see&#8221;), Experiencer (&#8220;the person or sentient being who wishes for the Event to occur&#8221;), Location_of_event (&#8220;the place involved in the desired Event&#8221;), Focal_participant (&#8220;entity that the Experiencer wishes to be affected by some Event&#8221;).</S>
    <S sid="102" ssid="71">Among these FEs, we can consider that Experiencer can be a holder and Focal_participant can be a topic (if any exists in a sentence).</S>
    <S sid="103" ssid="72">We manually built a mapping table to map FEs to holder or topic using as support the FE definitions in each opinion related frame and the annotated sample sentences.</S>
  </SECTION>
  <SECTION title="4 Experimental Results" number="3">
    <S sid="104" ssid="1">The goal of our experiment is first, to see how our holder and topic labeling system works on the FrameNet data, and second, to examine how it performs on online news media text.</S>
    <S sid="105" ssid="2">The first data set (Testset 1) consists of 10% of data described in Subsection 3.2 and the second (Testset 2) is manually annotated by 2 humans.</S>
    <S sid="106" ssid="3">(see Subsection 4.2).</S>
    <S sid="107" ssid="4">We report experimental results for both test sets.</S>
    <S sid="108" ssid="5">Gold Standard: In total, Testset 1 contains 2028 annotated sentences collected from FrameNet data set.</S>
    <S sid="109" ssid="6">(834 from frames related to opinion verb and 1194 from opinion adjectives) We measure the system performance using precision (the percentage of correct holders/topics among system&#8217;s labeling results), recall (the percentage of correct holders/topics that system retrieved), and F-score.</S>
    <S sid="110" ssid="7">Baseline: For the baseline system, we applied two different algorithms for sentences which have opinion-bearing verbs as target words and for those that have opinion-bearing adjectives as target words.</S>
    <S sid="111" ssid="8">For verbs, baseline system labeled a subject of a verb as a holder and an object as a topic.</S>
    <S sid="112" ssid="9">(e.g.</S>
    <S sid="113" ssid="10">&#8220;[holder He] condemned [topic the lawyer].&#8221;) For adjectives, the baseline marked the subject of a predicate adjective as a holder (e.g.</S>
    <S sid="114" ssid="11">&#8220;[holder I] was happy&#8221;).</S>
    <S sid="115" ssid="12">For the topics of adjectives, the baseline picks a modified word if the target adjective is a modifier (e.g.</S>
    <S sid="116" ssid="13">&#8220;That was a stupid [topic mistake]&#8221;.) and a subject word if the adjective is a predicate.</S>
    <S sid="117" ssid="14">([topic The view] is breathtaking in January.)</S>
    <S sid="118" ssid="15">Result: Table 3 and 4 show evaluation results of our system and the baseline system respectively.</S>
    <S sid="119" ssid="16">Our system performed much better than the baseline system in identifying topic and holder for both sets of sentences with verb target words and those with adjectives.</S>
    <S sid="120" ssid="17">Especially in recognizing topics of target opinion-bearing words, our system improved F-score from 30.4% to 66.5% for verb target words and from 38.2% to 70.3% for adjectives.</S>
    <S sid="121" ssid="18">It was interesting to see that the intuition that &#8220;A subject of opinionbearing verb is a holder and an object is a topic&#8221; which we applied for the baseline achieved relatively good F-score (56.9%).</S>
    <S sid="122" ssid="19">However, our system obtained much higher F-score (78.7%).</S>
    <S sid="123" ssid="20">Holder identification task achieved higher Fscore than topic identification which implies that identifying topics of opinion is a harder task.</S>
    <S sid="124" ssid="21">We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics that simple relations such as subject and object relations are not able to capture.</S>
    <S sid="125" ssid="22">For example, in a sentence &#8220;Her letter upset me&#8221;, simply looking for the subjective and objective of the verb upset is not enough to recognize the holder and topic.</S>
    <S sid="126" ssid="23">It is necessary to see a deeper level of semantic relations: &#8220;Her letter&#8221; is a stimulus and &#8220;me&#8221; is an experiencer of the verb upset.</S>
    <S sid="127" ssid="24">Gold Standard: Two humans 8 annotated 100 sentences randomly selected from news media texts.</S>
    <S sid="128" ssid="25">Those news data is collected from online news sources such as The New York Times, UN Office for the Coordination of Humanitarian Affairs, and BBC News9, which contain articles about various international affaires.</S>
    <S sid="129" ssid="26">Annotators identified opinion-bearing sentences with marking opinion word with its holder and topic if they existed.</S>
    <S sid="130" ssid="27">The inter-annotator agreement in identifying opinion sentences was 82%.</S>
    <S sid="131" ssid="28">Baseline: In order to identify opinion-bearing sentences for our baseline system, we used the opinion-bearing word set introduced in Phase 1 in Subsection 3.1.</S>
    <S sid="132" ssid="29">If a sentence contains an opinion-bearing verb or adjective, the baseline system started looking for its holder and topic.</S>
    <S sid="133" ssid="30">For holder and topic identification, we applied the same baseline algorithm as described in Subsection 4.1 to Testset 2.</S>
    <S sid="134" ssid="31">Result: Note that Testset 1 was collected from sentences of opinion-related frames in FrameNet and therefore all sentences in the set contained either opinion-bearing verb or adjective.</S>
    <S sid="135" ssid="32">(i.e.</S>
    <S sid="136" ssid="33">All sentences are opinion-bearing) However, sentences in Testset 2 were randomly collected from online news media pages and therefore not all of them are opinion-bearing.</S>
    <S sid="137" ssid="34">We first evaluated the task of opinion-bearing sentence identification.</S>
    <S sid="138" ssid="35">Table 5 shows the system results.</S>
    <S sid="139" ssid="36">When we mark all sentences as opinion-bearing, it achieved 43% and 38% of accuracy for the annotation result of Human1 and Human2 respectively.</S>
    <S sid="140" ssid="37">Our system performance (64% and 55%) is comparable with the unique assignment.</S>
    <S sid="141" ssid="38">We measured the holder and topic identification system with precision, recall, and F-score.</S>
    <S sid="142" ssid="39">As we can see from Table 6, our system achieved much higher precision than the baseline system for both Topic and Holder identification tasks.</S>
    <S sid="143" ssid="40">However, we admit that there is still a lot of room for improvement.</S>
    <S sid="144" ssid="41">The system achieved higher precision for topic identification, whereas it achieved higher recall for holder identification.</S>
    <S sid="145" ssid="42">In overall, our system attained higher F-score in holder identification task, including the baseline system.</S>
    <S sid="146" ssid="43">Based on Fscore, we believe that identifying topics of opinion is much more difficult than identifying holders.</S>
    <S sid="147" ssid="44">It was interesting to see the same phenomenon that the baseline system mainly assuming that subject and object of a sentence are likely to be opinion holder and topic, achieved lower scores for both holder and topic identification tasks in Testset 2 as in Testset 1.</S>
    <S sid="148" ssid="45">This implies that more sophisticated analysis of the relationship between opinion words (e.g. verbs and adjectives) and their topics and holders is crucial.</S>
    <S sid="149" ssid="46">We observed several difficulties in evaluating holder and topic identification.</S>
    <S sid="150" ssid="47">First, the boundary of an entity of holder or topic can be flexible.</S>
    <S sid="151" ssid="48">For example, in sentence &#8220;Senator Titus Olupitan who sponsored the bill wants the permission.&#8221;, not only &#8220;Senator Titus Olupitan&#8221; but also &#8220;Senator Titus Olupitan who sponsored the bill&#8221; is an eligible answer.</S>
    <S sid="152" ssid="49">Second, some correct holders and topics which our system found were evaluated wrong even if they referred the same entities in the gold standard because human annotators marked only one of them as an answer.</S>
    <S sid="153" ssid="50">In the future, we need more annotated data for improved evaluation.</S>
  </SECTION>
  <SECTION title="5 Conclusion and Future Work" number="4">
    <S sid="154" ssid="1">This paper presented a methodology to identify an opinion with its holder and topic given a sentence in online news media texts.</S>
    <S sid="155" ssid="2">We introduced an approach of exploiting semantic structure of a sentence, anchored to an opinion bearing verb or adjective.</S>
    <S sid="156" ssid="3">This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.</S>
    <S sid="157" ssid="4">Our method first identifies an opinion-bearing word, labels semantic roles related to the word in the sentence, and then finds a holder and a topic of the opinion word among labeled semantic roles.</S>
    <S sid="158" ssid="5">There has been little previous study in identifying opinion holders and topics partly because it requires a great amount of annotated data.</S>
    <S sid="159" ssid="6">To overcome this barrier, we utilized FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics.</S>
    <S sid="160" ssid="7">However, FrameNet has a limited number of words in its annotated corpus.</S>
    <S sid="161" ssid="8">For a broader coverage, we used a clustering technique to predict a most probable frame for an unseen word.</S>
    <S sid="162" ssid="9">Our experimental results showed that our system performs significantly better than the baseline.</S>
    <S sid="163" ssid="10">The baseline system results imply that opinion holder and topic identification is a hard task.</S>
    <S sid="164" ssid="11">We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics which simple relations such as subject and object relations are not able to capture.</S>
    <S sid="165" ssid="12">In the future, we plan to extend our list of opinion-bearing verbs and adjectives so that we can discover and apply more opinion-related frames.</S>
    <S sid="166" ssid="13">Also, it would be interesting to see how other types of part of speech such as adverbs and nouns affect the performance of the system.</S>
  </SECTION>
</PAPER>
