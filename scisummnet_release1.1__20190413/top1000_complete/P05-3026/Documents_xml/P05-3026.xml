<PAPER>
  <S sid="0">Multi-Engine Machine Translation Guided By Explicit Word Matching</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input.</S>
    <S sid="2" ssid="2">The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality.</S>
    <S sid="3" ssid="3">Our approach uses the individual MT engines as &#8220;black boxes&#8221; and does not require any explicit cooperation from the original MT systems.</S>
    <S sid="4" ssid="4">A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines.</S>
    <S sid="5" ssid="5">The highest scoring sentence hypothesis is selected as the final output of our system.</S>
    <S sid="6" ssid="6">Experiments, using several Arabicto-English systems of similar quality, show a substantial improvement in the quality of the translation output.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">A variety of different paradigms for machine translation (MT) have been developed over the years, ranging from statistical systems that learn mappings between words and phrases in the source language and their corresponding translations in the target language, to Interlingua-based systems that perform deep semantic analysis.</S>
    <S sid="8" ssid="2">Each approach and system has different advantages and disadvantages.</S>
    <S sid="9" ssid="3">While statistical systems provide broad coverage with little manpower, the quality of the corpus based systems rarely reaches the quality of knowledge based systems.</S>
    <S sid="10" ssid="4">With such a wide range of approaches to machine translation, it would be beneficial to have an effective framework for combining these systems into an MT system that carries many of the advantages of the individual systems and suffers from few of their disadvantages.</S>
    <S sid="11" ssid="5">Attempts at combining the output of different systems have proved useful in other areas of language technologies, such as the ROVER approach for speech recognition (Fiscus 1997).</S>
    <S sid="12" ssid="6">Several approaches to multi-engine machine translation systems have been proposed over the past decade.</S>
    <S sid="13" ssid="7">The Pangloss system and work by several other researchers attempted to combine lattices from many different MT systems (Frederking et Nirenburg 1994, Frederking et al 1997; Tidhar &amp; K&#252;ssner 2000; Lavie, Probst et al. 2004).</S>
    <S sid="14" ssid="8">These systems suffer from requiring cooperation from all the systems to produce compatible lattices as well as the hard research problem of standardizing confidence scores that come from the individual engines.</S>
    <S sid="15" ssid="9">In 2001, Bangalore et al used string alignments between the different translations to train a finite state machine to produce a consensus translation.</S>
    <S sid="16" ssid="10">The alignment algorithm described in that work, which only allows insertions, deletions and substitutions, does not accurately capture long range phrase movement.</S>
    <S sid="17" ssid="11">In this paper, we propose a new way of combining the translations of multiple MT systems based on a more versatile word alignment algorithm.</S>
    <S sid="18" ssid="12">A &#8220;decoding&#8221; algorithm then uses these alignments, in conjunction with confidence estimates for the various engines and a trigram language model, in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines.</S>
    <S sid="19" ssid="13">The highest scoring sentence hypothesis is selected as the final output of our system.</S>
    <S sid="20" ssid="14">We experimentally tested the new approach by combining translations obtained from combining three Arabic-to-English translation systems.</S>
    <S sid="21" ssid="15">Translation quality is scored using the METEOR MT evaluation metric (Lavie, Sagae et al 2004).</S>
    <S sid="22" ssid="16">Our experiments demonstrate that our new MEMT system achieves a substantial improvement over all of the original systems, and also outperforms an &#8220;oracle&#8221; capable of selecting the best of the original systems on a sentence-by-sentence basis.</S>
    <S sid="23" ssid="17">The remainder of this paper is organized as follows.</S>
    <S sid="24" ssid="18">In section 2 we describe the algorithm for generating multi-engine synthetic translations.</S>
    <S sid="25" ssid="19">Section 3 describes the experimental setup used to evaluate our approach, and section 4 presents the results of the evaluation.</S>
    <S sid="26" ssid="20">Our conclusions and directions for future work are presented in section 5.</S>
  </SECTION>
  <SECTION title="2 The MEMT Algorithm" number="2">
    <S sid="27" ssid="1">Our Multi-Engine Machine Translation (MEMT) system operates on the single &#8220;top-best&#8221; translation output produced by each of several MT systems operating on a common input sentence.</S>
    <S sid="28" ssid="2">MEMT first aligns the words of the different translation systems using a word alignment matcher.</S>
    <S sid="29" ssid="3">Then, using the alignments provided by the matcher, the system generates a set of synthetic sentence hypothesis translations.</S>
    <S sid="30" ssid="4">Each hypothesis translation is assigned a score based on the alignment information, the confidence of the individual systems, and a language model.</S>
    <S sid="31" ssid="5">The hypothesis translation with the best score is selected as the final output of the MEMT combination.</S>
    <S sid="32" ssid="6">The task of the matcher is to produce a wordto-word alignment between the words of two given input strings.</S>
    <S sid="33" ssid="7">Identical words that appear in both input sentences are potential matches.</S>
    <S sid="34" ssid="8">Since the same word may appear multiple times in the sentence, there are multiple ways to produce an alignment between the two input strings.</S>
    <S sid="35" ssid="9">The goal is to find the alignment that represents the best correspondence between the strings.</S>
    <S sid="36" ssid="10">This alignment is defined as the alignment that has the smallest number of &#8220;crossing edges.</S>
    <S sid="37" ssid="11">The matcher can also consider morphological variants of the same word as potential matches.</S>
    <S sid="38" ssid="12">To simultaneously align more than two sentences, the matcher simply produces alignments for all pair-wise combinations of the set of sentences.</S>
    <S sid="39" ssid="13">In the context of its use within our MEMT approach, the word-alignment matcher provides three main benefits.</S>
    <S sid="40" ssid="14">First, it explicitly identifies translated words that appear in multiple MT translations, allowing the MEMT algorithm to reinforce words that are common among the systems.</S>
    <S sid="41" ssid="15">Second, the alignment information allows the algorithm to ensure that aligned words are not included in a synthetic combination more than once.</S>
    <S sid="42" ssid="16">Third, by allowing long range matches, the synthetic combination generation algorithm can consider different plausible orderings of the matched words, based on their location in the original translations.</S>
    <S sid="43" ssid="17">After the matcher has word aligned the original system translations, the decoder goes to work.</S>
    <S sid="44" ssid="18">The hypothesis generator produces synthetic combinations of words and phrases from the original translations that satisfy a set of adequacy constraints.</S>
    <S sid="45" ssid="19">The generation algorithm is an iterative process and produces these translation hypotheses incrementally.</S>
    <S sid="46" ssid="20">In each iteration, the set of existing partial hypotheses is extended by incorporating an additional word from one of the original translations.</S>
    <S sid="47" ssid="21">For each partial hypothesis, a data-structure keeps track of the words from the original translations which are accounted for by this partial hypothesis.</S>
    <S sid="48" ssid="22">One underlying constraint observed by the generator is that the original translations are considered in principle to be word synchronous in the sense that selecting a word from one original translation normally implies &#8220;marking&#8221; a corresponding word in each of the other original translations as &#8220;used&#8221;.</S>
    <S sid="49" ssid="23">The way this is determined is explained below.</S>
    <S sid="50" ssid="24">Two partial hypotheses that have the same partial translation, but have a different set of words that have been accounted for are considered different.</S>
    <S sid="51" ssid="25">A hypothesis is considered &#8220;complete&#8221; if the next word chosen to extend the hypothesis is the explicit end-of-sentence marker from one of the original translation strings.</S>
    <S sid="52" ssid="26">At the start of hypothesis generation, there is a single hypothesis, which has the empty string as its partial translation and where none of the words in any of the original translations are marked as used.</S>
    <S sid="53" ssid="27">In each iteration, the decoder extends a hypothesis by choosing the next unused word from one of the original translations.</S>
    <S sid="54" ssid="28">When the decoder chooses to extend a hypothesis by selecting word w from original system A, the decoder marks w as used.</S>
    <S sid="55" ssid="29">The decoder then proceeds to identify and mark as used a word in each of the other original systems.</S>
    <S sid="56" ssid="30">If w is aligned to words in any of the other original translation systems, then the words that are aligned with w are also marked as used.</S>
    <S sid="57" ssid="31">For each system that does not have a word that aligns with w, the decoder establishes an artificial alignment between w and a word in this system.</S>
    <S sid="58" ssid="32">The intuition here is that this artificial alignment corresponds to a different translation of the same source-language word that corresponds to w. The choice of an artificial alignment cannot violate constraints that are imposed by alignments that were found by the matcher.</S>
    <S sid="59" ssid="33">If no artificial alignment can be established, then no word from this system will be marked as used.</S>
    <S sid="60" ssid="34">The decoder repeats this process for each of the original translations.</S>
    <S sid="61" ssid="35">Since the order in which the systems are processed matters, the decoder produces a separate hypothesis for each order.</S>
    <S sid="62" ssid="36">Each iteration expands the previous set of partial hypotheses, resulting in a large space of complete synthetic hypotheses.</S>
    <S sid="63" ssid="37">Since this space can grow exponentially, pruning based on scoring of the partial hypotheses is applied when necessary.</S>
    <S sid="64" ssid="38">A major component in the scoring of hypothesis translations is a confidence score that is assigned to each of the original translations, which reflects the translation adequacy of the system that produced it.</S>
    <S sid="65" ssid="39">We associate a confidence score with each word in a synthetic translation based on the confidence of the system from which it originated.</S>
    <S sid="66" ssid="40">If the word was contributed by several different original translations, we sum the confidences of the contributing systems.</S>
    <S sid="67" ssid="41">This word confidence score is combined multiplicatively with a score assigned to the word by a trigram language model.</S>
    <S sid="68" ssid="42">The score assigned to a complete hypothesis is its geometric average word score.</S>
    <S sid="69" ssid="43">This removes the inherent bias for shorter hypotheses that is present in multiplicative cumulative scores.</S>
    <S sid="70" ssid="44">The basic algorithm works well as long the original translations are reasonably word synchronous.</S>
    <S sid="71" ssid="45">This rarely occurs, so several additional constraints are applied during hypothesis generation.</S>
    <S sid="72" ssid="46">First, the decoder discards unused words in original systems that &#8220;linger&#8221; around too long.</S>
    <S sid="73" ssid="47">Second, the decoder limits how far ahead it looks for an artificial alignment, to prevent incorrect long-range artificial alignments.</S>
    <S sid="74" ssid="48">Finally, the decoder does not allow an artificial match between words that do not share the same part-of-speech.</S>
  </SECTION>
  <SECTION title="3 Experimental Setup" number="3">
    <S sid="75" ssid="1">We combined outputs of three Arabic-to-English machine translation systems on the 2003 TIDES Arabic test set.</S>
    <S sid="76" ssid="2">The systems were AppTek&#8217;s rule based system, CMU&#8217;s EBMT system, and Systran&#8217;s web-based translation system.</S>
    <S sid="77" ssid="3">We compare the results of MEMT to the individual online machine translation systems.</S>
    <S sid="78" ssid="4">We also compare the performance of MEMT to the score of an &#8220;oracle system&#8221; that chooses the best scoring of the individual systems for each sentence.</S>
    <S sid="79" ssid="5">Note that this oracle is not a realistic system, since a real system cannot determine at runtime which of the original systems is best on a sentence-by-sentence basis.</S>
    <S sid="80" ssid="6">One goal of the evaluation was to see how rich the space of synthetic translations produced by our hypothesis generator is.</S>
    <S sid="81" ssid="7">To this end, we also compare the output selected by our current MEMT system to an &#8220;oracle system&#8221; that chooses the best synthetic translation that was generated by the decoder for each sentence.</S>
    <S sid="82" ssid="8">This too is not a realistic system, but it allows us to see how well our hypothesis scoring currently performs.</S>
    <S sid="83" ssid="9">This also provides a way of estimating a performance ceiling of the MEMT approach, since our MEMT can only produce words that are provided by the original systems (Hogan and Frederking 1998).</S>
    <S sid="84" ssid="10">Due to the computational complexity of running the oracle system, several practical restrictions were imposed.</S>
    <S sid="85" ssid="11">First, the oracle system only had access to the top 1000 translation hypotheses produced by MEMT for each sentence.</S>
    <S sid="86" ssid="12">While this does not guarantee finding the best translation that the decoder can produce, this method provides a good approximation.</S>
    <S sid="87" ssid="13">We also ran the oracle experiment only on the first 140 sentences of the test sets due to time constraints.</S>
    <S sid="88" ssid="14">All the system performances are measured using the METEOR evaluation metric (Lavie, Sagae et al., 2004).</S>
    <S sid="89" ssid="15">METEOR was chosen since, unlike the more commonly used BLEU metric (Papineni et al., 2002), it provides reasonably reliable scores for individual sentences.</S>
    <S sid="90" ssid="16">This property is essential in order to run our oracle experiments.</S>
    <S sid="91" ssid="17">METEOR produces scores in the range of [0,1], based on a combination of unigram precision, unigram recall and an explicit penalty related to the average length of matched segments between the evaluated translation and its reference.</S>
  </SECTION>
  <SECTION title="4 Results" number="4">
    <S sid="92" ssid="1">On the 2003 TIDES data, the three original systems had similar METEOR scores.</S>
    <S sid="93" ssid="2">Table 1 shows the scores of the three systems, with their names obscured to protect their privacy.</S>
    <S sid="94" ssid="3">Also shown are the score of MEMT&#8217;s output and the score of the oracle system that chooses the best original translation on a sentence-by-sentence basis.</S>
    <S sid="95" ssid="4">The score of the MEMT system is significantly better than any of the original systems, and the sentence oracle.</S>
    <S sid="96" ssid="5">On the first 140 sentences, the oracle system that selects the best hypothesis translation generated by the MEMT generator has a METEOR score of 0.5883.</S>
    <S sid="97" ssid="6">This indicates that the scoring algorithm used to select the final MEMT output can be significantly further improved.</S>
  </SECTION>
  <SECTION title="5 Conclusions and Future Work" number="5">
    <S sid="98" ssid="1">Our MEMT algorithm shows consistent improvement in the quality of the translation compared any of the original systems.</S>
    <S sid="99" ssid="2">It scores better than an &#8220;oracle&#8221; that chooses the best original translation on a sentence-by-sentence basis.</S>
    <S sid="100" ssid="3">Furthermore, our MEMT algorithm produces hypotheses that are of yet even better quality, but our current scoring algorithm is not yet able to effectively select the best hypothesis.</S>
    <S sid="101" ssid="4">The focus of our future work will thus be on identifying features that support improved hypothesis scoring.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="6">
    <S sid="102" ssid="1">This research work was partly supported by a grant from the US Department of Defense.</S>
    <S sid="103" ssid="2">The word alignment matcher was developed by Satanjeev Banerjee.</S>
    <S sid="104" ssid="3">We wish to thank Robert Frederking, Ralf Brown and Jaime Carbonell for their valuable input and suggestions.</S>
  </SECTION>
</PAPER>
