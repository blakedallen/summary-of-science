[
  {
    "citance_No": 1, 
    "citing_paper_id": "E06-1031", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "Gregor, Leusch | Nicola, Ueffing | Hermann, Ney", 
    "raw_text": "Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR (Banerjee and Lavie, 2005)", 
    "clean_text": "Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR (Banerjee and Lavie, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-2124", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Chi-kiu, Lo | Meriem, Beloucif | Markus, Saers | Dekai, Wu", 
    "raw_text": "Surface-form oriented metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al, 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006) do not correctly reflect the meaning similarities of the input sentence", 
    "clean_text": "Surface-form oriented metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al, 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006) do not correctly reflect the meaning similarities of the input sentence.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W11-2602", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Wael, Salloum | Nizar, Habash", 
    "raw_text": "Results are presented in terms of BLEU (Papineniet al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005 )metrics.4 How ever, all optimizations were done against the BLEU metric", 
    "clean_text": "Results are presented in terms of BLEU (Papineniet al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005 ) metrics.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W12-3107", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Mengqiu, Wang | Christopher D., Manning", 
    "raw_text": "We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and Lavie, 2005) (v0.7), and TER) as our baselines", 
    "clean_text": "We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and Lavie, 2005) (v0.7), and TER) as our baselines.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "C10-2175", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Junguo, Zhu | Muyun, Yang | Bo, Wang | Sheng, Li | Tiejun, Zhao", 
    "raw_text": "In order to attack these problems, some metrics have been proposed to include more linguistic information into the process of matching ,e.g., Meteor (Banerjee and Lavie, 2005) metric and MaxSim (Channad Ng, 2008) metrics, which improve the lexical level by the synonym dictionary or stemming technique", 
    "clean_text": "In order to attack these problems, some metrics have been proposed to include more linguistic information into the process of matching ,e.g., Meteor (Banerjee and Lavie, 2005) metric and MaxSim (Channad Ng, 2008) metrics, which improve the lexical level by the synonym dictionary or stemming technique.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P11-1103", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Alexandra, Birch | Miles, Osborne", 
    "raw_text": "Another approach istaken by two other commonly used metrics, ME TEOR (Banerjee and Lavie, 2005) and TER (Snoveret al, 2006)", 
    "clean_text": "Another approach is taken by two other commonly used metrics, ME TEOR (Banerjee and Lavie, 2005) and TER (Snoveret al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W10-3707", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Santanu, Pal | Sudip Kumar, Naskar | Pavel, Pecina | Sivaji, Bandyopadhyay | Andy, Way", 
    "raw_text": "Instead we carry out extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002), WER, PER and TER (Snover et al, 2006)", 
    "clean_text": "Instead we carry out extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002), WER, PER and TER (Snover et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W07-0714", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Karolina, Owczarzak | Josef, van Genabith | Andy, Way", 
    "raw_text": "In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium? s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al, 2003), Translation Error Rate (TER) (Snover et al, 2006) 1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment", 
    "clean_text": "In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium? s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al, 2003), Translation Error Rate (TER) (Snover et al, 2006) 1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W07-0714", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Karolina, Owczarzak | Josef, van Genabith | Andy, Way", 
    "raw_text": "Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy", 
    "clean_text": "Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D10-1092", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Hideki, Isozaki | Tsutomu, Hirao | Kevin, Duh | Katsuhito, Sudoh | Hajime, Tsukada", 
    "raw_text": "edu/ ?snover/terp) for METEOR (Banerjee and Lavie, 2005)", 
    "clean_text": "We also used the paraphrase database TERp for METEOR (Banerjee and Lavie, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W06-3126", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jes&uacute;s, Gim&eacute;nez | Llu&iacute;s, M&agrave;rquez", 
    "raw_text": "For evaluation wehave selected a set of 8 metric variants corresponding to seven different families: BLEU (n= 4) (Papineni et al, 2001), NIST (n= 5) (Lin and Hovy, 2002), GTM F1-measure (e= 1, 2) (Melamed et al, 2003), 1-WER (Nie? en et al, 2000), 1-PER (Leusch et al, 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005)", 
    "clean_text": "For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n= 4) (Papineni et al, 2001), NIST (n= 5) (Lin and Hovy, 2002), GTM F1-measure (e= 1, 2) (Melamed et al, 2003), 1-WER (Nie? en et al, 2000), 1-PER (Leusch et al, 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W12-0111", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Andreas S&Atilde;&cedil;eborg, Kirkedal", 
    "raw_text": "BLEU (Papineni et al, 2002), TER (Snover et al, 2006) and METEOR (Banerjee and Lavie, 2005) scores will be reported", 
    "clean_text": "BLEU (Papineni et al, 2002), TER (Snover et al, 2006) and METEOR (Banerjee and Lavie, 2005) scores will be reported.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W09-0418", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Michael, Paul | Andrew, Finch | Eiichiro, Sumita", 
    "raw_text": "In this paper ,translation quality is evaluated according to (1) the BLEUmetrics which calculates the geometric mean of n gram precision by the system output with respect to reference translations (Papineni et al, 2002), and (2) the METEOR metrics that calculates uni gram overlaps between translations (BanerjeeandLavie, 2005)", 
    "clean_text": "In this paper ,translation quality is evaluated according to (1) the BLEU metrics which calculates the geometric mean of n gram precision by the system output with respect to reference translations (Papineni et al, 2002), and (2) the METEOR metrics that calculates uni gram overlaps between translations (Banerjee and Lavie, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D07-1055", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Richard, Zens | Sa&scaron;a, Hasan | Hermann, Ney", 
    "raw_text": "There exists a variety of different metrics ,e.g., word error rate, position-independent word error rate, BLEU score (Papineni et al, 2002), NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al, 2003)", 
    "clean_text": "There exists a variety of different metrics ,e.g., word error rate, position-independent word error rate, BLEU score (Papineni et al, 2002), NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N09-1058", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Amit, Goyal | Hal, Daum&eacute; III | Suresh, Venkatasubramanian", 
    "raw_text": "The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores", 
    "clean_text": "The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W09-0403", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Petr, Homola | Vladislav, Kubo&#x148; | Pavel, Pecina", 
    "raw_text": "(Banerjee and Lavie, 2005))?", 
    "clean_text": "e.g. Meteor (Banerjee and Lavie, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D11-1138", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Keith, Hall | Ryan, McDonald | Jason, Katz-Brown | Michael, Ringgaard", 
    "raw_text": "Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure", 
    "clean_text": "Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W10-3806", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Ventsislav, Zhechev | Josef, van Genabith", 
    "raw_text": "For the evaluation of our system, we used a number of widely accepted automatic metrics, namely BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2006) and inverse F-Score based on token-level precision and recall. We setup our system to only fully process in put sentences for which a TM match with an FMS over 50% was found, although all sentences were translated directly using the SMTbackend to check the overall pure SMT performance", 
    "clean_text": "For the evaluation of our system, we used a number of widely accepted automatic metrics, namely BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2006) and inverse F-Score based on token-level precision and recall.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P11-2071", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Hal, Daum&eacute; III | Jagadeesh, Jagarlamudi", 
    "raw_text": "In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al, 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005)", 
    "clean_text": "In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al, 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "N07-1006", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie,2005)", 
    "clean_text": "Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie,2005).", 
    "keep_for_gold": 0
  }
]
