<PAPER>
  <S sid="0">Forest-based Translation Rule Extraction</S>
  <ABSTRACT>
    <S sid="1" ssid="1">examples (partial) target tree-to-tree Ding and Palmer (2005) Translation rule extraction is a fundamental problem in machine translation, especially for syntax-based that need parse trees from either or both sides of the bitext.</S>
    <S sid="2" ssid="2">The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors.</S>
    <S sid="3" ssid="3">So we propose a novel approach which extracts rules a forest compactly encodes exponentially many parses.</S>
    <S sid="4" ssid="4">Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses.</S>
    <S sid="5" ssid="5">When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="6" ssid="1">examples (partial) Abstract source target tree-to-tree Ding and Palmer (2005) Translation rule extraction is a fundamental problem in machine translation, especially for linguistically syntax-based systems that need parse trees from either or both sides of the bitext.</S>
    <S sid="7" ssid="2">The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors.</S>
    <S sid="8" ssid="3">So we propose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses.</S>
    <S sid="9" ssid="4">Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses.</S>
    <S sid="10" ssid="5">When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.</S>
  </SECTION>
  <SECTION title="1 Introduction" number="2">
    <S sid="11" ssid="1">Automatic extraction of translation rules is a fundamental problem in statistical machine translation, especially for many syntax-based models where translation rules directly encode linguistic knowledge.</S>
    <S sid="12" ssid="2">Typically, these models extract rules using parse trees from both or either side(s) of the bitext.</S>
    <S sid="13" ssid="3">The former case, with trees on both sides, is often called tree-to-tree models; while the latter case, with trees on either source or target side, include both treeto-string and string-to-tree models (see Table 1).</S>
    <S sid="14" ssid="4">Leveraging from structural and linguistic information from parse trees, these models are believed to be better than their phrase-based counterparts in tree-to-string string-to-tree handling non-local reorderings, and have achieved promising translation results.1 However, these systems suffer from a major limitation, that the rule extractor only uses 1-best parse tree(s), which adversely affects the rule set quality due to parsing errors.</S>
    <S sid="15" ssid="5">To make things worse, modern statistical parsers are often trained on domains quite different from those used in MT.</S>
    <S sid="16" ssid="6">By contrast, formally syntax-based models (Chiang, 2005) do not rely on parse trees, yet usually perform better than these linguistically sophisticated counterparts.</S>
    <S sid="17" ssid="7">To alleviate this problem, an obvious idea is to extract rules from k-best parses instead.</S>
    <S sid="18" ssid="8">However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008).</S>
    <S sid="19" ssid="9">This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space.</S>
    <S sid="20" ssid="10">In addition, many subtrees are repeated across different parses, so it is also inefficient to extract rules separately from each of these very similar trees (or from the cross-product of k2 similar tree-pairs in tree-to-tree models).</S>
    <S sid="21" ssid="11">We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists.</S>
    <S sid="22" ssid="12">Experiments (Section 5) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system (Liu et al., 2006; Mi et al., 2008), which is also 0.5 points better than (and twice as fast as) extracting on 30-best parses.</S>
    <S sid="23" ssid="13">When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date.</S>
    <S sid="24" ssid="14">Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models (Galley et al., 2006) where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-to-tree models.</S>
  </SECTION>
  <SECTION title="2 Tree-based Translation" number="3">
    <S sid="25" ssid="1">We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006).</S>
    <S sid="26" ssid="2">Current tree-based systems perform translation in two separate steps: parsing and decoding.</S>
    <S sid="27" ssid="3">The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transformation rules.</S>
    <S sid="28" ssid="4">For example, consider the following example translating from Chinese to English: &#8220;Bush held a meeting2 with Sharon1&#8221; Figure 2 shows how this process works.</S>
    <S sid="29" ssid="5">The Chinese sentence (a) is first parsed into a parse tree (b), which will be converted into an English string in 5 steps.</S>
    <S sid="30" ssid="6">First, at the root node, we apply rule r1 shown in Figure 1, which translates the Chinese coordination construction (&#8220;... and ...&#8221;) into an English prepositional phrase.</S>
    <S sid="31" ssid="7">Then, from step (c) we continue applying rules to untranslated Chinese subtrees, until we get the complete English translation in (e).2 Sh&#175;al&#180;ong Bush held a meeting with Sharon More formally, a (tree-to-string) translation rule (Galley et al., 2004; Huang et al., 2006) is a tuple (lhs(r), rhs(r), 0(r)), where lhs(r) is the sourceside tree fragment, whose internal nodes are labeled by nonterminal symbols (like NP and VP), and whose frontier nodes are labeled by sourcelanguage words (like &#8220;y&#711;u&#8221;) or variables from a set X = {x1, x2,...}; rhs(r) is the target-side string expressed in target-language words (like &#8220;with&#8221;) and variables; and 0(r) is a mapping from X to nonterminals.</S>
    <S sid="32" ssid="8">Each variable xi E X occurs exactly once in lhs(r) and exactly once in rhs(r).</S>
    <S sid="33" ssid="9">For example, for rule r1 in Figure 1, These rules are being used in the reverse direction of the string-to-tree transducers in Galley et al. (2004).</S>
    <S sid="34" ssid="10">We now briefly explain the algorithm of Galley et al. (2004) that can extract these translation rules from a word-aligned bitext with source-side parses.</S>
    <S sid="35" ssid="11">Consider the example in Figure 3.</S>
    <S sid="36" ssid="12">The basic idea is to decompose the source (Chinese) parse into a series of tree fragments, each of which will form a rule with its corresponding English translation.</S>
    <S sid="37" ssid="13">However, not every fragmentation can be used for rule extraction, since it may or may not respect the alignment and reordering between the two languages.</S>
    <S sid="38" ssid="14">So we say a fragmentation is well-formed with respect to an alignment if the root node of every tree fragment corresponds to a contiguous span on the target side; the intuition is that there is a &#8220;translational equivalence&#8221; between the subtree rooted at the node and the corresponding target span.</S>
    <S sid="39" ssid="15">For example, in Figure 3, each node is annotated with its corresponding English span, where the NP node maps to a noncontiguous one &#8220;Bush U with Sharon&#8221;.</S>
    <S sid="40" ssid="16">More formally, we need a precise formulation to handle the cases of one-to-many, many-to-one, and many-to-many alignment links.</S>
    <S sid="41" ssid="17">Given a sourcetarget sentence pair (u, T) with alignment a, the (target) span of node v is the set of target words aligned to leaf nodes yield(v) under node v: For example, in Figure 3, every node in the parse tree is annotated with its corresponding span below the node, where most nodes have contiguous spans except for the NP node which maps to a gapped phrase &#8220;Bush U with Sharon&#8221;.</S>
    <S sid="42" ssid="18">But contiguity alone is not enough to ensure well-formedness, since there might be words within the span aligned to source words uncovered by the node.</S>
    <S sid="43" ssid="19">So we also define a span s to be faithful to node v if every word in it is only aligned to nodes dominated by v, i.e.</S>
    <S sid="44" ssid="20">: For example, sibling nodes VV and AS in the tree have non-faithful spans (crossed out in the Figure), because they both map to &#8220;held&#8221;, thus neither of them can be translated to &#8220;held&#8221; alone.</S>
    <S sid="45" ssid="21">In this case, a larger tree fragment rooted at VPB has to be extracted.</S>
    <S sid="46" ssid="22">Nodes with non-empty, contiguous, and faithful spans form the admissible set (shaded nodes in the figure), which serve as potential cut-points for rule extraction.3 With the admissible set computed, rule extraction is as simple as a depth-first traversal from the root: we &#8220;cut&#8221; the tree at all admissible nodes to form tree fragments and extract a rule for each fragment, with variables matching the admissible descendant nodes.</S>
    <S sid="47" ssid="23">For example, the tree in Figure 3 is cut into 6 pieces, each of which corresponds to a rule on the right.</S>
    <S sid="48" ssid="24">These extracted rules are called minimal rules, which can be glued together to form composed rules with larger tree fragments (e.g. r1 in Fig.</S>
    <S sid="49" ssid="25">1) (Galley et al., 2006).</S>
    <S sid="50" ssid="26">Our experiments use composed rules.</S>
  </SECTION>
  <SECTION title="3 Forest-based Rule Extraction" number="4">
    <S sid="51" ssid="1">We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees.</S>
    <S sid="52" ssid="2">Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989).</S>
    <S sid="53" ssid="3">For example, consider again the Chinese sentence in Example (1) above, which has (at least) two readings depending on the part-ofspeech of the word y&#711;u: it can be either a conjunction (CC &#8220;and&#8221;) as shown in Figure 3, or a preposition (P &#8220;with&#8221;) as shown in Figure 5, with only PP and VPB swapped from the English word order.</S>
    <S sid="54" ssid="4">These two parse trees can be represented as a single forest by sharing common subtrees such as NPB0, 1 and VPB3, 6, as shown in Figure 4.</S>
    <S sid="55" ssid="5">Such a forest has a structure of a hypergraph (Huang and Chiang, 2005), where items like NP0, 3 are called nodes, whose indices denote the source span, and combinations like we call hyperedges.</S>
    <S sid="56" ssid="6">We denote head(e) and tails(e) to be the consequent and antecedant items of hyperedge e, respectively.</S>
    <S sid="57" ssid="7">For example, We also denote BS(v) to be the set of incoming hyperedges of node v, being different ways of deriving it.</S>
    <S sid="58" ssid="8">For example, in Figure 4, BS(IP0, 6) = {e1, e2}.</S>
    <S sid="59" ssid="9">Like in tree-based extraction, we extract rules from a packed forest F in two steps: It turns out that the exact formulation developed for admissible set in the tree-based case can be applied to a forest without any change.</S>
    <S sid="60" ssid="10">The fragmentation step, however, becomes much more involved since we now face a choice of multiple parse hyperedges at each node.</S>
    <S sid="61" ssid="11">In other words, it becomes nondeterministic how to &#8220;cut&#8221; a forest into tree fragments, which is analogous to the non-deterministic pattern-match in forest-based decoding (Mi et al., 2008).</S>
    <S sid="62" ssid="12">For example there are two parse hyperedges e1 and e2 at the root node in Figure 4.</S>
    <S sid="63" ssid="13">When we follow one of them to grow a fragment, there again will be multiple choices at each of its tail nodes.</S>
    <S sid="64" ssid="14">Like in tree-based case, a fragment is said to be complete if all its leaf nodes are admissible.</S>
    <S sid="65" ssid="15">Otherwise, an incomplete fragment can grow at any non-admissible frontier node v, where following each parse hyperedge at v will split off a new fragment.</S>
    <S sid="66" ssid="16">For example, following e2 at the root node will immediately lead us to two admissible nodes, NPB0, 1 and VP1, 6 (we will highlight admissible nodes by gray shades Algorithm 1 Forest-based Rule Extraction. in this section like in Figures 3 and 4).</S>
    <S sid="67" ssid="17">So this fragment, frag1 = {e2}, is now complete and we can extract a rule, IP (x1:NPB x2:VP) &#8594; x1 x2.</S>
    <S sid="68" ssid="18">However, following the other hyperedge e1 IP0, 6 &#8594; NP0, 3 VPB3, 6 will leave the new fragment frag2 = {e1} incomplete with one non-admissible node NP0, 3.</S>
    <S sid="69" ssid="19">We then grow frag2 at this node by choosing hyperedge e3 NP0, 3 &#8594; NPB0, 1 CC1, 2 NPB2, 3 , and spin off anew fragment frag3 = {e1, e3}, which is now complete since all its four leaf nodes are admissible.</S>
    <S sid="70" ssid="20">We then extract a rule with four variables: This procedure is formalized by a breadth-first search (BFS) in Pseudocode 1.</S>
    <S sid="71" ssid="21">The basic idea is to visit each frontier node v, and keep a queue open of actively growing fragments rooted at v. We keep expanding incomplete fragments from open, and extract a rule if a complete fragment is found (line 10).</S>
    <S sid="72" ssid="22">Each fragment is associated with a frontier (variable front in the Pseudocode), being the subset of nonadmissible leaf nodes (recall that expansion stops at admissible nodes).</S>
    <S sid="73" ssid="23">So each initial fragment along hyperedge e is associated with an initial frontier (line 5), front = tails(e) \ admset.</S>
    <S sid="74" ssid="24">A fragment is complete if its frontier is empty (line 9), otherwise we pop one frontier node u to expand, spin off new fragments by following hyperedges of u, and update the frontier (lines 14-16), until all active fragments are complete and open queue is empty (line 7).</S>
    <S sid="75" ssid="25">A single parse tree can also be viewed as a trivial forest, where each node has only one incoming hyperedge.</S>
    <S sid="76" ssid="26">So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.</S>
    <S sid="77" ssid="27">2.2) can be considered a special case of our algorithm, where the queue open always contains one single active fragment.</S>
    <S sid="78" ssid="28">In tree-based extraction, for each sentence pair, each rule extracted naturally has a count of one, which will be used in maximum-likelihood estimation of rule probabilities.</S>
    <S sid="79" ssid="29">However, a forest is an implicit collection of many more trees, each of which, when enumerated, has its own probability accumulated from of the parse hyperedges involved.</S>
    <S sid="80" ssid="30">In other words, a forest can be viewed as a virtual weighted k-best list with a huge k. So a rule extracted from a non 1-best parse, i.e., using non 1-best hyperedges, should be penalized accordingly and should have a fractional count instead of a unit one, similar to the E-step in EM algorithms.</S>
    <S sid="81" ssid="31">Inspired by the parsing literature on pruning (Charniak and Johnson, 2005; Huang, 2008) we penalize a rule r by the posterior probability of its tree fragment frag = lhs(r).</S>
    <S sid="82" ssid="32">This posterior probability, notated &#945;&#946;(frag), can be computed in an InsideOutside fashion as the product of three parts: the outside probability of its root node, the probabilities of parse hyperedges involved in the fragment, and the inside probabilities of its leaf nodes, where &#945;(&#183;) and &#946;(&#183;) denote the outside and inside probabilities of tree nodes, respectively.</S>
    <S sid="83" ssid="33">For example in Figure 4, where TOP denotes the root node of the forest.</S>
    <S sid="84" ssid="34">Like in the M-step in EM algorithm, we now extend the maximum likelihood estimation to fractional counts for three conditional probabilities regarding a rule, which will be used in the experiments:</S>
  </SECTION>
  <SECTION title="4 Related Work" number="5">
    <S sid="85" ssid="1">The concept of packed forest has been previously used in translation rule extraction, for example in rule composition (Galley et al., 2006) and tree binarization (Wang et al., 2007).</S>
    <S sid="86" ssid="2">However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest.</S>
    <S sid="87" ssid="3">Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forest-based fragmentation (Algorithm 1) which we think is non-trivial.</S>
    <S sid="88" ssid="4">The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).</S>
    <S sid="89" ssid="5">The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below).</S>
    <S sid="90" ssid="6">This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding.</S>
    <S sid="91" ssid="7">BLEU score Our experiments will use both default 1-best decoding and forest-based decoding.</S>
    <S sid="92" ssid="8">As we will see in the next section, the best result comes when we combine the merits of both, i.e., using forests in both rule extraction and decoding.</S>
    <S sid="93" ssid="9">There is also a parallel work on extracting rules from k-best parses and k-best alignments (Venugopal et al., 2008), but both their experiments and our own below confirm that extraction on k-best parses is neither efficient nor effective.</S>
  </SECTION>
  <SECTION title="5 Experiments" number="6">
    <S sid="94" ssid="1">Our experiments are on Chinese-to-English translation based on a tree-to-string system similar to (Huang et al., 2006; Liu et al., 2006).</S>
    <S sid="95" ssid="2">Given a 1best tree T, the decoder searches for the best derivation d&#8727; among the set of all possible derivations D: where the first two terms are translation and language model probabilities, &#964;(d) is the target string (English sentence) for derivation d, and the last two terms are derivation and translation length penalties, respectively.</S>
    <S sid="96" ssid="3">The conditional probability P(d  |T) decomposes into the product of rule probabilities: where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.3, and the last two are lexical probabilities.</S>
    <S sid="97" ssid="4">These parameters Al ... As are tuned by minimum error rate training (Och, 2003) on the dev sets.</S>
    <S sid="98" ssid="5">We refer readers to Mi et al. (2008) for details of the decoding algorithm.</S>
    <S sid="99" ssid="6">We use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext.</S>
    <S sid="100" ssid="7">Following Huang (2008), we also modify this parser to output a packed forest for each sentence, which can be pruned by the marginal probability-based insideoutside algorithm (Charniak and Johnson, 2005; Huang, 2008).</S>
    <S sid="101" ssid="8">We will first report results trained on a small-scaled dataset with detailed analysis, and then scale to a larger one, where we also combine the technique of forest-based decoding (Mi et al., 2008).</S>
    <S sid="102" ssid="9">To test the effect of forest-based rule extraction, we parse the training set into parse forests and use three levels of pruning thresholds: pe = 2, 5, 8.</S>
    <S sid="103" ssid="10">Figure 6 plots the extraction speed and translation quality of forest-based extraction with various pruning thresholds, compared to 1-best and 30-best baselines.</S>
    <S sid="104" ssid="11">Using more than one parse tree apparently improves the BLEU score, but at the cost of much slower extraction, since each of the top-k trees has to be processed individually although they share many common subtrees.</S>
    <S sid="105" ssid="12">Forest extraction, by contrast, is much faster thanks to packing and produces consistently better BLEU scores.</S>
    <S sid="106" ssid="13">With pruning threshold pe = 8, forest-based extraction achieves a (case insensitive) BLEU score of 0.2533, which is an absolute improvement of 1.0% points over the 1-best baseline, and is statistically significant using the sign-test of Collins et al. (2005) (p &lt; 0.01).</S>
    <S sid="107" ssid="14">This is also 0.5 points better than (and twice as fast as) extracting on 30-best parses.</S>
    <S sid="108" ssid="15">These BLEU score results are summarized in Table 2, which also shows that decoding with forest-extracted rules is less than twice as slow as with 1-best rules, and only fractionally slower than with 30-best rules.</S>
    <S sid="109" ssid="16">We also investigate the question of how often rules extracted from non 1-best parses are used by the decoder.</S>
    <S sid="110" ssid="17">Table 3 shows the numbers of rules extracted from 1-best, 30-best and forest-based extractions, and the numbers that survive after filtering on the dev set.</S>
    <S sid="111" ssid="18">Basically in the forest-based case we can use about twice as many rules as in the 1best case, or about 1.5 times of 30-best extraction.</S>
    <S sid="112" ssid="19">But the real question is, are these extra rules really useful in generating the final (1-best) translation?</S>
    <S sid="113" ssid="20">The last row shows that 16.3% of the rules used in 1-best derivations are indeed only extracted from non 1-best parses in the forests.</S>
    <S sid="114" ssid="21">Note that this is a stronger condition than changing the distribution of rules by considering more parses; here we introduce new rules never seen on any 1-best parses.</S>
    <S sid="115" ssid="22">We also conduct experiments on a larger training dataset, FBIS, which contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively.</S>
    <S sid="116" ssid="23">We also use a bigger trigram model trained on the first 1/3 of the Xinhua portion of Gigaword corpus.</S>
    <S sid="117" ssid="24">To integrate with forest-based decoding, we use both 1-best trees and packed forests during both rule extraction and decoding phases.</S>
    <S sid="118" ssid="25">Since the data scale is larger than the small data, we are forced to use harsher pruning thresholds, with pe = 5 for extraction and pd = 10 for decoding.</S>
    <S sid="119" ssid="26">The final BLEU score results are shown in Table 4.</S>
    <S sid="120" ssid="27">With both tree-based and forest-based decoding, rules extracted from forests significantly outperform those extracted from 1-best trees (p &lt; 0.01).</S>
    <S sid="121" ssid="28">The final result with both forest-based extraction and forest-based decoding reaches a BLEU score of 0.2816, outperforming that of Hiero (Chiang, 2005), one of the best performing systems to date.</S>
    <S sid="122" ssid="29">These results confirm that our novel forest-based rule extraction approach is a promising direction for syntaxbased machine translation.</S>
  </SECTION>
  <SECTION title="6 Conclusion and Future Work" number="7">
    <S sid="123" ssid="1">In this paper, we have presented a novel approach that extracts translation rules from a packed forest encoding exponentially many trees, rather than from 1-best or k-best parses.</S>
    <S sid="124" ssid="2">Experiments on a state-ofthe-art tree-to-string system show that this method improves BLEU score significantly, with reasonable extraction speed.</S>
    <S sid="125" ssid="3">When combined with our previous work on forest-based decoding, the final result is even better than the hierarchical system Hiero.</S>
    <S sid="126" ssid="4">For future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (Galley et al., 2006) and tree-to-tree systems.</S>
  </SECTION>
  <SECTION title="Acknowledgement" number="8">
    <S sid="127" ssid="1">This work was funded by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.</S>
    <S sid="128" ssid="2">2006AA010108 (H. M.), and by NSF ITR EIA0205456 (L. H.).</S>
    <S sid="129" ssid="3">We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.</S>
  </SECTION>
</PAPER>
