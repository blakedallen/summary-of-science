Contrastive Estimation: Training Log-Linear Models On Unlabeled Data
Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and named-entity extraction (McCallum and Li, 2003).
CRFs are log-linear, allowing the incorporation of arbitrary features into the model.
To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist.
We describe a novel approach, contrastive estimation.
We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient.
Applied to a sequence labeling problem - POS tagging given a tagging dictionary and unlabeled text - contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
To define an approximation to the partition function for log-linear estimation, our contrastive estimation uses a technique to generate local neighborhoods of a parses.
We generate negative evidence for the contrastive estimation method by moving or removing a word in a sentence.
We define coarse grained POS tags on PTB.
We initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data.
We attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier.
We show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods.
The contrastive estimation technique we propose is globally normalized and thus capable of dealing with arbitrary features.
