[
  {
    "citance_No": 1, 
    "citing_paper_id": "N09-1025", 
    "citing_paper_authority": 56, 
    "citing_paper_authors": "David, Chiang | Kevin, Knight | Wei, Wang", 
    "raw_text": "To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment", 
    "clean_text": "To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "N12-1026", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Taro, Watanabe", 
    "raw_text": "As proposed by Haddow et al (2011), BLEU is approximately computed in the local batch ,sinceBLEU is not linearly decomposed into a sentence wise score (Chiang et al, 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU", 
    "clean_text": "As proposed by Haddow et al (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentence wise score (Chiang et al, 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P11-1001", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Andreas, Zollmann | Stephan, Vogel", 
    "raw_text": "The resulting number of grammar nonterminals based on a tag vocabulary of size t is thus given by 2t2+ t. An alternative way of accounting for phrase sizeis presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length", 
    "clean_text": "An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D12-1037", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Lemao, Liu | Hailong, Cao | Taro, Watanabe | Tiejun, Zhao | Mo, Yu | Conghui, Zhu", 
    "raw_text": "MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an on line one which updates with each example (Watanabe et al2007) or part of examples (Chiang et al2008)", 
    "clean_text": "MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P10-1017", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Jason, Riesa | Daniel, Marcu", 
    "raw_text": "We incorporate all our new features into a linear model and learn weights for each using the on line averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs in spired by Chiang et al (2008)", 
    "clean_text": "We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P13-1110", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Vladimir, Eidelman | Yuval, Marton | Philip, Resnik", 
    "raw_text": "has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008)", 
    "clean_text": "The results are especially notable for the basic feature setting - up to 1.2 BLEU and 4.6 TER improvement over MERT - since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P13-1110", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Vladimir, Eidelman | Yuval, Marton | Philip, Resnik", 
    "raw_text": "We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008)", 
    "clean_text": "We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P13-1110", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Vladimir, Eidelman | Yuval, Marton | Philip, Resnik", 
    "raw_text": "In future work we also intend to explore using additional sparse features that are known to be useful in translation ,e.g. syntactic features explored by Chiang et al (2008) .Finally, although motivated by statistical ma chine translation, RM is a gradient-based method that can easily be applied to other problems", 
    "clean_text": "In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W11-2113", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Xin, Song | Trevor, Cohn", 
    "raw_text": "It is formulated as S=?? w? f (?? c,?? r) (1) where?? w is the feature weights vector, f (?? c,?? r) is the feature function which takes candidate transla Chiang et al, 2008b) 124tion (?? c) and reference (?? c), and returns the feature vector", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W11-2130", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Barry, Haddow | Abhishek, Arun | Philipp, Koehn", 
    "raw_text": "Thesample hypothesis set (y?) is then the current hypothesis set (ys? 1) with ej replaced by e? j. The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score", 
    "clean_text": "The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P12-1002", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Patrick, Simianer | Stefan, Riezler | Chris, Dyer", 
    "raw_text": "For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals", 
    "clean_text": "For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P12-1002", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Patrick, Simianer | Stefan, Riezler | Chris, Dyer", 
    "raw_text": "Theperceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al (2008)", 
    "clean_text": "The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "E12-1013", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Artem, Sokolov | Fran&ccedil;ois, Yvon | Guillaume, Wisniewski", 
    "raw_text": "BLEU is defined for a pair of corpora, but, as an oracle decoder is working at the sentence-level, it should rely on an approximation of BLEU that can linear chain of arcs.4The algorithms described below can be straight for wardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe", 
    "clean_text": "The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D10-1059", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Samidh, Chatterjee | Nicola, Cancedda", 
    "raw_text": "Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008)", 
    "clean_text": "Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N12-1061", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jason, Riesa | Daniel, Marcu", 
    "raw_text": "We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al, 2006), and tune parameters of them T systemwithMIRA (Chiang et al, 2008)", 
    "clean_text": "We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al, 2006), and tune parameters of them T system with MIRA (Chiang et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D09-1147", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Adam, Pauls | John, DeNero | Dan, Klein", 
    "raw_text": "Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008) .We developed CoBLEU primarily to complement consensus decoding, which it does; it produces higher BLEU scores than coupling MERT with consensus decoding", 
    "clean_text": "Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P13-1156", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "ThuyLinh, Nguyen | Stephan, Vogel", 
    "raw_text": "Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their ChineseEnglish experiment", 
    "clean_text": "Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their Chinese-English experiment.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P10-1076", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Tong, Xiao | Jingbo, Zhu | Muhua, Zhu | Huizhen, Wang", 
    "raw_text": "The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account", 
    "clean_text": "The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P12-1099", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Majid, Razmara | George, Foster | Baskaran, Sankaran | Anoop, Sarkar", 
    "raw_text": "Inaddition, MERT would not be an appropriate optimizer when the number of features increases acer tain amount (Chiang et al, 2008) .Our approach differs from the model combination approach of DeNero et al (2010), a generalization of consensus or minimum Bayes risk decoding where the search space consists of those of multiple systems, in that model combination uses forest of derivations of all component models to do the combination", 
    "clean_text": "In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D09-1008", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Libin, Shen | Jinxi, Xu | Bing, Zhang | Spyros, Matsoukas | Ralph M., Weischedel", 
    "raw_text": "This problem was fixed by 78 Model MT06 MT08 BLEU TER BLEU TER lower mixed lower mixed lower mixed lower mixed Decoding (3-gram LM) BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69 SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46 CLM 37.66 35.81 53.45 55.19 32.97 31.01 55.99 57.77 LEN 38.09 36.26 53.98 55.81 33.23 31.34 56.51 58.41 LBL 38.37 36.53 54.14 55.99 33.25 31.34 56.60 58.49 LBL+LEN 38.36 36.59 53.95 55.60 33.72 31.83 56.79 58.65 LBL+LEN+CLM 38.41 36.57 53.83 55.70 33.83 31.79 56.55 58.51 Rescoring (5-gram LM) BASE 38.91 37.04 53.65 55.45 34.34 32.32 55.60 57.60 SLM 38.27 36.38 53.64 55.29 34.25 32.28 55.35 57.21 CLM 38.79 36.88 53.09 54.80 35.01 32.98 55.39 57.28 LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51 LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48 LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46 LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98 Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets. Chiang et al (2008), which used an on line learning method (Crammer and Singer, 2003) to handle a large set of features", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]
