<PAPER>
	<S sid="0">Learning Entailment Rules for Unary Templates</S><ABSTRACT>
		<S sid="1" ssid="1">Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.</S>
		<S sid="2" ssid="2">In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method.</S>
		<S sid="3" ssid="3">The results show that the learned unary rule-sets outperform the binary rule-set.</S>
		<S sid="4" ssid="4">In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">In many NLP applications, such as Question An swering (QA) and Information Extraction (IE), it is crucial to recognize whether a specific target meaning is inferred from a text.</S>
			<S sid="6" ssid="6">For example, a QA system has to deduce that ?SCO sued IBM?</S>
			<S sid="7" ssid="7">is inferred from ?SCO won a lawsuit against IBM?</S>
			<S sid="8" ssid="8">to answer ?Whom did SCO sue??.</S>
			<S sid="9" ssid="9">This type of reasoning has been identified as a core semanticinference paradigm by the generic Textual Entail ment framework (Giampiccolo et al, 2007).</S>
			<S sid="10" ssid="10">An important type of knowledge needed for such inference is entailment rules.</S>
			<S sid="11" ssid="11">An entailmentrule specifies a directional inference relation be tween two templates, text patterns with variables, such as ?X win lawsuit against Y ? X sue Y ?.</S>
			<S sid="12" ssid="12">Applying this rule by matching ?X win lawsuit against Y ? in the above text allows a QA system to c ? 2008.</S>
			<S sid="13" ssid="13">Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).</S>
			<S sid="14" ssid="14">Some rights reserved.infer ?X sue Y ? and identify ?IBM?, Y ?s instantiation, as the answer for the above question.</S>
			<S sid="15" ssid="15">Entail ment rules capture linguistic and world-knowledge inferences and are used as an important building block within different applications, e.g.</S>
			<S sid="16" ssid="16">(Romano et al, 2006).</S>
			<S sid="17" ssid="17">One reason for the limited performance of generic semantic inference systems is the lack of broad-scale knowledge-bases of entailment rules (in analog to lexical resources such as WordNet).</S>
			<S sid="18" ssid="18">Supervised learning of broad coverage rule-sets is an arduous task.</S>
			<S sid="19" ssid="19">This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.</S>
			<S sid="20" ssid="20">(Lin and Pantel, 2001; Szpektor et al, 2004; Sekine, 2005).</S>
			<S sid="21" ssid="21">Most unsupervised entailment rule acquisitionmethods learn binary rules, rules between tem plates with two variables, ignoring unary rules, rules between unary templates (templates withonly one variable).</S>
			<S sid="22" ssid="22">However, a predicate quite of ten appears in the text with just a single variable(e.g. intransitive verbs or passives), where infer ence requires unary rules, e.g. ?X take a nap?X sleep?</S>
			<S sid="23" ssid="23">(further motivations in Section 3.1).In this paper we focus on unsupervised learning of unary entailment rules.</S>
			<S sid="24" ssid="24">Two learning ap proaches are proposed.</S>
			<S sid="25" ssid="25">In our main approach, rules are learned by measuring how similar the variable instantiations of two templates in a corpusare.</S>
			<S sid="26" ssid="26">In addition to adapting state-of-the-art similar ity measures for unary rule learning, we propose a new measure, termed Balanced-Inclusion, which balances the notion of directionality in entailment with the common notion of symmetric semantic similarity.</S>
			<S sid="27" ssid="27">In a second approach, unary rules arederived from binary rules learned by state-of-the art binary rule learning methods.</S>
			<S sid="28" ssid="28">We tested the various unsupervised unary rule 849learning methods, as well as a binary rule learn ing method, on a test set derived from a standard IE benchmark.</S>
			<S sid="29" ssid="29">This provides the first comparisonbetween the performance of unary and binary rule sets.</S>
			<S sid="30" ssid="30">Several results rise from our evaluation: (a) while most work on unsupervised learning ignored unary rules, all tested unary methods outperformed the binary method; (b) it is better to learn unary rules directly than to derive them from a binary rule-base; (c) our proposed Balanced-Inclusion measure outperformed all other tested methods interms of F1 measure.</S>
			<S sid="31" ssid="31">Moreover, only BalancedInclusion improved F1 score over a baseline infer ence that does not use entailment rules at all .</S>
	</SECTION>
	<SECTION title="Background. " number="2">
			<S sid="32" ssid="1">This section reviews relevant distributional simi larity measures, both symmetric and directional, which were applied for either lexical similarity or unsupervised entailment rule learning.</S>
			<S sid="33" ssid="2">Distributional similarity measures follow the Distributional Hypothesis, which states that words that occur in the same contexts tend to have similar meanings (Harris, 1954).</S>
			<S sid="34" ssid="3">Various measures wereproposed in the literature for assessing such simi larity between two words, u and v. Given a word q, its set of features F q and feature weights w q (f) for f ? F q , a common symmetric similarity measure is Lin similarity (Lin, 1998a): Lin(u, v) = ? f?F u ?F v [w u (f) + w v (f)] ? f?F u w u (f) + ? f?F v w v (f) where the weight of each feature is the pointwise mutual information (pmi) between the word and the feature: w q (f) = log[ Pr(f |q) Pr(f) ].</S>
			<S sid="35" ssid="4">Weeds and Weir (2003) proposed to measure thesymmetric similarity between two words by av eraging two directional (asymmetric) scores: the coverage of each word?s features by the other.</S>
			<S sid="36" ssid="5">The coverage of u by v is measured by: Cover(u, v) = ? f?F u ?F v w u (f) ? f?F u w u (f) The average can be arithmetic or harmonic: WeedsA(u, v) = 1 2 [Cover(u, v) + Cover(v, u)] WeedsH(u, v) = 2 ? Cover(u, v) ? Cover(v, u) Cover(u, v) + Cover(v, u) Weeds et al also used pmi for feature weights.</S>
			<S sid="37" ssid="6">Binary rule learning algorithms adopted suchlexical similarity approaches for learning rules between templates, where the features of each tem plate are its variable instantiations in a corpus, such as {X=?SCO?, Y =?IBM?}</S>
			<S sid="38" ssid="7">for the example in Section 1.</S>
			<S sid="39" ssid="8">Some works focused on learningrules from comparable corpora, containing com parable documents such as different news articles from the same date on the same topic (Barzilay and Lee, 2003; Ibrahim et al, 2003).</S>
			<S sid="40" ssid="9">Such corpora are highly informative for identifying variations of the same meaning, since, typically, when variableinstantiations are shared across comparable docu ments the same predicates are described.</S>
			<S sid="41" ssid="10">However,it is hard to collect broad-scale comparable cor pora, as the majority of texts are non-comparable.</S>
			<S sid="42" ssid="11">A complementary approach is learning from the abundant regular, non-comparable, corpora.</S>
			<S sid="43" ssid="12">Yet,in such corpora it is harder to recognize varia tions of the same predicate.</S>
			<S sid="44" ssid="13">The DIRT algorithm(Lin and Pantel, 2001) learns non-directional binary rules for templates that are paths in a depen dency parse-tree between two noun variables X and Y . The similarity between two templates t and t ? is the geometric average: DIRT (t, t ? ) = ? Lin x (t, t ? ) ? Lin y (t, t ? ) where Lin xis the Lin similarity between X?s in stantiations of t and X?s instantiations of t ? in a corpus (equivalently for Lin y ).</S>
			<S sid="45" ssid="14">Some workstake the combination of the two variable instantiations in each template occurrence as a single complex feature, e.g. {X-Y =?SCO-IBM?}, and com pare between these complex features of t and t ?</S>
			<S sid="46" ssid="15">(Ravichandran and Hovy, 2002; Szpektor et al, 2004; Sekine, 2005).Directional Measures Most rule learning meth ods apply a symmetric similarity measure between two templates, viewing them as paraphrasing eachother.</S>
			<S sid="47" ssid="16">However, entailment is in general a direc tional relation.</S>
			<S sid="48" ssid="17">For example, ?X acquire Y ? X own Y ? and ?countersuit against X ? lawsuit against X?.</S>
			<S sid="49" ssid="18">(Weeds and Weir, 2003) propose a directional measure for learning hyponymy between twowords, ?l? r?, by giving more weight to the cov erage of the features of l by r (with ? &gt; 1 2 ): WeedsD(l, r)=?Cover(l, r)+(1??)Cover(r, l) When ?=1, this measure degenerates into Cover(l, r), termed Precision(l, r).</S>
			<S sid="50" ssid="19">With 850 Precision(l, r) we obtain a ?soft?</S>
			<S sid="51" ssid="20">version of the inclusion hypothesis presented in (Geffet and Dagan, 2005), which expects l to entail r if the ?important?</S>
			<S sid="52" ssid="21">features of l appear also in r. Similarly, the LEDIR algorithm (Bhagat et al, 2007) identifies the entailment direction between two binary templates, l and r, which participate in a relation learned by (the symmetric) DIRT, by measuring the proportion of instantiations of l that are covered by the instantiations of r. As far as we know, only (Shinyama et al, 2002)and (Pekar, 2006) learn rules between unary tem plates.</S>
			<S sid="53" ssid="22">However, (Shinyama et al, 2002) relies on comparable corpora for identifying paraphrasesand simply takes any two templates from comparable sentences that share a named entity instan tiation to be paraphrases.</S>
			<S sid="54" ssid="23">Such approach is notfeasible for non-comparable corpora where statis tical measurement is required.</S>
			<S sid="55" ssid="24">(Pekar, 2006) learnsrules only between templates related by local dis course (information from different documents is ignored).</S>
			<S sid="56" ssid="25">In addition, their template structure islimited to only verbs and their direct syntactic ar guments, which may yield incorrect rules, e.g. forlight verbs (see Section 5.2).</S>
			<S sid="57" ssid="26">To overcome this limitation, we use a more expressive template struc ture.</S>
	</SECTION>
	<SECTION title="Learning Unary Entailment Rules. " number="3">
			<S sid="58" ssid="1">3.1 Motivations.</S>
			<S sid="59" ssid="2">Most unsupervised rule learning algorithms focused on learning binary entailment rules.</S>
			<S sid="60" ssid="3">How ever, using binary rules for inference is not enough.</S>
			<S sid="61" ssid="4">First, a predicate that can have multiple arguments may still occur with only one of its arguments.For example, in ?The acquisition of TCA was successful?, ?TCA?</S>
			<S sid="62" ssid="5">is the only argument of ?acqui sition?.</S>
			<S sid="63" ssid="6">Second, some predicate expressions are unary by nature.</S>
			<S sid="64" ssid="7">For example, modifiers, such as ?the elected X?, or intransitive verbs.</S>
			<S sid="65" ssid="8">In addition, it appears more tractable to learn all variations for each argument of a predicate separately than to learn them for combinations of argument pairs.For these reasons, it seems that unary rule learn ing should be addressed in addition to binary rule learning.</S>
			<S sid="66" ssid="9">We are further motivated by the fact thatsome (mostly supervised) works in IE found learn ing unary templates useful for recognizing relevant named entities (Riloff, 1996; Sudo et al, 2003; Shinyama and Sekine, 2006), though they did notattempt to learn generic knowledge bases of entail ment rules.This paper investigates acquisition of unary entailment rules from regular non-comparable cor pora.</S>
			<S sid="67" ssid="10">We first describe the structure of unarytemplates and then explore two conceivable approaches for learning unary rules.</S>
			<S sid="68" ssid="11">The first ap proach directly assesses the relation between twogiven templates based on the similarity of their in stantiations in the corpus.</S>
			<S sid="69" ssid="12">The second approach,which was also mentioned in (Iftene and Balahur Dobrescu, 2007), derives unary rules from learned binary rules.</S>
			<S sid="70" ssid="13">3.2 Unary Template Structure.</S>
			<S sid="71" ssid="14">To learn unary rules we first need to define theirstructure.</S>
			<S sid="72" ssid="15">In this paper we work at the syntac tic representation level.</S>
			<S sid="73" ssid="16">Texts are represented by dependency parse trees (using the Minipar parser (Lin, 1998b)) and templates by parse sub-trees.</S>
			<S sid="74" ssid="17">Given a dependency parse tree, any sub-tree can be a candidate template, setting some of its nodesas variables (Sudo et al, 2003).</S>
			<S sid="75" ssid="18">However, the num ber of possible templates is exponential in the sizeof the sentence.</S>
			<S sid="76" ssid="19">In the binary rule learning litera ture, the main solution for exhaustively learning allrules between any pair of templates in a given corpus is to restrict the structure of templates.</S>
			<S sid="77" ssid="20">Typi cally, a template is restricted to be a path in a parse tree between two variable nodes (Lin and Pantel, 2001; Ibrahim et al, 2003).</S>
			<S sid="78" ssid="21">Following this approach, we chose the structure of unary templates to be paths as well, where oneend of the path is the template?s variable.</S>
			<S sid="79" ssid="22">How ever, paths with one variable have more expressive power than paths between two variables, since the combination of two unary paths may generate a binary template that is not a path.</S>
			<S sid="80" ssid="23">For example, the combination of ?X call indictable?</S>
			<S sid="81" ssid="24">and ?call Y indictable?</S>
			<S sid="82" ssid="25">is the template ?X call Y indictable?, which is not a path between X and Y . For every noun node v in a parsed sentence, we generate templates with v as a variable as follows: 1.</S>
			<S sid="83" ssid="26">Traverse the path from v towards the root of.</S>
			<S sid="84" ssid="27">the parse tree.</S>
			<S sid="85" ssid="28">Whenever a candidate pred icate is encountered (any noun, adjective or verb) the path from that node to v is taken as a template.</S>
			<S sid="86" ssid="29">We stop when the first verb orclause boundary (e.g. a relative clause) is encountered, which typically represent the syn tactic boundary of a specific predicate.</S>
			<S sid="87" ssid="30">851 2.</S>
			<S sid="88" ssid="31">To enable templates with control verbs and.</S>
			<S sid="89" ssid="32">light verbs, e.g. ?X help preventing?, ?Xmake noise?, whenever a verb is encountered we generate templates that are paths between v and the verb?s modifiers, either ob jects, prepositional complements or infinite or gerund verb forms (paths ending at stop words, e.g. pronouns, are not generated).</S>
			<S sid="90" ssid="33">3.</S>
			<S sid="91" ssid="34">To capture noun modifiers that act as predi-.</S>
			<S sid="92" ssid="35">cates, e.g. ?the losingX?, we extract template paths between v and each of its modifiers, nouns or adjectives, that are derived from a verb.</S>
			<S sid="93" ssid="36">We use the Catvar database to identify verb derivations (Habash and Dorr, 2003).</S>
			<S sid="94" ssid="37">As an example for the procedure, the templates extracted from the sentence ?The losing party played it safe?</S>
			<S sid="95" ssid="38">with ?party?</S>
			<S sid="96" ssid="39">as the variable are: ?losing X?, ?X play?</S>
			<S sid="97" ssid="40">and ?X play safe?.</S>
			<S sid="98" ssid="41">3.3 Direct Learning of Unary Rules.</S>
			<S sid="99" ssid="42">We applied the lexical similarity measures pre sented in Section 2 for unary rule learning.</S>
			<S sid="100" ssid="43">Each argument instantiation of template t in the corpus is taken as a feature f , and the pmi between t and f is used for the feature?s weight.</S>
			<S sid="101" ssid="44">We first adaptedDIRT for unary templates (unary-DIRT, apply ing Lin-similarity to the single feature vector), as well as its output filtering by LEDIR.</S>
			<S sid="102" ssid="45">The various Weeds measures were also applied 1 : symmetric arithmetic average, symmetric harmonic average, weighted arithmetic average and Precision.</S>
			<S sid="103" ssid="46">After initial analysis, we found that given a right hand side template r, symmetric measures such as Lin (in DIRT) generally tend to prefer (score higher) relations ?l, r?</S>
			<S sid="104" ssid="47">in which l and r are related but do not necessarily participate in an entailment or equivalence relation, e.g. the wrong rule ?kill X ? injure X?.</S>
			<S sid="105" ssid="48">On the other hand, directional measures such as Weeds Precision tend to prefer directional rules inwhich the entailing template is infrequent.</S>
			<S sid="106" ssid="49">If an in frequent template has common instantiations with another template, the coverage of its features istypically high, whether or not an entailment relation exists between the two templates.</S>
			<S sid="107" ssid="50">This behav ior generates high-score incorrect rules.</S>
			<S sid="108" ssid="51">Based on this analysis, we propose a new measure that balances the two behaviors, termed 1We applied the best performing parameter values pre sented in (Bhagat et al, 2007) and (Weeds and Weir, 2003).Balanced-Inclusion (BInc).</S>
			<S sid="109" ssid="52">BInc identifies entail ing templates based on a directional measure but penalizes infrequent templates using a symmetric measure: BInc(l, r) = ? Lin(l, r) ? Precision(l, r) 3.4 Deriving Unary Rules From Binary Rules.</S>
			<S sid="110" ssid="53">An alternative way to learn unary rules is to first learn binary entailment rules and then derive unary rules from them.</S>
			<S sid="111" ssid="54">We derive unary rules from a given binary rule-base in two steps.</S>
			<S sid="112" ssid="55">First, for each binary rule, we generate all possible unary rules that are part of that rule (each unary template is extracted following the same procedure describedin Section 3.2).</S>
			<S sid="113" ssid="56">For example, from ?X find solu tion to Y ? X solve Y ? we generate the unary rules ?X find?</S>
			<S sid="114" ssid="57">X solve?, ?X find solution?</S>
			<S sid="115" ssid="58">Xsolve?, ?solution to Y ? solve Y ? and ?find solu tion to Y ? solve Y ?.</S>
			<S sid="116" ssid="59">The score of each generated rule is set to be the score of the original binary rule.The same unary rule can be derived from different binary rules.</S>
			<S sid="117" ssid="60">For example, ?hire Y ? employ Y ? is derived both from ?X hire Y ? X em ploy Y ? and ?hire Y for Z ? employ Y for Z?, having a different score from each original binary rule.</S>
			<S sid="118" ssid="61">The second step of the algorithm aggregates the different scores yielded for each derived rule to produce the final rule score.</S>
			<S sid="119" ssid="62">Three aggregationfunctions were tested: sum (Derived-Sum), aver age (Derived-Avg) and maximum (Derived-Max).</S>
	</SECTION>
	<SECTION title="Experimental Setup. " number="4">
			<S sid="120" ssid="1">We want to evaluate learned unary and binary rule bases by their utility for NLP applications throughassessing the validity of inferences that are per formed in practice using the rule base.To perform such experiments, we need a test set of seed templates, which correspond to a set of target predicates, and a corpus annotated with allargument mentions of each predicate.</S>
			<S sid="121" ssid="2">The evaluation assesses the correctness of all argument ex tractions, which are obtained by matching in the corpus either the seed templates or templates that entail them according to the rule-base (the latter corresponds to rule-application).</S>
			<S sid="122" ssid="3">Following (Szpektor et al, 2008), we found the ACE 2005 event training set 2useful for this pur pose.</S>
			<S sid="123" ssid="4">This standard IE dataset includes 33 types of event predicates such as Injure, Sue and Divorce.</S>
			<S sid="124" ssid="5">2 http://projects.ldc.upenn.edu/ace/ 852All event mentions are annotated in the corpus, in cluding the instantiated arguments of the predicate.</S>
			<S sid="125" ssid="6">ACE guidelines specify for each event its possible arguments, each associated with a semantic role.</S>
			<S sid="126" ssid="7">For instance, some of the Injure event arguments are Agent, Victim and Time.To utilize the ACE dataset for evaluating entail ment rule applications, we manually represented each ACE event predicate by unary seed templates.</S>
			<S sid="127" ssid="8">For example, the seed templates for Injure are ?A injure?, ?injure V ? and ?injure in T ?.</S>
			<S sid="128" ssid="9">We mapped each event role annotation to the corresponding seed template variable, e.g. ?Agent?</S>
			<S sid="129" ssid="10">to A and ?Victim?</S>
			<S sid="130" ssid="11">to V in the above example.</S>
			<S sid="131" ssid="12">Templatesare matched using a syntactic matcher that han dles simple morpho-syntactic phenomena, as in (Szpektor and Dagan, 2007).</S>
			<S sid="132" ssid="13">A rule application is considered correct if the matched argument is annotated by the corresponding ACE role.</S>
			<S sid="133" ssid="14">For testing binary rule-bases, we automatically generated binary seed templates from any twounary seeds that share the same predicate.</S>
			<S sid="134" ssid="15">For ex ample, for Injure the binary seeds ?A injure V ?, ?A injure in T ? and ?injure V in T ? were automatically generated from the above unary seeds.</S>
			<S sid="135" ssid="16">We performed two adaptations to the ACE dataset to fit it better to our evaluation needs.</S>
			<S sid="136" ssid="17">First, our evaluation aims at assessing the correctness of inferring a specific target semantic meaning, which is denoted by a specific predicate, using rules.</S>
			<S sid="137" ssid="18">Thus, four events that correspond ambiguously tomultiple distinct predicates were ignored.</S>
			<S sid="138" ssid="19">For instance, the Transfer-Money event refers to both do nating and lending money, and thus annotations ofthis event cannot be mapped to a specific seed tem plate.</S>
			<S sid="139" ssid="20">We also omitted 3 events with less than 10mentions, and were left with 26 events (6380 argu ment mentions).</S>
			<S sid="140" ssid="21">Additionally, we regard all entailing mentions under the textual entailment definition as correct.</S>
			<S sid="141" ssid="22">However, event mentions are annotated as correct in ACE only if they explicitly describe the targetevent.</S>
			<S sid="142" ssid="23">For instance, a Divorce mention does entail a preceding marriage event but it does not ex plicitly describe it, and thus it is not annotated as a Marry event.</S>
			<S sid="143" ssid="24">To better utilize the ACE dataset, we considered for a target event the annotations of other events that entail it as being correct as well.We note that each argument was considered sep arately.</S>
			<S sid="144" ssid="25">For example, we marked a mention of a divorced person as entailing the marriage of that person, but did not consider the place and time of the divorce act to be those of the marriage .</S>
	</SECTION>
	<SECTION title="Results and Analysis. " number="5">
			<S sid="145" ssid="1">We implemented the unary rule learning algo rithms described in Section 3 and the binary DIRT algorithm (Lin and Pantel, 2001).</S>
			<S sid="146" ssid="2">We executed each method over the Reuters RCV1 corpus 3 , learning for each template r in the corpus the top100 rules in which r is entailed by another tem plate l, ?l? r?.</S>
			<S sid="147" ssid="3">All rules were learned in canonical form (Szpektor and Dagan, 2007).</S>
			<S sid="148" ssid="4">The rule-base learned by binary DIRT was taken as the input for deriving unary rules from binary rules.</S>
			<S sid="149" ssid="5">The performance of each acquired rule-base was measured for each ACE event.</S>
			<S sid="150" ssid="6">We measured the percentage of correct argument mentions extracted out of all correct argument mentions annotated for the event (recall) and out of all argument mentionsextracted for the event (precision).</S>
			<S sid="151" ssid="7">We also mea sured F1, their harmonic average, and report macro average Recall, Precision and F1 over the 26 event types.</S>
			<S sid="152" ssid="8">No threshold setting mechanism is suggested inthe literature for the scores of the different algo rithms, especially since rules for different right hand side templates have different score ranges.</S>
			<S sid="153" ssid="9">Thus, we follow common evaluation practice (Lin and Pantel, 2001; Geffet and Dagan, 2005) and test each learned rule-set by taking the top K rules for each seed template, whereK ranges from 0 to 100.WhenK=0, no rules are used and mentions are ex tracted only by direct matching of seed templates.</S>
			<S sid="154" ssid="10">Our rule application setting provides a rather simplistic IE system (for example, no named entity recognition or approximate template matching).</S>
			<S sid="155" ssid="11">It is thus useful for comparing different rule-bases,though the absolute extraction figures do not re flect the full potential of the rules.</S>
			<S sid="156" ssid="12">In Secion 5.2 we analyze the full-system?s errors to isolate the rules?</S>
			<S sid="157" ssid="13">contribution to overall system performance.</S>
			<S sid="158" ssid="14">5.1 Results.</S>
			<S sid="159" ssid="15">In this section we focus on the best performing variations of each algorithm type: binary DIRT, unary DIRT, unary Weeds Harmonic, BInc and Derived-Avg.</S>
			<S sid="160" ssid="16">We omitted the results of methods that were clearly inferior to others: (a) WeedsA, WeedsD and Weeds-Precision did not increase 3 http://about.reuters.com/researchandstandards/corpus/ 853Recall over not using rules because rules with in frequent templates scored highest and arithmetic averaging could not balance well these high scores; (b) out of the methods for deriving unary rules from binary rule-bases, Derived-Avg performed best; (c) filtering with (the directional) LEDIR did not improve the performance of unary DIRT.</S>
			<S sid="161" ssid="17">Figure 1 presents Recall, Precision and F1 of themethods for different cutoff points.</S>
			<S sid="162" ssid="18">First, we observe that even when matching only the seed tem plates (K=0), unary seeds outperform the binary seeds in terms of both Precision and Recall.</S>
			<S sid="163" ssid="19">This surprising behavior is consistent through all rulecutoff points: all unary learning algorithms per form better than binary DIRT in all parameters.</S>
			<S sid="164" ssid="20">The inferior behavior of binary DIRT is analyzed in Section 5.2.The graphs show that symmetric unary approaches substantially increase recall, but dramati cally decrease precision already at the top 10 rules.</S>
			<S sid="165" ssid="21">As a result, F1 only decreases for these methods.</S>
			<S sid="166" ssid="22">Lin similarity (DIRT) and Weeds-Harmonic show similar behaviors.</S>
			<S sid="167" ssid="23">They consistently outperform Derived-Avg.</S>
			<S sid="168" ssid="24">One reason for this is that incorrectunary rules may be derived even from correct bi nary rules.</S>
			<S sid="169" ssid="25">For example, from ?X gain seat on Y ? elect X to Y ? the incorrect unary rule ?X gain?</S>
			<S sid="170" ssid="26">electX?</S>
			<S sid="171" ssid="27">is also generated.</S>
			<S sid="172" ssid="28">This problem is less frequent when unary rules are directly scored based on their corpus statistics.</S>
			<S sid="173" ssid="29">The directional measure of BInc yields a more accurate rule-base, as can be seen by the much slower precision reduction rate compared to theother algorithms.</S>
			<S sid="174" ssid="30">As a result, it is the only algo rithm that improves over the F1 baseline of K=0,with the best cutoff point at K=20.</S>
			<S sid="175" ssid="31">BInc?s re call increases moderately compared to other unarylearning approaches, but it is still substantially bet ter than not using rules (a relative recall increase of 50% already at K=10).</S>
			<S sid="176" ssid="32">We found that many of the correct mentions missed by BInc but identified by other methods are due to occasional extractions of incorrect frequent rules, such as partial templates (see Section 5.2).</S>
			<S sid="177" ssid="33">This is reflected in the very low precision of the other methods.</S>
			<S sid="178" ssid="34">On the other hand, some correct rules were only learned by BInc, e.g. ?countersuit againstX?X sue?</S>
			<S sid="179" ssid="35">and ?X take wife ? X marry?.When only one argument is annotated for a specific event mention (28% of ACE predicate mentions, which account for 15% of all annotated arFigure 1: Average Precision, Recall and F1 at dif ferent top K rule cutoff points.</S>
			<S sid="180" ssid="36">guments), binary rules either miss that mention, orextract both the correct argument and another in correct one.</S>
			<S sid="181" ssid="37">To neutralize this bias, we also testedthe various methods only on event mentions an notated with two or more arguments and obtained similar results to those presented for all mentions.</S>
			<S sid="182" ssid="38">This further emphasizes the general advantage of using unary rules over binary rules.</S>
			<S sid="183" ssid="39">854 5.2 Analysis.</S>
			<S sid="184" ssid="40">Binary-DIRT We analyzed incorrect rules both for binary-DIRT and BInc by randomly sampling,for each algorithm, 200 rules that extracted incor rect mentions.</S>
			<S sid="185" ssid="41">We manually classified each rule ?l ? r? as either: (a) Correct - the rule is valid insome contexts of the event but extracted some in correct mentions; (b) Partial Template - l is only apart of a correct template that entails r. For exam ple, learning ?X decide?</S>
			<S sid="186" ssid="42">X meet?</S>
			<S sid="187" ssid="43">instead of ?X decide to meet ? X meet?; (e) Incorrect - other incorrect rules, e.g. ?charge X ? convict X?.Table 1 summarizes the analysis and demonstrates two problems of binary-DIRT.</S>
			<S sid="188" ssid="44">First, rela tive to BInc, it tends to learn incorrect rules for high frequency templates, and therefore extractedmany more incorrect mentions for the same num ber of incorrect rules.</S>
			<S sid="189" ssid="45">Second, a large percentage of incorrect mentions extracted are due to partial templates at the rule left-hand-side.</S>
			<S sid="190" ssid="46">Such rules are leaned because many binary templates have a more complex structure than paths between arguments.</S>
			<S sid="191" ssid="47">As explained in Section 3.2 the unary template structure we use is more expressive, enabling to learn the correct rules.</S>
			<S sid="192" ssid="48">For example, BInc learned?take Y into custody ? arrest Y ? while binary DIRT learned ?X take Y ? X arrest Y ?.</S>
			<S sid="193" ssid="49">System Level Analysis We manually analyzedthe reasons for false positives (incorrect extrac tions) and false negatives (missed extractions) of BInc, at its best performing cutoff point (K=20), by sampling 200 extractions of each type.</S>
			<S sid="194" ssid="50">From the false positives analysis (Table 2) we see that 39% of the errors are due to incorrect rules.</S>
			<S sid="195" ssid="51">The main reasons for learning such rules are those discussed in Section 3.3: (a) related templates that are not entailing; (b) infrequent templates.</S>
			<S sid="196" ssid="52">All learning methods suffer from these issues.</S>
			<S sid="197" ssid="53">As wasshown by our results, BInc provides a first step to wards reducing these problems.</S>
			<S sid="198" ssid="54">Yet, these issues require further research.</S>
			<S sid="199" ssid="55">Apart from incorrectly learned rules, incorrect template matching (e.g. due to parse errors) and context mismatch contribute together 46% of theerrors.</S>
			<S sid="200" ssid="56">Context mismatches occur when the entail ing template is matched in inappropriate contexts.</S>
			<S sid="201" ssid="57">For example, ?slam X ? attack X?</S>
			<S sid="202" ssid="58">should not be applied when X is a ball, only when it is a person.</S>
			<S sid="203" ssid="59">The rule-set net effect on system precision is better estimated by removing these errors and fixing the annotation errors, which yields 72% precision.</S>
			<S sid="204" ssid="60">Binary DIRT Balanced Inclusion Correct 16 (70) 38 (91) Partial Template 27 (2665) 6 (81) Incorrect 157 (2584) 156 (787) Total 200 (5319) 200 (959) Table 1: Rule type distribution of a sample of 200rules that extracted incorrect mentions.</S>
			<S sid="205" ssid="61">The corre sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses.</S>
			<S sid="206" ssid="62">Reason % mentions Incorrect Rule learned 39.0 Context mismatch 27.0 Match error 19.0 Annotation problem 15.0 Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K=20.</S>
			<S sid="207" ssid="63">Reason % mentions Rule not learned 61.5 Match error 25.0 Discourse analysis needed 12.0 Argument is predicative 1.5 Table 3: Distribution of reasons for false negatives (missed argument mentions) by BInc at K=20.</S>
			<S sid="208" ssid="64">Table 3 presents the analysis of false negatives.</S>
			<S sid="209" ssid="65">First, we note that 12% of the arguments cannotbe extracted by rules alone, due to necessary discourse analysis.</S>
			<S sid="210" ssid="66">Thus, a recall upper bound for en tailment rules is 88%.</S>
			<S sid="211" ssid="67">Many missed extractions aredue to rules that were not learned (61.5%).</S>
			<S sid="212" ssid="68">How ever, 25% of the mentions were missed because of incorrect syntactic matching of correctly learned rules.</S>
			<S sid="213" ssid="69">By assuming correct matches in these cases we isolate the recall of the rule-set (along with the seeds), which yields 39% recall.</S>
	</SECTION>
	<SECTION title="Conclusions. " number="6">
			<S sid="214" ssid="1">We presented two approaches for unsupervised ac quisition of unary entailment rules from regular (non-comparable) corpora.</S>
			<S sid="215" ssid="2">In the first approach, rules are directly learned based on distributionalsimilarity measures.</S>
			<S sid="216" ssid="3">The second approach de rives unary rules from a given rule-base of binary rules.</S>
			<S sid="217" ssid="4">Under the first approach we proposed a novel directional measure for scoring entailment rules, termed Balanced-Inclusion.</S>
			<S sid="218" ssid="5">We tested the different approaches utilizing a standard IE test-set and compared them to binary rule learning.</S>
			<S sid="219" ssid="6">Our results suggest the advantage of learning unary rules: (a) unary rule-bases perform 855 better than binary rules; (b) it is better to directly learn unary rules than to derive them from binary rule-bases.</S>
			<S sid="220" ssid="7">In addition, the Balanced-Inclusion measure outperformed all other tested methods.</S>
			<S sid="221" ssid="8">In future work, we plan to explore additional unary template structures and similarity scores, and to improve rule application utilizing context matching methods such as (Szpektor et al, 2008).</S>
			<S sid="222" ssid="9">Acknowledgements This work was partially supported by ISF grant 1095/05, the IST Programme of the EuropeanCommunity under the PASCAL Network of Ex cellence IST-2002-506778 and the NEGEV project (www.negev-initiative.org).</S>
	</SECTION>
</PAPER>
