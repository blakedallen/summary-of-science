<PAPER>
	<S sid="0">Recognizing Contextual Polarity In Phrase-Level Sentiment Analysis</S><ABSTRACT>
		<S sid="1" ssid="1">This paper presents a new approach to phrase-level sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions.</S>
		<S sid="2" ssid="2">With thisapproach, the system is able to automat ically identify the contextual polarity for a large subset of sentiment expressions,achieving results that are significantly bet ter than baseline.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="3" ssid="3">Sentiment analysis is the task of identifying positive and negative opinions, emotions, and evaluations.</S>
			<S sid="4" ssid="4">Most work on sentiment analysis has been done atthe document level, for example distinguishing pos itive from negative reviews.</S>
			<S sid="5" ssid="5">However, tasks suchas multi-perspective question answering and sum marization, opinion-oriented information extraction, and mining product reviews require sentence-levelor even phrase-level sentiment analysis.</S>
			<S sid="6" ssid="6">For exam ple, if a question answering system is to successfully answer questions about people?s opinions, it must be able to pinpoint expressions of positive and negative sentiments, such as we find in the sentences below: (1) African observers generally approved+ of his victory while Western governments denounced?</S>
			<S sid="7" ssid="7">it.</S>
			<S sid="8" ssid="8">(2) A succession of officers filled the TV screen to say they supported+ the people and that the killings were ?not tolerable?.?</S>
			<S sid="9" ssid="9">(3) ?We don?t hate+ the sinner,?</S>
			<S sid="10" ssid="10">he says, ?but we hate?</S>
			<S sid="11" ssid="11">the sin.?</S>
			<S sid="12" ssid="12">A typical approach to sentiment analysis is to start with a lexicon of positive and negative words and phrases.</S>
			<S sid="13" ssid="13">In these lexicons, entries are tagged with their a priori prior polarity: out of context, doesthe word seem to evoke something positive or some thing negative.</S>
			<S sid="14" ssid="14">For example, beautiful has a positiveprior polarity, and horrid has a negative prior polar ity.</S>
			<S sid="15" ssid="15">However, the contextual polarity of the phrase in which a word appears may be different from theword?s prior polarity.</S>
			<S sid="16" ssid="16">Consider the underlined polar ity words in the sentence below:(4) Philip Clapp, president of the National Environ ment Trust, sums up well the general thrust of the reaction of environmental movements: ?There is noreason at all to believe that the polluters are sud denly going to become reasonable.?Of these words, ?Trust,?</S>
			<S sid="17" ssid="17">?well,?</S>
			<S sid="18" ssid="18">?reason,?</S>
			<S sid="19" ssid="19">and ?rea sonable?</S>
			<S sid="20" ssid="20">have positive prior polarity, but they are not all being used to express positive sentiments.The word ?reason?</S>
			<S sid="21" ssid="21">is negated, making the contex tual polarity negative.</S>
			<S sid="22" ssid="22">The phrase ?no reason at all to believe?</S>
			<S sid="23" ssid="23">changes the polarity of the proposition that follows; because ?reasonable?</S>
			<S sid="24" ssid="24">falls within thisproposition, its contextual polarity becomes nega tive.</S>
			<S sid="25" ssid="25">The word ?Trust?</S>
			<S sid="26" ssid="26">is simply part of a referringexpression and is not being used to express a sentiment; thus, its contextual polarity is neutral.</S>
			<S sid="27" ssid="27">Simi larly for ?polluters?: in the context of the article, it simply refers to companies that pollute.</S>
			<S sid="28" ssid="28">Only ?well?</S>
			<S sid="29" ssid="29">has the same prior and contextual polarity.</S>
			<S sid="30" ssid="30">Many things must be considered in phrase-level sentiment analysis.</S>
			<S sid="31" ssid="31">Negation may be local (e.g., not good), or involve longer-distance dependencies such as the negation of the proposition (e.g., does not look very good) or the negation of the subject (e.g., 347 no one thinks that it?s good).</S>
			<S sid="32" ssid="32">In addition, certain phrases that contain negation words intensify ratherthan change polarity (e.g., not only good but amaz ing).</S>
			<S sid="33" ssid="33">Contextual polarity may also be influenced by modality (e.g., whether the proposition is asserted to be real (realis) or not real (irrealis) ? no reason at all to believe is irrealis, for example); word sense (e.g.,Environmental Trust versus He has won the people?s trust); the syntactic role of a word in the sen tence (e.g., polluters are versus they are polluters);and diminishers such as little (e.g., little truth, lit tle threat).</S>
			<S sid="34" ssid="34">(See (Polanya and Zaenen, 2004) for amore detailed discussion of contextual polarity in fluencers.)This paper presents new experiments in automat ically distinguishing prior and contextual polarity.</S>
			<S sid="35" ssid="35">Beginning with a large stable of clues marked with prior polarity, we identify the contextual polarity of the phrases that contain instances of those clues in the corpus.</S>
			<S sid="36" ssid="36">We use a two-step process that employs machine learning and a variety of features.</S>
			<S sid="37" ssid="37">The first step classifies each phrase containing a clue as neutral or polar.</S>
			<S sid="38" ssid="38">The second step takes all phrases marked in step one as polar and disambiguates theircontextual polarity (positive, negative, both, or neutral).</S>
			<S sid="39" ssid="39">With this approach, the system is able to auto matically identify the contextual polarity for a large subset of sentiment expressions, achieving resultsthat are significantly better than baseline.</S>
			<S sid="40" ssid="40">In addition, we describe new manual annotations of contextual polarity and a successful inter-annotator agree ment study.</S>
	</SECTION>
	<SECTION title="Manual Annotation Scheme. " number="2">
			<S sid="41" ssid="1">To create a corpus for the experiments below, weadded contextual polarity judgments to existing annotations in the Multi-perspective Question Answering (MPQA) Opinion Corpus1, namely to the an notations of subjective expressions2.</S>
			<S sid="42" ssid="2">A subjective expression is any word or phrase used to express an opinion, emotion, evaluation, stance, speculation, 1The MPQA Corpus is described in (Wiebe et al, 2005) and available at nrrc.mitre.org/NRRC/publications.htm.</S>
			<S sid="43" ssid="3">2In the MPQA Corpus, subjective expressions are direct subjective expressions with non-neutral expression intensity, plus all the expressive subjective elements.</S>
			<S sid="44" ssid="4">Please see (Wiebe et al, 2005) for more details on the existing annotations in the MPQA Corpus.etc. A general covering term for such states is private state (Quirk et al, 1985).</S>
			<S sid="45" ssid="5">In the MPQA Cor pus, subjective expressions of varying lengths are marked, from single words to long phrases.For this work, our focus is on sentiment expressions ? positive and negative expressions of emo tions, evaluations, and stances.</S>
			<S sid="46" ssid="6">As these are types of subjective expressions, to create the corpus, we just needed to manually annotate the existing subjective expressions with their contextual polarity.</S>
			<S sid="47" ssid="7">In particular, we developed an annotationscheme3 for marking the contextual polarity of sub jective expressions.</S>
			<S sid="48" ssid="8">Annotators were instructed to tag the polarity of subjective expressions as positive, negative, both, or neutral.</S>
			<S sid="49" ssid="9">The positive tag is for positive emotions (I?m happy), evaluations (Greatidea!), and stances (She supports the bill).</S>
			<S sid="50" ssid="10">The negative tag is for negative emotions (I?m sad), eval uations (Bad idea!), and stances (She?s against thebill).</S>
			<S sid="51" ssid="11">The both tag is applied to sentiment expres sions that have both positive and negative polarity.The neutral tag is used for all other subjective expressions: those that express a different type of sub jectivity such as speculation, and those that do not have positive or negative polarity.Below are examples of contextual polarity anno tations.</S>
			<S sid="52" ssid="12">The tags are in boldface, and the subjective expressions with the given tags are underlined.(5) Thousands of coup supporters celebrated (posi tive) overnight, waving flags, blowing whistles . . .</S>
			<S sid="53" ssid="13">(6) The criteria set by Rice are the following: thethree countries in question are repressive (nega tive) and grave human rights violators (negative) . . .</S>
			<S sid="54" ssid="14">(7) Besides, politicians refer to good and evil (both) only for purposes of intimidation and exaggeration.(8) Jerome says the hospital feels (neutral) no dif ferent than a hospital in the states.The annotators were asked to judge the contextual polarity of the sentiment that is ultimately be ing conveyed by the subjective expression, i.e., once the sentence has been fully interpreted.</S>
			<S sid="55" ssid="15">Thus, the subjective expression, they have not succeeded, and 3The annotation instructions are available at http://www.cs.pitt.edu/?twilson.</S>
			<S sid="56" ssid="16">348 will never succeed, was marked as positive in the sentence, They have not succeeded, and will never succeed, in breaking the will of this valiant people.</S>
			<S sid="57" ssid="17">The reasoning is that breaking the will of a valiantpeople is negative; hence, not succeeding in break ing their will is positive.</S>
	</SECTION>
	<SECTION title="Agreement Study. " number="3">
			<S sid="58" ssid="1">To measure the reliability of the polarity annotation scheme, we conducted an agreement study with two annotators, using 10 documents from the MPQA Corpus.</S>
			<S sid="59" ssid="2">The 10 documents contain 447 subjective expressions.</S>
			<S sid="60" ssid="3">Table 1 shows the contingency table for the two annotators?</S>
			<S sid="61" ssid="4">judgments.</S>
			<S sid="62" ssid="5">Overall agreement is 82%, with a Kappa (?)</S>
			<S sid="63" ssid="6">value of 0.72.</S>
			<S sid="64" ssid="7">Neutral Positive Negative Both Total Neutral 123 14 24 0 161 Positive 16 73 5 2 96 Negative 14 2 167 1 184 Both 0 3 0 3 6 Total 153 92 196 6 447 Table 1: Agreement for Subjective Expressions (Agreement: 82%, ?: 0.72) For 18% of the subjective expressions, at least oneannotator used an uncertain tag when marking po larity.</S>
			<S sid="65" ssid="8">If we consider these cases to be borderline and exclude them from the study, percent agreement increases to 90% and Kappa rises to 0.84.</S>
			<S sid="66" ssid="9">Thus, the annotator agreement is especially high when both are certain.</S>
			<S sid="67" ssid="10">(Note that all annotations are included in the experiments described below.)</S>
	</SECTION>
	<SECTION title="Corpus. " number="4">
			<S sid="68" ssid="1">In total, 15,991 subjective expressions from 425 documents (8,984 sentences) were annotated withcontextual polarity as described above.</S>
			<S sid="69" ssid="2">Of these sen tences, 28% contain no subjective expressions, 25% contain only one, and 47% contain two or more.</S>
			<S sid="70" ssid="3">Ofthe 4,247 sentences containing two or more subjec tive expressions, 17% contain mixtures of positive and negative expressions, and 62% contain mixturesof polar (positive/negative/both) and neutral subjec tive expressions.</S>
			<S sid="71" ssid="4">The annotated documents are divided into two sets.</S>
			<S sid="72" ssid="5">The first (66 documents/1,373 sentences/2,808 subjective expressions) is a development set, used for data exploration and feature development.</S>
			<S sid="73" ssid="6">Weuse the second set (359 documents/7,611 sen tences/13,183 subjective expressions) in 10-fold cross-validation experiments, described below.</S>
	</SECTION>
	<SECTION title="Prior-Polarity Subjectivity Lexicon. " number="5">
			<S sid="74" ssid="1">For the experiments in this paper, we use a lexicon of over 8,000 subjectivity clues.</S>
			<S sid="75" ssid="2">Subjectivity clues arewords and phrases that may be used to express pri vate states, i.e., they have subjective usages (though they may have objective usages as well).</S>
			<S sid="76" ssid="3">For this work, only single-word clues are used.</S>
			<S sid="77" ssid="4">To compile the lexicon, we began with a list of subjectivity clues from (Riloff and Wiebe, 2003).</S>
			<S sid="78" ssid="5">The words in this list were grouped in previous work according to their reliability as subjectivity clues.</S>
			<S sid="79" ssid="6">Words that are subjective in most contexts were marked strongly subjective (strongsubj), and those that may only have certain subjective usages were marked weakly subjective (weaksubj).</S>
			<S sid="80" ssid="7">We expanded the list using a dictionary and a thesaurus, and also added words from the GeneralInquirer positive and negative word lists (General Inquirer, 2000) which we judged to be potentially subjective.</S>
			<S sid="81" ssid="8">We also gave the new words reliability tags, either strongsubj or weaksubj.</S>
			<S sid="82" ssid="9">The next step was to tag the clues in the lexicon with their prior polarity.</S>
			<S sid="83" ssid="10">For words that came from positive and negative word lists (General-Inquirer, 2000; Hatzivassiloglou and McKeown, 1997), welargely retained their original polarity, either posi tive or negative.</S>
			<S sid="84" ssid="11">We assigned the remaining words one of the tags positive, negative, both or neutral.</S>
			<S sid="85" ssid="12">By far, the majority of clues, 92.8%, aremarked as having either positive (33.1%) or nega tive (59.7%) prior polarity.</S>
			<S sid="86" ssid="13">Only a small number of clues (0.3%) are marked as having both positive and negative polarity.</S>
			<S sid="87" ssid="14">6.9% of the clues in the lexicon are marked as neutral.</S>
			<S sid="88" ssid="15">Examples of these are verbs such as feel, look, and think, and intensifiers such asdeeply, entirely, and practically.</S>
			<S sid="89" ssid="16">These words are included because, although their prior polarity is neu tral, they are good clues that a sentiment is beingexpressed (e.g., feels slighted, look forward to).</S>
			<S sid="90" ssid="17">In cluding them increases the coverage of the system.</S>
			<S sid="91" ssid="18">349</S>
	</SECTION>
	<SECTION title="Experiments. " number="6">
			<S sid="92" ssid="1">The goal of the experiments described below is to classify the contextual polarity of the expressions that contain instances of the subjectivity clues in our lexicon.</S>
			<S sid="93" ssid="2">What the system specifically does is give each clue instance its own label.</S>
			<S sid="94" ssid="3">Note that thesystem does not try to identify expression bound aries.</S>
			<S sid="95" ssid="4">Doing so might improve performance and is a promising avenue for future research.</S>
			<S sid="96" ssid="5">6.1 Definition of the Gold Standard.</S>
			<S sid="97" ssid="6">We define the gold standard used to train and test the system in terms of the manual annotations described in Section 2.</S>
			<S sid="98" ssid="7">The gold standard class of a clue instance that is not in a subjective expression is neutral: since the clue is not even in a subjective expression, it is not contained in a sentiment expression.</S>
			<S sid="99" ssid="8">Otherwise, if a clue instance appears in just onesubjective expression (or in multiple subjective ex pressions with the same contextual polarity), then the class assigned to the clue instance is the class of the subjective expression(s).</S>
			<S sid="100" ssid="9">If a clue appears in at least one positive and one negative subjective expression (or in a subjective expression marked as both), then its class is both.</S>
			<S sid="101" ssid="10">If it is in a mixture of negative and neutral subjective expressions, its classis negative; if it is in a mixture of positive and neu tral subjective expressions, its class is positive.</S>
			<S sid="102" ssid="11">6.2 Performance of a Prior-Polarity Classifier.</S>
			<S sid="103" ssid="12">An important question is how useful prior polarityalone is for identifying contextual polarity.</S>
			<S sid="104" ssid="13">To answer this question, we create a classifier that simply assumes that the contextual polarity of a clue in stance is the same as the clue?s prior polarity, and weexplore the classifier?s performance on the develop ment set.</S>
			<S sid="105" ssid="14">This simple classifier has an accuracy of 48%.</S>
			<S sid="106" ssid="15">From the confusion matrix given in Table 2, we seethat 76% of the errors result from words with nonneutral prior polarity appearing in phrases with neu tral contextual polarity.</S>
			<S sid="107" ssid="16">6.3 Contextual Polarity Disambiguation.</S>
			<S sid="108" ssid="17">The fact that words with non-neutral prior polarity so frequently appear in neutral contexts led us to Prior-Polarity Classifier Neut Pos Neg Both Total Neut 798 784 698 4 2284 Pos 81 371 40 0 492 Gold Neg 149 181 622 0 952 Both 4 11 13 5 33 Total 1032 1347 1373 9 3761 Table 2: Confusion matrix for the prior-polarity classifier on the development set.adopt a two-step approach to contextual polarity dis ambiguation.</S>
			<S sid="109" ssid="18">For the first step, we concentrate on whether clue instances are neutral or polar in context (where polar in context refers to having a contextual polarity that is positive, negative or both).</S>
			<S sid="110" ssid="19">For the second step, we take all clue instances marked aspolar in step one, and focus on identifying their contextual polarity.</S>
			<S sid="111" ssid="20">For both steps, we develop classi fiers using the BoosTexter AdaBoost.HM (Schapire and Singer, 2000) machine learning algorithm with5000 rounds of boosting.</S>
			<S sid="112" ssid="21">The classifiers are evalu ated in 10-fold cross-validation experiments.</S>
			<S sid="113" ssid="22">6.3.1 Neutral-Polar Classification The neutral-polar classifier uses 28 features, listed in Table 3.</S>
			<S sid="114" ssid="23">Word Features: Word context is a bag of three word tokens: the previous word, the word itself, and the next word.</S>
			<S sid="115" ssid="24">The prior polarity and reliability class are indicated in the lexicon.Modification Features: These are binary rela tionship features.</S>
			<S sid="116" ssid="25">The first four involve relationships with the word immediately before or after: if theword is a noun preceded by an adjective, if the preceding word is an adverb other than not, if the pre ceding word is an intensifier, and if the word itself is an intensifier.</S>
			<S sid="117" ssid="26">A word is considered an intensifier if it appears in a list of intensifiers and if it precedesa word of the appropriate part-of-speech (e.g., an in tensifier adjective must come before a noun).</S>
			<S sid="118" ssid="27">The modify features involve the dependency parse tree for the sentence, obtained by first parsing the sentence (Collins, 1997) and then converting the tree into its dependency representation (Xia and Palmer, 2001).</S>
			<S sid="119" ssid="28">In a dependency representation, every node in the tree structure is a surface word (i.e., there areno abstract nodes such as NP or VP).</S>
			<S sid="120" ssid="29">The edge be tween a parent and a child specifies the grammatical relationship between the two words.</S>
			<S sid="121" ssid="30">Figure 1 shows 350 Word Features Sentence Features Structure Features word token strongsubj clues in current sentence: count in subject: binary word part-of-speech strongsubj clues in previous sentence: count in copular: binary word context strongsubj clues in next sentence: count in passive: binary prior polarity: positive, negative, both, neutral weaksubj clues in current sentence: count reliability class: strongsubj or weaksubj weaksubj clues in previous sentence: count Modification Features weaksubj clues in next sentence: count Document Feature preceeded by adjective: binary adjectives in sentence: count document topic preceeded by adverb (other than not): binary adverbs in sentence (other than not): count preceeded by intensifier: binary cardinal number in sentence: binary is intensifier: binary pronoun in sentence: binary modifies strongsubj: binary modal in sentence (other than will): binary modifies weaksubj: binary modified by strongsubj: binary modified by weaksubj: binary Table 3: Features for neutral-polar classification The human rights report a poses substantial challenge to USthe interpretation of good and evil det det det adj adj objsubj mod mod conj conjpobj pobj p p (pos) (neg) (pos) (neg) (pos) Figure 1: The dependency tree for the sentence The humanrights report poses a substantial challenge to the US interpre tation of good and evil.</S>
			<S sid="122" ssid="31">Prior polarity is marked in parentheses for words that match clues from the lexicon.an example.</S>
			<S sid="123" ssid="32">The modifies strongsubj/weaksubj fea tures are true if the word and its parent share an adj, mod or vmod relationship, and if its parent isan instance of a clue from the lexicon with strongsubj/weaksubj reliability.</S>
			<S sid="124" ssid="33">The modified by strongsubj/weaksubj features are similar, but look for rela tionships and clues in the word?s children.</S>
			<S sid="125" ssid="34">Structure Features: These are binary featuresthat are determined by starting with the word in stance and climbing up the dependency parse tree toward the root, looking for particular relationships, words, or patterns.</S>
			<S sid="126" ssid="35">The in subject feature is true if we find a subj relationship.</S>
			<S sid="127" ssid="36">The in copular feature is true if in subject is false and if a node along the pathis both a main verb and a copular verb.</S>
			<S sid="128" ssid="37">The in pas sive features is true if a passive verb pattern is found on the climb.</S>
			<S sid="129" ssid="38">Sentence Features: These are features that werefound useful for sentence-level subjectivity classifi cation by Wiebe and Riloff (2005).</S>
			<S sid="130" ssid="39">They includecounts of strongsubj and weaksubj clues in the current, previous and next sentences, counts of adjectives and adverbs other than not in the current sen tence, and binary features to indicate whether the sentence contains a pronoun, a cardinal number, and a modal other than will.Document Feature: There is one document feature representing the topic of the document.</S>
			<S sid="131" ssid="40">A doc ument may belong to one of 15 topics ranging fromspecific (e.g., the 2002 presidential election in Zim babwe) to more general (e.g., economics) topics.</S>
			<S sid="132" ssid="41">Table 4 gives neutral-polar classification resultsfor the 28-feature classifier and two simpler classi fiers that provide our baselines.</S>
			<S sid="133" ssid="42">The first row in the table lists the results for a classifier that uses just one feature, the word token.</S>
			<S sid="134" ssid="43">The second row showsthe results for a classifier that uses both the word to ken and the word?s prior polarity as features.</S>
			<S sid="135" ssid="44">The results for the 28-feature classifier are listed in thelast row.</S>
			<S sid="136" ssid="45">The 28-feature classifier performs signifi cantly better (1-tailed t-test, p ? .05) than the two simpler classifiers, as measured by accuracy, polar F-measure, and neutral F-measure (?</S>
			<S sid="137" ssid="46">= 1).</S>
			<S sid="138" ssid="47">It has an accuracy of 75.9%, with a polar F-measure of 63.4 and a neutral F-measure of 82.1.</S>
			<S sid="139" ssid="48">Focusing on the metrics for polar expressions, it?s interesting to note that using just the word token as a feature produces a classifier with a precision slightly better than the 28-feature classifier, but with a recall that is 20% lower.</S>
			<S sid="140" ssid="49">Adding a feature for the prior 351 Word Features word token word prior polarity: positive, negative, both, neutral Polarity Features negated: binary negated subject: binary modifies polarity: positive, negative, neutral, both, notmod modified by polarity: positive, negative, neutral, both, notmod conj polarity: positive, negative, neutral, both, notmod general polarity shifter: binary negative polarity shifter: binary positive polarity shifter: binary Table 6: Features for polarity classification polarity improves recall so that it is only 4.4% lower, but this hurts precision, which drops to 4.2% lower than the 28-feature classifier?s precision.</S>
			<S sid="141" ssid="50">It is only with all the features that we get the best result, good precision with the highest recall.</S>
			<S sid="142" ssid="51">The clues in the prior-polarity lexicon have 19,506 instances in the test set.</S>
			<S sid="143" ssid="52">According to the28-feature neutral-polar classifier, 5,671 of these in stances are polar in context.</S>
			<S sid="144" ssid="53">It is these clue instancesthat are passed on to the second step in the contex tual disambiguation process, polarity classification.</S>
			<S sid="145" ssid="54">6.3.2 Polarity Classification Ideally, this second step in the disambiguationprocess would be a three-way classification task, determining whether the contextual polarity is positive, negative or both.</S>
			<S sid="146" ssid="55">However, although the major ity of neutral expressions have been filtered out by the neutral-polar classification in step one, a numberstill remain.</S>
			<S sid="147" ssid="56">So, for this step, the polarity classifica tion task remains four-way: positive, negative, both, and neutral.Table 6 lists the features used by the polarity classifier.</S>
			<S sid="148" ssid="57">Word token and word prior polarity are un changed from the neutral-polar classifier.</S>
			<S sid="149" ssid="58">Negated is a binary feature that captures whether the word is being locally negated: its value is true if a negation word or phrase is found within the four preceedingwords or in any of the word?s children in the de pendency tree, and if the negation word is not in a phrase that intensifies rather than negates (e.g., notonly).</S>
			<S sid="150" ssid="59">The negated subject feature is true if the sub ject of the clause containing the word is negated.</S>
			<S sid="151" ssid="60">The modifies polarity, modified by polarity, and conj polarity features capture specific relationships between the word instance and other polarity words it may be related to.</S>
			<S sid="152" ssid="61">If the word and its parent in the dependency tree share an obj, adj, mod, or vmod relationship, the modifies polarity feature is set to the prior polarity of the word?s parent (if the parent is not in our prior-polarity lexicon, its prior polarity is set to neutral).</S>
			<S sid="153" ssid="62">The modified by polarity featureis similar, looking for adj, mod, and vmod relation ships and polarity clues within the word?s children.</S>
			<S sid="154" ssid="63">The conj polarity feature determines if the word is in a conjunction.</S>
			<S sid="155" ssid="64">If so, the value of this feature is its sibling?s prior polarity (as above, if the sibling is not in the lexicon, its prior polarity is neutral).</S>
			<S sid="156" ssid="65">Figure 1 helps to illustrate these features: modifies polarity isnegative for the word ?substantial,?</S>
			<S sid="157" ssid="66">modified by po larity is positive for the word ?challenge,?</S>
			<S sid="158" ssid="67">and conj polarity is negative for the word ?good.?</S>
			<S sid="159" ssid="68">The last three polarity features look in a window of four words before, searching for the presence ofparticular types of polarity influencers.</S>
			<S sid="160" ssid="69">General polarity shifters reverse polarity (e.g., little truth, lit tle threat).</S>
			<S sid="161" ssid="70">Negative polarity shifters typically make the polarity of an expression negative (e.g., lack of understanding).</S>
			<S sid="162" ssid="71">Positive polarity shifters typically make the polarity of an expression positive (e.g., abate the damage).</S>
			<S sid="163" ssid="72">The polarity classification results for this second step in the contextual disambiguation process are given in Table 5.</S>
			<S sid="164" ssid="73">Also listed in the table are resultsfor the two simple classifiers that provide our base lines.</S>
			<S sid="165" ssid="74">The first line in Table 5 lists the results forthe classifier that uses just one feature, the word token.</S>
			<S sid="166" ssid="75">The second line shows the results for the clas sifier that uses both the word token and the word?s prior polarity as features.</S>
			<S sid="167" ssid="76">The last line shows theresults for the polarity classifier that uses all 10 fea tures from Table 6.</S>
			<S sid="168" ssid="77">Mirroring the results from step one, the more complex classifier performs significantly better than the simpler classifiers, as measured by accuracyand all of the F-measures.</S>
			<S sid="169" ssid="78">The 10-feature classi fier achieves an accuracy of 65.7%, which is 4.3% higher than the more challenging baseline providedby the word + prior polarity classifier.</S>
			<S sid="170" ssid="79">Positive F measure is 65.1 (5.7% higher); negative F-measure is 77.2 (2.3% higher); and neutral F-measure is 46.2 (13.5% higher).</S>
			<S sid="171" ssid="80">Focusing on the metrics for positive and negative expressions, we again see that the simpler classifiers 352 Acc Polar Rec Polar Prec Polar F Neut Rec Neut Prec Neut F word token 73.6 45.3 72.2 55.7 89.9 74.0 81.2 word+priorpol 74.2 54.3 68.6 60.6 85.7 76.4 80.7 28 features 75.9 56.8 71.6 63.4 87.0 77.7 82.1 Table 4: Results for Step 1 Neutral-Polar Classification Positive Negative Both Neutral Acc Rec Prec F Rec Prec F Rec Prec F Rec Prec F word token 61.7 59.3 63.4 61.2 83.9 64.7 73.1 9.2 35.2 14.6 30.2 50.1 37.7 word+priorpol 63.0 69.4 55.3 61.6 80.4 71.2 75.5 9.2 35.2 14.6 33.5 51.8 40.7 10 features 65.7 67.1 63.3 65.1 82.1 72.9 77.2 11.2 28.4 16.1 41.4 52.4 46.2 Table 5: Results for Step 2 Polarity Classification.</S>
			<S sid="172" ssid="81">Experiment Features Removed AB1 negated, negated subject AB2 modifies polarity, modified by polarity AB3 conj polarity AB4 general, negative, and positive polarity shifters Table 7: Features for polarity classification take turns doing better or worse for precision andrecall.</S>
			<S sid="173" ssid="82">Using just the word token, positive precision is slightly higher than for the 10-feature clas sifier, but positive recall is 11.6% lower.</S>
			<S sid="174" ssid="83">Add the prior polarity, and positive recall improves, but at the expense of precision, which is 12.6% lower than for the 10-feature classifier.</S>
			<S sid="175" ssid="84">The results for negative expressions are similar.</S>
			<S sid="176" ssid="85">The word-token classifier does well on negative recall but poorly on negative precision.</S>
			<S sid="177" ssid="86">When prior polarity is added, negative recall improves but negative precision drops.</S>
			<S sid="178" ssid="87">It is only with the addition of the polarity features that we achieve both higher precisions and higher recalls.</S>
			<S sid="179" ssid="88">To explore how much the various polarity featurescontribute to the performance of the polarity classifier, we perform four experiments.</S>
			<S sid="180" ssid="89">In each experi ment, a different set of polarity features is excluded, and the polarity classifier is retrained and evaluated.</S>
			<S sid="181" ssid="90">Table 7 lists the features that are removed for each experiment.</S>
			<S sid="182" ssid="91">The only significant difference in performance in these experiments is neutral F-measure when the modification features (AB2) are removed.</S>
			<S sid="183" ssid="92">These ablation experiments show that the combination of features is needed to achieve significant results over baseline for polarity classification.</S>
	</SECTION>
	<SECTION title="Related Work. " number="7">
			<S sid="184" ssid="1">Much work on sentiment analysis classifies documents by their overall sentiment, for example deter mining whether a review is positive or negative (e.g., (Turney, 2002; Dave et al, 2003; Pang and Lee,2004; Beineke et al, 2004)).</S>
			<S sid="185" ssid="2">In contrast, our ex periments classify individual words and phrases.</S>
			<S sid="186" ssid="3">A number of researchers have explored learning words and phrases with prior positive or negative polarity(another term is semantic orientation) (e.g., (Hatzi vassiloglou and McKeown, 1997; Kamps and Marx, 2002; Turney, 2002)).</S>
			<S sid="187" ssid="4">In contrast, we begin with a lexicon of words with established prior polarities, and identify the contextual polarity of phrases inwhich instances of those words appear in the cor pus.</S>
			<S sid="188" ssid="5">To make the relationship between that task and ours clearer, note that some word lists used to evaluate methods for recognizing prior polarity areincluded in our prior-polarity lexicon (General Inquirer lists (General-Inquirer, 2000) used for evaluation by Turney, and lists of manually identified pos itive and negative adjectives, used for evaluation by Hatzivassiloglou and McKeown).Some research classifies the sentiments of sen tences.</S>
			<S sid="189" ssid="6">Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004), and Grefenstette et al.</S>
			<S sid="190" ssid="7">(2001)4 all begin by first creating prior-polaritylexicons.</S>
			<S sid="191" ssid="8">Yu and Hatzivassiloglou then assign a sen timent to a sentence by averaging the prior semanticorientations of instances of lexicon words in the sentence.</S>
			<S sid="192" ssid="9">Thus, they do not identify the contextual po larity of individual phrases containing clues, as we 4In (Grefenstette et al, 2001), the units that are classified are fixed windows around named entities rather than sentences.</S>
			<S sid="193" ssid="10">353 do in this paper.</S>
			<S sid="194" ssid="11">Kim and Hovy, Hu and Liu, andGrefenstette et al multiply or count the prior po larities of clue instances in the sentence.</S>
			<S sid="195" ssid="12">They also consider local negation to reverse polarity.</S>
			<S sid="196" ssid="13">However, they do not use the other types of features in our experiments, and they restrict their tags to positiveand negative (excluding our both and neutral categories).</S>
			<S sid="197" ssid="14">In addition, their systems assign one sen timent per sentence; our system assigns contextual polarity to individual expressions.</S>
			<S sid="198" ssid="15">As seen above,sentences often contain more than one sentiment ex pression.</S>
			<S sid="199" ssid="16">Nasukawa, Yi, and colleagues (Nasukawa and Yi, 2003; Yi et al, 2003) classify the contextual polarity of sentiment expressions, as we do.</S>
			<S sid="200" ssid="17">Thus, their workis probably most closely related to ours.</S>
			<S sid="201" ssid="18">They clas sify expressions that are about specific items, and use manually developed patterns to classify polarity.</S>
			<S sid="202" ssid="19">These patterns are high-quality, yielding quite highprecision, but very low recall.</S>
			<S sid="203" ssid="20">Their system classifies a much smaller proportion of the sentiment ex pressions in a corpus than ours does.</S>
	</SECTION>
	<SECTION title="Conclusions. " number="8">
			<S sid="204" ssid="1">In this paper, we present a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions.</S>
			<S sid="205" ssid="2">With this approach, we are able to automatically identify the contextual polarity for a large subset ofsentiment expressions, achieving results that are sig nificantly better than baseline.</S>
	</SECTION>
	<SECTION title="Acknowledgments. " number="9">
			<S sid="206" ssid="1">This work was supported in part by the NSF under grant IIS-0208798 and by the Advanced Research and Development Activity (ARDA).</S>
	</SECTION>
</PAPER>
