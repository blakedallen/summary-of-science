[
  {
    "citance_No": 1, 
    "citing_paper_id": "D07-1031", 
    "citing_paper_authority": 55, 
    "citing_paper_authors": "Mark, Johnson", 
    "raw_text": "The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known: see Besag (2004) for a tutorial introduction and Goldwater and Griffiths (2007) for an application of Gibbs sampling to HMM inference for semi 300 supervised and unsupervised POS tagging", 
    "clean_text": "The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known: see Besag (2004) for a tutorial introduction and Goldwater and Griffiths (2007) for an application of Gibbs sampling to HMM inference for semi 300 supervised and unsupervised POS tagging.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1102", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Marten, van Schijndel | Micha, Elsner", 
    "raw_text": "This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models", 
    "clean_text": "This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "C08-1008", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Jason, Baldridge", 
    "raw_text": "This idea could be used with my approach as well; the most obvious way would be to use prototype words to suggest extra categories (beyond the tag dictionary) for known words and a reduced set of categories for unknown words. Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007)", 
    "clean_text": "Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W11-2208", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Desai, Chen | Chris, Dyer | Shay B., Cohen | Noah A., Smith", 
    "raw_text": "Figure 1 gives an example for a French-English sentence pair. Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference", 
    "clean_text": "Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P08-1085", 
    "citing_paper_authority": 31, 
    "citing_paper_authors": "Yoav, Goldberg | Meni, Adler | Michael, Elhadad", 
    "raw_text": "in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ) .All the work mentioned above focuses on unsupervised English POS tagging", 
    "clean_text": "In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on \"diluted dictionaries\" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P10-1101", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Michael, Connor | Yael, Gertner | Cynthia, Fisher | Dan, Roth", 
    "raw_text": "Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007)", 
    "clean_text": "Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "N09-1010", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Benjamin, Snyder | Tahira, Naseem | Jacob, Eisenstein | Regina, Barzilay", 
    "raw_text": "Each integral can be compute din closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007)", 
    "clean_text": "Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N09-1010", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Benjamin, Snyder | Tahira, Naseem | Jacob, Eisenstein | Regina, Barzilay", 
    "raw_text": "Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here", 
    "clean_text": "Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N09-1010", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Benjamin, Snyder | Tahira, Naseem | Jacob, Eisenstein | Regina, Barzilay", 
    "raw_text": "Monolingual and bilingual baselines Wereimplemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline", 
    "clean_text": "We implemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P11-1087", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003)", 
    "clean_text": "Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P11-1087", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007)", 
    "clean_text": "The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P11-1087", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs", 
    "clean_text": "In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P11-1087", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "The system abbreviations are CGS10 (Christodoulopoulos et al., 2010), BBDK10 (Berg-Kirkpatrick et al, 2010) and GG07 (Goldwater and Griffiths, 2007)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N10-1068", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "David, Chiang | Jonathan, Graehl | Kevin, Knight | Adam, Pauls | Sujith, Ravi", 
    "raw_text": "We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007)", 
    "clean_text": "We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N10-1068", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "David, Chiang | Jonathan, Graehl | Kevin, Knight | Adam, Pauls | Sujith, Ravi", 
    "raw_text": "First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing: raise the probabilities in the sampling distribution to the 1T power, where T is a temperature parameter, decrease T to wards zero, and take a single sample", 
    "clean_text": "First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing: raise the probabilities in the sampling distribution to the 1T power, where T is a temperature parameter, decrease T to wards zero, and take a single sample.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "N10-1068", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "David, Chiang | Jonathan, Graehl | Kevin, Knight | Adam, Pauls | Sujith, Ravi", 
    "raw_text": "We use the same modeling approach as as Goldwater and Griffiths (2007), using a probabilistic tag bigram model in conjunction with a tag-to-word model", 
    "clean_text": "We use the same modeling approach as as Goldwater and Griffiths (2007), using a probabilistic tag bigram model in conjunction with a tag-to-word model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W10-2001", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Stella, Frank | Sharon, Goldwater | Frank, Keller", 
    "raw_text": "In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions tothe model, which incorporate sentence type information", 
    "clean_text": "In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W10-2001", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Stella, Frank | Sharon, Goldwater | Frank, Keller", 
    "raw_text": "We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model", 
    "clean_text": "We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W10-2001", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Stella, Frank | Sharon, Goldwater | Frank, Keller", 
    "raw_text": "In the experiments reported below, we use the Gibbs sampler described by (Goldwater and Griffiths, 2007) for the BHMM, and modify it as necessary for our own extended models", 
    "clean_text": "In the experiments reported below, we use the Gibbs sampler described by (Goldwater and Griffiths, 2007) for the BHMM, and modify it as necessary for our own extended models.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W10-2001", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Stella, Frank | Sharon, Goldwater | Frank, Keller", 
    "raw_text": "We treat sentence type (s) as an observed variable, on the assumption that it is observed via intonation or1Slight corrections need to be made to Equation 5 to ac count for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers", 
    "clean_text": "Slight corrections need to be made to Equation 5 to account for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers.", 
    "keep_for_gold": 0
  }
]
