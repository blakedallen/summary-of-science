<PAPER>
  <S sid="0">Applying Co-Training Methods To Statistical Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We propose a novel Co-Training method for statistical parsing.</S>
    <S sid="2" ssid="2">The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text.</S>
    <S sid="3" ssid="3">The algorithm iteratively labels the entire data set with parse trees.</S>
    <S sid="4" ssid="4">Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">The current crop of statistical parsers share a similar training methodology.</S>
    <S sid="6" ssid="2">They train from the Penn Treebank (Marcus et al., 1993); a collection of 40,000 sentences that are labeled with corrected parse trees (approximately a million word tokens).</S>
    <S sid="7" ssid="3">In this paper, we explore methods for statistical parsing that can be used to combine small amounts of labeled data with unlimited amounts of unlabeled data.</S>
    <S sid="8" ssid="4">In the experiment reported here, we use 9695 sentences of bracketed data (234467 word tokens).</S>
    <S sid="9" ssid="5">Such methods are attractive for the following reasons: In this paper we introduce a new approach that combines unlabeled data with a small amount of labeled (bracketed) data to train a statistical parser.</S>
    <S sid="10" ssid="6">We use a CoTraining method (Yarowsky, 1995; Blum and Mitchell, 1998; Goldman and Zhou, 2000) that has been used previously to train classifiers in applications like word-sense disambiguation (Yarowsky, 1995), document classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999) and apply this method to the more complex domain of statistical parsing.</S>
    <S sid="11" ssid="7">2 Unsupervised techniques in language processing While machine learning techniques that exploit annotated data have been very successful in attacking problems in NLP, there are still some aspects which are considered to be open issues: In the particular domain of statistical parsing there has been limited success in moving towards unsupervised machine learning techniques (see Section 7 for more discussion).</S>
    <S sid="12" ssid="8">A more promising approach is that of combining small amounts of seed labeled data with unlimited amounts of unlabeled data to bootstrap statistical parsers.</S>
    <S sid="13" ssid="9">In this paper, we use one such machine learning technique: Co-Training, which has been used successfully in several classification tasks like web page classification, word sense disambiguation and named-entity recognition.</S>
    <S sid="14" ssid="10">Early work in combining labeled and unlabeled data for NLP tasks was done in the area of unsupervised part of speech (POS) tagging.</S>
    <S sid="15" ssid="11">(Cutting et al., 1992) reported very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes.</S>
    <S sid="16" ssid="12">Tag dictionaries are predefined assignments of all possible POS tags to words in the test data.</S>
    <S sid="17" ssid="13">This impressive result triggered several follow-up studies in which the effect of hand tuning the tag dictionary was quantified as a combination of labeled and unlabeled data.</S>
    <S sid="18" ssid="14">The experiments in (Merialdo, 1994; Elworthy,1994) showed that only in very specific cases HMMs were effective in combining labeled and unlabeled data.</S>
    <S sid="19" ssid="15">However, (Brill, 1997) showed that aggressively using tag dictionaries extracted from labeled data could be used to bootstrap an unsupervised POS tagger with high accuracy (approx 95% on WSJ data).</S>
    <S sid="20" ssid="16">We exploit this approach of using tag dictionaries in our method as well (see Section 3.2 for more details).</S>
    <S sid="21" ssid="17">It is important to point out that, before attacking the problem of parsing using similar machine learning techniques, we face a representational problem which makes it difficult to define the notion of tag dictionary for a statistical parser.</S>
    <S sid="22" ssid="18">The problem we face in parsing is more complex than assigning a small fixed set of labels to examples.</S>
    <S sid="23" ssid="19">If the parser is to be generally applicable, it has to produce a fairly complex &#8220;label&#8221; given an input sentence.</S>
    <S sid="24" ssid="20">For example, given the sentence Pierre Vinken will join the board as a non-executive director, the parser is expected to produce an output as shown in Figure 1.</S>
    <S sid="25" ssid="21">Since the entire parse cannot be reasonably considered as a monolithic label, the usual method in parsing is to decompose the structure assigned in the following way: However, such a recursive decomposition of structure does not allow a simple notion of a tag dictionary.</S>
    <S sid="26" ssid="22">We solve this problem by decomposing the structure in an approach that is different from that shown above which uses context-free rules.</S>
    <S sid="27" ssid="23">The approach uses the notion of tree rewriting as defined in the Lexicalized Tree Adjoining Grammar (LTAG) formalism (Joshi and Schabes, 1992)1 which retains the notion of lexicalization that is crucial in the success of a statistical parser while permitting a simple definition of tag dictionary.</S>
    <S sid="28" ssid="24">For example, the parse in Figure 1 can be generated by assigning the structured labels shown in Figure 2 to each word in the sentence (for simplicity, we assume that the noun phrases are generated here as a single word).</S>
    <S sid="29" ssid="25">We use a tool described in (Xia et al., 2000) to convert the Penn Treebank into this representation.</S>
    <S sid="30" ssid="26">Combining the trees together by rewriting nodes as trees (explained in Section 2.1) gives us the parse tree in Figure 1.</S>
    <S sid="31" ssid="27">A history of the bi-lexical dependencies that define the probability model used to construct the parse is shown in Figure 3.</S>
    <S sid="32" ssid="28">This history is called the derivation tree.</S>
    <S sid="33" ssid="29">In addition, as a byproduct of this kind of representation we obtain more than the phrase structure of each sentence.</S>
    <S sid="34" ssid="30">We also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilisA stochastic LTAG derivation proceeds as follows (Schabes, 1992; Resnik, 1992).</S>
    <S sid="35" ssid="31">An initial tree is selected with probability Pinit and other trees selected by words in the sentence are combined using the operations of substitution and adjoining.</S>
    <S sid="36" ssid="32">These operations are explained below with examples.</S>
    <S sid="37" ssid="33">Each of these operations is performed with probability Pattach.</S>
    <S sid="38" ssid="34">Substitution is defined as rewriting a node in the frontier of a tree with probability Pattach which is said to be proper if: where T, 'q !</S>
    <S sid="39" ssid="35">T0 indicates that tree T0 is substituting into node 'q in tree T. An example of the operation of substitution is shown in Figure 4.</S>
    <S sid="40" ssid="36">Adjoining is defined as rewriting any internal node of a tree by another tree.</S>
    <S sid="41" ssid="37">This is a recursive rule and each adjoining operation is performed with probability Pattach which is proper if: Pattach here is the probability that T0 rewrites an internal node 'q in tree T or that no adjoining (NA) occurs at node 'q in T. The additional factor that accounts for no adjoining at a node is required for the probability to be well-formed.</S>
    <S sid="42" ssid="38">An example of the operation of adjoining is shown in Figure 5.</S>
    <S sid="43" ssid="39">Each LTAG derivation D which was built starting from tree a with n subsequent attachments has the probability: Note that assuming each tree is lexicalized by one word the derivation D corresponds to a sentence of n + 1 words.</S>
    <S sid="44" ssid="40">In the next section we show how to exploit this notion of tag dictionary to the problem of statistical parsing.</S>
  </SECTION>
  <SECTION title="3 Co-Training methods for parsing" number="2">
    <S sid="45" ssid="1">Many supervised methods of learning from a Treebank have been studied.</S>
    <S sid="46" ssid="2">The question we want to pursue in this paper is whether unlabeled data can be used to improve the performance of a statistical parser and at the same time reduce the amount of labeled training data necessary for good performance.</S>
    <S sid="47" ssid="3">We will assume the data that is input to our method will have the following characteristics: The pair of probabilistic models can be exploited to bootstrap new information from unlabeled data.</S>
    <S sid="48" ssid="4">Since both of these steps ultimately have to agree with each other, we can utilize an iterative method called CoTraining that attempts to increase agreement between a pair of statistical models by exploiting mutual constraints between their output.</S>
    <S sid="49" ssid="5">Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and namedentity identification (Collins and Singer, 1999).</S>
    <S sid="50" ssid="6">In all of these cases, using unlabeled data has resulted in performance that rivals training solely from labeled data.</S>
    <S sid="51" ssid="7">However, these previous approaches were on tasks that involved identifying the right label from a small set of labels (typically 2&#8211;3), and in a relatively small parameter space.</S>
    <S sid="52" ssid="8">Compared to these earlier models, a statistical parser has a very large parameter space and the labels that are expected as output are parse trees which have to be built up recursively.</S>
    <S sid="53" ssid="9">We discuss previous work in combining labeled and unlabeled data in more detail in Section 7.</S>
    <S sid="54" ssid="10">Effectively, by picking confidently labeled data from each model to add to the training data, one model is labeling data for the other model.</S>
    <S sid="55" ssid="11">In the representation we use, parsing using a lexicalized grammar is done in two steps: Each of these two steps involves ambiguity which can be resolved using a statistical model.</S>
    <S sid="56" ssid="12">By explicitly representing these two steps independently, we can pursue independent statistical models for each step: These two models have to agree with each other on the trees assigned to each word in the sentence.</S>
    <S sid="57" ssid="13">Not only do the right trees have to be assigned as predicted by the first model, but they also have to fit together to cover the entire sentence as predicted by the second model2.</S>
    <S sid="58" ssid="14">This represents the mutual constraint that each model places on the other.</S>
    <S sid="59" ssid="15">For the words that appear in the (unlabeled) training data, we collect a list of part-of-speech labels and trees that each word is known to select in the training data.</S>
    <S sid="60" ssid="16">This information is stored in a POS tag dictionary and a tree dictionary.</S>
    <S sid="61" ssid="17">It is important to note that no frequency or any other distributional information is stored.</S>
    <S sid="62" ssid="18">The only information stored in the dictionary is which tags or trees can be selected by each word in the training data.</S>
    <S sid="63" ssid="19">We use a count cutoff for trees in the labeled data and combine observed counts into an unobserved tree count.</S>
    <S sid="64" ssid="20">This is similar to the usual technique of assigning the token unknown to infrequent word tokens.</S>
    <S sid="65" ssid="21">In this way, trees unseen in the labeled data but in the tag dictionary are assigned a probability in the parser.</S>
    <S sid="66" ssid="22">The problem of lexical coverage is a severe one for unsupervised approaches.</S>
    <S sid="67" ssid="23">The use of tag dictionaries is a way around this problem.</S>
    <S sid="68" ssid="24">Such an approach has already been used for unsupervised part-of-speech tagging in (Brill, 1997) where seed data of which POS tags can be selected by each word is given as input to the unsupervised tagger.</S>
    <S sid="69" ssid="25">In future work, it would be interesting to extend models for unknown-word handling or other machine learning techniques in clustering or the learning of subcategorization frames to the creation of such tag dictionaries.</S>
  </SECTION>
  <SECTION title="4 Models" number="3">
    <S sid="70" ssid="1">As described before, we treat parsing as a two-step process.</S>
    <S sid="71" ssid="2">The two models that we use are: We select the most likely trees for each word by examining the local context.</S>
    <S sid="72" ssid="3">The statistical model we use to decide this is the trigram model that was used by B. Srinivas in his SuperTagging model (Srinivas, 1997).</S>
    <S sid="73" ssid="4">The model assigns an n-best lattice of tree assignments associated with the input sentence with each path corresponding to an assignment of an elementary tree for each word in the sentence.</S>
    <S sid="74" ssid="5">(for further details, see (Srinivas, 1997)). where T0 ... Tn is a sequence of elementary trees assigned to the sentence W0 ... Wn.</S>
    <S sid="75" ssid="6">We get (2) by using Bayes theorem and we obtain (3) from (2) by ignore the denominator and by applying the usual Markov assumptions.</S>
    <S sid="76" ssid="7">The output of this model is a probabilistic ranking of trees for the input sentence which is sensitive to a small local context window.</S>
    <S sid="77" ssid="8">Once the words in a sentence have selected a set of elementary trees, parsing is the process of attaching these trees together to give us a consistent bracketing of the sentences.</S>
    <S sid="78" ssid="9">Notation: Let T stand for an elementary tree which is lexicalized by a word: w and a part of speech tag: p. Let Pinit (introduced earlier in 2.1) stand for the probability of being root of a derivation tree defined as follows: including lexical information, this is written as: where the variable top indicates that T is the tree that begins the current derivation.</S>
    <S sid="79" ssid="10">There is a useful approximation for Pinit: Pr(T, w, pjtop = 1) ti Pr(labeljtop = 1) where label is the label of the root node of T. where N is the number of bracketing labels and a is a constant used to smooth zero counts.</S>
    <S sid="80" ssid="11">Let Pattach (introduced earlier in 2.1) stand for the probability of attachment of T' into another T: We decompose (8) into the following components: We do a similar decomposition for (9).</S>
    <S sid="81" ssid="12">For each of the equations above, we use a backoff model which is used to handle sparse data problems.</S>
    <S sid="82" ssid="13">We compute a backoff model as follows: Let e1 stand for the original lexicalized model and e2 be the backoff level which only uses part of speech information: For both Pinit and Pattach, let c = Count(e1).</S>
    <S sid="83" ssid="14">Then the backoff model is computed as follows: where A(c) = c (c&#65533;D) and D is the diversity of e1 (i.e. the number of distinct counts for e1).</S>
    <S sid="84" ssid="15">For Pattach we further smooth probabilities (10), (11) and (12).</S>
    <S sid="85" ssid="16">We use (10) as an example, the other two are handled in the same way. where k is the diversity of adjunction, that is: the number of different trees that can attach at that node.</S>
    <S sid="86" ssid="17">T' is the set of all trees T' that can possibly attach at Node in tree T. For our experiments, the value of a is set to 1 100;000.</S>
  </SECTION>
  <SECTION title="5 Co-Training algorithm" number="4">
    <S sid="87" ssid="1">We are now in the position to describe the Co-Training algorithm, which combines the models described in Section 4.1 and in Section 4.2 in order to iteratively label a large pool of unlabeled data.</S>
    <S sid="88" ssid="2">We use the following datasets in the algorithm: labeled a set of sentences bracketed with the correct parse trees. cache a small pool of sentences which is the focus of each iteration of the Co-Training algorithm. unlabeled a large set of unlabeled sentences.</S>
    <S sid="89" ssid="3">The only information we collect from this set of sentences is a tree-dictionary: tree-dict and part-of-speech dictionary: pos-dict.</S>
    <S sid="90" ssid="4">Construction of these dictionaries is covered in Section 3.2.</S>
    <S sid="91" ssid="5">In addition to the above datasets, we also use the usual development test set (termed dev in this paper), and a test set (called test) which is used to evaluate the bracketing accuracy of the parser.</S>
    <S sid="92" ssid="6">The Co-Training algorithm consists of the following steps which are repeated iteratively until all the sentences in the set unlabeled are exhausted.</S>
    <S sid="93" ssid="7">For the experiment reported here, n = 10, and k was set to be n in each iteration.</S>
    <S sid="94" ssid="8">We ran the algorithm for 12 iterations (covering 20480 of the sentences in unlabeled) and then added the best parses for all the remaining sentences.</S>
  </SECTION>
  <SECTION title="6 Experiment" number="5">
    <S sid="95" ssid="1">The experiments we report were done on the Penn Treebank WSJ Corpus (Marcus et al., 1993).</S>
    <S sid="96" ssid="2">The various settings for the Co-Training algorithm (from Section 5) are as follows: While it might seem expensive to run the parser over the cache multiple times, we use the pruning capabilities of the parser to good use here.</S>
    <S sid="97" ssid="3">During the iterations we set the beam size to a value which is likely to prune out all derivations for a large portion of the cache except the most likely ones.</S>
    <S sid="98" ssid="4">This allows the parser to run faster, hence avoiding the usual problem with running an iterative algorithm over thousands of sentences.</S>
    <S sid="99" ssid="5">In the initial runs we also limit the length of the sentences entered into the cache because shorter sentences are more likely to beat out the longer sentences in any case.</S>
    <S sid="100" ssid="6">The beam size is reset when running the parser on the test data to allow the parser a better chance at finding the most likely parse.</S>
    <S sid="101" ssid="7">We scored the output of the parser on Section 23 of the Wall Street Journal Penn Treebank.</S>
    <S sid="102" ssid="8">The following are some aspects of the scoring that might be useful for comparision with other results: No punctuations are scored, including sentence final punctuation.</S>
    <S sid="103" ssid="9">Empty elements are not scored.</S>
    <S sid="104" ssid="10">We used EVALB (written by Satoshi Sekine and Michael Collins) which scores based on PARSEVAL (Black et al., 1991); with the standard parameter file (as per standard practice, part of speech brackets were not part of the evaluation).</S>
    <S sid="105" ssid="11">Also, we used Adwait Ratnaparkhi&#8217;s part-of-speech tagger (Ratnaparkhi, 1996) to tag unknown words in the test data.</S>
    <S sid="106" ssid="12">We obtained 80.02% and 79.64% labeled bracketing precision and recall respectively (as defined in (Black et al., 1991)).</S>
    <S sid="107" ssid="13">The baseline model which was only trained on the 9695 sentences of labeled data performed at 72.23% and 69.12% precision and recall.</S>
    <S sid="108" ssid="14">These results show that training a statistical parser using our Cotraining method to combine labeled and unlabeled data strongly outperforms training only on the labeled data.</S>
    <S sid="109" ssid="15">It is important to note that unlike previous studies, our method of moving towards unsupervised parsing are directly compared to the output of supervised parsers.</S>
    <S sid="110" ssid="16">Certain differences in the applicability of the usual methods of smoothing to our parser cause the lower accuracy as compared to other state of the art statistical parsers.</S>
    <S sid="111" ssid="17">However, we have consistently seen increase in performance when using the Co-Training method over the baseline across several trials.</S>
    <S sid="112" ssid="18">It should be emphasised that this is a result based on less than 20% of data that is usually used by other parsers.</S>
    <S sid="113" ssid="19">We are experimenting with the use of an even smaller set of labeled data to investigate the learning curve.</S>
  </SECTION>
  <SECTION title="7 Previous Work: Combining Labeled and" number="6">
    <S sid="114" ssid="1">The two-step procedure used in our Co-Training method for statistical parsing was incipient in the SuperTagger (Srinivas, 1997) which is a statistical model for tagging sentences with elementary lexicalized structures.</S>
    <S sid="115" ssid="2">This was particularly so in the Lightweight Dependency Analyzer (LDA), which used shortest attachment heuristics after an initial SuperTagging stage to find syntactic dependencies between words in a sentence.</S>
    <S sid="116" ssid="3">However, there was no statistical model for attachments and the notion of mutual constraints between these two steps was not exploited in this work.</S>
    <S sid="117" ssid="4">Previous studies in unsupervised methods for parsing have concentrated on the use of inside-outside algorithm (Lari and Young, 1990; Carroll and Rooth, 1998).</S>
    <S sid="118" ssid="5">However, there are several limitations of the inside-outside algorithm for unsupervised parsing, see (Marcken, 1995) for some experiments that draw out the mismatch between minimizing error rate and iteratively increasing the likelihood of the corpus.</S>
    <S sid="119" ssid="6">Other approaches have tried to move away from phrase structural representations into dependency style parsing (Lafferty et al., 1992; Fong and Wu, 1996).</S>
    <S sid="120" ssid="7">However, there are still inherent computational limitations due to the vast search space (see (Pietra et al., 1994) for discussion).</S>
    <S sid="121" ssid="8">None of these approaches can even be realistically compared to supervised parsers that are trained and tested on the kind of representations and the complexity of sentences that are found in the Penn Treebank.</S>
    <S sid="122" ssid="9">(Chelba and Jelinek, 1998) combine unlabeled and labeled data for parsing with a view towards language modeling applications.</S>
    <S sid="123" ssid="10">The goal in their work is not to get the right bracketing or dependencies but to reduce the word error rate in a speech recognizer.</S>
    <S sid="124" ssid="11">Our approach is closely related to previous CoTraining methods (Yarowsky, 1995; Blum and Mitchell, 1998; Goldman and Zhou, 2000; Collins and Singer, 1999).</S>
    <S sid="125" ssid="12">(Yarowsky, 1995) first introduced an iterative method for increasing a small set of seed data used to disambiguate dual word senses by exploiting the constraint that in a segment of discourse only one sense of a word is used.</S>
    <S sid="126" ssid="13">This use of unlabeled data improved performance of the disambiguator above that of purely supervised methods.</S>
    <S sid="127" ssid="14">(Blum and Mitchell, 1998) further embellish this approach and gave it the name of CoTraining.</S>
    <S sid="128" ssid="15">Their definition of Co-Training includes the notion (exploited in this paper) that different models can constrain each other by exploiting different &#8216;views&#8217; of the data.</S>
    <S sid="129" ssid="16">They also prove some PAC results on learnability.</S>
    <S sid="130" ssid="17">They also discuss an application of classifying web pages by using their method of mutually constrained models.</S>
    <S sid="131" ssid="18">(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called CoBoosting).</S>
    <S sid="132" ssid="19">(Goldman and Zhou, 2000) provide a variant of Co-Training which is suited to the learning of decision trees where the data is split up into different equivalence classes for each of the models and they use hypothesis testing to determine the agreement between the models.</S>
    <S sid="133" ssid="20">In future work we would like to experiment whether some of these ideas could be incorporated into our model.</S>
    <S sid="134" ssid="21">In future work we would like to explore use of the entire 1M words of the WSJ Penn Treebank as our labeled data and to use a larger set of unbracketed WSJ data as input to the Co-Training algorithm.</S>
    <S sid="135" ssid="22">In addition, we plan to explore the following points that bear on understanding the nature of the Co-Training learning algorithm: &#8226; The contribution of the dictionary of trees extracted from the unlabeled set is an issue that we would like to explore in future experiments.</S>
    <S sid="136" ssid="23">Ideally, we wish to design a co-training method where no such information is used from the unlabeled set.</S>
    <S sid="137" ssid="24">&#8226; The relationship between co-training and EM bears investigation.</S>
    <S sid="138" ssid="25">(Nigam and Ghani, 2000) is a study which tries to separate two factors: (1) The gradient descent aspect of EM vs. the iterative nature of co-training and (2) The generative model used in EM vs. the conditional independence between the features used by the two models that is exploited in co-training.</S>
    <S sid="139" ssid="26">Also, EM has been used successfully in text classification in combination of labeled and unlabeled data (see (Nigam et al., 1999)).</S>
    <S sid="140" ssid="27">&#8226; In our experiments, unlike (Blum and Mitchell, 1998) we do not balance the label priors when picking new labeled examples for addition to the training data.</S>
    <S sid="141" ssid="28">One way to incorporate this into our algorithm would be to incorporate some form of sample selection (or active learning) into the selection of examples that are considered as labeled with high confidence (Hwa, 2000).</S>
  </SECTION>
  <SECTION title="8 Conclusion" number="7">
    <S sid="142" ssid="1">In this paper, we proposed a new approach for training a statistical parser that combines labeled with unlabeled data.</S>
    <S sid="143" ssid="2">It uses a Co-Training method where a pair of models attempt to increase their agreement on labeling the data.</S>
    <S sid="144" ssid="3">The algorithm takes as input a small corpus of 9695 sentences (234467 word tokens) of bracketed data, a large pool of unlabeled text and a tag dictionary of lexicalized structures for each word in this training set (based on the LTAG formalism).</S>
    <S sid="145" ssid="4">The algorithm presented iteratively labels the unlabeled data set with parse trees.</S>
    <S sid="146" ssid="5">We then train a statistical parser on the combined set of labeled and unlabeled data.</S>
    <S sid="147" ssid="6">We obtained 80.02% and 79.64% labeled bracketing precision and recall respectively.</S>
    <S sid="148" ssid="7">The baseline model which was only trained on the 9695 sentences of labeled data performed at 72.23% and 69.12% precision and recall.</S>
    <S sid="149" ssid="8">These results show that training a statistical parser using our Co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data.</S>
    <S sid="150" ssid="9">It is important to note that unlike previous studies, our method of moving towards unsupervised parsing can be directly compared to the output of supervised parsers.</S>
    <S sid="151" ssid="10">Unlike previous approaches to unsupervised parsing our method can be trained and tested on the kind of representations and the complexity of sentences that are found in the Penn Treebank.</S>
    <S sid="152" ssid="11">In addition, as a byproduct of our representation we obtain more than the phrase structure of each sentence.</S>
    <S sid="153" ssid="12">We also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilistic treatment.</S>
  </SECTION>
</PAPER>
