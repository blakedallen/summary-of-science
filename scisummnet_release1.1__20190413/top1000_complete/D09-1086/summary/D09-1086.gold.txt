Parser Adaptation and Projection with Quasi-Synchronous Grammar Features
We connect two scenarios in structured learning: adapting a parser trained on one corpus to another annotation style, and projecting syntactic annotations from one language to another.
We propose quasi-synchronous grammar (QG) features for these structured learning tasks.
That is, we score an aligned pair of source and target trees based on local features of the trees and the alignment.
Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to asynchronous grammar, which would insist on some form of structural parallelism.
In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence.
On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments.
Our experiments show that unsupervised QG projection improves on parses trained using only high-precision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone.
When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees.
We think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language.
