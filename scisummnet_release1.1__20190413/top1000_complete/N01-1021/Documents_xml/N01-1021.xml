<PAPER>
  <S sid="0">A Probabilistic Earley Parser As A Psycholinguistic Model</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In human sentence processing, cognitive load can be defined many ways.</S>
    <S sid="2" ssid="2">This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at point in a sentence: the surprisal of word its prefix on a phrase-structural language model.</S>
    <S sid="3" ssid="3">These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis.</S>
    <S sid="4" ssid="4">Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke&#8217;s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry.</S>
  </ABSTRACT>
  <SECTION title="Introduction" number="1">
    <S sid="5" ssid="1">What is the relation between a person&#8217;s knowledge of grammar and that same person&#8217;s application of that knowledge in perceiving syntactic structure?</S>
    <S sid="6" ssid="2">The answer to be proposed here observes three principles.</S>
    <S sid="7" ssid="3">Principle 1 The relation between the parser and grammar is one of strong competence.</S>
    <S sid="8" ssid="4">Strong competence holds that the human sentence processing mechanism directly uses rules of grammar in its operation, and that a bare minimum of extragrammatical machinery is necessary.</S>
    <S sid="9" ssid="5">This hypothesis, originally proposed by Chomsky (Chomsky, 1965, page 9) has been pursued by many researchers (Bresnan, 1982) (Stabler, 1991) (Steedman, 1992) (Shieber and Johnson, 1993), and stands in contrast with an approach directed towards the discovery of autonomous principles unique to the processing mechanism.</S>
    <S sid="10" ssid="6">Principle 2 Frequency affects performance.</S>
    <S sid="11" ssid="7">The explanatory success of neural network and constraint-based lexicalist theories (McClelland and St. John, 1989) (MacDonald et al., 1994) (Taboret al., 1997) suggests a statistical theory of language performance.</S>
    <S sid="12" ssid="8">The present work adopts a numerical view of competition in grammar that is grounded in probability.</S>
    <S sid="13" ssid="9">Principle 3 Sentence processing is eager.</S>
    <S sid="14" ssid="10">&#8220;Eager&#8221; in this sense means the experimental situations to be modeled are ones like self-paced reading in which sentence comprehenders are unrushed and no information is ignored at a point at which it could be used.</S>
    <S sid="15" ssid="11">The proposal is that a person&#8217;s difficulty perceiving syntactic structure be modeled by word-toword surprisal (Attneave, 1959, page 6) which can be directly computed from a probabilistic phrasestructure grammar.</S>
    <S sid="16" ssid="12">The approach taken here uses a parsing algorithm developed by Stolcke.</S>
    <S sid="17" ssid="13">In the course of explaining the algorithm at a very high level I will indicate how the algorithm, interpreted as a psycholinguistic model, observes each principle.</S>
    <S sid="18" ssid="14">After that will come some simulation results, and then a conclusion.</S>
  </SECTION>
  <SECTION title="1 Language models" number="2">
    <S sid="19" ssid="1">Stolcke&#8217;s parsing algorithm was initially applied as a component of an automatic speech recognition system.</S>
    <S sid="20" ssid="2">In speech recognition, one is often interested in the probability that some word will follow, given that a sequence of words has been seen.</S>
    <S sid="21" ssid="3">Given some lexicon of all possible words, a language model assigns a probability to every string of words from the lexicon.</S>
    <S sid="22" ssid="4">This defines a probabilistic language (Grenander, 1967) (Booth and Thompson, 1973) (Soule, 1974) (Wetherell, 1980).</S>
    <S sid="23" ssid="5">A language model helps a speech recognizer focus its attention on words that are likely continuations of what it has recognized so far.</S>
    <S sid="24" ssid="6">This is typically done using conditional probabilities of the form the probability that the nth word will actually be wn given that the words leading up to the nth have been w1, w2, ... wn&#8722;1.</S>
    <S sid="25" ssid="7">Given some finite lexicon, the probability of each possible outcome for Wn can be estimated using that outcome&#8217;s relative frequency in a sample.</S>
    <S sid="26" ssid="8">Traditional language models used for speech are ngram models, in which n &#8722; 1 words of history serve as the basis for predicting the nth word.</S>
    <S sid="27" ssid="9">Such models do not have any notion of hierarchical syntactic structure, except as might be visible through an nword window.</S>
    <S sid="28" ssid="10">Aware that the n-gram obscures many linguistically-significant distinctions (Chomsky, 1956, section 2.3), many speech researchers (Jelinek and Lafferty, 1991) sought to incorporate hierarchical phrase structure into language modeling (see (Stolcke, 1997)) although it was not until the late 1990s that such models were able to significantly improve on 3-grams (Chelba and Jelinek, 1998).</S>
    <S sid="29" ssid="11">Stolcke&#8217;s probabilistic Earley parser is one way to use hierarchical phrase structure in a language model.</S>
    <S sid="30" ssid="12">The grammar it parses is a probabilistic context-free phrase structure grammar (PCFG), e.g.</S>
    <S sid="31" ssid="13">Such a grammar defines a probabilistic language in terms of a stochastic process that rewrites strings of grammar symbols according to the probabilities on the rules.</S>
    <S sid="32" ssid="14">Then each sentence in the language of the grammar has a probability equal to the product of the probabilities of all the rules used to generate it.</S>
    <S sid="33" ssid="15">This multiplication embodies the assumption that rule choices are independent.</S>
    <S sid="34" ssid="16">Sentences with more than one derivation accumulate the probability of all derivations that generate them.</S>
    <S sid="35" ssid="17">Through recursion, infinite languages can be specified; an important mathematical question in this context is whether or not such a grammar is consistent &#8211; whether it assigns some probability to infinite derivations, or whether all derivations are guaranteed to terminate.</S>
    <S sid="36" ssid="18">Even if a PCFG is consistent, it would appear to have another drawback: it only assigns probabilities to complete sentences of its language.</S>
    <S sid="37" ssid="19">This is as inconvenient for speech recognition as it is for modeling reading times.</S>
    <S sid="38" ssid="20">Stolcke&#8217;s algorithm solves this problem by computing, at each word of an input string, the prefix probability.</S>
    <S sid="39" ssid="21">This is the sum of the probabilities of all derivations whose yield is compatible with the string seen so far.</S>
    <S sid="40" ssid="22">If the grammar is consistent (the probabilities of all derivations sum to 1.0) then subtracting the prefix probability from 1.0 gives the total probability of all the analyses the parser has disconfirmed.</S>
    <S sid="41" ssid="23">If the human parser is eager, then the &#8220;work&#8221; done during sentence processing is exactly this disconfirmation.</S>
  </SECTION>
  <SECTION title="2 Earley parsing" number="3">
    <S sid="42" ssid="1">The computation of prefix probabilities takes advantage of the design of the Earley parser (Earley, 1970) which by itself is not probabilistic.</S>
    <S sid="43" ssid="2">In this section I provide a brief overview of Stolcke&#8217;s algorithm but the original paper should be consulted for full details (Stolcke, 1995).</S>
    <S sid="44" ssid="3">Earley parsers work top-down, and propagate predictions confirmed by the input string back up through a set of states representing hypotheses the parser is entertaining about the structure of the sentence.</S>
    <S sid="45" ssid="4">The global state of the parser at any one time is completely defined by this collection of states, a chart, which defines a tree set.</S>
    <S sid="46" ssid="5">A state is a record that specifies An Earley parser has three main functions, predict, scan and complete, each of which can enter new states into the chart.</S>
    <S sid="47" ssid="6">Starting from a dummy start state in which the dot is just to the left of the grammar&#8217;s start symbol, predict adds new states for rules which could expand the start symbol.</S>
    <S sid="48" ssid="7">In these new predicted states, the dot is at the far left-hand side of each rule.</S>
    <S sid="49" ssid="8">After prediction, scan checks the input string: if the symbol immediately following the dot matches the current word in the input, then the dot is moved rightward, across the symbol.</S>
    <S sid="50" ssid="9">The parser has &#8220;scanned&#8221; this word.</S>
    <S sid="51" ssid="10">Finally, complete propagates this change throughout the chart.</S>
    <S sid="52" ssid="11">If, as a result of scanning, any states are now present in which the dot is at the end of a rule, then the left hand side of that rule has been recognized, and any other states having a dot immediately in front of the newly-recognized left hand side symbol can now have their dots moved as well.</S>
    <S sid="53" ssid="12">This happens over and over until no new states are generated.</S>
    <S sid="54" ssid="13">Parsing finishes when the dot in the dummy start state is moved across the grammar&#8217;s start symbol.</S>
    <S sid="55" ssid="14">Stolcke&#8217;s innovation, as regards prefix probabilities is to add two additional pieces of information to each state: &#945;, the forward, or prefix probability, and y the &#8220;inside&#8221; probability.</S>
    <S sid="56" ssid="15">He notes that path An (unconstrained) Earley path, or simply path, is a sequence of Earley states linked by prediction, scanning, or completion. constrained A path is said to be constrained by, or generate a string x if the terminals immediately to the left of the dot in all scanned states, in sequence, form the string x. .</S>
    <S sid="57" ssid="16">.</S>
    <S sid="58" ssid="17">.</S>
    <S sid="59" ssid="18">The significance of Earley paths is that they are in a one-to-one correspondence with left-most derivations.</S>
    <S sid="60" ssid="19">This will allow us to talk about probabilities of derivations, strings and prefixes in terms of the actions performed by Earley&#8217;s parser.</S>
    <S sid="61" ssid="20">(Stolcke, 1995, page 8) This correspondence between paths of parser operations and derivations enables the computation of the prefix probability &#8211; the sum of all derivations compatible with the prefix seen so far.</S>
    <S sid="62" ssid="21">By the correspondence between derivations and Earley paths, one would need only to compute the sum of all paths that are constrained by the observed prefix.</S>
    <S sid="63" ssid="22">But this can be done in the course of parsing by storing the current prefix probability in each state.</S>
    <S sid="64" ssid="23">Then, when a new state is added by some parser operation, the contribution from each antecedent state &#8211; each previous state linked by some parser operation &#8211; is summed in the new state.</S>
    <S sid="65" ssid="24">Knowing the prefix probability at each state and then summing for all parser operations that result in the same new state efficiently counts all possible derivations.</S>
    <S sid="66" ssid="25">Predicting a rule corresponds to multiplying by that rule&#8217;s probability.</S>
    <S sid="67" ssid="26">Scanning does not alter any probabilities.</S>
    <S sid="68" ssid="27">Completion, though, requires knowing y, the inside probability, which records how probable was the inner structure of some recognized phrasal node.</S>
    <S sid="69" ssid="28">When a state is completed, a bottom-up confirmation is united with a top-down prediction, so the &#945; value of the complete-ee is multiplied by the y value of the complete-er.</S>
    <S sid="70" ssid="29">Important technical problems involving leftrecursive and unit productions are examined and overcome in (Stolcke, 1995).</S>
    <S sid="71" ssid="30">However, these complications do not add any further machinery to the parsing algorithm per se beyond the grammar rules and the dot-moving conventions: in particular, there are no heuristic parsing principles or intermediate structures that are later destroyed.</S>
    <S sid="72" ssid="31">In this respect the algorithm observes strong competence &#8211; principle 1.</S>
    <S sid="73" ssid="32">In virtue of being a probabilistic parser it observes principle 2.</S>
    <S sid="74" ssid="33">Finally, in the sense that predict and complete each apply exhaustively at each new input word, the algorithm is eager, satisfying principle 3.</S>
  </SECTION>
  <SECTION title="3 Parallelism" number="4">
    <S sid="75" ssid="1">Psycholinguistic theories vary regarding the amount bandwidth they attribute to the human sentence processing mechanism.</S>
    <S sid="76" ssid="2">Theories of initial parsing preferences (Fodor and Ferreira, 1998) suggest that the human parser is fundamentally serial: a function from a tree and new word to a new tree.</S>
    <S sid="77" ssid="3">These theories explain processing difficulty by appealing to &#8220;garden pathing&#8221; in which the current analysis is faced with words that cannot be reconciled with the structures built so far.</S>
    <S sid="78" ssid="4">A middle ground is held by bounded-parallelism theories (Narayanan and Jurafsky, 1998) (Roark and Johnson, 1999).</S>
    <S sid="79" ssid="5">In these theories the human parser is modeled as a function from some subset of consistent trees and the new word, to a new tree subset.</S>
    <S sid="80" ssid="6">Garden paths arise in these theories when analyses fall out of the set of trees maintained from word to word, and have to be reanalyzed, as on strictly serial theories.</S>
    <S sid="81" ssid="7">Finally, there is the possibility of total parallelism, in which the entire set of trees compatible with the input is maintained somehow from word to word.</S>
    <S sid="82" ssid="8">On such a theory, garden-pathing cannot be explained by reanalysis.</S>
    <S sid="83" ssid="9">The probabilistic Earley parser computes all parses of its input, so as a psycholinguistic theory it is a total parallelism theory.</S>
    <S sid="84" ssid="10">The explanation for garden-pathing will turn on the reduction in the probability of the new tree set compared with the previous tree set &#8211; reanalysis plays no role.</S>
    <S sid="85" ssid="11">Before illustrating this kind of explanation with a specific example, it will be important to first clarify the nature of the linking hypothesis between the operation of the probabilistic Earley parser and the measured effects of the human parser.</S>
  </SECTION>
  <SECTION title="4 Linking hypothesis" number="5">
    <S sid="86" ssid="1">The measure of cognitive effort mentioned earlier is defined over prefixes: for some observed prefix, the cognitive effort expended to parse that prefix is proportional to the total probability of all the structural analyses which cannot be compatible with the observed prefix.</S>
    <S sid="87" ssid="2">This is consistent with eagerness since, if the parser were to fail to infer the incompatibility of some incompatible analysis, it would be delaying a computation, and hence not be eager.</S>
    <S sid="88" ssid="3">This prefix-based linking hypothesis can be turned into one that generates predictions about word-byword reading times by comparing the total effort expended before some word to the total effort after: in particular, take the comparison to be a ratio.</S>
    <S sid="89" ssid="4">Making the further assumption that the probabilities on PCFG rules are statements about how difficult it is to disconfirm each rule', then the ratio of 'This assumption is inevitable given principles 1 and 2.</S>
    <S sid="90" ssid="5">If there were separate processing costs distinct from the optimization costs postulated in the grammar, then strong competence is violated.</S>
    <S sid="91" ssid="6">Defining all grammatical structures as equally easy to disconfirm or perceive likewise voids the gradedness of grammaticality of any content. the &#945; value for the previous word to the &#945; value for the current word measures the combined difficulty of disconfirming all disconfirmable structures at a given word &#8211; the definition of cognitive load.</S>
    <S sid="92" ssid="7">Scaling this number by taking its log gives the surprisal, and defines a word-based measure of cognitive effort in terms of the prefix-based one.</S>
    <S sid="93" ssid="8">Of course, if the language model is sensitive to hierarchical structure, then the measure of cognitive effort so defined will be structure-sensitive as well. could account for garden path structural ambiguity.</S>
    <S sid="94" ssid="9">Grammar (1) generates the celebrated garden path sentence &#8220;the horse raced past the barn fell&#8221; (Bever, 1970).</S>
    <S sid="95" ssid="10">English speakers hearing these words one by one are inclined to take &#8220;the horse&#8221; as the subject of &#8220;raced,&#8221; expecting the sentence to end at the word &#8220;barn.&#8221; This is the main verb reading in figure 1.</S>
  </SECTION>
  <SECTION title="5 Plausibility of Probabilistic Context-Free Grammar" number="6">
    <S sid="96" ssid="1">The debate over the form grammar takes in the mind is clearly a fundamental one for cognitive science.</S>
    <S sid="97" ssid="2">Much recent psycholinguistic work has generated a wealth of evidence that frequency of exposure to linguistic elements can affect our processing (Mitchell et al., 1995) (MacDonald et al., 1994).</S>
    <S sid="98" ssid="3">However, there is no clear consensus as to the size of the elements over which exposure has clearest effect.</S>
    <S sid="99" ssid="4">Gibson and Pearlmutter identify it as an &#8220;outstanding question&#8221; whether or not phrase structure statistics are necessary to explain performance effects in sentence comprehension: Are phrase-level contingent frequency constraints necessary to explain comprehension performance, or are the remaining types of constraints sufficient.</S>
    <S sid="100" ssid="5">If phraselevel contingent frequency constraints are necessary, can they subsume the effects of other constraints (e.g. locality) ?</S>
    <S sid="101" ssid="6">(Gibson and Pearlmutter, 1998, page 13) Equally, formal work in linguistics has demonstrated the inadequacy of context-free grammars as an appropriate model for natural language in the general case (Shieber, 1985).</S>
    <S sid="102" ssid="7">To address this criticism, the same prefix probabilities could be computing using tree-adjoining grammars (Nederhof et al., 1998).</S>
    <S sid="103" ssid="8">With context-free grammars serving as the implicit backdrop for much work in human sentence processing, as well as linguistics2 simplicity seems as good a guide as any in the selection of a grammar formalism.</S>
  </SECTION>
  <SECTION title="6 Garden-pathing" number="7">
    <S sid="104" ssid="1">Probabilistic context-free grammar (1) will help illustrate the way a phrase-structured language model The human sentence processing mechanism is metaphorically led up the garden path by the main verb reading, when, upon hearing &#8220;fell&#8221; it is forced to accept the alternative reduced relative reading shown in figure 2.</S>
    <S sid="105" ssid="2">The confusion between the main verb and the reduced relative readings, which is resolved upon hearing &#8220;fell&#8221; is the empirical phenomenon at issue.</S>
    <S sid="106" ssid="3">As the parse trees indicate, grammar (1) analyzes reduced relative clauses as a VP adjoined to an NP3.</S>
    <S sid="107" ssid="4">In one sample of parsed text4 such adjunctions are about 7 times less likely than simple NPs made up of a determiner followed by a noun.</S>
    <S sid="108" ssid="5">The probabilities of the other crucial rules are likewise estimated by their relative frequencies in the sample.</S>
    <S sid="109" ssid="6">This simple grammar exhibits the essential character of the explanation: garden paths happen at points where the parser can disconfirm alternatives that together comprise a great amount of probability.</S>
    <S sid="110" ssid="7">Note the category ambiguity present with raced which can show up as both a past-tense verb (VBD) and a past participle (VBN).</S>
    <S sid="111" ssid="8">Figure 3 shows the reading time predictions5 derived via the linking hypothesis that reading time at word n is proportional to the surprisal log (&#945;&#945;&#8722;1).</S>
    <S sid="112" ssid="9">At &#8220;fell,&#8221; the parser garden-paths: up until that point, both the main-verb and reduced-relative structures are consistent with the input.</S>
    <S sid="113" ssid="10">The prefix probability before &#8220;fell&#8221; is scanned is more than 10 times greater than after, suggesting that the probability mass of the analyses disconfirmed at that point was indeed great.</S>
    <S sid="114" ssid="11">In fact, all of the probability assigned to the main-verb structure is now lost, and only parses that involve the low-probability NP rule survive &#8211; a rule introduced 5 words back.</S>
    <S sid="115" ssid="12">If this garden path effect is truly a result of both the main verb and the reduced relative structures being simultaneously available up until the final verb, 5Whether the quantitative values of the predicted reading times can be mapped onto a particular experiment involves taking some position on the oft-observed (Gibson and Sch&#168;utze, 1999) imperfect relationship between corpus frequency and psychological norms. then the effect should disappear when words intervene that cancel the reduced relative interpretation early on.</S>
    <S sid="116" ssid="13">To examine this possibility, consider now a different example sentence, this time from the language of grammar (2).</S>
    <S sid="117" ssid="14">The probabilities in grammar (2) are estimated from the same sample as before.</S>
    <S sid="118" ssid="15">It generates a sentence composed of words actually found in the sample, &#8220;the banker told about the buy-back resigned.&#8221; This sentence exhibits the same reduced relative clause structure as does &#8220;the horse raced past the barn fell.&#8221; Grammar (2) also generates6 the subject relative &#8220;the banker who was told about the buy-back resigned.&#8221; Now a comparison of two conditions is possible.</S>
    <S sid="119" ssid="16">RC only the banker who was told about the buyback resigned The words who was cancel the main verb reading, and should make that condition easier to process.</S>
    <S sid="120" ssid="17">This asymmetry is borne out in graphs 4 and 5.</S>
    <S sid="121" ssid="18">At &#8220;resigned&#8221; the probabilistic Earley parser predicts less reading time in the subject relative condition than in the reduced relative condition.</S>
    <S sid="122" ssid="19">This comparison verifies that the same sorts of phenomena treated in reanalysis and bounded parallelism parsing theories fall out as cases of the present, total parallelism theory.</S>
    <S sid="123" ssid="20">Although they used frequency estimates provided by corpus data, the previous two grammars were partially hand-built.</S>
    <S sid="124" ssid="21">They used a subset of the rules found in the sample of parsed text.</S>
    <S sid="125" ssid="22">A grammar including all rules observed in the entire sample supports the same sort of reasoning.</S>
    <S sid="126" ssid="23">In this grammar, instead of just 2 NP rules there are 532, along with 120 S rules.</S>
    <S sid="127" ssid="24">Many of these generate analyses compatible with prefixes of the reduced relative clause at various points during parsing, so the expectation is that the parser will be disconfirming many more hypotheses at each word than in the simpler example.</S>
    <S sid="128" ssid="25">Figure 6 shows the reading time predictions derived from this much richer grammar.</S>
    <S sid="129" ssid="26">Because the terminal vocabulary of this richer grammar is so much larger, a comparatively large amount of information is conveyed by the nouns &#8220;banker&#8221; and &#8220;buy-back&#8221; leading to high surprisal values at those words.</S>
    <S sid="130" ssid="27">However, the garden path effect is still observable at &#8220;resigned&#8221; where the prefix probability ratio is nearly 10 times greater than at either of the nouns.</S>
    <S sid="131" ssid="28">Amid the lexical effects, the probabilistic Earley parser is affected by the same structural ambiguity that affects English speakers.</S>
  </SECTION>
  <SECTION title="7 Subject/Object asymmetry" number="8">
    <S sid="132" ssid="1">The same kind of explanation supports an account of the subject-object relative asymmetry (cf. references in (Gibson, 1998)) in the processing of unreduced relative clauses.</S>
    <S sid="133" ssid="2">Since the Earley parser is designed to work with context-free grammars, the following example grammar adopts a GPSG-style analysis of relative clauses (Gazdar et al., 1985, page 155).</S>
    <S sid="134" ssid="3">The estimates of the ratios for the two S[+R] rules are obtained by counting the proportion of subject relatives among all relatives in the Treebank&#8217;s parsed Brown corpus7.</S>
    <S sid="135" ssid="4">Grammar (3) generates both subject and object relative clauses.</S>
    <S sid="136" ssid="5">S[+R] &#8594; NP[+R] VP is the rule that generates subject relatives and S[+R] &#8594; NP[+R] S/NP generates object relatives.</S>
    <S sid="137" ssid="6">One might expect there to be a greater processing load for object relatives as soon as enough lexical material is present to determine that the sentence is in fact an object relatives.</S>
    <S sid="138" ssid="7">The same probabilistic Earley parser (modified to handle null-productions) explains this asymmetry in the same way as it explains the garden path effect.</S>
    <S sid="139" ssid="8">Its predictions, under the same linking hypothesis as in the previous cases, are depicted in graphs 7 and 8.</S>
    <S sid="140" ssid="9">The mean surprisal for the object relative is about 5.0 whereas the mean surprisal for the subject relative is about 2.1.</S>
  </SECTION>
  <SECTION title="Conclusion" number="9">
    <S sid="141" ssid="1">These examples suggest that a &#8220;total-parallelism&#8221; parsing theory based on probabilistic grammar can characterize some important processing phenomena.</S>
    <S sid="142" ssid="2">In the domain of structural ambiguity in particular, the explanation is of a different kind than in traditional reanalysis models: the order of processing is not theoretically significant, but the estimate of its magnitude at each point in a sentence is.</S>
    <S sid="143" ssid="3">Results with empirically-derived grammars suggest an affirmative answer to Gibson and Pearlmutter&#8217;s quessThe difference in probability between subject and object rules could be due to the work necessary to set up storage for the filler, effectively recapitulating the HOLD Hypothesis (Wanner and Maratsos, 1978, page 119) tion: phrase-level contingent frequencies can do the work formerly done by other mechanisms.</S>
    <S sid="144" ssid="4">Pursuit of methodological principles 1, 2 and 3 has identified a model capable of describing some of the same phenomena that motivate psycholinguistic interest in other theoretical frameworks.</S>
    <S sid="145" ssid="5">Moreover, this recommends probabilistic grammars as an attractive possibility for psycholinguistics by providing clear, testable predictions and the potential for new mathematical insights.</S>
  </SECTION>
</PAPER>
