Lattice-based Minimum Error Rate Training for Statistical Machine Translation
Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.
To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations.
The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum.
Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder.
In this paper, we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice.
Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes.
The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system.
Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.
We find that first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository.
We present a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs.
We apply the SweepLine algorithm to the union to discard redundant linear functions and their associated hypotheses.
We theorize that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice.
In our MERT algorithm we compute the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space.
We extend the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it.
We find that the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10.
