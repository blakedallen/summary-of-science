[
  {
    "citance_No": 1, 
    "citing_paper_id": "P10-2042", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Trevor, Cohn | Philip, Blunsom", 
    "raw_text": "One way to solve the mixing problem is for the sampler to make more global moves ,e.g., with table label re sampling (Johnson and Goldwater,2009) or split-merge (Jain and Neal, 2000)", 
    "clean_text": "One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling (Johnson and Goldwater, 2009) or split-merge (Jain and Neal, 2000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1027", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mark, Johnson | Anne, Christophe | Emmanuel, Dupoux | Katherine, Demuth", 
    "raw_text": "Johnson and Goldwater (2009) showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations", 
    "clean_text": "Johnson and Goldwater (2009) showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1027", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mark, Johnson | Anne, Christophe | Emmanuel, Dupoux | Katherine, Demuth", 
    "raw_text": "collocation models The starting point and baseline for our extension is the adaptor grammar with syllable structurephonotactic constraints and three levels of collo cational structure (521), as prior work has found that this yields the highest word segmentation to ken f-score (Johnson and Goldwater, 2009)", 
    "clean_text": "The starting point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure (5-21), as prior work has found that this yields the highest word segmentation token f-score (Johnson and Goldwater, 2009).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P14-1027", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mark, Johnson | Anne, Christophe | Emmanuel, Dupoux | Katherine, Demuth", 
    "raw_text": "Adaptor Grammar model described in section 2.3 and compare it to the base line grammar with collocations and phonotactics from Johnson and Goldwater (2009)", 
    "clean_text": "Adaptor Grammar model described in section 2.3 and compare it to the baseline grammar with collocations and phonotactics from Johnson and Goldwater (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "E12-1021", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Jagadeesh, Jagarlamudi | Hal, Daum&eacute; III | Raghavendra, Udupa", 
    "raw_text": "We use the standard hyperparam eters values?= 1.0,?= 0.01 and?= 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyper parameters (Johnson and Goldwater, 2009)", 
    "clean_text": "We use the standard hyperparameters values \u03b1 = 1.0, \u03b2 = 0.01 and \u03c4 = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyperparameters (Johnson and Goldwater, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "S12-1011", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Karl Moritz, Hermann | Chris, Dyer | Stephen G., Pulman | Philip, Blunsom", 
    "raw_text": "The Dirich let parameters? are drawn independently from a? (1, 1) distribution, and are re sampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009)", 
    "clean_text": "The Dirichlet parameters \u03b1 are drawn independently from a \u0393(1, 1) distribution, and are resampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P13-2030", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Ruey-Cheng, Chen", 
    "raw_text": "Hierarchical Bayes methods have been main stream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Gold water et al, 2009) and adaptors grammar (Johnsonand Goldwater, 2009)", 
    "clean_text": "Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Goldwater et al, 2009) and adaptors grammar (Johnsonand Goldwater, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D10-1028", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Eric, Hardisty | Jordan, Boyd-Graber | Philip, Resnik", 
    "raw_text": "For the runs where adaptation was used we set the initial Pitman-Yor a and b parameters to 0.01 and 10respectively, then slice sample (Johnson and Goldwater, 2009) .We use the resulting sentence parses for classification", 
    "clean_text": "For the runs where adaptation was used we set the initial Pitman-Yor a and b parameters to 0.01 and 10 respectively, then slice sample (Johnson and Goldwater, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N10-1083", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Alexandre, Bouchard-C&ocirc;t&eacute; | John, DeNero | Dan, Klein", 
    "raw_text": "Model Inference Reg Eval POS Induction? Many-1 W SJ Basic-HMM EM? 63.1 (1.3) Feature-MRF LBFGS 0.1 59.6 (6.9) Feature-HMM EM 1.0 68.1 (1.7) LBFGS 1.0 75.5 (1.1) Grammar Induction? Dir W SJ 10 Basic-DMV EM? 47.8 Feature-DMV EM 0.05 48.3 LBFGS 10.0 63.0 (Cohen and Smith, 2009) 61.3 C T B 10 Basic-DMV EM? 42.5 Feature-DMV EM 1.0 49.9 LBFGS 5.0 53.6 (Cohen and Smith, 2009) 51.9 Word Alignment? AER N IS T ChE n Basic-Model 1 EM? 38.0 Feature-Model 1 EM? 35.6 Basic-HMM EM? 33.8 Feature-HMM EM? 30.0 Word Segmentation? F1 B R Basic-Unigram EM? 76.9 (0.1) Feature-Unigram EM 0.2 84.5 (0.5) LBFGS 0.2 88.0 (0.1) (Johnson and Goldwater, 2009) 87 Table 1: Locally normalized feature-based models outperform all proposed baselines for all four tasks", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N10-1083", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Alexandre, Bouchard-C&ocirc;t&eacute; | John, DeNero | Dan, Klein", 
    "raw_text": "This is the same set-up used by Liang and Klein (2009), Goldwater et al (2006), and Johnson and Goldwater (2009) .This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation", 
    "clean_text": "This is the same set-up used by Liang and Klein (2009), Goldwater et al (2006), and Johnson and Goldwater (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "N10-1083", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Alexandre, Bouchard-C&ocirc;t&eacute; | John, DeNero | Dan, Klein", 
    "raw_text": "It is substantially simpler than the non-parametric Bayesian models proposed by Johnson et al (2007), which require sampling procedures to perform inference and achieve an F1 of 87 (Johnson and Goldwater, 2009)", 
    "clean_text": "It is substantially simpler than the non-parametric Bayesian models proposed by Johnson et al (2007), which require sampling procedures to perform inference and achieve an F1 of 87 (Johnson and Goldwater, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P12-2017", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Benjamin, B&Atilde;&para;rschinger | Mark, Johnson", 
    "raw_text": "While in principle, increasing the number of rejuvenation steps and particles will make this gap smaller and smaller, we believe the existence of the gap to be interesting in its own right, suggesting a general difference in learning behaviour between batch and incremental learners, especially given the similar results in Johnson and Goldwater (2009)", 
    "clean_text": "While in principle, increasing the number of rejuvenation steps and particles will make this gap smaller and smaller, we believe the existence of the gap to be interesting in its own right, suggesting a general difference in learning behaviour between batch and incremental learners, especially given the similar results in Johnson and Goldwater (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P12-1046", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Hiroyuki, Shindo | Yusuke, Miyao | Akinori, Fujino | Masaaki, Nagata", 
    "raw_text": "morphology analysis, word segmentation (Johnsonand Goldwater, 2009), and dependency grammar induction (Cohen et al, 2010), rather than constituent syntax parsing", 
    "clean_text": "Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N10-1081", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Shay B., Cohen | David M., Blei | Noah A., Smith", 
    "raw_text": "We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem", 
    "clean_text": "We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N10-1081", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Shay B., Cohen | David M., Blei | Noah A., Smith", 
    "raw_text": "We use the standard Brent corpus (Brent and Cartwright,1996), which includes 9,790 unsegmented phonemic representations of utterances of child-directed speech from the Bernstein-Ratner (1987) corpus. Johnson and Goldwater (2009) test three gram mars for this segmentation task", 
    "clean_text": "We use the standard Brent corpus (Brent and Cartwright,1996), which includes 9,790 unsegmented phonemic representations of utterances of child-directed speech from the Bernstein-Ratner (1987) corpus. Johnson and Goldwater (2009) test three grammars for this segmentation task.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "N10-1081", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Shay B., Cohen | David M., Blei | Noah A., Smith", 
    "raw_text": "GUnigram and GSyllable can be found in Johnson and Goldwater (2009)", 
    "clean_text": "GUnigram and GSyllable can be found in Johnson and Goldwater (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "N10-1081", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Shay B., Cohen | David M., Blei | Noah A., Smith", 
    "raw_text": "For example, with GUnigramconvergence typically takes 40 iterations with variational inference, while Johnson and Goldwater (2009) ran their sampler for 2,000 iterations, for which 1,000 were for burning in", 
    "clean_text": "For example, with GUnigram convergence typically takes 40 iterations with variational inference, while Johnson and Goldwater (2009) ran their sampler for 2,000 iterations, for which 1,000 were for burning in.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "N10-1081", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Shay B., Cohen | David M., Blei | Noah A., Smith", 
    "raw_text": "This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al, 2009)", 
    "clean_text": "This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "D10-1081", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Valentin, Zhikov | Hiroya, Takamura | Manabu, Okumura", 
    "raw_text": "Johnson and Goldwater (2009) have proposed a novel method based on adaptor grammars, whose accuracy surpasses the aforementioned methods by a large margin, when appropriate assumptions are made regarding the structural units of a language", 
    "clean_text": "Johnson and Goldwater (2009) have proposed a novel method based on adaptor grammars, whose accuracy surpasses the aforementioned methods by a large margin, when appropriate assumptions are made regarding the structural units of a language.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W10-2912", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Constantine, Lignos | Charles D., Yang", 
    "raw_text": "This assumption may make the child learner? s job unnecessarily hard; since syllables are hierarchical structures consisting of segments, treating the linguistic data as unstructured segment sequences makes the problem harder than it actually is. For a given utterance, there are fewer syllables than segments, and hence fewer segmentation possibilities. Modeling the corpus using hierarchical gram mars that can model the input at varying levels (word collocations, words, syllables, onsets, etc.) provide the learner the most flexibility, allowing the learner to build structure from the individual phonemes and apply distributions at each level of abstraction (Johnson and Goldwater, 2009)", 
    "clean_text": "Modeling the corpus using hierarchical grammars that can model the input at varying levels (word collocations, words, syllables, onsets, etc.) provide the learner the most flexibility, allowing the learner to build structure from the individual phonemes and apply distributions at each level of abstraction (Johnson and Goldwater, 2009).", 
    "keep_for_gold": 0
  }
]
