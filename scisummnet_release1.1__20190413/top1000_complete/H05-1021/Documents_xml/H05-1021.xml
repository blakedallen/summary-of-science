<PAPER>
	<S sid="0">Local Phrase Reordering Models For Statistical Machine Translation</S><ABSTRACT>
		<S sid="1" ssid="1">We describe stochastic models of localphrase movement that can be incorporated into a Statistical Machine Translation (SMT) system.</S>
		<S sid="2" ssid="2">These models pro vide properly formulated, non-deficient, probability distributions over reorderedphrase sequences.</S>
		<S sid="3" ssid="3">They are implemented by Weighted Finite State Trans ducers.</S>
		<S sid="4" ssid="4">We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translationmodel incorporating reordering.</S>
		<S sid="5" ssid="5">Our ex periments show that the reordering modelyields substantial improvements in trans lation performance on Arabic-to-English and Chinese-to-English MT tasks.</S>
		<S sid="6" ssid="6">We also show that the procedure scales as the bitext size is increased.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="7" ssid="7">Word and Phrase Reordering is a crucial component of Statistical Machine Translation (SMT) systems.However allowing reordering in translation is computationally expensive and in some cases even prov ably NP-complete (Knight, 1999).</S>
			<S sid="8" ssid="8">Therefore any translation scheme that incorporates reordering must necessarily balance model complexity against the ability to realize the model without approximation.In this paper our goal is to formulate models of lo cal phrase reordering in such a way that they can be embedded inside a generative phrase-based model ? This work was supported by an ONR MURI Grant N00014-01-1-0685.</S>
			<S sid="9" ssid="9">of translation (Kumar et al, 2005).</S>
			<S sid="10" ssid="10">Although thismodel of reordering is somewhat limited and can not capture all possible phrase movement, it forms a proper parameterized probability distribution over reorderings of phrase sequences.</S>
			<S sid="11" ssid="11">We show that with this model it is possible to perform Maximum APosteriori (MAP) decoding (with pruning) and Ex pectation Maximization (EM) style re-estimation of model parameters over large bitext collections.</S>
			<S sid="12" ssid="12">We now discuss prior work on word and phrase reordering in translation.</S>
			<S sid="13" ssid="13">We focus on SMT systemsthat do not require phrases to form syntactic con stituents.</S>
			<S sid="14" ssid="14">The IBM translation models (Brown et al, 1993)describe word reordering via a distortion model de fined over word positions within sentence pairs.</S>
			<S sid="15" ssid="15">The Alignment Template Model (Och et al, 1999) usesphrases rather than words as the basis for transla tion, and defines movement at the level of phrases.</S>
			<S sid="16" ssid="16">Phrase reordering is modeled as a first order Markovprocess with a single parameter that controls the de gree of movement.</S>
			<S sid="17" ssid="17">Our current work is inspired by the block(phrase-pair) orientation model introduced by Tillmann (2004) in which reordering allows neighbor ing blocks to swap.</S>
			<S sid="18" ssid="18">This is described as a sequence of orientations (left, right, neutral) relative to themonotone block order.</S>
			<S sid="19" ssid="19">Model parameters are blockspecific and estimated over word aligned trained bi text using simple heuristics.</S>
			<S sid="20" ssid="20">Other researchers (Vogel, 2003; Zens and Ney, 2003; Zens et al, 2004) have reported performance gains in translation by allowing deviations from monotone word and phrase order.</S>
			<S sid="21" ssid="21">In these cases, 161 0c 4c 5c 0d 1d 1v 2v 3v 4v 5v 6v 7v 1f 2f 3f 4f 5f 6f 7f 8f 9f 2d 3d 4d 5d 2c 3c1c x 1 x 2 x 3 x 4 x 5 1e 5e 7e2e 3e 4e 6e 9e8e u 1 u 2 u 3 u 4 u 5 y 1 y 5y 4y 3y 2 doivent de_25_%exportationsgrains fl?chir exportations grains de_25_%doivent fl?chir 1 les exportations de les exportations de grains doivent fl?chir de 25 % grains doivent fl?chir de_25_% 1.exportations doiventgrains fl?chir de_25_% grain exports are_projected_to by_25_% grain exports are projected to fall by 25 %Sentence fall Source Language Target Language Sentence Figure 1: TTM generative translation process; here, I = 9,K = 5, R = 7, J = 9.reordering is not governed by an explicit probabilis tic model over reordered phrases; a language model is employed to select the translation hypothesis.</S>
			<S sid="22" ssid="22">Wealso note the prior work of Wu (1996), closely re lated to Tillmann?s model.</S>
	</SECTION>
	<SECTION title="The WFST Reordering Model. " number="2">
			<S sid="23" ssid="1">The Translation Template Model (TTM) is a genera tive model of phrase-based translation (Brown et al, 1993).</S>
			<S sid="24" ssid="2">Bitext is described via a stochastic processthat generates source (English) sentences and trans forms them into target (French) sentences (Fig 1 and Eqn 1).</S>
			<S sid="25" ssid="3">P (fJ1 , v R 1 , d K 0 , c K 0 , y K 1 , x K 1 , u K 1 ,K, e I 1) = P (eI1)?</S>
			<S sid="26" ssid="4">Source Language Model G P (uK1 ,K|e I 1)?</S>
			<S sid="27" ssid="5">Source Phrase Segmentation W P (xK1 |u K 1 ,K, e I 1)?</S>
			<S sid="28" ssid="6">Phrase Translation and Reordering R P (vR1 , d K 0 , c K 0 , y K 1 |x K 1 , u K 1 ,K, e I 1)?</S>
			<S sid="29" ssid="7">Target Phrase Insertion ? P (fJ1 |v R 1 , d K 0 , c K 0 , y K 1 , x K 1 , u K 1 ,K, e I 1) Target Phrase Segmentation ?</S>
			<S sid="30" ssid="8">(1) The TTM relies on a Phrase-Pair Inventory (PPI) consisting of target language phrases and theirsource language translations.</S>
			<S sid="31" ssid="9">Translation is mod eled via component distributions realized as WFSTs (Fig 1 and Eqn 1) : Source Language Model (G),Source Phrase Segmentation (W ), Phrase Transla tion and Reordering (R), Target Phrase Insertion (?), and Target Phrase Segmentation (?)</S>
			<S sid="32" ssid="10">(Kumar et al., 2005).TTM Reordering Previously, the TTM was for mulated with reordering prior to translation; here,we perform reordering of phrase sequences follow ing translation.</S>
			<S sid="33" ssid="11">Reordering prior to translation was found to be memory intensive and unwieldy (Kumaret al, 2005).</S>
			<S sid="34" ssid="12">In contrast, we will show that the cur rent model can be used for both phrase alignment and translation.</S>
			<S sid="35" ssid="13">2.1 The Phrase Reordering Model.</S>
			<S sid="36" ssid="14">We now describe two WFSTs that allow local reordering within phrase sequences.</S>
			<S sid="37" ssid="15">The simplest allows swapping of adjacent phrases.</S>
			<S sid="38" ssid="16">The second allows phrase movement within a three phrase win dow.</S>
			<S sid="39" ssid="17">Our formulation ensures that the overall modelprovides a proper parameterized probability distribution over reordered phrase sequences; we empha size that the resulting distribution is not degenerate.</S>
			<S sid="40" ssid="18">Phrase reordering (Fig 2) takes as its input a French phrase sequence in English phrase order x1, x2, ..., xK . This is then reordered into French phrase order y1, y2, ..., yK . Note that words within phrases are not affected.</S>
			<S sid="41" ssid="19">We make the following conditional independence assumption: P (yK1 |x K 1 , u K 1 ,K, e I 1) = P (y K 1 |x K 1 , u K 1 ).</S>
			<S sid="42" ssid="20">(2)Given an input phrase sequence xK1 we now associate a unique jump sequence bK1 with each per missible output phrase sequence yK1 . The jump bk measures the displacement of the kth phrase xk, i.e. xk ? yk+bk , k ? {1, 2, ...,K}.</S>
			<S sid="43" ssid="21">(3) The jump sequence bK1 is constructed such that yK1is a permutation of xK1 . This is enforced by con structing all models so that ?K k=1 bk = 0.</S>
			<S sid="44" ssid="22">We now redefine the model in terms of the jump sequence P (yK1 |x K 1 , u K 1 ) (4) = { P (bK1 |x K 1 , u K 1 ) yk+bk = xk ?k 0 otherwise, 162 x 2 x 3 x 4 x 5x 1 y 2 y 3 y 4 y 5y 1 3b = 01b = +12b = ?1 4b = 0 5b = 0 doivent de_25_%exportations fl?chir exportations grains de_25_%doivent fl?chir grains Figure 2: Phrase reordering and jump sequence.</S>
			<S sid="45" ssid="23">where yK1 is determined by xK1 and bK1 . Each jump bk depends on the phrase-pair (xk, uk) and preceding jumps bk?11 P (bK1 |x K 1 , u K 1 ) = K?</S>
			<S sid="46" ssid="24">k=1 P (bk|xk, uk, ?k?1), (5) where ?k?1 is an equivalence classification (state) of the jump sequence bk?11 . The jump sequence bK1 can be described by a deterministic finite state machine.</S>
			<S sid="47" ssid="25">?(bk?11 ) is the state arrived at by bk?11 ; we will use ?k?1 to denote ?(bk?11 ).We will investigate phrase reordering by restrict ing the maximum allowable jump to 1 phrase and to 2 phrases; we will refer to these reordering models as MJ-1 and MJ-2.</S>
			<S sid="48" ssid="26">In the first case, bk ? {0,+1,?1} while in the second case, bk ? {0,+1,?1,+2,?2}.</S>
			<S sid="49" ssid="27">2.2 Reordering WFST for MJ-1.</S>
			<S sid="50" ssid="28">We first present the Finite State Machine of the phrase reordering process (Fig 3) which has twoequivalence classes (FSM states) for any given his tory bk?11 ; ?(b k?1 1 ) ? {1, 2}.</S>
			<S sid="51" ssid="29">A jump of +1 has to be followed by a jump of ?1, and 1 is the start and end state; this ensures ?K k=1 bk = 0.</S>
			<S sid="52" ssid="30">1 b=+1 b=?1 b=0 2 Figure 3: Phrase reordering process for MJ-1.</S>
			<S sid="53" ssid="31">Under this restriction, the probability of the jump bk (Eqn 5) can be simplified as P (bk|xk, uk, ?(b k?1 1 )) = (6)?</S>
			<S sid="54" ssid="32">?1(xk, uk) bk = +1, ?k?1 = 1 1 ? ?1(xk, uk) bk = 0, ?k?1 = 1 1 bk = ?1, ?k?1 = 2.</S>
			<S sid="55" ssid="33">There is a single parameter jump probability ?1(x, u) = P (b = +1|x, u) associated with each phrase-pair (x, u) in the phrase-pair inventory.</S>
			<S sid="56" ssid="34">This is the probability that the phrase-pair (x, u) appears out of order in the transformed phrase sequence.We now describe the MJ-1 WFST.</S>
			<S sid="57" ssid="35">In the presentation, we use upper-case letters to denote the En glish phrases (uk) and lower-case letters to denote the French phrases (xk and yk).</S>
			<S sid="58" ssid="36">The PPI for this example is given in Table 1.</S>
			<S sid="59" ssid="37">English French Parameters u x P (x|u) ?1(x, u) A a 0.5 0.2 A d 0.5 0.2 B b 1.0 0.4 C c 1.0 0.3 D d 1.0 0.8Table 1: Example phrase-pair inventory with trans lation and reordering probabilities.</S>
			<S sid="60" ssid="38">The input to the WFST (Fig 4) is a lattice of French phrase sequences derived from the Frenchsentence to be translated.</S>
			<S sid="61" ssid="39">The outputs are the cor responding English phrase sequences.</S>
			<S sid="62" ssid="40">Note that the reordering is performed on the English side.</S>
			<S sid="63" ssid="41">The WFST is constructed by adding a self-loop for each French phrase in the input lattice, and a 2-arc path for every pair of adjacent French phrases in the lattice.</S>
			<S sid="64" ssid="42">The WFST incorporates the translation model P (x|u) and the reordering model P (b|x, u).</S>
			<S sid="65" ssid="43">The score on a self-loop with labels (u, x) is P (x|u) ?</S>
			<S sid="66" ssid="44">(1 ? ?1(x, u)); on a 2-arc path with labels (u1, x1) and (u2, x2), the score on the 1st arc is P (x2|u1) ? ?1(x2, u1) and on the 2nd arc is P (x1|u2).</S>
			<S sid="67" ssid="45">In this example, the input to this transducer is asingle French phrase sequence V : a, b, c. We per form the WFST composition R?V , project the result on the input labels, and remove the epsilons to form the acceptor (R?V )1 which contains the six English phrase sequences (Fig 4).</S>
			<S sid="68" ssid="46">Translation Given a French sentence, a lattice of translations is obtained using the weighted finite state composition: T = G ? W ? R ? ?</S>
			<S sid="69" ssid="47">T . The most-likely translation is obtained as the path with the highest probability in T . Alignment Given a sentence-pair (E,F ), a lattice of phrase alignments is obtained by the finite state composition: B = S ? W ? R ? ?</S>
			<S sid="70" ssid="48">T , where 163 A : b / 0.1 A B D 0.4 x 0.6 x 0.2 = 0.480 B A D 0.4 x 0.5 x 0.2 = 0.040 A D B 0.4 x 0.8 x 0.4 = 0.128 A A B 0.4 x 0.1 x 0.4 = 0.016 A B A 0.4 x 0.6 x 0.4 = 0.096 B A A 0.4 x 0.5 x 0.4 = 0.080 VR 1( ) A : b / 0.5 R V a b d B : b / 0.6D : d / 0.2 A : d / 0.4A : a / 0.4 B : a / 0.4 B : d / 0.4 D : b / 0.8 Figure 4: WFST for the MJ-1 model.</S>
			<S sid="71" ssid="49">S is an acceptor for the English sentence E, and T is an acceptor for the French sentence F . TheViterbi alignment is found as the path with the high est probability in B. The WFST composition gives the word-to-word alignments between the sentences.</S>
			<S sid="72" ssid="50">However, to obtain the phrase alignments, we need to construct additional FSTs not described here.</S>
			<S sid="73" ssid="51">2.3 Reordering WFST for MJ-2.</S>
			<S sid="74" ssid="52">MJ-2 reordering restricts the maximum allowablejump to 2 phrases and also insists that the reorder ing take place within a window of 3 phrases.</S>
			<S sid="75" ssid="53">This latter condition implies that for an input sequence {a, b, c, d}, we disallow the three output sequences: {b, d, a, c; c, a, d, b; c, d, a, b; }.</S>
			<S sid="76" ssid="54">In the MJ-2 finite state machine, a given history bk?11 can lead to one of the six states in Fig 5.</S>
			<S sid="77" ssid="55">b=0 1 23 45 6 b=?1 b=+1b=?1 b=+2 b=0 b=?2 b=?1 b=+1 b=?2 Figure 5: Phrase reordering process for MJ-2.</S>
			<S sid="78" ssid="56">The jump probability of Eqn 5 becomes P (bk|xk, uk, ?k?1) = ? ????</S>
			<S sid="79" ssid="57">?1(xk, uk) bk = 1, ?k?1 = 1 ?2(xk, uk) bk = 2, ?k?1 = 1{ 1 ? ?1(xk, uk) ??2(xk, uk) bk = 0, ?k?1 = 1 (7) { ?1(xk, uk) bk = 1, ?k?1 = 2 1 ? ?1(xk, uk) bk = ?1, ?k?1 = 2 (8) { 0.5 bk = 0, ?k?1 = 3 0.5 bk = ?1, ?k?1 = 3.</S>
			<S sid="80" ssid="58">(9) { 1 bk = ?2, ?k?1 = 4 (10) { 1 bk = ?2, ?k?1 = 5 (11) { 1 bk = ?1, ?k?1 = 6 (12) We note that the distributions (Eqns 7 and 8) are based on two parameters ?1(x, u) and ?2(x, u) for each phrase-pair (x, u).</S>
			<S sid="81" ssid="59">Suppose the input is a phrase sequence a, b, c, the MJ-2 model (Fig 5) allows 6 possible reorderings:a, b, c; a, c, b; b, a, c; b, c, a; c, a, b; c, b, a. The distri bution Eqn 9 ensures that the sequences b, c, a andc, b, a are assigned equal probability.</S>
			<S sid="82" ssid="60">The distribu tions in Eqns 10-12 ensure that the maximum jump is 2 phrases and the reordering happens within awindow of 3 phrases.</S>
			<S sid="83" ssid="61">By insisting that the pro cess start and end at state 1 (Fig 5), we ensure that the model is not deficient.</S>
			<S sid="84" ssid="62">A WFST implementing the MJ-2 model can be easily constructed for bothphrase alignment and translation, following the con struction described for the MJ-1 model.</S>
	</SECTION>
	<SECTION title="Estimation of the Reordering Models. " number="3">
			<S sid="85" ssid="1">The Translation Template Model relies on an in ventory of target language phrases and their source language translations.</S>
			<S sid="86" ssid="2">Our goal is to estimate the reordering model parameters P (b|x, u) for each phrase-pair (x, u) in this inventory.</S>
			<S sid="87" ssid="3">However, when translating a given test set, only a subset of the phrase-pairs is needed.</S>
			<S sid="88" ssid="4">Although there may be an advantage in estimating the model parameters under an inventory that covers all the training bitext, we fix the phrase-pair inventory to cover only the phrases on the test set.</S>
			<S sid="89" ssid="5">Estimation of the reordering model parameters over the training bitext is then performed under this test-set specific inventory.</S>
			<S sid="90" ssid="6">164 We employ the EM algorithm to obtain Maximum Likelihood (ML) estimates of the reordering model parameters.</S>
			<S sid="91" ssid="7">Applying EM to the MJ-1 reordering model gives the following ML parameter estimates for each phrase-pair (u, x).</S>
			<S sid="92" ssid="8">??1(x, u) = Cx,u(0,+1) Cx,u(0,+1) + Cx,u(0, 0) .</S>
			<S sid="93" ssid="9">(13) Cx,u(?, b) is defined for ? = 1, 2 and b = ?1, 0,+1.</S>
			<S sid="94" ssid="10">Any permissible phrase alignment of a sentence pair corresponds to a bK1 sequence, which in turn specifies a ?K1 sequence.</S>
			<S sid="95" ssid="11">Cx,u(?, b) is the expected number of times the phrase-pair x, u isaligned with a jump of b phrases when the jump history is ?.</S>
			<S sid="96" ssid="12">We do not use full EM but a Viterbi train ing procedure that obtains the counts for the best (Viterbi) alignments.</S>
			<S sid="97" ssid="13">If a phrase-pair (x, u) is never seen in the Viterbi alignments, we back-off to a flat parameter ?1(x, u) = 0.05.</S>
			<S sid="98" ssid="14">The ML parameter estimates for the MJ-2 modelare given in Table 2, with Cx,u(?, b) defined similarly.</S>
			<S sid="99" ssid="15">In our training scenario, we use WFST op erations to obtain Viterbi phrase alignments of the training bitext where the initial reordering model parameters (?0(x, u)) are set to a uniform value of 0.05.</S>
			<S sid="100" ssid="16">The counts Cx,u(s, b) are then obtained over the phrase alignments.</S>
			<S sid="101" ssid="17">Finally the ML estimates of the parameters are computed using Eqn 13 (MJ-1) or Eqn 14 (MJ-2).</S>
			<S sid="102" ssid="18">We will refer to the Viterbi trained models as MJ-1 VT and MJ-2 VT. Table 3 shows the MJ-1 VT parameters for some example phrase-pairs in the Arabic-English (A-E) task.</S>
			<S sid="103" ssid="19">u x ?1(x, u) which is the closest Aqrb 1.0 international trade tjArp EAlmyp 0.8 the foreign ministry wzArp xArjyp 0.6 arab league jAmEp dwl Erbyp 0.4 Table 3: MJ-1 parameters for A-E phrase-pairs.To validate alignment under a PPI, we mea sure performance of the TTM word alignmentson French-English (500 sent-pairs) and Chinese English (124 sent-pairs) (Table 4).</S>
			<S sid="104" ssid="20">As desired, the Alignment Recall (AR) and Alignment Error Rate(AER) improve modestly while Alignment Preci sion (AP) remains constant.</S>
			<S sid="105" ssid="21">This suggests that themodels allow more words to be aligned and thus im prove the recall; MJ-2 gives a further improvementin AR and AER relative to MJ-1.</S>
			<S sid="106" ssid="22">Alignment preci Reordering Metrics (%) Frn-Eng Chn-Eng AP AR AER AP AR AER None 94.2 84.8 10.0 85.1 47.1 39.3 MJ-1 VT 94.1 86.8 9.1 85.3 49.4 37.5 MJ-2 VT 93.9 87.4 8.9 85.3 50.9 36.3 Table 4: Alignment Performance with Reordering.</S>
			<S sid="107" ssid="23">sion depends on the quality of the word alignments within the phrase-pairs and does not change muchby allowing phrase reordering.</S>
			<S sid="108" ssid="24">This experiment val idates the estimation procedure based on the phrase alignments; however, we do not advocate the use of TTM as an alternate word alignment technique.</S>
	</SECTION>
	<SECTION title="Translation Experiments. " number="4">
			<S sid="109" ssid="1">We perform our translation experiments on the large data track of the NIST Arabic-to-English (A-E) andChinese-to-English (C-E) MT tasks; we report re sults on the NIST 2002, 2003, and 2004 evaluation test sets 1.</S>
			<S sid="110" ssid="2">4.1 Exploratory Experiments.</S>
			<S sid="111" ssid="3">In these experiments the training data is restricted to FBIS bitext in C-E and the news bitexts in A-E.</S>
			<S sid="112" ssid="4">The bitext consists of chunk pairs aligned at sentence and sub-sentence level (Deng et al, 2004).</S>
			<S sid="113" ssid="5">In A-E, the training bitext consists of 3.8M English words, 3.2M Arabic words and 137K chunk pairs.</S>
			<S sid="114" ssid="6">In C-E, the training bitext consists of 11.7M English words, 8.9M Chinese words and 674K chunk pairs.Our Chinese text processing consists of word seg mentation (using the LDC segmenter) followed bygrouping of numbers.</S>
			<S sid="115" ssid="7">For Arabic our text pro cessing consisted of a modified Buckwalter analysis(LDC2002L49) followed by post processing to sep arate conjunctions, prepostions and pronouns, andAl-/w- deletion.</S>
			<S sid="116" ssid="8">The English text is processed us ing a simple tokenizer based on the text processing utility available in the the NIST MT-eval toolkit.</S>
			<S sid="117" ssid="9">The Language Model (LM) training data consistsof approximately 400M words of English text de rived from Xinhua and AFP (English Gigaword), the English side of FBIS, the UN and A-E News texts, and the online archives of The People?s Daily.</S>
			<S sid="118" ssid="10">Table 5 gives the performance of the MJ-1 andMJ-2 reordering models when translation is per formed using a 4-gram LM.</S>
			<S sid="119" ssid="11">We report performance on the 02, 03, 04 test sets and the combined test set 1http://www.nist.gov/speech/tests/mt/ 165 ??1(x, u) = Cx,u(1,+1) + Cx,u(2,+1) Cx,u(1,+1) + Cx,u(1, 0) + Cx,u(1,+2) + Cx,u(2,+1) + Cx,u(2,?1) ??2(x, u) = (Cx,u(1, 0) + Cx,u(2,?1) + Cx,u(1,+2))Cx,u(1,+2) (Cx,u(1,+1) + Cx,u(1, 0) + Cx,u(1,+2) + Cx,u(2,+1) + Cx,u(2,?1))(Cx,u(1,+2) + Cx,u(1, 0)) Table 2: ML parameter estimates for MJ-2 model.</S>
			<S sid="120" ssid="12">Reordering BLEU (%) Arabic-English Chinese-English 02 03 04 ALL 02 03 04 ALL None 37.5 40.3 36.8 37.8 ? 0.6 24.2 23.7 26.0 25.0 ? 0.5 MJ-1 flat 40.4 43.9 39.4 40.7 ? 0.6 25.7 24.5 27.4 26.2 ? 0.5 MJ-1 VT 41.3 44.8 40.3 41.6 ? 0.6 25.8 24.5 27.8 26.5 ? 0.5 MJ-2 flat 41.0 44.4 39.7 41.1 ? 0.6 26.4 24.9 27.7 26.7 ? 0.5 MJ-2 VT 41.7 45.3 40.6 42.0 ? 0.6 26.5 24.9 27.9 26.8 ? 0.5 Table 5: Performance of MJ-1 and MJ-2 reordering models with a 4-gram LM.</S>
			<S sid="121" ssid="13">(ALL=02+03+04).</S>
			<S sid="122" ssid="14">For the combined set (ALL), wealso show the 95% BLEU confidence interval com puted using bootstrap resampling (Och, 2003).Row 1 gives the performance when no reordering model is used.</S>
			<S sid="123" ssid="15">The next two rows show the in fluence of the MJ-1 reordering model; in row 2, a flat probability of ?1(x, u) = 0.05 is used for all phrase-pairs; in row 3, a reordering probability isestimated for each phrase-pair using Viterbi Train ing (Eqn 13).</S>
			<S sid="124" ssid="16">The last two rows show the effect ofthe MJ-2 reordering model; row 4 uses flat proba bilities (?1(x, u) = 0.05, ?2(x, u) = 0.01) for all phrase-pairs; row 5 applies reordering probabilities estimating with Viterbi Training for each phrase-pair (Table 2).On both language-pairs, we observe that reorder ing yields significant improvements.</S>
			<S sid="125" ssid="17">The gains from phrase reordering are much higher on A-E relative to C-E; this could be related to the fact that the word order differences between English and Arabic are much higher than the differences between Englishand Chinese.</S>
			<S sid="126" ssid="18">MJ-1 VT outperforms flat MJ-1 show ing that there is value in estimating the reordering parameters from bitext.</S>
			<S sid="127" ssid="19">Finally, the MJ-2 VT model performs better than the flat MJ-2 model, but onlymarginally better than the MJ-1 VT model.</S>
			<S sid="128" ssid="20">There fore estimation does improve the MJ-2 model but allowing reordering beyond a window of 1 phrase is not useful when translating either Arabic or Chinese into English in this framework.The flat MJ-1 model outperforms the no reordering case and the flat MJ-2 model is better than the flat MJ-1 model; we hypothesize that phrase reordering increases search space of translations thatallows the language model to select a higher qual ity hypothesis.</S>
			<S sid="129" ssid="21">This suggests that these models of phrase reordering actually require strong languagemodels to be effective.</S>
			<S sid="130" ssid="22">We now investigate the inter action between language models and reordering.Our goal here is to measure translation performance of reordering models over variable span n gram LMs (Table 6).</S>
			<S sid="131" ssid="23">We observe that both MJ-1 and MJ-2 models yield higher improvements under higher order LMs: e.g. on A-E, gains under 3g (3.6 BLEU points on MJ-1, 0.2 points on MJ-2) are higher than the gains with 2g (2.4 BLEU points on MJ-1, 0.1 points on MJ-2).</S>
			<S sid="132" ssid="24">Reordering BLEU (%) A-E C-E 2g 3g 4g 2g 3g 4g None 21.0 36.8 37.8 16.1 24.8 25.0 MJ-1 VT 23.4 40.4 41.6 16.2 25.9 26.5 MJ-2 VT 23.5 40.6 42.0 16.0 26.1 26.8 Table 6: Reordering with variable span n-gram LMs on Eval02+03+04 set.We now measure performance of the reorder ing models across the three test set genres used in the NIST 2004 evaluation: news, editorials, andspeeches.</S>
			<S sid="133" ssid="25">On A-E, MJ-1 and MJ-2 yield larger im provements on News relative to the other genres;on C-E, the gains are larger on Speeches and Ed itorials relative to News.</S>
			<S sid="134" ssid="26">We hypothesize that thePhrase-Pair Inventory, reordering models and lan guage models could all have been biased away from the test set due to the training data.</S>
			<S sid="135" ssid="27">There may also be less movement across these other genres.</S>
			<S sid="136" ssid="28">166 Reordering BLEU (%) A-E C-E News Eds Sphs News Eds Sphs None 41.1 30.8 33.3 23.6 25.9 30.8 MJ-1 VT 45.6 32.6 35.7 24.8 27.8 33.3 MJ-2 VT 46.2 32.7 35.5 24.8 27.8 33.7 Table 7: Performance across Eval 04 test genres.</S>
			<S sid="137" ssid="29">BLEU (%) Arabic-English Chinese-English Reordering 02 03 04n 02 03 04n None 40.2 42.3 43.3 28.9 27.4 27.3 MJ-1 VT 43.1 45.0 45.6 30.2 28.2 28.9 MET-Basic 44.8 47.2 48.2 31.3 30.3 30.3 MET-IBM1 45.2 48.2 49.7 31.8 30.7 31.0 Table 8: Translation Performance on Large Bitexts.</S>
			<S sid="138" ssid="30">4.2 Scaling to Large Bitext Training Sets.</S>
			<S sid="139" ssid="31">We here describe the integration of the phrase re ordering model in an MT system trained on largebitexts.</S>
			<S sid="140" ssid="32">The text processing and language models have been described in ? 4.1.</S>
			<S sid="141" ssid="33">Alignment Mod els are trained on all available bitext (7.6M chunk pairs/207.4M English words/175.7M Chinese words on C-E and 5.1M chunk pairs/132.6M English words/123.0M Arabic words on A-E), and word alignments are obtained over the bitext.</S>
			<S sid="142" ssid="34">Phrase-pairs are then extracted from the word alignments (Koehn et al, 2003).</S>
			<S sid="143" ssid="35">MJ-1 model parameters are estimated over all bitext on A-E and over the non-UN bitext on C-E.</S>
			<S sid="144" ssid="36">Finally we use Minimum Error Training(MET) (Och, 2003) to train log-linear scaling fac tors that are applied to the WFSTs in Equation 1.</S>
			<S sid="145" ssid="37">04news (04n) is used as the MET training set.</S>
			<S sid="146" ssid="38">Table 8 reports the performance of the system.Row 1 gives the performance without phrase re ordering and Row 2 shows the effect of the MJ-1 VT model.</S>
			<S sid="147" ssid="39">The MJ-1 VT model is used in an initial decoding pass with the four-gram LM to generate translation lattices.</S>
			<S sid="148" ssid="40">These lattices are then rescored under parameters obtained using MET (MET-basic), and 1000-best lists are generated.</S>
			<S sid="149" ssid="41">The 1000-best lists are augmented with IBM Model-1 (Brown et al., 1993) scores and then rescored with a second setof MET parameters.</S>
			<S sid="150" ssid="42">Rows 3 and 4 show the perfor mance of the MET-basic and MET-IBM1 models.</S>
			<S sid="151" ssid="43">We observe that the maximum likelihood phrasereordering model (MJ-1 VT) yields significantly improved translation performance relative to the mono tone phrase order translation baseline.</S>
			<S sid="152" ssid="44">This confirms the translation performance improvements found over smaller training bitexts.</S>
			<S sid="153" ssid="45">We also find additional gains by applying MET to optimize the scaling parameters that are applied to the WFST component distributions within the TTM(Equation 1).</S>
			<S sid="154" ssid="46">In this procedure, the scale factor applied to the MJ-1 VT Phrase Translation and Re ordering component is estimated along with scale factors applied to the other model components; in other words, the ML-estimated phrase reorderingmodel itself is not affected by MET, but the likeli hood that it assigns to a phrase sequence is scaled by a single, discriminatively optimized weight.</S>
			<S sid="155" ssid="47">The improvements from MET (see rows MET-Basic andMET- IBM1) demonstrate that the MJ-1 VT reordering models can be incorporated within a discrimi native optimized translation system incorporating a variety of models and estimation procedures.</S>
	</SECTION>
	<SECTION title="Discussion. " number="5">
			<S sid="156" ssid="1">In this paper we have described local phrase reorder ing models developed for use in statistical machine translation.</S>
			<S sid="157" ssid="2">The models are carefully formulated so that they can be implemented as WFSTs, and we show how the models can be incorporated into the Translation Template Model to perform phrasealignment and translation using standard WFST operations.</S>
			<S sid="158" ssid="3">Previous approaches to WFST-based re ordering (Knight and Al-Onaizan, 1998; Kumarand Byrne, 2003; Tsukada and Nagata, 2004) con structed permutation acceptors whose state spaces grow exponentially with the length of the sentence to be translated.</S>
			<S sid="159" ssid="4">As a result, these acceptors have to be pruned heavily for use in translation.</S>
			<S sid="160" ssid="5">In contrast, ourmodels of local phrase movement do not grow explosively and do not require any pruning or approx imation in their construction.</S>
			<S sid="161" ssid="6">In other related work,Bangalore and Ricardi (2001) have trained WF STs for modeling reordering within translation; their WFST parses word sequences into trees containing reordering information, which are then checked for well-formed brackets.</S>
			<S sid="162" ssid="7">Unlike this approach, our model formulation does not use a tree representation and also ensures that the output sequences are validpermutations of input phrase sequences; we empha size again that the probability distribution induced over reordered phrase sequences is not degenerate.Our reordering models do resemble those of (Till mann, 2004; Tillmann and Zhang, 2005) in that we 167 treat the reordering as a sequence of jumps relativeto the original phrase sequence, and that the likelihood of the reordering is assigned through phrase pair specific parameterized models.</S>
			<S sid="163" ssid="8">We note thatour implementation allows phrase reordering beyond simply a 1-phrase window, as was done by Till mann.</S>
			<S sid="164" ssid="9">More importantly, our model implements a generative model of phrase reordering which can be incorporated directly into a generative model of theoverall translation process.</S>
			<S sid="165" ssid="10">This allows us to per form ?embedded?</S>
			<S sid="166" ssid="11">EM-style parameter estimation, in which the parameters of the phrase reordering model are estimated using statistics gathered under the complete model that will actually be used in translation.</S>
			<S sid="167" ssid="12">We believe that this estimation of model parameters directly from phrase alignments obtainedunder the phrase translation model is a novel contri bution; prior approaches derived the parameters of the reordering models from word aligned bitext, e.g. within the phrase pair extraction procedure.We have shown that these models yield improve ments in alignment and translation performance on Arabic-English and Chinese-English tasks, and that the reordering model can be integrated into largeevaluation systems.</S>
			<S sid="168" ssid="13">Our experiments show that discriminative training procedures such Minimum Er ror Training also yield additive improvements by tuning TTM systems which incorporate ML-trained reordering models.</S>
			<S sid="169" ssid="14">This is essential for integrating our reordering model inside an evaluation system,where a variety of techniques are applied simultane ously.The MJ-1 and MJ-2 models are extremely simple models of phrase reordering.</S>
			<S sid="170" ssid="15">Despite their sim plicity, these models provide large improvements in BLEU score when incorporated into a monotone phrase order translation system.</S>
			<S sid="171" ssid="16">Moreover, they can be used to produced translation lattices for use by more sophisticated reordering models that allow longer phrase order movement.</S>
			<S sid="172" ssid="17">Future work will build on these simple structures to produce more powerful models of word and phrase movement in translation.</S>
	</SECTION>
</PAPER>
