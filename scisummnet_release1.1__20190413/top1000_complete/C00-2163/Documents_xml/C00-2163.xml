<PAPER>
  <S sid="0" ssid="0">A Compar i son  of  A l ignment  Mode ls  for S ta t i s t i ca l  Mach ine Trans la t ion Franz Josef Och and Hermann Ney Lehrstuhl fiir Informatik VI, Comlmter Science Department RWTH Aachen - University of Technology D-52056 Aachen, Germany {och, ney}~inf ormat ik.</S>
  <S sid="1" ssid="1">ruth-aachen, de Abst ract In this paper, we t)resent and compare various align- nmnt models for statistical machine translation.</S>
  <S sid="2" ssid="2">We propose to measure tile quality of an aligmnent model using the quality of the Viterbi alignment comt)ared to a manually-produced alignment and de- scribe a refined mmotation scheme to produce suit- able reference alignments.</S>
  <S sid="3" ssid="3">We also con,pare the im- pact of different; alignment models on tile translation quality of a statistical machine translation system.</S>
  <S sid="4" ssid="4">1 I n t roduct ion In statistical machine translation (SMT) it is neces- smy to model the translation probability P r ( f l  a Ic~).</S>
  <S sid="5" ssid="5">Here .fi = f denotes tile (15ench) source and e{ = e denotes the (English) target string.</S>
  <S sid="6" ssid="6">Most SMT models (Brown et al., 1993; Vogel et al., 1996) try to model word-to-word corresl)ondences between source and target words using an alignment nmpl)ing from source l)osition j to target position i = aj.</S>
  <S sid="7" ssid="7">We can rewrite tim t)robal)ility Pr(fille~) t) 3, in- troducing the hidden alignments ai 1 := al ...aj...a.l (aj C {0 , .</S>
  <S sid="8" ssid="8">, /} ) : Pr(f~lel) = ~Pr(f i  ,a~le{) .1 ?</S>
  <S sid="9" ssid="9">j -1  I~ = E H Pr(fj ajlf i -"al e l ) q, j=l To allow fbr French words wlfich do not directly cor- respond to any English word an artificial empty word c0 is added to the target sentence at position i=0.</S>
  <S sid="10" ssid="10">The different alignment models we present pro- vide different decoInt)ositions of Pr(f~,a~le().</S>
  <S sid="11" ssid="11">An alignnlent 5~ for which holds a~ = argmax Pr(fi , al[eI) at for a specific model is called V i terb i  al ignment of" this model.</S>
  <S sid="12" ssid="12">In this paper we will describe extensions to tile Hidden-Markov alignment model froln (Vogel et al., 1.996) and compare tlmse to Models 1 - 4 of (Brown et al., 1993).</S>
  <S sid="13" ssid="13">We t)roI)ose to measure the quality of an alignment nlodel using the quality of tlle Viterbi alignment compared to a manually-produced align- ment.</S>
  <S sid="14" ssid="14">This has the advantage that once having pro- duced a reference alignlnent, the evaluation itself can be performed automatically.</S>
  <S sid="15" ssid="15">In addition, it results in a very precise and relia.ble valuation criterion which is well suited to assess various design decisions in modeling and training of statistical alignment mod- els.</S>
  <S sid="16" ssid="16">It, is well known that manually pertbrming a word aligmnent is a COlnplicated and ambiguous task (Melamed, 1998).</S>
  <S sid="17" ssid="17">Therefore, to produce tlle refer- ence alignment we use a relined annotation scheme which reduces the complications and mnbiguities oc- curring in the immual construction of a word align- ment.</S>
  <S sid="18" ssid="18">As we use tile alignment models for machine translation purposes, we also evahlate the resulting translation quality of different nlodels.</S>
  <S sid="19" ssid="19">2 Al ignment  w i th  HMM In the Hidden-Markov alignment model we assume a first-order dependence for tim aligmnents aj and that the translation probability depends Olfly on aj and not  Oil (tj_l: - ~- el) =p(ajl.</S>
  <S sid="20" ssid="20">j-,,Z)p(J~l%) Pr(fj,(glf~ ,% , Later, we will describe a refinement with a depen- dence on e,,j_, iu the alignment model.</S>
  <S sid="21" ssid="21">Putting everything together, we have the following basic HMM-based modeh .1 *(flJl~I) = ~ I I  [~,(-jla~.-,, z).</S>
  <S sid="22" ssid="22">p(fj l%)] (1) at j= l with the alignment I)robability p(ili,I ) and the translation probability p(fle).</S>
  <S sid="23" ssid="23">To find a Viterbi aligninent for the HMM-based model we resort to dynamic progralnming (Vogel et al., 1996).</S>
  <S sid="24" ssid="24">The training of tlm HMM is done by the EM- algorithm.</S>
  <S sid="25" ssid="25">In the E-step the lexical and alignment 1086 counts for one sentenee-i)air (f, e) are calculated: c(flc; f, e) = E P"(a l f   e) ~ 5(f, f~)5(e, c~) a i,j ,.</S>
  <S sid="26" ssid="26">:(ill, z; f, e) = E /  ,  (a i r ,  e) aj) a j In the M-step the lexicon and translation probabili- ties are: p(f le) o&lt; ~-~c(fle;f(~),e (~)) 8 P( i l i  , I )  o (Ec ( i l i  , I ; fO) ,e (~) ) 8 To avoid the smlunation ov(;r all possible aligmnents a, (Vogel et el., 1996) use the maximum apllroxima- tion where only the Viterbi alignlnent )ath is used to collect counts.</S>
  <S sid="27" ssid="27">We used the Baron-Welch-algorithm (Baum, 1972) to train the model parameters in out ext)eriments.</S>
  <S sid="28" ssid="28">Theret/y it is possible to t)erti)rm an efl-iciellt training using; all aligmnents.</S>
  <S sid="29" ssid="29">To make the alignlnenl; t)arameters indo,1)en(lent tronl absolute word i)ositions we assmne that the alignment i)robabilities p(i[i, I )  (lel)end only Oil the jmnp width (i - i).</S>
  <S sid="30" ssid="30">Using a set of non-negative t)arameters {c(i - i  )} ,  we can write the alignment probabilities ill the fl)rm: ~(i - i) (2) p(i l i  ,  I)  = c(,,:" - i  ) This form ensures that for eadl word posilion it, i = 1, ..., I , the aligmnent probat)ilities atis(y th(, normalization constraint.</S>
  <S sid="31" ssid="31">Extension:  refined a l igmnent mode l The count table e(i - i) has only 2.1  ......... - 1 en- tries.</S>
  <S sid="32" ssid="32">This might be suitable for small corpora, but fi)r large corpora it is possil)le to make a more re- fine(1 model of Pr (a j  ~i-I  i - I  Ji ,% ,c~).</S>
  <S sid="33" ssid="33">Est)ecially, we analyzed the effect of a det)endence on c,b_ ~ or .fj.</S>
  <S sid="34" ssid="34">As a dependence on all English words wouht result ill a huge mmflmr of aligmnent 1)arameters we use as (Brown et el., 1993) equivalence classes G over tlle English and the French words.</S>
  <S sid="35" ssid="35">Here G is a mallping of words to (:lasses.</S>
  <S sid="36" ssid="36">This real)ping is trained au- tonmtically using a modification of the method de- scrilled ill (Kneser and Ney, 1991.).</S>
  <S sid="37" ssid="37">We use 50 classes in our exlmriments.</S>
  <S sid="38" ssid="38">The most general form of align- ment distribution that we consider in the ItMM is p(aj - a.+_, la(%), G(f~), h - Extension:  empty  word In the original formulation of the HMM alignment model there ix no empty word which generates Fren(:h words having no directly aligned English word.</S>
  <S sid="39" ssid="39">A direct inchlsion of an eml/ty wor(t ill the HMM model by adding all c o as in (Brown et al., 1.993) is not 1)ossit)le if we want to model the j un lp distances i - i, as the I)osition i = 0 of tim emt)ty word is chosen arbitrarily.</S>
  <S sid="40" ssid="40">Therefore, to introduce the eml)ty word we extend the HMM network by I empty words ci+ 1.2I The English word ci has a co l rest)onding eml)ty word el+ I.</S>
  <S sid="41" ssid="41">The I)osition of the eml)ty word encodes the previously visited English word.</S>
  <S sid="42" ssid="42">We enforce the following constraints for the tran- sitions in the HMM network (i _&lt; I, i _&lt; I): p(i  + I l i  , I )  = pff .</S>
  <S sid="43" ssid="43">5( i , i  ) V(i + I l l   + I, I )  = J J .</S>
  <S sid="44" ssid="44">5( i , i  ) p(i l i   + I, 1) = p(iIi  ,1) The parameter pff is the 1)robability of a transition to the emt)ty word.</S>
  <S sid="45" ssid="45">In our extleriments we set pIl = 0.2.</S>
  <S sid="46" ssid="46">Smooth ing For a t)etter estimation of infrequent events we in- troduce the following smoothing of alignment )rob- abilities: 1 F(a j I~ j - , ,~)  = ~" ~- + (1 - , , ) .p (a j la j _ l  , I ) in our exlleriments we use (t = 0.4.</S>
  <S sid="47" ssid="47">3 Mode l  1 and  Mode l  2 l~cl)lacing the (l(~,t)endence on aj - l  in the HMM alignment mo(M I)y a del)endence on j, we olltain a model wlfich (:an lie seen as a zero-order Hid(l(m- Markov Model which is similar to Model 2 1)rot)ose(t t/y (Brown et al., 1993).</S>
  <S sid="48" ssid="48">Assmning a mfiform align- ment prol)ability p(i l j ,  I )  = 1/1, we obtain Model 1.</S>
  <S sid="49" ssid="49">Assuming that the dominating factor in the align- ment model of Model 2 is the distance relative to the diagonal line of the (j, i) plane the too(tel p(i l j  , I)  can 1)e structured as tbllows (Vogel et al., 1996): ,(i -, - (3) v(ilj, 5 = Ei,=t r (  i   l This model will be referred to as diagonal-oriented Model 2.</S>
  <S sid="50" ssid="50">4 Mode l  3 and  Mode l  4 Model:  The fertility models of (Brown et el., 1993) explicitly model the probability l,(?lc) that the En- glish word c~ is aligned to 4,, = E J ]~rench words.</S>
  <S sid="51" ssid="51">1087 Model 3 of (Brown et al., 1993) is a zero-order alignment model like Model 2 including in addi- tion fertility paranmters.</S>
  <S sid="52" ssid="52">Model 4 of (Brown et al., 1993) is also a first-order alignment model (along the source positions) like the HMM, trot includes also fertilities.</S>
  <S sid="53" ssid="53">In Model 4 the alignment position j of an English word depends on the alignment po- sition of tile previous English word (with non-zero fertility) j  .</S>
  <S sid="54" ssid="54">It models a jump distance j - j   (for con- secutive English words) while in the HMM a jump distance i - i   (for consecutive French words) is mod- eled.</S>
  <S sid="55" ssid="55">Tile full description of Model 4 (Brown et al., 1993) is rather complica.ted as there have to be con- sidered tile cases that English words have fertility larger than one and that English words have fertil- ity zero.</S>
  <S sid="56" ssid="56">For training of Model 3 and Model 4, we use an extension of the program GlZA (A1-Onaizan et al., 1999).</S>
  <S sid="57" ssid="57">Since there is no efficient way in these mod- els to avoid tile explicit summation over all align- ments in the EM-algorithin, the counts are collected only over a subset of promising alignments.</S>
  <S sid="58" ssid="58">It is not known an efficient algorithm to compute the Viterbi alignment for the Models 3 and 4.</S>
  <S sid="59" ssid="59">Therefore, the Viterbi alignment is comlmted only approximately using the method described in (Brown et al., 1993).</S>
  <S sid="60" ssid="60">The models 1-4 are trained in succession with the tinal parameter values of one model serving as the starting point tbr the next.</S>
  <S sid="61" ssid="61">A special problein in Model 3 and Model 4 con- cerns the deficiency of tile model.</S>
  <S sid="62" ssid="62">This results in problems in re-estimation of the parameter which describes the fertility of the empty word.</S>
  <S sid="63" ssid="63">In nor- real EM-training, this parameter is steadily decreas- ing, producing too many aligmnents with tile empty word.</S>
  <S sid="64" ssid="64">Therefore we set tile prot)ability for aligning a source word with tile emt)ty word at a suitably chosen constant value.</S>
  <S sid="65" ssid="65">As in tile HMM we easily can extend the depen- dencies in the alignment model of Model 4 easily using the word class of the previous English word E = G(ci,), or the word class of the French word F = G(I j)  (Brown et al., 1993).</S>
  <S sid="66" ssid="66">5 Inc lud ing  a Manual Dictionary We propose here a simple method to make use of a bilingual dictionary as an additional knowledge source in the training process by extending the train- ing corpus with the dictionary entries.</S>
  <S sid="67" ssid="67">Thereby, the dictionary is used already in EM-training and can improve not only the alignment fox" words which are in the dictionary but indirectly also for other words.</S>
  <S sid="68" ssid="68">The additional sentences in the training cortms are weighted with a factor Fl~x during the EM-training of the lexicon probabilities.</S>
  <S sid="69" ssid="69">We assign tile dictionary entries which really co- occur in the training corpus a high weight Fle.~.</S>
  <S sid="70" ssid="70">and the remaining entries a vexy low weight.</S>
  <S sid="71" ssid="71">In our ex- periments we use Flex = 10 for the co-occurring dic- tionary entries which is equivalent to adding every dictionary entry ten times to the training cortms.</S>
  <S sid="72" ssid="72">6 The Al ignment Template  System The statistical machine-translation method descri- bed in (Och et al., 1999) is based on a word aligned traiifing corIms and thereby makes use of single- word based alignment models.</S>
  <S sid="73" ssid="73">Tile key element of tiffs apt/roach are the alignment emplates which are pairs of phrases together with an alignment between the words within tile phrases.</S>
  <S sid="74" ssid="74">The advantage of the alignment emplate approach over word based statistical translation models is that word context and local re-orderings are explicitly taken into ac- count.</S>
  <S sid="75" ssid="75">We typically observe that this approach pro- duces better translations than the single-word based models.</S>
  <S sid="76" ssid="76">The alignment templates are automatically trailmd using a parallel trailxing corlms.</S>
  <S sid="77" ssid="77">For more information about the alignment template approach see (Och et at., 1999).</S>
  <S sid="78" ssid="78">7 Resu l ts We present results on the Verbmobil Task which is a speech translation task ill the donmin of appoint- nxent scheduling, travel planning, and hotel reserva- tion (Wahlster, 1993).</S>
  <S sid="79" ssid="79">We measure the quality of tile al)ove inentioned aligmnent models with xespect to alignment quality and translation quality.</S>
  <S sid="80" ssid="80">To obtain a refereuce aligmnent for evaluating alignlnent quality, we manually aligned about 1.4 percent of onr training corpus.</S>
  <S sid="81" ssid="81">We allowed the hu- mans who pertbrmed the alignment o specify two different kinds of alignments: an S (sure) a, lignment which is used for alignmelxts which are unambigu- ously and a P (possible) alignment which is used for alignments which might or might not exist.</S>
  <S sid="82" ssid="82">The P relation is used especially to align words within idiomatic expressions, free translations, and missing function words.</S>
  <S sid="83" ssid="83">It is guaranteed that S C P. Figure 1 shows all example of a manually aligned sentence with S and P relations.</S>
  <S sid="84" ssid="84">The hunxan-annotated align- ment does not prefer rely translation direction and lnay therefore contain many-to-one and one-to-many relationships.</S>
  <S sid="85" ssid="85">The mmotation has been performed by two annotators, producing sets $1, 1~, S2, P2.</S>
  <S sid="86" ssid="86">Tile reference aliglunent is produced by forming the intersection of the sure aligmnents (S = $1 rqS2) and the ration of the possible atignumnts (P = P1 U P2).</S>
  <S sid="87" ssid="87">Tim quality of an alignment A = { (j, aj) } is mea- sured using the following alignment error rate: AER(S, P; A) = 1 - IA o Sl + IA o Pl IAI + ISl 1088 that  .</S>
  <S sid="88" ssid="88">[---l  [ - "~ " .</S>
  <S sid="89" ssid="89">C l l -1  " e .</S>
  <S sid="90" ssid="90">-~t ~1 J~ o Figure i: Exmnple of a manually annotated align- ment with sure (filled dots) and possible commotions.</S>
  <S sid="91" ssid="91">Obviously, if we colnpare the sure alignnlents of ev- ery sitigle annotator with the reference a.ligmnent we obtain an AEI{ of zero percent.</S>
  <S sid="92" ssid="92">~[ifl)le l.: Cort)us characteristics for alignment quality experiments.</S>
  <S sid="93" ssid="93">Train Sente iH : ( i s Words Vocalmlary Dictionary Entries Words Test Sentences Words German I English 34 446 329 625 / 343 076 5 936 ] 3 505 4 183 4 533 I 5 324 354 3 109 I 3 233 Tal)le 1 shows the characteristics of training and test corlms used in the alignment quality ext)eri- inents.</S>
  <S sid="94" ssid="94">The test cortms for these ext)eriments (not for the translation exl)eriments) is 1)art of the train- ing corpus.</S>
  <S sid="95" ssid="95">Table 2 shows the aligmnent quality of different alignment models.</S>
  <S sid="96" ssid="96">Here the alignment models of IIMM and Model 4 do not include a dependence on word classes.</S>
  <S sid="97" ssid="97">We conclude that more sophisti- cated alignment lnodels are crtlcial tbr good align- ment quality.</S>
  <S sid="98" ssid="98">Consistently, the use of a first-order aligmnent model, modeling an elnpty word and fer- tilities result in better alignments.</S>
  <S sid="99" ssid="99">Interestingly, the siinl)ler HMM aligninent model outt)erforms Model 3 which shows the importance of first-order align- ment models.</S>
  <S sid="100" ssid="100">The best t)erformanee is achieved with Model 4.</S>
  <S sid="101" ssid="101">The improvement by using a dictio- nary is small eomI)ared to the effect of using 1)etter a.lignmellt models.</S>
  <S sid="102" ssid="102">We see a significant dillerence in alignment quality if we exchange source and tar- get languages.</S>
  <S sid="103" ssid="103">This is due to the restriction in all alignment models that a source language word can 1)e aligned to at most one target language word.</S>
  <S sid="104" ssid="104">If German is source language the trequelltly occurring German word coml)ounds, camlot be aligned cor- rectly, as they typically correspond to two or more English words.</S>
  <S sid="105" ssid="105">WaNe 3 shows the effect of including a det)endence on word classes in the aligmnent model of ItMM or Model 4.</S>
  <S sid="106" ssid="106">By using word classes the results can be Table 3: Eftcot of including a det)endence on word classes in the aligmnent model.</S>
  <S sid="107" ssid="107">AER [%] Det)endencies -IIMM I Model 4 no 8.0 6.5 source 7.5 6.0 target 7.1 6.1 source ?</S>
  <S sid="108" ssid="108">target 7.6 6.1 improved by 0.9% when using the ItMM and by 0.5% when using Model 4.</S>
  <S sid="109" ssid="109">For the translation experiments we used a differ- ent training and an illdetmndent test corpus (Table 4).</S>
  <S sid="110" ssid="110">Table 4: Corlms characteristics for translation (tual- it;.</S>
  <S sid="111" ssid="111">), exlmriments.</S>
  <S sid="112" ssid="112">Train S ~e,t Sentences Words Vocabulary Se l l te l lees Words PP (trigram LM) I German English 58332 519523 549921 7 940 4 673 147 1968 2173 (40.3) 28.8 For tile evMuation of the translation quality we used the automatically comlmtable Word Error Rate (WEll.)</S>
  <S sid="113" ssid="113">and the Subjective Sentence Error Rate (SSEll,) (Niefien et al., 2000).</S>
  <S sid="114" ssid="114">The WEll, corre- spomls to the edit distance t)etween the produced translation and one t)redefined reference translation.</S>
  <S sid="115" ssid="115">To obtain the SSER the translations are classified by human experts into a small number of quality classes ranging from "l)ertbet" to "at)solutely wrong".</S>
  <S sid="116" ssid="116">In comparison to the WEll,, this criterion is more mean- ingflfl, but it is also very exl)ensive to measure.</S>
  <S sid="117" ssid="117">The translations are produced by the aligmnent template system mentioned in the previous ection.</S>
  <S sid="118" ssid="118">1089 Table 2: Alignment error rate (AER [%]) of ditl~rent alignment models tbr the translations directions English into German (German words have fertilities) and German into English.</S>
  <S sid="119" ssid="119">English -+ German German -~ English Dictionary no yes no yes Empty Word no lYes yes no l yes yes Model 1 17.8 16.9 16.0 22.9 21.7 20.3 Model 2 12.8 12.5 11.7 17.5 17.1 15.7 Model 2(diag) 11.8 10.5 9.8 16.4 15.1 13.3 Mode l  3 10.5 9.3 8.5 15.7 14.5 12.1 HMM 10.5 9.2 8.0 14.1 12.9 11.5 Model 4 9.0 7.8 6.5 14.0 12.5 10.8 Table 5: Effect of different alignment models on translation quality.</S>
  <S sid="120" ssid="120">Alignlnent Model in Training WER[%] SSER[%] Model 1 49.8 22.2 HMM 47.7 19.3 Model 4 48.6 16.8 The results are shown in Table 5.</S>
  <S sid="121" ssid="121">We see a clear improvement in translation quality as measured by SSER whereas WER is inore or less the same for all models.</S>
  <S sid="122" ssid="122">The imwovement is due to better lexicons and better alignment templates extracted from the resulting aliglunents.</S>
  <S sid="123" ssid="123">8 Conclusion We have evaluated vmious statistical alignment models by conlparing the Viterbi alignment of the model with a human-made alignment.</S>
  <S sid="124" ssid="124">We have shown that by using inore sophisticated models the quality of the alignments improves ignificantly.</S>
  <S sid="125" ssid="125">Fur- ther improvements in producing better alignments are expected from using the HMM alignment model to bootstrap the fertility models, fronl making use of cognates, and from statistical lignment models that are based on word groups rather than single words.</S>
  <S sid="126" ssid="126">Acknowledgment This article has been partially supported as part of the Verbmobil project (contract nmnber 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology.</S>
  <S sid="127" ssid="127">References Y. A1-Onaizan, J. Cur]n, M. Jahr, K. Knight, J. Laf- ferty, I. D. Melamed, F. a. Och, D. Purdy, N. A. Smith, and D. Yarowsky.</S>
  <S sid="128" ssid="128">Statistical ina- chine translation, final report, JHU workshop.</S>
  <S sid="129" ssid="129">edu/ws99/proj ects/mt/ f inal_report/mr- f inal-report, ps.</S>
  <S sid="130" ssid="130">An Inequality and Associated Maximization Technique in Statistical Estimation for Probabilistie Functions of Markov Processes.</S>
  <S sid="131" ssid="131">Inequalities, 3:1 8.</S>
  <S sid="132" ssid="132">P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer.</S>
  <S sid="133" ssid="133">The mathenlatics ofsta- tistical machine trmlslation: Parameter estima- tion.</S>
  <S sid="134" ssid="134">Computational Linguistics, 19(2):263-311.</S>
  <S sid="135" ssid="135">R. Kneser and H. Ney.</S>
  <S sid="136" ssid="136">Forming Word Classes by Statistical Clustering for Statistical Langm~ge Modelling.</S>
  <S sid="137" ssid="137">Quantitative Linguistics Conf.</S>
  <S sid="138" ssid="138">Manual mmotation of transla- tional equivalence: The Blinker project.</S>
  <S sid="139" ssid="139">Technical Report 98-07, IRCS.</S>
  <S sid="140" ssid="140">S. Niegen, F. J.</S>
  <S sid="141" ssid="141">()ch, G. Leusch, and H. Ney.</S>
  <S sid="142" ssid="142">An evaluation tool ]or machine translation: Fast evaluation for mt research.</S>
  <S sid="143" ssid="143">In Proceedings of the Second International Conference on Language Resources and Evaluation, pages 39-45, Athens, Greece, May June.</S>
  <S sid="144" ssid="144">F. J. Och, C. Tilhnalm, mid H. Ney.</S>
  <S sid="145" ssid="145">Improved alignment models for statistical machine transla- tion.</S>
  <S sid="146" ssid="146">of the Joint SIGDAT Co~?</S>
  <S sid="147" ssid="147">on Empirical Methods in Natural Language Process- ing and Very LaTye Corpora, pages 20-28, Univer- sity of Marylmld, College Park, MD, USA, June.</S>
  <S sid="148" ssid="148">S. Vogel, H. Ney, and C. Tilhnann.</S>
  <S sid="149" ssid="149">HMM- based word alignment in statistical translation.</S>
  <S sid="150" ssid="150">In COLING 96: The 16th Int.</S>
  <S sid="151" ssid="151">on Compu- tational Linguistics, pages 836-841, Copenhagen, August.</S>
  <S sid="152" ssid="152">Verbmobil: Translation of face- to-face dialogs.</S>
  <S sid="153" ssid="153">of the MT Summit IV, pages 127-135, Kobe, Jat)an.</S>
</PAPER>
