<PAPER>
	<S sid="0">A Topic Model for Word Sense Disambiguation</S><ABSTRACT>
		<S sid="1" ssid="1">We develop latent Dirichlet alocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable.</S>
		<S sid="2" ssid="2">We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpusand learning the domains in which to consider each word.</S>
		<S sid="3" ssid="3">Using the WORDNET hierarchy, we embed the construction of Ab ney and Light (1999) in the topic model and show that automatically learned domainsimprove WSD accuracy compared to alter native contexts.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="4" ssid="4">Word sense disambiguation (WSD) is the task of determining the meaning of an ambiguous word in its context.</S>
			<S sid="5" ssid="5">It is an important problem in natural language processing (NLP) because effective WSD can improve systems for tasks such as information retrieval, machine translation, and summarization.In this paper, we develop latent Dirichlet alocation with WORDNET (LDAWN), a generative prob abilistic topic model for WSD where the sense of the word is a hidden random variable that is inferred from data.</S>
			<S sid="6" ssid="6">There are two central advantages to this approach.First, with LDAWN we automatically learn the con text in which a word is disambiguated.</S>
			<S sid="7" ssid="7">Rather than disambiguating at the sentence-level or the document-level, our model uses the other words that share the same hidden topic across many documents.</S>
			<S sid="8" ssid="8">Second, LDAWN is a fully-fledged generative model.</S>
			<S sid="9" ssid="9">Generative models are modular and can beeasily combined and composed to form more complicated models.</S>
			<S sid="10" ssid="10">(As a canonical example, the ubiq uitous hidden Markov model is a series of mixturemodels chained together.)</S>
			<S sid="11" ssid="11">Thus, developing a gen erative model for WSD gives other generative NLP algorithms a natural way to take advantage of the hidden senses of words.</S>
			<S sid="12" ssid="12">In general, topic models are statistical models of text that posit a hidden space of topics in which the corpus is embedded (Blei et al, 2003).</S>
			<S sid="13" ssid="13">Given a corpus, posterior inference in topic models amounts to automatically discovering the underlying themesthat permeate the collection.</S>
			<S sid="14" ssid="14">Topic models have re cently been applied to information retrieval (Wei and Croft, 2006), text classification (Blei et al, 2003), and dialogue segmentation (Purver et al, 2006).</S>
			<S sid="15" ssid="15">While topic models capture the polysemous use of words, they do not carry the explicit notion of sense that is necessary for WSD.</S>
			<S sid="16" ssid="16">LDAWN extends the topic modeling framework to include a hidden meaning in the word generation process.</S>
			<S sid="17" ssid="17">In this case, posterior inference discovers both the topics of the corpus and the meanings assigned to each of its words.</S>
			<S sid="18" ssid="18">After introducing a disambiguation scheme basedon probabilistic walks over the WORDNET hierar chy (Section 2), we embed the WORDNET-WALK in a topic model, where each topic is associated withwalks that prefer different neighborhoods of WORDNET (Section 2.1).</S>
			<S sid="19" ssid="19">Then, we describe a Gibbs sam pling algorithm for approximate posterior inference that learns the senses and topics that best explain a corpus (Section 3).</S>
			<S sid="20" ssid="20">Finally, we evaluate our system on real-world WSD data, discuss the properties of the topics and disambiguation accuracy results, and draw connections to other WSD algorithms from the research literature.</S>
			<S sid="21" ssid="21">1024 1740 entity 1930 3122object 20846 15024 animal 1304946 1305277 artifact male 2354808 2354559 foalcolt 3042424 colt 4040311 revolver Synset ID Word six-gun six-shooter 0.00 0.25 0.58 0.00 0.04 0.02 0.010.16 0.05 0.04 0.690.00 0.00 0.381.000.42 0.00 0.000.57 1.00 0.38 0.07 Figure 1: The possible paths to reach the word ?colt?</S>
			<S sid="22" ssid="22">in WORDNET.</S>
			<S sid="23" ssid="23">Dashed lines represent omitted links.</S>
			<S sid="24" ssid="24">All words in the synset containing ?revolver?</S>
			<S sid="25" ssid="25">are shown, but only one word from other synsets is shown.</S>
			<S sid="26" ssid="26">Edge labels are probabilities of transitioningfrom synset i to synset j. Note how this favors frequent terms, such as ?revolver,?</S>
			<S sid="27" ssid="27">over ones like ?six shooter.?</S>
	</SECTION>
	<SECTION title="Topic models and WordNet. " number="2">
			<S sid="28" ssid="1">The WORDNET-WALK is a probabilistic process ofword generation that is based on the hyponomy relationship in WORDNET (Miller, 1990).</S>
			<S sid="29" ssid="2">WORD NET, a lexical resource designed by psychologistsand lexicographers to mimic the semantic organiza tion in the human mind, links ?synsets?</S>
			<S sid="30" ssid="3">(short forsynonym sets) with myriad connections.</S>
			<S sid="31" ssid="4">The spe cific relation we?re interested in, hyponomy, points from general concepts to more specific ones and is sometimes called the ?is-a?</S>
			<S sid="32" ssid="5">relationship.</S>
			<S sid="33" ssid="6">As first described by Abney and Light (1999), we imagine an agent who starts at synset [entity], which points to every noun in WORDNET 2.1 by some sequence of hyponomy relations, and then chooses the next node in its random walk from the hyponyms of its current position.</S>
			<S sid="34" ssid="7">The agent repeatsthis process until it reaches a leaf node, which corre sponds to a single word (each of the synset?s words are unique leaves of a synset in our construction).For an example of all the paths that might generate the word ?colt?</S>
			<S sid="35" ssid="8">see Figure 1.</S>
			<S sid="36" ssid="9">The WORDNET WALK is parameterized by a set of distributions over children for each synset s in WORDNET, ?s. Symbol Meaning K number of topics ?k,s multinomial probability vector over the successors of synset s in topic k S scalar that, when multiplied by ?s gives the prior for ?k,s ?s normalized vector whose ith entry, when multiplied by S, gives the prior probability for going from s to i ?d multinomial probability vector over the topics that generate document d ? prior for ? z assignment of a word to a topic ? a path assignment through WORDNET ending at a word.?i,j one link in a path ? going from syn set i to synset j.Table 1: A summary of the notation used in the paper.</S>
			<S sid="37" ssid="10">Bold vectors correspond to collections of vari ables (i.e. zu refers to a topic of a single word, butz1:D are the topics assignments of words in docu ment 1 through D).</S>
			<S sid="38" ssid="11">2.1 A topic model for WSDThe WORDNET-WALK has two important proper ties.</S>
			<S sid="39" ssid="12">First, it describes a random process for word generation.</S>
			<S sid="40" ssid="13">Thus, it is a distribution over words and thus can be integrated into any generative model of text, such as topic models.</S>
			<S sid="41" ssid="14">Second, the synsetthat produces each word is a hidden random vari able.</S>
			<S sid="42" ssid="15">Given a word assumed to be generated by a WORDNET-WALK, we can use posterior inference to predict which synset produced the word.</S>
			<S sid="43" ssid="16">These properties allow us to develop LDAWN, which is a fusion of these WORDNET-WALKs and latent Dirichlet alocation (LDA) (Blei et al, 2003),a probabilistic model of documents that is an im provement to pLSI (Hofmann, 1999).</S>
			<S sid="44" ssid="17">LDA assumes that there are K ?topics,?</S>
			<S sid="45" ssid="18">multinomial distributionsover words, which describe a collection.</S>
			<S sid="46" ssid="19">Each docu ment exhibits multiple topics, and each word in each document is associated with one of them.</S>
			<S sid="47" ssid="20">Although the term ?topic?</S>
			<S sid="48" ssid="21">evokes a collection of ideas that share a common theme and although the topics derived by LDA seem to possess semantic coherence, there is no reason to believe this would 1025 be true of the most likely multinomial distributions that could have created the corpus given the assumed generative model.</S>
			<S sid="49" ssid="22">That semantically similar words are likely to occur together is a byproduct of how language is actually used.In LDAWN, we replace the multinomial topic dis tributions with a WORDNET-WALK, as described above.</S>
			<S sid="50" ssid="23">LDAWN assumes a corpus is generated bythe following process (for an overview of the nota tion used in this paper, see Table 1).</S>
			<S sid="51" ssid="24">1.</S>
			<S sid="52" ssid="25">For each topic, k ? {1, . . .</S>
			<S sid="53" ssid="26">,K}.</S>
			<S sid="54" ssid="27">(a) For each synset s, randomly choose transition prob abilities ?k,s ? Dir(S?s).</S>
			<S sid="55" ssid="28">2.</S>
			<S sid="56" ssid="29">For each document d ? {1, . . .</S>
			<S sid="57" ssid="30">, D}.</S>
			<S sid="58" ssid="31">(a) Select a topic distribution ?d ? Dir(?)</S>
			<S sid="59" ssid="32">(b) For each word n ? {1, . . .</S>
			<S sid="60" ssid="33">, Nd} i. Select a topic z ? Mult(1, ?d) ii.</S>
			<S sid="61" ssid="34">Create a path ?d,n starting with ?0 as the root node.</S>
			<S sid="62" ssid="35">iii.</S>
			<S sid="63" ssid="36">From children of ?i: A. Choose the next node in the walk ?i+1 ? Mult(1, ?z,?i)B. If ?i+1 is a leaf node, generate the associ ated word.</S>
			<S sid="64" ssid="37">Otherwise, repeat.</S>
			<S sid="65" ssid="38">Every element of this process, including thesynsets, is hidden except for the words of the doc uments.</S>
			<S sid="66" ssid="39">Thus, given a collection of documents, our goal is to perform posterior inference, which is the task of determining the conditional distribution of the hidden variables given the observations.</S>
			<S sid="67" ssid="40">In thecase of LDAWN, the hidden variables are the parameters of the K WORDNET-WALKs, the topic assign ments of each word in the collection, and the synset path of each word.</S>
			<S sid="68" ssid="41">In a sense, posterior inference reverses the process described above.</S>
			<S sid="69" ssid="42">Specifically, given a document collection w1:D, the full posterior is p(?1:K ,z1:D,?1:D,?1:D |w1:D, ?, S?)</S>
			<S sid="70" ssid="43">?(?K k=1 p(?k |S?) ?D d=1 p(?d | ?) ?Nd n=1 p(?d,n |?1:K)p(wd,n |?d,n) ) , (1) where the constant of proportionality is the marginal likelihood of the observed data.Note that by encoding the synset paths as a hid den variable, we have posed the WSD problem asa question of posterior probabilistic inference.</S>
			<S sid="71" ssid="44">Fur ther note that we have developed an unsupervised model.</S>
			<S sid="72" ssid="45">No labeled data is needed to disambiguate a corpus.</S>
			<S sid="73" ssid="46">Learning the posterior distribution amounts to simultaneously decomposing a corpus into topics and its words into their synsets.</S>
			<S sid="74" ssid="47">The intuition behind LDAWN is that the words in a topic will have similar meanings and thus share paths within WORDNET.</S>
			<S sid="75" ssid="48">For example, WORDNET has two senses for the word ?colt;?</S>
			<S sid="76" ssid="49">one referring to a young male horse and the other to a type of handgun (see Figure 1).Although we have no a priori way of know ing which of the two paths to favor for a document, we assume that similar concepts will also appear in the document.</S>
			<S sid="77" ssid="50">Documents with unambiguous nouns such as ?six-shooter?</S>
			<S sid="78" ssid="51">and ?smoothbore?</S>
			<S sid="79" ssid="52">would make paths that pass through the synset [firearm, piece,small-arm] more likely than those go ing through [animal, animate being, beast, brute, creature, fauna].</S>
			<S sid="80" ssid="53">In practice, we hope to see a WORDNET-WALK that looks like Figure 2, which points to the right sense of cancer for a medical context.LDAWN is a Bayesian framework, as each vari able has a prior distribution.</S>
			<S sid="81" ssid="54">In particular, the Dirichlet prior for ?s, specified by a scaling factor S and a normalized vector ?s fulfills two functions.</S>
			<S sid="82" ssid="55">First, as the overall strength of S increases, we place a greater emphasis on the prior.</S>
			<S sid="83" ssid="56">This is equivalent to the need for balancing as noted by Abney and Light (1999).</S>
			<S sid="84" ssid="57">The other function that the Dirichlet prior serves is to enable us to encode any information we have about how we suspect the transitions to childrennodes will be distributed.</S>
			<S sid="85" ssid="58">For instance, we might ex pect that the words associated with a synset will beproduced in a way roughly similar to the token prob ability in a corpus.</S>
			<S sid="86" ssid="59">For example, even though ?meal?</S>
			<S sid="87" ssid="60">might refer to both ground cereals or food eaten ata single sitting and ?repast?</S>
			<S sid="88" ssid="61">exclusively to the lat ter, the synset [meal, repast, food eatenat a single sitting] still prefers to transi tion to ?meal?</S>
			<S sid="89" ssid="62">over ?repast?</S>
			<S sid="90" ssid="63">given the overall corpus counts (see Figure 1, which shows prior transition probabilities for ?revolver?).By setting ?s,i, the prior probability of transitioning from synset s to node i, proportional to the to tal number of observed tokens in the children of i, 1026we introduce a probabilistic variation on information content (Resnik, 1995).</S>
			<S sid="91" ssid="64">As in Resnik?s defini tion, this value for non-word nodes is equal to thesum of all the frequencies of hyponym words.</S>
			<S sid="92" ssid="65">Un like Resnik, we do not divide frequency among all senses of a word; each sense of a word contributes its full frequency to ?.</S>
	</SECTION>
	<SECTION title="Posterior Inference with Gibbs Sampling. " number="3">
			<S sid="93" ssid="1">As described above, the problem of WSD corresponds to posterior inference: determining the probability distribution of the hidden variables given ob served words and then selecting the synsets of themost likely paths as the correct sense.</S>
			<S sid="94" ssid="2">Directly com puting this posterior distribution, however, is not tractable because of the difficulty of calculating the normalizing constant in Equation 1.To approximate the posterior, we use Gibbs sampling, which has proven to be a successful approx imate inference technique for LDA (Griffiths and Steyvers, 2004).</S>
			<S sid="95" ssid="3">In Gibbs sampling, like all Markov chain Monte Carlo methods, we repeatedly sample from aMarkov chain whose stationary distribution is the posterior of interest (Robert and Casella, 2004).</S>
			<S sid="96" ssid="4">Even though we don?t know the full posterior, the samples can be used to form an empirical estimate of the target distribution.</S>
			<S sid="97" ssid="5">In LDAWN, the samples contain a configuration of the latent semantic states of the system, revealing the hidden topics and paths that likely led to the observed data.Gibbs sampling reproduces the posterior distri bution by repeatedly sampling each hidden variable conditioned on the current state of the other hidden variables and observations.</S>
			<S sid="98" ssid="6">More precisely, the state is given by a set of assignments where each wordis assigned to a path through one of K WORDNET WALK topics: uth word wu has a topic assignment zu and a path assignment ?u. We use z?u and ??u to represent the topic and path assignments of all words except for u, respectively.</S>
			<S sid="99" ssid="7">Sampling a new topic for the word wu requires us to consider all of the paths that wu can take in each topic and the topics of the other words in the document u is in.</S>
			<S sid="100" ssid="8">The probability of wu taking on topic i is proportional to p(zu = i |z?u) ? ?</S>
			<S sid="101" ssid="9">p(?</S>
			<S sid="102" ssid="10">|??u)1[wu ? ?], (2) which is the probability of selecting z from ?d times the probability of a path generating wu from a path in the ith WORDNET-WALK.</S>
			<S sid="103" ssid="11">The first term, the topic probability of the uth word, is based on the assignments to the K topics for words other than u in this document, p(zu = i|z?u) = n(d)?u,i + ?i ? j n (d) ?u,j + ?K j=1 ?j , (3) where n(d)?u,j is the number of words other than u in topic j for the document d that u appears in.</S>
			<S sid="104" ssid="12">The second term in Equation 2 is a sum over the probabilities of every path that could have generatedthe word wu.</S>
			<S sid="105" ssid="13">In practice, this sum can be com puted using a dynamic program for all nodes that have unique parent (i.e. those that can?t be reached by more than one path).</S>
			<S sid="106" ssid="14">Although the probability ofa path is specific to the topic, as the transition prob abilities for a synset are different across topics, we will omit the topic index in the equation, p(?u = ?|??u, ) = ?l?1 i=1 ? ?u ?i,?i+1 .</S>
			<S sid="107" ssid="15">(4) 3.1 Transition Probabilities.</S>
			<S sid="108" ssid="16">Computing the probability of a path requires us to take a product over our estimate of the probability from transitioning from i to j for all nodes i and j in the path ?.</S>
			<S sid="109" ssid="17">The other path assignments within this topic, however, play an important role in shaping the transition probabilities.</S>
			<S sid="110" ssid="18">From the perspective of a single node i, only paths that pass through that node affect the probability of u also passing through that node.</S>
			<S sid="111" ssid="19">It?s convenient tohave an explicit count of all of the paths that tran sition from i to j in this topic?s WORDNET-WALK, so we use T?ui,j to represent all of the paths that go from i to j in a topic other than the path currently assigned to u. Given the assignment of all other words to paths, calculating the probability of transitioning from i to j with word u requires us to consider the prior ? and the observations Ti,j in our estimate of the expected value of the probability of transitioning from i to j, ??ui,j = T?ui,j + Si?i,j Si + ? k T ?u i,k .</S>
			<S sid="112" ssid="20">(5) 1027 As mentioned in Section 2.1, we paramaterize the prior for synset i as a vector ?i, which sums to one, and a scale parameter S. The next step, once we?ve selected a topic, is to select a path within that topic.</S>
			<S sid="113" ssid="21">This requires the computation of the path probabilities as specified in Equation 4 for all of the paths wu can take in thesampled topic and then sampling from the path prob abilities.</S>
			<S sid="114" ssid="22">The Gibbs sampler is essentially a randomized hill climbing algorithm on the posterior likelihood as a function of the configuration of hidden variables.</S>
			<S sid="115" ssid="23">The numerator of Equation 1 is proportional to that posterior and thus allows us to track the sampler?s progress.</S>
			<S sid="116" ssid="24">We assess convergence to a local mode of the posterior by monitoring this quantity.</S>
	</SECTION>
	<SECTION title="Experiments. " number="4">
			<S sid="117" ssid="1">In this section, we describe the properties of the topics induced by running the previously described Gibbs sampling method on corpora and how these topics improve WSD accuracy.</S>
			<S sid="118" ssid="2">Of the two data sets used during the course of our evaluation, the primary dataset was SEMCOR (Miller et al, 1993), which is a subset of the Brown corpus with many nouns manually labeled with the correct WORDNET sense.</S>
			<S sid="119" ssid="3">The words in this dataset are lemmatized, and multi-word expressions that are present in WORDNET are identified.</S>
			<S sid="120" ssid="4">Only the wordsin SEMCOR were used in the Gibbs sampling pro cedure; the synset assignments were only used for assessing the accuracy of the final predictions.</S>
			<S sid="121" ssid="5">We also used the British National Corpus, whichis not lemmatized and which does not have multi word expressions.</S>
			<S sid="122" ssid="6">The text was first run through a lemmatizer, and then sequences of words which matched a multi-word expression in WORDNET were joined together into a single word.</S>
			<S sid="123" ssid="7">We took nouns that appeared in SEMCOR twice or in theBNC at least 25 times and used the BNC to compute the information-content analog ? for individ ual nouns (For example, the probabilities in Figure 1 correspond to ?).</S>
			<S sid="124" ssid="8">4.1 Topics.</S>
			<S sid="125" ssid="9">Like the topics created by structures such as LDA, the topics in Table 2 coalesce around reasonable themes.</S>
			<S sid="126" ssid="10">The word list was compiled by summingover all of the possible leaves that could have gen erated each of the words and sorting the words by decreasing probability.</S>
			<S sid="127" ssid="11">In the vast majority of cases, a single synset?s high probability is responsible for the words?</S>
			<S sid="128" ssid="12">positions on the list.</S>
			<S sid="129" ssid="13">Reassuringly, many of the top senses for the present words correspond to the most frequent sense in SEMCOR.</S>
			<S sid="130" ssid="14">For example, in Topic 4, the senses for ?space?</S>
			<S sid="131" ssid="15">and ?function?</S>
			<S sid="132" ssid="16">correspond to the top sensesin SEMCOR, and while the top sense for ?set?</S>
			<S sid="133" ssid="17">corresponds to ?an abstract collection of numbers or symbols?</S>
			<S sid="134" ssid="18">rather than ?a group of the same kind that be long together and are so used,?</S>
			<S sid="135" ssid="19">it makes sense given the math-based words in the topic.</S>
			<S sid="136" ssid="20">?Point,?</S>
			<S sid="137" ssid="21">however, corresponds to the sense used in the phrase ?I got to the point of boiling the water,?</S>
			<S sid="138" ssid="22">which is neither the top SEMCOR sense nor a sense which makes sense given the other words in the topic.</S>
			<S sid="139" ssid="23">While the topics presented in Table 2 resemble the topics one would obtain through models likeLDA (Blei et al, 2003), they are not identical.</S>
			<S sid="140" ssid="24">Be cause of the lengthy process of Gibbs sampling, we initially thought that using LDA assignments as aninitial state would converge faster than a random initial assignment.</S>
			<S sid="141" ssid="25">While this was the case, it con verged to a state that less probable than the randomlyinitialized state and no better at sense disambigua tion (and sometimes worse).</S>
			<S sid="142" ssid="26">The topics presented in 2 represent words both that co-occur together in a corpus and co-occur on paths through WORDNET.</S>
			<S sid="143" ssid="27">Because topics created through LDA only have the first property, they usually do worse in terms of both total probability and disambiguation accuracy (see Figure 3).</S>
			<S sid="144" ssid="28">Another interesting property of topics in LDAWN is that, with higher levels of smoothing, words that don?t appear in a corpus (or appear rarely) but are in similar parts of WORDNET might have relatively high probability in a topic.</S>
			<S sid="145" ssid="29">For example, ?maturity?</S>
			<S sid="146" ssid="30">in topic two in Table 2 is sandwiched between ?foot?</S>
			<S sid="147" ssid="31">and ?center,?</S>
			<S sid="148" ssid="32">both of which occur about five timesmore than ?maturity.?</S>
			<S sid="149" ssid="33">This might improve LDA based information retrieval schemes (Wei and Croft, 2006) . 1028 1740 1930 0.23 0.76 3122 0.42 0.01 2236 0.10 0.00 0.00 0.00 0.00 7626 someone 0.00 9609711 0.00 9120316 1743824 0.00 cancer 7998922 genus0.04 0.04 8564599 star_sign 0.06 8565580 0.06 cancer 0.5 9100327 cancer 1 constellation 0.01 0.01 cancer 0.5 crab 0.5 13875408 0.58 0.19 14049094 14046733 tumor 0.97 14050958 0.00 malignancy 0.06 0.94 14051451 0.90 cancer 0.96 Synset ID Transition Prob Word 1957888 1.0 Figure 2: The possible paths to reach the word ?cancer?</S>
			<S sid="150" ssid="34">in WORDNET along with transition probabilities from the medically-themed Topic 2 in Table 2, with the most probable path highlighted.</S>
			<S sid="151" ssid="35">The dashed lines represent multiple links that have been consolidated, and synsets are represented by their offsets within WORDNET 2.1.</S>
			<S sid="152" ssid="36">Some words for immediate hypernyms have also been included to give context.</S>
			<S sid="153" ssid="37">In all other topics, the person, animal, or constellation senses were preferred.</S>
			<S sid="154" ssid="38">Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 president growth material point water plant music party age object number house change film city treatment color value road month work election feed form function area worker life administration day subject set city report time official period part square land mercer world office head self space home requirement group bill portion picture polynomial farm bank audience yesterday length artist operator spring farmer play court level art component bridge production thing meet foot patient corner pool medium style police maturity communication direction site petitioner year service center movement curve interest relationship show Table 2: The most probable words from six randomly chosen WORDNET-walks from a thirty-two topic model trained on the words in SEMCOR.</S>
			<S sid="155" ssid="39">These are summed over all of the possible synsets that generate the words.</S>
			<S sid="156" ssid="40">However, the vast majority of the contributions come from a single synset.</S>
			<S sid="157" ssid="41">1029 0.275 0.28 0.285 0.29 0.295 0.3 0.305 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy Iteratio n Unsee ded Seede d with LDA 96000 94000 92000 90000 88000 86000 84000 82000 80000 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Model Probability Iteratio n Unsee ded Seede d with LDA Figure 3: Topics seeded with LDA initially have a higher disambiguation accuracy, but are quickly matched by unseeded topics.</S>
			<S sid="158" ssid="42">The probability for the seeded topics starts lower and remains lower.</S>
			<S sid="159" ssid="43">4.2 Topics and the Weight of the Prior.</S>
			<S sid="160" ssid="44">Because the Dirichlet smoothing factor in partdetermines the topics, it also affects the disambiguation.</S>
			<S sid="161" ssid="45">Figure 4 shows the modal disambigua tion achieved for each of the settings of S = {0.1, 1, 5, 10, 15, 20}.</S>
			<S sid="162" ssid="46">Each line is one setting of K and each point on the line is a setting of S. Each data point is a run for the Gibbs sampler for 10,000 iterations.</S>
			<S sid="163" ssid="47">The disambiguation, taken at the mode,improved with moderate settings of S, which sug gests that the data are still sparse for many of thewalks, although the improvement vanishes if S dom inates with much larger values.</S>
			<S sid="164" ssid="48">This makes sense, as each walk has over 100,000 parameters, there are fewer than 100,000 words in SEMCOR, and each 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 S=20 S=15 S=10 S=5 S=1 S=0.1 Accuracy Smoot hing F actor 64 top ics 32 top ics 16 top ics 8 topic s 4 topic s 2 topic s 1 topic Rando m Figure 4: Each line represents experiments with a setnumber of topics and variable amounts of smooth ing on the SEMCOR corpus.</S>
			<S sid="165" ssid="49">The random baselineis at the bottom of the graph, and adding topics im proves accuracy.</S>
			<S sid="166" ssid="50">As smoothing increases, the prior(based on token frequency) becomes stronger.</S>
			<S sid="167" ssid="51">Ac curacy is the percentage of correctly disambiguated polysemous words in SEMCOR at the mode.word only serves as evidence to at most 19 parame ters (the length of the longest path in WORDNET).</S>
			<S sid="168" ssid="52">Generally, a greater number of topics increased the accuracy of the mode, but after around sixteen topics, gains became much smaller.</S>
			<S sid="169" ssid="53">The effect of ? is also related to the number of topics, as a value of S for a very large number of topics might overwhelm the observed data, while the same value of S might be the perfect balance for a smaller number of topics.For comparison, the method of using a WORDNET WALK applied to smaller contexts such as sentences or documents achieves an accuracy of between 26% and 30%, depending on the level of smoothing.</S>
	</SECTION>
	<SECTION title="Error Analysis. " number="5">
			<S sid="170" ssid="1">This method works well in cases where the delineation can be readily determined from the over all topic of the document.</S>
			<S sid="171" ssid="2">Words such as ?kid,?</S>
			<S sid="172" ssid="3">?may,?</S>
			<S sid="173" ssid="4">?shear,?</S>
			<S sid="174" ssid="5">?coach,?</S>
			<S sid="175" ssid="6">?incident,?</S>
			<S sid="176" ssid="7">?fence,?</S>
			<S sid="177" ssid="8">?bee,?</S>
			<S sid="178" ssid="9">and (previously used as an example) ?colt?</S>
			<S sid="179" ssid="10">were all perfectly disambiguated by this method.</S>
			<S sid="180" ssid="11">Figure 2 shows the WORDNET-WALK corresponding to a medical topic that correctly disambiguates ?cancer.?</S>
			<S sid="181" ssid="12">Problems arose, however, with highly frequent 1030 words, such as ?man? and ?time?</S>
			<S sid="182" ssid="13">that have many senses and can occur in many types of documents.</S>
			<S sid="183" ssid="14">For example, ?man? can be associated with manypossible meanings: island, game equipment, ser vant, husband, a specific mammal, etc. Although we know that the ?adult male?</S>
			<S sid="184" ssid="15">sense should be preferred, the alternative meanings will also be likely if they can be assigned to a topicthat shares common paths in WORDNET; the doc uments contain, however, many other places, jobs, and animals which are reasonable explanations (toLDAWN) of how ?man? was generated.</S>
			<S sid="185" ssid="16">Unfortunately, ?man? is such a ubiquitous term that top ics, which are derived from the frequency of wordswithin an entire document, are ultimately uninfor mative about its usage.</S>
			<S sid="186" ssid="17">While mistakes on these highly frequent terms significantly hurt our accuracy, errors associated with less frequent terms reveal that WORDNET?sstructure is not easily transformed into a probabilis tic graph.</S>
			<S sid="187" ssid="18">For instance, there are two senses ofthe word ?quarterback,?</S>
			<S sid="188" ssid="19">a player in American football.</S>
			<S sid="189" ssid="20">One is position itself and the other is a per son playing that position.</S>
			<S sid="190" ssid="21">While one would expect co-occurrence in sentences such as ?quarterback is a easy position, so our quarterback is happy,?</S>
			<S sid="191" ssid="22">the paths to both terms share only the root node, thus making it highly unlikely a topic would cover both senses.</S>
			<S sid="192" ssid="23">Because of WORDNET?s breadth, rare senses also impact disambiguation.</S>
			<S sid="193" ssid="24">For example, the metonymical use of ?door?</S>
			<S sid="194" ssid="25">to represent a wholebuilding as in the phrase ?girl next door?</S>
			<S sid="195" ssid="26">is under the same parent as sixty other synsets contain ing ?bridge,?</S>
			<S sid="196" ssid="27">?balcony,?</S>
			<S sid="197" ssid="28">?body,?</S>
			<S sid="198" ssid="29">?arch,?</S>
			<S sid="199" ssid="30">?floor,?</S>
			<S sid="200" ssid="31">and ?corner.?</S>
			<S sid="201" ssid="32">Surrounded by such common terms thatare also likely to co-occur with the more conventional meanings of door, this very rare sense be comes the preferred disambiguation of ?door.?</S>
	</SECTION>
	<SECTION title="Related Work. " number="6">
			<S sid="202" ssid="1">Abney and Light?s initial probabilistic WSD ap proach (1999) was further developed into a Bayesian network model by Ciaramita and Johnson (2000), who likewise used the appearance of monosemous terms close to ambiguous ones to ?explain away?</S>
			<S sid="203" ssid="2">the usage of ambiguous terms in selectional restrictions.</S>
			<S sid="204" ssid="3">We have adapted these approaches and put them into the context of a topic model.</S>
			<S sid="205" ssid="4">Recently, other approaches have created ad hoc connections between synsets in WORDNET and then considered walks through the newly created graph.</S>
			<S sid="206" ssid="5">Given the difficulties of using existing connections in WORDNET, Mihalcea (2005) proposed creating links between adjacent synsets that might comprise a sentence, initially setting weights to be equal to the Lesk overlap between the pairs, and then using the PageRank algorithm to determine the stationary distribution over synsets.</S>
			<S sid="207" ssid="6">6.1 Topics and Domains.</S>
			<S sid="208" ssid="7">Yarowsky was one of the first to contend that ?there is one sense for discourse?</S>
			<S sid="209" ssid="8">(1992).</S>
			<S sid="210" ssid="9">This has lead to the approaches like that of Magnini (Magnini et al., 2001) that attempt to find the category of a text, select the most appropriate synset, and then assign the selected sense using domain annotation attached to WORDNET.</S>
			<S sid="211" ssid="10">LDAWN is different in that the categories are notan a priori concept that must be painstakingly annotated within WORDNET and require no augmenta tion of WORDNET.</S>
			<S sid="212" ssid="11">This technique could indeed be used with any hierarchy.</S>
			<S sid="213" ssid="12">Our concepts are the ones that best partition the space of documents and do the best job of describing the distinctions of diction that separate documents from different domains.</S>
			<S sid="214" ssid="13">6.2 Similarity Measures.</S>
			<S sid="215" ssid="14">Our approach gives a probabilistic method of using information content (Resnik, 1995) as a start ing point that can be adjusted to cluster words ina given topic together; this is similar to the Jiang Conrath similarity measure (1997), which has beenused in many applications in addition to disambigua tion.</S>
			<S sid="216" ssid="15">Patwardhan (2003) offers a broad evaluation of similarity measures for WSD.</S>
			<S sid="217" ssid="16">Our technique for combining the cues of topicsand distance in WORDNET is adjusted in a way sim ilar in spirit to Buitelaar and Sacaleanu (2001), but we consider the appearance of a single term to be evidence for not just that sense and its immediate neighbors in the hyponomy tree but for all of the sense?s children and ancestors.</S>
			<S sid="218" ssid="17">Like McCarthy (2004), our unsupervised system acquires a single predominant sense for a domain based on a synthesis of information derived from a 1031textual corpus, topics, and WORDNET-derived sim ilarity, a probabilistic information content measure.</S>
			<S sid="219" ssid="18">By adding syntactic information from a thesaurusderived from syntactic features (taken from Lin?s au tomatically generated thesaurus (1998)), McCarthy achieved 48% accuracy in a similar evaluation onSEMCOR; LDAWN is thus substantially less effec tive in disambiguation compared to state-of-the-artmethods.</S>
			<S sid="220" ssid="19">This suggests, however, that other meth ods might be improved by adding topics and that ourmethod might be improved by using more informa tion than word counts.</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number="7">
			<S sid="221" ssid="1">The LDAWN model presented here makes two contributions to research in automatic word sense disambiguation.</S>
			<S sid="222" ssid="2">First, we demonstrate a method for au tomatically partitioning a document into topics that includes explicit semantic information.</S>
			<S sid="223" ssid="3">Second, we show that, at least for one simple model of WSD,embedding a document in probabilistic latent struc ture, i.e., a ?topic,?</S>
			<S sid="224" ssid="4">can improve WSD.</S>
			<S sid="225" ssid="5">There are two avenues of research with LDAWN that we will explore.</S>
			<S sid="226" ssid="6">First, the statistical nature ofthis approach allows LDAWN to be used as a com ponent in larger models for other language tasks.Other probabilistic models of language could insert the ability to query synsets or paths of WORDNET.</S>
			<S sid="227" ssid="7">Similarly, any topic based information retrieval scheme could employ topics that include se mantically relevant (but perhaps unobserved) terms.Incorporating this model in a larger syntactically aware model, which could benefit from the local context as well as the document level context, is an important component of future research.</S>
			<S sid="228" ssid="8">Second, the results presented here show a marked improvement in accuracy as more topics are added to the baseline model, although the final result is not comparable to state-of-the-art techniques.</S>
			<S sid="229" ssid="9">As most errors were attributable to the hyponomy structure of WORDNET, incorporating the novel use of topicmodeling presented here with a more mature unsu pervised WSD algorithm to replace the underlyingWORDNET-WALK could lead to advances in state of-the-art unsupervised WSD accuracy.</S>
	</SECTION>
</PAPER>
