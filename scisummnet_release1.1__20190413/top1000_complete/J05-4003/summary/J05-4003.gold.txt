Improving Machine Translation Performance By Exploiting Non-Parallel Corpora
We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.
We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.
Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.
We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.
We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.
Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.
We use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles.
We filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary).
We define features primarily based on IBM Model 1 alignments (Brown et al, 1993).
