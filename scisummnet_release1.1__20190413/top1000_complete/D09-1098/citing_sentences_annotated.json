[
  {
    "citance_No": 1, 
    "citing_paper_id": "D09-1025", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Marco, Pennacchiotti | Patrick, Pantel", 
    "raw_text": "Full algorithmic details are presented in (Pantel et al, 2009)", 
    "clean_text": "Full algorithmic details are presented in (Pantel et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D09-1025", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Marco, Pennacchiotti | Patrick, Pantel", 
    "raw_text": "Following Lin (1998), example systems include Fleischman and Hovy (2002), Cimiano and Volker (2005), Tanev and Magnini (2006), and Pantel et al (2009)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D09-1025", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Marco, Pennacchiotti | Patrick, Pantel", 
    "raw_text": "KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone", 
    "clean_text": "KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P14-1058", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Danushka, Bollegala | David, Weir | John, Carroll", 
    "raw_text": "Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012)", 
    "clean_text": "Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P13-1092", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Hoifung, Poon", 
    "raw_text": "To score relevant word snot appearing in the database (due to incompleteness of the database or lexical variations), GUSPuses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case)", 
    "clean_text": "To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P13-1092", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Hoifung, Poon", 
    "raw_text": "Since case information is important for parser sand taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia", 
    "clean_text": "Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P10-2068", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "David, Vickrey | Oscar, Kipersztok | Daphne, Koller", 
    "raw_text": "Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance", 
    "clean_text": "Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N10-1087", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Zornitsa, Kozareva | Eduard, Hovy", 
    "raw_text": "Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009)", 
    "clean_text": "Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N10-1087", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Zornitsa, Kozareva | Eduard, Hovy", 
    "raw_text": "As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion", 
    "clean_text": "As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N10-1087", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Zornitsa, Kozareva | Eduard, Hovy", 
    "raw_text": "According to (Pantel et al, 2009) 10to20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found", 
    "clean_text": "According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "C10-1057", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Alpa, Jain | Patrick, Pantel", 
    "raw_text": "Wecomputed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1)", 
    "clean_text": "We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "C10-2069", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Jey Han, Lau | David, Newman | Sarvnaz, Karimi | Timothy, Baldwin", 
    "raw_text": "To this end, we use WordNet to find hypernym relations between pairs of words in atopic and obtain a set of boolean-valued relation ships for each topic word. Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia.1 This takes the form of representing the distributional similarity between each pairing of terms sim (wi|wj); if wi is not in the top-200 most similar terms for a given wj, we assume it to have a similarity of 0", 
    "clean_text": "Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P10-1026", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Jun'ichi, Kazama | Stijn, De Saeger | Kow, Kuroda | Masaki, Murata | Kentaro, Torisawa", 
    "raw_text": "We evaluated our method in the calculation of similarities between nouns in Japanese.Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009) .Given a word set, which is expected to contain similar words, we assume that a good similarity measure should output, for each word in the set, the other words in the set as similar words. For given word sets, we can construct input-and answers pairs, where the answers for each word are the other words in the set the word appears in", 
    "clean_text": "Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W11-0315", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Fei, Huang | Alexander, Yates | Arun, Ahuja | Doug, Downey", 
    "raw_text": "pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus", 
    "clean_text": "To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P11-2128", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Kugatsu, Sadamitsu | Kuniko, Saito | Kenji, Imamura | Genichiro, Kikui", 
    "raw_text": "The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009)", 
    "clean_text": "The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P11-2128", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Kugatsu, Sadamitsu | Kuniko, Saito | Kenji, Imamura | Genichiro, Kikui", 
    "raw_text": "Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009)", 
    "clean_text": "Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "C10-1058", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Alpa, Jain | Marco, Pennacchiotti", 
    "raw_text": "CL-Web: A state-of-the-art open domain method based on features extracted from the Webdocuments data set (Pantel et al, 2009)", 
    "clean_text": "CL-Web: A state-of-the-art open domain method based on features extracted from the Web documents data set (Pantel et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P10-1150", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Zornitsa, Kozareva | Eduard, Hovy", 
    "raw_text": "As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult", 
    "clean_text": "As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P10-2066", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Xiao-Li, Li | Lei, Zhang | Bing, Liu | See-Kiong, Ng", 
    "raw_text": "Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009)", 
    "clean_text": "Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P10-2066", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Xiao-Li, Li | Lei, Zhang | Bing, Liu | See-Kiong, Ng", 
    "raw_text": "This com bi nation was also used in (Pantel et al, 2009)", 
    "clean_text": "This combination was also used in (Pantel et al, 2009).", 
    "keep_for_gold": 0
  }
]
