Web-Scale Distributional Similarity and Entity Set Expansion
Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task.
Parallelization and optimizations are necessary.
We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.
The pairwise similarity between 500 million terms is computed in 50 hours using 200 quadcore nodes.
We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size.
We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.
Our DASH stores the case for each phrase in Wikipedia.
We find that 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found.
Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts of all occurrences of all the seeds in the corpus.
