<PAPER>
  <S sid="0">Detection of Grammatical Errors Involving Prepositions</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper presents ongoing work on the detection of preposition errors of non-native speakers of English.</S>
    <S sid="2" ssid="2">Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students.</S>
    <S sid="3" ssid="3">To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.</S>
    <S sid="4" ssid="4">Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">The National Clearinghouse for English Language Acquisition (2002) estimates that 9.6% of the students in the US public school population speak a language other than English and have limited English proficiency.</S>
    <S sid="6" ssid="2">Clearly, there is a substantial and increasing need for tools for instruction in English as a Second Language (ESL).</S>
    <S sid="7" ssid="3">In particular, preposition usage is one of the most difficult aspects of English grammar for non-native speakers to master.</S>
    <S sid="8" ssid="4">Preposition errors account for a significant proportion of all ESL grammar errors.</S>
    <S sid="9" ssid="5">They represented the largest category, about 29%, of all the errors by 53 intermediate to advanced ESL students (Bitchener et al., 2005), and 18% of all errors reported in an intensive analysis of one Japanese writer (Murata and Ishara, 2004).</S>
    <S sid="10" ssid="6">Preposition errors are not only prominent among error types, they are also quite frequent in ESL writing.</S>
    <S sid="11" ssid="7">Dalgish (1985) analyzed the essays of 350 ESL college students representing 15 different native languages and reported that preposition errors were present in 18% of sentences in a sample of text produced by writers from first languages as diverse as Korean, Greek, and Spanish.</S>
    <S sid="12" ssid="8">The goal of the research described here is to provide software for detecting common grammar and usage errors in the English writing of non-native English speakers.</S>
    <S sid="13" ssid="9">Our work targets errors involving prepositions, specifically those of incorrect preposition selection, such as arrive to the town, and those of extraneous prepositions, as in most ofpeople.</S>
    <S sid="14" ssid="10">We present an approach that combines machine learning with rule-based filters to detect preposition errors in a corpus of ESL essays.</S>
    <S sid="15" ssid="11">Even though this is work in progress, we achieve precision of 0.8 with a recall of 0.3.</S>
    <S sid="16" ssid="12">The paper is structured as follows: in the next section, we describe the difficulty in learning English preposition usage; in Section 3, we discuss related work; in Sections 4-7 we discuss our methodology and evaluation.</S>
  </SECTION>
  <SECTION title="2 Problem of Preposition Usage" number="2">
    <S sid="17" ssid="1">Why are prepositions so difficult to master?</S>
    <S sid="18" ssid="2">Perhaps it is because they perform so many complex roles.</S>
    <S sid="19" ssid="3">In English, prepositions appear in adjuncts, they mark the arguments of predicates, and they combine with other parts of speech to express new meanings.</S>
    <S sid="20" ssid="4">The choice of preposition in an adjunct is largely constrained by its object (in the summer, on Friday, at noon) and the intended meaning (at the beach, on the beach, near the beach, by the beach).</S>
    <S sid="21" ssid="5">Since adjuncts are optional and tend to be flexible in their position in a sentence, the task facing the learner is quite complex.</S>
    <S sid="22" ssid="6">Prepositions are also used to mark the arguments of a predicate.</S>
    <S sid="23" ssid="7">Usually, the predicate is expressed by a verb, but sometimes it takes the form of an adjective (He was fond of beer), a noun (They have a thirst for knowledge), or a nominalization (The child&#8217;s removal from the classroom).</S>
    <S sid="24" ssid="8">The choice of the preposition as an argument marker depends on the type of argument it marks, the word that fills the argument role, the particular word used as the predicate, and whether the predicate is a nominalization.</S>
    <S sid="25" ssid="9">Even with these constraints, there are still variations in the ways in which arguments can be expressed.</S>
    <S sid="26" ssid="10">Levin (1993) catalogs verb alternations such as They loaded hay on the wagon vs.</S>
    <S sid="27" ssid="11">They loaded the wagon with hay, which show that, depending on the verb, an argument may sometimes be marked by a preposition and sometimes not.</S>
    <S sid="28" ssid="12">English has hundreds of phrasal verbs, consisting of a verb and a particle (some of which are also prepositions).</S>
    <S sid="29" ssid="13">To complicate matters, phrasal verbs are often used with prepositions (i.e., give up on someone; give in to their demands).</S>
    <S sid="30" ssid="14">Phrasal verbs are particularly difficult for non-native speakers to master because of their non-compositionality of meaning, which forces the learner to commit them to rote memory.</S>
  </SECTION>
  <SECTION title="3 Related Work" number="3">
    <S sid="31" ssid="1">If mastering English prepositions is a daunting task for the second language learner, it is even more so for a computer.</S>
    <S sid="32" ssid="2">To our knowledge, only three other groups have attempted to automatically detect errors in preposition usage.</S>
    <S sid="33" ssid="3">Eeg-Olofsson et al. (2003) used 31 handcrafted matching rules to detect extraneous, omitted, and incorrect prepositions in Swedish text written by native speakers of English, Arabic, and Japanese.</S>
    <S sid="34" ssid="4">The rules, which were based on the kinds of errors that were found in a training set of text produced by non-native Swedish writers, targeted spelling errors involving prepositions and some particularly problematic Swedish verbs.</S>
    <S sid="35" ssid="5">In a test of the system, 11 of 40 preposition errors were correctly detected.</S>
    <S sid="36" ssid="6">Izumi et al. (2003) and (2004) used errorannotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier (Ratnaparkhi, 1998) to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions.</S>
    <S sid="37" ssid="7">The classifier relied on lexical and syntactic features.</S>
    <S sid="38" ssid="8">Overall performance for the 13 error types reached 25.1% precision with 7.1% recall on an independent test set of sentences from the same source, but the researchers do not separately report the results for preposition error detection.</S>
    <S sid="39" ssid="9">The approach taken by Izumi and colleagues is most similar to the one we have used, which is described in the next section.</S>
    <S sid="40" ssid="10">More recently, (Lee and Seneff, 2006) used a language model and stochastic grammar to replace prepositions removed from a dialogue corpus.</S>
    <S sid="41" ssid="11">Even though they reported a precision of 0.88 and recall of 0.78, their evaluation was on a very restricted domain with only a limited number of prepositions, nouns and verbs.</S>
  </SECTION>
  <SECTION title="4 The Selection Model" number="4">
    <S sid="42" ssid="1">A preposition error can be a case of incorrect preposition selection (They arrived to the town), use of a preposition in a context where it is prohibited (They came to inside), or failure to use a preposition in a context where it is obligatory (e.g., He is fond this book).</S>
    <S sid="43" ssid="2">To detect the first type of error, incorrect selection, we have employed a maximum entropy (ME) model to estimate the probability of each of 34 prepositions, based on the features in their local contexts.</S>
    <S sid="44" ssid="3">The ME Principle says that the best model will satisfy the constraints found in the training, and for those situations not covered in the training, the best model will assume a distribution of maximum entropy.</S>
    <S sid="45" ssid="4">This approach has been shown to perform well in combining heterogeneous forms of evidence, as in word sense disambiguation (Ratnaparkhi, 1998).</S>
    <S sid="46" ssid="5">It also has the desirable property of handling interactions among features without having to rely on the assumption of feature independence, as in a Naive Bayesian model.</S>
    <S sid="47" ssid="6">Our ME model was trained on 7 million &#8220;events&#8221; consisting of an outcome (the preposition that appeared in the training text) and its associated context (the set of feature-value pairs that accompanied it).</S>
    <S sid="48" ssid="7">These 7 million prepositions and their contexts were extracted from the MetaMetrics corpus of 1100 and 1200 Lexile text (11th and 12th grade) and newspaper text from the San Jose Mercury News.</S>
    <S sid="49" ssid="8">The sentences were then POS-tagged (Ratnaparkhi, 1998) and then chunked into noun phrases and verb phrases by a heuristic chunker.</S>
    <S sid="50" ssid="9">The maximum entropy model was trained with 25 contextual features.</S>
    <S sid="51" ssid="10">Some of the features represented the words and tags found at specific locations adjacent to the preposition; others represented the head words and tags of phrases that preceded or followed the preposition.</S>
    <S sid="52" ssid="11">Table 1 shows a subset of the feature list.</S>
    <S sid="53" ssid="12">Some features had only a few values while others had many.</S>
    <S sid="54" ssid="13">PHR pre is the &#8220;preceding phrase&#8221; feature that indicates whether the preposition was preceded by a noun phrase (NP) or a verb phrase (VP).</S>
    <S sid="55" ssid="14">In the example in Table 2, the preposition into is preceded by an NP.</S>
    <S sid="56" ssid="15">In a sentence that begins After the crowd was whipped up into a frenzy of anticipation, the preposition into is preceded by a VP.</S>
    <S sid="57" ssid="16">There were only two feature#value pairs for this feature: PHR pre#NP and PHR pre#VP.</S>
    <S sid="58" ssid="17">Other features had hundreds or even thousands of different values because they represented the occurrence of specific words that preceded or followed a preposition.</S>
    <S sid="59" ssid="18">Any feature#value pairs which occurred with very low frequency in the training (less than 10 times in the 7 million contexts) were eliminated to avoid the need for smoothing their probabilities.</S>
    <S sid="60" ssid="19">Lemma forms of words were used as feature values to further reduce the total number and to allow the model to generalize across inflectional variants.</S>
    <S sid="61" ssid="20">Even after incorporating these reductions, the number of values was still very large.</S>
    <S sid="62" ssid="21">As Table 1 indicates, TGR, the word sequence including the preposition and the two words to its right, had 54,906 different values.</S>
    <S sid="63" ssid="22">Summing across all features, the model contained a total of about 388,000 feature#value pairs.</S>
    <S sid="64" ssid="23">Table 2 shows an example of where some of the features are derived from.</S>
  </SECTION>
  <SECTION title="5 Evaluation on Grammatical Text" number="5">
    <S sid="65" ssid="1">The model was tested on 18,157 preposition contexts extracted from 12 files randomly selected from a portion of 1100 Lexile text (11th grade) that had not been used for training.</S>
    <S sid="66" ssid="2">For each context, the model predicted the probability of each preposition given the contextual representation.</S>
    <S sid="67" ssid="3">The highest probability preposition was then compared to the preposition that had actually been used by the writer.</S>
    <S sid="68" ssid="4">Because the test corpus consisted of published, edited text, we assumed that this material contained few, if any, errors.</S>
    <S sid="69" ssid="5">In this and subsequent tests, the model was used to classify each context as one of 34 classes (prepositions).</S>
    <S sid="70" ssid="6">Results of the comparison between the classifier and the test set showed that the overall proportion of agreement between the text and the classifier was 0.69.</S>
    <S sid="71" ssid="7">The value of kappa was 0.64.</S>
    <S sid="72" ssid="8">When we examined the errors, we discovered that, frequently, the classifier&#8217;s most probable preposition (the one it assigned) differed from the second most probable by just a few percentage points.</S>
    <S sid="73" ssid="9">This corresponded to a situation in which two or more prepositions were likely to be found in a given context.</S>
    <S sid="74" ssid="10">Consider the context They thanked him for his consideration this matter, where either of or in could fill the blank.</S>
    <S sid="75" ssid="11">Because the classifier was forced to make a choice in this and other close cases, it incurred a high probability of making an error.</S>
    <S sid="76" ssid="12">To avoid this situation, we re-ran the test allowing the classifier to skip any preposition if its top ranked and second ranked choices differed by less than a specified amount.</S>
    <S sid="77" ssid="13">In other words, we permitted it to respond only when it was confident of its decision.</S>
    <S sid="78" ssid="14">When the difference between the first and second ranked choices was 0.60 or greater, 50% of the cases received no decision, but for the remaining half of the test cases, the proportion of agreement was 0.90 and kappa was 0.88.</S>
    <S sid="79" ssid="15">This suggests that a considerable improvement in performance can be achieved by using a more conservative approach based on a higher confidence level for the classifier.</S>
  </SECTION>
  <SECTION title="6 Evaluation on ESL Essays" number="6">
    <S sid="80" ssid="1">To evaluate the ME model&#8217;s suitability for analyzing ungrammatical text, 2,000 preposition contexts were extracted from randomly selected essays written on ESL tests by native speakers of Chinese, Japanese, and Russian.</S>
    <S sid="81" ssid="2">This set of materials was used to look for problems that were likely to arise as a consequence of the mismatch between the training corpus (edited, grammatical text) and the testing corpus (ESL essays with errors of various kinds).</S>
    <S sid="82" ssid="3">When the model was used to classify prepositions in the ESL essays, it became obvious, almost immediately, that a number of new performance issues would have to be addressed.</S>
    <S sid="83" ssid="4">The student essays contained many misspelled words.</S>
    <S sid="84" ssid="5">Because misspellings were not in the training, the model was unable to use the features associated with them (e.g., FHword#frinzy) in its decision making.</S>
    <S sid="85" ssid="6">The tagger was also affected by spelling errors, so to avoid these problems, the classifier was allowed to skip any context that contained misspelled words in positions adjacent to the preposition or in its adjacent phrasal heads.</S>
    <S sid="86" ssid="7">A second problem resulted from punctuation errors in the student writing.</S>
    <S sid="87" ssid="8">This usually took the form of missing commas, as in I disagree because from my point of view there is no evidence.</S>
    <S sid="88" ssid="9">In the training corpus, commas generally separated parenthetical expressions, such as from my point of view, from the rest of the sentence.</S>
    <S sid="89" ssid="10">Without the comma, the model selected of as the most probable preposition following because, instead of from.</S>
    <S sid="90" ssid="11">A set of heuristics was used to locate common sites of comma errors and skip these contexts.</S>
    <S sid="91" ssid="12">There were two other common sources of classification error: antonyms and benefactives.</S>
    <S sid="92" ssid="13">The model very often confused prepositions with opposite meanings (like with/without and from/to), so when the highest probability preposition was an antonym of the one produced by the writer, we blocked the classifier from marking the usage as an error.</S>
    <S sid="93" ssid="14">Benefactive phrases of the form for + person/organization (for everyone, for my school) were also difficult for the model to learn, most likely because, as adjuncts, they are free to appear in many different places in a sentence and the preposition is not constrained by its object, resulting in their frequency being divided among many different contexts.</S>
    <S sid="94" ssid="15">When a benefactive appeared in an argument position, the model&#8217;s most probable preposition was generally not the preposition for.</S>
    <S sid="95" ssid="16">In the sentence They described a part for a kid, the preposition of has a higher probability.</S>
    <S sid="96" ssid="17">The classifier was prevented from marking for + person/organization as a usage error in such contexts.</S>
    <S sid="97" ssid="18">To summarize, the classifier consisted of the ME model plus a program that blocked its application in cases of misspelling, likely punctuation errors, antonymous prepositions, and benefactives.</S>
    <S sid="98" ssid="19">Another difference between the training corpus and the testing corpus was that the latter contained grammatical errors.</S>
    <S sid="99" ssid="20">In the sentence, This was my first experience about choose friends, there is a verb error immediately following the preposition.</S>
    <S sid="100" ssid="21">Arguably, the preposition is also wrong since the sequence about choose is ill-formed.</S>
    <S sid="101" ssid="22">When the classifier marked the preposition as incorrect in an ungrammatical context, it was credited with correctly detecting a preposition error.</S>
    <S sid="102" ssid="23">Next, the classifier was tested on the set of 2,000 preposition contexts, with the confidence threshold set at 0.9.</S>
    <S sid="103" ssid="24">Each preposition in these essays was judged for correctness of usage by one or two human raters.</S>
    <S sid="104" ssid="25">The judged rate of occurrence of preposition errors was 0.109 for Rater 1 and 0.098 for Rater 2, i.e., about 1 out of every 10 prepositions was judged to be incorrect.</S>
    <S sid="105" ssid="26">The overall proportion of agreement between Rater1 and Rater 2 was 0.926, and kappa was 0.599.</S>
    <S sid="106" ssid="27">Table 3 (second column) shows the results for the Classifier vs. Rater 1, using Rater 1 as the gold standard.</S>
    <S sid="107" ssid="28">Note that this is not a blind test of the classifier inasmuch as the classifier&#8217;s confidence threshold was adjusted to maximize performance on this set.</S>
    <S sid="108" ssid="29">The overall proportion of agreement was 0.942, but kappa was only 0.365 due to the high level of agreement expected by chance, as the Classifier used the response category of &#8220;correct&#8221; more than 97% of the time.</S>
    <S sid="109" ssid="30">We found similar results when comparing the judgements of the Classifier to Rater 2: agreement was high and kappa was low.</S>
    <S sid="110" ssid="31">In addition, for both raters, precision was much higher than recall.</S>
    <S sid="111" ssid="32">As noted earlier, the table does not include the cases that the classifier skipped due to misspelling, antonymous prepositions, and benefactives.</S>
    <S sid="112" ssid="33">Both precision and recall are low in these comparisons to the human raters.</S>
    <S sid="113" ssid="34">We are particularly concerned about precision because the feedback that students receive from an automated writing analysis system should, above all, avoid false positives, i.e., marking correct usage as incorrect.</S>
    <S sid="114" ssid="35">We tried to improve precision by adding to the system a naive Bayesian classifier that uses the same features found in Table 1.</S>
    <S sid="115" ssid="36">As expected, its performance is not as good as the ME model (e.g., precision = 0.57 and recall = 0.29 compared to Rater 1 as the gold standard), but when the Bayesian classifier was given a veto over the decision of the ME classifier, overall precision did increase substantially (to 0.88), though with a reduction in recall (to 0.16).</S>
    <S sid="116" ssid="37">To address the problem of low recall, we have targeted another type of ESL preposition error: extraneous prepositions.</S>
  </SECTION>
  <SECTION title="7 Prepositions in Prohibited Contexts" number="7">
    <S sid="117" ssid="1">Our strategy of training the ME classifier on grammatical, edited text precluded detection of extraneous prepositions as these did not appear in the training corpus.</S>
    <S sid="118" ssid="2">Of the 500-600 errors in the ESL test set, 142 were errors of this type.</S>
    <S sid="119" ssid="3">To identify extraneous preposition errors we devised two rule-based filters which were based on analysis of the development set.</S>
    <S sid="120" ssid="4">Both used POS tags and chunking information.</S>
    <S sid="121" ssid="5">Plural Quantifier Constructions This filter addresses the second most common extraneous preposition error where the writer has added a preposition in the middle of a plural quantifier construction, for example: some ofpeople.</S>
    <S sid="122" ssid="6">This filter works by checking if the target word is preceded by a quantifier (such as &#8220;some&#8221;, &#8220;few&#8221;, or &#8220;three&#8221;), and if the head noun of the quantifier phrase is plural.</S>
    <S sid="123" ssid="7">Then, if there is no determiner in the phrase, the target word is deemed an extraneous preposition error.</S>
    <S sid="124" ssid="8">Repeated Prepositions These are cases such as people can find friends with with the same interests where a preposition occurs twice in a row.</S>
    <S sid="125" ssid="9">Repeated prepositions were easily screened by checking if the same lexical item and POS tag were used for both words.</S>
    <S sid="126" ssid="10">These filters address two types of extraneous preposition errors, but there are many other types (for example, subcategorization errors, or errors with prepositions inserted incorrectly in the beginning of a sentence initial phrase).</S>
    <S sid="127" ssid="11">Even though these filters cover just one quarter of the 142 extraneous errors, they did improve precision from 0.778 to 0.796, and recall from 0.259 to 0.304 (comparing to Rater 1).</S>
  </SECTION>
  <SECTION title="8 Conclusions and Future Work" number="8">
    <S sid="128" ssid="1">We have presented a combined machine learning and rule-based approach that detects preposition errors in ESL essays with precision of 0.80 or higher (0.796 with the ME classifier and Extraneous Preposition filters; and 0.88 with the combined ME and Bayesian classifiers).</S>
    <S sid="129" ssid="2">Our work is novel in that we are the first to report specific performance results for a preposition error detector trained and evaluated on general corpora.</S>
    <S sid="130" ssid="3">While the training for the ME classifier was done on a separate corpus, and it was this classifier that contributed the most to the high precision, it should be noted that some of the filters were tuned on the evaluation corpus.</S>
    <S sid="131" ssid="4">Currently, we are in the course of annotating additional ESL essays for preposition errors in order to obtain a larger-sized test set.</S>
    <S sid="132" ssid="5">While most NLP systems are a balancing act between precision and recall, the domain of designing grammatical error detection systems is distinguished in its emphasis on high precision over high recall.</S>
    <S sid="133" ssid="6">Essentially, a false positive, i.e., an instance of an error detection system informing a student that a usage is incorrect when in fact it is indeed correct, must be reduced at the expense of a few genuine errors slipping through the system undetected.</S>
    <S sid="134" ssid="7">Given this, we chose to set the threshold for the system so that it ensures high precision which in turn resulted in a recall figure (0.3) that leaves us much room for improvement.</S>
    <S sid="135" ssid="8">Our plans for future system development include: 1.</S>
    <S sid="136" ssid="9">Using more training data.</S>
    <S sid="137" ssid="10">Even a cursory examination of the training corpus reveals that there are many gaps in the data.</S>
    <S sid="138" ssid="11">Seven million seems like a large number of examples, but the selection of prepositions is highly dependent on the presence of other specific words in the context.</S>
    <S sid="139" ssid="12">Many fairly common combinations of Verb+Preposition+Noun or Noun+Preposition+Noun are simply not attested, even in a sizable corpus.</S>
    <S sid="140" ssid="13">Consistent with this, there is a strong correlation between the relative frequency of a preposition and the classifier&#8217;s ability to predict its occurrence in edited text.</S>
    <S sid="141" ssid="14">That is, prediction is better for prepositions that have many examples in the training set and worse for those with fewer examples.</S>
    <S sid="142" ssid="15">This suggests the need for much more data. model in this study contains no semantic information.</S>
    <S sid="143" ssid="16">One way to extend and improve its coverage might be to include features of verbs and their noun arguments from sources such as FrameNet (http://framenet.icsi.berkeley.edu/), which detail the semantics of the frames in which many English words appear.</S>
  </SECTION>
</PAPER>
