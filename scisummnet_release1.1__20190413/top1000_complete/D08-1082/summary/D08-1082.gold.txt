A Generative Model for Parsing Natural Language to Meaning Representations
In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures.
The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning.
We introduce dynamic programming techniques for efficient training and decoding.
In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora.
The generative model degrades robustly when presented with instances that are different from those seen in training.
This allows a notable improvement in recall compared to previous models.
Our hybrid tree model use a tree transformation based approach.
We present a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form.
We propose 3 models for generative semantic parsing :unigram, bigram, and mix gram (interpolation between the two).
