Unsupervised Semantic Parsing
We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic.
Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning.
The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them.
We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions.
USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.
We consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments.
We model joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006), selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures.
We group parameters and impose local normalization constraints within each group.
