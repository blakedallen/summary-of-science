[
  {
    "citance_No": 1, 
    "citing_paper_id": "P99-1067", 
    "citing_paper_authority": 109, 
    "citing_paper_authors": "Reinhard, Rapp", 
    "raw_text": "It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schtitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P06-2075", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Shachar, Mirkin | Ido, Dagan | Maayan, Zhitomirsky-Geffet", 
    "raw_text": "Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research", 
    "clean_text": "Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P06-2075", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Shachar, Mirkin | Ido, Dagan | Maayan, Zhitomirsky-Geffet", 
    "raw_text": "This scheme utilizes the symmetric similarity measure of (Lin, 1998) to induce improved feature weights via bootstrapping", 
    "clean_text": "This scheme utilizes the symmetric similarity measure of (Lin, 1998) to induce improved feature weights via bootstrapping.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "N06-3007", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Irina, Matveeva", 
    "raw_text": "We will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following (Lin, 1998) .Currently we are using string matching to compute the named entity based measure of similarity", 
    "clean_text": "We will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following (Lin, 1998).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "C08-1054", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Daisuke, Kawahara | Sadao, Kurohashi", 
    "raw_text": "Chantree et al (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation", 
    "clean_text": "Chantree et al (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D09-1084", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Danushka, Bollegala | Yutaka, Matsuo | Mitsuru, Ishizuka", 
    "raw_text": "Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation (Resnik, 1995), synonym extraction (Lin, 1998a), and automatic thesauri generation (Curran, 2002)", 
    "clean_text": "Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation (Resnik, 1995), synonym extraction (Lin, 1998a), and automatic thesauri generation (Curran, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D09-1084", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Danushka, Bollegala | Yutaka, Matsuo | Mitsuru, Ishizuka", 
    "raw_text": "Lin (1998b) defined the similarity between two concepts as the information that is in common to both concepts and the information contained in each individual concept", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D09-1084", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Danushka, Bollegala | Yutaka, Matsuo | Mitsuru, Ishizuka", 
    "raw_text": "Method Correlation Edge-counting 0.664 Jiang& amp; Conrath (1998) 0.848 Lin (1998a) 0.822 Resnik (1995) 0.745 Li et al (2003) 0.891ball) and (Jerusalem, Israel))", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W03-0418", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Dominic R., Jones | Cynthia A., Thompson", 
    "raw_text": "Lin (1998) created a thesaurus using syntactic relationships with other words. Rooth et al (1999) used clustering to create clusters similar to Levin verb classes (Levin, 1993)", 
    "clean_text": "Lin (1998) created a thesaurus using syntactic relationships with other words.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W08-2211", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Rob, Koeling | Diana, McCarthy", 
    "raw_text": "Like McCarthy et al (2004) we use k= 50 and obtain our thesaurus using the distributional similarity metric described by Lin (1998)", 
    "clean_text": "Like McCarthy et al (2004) we use k= 50 and obtain our thesaurus using the distributional similarity metric described by Lin (1998).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W08-2211", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Rob, Koeling | Diana, McCarthy", 
    "raw_text": "The thesaurus was acquired using the method described by Lin (1998)", 
    "clean_text": "The thesaurus was acquired using the method described by Lin (1998).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W08-2211", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Rob, Koeling | Diana, McCarthy", 
    "raw_text": "For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin (1998)", 
    "clean_text": "For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin (1998).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P13-1055", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Olivier, Ferret", 
    "raw_text": "This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al, 2008)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P13-1055", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Olivier, Ferret", 
    "raw_text": "As in (Lin, 1998) or (Cur ran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus", 
    "clean_text": "As in (Lin, 1998) or (Cur ran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P13-1055", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Olivier, Ferret", 
    "raw_text": "This seems to be a reasonable compromise between the approach of (Freitag et al, 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998)", 
    "clean_text": "This seems to be a reasonable compromise between the approach of (Freitag et al, 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P13-1055", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Olivier, Ferret", 
    "raw_text": "Finally, the results of Table 2 are compatible with those of (Lin, 1998) for instance (R-prec", 
    "clean_text": "Finally, the results of Table 2 are compatible with those of (Lin, 1998) for instance (R-prec. = 11.6 and MAP = 8.1 with WM as reference for all entries of the thesaurus at http://webdocs.cs.ualberta.ca/lindek/Downloads/sim.tgz) if we take into account the fact that the thesaurus of Lin was built from a much larger corpus and with syntactic co-occurrences.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W09-1123", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Peter, Wittek | S&aacute;ndor, Dar&aacute;nyi | Chew Lim, Tan", 
    "raw_text": "For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998)", 
    "clean_text": "For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D07-1034", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Oi Yee, Kwong | Benjamin K., T'sou", 
    "raw_text": "For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes", 
    "clean_text": "For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P11-1148", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Tim, Van de Cruys | Marianna, Apidianaki", 
    "raw_text": "One of the most important approaches is Lin (1998)", 
    "clean_text": "One of the most important approaches is Lin (1998).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "H05-1053", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Rob, Koeling | Diana, McCarthy | John, Carroll", 
    "raw_text": "et al (2004) we use \u0003\u0005\u0004\u0007\u0006 \b and obtain our thesaurus using the distributional similarity metric described by Lin (1998)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]
