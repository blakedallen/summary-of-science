<PAPER>
  <S sid="0">Decoding Complexity In Word-Replacement Translation Models</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Statistical machine translation is a relatively new approach to the long-standing problem of translating human languages by computer.</S>
    <S sid="2" ssid="2">Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts.</S>
    <S sid="3" ssid="3">The general architecture is the source-channel model: an English string is statistically generated (source), then statistically transformed into French (channel).</S>
    <S sid="4" ssid="4">In order to translate (or &amp;quot;decode&amp;quot;) a French string, we look for the most likely English source.</S>
    <S sid="5" ssid="5">We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S>
    <S sid="6" ssid="6">We trace this complexity to factors not present in other decoding problems.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="7" ssid="1">Statistical machine translation is a relatively new approach to the long-standing problem of translating human languages by computer.</S>
    <S sid="8" ssid="2">Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts.</S>
    <S sid="9" ssid="3">The general architecture is the source-channel model: an English string is statistically generated (source), then statistically transformed into French (channel).</S>
    <S sid="10" ssid="4">In order to translate (or &amp;quot;decode&amp;quot;) a French string, we look for the most likely English source.</S>
    <S sid="11" ssid="5">We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S>
    <S sid="12" ssid="6">We trace this complexity to factors not present in other decoding problems.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="13" ssid="1">Statistical models are widely used in attacking natural language problems.</S>
    <S sid="14" ssid="2">The sourcechannel framework is especially popular, finding applications in part-of-speech tagging, accent restoration, transliteration, speech recognition, and many other areas.</S>
    <S sid="15" ssid="3">In this framework, we build an underspecified model of how certain structures (such as strings) are generated and transformed.</S>
    <S sid="16" ssid="4">We then instantiate the model through training on a database of sample structures and transformations.</S>
    <S sid="17" ssid="5">Recently, Brown et al. (1993) built a source-channel model of translation between English and French.</S>
    <S sid="18" ssid="6">They assumed that English strings are produced according to some stochastic process (source model) and transformed stochastically into French strings (channel model).</S>
    <S sid="19" ssid="7">To translate French to English, it is necessary to find an English source string that is likely according to the models.</S>
    <S sid="20" ssid="8">With a nod to its cryptographic antecedents, this kind of translation is called decoding.</S>
    <S sid="21" ssid="9">This paper looks at decoding complexity.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="22" ssid="1">The prototype source-channel application in natural language is part-of-speech tagging (Church 1988).</S>
    <S sid="23" ssid="2">We review it here for purposes of comparison with machine translation.</S>
    <S sid="24" ssid="3">Source strings comprise sequences of part-of-speech tags like noun, verb, etc.</S>
    <S sid="25" ssid="4">A simple source model assigns a probability to a tag sequence ti tm based on the probabilities of the tag pairs inside it.</S>
    <S sid="26" ssid="5">Target strings are English sentences, e.g., w1 wm.</S>
    <S sid="27" ssid="6">The channel model assumes each tag is probabilistically replaced by a word (e.g., noun by dog) without considering context.</S>
    <S sid="28" ssid="7">More concretely, we have: We can assign parts-of-speech to a previously unseen word sequence w1 ...</S>
    <S sid="29" ssid="8">Win by finding the sequence ti ... 4, that maximizes P(ti ... tm I wi ... Wm).</S>
    <S sid="30" ssid="9">By Bayes' rule, we can equivalently maximize P(ti ... tm)&#8226;P(wi ... Wm' ti .</S>
    <S sid="31" ssid="10">.</S>
    <S sid="32" ssid="11">.</S>
    <S sid="33" ssid="12">G), which we can calculate directly from the b and s tables above.</S>
    <S sid="34" ssid="13">Three interesting complexity problems in the source-channel framework are: The first problem is solved in 0(m) time for part-of-speech tagging&#8212;we simply count tag pairs and word/tag pairs, then normalize.</S>
    <S sid="35" ssid="14">The second problem seems to require enumerating all 0(e) potential source sequences to find the best, but can actually be solved in 0(mv2) time with dynamic programming.</S>
    <S sid="36" ssid="15">We turn to the third problem in the context of another application: cryptanalysis.</S>
  </SECTION>
  <SECTION title="3." number="4">
    <S sid="37" ssid="1">In a substitution cipher, a plaintext message like HELLO WORLD is transformed into a ciphertext message like EOPPX YXAPF via a fixed letter-substitution table.</S>
    <S sid="38" ssid="2">As with tagging, we can assume an alphabet of v source tokens, a bigram source model, a substitution channel model, and an m-token coded text.</S>
    <S sid="39" ssid="3">If the coded text is annotated with corresponding English, then building source and channel models is trivially 0(m).</S>
    <S sid="40" ssid="4">Comparing the situation to part-of-speech tagging: Then the problem becomes one of acquiring a channel model, i.e., a table s(f le) with an entry for each code-letter/plaintext-letter pair.</S>
    <S sid="41" ssid="5">Starting with an initially uniform table, we can use the estimation-maximization (EM) algorithm to iteratively revise s(f. le) so as to increase the probability of the observed corpus P(f).</S>
    <S sid="42" ssid="6">Figure 1 shows a naive EM implementation that runs in 0(mvin) time.</S>
    <S sid="43" ssid="7">There is an efficient 0(mv2) EM implementation based on dynamic programming that accomplishes the same thing.</S>
    <S sid="44" ssid="8">Once the s(f le) table has been learned, there is a similar 0(mv2) algorithm for optimal decoding.</S>
    <S sid="45" ssid="9">Such methods can break English letter-substitution ciphers of moderate size.</S>
    <S sid="46" ssid="10">Given coded text f of length m, a plaintext vocabulary of v tokens, and a source model b: A naive application of the EM algorithm to break a substitution cipher.</S>
    <S sid="47" ssid="11">It runs in 0(men) time.</S>
  </SECTION>
  <SECTION title="4." number="5">
    <S sid="48" ssid="1">In our discussion of substitution ciphers, we were on relatively sure ground&#8212;the channel model we assumed in decoding is actually the same one used by the cipher writer for encoding.</S>
    <S sid="49" ssid="2">That is, we know that plaintext is converted to ciphertext, letter by letter, according to some table.</S>
    <S sid="50" ssid="3">We have no such clear conception about how English gets converted to French, although many theories exist.</S>
    <S sid="51" ssid="4">Brown et al. (1993) recently cast some simple theories into a source-channel framework, using the bilingual Canadian parliament proceedings as training data.</S>
    <S sid="52" ssid="5">We may assume: Bilingual texts seem to exhibit English words getting substituted with French ones, though not one-for-one and not without changing their order.</S>
    <S sid="53" ssid="6">These are important departures from the two applications discussed earlier.</S>
    <S sid="54" ssid="7">In the main channel model of Brown et al. (1993), each English word token e, in a source sentence is assigned a &amp;quot;fertility&amp;quot; 0&#8222; which dictates how many French words it will produce.</S>
    <S sid="55" ssid="8">These assignments are made stochastically according to a table n(01 e).</S>
    <S sid="56" ssid="9">Then actual French words are produced according to s(f le) and permuted into new positions according to a distortion table d(j1i, m, I).</S>
    <S sid="57" ssid="10">Here, j and i are absolute target/source word positions within a sentence, and m and I are target/source sentence lengths.</S>
    <S sid="58" ssid="11">Inducing n, s, and d parameter estimates is easy if we are given annotations in the form of word alignments.</S>
    <S sid="59" ssid="12">An alignment is a set of connections between English and French words in a sentence pair.</S>
    <S sid="60" ssid="13">In Brown et al. (1993), alignments are asymmetric&#8212; each French word is connected to exactly one English word.</S>
    <S sid="61" ssid="14">Word-aligned data is usually not available, but large sets of unaligned bilingual sentence pairs do sometimes exist.</S>
    <S sid="62" ssid="15">A single sentence pair will have rn possible alignments&#8212;for each French word position 1 m, there is a choice of I English positions to connect to.</S>
    <S sid="63" ssid="16">A naive EM implementation will collect n, s, and d counts by considering each alignment, but this is expensive.</S>
    <S sid="64" ssid="17">(By contrast, part-of-speech tagging involves a single alignment, leading to 0(m) training).</S>
    <S sid="65" ssid="18">Lacking a polynomial reformulation, Brown et al. (1993) decided to collect counts only over a subset of likely alignments.</S>
    <S sid="66" ssid="19">To bootstrap, they required some initial idea of what alignments are reasonable, so they began with several iterations of a simpler channel model (called Model 1) that has nicer computational properties.</S>
    <S sid="67" ssid="20">In the following description of Model 1, we represent an alignment formally as a vector al, .</S>
    <S sid="68" ssid="21">. with values al ranging over English word positions 1 ... 1.. Model 1 Channel Parameters: c(m 1/) and s(f le).</S>
    <S sid="69" ssid="22">Given a source sentence e of length 1: Because the same e may produce the same f by means of many different alignments, we must sum over all of them to obtain P(fle): Figure 2 illustrates naive EM training for Model 1.</S>
    <S sid="70" ssid="23">If we compute P(fle) once per iteration, outside the &amp;quot;for a&amp;quot; loops, then the complexity is 0(m/m) per sentence pair, per iteration.</S>
    <S sid="71" ssid="24">More efficient 0(/m) training was devised by Brown et al. (1993).</S>
    <S sid="72" ssid="25">Instead of proWe next consider decoding.</S>
    <S sid="73" ssid="26">We seek a string e that maximizes P(e If), or equivalently maximizes P(e) &#8226; P(fle).</S>
    <S sid="74" ssid="27">A naive algorithm would evaluate all possible source strings, whose lengths are potentially unbounded.</S>
    <S sid="75" ssid="28">If we limit our search to strings at most twice the length m of our observed French, then we have a naive 0(m2v2m) method: Given a string f of length m We may now hope to find a way of reorganizing this computation, using tricks like the ones above.</S>
    <S sid="76" ssid="29">Unfortunately, we are unlikely to succeed, as we now show.</S>
    <S sid="77" ssid="30">For proof purposes, we define our optimization problem with an associated yes-no decision problem:</S>
  </SECTION>
  <SECTION title="Definition: M1-OPTIMIZE" number="6">
    <S sid="78" ssid="1">Given a string f of length m and a set of parameter tables (b, E, s), return a string e of length 1 &lt; 2m that maximizes P(elf), or equivalently maximizes Given a string f of length m, a set of parameter tables (b, &#8364;, s), and a real number k, does there exist a string e of length / &lt; 2m such that P(e) &#8226; P(fle) &gt; k?</S>
    <S sid="79" ssid="2">We will leave the relationship between these two problems somewhat open and intuitive, noting only that M1-DECIDE's intractability does not bode well for MlOPTIMIZE.</S>
    <S sid="80" ssid="3">To show inclusion in NP, we need only nondeterministically choose e for any problem instance and verify that it has the requisite P(e) &#8226; P(fle) in 0(m2) time.</S>
    <S sid="81" ssid="4">Next we give separate polynomial-time reductions from two NP-complete problems.</S>
    <S sid="82" ssid="5">Each reduction highlights a different source of complexity.</S>
    <S sid="83" ssid="6">The Hamilton Circuit Problem asks: given a directed graph G with vertices labeled 0, , n, does G have a path that visits each vertex exactly once and returns to its starting point?</S>
    <S sid="84" ssid="7">We transform any Hamilton Circuit instance into an M1-DECIDE instance as follows.</S>
    <S sid="85" ssid="8">First, we create a French vocabulary fn, associating word fi with vertex i in the graph.</S>
    <S sid="86" ssid="9">We create a slightly larger English vocabulary eo, 'en, with eo serving as the &amp;quot;boundary&amp;quot; word for source model scoring.</S>
    <S sid="87" ssid="10">Ultimately, we will ask Ml-DECIDE to decode the string fi . fn.</S>
    <S sid="88" ssid="11">We create channel model tables as follows: These tables ensure that any decoding e of fi &#8211;ft, will contain the n words el, &#8226; &#8226; &#8226; , en (in some order).</S>
    <S sid="89" ssid="12">We now create a source model.</S>
    <S sid="90" ssid="13">For every pair (i,j) such that 0 &lt; i,j &lt; n: Finally, we set k to zero.</S>
    <S sid="91" ssid="14">To solve a Hamilton Circuit Problem, we transform it as above (in quadratic time), then invoke Ml-DECIDE with inputs b, e, s, k, and fi .</S>
    <S sid="92" ssid="15">&#8226; If M1-DECIDE returns yes, then there must be some string e with both P(e) and P(fle) nonzero.</S>
    <S sid="93" ssid="16">The channel model lets us conclude that if P(fle) is nonzero, then e contains the n words el, , en in some order.</S>
    <S sid="94" ssid="17">If P(e) is nonzero, then every bigram in e (including the two boundary bigrams involving eo) has nonzero probability.</S>
    <S sid="95" ssid="18">Because each English word in e corresponds to a unique vertex, we can use the order of words in e to produce an ordering of vertices in G. We append vertex 0 to the beginning and end of this list to produce a Hamilton Circuit.</S>
    <S sid="96" ssid="19">The source model construction guarantees an edge between each vertex and the next.</S>
    <S sid="97" ssid="20">If Ml-DECIDE returns no, then we know that every string e includes at least one zero value in the computation of either P(e) or P(fle).</S>
    <S sid="98" ssid="21">From any proposed Hamilton Circuit&#8212;i.e., some ordering of vertices in G&#8212;we can construct a string e using the same ordering.</S>
    <S sid="99" ssid="22">This e will have P(fle) = 1 according to the channel model.</S>
    <S sid="100" ssid="23">Therefore, P(e) = 0.</S>
    <S sid="101" ssid="24">By the source model, this can only happen if the proposed &amp;quot;circuit&amp;quot; is actually broken somewhere.</S>
    <S sid="102" ssid="25">So no Hamilton Circuit exists.</S>
    <S sid="103" ssid="26">Figure 3 illustrates the intuitive correspondence between selecting a good word order and finding a Hamilton Circuit.</S>
    <S sid="104" ssid="27">We note that Brew (1992) discusses the NPcompleteness of a related problem, that of finding some permutation of a string that is acceptable to a given context-free grammar.</S>
    <S sid="105" ssid="28">Both of these results deal with decision problems.</S>
    <S sid="106" ssid="29">Returning to optimization, we recall another circuit task called the Traveling Selecting a good source word order is like solving the Hamilton Circuit Problem.</S>
    <S sid="107" ssid="30">If we assume that the channel model offers deterministic, word-for-word translations, then the bigram source model takes responsibility for ordering them.</S>
    <S sid="108" ssid="31">Some word pairs in the source language may be illegal.</S>
    <S sid="109" ssid="32">In that case, finding a legal word ordering is like finding a complete circuit in a graph.</S>
    <S sid="110" ssid="33">(In the graph shown above, a sample circuit is boundary &#8212;&gt; this year comma &#8212;&gt; my &#8212;4 birthday falls &#8212;&gt; on a Thursday &#8212;&gt; boundary).</S>
    <S sid="111" ssid="34">If word pairs have probabilities attached to them, then word ordering resembles the finding the least-cost circuit, also known as the Traveling Salesman Problem.</S>
    <S sid="112" ssid="35">Salesman Problem.</S>
    <S sid="113" ssid="36">It introduces edge costs c111 and seeks a minimum-cost circuit.</S>
    <S sid="114" ssid="37">By viewing edge costs as log probabilities, we can cast the Traveling Salesman Problem as one of optimizing P(e), that is, of finding the best source word order in Model 1 decoding.</S>
    <S sid="115" ssid="38">4.2 Reduction 2 (from Minimum Set Cover Problem) The Minimum Set Cover Problem asks: given a collection C of subsets of finite set S. and integer n, does C contain a cover for S of size &lt; n, i.e., a subcollection whose union is S?</S>
    <S sid="116" ssid="39">We now transform any instance of Minimum Set Cover into an instance of M1-DECIDE, using polynomial time.</S>
    <S sid="117" ssid="40">This time, we assume a rather neutral source model in which all strings of a given length are equally likely, but we construct a more complex channel.</S>
    <S sid="118" ssid="41">We first create a source word e, for each subset in C, and let g, be the size of that subset.</S>
    <S sid="119" ssid="42">We create a table b(e,jej) with values set uniformly to the reciprocal of the source vocabulary size (i.e., the number of subsets in C).</S>
    <S sid="120" ssid="43">Assuming S has m elements, we next create target words ,fin corresponding to each of those elements, and set up channel model tables as follows: 1/g, if the element in S corresponding to j5 is also in the subset s(fi le,) = corresponding to e, Finally, we set k to zero.</S>
    <S sid="121" ssid="44">This completes the reduction.</S>
    <S sid="122" ssid="45">To solve an instance of Minimum Set Cover in polynomial time, we transform it as above, then call MlDECIDE with inputs b, E, s, k, and the words &#8226; &#8226; &#8226; n in any order.</S>
    <S sid="123" ssid="46">Selecting a concise set of source words is like solving the Minimum Set Cover Problem.</S>
    <S sid="124" ssid="47">A channel model with overlapping, one-to-many dictionary entries will typically license many decodings.</S>
    <S sid="125" ssid="48">The source model may prefer short decodings over long ones.</S>
    <S sid="126" ssid="49">Searching for a decoding of length &lt; n is difficult, resembling the problem of covering a finite set with a small collection of subsets.</S>
    <S sid="127" ssid="50">In the example shown above, the smallest acceptable set of source words is {and, cooked, however, left, comma, period} .</S>
    <S sid="128" ssid="51">If M1-DECIDE returns yes, then some decoding e with P(e) &#8226; P(fle) &gt; 0 must exist.</S>
    <S sid="129" ssid="52">We know that e must contain n or fewer words&#8212;otherwise P(fle) = 0 by the E table.</S>
    <S sid="130" ssid="53">Furthermore, the s table tells us that every word fi is covered by at least one English word in e. Through the one-to-one correspondence between elements of e and C, we produce a set cover of size &lt; n for S. Likewise, if M1-DECIDE returns no, then all decodings have P(e) &#8226; P(fle) = 0.</S>
    <S sid="131" ssid="54">Because there are no zeroes in the source table b, every e has P(fle) = 0.</S>
    <S sid="132" ssid="55">Therefore either (1) the length of e exceeds n, or (2) some fi is left uncovered by the words in e. Because source words cover target words in exactly the same fashion as elements of C cover S, we conclude that there is no set cover of size &lt; n for S. Figure 4 illustrates the intuitive correspondence between source word selection and minimum set covering.</S>
  </SECTION>
  <SECTION title="5." number="7">
    <S sid="133" ssid="1">The two proofs point up separate factors in MT decoding complexity.</S>
    <S sid="134" ssid="2">One is wordorder selection.</S>
    <S sid="135" ssid="3">But even if any word order will do, there is still the problem of picking a concise decoding in the face of overlapping bilingual dictionary entries.</S>
    <S sid="136" ssid="4">The former is more closely tied to the source model, and the latter to the channel model, though the complexity arises from the interaction of the two.</S>
    <S sid="137" ssid="5">We should note that Model 1 is an intentionally simple translation model, one whose primary purpose in machine translation has been to allow bootstrapping into more complex translation models (e.g., IBM Models 2-5).</S>
    <S sid="138" ssid="6">It is easy to show that the intractability results also apply to stronger &amp;quot;fertility/distortion&amp;quot; models; we assign zero probability to fertilities other than 1, and we set up uniform distortion tables.</S>
    <S sid="139" ssid="7">Simple translation models like Model 1 find more direct use in other applications (e.g., lexicon construction, idiom detection, psychological norms, and cross-language information retrieval), so their computational properties are of wider interest.</S>
    <S sid="140" ssid="8">The proofs we presented are based on a worst-case analysis.</S>
    <S sid="141" ssid="9">Real s, e, and b tables may have properties that permit faster optimal decoding than the artificial tables constructed above.</S>
    <S sid="142" ssid="10">It is also possible to devise approximation algorithms like those devised for other NP-complete problems.</S>
    <S sid="143" ssid="11">To the extent that word ordering is like solving the Traveling Salesman Problem, it is encouraging substantial progress continues to be made on Traveling Salesman algorithms.</S>
    <S sid="144" ssid="12">For example, it is often possible to get within two percent of the optimal tour in practice, and some researchers have demonstrated an optimal tour of over 13,000 U.S. cities.</S>
    <S sid="145" ssid="13">(The latter experiment relied on things like distance symmetry and the triangle inequality constraint, however, which do not hold in word ordering.)</S>
    <S sid="146" ssid="14">So far, statistical translation research has either opted for heuristic beam-search algorithms or different channel models.</S>
    <S sid="147" ssid="15">For example, some researchers avoid bag generation by preprocessing bilingual texts to remove word-order differences, while others adopt channels that eliminate syntactically unlikely alignments.</S>
    <S sid="148" ssid="16">Finally, expensive decoding also suggests expensive training from unannotated (monolingual) texts, which presents a challenging bottleneck for extending statistical machine translation to language pairs and domains where large bilingual corpora do not exist.</S>
  </SECTION>
</PAPER>
