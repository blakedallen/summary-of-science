<PAPER>
  <S sid="0">Unsupervised Multilingual Learning for Morphological Segmentation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">For centuries, the deep connection between languages has brought about major discoveries about human communication.</S>
    <S sid="2" ssid="2">In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning.</S>
    <S sid="3" ssid="3">In particular, we study the task of morphological segmentation of multiple languages.</S>
    <S sid="4" ssid="4">We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme pator We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English.</S>
    <S sid="5" ssid="5">Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models.</S>
    <S sid="6" ssid="6">Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">For centuries, the deep connection between human languages has fascinated linguists, anthropologists and historians (Eco, 1995).</S>
    <S sid="8" ssid="2">The study of this connection has made possible major discoveries about human communication: it has revealed the evolution of languages, facilitated the reconstruction of proto-languages, and led to understanding language universals.</S>
    <S sid="9" ssid="3">The connection between languages should be a powerful source of information for automatic linguistic analysis as well.</S>
    <S sid="10" ssid="4">In this paper we investigate two questions: (i) Can we exploit cross-lingual correspondences to improve unsupervised language learning?</S>
    <S sid="11" ssid="5">(ii) Will this joint analysis provide more or less benefit when the languages belong to the same family?</S>
    <S sid="12" ssid="6">We study these two questions in the context of unsupervised morphological segmentation, the automatic division of a word into morphemes (the basic units of meaning).</S>
    <S sid="13" ssid="7">For example, the English word misunderstanding would be segmented into mis understand - ing.</S>
    <S sid="14" ssid="8">This task is an informative testbed for our exploration, as strong correspondences at the morphological level across various languages have been well-documented (Campbell, 2004).</S>
    <S sid="15" ssid="9">The model presented in this paper automatically induces a segmentation and morpheme alignment from a multilingual corpus of short parallel phrases.1 For example, given parallel phrases meaning in my land in English, Arabic, Hebrew, and Aramaic, we wish to segment and align morphemes as follows:</S>
  </SECTION>
  <SECTION title="Arabic: Hebrew: Aramaic:" number="2">
    <S sid="16" ssid="1">This example illustrates the potential benefits of unsupervised multilingual learning.</S>
    <S sid="17" ssid="2">The three Semitic languages use cognates (words derived from a common ancestor) to represent the word land.</S>
    <S sid="18" ssid="3">They also use an identical suffix (-y) to represent the first person possessive pronoun (my).</S>
    <S sid="19" ssid="4">These similarities in form should guide the model by constraining the space of joint segmentations.</S>
    <S sid="20" ssid="5">The corresponding English phrase lacks this resemblance to its Semitic counterparts.</S>
    <S sid="21" ssid="6">However, in this as in many cases, no segmentation is required for English as all the morphemes are expressed as individual words.</S>
    <S sid="22" ssid="7">For this reason, English should provide a strong source of disambiguation for highly inflected languages, such as Arabic and Hebrew.</S>
    <S sid="23" ssid="8">In general, we pose the following question.</S>
    <S sid="24" ssid="9">In which scenario will multilingual learning be most effective?</S>
    <S sid="25" ssid="10">Will it be for related languages, which share a common core of linguistic features, or for distant languages, whose linguistic divergence can provide strong sources of disambiguation?</S>
    <S sid="26" ssid="11">As a first step towards answering this question, we propose a model which can take advantage of both similarities and differences across languages.</S>
    <S sid="27" ssid="12">This joint bilingual model identifies optimal morphemes for two languages and at the same time finds compact multilingual representations.</S>
    <S sid="28" ssid="13">For each language in the pair, the model favors segmentations which yield high frequency morphemes.</S>
    <S sid="29" ssid="14">Moreover, bilingual morpheme pairs which consistently share a common semantic or syntactic function are treated as abstract morphemes, generated by a single language-independent process.</S>
    <S sid="30" ssid="15">These abstract morphemes are induced automatically by the model from recurring bilingual patterns.</S>
    <S sid="31" ssid="16">For example, in the case above, the tuple (in, fy, b-, b-) would constitute one of three abstract morphemes in the phrase.</S>
    <S sid="32" ssid="17">When a morpheme occurs in one language without a direct counterpart in the other language, our model can explain away the stray morpheme as arising through a language-specific process.</S>
    <S sid="33" ssid="18">To achieve this effect in a probabilistic framework, we formulate a hierarchical Bayesian model with Dirichlet Process priors.</S>
    <S sid="34" ssid="19">This framework allows us to define priors over the infinite set of possible morphemes in each language.</S>
    <S sid="35" ssid="20">In addition, we define a prior over abstract morphemes.</S>
    <S sid="36" ssid="21">This prior can incorporate knowledge of the phonetic relationship between the two alphabets, giving potential cognates greater prior likelihood.</S>
    <S sid="37" ssid="22">The resulting posterior distributions concentrate their probability mass on a small group of recurring and stable patterns within and between languages.</S>
    <S sid="38" ssid="23">We test our model on a multilingual corpus of short parallel phrases drawn from the Hebrew Bible and Arabic, Aramaic, and English translations.</S>
    <S sid="39" ssid="24">The Semitic language family, of which Hebrew, Arabic, and Aramaic are members, is known for a highly productive morphology (Bravmann, 1977).</S>
    <S sid="40" ssid="25">Our results indicate that cross-lingual patterns can indeed be exploited successfully for the task of unsupervised morphological segmentation.</S>
    <S sid="41" ssid="26">When modeled in tandem, gains are observed for all language pairs, reducing relative error by as much as 24%.</S>
    <S sid="42" ssid="27">Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning.</S>
    <S sid="43" ssid="28">However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="3">
    <S sid="44" ssid="1">Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to semantic role labeling (Yarowsky et al., 2000; Diab and Resnik, 2002; Xi and Hwa, 2005; Pad&#180;o and Lapata, 2006).</S>
    <S sid="45" ssid="2">Most of this research assumes that one language has annotations for the task of interest.</S>
    <S sid="46" ssid="3">Given a parallel corpus, the annotations are projected from this source language to its counterpart, and the resulting annotations are used for supervised training in the target language.</S>
    <S sid="47" ssid="4">In fact, Rogati et al., (2003) employ this method to learn arabic morphology assuming annotations provided by an English stemmer.</S>
    <S sid="48" ssid="5">An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006).</S>
    <S sid="49" ssid="6">While their approach does not require a parallel corpus it does assume the availability of annotations in one language.</S>
    <S sid="50" ssid="7">Rather than being fully projected, the source annotations provide co-occurrence statistics used by a model in the resource-poor target language.</S>
    <S sid="51" ssid="8">The key assumption here is that certain distributional properties are invariant across languages from the same language families.</S>
    <S sid="52" ssid="9">An example of such a property is the distribution of part-of-speech bigrams.</S>
    <S sid="53" ssid="10">Hana et al., (2004) demonstrate that adding such statistics from an annotated Czech corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version.</S>
    <S sid="54" ssid="11">The approach presented here differs from previous work in two significant ways.</S>
    <S sid="55" ssid="12">First, we do not assume supervised data in any of the languages.</S>
    <S sid="56" ssid="13">Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time.</S>
    <S sid="57" ssid="14">This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language.</S>
    <S sid="58" ssid="15">Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007).</S>
    <S sid="59" ssid="16">Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution.</S>
    <S sid="60" ssid="17">The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings.</S>
    <S sid="61" ssid="18">Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007).</S>
    <S sid="62" ssid="19">In these approaches, models with short description length are preferred.</S>
    <S sid="63" ssid="20">Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon.</S>
    <S sid="64" ssid="21">A locally optimal segmentation is identified using a task-specific greedy search.</S>
    <S sid="65" ssid="22">In contrast to previous approaches, our model induces morphological segmentation for multiple related languages simultaneously.</S>
    <S sid="66" ssid="23">By representing morphemes abstractly through the simultaneous alignment and segmentation of data in two languages, our algorithm capitalizes on deep connections between morpheme usage across different languages.</S>
  </SECTION>
  <SECTION title="3 Multilingual Morphological Segmentation" number="4">
    <S sid="67" ssid="1">The underlying assumption of our work is that structural commonality across different languages is a powerful source of information for morphological analysis.</S>
    <S sid="68" ssid="2">In this section, we provide several examples that motivate this assumption.</S>
    <S sid="69" ssid="3">The main benefit of joint multilingual analysis is that morphological structure ambiguous in one language is sometimes explicitly marked in another language.</S>
    <S sid="70" ssid="4">For example, in Hebrew, the preposition meaning &#8220;in&#8221;, b-, is always prefixed to its nominal argument.</S>
    <S sid="71" ssid="5">On the other hand, in Arabic, the most common corresponding particle is fy, which appears as a separate word.</S>
    <S sid="72" ssid="6">By modeling crosslingual morpheme alignments while simultaneously segmenting, the model effectively propagates information between languages and in this case would be encouraged to segment the Hebrew prefix b-.</S>
    <S sid="73" ssid="7">Cognates are another important means of disambiguation in the multilingual setting.</S>
    <S sid="74" ssid="8">Consider translations of the phrase &#8220;...and they wrote it...&#8221;: In both languages, the triliteral root ktb is used to express the act of writing.</S>
    <S sid="75" ssid="9">By considering the two phrases simultaneously, the model can be encouraged to split off the respective Hebrew and Arabic prefixes w- and f- in order to properly align the cognate root ktb.</S>
    <S sid="76" ssid="10">In the following section, we describe a model that can model both generic cross-lingual patterns (fy and b-), as well as cognates between related languages (ktb for Hebrew and Arabic).</S>
  </SECTION>
  <SECTION title="4 Model" number="5">
    <S sid="77" ssid="1">Overview In order to simultaneously model probabilistic dependencies across languages as well as morpheme distributions within each language, we employ a hierarchical Bayesian model.2 Our segmentation model is based on the notion that stable recurring string patterns within words are indicative of morphemes.</S>
    <S sid="78" ssid="2">In addition to learning independent morpheme patterns for each language, the model will prefer, when possible, to join together frequently occurring bilingual morpheme pairs into single abstract morphemes.</S>
    <S sid="79" ssid="3">The model is fully unsupervised and is driven by a preference for stable and high frequency cross-lingual morpheme patterns.</S>
    <S sid="80" ssid="4">In addition the model can incorporate character-to-character phonetic correspondences between alphabets as prior information, thus allowing the implicit modeling of cognates.</S>
    <S sid="81" ssid="5">Our aim is to induce a model which concentrates probability on highly frequent patterns while still allowing for the possibility of those previously unseen.</S>
    <S sid="82" ssid="6">Dirichlet processes are particularly suitable for such conditions.</S>
    <S sid="83" ssid="7">In this framework, we can encode prior knowledge over the infinite sets of possible morpheme strings as well as abstract morphemes.</S>
    <S sid="84" ssid="8">Distributions drawn from a Dirichlet process nevertheless produce sparse representations with most probability mass concentrated on a small number of observed and predicted patterns.</S>
    <S sid="85" ssid="9">Our model utilizes a Dirichlet process prior for each language, as well as for the cross-lingual links (abstract morphemes).</S>
    <S sid="86" ssid="10">Thus, a distribution over morphemes and morpheme alignments is first drawn from the set of Dirichlet processes and then produces the observed data.</S>
    <S sid="87" ssid="11">In practice, we never deal with such distributions directly, but rather integrate over them during Gibbs sampling.</S>
    <S sid="88" ssid="12">In the next section we describe our model&#8217;s &#8220;generative story&#8221; for producing the data we observe.</S>
    <S sid="89" ssid="13">We formalize our model in the context of two languages &#163; and F. However, the formulation can be extended to accommodate evidence from multiple languages as well.</S>
    <S sid="90" ssid="14">We provide an example of parallel phrase generation in Figure 1.</S>
    <S sid="91" ssid="15">High-level Generative Story We have a parallel corpus of several thousand short phrases in the two languages &#163; and F. Our model provides a generative story explaining how these parallel phrases were probabilistically created.</S>
    <S sid="92" ssid="16">The core of the model consists of three components: a distribution A over bilingual morpheme pairs (abstract morphemes), a distribution E over stray morphemes in language &#163; occurring without a counterpart in language F, and a similar distribution F for stray morphemes in language F. As usual for hierarchical Bayesian models, the generative story begins by drawing the model parameters themselves &#8211; in our case the three distributions A, E, and F. These three distributions are drawn from three separate Dirichlet processes, each with appropriately defined base distributions.</S>
    <S sid="93" ssid="17">The Dirichlet processes ensure that the resulting distributions concentrate their probability mass on a small number of morphemes while holding out reasonable probability for unseen possibilities.</S>
    <S sid="94" ssid="18">Once A, E, and F have been drawn, we model our parallel corpus of short phrases as a series of independent draws from a phrase-pair generation model.</S>
    <S sid="95" ssid="19">For each new phrase-pair, the model first chooses the number and type of morphemes to be generated.</S>
    <S sid="96" ssid="20">In particular, it must choose how many unaligned stray morphemes from language &#163;, unaligned stray morphemes from language F, and abstract morphemes are to compose the parallel phrases.</S>
    <S sid="97" ssid="21">These three numbers, respectively denoted as m, n, and k, are drawn from a Poisson distribution.</S>
    <S sid="98" ssid="22">This step is illustrated in Figure 1 part (a).</S>
    <S sid="99" ssid="23">The model then proceeds to independently draw m language &#163; morphemes from distribution E, n language-F morphemes from distribution F, and k abstract morphemes from distribution A.</S>
    <S sid="100" ssid="24">This step is illustrated in part (b) of Figure 1.</S>
    <S sid="101" ssid="25">The m + k resulting language-&#163; morphemes are then ordered and fused to form a phrase in language &#163;, and likewise for the n + k resulting languageF morphemes.</S>
    <S sid="102" ssid="26">The ordering and fusing decisions are modeled as draws from a uniform distribution over the set of all possible orderings and fusings for sizes m, n, and k. These final steps are illustrated in parts (c)-(d) of Figure 1.</S>
    <S sid="103" ssid="27">Now we describe the model more formally.</S>
    <S sid="104" ssid="28">Stray Morpheme Distributions Sometimes a morpheme occurs in a phrase in one language without a corresponding foreign language morpheme in the parallel phrase.</S>
    <S sid="105" ssid="29">We call these &#8220;stray morphemes,&#8221; and we employ language-specific morpheme distributions to model their generation.</S>
    <S sid="106" ssid="30">For each language, we draw a distribution over all possible morphemes (finite-length strings composed of characters in the appropriate alphabet) from a Dirichlet process with concentration parameter &#945; and base distribution Pe or Pf respectively: The base distributions Pe and Pf can encode prior knowledge about the properties of morphemes in each of the two languages, such as length and character n-grams.</S>
    <S sid="107" ssid="31">For simplicity, we use a geometric distribution over the length of the string with a final end-morpheme character.</S>
    <S sid="108" ssid="32">The distributions E and F which result from the respective Dirichlet processes place most of their probability mass on a small number of morphemes with the degree of concentration As before, the resulting distribution A will give non-zero probability to all abstract morphemes (e, f).</S>
    <S sid="109" ssid="33">The base distribution acts as a prior on such pairs.</S>
    <S sid="110" ssid="34">To define we can simply use a mixture of geometric distributions in the lengths of the component morphemes.</S>
    <S sid="111" ssid="35">However, if the languages &#163; and are related and the regular phonetic correspondences between the letter in the two alphabets are known, then we can use to assign higher likelihood to potential cognates.</S>
    <S sid="112" ssid="36">In particular we define the prior f) to be the probabilistic string-edit distance (Ristad and Yianilos, 1998) between and the known phonetic correspondences to parameterize the string-edit model.</S>
    <S sid="113" ssid="37">In particular, insertion and deletion probabilities are held constant for all characters, and substitution probabilities are determined based on the known sound correspondences.</S>
    <S sid="114" ssid="38">We report results for both the simple geometric prior as well as the string-edit prior.</S>
    <S sid="115" ssid="39">Phrase Generation To generate a bilingual parallel phrase, we first draw m, n, and k independently from a Poisson distribution.</S>
    <S sid="116" ssid="40">These three integers represent the number and type of the morphemes d the number of coupled bilingual morpheme pairs, respectively. controlled by the prior Nevertheless, some nonzero probability is reserved for every possible string.</S>
    <S sid="117" ssid="41">We note that these single-language morpheme distributions also serve as monolingual segmentation models, and similar models have been successfully applied to the task of word boundary detection (Goldwater et al., 2006).</S>
    <S sid="118" ssid="42">Abstract Morpheme Distribution To model the connections between morphemes across languages, we further define a model for bilingual morpheme pairs, or abstract morphemes.</S>
    <S sid="119" ssid="43">This model assigns probabilities to all pairs of morphemes is, all pairs of finite strings from the respective alphabets (e, f).</S>
    <S sid="120" ssid="44">Intuitively, we wish to assign high probability to pairs of morphemes that play similar syntactic or semantic roles (e.g.</S>
    <S sid="121" ssid="45">(fy, b-) for in Arabic and Hebrew).</S>
    <S sid="122" ssid="46">These morpheme pairs can thus be viewed as representing abstract morphemes.</S>
    <S sid="123" ssid="47">As with the stray morpheme models, we wish to define a distribution which concentrates probability mass on a small number of highly co-occurring morpheme pairs while still holding out some probability for all other pairs.</S>
    <S sid="124" ssid="48">We define this abstract morpheme model A as a draw from an that compose the parallel phrase, giving the number of stray morphemes in each language &#163; and an m, n, k ti Given these values, we now draw the appropriate number of stray an d abstract morphemes from the corresponding distributions: e1, ..., em &#8764; E f1, ..., fn &#8764; F (ei, fi), ..., (ek, f&#65533;k) &#8764; A The sets of morphemes drawn for each language are then ordered: &#732;e1, ..., &#732;em+k &#8764; ORDER|e1, ..., em, ei, ..., ek &#732;f1, ..., &#732;fn+k &#8764; ORDER|f1, ..., fn, f1, ..., fk Finally the ordered morphemes are fused into the words that form the parallel phrases: To keep the model as simple as possible, we employ uniform distributions over the sets of orderings and fusings.</S>
    <S sid="125" ssid="49">In other words, given a set of r morphemes (for each language), we define the distribution over permutations of the morphemes to simply be ORDER(&#183;|r) = 1&#65533;&#65533;.</S>
    <S sid="126" ssid="50">Then, given a fixed morpheme order, we consider fusing each adjacent morpheme into a single word.</S>
    <S sid="127" ssid="51">Again, we simply model the distribution over the r &#8722; 1 fusing decisions uniformly as FUSE(&#183;|r) = 1 2''&#8722;1 .</S>
    <S sid="128" ssid="52">Implicit Alignments Note that nowhere do we explicitly assign probabilities to morpheme alignments between parallel phrases.</S>
    <S sid="129" ssid="53">However, our model allows morphemes to be generated in precisely one of two ways: as a lone stray morpheme or as part of a bilingual abstract morpheme pair.</S>
    <S sid="130" ssid="54">Thus, our model implicitly assumes that each morpheme is either unaligned, or aligned to exactly one morpheme in the opposing language.</S>
    <S sid="131" ssid="55">If we are given a parallel phrase with already segmented morphemes we can easily induce the distribution over alignments implied by our model.</S>
    <S sid="132" ssid="56">As we will describe in the next section, drawing from these induced alignment distributions plays a crucial role in our inference procedure.</S>
    <S sid="133" ssid="57">Inference Given our corpus of short parallel bilingual phrases, we wish to make segmentation decisions which yield a set of morphemes with high joint probability.</S>
    <S sid="134" ssid="58">To assess the probability of a potential morpheme set, we need to marginalize over all possible alignments (i.e. possible abstract morpheme pairings and stray morpheme assignments).</S>
    <S sid="135" ssid="59">We also need to marginalize over all possible draws of the distributions A, E, and F from their respective Dirichlet process priors.</S>
    <S sid="136" ssid="60">We achieve these aims by performing Gibbs sampling.</S>
    <S sid="137" ssid="61">Sampling We follow (Neal, 1998) in the derivation of our blocked and collapsed Gibbs sampler.</S>
    <S sid="138" ssid="62">Gibbs sampling starts by initializing all random variables to arbitrary starting values.</S>
    <S sid="139" ssid="63">At each iteration, the sampler selects a random variable Xi, and draws a new value for Xi from the conditional distribution of Xi given the current value of the other variables: P(Xi|X_i).</S>
    <S sid="140" ssid="64">The stationary distribution of variables derived through this procedure is guaranteed to converge to the true joint distribution of the random variables.</S>
    <S sid="141" ssid="65">However, if some variables can be jointly sampled, then it may be beneficial to perform block sampling of these variables to speed convergence.</S>
    <S sid="142" ssid="66">In addition, if a random variable is not of direct interest, we can avoid sampling it directly by marginalizing it out, yielding a collapsed sampler.</S>
    <S sid="143" ssid="67">We utilize variable blocking by jointly sampling multiple segmentation and alignment decisions.</S>
    <S sid="144" ssid="68">We also collapse our Gibbs sampler in the standard way, by using predictive posteriors marginalized over all possible draws from the Dirichlet processes (resulting in Chinese Restaurant Processes).</S>
    <S sid="145" ssid="69">Resampling For each bilingual phrase, we resample each word in the phrase in turn.</S>
    <S sid="146" ssid="70">For word w in language E, we consider at once all possible segmentations, and for each segmentation all possible alignments.</S>
    <S sid="147" ssid="71">We keep fixed the previously sampled segmentation decisions for all other words in the phrase as well as sampled alignments involving morphemes in other words.</S>
    <S sid="148" ssid="72">We are thus considering at once: all possible segmentations of w along with all possible alignments involving morphemes in w with some subset of previously sampled languageF morphemes.3 The sampling formulas are easily derived as products of the relevant Chinese Restaurant Processes (with a minor adjustment to take into account the number of stray and abstract morphemes resulting from each decision).</S>
    <S sid="149" ssid="73">See (Neal, 1998) for general formulas for Gibbs sampling from distributions with Dirichlet process priors.</S>
    <S sid="150" ssid="74">All results reported are averaged over five runs using simulated annealing.</S>
  </SECTION>
  <SECTION title="5 Experimental Set-Up" number="6">
    <S sid="151" ssid="1">Morpheme Definition For the purpose of these experiments, we define morphemes to include conjunctions, prepositional and pronominal affixes, plural and dual suffixes, particles, definite articles, and roots.</S>
    <S sid="152" ssid="2">We do not model cases of infixed morpheme transformations, as those cannot be modeled by linear segmentation.</S>
    <S sid="153" ssid="3">Dataset As a source of parallel data, we use the Hebrew Bible and translations.</S>
    <S sid="154" ssid="4">For the Hebrew version, we use an edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006).</S>
    <S sid="155" ssid="5">This Bible edition is augmented by gold standard morphological analysis (including segmentation) performed by biblical scholars.</S>
    <S sid="156" ssid="6">For the Arabic, Aramaic, and English versions, fied by augmenting the model with a pair of &#8220;morphemeidentity&#8221; variables deterministically drawn from each abstract morpheme.</S>
    <S sid="157" ssid="7">Thus the identity of the drawn morphemes can be retained even while resampling their generation mechanism. we use the Van Dyke Arabic translation,4 Targum Onkelos,5 and the Revised Standard Version (Nelson, 1952), respectively.</S>
    <S sid="158" ssid="8">We obtained gold standard segmentations of the Arabic translation with a hand-crafted Arabic morphological analyzer which utilizes manually constructed word lists and compatibility rules and is further trained on a large corpus of hand-annotated Arabic data (Habash and Rambow, 2005).</S>
    <S sid="159" ssid="9">The accuracy of this analyzer is reported to be 94% for full morphological analyses, and 98%-99% when part-of-speech tag accuracy is not included.</S>
    <S sid="160" ssid="10">We don&#8217;t have gold standard segmentations for the English and Aramaic portions of the data, and thus restrict our evaluation to Hebrew and Arabic.</S>
    <S sid="161" ssid="11">To obtain our corpus of short parallel phrases, we preprocessed each language pair using the Giza++ alignment toolkit.6 Given word alignments for each language pair, we extract a list of phrase pairs that form independent sets in the bipartite alignment graph.</S>
    <S sid="162" ssid="12">This process allows us to group together phrases like fy s.bah. in Arabic and bbqr in Hebrew while being reasonably certain that all the relevant morphemes are contained in the short extracted phrases.</S>
    <S sid="163" ssid="13">The number of words in such phrases ranges from one to four words in the Semitic languages and up to six words in English.</S>
    <S sid="164" ssid="14">Before performing any experiments, a manual inspection of the generated parallel phrases revealed that many infrequent phrase pairs occurred merely as a result of noisy translation and alignment.</S>
    <S sid="165" ssid="15">Therefore, we eliminated all parallel phrases that occur fewer than five times.</S>
    <S sid="166" ssid="16">As a result of this process, we obtain 6,139 parallel short phrases in Arabic, Hebrew, Aramaic, and English.</S>
    <S sid="167" ssid="17">The average number of morphemes per word in the Hebrew data is 1.8 and is 1.7 in Arabic.</S>
    <S sid="168" ssid="18">For the bilingual models which employs probabilistic string-edit distance as a prior on abstract morphemes, we parameterize the string-edit model with the chart of Semitic consonant relationships listed on page xxiv of (Thackston, 1999).</S>
    <S sid="169" ssid="19">All pairs of corresponding letters are given equal substitution probability, while all other letter pairs are given substitution probability of zero.</S>
    <S sid="170" ssid="20">Evaluation Methods Following previous work, we evaluate the performance of our automatic segmentation algorithm using F-score.</S>
    <S sid="171" ssid="21">This measure is the harmonic mean of recall and precision, which are calculated on the basis of all possible segmentation points.</S>
    <S sid="172" ssid="22">The evaluation is performed on a random set of 1/5 of the parallel phrases which is unseen during the training phase.</S>
    <S sid="173" ssid="23">During testing, we do not allow the models to consider any multilingual evidence.</S>
    <S sid="174" ssid="24">This restriction allows us to simulate future performance on purely monolingual data.</S>
    <S sid="175" ssid="25">Baselines Our primary purpose is to compare the performance of our bilingual model with its fully monolingual counterpart.</S>
    <S sid="176" ssid="26">However, to demonstrate the competitiveness of this baseline model, we also provide results using MORFESSOR (Creutz and Lagus, 2007), a state-of-the-art unsupervised system for morphological segmentation.</S>
    <S sid="177" ssid="27">While developed originally for Finnish, this system has been successfully applied to a range of languages including German, Turkish and English.</S>
    <S sid="178" ssid="28">The probabilistic formulation of this model is close to our monolingual segmentation model, but it uses a greedy search specifically designed for the segmentation task.</S>
    <S sid="179" ssid="29">We use the publicly available implementation of this system.</S>
    <S sid="180" ssid="30">To provide some idea of the inherent difficulty of this segmentation task, we also provide results from a random baseline which makes segmentation decisions based on a coin weighted with the true segmentation frequency.</S>
  </SECTION>
  <SECTION title="6 Results" number="7">
    <S sid="181" ssid="1">Table 1 shows the performance of the various automatic segmentation methods.</S>
    <S sid="182" ssid="2">The first three rows provide baselines, as mentioned in the previous section.</S>
    <S sid="183" ssid="3">Our primary baseline is MONOLINGUAL, which is the monolingual counterpart to our model and only uses the language-specific distributions E or F. The next three rows shows the performance of various bilingual models that don&#8217;t use character-tocharacter phonetic correspondences to capture cognate information.</S>
    <S sid="184" ssid="4">We find that with the exception of the HEBREW(+ARAMAIC) pair, the bilingual models show marked improvement over MONOLINGUAL.</S>
    <S sid="185" ssid="5">We notice that in general, adding English &#8211; which has comparatively little morphological ambiguity &#8211; is about as useful as adding a more closely related Semitic language.</S>
    <S sid="186" ssid="6">However, once characterto-character phonetic correspondences are added as an abstract morpheme prior (final two rows), we find the performance of related language pairs outstrips English, reducing relative error over MONOLINGUAL by 10% and 24% for the Hebrew/Arabic pair.</S>
  </SECTION>
  <SECTION title="7 Conclusions and Future Work" number="8">
    <S sid="187" ssid="1">We started out by posing two questions: (i) Can we exploit cross-lingual patterns to improve unsupervised analysis?</S>
    <S sid="188" ssid="2">(ii) Will this joint analysis provide more or less benefit when the languages belong to the same family?</S>
    <S sid="189" ssid="3">The model and results presented in this paper answer the first question in the affirmative, at least for the task of morphological segmentation.</S>
    <S sid="190" ssid="4">We also provided some evidence that considering closely related languages may be more beneficial than distant pairs if the model is able to explicitly represent shared language structure (the characterto-character phonetic correspondences in our case).</S>
    <S sid="191" ssid="5">In the future, we hope to apply similar multilingual models to other core unsupervised analysis tasks, including part-of-speech tagging and grammar induction, and to further investigate the role that language relatedness plays in such models.</S>
    <S sid="192" ssid="6">7</S>
  </SECTION>
</PAPER>
