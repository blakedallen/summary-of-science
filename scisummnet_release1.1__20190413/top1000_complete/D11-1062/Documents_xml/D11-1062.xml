<PAPER>
  <S sid="0">Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing.</S>
    <S sid="2" ssid="2">Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets.</S>
    <S sid="3" ssid="3">In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: the scarcity of data available to train evaluate systems, and the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality.</S>
    <S sid="4" ssid="4">We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators.</S>
    <S sid="5" ssid="5">The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Cross-lingual Textual Entailment (CLTE) has been recently proposed by (Mehdad et al., 2010; Mehdad et al., 2011) as an extension of Textual Entailment (Dagan and Glickman, 2004).</S>
    <S sid="7" ssid="2">The task consists of deciding, given a text (T) and an hypothesis (H) in different languages, if the meaning of H can be inferred from the meaning of T. As in other NLP applications, both for monolingual and cross-lingual TE,</S>
  </SECTION>
  <SECTION title="Alessandro Marchetti CELCT Trento, Italy" number="2">
    <S sid="8" ssid="1">the availability of large quantities of annotated data is an enabling factor for systems development and evaluation.</S>
    <S sid="9" ssid="2">Until now, however, the scarcity of such data on the one hand, and the costs of creating new datasets of reasonable size on the other, have represented a bottleneck for a steady advancement of the state of the art.</S>
    <S sid="10" ssid="3">In the last few years, monolingual TE corpora for English and other European languages have been created and distributed in the framework of several evaluation campaigns, including the RTE Challenge1, the Answer Validation Exercise at CLEF2, and the Textual Entailment task at EVALITA3.</S>
    <S sid="11" ssid="4">Despite the differences in the design of the tasks, all the released datasets were collected through similar procedures, always involving expensive manual work done by expert annotators.</S>
    <S sid="12" ssid="5">Moreover, in the data creation process, large amounts of hand-crafted T-H pairs often have to be discarded in order to retain only those featuring full agreement, in terms of the assigned entailment judgements, among multiple annotators.</S>
    <S sid="13" ssid="6">The amount of discarded pairs is usually high, contributing to increase the costs of creating textual entailment datasets4.</S>
    <S sid="14" ssid="7">The issues related to the shortage of datasets and the high costs for their creation are more evident in the CLTE scenario, where: i) the only dataset currently available is an English-Spanish corpus obtained by translating the RTE-3 corpus (Negri and Mehdad, 2010), and ii) the application of the standard methods adopted to build RTE pairs requires proficiency in multiple languages, thus significantly increasing the costs of the data creation process.</S>
    <S sid="15" ssid="8">To address these issues, in this paper we devise a cost-effective methodology to create cross-lingual textual entailment corpora.</S>
    <S sid="16" ssid="9">In particular, we focus on the following problems: (1) Is it possible to collect T-H pairs minimizing the intervention of expert annotators?</S>
    <S sid="17" ssid="10">To address this question, we explore the feasibility of crowdsourcing the corpus creation process.</S>
    <S sid="18" ssid="11">As a contribution beyond the few works on TE/CLTE data acquisition, we define an effective methodology that: i) does not involve experts in the most complex (and costly) stages of the process, ii) does not require preprocessing tools, and iii) does not rely on the availability of already annotated RTE corpora. to non-experts, difficult to accomplish, and not suitable for the application of the quality-check mechanisms provided by current crowdsourcing services.</S>
    <S sid="19" ssid="12">Our &#8220;divide and conquer&#8221; solution represents the first attempt to address a complex task involving content generation and labelling through the definition of a cheap and reliable pipeline of simple tasks which are easy to define, accomplish, and control. guages.</S>
    <S sid="20" ssid="13">Moreover, since the core monolingual tasks of the process are carried out by manipulating English texts, we are able to address the very large community of English speaking workers, with a considerable reduction of costs and execution time.</S>
    <S sid="21" ssid="14">Finally, as a by-product of our method, the acquired pairs are fully aligned for all language combinations, thus enabling meaningful comparisons between scenarios of different complexity (monolingual TE, and CLTE between close or distant languages).</S>
    <S sid="22" ssid="15">We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora (Sammons et al., 2010; Bentivogli et al., 2010), the proposed approach and the resulting dataset5 will contribute to meeting the strong need for resources to develop and evaluate novel solutions for textual entailment.</S>
  </SECTION>
  <SECTION title="2 Related Works" number="3">
    <S sid="23" ssid="1">Crowdsourcing services, such as Amazon Mechanical Turk6 (MTurk) and CrowdFlower7, have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010).</S>
    <S sid="24" ssid="2">The idea is that the acquisition and annotation of large amounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence Tasks (HITs) routed to a crowd of non-expert workers (aka &#8220;Turkers&#8221;) hired through on-line marketplaces.</S>
    <S sid="25" ssid="3">As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al., 2008), which shows high agreement between non-expert annotations of the RTE-1 dataset and existing gold standard labels assigned by expert labellers.</S>
    <S sid="26" ssid="4">Focusing on the actual generation of monolingual entailment pairs, (Wang and Callison-Burch, 2010) experiments the use of MTurk to collect facts and counter facts related to texts extracted from an existing RTE corpus annotated with named entities.</S>
    <S sid="27" ssid="5">Taking a step beyond the task of annotating existing datasets, and showing the feasibility of involving non-experts also in the generation of TE pairs, this approach is more relevant to our objectives.</S>
    <S sid="28" ssid="6">However, at least two major differences with our work have to be remarked.</S>
    <S sid="29" ssid="7">First, they still use available RTE data to obtain a monolingual TE corpus, whereas we pursue the more ambitious goal of generating from scratch aligned CLTE corpora for different language combinations.</S>
    <S sid="30" ssid="8">To this aim, we do not resort to already annotated data, nor languagespecific preprocessing tools.</S>
    <S sid="31" ssid="9">Second, their approach involves qualitative analysis of the collected data only a posteriori, after manual removal of invalid and trivial generated hypotheses.</S>
    <S sid="32" ssid="10">In contrast, our approach integrates quality control mechanisms at all stages of the data collection/annotation process, thus minimizing the recourse to experts to check the quality of the collected material.</S>
    <S sid="33" ssid="11">Related research in the CLTE direction is reported in (Negri and Mehdad, 2010), which describes the creation of an English-Spanish corpus obtained from the RTE-3 dataset by translating the English hypotheses into Spanish.</S>
    <S sid="34" ssid="12">Translations have been crowdsourced adopting a methodology based on translation-validation cycles, defined as separate HITs.</S>
    <S sid="35" ssid="13">Although simplifying the CLTE corpus creation problem, which is recast as the task of translating already available annotated data, this solution is relevant to our work for the idea of combining gold standard units and &#8220;validation HITS&#8221; as a way to control the quality of the collected data at runtime.</S>
  </SECTION>
  <SECTION title="3 Quality Control of Crowdsourced Data" number="4">
    <S sid="36" ssid="1">The design of data acquisition HITs has to take into account several factors, each having a considerable impact on the difficulty of instructing the workers, the quality and quantity of the collected data, the time and overall costs of the acquisition.</S>
    <S sid="37" ssid="2">A major distinction has to be made between jobs requiring data annotation, and those involving content generation.</S>
    <S sid="38" ssid="3">In the former case, Turkers are presented with the task of labelling input data referring to a fixed set of possible values (e.g. making a choice between multiple alternatives, assigning numerical scores to rank the given data).</S>
    <S sid="39" ssid="4">In the latter case, Turkers are faced with creative tasks consisting in the production of textual material (e.g. writing a correct translation, or a summary of a given text).</S>
    <S sid="40" ssid="5">The ease of controlling the quality of the acquired data depends on the nature of the job.</S>
    <S sid="41" ssid="6">For annotation jobs, quality control mechanisms can be easily set up by calculating Turkers&#8217; agreement, by applying voting schemes, or by adding hidden gold units to the data to be annotated8.</S>
    <S sid="42" ssid="7">In contrast, the quality of the results of content generation jobs is harder to assess, due to the fact that multiple valid results are acceptable (e.g. the same content can be expressed, translated, or summarized in different ways).</S>
    <S sid="43" ssid="8">In such situations the standard quality control mechanisms are not directly applicable, and the detection of errors requires either costly manual verification at the end of the acquisition process, or more complex and creative solutions integrating HITs for quality check.</S>
    <S sid="44" ssid="9">Most of the approaches to content generation proposed so far rely on post hoc verification to filter out undesired low-quality data (Mrozinski et al., 2008; Mihalcea and Strapparava, 2009; Wang and Callison-Burch, 2010).</S>
    <S sid="45" ssid="10">The few solutions integrating validation HITs address the translation of single sentences, a task that is substantially different from ours (Negri and Mehdad, 2010; Bloodgood and Callison-Burch, 2010).</S>
    <S sid="46" ssid="11">Compared to sentence translation, the task of creating CLTE pairs is both harder to explain without recurring to notions that are difficult to understand to non-experts (e.g.</S>
    <S sid="47" ssid="12">&#8220;semantic equivalence&#8221;, &#8220;unidirectional entailment&#8221;), and harder to execute without mastering these notions.</S>
    <S sid="48" ssid="13">To tackle these issues the &#8220;divide and conquer&#8221; approach described in the next section consists in the decomposition of a difficult content generation job into easier subtasks that are: i) self-contained and easy to explain, ii) easy to execute without any NLP expertise, and iii) suitable for the integration of a variety of runtime control mechanisms (regional qualifications, gold units, &#8220;validation HITs&#8221;) able to ensure a good quality of the collected material.</S>
    <S sid="49" ssid="14">8Both MTurk and CrowdFlower provide means to check workers&#8217; reliability, and weed out untrusted ones without money waste.</S>
    <S sid="50" ssid="15">These include different types of qualification mechanisms, the possibility of giving work only to known trusted Turkers (only with MTurk), and the possibility of adding hidden gold standard units in the data to be annotated (offered as a built-in mechanism only by CrowdFlower). tions (e.g.</S>
    <S sid="51" ssid="16">L2/L2, L2/L3).</S>
    <S sid="52" ssid="17">The execution of the two &#8220;multilingual&#8221; stages is not strictly necessary but depends on: i) the availability of parallel sentences to start the process, and ii) the actual objectives in terms of language combinations to be covered10.</S>
    <S sid="53" ssid="18">As regards the first stage, in this work we started from a set of 467 English/Italian/German aligned sentences extracted from parallel documents downloaded from the Cafebabel European Magazine11.</S>
    <S sid="54" ssid="19">Concerning the second multilingual stage, we performed only one round of translations from English to Italian to extend the 3 combinations obtained without translations (ENG/ENG, ENG/ITA, and ENG/GER) with the new language combinations ITA/ITA, ITA/ENG, and ITA/GER.</S>
  </SECTION>
  <SECTION title="4 CLTE Corpus Creation Methodology" number="5">
    <S sid="55" ssid="1">Our approach builds on a pipeline of HITs routed to MTurk&#8217;s workforce through the CrowdFlower interface.</S>
    <S sid="56" ssid="2">The objective is to collect aligned T-H pairs for different language combinations, reproducing an RTE-like annotation style.</S>
    <S sid="57" ssid="3">However, our annotation is not limited to the standard RTE framework, where only unidirectional entailment from T to H is considered.</S>
    <S sid="58" ssid="4">As a useful extension, we annotate any possible entailment relation between the two text fragments, including: i) bidirectional entailment (i.e. semantic equivalence between T and H), ii) unidirectional entailment from T to H, and iii) unidirectional entailment from H to T. The resulting pairs can be easily used to generate not only standard RTE datasets9, but also general-purpose collections featuring multi-directional entailment relations.</S>
    <S sid="59" ssid="5">We collect large amounts of CLTE pairs carrying out the most difficult part of the process (the creation of entailment-annotated pairs) at a monolingual level.</S>
    <S sid="60" ssid="6">Starting from a set of parallel sentences in n languages, (e.g.</S>
    <S sid="61" ssid="7">L1, L2, L3), n entailment corpora are created: one monolingual (L1/L1), and n-1 crosslingual (L1/L2, and L1/L3).</S>
    <S sid="62" ssid="8">The monolingual corpus is obtained by modifying the sentences only in one language (L1).</S>
    <S sid="63" ssid="9">Original and modified sentences are then paired and annotated to form an entailment dataset for L1.</S>
    <S sid="64" ssid="10">The CLTE corpora are obtained by combining the modified sentences in L1 with the original sentences in L2 and L3, and projecting to the multilingual pairs the annotations assigned to the monolingual pairs.</S>
    <S sid="65" ssid="11">In principle, only two stages of the process require crowdsourcing multilingual tasks, but do not concern entailment annotations.</S>
    <S sid="66" ssid="12">The first one, at the beginning of the process, aims to obtain a set of parallel sentences to start with, and can be done in different ways (e.g. crowdsourcing the translation of a set of sentences).</S>
    <S sid="67" ssid="13">The second one, at the end of the process, consists of translating the modified L1 sentences into other languages (e.g.</S>
    <S sid="68" ssid="14">L2) in order to extend the corpus to cover new language combina9With the positive examples drawn from bidirectional and unidirectional entailments from T to H, and the negative ones drawn from unidirectional entailments from H to T. The main steps of our corpus creation process, depicted in Figure 1, can be summarized as follows: Step1: Sentence modification.</S>
    <S sid="69" ssid="15">The original English sentences (ENG) are modified through (monolingual) generation HITs asking Turkers to: i) preserve the meaning of the original sentences using different surface forms, or ii) slightly change their meaning by adding or removing content.</S>
    <S sid="70" ssid="16">Our assumption, in line with (Bos et al., 2009), is that 10Starting from parallel sentences in n languages, the n corpora obtained without recurring to translations can be augmented, by means of translation HITs, to create the full set of language combinations.</S>
    <S sid="71" ssid="17">Each round of translation adds 1 monolingual corpus, and n-1 CLTE corpora. another way to think about entailment is to consider whether one text T1 adds new information to the content of another text T: if so, then T is entailed by T1.</S>
    <S sid="72" ssid="18">The result of this phase is a set of texts (ENG1) that can be of three types: Step2: TE Annotation.</S>
    <S sid="73" ssid="19">Entailment pairs composed of the original sentences (ENG) and the modified ones (ENG1) are used as input of (monolingual) annotation HITs asking Turkers to decide which of the two texts contains more information.</S>
    <S sid="74" ssid="20">As a result, each ENG/ENG1 pair is annotated as an example of uni-/bidirectional entailment, and stored in the monolingual English corpus.</S>
    <S sid="75" ssid="21">Since the original ENG texts are aligned with the ITA and GER texts, the entailment annotations of ENG/ENG1 pairs can be projected to the other language pairs and the ITA/ENG1 and GER/ENG1 pairs are stored in the CLTE corpus.</S>
    <S sid="76" ssid="22">The possibility of projecting TE annotations is based on the assumption that the semantic information is mostly preserved during the translation process.</S>
    <S sid="77" ssid="23">This particularly holds at the denotative level (i.e. regarding the truth values of the sentence) which is crucial to semantic inference.</S>
    <S sid="78" ssid="24">At other levels (e.g. lexical) there might be slight semantic variations which, however, are very unlikely to play a crucial role in determining entailment relations.</S>
    <S sid="79" ssid="25">Step3: Translation.</S>
    <S sid="80" ssid="26">The modified sentences (ENG1) are translated into Italian (ITA1) through (multilingual) generation HITs reproducing the approach described in (Negri and Mehdad, 2010).</S>
    <S sid="81" ssid="27">As a result, three new datasets are produced by automatically projecting annotations: the monolingual ITA/ITA1, and the cross-lingual ENG/ITA1 and GER/ITA1.</S>
    <S sid="82" ssid="28">Since the solution adopted for sentence translation does not present novelty factors, the remainder of this paper will omit further details on it.</S>
    <S sid="83" ssid="29">Instead, the following sections will focus on the more challenging tasks of sentence modification and TE annotation.</S>
    <S sid="84" ssid="30">Sentence modification and TE annotation have been decomposed into a pipeline of simpler monolingual English sub-tasks.</S>
    <S sid="85" ssid="31">Such pipeline, depicted in Figure 2, involves several types of generation/annotation HITs designed to be easily understandable to nonexperts.</S>
    <S sid="86" ssid="32">Each HIT consists of: i) a set of instructions for a specific task (e.g. paraphrasing a text), ii) the data to be manipulated (e.g. an English sentence), and iii) a test to check workers&#8217; reliability.</S>
    <S sid="87" ssid="33">To cope with the quality control issues discussed in Section 3, such tests are realized using gold standard units, either hidden in the data to be annotated (annotation HITs) or defined as test questions that workers must correctly answer (generation HITs).</S>
    <S sid="88" ssid="34">Moreover, regional qualifications are applied to all HITs.</S>
    <S sid="89" ssid="35">As a further quality check, all the annotation HITs consider Turkers&#8217; agreement as a way to filter out low quality results (only annotations featuring agreement among 4 out of 5 workers are retained).</S>
    <S sid="90" ssid="36">The six HITs defined for each subtask can be described as follows: new sentence workers are asked to judge which of two given English sentences is more detailed.</S>
    <S sid="91" ssid="37">4b.</S>
    <S sid="92" ssid="38">Remove Information (generation).</S>
    <S sid="93" ssid="39">Modify an English text to create a more general one by removing part of its content.</S>
    <S sid="94" ssid="40">As a reliability test, before generating the new sentence workers are asked to judge which of two given English sentences is less detailed.</S>
  </SECTION>
  <SECTION title="5." number="6">
    <S sid="95" ssid="1">cide which of two English sentences (the original ENG, and a modified ENG1) provides more information.</S>
    <S sid="96" ssid="2">These HITs are combined in an iterative process that alternates text generation, grammaticality check, and entailment annotation steps.</S>
    <S sid="97" ssid="3">As a result, for each original ENG text we obtain multiple ENG1 variants of the three types (paraphrases, more general texts, and more specific texts) and, in turn, a set of annotated monolingual (ENG/ENG1) TE pairs.</S>
    <S sid="98" ssid="4">As described in Section 4.1, the resulting monolingual English TE corpus (ENG/ENG1) is used to create the following mono/cross-lingual TE corpora:</S>
  </SECTION>
  <SECTION title="5 The Resulting CLTE Corpora" number="7">
    <S sid="99" ssid="1">This section provides a quantitative and qualitative analysis of the results of our corpus creation methodology, focusing on the collected ENG-ENG1 monolingual dataset.</S>
    <S sid="100" ssid="2">It has to be remarked that, as an effect of the adopted methodology, all the observations and the conclusions drawn hold for the collected CLTE corpora as well.</S>
    <S sid="101" ssid="3">Table 1 provides some details about each step of the pipeline shown in Figure 2.</S>
    <S sid="102" ssid="4">For each HIT the table presents: i) the number of items (sentences, or pairs of sentences) given in input, ii) the number of items (sentences or annotations) produced as output, iii) the number of items discarded when the agreement threshold was not reached, iv) the number of entailment pairs added to the corpus, v) the time (days and hours) required by the MTurk workforce to complete the job, and vi) the cost of the job.</S>
    <S sid="103" ssid="5">In HIT-1 (Paraphrase) 1,414 paraphrases were collected asking three different meaning-preserving modifications of each of the 467 original sentences12.</S>
    <S sid="104" ssid="6">From a practical point of view, such redundancy aims to ensure a sufficient number of grammatically correct and semantically equivalent modified sentences.</S>
    <S sid="105" ssid="7">From a theoretical point of view, collecting many variants of a small pool of original sentences aims to create pairs featuring different entailment relations with similar superficial forms.</S>
    <S sid="106" ssid="8">This, in principle, should allow to obtain a dataset which requires TE systems to focus more on deeper semantic phenomena than on the surface realization of the pairs.</S>
    <S sid="107" ssid="9">The collected paraphrases were sent as input to HIT-2 (Grammaticality).</S>
    <S sid="108" ssid="10">After this validation HIT, the number of acceptable paraphrases was reduced to 1,326 (with 88 discarded sentences, corresponding to 6.22% of the total).</S>
    <S sid="109" ssid="11">The retained paraphrases were paired with their corresponding original sentences, and sent to HIT-3 (Bidirectional Entailment) to be judged for semantic equivalence.</S>
    <S sid="110" ssid="12">The pairs marked as bidirectional entailments (1,205) were divided in three groups: 25% of the pairs (301) were directly stored in the final corpus, while the ENG1 paraphrases of the remaining 75% (904) were equally distributed to the next modification steps.</S>
    <S sid="111" ssid="13">In both HIT-4a (Add Information) and HIT-4b (Remove information) two new modified sentences were asked for each of the 452 paraphrases received as input.</S>
    <S sid="112" ssid="14">The sentences collected in these generation tasks were respectively 916 and 923.</S>
    <S sid="113" ssid="15">The new modified sentences were sent back to HIT-2 (Grammaticality) and HIT-3 (Bidirectional Entailment).</S>
    <S sid="114" ssid="16">As a result 1,438 new pairs were created; out of these, 148 resulted to be bidirectional entailments and were stored in the corpus.</S>
    <S sid="115" ssid="17">Finally, the 1,298 entailment pairs judged as nonbidirectional in the two previously completed HIT3 (8+1,290) were given as input to HIT-5 (Unidirectional Entailment).</S>
    <S sid="116" ssid="18">The pairs which passed the agreement threshold were classified according to the judgement received, and stored in the corpus as unidirectional entailment pairs.</S>
    <S sid="117" ssid="19">The analysis of Table 1 allows to formulate some considerations.</S>
    <S sid="118" ssid="20">First, the percentage of discarded items confirms the effectiveness of decomposing complex generation tasks into simpler subtasks that integrate validation HITs and quality checks based on non-experts&#8217; agreement.</S>
    <S sid="119" ssid="21">In fact, on average, around 9.5% of the generated items were discarded without experts&#8217; intervention13.</S>
    <S sid="120" ssid="22">Second, the amount of discarded items gives evidence about the relative difficulty of each HIT.</S>
    <S sid="121" ssid="23">As expected, we observe lower rejection rates, corresponding to higher inter-annotator agreement, for grammaticality HITs (5.55% on average) than for more complex entailment-related tasks (12.02% on average).</S>
    <S sid="122" ssid="24">Looking at costs and execution time, it is hard to draw definite conclusions due to several factors that influence the progress of the crowdsourced jobs (e.g. the fluctuations of Turkers&#8217; performances, the time of the day at which jobs are posted, the difficulty to set the optimal cost for a given HIT14).</S>
    <S sid="123" ssid="25">On the one hand, as expected, the more creative &#8220;Add Info&#8221; task proved to be more demanding than the &#8220;Remove Info&#8221;: even though it was paid more, 13Moreover, it is worthwhile noticing that around 20% of the collected items were automatically rejected (and not paid) due to failures on the gold standard controls created both for generation and annotation tasks.</S>
    <S sid="124" ssid="26">14The payment for each HIT was set on the basis of a previous feasibility study aimed at determining the best trade-off between cost and execution time.</S>
    <S sid="125" ssid="27">However, replicating our approach would not necessarily result in the same costs. it still took little more time to be completed.</S>
    <S sid="126" ssid="28">On the other hand, although the &#8220;Unidirectional Entailment&#8221; task was expected to be more difficult and thus rewarded more than the &#8220;Bidirectional Entailment&#8221; one, in the end it took notably less time to be completed.</S>
    <S sid="127" ssid="29">Nevertheless, the overall figures (435 USD, and about 22.5 days of MTurk work to complete the process)15 clearly demonstrate the effectiveness of the approach.</S>
    <S sid="128" ssid="30">Even considering the time needed for an expert to manage the pipeline (i.e. one week to prepare gold units, and to handle the I/O of each HIT), these figures show that our methodology provides a cheaper and faster way to collect entailment data in comparison with the RTE average costs reported in Section 1.</S>
    <S sid="129" ssid="31">As regards the amount of data collected, the resulting corpus contains 1,620 pairs with the following distribution of entailment relations: i) 449 bidirectional entailments, ii) 491 ENG&#8594;ENG1 unidirectional entailments, and iii) 680 ENG&#8592;ENG1 unidirectional entailments.</S>
    <S sid="130" ssid="32">It must be noted that our methodology does not lead to the creation of pairs where some information is provided in one text and not in the other, and viceversa, as Example 1 shows: ENG: New theories were emerging in the field of psychology.</S>
    <S sid="131" ssid="33">ENG1: New theories were rising, which announced a kind of veiled racism.</S>
    <S sid="132" ssid="34">These negative examples in both directions represent a natural extension of the dataset, relevant also for specific application-oriented scenarios, and their creation will be addressed in future work.</S>
    <S sid="133" ssid="35">Besides the achievement of our primary objectives, the adopted approach led to some interesting by-products.</S>
    <S sid="134" ssid="36">First, the generated corpora are perfectly suitable to produce entailment datasets similar to those used in the traditional RTE evaluation framework.</S>
    <S sid="135" ssid="37">In particular, considering any possible entailment relation between two text fragments, our annotation subsumes the one proposed in RTE campaigns.</S>
    <S sid="136" ssid="38">This allows for the cost-effective generation of RTE-like annotations from the acquired cor15Although by projecting annotations the ENG1/ITA and ENG1/GER CLTE corpora came for free, the ITA1/ITA, ITA1/ENG, and ITA1/GER combinations created by crowdsourcing translations added 45 USD and approximately 5 days to these figures. pora by combining ENG&#8596;ENG1 and ENG&#8594;ENG1 pairs to form 940 positive examples (449+491), keeping the 680 ENG&#8592;ENG1 as negative examples.</S>
    <S sid="137" ssid="39">Moreover, by swapping ENG and ENG1 in the unidirectional entailment pairs, 491 additional negative examples and 680 positive examples can be easily obtained.</S>
    <S sid="138" ssid="40">Finally, the output of HITs 1-2-3 in Table 1 represents per se a valuable collection of 1,205 paraphrases.</S>
    <S sid="139" ssid="41">This suggests the great potential of crowdsourcing for paraphrase acquisition.</S>
    <S sid="140" ssid="42">Through manual verification of more than 50% of the corpus (900 pairs), a total number of 53 pairs (5.9%) were found incorrect.</S>
    <S sid="141" ssid="43">The different errors were classified as follows: Type 1: Sentence modification errors.</S>
    <S sid="142" ssid="44">Generation HITs are a minor source of errors, being responsible for 10 problematic pairs.</S>
    <S sid="143" ssid="45">These errors are either introduced by generating a false statement (Example 2), or by forming a not fully understandable, awkward, or non-natural sentence (Example 3).</S>
    <S sid="144" ssid="46">Type 2: TE annotation errors.</S>
    <S sid="145" ssid="47">The notion of containing more/less information, used in the &#8220;Unidirectional Entailment&#8221; HIT, can mostly be applied straightforwardly to the entailment definition.</S>
    <S sid="146" ssid="48">However, the concept of &#8220;more/less detailed&#8221;, which generally works for factual statements, in some cases is not applicable.</S>
    <S sid="147" ssid="49">In fact, the MTurk workers have regularly interpreted the instructions about the amount of information as concerning the quantity of concepts contained in a sentence.</S>
    <S sid="148" ssid="50">This is not always corresponding to the actual entailment relation between the sentences.</S>
    <S sid="149" ssid="51">As a consequence, 43 pairs featuring wrong entailment annotations were encountered.</S>
    <S sid="150" ssid="52">These errors can be classified as follows: a) 13 pairs, where the added/removed information changes the meaning of the sentence.</S>
    <S sid="151" ssid="53">In these cases, the modified sentence was judged more/less specific than the original one, leading to unidirectional entailment annotation.</S>
    <S sid="152" ssid="54">On the contrary, in terms of the standard entailment definition, the correct annotation is &#8220;no entailment&#8221; (as in Example 4, which was annotated as ENG&#8594;ENG1): These pairs were labelled as unidirectional entailments (in the example above ENG&#8594;ENG1), under the assumption that a proper name is more specific and informative than a pronoun.</S>
    <S sid="153" ssid="55">However, adhering to the TE definition, co-referring expressions are equivalent, and their realization does not play any role in the entailment decision.</S>
    <S sid="154" ssid="56">This implies that the correct entailment annotation is &#8220;bidirectional&#8221;. c) 9 pairs where the sentences are semantically equivalent, but contain a piece of information which is explicit in one sentence, and implicit in the other.</S>
    <S sid="155" ssid="57">In these cases, Turkers judged the sentence containing the explicit mention as more specific, and thus the pair was annotated as unidirectional entailment.</S>
    <S sid="156" ssid="58">In Example 6, the expression &#8220;the trigger&#8221; in ENG1 implicitly means &#8220;the click of the trigger&#8221;, making the two sentences equivalent, and the entailment bidirectional (instead of ENG&#8594;ENG1). d) 7 pairs where the information removed from or added to the sentence is not relevant to the entailment relation.</S>
    <S sid="157" ssid="59">In these cases, the modified sentence was judged less/more specific than the original one (and thus considered as unidirectional entailment), even though the correct judgement is &#8220;bidirectional&#8221;, as in: e) 4 pairs where the added/removed information concerns universally quantified general statements, about which the interpretation of &#8220;more/less specific&#8221; given by Turkers resulted in the wrong annotation.</S>
    <S sid="158" ssid="60">In Example 8, the additional information (&#8220;multicultural&#8221;) restricts the set to which it refers (&#8220;couples&#8221;) making ENG entailed by ENG1, and not vice versa as resulted from Turkers&#8217; annotation.</S>
    <S sid="159" ssid="61">In light of this analysis, we conclude that the sentence modification methodology proved to be successful, as the low number of Type 1 errors shows.</S>
    <S sid="160" ssid="62">Considering that the most expensive phase in the creation of a TE dataset is the generation of the pairs, this is a significant achievement.</S>
    <S sid="161" ssid="63">Differently, the entailment assessment phase appears to be more problematic, accounting for the majority of errors.</S>
    <S sid="162" ssid="64">As shown by Type 2 errors, this is due to a partial misalignment between the instructions given in our HITs, and the formal definition of textual entailment.</S>
    <S sid="163" ssid="65">For this reason, further experimentation will explore different ways to instruct workers (e.g. asking to consider proper names and pronouns as equivalent) in order to reduce the amount of errors produced.</S>
    <S sid="164" ssid="66">As a final remark, considering that in the creation of a TE dataset the manual check of the annotated pairs represents a minor cost, even the involvement of experts to filter out wrong annotations would not decrease the cost-effectiveness of the proposed methodology.</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="8">
    <S sid="165" ssid="1">There is an increasing need of annotated data to develop new solutions to the Textual Entailment problem, explore new entailment-related tasks, and set up experimental frameworks targeting real-world applications.</S>
    <S sid="166" ssid="2">Following the recent trends promoting annotation efforts that go beyond the established RTE Challenge framework (unidirectional entailment between monolingual T-H pairs), in this paper we addressed the multilingual dimension of the problem.</S>
    <S sid="167" ssid="3">Our primary goal was the creation of large-scale collections of entailment pairs for different language combinations.</S>
    <S sid="168" ssid="4">Besides that, we considered cost effectiveness and replicability as additional requirements.</S>
    <S sid="169" ssid="5">To achieve our objectives, we developed a &#8220;divide and conquer&#8221; methodology based on crowdsourcing.</S>
    <S sid="170" ssid="6">Our approach presents several key innovations with respect to the related works on TE data acquisition.</S>
    <S sid="171" ssid="7">These include the decomposition of a complex content generation task in a pipeline of simpler subtasks accessible to a large crowd of non-experts, and the integration of quality control mechanisms at each stage of the process.</S>
    <S sid="172" ssid="8">The result of our work is the first large-scale dataset containing both monolingual and cross-lingual corpora for several combinations of texts-hypotheses in English, Italian, and German.</S>
    <S sid="173" ssid="9">Among the advantages of our method it is worth mentioning: i) the full alignment between the created corpora, ii) the possibility to easily extend the dataset to new languages, and iii) the feasibility of creating general-purpose corpora, featuring multi-directional entailment relations, that subsume the traditional RTE-like annotation.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="9">
    <S sid="174" ssid="1">This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).</S>
    <S sid="175" ssid="2">The authors would like to thank Emanuele Pianta for the helpful discussions, and Giovanni Moretti for the valuable support in the creation of the CLTE dataset.</S>
  </SECTION>
</PAPER>
