Distortion Models For Statistical Machine Translation
In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.
We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.
We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.
Our lexicalized distortion model predicts the jump from the last translated word to the next one, with a class for each possible jump length.
We find that deterministic word reordering is beyond the scope of optimization and cannot be undone by the decoder.
