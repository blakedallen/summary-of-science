[
  {
    "citance_No": 1, 
    "citing_paper_id": "P03-1058", 
    "citing_paper_authority": 41, 
    "citing_paper_authors": "Hwee Tou, Ng | Bin, Wang | Yee Seng, Chan", 
    "raw_text": "In this paper, we used the WSD program reported in (Lee and Ng, 2002)", 
    "clean_text": "In this paper, we used the WSD program reported in (Lee and Ng, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P13-1055", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Olivier, Ferret", 
    "raw_text": "More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word E:? neighboring words;? Part-of-Speech (POS) of neighboring words;? local collocations", 
    "clean_text": "More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P13-1055", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Olivier, Ferret", 
    "raw_text": "Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial the saurus. For the neighboring words features, we con sider all plain words (common and proper nouns, verbs and adjectives) and adverbs that are present in the same sentence of an occurrence of E. Each neighboring word is represented under its lemma form as a binary feature whose value is equal to 1 when it is present in the same sentence as E. For the second type of features, we take more precisely the POS of the three words before E and those of the three words after E. Each pair{ POS, position} corresponds to a binary feature for the SVM classifier", 
    "clean_text": "Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W06-2911", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Rie Kubota, Ando", 
    "raw_text": "Features We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stop words) in the given con text (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syn 79 tactic relations: syntactic information obtained from parser output", 
    "clean_text": "We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stop words) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syntactic relations: syntactic information obtained from parser output.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W06-2911", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Rie Kubota, Ando", 
    "raw_text": "Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM", 
    "clean_text": "Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D09-1047", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Daniel, Dahlmeier | Hwee Tou, Ng | Tanja, Schultz", 
    "raw_text": "Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD", 
    "clean_text": "Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D09-1047", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Daniel, Dahlmeier | Hwee Tou, Ng | Tanja, Schultz", 
    "raw_text": "For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002)", 
    "clean_text": "For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "H05-1114", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Zhengyu, Niu | Ji, Donghong | Chew Lim, Tan", 
    "raw_text": "Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002 )forword sense disambiguation", 
    "clean_text": "Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "H05-1114", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Zhengyu, Niu | Ji, Donghong | Chew Lim, Tan", 
    "raw_text": "SVM) (Lee and Ng, 2002) or semi-supervised methods (ex", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "H05-1114", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Zhengyu, Niu | Ji, Donghong | Chew Lim, Tan", 
    "raw_text": "For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information: part-of-speech of neighboring words with position information ,un ordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations)", 
    "clean_text": "For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P07-1005", 
    "citing_paper_authority": 61, 
    "citing_paper_authors": "Yee Seng, Chan | Hwee Tou, Ng | David, Chiang", 
    "raw_text": "Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002)", 
    "clean_text": "Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P07-1005", 
    "citing_paper_authority": 61, 
    "citing_paper_authors": "Yee Seng, Chan | Hwee Tou, Ng | David, Chiang", 
    "raw_text": "For our experiments, we use the SVM implementation of (Chang and Lin, 2001) as it is able to work on multi class problems to output the classification probability for each class. Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002)", 
    "clean_text": "Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "C08-1009", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Samuel, Brody | Mirella, Lapata", 
    "raw_text": "These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness)", 
    "clean_text": "These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P07-1007", 
    "citing_paper_authority": 23, 
    "citing_paper_authors": "Yee Seng, Chan | Hwee Tou, Ng", 
    "raw_text": "These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002)", 
    "clean_text": "These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P06-1012", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Yee Seng, Chan | Hwee Tou, Ng", 
    "raw_text": "Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier", 
    "clean_text": "Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D08-1105", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Zhi, Zhong | Hwee Tou, Ng | Yee Seng, Chan", 
    "raw_text": "For the experiments reported in this paper, we fol low the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words", 
    "clean_text": "For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D07-1108", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Junfu, Cai | Wee Sun, Lee | Yee Whye, Teh", 
    "raw_text": "Finally, in testing, the learnt model is applied on the test data to assign the correct sense to any ambiguous word. The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations ,syntac tic patterns and global features such as single words in the surrounding context (bag-of-words) (LeeandNg, 2002)", 
    "clean_text": "The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D07-1108", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Junfu, Cai | Wee Sun, Lee | Yee Whye, Teh", 
    "raw_text": "Local Collocations Collocation Ci, j refers to the ordered sequence of tokens (words or punctuations) surrounding w. The starting and ending position of the sequence are denoted i and j respectively, where a negative value refers to the token position prior to w. We adopt the same 11 collocation features as (Lee and Ng, 2002), namely C? 1,? 1, C1,1, C? 2,? 2, C2,2, C? 2,? 1, C? 1,1, C1,2, C? 3,? 1, C? 2,1, C? 1,2, and C1,3", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "D07-1108", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Junfu, Cai | Wee Sun, Lee | Yee Whye, Teh", 
    "raw_text": "Syntactic Relations We adopt the same syntactic relations as (Lee and Ng, 2002)", 
    "clean_text": "We adopt the same syntactic relations as (Lee and Ng, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D07-1108", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Junfu, Cai | Wee Sun, Lee | Yee Whye, Teh", 
    "raw_text": "In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense)", 
    "clean_text": "In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense).", 
    "keep_for_gold": 0
  }
]
