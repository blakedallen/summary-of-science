Adaptation Of Maximum Entropy Capitalizer: Little Data Can Help A Lot
A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.
The technique is applied to the problem of automatically capitalizing uniformly cased text.
Automatic capitalization is a practically relevant problem: speech recognition output needs to be capitalized; also, modern word processors perform capitalization among other text proofing algorithms such as spelling correction and grammar checking.
Capitalization can be also used as a preprocessing step in named entity extraction or machine translation.
A “background” capitalizer trained on 20 M words of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets – one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text – from 1996.
The “in-domain” performance of the WSJ capitalizer is 45% better relative to the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994. When evaluating on the mismatched “out-of-domain” test data, the 1-gram baseline is outperformed by 60% relative; the improvement brought by the adaptation technique using a very small amount of matched BN data – 25–70k words – is about 20–25% relative.
Overall, automatic capitalization error rate of 1.4% is achieved on BN data.
The performance gain obtained by employing our adaptation technique using a tiny amount of out-of-domain training data on top of the background data is striking: as little as 0.14 M words of in-domain data brings more improvement than using 10 times more background training data (from 2 M words to 20 M words).
we proposed method for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior.
we use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data.
