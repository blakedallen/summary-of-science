[
  {
    "citance_No": 1, 
    "citing_paper_id": "P10-1129", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "S. R. K., Branavan | Luke, Zettlemoyer | Regina, Barzilay", 
    "raw_text": "Interpreting Instructions Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al (2009) (see Section 4 for more detail)", 
    "clean_text": "Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al (2009) (see Section 4 for more detail).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P10-1129", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "S. R. K., Branavan | Luke, Zettlemoyer | Regina, Barzilay", 
    "raw_text": "In this section, we review the details of this framework.4Previous work (Branavan et al, 2009) is only able to handle low-level instructions", 
    "clean_text": "Previous work (Branavan et al, 2009) is only able to handle low-level instructions.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P10-1129", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "S. R. K., Branavan | Luke, Zettlemoyer | Regina, Barzilay", 
    "raw_text": "This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments", 
    "clean_text": "This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P10-1129", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "S. R. K., Branavan | Luke, Zettlemoyer | Regina, Barzilay", 
    "raw_text": "Features In addition to the look-ahead features described in Section 5.2, the policy also includes the set of features used by Branavan et al (2009)", 
    "clean_text": "In addition to the look-ahead features described in Section 5.2, the policy also includes the set of features used by Branavan et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P10-1129", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "S. R. K., Branavan | Luke, Zettlemoyer | Regina, Barzilay", 
    "raw_text": "Baselines As a baseline, we compare our method against the results reported by Branavan et al (2009), denoted here as BCZB09", 
    "clean_text": "Baselines As a baseline, we compare our method against the results reported by Branavan et al (2009), denoted here as BCZB09.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P10-1129", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "S. R. K., Branavan | Luke, Zettlemoyer | Regina, Barzilay", 
    "raw_text": "Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009)", 
    "clean_text": "Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W12-2802", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Josh, Joseph | Pratiksha, Thaker | Stefanie, Tellex | Nicholas, Roy", 
    "raw_text": "Previous approaches have represented word meanings as symbols in some specific symbolic language, either programmed by hand [Winograd, 1971, MacMahon et al, 2006] or learned [Matuszek et al, 2010, Chen and Mooney, 2011, Liang et al,2011, Branavan et al, 2009]", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W12-2802", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Josh, Joseph | Pratiksha, Thaker | Stefanie, Tellex | Nicholas, Roy", 
    "raw_text": "To learn from an unaligned corpus, we derivea new training algorithm that combines the Generalized Grounding Graph (G3) framework introduced by Tellex et al [2011] with the policy gradient method described by Branavan et al [2009]", 
    "clean_text": "To learn from an unaligned corpus, we derive a new training algorithm that combines the Generalized Grounding Graph (G3) framework introduced by Tellex et al [2011] with the policy gradient method described by Branavan et al [2009].", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W12-2802", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Josh, Joseph | Pratiksha, Thaker | Stefanie, Tellex | Nicholas, Roy", 
    "raw_text": "Previous approaches capable of learning from unaligned data [Vogel and Jurafsky, 2010, Branavan et al, 2009] used sequential models that could not capture the hierarchical structure of language", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P10-1083", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Adam, Vogel | Daniel, Jurafsky", 
    "raw_text": "We also compare against the policy gradient learning algorithm of Branavan et al (2009)", 
    "clean_text": "We also compare against the policy gradient learning algorithm of Branavan et al (2009).", 
    "keep_for_gold": 0
  }
]
