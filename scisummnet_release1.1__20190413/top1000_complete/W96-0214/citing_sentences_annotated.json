[
  {
    "citance_No": 1, 
    "citing_paper_id": "W96-0111", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Rens, Bod", 
    "raw_text": "In Goodman (1996), an efficient parsing strategy is given that maximizes the expected number of correct constituents", 
    "clean_text": "In Goodman (1996), an efficient parsing strategy is given that maximizes the expected number of correct constituents.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W96-0111", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Rens, Bod", 
    "raw_text": "1 The DOP1 model, and some variations of it, have been tested by Bod (1993-1995), Sima &apos; an (1995-1996), Sekine& amp; Grishman (1995), Goodman (1996), and Charniak (1996)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W96-0111", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Rens, Bod", 
    "raw_text": "2 Although Sima &apos; an (1996) and Goodman (1996) also report experiments on unedited ATIS trees, their results do not refer to the most probable parse but to the most probable derivation and the maximum constituents parse respectively", 
    "clean_text": "Although Sima'an (1996) and Goodman (1996) also report experiments on unedited ATIS trees, their results do not refer to the most probable parse but to the most probable derivation and the maximum constituents parse respectively.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W12-1904", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Francis, Ferraro | Benjamin, Van Durme | Matt, Post", 
    "raw_text": "This does not scale well to large tree banks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001)", 
    "clean_text": "This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P09-1067", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Zhifei, Li | Jason M., Eisner | Sanjeev P., Khudanpur", 
    "raw_text": "Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al, 2005)", 
    "clean_text": "Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P02-1034", 
    "citing_paper_authority": 111, 
    "citing_paper_authors": "Michael John, Collins | Nigel, Duffy", 
    "raw_text": "It might seem paradoxical to be able to efficiently learn and apply a model with an exponential number of features.1 The key to our algorithms is the 1Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper", 
    "clean_text": "Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P02-1034", 
    "citing_paper_authority": 111, 
    "citing_paper_authors": "Michael John, Collins | Nigel, Duffy", 
    "raw_text": "The algorithms in this paper have a different flavor, avoiding the need to explicitly deal with feature vectors that track all subtrees, and also avoiding the need to sum over an exponential number of derivations underlying a given tree. (Goodman 1996) gives a polynomial time con version of a DOP model into an equivalent PCFG whose size is linear in the size of the training set", 
    "clean_text": "(Goodman 1996) gives a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P01-1010", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Rens, Bod", 
    "raw_text": "Goodman (1996, 1998) showed how DOP1can be reduced to a compact stochastic context free grammar (SCFG) which contains exactly eight SCFG rules for each node in the training set trees", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "D07-1058", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Willem, Zuidema", 
    "raw_text": "The relation betweenDOP and enrichment/conditioning models was clarified by Joshua Goodman, who devised an efficient PCFG transform of the DOP1 model (Goodman, 1996)", 
    "clean_text": "The relation between DOP and enrichment/conditioning models was clarified by Joshua Goodman, who devised an efficient PCFG transform of the DOP1 model (Goodman, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P05-1010", 
    "citing_paper_authority": 78, 
    "citing_paper_authors": "Takuya, Matsuzaki | Yusuke, Miyao | Jun'ichi, Tsujii", 
    "raw_text": "Although we omit the details, we can prove the NP-hardness by observing that a stochastic tree substitution grammar (STSG) can be represented by a PCFG-LA model in a similar way to one described by Goodman (1996a), and then using the NP-hardness of STSG parsing (Simaa? n, 2002)", 
    "clean_text": "Although we omit the details, we can prove the NP-hardness by observing that a stochastic tree substitution grammar (STSG) can be represented by a PCFG-LA model in a similar way to one described by Goodman (1996a), and then using the NP-hardness of STSG parsing (Sima'an, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D11-1008", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Federico, Sangati | Willem, Zuidema", 
    "raw_text": "Their corresponding curves peak at 4 (depth), 1 (words), and 4 (substitution sites) .Implicit grammars Goodman (1996, 2003 )de fined a transformation for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree linear in the size of the trees", 
    "clean_text": "Implicit grammars Goodman (1996, 2003 ) defined a transformation for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree linear in the size of the trees.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D09-1039", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Daniel, Galron | Sergio, Penkale | Andy, Way | I. Dan, Melamed", 
    "raw_text": "Probabilities for the PCFG rules are computed mono lingually as in the standard Goodman reduction for DOP (Goodman, 1996)", 
    "clean_text": "Probabilities for the PCFG rules are computed monolingually as in the standard Goodman reduction for DOP (Goodman, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P10-1112", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Mohit, Bansal | Dan, Klein", 
    "raw_text": "In particular, we found that GI was smaller than explicit extraction of all depth 1 and 2unbinarized fragments for our 4The difference is that Goodman (1996a) collapses our BEGIN and END rules into the binary productions, giving a larger grammar which is less convenient for weighting", 
    "clean_text": "The difference is that Goodman (1996a) collapses our BEGIN and END rules into the binary productions, giving a larger grammar which is less convenient for weighting.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P10-1112", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Mohit, Bansal | Dan, Klein", 
    "raw_text": "Goodman (1996b), Petrov and Klein (2007), and Matsuzaki et al (2005) describe the details of constituent, rule-sum and variational objectives respectively", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P11-2127", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mohit, Bansal | Dan, Klein", 
    "raw_text": "During Viterbi shortest-derivation parsing (after coarse-pruning), if two derivations have the same cost (i.e., the number of switches), then we break the tie between them by choosing the derivation which has a higher sum of coarse posteriors (i.e., the sum of the coarse PCFG chart-cell posteriors P (X, i ,j|s) used to build the derivation) .4The coarse PCFG has an extremely beneficial interaction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (see4This is similar to the maximum recall objective for approximate inference (Goodman, 1996b)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P98-1021", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Rens, Bod", 
    "raw_text": "The most probable parse can be estimated by iterative Monte Carlo sampling (Bod 1995), but efficient algorithms exist only for sub-optimal solutions such as the most likely derivation of a sentence (Bod 1995, Sima &apos; an 1995) or the& quot ;labelled recall parse& quot; of a sentence (Goodman 1996)", 
    "clean_text": "The most probable parse can be estimated by iterative Monte Carlo sampling (Bod 1995), but efficient algorithms exist only for sub-optimal solutions such as the most likely derivation of a sentence (Bod 1995, Simaa'as; an 1995) or the labelled recall parse of a sentence (Goodman 1996).", 
    "keep_for_gold": 0
  }
]
