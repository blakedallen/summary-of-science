<PAPER>
	<S sid="0">SemEval-2007 Task 19: Frame Semantic Structure Extraction</S><ABSTRACT>
		<S sid="1" ssid="1">This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http: //framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntacticdependents (including subjects).</S>
		<S sid="2" ssid="2">The train ing data was FN annotated sentences.</S>
		<S sid="3" ssid="3">In testing, participants automatically annotated three previously unseen texts to match goldstandard (human) annotation, including pre dicting previously unseen frames and roles.</S>
		<S sid="4" ssid="4">Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">The task of labeling frame-evoking words with ap propriate frames is similar to WSD, while the task of assigning frame elements is called Semantic Role Labeling (SRL), and has been the subject of several shared tasks at ACL and CoNLL.</S>
			<S sid="6" ssid="6">For example, in the sentence ?Matilde said, ?I rarely eat rutabaga,??</S>
			<S sid="7" ssid="7">said evokes the Statement frame, and eat evokes the Ingestion frame.</S>
			<S sid="8" ssid="8">The role of SPEAKER in the Statement frame is filled by Matilda, and the roleof MESSAGE, by the whole quotation.</S>
			<S sid="9" ssid="9">In the Inges tion frame, I is the INGESTOR and rutabaga fills theINGESTIBLES role.</S>
			<S sid="10" ssid="10">Since the ingestion event is con tained within the MESSAGE of the Statement event, we can represent the fact that the message conveyed was about ingestion, just by annotating the sentence with respect to these two frames.</S>
			<S sid="11" ssid="11">After training on FN annotations, the participants?</S>
			<S sid="12" ssid="12">systems labeled three new texts automatically.</S>
			<S sid="13" ssid="13">The evaluation measured precision and recall for frames and frame elements, with partial credit for incorrect but closely related frames.</S>
			<S sid="14" ssid="14">Two types of evaluation were carried out: Label matching evaluation, in which the participant?s labeled data was compareddirectly with the gold standard labeled data, and Se mantic dependency evaluation, in which both thegold standard and the submitted data were first converted to semantic dependency graphs in XML for mat, and then these graphs were compared.</S>
			<S sid="15" ssid="15">There are three points that make this task harder and more interesting than earlier SRL tasks: (1) while previous tasks focused on role assignment, the current task also comprises the identification of the appropriate FrameNet frame, similar to WSD, (2)the task comprises not only the labeling of individual predicates and their arguments, but also the integration of all labels into an overall semantic depen dency graph, a partial semantic representation of the overall sentence meaning based on frames and roles, and (3) the test data includes occurrences of frames that are not seen in the training data.</S>
			<S sid="16" ssid="16">For these cases, participant systems have to identify theclosest known frame.</S>
			<S sid="17" ssid="17">This is a very realistic scenario, encouraging the development of robust systems showing graceful degradation in the face of un known events.</S>
			<S sid="18" ssid="18">99</S>
	</SECTION>
	<SECTION title="Frame semantics and FrameNet. " number="2">
			<S sid="19" ssid="1">The basic concept of Frame Semantics is that many words are best understood as part of a group of terms that are related to a particular type of eventand the participants and ?props?</S>
			<S sid="20" ssid="2">involved in it (Fill more, 1976; Fillmore, 1982).</S>
			<S sid="21" ssid="3">The classes of events are the semantic frames of the lexical units (LUs) that evoke them, and the roles associated with the event are referred to as frame elements (FEs).</S>
			<S sid="22" ssid="4">The same type of analysis applies not only to events butalso to relations and states; the frame-evoking expressions may be single words or multi-word ex pressions, which may be of any syntactic category.</S>
			<S sid="23" ssid="5">Note that these FE names are quite frame-specific; generalizations over them are expressed via explicit FE-FE relations.</S>
			<S sid="24" ssid="6">The Berkeley FrameNet project (hereafter FN) (Fillmore et al, 2003) is creating a computer- and human-readable lexical resource for English, based on the theory of frame semantics and supported by corpus evidence.</S>
			<S sid="25" ssid="7">The current release (1.3) of the FrameNet data, which has been freely available for instructional and research purposes since the fall of 2006, includes roughly 780 frames with roughly 10,000 word senses (lexical units).</S>
			<S sid="26" ssid="8">It also contains roughly 150,000 annotation sets, of which 139,000are lexicographic examples, with each sentence an notated for a single predicator.</S>
			<S sid="27" ssid="9">The remainder are from full-text annotation in which each sentence isannotated for all predicators; 1,700 sentences are annotated in the full-text portion of the database, ac counting for roughly 11,700 annotation sets, or 6.8 predicators (=annotation sets) per sentence.</S>
			<S sid="28" ssid="10">Nearly all of the frames are connected into a single graph by frame-to-frame relations, almost all of which have associated FE-to-FE relations (Fillmore et al, 2004a) 2.1 Frame Semantics of texts.</S>
			<S sid="29" ssid="11">The ultimate goal is to represent the lexical se mantics of all the sentences in a text, based onthe relations between predicators and their depen dents, including both phrases and clauses, which may, in turn, include other predicators; although this has been a long-standing goal of FN (Fillmore and Baker, 2001), automatic means of doing this are only now becoming available.</S>
			<S sid="30" ssid="12">Consider a sentence from one of the testing texts: (1) This geography is important in understanding Dublin.</S>
			<S sid="31" ssid="13">In the frame semantic analysis of this sentence, there are two predicators which FN has analyzed: important and understanding, as well as one which we have not yet analyzed, geography.</S>
			<S sid="32" ssid="14">In addition,Dublin is recognized by the NER system as a loca tion.</S>
			<S sid="33" ssid="15">In the gold standard annotation, we have the annotation shown in (2) for the Importance frame, evoked by the target important, and the annotationshown in (3) for the Grasp frame, evoked by under standing.(2) [FACTOR This geography] [COP is] IMPOR-TANT [UNDERTAKING in understanding Dublin].[INTERESTED PARTY INI](3) This geography is important in UNDER STANDING [PHENOMENON Dublin].</S>
			<S sid="34" ssid="16">[COGNIZERCNI] The definitions of the two frames begin like this: Importance: A FACTOR affects the outcome of anUNDERTAKING, which can be a goal-oriented activ ity or the maintenance of a desirable state, the work in a FIELD, or something portrayed as affecting an INTERESTED PARTY.</S>
			<S sid="35" ssid="17">Grasp: A COGNIZER possesses knowledge about the workings, significance, or meaning of an idea or object, which we call PHENOMENON, and is able to make predictions about the behavior or occurrence of the PHENOMENON.</S>
			<S sid="36" ssid="18">Using these definitions and the labels, and the fact that the target and FEs of one frame are subsumedby an FE of the other, we can compose the meanings of the two frames to produce a detailed para phrase of the meaning of the sentence: Something denoted by this geography is a factor which affects the outcome of the undertaking of understanding the location called ?Dublin?</S>
			<S sid="37" ssid="19">by any interested party.</S>
			<S sid="38" ssid="20">We have not dealt with geography as a frame-evoking expression, although we would eventually like to.</S>
			<S sid="39" ssid="21">(The preposition in serves only as a marker of the frame element UNDERTAKING.)</S>
			<S sid="40" ssid="22">In (2), the INTERESTED PARTY is not a label onany part of the text; rather, it is marked INI, for ?indefinite null instantiation?, meaning that it is con ceptually required as part of the frame definition, absent from the sentence, and not recoverable from the context as being a particular individual?meaning 100that this geography is important for anyone in general?s understanding of Dublin.</S>
			<S sid="41" ssid="23">In (3), the COG NIZER is ?constructionally null instantiated?, as thegerund understanding licenses omission of its sub ject.</S>
			<S sid="42" ssid="24">The marking of null instantiations is important in handling text coherence and was part of the goldstandard, but as far as we know, none of the participants attempted it, and it was ignored in the evalua tion.Note that we have collapsed the two null instantiated FEs, the INTERESTED PARTY of the impor tance frame and the COGNIZER in the Grasp frame, since they are not constrained to be distinct.</S>
			<S sid="43" ssid="25">2.2 Semantic dependency graphs.</S>
			<S sid="44" ssid="26">Since the role fillers are dependents (broadly speak ing) of the predicators, the full FrameNet annotation of a sentence is roughly equivalent to a dependency parse, in which some of the arcs are labeled with rolenames; and a dependency graph can be derived algorithmically from FrameNet annotation; an early ver sion of this was proposed by (Fillmore et al, 2004b)Fig.</S>
			<S sid="45" ssid="27">1 shows the semantic dependency graph derived from sentence (1); this graphical representa tion was derived from a semantic dependency XML file (see Sec.</S>
			<S sid="46" ssid="28">5).</S>
			<S sid="47" ssid="29">It shows that the top frame in this sentence is evoked by the word important, although the syntactic head is the copula is (here given the more general label ?Support?).</S>
			<S sid="48" ssid="30">The labels on thearcs are either the names of frame elements or indications of which of the daughter nodes are seman tic heads, which is important in some versions of the evaluation.</S>
			<S sid="49" ssid="31">The labels on nodes are either frame names (also colored gray), syntactic phrases types (e.g. NP), or the names of certain other syntactic ?connectors?, in this case, Marker and Support.</S>
	</SECTION>
	<SECTION title="Definition of the task. " number="3">
			<S sid="50" ssid="1">3.1 Training data.</S>
			<S sid="51" ssid="2">The major part of the training data for the task con sisted of the current data release from FrameNet(Release 1.3), described in Sec.2 This was supple mented by additional training data made availablethrough SemEval to participants in this task.</S>
			<S sid="52" ssid="3">In addition to updated versions of some of the full-text an notation from Release 1.3, three files from the ANC were included: from Slate.com, ?Stephanopoulos Importance: important Marker: in Undertaking NP Factor Grasp: understanding SemHead This geography Head NE:location: Dublin DenotedFE: location Phenomenon &lt;s&gt; Supp: is Head . SemHead Figure 1: Sample Semantic Dependency Graph Crimes?</S>
			<S sid="53" ssid="4">and ?Entrepreneur as Madonna?, and from the Berlitz travel guides, ?History of Jerusalem?.</S>
			<S sid="54" ssid="5">3.2 Testing data.</S>
			<S sid="55" ssid="6">The testing data was made up of three texts, none of which had been seen before; the gold standard consisted of manual annotations (by the FrameNetteam) of these texts for all frame evoking expressions and the fillers of the associated frame elements.</S>
			<S sid="56" ssid="7">All annotation of the testing data was carefully reviewed by the FN staff to insure its cor rectness.</S>
			<S sid="57" ssid="8">Since most of the texts annotated in the FN database are from the NTI website (www.nti.org), we decided to take two of the three test ing texts from there also.</S>
			<S sid="58" ssid="9">One, ?China Overview?, was very similar to other annotated texts such as ?Taiwan Introduction?, ?Russia Overview?, etc. available in Release 1.3.</S>
			<S sid="59" ssid="10">The other NTI text, ?Work Advances?, while in the same domain, was shorter and closer to newspaper style than the rest of the NTI texts.</S>
			<S sid="60" ssid="11">Finally, the ?Introduction to 101 Sents NEs Frames Tokens Types Work 14 31 174 77 China 39 90 405 125 Dublin 67 86 480 165 Totals 120 207 1059 272 Table 1: Summary of Testing DataDublin?, taken from the American National Cor pus (ANC, www.americannationalcorpus.</S>
			<S sid="61" ssid="12">org) Berlitz travel guides, is of quite a different genre, although the ?History of Jerusalem?</S>
			<S sid="62" ssid="13">text in the training data was somewhat similar.</S>
			<S sid="63" ssid="14">Table 1 gives some statistics on the three testing files.</S>
			<S sid="64" ssid="15">To give a flavor of the texts, here are two sentences; frame evoking words are in boldface: From ?Work Advances?: ?The Iranians are now willing to accept the installation of cameras only outside the cascade halls, which will not enable the IAEA to monitor the entire uranium enrichment process,?</S>
			<S sid="65" ssid="16">the diplomat said.</S>
			<S sid="66" ssid="17">From ?Introduction to Dublin?: And in this city, where literature and theater have historicallydominated the scene, visual arts are finally com ing into their own with the new Museum of Modern Art and the many galleries that display the work of modern Irish artists.</S>
	</SECTION>
	<SECTION title="Participants. " number="4">
			<S sid="67" ssid="1">A number of groups downloaded the training or test ing data, but in the end, only three groups submitted results: the UTD-SRL group and the LTH group, who submitted full results, and the CLR group who submitted results for frames only.</S>
			<S sid="68" ssid="2">It should also be noted that the LTH group had the testing data for longer than the 10 days allowed by the rules of the exercise, which means that the results of the two teams are not exactly comparable.</S>
			<S sid="69" ssid="3">Also, the results from the CLR group were initially formatted slightly differently from the gold standard with regard to character spacing; a later reformatting allowed their results to be scored with the other groups?.</S>
			<S sid="70" ssid="4">The LTH system used only SVM classifiers, while the UTD-SRL system used a combination of SVM and ME classifiers, determined experimentally.</S>
			<S sid="71" ssid="5">The CLR system did not use classifiers, but hand-written symbolic rules.</S>
			<S sid="72" ssid="6">Please consult the separate system papers for details about the features used.</S>
	</SECTION>
	<SECTION title="Evaluation. " number="5">
			<S sid="73" ssid="1">The labels-only matching was similar to previousshared tasks, but the dependency structure evaluation deserves further explanation: The XML seman tic dependency structure was produced by a program called fttosem, implemented in Perl, which goes sentence by sentence through a FrameNet full-text XML file, taking LU, FE, and other labels and using them to structure a syntactically unparsed piece of a sentence into a syntactic-semantic tree.</S>
			<S sid="74" ssid="2">Two basic principles allow us to produce this tree: (1) LUs are the sole syntactic head of a phrase whose semantics is expressed by their frame and (2) each label span is interpreted as the boundaries of a syntactic phrase, so that when a larger label span subsumes a smaller one, the larger span can be interpreted as a the highernode in a hierarchical tree.</S>
			<S sid="75" ssid="3">There are a fair num ber of complications, largely involving identifyingmismatches between syntactic and semantic headedness.</S>
			<S sid="76" ssid="4">Some of these (support verbs, copulas, modifiers, transparent nouns, relative clauses) are annotated in the data with their own labels, while others (syntactic markers, e.g. prepositions, and auxil iary verbs) must be identified using simple syntactic heuristics and part-of-speech tags.</S>
			<S sid="77" ssid="5">For this evaluation, a non-frame node counts as matching provided that it includes the head of the gold standard, whether or not non-head children ofthat node are included.</S>
			<S sid="78" ssid="6">For frame nodes, the partici pants got full credit if the frame of the node matched the gold standard.</S>
			<S sid="79" ssid="7">5.1 Partial credit for related frames.</S>
			<S sid="80" ssid="8">One of the problems inherent in testing against un seen data is that it will inevitably contain lexical units that have not previously been annotated in FrameNet, so that systems which do not generalizewell cannot get them right.</S>
			<S sid="81" ssid="9">In principle, the deci sion as to what frame to add a new LU to should be helped by the same criteria that are used to assign polysemous lemmas to existing frames.</S>
			<S sid="82" ssid="10">However,in practice this assignment is difficult, precisely be cause, unlike WSD, there is no assumption that all the senses of each lemma are defined in advance; if 102 the system can?t be sure that a new use of a lemma is in one of the frames listed for that lemma, thenit must consider all the 800+ frames as possibili ties.</S>
			<S sid="83" ssid="11">This amounts to the automatic induction of fine-grained semantic similarity from corpus data, a notoriously difficult problem (Stevenson and Joanis, 2003; Schulte im Walde, 2003).For LUs which clearly do not fit into any exist ing frames, the problem is still more difficult.</S>
			<S sid="84" ssid="12">In the course of creating the gold standard annotation of the three testing texts, the FN team created almost 40 new frames.</S>
			<S sid="85" ssid="13">We cannot ask that participants hit uponthe new frame name, but the new frames are not created in a vacuum; as mentioned above, they are almost always added to the existing structure of frame to-frame relations; this allows us to give credit for assignment to frames which are not the precise onein the gold standard, but are close in terms of frame to-frame relations.</S>
			<S sid="86" ssid="14">Whenever participants?</S>
			<S sid="87" ssid="15">proposed frames were wrong but connected to the right frameby frame relations, partial credit was given, decreas ing by 20% for each link in the frame-frame relationgraph between the proposed frame and the gold stan dard.</S>
			<S sid="88" ssid="16">For FEs, each frame element had to match the gold standard frame element and contain at least the same head word in order to gain full credit; again, partial credit was given for frame elements related via FE-to-FE relations.</S>
	</SECTION>
	<SECTION title="Results. " number="6">
			<S sid="89" ssid="1">Text Group Recall Prec.</S>
			<S sid="90" ssid="2">F1 Dublin UTD-SRL 0.4188 0.7716 0.5430 China UTD-SRL 0.5498 0.8009 0.6520 Work UTD-SRL 0.5251 0.8382 0.6457 Dublin LTH 0.5184 0.7156 0.6012 China LTH 0.6261 0.7731 0.6918 Work LTH 0.6606 0.8642 0.7488 Dublin CLR 0.3984 0.6469 0.4931 China CLR 0.4621 0.6302 0.5332 Work CLR 0.5054 0.7452 0.6023 Table 2: Frame Recognition onlyThe strictness of the requirement of exact boundary matching (which depends on an accurate syntac tic parse) is compounded by the cascading effect of semantic classification errors, as seen by comparing Text Group Recall Prec.</S>
			<S sid="91" ssid="3">F1 Label matching only Dublin UTD-SRL 0.27699 0.55663 0.36991 China UTD-SRL 0.31639 0.51715 0.39260 Work UTD-SRL 0.31098 0.62408 0.41511 Dublin LTH 0.36536 0.55065 0.43926 China LTH 0.39370 0.54958 0.45876 Work LTH 0.41521 0.61069 0.49433 Semantic dependency matching Dublin UTD-SRL 0.26238 0.53432 0.35194 China UTD-SRL 0.31489 0.53145 0.39546 Work UTD-SRL 0.30641 0.61842 0.40978 Dublin LTH 0.36345 0.54857 0.43722 China LTH 0.40995 0.57410 0.47833 Work LTH 0.45970 0.67352 0.54644Table 3: Results for combined Frame and FE recog nition the F-scores in Table 3 with those in Table 2.</S>
			<S sid="92" ssid="4">The difficulty of the task is reflected in the F-scores of around 35% for the most difficult text in the most difficult condition, but participants still managed to reach F-scores as high as 75% for the more limited task of Frame Identification (Table 2), which more closely matches traditional Senseval tasks, despite the lack of a full sense inventory.</S>
			<S sid="93" ssid="5">The difficulty posed by having such an unconstrained task led to understandably low recall scores in all participants (between 25 and 50%).</S>
			<S sid="94" ssid="6">The systems submitted by the teams differed in their sensitivity to differences in the texts: UTD-SRL?s system varied by around 10% across texts, while LTH?s varied by 15%.</S>
			<S sid="95" ssid="7">There are some rather encouraging results also.The participants rather consistently performed bet ter with our more complex, but also more useful andrealistic scoring, including partial credit and grad ing on semantic dependency rather than exact span match (compare the top and bottom halves of Table 3).</S>
			<S sid="96" ssid="8">The participants all performed relatively well onthe frame-recognition task, with precision scores av eraging 63% and topping 85%.</S>
	</SECTION>
	<SECTION title="Discussion. " number="7">
			<S sid="97" ssid="1">The testing data for this task turned out to be espe cially challenging with regard to new frames, since, in an effort to annotate especially thoroughly, almost 10340 new frames were created in the process of an notating these three specific passages.</S>
			<S sid="98" ssid="2">One result of this was that the test passages had more unseenframes than a random unseen passage, which prob ably lowered the recall on frames.</S>
			<S sid="99" ssid="3">It appears that this was not entirely compensated by giving partial credit for related frames.</S>
			<S sid="100" ssid="4">This task is a more advanced and realistic version of the Automatic Semantic Role Labeling task of Senseval-3 (Litkowski, 2004).</S>
			<S sid="101" ssid="5">Unlike that task, the testing data was previously unseen, participants had to determine the correct frames as a first step, and participants also had to determine FE boundaries, which were given in the Senseval-3.</S>
			<S sid="102" ssid="6">A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together.</S>
			<S sid="103" ssid="7">Thus, the FN SRL results are translatable fairly directly intoformal representations which can be used for rea soning, question answering, etc.</S>
			<S sid="104" ssid="8">(Scheffczyk et al., 2006; Frank and Semecky, 2004; Sinha and Narayanan, 2005).</S>
			<S sid="105" ssid="9">Despite the problems with recall, the participants have expressed a determination to work to improvethese results, and the FN staff are eager to collabo rate in this effort.</S>
			<S sid="106" ssid="10">A project is now underway at ICSI to speed up frame and LU definition, and another tospeed up the training of SRL systems is just begin ning, so the prospects for improvement seem good.This material is based in part upon work sup ported by the National Science Foundation under Grant No.</S>
			<S sid="107" ssid="11">IIS-0535297.</S>
	</SECTION>
</PAPER>
