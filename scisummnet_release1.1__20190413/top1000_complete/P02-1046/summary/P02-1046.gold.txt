Bootstrapping
This paper refines the analysis of co-training, defines and evaluates a new co-training algorithm that has theoretical justification, gives a theoretical justification for the Yarowsky algorithm, and shows that co-training and the Yarowsky algorithm are based on different independence assumptions.
We show that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption.
We refine Dasgupta et al's result by relaxing the view independence assumption with a new constraint.
We propose the Greedy Agreement Algorithm, which, based on two independent views of the data, learns two binary classifiers from a set of hand-typed seed rules.
We show that if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T, then the precision of the final classifier is larger than T.
We argue that the conditional independence assumption is remarkably strong and is rarely satisfied in real data sets, showing that a weaker independence assumption suffices.
