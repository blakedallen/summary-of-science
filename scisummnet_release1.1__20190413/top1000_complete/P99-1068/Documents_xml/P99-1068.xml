<PAPER>
  <S sid="0">Mining The Web For Bilingual Text</S>
  <ABSTRACT>
    <S sid="1" ssid="1">STRAND (Resnik, 1998) is a languageindependent system for automatic discovery of text in parallel translation on the World Wide Web.</S>
    <S sid="2" ssid="2">This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance.</S>
    <S sid="3" ssid="3">The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">Text in parallel translation is a valuable resource in natural language processing.</S>
    <S sid="5" ssid="2">Statistical methods in machine translation (e.g.</S>
    <S sid="6" ssid="3">(Brown et al., 1990)) typically rely on large quantities of bilingual text aligned at the document or sentence level, and a number of approaches in the burgeoning field of crosslanguage information retrieval exploit parallel corpora either in place of or in addition to mappings between languages based on information from bilingual dictionaries (Davis and Dunning, 1995; Landauer and Littman, 1990; Hull and Oard, 1997; Oard, 1997).</S>
    <S sid="7" ssid="4">Despite the utility of such data, however, sources of bilingual text are subject to such limitations as licensing restrictions, usage fees, restricted domains or genres, and dated text (such as 1980's Canadian politics); or such sources simply may not exist for language pairs of interest.</S>
    <S sid="8" ssid="5">Although the majority of Web content is in English, it also shows great promise as a source of multilingual content.</S>
    <S sid="9" ssid="6">Using figures from the Babel survey of multilinguality on the Web (http : f/www. isoc . org/), it is possible to estimate that as of June, 1997, there were on the order of 63000 primarily non-English Web servers, ranging over 14 languages.</S>
    <S sid="10" ssid="7">Moreover, a followup investigation of the non-English servers suggests that nearly a third contain some useful cross-language data, such as parallel English on the page or links to parallel English pages &#8212; the follow-up also found pages in five languages not identified by the Babel study (Catalan, Chinese, Hungarian, Icelandic, and Arabic; Michael Littman, personal communication).</S>
    <S sid="11" ssid="8">Given the continued explosive increase in the size of the Web, the trend toward business organizations that cross national boundaries, and high levels of competition for consumers in a global marketplace, it seems impossible not to view multilingual content on the Web as an expanding resource.</S>
    <S sid="12" ssid="9">Moreover, it is a dynamic resource, changing in content as the world changes.</S>
    <S sid="13" ssid="10">For example, Diekema et al., in a presentation at the 1998 TREC-7 conference (Voorhees and Harman, 1998), observed that the performance of their cross-language information retrieval was hurt by lexical gaps such as Bosnial Bosnie &#8212; this illustrates a highly topical missing pair in their static lexical resource (which was based on WordNet 1.5).</S>
    <S sid="14" ssid="11">And Gey et al., also at TREC-7, observed that in doing cross-language retrieval using commercial machine translation systems, gaps in the lexicon (their example was acupuncture' Akupunktur) could make the difference between precision of 0.08 and precision of 0.83 on individual queries.</S>
    <S sid="15" ssid="12">Resnik (1998) presented an algorithm called STRAND (Structural Translation Recognition for Acquiring Natural Data) designed to explore the Web as a source of parallel text, demonstrating its potential with a small-scale evaluation based on the author's judgments.</S>
    <S sid="16" ssid="13">After briefly reviewing the STRAND architecture and preliminary results (Section 2), this paper goes beyond that preliminary work in two significant ways.</S>
    <S sid="17" ssid="14">First, the framework is extended to include a filtering stage that uses automatic language identification to eliminate an important class of false positives: documents that appear structurally to be parallel translations but are in fact not in the languages of interest.</S>
    <S sid="18" ssid="15">The system is then run on a somewhat larger scale and evaluated formally for English and Spanish using measures of agreement with independent human judges, precision, and recall (Section 3).</S>
    <S sid="19" ssid="16">Second, the algorithm is scaled up more seriously to generate large numbers of parallel documents, this time for English and French, and again subjected to formal evaluation (Section 4).</S>
    <S sid="20" ssid="17">The concrete end result reported here is an automatically acquired English-French parallel corpus of Web documents comprising 2491 document pairs, approximately 1.5 million words per language (without markup), containing little or no noise.</S>
  </SECTION>
  <SECTION title="2 STRAND Preliminaries" number="2">
    <S sid="21" ssid="1">This section is a brief summary of the STRAND system and previously reported preliminary results (Resnik, 1998).</S>
    <S sid="22" ssid="2">The STRAND architecture is organized as a pipeline, beginning with a candidate generation stage that (over-)generates candidate pairs of documents that might be parallel translations.</S>
    <S sid="23" ssid="3">(See Figure 1.)</S>
    <S sid="24" ssid="4">The first implementation of the generation stage used a query to the Altavista search engine to generate pages that could be viewed as &amp;quot;parents&amp;quot; of pages in parallel translation, by asking for pages containing one portion of anchor text (the readable material in a hyperlink) containing the string &amp;quot;English&amp;quot; within a fixed distance of another anchor text containing the string &amp;quot;Spanish&amp;quot;.</S>
    <S sid="25" ssid="5">(The matching process was case-insensitive.)</S>
    <S sid="26" ssid="6">This generated many good pairs of pages, such as those pointed to by hyperlinks reading Click here for English version and Click here for Spanish version, as well as many bad pairs, such as university pages containing links to English Literature in close proximity to Spanish Literature.</S>
    <S sid="27" ssid="7">The candidate generation stage is followed by a candidate evaluation stage that represents the core of the approach, filtering out bad candidates from the set of generated page pairs.</S>
    <S sid="28" ssid="8">It employs a structural recognition algorithm exploiting the fact that Web pages in parallel translation are invariably very similar in the way they are structured &#8212; hence the 's' in STRAND.</S>
    <S sid="29" ssid="9">For example, see Figure 2.</S>
    <S sid="30" ssid="10">The structural recognition algorithm first runs both documents through a transducer that reduces each to a linear sequence of tokens corresponding to HTML markup elements, interspersed with tokens representing undifferentiated &amp;quot;chunks&amp;quot; of text.</S>
    <S sid="31" ssid="11">For example, the transducer would replace the HTML source text &lt;TITLE&gt;ACL '99 Conference Home Page&lt;/TITLE&gt; with the three tokens [BEGIN: TITLE], [Chunk:24], and [END: TITLE].</S>
    <S sid="32" ssid="12">The number inside the chunk token is the length of the text chunk, not counting whitespace; from this point on only the length of the text chunks is used, and therefore the structural filtering algorithm is completely language independent.</S>
    <S sid="33" ssid="13">Given the transducer's output for each document, the structural filtering stage aligns the two streams of tokens by applying a standard, widely available dynamic programming algorithm for finding an optimal alignment between two linear sequences.1 This alignment matches identical markup tokens to each other as much as possible, identifies runs of unmatched tokens that appear to exist only in one sequence but not the other, and marks pairs of non-identical tokens that were forced to be matched to each other in order to obtain the best alignment pos'Known to many programmers as diff.</S>
    <S sid="34" ssid="14">1111111111111110111111 Le vmdredi 25 cacao 1996.</S>
    <S sid="35" ssid="15">40 membres nit de la Seeman:anon oat assisth au stninaiste sr les psalm. exemplars en maniac de reglen.tation qui visait I aida la rnininbto 6.. familiarises avec P.ereglemsitation can.</S>
    <S sid="36" ssid="16">NINA. de rechange asa progr.unes de rtglanentasion.</S>
    <S sid="37" ssid="17">L'animanur.</S>
    <S sid="38" ssid="18">M. Zane BMW., aired:MP gannet Direction 40.</S>
    <S sid="39" ssid="19">61,.. de wasoonnation. owes la Mance en mkt.</S>
    <S sid="40" ssid="20">It, aphasia dtIndusthe Caned. pe rune de 'Introduction de nif animus de diversification 496 .6*.. A. an.ation da services comma les coda Yokota= l' snort, lons.tion de l'indastrie. a add que pseud. mochainement un document stu I. divenifirstse des modes de postai. da service qui .intrait de divers suj.. comas les anfmnisnes de premvion de rethange des semi. les phi. aggroprifs que A. problemes ...leafs par Nunn Cada volontalms Us code volontaire un ensemble d'engageme. scernaliffs &#8226; ne faisant pas eaplidtanent pink d'un Mgirrio lOgialatif ou rEglementaire - cone0 pom lateen.. faxonnex conneller ou israluer is canpartement de ceux qui la ont pan II.</S>
    <S sid="41" ssid="21">Istiliminent pax a pnnsuivi 6.9.. GINS analyse thincipal.</S>
    <S sid="42" ssid="22">Affair.. Ag).</S>
    <S sid="43" ssid="23">.6*.. au Racentariat du Conseil du Treece.</S>
    <S sid="44" ssid="24">Adroit du gaivemement de Makatea.. n, alma simplanent aux mMiciparas une solution de red.., I). reglemenmtioe p..). gomernernmat Au moment a) Is thglementation f objes true examen adru du public. ks gouvornemerns I lishelle town. van des approchen volonmirsat en ..5,190,14 A. la rigkoausion et mans comae submits i celle-d. les codes volontaires prima40 tin cumin menthe &amp;manages. notanstent sible.2 At this point, if there were too many unmatched tokens, the candidate pair is taken to be prima facie unacceptable and immediately filtered out.</S>
    <S sid="45" ssid="25">Otherwise, the algorithm extracts from the alignment those pairs of chunk tokens that were matched to each other in order to obtain the best alignments.3 It then computes the correlation between the lengths of these non-markup text chunks.</S>
    <S sid="46" ssid="26">As is well known, there is a reliably linear relationship in the lengths of text translations &#8212; small pieces of source text translate to small pieces of target text, medium to medium, and large to large.</S>
    <S sid="47" ssid="27">Therefore we can apply a standard statistical hypothesis test, and if p &lt; .05 we can conclude that the lengths are reliably correlated and accept the page pair as likely to be translations of each other.</S>
    <S sid="48" ssid="28">Otherwise, this candidate page pair is filtered out.4 2An anonymous reviewer observes that diff has no preference for aligning chunks of similar lengths, which in some cases might lead to a poor alignment when a good one exists.</S>
    <S sid="49" ssid="29">This could result in a failure to identify true translations and is worth investigating further.</S>
    <S sid="50" ssid="30">3Chunk tokens with exactly equal lengths are excluded; see (Resnik, 1998) for reasons and other details of the algorithm.</S>
    <S sid="51" ssid="31">'The level of significance (p &lt; .05) was the initial selection during algorithm development, and never changed.</S>
    <S sid="52" ssid="32">This, the unmatched-tokens threshold for prima facie rejection due to mismatches (20%), and the maximum distance between hyperlinks in the generaIn the preliminary evaluation, I generated a test set containing 90 English-Spanish candidate pairs, using the candidate generation stage as just described.</S>
    <S sid="53" ssid="33">I evaluated these candidates by hand, identifying 24 as true translation pairs.5 Of these 24, STRAND identified 15 as true translation pairs, for a recall of 62.5%.</S>
    <S sid="54" ssid="34">Perhaps more important, it only generated 2 additional translation pairs incorrectly, for a precision of 15/17 = 88.2%.</S>
  </SECTION>
  <SECTION title="3 Adding Language Identification" number="3">
    <S sid="55" ssid="1">In the original STRAND architecture, additional filtering stages were envisaged as possible (see Figure 1), including such languagedependent processes as automatic language identification and content-based comparison of structually aligned document segments using cognate matching or existing bilingual dictionaries.</S>
    <S sid="56" ssid="2">Such stages were initially avoided in order to keep the system simple, lightweight, and independent of linguistic resources.</S>
    <S sid="57" ssid="3">Howtion stage (10 lines), are parameters of the algorithm that were determined during development using a small amount of arbitrarily selected French-English data downloaded from the Web.</S>
    <S sid="58" ssid="4">These values work well in practice and have not been varied systematically; their values were fixed in advance of the preliminary evaluation and have not been changed since.</S>
    <S sid="59" ssid="5">5 The complete test set and my judgments for this preliminary evaluation can be found at http://umiacs.umd.edu/n&#8226;resnikiamta98/. ever, in conducting an error analysis for the preliminary evaluation, and further exploring the characteristics of parallel Web pages, it became evident that such processing would be important in addressing one large class of potential false positives.</S>
    <S sid="60" ssid="6">Figure 3 illustrates: it shows two documents that are generated by looking for &amp;quot;parent&amp;quot; pages containing hyperlinks to English and Spanish, which pass the structural filter with flying colors.</S>
    <S sid="61" ssid="7">The problem is potentially acute if the generation stage happens to yield up many pairs of pages that come from online catalogues or other Web sites having large numbers of pages with a conventional structure.</S>
    <S sid="62" ssid="8">There is, of course, an obvious solution that will handle most such cases: making sure that the two pages are actually written in the languages they are supposed to be written in.</S>
    <S sid="63" ssid="9">In order to filter out candidate page pairs that fail this test, statistical language identification based on character n-grams was added to the system (Dunning, 1994).</S>
    <S sid="64" ssid="10">Although this does introduce a need for language-specific training data for the two languages under consideration, it is a very mild form of language dependence: Dunning and others have shown that when classifying strings on the order of hundreds or thousands of characters, which is typical of the non-markup text in Web pages, it is possible to discriminate languages with accuracy in the high 90% range for many or most language pairs given as little as 50k characters per language as training material.</S>
    <S sid="65" ssid="11">For the language filtering stage of STRAND, the following criterion was adopted: given two documents d1 and d2 that are supposed to be in languages L1 and L2, keep the document pair iff Pr(Lildi) &gt; Pr(L214) and Pr(L21d2) &gt; Pr(Li I d2).</S>
    <S sid="66" ssid="12">For English and Spanish, this translates as a simple requirement that the &amp;quot;English&amp;quot; page look more like English than Spanish, and that the &amp;quot;Spanish&amp;quot; page look more like Spanish than English.</S>
    <S sid="67" ssid="13">Language identification is performed on the plain-text versions of the pages.</S>
    <S sid="68" ssid="14">Character 5-gram models for languages under consideration are constructed using 100k characters of training data from the European Corpus Initiative (Ed), available from the Linguistic Data Consortium (LDC).</S>
    <S sid="69" ssid="15">In a formal evaluation, STRAND with the new language identification stage was run for English and Spanish, starting from the top 1000 hits yielded up by Altavista in the candidate generation stage, leading to a set of 913 candidate pairs.</S>
    <S sid="70" ssid="16">A test set of 179 items was generated for annotation by human judges, containing: It was impractical to manually evaluate all pairs filtered out structurally, owing to the time required for judgments and the desire for two independent judgments per pair in order to assess inter-judge reliability.</S>
    <S sid="71" ssid="17">The two judges were both native speakers of Spanish with high proficiency in English, neither previously familiar with the project.</S>
    <S sid="72" ssid="18">They worked independently, using a Web browser to access test pairs in a fashion that allowed them to view pairs side by side.</S>
    <S sid="73" ssid="19">The judges were told they were helping to evaluate a system that identifies pages on the Web that are translations of each other, and were instructed to make decisions according to the following criterion: Is this pair of pages intended to show the same material to two different users, one a reader of English and the other a reader of Spanish?</S>
    <S sid="74" ssid="20">The phrasing of the criterion required some consideration, since in previous experience with human judges and translations I have found that judges are frequently unhappy with the quality of the translations they are looking at.</S>
    <S sid="75" ssid="21">For present purposes it was required neither that the document pair represent a perfect translation (whatever that might be), nor even necessarily a good one: STRAND was being tested not on its ability to determine translation quality, which might or might not be a criterion for inclusion in a parallel corpus, but rather its ability to facilitate the task of locating page pairs that one might reasonably include in a corpus undifferentiated by quality (or potentially postfiltered manually).</S>
    <S sid="76" ssid="22">The judges were permitted three responses: When computing evaluation measures, page pairs classified in the third category by a human judge, for whatever reason, were excluded from consideration.</S>
    <S sid="77" ssid="23">Table 1 shows agreement measures between the two judges, between STRAND and each individual judge, and the agreement between STRAND and the intersection of the two judges' annotations &#8212; that is, STRAND evaluated against only those cases where the two judges agreed, which are therefore the items we can regard with the highest confidence.</S>
    <S sid="78" ssid="24">The table also shows Cohen's lc, an agreement measure that corrects for chance agreement (Carletta, 1996); the most important value in the table is the value of 0.7 for the two human judges, which can be interpreted as sufficiently high to indicate that the task is reasonably well defined.</S>
    <S sid="79" ssid="25">(As a rule of thumb, classification tasks with ic &lt; 0.6 are generally thought of as suspect in this regard.)</S>
    <S sid="80" ssid="26">The value of N is the number of pairs that were included, after excluding those for which the human judgement in the comparison was undecided.</S>
    <S sid="81" ssid="27">Since the cases where the two judges agreed can be considered the most reliable, these were used as the basis for the computation of recall and precision.</S>
    <S sid="82" ssid="28">For this reason, and because the human-judged set included only a sample of the full set evaluated by STRAND, it was necessary to extrapolate from the judged (by both judges) set to the full set in order to compute recall/precision figures; hence these figures are reported as estimates.</S>
    <S sid="83" ssid="29">Precision is estimated as the proportion of pages judged GOOD by STRAND that were also judged to be good (i.e.</S>
    <S sid="84" ssid="30">&amp;quot;yes&amp;quot;) by both judges &#8212; this figure is 92.1% Recall is estimated as the number of pairs that should have been judged GOOD by STRAND (i.e. that recieved a &amp;quot;yes&amp;quot; from both judges) that STRAND indeed marked GOOD &#8212; this figure is 47.3%.</S>
    <S sid="85" ssid="31">These results can be read as saying that of every 10 document pairs included by STRAND in a parallel corpus acquired fully automatically from the Web, fewer than 1 pair on average was included in error.</S>
    <S sid="86" ssid="32">Equivalently, one could say that the resulting corpus contains only about 8% noise.</S>
    <S sid="87" ssid="33">Moreover, at least for the confidently judged cases, STRAND is in agreement with the combined human judgment more often than the human judges agree with each other.</S>
    <S sid="88" ssid="34">The recall figure indicates that for every true translation pair it accepts, STRAND must also incorrectly reject a true translation pair.</S>
    <S sid="89" ssid="35">Alternatively, this can be interpreted as saying that the filtering process has the system identifying about half of the pairs it could in principle have found given the candidates produced by the generation stage.</S>
    <S sid="90" ssid="36">Error analysis suggests that recall could be increased (at a possible cost to precision) by making structural filtering more intelligent; for example, ignoring some types of markup (such as italics) when computing alignments.</S>
    <S sid="91" ssid="37">However, I presume that if the number M of translation pairs on the Web is large, then half of M is also large.</S>
    <S sid="92" ssid="38">Therefore I focus on increasing the total yield by attempting to bring the number of generated candidate pairs closer to M, as described in the next section.</S>
  </SECTION>
  <SECTION title="4 Scaling Up Candidate Generation" number="4">
    <S sid="93" ssid="1">The preliminary experiments and the new experiment reported in the previous section made use of the Altavista search engine to locate &amp;quot;parent&amp;quot; pages, pointing off to multiple language versions of the same text.</S>
    <S sid="94" ssid="2">However, the same basic mechanism is easily extended to locate &amp;quot;sibling&amp;quot; pages: cases where the page in one language contains a link directly to the translated page in the other language.</S>
    <S sid="95" ssid="3">Exploration of the Web suggests that parent pages and sibling pages cover the major relationships between parallel translations on the Web.</S>
    <S sid="96" ssid="4">Some sites with bilingual text are arranged according to a third principle: they contain a completely separate monolingual sub-tree for each language, with only the single top-level home page pointing off to the root page of single-language version of the site.</S>
    <S sid="97" ssid="5">As a first step in increasing the number of generated candidate page pairs, STRAND was extended to permit both parent and sibling search criteria.</S>
    <S sid="98" ssid="6">Relating monolingual sub-trees is an issue for future work.</S>
    <S sid="99" ssid="7">In principle, using Altavista queries for the candidate generation stage should enable STRAND to locate every page pair in the Altavista index that meets the search criteria.</S>
    <S sid="100" ssid="8">This likely to be an upper bound on the candidates that can be obtained without building a Web crawler dedicated to the task, since one of Altavista's distinguishing features is the size of its index.</S>
    <S sid="101" ssid="9">In practice, however, the user interface for Altavista appears to limit the number of hits returned to about the first 1000.</S>
    <S sid="102" ssid="10">It was possible to break this barrier by using a feature of Altavista's &amp;quot;Advanced Search&amp;quot;: including a range of dates in a query's selection criteria.</S>
    <S sid="103" ssid="11">Having already redesigned the STRAND generation component to permit multiple queries (in order to allow search for both parent and sibling pages), each query in the query set was transformed into a set of mutually exclusive queries based on a one-day range; for example, one version of a query would restrict the result to pages last updated on 30 November 1998, the next 29 November 1998, and so forth.</S>
    <S sid="104" ssid="12">Although the solution granularity was not perfect &#8212; searches for some days still bumped up against the 1000-hit maximum &#8212; use of both parent and sibling queries with date-range restricted queries increased the productivity of the candidate generation component by an order of magnitude.</S>
    <S sid="105" ssid="13">The scaled-up system was run for English-French document pairs in late November, 1998, and the generation component produced 16763 candidate page pairs (with duplicates removed), an 18-fold increase over the previous experiment.</S>
    <S sid="106" ssid="14">After eliminating 3153 page pairs that were either exact duplicates or irretrievable, STRAND'S structural filtering removed 9820 candidate page pairs, and the language identification component removed another 414.</S>
    <S sid="107" ssid="15">The remaining pairs identified as GOOD &#8212; i.e. those that STRAND considered to be parallel translations &#8212; comprise a parallel corpus of 3376 document pairs.</S>
    <S sid="108" ssid="16">A formal evaluation, conducted in the same fashion as the previous experiment, yields the agreement data in Table 2.</S>
    <S sid="109" ssid="17">Using the cases where the two human judgments agree as ground truth, precision of the system is estimated at 79.5%, and recall at 70.3%.</S>
    <S sid="110" ssid="18">A look at STRAND's errors quickly identifies the major source of error as a shortcoming of the language identification module: its implicit assumption that every document is either in English or in French.</S>
    <S sid="111" ssid="19">This assumption was violated by a set of candidates in the test set, all from the same site, that pair Dutch pages with French.</S>
    <S sid="112" ssid="20">The language identification criterion adopted in the previous section requires only that the Dutch pages look more like English than like French, which in most cases is true.</S>
    <S sid="113" ssid="21">This problem is easily resolved by training the existing language identification component with a wider range of languages, and then adopting a stricter filtering criterion requiring that Pr(Englishldi) &gt; Pr(Lidi) for every language L in that range, and that d2 meet the corresponding requirement for French.'</S>
    <S sid="114" ssid="22">Doing so leads to the results in Table 3.</S>
    <S sid="115" ssid="23">This translates into an estimated 100% precision against 64.1% recall, with a yield of 2491 documents, approximately 1.5 million words per language as counted after removal of HTML markup.</S>
    <S sid="116" ssid="24">That is, with a reasonable though admittedly post-hoc revision of the language identification criterion, comparison with human subjects suggests the acquired corpus is nontrivial and essentially noise free, and moreover, that the system excludes only a third of the pages that should have been kept.</S>
    <S sid="117" ssid="25">Naturally this will need to be verified in a new evaluation on fresh data.</S>
    <S sid="118" ssid="26">6Language ID across a wide range of languages is not difficult to obtain.</S>
    <S sid="119" ssid="27">E.g. see the 13-language set of the freely available CMU stochastic language iden</S>
  </SECTION>
  <SECTION title="5 Conclusions" number="5">
    <S sid="120" ssid="1">This paper places acquisition of parallel text from the Web on solid empirical footing, making a number of contributions that go beyond the preliminary study.</S>
    <S sid="121" ssid="2">The system has been extended with automated language identification, and scaled up to the point where a nontrivial parallel corpus of English and French can be produced completely automatically from the World Wide Web.</S>
    <S sid="122" ssid="3">In the process, it was discovered that the most lightweight use of language identification, restricted to just the the language pair of interest, needed to be revised in favor of a strategy that includes identification over a wide range of languages.</S>
    <S sid="123" ssid="4">Rigorous evaluation using human judges suggests that the technique produces an extremely clean corpus &#8212; noise estimated at between 0 and 8% &#8212; even without human intervention, requiring no more resources per language than a relatively small sample of text used to train automatic language identification.</S>
    <S sid="124" ssid="5">Two directions for future work are apparent.</S>
    <S sid="125" ssid="6">First, experiments need to be done using languages that are less common on the Web.</S>
    <S sid="126" ssid="7">Likely first pairs to try include English-Korean, English-Italian, and English-Greek.</S>
    <S sid="127" ssid="8">Inspection of Web sites &#8212; those with bilingual text identified by STRAND and those without &#8212; suggests that the strategy of using Altavista to generate candidate pairs could be improved upon significantly by adding a true Web crawler to &amp;quot;mine&amp;quot; sites where bilingual text is known to be available, e.g. sites uncovered by a first pass of the system using the Altavista engine.</S>
    <S sid="128" ssid="9">I would conjecture that for English-French there is an order of magnitude more bilingual text on the Web than that uncovered in this early stage of research.</S>
    <S sid="129" ssid="10">A second natural direction is the application of Web-based parallel text in applications such as lexical acquisition and cross-language information retrieval &#8212; especially since a sideeffect of the core STRAND algorithm is aligned &amp;quot;chunks&amp;quot;, i.e. non-markup segments found to correspond to each other based on alignment of the markup.</S>
    <S sid="130" ssid="11">Preliminary experiments using even small amounts of these data suggest that standard techniques, such as cross-language lexical association, can uncover useful data.</S>
  </SECTION>
</PAPER>
