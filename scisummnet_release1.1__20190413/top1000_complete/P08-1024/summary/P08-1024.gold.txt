A Discriminative Latent Variable Model for Statistical Machine Translation
Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.
We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.
We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.
Results show that accounting for multiple derivations does indeed improve performance.
Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.
We show that marginalizing out the different segmentations during decoding leads to improved performance.
We present a latent variable model that describes the relationship between translation and derivation clearly.
For the hierarchical phrase-based approach, we present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations.
