<PAPER>
  <S sid="0" ssid="0">Word Sense Disambiguation using Conceptual Density Eneko Agirre* Lengoaia eta Sistema Informatikoak saila.</S>
  <S sid="1" ssid="1">Euskal Herriko Universitatea.</S>
  <S sid="2" ssid="2">649, 200800 Donostia.</S>
  <S sid="3" ssid="3">jibagbee@si.heu.es German Rigau** Departament de Llenguatges i Sistemes Informhtics.</S>
  <S sid="4" ssid="4">Universitat Polit~cnica de Catalunya.</S>
  <S sid="5" ssid="5">Pau Gargallo 5, 08028 Barcelona.</S>
  <S sid="6" ssid="6">g.rigau@lsi.upc.es Abst ract .</S>
  <S sid="7" ssid="7">This paper presents a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus.</S>
  <S sid="8" ssid="8">The method relies on the use oil the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose.</S>
  <S sid="9" ssid="9">This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process.</S>
  <S sid="10" ssid="10">The results of the experiments have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus.</S>
  <S sid="11" ssid="11">1 Int roduct ion Much of recent work in lexical ambiguity resolution offers the prospect hat a disambiguation system might be able to receive as input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency.</S>
  <S sid="12" ssid="12">The most extended approach use the context of the word to be disambiguatcd together with inlormation about each of its word senses to solve this problem.</S>
  <S sid="13" ssid="13">Interesting experiments have been performed in recent years using preexisting lexical knowledge resources: [Cowie el al.</S>
  <S sid="14" ssid="14">92], [Wilks et al.</S>
  <S sid="15" ssid="15">93] with LDOCE, [Yarowsky 92] with Rogets International Thesaurus, and [Sussna 93], [Voorhees 9311, [Richardson etal.</S>
  <S sid="16" ssid="16">94], [Resnik 95] with WordNet.</S>
  <S sid="17" ssid="17">Although each of these techniques looks promising for disambiguation, either they have been only applied to a small number of words, a few sentences or not in a public domain corpus.</S>
  <S sid="18" ssid="18">For this reason we have tried to disambiguate all the nouns from real *Eneko Agirre was supported by a grant from the Basque Goverment.</S>
  <S sid="19" ssid="19">Part of this work is included in projects 141226-TA248/95 of the Basque Country University and PI95-054 of the Basque Government.</S>
  <S sid="20" ssid="20">**German Rigau was supported by a grant from the Ministerio de Educaci6n y Ciencia.</S>
  <S sid="21" ssid="21">texts in the public domain sense tagged version of the Brown corpus [Francis &amp; Kucera 67], [Miller et al.</S>
  <S sid="22" ssid="22">93], also called Semantic Concordance or SemCor for short 1, The words in SemCor are tagged with word senses from WordNet, a broad semantic taxonomy for English [Miller 90] 2.</S>
  <S sid="23" ssid="23">Thus, SemCor provides an appropriate nvironment for testing our procedures and comparing among alternatives in a fully automatic way.</S>
  <S sid="24" ssid="24">The automatic decision procedure for lexical ambiguity resolution presented in this paper is based on an elaboration of the conceptual distance among concepts: Conceptual Density [Agirre &amp; Rigau 95].</S>
  <S sid="25" ssid="25">Thc system needs to know how words are clustered in semantic classes, and how semantic classes are hierarchically organised.</S>
  <S sid="26" ssid="26">For this purpose, we have used WordNet.</S>
  <S sid="27" ssid="27">Our system tries to resolve the lexical ambiguity ot nouns by finding the combination of senses from a set of contiguous nouns that maximises the Conceptual Density among senses.</S>
  <S sid="28" ssid="28">The perlbrmance ofthe procedure was tested on four SemCor texts chosen at random.</S>
  <S sid="29" ssid="29">For comparison purposes two other approaches, [Sussna 93] and [Yarowsky 92], were also tried.</S>
  <S sid="30" ssid="30">The results show that our algorithm performs better on the test set.</S>
  <S sid="31" ssid="31">Following this short introduction the Conceptual Dcnsity formula is presented.</S>
  <S sid="32" ssid="32">The main procedure to resolve lexical ambiguity of nouns using Conceptual Density is sketched on section 3.</S>
  <S sid="33" ssid="33">Section 4 describes extensively the experiments and its results.</S>
  <S sid="34" ssid="34">Finally, sections 5 and 6 deal with further work and conclusions.</S>
  <S sid="35" ssid="35">1Semcor comprises approximately 250,000 words.</S>
  <S sid="36" ssid="36">Tile tagging was done manually, and the error rate measured by the authors is around 10% for polysemous words.</S>
  <S sid="37" ssid="37">2The senses of a word are represented by synonym sets (or synscts), one for each word sense.</S>
  <S sid="38" ssid="38">The nominal part of WordNct can be viewed as a tangled hierarchy of hypo/hypernymy relations among synsets.</S>
  <S sid="39" ssid="39">Nominal relations include also three kinds of meronymic relations, which can be paraphrased asmember-of, made- of and component-part-of.</S>
  <S sid="40" ssid="40">The version used in this work is WordNet 1.4, The coverage in WordNet of senses lot open-class words in SemCor reaches 96% according to the authors.</S>
  <S sid="41" ssid="41">3_(5 2 Conceptual Density and Word Sense Disambiguation Conceptual distance tries to provide a basis for measuring closeness in meaning among words, taking as reference a structured hierarchical net.</S>
  <S sid="42" ssid="42">Conceptual distance between two concepts is defined in IRada et al.</S>
  <S sid="43" ssid="43">89] as the length of the shortest path that connects the concepts in a hierarchical semantic net.</S>
  <S sid="44" ssid="44">In a similar approach, [Sussna 931 employs the notion of conceptual distance between etwork nodes in order to improve precision during document indexing.</S>
  <S sid="45" ssid="45">[Resnik 95] captures semantic similarfly (closely related to conceptual distance) by means of the information content of the concepts in a hierarchical net.</S>
  <S sid="46" ssid="46">In general these alw;oaches focus on nouns.</S>
  <S sid="47" ssid="47">The measure ()1 conceptual distance among concepts we are looking for should be scnsflive Io: ?</S>
  <S sid="48" ssid="48">the length of the shortest palh that connects lhe concepts involved.</S>
  <S sid="49" ssid="49">the depth in the hierarchy: concepts in a deeper part of the hierarchy should be ranked closer.</S>
  <S sid="50" ssid="50">the density of concepts in the hierarchy: concepts in a dense part of the hierarchy are relatively closer than those in a more sparse region.</S>
  <S sid="51" ssid="51">- tile measure should be independent of the lltllllber o1 concepts we are measuring.</S>
  <S sid="52" ssid="52">We have experimented willl several fornmlas that follow the four criteria presented above.</S>
  <S sid="53" ssid="53">The experiments reported here were pcrformcd using the Conceptual Density formuhl [Agirre &amp; Rigau 95], which compares areas of subhierarchies.</S>
  <S sid="54" ssid="54">To illustrate how Conceptual 1)ensity can help to disambiguate a word, in figure I lhe word W has four senses and several context words.</S>
  <S sid="55" ssid="55">Each sense of the words belongs to a subhierarchy of WordNct.</S>
  <S sid="56" ssid="56">Tile dots in the subhierarchies represent the senses of eilhcr the word to be disambiguated (W) or the words in the context.</S>
  <S sid="57" ssid="57">Conceptual Density will yield the highest density for lhe subhierarchy containing more senses of lhose, rehttive to the total amount of senses in the subhierarchy.</S>
  <S sid="58" ssid="58">Tim sense o1 W contained in the subhierarchy with highest Conceptual l)ensity will be chosen as the sense disambiguating W in the given context.</S>
  <S sid="59" ssid="59">In figure 1, sense2 would be chosen.</S>
  <S sid="60" ssid="60">W W0~d to be disarlJ0iguated: W Context words: wl w2 w3 w4 ...</S>
  <S sid="61" ssid="61">Figure 1: senses of a word in WordNet Given a concept c, at the top of a sulfifierarchy, and given nhyp (mean number of hyponyms per node), the Conceptual Density for c when its subhierarchy contains a number m (nmrks) of senses of the words to disambiguate is given by the [ormula below: m- I Z .0 20 nh37~ CI)(c, m)- ,::0 descendants,, (1) l;ornlula I shows a lmralneter that was COlnputed experimentally.</S>
  <S sid="62" ssid="62">The 0.20 tries to smooth the exponential i, as m ranges between I and tim total number of senses in WordNet.</S>
  <S sid="63" ssid="63">Several values were Ified for the parameter, and it was found that the best lmrlormanee was attained consistently when the parameter was near 0.20.</S>
  <S sid="64" ssid="64">3 The Disambiguation Algorithm Using Conceptual Density Given a window size, the program moves the window one noun at a time from the beginning of the document owards its end, disambiguating in each step the noun in the middle of the window and considering the other nouns in the window as contexl.</S>
  <S sid="65" ssid="65">Non-noun words are ,lot taken into account.</S>
  <S sid="66" ssid="66">The algorilhm Io disambiguate a given noun w in tile middle of a window o1 nouns W (c.f.</S>
  <S sid="67" ssid="67">figure 2) roughly proceeds its folk)ws: S tep  ].)</S>
  <S sid="68" ssid="68">S tep  2) S tep  3) Step 4) Step 5) t:r:ee :-: compute  t ree(words  in  w indow) loop tree ::: compute  conc(~ptua] d i s tanco( t ree) concept  -= se].occt concept  w i th  llighest-._weigth(tree) J.f concept  :: null.</S>
  <S sid="69" ssid="69">t:hen exJ_tloop tree := inark d:[sambigui.tted senses  ( t ree,concept) end ]  oop output  disambJguatJ .on ~esu].t (tree) Figure 2: algori(hm for each window 17 First, the algorithm represents in a lattice the nouns present in the window, their senses and hypernyms (step 1).</S>
  <S sid="70" ssid="70">Then, the program computes the Conceptual Density of each concept in WordNet according to the senses it contains in its subhierarchy (step 2).</S>
  <S sid="71" ssid="71">It selects the concept c with highest Conceptual Density (step 3) and selects the senses below it as the correct senses for the respective words (step 4).</S>
  <S sid="72" ssid="72">The algorithm proceeds then to compute the density for the remaining senses in the lattice, and continues to disambiguate he nouns left in W (back to steps 2, 3 and 4).</S>
  <S sid="73" ssid="73">When no further disambiguation is possible, the senses left for w are processed and the result is presented (step 5).</S>
  <S sid="74" ssid="74">Besides completely disambiguating a word or failing to do so, in some cases the disambiguation algorithm returns everal possible senses for a word.</S>
  <S sid="75" ssid="75">In the experiments we considered these partial outcomes as failure to disambiguate.</S>
  <S sid="76" ssid="76">4 The  Exper iments 4.1 The texts We selected four texts from SemCor at random: br- a01 (where a stands for gender "Press: Reportage"), br-b20 (b for "Press: Editorial"), br-j09 (j means "Learned: Science") and br-r05 (r for "Humour").</S>
  <S sid="77" ssid="77">Table 1 shows some statistics for each text.</S>
  <S sid="78" ssid="78">text words nouns nouns monosemous in WN br-a01 2079 564 464 149 (32%) br-ab20 2153 453 377 128 (34%) br-.i09 2495 620 586 205 (34%) br-r05 2407 457 431 120 (27%) total 9134 2094 1858 602 (32%) Table 1 : data for each text An average of 11% of all nouns in these four texts were not found in WordNet.</S>
  <S sid="79" ssid="79">According to this data, the amount of monosemous nouns in these texts is bigger (32% average) than the one calculated for the open-class words fiom the whole SemCor (27.2% according to [Miller et al.</S>
  <S sid="80" ssid="80">For our experiments, these texts play both the role of input files (without semantic tags) and (tagged) test files.</S>
  <S sid="81" ssid="81">When they are treated as input files, we throw away all non-noun words, only leaving the lemmas of the nouns present in WordNet.</S>
  <S sid="82" ssid="82">4.2 Resul ts  and eva luat ion One of the goals of the experiments was to decide among different variants of the Conceptual Density formula.</S>
  <S sid="83" ssid="83">Results are given averaging the results of the four files.</S>
  <S sid="84" ssid="84">Partial disambiguation is treated as failure to disambiguate.</S>
  <S sid="85" ssid="85">Precision (that is, the percentage of actual answers which were correct) and recall (that is, the percentage ofpossible answers which were correct) are given in terms of polysemous nouns only.</S>
  <S sid="86" ssid="86">Graphs are drawn against the size of the context 3 .</S>
  <S sid="87" ssid="87">meronymy does  not  improve per fo rmance  as expected.</S>
  <S sid="88" ssid="88">A priori, the more relations are taken in account (e.i.</S>
  <S sid="89" ssid="89">meronymic relations, in addition to the hypo/hypernymy relation) the better density would capture semantic relatedness, and therefore better esults can be expected.</S>
  <S sid="90" ssid="90">44 ~I~A 43 v 42 O -4 41 40 39 % meron - - -o - - -  hyper 38 I I i I Window Size Figure 3: meronymy and hyperonymy The experiments ( ee figure 3) showed that there is not much difference; adding meronymic information does not improve precision, and raises coverage only 3% (approximately).</S>
  <S sid="91" ssid="91">Nevertheless, in the rest of the results reported below, meronymy and hypernymy were used.</S>
  <S sid="92" ssid="92">g lobal  nhyp  is as good as local  nhyp.</S>
  <S sid="93" ssid="93">The average number of hypouyms or nhyp (c.f.</S>
  <S sid="94" ssid="94">formula 1) can be approximated in two ways.</S>
  <S sid="95" ssid="95">If an independent hyp is computed for every concept in WordNet we call it local nhyp.</S>
  <S sid="96" ssid="96">If instead, a unique nhyp is computed using the whole hierarchy, we have global nhyp.</S>
  <S sid="97" ssid="97">44 43 A 42 .o 41 - ?~ 40- 39 38 local I I i I o ~, o ,~ ,9, Window Size Figure 4: local nhyp vs. global nhyp 3context size is given in terms of nouns.</S>
  <S sid="98" ssid="98">18 While local nhyp is the actual average for a given concept, global nhyp gives only an estimation.</S>
  <S sid="99" ssid="99">The results (c.f.</S>
  <S sid="100" ssid="100">figure 4) show that local nhyp performs only slightly better.</S>
  <S sid="101" ssid="101">Therefore global nhyp is favoured and was used in subsequent experiments.</S>
  <S sid="102" ssid="102">context  s i ze :  d i f fe rent  behav ionr  for each text.</S>
  <S sid="103" ssid="103">One could assume that the more context lhere is, the better the disambiguation results would be.</S>
  <S sid="104" ssid="104">Our experiments how that each file from SemCor has a different behaviour (c.f.</S>
  <S sid="105" ssid="105">figure 5) while br-b20 shows clear improvement for bigger window sizes, br-r05 gets a local maximum at a 10 size window, etc.</S>
  <S sid="106" ssid="106">50 45 v .o 4o ID I:1, 35 30 - - t3 - - -  br-a01 + br-b20 + br-r05 ----o---- br-j09 I I o ~, g - -  average I I Window Size Figure 5: context size and different filcs As each text is structured a list of sentences, lacking any indication of headings, sections, paragraph endings, text changes, etc.</S>
  <S sid="107" ssid="107">the program gathers the context without knowing whether the nouns actually occur in coherent pieces of text.</S>
  <S sid="108" ssid="108">This could account for the fact that in br-r05, composed mainly by short pieces of dialogues, the best results are for window size 10, the average size of this dialogue pieces.</S>
  <S sid="109" ssid="109">Likewise, the results for br-a01, which contains short journalistic texts, are hest for window sizes from 15 to 25, decreasing significatly for size 30.</S>
  <S sid="110" ssid="110">Ill addition, the actual nature of each text is for sure an impommt factor, difficult to measure, which could account for the different behawfiur on its own.</S>
  <S sid="111" ssid="111">In order to give an overall view of the performance, we consider the average hehaviour.</S>
  <S sid="112" ssid="112">file vs. sense.</S>
  <S sid="113" ssid="113">WordNct groups noun senses in 24 lexicographers files.</S>
  <S sid="114" ssid="114">The algorithm assigns a noun both an specific sense and a file label.</S>
  <S sid="115" ssid="115">Both file matches and sense matches are interesting to count.</S>
  <S sid="116" ssid="116">Whilc the sense level gives a fine graded measure of the algorithm, the file level gives an indication of the perlormance if we were interested in a less sharp level of disambiguation.</S>
  <S sid="117" ssid="117">The granularity of the sense distinctions made in [Hearst, 91], [Yarowsky 92] and [Gale t al.</S>
  <S sid="118" ssid="118">93] also called homographs in [Guthrie t al.</S>
  <S sid="119" ssid="119">931], can be compared to that of the file level in WordNct.</S>
  <S sid="120" ssid="120">For instance, in [Yarowsky 92] two homographs of tile noun }liNg are  considered, one characterised as MUSIC and the other as ANIMAL, INSECT.</S>
  <S sid="121" ssid="121">In WordNet, the 6 senses of I~t~s related to music appear in the following files: ARTIFACT, ATTRIBUTE, COMMUNICATION and PERSON.</S>
  <S sid="122" ssid="122">The 3 senses related to animals appear in the files ANIMAL and FOOD.</S>
  <S sid="123" ssid="123">This mcans that while the homograph level in [Yarowsky 92] distinguishes two sets of senses, the file level in WordNet distinguishes six sets of senses, still finer in granularity.</S>
  <S sid="124" ssid="124">Figure 6 shows that, as expected, file-level matches attain better performance (71.2% overall and 53.9% for polysemic nouns) than sense-level matches.</S>
  <S sid="125" ssid="125">o  45 ID 40 35 - - -0 - -  Sense I I "  - I  I Window Size Figure 6: sense level vs. file level ?</S>
  <S sid="126" ssid="126">eva luat ion  o f  the  results Figure 7 shows that, overall, coverage over polyscmous nonns increases ignificantly with the window size, without losing precision.</S>
  <S sid="127" ssid="127">Coverage tends to get stabilised near 80%, getting little improvement for window sizes bigger than 20.</S>
  <S sid="128" ssid="128">The figure also shows the guessing baseline, given hy selecting senses at random.</S>
  <S sid="129" ssid="129">This baseline was first calculated analytically and later checked experimentally.</S>
  <S sid="130" ssid="130">We also compare the performance of our algorithm with that of the "most frequent" heuristic.</S>
  <S sid="131" ssid="131">The frequency counts for each sense were collected using the rest of SemCor, and then applied to the [our texts.</S>
  <S sid="132" ssid="132">While the precision is similar to that of our algorithm, the coverage is 8% worse.</S>
  <S sid="133" ssid="133">3_9 80- 70 6O - 50- 40- Coverage:  ~ semantic density .</S>
  <S sid="134" ssid="134">most  frequent Precis ion: - - - -0- - -  semantic density .</S>
  <S sid="135" ssid="135">most  frequent guessing 3o - - T  [ T 1 Window Size Figure 7: precision and coverage All the data for the best window size can be seen in table 2.</S>
  <S sid="136" ssid="136">The precision and coverage shown in all the preceding graphs were relative to the polysemous nouns only.</S>
  <S sid="137" ssid="137">Including monosemic nouns precision raises, as shown in table 2, from 43% to 64.5%, and the coverage increases from 79.6% to 86.2%.</S>
  <S sid="138" ssid="138">% w:30 II Cove,-.</S>
  <S sid="139" ssid="139">I Prec [Recall overall File 86.2 71.2 61.4 Sense 64.5 55.5 polysemic File 79.6 53.9 42.8 Sense 43 34.2 Table 2: overall data for the best window size 4.3  Compar i son  w i th  o ther  works The raw results presented here seem to be poor when compared to those shown in [Hearst 91], [Gale et al.</S>
  <S sid="140" ssid="140">93] and [Yarowsky 9211.</S>
  <S sid="141" ssid="141">We think that several factors make the comparison difficult.</S>
  <S sid="142" ssid="142">Most of those works focus in a selected set of a few words, generally with a couple of senses of very different meaning (coarse-grained istinctions), and for which their algorithm could gather enough evidence.</S>
  <S sid="143" ssid="143">On the contrary, we tested our method with all the nouns in a subset of an unfestricted public domain corpus (more than 9.000 words), making fine-grained distinctions among all the senses in WordNct.</S>
  <S sid="144" ssid="144">An approach that uses hierarchical knowledge is that of [Resnik 9511, which additionally uses the information content of each concept gathered from corpora.</S>
  <S sid="145" ssid="145">Unfortunately he applies his method on a different ask, that of disambiguating sets of related nouns.</S>
  <S sid="146" ssid="146">The evaluation is done on a set of related nouns from Rogers Thesaurus tagged by hand.</S>
  <S sid="147" ssid="147">The fact that some senses were discarded because the human judged them not reliable makes comparison even more difficult.</S>
  <S sid="148" ssid="148">In order to compare our approach we decided to implement [Yarowsky 92] and [Sussna 93], and test them on our texts.</S>
  <S sid="149" ssid="149">For [Yarowsky 92] we had to adapt it to work with WordNet.</S>
  <S sid="150" ssid="150">His method relies on cooccurrence data gathered on Rogets Thesaurus semantic ategories.</S>
  <S sid="151" ssid="151">Instead, on our experiment we use saliency values 4 based on the lexicographic file tags in SemCor.</S>
  <S sid="152" ssid="152">The results for a window size of 50 nouns are those shown in table 35.</S>
  <S sid="153" ssid="153">Tile precision attained by our algorithm is higher.</S>
  <S sid="154" ssid="154">To compare figures better consider the results in table 4, were the coverage of our algorithm was easily extended using the version presented below, increasing recall to 70.1%.</S>
  <S sid="155" ssid="155">[+ ii+?v+ i c ,=11 I C.Density 86.2 71.2 J 6 .4 Yarowsky 100.0 64.0 1 64.0 Table 3: comparison with [Yarowsky 9211 From the methods based on Conceptual Distance, [Sussna 9311 is the most similar to ours.</S>
  <S sid="156" ssid="156">Sussna disambiguates everal documents from a public corpus using WordNet.</S>
  <S sid="157" ssid="157">The test set was tagged by hand, allowing more than one correct senses for a single word.</S>
  <S sid="158" ssid="158">The method he uses has to overcome a combinatorial explosion 6 controlling the size of the window and "freezing" the senses for all the nouns preceding the noun to be disambiguated.</S>
  <S sid="159" ssid="159">In order to fieeze the winning sense Sussnas algorithm is forced to make a unique choice.</S>
  <S sid="160" ssid="160">When Conceptual Distance is not able to choose a single sense, the algorithm chooses one at random.</S>
  <S sid="161" ssid="161">Conceptual Density overcomes the combinatorial explosion extending the notion of conceptual distance from a pair of words to n words, and therefore can yield more than one correct sense for a word.</S>
  <S sid="162" ssid="162">For comparison, we altered our algorithm to also make random choices when unable to choose a single sense.</S>
  <S sid="163" ssid="163">We applied the algorithm Sussna considers best, 4We tried both mutual information and association ratio, and the later performed better.</S>
  <S sid="164" ssid="164">5The results of our algorithm are those for window size 30, file matches and overall.</S>
  <S sid="165" ssid="165">6In our replication of his experiment he mutual constraint for the first 10 nouns (tile optimal window size according to his experiments) of file br-r05 had to deal with more than 200,000 synset pairs.</S>
  <S sid="166" ssid="166">20 discarding the factors that do not  affect performance significantly 7,and obtain the results in table 4.</S>
  <S sid="167" ssid="167">C.l)ensity File I00.0 70.1 Sense 6(1.1 Sussna File 100.0 64.5 Sense 52.3 Table 4: comparison with [St, ssna 931 A more thorougla comparison with these methods could he desirable, hut not possible in this paper lor the sake of conciseness.</S>
  <S sid="168" ssid="168">might be only one of a number of complementary evidences of the plausibility ola certain word sense.</S>
  <S sid="169" ssid="169">Furthermore, WordNet 1.4 is not a complete lexical database (current version is 1.5).</S>
  <S sid="170" ssid="170">Tune  the sense  d is t inc t ions  to  the  leve l best  su i ted  fo r  the  app l i ca t ion .</S>
  <S sid="171" ssid="171">On the one hand the sense distinctions made by WordNet 1.4 arc not always satislactory.</S>
  <S sid="172" ssid="172">On tire other hand, our algorithm is not designed to work on the file level, e.g.</S>
  <S sid="173" ssid="173">il the sense level is unable to distinguish among two senses, the file level also fails, even if both senses were fronl the same file.</S>
  <S sid="174" ssid="174">If the senses were collapsed at the file level, the coverage and precision of tile algorithm at the file level might be even better.</S>
  <S sid="175" ssid="175">5 Further Work We would like to have included in this paper a study on whether there is or not a correlation among correct and erroneous sense assignations and the degree of Conceptual Density, that is, the actual figure held by fommla I.</S>
  <S sid="176" ssid="176">If this was the case, the error rate could be furtber decreased setting a ccrtain lhreshold for Conceptual Density wdues of wilming senses.</S>
  <S sid="177" ssid="177">We would also like to evaluate the uselulness of partia~l disambiguation: decrease of ambiguity, number of times correct sense is among the chosen ones, etc.</S>
  <S sid="178" ssid="178">There are some factors that could raise the performmace of our algorithm: ?</S>
  <S sid="179" ssid="179">Work  on  coherent  chunks  o f  text .</S>
  <S sid="180" ssid="180">Unfortunately any information about discourse structure is absent in SemCor, apart from sentence endings Thc performance would gain from the fact lhat sentences from unrelated topics wouht not be considered in the disamhiguation window.</S>
  <S sid="181" ssid="181">Ex tend  and  improve  the semant ic  data .</S>
  <S sid="182" ssid="182">WordNet provides sinonymy, hypernymy and meronyny relations for nouns, but other relations are missing.</S>
  <S sid="183" ssid="183">For instance, WordNet lacks eross-categorial semantic relations, which could he very useful to extend the notion of Conceptual Density of nouns to Conceptual Density of words.</S>
  <S sid="184" ssid="184">Apart from extending lhe disambiguation to verbs, adjectives and adverbs, cross-catcgorial relations would allow to capture better lhe relations alnong senses and provide firmer grounds for disambiguating.</S>
  <S sid="185" ssid="185">These other relations could be extracted from other knowledge sources, both corpus-based or MRD-based.</S>
  <S sid="186" ssid="186">If those relations could be given on WordNet senses, Conceptual Density could profit from them.</S>
  <S sid="187" ssid="187">It is ot, r belief, following the ideas of [McRoy 92] that full- fledged lexical ambiguity resolution should combine several information sources.</S>
  <S sid="188" ssid="188">Conceptual Density "/Initial mutual constraint size is 10 and window sizeis 41.</S>
  <S sid="189" ssid="189">Meronymic links are also considered.</S>
  <S sid="190" ssid="190">All the links have the same weigth.</S>
  <S sid="191" ssid="191">6 Conclusion The automatic method for the disambiguation of nouns presented in this papcr is ready-usable in any general domain and on free-running text, given part of speech tags.</S>
  <S sid="192" ssid="192">It does not need any training and uses word sense tags from WordNet, an extensively used Icxieal data base.</S>
  <S sid="193" ssid="193">Conceptual Density has been used for other tasks apart from the disambiguation of free-running test.</S>
  <S sid="194" ssid="194">Its application for automatic spelling correction is outlined in tAgirre ct al.</S>
  <S sid="195" ssid="195">It was also used on Computational Lexicography, enriching dictionary senses with semantic tags extracted from WordNet [Rigau 9411, or linking bilingual dictionaries to WordNet [Rigau and Agirre 96].</S>
  <S sid="196" ssid="196">In the experiments, the algorithm disambiguated [our texts (about 10,000 words long) of SemCor, a subset of the Brown corpus.</S>
  <S sid="197" ssid="197">The results were obtained automatically comparing the tags in SemCor with those computed by the algorithm, which would allow the comparison with other disambiguation methods.</S>
  <S sid="198" ssid="198">Two other methods, [Sussna 93] and [Yarowsky 92], were also tried on the same texts, showing that our algorithm performs better.</S>
  <S sid="199" ssid="199">Results are promising, considering the difficnlty of the task (free running text, large number of senses per word in WordNet), and the htck o1 any discourse structure of the texts.</S>
  <S sid="200" ssid="200">Two types el results can be obtaincd: the specific scnse or a coarser, file level, tag.</S>
  <S sid="201" ssid="201">Acknowledgements This work, partially described ill [Agirre &amp;Rigau 9611, was started in the Computing Research Laboratory in New Mexico State University.</S>
  <S sid="202" ssid="202">We wish to thank all the staff of the CRL and specially Jim Cowie, Joe Guthtrie, Louise Guthrie and David l"arwell.</S>
  <S sid="203" ssid="203">We woukl also like to thank Xabier Arregi, Jose mari Arriola, Xabier Artola, Arantza Dfaz de llarraza, Kepa Sarasola nd Aitor Soroa fiom the Computer Science Faculty of EHU and Franeesc Ribas, ltoracio Rodrfguez and Alicia Ageno from the Computer Science Department of UPC.</S>
  <S sid="204" ssid="204">22 References Agirre E., Arregi X., Diaz de Ilarraza A. and Sarasola K. 1994.</S>
  <S sid="205" ssid="205">Conceptual Distance and Automatic Spelling Correction.</S>
  <S sid="206" ssid="206">in Workshop on Speech recognition and handwriting.</S>
  <S sid="207" ssid="207">Leeds, England.</S>
  <S sid="208" ssid="208">Agirre E., Rigau G. 1995.</S>
  <S sid="209" ssid="209">A Proposal for Word Sense Disambiguation using conceptual Distance, International Conference on Recent Advances in Natural Language Processing.</S>
  <S sid="210" ssid="210">Tzigov Chark, Bulgaria.</S>
  <S sid="211" ssid="211">Agirre, E. and Rigau G. 1996.</S>
  <S sid="212" ssid="212">An Experiment in Word SenseDisambiguation of the Brown Corpus Using WordNet.</S>
  <S sid="213" ssid="213">Memoranda in Computer and Cognitive Science, MCCS-96-291, Computing Research Laboratory, New Mexico State University, Las Cruces, New Mexico.</S>
  <S sid="214" ssid="214">Cowie J., Guthrie J., Guthrie L. 1992.</S>
  <S sid="215" ssid="215">Lexical Disambiguation using Simulated annealing, in proceedings of DARPA WorkShop on Speech and Natural Language, New York.</S>
  <S sid="216" ssid="216">Francis S. and Kucera H. 1967.</S>
  <S sid="217" ssid="217">Computing analysis of present-day American English, Providenc, RI: Brown University Press, 1967.</S>
  <S sid="218" ssid="218">Gale W., Church K. and Yarowsky D. 1993.</S>
  <S sid="219" ssid="219">A Method for Disambiguating Word Sense sin a Large Corpus, in Computers and the Humanities, n. 26.</S>
  <S sid="220" ssid="220">Guthrie L., Guthrie J. and Cowie J.</S>
  <S sid="221" ssid="221">Resolving Lexical Ambiguity, in Memoranda in Computer and Cognitive Science MCCS-93-260, Computing Research Laboratory, New Mexico State University.</S>
  <S sid="222" ssid="222">Las Cruces, New Mexico.</S>
  <S sid="223" ssid="223">Hearst M. 1991.</S>
  <S sid="224" ssid="224">Towards Noun Homonym Disambiguation Using Local Context in Large Text Corpora, in Proceedings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research.</S>
  <S sid="225" ssid="225">Waterloo, Ontario.</S>
  <S sid="226" ssid="226">Using Multiple Knowledge Sources for Word Sense Discrimination, Computational Linguistics, vol.</S>
  <S sid="227" ssid="227">Miller G. 1990.</S>
  <S sid="228" ssid="228">Five papers on WordNet, Special Issue of International Journal of Lexicogrphy 3(4).</S>
  <S sid="229" ssid="229">Miller G. Leacock C., Randee T. and Bunker R. 1993.</S>
  <S sid="230" ssid="230">A Semantic Concordance, in proceedings of the 3rd DARPA Workshop on Human Language Technology, 303-308, Plainsboro, New Jersey.</S>
  <S sid="231" ssid="231">Miller G., Chodorow M., Landes S., Leacock C. and Thomas R. 1994.</S>
  <S sid="232" ssid="232">Using a Semantic Concordance for sense Identification, in proceedings of ARPA Workshop on Human Language Technology, 232- 235.</S>
  <S sid="233" ssid="233">Rada R., Mill H., Bicknell E. and Blettner M. 1989.</S>
  <S sid="234" ssid="234">Development an Applicationof a Metric on Semantic Nets, in IEEE Transactions on Systems, Man and Cybernetics, vol.</S>
  <S sid="235" ssid="235">Resnik P. 1995.</S>
  <S sid="236" ssid="236">Disambiguating Noun Groupings with Respect o WordNet Senses, in Proceedings of the Third Workshop on Very Large Corpora, MIT.</S>
  <S sid="237" ssid="237">Richardson R., Smeaton A.F.</S>
  <S sid="238" ssid="238">Using WordNet as a Konwledge Base for Measuring Semantic Similarity between Words, in Working Paper CA-1294, School of Computer Applications, Dublin City University.</S>
  <S sid="239" ssid="239">Dublin, heland.</S>
  <S sid="240" ssid="240">An experiment on Automatic Semantic Tagging of Dictionary Senses, WorkShop "The Future of Dictionary", Aix-les- Bains, France.</S>
  <S sid="241" ssid="241">published as Research Report LSI- 95-31-R. Computer Science Department.</S>
  <S sid="242" ssid="242">Rigau G. and Agirre E. 1996.</S>
  <S sid="243" ssid="243">Linking Bilingual Dictionaries to WordNet, in proceedings of the 7th Euralex International Congress on Lexcography (Euralex96), Gothenburg, Sweden, 1996.</S>
  <S sid="244" ssid="244">Sussna M. 1993.</S>
  <S sid="245" ssid="245">Word Sense Disambiguation for Free-text Indexing Using a Massive Semantic Network, in Proceedings of the Second International Conference on Information and knowledge Management.</S>
  <S sid="246" ssid="246">Arlington, Virginia.</S>
  <S sid="247" ssid="247">Voorhees E. 1993.</S>
  <S sid="248" ssid="248">Using WordNet o Disambiguate Word Senses for Text Retrival, in proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Developement in Information Retrieval, pages 171-180, PA. Wilks Y., Fass D., Guo C., McDonal J., Plate T. and Slator B.</S>
  <S sid="249" ssid="249">Providing Machine Tractablle Dictionary Tools, in Semantics and the Lexicon (Pustejovsky J.</S>
  <S sid="250" ssid="250">Yarowsky, D. 1992.</S>
  <S sid="251" ssid="251">Word-Sense Disambiguation Using Statistical Models of Rogets Categories Trained on Ixtrge Corpora, in proceedings of the 15th International Conference on Computational Linguistics (Coling92).</S>
  <S sid="252" ssid="252">Nantes, France.</S>
</PAPER>
