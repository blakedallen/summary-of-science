Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis
Determining the polarity of a sentiment-bearing expression requires more than a simple bag-of-words approach.
In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity.
In this paper, we view such subsentential interactions in light of compositional semantics, and present a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure.
Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%).
We also find that “content-word negators”, not widely employed in previous work, play an important role in determining expression-level polarity.
Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy uniformly decreases as additional, potentially disambiguating, context is considered.
Content-word negators are words that are not function words, but act semantically as negators.
We combine different kinds of negators with lexical polarity items through various compositional semantic models, both heuristic and machine learned, to improve phrasal sentiment analysiss.
We propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity.
We hand-code compositional rules in order to model compositional effects of combining different words in the phrase.
We categorized polarity reversing words into two categories: function-word negators such as not and content-word negators such as eliminate.
