<PAPER>
  <S sid="0" ssid="0">More  accurate  tes ts  Ibr the  s ta t i s t i ca l  s ign i f i cance  of resu l t d i f ferences  * Alexander  Yeh Mitre Corp. 202 Burli l lgl;on Rd.</S>
  <S sid="1" ssid="1">Bedford,  MA 01730 USA asy~mit rc .o rg Abstract Statisti(:a,1 signiticance testing of (litlerelmeS in v;~hl(`-s of metri(:s like recall, i)rccision and bat- au(:(~(l F-s(:()rc is a ne(:(`-ssary t)art of eml)irical ual;ural language 1)ro(:essing.</S>
  <S sid="2" ssid="2">Unfortunately, we lind in a set of (;Xl)erinlc]d;s (;hal; many (:ore- inertly used tesl;s ofte, n underest imate t.he s ignif icancc an(l so are less likely to detect differences that exist 1)el;ween ditlercnt techniques.</S>
  <S sid="3" ssid="3">This undelesi;imation comes from an in(let)endcn((~ a,-;SUlnl)tion that is often violated.</S>
  <S sid="4" ssid="4">~fe l)oint out some useful l;e,%s (;hal; (lo nol; make this assuml)- lion, including computationally--intcnsive ran- d()mizat,ion 1;cs|;s. 1 I n t rodu( - t ion In Clnl)irical natural  ]al~gUag(~ l)rocessing, on(, is ottcal |:(~st;ing whether some new technique 1)ro(lu(es im])rove(l lesull;s (as mcasur(xl ])y one ()1 111017(` - IIICI;I]CS) Oit son Ic  i;esl; (lai;~L set; V]l(`-ll (;Olll])aIe(l l i ( )sol l le (; l lrrel lt  ( l )ascl i lm) l;c(:]lnique.</S>
  <S sid="5" ssid="5">5/]lell ,]le lCsllll;s are better  with the new tcch- ni(lUe , a question arises as t() wh(,l;h(;r these l:(`-- sult; (litleren(:es are due t() the new technique a(:t;ually 1)eing l)cl;t(x or just; due 1;o (:han(:e. Un- tortmmtely, one usually Callll()t) directly answer the qnesl;ion "what is the 1)robatfility that 1;11(; now l;(x:hni(luC, is t)el;lx~r givell l;he results on the t(,sl, dal;a sol;": I)(new technique is better  [ test  set results) ]~ul; with statistics, one cml answer the follow- ing proxy question: if the new technique was a(&gt; tually no ditterent han the old t(,(hnique ((;he * This paper reports on work l)erfonncd at the MITR1,; Corporation under the SUl)porl: of the MITIlJ,; ,qponsored Research l)rogrmn.</S>
  <S sid="6" ssid="6">Warren Grcit[, l ,ynette Il irschlnm b Christilm l)orall, John llen(lerson, Kelmeth Church, Ted l)unning, Wessel Kraaij, Milch Marcus and an anony- mous reviewer l)rovided hell)rid suggestions.</S>
  <S sid="7" ssid="7">Copyright @2000 The MITRE Corl)oration.</S>
  <S sid="8" ssid="8">All rights r(~s(nvcd.</S>
  <S sid="9" ssid="9">null hyl)othesis), wh~tt is 1:11(; 1)robat)ility that the results on the test set would l)e at least this skewed in the new techniques favor (Box eta] .</S>
  <S sid="10" ssid="10">Thai; is, what is P(test  se, t results at least this skew(A in the new techni(lues favor I new technique is no (liffercnt than the old) If the i)robtfl)ility is small enough (5% off;on is used as the threshold), then one will rqiect the mill hyi)otheMs and say that the differences in 1;he results are :sta.tisl;ically siglfilicant" aI; that thrt,shold level.</S>
  <S sid="11" ssid="11">This 1)al)(n" examines some of th(`- 1)ossil)le me?hods for trying to detect statistically signif- leant diflelenc(`-s in three commonly used met- li(:s: telall, 1)re(ision and balanced F-score.</S>
  <S sid="12" ssid="12">Many of these met;Ire(Is arc foun(t to be i)rol)lem- a.ti(" ill a, so, t; of eXl)erinw, nts that are performed.</S>
  <S sid="13" ssid="13">Thes(~ methods have a, tendency to ullderesti- mat(`- th(, signili(:ance, of the results, which tends t() 1hake one, 1)elieve thai; some new techni(tuc is no 1)el;l;er l;lmn the (:urrent technique even when il; is.</S>
  <S sid="14" ssid="14">This mtderest imate comes flom these lnc|h- ells assuming l;hat; the te(:hlfi(tues being con&gt; lmrcd produce indepen(lc, nt results when in our eXl)eriments , the techniques 1)eing COml)ared tend to 1)reduce l)ositively corr(`-lated results.</S>
  <S sid="15" ssid="15">To handle this problem, we, point out some st~ttistical tests, like the lnatche(t-pair t, sign and Wilcoxon tests (Harnett,  1982, See.</S>
  <S sid="16" ssid="16">8.7 and 15.5), which do not make this assulnption.</S>
  <S sid="17" ssid="17">One Call ITS(, l;llcse tes ts  Oll I;hc recall nlel;r ic, but  l;he precision an(l 1)alanced F-score metric have too COml)lex a tbrm for these tests.</S>
  <S sid="18" ssid="18">For such com- 1)lex lne|;ri(;s~ we llSe a colnplll;e-in|;Clisiv(~ ran- domization test (Cohen, 1995, Sec.</S>
  <S sid="19" ssid="19">5.3), which also ~tvoids this indet)en(lence assmnption.</S>
  <S sid="20" ssid="20">947 The next section describes many of the stan- dard tests used and their problem of assuming certain forms of independence.</S>
  <S sid="21" ssid="21">The first subsec- rio11 describes tests where this assumption ap- pears in estimating the standard deviation of the difference between the techniques results.</S>
  <S sid="22" ssid="22">The second subsection describes using contin- gency tables and the X 2 test.</S>
  <S sid="23" ssid="23">Following this is a section on methods that do not 1hake this inde- pendence assumption.</S>
  <S sid="24" ssid="24">Subsections in turn de- scribe some analytical tests, how they can apply to recall but not precision or the F-score, and how to use randomization tests to test preci- sion and F-score.</S>
  <S sid="25" ssid="25">We conclude with a discussion of dependencies within a test sets instances, a topic that we have yet to deal with.</S>
  <S sid="26" ssid="26">2 Tests  that  assume independence between compared  resu l ts 2.1 F ind ing  and using the variance of a result difference For each metric, after determining how well a new and current technique t)efforms on solne test set according to that metric, one takes the diflbrence between those results and asks "is that difference significant?"</S>
  <S sid="27" ssid="27">A way to test this is to expect 11o difference in the results (the null hypothesis) and to ask, as- suming this expectation, how mmsual are these results?</S>
  <S sid="28" ssid="28">One way to answer this question is to assulne that the diffb, rence has a normal or t dis- tribution (Box et al., 1978, Sec.</S>
  <S sid="29" ssid="29">Then one calculates the following: (d - Z  [4 ) / s  d = d/,~,~ (1) where d = x l -  x2 is the difference found be- tween xl and x2, the results for the new and current echniques, respectively.</S>
  <S sid="30" ssid="30">E[d] is the ex- pected difference (which is 0 under the null hy- pothesis) and Sd is an estimate of the standard deviation of d. Standard eviation is the square root of the variance, a measure of how much a random variable is expected to vary.</S>
  <S sid="31" ssid="31">The results of equation 1are compared to tables (c.f.</S>
  <S sid="32" ssid="32">(1978, Appendix)) to find out what the chances are of equaling or exceeding the equa- tion 1 results if the null hypothesis were true.</S>
  <S sid="33" ssid="33">The larger the equation 1 results, the more un- usual it would be under the null hypothesis.</S>
  <S sid="34" ssid="34">A complication of using equation 1 is that one usually does not have Sd, but only st and s2, where Sl is the estimate for XlS standard deviation and similarly for s2.</S>
  <S sid="35" ssid="35">Ilow does one get the former fiom the latter?</S>
  <S sid="36" ssid="36">It turns out that (Box et al., 1978, Ch.</S>
  <S sid="37" ssid="37">3) o -2 o-12 + a~ d = --  2p12a10-2 where cri is the true standard eviation (instead of the estimate si) and pl2 is the correlation coefficient between xl and :c2.</S>
  <S sid="38" ssid="38">Analogously, it turns out that 2 z S d 82 -t- 82 - -  2 r128182 (2) where r12 is an estimate for P12.</S>
  <S sid="39" ssid="39">So not only does cr d (and Sd) depend on the properties of xl and x2 in isolation, it also depends on how Xl and .~2 interact, as measured by P12 (and rr)).</S>
  <S sid="40" ssid="40">When Xl and x2 are independent, p12 = 0, and then (Td = ~-+ c7~ and analogously, Sd = ~ + s~.</S>
  <S sid="41" ssid="41">When P~2 is positive, ;1; 1 and x2 are positively correlated: a rise in xl or x2 tends to be accompanied by a rise in the other result.</S>
  <S sid="42" ssid="42">When P12 is negative, :cl and x2 are negatively correlated: a rise in :cl or x9 tends to be accompmfied by a decline in the other result.</S>
  <S sid="43" ssid="43">-1  &lt; P12 &lt; 1 (Larsen and Marx, 1986, Sec.</S>
  <S sid="44" ssid="44">The assu lnpt ion  of independence is often used in fornlnlas to determine the statistical signifi- cance of the difference d = .~:1 - x2.</S>
  <S sid="45" ssid="45">But how accurate is this assumption?</S>
  <S sid="46" ssid="46">One nfight expect sonic positive correlation from both results com- ing from the same test set;.</S>
  <S sid="47" ssid="47">One may also expect some positive correlation when either both tech- niques are just variations of each other 1 or both techniques are trained on the same set of train- ing data (and so are missing the same examples relative to the test set).</S>
  <S sid="48" ssid="48">This assumption was tested during some experiments for finding granunatical relations (subject, object, various types of nxodifiers, etc.).</S>
  <S sid="49" ssid="49">The metric used was the fraction of the relations of interest in the test set that were re- called (tbund) by some technique.</S>
  <S sid="50" ssid="50">The relations of interest were w~rious ubsets of the 748 rela- tion instances in that test set.</S>
  <S sid="51" ssid="51">An example sub- set is all the modifier elations.</S>
  <S sid="52" ssid="52">Another subset is just that of all the time modifier elations.</S>
  <S sid="53" ssid="53">1 These  var ia t ions  are often des igned to usual ly  behave i l l  the  stone way and  on ly  differ in jus t  a few cases.</S>
  <S sid="54" ssid="54">948 First, two diflerent e(:hniques, one ltlelllory- t)ased and the other tlansti)rlnation-rule based, wei"e trained on the same training set, and then both teste(1 on that ticst set;.</S>
  <S sid="55" ssid="55">l~.e(:all eonlt)a.risons we, re made tbr ten subsets of tim relations and the r12 was found for each cOral)arisen.</S>
  <S sid="56" ssid="56">From Box et al.</S>
  <S sid="57" ssid="57">3) " 12 = ~( I ,  J lk -- Y l ) (~2k  - -  ~2) / ( g l t~2(  71 - -  l . )</S>
  <S sid="58" ssid="58">) k where Yil~ = ] if the ith technique recalls the tcl;h relation and = 0 if not.</S>
  <S sid="59" ssid="59">lz, is the nmnl)cr of relations in the subset.</S>
  <S sid="60" ssid="60">]i and si are mean and stmJ(lard de, vial, ion estimate.s (based on the YikS), rest)ectively, fl)r the ith technique.</S>
  <S sid="61" ssid="61">For the ten subsets, only Clio COlnl)arison had a r12 (::lose to 0 (It was -0.05).</S>
  <S sid="62" ssid="62">The other nine c()ml)arisons had r12s 1)etw(x,n 0.29 and 0.53.</S>
  <S sid="63" ssid="63">The ten coral)arisen inedian value was 0.38.</S>
  <S sid="64" ssid="64">Next;, the transformatiol&gt;rulc t)ased t.cch- nique was rUll with diflerent sets of start ing con- ditions and/or  different, but overlapl)ing , sub- sets of the training set.</S>
  <S sid="65" ssid="65">Recall comparisons were ma(le on the same test (lata.</S>
  <S sid="66" ssid="66">set 1)etween l;he d i f fcrent variations.</S>
  <S sid="67" ssid="67">Many of the comparisons were, of how well two wu:iations recalled a particular subset of the relations.</S>
  <S sid="68" ssid="68">A total of 40 compar- isons were made..</S>
  <S sid="69" ssid="69">The r]2s on all d0 were 1)osi- tire.</S>
  <S sid="70" ssid="70">3 of the r,2s w(~re ill the 0.20-0.30 range.</S>
  <S sid="71" ssid="71">24 of the rj2s wore in the 0.50--0.79 range.</S>
  <S sid="72" ssid="72">13 of the r]2s were in the 0.80-1.0() range.</S>
  <S sid="73" ssid="73">So in our ext)erin~ents, we were usually eom- t)aring 1)ositivcly correlated results.</S>
  <S sid="74" ssid="74">How much error is introdu(:e(t t)y assuming independence?</S>
  <S sid="75" ssid="75">An easy--to-analyze case is when the stan- dard devial,ions for the results being eoml)ared a:tc the same.</S>
  <S sid="76" ssid="76">?~ J}hen equation 2 reduces to s , , -  sV /2 ( l -  r12), where s = sl = ,s2.</S>
  <S sid="77" ssid="77">If one, assumes the re.sults me indcpel:dent (~/SSllllle r,2 = 0), then sd :-~ .sv/22.</S>
  <S sid="78" ssid="78">Call this wflue sd-i,7,g.</S>
  <S sid="79" ssid="79">As flu increases in value, Sd decreases: [().38 d 0.T87(sd_i,,d) 1.27 [p.ao I 1.41 [O.80J 0.447(Sd.__i.,,.d) 2.24 lhe rightmost cohunn above indicates the mag- nitude by which erroneously assuming indepen- &gt;[lifts is actually roughly true in the coml)arisons nmde, and is assumed to be true in many of the standard Wsts for statistical significance.</S>
  <S sid="80" ssid="80">(lence (using 8d_in d ill 1)lace of sd) will increase the standard eviation estimate.</S>
  <S sid="81" ssid="81">In equation 1, sd forms the denominator of the ratio d/.s d. So erroneously assmning independence will mean that  the mmmrator  d, the difference between the two results: will nee(t to increase by that same factor in order f()r equation 1 to have the same wtlue as without the indel)endence assmnt)tion.</S>
  <S sid="82" ssid="82">Since the value of that  equation indicates the statistical significance of d, assunfing indepen- dence will mean that  e1 will have to be larger than without the.</S>
  <S sid="83" ssid="83">assumption to achieve the same al)parent level of statistical significance.</S>
  <S sid="84" ssid="84">l?roln tile tal)le above, when r12 = 0.50, (1 will need to 1)c about 41% larger.</S>
  <S sid="85" ssid="85">Another way to look at this is that  assuming indei)en(lenee will make the same.</S>
  <S sid="86" ssid="86">v~due, of d appear less statist;i- cally signifiealtt.</S>
  <S sid="87" ssid="87">The common tests of statistical significance use this assumt)tion.</S>
  <S sid="88" ssid="88">The, tesl; klloWlt as the 1, (Box et; al., 1978, Sec.</S>
  <S sid="89" ssid="89">4.1) or two-saml)le t (Harnett,  1982, See.</S>
  <S sid="90" ssid="90">8.7) test does.</S>
  <S sid="91" ssid="91">This test uses equation 1 and then compares the resulting va.lue against he t; distr ibution tal)les.</S>
  <S sid="92" ssid="92">This test has a (:Oml)licated form for sd l)eeause: 1. :c!</S>
  <S sid="93" ssid="93">and :c2 can t)e 1)ased on (tiffering num- 1)ers of saml)les.</S>
  <S sid="94" ssid="94">Call these retail)ors n~ and n2 r(;sl)ectivcly.</S>
  <S sid="95" ssid="95">111l this t(;st, the z i  s  are each an ni sam- pie average, of altother varial)le ((:all it yi).</S>
  <S sid="96" ssid="96">[his is important  because the sis in this test are standmd deviation estimates tor the yis, not the xi s .</S>
  <S sid="97" ssid="97">The relationship be- tween them is that  si for Jli is the same as ( for :,:,:.</S>
  <S sid="98" ssid="98">The test itself assumes that !11 and Y2 have the same standard eviation (call this com- mon value s).</S>
  <S sid="99" ssid="99">The denominator estimates ,s using a weighte(1 average of 81 and s2.</S>
  <S sid="100" ssid="100">The weighting is b~sed on nl and r7,2.</S>
  <S sid="101" ssid="101">From Harnett (1982, Scc.</S>
  <S sid="102" ssid="102">8.7), the denominator Sd ~- nl  + n2 - 2 711 -b r~,2 ) i7,177,2 When nl = n2 (call this common value n), ~1 and s2 will be given equal weight, and Sd siml)li- fie.s to ~ + ,s~)/n.</S>
  <S sid="103" ssid="103">Making the substitut ion described above of si v/57 tbr si leads to an Sd of 949 s 2 the fbrm had earlier for the -t-, 2, we us ing independence assumption.</S>
  <S sid="104" ssid="104">Another test that both makes this assulnt)- tion and uses a tbrm of equation 1 is a test tbr binonlial data (Harnett, 1982, Sec.</S>
  <S sid="105" ssid="105">8.1.1) which uses the "taet" that binomial distributions tend to approximate normal distributions.</S>
  <S sid="106" ssid="106">In this test, the zis being compared are the fraction of the items of interest that are recovered by the ith technique.</S>
  <S sid="107" ssid="107">In this test, the denomina- tor sd of equation 1also has a complicated fbrm, both due to the reasons mentioned for the t, test above and to the fact that with a binomial dis- tribution, the standard eviation is a flmction of the number of samples and the mean wflue.</S>
  <S sid="108" ssid="108">2.2 Using cont ingency tab les  and  X 2 to test  precis ion A test that does not use equation 1 but still makes an assunlption of independence l)etween a:l and a:u is that of using contingency tables with the chi-squared 0,52) distribution (Box et al., 1978, Sec.</S>
  <S sid="109" ssid="109">When tile assmnption is valid, this test is good for comparing differences ill the precision metric.</S>
  <S sid="110" ssid="110">Precision is the fraction of the items "Ibund" 1)y some technique that are actually of interest.</S>
  <S sid="111" ssid="111">Precision = l~,/(I~, + S), where R is the number of items that are of inter- est and me Recalled (fbund) by tile technique, and S is the munber of items that are found by tile technique that turn out to be Spurious (not of interest).</S>
  <S sid="112" ssid="112">One can test whether the precision results from two techniques are different by us- ing a 2 x 2 contingency table to test whether the ratio R/S  is different for the two techniques.</S>
  <S sid="113" ssid="113">One makes tile latter test, by seeing if tile as- sumption that the ratios for the two techniques are the same (the null hypothesis) leads to a sta- tistically significant result when using a X 2 dis- tribution with one degree of freedom.</S>
  <S sid="114" ssid="114">A 2 x 2 ta- ble has 4 cells.</S>
  <S sid="115" ssid="115">The top 2 cells are filled with the R and S of one technique and the bottom 2 cells get the R and S of the other technique.</S>
  <S sid="116" ssid="116">In this test, the valuc in each cell is assumed to have a Poisson distribution.</S>
  <S sid="117" ssid="117">When the cell values are not too small, these Poisson distributions are approximately Normal (Gaussiml).</S>
  <S sid="118" ssid="118">As a result, when the cell values are independent, smnming tlle normalized squares of the difference between each cell and its expected value leads to a X 2 distribution (Box el; al., 1978, Sec.</S>
  <S sid="119" ssid="119">How well does this test work in our experi- ments?</S>
  <S sid="120" ssid="120">Precision is a non-linear time(ion of two random wuiables R and S, so we did not try to estimate the correlation coefficient ]or precision.</S>
  <S sid="121" ssid="121">However, we can easily estimate the correlation coefficients for the Rs.</S>
  <S sid="122" ssid="122">They are the r12s found in section 2.1.</S>
  <S sid="123" ssid="123">As that section mentions, the r12s fbund are just about always positive.</S>
  <S sid="124" ssid="124">So at least in our experiments, the Rs are not ill- dependent, but are positively correlated, which violates the assumptions of the test.</S>
  <S sid="125" ssid="125">An example of how this test behaves is the following comparison of the precision of two dif- ferent methods at finding the modifier elations using tile stone training and test set.</S>
  <S sid="126" ssid="126">The corm- lation coefficient estilnate tor R is 0.35 mid the data is Method 17, 5 t?recision 1 47 48 4!</S>
  <S sid="127" ssid="127">)% 2 25 14 64% Placing the l~, and S values into a 2 x 2 table leads to a X 2 value of 2.38. a At t degree of freedom, tile X 2 tables indicate that if the null hypothesis were true, there would 1)e a 10% to 20% chance of producing a X 2 value at least this large.</S>
  <S sid="128" ssid="128">So according to this test, this nnlch of an observed difference in precision wouht not be unusual if no actual differ(,ncc in the precision exists between the two nw, thods.</S>
  <S sid="129" ssid="129">This test assumes independence b tween the /~, wdues.</S>
  <S sid="130" ssid="130">When we use a 22(I (=1048576) trial approximate rmldomization test (section 3.3), which makes no such assumptions, then we find that this latter test indicates that under the null hypothesis, there is less than a 4% chance of producing a difference in precision results as large as the one observed.</S>
  <S sid="131" ssid="131">So this latter test in- dicates that this nmch of an observed ifference in precision would be mmsual if no actual dif- ference ill the precision exists between the two methods.</S>
  <S sid="132" ssid="132">It should be mentioned that the manner of testing here is slightly different han the man- ner in the rest of this paper.</S>
  <S sid="133" ssid="133">The X 2 test looks at the square of the difference of two results, and rejects the mill hylmthesis (the compared techniques are the same) when this square is aVe do not use Yates adjustment to compensate lbr the numbers in the table being integers.</S>
  <S sid="134" ssid="134">1)oing so would lmve made the results even worse.</S>
  <S sid="135" ssid="135">950 large, whel;he, r l;lm largeness is (:aused l)y t;he new t;eehni(lue t)l"o(lucing a, much l)(fl;l;er result; titan l;he current, l;e(:hlfique or vice-versa.</S>
  <S sid="136" ssid="136">So 1,o l)e fair, we eolnl)ared l;he X 2 resull;s with a l;wo-sided version of l;hc rmldon~iz~fl;ion t;esl,: es- l;inm|;e, l;he likelihood glu~l; l;he obseaved magni- l;u(le of t, he resull; (lifleren(:e would 1)c matched or exceeded (regardless of which l;echnique pro- duced l;he betl;er resull;) raider the mill hyl)oth- esis.</S>
  <S sid="137" ssid="137">A one-sided version of the test;, which is colnt)aral)le t;o what we use in l;he rest of the t)a - per, esl;inml;es l;he likelihood of a (tifferenl; oul;- come under t;he null hyt)oChesis: that of m~l:cll- ing or exceeding t;he (lit[erence of how lllllch l)?,l;ter i;he new (possibly 1)ett, er) l;e(:lmi(lues oh- s(aved result is than l;he currenl; l;e(hniques o|)- serve,(1 lesull;, ht t;he ahoy(; scenario, a one-sided t(;sl; t)rodu(:es ~ 3(~, tigure insl;ead of s~ d:% figure.</S>
  <S sid="138" ssid="138">3 Tests  w i thout  that  independence assumpt ion a.1 Tests  for  matched pa i rs At; l;his point, one may wonder it all st;al;isl;ical t;CSt;S lllil](e SllC]l s/,]l int lepealdenct~ asSllltl])l;ioll.</S>
  <S sid="139" ssid="139">Th(, miswer is no, lml; t;]l()se lesl:s l;hal; (to nol; lltC}~Slll;e, how ll l l lch {;we l;e(;]lni(llles illl;(,ra(:l; (to need  i;o lmtke, some assmnpl ; ioH  al)oul; t;]m|; ill- I;(;r~t(l;ion mid l;yl)it:a.ll E l;]ml; assuml)l;ioll is in(te- t)(~]ldell(;e. Fhose I;esl;s I;ll~H; Hol;i(;c in S()lll(~ \r;~Br how much l;wo tc(:hniqucs hm;ra(:l; (;~1,11 lib(; ~h()se ol)servations insl;ead of relying on assumt)l:ions.</S>
  <S sid="140" ssid="140">One w,~y t;o measure how 1;we l;e(:lmi(lucs in- i;erac(; is 1;o comtm.re ]tow similarly (;he, t;wo t;ecl&gt; ni(tues tea.el; 1;o various l)arl;s ()f 1;he l;(;s[; seA;.</S>
  <S sid="141" ssid="141">"his is done in the mal;t:hed-lm.ir 1, I;esl; (Hm- nctl;, 1982, Se(:.</S>
  <S sid="142" ssid="142">This l;csI; tin(ls the ditha- once bet;we, n how t;eclmiques 1 and 2 l)eribrm on e~t(::h l;esl; set, Saml)le.</S>
  <S sid="143" ssid="143">The/ ,  dist;ri|)ul;ion and a fOIln of eqm~l;ion l m:e used.</S>
  <S sid="144" ssid="144">The null }lyl)ol;h- esis is st;ill l;]l~tl; ~he mtmeral;or d ]ms ~t 0 me,m, but el is now l;he stun of these difference values (divided 1)y t;he number of Smnl)les), instead of being :r~ - :re.</S>
  <S sid="145" ssid="145">Similmly, the (lenomimd;or .sd is now esl;inml;ing l;he si;a.ndmd (leviation of l;hese diflerenee wdues, instead of being a funcl;ion of s:l and su.</S>
  <S sid="146" ssid="146">.Flfis means for example, (;hal; even if t;lm values flom l,eclmiques l and 2 vary on (lii- ti:rent; test; Smnl)les , Sd will now 1)( 0 if on each tesI; smnl)le, l;echnique ]1)reduces a. value l;lmt is the  ssulle C()llS|;allI; tHI1OlllIi; lllOle t;han l;he va,]ue flom t, echnique 2.</S>
  <S sid="147" ssid="147">Two ol;h(,r tests for eomlmring how (;we tech- ni(lueS 1)ert()rm 1) 3, comtmring how well l;hey perform on each I;est Smnl)le arc the sign mid Wilcoxon tests (Harnel;t;, 1!</S>
  <S sid="148" ssid="148">Un- like, t;]le nl~tl;ched-tmir t: t;esI;~ neither of t, hese l;wo I;CSI;5 slSSllllte t;ln~l; I hc sum of l;he (litlcrences has a normal (Gaussian) (listribul;ion.</S>
  <S sid="149" ssid="149">The i;wo tests are, so-calh~d nonl)a.rmut%ri(: l;esl;s, which (lo not; make assuml)l, ions a.1)out; how l, he rcsull;s axe dis- lnil)ut, ed (thrnel,l,, 1982, Ch.</S>
  <S sid="150" ssid="150">ilhe sign |;est is I;he simplier of lJm I;wo.</S>
  <S sid="151" ssid="151">It uses a 1)inomial dist,rilm|;ion to examine the munber of l;esl; smni)les where t;e(:hlfi(lUe ] 1)crforms ])el;- l;er t;ha.n l;e(:hnique 2 ve, rsus l;he munl)er where 1;he Ol)posite occurs.</S>
  <S sid="152" ssid="152">The null hyl)ol;hesis is l;h~d; 1;he t;wo t;eclmiques 1)ertorm equally well.</S>
  <S sid="153" ssid="153">Unlike the sign t;esl;, t;he Vfilcoxon |;esl; also uses inlblnlal;ion on how large a difference xisl;s 1)el;ween t, hc l;wo l;echniques r(,,sull;s on each of l;hc l;csl; smnpl(;s. 3.2 Us ing  the  tes ts  for matched-pa i rs All three of l;hc ma.l,(:he(1-tmir t, sign and Wilcoxon t;csl;s can 1)e a.pl)lied t;o t;hc re, call met- ric, whicll is the flact;ion of |;he il;ems of inl;crcsl; in ~,he l:csl; sol; l;lml; a, I;e, ehniquc recalls (finds).</S>
  <S sid="154" ssid="154">Each il;em of inl,eresi; in |;he l;esl; (la~;a serves as a. l;cst sainlflU.</S>
  <S sid="155" ssid="155">?e use t;he sign l;esl; b(,causc iI; 11Htkcs fcwel" assumi)i;ions 1;hart i;he nml;chcd-l)air 1: I;est and is simplier l;han the Wihoxon I;esi;.</S>
  <S sid="156" ssid="156">111 addit;ion, the fro:i; glml; t~he sign l;e, st ignores l;he size of 1;he result; diflerence on eacll l;esl; Smnl)le (tocs llOI; nml;ter here.</S>
  <S sid="157" ssid="157">?iI:h I;he recall met;rio, each sa.mple of int;eresl; is either found or nol; by a. t;eehnique.</S>
  <S sid="158" ssid="158">There are no interlnedbtte values.</S>
  <S sid="159" ssid="159">While 1;he 1;hree l;esl;s described in sccl;ion 3.1 can be used on the re(:~dl mctxic, 1;hey CallllO|; bc ""   used on ell;lint t;hc precision or slamgh|fforwardly 1)abmced F-score met;rics.</S>
  <S sid="160" ssid="160">This is because both precision and F-score ~tre more coml)licated non- linem flmci;ions of rml(lom varial)lcs than recall.</S>
  <S sid="161" ssid="161">In fst(:t bol;h can be l;hought of as non-linem" flm(:l;ions involving recall.</S>
  <S sid="162" ssid="162">As described in Sec- tion 2.2, precision = 1~./(1~ + S), where I~ is i;he nmnl)er of iWms t;lmt; are of inl:eresl; that; are /c- called by a W, chnique mid S is l;he mmfl)er of it;e, ms (fi)und 1)y s~ technique) that; are nol; of interest;.</S>
  <S sid="163" ssid="163">The 1)~dmmed F-score = 2ab/(a + b), where a is recall and b is precision.</S>
  <S sid="164" ssid="164">951 3.3 Using randomizat ion fbr precision and F -score A class of technique that ean handke all ldnds of flmetions of random variables without the above problenls is the computationally-intellsive ran- domization tests (Noreen, 1989, Ch.</S>
  <S sid="165" ssid="165">2) (Cohen, 1995, Sec.</S>
  <S sid="166" ssid="166">These tests have previously used on such flmctions during the "message un- derstanding" (MUC) evaluations (Chinchor et al., 1993).</S>
  <S sid="167" ssid="167">The randomization test we use is like a randomization version of the paired sample (matched-1)air) t test (Cohen, 1995, Sec.</S>
  <S sid="168" ssid="168">This is a type of stratified shuffling (Noreen, 198!</S>
  <S sid="169" ssid="169">When eomt)aring two tech- niques, we gather-u I) all the responses (whether actually of interest or not) produced by one of the two techniques when examining the test data, but not both techniques.</S>
  <S sid="170" ssid="170">Under the 111111 hyl)othesis , the two techniques are not really different, so any resl)onse produced by one of the teehniques eonld have just as likely come flom the other.</S>
  <S sid="171" ssid="171">So we shuffle these responses, reassign each response to one of the two tech- niques (equally likely to either technique) and see how likely such a shuffle 1)roduces a differ- ence (new technique lninus old technique) in the metric(s) of interest @1 our ease, precision and l?-score) that is at least; as large as the difference observed when using the two techniques on the test data.</S>
  <S sid="172" ssid="172">n responses to shuttle and assign 4 leads to 2 ~ diflerent w~ys to shuffle and assign I;hose re- sponses.</S>
  <S sid="173" ssid="173">So when n. is small, one can try each of the different shuttles once and produce an exact randomization.</S>
  <S sid="174" ssid="174">V~;hen n gets large, the mmfl)er of different shutttes gets too large to be exhaustively evaluated.</S>
  <S sid="175" ssid="175">~J?hen one performs a.u approximate randomization where each shuffle is perfornmd with randoln assignments.</S>
  <S sid="176" ssid="176">For us, when n &lt; 20 (2" .&lt;_.</S>
  <S sid="177" ssid="177">1048576), we use an exact randomization.</S>
  <S sid="178" ssid="178">For n &gt; 20, we use an approximate randomization with 1048576 shuf ties.</S>
  <S sid="179" ssid="179">Because an approximate randomization uses random nmnbers, which both lead to oc~ casional unusual results and may involve using a not-so-good pseudo-random 1111111])(;I" genera- tol "~, we perfbrm the following cheeks: 4Note that responses produced by both or neither techniques do not need to be shulIled and ,~ssigned.</S>
  <S sid="180" ssid="180">5One examI)le is the RANDU routine on the IBM360 (Forsythe t al., 1977, See.</S>
  <S sid="181" ssid="181">We run the 1048576 shuttles a seeond time and colnpare the two sets of results.</S>
  <S sid="182" ssid="182">We also use tile same shutttes to calcu- late the statistical significance for the recall metric, and compare this significance value with the significance value found for recall analytically by the sign test.</S>
  <S sid="183" ssid="183">An example of using randomization is to com- pare two different methods on finding modifier relations ill the same test set,.</S>
  <S sid="184" ssid="184">The results on the test; set, are: Method ~ ~  Precision F-score t i _l_556t  I 49.5% 47.5% Zl: 64.1% 35.2% Two questions being tested are whether the ap- parent ilnt)rovement in reca.ll and F-score f!rom using method I is significant.</S>
  <S sid="185" ssid="185">Also being tested is whether the apparent imt)rovenmnt; in pleci- sion flom using method Ii is significant.</S>
  <S sid="186" ssid="186">In this example, there are 10"1 relations that should be found (are of interest).</S>
  <S sid="187" ssid="187">Of these, 19 are recalled by both methods, 28 are recalled by method I but not; II, and 6 are recalled by II but not I.</S>
  <S sid="188" ssid="188">The correlation coeificient estilnate between the methods recalls is 0.35.</S>
  <S sid="189" ssid="189">In addi- tion, 5 stmrious (not of interest) relations arc found by both methods, with method I find- ing an additional 43 Sl)uriolls relationships (not found by method II) and me?hod II finding an additional 9 relationships.</S>
  <S sid="190" ssid="190">There are a total of 28+6+43+9=86 relations that are found (whether of interest oi not) by one method, but not the other.</S>
  <S sid="191" ssid="191">This is too many to t)erfornl an exact randolnizgtion, so a 1048576 trial apt)roximate randomization is perfornmd.</S>
  <S sid="192" ssid="192">In 96 of these trials, method Is recall is greater than method iIs recall by at, least (45.6%-24.3%).</S>
  <S sid="193" ssid="193">Similarly, in 14794 of the trials, the F-score difference is at least (47.5%-35.2%).</S>
  <S sid="194" ssid="194">In 25770 of the trials, method IIs precision is greater than method Is precision by at least; (64.1%-49.5%).</S>
  <S sid="195" ssid="195">N:om (Noreen, 1989, Sec.</S>
  <S sid="196" ssid="196">aA.a), the significance level (probability under the null hypothesis) is at most (.,e + 1)/(,~t + 1), where ,,.</S>
  <S sid="197" ssid="197">: is the nul~lt/er of trials that meet the criterion alld 1t, t is the number of trials.</S>
  <S sid="198" ssid="198">So fbr recall, the significance level is at most (96+1)/(1048576+1) =0.00009.</S>
  <S sid="199" ssid="199">952 Similarly, for F-score, the significance level is at most 0.</S>
  <S sid="200" ssid="200">()1 d: and for l)re(:ision, the level is at lllOSt 0.025.</S>
  <S sid="201" ssid="201">A secon(l 1048576 trial t)ro(luces imilar results, as does a sign test on recall.</S>
  <S sid="202" ssid="202">lhus, we see that all three dit[ere.n(:es are statistically sig- lfiIica.nt.</S>
  <S sid="203" ssid="203">: hand l ing  in ter -smnple dependenc ies An assmnption made by all I;he methods men- tioned in this I)~tl)er is ttmt the nlenlbcrs of the Lest set are all independent of one anothex.</S>
  <S sid="204" ssid="204">Tlmt is, knowing how a method l)(rforms on one test sot sanlple should not give any information on how that method  ] )e l  for l l ls  on  other test set samples.</S>
  <S sid="205" ssid="205">This assulnl)tJon is not always true.</S>
  <S sid="206" ssid="206">Church and Mercer (1993) give some exaln- ples of dependence bctwe.en test set insl;ances ill na tura l  la.llguage.</S>
  <S sid="207" ssid="207">One tyt)e of  dci)endence is that of a lexemes part of speech on the l)ml;s of speech of  neighl)oring lexenm,~ (th(,ir section 2.1).</S>
  <S sid="208" ssid="208">Sinfilar is the concept of collo- ca, t;ion, where the prolml)ility of a lexemes al&gt; l)earance is influenced by the.</S>
  <S sid="209" ssid="209">lexemes ai)pea.rin: ~ i1~ nearby positions (their section 3).</S>
  <S sid="210" ssid="210">A type of (tet)en(lence that is less local is that often, a. con-- tent words al)pe.arance in a piece of text gr(;atly increases the cha.n(es of th~tt s;ulle wor(1 ~q)l)ear - illg b~ter in that 1)iece of texl; (their se(:l;ion 2.;/).</S>
  <S sid="211" ssid="211">the effects when SOllle d{:t)endency exists?</S>
  <S sid="212" ssid="212">The expected (average) value of the in- stallC(~ results will stay the, same.</S>
  <S sid="213" ssid="213">However, the (lmnees of getting an llllllSllal resl l l t  (;a,lt c]la.ll~re.</S>
  <S sid="214" ssid="214">As an eXmnl)le , take five flips of a Nit coin.</S>
  <S sid="215" ssid="215">When no dependen(:ies exist 1)etween the tlil)s , the clmnces of the extreme result tha.t all the flit)s l:md on :~ particular side is faMy small ((1/2) 5 -- i[/32).</S>
  <S sid="216" ssid="216">When the ttil)s are positively correlated, these chmices increase.</S>
  <S sid="217" ssid="217">When the first flip lands on that side, the chances of the other four tlil)s doing the same are now ea.ch greater tlmn 1/2.</S>
  <S sid="218" ssid="218">Since statistical significance testing involves finding the chances of getting an mmsmd (skcwe(1) result under some null hyt)othesis, one needs to determine those del)endencies in order to accurately determine those dmnces, l)eter- mining the etks:t of these dependencies is some- thing that is yet to l)e done,.</S>
  <S sid="219" ssid="219">5 Conc lus ions In elnpirical natural language processing, one is often COml)aring differences in values of met- rics like recall, precision and balanced F-score.</S>
  <S sid="220" ssid="220">Many of the statistics tests commonly used to make such comparisons assume the indepen- dence between the results being compared.</S>
  <S sid="221" ssid="221">?e ran ~ set of m~tural language processing exper- iments and tbund that this assuml)tion is often violated in .~uch a way as t,o understate the sta- l, istical significance of the difli;rences between the results.</S>
  <S sid="222" ssid="222">We point out some analyt;ica.1 statis- tics tests like lnatched-l)air t,, sign mid Wilcoxon tests, which do not midge this assmnption and show that they (;tl,ll ])e l lsed Oll a l l letr ic like recall, l?br more complicated 1nettles like pre- cision and balanced F-score, wc use a compute-- intensive randonfization test, which also avoids this assumption.</S>
  <S sid="223" ssid="223">A next topic to address is that of possible dependencies l)etween test set  sam- ples.</S>
  <S sid="224" ssid="224">Re ferences G. Box, W. Hunter, and J. Hmlter.</S>
  <S sid="225" ssid="225">1978. ,gta, iisl.ics for" &lt;rpc.rim.ent, er.~.</S>
  <S sid="226" ssid="226">John Wiley and S()llS.</S>
  <S sid="227" ssid="227">N. Chinchor, L. Hirschman, and l).</S>
  <S sid="228" ssid="228">]~vahmtillg message understanding systems: an analysis of the, third message understand- ing conferc.nce (muc-.3).</S>
  <S sid="229" ssid="229">Coll~,pltt(ttiollgl Li ~,- gui,stic.s, 1!)(3).</S>
  <S sid="230" ssid="230">K. Church mid 171.. Mercer.</S>
  <S sid="231" ssid="231">Introduction 1;o the sl)ecial issue on computational linguis- tics using large corpora.</S>
  <S sid="232" ssid="232">Cornpu, tational Lin- guistic.s, 1!)(1.</S>
  <S sid="233" ssid="233">Empirical Meth, ods for Artifi- cial Intelligence.. MIT Press, MA, USA.</S>
  <S sid="234" ssid="234">G. Forsythe, M. M~dcolm, and C. Moler.</S>
  <S sid="235" ssid="235">Com, putcr methods for ~nathcm, atical compu- l.ati~m,.s.</S>
  <S sid="236" ssid="236">Prentice-lI~dl~ N,J, USA.</S>
  <S sid="237" ssid="237">Statistical Methods.</S>
  <S sid="238" ssid="238">Addison-XYesley Publishing Co., 3rd edi- tion.</S>
  <S sid="239" ssid="239">R. Larsen and M. Marx.</S>
  <S sid="240" ssid="240">An Introduc- tion to Ma, th, cmatical Statistics and Its Appli- cations.</S>
  <S sid="241" ssid="241">Prentice-Hall, N J, USA, 2nd edition.</S>
  <S sid="242" ssid="242">Computer-intensive met;hods .</S>
  <S sid="243" ssid="243">[br testing h, ypoth, cscs: an int, rodttction.</S>
  <S sid="244" ssid="244">Jolm Wiley and Sons, Inc. 953</S>
</PAPER>
