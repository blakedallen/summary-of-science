<PAPER>
  <S sid="0" ssid="0">Word-Sense Disambiguation Using Statistical Models of Rogets Categories Trained on Large Corpora David Yarowsky AT&amp;T Bell  Laboratories 600 Mountain Avenue Murray  Hil l  N J, 07974 yarowsky@research.att .com Abst rac t This paper describes a program that disambignates English word senses in unrestricted text using statistical models of the major Rogets Thesaurus categories.</S>
  <S sid="1" ssid="1">Rogets categories serve as approximations of conceptual classes.</S>
  <S sid="2" ssid="2">The categories li ted for a word in Rogers index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful evel of sense disambiguatiou.</S>
  <S sid="3" ssid="3">The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework.</S>
  <S sid="4" ssid="4">Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon.</S>
  <S sid="5" ssid="5">Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unresUicted monolingual text without human intervention.</S>
  <S sid="6" ssid="6">Applied to the 10 million word Groliers Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature.</S>
  <S sid="7" ssid="7">Problem Formulation This paper presents an approach to word sense disambiguation that uses classes of words to derive models useful for disambignating individual words in context.</S>
  <S sid="8" ssid="8">"Sense" is not a well defined concept; it has been based on subjective and often subtle distinctions in topic, register, dialect, collocation, part of speech and valency.</S>
  <S sid="9" ssid="9">For the purposes of this study, we will define the senses of a word as the categories li ted for that word in Rogers International Thesaurus (Fourth Edition - Chapman, 1977).</S>
  <S sid="10" ssid="10">1Sense disambiguation will constitute 1.</S>
  <S sid="11" ssid="11">Note that his edition of Rogers Thesaurus is much more 0ttm$ive than the 1911 vmsion, though somewhat more difficult to obtain in electronic form, One could me other other concept hlemrehics, such as WordNet (Miller, 1990) or the LDOCE mbject codes (Slator, 1991).</S>
  <S sid="12" ssid="12">All that it necessary is ?</S>
  <S sid="13" ssid="13">set of semamic categories and ?</S>
  <S sid="14" ssid="14">list of the words in each category.</S>
  <S sid="15" ssid="15">selecting the listed category which is most probable given the surrounding context.</S>
  <S sid="16" ssid="16">This may appear to be a particularly crude approximation, but as shown in the example below and in the table of results, it is surprisingly successful.</S>
  <S sid="17" ssid="17">I nput  Output Tvr.admillsauachedto cranu were used to lift heavy TOOLS for supplying powe?</S>
  <S sid="18" ssid="18">for cranes, hoists, and lift s. TOOLS hovetlfitheisht,atower crane is oftea med .SB TM* TOOLS ?labocate oaumhip ribalds cranes build ?</S>
  <S sid="19" ssid="19">nest of vegetafi ANIMAL are  more closely tv.lated to cranes and rails .Sn They ran ANIMAL low tees ,PP At least five crane species are in danger of ANIMAl.</S>
  <S sid="20" ssid="20">Not only do the Roget categories succeed in partitioning the major senses, but the sense tags they provide as output are far more mnemonic than a dictionary numbering such as "crane 1.2".</S>
  <S sid="21" ssid="21">Should such a dictionary sense number be desired as output, section 5 will outline how a linkage between Roget categories and dictionary definitions can be made.</S>
  <S sid="22" ssid="22">We will also focus on sense distinctions within a given part of speech.</S>
  <S sid="23" ssid="23">Distinctions between parts of speech, should be based on local syntactic evidence.</S>
  <S sid="24" ssid="24">We use a stochastic part-of-speech tagger (Church, 1989) for this purpose, run as a preprocessor.</S>
  <S sid="25" ssid="25">2, P roposed  Method The strategy proposed here is based on the following three observations: 1) Different conceptual classes of words, such as AmMALS or MACH~mS tend to appear in recognizably different contexts.</S>
  <S sid="26" ssid="26">2) Different word senses tend to belong to different conceptual c asses (crane can be an ANIMAL or a MACHINE).</S>
  <S sid="27" ssid="27">3) If one can build a context discriminator for the conceptual c asses, one has effectively built a context discriminator for the word senses that are members of those classes.</S>
  <S sid="28" ssid="28">Furthermore, the context indicators for a Roget category (e.g.</S>
  <S sid="29" ssid="29">gear, piston and engine for the category TOOLS/MACHINERY) will also tend to be context indicators for the members of that category (such as the machinery sense of crane).</S>
  <S sid="30" ssid="30">ACRES DE COLING-92.</S>
  <S sid="31" ssid="31">NA~rrES, 23-28 AO~r 1992 4 5 4 PRec.</S>
  <S sid="32" ssid="32">OF COLING-92, NANTES, AUG. 23-28, 1992 We attempt to identify, weight and utilize th~e indicative words &amp;s follows.</S>
  <S sid="33" ssid="33">For each of the 1042 Roget Categories: 1.</S>
  <S sid="34" ssid="34">Collect contexts which are representative of the Roget category 2.</S>
  <S sid="35" ssid="35">Identify salient words in the collective context and determine weights for each word, and 3.</S>
  <S sid="36" ssid="36">Use the resulting weights to predict he appropriate category for a polysemous word occurring in novel text.</S>
  <S sid="37" ssid="37">2.1 Step 1: Col lect  Contexts  wh ich  are Representative of the  Roget  category The goal of this step is to collect a set of words t/tat are typically found in the context of a Roget category.</S>
  <S sid="38" ssid="38">To do this, we extract concordances of 100 surrounding words for e2~h occurrence of each member of the category ill the corpus.</S>
  <S sid="39" ssid="39">Below is a sample set of partial concordances for words in the category "IOOLS.tMACIIINERY (348).</S>
  <S sid="40" ssid="40">The complete set contains 30,924 lines, selected from the particular training corpus used in this study, the 10 million word.</S>
  <S sid="41" ssid="41">June 1991 electronic version of Groliers Encyclopedia.</S>
  <S sid="42" ssid="42">CARVING .SB The gutter uipment such as a hydraulic on .SB Resembling apower uipmant, valves for nuclear 00 BC, flint-odged wooden 1-penetrating c~bide-tipped ~lt heightens the colors .SB lxaditionM ABC method and nter of ro~tion .PP A rower rshy areas .SB The crowned adz has a concave blade for fonn shovel capable of lifting 26 cubic shovel mounted on a floating hul generators, oil-refinery turbines sickles were used to gather wild drills forced manufacturers to fi Drills live in the forests of equa drill were unchanged, and dissa crane is an assembly of fabricat crane, however, occasionally For optimal training, die concordance set should only include references to the given category.</S>
  <S sid="43" ssid="43">But in practice it will unavoidably include spurious examples ince mmty of the words are polysemous (such its drill mid crane in lines 7, 8, and 10 above).</S>
  <S sid="44" ssid="44">While the level of noise introduced through polysemy is substantial, it can usually be tolerated because the spurious senses are distributed through tile 1(}41 other categories, whereas the signal is coneenwated in just one.</S>
  <S sid="45" ssid="45">Only if several words had secondary senses in the state category would context typical for the other category appear significant in this context.</S>
  <S sid="46" ssid="46">However, if one of these spurious senses was frexluent and dominated the set of examples, file situation could be disastrous.</S>
  <S sid="47" ssid="47">An attempt is made to weight the concordance data to minim~e this effect and to make the sample representative of all tools attd tnachinery, not just the more common ones.</S>
  <S sid="48" ssid="48">If a word such as drill occurs k tinies in the coqms, all words ill the context of drill contribnte weight 1/k to frequency sunos.</S>
  <S sid="49" ssid="49">Despite its flaws, this weighted matrix will serve as a representative, albeit noisy, sample of the typical context ofIYOOI.S/MACItlNERY in Groliers encyclopedia.</S>
  <S sid="50" ssid="50">2.2 Step 2: Identify ,salient words in the col lect ive cmttext ,  and  we ight  appropr ia te ly Intuitively, a salient word 2 is one which appears siguificantly more often in the context of a category than at other txfints in the corpus, and hence is a better than average indicator for the category.</S>
  <S sid="51" ssid="51">We formalize this wifl] a nmtual-in formation-like estimate: Pr(wlRCat) / Pr(w), tile probability of a word (w) appearing in the context of a Roget category divided by its overall probability in rile corpus.</S>
  <S sid="52" ssid="52">It is imlmrtant to exerci~ some care in estimating Pr(wlRCat).</S>
  <S sid="53" ssid="53">In principle, one could situply count tile number of times that w appears in the collective contexL However, this estimate, which is known as the tuaximnnt likelih(x,d estimate (MLE), can be unreliable, especially when w does not apl~-~ar vely often in the collective coutexl.</S>
  <S sid="54" ssid="54">We have smoothed file local estimates of Pr(wlRCat) with global estinmtes of Pr(w) to obtain a more reliable estimate.</S>
  <S sid="55" ssid="55">Estimates obtained from the local context are subject to measurement errors whereas estimates obtained liom the global context are subject o being irrelevant.</S>
  <S sid="56" ssid="56">By interpoiathlg between the two, we attempt to find a compromise between the two sources of error, qllis procexlure is b~sed on recent work pioneewM by Willimn Gale, attd is explained in detail in another paper (Gale, Church and Yarowsky, 1992).</S>
  <S sid="57" ssid="57">Space does not permit a complete description here.</S>
  <S sid="58" ssid="58">Below are salient words tor Roget categories 348 and 414.</S>
  <S sid="59" ssid="59">*lllose ~lected are tile ntosl important 1o rite models, where importance is delined as the product of salience and local fiequency.</S>
  <S sid="60" ssid="60">That is to say important words ate distinctive and fi~equcat.</S>
  <S sid="61" ssid="61">The nnmhers in parentheses are the log of the salience (logPr(wlRCat) /Pr(w)) ,  which we will henceforth refer to as the words weight in the statistical model of the category.</S>
  <S sid="62" ssid="62">Fo~ illustrative simplicity, we will refer to words in context, In pnlctice, all op~]lil~t$ ale ac~uMly p~rfonned onthe Iemma~ of the words (eal/V = eat,eatg.elling,ate,elae~l), lind inflecdonml dlnincdons tire igltored.</S>
  <S sid="63" ssid="63">While thi* achieves more concentrated and bclter estimated ttttiUics, it throws away uneful information which natty be ext~loited in future work.</S>
  <S sid="64" ssid="64">ACIES DE cOL1NG-92, NANTES, 23-28 Ao(rr 1992 4 S g PROC.</S>
  <S sid="65" ssid="65">o1: COLING-92, NAN rJ,S, A[Jo.</S>
  <S sid="66" ssid="66">23-28, 1992 ANIMAL,INSECT (Category 414): species (2.3), family (1.7), bird (2.6), fish (2.4), breed (2.2), cm (2.2), animal (1.7), tail (2.7), egg (2.2), wild (2.6), common (1.3), coat (2.5), female (2.0), inhabit (2.2), eat (2,2), nest (2.5) .... TOOLS/MACHINERY (Category 348): tool (3.1), machine (2.7), engine (2.6), blade (3.8), cut (2.6), saw (5.1), lever (4.1), pump (3.5), device (2.2), gear (3.5), knife(3.8), wheel (2.8), shaft(3.3), wood(2.0), tooth(2.5), piston(3.6) .... Notice that these are not a list of members of the category; they are the words which are likely to co-occur with the members of the category.</S>
  <S sid="67" ssid="67">The complete list for TOOLS/MACH1NFJI.Y includes a broad set of relations, such as meronomy (blade, engine, gear, wheel, shaft, tooth, piston and cylinder), typical functions of machines (cut, rotate, move, turn, pull), typical objecls of those actions (wood, metal), as well as typical modifiers for machines (electric, mechanical, pneumatic).</S>
  <S sid="68" ssid="68">The list for a category typically contains over 3000 words, and is far richer than can be derived from a dictionary definition.</S>
  <S sid="69" ssid="69">2.3 Step 3: Use the resul t ing weights  to predict the appropr ia te  category  for a word  in novel text When any of the salient words derived in step 2 appear in the context of an ambiguous word, there is evidence that the word belongs to the indicated category.</S>
  <S sid="70" ssid="70">If several such words appear, the evidence is compounded.</S>
  <S sid="71" ssid="71">Using Bayes rule, we sum their weights, over all words in context, and determine the category for which the sum is greatest ~.</S>
  <S sid="72" ssid="72">ARGMAX ~ log Pr(w[RCat)  x Pr(RCat) Rca: w i~ co,,~1 Pr (w) The context is defined to extend 50 words to the left and 50 words to the right of the polysemous word.</S>
  <S sid="73" ssid="73">This range was shown by Gale, Church and Yarowsky (1992) to be useful for this type of broad topic classification, in contrast to the relatively narrow (+3-6 word) window used in previous studies (e.g.</S>
  <S sid="74" ssid="74">The reader may have noticed that he Pr(w) factor can be omitted since it will not change the results of the maximization.</S>
  <S sid="75" ssid="75">It is included here for expository convenience sothat it is possible to ~-,npare r sults across words with very different probabilities, nae factor also become?</S>
  <S sid="76" ssid="76">impoc.ant when an incomplete t of indicators iJ stored be, cause of comlmtational spac~ constraints.</S>
  <S sid="77" ssid="77">Currently we assume a uniform prior- probability for each Roget category (Pr(Rcal)).</S>
  <S sid="78" ssid="78">tense classification is based exclusively on otmte~tual information, i dependent of he underlying prd3abillt y of a given Re?el category appearing at any point in the colpos.</S>
  <S sid="79" ssid="79">maximization over RCats is constrained to consider only those categories under which the polysemous word is listed, generally on the order of a half dozen or so.</S>
  <S sid="80" ssid="80">4 For example the word crane appears 74 times in Groliers; 36 occurrences refer to the animal sense and 38 refer to the heavy machinery sense.</S>
  <S sid="81" ssid="81">The system correctly classified all but one of the machinery senses, yielding 99% overall accuracy.</S>
  <S sid="82" ssid="82">The one miselassified case had a low score for all models, indicating a lack of confidence in any classification.</S>
  <S sid="83" ssid="83">It is useful to look at one example in some more detail.</S>
  <S sid="84" ssid="84">Consider the following instance of crane and its context of + l0 words: 5 lift water and to grind grain .PP Treadmills attached to cranes were used to lift heavy objects from Roman times, The table below shows the strongest indicators identified for the two categories in the sentence above.</S>
  <S sid="85" ssid="85">The model weights, as noted above, are equivalent to log Pr(wlRCat  ) / Pr(w).</S>
  <S sid="86" ssid="86">Several indicators were found for the TOOLS/MACHtNE class.</S>
  <S sid="87" ssid="87">There is very little evidence for the ANIMAL sense of crane, with the possible exception of water.</S>
  <S sid="88" ssid="88">The preponderance of evidence favors the former classification, which happens to be correct.</S>
  <S sid="89" ssid="89">The difference between the two total scores indicate strong confidence in the answer.</S>
  <S sid="90" ssid="90">Weight ANIMALINSECT Weight water 0.76 lift 2.44 lift 2.44 grain 1.68 used 1.32 heavy 1.28 Treadmills 1.16 attached 0.58 grind 0.29 water 0,11 TOTAL 11,30 TOTAL 0.76 4.</S>
  <S sid="91" ssid="91">Although it is often useful to restrict the search in this way.</S>
  <S sid="92" ssid="92">the restriction does ometimes l ad to uc~ble, especially when there are gaps in the thesaurus.</S>
  <S sid="93" ssid="93">For example, the category AMIJSI~I,,g~-r (# 876) lisa ?</S>
  <S sid="94" ssid="94">number of card playin 8terms, lint for some reason, the word suit is not included in this list.</S>
  <S sid="95" ssid="95">As it happens, the Gruliers Encydopndia contains 54 instances ofthe card-playing sense of suit, all of which ale mislabeled if the search is limited to just those categories of suit that are listed in RogeCs.</S>
  <S sid="96" ssid="96">However, if we open up the search to consider all 1042 care?odes, then we find that all 54 instances ofsu//are correctly abeled ils/o?/usE,~,~cr, andmo~over.</S>
  <S sid="97" ssid="97">the scca~ is large in all 54 instances, indicating great confidence in the assignment.</S>
  <S sid="98" ssid="98">I  is poJsiblc that the unrestricted search mode might be ?</S>
  <S sid="99" ssid="99">good way to attemps tofill in omisfions in the ?ha?auras.</S>
  <S sid="100" ssid="100">when suit is added to the ,oa~t/s~E~rr category, overall accuracy improves from 68% to 92%.</S>
  <S sid="101" ssid="101">5, "Ibis narrower window is used for iaust rative simplicity.</S>
  <S sid="102" ssid="102">ACRES DE COLING-92, NANTES, 23-28 ^ OUT 1992 4 5 6 PREC.</S>
  <S sid="103" ssid="103">OF COLING-92, NANTES, AUG. 23-28, 1992 TABLE i Sen~ R~etCat~o~ N Co~.</S>
  <S sid="104" ssid="104">STAR (Hirst, 1987: N/A) Space Object UNIVERSE 1422 96% Celebrity r~rcrm~TArNt.X 222 95% Staa" Shaped Object INSlOlqIA ..... _5_6_ ...... .82.% 1700 96% MOLE (HirsL 1987: N/A *) Quantity OII!MlCALS 95 98% Mammal ANIMALJNSECI" 46 100% Skin Blemish DISEASE 13 100% Digging Machine SUPPORT 4 100~o 160 99% GALLEY (LusL 1986: 50-70% overall) Ancient Slfip SIIIP,BOAT 35 97% Printers Tray PRI~flNG 5 100% Ships Kitchen .</S>
  <S sid="105" ssid="105">COOKING 2 50% 42 95% CONE (Lesk, 1986; 50-70% overall *) Part of Trec PLANT 71 99% Shape of Ohject ANGULARITY 89 61% Part of Eye VISION 13 69% 173 77% BA&amp;q (HearsL 1991: 100%; Speech Synthesis) Musical Senses MUSIC 158 99% Fish ANIMAL,INSECrf 69 100% 227 99% BOW (Clear, 1989: &lt; 67%; Speech Synthesis) Weapon ARMS 59 92% Front of Ship StllP,BOAT 34 94% Violin Part MUSICAL INSTR 30 100% Ribbon ORNAMENTAalON 4 25% Bend in Object CONV~.XnV 2 50e Lowering Head RESPF.CT .______0_ .</S>
  <S sid="106" ssid="106">5_-___ 129 91% ~- -~Clear ,  1989: &lt; 65%) Preference PARI]CULA RIIY 228 93% Flavor SENSATION 80 93% 308 93% INTEREST (Black, 1988: 72%; Zemik, 1990: &gt; 70%) Curiosity REASONING 359 88% Advantage IN/UffI1CE 163 34% Financial DEBT 59 90% Share PROPERTY 21 38% 602 72% ISSUE (Zemik, 1990: &lt; 70%) Topic rotrtacs 831 94% Periodical BOOKS.PERIODI 28 89% Stock SECURn1Es 9 1OO% 868 94% Sense Roget Category N Corr.</S>
  <S sid="107" ssid="107">DUTY (Gale et el, 1992: 96%) Obligation DUTY 347 96% Tax PRICE.Iq:~.</S>
  <S sid="108" ssid="108">52 96% 399 96% SENTENCE (Gale et al, 1992, 90% *) Florishment t .EGALACI1ON 128 99% Set of Words GRAMMAR 213 98% 341 98% SLUG (Hirsk 1987: N/A *) Animal ANI MAL,INSI!CT 24 100% Type Strip I,RINTtNO 8 100% Mass Unit WEIGItT 3 100% Fake Coin MONEY 2 50% Metallurgy 1MPUIZE.IMPAC-f I 100% Bullet ARMS 1 100% 39 97% Notes: 1) N refers to the total number of each sense obseawed in the test corpus.</S>
  <S sid="109" ssid="109">indicates file percemage of those tagged correctly.</S>
  <S sid="110" ssid="110">2) Because thexe is no independent ground truth to indicate which is the "correct" Roget category for a given word, the decision is a subjective judgement made by a single human judge, in this case the author.</S>
  <S sid="111" ssid="111">3) As previously noted, the Roger index is incomplete, hi four cases, identified by *, one missing category has been added to the list of possibilities for a word.</S>
  <S sid="112" ssid="112">These ontissions in the lexicon have been identified as outlined in Footnote 4.</S>
  <S sid="113" ssid="113">Without these additions, overall system performance would decrease by 5%.</S>
  <S sid="114" ssid="114">4) Uses which an English speaker may consider a single sense are often realized by several Roget categories.</S>
  <S sid="115" ssid="115">For the purposes of succinct representation, such categories have been merged, and the name of file dominant category used in the table.</S>
  <S sid="116" ssid="116">As of this writing, the process has not been fully automated.</S>
  <S sid="117" ssid="117">For many applications such as speech synthesis and assignment to an established ictionary sense number or possible French translations, this merging of Roget classes is not necessary.</S>
  <S sid="118" ssid="118">The primary criterion for success is that words are partitioned into pure sense clusters.</S>
  <S sid="119" ssid="119">Words having a different sense from the majority sense of a partition are graded as errors.</S>
  <S sid="120" ssid="120">5) Examples with the ammtation speech synthesis have multiple pronunciations corresponding to sense distinctions.</S>
  <S sid="121" ssid="121">Their disambiguafion is important in speech processing.</S>
  <S sid="122" ssid="122">6) All results are based on 100% recall.</S>
  <S sid="123" ssid="123">AcrEs DE COLlNG-92, NAI~ES, 23-28 AOt3T 1992 4 5 7 PROC.</S>
  <S sid="124" ssid="124">OF COLING-92, NArCrES, AUG. 23-28, 1992 3.</S>
  <S sid="125" ssid="125">Eva luat ion The algorithm described above was applied to 12 polysemous words previously discussed in the sense disambignation literature.</S>
  <S sid="126" ssid="126">Table 1 (previous l~lge) shows the systenls performance.</S>
  <S sid="127" ssid="127">Authors who have discussed these words are listed in parentheses, along with the reported accuracy of their systems.</S>
  <S sid="128" ssid="128">Direct comparisons of performance between researchers is difficult, compounded by variances in corpora nd grading criteria; using the same words is an attempt o minimize these differences.</S>
  <S sid="129" ssid="129">Regrettably, most authors have reported their results in qualitative terms.</S>
  <S sid="130" ssid="130">The exceptions include Zemik (1990) who cited "recall and precision of over 70%" for one word (interest) and observed that results for other words, including /ssue, were "less positive."</S>
  <S sid="131" ssid="131">Clear (1989) reported results for two words (65% and 67%), apparently at 85% recall.</S>
  <S sid="132" ssid="132">Leak (1986) claimed overall "50-70%" accuracies, although it is unclear under which parameters and constraints.</S>
  <S sid="133" ssid="133">In a 5 word test set, Black (1988) observed 75% mean accuracy using his optimal method on high entropy, 4-way sense distinctions.</S>
  <S sid="134" ssid="134">Hearst (1991) achieved 84% on simpler 2-way distinctions, editing out additional senses from the test set.</S>
  <S sid="135" ssid="135">Gale, Church and Yarowsky (1992) reported 92% accuracy, also on 2-way distinctions.</S>
  <S sid="136" ssid="136">Out eunent work compares favorably with these results, with 92% accuracy on a mean 3-way sense distinction 6.</S>
  <S sid="137" ssid="137">The performance is especially promising iven that no hand tagging or special corpora were required in training, unlike all other systems considered.</S>
  <S sid="138" ssid="138">Limitations of the Method The procedure described here is based on broad context models.</S>
  <S sid="139" ssid="139">It performs best on words with senses which can be distinguished by their broad context.</S>
  <S sid="140" ssid="140">These are most typically concrete nouns.</S>
  <S sid="141" ssid="141">Performance is weaker on the following: Topic Independent Distinctions: One of the reasons that interest is disambiguated poorly is that it can appear in almost any context.</S>
  <S sid="142" ssid="142">While its "curiosity" sense is often indicated by the presence of an academic subject or hobbie, the "advantage" sense (to be in ones interests) has few topic constraints.</S>
  <S sid="143" ssid="143">Distinguishing between two such abstractions i difficult.</S>
  <S sid="144" ssid="144">7 However, the financial 6.</S>
  <S sid="145" ssid="145">This result is a fair ra~lure of pedorr~nee on words used in p~vi{ms studies, and may he useful for comparison acms l  systems.</S>
  <S sid="146" ssid="146">However, as wolrd$ pmvioully discuJscd inthe literature may not he t~preu~tafive of typical English polyk-my, mean performance on ?</S>
  <S sid="147" ssid="147">eomlTletely random u~ of words hould iffer, 7.</S>
  <S sid="148" ssid="148">Black (1988) has noted that his disfnction for interest is strongly corrected with th?</S>
  <S sid="149" ssid="149">~urality (~" the word, afuture we cura~ntly dont utilize.</S>
  <S sid="150" ssid="150">sense of interest is readily identifiable, and can be distinguished from the non-financial uses with 92% accuracy.</S>
  <S sid="151" ssid="151">Other distinctions between topic independent and topic constrained senses appear successful as well (e.g.</S>
  <S sid="152" ssid="152">taste, issue, duty and sentence).</S>
  <S sid="153" ssid="153">Minor Sense Distinctions within a Category: Distinctions between the medicinal and narcotic senses of drug ate not captured by the system because they both belong to the same Roget category (REMEDY).</S>
  <S sid="154" ssid="154">Similar problems occur with the musical senses of bass.</S>
  <S sid="155" ssid="155">Rogets Thesaurus offers a rich sub-hierarchy within each category, however.</S>
  <S sid="156" ssid="156">Future implementations will likely use this information, which is currently ignored.</S>
  <S sid="157" ssid="157">Verbs: Verbs have not been considered in this particular study, and it appears that they may benefit from more local models of their typical arguments.</S>
  <S sid="158" ssid="158">The unmodified system does seem to perform well on verbs which show clear topic distinctions such as fire.</S>
  <S sid="159" ssid="159">Its weapon, engine, furnace, employee, imagination and pottery senses have been disambiguated with 85% accuracy.</S>
  <S sid="160" ssid="160">Pre-Nominal Modifiers: The disambiguation of pre- nominal modifiers (adjectives and compound nominals) is heavily dependent on the noun modified, and much less so on distant context.</S>
  <S sid="161" ssid="161">While class-based Bayesian discrimination may be useful here as well, the optimal window size is much narrower.</S>
  <S sid="162" ssid="162">Idioms: These broad context, topic-based discriminators are also less successful in dealing with a word like hand, which is usually found in fixed expressions such us on the other hand and close at hand.</S>
  <S sid="163" ssid="163">These fixed expressions have more function than content, and therefore, they do not lend themselves to a method that depends on differences in content.</S>
  <S sid="164" ssid="164">The situation is far from hopeless, as many idioms are listed directly in Rogets Thesaurus and can be associated with a category through simple table lookup.</S>
  <S sid="165" ssid="165">Other research, such as Smadja and McKeown (1990), have shown more general ways of identifying and handling these fixed expressions and collocations.</S>
  <S sid="166" ssid="166">Given the broad set of issues involved in sense disambiguation, it is reasonable touse several specialized tools in cooperation.</S>
  <S sid="167" ssid="167">We akeady handle part of speech distinctions through other methods; an efficient idiom recognizer would be an appropriate addition as well.</S>
  <S sid="168" ssid="168">Linking Roget Categories with other Sense  Representations The Roget category names tend to be highly mnemonic and may well suffice as sense tags.</S>
  <S sid="169" ssid="169">However, one may want to link the Roget tags with an established reference such as the sense numbers one finds in a dictionary.</S>
  <S sid="170" ssid="170">We accomplish this by applying the models described above to the text of the definitions in a dictionary, creating a table of correspondences between Roget categories and ACRES DE COLING-92, NANTES, 23-28 AOUT 1992 4 5 8 PROC.</S>
  <S sid="171" ssid="171">OF COLING-92, NANTES, AUG. 23-28, 1992 sense numbers.</S>
  <S sid="172" ssid="172">Results for the word crane are illustrated below for two dictionaries: (1) COBUILD (Sinclair, 1987), and (2) Collins English Dictionary, First Edition (CED1) (Hanks, 1979).</S>
  <S sid="173" ssid="173">RCAT Sense # Definition TOOLS crane 1.1 ANIMAL crane 1.2 ANIMAL crane l ANIMAL crane 2 TOOLS crane 3 "IDOLS crane 4 a machine with a long movable large bird with a long neck and any large long-necked long-leg any similar bird, such as a her a device for lifting and moving a large trolley carrying aboom It may also be possible to link Roget category tags with "natural" sense tags, such as translations in a foreign language.</S>
  <S sid="174" ssid="174">We use a word-aligned parallel bilingual corpus uch as the French-English Canadian Hansards for this purpose.</S>
  <S sid="175" ssid="175">For example, consider the polysemous word duty which can be translated into French its devoir or droit, depending on the sense (obligation or tax, respectively).</S>
  <S sid="176" ssid="176">When the Grolier-trained models are applied to the English side of the Hansards, the words tagged PRICE.FI~ most commonly aligned with the French words droits (256), droit (96) and douane (67).</S>
  <S sid="177" ssid="177">Words labeled OUT/(the Roget category for Obligation) most frequently aligned with devoir (205).</S>
  <S sid="178" ssid="178">These correlations may have useful implications for machine translation and bilingual lexicography.</S>
  <S sid="179" ssid="179">Other Sense Disambiguation Methods: The Knowledge  Acqu is i t ion  Bot t leneck Word sense disambiguation is a long-standing problem in computational linguistics (Kaplan, 1950 ; Yngve, 1955; Bar-Hillel, 1960), with important implications for a variety of practical applications including speech synthesis, information retrieval, and machine translation.</S>
  <S sid="180" ssid="180">Most approaches may be characterized by the lollowing generalizations: 1) They tend to focus on the search for sets of word-specific features or indicators (typically words in context) which can disambignate the senses of a word.</S>
  <S sid="181" ssid="181">2) Efforts to acquire these indicators have faced a knowledge acquisition bottleneck, characterized byeither substantial human involvement for each word, and/or incomplete vocalmlary coverage.</S>
  <S sid="182" ssid="182">The AI community has enjoyed some success hand- coding detailed "word experts" (Small and Rieger, 1982; HirsL 1987), but this labor intensive process has severely limited coverage beyond small vocabularies.</S>
  <S sid="183" ssid="183">Others such as Lesk (1986), Walker (1987), Veronis and Ide (1990), and Guthrie et al.</S>
  <S sid="184" ssid="184">(1991) have turned to machine readable dictionaries (MRDs) in an effort to achieve broad vocabulary coverage.</S>
  <S sid="185" ssid="185">MRDs have the useful property that some indicative words for each sense are directly available in numbered definitions and examples.</S>
  <S sid="186" ssid="186">However, definitions arc often too short to provide an adequate set of indicators, and those words which are found lack significance weights to identify which are crucial and which are merely chaff.</S>
  <S sid="187" ssid="187">Dictionaries provide well structured but incomplete information.</S>
  <S sid="188" ssid="188">Recently, many have turned to text corpora to broaden the range and volume of available examples.</S>
  <S sid="189" ssid="189">Unlike dictionaries, however, raw corpora do not indicate which sense of a word occurs at a given instance.</S>
  <S sid="190" ssid="190">Several researchers (Kelly and Stone, 1975; Black, 1988) have overcome this through and tagging of training examples, and were able to discover useful discriminatory patterns from the partitioned contexts.</S>
  <S sid="191" ssid="191">This also has proved labor intensive.</S>
  <S sid="192" ssid="192">Others (Weiss, 1973; Zeroik, 1990; Hearst, 1991) have attempted to partially automate the hand- tagging process through bootstrapping.</S>
  <S sid="193" ssid="193">Yet this has still required significant human intervention for each word in the vocabulary.</S>
  <S sid="194" ssid="194">(1991), Dagan (1991), and Gale ct at.</S>
  <S sid="195" ssid="195">(1992) have looked to parallel bilingual corpora to further automate training set acquisition.</S>
  <S sid="196" ssid="196">By identifying word correspondences in a bilingual text such as the Canadian Parliamentary Proceedings (Hansards), the translations found fur each English word may serve as sense tags.</S>
  <S sid="197" ssid="197">For example, the senses of sentence may be identified through their correspondence in the French to phrase (grammatical sentence) or peine (legal sentence).</S>
  <S sid="198" ssid="198">While this method has been used successfully on a portion of the vocabulary, its coverage is also limited.</S>
  <S sid="199" ssid="199">Currently available bilingual corpora lack size or diversity: over hulf of the words considered in this study either never appear in the Hansards or lack examples of secondary senses.</S>
  <S sid="200" ssid="200">More fundamentally, many words are mutually ambiguous across languages.</S>
  <S sid="201" ssid="201">French would be of little use in disambiguating the word interest, as all major senses translate as int~rdt.</S>
  <S sid="202" ssid="202">More promising is a non-lndo European language such as Japanese, which should avoid such mutual ambiguity for etymological reasons.</S>
  <S sid="203" ssid="203">Until more diverse, large bilingual corpora become available, the coverage of these methods will remain limited.</S>
  <S sid="204" ssid="204">Each of these approaches have faced a fundamental obstacle: word sense is an abstract concept hat is not identified in natural texL Hence any system which hopes to acquire discriminators for specific senses of a word will need to isolate samples of those senses.</S>
  <S sid="205" ssid="205">While this process has been partially automated, it appears to require substantial human intervention to handle an unrestricted vocabulary.</S>
  <S sid="206" ssid="206">Conclusion This paper has described an approach to word sense disambiguation using statistical models of word classes.</S>
  <S sid="207" ssid="207">This method overcomes the knowledge acquisition bottleneck faced by word-specific sense discriminators.</S>
  <S sid="208" ssid="208">By entirely circumventing the issue of polysemy AClXS DE COLING-92, NANqVS, 23-28 Aovr 1992 4 5 9 PRec.</S>
  <S sid="209" ssid="209">OF COLING-92, NAI~fES, AUG. 23-28.</S>
  <S sid="210" ssid="210">1992 resolution in training material acquisition, the system has acquired an extensive set of sense discriminators from unrestricted monolingnal texts withoat haman intervention.</S>
  <S sid="211" ssid="211">Class models also offer the additional advantages of smaller model storage requirements and increased implementation efficiency due to reduced dimensionality.</S>
  <S sid="212" ssid="212">Also, they can correctly identify a word sense which occurs rarely or only once in the corpus - - performance unattainable by statistically trained word- specific models.</S>
  <S sid="213" ssid="213">These advances are not without cost, as class-based models have diluted discriminating power and may not capture highly indicative collocations specific to only one word.</S>
  <S sid="214" ssid="214">Despite the inherent handicaps, the system performs better than several previous approaches, based on a direct comparison of results for the same words.</S>
  <S sid="215" ssid="215">Acknowledgements Special thanks are due to Ken Church and Barbara Grosz for their invaluable help in restructuring this paper, and to Bill Gale for the theoretical fonndalions on which this work rests.</S>
  <S sid="216" ssid="216">The author is also grateful to Marts Hearst, Femando Pemira, Donald Hindle, Richard Sproat, and Michael Riley for their comments and suggestions.</S>
  <S sid="217" ssid="217">References Bar-Hillel (1960).</S>
  <S sid="218" ssid="218">"Automatic Translation fLanguages," in Advances in Coenpmera, Donald Booth and R. E. Meagher, eds., Academic, New York.</S>
  <S sid="219" ssid="219">Black, Ez~ (1988), "An Experiment in Computational Discrimination of English Word Sea~s."</S>
  <S sid="220" ssid="220">IBM Journal of Research and Dev?lopmenl, v 32. pp 185-194.</S>
  <S sid="221" ssid="221">Brown, Pr.mr, Stephen Della Pietra, Vincent Della Pinata, and Robert Mercer (1991), "Word Sense Disambiguition using Statistical Methods," Prooteding$ ofthe 2?th Annual Meeting of the Association [or Computational Linguistics, pp 264-270.</S>
  <S sid="222" ssid="222">Brown, Peter, Vil,,c~at Delh Pintra, Peter deSouza, nd Rck~rt Mercer (1990), "clasa-based n-gram Modeht of Natural Language," Proceedings of the IBM Natural Language ITL, Paris, Fnmce, pp 283- 298.</S>
  <S sid="223" ssid="223">C~apman, Robert (1977).</S>
  <S sid="224" ssid="224">Rogets International Thesaur~ (Fourth Edition), Haq~r and Row, New York, Choueka, Ymmov, and Serge Lusignam (1985).</S>
  <S sid="225" ssid="225">"Disambiguation by Short Contexts," Computera and the ltwnanities, v 19. pp.</S>
  <S sid="226" ssid="226">Omtch, Kmneth (1989), "A Stochastic Parts Program an Noun Phnse Parser for Un~strict~d Text, ~ Proceeding, IEEE International Conference on Acovatics, Speech and Signal Processing, Glasgow.</S>
  <S sid="227" ssid="227">Clear, Jeremy (1989).</S>
  <S sid="228" ssid="228">"An Experiment in Automatic Word Sense ld~RificJtlon."</S>
  <S sid="229" ssid="229">Internal Doctwnent, Oxford Univerlity Press, Oxford.</S>
  <S sid="230" ssid="230">Courdl, Garriton (1989).</S>
  <S sid="231" ssid="231">A Connectionist Appre.ach to Word Sense Disamblguatioa, Pitman, London.</S>
  <S sid="232" ssid="232">Dagan, 13o, Alon leaS, and Olrike Schwall (1991), "Two Languages am Infmmative than One," Proceedings ofthe 29th Annual Meeting of the Aesoclation for Computatiosal Linguistics.</S>
  <S sid="233" ssid="233">Gale, William, Kenneth (:hutch, and David Yarowsky (1992), "Ditcriminatlon Decisions for 100,000-Dimensional Sp ces" AT&amp;T Statistical Retear?.h Report No.</S>
  <S sid="234" ssid="234">Gale, William, Kenneth Church, and David Yarowsky (1992), "A Method for Disarnbiguating Word S~ses in ?</S>
  <S sid="235" ssid="235">Large Ca~.ts," to appear in Computers and llumdnitits, Gnmger, Richard (1977), "FOUL-UP A program that figures out meanings ofwo~ from ?~ntext," HCAII-77, pp.</S>
  <S sid="236" ssid="236">Guthile, J., L Guthrle, Y.</S>
  <S sid="237" ssid="237">Walks, and H. Aidinejad (1991), "Subject- Dependent Co-oc.cunea~ and Word Sense Disambiguation," Proceedings of the 29th Annual M, teeing of the Association for Compulmlanal Linguistics, pp 146-152.</S>
  <S sid="238" ssid="238">Hanks, Patrick (ed.)</S>
  <S sid="239" ssid="239">(1979), Collins English Dictionary, Collins, London and Glasgow, He.am, Matti (1991), *Noun Hctnograph Disambiguation Using Local Context in Large Text Corpora," Using Corpora, Univenrity of Waterloo, Waterloo, Ontario, ]tirst, Graerne.</S>
  <S sid="240" ssid="240">(1987), Stmamic luterpretation a d the Resolution of Ambiguity, Cambridge University Pl~ss, Cambridge.</S>
  <S sid="241" ssid="241">K~plan, Abraham 0950), "An Experimental Study of Ambiguity in Context," cited in Mechanical Translation, v. I, nos.</S>
  <S sid="242" ssid="242">Kelly, ,Edward, and Phillip Stone (1975), Computer Recognition of English Word Senses, North-HoUand, Amsterdam.</S>
  <S sid="243" ssid="243">Lask, Michael (1986), "Automatic Sense Disambiguadoa: How to tell ?</S>
  <S sid="244" ssid="244">Pine Cone from an Ice Cream Cone," Proceeding of the 1986 SIGDOC Conference, Association for Ct~nputing Machinery, New York.</S>
  <S sid="245" ssid="245">Miller, George (1990), "Woednea: An On-line Leaical Database," InterncUionalJournal ofLexicography, 4(3), 1990.</S>
  <S sid="246" ssid="246">(Special Issue).</S>
  <S sid="247" ssid="247">Moate].ler, F edrick, and David Wallace (1964).</S>
  <S sid="248" ssid="248">Inference and Disputed Authorxhip: The Federalist, Addison-Wesley, Reading, Mastacinamtts.</S>
  <S sid="249" ssid="249">Salton, G. (1989), Automatic Text Processing, Addis0n-Wesley Publishing Co. Sinclair, J., Ilanks, P., Fox, G., Moon, R., Stock, P. et el.</S>
  <S sid="250" ssid="250">(1987) Collin~ Cobuild English Language Dictionary, Collins, London and Glasgow.</S>
  <S sid="251" ssid="251">Slator, Brian (1991), "Using Context for Sense Prefenmce," in Zcmik (ed.)</S>
  <S sid="252" ssid="252">Lexical AcqioMtion: E:9~ioitiog On-Line ResoW:ces to B~Id a L~icon, Lawrence Edbamn, Hillsdale, NJ.</S>
  <S sid="253" ssid="253">Smadja, F. and K. McKeown (1990), "Automatically Ext~cting and Representing Collocations for Language Generation," Proceedings of the 21tth Annual Meeting of the Association for Computational Linguistics.</S>
  <S sid="254" ssid="254">Small, S. and C. Rieger (1982), "Parring and Contprehending with Word Experts (A Theory and its Realization),* in StrategiesfgrNalural Language Processing, W, Lehnert and M, Ringle, eds., Lawrence Erlbaum Associates, Hillsdale, NJ.</S>
  <S sid="255" ssid="255">Walker, Donald (1987), "Ka]owledge R source Tools for Aeo~ssing Large Text Files," in Machine Translation: Theoretical and Methodological Issues, Serges Nirenberg, ed., Cambridge University Pm~s, Cambridge, England.</S>
  <S sid="256" ssid="256">Weiss, Stephen (1973), "Learning to Disamb/guate," Information Storage and Rari~val.</S>
  <S sid="257" ssid="257">v. 9. pp 33-4 I, Veronis, Jean and Nancy lde (1990), "Word Sense Disamliiguation wilh Very Large Neural Networks Extracted from Machine Readable Dictionaries," in Proceedings COLING-90 , pp 389-394.</S>
  <S sid="258" ssid="258">Yngve, Victor (1955), "Syntax and the Problem of Multiple Meaning," in Machine Translation of Languages.</S>
  <S sid="259" ssid="259">William Locke and Donald Booth, eds., Wiley, New York.</S>
  <S sid="260" ssid="260">Zemik, Un (1990) "Tagging Word Senses in a Corpus: The Nee.die in the Haystack Revisited," in Text-Bated Intelligenl Systems: Currem Research in Text Analysis, Information Extraction, and Retrisval, P.S.</S>
  <S sid="261" ssid="261">Jacobs, ed., GE Research &amp; Devdopmemt Center, Schetw..oady, New York.</S>
  <S sid="262" ssid="262">AcrEs DE COLING-92, NANTES.</S>
  <S sid="263" ssid="263">23-28 AO~r 1992 4 6 0 PROC.</S>
  <S sid="264" ssid="264">OF COL1NG-92, NANrFES, AUG. 23-28, 1992</S>
</PAPER>
