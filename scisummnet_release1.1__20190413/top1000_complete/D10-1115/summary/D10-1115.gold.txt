Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space
We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors.
Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training.
A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter.
We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
We find that the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out).
The adjective-specific linear map (alm) model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density.
