<PAPER>
  <S sid="0">Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors.</S>
    <S sid="2" ssid="2">Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training.</S>
    <S sid="3" ssid="3">A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter.</S>
    <S sid="4" ssid="4">We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">An influential approach for representing the meaning of a word in NLP is to treat it as a vector that codes the pattern of co-occurrence of that word with other expressions in a large corpus of language (Sahlgren, 2006; Turney and Pantel, 2010).</S>
    <S sid="6" ssid="2">This approach to semantics (sometimes called distributional semantics) naturally captures word clustering, scales well to large lexicons and doesn&#8217;t require words to be manually disambiguated (Sch&#168;utze, 1997).</S>
    <S sid="7" ssid="3">However, until recently it has been limited to the level of content words (nouns, adjectives, verbs), and it hasn&#8217;t tackled in a general way compositionality (Frege, 1892; Partee, 2004), that crucial property of natural language which allows speakers to derive the meaning of a complex linguistic constituent from the meaning of its immediate syntactic subconstituents.</S>
    <S sid="8" ssid="4">Formal semantics (FS), the research program stemming from Montague (1970b; 1973), has opposite strengths and weaknesses.</S>
    <S sid="9" ssid="5">Its core semantic notion is the sentence, not the word; at the lexical level, it focuses on the meaning of function words; one of its main goals is to formulate recursive compositional rules that derive the quantificational properties of complex sentences and their antecedent-pronoun dependencies.</S>
    <S sid="10" ssid="6">Given its focus on quantification, FS treats the meanings of nouns and verbs as pure extensions: nouns and (intransitive) verbs are properties, and thus denote sets of individuals.</S>
    <S sid="11" ssid="7">Adjectives are also often assumed to denote properties: in this view redadj would be the set of &#8216;entities which are red&#8217;, plasticadj, the set of &#8216;objects made of plastic&#8217;, and so forth.</S>
    <S sid="12" ssid="8">In the simplest case, the meaning of an attributive adjective-noun (AN) constituent can be obtained as the intersection of the adjective and noun extensions AnN: [ red car ] = {... red objects... } n {... cars... } However, the intersective method of combination is well-known to fail in many cases (Kamp, 1975; Montague, 1970a; Siegel, 1976): for instance, a fake gun is not a gun.</S>
    <S sid="13" ssid="9">Even for red, the manner in which the color combines with a noun will be different in red Ferrari (the outside), red watermelon (the inside), red traffic light (the signal).</S>
    <S sid="14" ssid="10">These problems have prompted a more flexible FS representation for attributive adjectives &#8212; functions from the meaning of a noun onto the meaning of a modified noun (Montague, 1970a).</S>
    <S sid="15" ssid="11">This mapping could now be sensitive to the particular noun the adjective receives, and it does not need to return a subset of the original noun denotation (as in the case of fake N).</S>
    <S sid="16" ssid="12">However, FS has nothing to say on how these functions should be constructed.</S>
    <S sid="17" ssid="13">In the last few years there have been attempts to build compositional models that use distributional semantic representations as inputs (see Section 2 below), most of them focusing on the combination of a verb and its arguments.</S>
    <S sid="18" ssid="14">This paper addresses instead the combination of nouns and attributive adjectives.</S>
    <S sid="19" ssid="15">This case was chosen as an interesting testbed because it has the property of recursivity (it applies in black dog, but also in large black dog, etc.</S>
    <S sid="20" ssid="16">), and because very frequent adjectives such as different are at the border between content and function words.</S>
    <S sid="21" ssid="17">Following the insight of FS, we treat attributive adjectives as functions over noun meanings; however, noun meanings are vectors, not sets, and the functions are learnt from corpus-based noun-AN vector pairs.</S>
    <S sid="22" ssid="18">Original contribution We propose and evaluate a new method to derive distributional representations for ANs, where an adjective is a linear function from a vector (the noun representation) to another vector (the AN representation).</S>
    <S sid="23" ssid="19">The linear map for a specific adjective is learnt, using linear regression, from pairs of noun and AN vectors extracted from a corpus.</S>
    <S sid="24" ssid="20">Outline Distributional approaches to compositionality are shortly reviewed in Section 2.</S>
    <S sid="25" ssid="21">In Section 3, we introduce our proposal.</S>
    <S sid="26" ssid="22">The experimental setting is described in Section 4.</S>
    <S sid="27" ssid="23">Section 5 provides some empirical justification for using corpusharvested AN vectors as the target of our function learning and evaluation benchmark.</S>
    <S sid="28" ssid="24">In Section 6, we show that our model outperforms other approaches at the task of approximating such vectors for unseen ANs.</S>
    <S sid="29" ssid="25">In Section 7, we discuss how adjectival meaning can be represented in our model and evaluate this representation in an adjective clustering task.</S>
    <S sid="30" ssid="26">Section 8 concludes by sketching directions for further work.</S>
  </SECTION>
  <SECTION title="2 Related work" number="2">
    <S sid="31" ssid="1">The literature on compositionality in vector-based semantics encompasses various related topics, some of them not of direct interest here, such as how to encode word order information in context vectors (Jones and Mewhort, 2007; Sahlgren et al., 2008) or sophisticated composition methods based on tensor products, quantum logic, etc., that have not yet been empirically tested on large-scale corpus-based semantic space tasks (Clark and Pulman, 2007; Rudolph and Giesbrecht, 2010; Smolensky, 1990; Widdows, 2008).</S>
    <S sid="32" ssid="2">Closer to our current purposes is the general framework for vector composition proposed by Mitchell and Lapata (2008), subsuming various earlier proposals.</S>
    <S sid="33" ssid="3">Given two vectors u and v, they identify two general classes of composition models, (linear) additive models: where A and B are weight matrices, and multiplicative models: where C is a weight tensor projecting the uv tensor product onto the space of p. Mitchell and Lapata derive two simplified models from these general forms.</S>
    <S sid="34" ssid="4">Their simplified additive model p = &#945;u + Qv was a common approach to composition in the earlier literature, typically with the scalar weights set to 1 or to normalizing constants (Foltz et al., 1998; Kintsch, 2001; Landauer and Dumais, 1997).</S>
    <S sid="35" ssid="5">Mitchell and Lapata also consider a constrained version of the multiplicative approach that reduces to componentwise multiplication, where the i-th component of the composed vector is given by: pi = uivi.</S>
    <S sid="36" ssid="6">The simplified additive model produces a sort of (statistical) union of features, whereas component-wise multiplication has an intersective effect.</S>
    <S sid="37" ssid="7">They also evaluate a weighted combination of the simplified additive and multiplicative functions.</S>
    <S sid="38" ssid="8">The best results on the task of paraphrasing noun-verb combinations with ambiguous verbs (sales slump is more like declining than slouching) are obtained using the multiplicative approach, and by weighted combination of addition and multiplication (we do not test model combinations in our current experiments).</S>
    <S sid="39" ssid="9">The multiplicative approach also performs best (but only by a small margin) in a later application to language modeling (Mitchell and Lapata, 2009).</S>
    <S sid="40" ssid="10">Erk and Pad&#180;o (2008; 2009) adopt the same formalism but focus on the nature of input vectors, suggesting that when a verb is composed with a noun, the noun component is given by an average of verbs that the noun is typically object of (along similar lines, Kintsch (2001) also focused on composite input vectors, within an additive framework).</S>
    <S sid="41" ssid="11">Again, the multiplicative model works best in Erk and Pad&#180;o&#8217;s experiments.</S>
    <S sid="42" ssid="12">The above-mentioned researchers do not exploit corpus evidence about the p vectors that result from composition, despite the fact that it is straightforward (at least for short constructions) to extract direct distributional evidence about the composite items from the corpus (just collect co-occurrence information for the composite item from windows around the contexts in which it occurs).</S>
    <S sid="43" ssid="13">The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the cooccurrence vectors of observed ANs to train a supervised composition model (we became aware of Guevara&#8217;s approach after we had developed our own model, that also exploits observed ANs for training).</S>
    <S sid="44" ssid="14">Guevara adopts the full additive composition form from Equation (1) and he estimates the A and B weights using partial least squares regression.</S>
    <S sid="45" ssid="15">The training data are pairs of adjective-noun vector concatenations, as input, and corpus-derived AN vectors, as output.</S>
    <S sid="46" ssid="16">Guevara compares his model to the simplified additive and multiplicative models of Mitchell and Lapata.</S>
    <S sid="47" ssid="17">Observed ANs are nearer, in the space of observed and predicted test set ANs, to the ANs generated by his model than to those from the alternative approaches.</S>
    <S sid="48" ssid="18">The additive model, on the other hand, is best in terms of shared neighbor count between observed and predicted ANs.</S>
    <S sid="49" ssid="19">In our empirical tests, we compare our approach to the simplified additive and multiplicative models of Mitchell and Lapata (the former with normalization constants as scalar weights) as well as to Guevara&#8217;s approach.</S>
  </SECTION>
  <SECTION title="3 Adjectives as linear maps" number="3">
    <S sid="50" ssid="1">As discussed in the introduction, we will take adjectives in attributive position to be functions from one noun meaning to another.</S>
    <S sid="51" ssid="2">To start simple, we assume here that adjectives in the attributive position (AN) are linear functions from n-dimensional (noun) vectors onto n-dimensional vectors, an operation that can be expressed as multiplication of the input noun column vector by a n x n matrix, that is our representation for the adjective (in the language of linear algebra, an adjective is an endomorphic linear map in noun space).</S>
    <S sid="52" ssid="3">In the framework of Mitchell and Lapata, our approach derives from the additive form in Equation (1) with the matrix multiplying the adjective vector (say, A) set to 0: p=Bv where p is the observed AN vector, B the weight matrix representing the adjective at hand, and v a noun vector.</S>
    <S sid="53" ssid="4">In our approach, the weight matrix B is specific to a single adjective &#8211; as we will see in Section 7 below, it is our representation of the meaning of the adjective.</S>
    <S sid="54" ssid="5">Like Guevara, we estimate the values in the weight matrix by partial least squares regression.</S>
    <S sid="55" ssid="6">In our case, the independent variables for the regression equations are the dimensions of the corpusbased vectors of the component nouns, whereas the AN vectors provide the dependent variables.</S>
    <S sid="56" ssid="7">Unlike Guevara, (i) we train separate models for each adjective (we learn adjective-specific functions, whereas Guevara learns a generic &#8220;AN-slot&#8221; function) and, consequently, (ii) corpus-harvested adjective vectors play no role for us (their values would be constant across the training input vectors).</S>
    <S sid="57" ssid="8">A few considerations are in order.</S>
    <S sid="58" ssid="9">First, although we use a supervised learning method (least squares regression), we do not need hand-annotated data, since the target AN vectors are automatically collected from the corpus just like vectors for single words are.</S>
    <S sid="59" ssid="10">Thus, there is no extra &#8220;external knowledge&#8221; cost with respect to unsupervised approaches.</S>
    <S sid="60" ssid="11">Second, our approach rests on the assumption that the corpus-derived AN vectors are interesting objects that should constitute the target of what a composition process tries to approximate.</S>
    <S sid="61" ssid="12">We provide preliminary empirical support for this assumption in Section 5 below.</S>
    <S sid="62" ssid="13">Third, we have some reasonable hope that our functions can capture to a certain extent the polysemous nature of adjectives: we could learn, for example, a green matrix with large positive weights mapping from noun features that pertain to concrete objects to color dimensions of the output vector (green chair), as well as large positive weights from features characterizing certain classes of abstract concepts to political/social dimensions in the output (green initiative).</S>
    <S sid="63" ssid="14">Somewhat optimistically, we hope that chair will have near-0 values on the relevant abstract dimensions, like initiative on the concrete features, and thus the weights will not interfere.</S>
    <S sid="64" ssid="15">We do not evaluate this claim specifically, but our quantitative evaluation in Section 6 shows that our approach does best with high frequency, highly ambiguous adjectives.</S>
    <S sid="65" ssid="16">Fourth, the approach is naturally syntax-sensitive, since we train it on observed data for a specific syntactic position: we would train separate linear models for, say, the same adjective in attributive (AN) and predicative (N is A) position.</S>
    <S sid="66" ssid="17">As a matter of fact, the current model is too syntax-sensitive and does not capture similarities across different constructions.</S>
    <S sid="67" ssid="18">Finally, although adjective representations are not directly harvested from corpora, we can still meaningfully compare adjectives to each other or other words by using their estimated matrix, or an average vector for the ANs that contain them: both options are tested in Section 7 below.</S>
  </SECTION>
  <SECTION title="4 Experimental setup" number="4">
    <S sid="68" ssid="1">We built a large corpus by concatenating the Web-derived ukWaC corpus (http://wacky. sslmit.unibo.it/), a mid-2009 dump of the English Wikipedia (http://en.wikipedia. org) and the British National Corpus (http: //www.natcorp.ox.ac.uk/).</S>
    <S sid="69" ssid="2">This concatenated corpus, tokenized, POS-tagged and lemmatized with the TreeTagger (Schmid, 1995), contains about 2.83 billion tokens (excluding punctuation, digits, etc.).</S>
    <S sid="70" ssid="3">The ukWaC and Wikipedia sections can be freely downloaded, with full annotation, from the ukWaC site.</S>
    <S sid="71" ssid="4">We performed some of the list extraction and checking operations we are about to describe on a more manageable data-set obtained by selecting the first 100M tokens of ukWaC; we refer to this subset as the sample corpus below.</S>
    <S sid="72" ssid="5">We could in principle limit ourselves to collecting vectors for the ANs to be analyzed (the AN test set) and their components.</S>
    <S sid="73" ssid="6">However, to make the analysis more challenging and interesting, we populate the semantic space where we will look at the behaviour of the ANs with a large number of adjectives and nouns, as well as further ANs not in the test set.</S>
    <S sid="74" ssid="7">We refer to the overall list of items we build semantic vectors for as the extended vocabulary.</S>
    <S sid="75" ssid="8">We use a subset of the extended vocabulary containing only nouns and adjectives (the core vocabulary) for feature selection and dimensionality reduction, so that we do not implicitly bias the structure of the semantic space by our choice of ANs.</S>
    <S sid="76" ssid="9">To construct the AN test set, we first selected 36 adjectives across various classes: size (big, great, huge, large, major, small, little), denominal (American, European, national, mental, historical, electronic), colors (white, black, red, green) positive evaluation (nice, excellent, important, appropriate), temporal (old, recent, new, young, current), modal (necessary, possible), plus some common abstract antonymous pairs (difficult, easy, good, bad, special, general, different, common).</S>
    <S sid="77" ssid="10">We were careful to include intersective cases such as electronic as well as non-intersective adjectives that are almost function words (the modals, different, etc.).</S>
    <S sid="78" ssid="11">We extracted all nouns that occurred at least 300 times in post-adjectival position in the sample corpus, excluding some extremely frequent temporal and measure expressions such as time and range, for a total of 1,420 distinct nouns.</S>
    <S sid="79" ssid="12">By crossing the selected adjectives and nouns, we constructed a test set containing 26,440 ANs, all attested in the sample corpus (734 ANs per adjective on average, ranging from 1,337 for new to 202 for mental).</S>
    <S sid="80" ssid="13">The core vocabulary contains the top 8K most frequent noun lemmas and top 4K adjective lemmas from the concatenated corpus (excluding the top 50 most frequent nouns and adjectives).</S>
    <S sid="81" ssid="14">The extended vocabulary contains this core plus (i) the 26,440 test ANs, (ii) the 16 adjectives and 43 nouns that are components of these ANs and that are not in the core set, and (iii) 2,500 more ANs randomly sampled from those that are attested in the sample corpus, have a noun from the same list used for the test set ANs, and an adjective that occurred at least 5K times in the sample corpus.</S>
    <S sid="82" ssid="15">In total, the extended vocabulary contains 40,999 entries: 8,043 nouns, 4,016 adjectives and 28,940 ANs.</S>
    <S sid="83" ssid="16">Full co-occurrence matrix The 10K lemmas (nouns, adjectives or verbs) that co-occur with the largest number of items in the core vocabulary constitute the dimensions (columns) of our cooccurrence matrix.</S>
    <S sid="84" ssid="17">Using the concatenated corpus, we extract sentence-internal co-occurrence counts of all the items in the extended vocabulary with the 10K dimension words.</S>
    <S sid="85" ssid="18">We then transform the raw counts into Local Mutual Information (LMI) scores (LMI is an association measure that closely approximates the Log-Likelihood Ratio, see Evert (2005)).</S>
    <S sid="86" ssid="19">Dimensionality reduction Since, for each test set adjective, we need to estimate a regression model for each dimension, we want a compact space with relatively few, dense dimensions.</S>
    <S sid="87" ssid="20">A natural way to do this is to apply the Singular Value Decomposition (SVD) to the co-occurrence matrix, and represent the items of interest with their coordinates in the space spanned by the first n right singular vectors.</S>
    <S sid="88" ssid="21">Applying SVD is independently justified because, besides mitigating the dimensionality problem, it often improves the quality of the semantic space (Landauer and Dumais, 1997; Rapp, 2003; Sch&#168;utze, 1997).</S>
    <S sid="89" ssid="22">To avoid bias in favour of dimensions that capture variance in the test set ANs, we applied SVD to the core vocabulary subset of the co-occurrence matrix (containing only adjective and noun rows).</S>
    <S sid="90" ssid="23">The core 12K 10K matrix was reduced using SVD to a 12K&#215;300 matrix.</S>
    <S sid="91" ssid="24">The other row vectors of the full co-occurrence matrix (including the ANs) were projected onto the same reduced space by multiplying them by a matrix containing the first n right singular vectors as columns.</S>
    <S sid="92" ssid="25">Merging the items used to compute the SVD and those projected onto the resulting space, we obtain a 40,999&#215;300 matrix representing 8,043 nouns, 4,016 adjectives and 28,940 ANs.</S>
    <S sid="93" ssid="26">This reduced matrix constitutes a realistically sized semantic space, that also contains many items that are not part of our test set, but will be potential neighbors of the observed and predicted test ANs in the experiments to follow.</S>
    <S sid="94" ssid="27">The quality of the SVD reduction itself was independently validated on a standard similarity judgment data-set (Rubenstein and Goodenough, 1965), obtaining similar (and state-of-the-art-range) Pearson correlations of vector cosines and human judgments in both the original (r = .70) and reduced (r = .72) spaces.</S>
    <S sid="95" ssid="28">There are several parameters involved in constructing a semantic space (choice of full and reduced dimensions, co-occurrence span, weighting method).</S>
    <S sid="96" ssid="29">Since our current focus is on alternative composition methods evaluated on a shared semantic space, exploring parameters pertaining to the construction of the semantic space is not one of our priorities, although we cannot of course exclude that the nature of the underlying semantic space affects different composition methods differently.</S>
    <S sid="97" ssid="30">In the proposed adjective-specific linear map (alm) method, an AN is generated by multiplying an adjective weight matrix with a noun (column) vector.</S>
    <S sid="98" ssid="31">The j weights in the i-th row of the matrix are the coefficients of a linear regression predicting the values of the i-th dimension of the AN vector as a linear combination of the j dimensions of the component noun.</S>
    <S sid="99" ssid="32">The linear regression coefficients are estimated separately for each of the 36 tested adjectives from the corpus-observed noun-AN pairs containing that adjective (observed adjective vectors are not used).</S>
    <S sid="100" ssid="33">Since we are working in the 300-dimensional right singular vector space, for each adjective we have 300 regression problems with 300 independent variables, and the training data (the noun-AN pairs available for each test set adjective) range from about 200 to more than 1K items.</S>
    <S sid="101" ssid="34">We estimate the coefficients using (multivariate) partial least squares regression (PLSR) as implemented in the R pls package (Mevik and Wehrens, 2007).</S>
    <S sid="102" ssid="35">With respect to standard least squares estimation, this technique is more robust against over-training by effectively using a smaller number of orthogonal &#8220;latent&#8221; variables as predictors (Hastie et al., 2009, Section 3.4), and it exploits the multivariate nature of the problem (different regressions for each AN vector dimension to be predicted) when determining the latent dimensions.</S>
    <S sid="103" ssid="36">The number of latent variables to be used in the core regression are a free parameter of PLSR.</S>
    <S sid="104" ssid="37">For efficiency reasons, we did not optimize it.</S>
    <S sid="105" ssid="38">We picked instead 50 latent variables, by the ruleof-thumb reasoning that for any adjective we can use at least 200 noun-AN pairs for training, and the independent-variable-to-training-item ratio will thus never be above 1/4.</S>
    <S sid="106" ssid="39">We adopt a leave-one-out training regime, so that each target AN is generated by an adjective matrix that was estimated from all the other ANs with the same adjective, minus the target.</S>
    <S sid="107" ssid="40">We use PLSR with 50 latent variables also for our re-implementation of Guevara&#8217;s (2010) single linear map (slm) approach, in which a single regression matrix is estimated for all ANs across adjectives.</S>
    <S sid="108" ssid="41">The training data in this case are given by the concatenation of the observed adjective and noun vectors (600 independent variables) coupled with the corresponding AN vectors (300 dependent variables).</S>
    <S sid="109" ssid="42">For each target AN, we randomly sample 2,000 other adjective-noun-AN tuples for training (with larger training sets we run into memory problems), and use the resulting coefficient matrix to generate the AN vector from the concatenated target adjective and noun vectors.</S>
    <S sid="110" ssid="43">Additive AN vectors (add method) are obtained by summing the corresponding adjective and noun vectors after normalizing them (non-normalized addition was also tried, but it did not work nearly as well as the normalized variant).</S>
    <S sid="111" ssid="44">Multiplicative vectors (mult method) were obtained by componentwise multiplication of the adjective and noun vectors (normalization does not matter here since it amounts to multiplying the composite vector by a scalar, and the cosine similarity measure we use is scale-invariant).</S>
    <S sid="112" ssid="45">Finally, the adj and noun baselines use the adjective and noun vectors, respectively, as surrogates of the AN vector.</S>
    <S sid="113" ssid="46">For the add, mult, adj and noun methods, we ran the tests of Section 6 not only in the SVD-reduced space, but also in the original 10K-dimensional cooccurrence space.</S>
    <S sid="114" ssid="47">Only the mult method achieved better performance in the original space.</S>
    <S sid="115" ssid="48">We conjecture that this is because the SVD dimensions can have negative values, leading to counter-intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values).</S>
    <S sid="116" ssid="49">We tried to alleviate this problem by assigning a 0 to composite dimensions where the two input vectors had different signs.</S>
    <S sid="117" ssid="50">The resulting performance was better but still below that of mult in original space.</S>
    <S sid="118" ssid="51">Thus, in Section 6 we report mult results from the full co-occurrence matrix; reduced space results for all other methods.</S>
    <S sid="119" ssid="52">5 Study 1: ANs in semantic space The actual distribution of ANs in the corpus, as recorded by their co-occurrence vectors, is fundamental to what we are doing.</S>
    <S sid="120" ssid="53">Our method relies on the hypothesis that the semantics of AN composition does not depend on the independent distribution of adjectives themselves, but on how adjectives transform the distribution of nouns, as evidenced by observed pairs of noun-AN vectors.</S>
    <S sid="121" ssid="54">Moreover, coherently with this view, our evaluation below will be based on how closely the models approximate the observed vectors of unseen ANs.</S>
    <S sid="122" ssid="55">That our goal in modeling composition should be to approximate the vectors of observed ANs is in a sense almost trivial.</S>
    <S sid="123" ssid="56">Whether we synthesize an AN for generation or decoding purposes, we would want the synthetic AN to look as much as possible like a real AN in its natural usage contexts, and cooccurrence vectors of observed ANs are a summary of their usage in actual linguistic contexts.</S>
    <S sid="124" ssid="57">However, it might be the case that the specific resources we used for our vector construction procedure are not appropriate, so that the specific observed AN vectors we extract are not reliable (e.g., they are so sparse in the original space as to be uninformative, or they are strictly tied to the domains of the input corpora).</S>
    <S sid="125" ssid="58">We provide here some preliminary qualitative evidence that this is in general not the case, by tapping into our own intuitions on where ANs should be located in semantic space, and thus on how sensible their neighbors are.</S>
    <S sid="126" ssid="59">First, we computed centroids from normalized SVD space vectors of all the ANs that share the same adjective (e.g., the normalized vectors of American adult, American menu, etc., summed to construct the American N centroid).</S>
    <S sid="127" ssid="60">We looked at the nearest neighbors of these centroids in semantic space among the 41K items (adjectives, nouns and ANs) in our extended vocabulary (here and in all experiments below, similarity is quantified by the cosine of the angle between two vectors).</S>
    <S sid="128" ssid="61">As illustrated for a random sample of 9 centroids in Table 1 (but applying to the remaining 27 adjectives as well), centroids are positioned in intuitively reasonable areas of the space, typically near the adjective itself or the corresponding noun (the noun green near green N), prototypical ANs for that adjective (black face), elements related to the definition of the adjective (mental activity, historical event, green colour, quick and little cost for easy N), and so on.</S>
    <S sid="129" ssid="62">American N black N easy N Am. representative black face easy start Am. territory black hand quick Am. source black (n) little cost green N historical N mental N green (n) historical mental activity red road hist. event mental experience green colour hist. content mental energy necessary N nice N young N necessary nice youthful necessary degree good bit young doctor sufficient nice break young staff How about the neighbors of specific ANs?</S>
    <S sid="130" ssid="63">Table 2 reports the nearest 3 neighbors of 9 randomly selected ANs involving different adjectives (we inspected a larger random set, coming to similar conclusions to the ones emerging from this table). bad electronic historical luck communication map bad elec. storage topographical bad weekend elec. transmission atlas good spirit purpose hist. material important route nice girl little war important transport good girl great war important road big girl major war major road guy small war red cover special collection young husband black cover general collection small son hardback small collection small daughter red label archives mistress The nearest neighbors of the corpus-based AN vectors in Table 2 make in general intuitive sense.</S>
    <S sid="131" ssid="64">Importantly, the neighbors pick up the composite meaning rather than that of the adjective or noun alone.</S>
    <S sid="132" ssid="65">For example, cover is an ambiguous word, but the hardback neighbor relates to its &#8220;front of a book&#8221; meaning that is the most natural one in combination with red.</S>
    <S sid="133" ssid="66">Similarly, it makes more sense that a young husband (rather than an old one) would have small sons and daughters (not to mention the mistress!).</S>
    <S sid="134" ssid="67">We realize that the evidence presented here is of a very preliminary and intuitive nature.</S>
    <S sid="135" ssid="68">Indeed, we will argue in the next section that there are cases in which the corpus-derived AN vector might not be a good approximation to our semantic intuitions about the AN, and a model-composed AN vector is a better semantic surrogate.</S>
    <S sid="136" ssid="69">One of the most important avenues for further work will be to come to a better characterization of the behaviour of corpus-observed ANs, where they work and where the don&#8217;t.</S>
    <S sid="137" ssid="70">Still, the neighbors of average and ANspecific vectors of Tables 1 and 2 suggest that, for the bulk of ANs, such corpus-based co-occurrence vectors are semantically reasonable.</S>
  </SECTION>
  <SECTION title="6 Study 2: Predicting AN vectors" number="5">
    <S sid="138" ssid="1">Having tentatively established that the sort of vectors we can harvest for ANs by directly collecting their corpus co-occurrences are reasonable representations of their composite meaning, we move on to the core question of whether it is possible to reconstruct the vector for an unobserved AN from information about its components.</S>
    <S sid="139" ssid="2">We use nearness to the corpus-observed vectors of held-out ANs as a very direct way to evaluate the quality of modelgenerated ANs, since we just saw that the observed ANs look reasonable (but see the caveats at the end of this section).</S>
    <S sid="140" ssid="3">We leave it to further work to assess the quality of the generated ANs in an applied setting, for example adapting Mitchell and Lapata&#8217;s paraphrasing task to ANs.</S>
    <S sid="141" ssid="4">Since the observed vectors look like plausible representations of composite meaning, we expect that the closer the modelgenerated vectors are to the observed ones, the better they should also perform in any task that requires access to the composite meaning, and thus that the results of the current evaluation should correlate with applied performance.</S>
    <S sid="142" ssid="5">More in detail, we evaluate here the composition methods (and the adjective and noun baselines) by computing, for each of them, the cosine of the test set AN vectors they generate (the &#8220;predicted&#8221; ANs) with the 41K vectors representing our extended vocabulary in semantic space, and looking at the position of the corresponding observed ANs (that were not used for training, in the supervised approaches) in the cosine-ranked lists.</S>
    <S sid="143" ssid="6">The lower the rank, the better the approximation.</S>
    <S sid="144" ssid="7">For efficiency reasons, we flatten out the ranks after the top 1,000 neighbors.</S>
    <S sid="145" ssid="8">The results are summarized in Table 3 by the median and the other quartiles, calculated across all 26,440 ANs in the test set.</S>
    <S sid="146" ssid="9">These measures (unlike mean and variance) are not affected by the cut-off after 1K neighbors.</S>
    <S sid="147" ssid="10">To put the reported results into perspective, a model with a first quartile rank of 999 does very significantly better than chance (the binomial probability of 1/4 or more of 26,440 trials being successful with 7r = 0.024 is virtually 0, where the latter quantity is the probability of an observed AN being at rank 999 or lower according to a geometric distribution with 7r=1/40999).</S>
    <S sid="148" ssid="11">Our proposed method, alm, emerges as the best approach.</S>
    <S sid="149" ssid="12">The difference with the second best model, add (the only other model that does better than the non-trivial baseline of using the component noun vector as a surrogate for AN), is highly statistically significant (Wilcoxon signed rank test, p &lt; 0.00001).</S>
    <S sid="150" ssid="13">If we randomly downsample the AN set to keep an equal number of ANs per adjective (200), the difference is still significant with p below the same threshold, indicating that the general result is not due to a better performance of alm on a few common adjectives.1 Among the alternative models, the fact that the performance of add is decidedly better than that of mult is remarkable, since earlier studies found that 1The semantic space in which we rank the observed ANs with respect to their predicted counterparts also contain the observed vectors of nouns and ANs that were used to train alm.</S>
    <S sid="151" ssid="14">We do not see how this should affect performance, but we nevertheless repeated the evaluation leaving out, for each AN, the observed items used in training, and we obtained the same results reported in the main text (same ordering of method performance, and very significant difference between alm and add). multiplicative models are, in general, better than additive ones in compositionality tasks (see Section 2 above).</S>
    <S sid="152" ssid="15">This might depend on the nature of AN composition, but there are also more technical issues at hand: (i) we are not sure that previous studies normalized before summing like we did, and (ii) the multiplicative model, as discussed in Section 4, does not benefit from SVD reduction.</S>
    <S sid="153" ssid="16">The single linear mapping model (slm) proposed by Guevara (2010) is doing even worse than the multiplicative method, suggesting that a single set of weights does not provide enough flexibility to model a variety of adjective transformations successfully.</S>
    <S sid="154" ssid="17">This is at odds with Guevara&#8217;s experiment in which slm outperformed mult and add on the task of ranking predicted ANs with respect to a target observed AN.</S>
    <S sid="155" ssid="18">Besides various differences in task definition and model implementation, Guevara trained his model on ANs that include a wide variety of adjectives, whereas our training data were limited to ANs containing one of our 36 test set adjectives.</S>
    <S sid="156" ssid="19">Future work should re-evalute the performance of Guevara&#8217;s approach in our task, but under his training regime.</S>
    <S sid="157" ssid="20">Looking now at the alm results in more detail, the best median ranks are obtained for very frequent adjectives.</S>
    <S sid="158" ssid="21">The top ones are new (median rank: 34), great (79), American (82), large (82) and different (97).</S>
    <S sid="159" ssid="22">There is a high inverse correlation between median rank and adjective frequency (Spearman&#8217;s p = &#8722;0.56).</S>
    <S sid="160" ssid="23">Although from a statistical perspective it is expected that we get better results where we have more data, from a linguistic point of view it is interesting that alm works best with extremely frequent, highly polysemous adjectives like new, large and different, that border on function words &#8211; a domain where distributional semantics has generally not been tested.</S>
    <S sid="161" ssid="24">Although, in relative terms and considering the difficulty of the task, alm performs well, it is still far from perfect &#8211; for 27% alm-predicted ANs, the observed vector is not even in the top 1K neighbor set!</S>
    <S sid="162" ssid="25">A qualitative look at some of the most problematic examples indicates however that a good proportion of them might actually not be instances where our model got the AN vector wrong, but cases of anomalous observed ANs.</S>
    <S sid="163" ssid="26">The left side of Table 4 compares the nearest neighbors (excluding each other) of the observed and alm-predicted vectors in 10 ranSIMILAR DISSIMILAR adj N obs. neighbor pred. neighbor adj N obs. neighbor pred. neighbor common understanding common approach common vision American affair Am. development Am. policy different authority diff. objective diff. description current dimension left (a) current element different partner diff. organisation diff. department good complaint current complaint good beginning general question general issue same great field excellent field gr. distribution historical introduction hist. background same historical thing different today hist. reality necessary qualification nec. experience same important summer summer big holiday new actor new cast same large pass historical region large dimension recent request recent enquiry same special something little animal special thing small drop droplet drop white profile chrome (n) white show young engineer young designer y. engineering young photo important song young image where rank of observed w.r.t. predicted is 1.</S>
    <S sid="164" ssid="27">Right: nearest neighbors of predicted and observed ANs for random set where rank of observed w.r.t. predicted is &gt; 1K. domly selected cases where the observed AN is the nearest neighbor of the predicted one.</S>
    <S sid="165" ssid="28">Here, the ANs themselves make sense, and the (often shared) neighbors are also sensible (recent enquiry for recent request, common approach and common vision for common understanding, etc.).</S>
    <S sid="166" ssid="29">Moving to the right, we see 10 random examples of ANs where the observed AN was at least 999 neighbors apart from the alm prediction.</S>
    <S sid="167" ssid="30">First, we notice some ANs that are difficult to interpret out-of-context (important summer, white profile, young photo, large pass, ... ).</S>
    <S sid="168" ssid="31">Second, at least subjectively, we find that in many cases the nearest neighbor of predicted AN is actually more sensible than that of observed AN: current element (vs. left) for current dimension, historical reality (vs. different today) for historical thing, special thing (vs. little animal) for special something, young image (vs. important song) for young photo.</S>
    <S sid="169" ssid="32">In the other cases, the predicted AN neighbor is at least not obviously worse than the observed AN neighbor.</S>
    <S sid="170" ssid="33">There is a high inverse correlation between the frequency of occurrence of an AN and the rank of the observed AN with respect to the predicted one (p =&#8722;0.48), suggesting that our model is worse at approximating the observed vectors of rare forms, that might, in turn, be those for which the corpusbased representation is less reliable.</S>
    <S sid="171" ssid="34">In these cases, dissimilarities between observed and expected vectors, rather than signaling problems with the model, might indicate that the predicted vector, based on a composition function learned from many examples, is better than the one directly extracted from the corpus.</S>
    <S sid="172" ssid="35">The examples in the right panel of Table 4 bring some preliminary support to this hypothesis, to be systematically explored in future work.</S>
  </SECTION>
  <SECTION title="7 Study 3: Comparing adjectives" number="6">
    <S sid="173" ssid="1">If adjectives are functions, and not corpus-derived vectors, is it still possible to compare them meaningfully?</S>
    <S sid="174" ssid="2">We explore two ways to accomplish this in our framework: one is to represent adjectives by the average of the AN vectors that contain them (the centroid vectors whose neighbors are illustrated in Table 1 above), and the other to compare them based on the 300&#215;300 weight matrices we estimate from noun-AN pairs (we unfold these matrices into 90K-dimensional vectors).</S>
    <S sid="175" ssid="3">We compare the quality of these representations to that of the standard approach in distributional semantics, i.e., representing the adjectives directly with their corpus co-occurrence profile vectors (in our case, projected onto the SVD-reduced space).</S>
    <S sid="176" ssid="4">We evaluate performance on the task of clustering those 19 adjectives in our set that can be relatively straightforwardly categorized into general classes comprising a minimum of 4 items.</S>
    <S sid="177" ssid="5">The test set built according to these criteria contains 4 classes: color (white, black, red, green), positive evaluation (nice, excellent, important, major, appropriate), time (recent, new, current, old, young), and size (big, huge, little, small, large).</S>
    <S sid="178" ssid="6">We cluster with the CLUTO toolkit (Karypis, 2003), using the repeated bisections with global optimization method, accepting all of CLUTO&#8217;s default values for this choice.</S>
    <S sid="179" ssid="7">Cluster quality is evaluated by percentage purity (Zhao and Karypis, 2003).</S>
    <S sid="180" ssid="8">If nir is the number of items from the i-th true (gold standard) class assigned to the r-th cluster, n is the total number of items and k the number of clusters, then: Purity = n &#65533;r&#8212;1rmax(nir).</S>
    <S sid="181" ssid="9">We calculate i empirical 95% confidence intervals around purity by a heuristic bootstrap procedure based on 10K resamplings of the data set (Efron and Tibshirani, 1994).</S>
    <S sid="182" ssid="10">The random baseline distribution is obtained by 10K random assignments of adjectives to the clusters, under the constraint that no cluster is empty.</S>
    <S sid="183" ssid="11">Table 5 shows that all methods are significantly better than chance.</S>
    <S sid="184" ssid="12">Our two &#8220;indirect&#8221; representations achieve similar performance, and they are (slightly) better than the traditional method based on adjective co-occurrence vectors.</S>
    <S sid="185" ssid="13">We conclude that, although our approach does not provide a direct encoding of adjective meaning in terms of such independently collected vectors, it does have meaningful ways to represent their semantic properties.</S>
  </SECTION>
  <SECTION title="8 Conclusion" number="7">
    <S sid="186" ssid="1">The work we reported constitutes an encouraging start for our approach to modeling (AN) composition.</S>
    <S sid="187" ssid="2">We suggested, along the way, various directions for further studies.</S>
    <S sid="188" ssid="3">We consider the following issues to be the most pressing ones.</S>
    <S sid="189" ssid="4">We currently train each adjective-specific model separately: We should explore hierarchical modeling approaches that exploit similarities across adjectives (and possibly syntactic constructions) to estimate better models.</S>
    <S sid="190" ssid="5">Evaluation-wise, the differences between observed and predicted ANs must be analyzed more extensively, to support the claim that, when their vectors differ, model-based prediction improves on the observed vector.</S>
    <S sid="191" ssid="6">Evaluation in a more applied task should also be pursued &#8211; in particular, we will design a paraphrasing task similar to the one proposed by Mitchell and Lapata to evaluate noun-verb constructions.</S>
    <S sid="192" ssid="7">Since we do not collect vectors for the &#8220;functor&#8221; component of a composition process (for AN constructions, the adjective), our approach naturally extends to processes that involve bound morphemes, such as affixation, where we would not need to collect independent co-occurrence information for the affixes.</S>
    <S sid="193" ssid="8">For example, to account for re- prefixation we do not need to collect a re- vector (required by all other approaches to composition), but simply vectors for a set of V/reV pairs, where both members of the pairs are words (e.g., consider/reconsider).</S>
    <S sid="194" ssid="9">Our approach can also deal, out-of-the-box, with recursive constructions (sad little red hat), and can be easily extended to more abstract constructions, such as determiner N (mapping dog to the/a/one dog).</S>
    <S sid="195" ssid="10">Still, we need to design a good testing scenario to evaluate the quality of such model-generated constructions.</S>
    <S sid="196" ssid="11">Ultimately, we want to compose larger and larger constituents, up to full sentences.</S>
    <S sid="197" ssid="12">It remains to be seen if the approach we proposed will scale up to such challenges.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="198" ssid="1">We thank Gemma Boleda, Emilano Guevara, Alessandro Lenci, Louise McNally and the anonymous reviewers for useful information, advice and comments.</S>
  </SECTION>
</PAPER>
