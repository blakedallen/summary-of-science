Re-Evaluation The Role Of Bleu In Machine Translation Research
We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric.
We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu's correlation with human judgments of quality.
This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
The problems of Blue include: (1) synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; (2) The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; (3) The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall.
Blue has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems.
We find that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation.
