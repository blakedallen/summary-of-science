The Web As A Baseline: Evaluating The Performance Of Unsupervised Web-Based Models For A Range Of NLP Tasks
Previous work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks.
So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets.
The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams.
For the majority of tasks, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus.
However, in most cases, web-based models fail to outperform more sophisticated state-of-the-art models trained on small corpora.
We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models.
Our web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as the training corpus.
