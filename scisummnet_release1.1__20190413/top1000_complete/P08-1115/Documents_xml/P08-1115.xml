<PAPER>
  <S sid="0">Generalizing Word Lattice Translation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well.</S>
    <S sid="2" ssid="2">We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models.</S>
    <S sid="3" ssid="3">Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models.</S>
    <S sid="4" ssid="4">Our experiments evaluating the approach demonstrate substantial gains for Chinese- English and Arabic-English translation.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">When Brown and colleagues introduced statistical machine translation in the early 1990s, their key insight &#8211; harkening back to Weaver in the late 1940s &#8211; was that translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990).</S>
    <S sid="6" ssid="2">They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models).</S>
    <S sid="7" ssid="3">Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007).</S>
    <S sid="8" ssid="4">Why, however, should this advantage be limited to translation from spoken input?</S>
    <S sid="9" ssid="5">Even for text, there are often multiple ways to derive a sequence of words from the input string.</S>
    <S sid="10" ssid="6">Segmentation of Chinese, decompounding in German, morphological analysis for Arabic &#8212; across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence.</S>
    <S sid="11" ssid="7">Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1).</S>
    <S sid="12" ssid="8">In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input.</S>
    <S sid="13" ssid="9">In addition, we generalize lattice decoding algorithmically, extending it for the first time to hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007).</S>
    <S sid="14" ssid="10">Formally, the approach we take can be thought of as a &#8220;noisier channel&#8221;, where an observed signal o gives rise to a set of source-language strings f' E F(o) and we seek An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f. There, Bertoldi and others have recently found that, rather than translating a single-best transcription f, it is advantageous to allow the MT decoder to = arg max max Pr(e)Pr(f'|e)Pr(o|f')&#65533;(4) e f&#65533;EF(o) Following Och and Ney (2002), we use the maximum entropy framework (Berger et al., 1996) to directly model the posterior Pr(e, f'|o) with parameters tuned to minimize a loss function representing the quality only of the resulting translations.</S>
    <S sid="15" ssid="11">Thus, we make use of the following general decision rule: In principle, one could decode according to (2) simply by enumerating and decoding each f&#65533; &#8712; F(o); however, for any interestingly large F(o) this will be impractical.</S>
    <S sid="16" ssid="12">We assume that for many interesting cases of F(o), there will be identical substrings that express the same content, and therefore a lattice representation is appropriate.</S>
    <S sid="17" ssid="13">In Section 2, we discuss decoding with this model in general, and then show how two classes of translation models can easily be adapted for lattice translation; we achieve a unified treatment of finite-state and hierarchical phrase-based models by treating lattices as a subcase of weighted finite state automata (FSAs).</S>
    <S sid="18" ssid="14">In Section 3, we identify and solve issues that arise with reordering in non-linear FSAs, i.e.</S>
    <S sid="19" ssid="15">FSAs where every path does not pass through every node.</S>
    <S sid="20" ssid="16">Section 4 presents two applications of the noisier channel paradigm, demonstrating substantial performance gains in Arabic-English and Chinese-English translation.</S>
    <S sid="21" ssid="17">In Section 5 we discuss relevant prior work, and we conclude in Section 6.</S>
  </SECTION>
  <SECTION title="2 Decoding" number="2">
    <S sid="22" ssid="1">Most statistical machine translation systems model translational equivalence using either finite state transducers or synchronous context free grammars (Lopez, to appear 2008).</S>
    <S sid="23" ssid="2">In this section we discuss the issues associated with adapting decoders from both classes of formalism to process word lattices.</S>
    <S sid="24" ssid="3">The first decoder we present is a SCFG-based decoder similar to the one described in Chiang (2007).</S>
    <S sid="25" ssid="4">The second is a phrase-based decoder implementing the model of Koehn et al. (2003).</S>
    <S sid="26" ssid="5">A word lattice G = hV, Ei is a directed acyclic graph that formally is a weighted finite state automaton (FSA).</S>
    <S sid="27" ssid="6">We further stipulate that exactly one node has no outgoing edges and is designated the &#8216;end node&#8217;.</S>
    <S sid="28" ssid="7">Figure 1 illustrates three classes of word lattices.</S>
    <S sid="29" ssid="8">A word lattice is useful for our purposes because it permits any finite set of strings to be represented and allows for substrings common to multiple members of the set to be represented with a single piece of structure.</S>
    <S sid="30" ssid="9">Additionally, all paths from one node to another form an equivalence class representing, in our model, alternative expressions of the same underlying communicative intent.</S>
    <S sid="31" ssid="10">For translation, we will find it useful to encode G in a chart based on a topological ordering of the nodes, as described by Cheppalier et al. (1999).</S>
    <S sid="32" ssid="11">The nodes in the lattices shown in Figure 1 are labeled according to an appropriate numbering.</S>
    <S sid="33" ssid="12">The chart-representation of the graph is a triple of 2-dimensional matrices hF, p, Ri, which can be constructed from the numbered graph.</S>
    <S sid="34" ssid="13">Fi,j is the word label of the jth transition leaving node i.</S>
    <S sid="35" ssid="14">The corresponding transition cost is pi,j.</S>
    <S sid="36" ssid="15">Ri,j is the node number of the node on the right side of the jth transition leaving node i.</S>
    <S sid="37" ssid="16">Note that Ri,j &gt; i for all i, j.</S>
    <S sid="38" ssid="17">Table 1 shows the word lattice from Figure 1 represented in matrix form as hF, p, Ri.</S>
    <S sid="39" ssid="18">Chiang (2005) introduced hierarchical phrase-based translation models, which are formally based on synchronous context-free grammars (SCFGs).</S>
    <S sid="40" ssid="19">Translation proceeds by parsing the input using the source language side of the grammar, simultaneously building a tree on the target language side via the target side of the synchronized rules.</S>
    <S sid="41" ssid="20">Since decoding is equivalent to parsing, we begin by presenting a parser for word lattices, which is a generalization of a CKY parser for lattices given in Cheppalier et al. (1999).</S>
    <S sid="42" ssid="21">Following Goodman (1999), we present our lattice parser as a deductive proof system in Figure 2.</S>
    <S sid="43" ssid="22">The parser consists of two kinds of items, the first with the form [X &#8212;* &#945; &#8226; Q, i, j] representing rules that have yet to be completed and span node i to node j.</S>
    <S sid="44" ssid="23">The other items have the form [X, i, j] and indicate that non-terminal X spans [i, j].</S>
    <S sid="45" ssid="24">As with sentence parsing, the goal is a deduction that covers the spans of the entire input lattice [5, 0, |V  |&#8722; 1].</S>
    <S sid="46" ssid="25">The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an E-edge without advancing the dot in an incomplete rule 3) advance the dot across a nonterminal symbol given appropriate antecedents.</S>
    <S sid="47" ssid="26">A target language model is necessary to generate fluent output.</S>
    <S sid="48" ssid="27">To do so, the grammar is intersected with an n-gram LM.</S>
    <S sid="49" ssid="28">To mitigate the effects of the combinatorial explosion of non-terminals the LM intersection entails, we use cube-pruning to only consider the most promising expansions (Chiang, 2007).</S>
    <S sid="50" ssid="29">A second important class of translation models includes those based formally on FSTs.</S>
    <S sid="51" ssid="30">We present a description of the decoding process for a word lattice using a representative FST model, the phrase-based translation model described in Koehn et al. (2003).</S>
    <S sid="52" ssid="31">Phrase-based models translate a foreign sentence f into the target language e by breaking up f into a sequence of phrases f1, where each phrase fz can contain one or more contiguous words and is translated into a target phrase ez of one or more contiguous words.</S>
    <S sid="53" ssid="32">Each word in f must be translated exactly once.</S>
    <S sid="54" ssid="33">To generalize this model to word lattices, it is necessary to choose both a path through the lattice and a partitioning of the sentence this induces into a sequence of phrases f1.</S>
    <S sid="55" ssid="34">Although the number of source phrases in a word lattice can be exponential in the number of nodes, enumerating the possible translations of every span in a lattice is in practice tractable, as described by Bertoldi et al. (2007).</S>
    <S sid="56" ssid="35">We adapted the Moses phrase-based decoder to translate word lattices (Koehn et al., 2007).</S>
    <S sid="57" ssid="36">The unmodified decoder builds a translation hypothesis from left to right by selecting a range of untranslated words and adding translations of this phrase to the end of the hypothesis being extended.</S>
    <S sid="58" ssid="37">When no untranslated words remain, the translation process is complete.</S>
    <S sid="59" ssid="38">The word lattice decoder works similarly, only now the decoder keeps track not of the words that have been covered, but of the nodes, given a topological ordering of the nodes.</S>
    <S sid="60" ssid="39">For example, assuming the third lattice in Figure 1 is our input, if the edge with word a is translated, this will cover two untranslated nodes [0,1] in the coverage vector, even though it is only a single word.</S>
    <S sid="61" ssid="40">As with sentencebased decoding, a translation hypothesis is complete when all nodes in the input lattice are covered.</S>
    <S sid="62" ssid="41">The changes described thus far are straightforward adaptations of the underlying phrase-based sentence decoder; however, dealing properly with non-monotonic decoding of word lattices introduces some minor complexity that is worth mentioning.</S>
    <S sid="63" ssid="42">In the sentence decoder, any translation of any span of untranslated words is an allowable extension of a partial translation hypothesis, provided that the coverage vectors of the extension and the partial hypothesis do not intersect.</S>
    <S sid="64" ssid="43">In a non-linear word lattice, a further constraint must be enforced ensuring that there is always a path from the starting node of the translation extension&#8217;s source to the node representing the nearest right edge of the already-translated material, as well as a path from the ending node of the translation extension&#8217;s source to future translated spans.</S>
    <S sid="65" ssid="44">Figure 3 illustrates the problem.</S>
    <S sid="66" ssid="45">If [0,1] is translated, the decoder must not consider translating [2,3] as a possible extension of this hypothesis since there is no path from node 1 to node 2 and therefore the span [1,2] would never be covered.</S>
    <S sid="67" ssid="46">In the parser that forms the basis of the hierarchical decoder described in Section 2.3, no such restriction is necessary since grammar rules are processed in a strictly left-to-right fashion without any skips.</S>
  </SECTION>
  <SECTION title="3 Distortion in a non-linear word lattice" number="3">
    <S sid="68" ssid="1">In both hierarchical and phrase-based models, the distance between words in the source sentence is used to limit where in the target sequence their translations will be generated.</S>
    <S sid="69" ssid="2">In phrase based translation, distortion is modeled explicitly.</S>
    <S sid="70" ssid="3">Models that support non-monotonic decoding generally include a distortion cost, such as |ai &#8722; bi&#8722;1 &#8722; 1 |where ai is the starting position of the foreign phrase fi and bi&#8722;1 is the ending position of phrase fi&#8722;1 (Koehn et al., 2003).</S>
    <S sid="71" ssid="4">The intuition behind this model is that since most translation is monotonic, the cost of skipping ahead or back in the source should be proportional to the number of words that are skipped.</S>
    <S sid="72" ssid="5">Additionally, a maximum distortion limit is used to restrict the size of the search space.</S>
    <S sid="73" ssid="6">In linear word lattices, such as confusion networks, the distance metric used for the distortion penalty and for distortion limits is well defined; however, in a non-linear word lattice, it poses the problem illustrated in Figure 4.</S>
    <S sid="74" ssid="7">Assuming the leftto-right decoding strategy described in the previous section, if c is generated by the first target word, the distortion penalty associated with &#8220;skipping ahead&#8221; should be either 3 or 2, depending on what path is chosen to translate the span [0,3].</S>
    <S sid="75" ssid="8">In large lattices, where a single arc may span many nodes, the possible distances may vary quite substantially depending on what path is ultimately taken, and handling this properly therefore crucial.</S>
    <S sid="76" ssid="9">Although hierarchical phrase-based models do not model distortion explicitly, Chiang (2007) suggests using a span length limit to restrict the window in which reordering can take place.1 The decoder enforces the constraint that a synchronous rule learned from the training data (the only mechanism by which reordering can be introduced) can span maximally A words in f. Like the distortion cost used in phrase-based systems, A is also poorly defined for non-linear lattices.</S>
    <S sid="77" ssid="10">Since we want a distance metric that will restrict as few local reorderings as possible on any path, we use a function &#65533;(a, b) returning the length of the shortest path between nodes a and b.</S>
    <S sid="78" ssid="11">Since this function is not dependent on the exact path chosen, it can be computed in advance of decoding using an allpairs shortest path algorithm (Cormen et al., 1989).</S>
    <S sid="79" ssid="12">We tested the effect of the distance metric on translation quality using Chinese word segmentation lattices (Section 4.1, below) using both a hierarchical and phrase-based system modified to translate word lattices.</S>
    <S sid="80" ssid="13">We compared the shortest-path distance metric with a baseline which uses the difference in node number as the distortion distance.</S>
    <S sid="81" ssid="14">For an additional datapoint, we added a lexicalized reordering model that models the probability of each phrase pair appearing in three different orientations (swap, monotone, other) in the training corpus (Koehn et al., 2005).</S>
    <S sid="82" ssid="15">Table 2 summarizes the results of the phrasebased systems.</S>
    <S sid="83" ssid="16">On both test sets, the shortest path metric improved the BLEU scores.</S>
    <S sid="84" ssid="17">As expected, the lexicalized reordering model improved translation quality over the baseline; however, the improvement was more substantial in the model that used the shortest-path distance metric (which was already a higher baseline).</S>
    <S sid="85" ssid="18">Table 3 summarizes the results of our experiment comparing the performance of two distance metrics to determine whether a rule has exceeded the decoder&#8217;s span limit.</S>
    <S sid="86" ssid="19">The pattern is the same, showing a clear increase in BLEU for the shortest path metric over the baseline.</S>
  </SECTION>
  <SECTION title="4 Exploiting Source Language Alternatives" number="4">
    <S sid="87" ssid="1">Chinese word segmentation.</S>
    <S sid="88" ssid="2">A necessary first step in translating Chinese using standard models is segmenting the character stream into a sequence of words.</S>
    <S sid="89" ssid="3">Word-lattice translation offers two possible improvements over the conventional approach.</S>
    <S sid="90" ssid="4">First, a lattice may represent multiple alternative segmentations of a sentence; input represented in this way will be more robust to errors made by the segmenter.2 Second, different segmentation granularities may be more or less optimal for translating different spans.</S>
    <S sid="91" ssid="5">By encoding alternatives in the input in a word lattice, the decision as to which granularity to use for a given span can be resolved during decoding rather than when constructing the system.</S>
    <S sid="92" ssid="6">Figure 5 illustrates a lattice based on three different segmentations.</S>
    <S sid="93" ssid="7">Arabic morphological variation.</S>
    <S sid="94" ssid="8">Arabic orthography is problematic for lexical and phrase-based MT approaches since a large class of functional elements (prepositions, pronouns, tense markers, conjunctions, definiteness markers) are attached to their host stems.</S>
    <S sid="95" ssid="9">Thus, while the training data may provide good evidence for the translation of a particular stem by itself, the same stem may not be attested when attached to a particular conjunction.</S>
    <S sid="96" ssid="10">The general solution taken is to take the best possible morphological analysis of the text (it is often ambiguous whether a piece of a word is part of the stem or merely a neighboring functional element), and then make a subset of the bound functional elements in the language into freestanding tokens.</S>
    <S sid="97" ssid="11">Figure 6 illustrates the unsegmented Arabic surface form as well as the morphological segmentation variant we made use of.</S>
    <S sid="98" ssid="12">The limitation of this approach is that as the amount and variety of training data increases, the optimal segmentation strategy changes: more aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006).</S>
    <S sid="99" ssid="13">Lattices allow the decoder to make decisions about what granularity of segmentation to use subsententially.</S>
    <S sid="100" ssid="14">In our experiments we used two state-of-the-art Chinese word segmenters: one developed at Harbin Institute of Technology (Zhao et al., 2001), and one developed at Stanford University (Tseng et al., 2005).</S>
    <S sid="101" ssid="15">In addition, we used a character-based segmentation.</S>
    <S sid="102" ssid="16">In the remaining of this paper, we use cs for character segmentation, hs for Harbin segmentation and ss for Stanford segmentation.</S>
    <S sid="103" ssid="17">We built two types of lattices: one that combines the Harbin and Stanford segmenters (hs+ss), and one which uses all three segmentations (hs+ss+cs).</S>
    <S sid="104" ssid="18">Data and Settings.</S>
    <S sid="105" ssid="19">The systems used in these experiments were trained on the NIST MT06 Eval corpus without the UN data (approximatively 950K sentences).</S>
    <S sid="106" ssid="20">The corpus was analyzed with the three segmentation schemes.</S>
    <S sid="107" ssid="21">For the systems using word lattices, the training data contained the versions of the corpus appropriate for the segmentation schemes used in the input.</S>
    <S sid="108" ssid="22">That is, for the hs+ss condition, the training data consisted of two copies of the corpus: one segmented with the Harbin segmenter and the other with the Stanford segmenter.3 A trigram English language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) was trained on the English side of our training data as well as portions of the Gigaword v2 English Corpus, and was used for all experiments.</S>
    <S sid="109" ssid="23">The NIST MT03 test set was used as a development set for optimizing the interpolation weights using minimum error rate training (Och, 2003).</S>
    <S sid="110" ssid="24">The testing was done on the NIST 2005 and 2006 evaluation sets (MT05, MT06).</S>
    <S sid="111" ssid="25">Experimental results: Word-lattices improve translation quality.</S>
    <S sid="112" ssid="26">We used both a phrase-based translation model, decoded using our modified version of Moses (Koehn et al., 2007), and a hierarchical phrase-based translation model, using our modified version of Hiero (Chiang, 2005; Chiang, 2007).</S>
    <S sid="113" ssid="27">These two translation model types illustrate the applicability of the theoretical contributions presented in Section 2 and Section 3.</S>
    <S sid="114" ssid="28">We observed that the coverage of named entities (NEs) in our baseline systems was rather poor.</S>
    <S sid="115" ssid="29">Since names in Chinese can be composed of relatively long strings of characters that cannot be translated individually, when generating the segmentation lattices that included cs arcs, we avoided segmenting NEs of type PERSON, as identified using a Chinese NE tagger (Florian et al., 2004).</S>
    <S sid="116" ssid="30">The results are summarized in Table 4.</S>
    <S sid="117" ssid="31">We see that using word lattices improves BLEU scores both in the phrase-based model and hierarchical model as compared to the single-best segmentation approach.</S>
    <S sid="118" ssid="32">All results using our word-lattice decoding for the hierarchical models (hs+ss and hs+ss+cs) are significantly better than the best segmentation (ss).4 For the phrase-based model, we obtain significant gains using our word-lattice decoder using all three segmentations on MT05.</S>
    <S sid="119" ssid="33">The other results, while better than the best segmentation (hs) by at least 0.3 BLEU points, are not statistically significant.</S>
    <S sid="120" ssid="34">Even if the results are not statistically significant for MT06, there is a high decrease in OOV items when using word-lattices.</S>
    <S sid="121" ssid="35">For example, for MT06 the number of OOVs in the hs translation is 484.</S>
    <S sid="122" ssid="36">The number of OOVs decreased by 19% for hs+ss and by 75% for hs+ss+cs.</S>
    <S sid="123" ssid="37">As mentioned in Section 3, using lexical reordering for word-lattices further improves the translation quality.</S>
    <S sid="124" ssid="38">We created lattices from an unsegmented version of the Arabic test data and generated alternative arcs where clitics as well as the definiteness marker and the future tense marker were segmented into tokens.</S>
    <S sid="125" ssid="39">We used the Buckwalter morphological analyzer and disambiguated the analysis using a simple unigram model trained on the Penn Arabic Treebank.</S>
    <S sid="126" ssid="40">Data and Settings.</S>
    <S sid="127" ssid="41">For these experiments we made use of the entire NIST MT08 training data, although for training of the system, we used a subsampling method proposed by Kishore Papineni that aims to include training sentences containing ngrams in the test data (personal communication).</S>
    <S sid="128" ssid="42">For all systems, we used a 5-gram English LM trained on 250M words of English training data.</S>
    <S sid="129" ssid="43">The NIST MT03 test set was used as development set for optimizing the interpolation weights using MER training (Och, 2003).</S>
    <S sid="130" ssid="44">Evaluation was carried out on the NIST 2005 and 2006 evaluation sets (MT05, MT06).</S>
    <S sid="131" ssid="45">Experimental results: Word-lattices improve translation quality.</S>
    <S sid="132" ssid="46">Results are presented in Table 5.</S>
    <S sid="133" ssid="47">Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models.</S>
  </SECTION>
  <SECTION title="5 Prior work" number="5">
    <S sid="134" ssid="1">Lattice Translation.</S>
    <S sid="135" ssid="2">The &#8216;noisier channel&#8217; model of machine translation has been widely used in spoken language translation as an alternative to selecting the single-best hypothesis from an ASR system and translating it (Ney, 1999; Casacuberta et al., 2004; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007).</S>
    <S sid="136" ssid="3">Several authors (e.g.</S>
    <S sid="137" ssid="4">Saleem et al. (2005) and Bertoldi et al.</S>
    <S sid="138" ssid="5">(2007)) comment directly on the impracticality of using n-best lists to translate speech.</S>
    <S sid="139" ssid="6">Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation.</S>
    <S sid="140" ssid="7">Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of Bangalore and Riccardi (2000).</S>
    <S sid="141" ssid="8">Mathias (2007) and Saleem et al. (2004) only report results of monotonic decoding for the systems they describe.</S>
    <S sid="142" ssid="9">Bertoldi et al. (2007) solve the problem by requiring that their input be in the format of a confusion network, which enables the standard distortion penalty to be used.</S>
    <S sid="143" ssid="10">Finally, the system described by Zhang et al. (2005) uses IBM Model 4 features to translate lattices.</S>
    <S sid="144" ssid="11">For the distortion model, they use the maximum probability value over all possible paths in the lattice for each jump considered, which is similar to the approach we have taken.</S>
    <S sid="145" ssid="12">Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases.</S>
    <S sid="146" ssid="13">Applications of source lattices outside of the domain of spoken language translation have been far more limited.</S>
    <S sid="147" ssid="14">Costa-juss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language.</S>
    <S sid="148" ssid="15">Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model.</S>
    <S sid="149" ssid="16">The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simplifications of a surface token when the surface token can not be translated.</S>
    <S sid="150" ssid="17">Parsing and formal language theory.</S>
    <S sid="151" ssid="18">There has been considerable work on parsing word lattices, much of it for language modeling applications in speech recognition (Ney, 1991; Cheppalier and Rajman, 1998).</S>
    <S sid="152" ssid="19">Additionally, Grune and Jacobs (2008) refines an algorithm originally due to Bar-Hillel for intersecting an arbitrary FSA (of which word lattices are a subset) with a CFG.</S>
    <S sid="153" ssid="20">Klein and Manning (2001) formalize parsing as a hypergraph search problem and derive an O(n3) parser for lattices.</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="6">
    <S sid="154" ssid="1">We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations.</S>
    <S sid="155" ssid="2">Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algorithm for lattice decoding using a hierarchical phrase-based model.</S>
    <S sid="156" ssid="3">Additionally, we have shown that although word lattices complicate modeling of word reordering, a simple heuristic offers good performance and enables many standard distortion models to be used directly with lattice input.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="157" ssid="1">This research was supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No.</S>
    <S sid="158" ssid="2">HR0011-06-2-0001.</S>
    <S sid="159" ssid="3">The authors wish to thank Niyu Ge for the Chinese named-entity analysis, Pi-Chuan Chang for her assistance with the Stanford Chinese segmenter, and Tie-Jun Zhao and Congui Zhu for making the Harbin Chinese segmenter available to us.</S>
  </SECTION>
</PAPER>
