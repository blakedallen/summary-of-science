A Comparison of Vector-based Representations for Semantic Composition
In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods.
We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication.
Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora.
We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection.
The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.
We compute a weighted linear combination of the embeddings for words that appear in the document to be classified.
We compare count and predict representations as input to composition functions.
For paraphrase detection, we use cosine similarity between sentence pairs together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length.
Add and mult attained the top performance with the simple models for both figures of merit.
