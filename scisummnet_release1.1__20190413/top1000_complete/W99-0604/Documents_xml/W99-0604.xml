<PAPER>
  <S sid="0">Improved Alignment Models For Statistical Machine Translation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">PP &#8212; 31.5 In all experiments, we use the following three error criteria: &#8226; WER (word error rate): The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the target string.</S>
    <S sid="2" ssid="2">This performance criterion is widely used in speech recognition.</S>
    <S sid="3" ssid="3">&#8226; PER (position-independent word error rate): A shortcoming of the WER is the fact that it requires a perfect word order.</S>
    <S sid="4" ssid="4">This is 26 2: for Text and Speech Input: error rate (WER), positionindependent word error rate (PER) and subjective sentence error rate (SSER) with/without preprocessing (147 sentences = 1 968 words of the Verbmobil task).</S>
    <S sid="5" ssid="5">Input Preproc.</S>
    <S sid="6" ssid="6">WER[%] PER[Vo] SSER[%] Single-Word Based Approach Text No 53.4 38.3 35.7 Yes 56.0 41.2 35.3 Speech No 67.8 50.1 54.8 Yes 67.8 51.4 52.7 Alignnient Templates Text No 49.5 35.3 31.5 Yes 48.3 35.1 27.2 Speech No 63.5 45.6 52.4 Yes 62.8 45.6 50.3 particularly a problem for the Verbmobil task, where the word order of the German- English sentence pair can be quite different.</S>
    <S sid="7" ssid="7">As a result, the word order of the automatically generated target sentence can be different from that of the target sentence, but nevertheless acceptable so that the WER measure alone could be misleading.</S>
    <S sid="8" ssid="8">In order to overcome this problem, we introduce as additional measure the positionindependent word error rate (PER).</S>
    <S sid="9" ssid="9">This measure compares the words in the two senthe word order into account.</S>
    <S sid="10" ssid="10">Words that have no matching counterparts are counted as substitution errors.</S>
    <S sid="11" ssid="11">Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or deletion errors in addition to substitution errors.</S>
    <S sid="12" ssid="12">The PER is guaranteed to be less than or equal to the WER.</S>
    <S sid="13" ssid="13">&#8226; SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary.</S>
    <S sid="14" ssid="14">Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0.</S>
    <S sid="15" ssid="15">A score of 0.0 means that the translation is semantically and syntactically correct, a score of 0.5 means that a sentence is semantically correct but syntactically wrong and a score of 1.0 means that the sentence is semantically wrong.</S>
    <S sid="16" ssid="16">The human examiner was offered the translated sentences of the two approaches at the same As a result we expect a better possibility of reproduction.</S>
    <S sid="17" ssid="17">The results of the translation experiments using the single-word based approach and the alignment template approach on text input and on speech input are summarized in Table 2.</S>
    <S sid="18" ssid="18">The results are shown with and without the use of domain-specific preprocessing.</S>
    <S sid="19" ssid="19">The alignment template approach produces better translation results than the single-word based approach.</S>
    <S sid="20" ssid="20">From this we draw the conclusion that it is important to model word groups in source and target language.</S>
    <S sid="21" ssid="21">Considering the recognition word error rate of 31% the degradation of about 20% by speech input can be expected.</S>
    <S sid="22" ssid="22">The average translation time on an Alpha workstation for a single sentence is about one second for the alignment template appreach and 30 seconds for the single-word based search procedure.</S>
    <S sid="23" ssid="23">Within the Verbmobil project other translation modules based on rule-based, examplebased and dialogue-act-based translation are used.</S>
    <S sid="24" ssid="24">We are not able to present results with these methods using our test corpus.</S>
    <S sid="25" ssid="25">But in the current Verbmobil prototype the preliminary evaluations show that the statistical methods produce comparable or better results than other systems.</S>
    <S sid="26" ssid="26">An advantage of the systhat it is robust and always produces a translation result even if the input of the speech recognizer is quite incorrect.</S>
    <S sid="27" ssid="27">5 Summary We have described two approaches to perform statistical machine translation which extend the baseline alignment models.</S>
    <S sid="28" ssid="28">The single-word 27 based approach allows for the the possibility of one-to-many alignments.</S>
    <S sid="29" ssid="29">The alignment template approach uses two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.</S>
    <S sid="30" ssid="30">As a result the context of words has a greater influence and the changes in word order from source to target language can be learned explicitly.</S>
    <S sid="31" ssid="31">An advantage of both methods is that they learn fully automatically by using a bilingual training corpus and are capable of achieving better translation results on a limited-domain task than other example-based or rule-based translation systems.</S>
    <S sid="32" ssid="32">Acknowledgment This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).</S>
  </ABSTRACT>
  <SECTION title="1 Statistical Machine Translation" number="1">
    <S sid="33" ssid="1">The goal of machine translation is the translation of a text given in some source language into a target language.</S>
    <S sid="34" ssid="2">We are given a source string f fi...fj...fj, which is to be translated into a target string ef = ei...e,...el.</S>
    <S sid="35" ssid="3">Among all possible target strings, we will choose the string with the highest probability: The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S>
    <S sid="36" ssid="4">Pr(ef) is the language model of the target language, whereas Pr(fillef) is the translation model.</S>
    <S sid="37" ssid="5">Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.</S>
    <S sid="38" ssid="6">The model is often further restricted that each source word is assigned exactly one target word.</S>
    <S sid="39" ssid="7">These alignment models are similar to the concept of Hidden Markov models (HMM) in speech recognition.</S>
    <S sid="40" ssid="8">The alignment mapping is j &#8212;&gt; i = aj from source position j to target position i = a3.</S>
    <S sid="41" ssid="9">The use of this alignment model raises major problems as it fails to capture dependencies between groups of words.</S>
    <S sid="42" ssid="10">As experiments have shown it is difficult to handle different word order and the translation of compound nouns.</S>
    <S sid="43" ssid="11">In this paper, we will describe two methods for statistical machine translation extending the baseline alignment model in order to account for these problems.</S>
    <S sid="44" ssid="12">In section 2, we shortly review the single-word based approach described in (Tillmann et al., 1997) with some recently implemented extensions allowing for one-to-many alignments.</S>
    <S sid="45" ssid="13">In section 3 we describe the alignment template approach which explicitly models shallow phrases and in doing so tries to overcome the above mentioned restrictions of singleword alignments.</S>
    <S sid="46" ssid="14">The described method is an improvement of (Och and Weber, 1998), resulting in an improved training and a faster search organization.</S>
    <S sid="47" ssid="15">The basic idea is to model two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words within these phrases.</S>
    <S sid="48" ssid="16">Similar aims are pursued by (Alshawi et al., 1998; Wang and Waibel, 1998) but differently approached.</S>
    <S sid="49" ssid="17">In section 4 we compare the two methods using the Verbmobil task.</S>
  </SECTION>
  <SECTION title="2 Single-Word Based Approach" number="2">
    <S sid="50" ssid="1">In this section, we shortly review a translation approach based on the so-called monotonicity requirement (Tillmann et al., 1997).</S>
    <S sid="51" ssid="2">Our aim is to provide a basis for comparing the two different translation approaches presented.</S>
    <S sid="52" ssid="3">In Eq.</S>
    <S sid="53" ssid="4">(1), Pr (el) is the language model, which is a trigram language model in this case.</S>
    <S sid="54" ssid="5">For the translation model Pr (ll lei ) we make the assumption that each source word is aligned to exactly one target word (a relaxation of this assumption is described in section 2.2).</S>
    <S sid="55" ssid="6">For our model, the probability of alignment al for position j depends on the previous alignment position a3_1 (Vogel et al., 1996).</S>
    <S sid="56" ssid="7">Using this assumption, there are two types of probabilities: the alignment probabilities denoted by p(a31a3 _1) and the lexicon probabilities denoted by P(f3 lea2).</S>
    <S sid="57" ssid="8">The string translation probability can be re-written: For the training of the above model parameters, we use the maximum likelihood criterion in the so-called maximum approximation.</S>
    <S sid="58" ssid="9">When aligning the words in parallel texts (for IndoEuropean language pairs like Spanish-English, French-English, Italian-German,...), we typically observe a strong localization effect.</S>
    <S sid="59" ssid="10">In many cases, although not always, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.</S>
    <S sid="60" ssid="11">In this approach, we first assume that the alignments satisfy the monotonicity requirement.</S>
    <S sid="61" ssid="12">Within the translation search, we will introduce suitably restricted permutations of the source string, to satisfy this requirement.</S>
    <S sid="62" ssid="13">For the alignment model, the monotonicity property allows only transitions from a3_1 to ai with a jump width 6: 6 a3 &#8212; ai_i E ICI, 1, 21.</S>
    <S sid="63" ssid="14">Theses jumps correspond to the following three cases (6 = 0, 1, 2): new target word is generated.</S>
    <S sid="64" ssid="15">&#8226; 8 = 2 (skip transition = non-aligned word): This case corresponds to skipping a word, i.e. there is a word in the target string with no aligned word in the source string.</S>
    <S sid="65" ssid="16">The possible alignments using the monotonicity assumption are illustrated in Fig.</S>
    <S sid="66" ssid="17">1.</S>
    <S sid="67" ssid="18">Monotone alignments are paths through this uniform trellis structure.</S>
    <S sid="68" ssid="19">Using the concept of monotone alignments a search procedure can be formulated which is equivalent to finding the best path through a translation lattice, where the following auxiliary quantity is evaluated using dynamic programming: Here, e and e' are Qe, (j, e) probability of the best partial hypothesis (el, an with ei = e, = e' and a3 = i. the two final words of the hypothesized target string.</S>
    <S sid="69" ssid="20">The auxiliary quantity is evaluated in a position-synchronous way, where j is the processed position in the source string.</S>
    <S sid="70" ssid="21">The result of this search is a mapping: j (a3, ea,), where each source word is mapped to a target position a3 and a word ea, at this position.</S>
    <S sid="71" ssid="22">For a trigram language model the following DP recursion equation is evaluated: p(8) is the alignment probability for the three cases above, pe I., .) denoting the trigram language model. e, e', e&amp;quot;, e&amp;quot; are the four final words which are considered in the dynamic programming taking into account the monotonicity restriction and a trigram language model.</S>
    <S sid="72" ssid="23">The DP equation is evaluated recursively to find the best partial path to each grid point (j, , e).</S>
    <S sid="73" ssid="24">No explicit length model for the length of the generated target string ef given the source string fi/ is used during the generation process.</S>
    <S sid="74" ssid="25">The length model is implicitly given by the alignment probabilities.</S>
    <S sid="75" ssid="26">The optimal translation is obtained by carrying out the following optimization: where J is the length of the input sentence and $ is a symbol denoting the sentence end.</S>
    <S sid="76" ssid="27">The complexity of the algorithm for full search is J- E4, where E is the size of the target language vocabulary.</S>
    <S sid="77" ssid="28">However, this is drastically reduced by beam-search.</S>
    <S sid="78" ssid="29">The baseline alignment model does not permit that a source word is aligned with two or more target words.</S>
    <S sid="79" ssid="30">Therefore, lexical correspondences like 'Zahnarzttermin' for dentist's appointment cause problems because a single source word must be mapped on two or more target words.</S>
    <S sid="80" ssid="31">To solve this problem for the alignment in training, we first reverse the translation direction, i. e. English is now the source language, and German is the target language.</S>
    <S sid="81" ssid="32">For this reversed translation direction, we perform the usual training and then check the alignment paths obtained in the maximum approximation.</S>
    <S sid="82" ssid="33">Whenever a German word is aligned with a sequence of the adjacent English words, this sequence is added to the English vocabulary as an additional entry.</S>
    <S sid="83" ssid="34">As a result, we have an extended English vocabulary.</S>
    <S sid="84" ssid="35">Using this new vocabulary, we then perform the stan2.3 Extension to Handle Non-Monotonicity Our approach assumes that the alignment is monotone with respect to the word order for the lion's share of all word alignments.</S>
    <S sid="85" ssid="36">For the translation direction German-English the monotonicity constraint is violated mainly with respect to the verb group.</S>
    <S sid="86" ssid="37">In German, the verb group usually consists of a left and a right verbal brace, whereas in English the words of the verb group usually form a sequence of consecutive words.</S>
    <S sid="87" ssid="38">For our DP search, we use a leftto-right beam-search concept having been introduced in speech recognition, where we rely on beam-search as an efficient pruning technique in order to handle potentially huge search spaces.</S>
    <S sid="88" ssid="39">Our ultimate goal is speech translation aiming at a tight integration of speech recognition and translation (Ney, 1999).</S>
    <S sid="89" ssid="40">The results presented were obtained by using a quasi-monotone search procedure, which proceeds from left to right along the position of the source sentence but allows for a small number of source positions that are not processed monotonically.</S>
    <S sid="90" ssid="41">The word re-orderings of the source sentence positions were restricted to the words of the German verb group.</S>
    <S sid="91" ssid="42">Details of this approach will be presented elsewhere.</S>
  </SECTION>
  <SECTION title="3 Alignment Template Approach" number="3">
    <S sid="92" ssid="1">A general deficiency of the baseline alignment models is that they are only able to model correspondences between single words.</S>
    <S sid="93" ssid="2">A first countermeasure was the refined alignment model described in section 2.2.</S>
    <S sid="94" ssid="3">A more systematic approach is to consider whole phrases rather than single words as the basis for the alignment models.</S>
    <S sid="95" ssid="4">In other words, a whole group of adjacent words in the source sentence may be aligned with a whole group of adjacent words in the target language.</S>
    <S sid="96" ssid="5">As a result the context of words has a greater influence and the changes in word order from source to target language can be learned explicitly.</S>
    <S sid="97" ssid="6">3.1 The word level alignment: alignment templates In this section we will describe how we model the translation of shallow phrases.</S>
    <S sid="98" ssid="7">The key element of our translation model are the alignment templates._ An alignment template z is a triple (F, E, A) which describes the alignment A between a source class sequence F and a target class sequence E. The alignment A is represented as a matrix with binary values.</S>
    <S sid="99" ssid="8">A matrix element with value 1 means that the words at the corresponding positions are aligned and the value 0 means that the words are not aligned.</S>
    <S sid="100" ssid="9">If a source word is not aligned to a target word then it is aligned to the empty word eo which shall be at the imaginary position i = 0.</S>
    <S sid="101" ssid="10">This alignment representation is a generalization of the baseline alignments described in (Brown et al., 1993) and allows for many-to-many alignments.</S>
    <S sid="102" ssid="11">The classes used in P and E are automatically trained bilingual classes using the method described in (Och, 1999) and constitute a partition of the vocabulary of source and target language.</S>
    <S sid="103" ssid="12">The class functions T and e map words to their classes.</S>
    <S sid="104" ssid="13">The use of classes instead of words themselves has the advantage of a better generalization.</S>
    <S sid="105" ssid="14">If there exist classes in source and target language which contain all towns it is possible that an alignment template learned using a special town can be generalized to all towns.</S>
    <S sid="106" ssid="15">In Fig.</S>
    <S sid="107" ssid="16">2 an example of an alignment template is shown.</S>
    <S sid="108" ssid="17">An alignment template z -= (F, E, A) is applicable to a sequence of source words I if the alignment template classes and the classes of the source words are equal: .F( f) = F. The application of the alignment template z constrains the target words-6 to correspond to the target class sequence: e(e) = E. The application of an alignment template does not determine the target words, but only constrains them.</S>
    <S sid="109" ssid="18">For the selection of words from classes we use a statistical model for p(e-lz, f) based on the lexicon probabilities of a statistical lexicon p(f le).</S>
    <S sid="110" ssid="19">We assume a mixture alignment between the source and target language words constrained by the alignment matrix A: In order to describe the phrase level alignment in a formal way, we first decompose both the source sentence fiJ and the target sentence ef into a sequence of phrases (k = 1,. .</S>
    <S sid="111" ssid="20">.</S>
    <S sid="112" ssid="21">, K): In order to simplify the notation and the presentation, we ignore the fact that there can be a large number of possible segmentations and assume that there is only one segmentation.</S>
    <S sid="113" ssid="22">In the previous section, we have described the alignment within the phrases.</S>
    <S sid="114" ssid="23">For the alignment af&#8216; between the source phrases &#235; and the target phrases fr, we obtain the following equation: For the phrase level alignment we use a first-order alignment model p(aklaki-1 K) = P(ak lak-i, K) which is in addition constrained to be a permutation of the K phrases.</S>
    <S sid="115" ssid="24">For the translation of one phrase, we introduce the alignment template as an unknown variable: The probability p(z1e) to apply an alignment template gets estimated by relative frequencies (see next section).</S>
    <S sid="116" ssid="25">The probability p(f lz, e) is decomposed by Eq.</S>
    <S sid="117" ssid="26">(2).</S>
    <S sid="118" ssid="27">In this section we show how we obtain the parameters of our translation model by using a parallel training corpus: rections f e and e &#8212;+ f by applying the EM-algorithm.</S>
    <S sid="119" ssid="28">However we do not apply maximum approximation in training, thereby obtaining slightly improved alignments.</S>
    <S sid="120" ssid="29">2.</S>
    <S sid="121" ssid="30">For each translation direction we calculate the Viterbi-alignment of the translation models determined in the previous step.</S>
    <S sid="122" ssid="31">Thus we get two alignment vectors crj1. and bf for each sentence.</S>
    <S sid="123" ssid="32">We increase the quality of the alignments by combining the two alignment vectors into one alignment matrix using the following method.</S>
    <S sid="124" ssid="33">A1 = {(aj , j)lj = 1 .</S>
    <S sid="125" ssid="34">.</S>
    <S sid="126" ssid="35">.</S>
    <S sid="127" ssid="36">J} and A2 = {(i,bi)li = 1 ... /} denote the set of links in the two Viterbi-alignments.</S>
    <S sid="128" ssid="37">In a first step the intersection A = A1 n A2 is determined.</S>
    <S sid="129" ssid="38">The elements within A are justified by both Viterbi-alignments and are therefore very reliable.</S>
    <S sid="130" ssid="39">We now extend the alignment A iteratively by adding links (i, j) occurring only in A1 or in A2 if they have a neighbouring link already in A or if neither the word fi nor the word ei are aligned in A.</S>
    <S sid="131" ssid="40">The alignment (i, j) has the neighbouring links (i &#8212; 1,j), (i, j &#8212; 1), (i + 1,j), and (i, j + 1).</S>
    <S sid="132" ssid="41">In the Verbmobil task (Table 1) the precision of the baseline Viterbi alignments is 83.3 percent with English as source language and 81.8 percent with German as source language.</S>
    <S sid="133" ssid="42">Using this heuristic we get an alignment matrix with a precision of 88.4 percent without loss in recall.</S>
    <S sid="134" ssid="43">3.</S>
    <S sid="135" ssid="44">We estimate a bilingual word lexicon p(f le) by the relative frequencies of the alignment determined in the previous step: Here nA(f,e) is the frequency that the word f is aligned to e and n(e) is the frequency of e in the training corpus.</S>
    <S sid="136" ssid="45">4.</S>
    <S sid="137" ssid="46">We determine word classes for source and target language.</S>
    <S sid="138" ssid="47">A naive approach for doing this would be the use of monolingually optimized word classes in source and target language.</S>
    <S sid="139" ssid="48">Unfortunately we can not expect that there is a direct correspondence between independently optimized classes.</S>
    <S sid="140" ssid="49">Therefore monolingually optimized word classes do not seem to be useful for machine translation.</S>
    <S sid="141" ssid="50">We determine correlated bilingual classes by using the method described in (Och, 1999).</S>
    <S sid="142" ssid="51">The basic idea of this method is to apply a maximum-likelihood approach to the joint probability of the parallel training corpus.</S>
    <S sid="143" ssid="52">The resulting optimization criterion for the bilingual word classes is similar to the one used in monolingual maximumlikelihood word clustering.</S>
    <S sid="144" ssid="53">5.</S>
    <S sid="145" ssid="54">We count all phrase-pairs of the training corpus which are consistent with the alignment matrix determined in step 2.</S>
    <S sid="146" ssid="55">A phrase-pair is consistent with the alignment if the words within the source phrase are only aligned to words within the target phrase.</S>
    <S sid="147" ssid="56">Thus we obtain a count n(z) of how often an alignment template occurred in the aligned training corpus.</S>
    <S sid="148" ssid="57">The probability of using an alignment template needed by Eq.</S>
    <S sid="149" ssid="58">(5) is estimated by relative frequency: Fig.</S>
    <S sid="150" ssid="59">3 shows some of the extracted alignment templates.</S>
    <S sid="151" ssid="60">The extraction algorithm does not perform a selection of good or bad alignment templates - it simply extracts all possible alignment templates.</S>
    <S sid="152" ssid="61">For decoding we use the following search criterion: This decision rule is an approximation to Eq.</S>
    <S sid="153" ssid="62">(1) which would use the translation probability p(lef).</S>
    <S sid="154" ssid="63">Using the simplification it is easy to integrate translation and language model in the search process as both models predict target words.</S>
    <S sid="155" ssid="64">As experiments have shown this simplification does not affect the quality of translation results.</S>
    <S sid="156" ssid="65">To allow the influence of long contexts we use a class-based five-gram language model with backing-off.</S>
    <S sid="157" ssid="66">The search space denoted by Eq.</S>
    <S sid="158" ssid="67">(8) is very large.</S>
    <S sid="159" ssid="68">Therefore we apply two preprocessing steps before the translation of a sentence: 1.</S>
    <S sid="160" ssid="69">We_determine the set of all source phrases in f for which an applicable alignment template exists.</S>
    <S sid="161" ssid="70">Every possible application of an alignment template to a sub-sequence of the source sentence is called alignment template instantiation.</S>
    <S sid="162" ssid="71">2.</S>
    <S sid="163" ssid="72">We now perform a segmentation of the input sentence.</S>
    <S sid="164" ssid="73">We search for a sequence of This is done efficiently by dynamic programming.</S>
    <S sid="165" ssid="74">Because of the simplified decision rule (Eq.</S>
    <S sid="166" ssid="75">(8)) it is used in Eq.</S>
    <S sid="167" ssid="76">(9) P(zlik) instead of p(zI4).</S>
    <S sid="168" ssid="77">Afterwards the actual translation process begins.</S>
    <S sid="169" ssid="78">It has a search organization along the positions of the target language string.</S>
    <S sid="170" ssid="79">In search we produce partial hypotheses, each of which contains the following information: A partial hypothesis is extended by appending one target word.</S>
    <S sid="171" ssid="80">The set of all partial hypotheses can be structured as a graph with a source node representing the sentence start, leaf nodes representing full translations and intermediate nodes representing partial hypotheses.</S>
    <S sid="172" ssid="81">We recombine partial hypotheses which cannot be distinguished by neither language model nor translation model.</S>
    <S sid="173" ssid="82">When the elements 1 - 5 of two partial hypotheses do not allow to distinguish between two hypotheses it is possible to drop the hypothesis with higher costs for the subsequent search process.</S>
    <S sid="174" ssid="83">We also use beam-search in order to handle the huge search space.</S>
    <S sid="175" ssid="84">We compare in beamsearch hypotheses which cover different parts of the input sentence.</S>
    <S sid="176" ssid="85">This makes the comparison of the costs somewhat problematic.</S>
    <S sid="177" ssid="86">Therefore we integrate an (optimistic) estimation of the remaining costs to arrive at a full translation.</S>
    <S sid="178" ssid="87">This can be done efficiently by determining in advance for each word in the source language sentence a lower bound for the costs of the translation of this word.</S>
    <S sid="179" ssid="88">Together with the bit-vector stored in a partial hypothesis it is possible to achieve an efficient estimation of the remaining costs.</S>
  </SECTION>
  <SECTION title="4 Translation results" number="4">
    <S sid="180" ssid="1">The &amp;quot;Verbmobil Task&amp;quot; (Wahlster, 1993) is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation.</S>
    <S sid="181" ssid="2">The task is difficult because it consists of spontaneous speech and the syntactic structures of the sentences are less restricted and highly variable.</S>
    <S sid="182" ssid="3">The translation direction is from German to English which poses special problems due to the big difference in the word order of the two languages.</S>
    <S sid="183" ssid="4">We present results on both the text transcription and the speech recognizer output using the alignment template approach and the single-word based approach.</S>
    <S sid="184" ssid="5">The text input was obtained by manually transcribing the spontaneously spoken sentences.</S>
    <S sid="185" ssid="6">There was no constraint on the length of the sentences, and some of the sentences in the test corpus contain more than 50 words.</S>
    <S sid="186" ssid="7">Therefore, for text input, each sentence is split into shorter units using the punctuation marks.</S>
    <S sid="187" ssid="8">The segments thus obtained were translated separately, and the final translation was obtained by concatenation.</S>
    <S sid="188" ssid="9">In the case of speech input, the speech recognizer along with a prosodic module produced so-called prosodic markers which are equivalent to punctuation marks in written language.</S>
    <S sid="189" ssid="10">The experiments for speech input were performed on the single-best sentence of the recognizer.</S>
    <S sid="190" ssid="11">The recognizer had a word error rate of 31.0%.</S>
    <S sid="191" ssid="12">Considering only the real words without the punctuation marks, the word error rate was smaller, namely 20.3%.</S>
    <S sid="192" ssid="13">A summary of the corpus used in the experiments is given in Table 1.</S>
    <S sid="193" ssid="14">Here the term word refers to full-form word as there is no morphological processing involved.</S>
    <S sid="194" ssid="15">In some of our experiments we use a domain-specific preprocessing which consists of a list of 803 (for German) and 458 (for English) word-joinings and wordsplittings for word compounds, numbers, dates and proper names.</S>
    <S sid="195" ssid="16">To improve the lexicon probabilities and to account for unseen words we added a manually created German-English dictionary with 13 388 entries.</S>
    <S sid="196" ssid="17">The classes used were constrained so that all proper names were included in a single class.</S>
    <S sid="197" ssid="18">Apart from this, the classes were automatically trained using the described bilingual clustering method.</S>
    <S sid="198" ssid="19">For each of the two languages 400 classes were used.</S>
    <S sid="199" ssid="20">For the single-word based approach, we used the manual dictionary as well as the preprocessing steps described above.</S>
    <S sid="200" ssid="21">Neither the translation model nor the language model used classes in this case.</S>
    <S sid="201" ssid="22">In principal, when re-ordering words of the source string, words of the German verb group could be moved over punctuation marks, although it was penalized by a constant cost.</S>
    <S sid="202" ssid="23">The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the target string.</S>
    <S sid="203" ssid="24">This performance criterion is widely used in speech recognition.</S>
    <S sid="204" ssid="25">A shortcoming of the WER is the fact that it requires a perfect word order.</S>
    <S sid="205" ssid="26">This is Table 2: Experiments for Text and Speech Input: Word error rate (WER), positionindependent word error rate (PER) and subjective sentence error rate (SSER) with/without preprocessing (147 sentences = 1 968 words of the Verbmobil task). particularly a problem for the Verbmobil task, where the word order of the GermanEnglish sentence pair can be quite different.</S>
    <S sid="206" ssid="27">As a result, the word order of the automatically generated target sentence can be different from that of the target sentence, but nevertheless acceptable so that the WER measure alone could be misleading.</S>
    <S sid="207" ssid="28">In order to overcome this problem, we introduce as additional measure the positionindependent word error rate (PER).</S>
    <S sid="208" ssid="29">This measure compares the words in the two sentences without taking the word order into account.</S>
    <S sid="209" ssid="30">Words that have no matching counterparts are counted as substitution errors.</S>
    <S sid="210" ssid="31">Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or deletion errors in addition to substitution errors.</S>
    <S sid="211" ssid="32">The PER is guaranteed to be less than or equal to the WER.</S>
    <S sid="212" ssid="33">For a more detailed analysis, subjective judgments by test persons are necessary.</S>
    <S sid="213" ssid="34">Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0.</S>
    <S sid="214" ssid="35">A score of 0.0 means that the translation is semantically and syntactically correct, a score of 0.5 means that a sentence is semantically correct but syntactically wrong and a score of 1.0 means that the sentence is semantically wrong.</S>
    <S sid="215" ssid="36">The human examiner was offered the translated sentences of the two approaches at the same time.</S>
    <S sid="216" ssid="37">As a result we expect a better possibility of reproduction.</S>
    <S sid="217" ssid="38">The results of the translation experiments using the single-word based approach and the alignment template approach on text input and on speech input are summarized in Table 2.</S>
    <S sid="218" ssid="39">The results are shown with and without the use of domain-specific preprocessing.</S>
    <S sid="219" ssid="40">The alignment template approach produces better translation results than the single-word based approach.</S>
    <S sid="220" ssid="41">From this we draw the conclusion that it is important to model word groups in source and target language.</S>
    <S sid="221" ssid="42">Considering the recognition word error rate of 31% the degradation of about 20% by speech input can be expected.</S>
    <S sid="222" ssid="43">The average translation time on an Alpha workstation for a single sentence is about one second for the alignment template appreach and 30 seconds for the single-word based search procedure.</S>
    <S sid="223" ssid="44">Within the Verbmobil project other translation modules based on rule-based, examplebased and dialogue-act-based translation are used.</S>
    <S sid="224" ssid="45">We are not able to present results with these methods using our test corpus.</S>
    <S sid="225" ssid="46">But in the current Verbmobil prototype the preliminary evaluations show that the statistical methods produce comparable or better results than the other systems.</S>
    <S sid="226" ssid="47">An advantage of the system is that it is robust and always produces a translation result even if the input of the speech recognizer is quite incorrect.</S>
  </SECTION>
  <SECTION title="5 Summary" number="5">
    <S sid="227" ssid="1">We have described two approaches to perform statistical machine translation which extend the baseline alignment models.</S>
    <S sid="228" ssid="2">The single-word based approach allows for the the possibility of one-to-many alignments.</S>
    <S sid="229" ssid="3">The alignment template approach uses two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.</S>
    <S sid="230" ssid="4">As a result the context of words has a greater influence and the changes in word order from source to target language can be learned explicitly.</S>
    <S sid="231" ssid="5">An advantage of both methods is that they learn fully automatically by using a bilingual training corpus and are capable of achieving better translation results on a limited-domain task than other example-based or rule-based translation systems.</S>
  </SECTION>
  <SECTION title="Acknowledgment" number="6">
    <S sid="232" ssid="1">This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).</S>
  </SECTION>
</PAPER>
