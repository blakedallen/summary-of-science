<PAPER>
  <S sid="0">Automatically Labeling Semantic Classes</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc.</S>
    <S sid="2" ssid="2">The current state of the art discovers many semantic classes but fails to label their concepts.</S>
    <S sid="3" ssid="3">We propose an algorithm labeling semantic and for leveraging them to extract relationships using a top-down approach.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">The natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977).</S>
    <S sid="5" ssid="2">However, WordNet (Miller 1990) and Cyc (Lenat 1995) aside, the community has had little success in actually building large semantic repositories.</S>
    <S sid="6" ssid="3">Such broad-coverage lexical resources are extremely useful in applications such as word sense disambiguation (Leacock, Chodorow and Miller 1998) and question answering (Pasca and Harabagiu 2001).</S>
    <S sid="7" ssid="4">Current manually constructed ontologies such as WordNet and Cyc have important limitations.</S>
    <S sid="8" ssid="5">First, they often contain rare senses.</S>
    <S sid="9" ssid="6">For example, WordNet includes a rare sense of computer that means `the person who computes'.</S>
    <S sid="10" ssid="7">Using WordNet to expand queries to an information retrieval system, the expansion of computer will include words like estimator and reckoner.</S>
    <S sid="11" ssid="8">Also, the words dog, computer and company all have a sense that is a hyponym of person.</S>
    <S sid="12" ssid="9">Such rare senses make it difficult for a coreference resolution system to use WordNet to enforce the constraint that personal pronouns (e.g. he or she) must refer to a person.</S>
    <S sid="13" ssid="10">The second problem with these lexicons is that they miss many domain specific senses.</S>
    <S sid="14" ssid="11">For example, WordNet misses the user-interface-object sense of the word dialog (as often used in software manuals).</S>
    <S sid="15" ssid="12">WordNet also contains a very poor coverage of proper nouns.</S>
    <S sid="16" ssid="13">There is a need for (semi-) automatic approaches to building and extending ontologies as well as for validating the structure and content of existing ones.</S>
    <S sid="17" ssid="14">With the advent of the Web, we have access to enormous amounts of text.</S>
    <S sid="18" ssid="15">The future of ontology growing lies in leveraging this data by harvesting it for concepts and semantic relationships.</S>
    <S sid="19" ssid="16">Moreover, once such knowledge is discovered, mechanisms must be in place to enrich current ontologies with this new knowledge.</S>
    <S sid="20" ssid="17">To address some of the coverage and specificity problems in WordNet and Cyc, Pantel and Lin (2002) proposed and algorithm, called CBC, for automatically extracting semantic classes.</S>
    <S sid="21" ssid="18">Their classes consist of clustered instances like the three shown below: A limitation of these concepts is that CBC does not discover their actual names.</S>
    <S sid="22" ssid="19">That is, CBC discovers a semantic class of Canadian provinces such as Manitoba, Alberta, and Ontario, but stops short of labeling the concept as Canadian Provinces.</S>
    <S sid="23" ssid="20">Some applications such as question answering would benefit from class labels.</S>
    <S sid="24" ssid="21">For example, given the concept list (B) and a label goalie/goaltender, a QA system could look for answers to the question &amp;quot;Which goaltender won the most Hart Trophys?&amp;quot; in the concept.</S>
    <S sid="25" ssid="22">In this paper, we propose an algorithm for automatically inducing names for semantic classes and for finding instance/concept (is-a) relationships.</S>
    <S sid="26" ssid="23">Using concept signatures (templates describing the prototypical syntactic behavior of instances of a concept), we extract concept names by searching for simple syntactic patterns such as &amp;quot;concept apposition-of instance&amp;quot;.</S>
    <S sid="27" ssid="24">Searching concept signatures is more robust than searching the syntactic features of individual instances since many instances suffer from sparse features or multiple senses.</S>
    <S sid="28" ssid="25">Once labels are assigned to concepts, we can extract a hyponym relationship between each instance of a concept and its label.</S>
    <S sid="29" ssid="26">For example, once our system labels list (C) as color, we may extract relationships such as: pink is a color, red is a color, turquoise is a color, etc.</S>
    <S sid="30" ssid="27">Our results show that of the 159,000 hyponyms we extract using this simple method, 68% are correct.</S>
    <S sid="31" ssid="28">Of the 65,000 proper name hyponyms we discover, 81.5% are correct.</S>
    <S sid="32" ssid="29">The remainder of this paper is organized as follows.</S>
    <S sid="33" ssid="30">In the next section, we review previous algorithms for extracting semantic classes and hyponym relationships.</S>
    <S sid="34" ssid="31">Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships.</S>
    <S sid="35" ssid="32">Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work.</S>
  </SECTION>
  <SECTION title="2 Previous Work" number="2">
    <S sid="36" ssid="1">There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al.</S>
    <S sid="37" ssid="2">2003).</S>
    <S sid="38" ssid="3">One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998).</S>
    <S sid="39" ssid="4">The output of these programs is a ranked list of similar words to each word.</S>
    <S sid="40" ssid="5">For example, Lin's approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous words.</S>
    <S sid="41" ssid="6">For example, in (D), the color and fruit senses of orange are mixed up.</S>
    <S sid="42" ssid="7">Lin and Pantel (2001) proposed a clustering algorithm, UNICON, which generates similar lists but discriminates between senses of words.</S>
    <S sid="43" ssid="8">Later, Pantel and Lin (2002) improved the precision and recall of UNICON clusters with CBC (Clustering by Committee).</S>
    <S sid="44" ssid="9">Using sets of representative elements called committees, CBC discovers cluster centroids that unambiguously describe the members of a possible class.</S>
    <S sid="45" ssid="10">The algorithm initially discovers committees that are well scattered in the similarity space.</S>
    <S sid="46" ssid="11">It then proceeds by assigning elements to their most similar committees.</S>
    <S sid="47" ssid="12">After assigning an element to a cluster, CBC removes their overlapping features from the element before assigning it to another cluster.</S>
    <S sid="48" ssid="13">This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses.</S>
    <S sid="49" ssid="14">CBC discovered both the color sense of orange, as shown in list (C) of Section 1, and the fruit sense shown below: (E) peach, pear, apricot, strawberry, banana, mango, melon, apple, pineapple, cherry, plum, lemon, grapefruit, orange, berry, raspberry, blueberry, kiwi, ...</S>
    <S sid="50" ssid="15">There have also been several approaches to discovering hyponym (is-a) relationships from text.</S>
    <S sid="51" ssid="16">Hearst (1992) used seven lexico-syntactic patterns, for example &amp;quot;such NP as {NP,}*{(or|and)} NP&amp;quot; and &amp;quot;NP {, NP}*{,} or other NP&amp;quot;.</S>
    <S sid="52" ssid="17">Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations.</S>
    <S sid="53" ssid="18">They reported an accuracy of about 55% precision on a corpus of 100,000 words.</S>
    <S sid="54" ssid="19">Girju, Badulescu and Moldovan (2003) improved upon this work by using a machine learning filter.</S>
    <S sid="55" ssid="20">Mann (2002) and Fleischman et al. (2003) used part of speech patterns to extract a subset of hyponym relations involing proper nouns.</S>
  </SECTION>
  <SECTION title="3 Labeling Classes" number="3">
    <S sid="56" ssid="1">The research discussed above on discovering hyponym relationships all take a bottom up approach.</S>
    <S sid="57" ssid="2">That is, they use patterns to independently discover semantic relationships of words.</S>
    <S sid="58" ssid="3">However, for infrequent words, these patterns do not match or, worse yet, generate incorrect relationships.</S>
    <S sid="59" ssid="4">Ours is a top down approach.</S>
    <S sid="60" ssid="5">We make use of cooccurrence statistics of semantic classes discovered by algorithms like CBC to label their concepts.</S>
    <S sid="61" ssid="6">Hyponym relationships may then be extracted easily: one hyponym per instance/concept label pair.</S>
    <S sid="62" ssid="7">For example, if we labeled concept (A) from Section 1 with disease, then we could extract is-a relationships such as: diabetes is a disease, cancer is a disease, and lupus is a disease.</S>
    <S sid="63" ssid="8">A concept instance such as lupus is assigned a hypernym disease not because it necessarily occurs in any particular syntactic relationship with disease, but because it belongs to the class of instances that does.</S>
    <S sid="64" ssid="9">The input to our labeling algorithm is a list of semantic classes, in the form of clusters of words, which may be generated from any source.</S>
    <S sid="65" ssid="10">In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).</S>
    <S sid="66" ssid="11">The output of the system is a ranked list of concept names for each semantic class.</S>
    <S sid="67" ssid="12">In the first phase of the algorithm, we extract feature vectors for each word that occurs in a semantic class.</S>
    <S sid="68" ssid="13">Phase II then uses these features to compute grammatical signatures of concepts using the CBC algorithm.</S>
    <S sid="69" ssid="14">Finally, we use simple syntactic patterns to discover class names from each class' signature.</S>
    <S sid="70" ssid="15">Below, we describe these phases in detail.</S>
    <S sid="71" ssid="16">We represent each word (concept instance) by a feature vector.</S>
    <S sid="72" ssid="17">Each feature corresponds to a context in which the word occurs.</S>
    <S sid="73" ssid="18">For example, &amp;quot;catch _&amp;quot; is a verbobject context.</S>
    <S sid="74" ssid="19">If the word wave occurred in this context, then the context is a feature of wave.</S>
    <S sid="75" ssid="20">We first construct a frequency count vector C(e) = (ce1, ce2, &#65533;, cem), where m is the total number of features and cef is the frequency count of feature f occurring in word e. Here, cef is the number of times word e occurred in a grammatical context f. For example, if the word wave occurred 217 times as the object of the verb catch, then the feature vector for wave will have value 217 for its &amp;quot;object-of catch&amp;quot; feature.</S>
    <S sid="76" ssid="21">In Section 4.1, we describe how we obtain these features.</S>
    <S sid="77" ssid="22">We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miem) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: Following (Pantel and Lin 2002), we construct a committee for each semantic class.</S>
    <S sid="78" ssid="23">A committee is a set of representative elements that unambiguously describe the members of a possible class.</S>
    <S sid="79" ssid="24">For each class c, we construct a matrix containing the similarity between each pair of words ei and ej in c using the cosine coefficient of their mutual information vectors (Salton and McGill 1983): For each word e, we then cluster its most similar instances using group-average clustering (Han and Kamber 2001) and we store as a candidate committee the highest scoring cluster c' according to the following metric: where |c' |is the number of elements in c' and avgsim(c') is the average pairwise similarity between words in c'.</S>
    <S sid="80" ssid="25">The assumption is that the best representative for a concept is a large set of very similar instances.</S>
    <S sid="81" ssid="26">The committee for class c is then the highest scoring candidate committee containing only words from c. For example, below are the committee members discovered for the semantic classes (A), (B), and (C) from Section 1: equency count of all features of all words.</S>
    <S sid="82" ssid="27">Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989).</S>
    <S sid="83" ssid="28">A well-known problem is that mutual information is biased towards infrequent elements/features.</S>
    <S sid="84" ssid="29">We therefore multiply mief with the following discounting factor: n m where n is the number of words and N = cef &#215; By averaging the feature vectors of the committee members of a particular semantic class, we obtain a grammatical template, or signature, for that class.</S>
    <S sid="85" ssid="30">For example, Figure 1 shows an excerpt of the grammatical signature for concept (B) in Section 1.</S>
    <S sid="86" ssid="31">The vector is obtained by averaging the feature vectors for the words Curtis Joseph, John Vanbiesbrouck, Mike Richter, and Tommy Salo (the committee of this concept).</S>
    <S sid="87" ssid="32">The &amp;quot;-V:subj:N:sprawl&#65533; feature indicates a subject-verb relationship between the concept and the verb sprawl while &amp;quot;N:appo:N:goaltender&amp;quot; indicates an apposition relationship between the concept and the noun goaltender.</S>
    <S sid="88" ssid="33">The (-) in a relationship means that the right hand side of the relationship is the head (e.g. sprawl is the head of the subject-verb relationship).</S>
    <S sid="89" ssid="34">The two columns of numbers indicate the frequency and mutual information score for each feature respectively.</S>
    <S sid="90" ssid="35">In order to discover the characteristics of human naming conventions, we manually named 50 concepts discovered by CBC.</S>
    <S sid="91" ssid="36">For each concept, we extracted the relationships between the concept committee and the assigned label.</S>
    <S sid="92" ssid="37">We then added the mutual information scores for each extracted relationship among the 50 concepts.</S>
    <S sid="93" ssid="38">The top-4 highest scoring relationships are: To name a class, we simply search for these syntactic relationships in the signature of a concept.</S>
    <S sid="94" ssid="39">We sum up the mutual information scores for each term that occurs in these relationships with a committee of a class.</S>
    <S sid="95" ssid="40">The highest scoring term is the name of the class.</S>
    <S sid="96" ssid="41">For example, the top-5 scoring terms that occurred in these relationships with the signature of the concept represented by the committee {Curtis Joseph, John Vanbiesbrouck, Mike Richter, Tommy Salo} are: goalie 40.37 goaltender 33.64 goalkeeper 19.22 player 14.55 backup 9.40 The numbers are the total mutual information scores of each name in the four syntactic relationships.</S>
  </SECTION>
  <SECTION title="4 Evaluation" number="4">
    <S sid="97" ssid="1">In this section, we present an evaluation of the class labeling algorithm and of the hyponym relationships discovered by our system.</S>
    <S sid="98" ssid="2">We used Minipar (Lin 1994), a broad coverage parser, to parse 3GB of newspaper text from the Aquaint (TREC-9) collection.</S>
    <S sid="99" ssid="3">We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1.</S>
    <S sid="100" ssid="4">We used the 1432 noun clusters extracted by CBC1 as the list of concepts to name.</S>
    <S sid="101" ssid="5">For each concept, we then used our algorithm described in Section 3 to extract the top-20 names for each concept.</S>
    <S sid="102" ssid="6">Out of the 1432 noun concepts, we were unable to name 21 (1.5%) of them.</S>
    <S sid="103" ssid="7">This occurs when a concept's committee members do not occur in any of the four syntactic relationships described in Section 0.</S>
    <S sid="104" ssid="8">We performed a manual evaluation of the remaining 1411 concepts.</S>
    <S sid="105" ssid="9">We randomly selected 125 concepts and their top-5 highest ranking names according to our algorithm.</S>
    <S sid="106" ssid="10">Table 1 shows the first 10 randomly selected concepts (each concept is represented by three of its committee members).</S>
    <S sid="107" ssid="11">For each concept, we added to the list of names a human generated name (obtained from an annotator looking at only the concept instances).</S>
    <S sid="108" ssid="12">We also appended concept names extracted from WordNet.</S>
    <S sid="109" ssid="13">For each concept that contains at least five instances in the WordNet hierarchy, we named the concept with the most frequent common ancestor of each pair of instances.</S>
    <S sid="110" ssid="14">Up to five names were generated by WordNet for each concept.</S>
    <S sid="111" ssid="15">Because of the low coverage of proper nouns in WordNet, only 33 of the 125 concepts we evaluated had WordNet generated labels.</S>
    <S sid="112" ssid="16">We presented to three human judges the 125 randomly selected concepts together with the system, human, and WordNet generated names randomly ordered.</S>
    <S sid="113" ssid="17">That way, there was no way for a judge to know the source of a label nor the system's ranking of the labels.</S>
    <S sid="114" ssid="18">For each name, we asked the judges to assign a score of correct, partially correct, or incorrect.</S>
    <S sid="115" ssid="19">We then computed the mean reciprocal rank (MRR) of the system, human, and WordNet labels.</S>
    <S sid="116" ssid="20">For each concept, a naming scheme receives a score of 1 / M where M is the rank of the first name judged correct.</S>
    <S sid="117" ssid="21">Table 2 shows the results.</S>
    <S sid="118" ssid="22">Table 3 shows similar results for a more lenient evaluation where M is the rank of the first name judged correct or partially correct.</S>
    <S sid="119" ssid="23">Our system achieved an overall MRR score of 77.1%.</S>
    <S sid="120" ssid="24">We performed much better than the baseline WordNet (19.9%) because of the lack of coverage (mostly proper nouns) in the hierarchy.</S>
    <S sid="121" ssid="25">For the 33 concepts that WordNet named, it achieved a score of 75.3% and a lenient score of 82.7%, which is high considering the simple algorithm we used to extract labels using WordNet.</S>
    <S sid="122" ssid="26">The Kappa statistic (Siegel and Castellan Jr. 1988) measures the agreements between a set of judges' assessments correcting for chance agreements: where P(A) is the probability of agreement between the judges and P(E) is the probability that the judges agree by chance on an assessment.</S>
    <S sid="123" ssid="27">An experiment with K &#8805; 0.8 is generally viewed as reliable and 0.67 &lt; K &lt; 0.8 allows tentative conclusions.</S>
    <S sid="124" ssid="28">The Kappa statistic for our experiment is K = 0.72.</S>
    <S sid="125" ssid="29">The human labeling is at a disadvantage since only one label was generated per concept.</S>
    <S sid="126" ssid="30">Therefore, the human scores either 1 or 0 for each concept.</S>
    <S sid="127" ssid="31">Our system's highest ranking name was correct 72% of the time.</S>
    <S sid="128" ssid="32">Table 4 shows the percentage of semantic classes with a correct label in the top 1-5 ranks returned by our system.</S>
    <S sid="129" ssid="33">Overall, 41.8% of the top-5 names extracted by our system were judged correct.</S>
    <S sid="130" ssid="34">The overall accuracy for the top-4, top-3, top-2, and top-1 names are 44.4%, 48.8%, 58.5%, and 72% respectively.</S>
    <S sid="131" ssid="35">Hence, the name ranking of our algorithm is effective.</S>
    <S sid="132" ssid="36">The 1432 CBC concepts contain 18,000 unique words.</S>
    <S sid="133" ssid="37">For each concept to which a word belongs, we extracted up to 3 hyponyms, one for each of the top-3 labels for the concept.</S>
    <S sid="134" ssid="38">The result was 159,000 hyponym relationships.</S>
    <S sid="135" ssid="39">24 are shown in the Appendix.</S>
    <S sid="136" ssid="40">Two judges annotated two random samples of 100 relationships: one from all 159,000 hyponyms and one from the subset of 65,000 proper nouns.</S>
    <S sid="137" ssid="41">For each instance, the judges were asked to decide whether the hyponym relationship was correct, partially correct or incorrect.</S>
    <S sid="138" ssid="42">Table 5 shows the results.</S>
    <S sid="139" ssid="43">The strict measure counts a score of 1 for each correctly judged instance and 0 otherwise.</S>
    <S sid="140" ssid="44">The lenient measure also gives a score of 0.5 for each instance judged partially correct.</S>
    <S sid="141" ssid="45">Many of the CBC concepts contain noise.</S>
    <S sid="142" ssid="46">For example, the wine cluster: Zinfandel, merlot, Pinot noir, Chardonnay, Cabernet Sauvignon, cabernet, riesling, Sauvignon blanc, Chenin blanc, sangiovese, syrah, Grape, Chianti ... contains some incorrect instances such as grape, appelation, and milk chocolate.</S>
    <S sid="143" ssid="47">Each of these instances will generate incorrect hyponyms such as grape is wine and milk chocolate is wine.</S>
    <S sid="144" ssid="48">This hyponym extraction task would likely serve well for evaluating the accuracy of lists of semantic classes.</S>
    <S sid="145" ssid="49">Table 5 shows that the hyponyms involving proper nouns are much more reliable than common nouns.</S>
    <S sid="146" ssid="50">Since WordNet contains poor coverage of proper nouns, these relationships could be useful to enrich it.</S>
    <S sid="147" ssid="51">Semantic extraction tasks are notoriously difficult to evaluate for recall.</S>
    <S sid="148" ssid="52">To approximate recall, we conducted two question answering (QA) tasks: answering definition questions and performing QA information retrieval.</S>
    <S sid="149" ssid="53">We chose the 50 definition questions that appeared in the QA track of TREC2003 (Voorhees, 2003).</S>
    <S sid="150" ssid="54">For example: &amp;quot;Who is Aaron Copland?&amp;quot; and &amp;quot;What is the Kama Sutra?&amp;quot; For each question we looked for at most five corresponding concepts in our hyponym list.</S>
    <S sid="151" ssid="55">For example, for Aaron Copland, we found the following hypernyms: composer, music, and gift.</S>
    <S sid="152" ssid="56">We compared our system with the concepts in WordNet and Fleischman et al. 's instance/concept relations (Fleischman et al.</S>
    <S sid="153" ssid="57">2003).</S>
    <S sid="154" ssid="58">Table 6 shows the percentage of correct answers in the top-1 and top-5 returned answers from each system.</S>
    <S sid="155" ssid="59">All systems seem to have similar performance on the top-1 answers, but our system has many more answers in the top-5.</S>
    <S sid="156" ssid="60">This shows that our system has comparatively higher recall for this task.</S>
  </SECTION>
  <SECTION title="Information (Passage) Retrieval" number="5">
    <S sid="157" ssid="1">Passage retrieval is used in QA to supply relevant information to an answer pinpointing module.</S>
    <S sid="158" ssid="2">The higher the performance of the passage retrieval module, the higher will be the performance of the answer pinpointing module.</S>
    <S sid="159" ssid="3">The passage retrieval module can make use of the hyponym relationships that are discovered by our system.</S>
    <S sid="160" ssid="4">Given a question such as &amp;quot;What color ...&amp;quot;, the likelihood of a correct answer being present in a retrieved passage is greatly increased if we know the set of all possible colors and index them in the document collection appropriately.</S>
    <S sid="161" ssid="5">We used the hyponym relations learned by our system to perform semantic indexing on a QA passage retrieval task.</S>
    <S sid="162" ssid="6">We selected the 179 questions from the QA track of TREC-2003 that had an explicit semantic answer type (e.g.</S>
    <S sid="163" ssid="7">&amp;quot;What band was Jerry Garcia with?&amp;quot; and &amp;quot;What color is the top stripe on the U.S. flag?&amp;quot;).</S>
    <S sid="164" ssid="8">For each expected semantic answer type corresponding to a given question (e.g. band and color), we indexed the entire TREC-2002 IR collection with our system's hyponyms.</S>
    <S sid="165" ssid="9">We compared the passages returned by the passage retrieval module with and without the semantic indexing.</S>
    <S sid="166" ssid="10">We counted how many of the 179 questions had a correct answer returned in the top-1 and top-100 passages.</S>
    <S sid="167" ssid="11">Table 7 shows the results.</S>
    <S sid="168" ssid="12">Our system shows small gains in the performance of the IR output.</S>
    <S sid="169" ssid="13">In the top-1 category, the performance improved by 20%.</S>
    <S sid="170" ssid="14">This may lead to better answer selections.</S>
  </SECTION>
  <SECTION title="5 Conclusions and Future Work" number="6">
    <S sid="171" ssid="1">Current state of the art concept discovery algorithms generate lists of instances of semantic classes but stop short of labeling the classes with concept names.</S>
    <S sid="172" ssid="2">Class labels would serve useful in applications such as question answering to map a question concept into a semantic class and then search for answers within that class.</S>
    <S sid="173" ssid="3">We propose here an algorithm for automatically labeling concepts that searches for syntactic patterns within a grammatical template for a class.</S>
    <S sid="174" ssid="4">Of the 1432 noun concepts discovered by CBC, our system labelled 98.5% of them with an NRR score of 77.1% in a human evaluation.</S>
    <S sid="175" ssid="5">Hyponym relationships were then easily extracted, one for each instance/concept label pair.</S>
    <S sid="176" ssid="6">We extracted 159,000 hyponyms and achieved a precision of 68%.</S>
    <S sid="177" ssid="7">On a subset of 65,000 proper names, our performance was 81.5%.</S>
    <S sid="178" ssid="8">This work forms an important attempt to building large-scale semantic knowledge bases.</S>
    <S sid="179" ssid="9">Without being able to automatically name a cluster and extract hyponym/hypernym relationships, the utility of automatically generated clusters or manually compiled lists of terms is limited.</S>
    <S sid="180" ssid="10">Of course, it is a serious open question how many names each cluster (concept) should have, and how good each name is.</S>
    <S sid="181" ssid="11">Our method begins to address this thorny issue by quantifying the name assigned to a class and by simultaneously assigning a number that can be interpreted to reflect the strength of membership of each element to the class.</S>
    <S sid="182" ssid="12">This is potentially a significant step away from traditional all-or-nothing semantic/ontology representations to a concept representation scheme that is more nuanced and admits multiple names and graded set memberships.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="7">
    <S sid="183" ssid="1">The authors wish to thank the reviewers for their helpful comments.</S>
    <S sid="184" ssid="2">This research was partly supported by NSF grant #EIA-0205111.</S>
  </SECTION>
</PAPER>
