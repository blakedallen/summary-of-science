<PAPER>
  <S sid="0">Enriching The Knowledge Sources Used In A Maximum Entropy Part-Of-Speech Tagger</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.</S>
    <S sid="2" ssid="2">In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.</S>
    <S sid="3" ssid="3">The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words.</S>
  </ABSTRACT>
  <SECTION title="Introduction'" number="1">
    <S sid="4" ssid="1">There are now numerous systems for automatic assignment of parts of speech (&amp;quot;tagging&amp;quot;), employing many different machine learning methods.</S>
    <S sid="5" ssid="2">Among recent top performing methods are Hidden Markov Models (Brants 2000), maximum entropy approaches (Ratnaparkhi 1996), and transformation-based learning (Brill 1994).</S>
    <S sid="6" ssid="3">An overview of these and other approaches can be found in Manning and Schiitze (1999, ch.</S>
    <S sid="7" ssid="4">10).</S>
    <S sid="8" ssid="5">However, all these methods use largely the same information sources for tagging, and often almost the same features as well, and as a consequence they also offer very similar levels of performance.</S>
    <S sid="9" ssid="6">This stands in contrast to the (manually-built) EngCG tagger, which achieves better performance by using lexical and contextual information sources and generalizations beyond those available to such statistical taggers, as Samuelsson and Voutilainen (1997) demonstrate. '</S>
    <S sid="10" ssid="7">We thank Dan Klein and Michael Saunders for useful discussions, and the anonymous reviewers for many helpful comments.</S>
    <S sid="11" ssid="8">This paper explores the notion that automatically built tagger performance can be further improved by expanding the knowledge sources available to the tagger.</S>
    <S sid="12" ssid="9">We pay special attention to unknown words, because the markedly lower accuracy on unknown word tagging means that this is an area where significant performance gains seem possible.</S>
    <S sid="13" ssid="10">We adopt a maximum entropy approach because it allows the inclusion of diverse sources of information without causing fragmentation and without necessarily assuming independence between the predictors.</S>
    <S sid="14" ssid="11">A maximum entropy approach has been applied to partof-speech tagging before (Ratnaparkhi 1996), but the approach's ability to incorporate nonlocal and non-HMM-tagger-type evidence has not been fully explored.</S>
    <S sid="15" ssid="12">This paper describes the models that we developed and the experiments we performed to evaluate them.</S>
  </SECTION>
  <SECTION title="1 The Baseline Maximum Entropy Model" number="2">
    <S sid="16" ssid="1">We started with a maximum entropy based tagger that uses features very similar to the ones proposed in Ratnaparkhi (1996).</S>
    <S sid="17" ssid="2">The tagger learns a loglinear conditional probability model from tagged text, using a maximum entropy method.</S>
    <S sid="18" ssid="3">The model assigns a probability for every tag t in the set T of possible tags given a word and its context h, which is usually defined as the sequence of several words and tags preceding the word.</S>
    <S sid="19" ssid="4">This model can be used for estimating the probability of a tag sequence ti...t&#8222; given a sentence w1.. .w: ,=.</S>
    <S sid="20" ssid="5">As usual, tagging is the process of assigning the maximum likelihood tag sequence to a string of words.</S>
    <S sid="21" ssid="6">The idea of maximum entropy modeling is to choose the probability distribution p that has the highest entropy out of those distributions that satisfy a certain set of constraints.</S>
    <S sid="22" ssid="7">The constraints restrict the model to behave in accordance with a set of statistics collected from the training data.</S>
    <S sid="23" ssid="8">The statistics are expressed as the expected values of appropriate functions defined on the contexts h and tags t. In particular, the constraints demand that the expectations of the features for the model match the empirical expectations of the features over the training data.</S>
    <S sid="24" ssid="9">For example, if we want to constrain the model to tag make as a verb or noun with the same frequency as the empirical model induced by the training data, we define the features: fl(h,t) = 1 iff w = make and t = NN f2(h,t) = 1 iff w, = make and t = VB Some commonly used statistics for part of speech tagging are: how often a certain word was tagged in a certain way; how often two tags appeared in sequence or how often three tags appeared in sequence.</S>
    <S sid="25" ssid="10">These look a lot like the statistics a Markov Model would use.</S>
    <S sid="26" ssid="11">However, in the maximum entropy framework it is possible to easily define and incorporate much more complex statistics, not restricted to n-gram sequences.</S>
    <S sid="27" ssid="12">The constraints in our model are that the expectations of these features according to the joint distribution p are equal to the expectations of the features in the empirical (training data) distribution E p(h,,) (h,t) = Ei5(h,t) (h, t) Having defined a set of constraints that our model should accord with, we proceed to find the model satisfying the constraints that maximizes the conditional entropy of p. The intuition is that such a model assumes nothing apart from that it should satisfy the given constraints.</S>
    <S sid="28" ssid="13">Following Berger et al. (1996), we approximate p(h,t) , the joint distribution of contexts and tags, by the product of r, (h), the empirical distribution of histories h, and the conditional distribution p(t I h): p(h,t) p(h) p(t I h) .</S>
    <S sid="29" ssid="14">Then for the example above, our constraints would be the following, for j e {1,2}: This approximation is used to enable efficient computation.</S>
    <S sid="30" ssid="15">The expectation for a feature f is: where H is the space of possible contexts h when predicting a part of speech tag t. Since the contexts contain sequences of words and tags and other information, the space H is huge.</S>
    <S sid="31" ssid="16">But using this approximation, we can instead sum just over the smaller space of observed contexts X in the training sample, because the empirical prior i5(h) is zero for unseen contexts h: The model that is a solution to this constrained optimization task is an exponential (or equivalently, loglinear) model with the parametric form: where the denominator is a normalizing term (sometimes referred to as the partition function).</S>
    <S sid="32" ssid="17">The parameters Xi correspond to weights for the features fj.</S>
    <S sid="33" ssid="18">We will not discuss in detail the characteristics of the model or the parameter estimation procedure used &#8212; Improved Iterative Scaling.</S>
    <S sid="34" ssid="19">For a more extensive discussion of maximum entropy methods, see Berger et al. (1996) and Jelinek (1997).</S>
    <S sid="35" ssid="20">However, we note that our parameter estimation algorithm directly uses equation (1).</S>
    <S sid="36" ssid="21">Ratnaparkhi (1996: 134) suggests use of an approximation summing over the training data, which does not sum over possible tags: However, we believe this passage is in error: such an estimate is ineffective in the iterative scaling algorithm.</S>
    <S sid="37" ssid="22">Further, we note that expectations of the form (1) appear in Ratnaparkhi (1998: 12).</S>
    <S sid="38" ssid="23">In our baseline model, the context available when predicting the part of speech tag of a word wi in a sentence of words {w1... wn} with tags {t1... t&#8222;) is { wi.,11.</S>
    <S sid="39" ssid="24">The features that define the constraints on the model are obtained by instantiation of feature templates as in Ratnaparkhi (1996).</S>
    <S sid="40" ssid="25">Special feature templates exist for rare words in the training data, to increase the model's prediction capacity for unknown words.</S>
    <S sid="41" ssid="26">The actual feature templates for this model are shown in the next table.</S>
    <S sid="42" ssid="27">They are a subset of the features used in Ratnaparkhi (1996).</S>
    <S sid="43" ssid="28">No.</S>
    <S sid="44" ssid="29">Feature Type Template General feature templates can be instantiated by arbitrary contexts, whereas rare feature templates are instantiated only by histories where the current word wi is rare.</S>
    <S sid="45" ssid="30">Rare words are defined to be words that appear less than a certain number of times in the training data (here, the value 7 was used).</S>
    <S sid="46" ssid="31">In order to be able to throw out features that would give misleading statistics due to sparseness or noise in the data, we use two different cutoff values for general and rare feature templates (in this implementation, 5 and 45 respectively).</S>
    <S sid="47" ssid="32">As seen in Table 1 the features are conjunctions of a boolean function on the history h and a boolean function on the tag t. Features whose first conjuncts are true for more than the corresponding threshold number of histories in the training data are included in the model.</S>
    <S sid="48" ssid="33">The feature templates in Ratnaparkhi (1996) that were left out were the ones that look at the previous word, the word two positions before the current, and the word two positions after the current.</S>
    <S sid="49" ssid="34">These features are of the same form as template 4 in Table 1, but they look at words in different positions.</S>
    <S sid="50" ssid="35">Our motivation for leaving these features out was the results from some experiments on successively adding feature templates.</S>
    <S sid="51" ssid="36">Adding template 4 to a model that incorporated the general feature templates 1 to 3 only and the rare feature templates 5-8 significantly increased the accuracy on the development set &#8212; from 96.0% to 96.52%.</S>
    <S sid="52" ssid="37">The addition of a feature template that looked at the preceding word and the current tag to the resulting model slightly reduced the accuracy.</S>
    <S sid="53" ssid="38">The model was trained and tested on the part-ofspeech tagged WSJ section of the Penn Treebank.</S>
    <S sid="54" ssid="39">The data was divided into contiguous parts: sections 0-20 were used for training, sections 21-22 as a development test set, and sections 23-24 as a final test set.</S>
    <S sid="55" ssid="40">The data set sizes are shown below together with numbers of unknown words.</S>
    <S sid="56" ssid="41">The testing procedure uses a beam search to find the tag sequence with maximal probability given a sentence.</S>
    <S sid="57" ssid="42">In our experiments we used a beam of size 5.</S>
    <S sid="58" ssid="43">Increasing the beam size did not result in improved accuracy.</S>
    <S sid="59" ssid="44">The preceding tags for the word at the beginning of the sentence are regarded as having the pseudo-tag NA.</S>
    <S sid="60" ssid="45">In this way, the information that a word is the first word in a sentence is available to the tagger.</S>
    <S sid="61" ssid="46">We do not have a special end-of-sentence symbol.</S>
    <S sid="62" ssid="47">We used a tag dictionary for known words in testing.</S>
    <S sid="63" ssid="48">This was built from tags found in the training data but augmented so as to capture a few basic systematic tag ambiguities that are found in English.</S>
    <S sid="64" ssid="49">Namely, for regular verbs the -ed form can be either a VBD or a VBN and similarly the stem form can be either a VBP or VB.</S>
    <S sid="65" ssid="50">Hence for words that had occurred with only one of these tags in the training data the other was also included as. possible for assignment.</S>
    <S sid="66" ssid="51">The results on the test set for the Baseline model are shown in Table 3.</S>
    <S sid="67" ssid="52">This table also shows the results reported in Ratnaparkhi (1996: 142) for convenience.</S>
    <S sid="68" ssid="53">The accuracy figure for our model is higher overall but lower for unknown words.</S>
    <S sid="69" ssid="54">This may stem from the differences between the two models' feature templates, thresholds, and approximations of the expected values for the features, as discussed in the beginning of the section, or may just reflect differences in the choice of training and test sets (which are not precisely specified in Ratnaparkhi (1996)).</S>
    <S sid="70" ssid="55">The differences are not great enough to justify any definite statement about the different use of feature templates or other particularities of the model estimation.</S>
    <S sid="71" ssid="56">One conclusion that we can draw is that at present the additional word features used in Ratnaparkhi (1996) &#8212; looking at words more than one position away from the current &#8212; do not appear to be helping the overall performance of the models.</S>
    <S sid="72" ssid="57">A large number of words, including many of the most common words, can have more than one syntactic category.</S>
    <S sid="73" ssid="58">This introduces a lot of ambiguities that the tagger has to resolve.</S>
    <S sid="74" ssid="59">Some of the ambiguities are easier for taggers to resolve and others are harder.</S>
    <S sid="75" ssid="60">Some of the most significant confusions that the Baseline model made on the test set can be seen in Table 5.</S>
    <S sid="76" ssid="61">The row labels in Table 5 signify the correct tags, and the column labels signify the assigned tags.</S>
    <S sid="77" ssid="62">For example, the number 244 in the (NN, JJ) position is the number of words that were NNs but were incorrectly assigned the JJ category.</S>
    <S sid="78" ssid="63">These particular confusions, shown in the table, account for a large percentage of the total error (2652/3651 = 72.64%).</S>
    <S sid="79" ssid="64">Table 6 shows part of the Baseline model's confusion matrix for just unknown words.</S>
    <S sid="80" ssid="65">Table 4 shows the Baseline model's overall assignment accuracies for different parts of speech.</S>
    <S sid="81" ssid="66">For example, the accuracy on nouns is greater than the accuracy on adjectives.</S>
    <S sid="82" ssid="67">The accuracy on NNPS (plural proper nouns) is a surprisingly low 41.1%.</S>
    <S sid="83" ssid="68">Tagger errors are of various types.</S>
    <S sid="84" ssid="69">Some are the result of inconsistency in labeling in the training data (Ratnaparkhi 1996), which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context.</S>
    <S sid="85" ssid="70">For instance, the status of various noun premodifiers (whether chief or maximum is NN or JJ, or whether a word in -ing is acting as a JJ or VBG) is of this type.</S>
    <S sid="86" ssid="71">Some, such as errors between NN/NNP/NNPS/NNS largely reflect difficulties with unknown words.</S>
    <S sid="87" ssid="72">But other cases, such as VBNNBD and VBNBP/NN, represent systematic tag ambiguity patterns in English, for which the right answer is invariably clear in context, and for which there are in general good structural contextual clues that one should be able to use to disambiguate.</S>
    <S sid="88" ssid="73">Finally, in another class of cases, of which the most prominent is probably the RP/IN/RB ambiguity of words like up, out, and on, the linguistic distinctions, while having a sound empirical basis (e.g., see Baker (1995: 198-201), are quite subtle, and often require semantic intuitions.</S>
    <S sid="89" ssid="74">There are not good syntactic cues for the correct tag (and furthermore, human taggers not infrequently make errors).</S>
    <S sid="90" ssid="75">Within this classification, the greatest hopes for tagging improvement appear to come from minimizing errors in the second and third classes of this classification.</S>
    <S sid="91" ssid="76">In the following sections we discuss how we include additional knowledge sources to help in the assignment of tags to forms of verbs, capitalized unknown words, particle words, and in the overall accuracy of part of speech assignments.</S>
  </SECTION>
  <SECTION title="2 Improving the Unknown Words Model" number="3">
    <S sid="92" ssid="1">The accuracy of the baseline model is markedly lower for unknown words than for previously seen ones.</S>
    <S sid="93" ssid="2">This is also the case for all other taggers, and reflects the importance of lexical information to taggers: in the best accuracy figures published for corpus-based taggers, known word accuracy is around 97%, whereas unknown word accuracy is around 85%.</S>
    <S sid="94" ssid="3">In following experiments, we examined ways of using additional features to improve the accuracy of tagging unknown words.</S>
    <S sid="95" ssid="4">As previously discussed in Mikheev (1999), it is possible to improve the accuracy on capitalized words that might be proper nouns or the first word in a sentence, etc.</S>
    <S sid="96" ssid="5">For example, the error on the proper noun category (NNP) accounts for a significantly larger percent of the total error for unknown words than for known words.</S>
    <S sid="97" ssid="6">In the Baseline model, of the unknown word error 41.3% is due to words being NNP and assigned to some other category, or being of other category and assigned NNP.</S>
    <S sid="98" ssid="7">The percentage of the same type of error for known words is 16.2%.</S>
    <S sid="99" ssid="8">The incorporation of the following two feature schemas greatly improved NNP accuracy: Conversely, empirically it was found that the prefix features for rare words were having a net negative effect on accuracy.</S>
    <S sid="100" ssid="9">We do not at present have a good explanation for this phenomenon.</S>
    <S sid="101" ssid="10">The addition of the features (1) and (2) and the removal of the prefix features considerably improved the accuracy on unknown words and the overall accuracy.</S>
    <S sid="102" ssid="11">The results on the test set after adding these features are shown below: 96.76% 86.76% Unknown word error is reduced by 15% as compared to the Baseline model.</S>
    <S sid="103" ssid="12">It is important to note that (2) is composed of information already 'known' to the tagger in some sense.</S>
    <S sid="104" ssid="13">This feature can be viewed as the conjunction of two features, one of which is already in the baseline model, and the other of which is the negation of a feature existing in the baseline model &#8212; since for words at the beginning of a sentence, the preceding tag is the pseudo-tag NA, and there is a feature looking at the preceding tag.</S>
    <S sid="105" ssid="14">Even though our maximum entropy model does not require independence among the predictors, it provides for free only a simple combination of feature weights, and additional 'interaction terms' are needed to model non-additive interactions (in log-space terms) between features.</S>
  </SECTION>
  <SECTION title="3 Features for Disambiguating Verb Forms" number="4">
    <S sid="106" ssid="1">Two of the most significant sources of classifier errors are the VBN/VBD ambiguity and the VBP/VB ambiguity.</S>
    <S sid="107" ssid="2">As seen in Table 5, VBNNBD confusions account for 6.9% of the total word error.</S>
    <S sid="108" ssid="3">The VBP/VB confusions are a smaller 3.7% of the errors.</S>
    <S sid="109" ssid="4">In many cases it is easy for people (and for taggers) to determine the correct form.</S>
    <S sid="110" ssid="5">For example, if there is a to infinitive or a modal directly preceding the VB/VBP ambiguous word, the form is certainly non-finite.</S>
    <S sid="111" ssid="6">But often the modal can be several positions away from the current position &#8212; still obvious to a human, but out of sight for the baseline model.</S>
    <S sid="112" ssid="7">To help resolve a VB/VBP ambiguity in such cases, we can add a feature that looks at the preceding several words (we have chosen 8 as a threshold), but not across another verb, and activates if there is a to there, a modal verb, or a form of do, let, make, or help (verbs that frequently take a bare infinitive complement).</S>
    <S sid="113" ssid="8">Rather than having a separate feature look at each preceding position, we define one feature that looks at the chosen number of positions to the left.</S>
    <S sid="114" ssid="9">This both increases the scope of the available history for the tagger and provides a better statistic because it avoids fragmentation.</S>
    <S sid="115" ssid="10">We added a similar feature for resolving VBDNBN confusions.</S>
    <S sid="116" ssid="11">It activates if there is a have or be auxiliary form in the preceding several positions (again the value 8 is used in the implementation).</S>
    <S sid="117" ssid="12">The form of these two feature templates was motivated by the structural rules of English and not induced from the training data, but it should be possible to look for &amp;quot;predictors&amp;quot; for certain parts of speech in the preceding words in the sentence by, for example, computing association strengths.</S>
    <S sid="118" ssid="13">The addition of the two feature schemas helped reduce the VBNBP and VBD/VBN confusions.</S>
    <S sid="119" ssid="14">Below is the performance on the test set of the resulting model when features for disambiguating verb forms are added to the model of Section 2.</S>
    <S sid="120" ssid="15">The number of VBNBP confusions was reduced by 23.1% as compared to the baseline.</S>
    <S sid="121" ssid="16">The number of VBD/VBN confusions was reduced by 12.3%.</S>
    <S sid="122" ssid="17">96.83% 86.87%</S>
  </SECTION>
  <SECTION title="4 Features for Particle Disambiguation" number="5">
    <S sid="123" ssid="1">As discussed in section 1.3 above, the task of determining RB/RP/IN tags for words like down, out, up is difficult and in particular examples, there are often no good local syntactic indicators.</S>
    <S sid="124" ssid="2">For instance, in (2), we find the exact same sequence of parts of speech, but (2a) is a particle use of on, while (2b) is a prepositional use.</S>
    <S sid="125" ssid="3">Consequently, the accuracy on the rarer RP (particles) category is as low as 41.5% for the Baseline model (cf.</S>
    <S sid="126" ssid="4">Table 4).</S>
    <S sid="127" ssid="5">(2) a. Kim took on the monster. b. Kim sat on the monster.</S>
    <S sid="128" ssid="6">We tried to improve the tagger's capability to resolve these ambiguities through adding information on verbs' preferences to take specific words as particles, or adverbs, or prepositions.</S>
    <S sid="129" ssid="7">There are verbs that take particles more than others, and particular words like out are much more likely to be used as a particle in the context of some verb than other words ambiguous between these tags.</S>
    <S sid="130" ssid="8">We added two different feature templates to capture this information, consisting as usual of a predicate on the history h, and a condition on the tag t. The first predicate is true if the current word is often used as a particle, and if there is a verb at most 3 positions to the left, which is &amp;quot;known&amp;quot; to have a good chance of taking the current word as a particle.</S>
    <S sid="131" ssid="9">The verb-particle pairs that are known by the system to be very common were collected through analysis of the training data in a preprocessing stage.</S>
    <S sid="132" ssid="10">The second feature template has the form: The last verb is v and the current word is w and w has been tagged as a particle and the current tag is t. The last verb is the pseudo-symbol NA if there is no verb in the previous three positions.</S>
    <S sid="133" ssid="11">These features were some help in reducing the RB/IN/RP confusions.</S>
    <S sid="134" ssid="12">The accuracy on the RP category rose to 44.3%.</S>
    <S sid="135" ssid="13">Although the overall confusions in this class were reduced, some of the errors were increased, for example, the number of INs classified as RBs rose slightly.</S>
    <S sid="136" ssid="14">There seems to be still considerable room to improve these results, though the attainable accuracy is limited by the accuracy with which these distinctions are marked in the Penn Treebank (on a quick informal study, this accuracy seems to be around 85%).</S>
    <S sid="137" ssid="15">The next table shows the final performance on the test set.</S>
    <S sid="138" ssid="16">For ease of comparison, the accuracies of all models on the test and development sets are shown in Table 7.</S>
    <S sid="139" ssid="17">We note that accuracy is lower on the development set.</S>
    <S sid="140" ssid="18">This presumably corresponds with Charniak's (2000: 136) observation that Section 23 of the Penn Treebank is easier than some others.</S>
    <S sid="141" ssid="19">Table 8 shows the different number of feature templates of each kind that have been instantiated for the different models as well as the total number of features each model has.</S>
    <S sid="142" ssid="20">It can be seen that the features which help disambiguate verb forms, which look at capitalization and the first of the feature templates for particles are a very small number as compared to the features of the other kinds.</S>
    <S sid="143" ssid="21">The improvement in classification accuracy therefore comes at the price of adding very few parameters to the maximum entropy model and does not result in increased model complexity.</S>
  </SECTION>
  <SECTION title="Conclusion" number="6">
    <S sid="144" ssid="1">Even when the accuracy figures for corpus-based part-of-speech taggers start to look extremely similar, it is still possible to move performance levels up.</S>
    <S sid="145" ssid="2">The work presented in this paper explored just a few information sources in addition to the ones usually used for tagging.</S>
    <S sid="146" ssid="3">While progress is slow, because each new feature applies only to a limited range of cases, nevertheless the improvement in accuracy as compared to previous results is noticeable, particularly for the individual decisions on which we focused.</S>
    <S sid="147" ssid="4">The potential of maximum entropy methods has not previously been fully exploited for the task of assignment of parts of speech.</S>
    <S sid="148" ssid="5">We incorporated into a maximum entropy-based tagger more linguistically sophisticated features, which are non-local and do not look just at particular positions in the text.</S>
    <S sid="149" ssid="6">We also added features that model the interactions of previously employed predictors.</S>
    <S sid="150" ssid="7">All of these changes led to modest increases in tagging accuracy.</S>
    <S sid="151" ssid="8">This paper has thus presented some initial experiments in improving tagger accuracy through using additional information sources.</S>
    <S sid="152" ssid="9">In the future we hope to explore automatically discovering information sources that can be profitably incorporated into maximum entropy part-of-speech prediction.</S>
  </SECTION>
</PAPER>
