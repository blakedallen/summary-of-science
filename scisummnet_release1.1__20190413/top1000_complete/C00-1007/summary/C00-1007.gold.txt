Exploiting A Probabilistic Hierarchical Model For Generation
Previous stochastic approaches to generation do not include a tree-based representation of syntax.
While this may be adequate or even advantageous for some applications, other applications profit from using as much syntactic knowledge as is available, leaving to a stochastic model only those issues that are not determined by the grammar.
We present initial results showing that a tree-based model derived from a tree-annotated corpus improves on a tree model derived from an unannotated corpus, and that a tree-based stochastic model with a hand-crafted grammar outperforms both.
Our system, FERGUS takes dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus.
The Fergus system employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees.
