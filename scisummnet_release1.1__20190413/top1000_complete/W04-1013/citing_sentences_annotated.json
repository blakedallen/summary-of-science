[
  {
    "citance_No": 1, 
    "citing_paper_id": "P04-1077", 
    "citing_paper_authority": 48, 
    "citing_paper_authors": "Chin-Yew, Lin | Franz Josef, Och", 
    "raw_text": "ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004)", 
    "clean_text": "ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-2052", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Yuta, Kikuchi | Tsutomu, Hirao | Hiroya, Takamura | Manabu, Okumura | Masaaki, Nagata", 
    "raw_text": "We used ROUGE (Lin, 2004) as an evaluation criterion", 
    "clean_text": "We used ROUGE (Lin, 2004) as an evaluation criterion.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N10-1012", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "David, Newman | Jey Han, Lau | Karl, Grieser | Timothy, Baldwin", 
    "raw_text": "Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al, 2002) and automatic summarisation (Lin, 2004)", 
    "clean_text": "Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al, 2002) and automatic summarisation (Lin, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "S12-1091", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Eric, Yeh | Eneko, Agirre", 
    "raw_text": "1, with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure", 
    "clean_text": "with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "C08-1019", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "John M., Conroy | Hoa Trang, Dang", 
    "raw_text": "ROUGE (Lin, 2004) and its linguistically motivated descendent, Basic Elements (BE) (Hovy et al, 2005), evaluate a summary by computing its overlap with a set of model (human) summaries; ROUGE considers lexical n-grams as the unit for comparing the overlap between summaries, while Basic Elements uses larger units of comparison based on the output of syntactic parsers", 
    "clean_text": "ROUGE (Lin, 2004) and its linguistically motivated descendent, Basic Elements (BE) (Hovy et al, 2005), evaluate a summary by computing its overlap with a set of model (human) summaries;.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "N06-2006", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Pierre, Chatain | Edward, Whittaker | Joanna, Mrozinski | Sadaoki, Furui", 
    "raw_text": "Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results.ROUGE F-measure scores are given for ROUGE 2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4 (skip-bigram), using the model average (average score across all references) metric", 
    "clean_text": "Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W08-1113", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Mary Ellen, Foster", 
    "raw_text": "Such metrics have been introduced in other fields, including PAR ADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation,1 and ROUGE (Lin, 2004) for sum marisation", 
    "clean_text": "Such metrics have been introduced in other fields, including PAR ADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N12-1041", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Yun-Nung, Chen | Florian, Metze", 
    "raw_text": "Automated evaluation utilizes the standard DUCevaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system generated summary against a set of human generated peer summaries", 
    "clean_text": "Automated evaluation utilizes the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system generated summary against a set of human generated peer summaries.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W08-1808", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Richard, Shaw | Ben, Solway | Robert J., Gaizauskas | Mark A., Greenwood", 
    "raw_text": "We considered a variety of tools like ROUGE (Lin, 2004) and ME TEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task", 
    "clean_text": "We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P13-1039", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jackie Chi Kit, Cheung | Gerald, Penn", 
    "raw_text": "For the evaluation measure, we used the standard ROUGE suite of automatic evaluation measures (Lin, 2004)", 
    "clean_text": "For the evaluation measure, we used the standard ROUGE suite of automatic evaluation measures (Lin, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "N09-1066", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Saif, Mohammad | Bonnie Jean, Dorr | Melissa, Egan | Ahmed, Hassan | Pradeep, Muthukrishnan | Vahed, Qazvinian | Dragomir R., Radev | David, Zajic", 
    "raw_text": "6.1.2 ROUGE evaluation Table 4 presents ROUGE scores (Lin, 2004) of each of human-generated 250-word surveys against each other", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N07-1005", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Kiyotaka, Uchimoto | Katsunori, Kotani | Yujie, Zhang | Hitoshi, Isahara", 
    "raw_text": "In our research, 23 scores, namely BLEU (Papineni et al, 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al, 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (ex act) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al, 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4and4 variants (LCS, S?, SU?, W-1.2), were used to calculate each similarity S i. Therefore, the value of m. in Eq", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P11-1049", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Dan, Gillick | Dan, Klein", 
    "raw_text": "Theyreport a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004)", 
    "clean_text": "They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P11-1049", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Dan, Gillick | Dan, Klein", 
    "raw_text": "ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004)", 
    "clean_text": "ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W06-1643", 
    "citing_paper_authority": 23, 
    "citing_paper_authors": "Michel, Galley", 
    "raw_text": "Empirical evaluations using two standard summarization metrics? the Pyra mid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)? show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%", 
    "clean_text": "Empirical evaluations using two standard summarization metrics the Pyra mid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004) show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W06-1643", 
    "citing_paper_authority": 23, 
    "citing_paper_authors": "Michel, Galley", 
    "raw_text": "Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)", 
    "clean_text": "Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D11-1042", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Enrique, Amig&oacute; | Julio, Gonzalo | Jes&uacute;s, Gim&eacute;nez | M. Felisa, Verdejo", 
    "raw_text": "Recent work suggests exploiting external knowledge sources and/or deep linguistic an notation, and measure combination (see Section 2) .However, original measures based on lexical matching, such as BLEU (Papineni et al, 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively", 
    "clean_text": "However, original measures based on lexical matching, such as BLEU (Papineni et al, 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D11-1042", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Enrique, Amig&oacute; | Julio, Gonzalo | Jes&uacute;s, Gim&eacute;nez | M. Felisa, Verdejo", 
    "raw_text": "In the case of automatic summarization (AS), we have employed the standard variants of ROUGE (Lin, 2004)", 
    "clean_text": "In the case of automatic summarization (AS), we have employed the standard variants of ROUGE (Lin, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P10-2060", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Hitoshi, Nishikawa | Takaaki, Hasegawa | Yoshihiro, Matsuo | Genichiro, Kikui", 
    "raw_text": "We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability measures", 
    "clean_text": "We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability measures.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P10-2060", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Hitoshi, Nishikawa | Takaaki, Hasegawa | Yoshihiro, Matsuo | Genichiro, Kikui", 
    "raw_text": "We used ROUGE (Lin, 2004) for evaluating the content of summaries", 
    "clean_text": "We used ROUGE (Lin, 2004) for evaluating the content of summaries.", 
    "keep_for_gold": 0
  }
]
