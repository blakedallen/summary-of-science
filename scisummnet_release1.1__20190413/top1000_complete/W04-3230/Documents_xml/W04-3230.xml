<PAPER>
  <S sid="0">Applying Conditional Random Fields To Japanese Morphological Analysis</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper presents Japanese morphological analysis based on conditional random fields (CRFs).</S>
    <S sid="2" ssid="2">Previous work in CRFs assumed that observation sequence (word) boundaries were fixed.</S>
    <S sid="3" ssid="3">However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible.</S>
    <S sid="4" ssid="4">We show how CRFs can be applied to situations where word boundary ambiguity exists.</S>
    <S sid="5" ssid="5">CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis.</S>
    <S sid="6" ssid="6">First, flexible feature designs for hierarchical tagsets become possible.</S>
    <S sid="7" ssid="7">Second, influences of label and length bias are minimized.</S>
    <S sid="8" ssid="8">We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task.</S>
    <S sid="9" ssid="9">Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="10" ssid="1">Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features.</S>
    <S sid="11" ssid="2">They are considered to be the state-of-the-art framework to date.</S>
    <S sid="12" ssid="3">Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004).</S>
    <S sid="13" ssid="4">Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label sequence (e.g. part-of-speech).</S>
    <S sid="14" ssid="5">However, word boundaries are not clear in non-segmented languages.</S>
    <S sid="15" ssid="6">One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages.</S>
    <S sid="16" ssid="7">In this paper, we show how CRFs can be applied to situations where word boundary ambiguity exists.</S>
    <S sid="17" ssid="8">CRFs offer a solution to the problems in Japanese morphological analysis with hidden Markov models (HMMs) (e.g., (Asahara and Matsumoto, 2000)) or with maximum entropy Markov models (MEMMs) (e.g., (Uchimoto et al., 2001)).</S>
    <S sid="18" ssid="9">First, as HMMs are generative, it is hard to employ overlapping features stemmed from hierarchical tagsets and nonindependent features of the inputs such as surrounding words, word suffixes and character types.</S>
    <S sid="19" ssid="10">These features have usually been ignored in HMMs, despite their effectiveness in unknown word guessing.</S>
    <S sid="20" ssid="11">Second, as mentioned in the literature, MEMMs could evade neither from label bias (Lafferty et al., 2001) nor from length bias (a bias occurring because of word boundary ambiguity).</S>
    <S sid="21" ssid="12">Easy sequences with low entropy are likely to be selected during decoding in MEMMs.</S>
    <S sid="22" ssid="13">The consequence is serious especially in Japanese morphological analysis due to hierarchical tagsets as well as word boundary ambiguity.</S>
    <S sid="23" ssid="14">The key advantage of CRFs is their flexibility to include a variety of features while avoiding these bias.</S>
    <S sid="24" ssid="15">In what follows, we describe our motivations of applying CRFs to Japanese morphological analysis (Section 2).</S>
    <S sid="25" ssid="16">Then, CRFs and their parameter estimation are provided (Section 3).</S>
    <S sid="26" ssid="17">Finally, we discuss experimental results (Section 4) and give conclusions with possible future directions (Section 5).</S>
  </SECTION>
  <SECTION title="2 Japanese Morphological Analysis" number="2">
    <S sid="27" ssid="1">Word boundary ambiguity cannot be ignored when dealing with non-segmented languages.</S>
    <S sid="28" ssid="2">A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al., 2004).</S>
    <S sid="29" ssid="3">Input: &#8220; &#8221; (I live in Metropolis of Tokyo .)</S>
    <S sid="30" ssid="4">However, B/I tagging is not a standard method in 20-year history of corpus-based Japanese morphological analysis.</S>
    <S sid="31" ssid="5">This is because B/I tagging cannot directly reflect lexicons which contain prior knowledge about word segmentation.</S>
    <S sid="32" ssid="6">We cannot ignore a lexicon since over 90% accuracy can be achieved even using the longest prefix matching with the lexicon.</S>
    <S sid="33" ssid="7">Moreover, B/I tagging produces a number of redundant candidates which makes the decoding speed slower.</S>
    <S sid="34" ssid="8">Traditionally in Japanese morphological analysis, we assume that a lexicon, which lists a pair of a word and its corresponding part-of-speech, is available.</S>
    <S sid="35" ssid="9">The lexicon gives a tractable way to build a lattice from an input sentence.</S>
    <S sid="36" ssid="10">A lattice represents all candidate paths or all candidate sequences of tokens, where each token denotes a word with its partof-speech 1.</S>
    <S sid="37" ssid="11">Figure 1 shows an example where a total of 6 candidate paths are encoded and the optimal path is marked with bold type.</S>
    <S sid="38" ssid="12">As we see, the set of labels to predict and the set of states in the lattice are different, unlike English part-of-speech tagging that word boundary ambiguity does not exist.</S>
    <S sid="39" ssid="13">Formally, the task of Japanese morphological analysis can be defined as follows.</S>
    <S sid="40" ssid="14">Let x be an input, unsegmented sentence.</S>
    <S sid="41" ssid="15">Let y be a path, a sequence of tokens where each token is a pair of word wi and its part-of-speech ti.</S>
    <S sid="42" ssid="16">In other words, y = ((w1, t1), ... , (w#y, t#y)) where #y is the number of tokens in the path y.</S>
    <S sid="43" ssid="17">Let Y(x) be a set of candidate paths in a lattice built from the input sentence x and a lexicon.</S>
    <S sid="44" ssid="18">The goal is to select a correct path y&#710; from all candidate paths in the Y(x).</S>
    <S sid="45" ssid="19">The distinct property of Japanese morphological analysis is that the number of tokens y varies, since the set of labels and the set of states are not the same.</S>
    <S sid="46" ssid="20">Japanese part-of-speech (POS) tagsets used in the two major Japanese morphological analyzers ChaSen2 and JUMAN3 take the form of a hierarchical structure.</S>
    <S sid="47" ssid="21">For example, IPA tagset4 used in ChaSen consists of three categories: part-ofspeech, conjugation form (cform), and conjugate type (ctype).</S>
    <S sid="48" ssid="22">The cform and ctype are assigned only to words that conjugate, such as verbs and adjectives.</S>
    <S sid="49" ssid="23">The part-of-speech has at most four levels of subcategories.</S>
    <S sid="50" ssid="24">The top level has 15 different categories, such as Noun, Verb, etc.</S>
    <S sid="51" ssid="25">Noun is subdivided into Common Noun, Proper Noun and so on.</S>
    <S sid="52" ssid="26">Proper Noun is again subdivided into Person, Organization or Place, etc.</S>
    <S sid="53" ssid="27">The bottom level can be thought as the word level (base form) with which we can completely discriminate all words as different POS.</S>
    <S sid="54" ssid="28">If we distinguish each branch of the hierarchical tree as a different label (ignoring the word level), the total number amounts to about 500, which is much larger than the typical English POS tagset such as Penn Treebank.</S>
    <S sid="55" ssid="29">The major effort has been devoted how to interpolate each level of the hierarchical structure as well as to exploit atomic features such as word suffixes and character types.</S>
    <S sid="56" ssid="30">If we only use the bottom level, we suffer from the data sparseness problem.</S>
    <S sid="57" ssid="31">On the other hand, if we use the top level, we lack in granularity of POS to capture fine differences.</S>
    <S sid="58" ssid="32">For instance, some suffixes (e.g., san or kun) appear after names, and are helpful to detect words with Name POS.</S>
    <S sid="59" ssid="33">In addition, the conjugation form (cfrom) must be distinguished appearing only in the succeeding position in a bi-gram, since it is dominated by the word appearing in the next.</S>
    <S sid="60" ssid="34">Asahara et al. extended HMMs so as to incorporate 1) position-wise grouping, 2) word-level statistics, and 3) smoothing of word and POS level statistics (Asahara and Matsumoto, 2000).</S>
    <S sid="61" ssid="35">However, the proposed method failed to capture non-independent features such as suffixes and character types and selected smoothing parameters in an ad-hoc way.</S>
    <S sid="62" ssid="36">It is known that maximum entropy Markov models (MEMMs) (McCallum et al., 2000) or other discriminative models with independently trained nextstate classifiers potentially suffer from the label bias (Lafferty et al., 2001) and length bias.</S>
    <S sid="63" ssid="37">In Japanese morphological analysis, they are extremely serious problems.</S>
    <S sid="64" ssid="38">This is because, as shown in Figure 1, the branching variance is considerably high, and the number of tokens varies according to the output path.</S>
    <S sid="65" ssid="39">An example of the label bias is illustrated in Figure 2:(a) where the path is searched by sequential combinations of maximum entropy models (MEMMs), i.e., if MEMMs learn the correct path A-D with independently trained maximum entropy models, the path B-E will have a higher probability and then be selected in decoding.</S>
    <S sid="66" ssid="40">This is because the token B has only the single outgoing token E, and the transition probability for B-E is always 1.0.</S>
    <S sid="67" ssid="41">Generally speaking, the complexities of transitions vary according to the tokens, and the transition probabilities with low-entropy will be estimated high in decoding.</S>
    <S sid="68" ssid="42">This problem occurs because the training is performed only using the correct path, ignoring all other transitions.</S>
    <S sid="69" ssid="43">Moreover, we cannot ignore the influence of the length bias either.</S>
    <S sid="70" ssid="44">By the length bias, we mean that short paths, consisting of a small number of tokens, are preferred to long path.</S>
    <S sid="71" ssid="45">Even if the transition probability of each token is small, the total probability of the path will be amplified when the path is short 2:(b)).</S>
    <S sid="72" ssid="46">Length bias occurs in Japanese morphological analysis because the number of output tokens y varies by use of prior lexicons.</S>
    <S sid="73" ssid="47">Uchimoto et al. attempted a variant of MEMMs for Japanese morphological analysis with a number of features including suffixes and character types (Uchimoto et al., 2001; Uchimoto et al., 2002; Uchimoto et al., 2003).</S>
    <S sid="74" ssid="48">Although the performance of unknown words were improved, that of known words degraded due to the label and length bias.</S>
    <S sid="75" ssid="49">Wrong segmentation had been reported in sentences which are analyzed correctly by naive rule-based or HMMs-based analyzers.</S>
  </SECTION>
  <SECTION title="3 Conditional Random Fields" number="3">
    <S sid="76" ssid="1">Conditional random fields (CRFs) (Lafferty et al., 2001) overcome the problems described in Section 2.2.</S>
    <S sid="77" ssid="2">CRFs are discriminative models and can thus capture many correlated features of the inputs.</S>
    <S sid="78" ssid="3">This allows flexible feature designs for hierarchical tagsets.</S>
    <S sid="79" ssid="4">CRFs have a single exponential model for the joint probability of the entire paths given the input sentence, while MEMMs consist of a sequential combination of exponential models, each of which estimates a conditional probability of next tokens given the current state.</S>
    <S sid="80" ssid="5">This minimizes the influences of the label and length bias.</S>
    <S sid="81" ssid="6">As explained in Section 2.1, there is word boundary ambiguity in Japanese, and we choose to use a lattice instead of B/I tagging.</S>
    <S sid="82" ssid="7">This implies that the set of labels and the set of states are different, and the number of tokens #y varies according to a path.</S>
    <S sid="83" ssid="8">In order to accomodate this, we define CRFs for Japanese morphological analysis as the conditional probability of an output path y = ((w1, t1), ... , (w#y, t#y)) given an input sequence x: where Zx is a normalization factor over all candidate paths, i.e., fk(hwi&#8722;1, ti&#8722;1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi&#8722;1, ti&#8722;1i 5. &#955;k(&#8712; A = {&#955;1, ... , &#955;K} &#8712; RK) is a learned weight or parameter associated with feature function fk.</S>
    <S sid="84" ssid="9">Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)).</S>
    <S sid="85" ssid="10">The previous applications of CRFs assign a conditional probability for a label sequence y = y1, ... , yT given an input sequence x = x1, ... , xT as: In our formulation, CRFs deal with word boundary ambiguity.</S>
    <S sid="86" ssid="11">Thus, the the size of output sequence T is not fixed through all candidates y &#8712; Y(x).</S>
    <S sid="87" ssid="12">The index i is not tied with the input x as in the original CRFs, but unique to the output y &#8712; Y(x).</S>
    <S sid="88" ssid="13">Here, we introduce the global feature vecthe global feature vector, P(y|x) can also be represented as P(y|x) = Zx1 exp(A &#183; F(y, x)).</S>
    <S sid="89" ssid="14">The most probable path y&#710; for the input sentence x is then given by To maximize L&#923;, we have to maximize the difference between the inner product (or score) of the correct path A &#183; F(yj, xj) and those of all other candidates A &#183; F(y, xj), y &#8712; Y(xj).</S>
    <S sid="90" ssid="15">CRFs is thus trained to discriminate the correct path from all other candidates, which reduces the influences of the label and length bias in encoding.</S>
    <S sid="91" ssid="16">At the optimal point, the first-derivative of the log-likelihood becomes 0, thus, where Ok = Ej Fk(yj, xj) is the count of feature k observed in the training data T, and Ek = Ej EP(y|xj)[Fk(y, xj)] is the expectation of feature k over the model distribution P(y|x) and T. The expectation can efficiently be calculated using a variant of the forward-backward algorithm. where fk is an abbreviation for fk(hw', t'i, hw, ti), B(x) is a set of all bi-gram sequences observed in the lattice for x, and &#945;(w,t) and &#946;(w,t) are the forward-backward costs given by the following recursive definitions: which can be found with the Viterbi algorithm.</S>
    <S sid="92" ssid="17">An interesting note is that the decoding process of CRFs can be reduced into a simple linear combinations over all global features. where LT (hw, ti) and RT (hw, ti) denote a set of tokens each of which connects to the token hw, ti from the left and the right respectively.</S>
    <S sid="93" ssid="18">Note that initial costs of two virtual tokens, &#945;(wbos,tbos) and &#946;(weos,teos), are set to be 1.</S>
    <S sid="94" ssid="19">A normalization constant is then given by Zx = &#945;(weos,teos)(= &#946;(wbos,tbos)).</S>
    <S sid="95" ssid="20">We attempt two types of regularizations in order to avoid overfitting.</S>
    <S sid="96" ssid="21">They are a Gaussian prior (L2norm) (Chen and Rosenfeld, 1999) and a Laplacian prior (L1-norm) (Goodman, 2004; Peng and McCallum, 2004) CRFs are trained using the standard maximum likelihood estimation, i.e., maximizing the loglikelihood L&#923; of a given training set T = {hxj,yji}N j=1, Below, we refer to CRFs with L1-norm and L2norm regularization as L1-CRFs and L2-CRFs respectively.</S>
    <S sid="97" ssid="22">The parameter C E R+ is a hyperparameter of CRFs determined by a cross validation.</S>
    <S sid="98" ssid="23">L1-CRFs can be reformulated into the constrained optimization problem below by letting Ak = A+k &#8722; A&#8722;k : At the optimal point, the following Karush-KuhunTucker conditions satisfy: A+k &#183; [C &#183; (Ok &#8722; Ek) &#8722; 1/2] = 0, A&#8722;k &#183; [C &#183; (Ek &#8722; Ok) &#8722; 1/2] = 0, and |C &#183; (Ok &#8722; Ek) |&lt; 1/2.</S>
    <S sid="99" ssid="24">These conditions mean that both A+k and A&#8722;k are set to be 0 (i.e., Ak = 0), when |C &#183; (Ok &#8722; Ek) |&lt; 1/2.</S>
    <S sid="100" ssid="25">A non-zero weight is assigned to Ak, only when |C &#183; (Ok &#8722; Ek) |= 1/2.</S>
    <S sid="101" ssid="26">L2-CRFs, in contrast, give the optimal solution when &#948;L&#923; &#948;&#955;k = C &#183; (Ok &#8722; Ek) &#8722; Ak = 0.</S>
    <S sid="102" ssid="27">Omitting the proof, (Ok &#8722; Ek) =&#65533; 0 can be shown and L2-CRFs thus give a non-sparse solution where all Ak have non-zero weights.</S>
    <S sid="103" ssid="28">The relationship between two reguralizations have been studied in Machine Learning community.</S>
    <S sid="104" ssid="29">(Perkins et al., 2003) reported that L1-regularizer should be chosen for a problem where most of given features are irrelevant.</S>
    <S sid="105" ssid="30">On the other hand, L2regularizer should be chosen when most of given features are relevant.</S>
    <S sid="106" ssid="31">An advantage of L1-based regularizer is that it often leads to sparse solutions where most of Ak are exactly 0.</S>
    <S sid="107" ssid="32">The features assigned zero weight are thought as irrelevant features to classifications.</S>
    <S sid="108" ssid="33">The L2-based regularizer, also seen in SVMs, produces a non-sparse solution where all of Ak have non-zero weights.</S>
    <S sid="109" ssid="34">All features are used with L2-CRFs.</S>
    <S sid="110" ssid="35">The optimal solutions of L2-CRFs can be obtained by using traditional iterative scaling algorithms (e.g., IIS or GIS (Pietra et al., 1997)) or more efficient quasi-Newton methods (e.g., L-BFGS (Liu and Nocedal, 1989)).</S>
    <S sid="111" ssid="36">For L1-CRFs, constrained optimizers (e.g., L-BFGS-B (Byrd et al., 1995)) can be used.</S>
  </SECTION>
  <SECTION title="4 Experiments and Discussion" number="4">
    <S sid="112" ssid="1">We use two widely-used Japanese annotated corpora in the research community, Kyoto University Corpus ver 2.0 (KC) and RWCP Text Corpus (RWCP), for our experiments on CRFs.</S>
    <S sid="113" ssid="2">Note that each corpus has a different POS tagset and details (e.g., size of training and test dataset) are summarized in Table 1.</S>
    <S sid="114" ssid="3">One of the advantages of CRFs is that they are flexible enough to capture many correlated features, including overlapping and non-independent features.</S>
    <S sid="115" ssid="4">We thus use as many features as possible, which could not be used in HMMs.</S>
    <S sid="116" ssid="5">Table 2 summarizes the set of feature templates used in the KC data.</S>
    <S sid="117" ssid="6">The templates for RWCP are essentially the same as those of KC except for the maximum level of POS subcatgeories.</S>
    <S sid="118" ssid="7">Word-level templates are employed when the words are lexicalized, i.e., those that belong to particle, auxiliary verb, or suffix6.</S>
    <S sid="119" ssid="8">For an unknown word, length of the word, up to 2 suffixes/prefixes and character types are used as the features.</S>
    <S sid="120" ssid="9">We use all features observed in the lattice without any cut-off thresholds.</S>
    <S sid="121" ssid="10">Table 1 also includes the number of features in both data sets.</S>
    <S sid="122" ssid="11">We evaluate performance with the standard Fscore (F&#946;=1) defined as follows: where Recall = # of correct tokens # of tokens in test corpus # of correct tokens P recision = .</S>
    <S sid="123" ssid="12"># of tokens in system output In the evaluations of F-scores, three criteria of correctness are used: seg: (only the word segmentation is evaluated), top: (word segmentation and the top level of POS are evaluated), and all: (all information is used for evaluation).</S>
    <S sid="124" ssid="13">The hyperparameters C for L1-CRFs and L2CRFs are selected by cross-validation.</S>
    <S sid="125" ssid="14">Experiments are implemented in C++ and executed on Linux with XEON 2.8 GHz dual processors and 4.0 Gbyte of main memory.</S>
    <S sid="126" ssid="15">Tables 3 and 4 show experimental results using KC and RWCP respectively.</S>
    <S sid="127" ssid="16">The three F-scores (seg/top/all) for our CRFs and a baseline bi-gram HMMs are listed.</S>
    <S sid="128" ssid="17">In Table 3 (KC data set), the results of a variant of maximum entropy Markov models (MEMMs) (Uchimoto et al., 2001) and a rule-based analyzer (JUMAN7) are also shown.</S>
    <S sid="129" ssid="18">To make a fare comparison, we use exactly the same data as (Uchimoto et al., 2001).</S>
    <S sid="130" ssid="19">In Table 4 (RWCP data set), the result of an extended Hidden Markov Models (E-HMMs) (Asahara and Matsumoto, 2000) trained and tested with the same corpus is also shown.</S>
    <S sid="131" ssid="20">E-HMMs is applied to the current implementation of ChaSen.</S>
    <S sid="132" ssid="21">Details of E-HMMs are described in Section 4.3.2.</S>
    <S sid="133" ssid="22">We directly evaluated the difference of these systems using McNemar&#8217;s test.</S>
    <S sid="134" ssid="23">Since there are no standard methods to evaluate the significance of F scores, we convert the outputs into the characterbased B/I labels and then employ a McNemar&#8217;s paired test on the labeling disagreements.</S>
    <S sid="135" ssid="24">This evaluation was also used in (Sha and Pereira, 2003).</S>
    <S sid="136" ssid="25">The results of McNemar&#8217;s test suggest that L2-CRFs is significantly better than other systems including L1CRFs8.</S>
    <S sid="137" ssid="26">The overall results support our empirical success of morphological analysis based on CRFs.</S>
    <S sid="138" ssid="27">Uchimoto el al. proposed a variant of MEMMs trained with a number of features (Uchimoto et al., 2001).</S>
    <S sid="139" ssid="28">Although they improved the accuracy for unknown words, they fail to segment some sentences which are correctly segmented with HMMs or rulebased analyzers.</S>
    <S sid="140" ssid="29">Figure 3 illustrates the sentences which are incorrectly segmented by Uchimoto&#8217;s MEMMs.</S>
    <S sid="141" ssid="30">The correct paths are indicated by bold boxes.</S>
    <S sid="142" ssid="31">Uchimoto et al. concluded that these errors were caused by nonstandard entries in the lexicon.</S>
    <S sid="143" ssid="32">In Figure 3, &#8220;&#12525;&#12510; &#12531;&#12399;&#8221; (romanticist) and &#8220;&#12394;&#12356;&#24515;&#8221; (one&#8217;s heart) are unusual spellings and they are normally written as &#8220;&#12525;&#12510;&#12531;&#27966;&#8221; and &#8220;&#20869;&#24515;&#8221; respectively.</S>
    <S sid="144" ssid="33">However, we conjecture that these errors are caused by the influence of the length bias.</S>
    <S sid="145" ssid="34">To support our claim, these sentences are correctly segmented by CRFs, HMMs and rule-based analyzers using the same lexicon as (Uchimoto et al., 2001).</S>
    <S sid="146" ssid="35">By the length bias, short paths are preferred to long paths.</S>
    <S sid="147" ssid="36">Thus, single token &#8220;&#12525;&#12510;&#12531;&#12399;&#8221; or &#8220;&#12394;&#12356;&#24515;&#8221; is likely to be selected compared to multiple tokens &#8220;&#12525;&#12510;&#12531; / &#12399;&#8221; or &#8220;&#12394; &#12356; / &#24515;&#8221;.</S>
    <S sid="148" ssid="37">Moreover, &#8220;&#12525;&#12510;&#12531;&#8221; and &#8220;&#12525;&#12510;&#12531;&#12399;&#8221; have exactly the same POS (Noun), and transition probabilities of these tokens become almost equal.</S>
    <S sid="149" ssid="38">Consequentially, there is no choice but to select a short path (single token) in order to maximize the whole sentence probability.</S>
    <S sid="150" ssid="39">Table 5 summarizes the number of errors in HMMs, CRFs and MEMMs, using the KC data set.</S>
    <S sid="151" ssid="40">Two types of errors, l-error and s-error, are given in this table. l-error (or s-error) means that a system incorrectly outputs a longer (or shorter) token than the correct token respectively.</S>
    <S sid="152" ssid="41">By length bias, long tokens are preferred to short tokens.</S>
    <S sid="153" ssid="42">Thus, larger number of l-errors implies that the result is highly influenced by the length bias.</S>
    <S sid="154" ssid="43">While the relative rates of l-error and s-error are almost the same in HMMs and CRFs, the number of l-errors with MEMMs amounts to 416, which is 70% of total errors, and is even larger than that of naive HMMs (306).</S>
    <S sid="155" ssid="44">This result supports our claim that MEMMs is not sufficient to be applied to Japanese morphological analysis where the length bias is inevitable.</S>
    <S sid="156" ssid="45">Asahara et al. extended the original HMMs by 1) position-wise grouping of POS tags, 2) word-level statistics, and 3) smoothing of word and POS level statistics (Asahara and Matsumoto, 2000).</S>
    <S sid="157" ssid="46">All of these techniques are designed to capture hierarchical structures of POS tagsets.</S>
    <S sid="158" ssid="47">For instance, in the position-wise grouping, optimal levels of POS hierarchies are changed according to the contexts.</S>
    <S sid="159" ssid="48">Best hierarchies for each context are selected by handcrafted rules or automatic error-driven procedures.</S>
    <S sid="160" ssid="49">CRFs can realize such extensions naturally and straightforwardly.</S>
    <S sid="161" ssid="50">In CRFs, position-wise grouping and word-POS smoothing are simply integrated into a design of feature functions.</S>
    <S sid="162" ssid="51">Parameters &#955;k for each feature are automatically configured by general maximum likelihood estimation.</S>
    <S sid="163" ssid="52">As shown in Table 2, we can employ a number of templates to capture POS hierarchies.</S>
    <S sid="164" ssid="53">Furthermore, some overlapping features (e.g., forms and types of conjugation) can be used, which was not possible in the extended HMMs.</S>
    <S sid="165" ssid="54">L2-CRFs perform slightly better than L1-CRFs, which indicates that most of given features (i.e., overlapping features, POS hierarchies, suffixes/prefixes and character types) are relevant to both of two datasets.</S>
    <S sid="166" ssid="55">The numbers of active (nonzero) features used in L1-CRFs are much smaller (about 1/8 - 1/6) than those in L2-CRFs: (L2CRFs: 791,798 (KC) / 580,032 (RWCP) v.s., L1CRFs: 90,163 (KC) / 101,757 (RWCP)).</S>
    <S sid="167" ssid="56">L1-CRFs are worth being examined if there are some practical constraints (e.g., limits of memory, disk or CPU resources).</S>
  </SECTION>
  <SECTION title="5 Conclusions and Future Work" number="5">
    <S sid="168" ssid="1">In this paper, we present how conditional random fields can be applied to Japanese morphological analysis in which word boundary ambiguity exists.</S>
    <S sid="169" ssid="2">By virtue of CRFs, 1) a number of correlated features for hierarchical tagsets can be incorporated which was not possible in HMMs, and 2) influences of label and length bias are minimized which caused errors in MEMMs.</S>
    <S sid="170" ssid="3">We compare results between CRFs, MEMMs and HMMs in two Japanese annotated corpora, and CRFs outperform the other approaches.</S>
    <S sid="171" ssid="4">Although we discuss Japanese morphological analysis, the proposed approach can be applicable to other non-segmented languages such as Chinese or Thai.</S>
    <S sid="172" ssid="5">There exist some phenomena which cannot be analyzed only with bi-gram features in Japanese morphological analysis.</S>
    <S sid="173" ssid="6">To improve accuracy, tri-gram or more general n-gram features would be useful.</S>
    <S sid="174" ssid="7">CRFs have capability of handling such features.</S>
    <S sid="175" ssid="8">However, the numbers of features and nodes in the lattice increase exponentially as longer contexts are captured.</S>
    <S sid="176" ssid="9">To deal with longer contexts, we need a practical feature selection which effectively trades between accuracy and efficiency.</S>
    <S sid="177" ssid="10">For this challenge, McCallum proposes an interesting research avenue to explore (McCallum, 2003).</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="6">
    <S sid="178" ssid="1">We would like to thank Kiyotaka Uchimoto and Masayuki Asahara, who explained the details of their Japanese morphological analyzers.</S>
  </SECTION>
</PAPER>
