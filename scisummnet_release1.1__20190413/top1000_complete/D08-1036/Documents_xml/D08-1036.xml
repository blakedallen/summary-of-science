<PAPER>
  <S sid="0">A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers</S>
  <ABSTRACT>
    <S sid="1" ssid="1">There is growing interest in applying Bayesian techniques to NLP problems.</S>
    <S sid="2" ssid="2">There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.</S>
    <S sid="3" ssid="3">This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes.</S>
    <S sid="4" ssid="4">Recent papers have given contradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM.</S>
    <S sid="5" ssid="5">We invesigate a variety of samplers for HMMs, including some that these earlier papers did not study.</S>
    <S sid="6" ssid="6">We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers.</S>
    <S sid="7" ssid="7">In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="8" ssid="1">Probabilistic models now play a central role in computational linguistics.</S>
    <S sid="9" ssid="2">These models define a probability distribution P(x) over structures or analyses x.</S>
    <S sid="10" ssid="3">For example, in the part-of-speech (POS) tagging application described in this paper, which involves predicting the part-of-speech tag ti of each word wi in the sentence w = (w1,... , wn), the structure x = (w, t) consists of the words w in a sentence together with their corresponding parts-ofspeech t = (t1, ... , tn).</S>
    <S sid="11" ssid="4">In general the probabilistic models used in computational linguistics have adjustable parameters 0 which determine the distribution P(x 1 0).</S>
    <S sid="12" ssid="5">In this paper we focus on bitag Hidden Markov Models (HMMs).</S>
    <S sid="13" ssid="6">Since our goal here is to compare algorithms rather than achieve the best performance, we keep the models simple by ignoring morphology and capitalization (two very strong cues in English) and treat each word as an atomic entity.</S>
    <S sid="14" ssid="7">This means that the model parameters 0 consist of the HMM stateto-state transition probabilities and the state-to-word emission probabilities.</S>
    <S sid="15" ssid="8">In virtually all statistical approaches the parameters 0 are chosen or estimated on the basis of training data d. This paper studies unsupervised estimation, so d = w = (w1, ... , wn) consists of a sequence of words wi containing all of the words of training corpus appended into a single string, as explained below.</S>
    <S sid="16" ssid="9">Maximum Likelihood (ML) is the most common estimation method in computational linguistics.</S>
    <S sid="17" ssid="10">A Maximum Likelihood estimator sets the parameters to the value 0&#65533; that makes the likelihood Ld of the data d as large as possible: In this paper we use the Inside-Outside algorithm, which is a specialized form of ExpectationMaximization, to find HMM parameters which (at least locally) maximize the likelihood function Ld.</S>
    <S sid="18" ssid="11">Recently there is increasing interest in Bayesian methods in computational linguistics, and the primary goal of this paper is to compare the performance of various Bayesian estimators with each other and with EM.</S>
    <S sid="19" ssid="12">A Bayesian approach uses Bayes theorem to factorize the posterior distribution P(0  |d) into the Priors can be useful because they can express preferences for certain types of models.</S>
    <S sid="20" ssid="13">To take an example from our POS-tagging application, most words belong to relatively few parts-of-speech (e.g., most words belong to a single POS, and while there are some words which are both nouns and verbs, very few are prepositions and adjectives as well).</S>
    <S sid="21" ssid="14">One might express this using a prior which prefers HMMs in which the state-to-word emissions are sparse, i.e., each state emits few words.</S>
    <S sid="22" ssid="15">An appropriate Dirichlet prior can express this preference.</S>
    <S sid="23" ssid="16">While it is possible to use Bayesian inference to find a single model, such as the Maximum A Posteriori or MAP value of 0 which maximizes the posterior P(0  |d), this is not necessarily the best approach (Bishop, 2006; MacKay, 2003).</S>
    <S sid="24" ssid="17">Instead, rather than commiting to a single value for the parameters 0 many Bayesians often prefer to work with the full posterior distribution P(0  |d), as this naturally reflects the uncertainty in 0&#8217;s value.</S>
    <S sid="25" ssid="18">In all but the simplest models there is no known closed form for the posterior distribution.</S>
    <S sid="26" ssid="19">However, the Bayesian literature describes a number of methods for approximating the posterior P(0  |d).</S>
    <S sid="27" ssid="20">Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers (Goldwater and Griffiths, 2007; Johnson, 2007).</S>
    <S sid="28" ssid="21">These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t  |w) of POS tags (i.e., HMM hidden states) t given words w. This recent literature reports contradictory results about these Bayesian inference methods.</S>
    <S sid="29" ssid="22">Johnson (2007) compared two Bayesian inference algorithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM.</S>
    <S sid="30" ssid="23">On the other hand, Goldwater and Griffiths (2007) reported that the same kind of Gibbs sampler produced much better results than EM on their unsupervised POS tagging task.</S>
    <S sid="31" ssid="24">One of the primary motivations for this paper was to understand and resolve the difference in these results.</S>
    <S sid="32" ssid="25">We replicate the results of both papers and show that the difference in their results stems from differences in the sizes of the training data and numbers of states in their models.</S>
    <S sid="33" ssid="26">It turns out that the Gibbs sampler used in these earlier papers is not the only kind of sampler for HMMs.</S>
    <S sid="34" ssid="27">This paper compares the performance of four different kinds of Gibbs samplers, Variational Bayes and Expectation Maximization on unsupervised POS tagging problems of various sizes.</S>
    <S sid="35" ssid="28">Our goal here is to try to learn how the performance of these different estimators varies as we change the number of hidden states in the HMMs and the size of the training data.</S>
    <S sid="36" ssid="29">In theory, the Gibbs samplers produce streams of samples that eventually converge on the true posterior distribution, while the Variational Bayes (VB) estimator only produces an approximation to the posterior.</S>
    <S sid="37" ssid="30">However, as the size of the training data distribution increases the likelihood function and therefore the posterior distribution becomes increasingly peaked, so one would expect this variational approximation to become increasingly accurate.</S>
    <S sid="38" ssid="31">Further the Gibbs samplers used in this paper should exhibit reduced mobility as the size of training data increases, so as the size of the training data increases eventually the Variational Bayes estimator should prove to be superior.</S>
    <S sid="39" ssid="32">However the two point-wise Gibbs samplers investigated here, which resample the label of each word conditioned on the labels of its neighbours (amongst other things) only require O(m) steps per sample (where m is the number of HMM states), while EM, VB and the sentence-blocked Gibbs samplers require O(m2) steps per sample.</S>
    <S sid="40" ssid="33">Thus for HMMs with many states it is possible to perform one or two orders of magnitude more iterations of the point-wise Gibbs samplers in the same run-time as the other samplers, so it is plausible that they would yield better results.</S>
  </SECTION>
  <SECTION title="2 Inference for HMMs" number="2">
    <S sid="41" ssid="1">There are a number of excellent textbook presentations of Hidden Markov Models (Jelinek, 1997; Manning and Sch&#168;utze, 1999), so we do not present them in detail here.</S>
    <S sid="42" ssid="2">Conceptually, a Hidden Markov Model uses a Markov model to generate the sequence of states t = (t1,... , tn) (which will be interpreted as POS tags), and then generates each word wi conditioned on the corresponding state ti.</S>
    <S sid="43" ssid="3">We insert endmarkers at the beginning and end of the corpus and between sentence boundaries, and constrain the estimators to associate endmarkers with a special HMM state that never appears elsewhere in the corpus (we ignore these endmarkers during evaluation).</S>
    <S sid="44" ssid="4">This means that we can formally treat the training corpus as one long string, yet each sentence can be processed independently by a firstorder HMM.</S>
    <S sid="45" ssid="5">In more detail, the HMM is specified by a pair of multinomials &#952;t and &#966;t associated with each state t, where &#952;t specifies the distribution over states t0 following t and &#966;t specifies the distribution over words w given state t. The Bayesian model we consider here puts a fixed uniform Dirichlet prior on these multinomials.</S>
    <S sid="46" ssid="6">Because Dirichlets are conjugate to multinomials, this greatly simplifies inference.</S>
    <S sid="47" ssid="7">A multinomial &#952; is distributed according to the Dirichlet distribution Dir(&#945;) iff: In our experiments we set &#945; and &#945;0 to the uniform values (i.e., all components have the same value &#945; or &#945;0), but it is possible to estimate these as well (Goldwater and Griffiths, 2007).</S>
    <S sid="48" ssid="8">Informally, &#945; controls the sparsity of the state-to-state transition probabilities while &#945;0 controls the sparsity of the state-toword emission probabilities.</S>
    <S sid="49" ssid="9">As &#945;0 approaches zero the prior strongly prefers models in which each state emits as few words as possible, capturing the intuition that most word types only belong to one POS mentioned earlier.</S>
    <S sid="50" ssid="10">Expectation-Maximization is a procedure that iteratively re-estimates the model parameters (&#952;, &#966;), converging on a local maximum of the likelihood.</S>
    <S sid="51" ssid="11">Specifically, if the parameter estimate at iteration ` is (&#952;(`), &#966;(`)), then the re-estimated parameters at itwhere n0w,t is the number of times word w occurs with state t, nt,,t is the number of times state t0 follows t and nt is the number of occurences of state t; all expectations are taken with respect to the model (&#952;(`),&#966;(`)).</S>
    <S sid="52" ssid="12">The experiments below used the ForwardBackward algorithm (Jelinek, 1997), which is a dynamic programming algorithm for calculating the likelihood and the expectations in (2) in O(nm2) time, where n is the number of words in the training corpus and m is the number of HMM states.</S>
    <S sid="53" ssid="13">Variational Bayesian inference attempts to find a function Q(t, &#952;, &#966;) that minimizes an upper bound (3) to the negative log likelihood.</S>
    <S sid="54" ssid="14">The upper bound (3) is called the Variational Free Energy.</S>
    <S sid="55" ssid="15">We make a &#8220;mean-field&#8221; assumption that the posterior can be well approximated by a factorized model Q in which the state sequence t does not covary with the model parameters &#952;, &#966;: The calculus of variations is used to minimize the KL divergence between the desired posterior distribution and the factorized approximation.</S>
    <S sid="56" ssid="16">It turns out that if the likelihood and conjugate prior belong to exponential families then the optimal Q1 and Q2 do too, and there is an EM-like iterative procedure that finds locally-optimal model parameters (Bishop, 2006).</S>
    <S sid="57" ssid="17">This procedure is especially attractive for HMM inference, since it involves only a minor modification to the M-step of the Forward-Backward algorithm.</S>
    <S sid="58" ssid="18">MacKay (1997) and Beal (2003) describe Variational Bayesian (VB) inference for HMMs.</S>
    <S sid="59" ssid="19">In general, the E-step for VB inference for HMMs is the same as in EM, while the M-step is as follows: where m0 and m are the number of word types and states respectively, IF is the digamma function and the remaining quantities are as in (2).</S>
    <S sid="60" ssid="20">This means that a single iteration can be performed in O(nm2) time, just as for the EM algorithm.</S>
    <S sid="61" ssid="21">The goal of Markov Chain Monte Carlo (MCMC) algorithms is to produce a stream of samples from the posterior distribution P(t  |w, &#945;).</S>
    <S sid="62" ssid="22">Besag (2004) provides a tutorial on MCMC techniques for HMM inference.</S>
    <S sid="63" ssid="23">A Gibbs sampler is a simple kind of MCMC algorithm that is well-suited to sampling highdimensional spaces.</S>
    <S sid="64" ssid="24">A Gibbs sampler for P(z) where z = (z1, ... , zn) proceeds by sampling and updating each zi in turn from P(zi  |z&#8722;i), where z&#8722;i = (z1,... , zi&#8722;1, zi+1, ... , zn), i.e., all of the z except zi (Geman and Geman, 1984; Robert and Casella, 2004).</S>
    <S sid="65" ssid="25">We evaluate four different Gibbs samplers in this paper, which vary along two dimensions.</S>
    <S sid="66" ssid="26">First, the sampler can either be pointwise or blocked.</S>
    <S sid="67" ssid="27">A pointwise sampler resamples a single state ti (labeling a single word wi) at each step, while a blocked sampler resamples the labels for all of the words in a sentence at a single step using a dynamic programming algorithm based on the Forward-Backward algorithm.</S>
    <S sid="68" ssid="28">(In principle it is possible to use block sizes other than the sentence, but we did not explore this here).</S>
    <S sid="69" ssid="29">A pointwise sampler requires O(nm) time per iteration, while a blocked sampler requires O(nm2) time per iteration, where m is the number of HMM states and n is the length of the training corpus.</S>
    <S sid="70" ssid="30">Second, the sampler can either be explicit or collapsed.</S>
    <S sid="71" ssid="31">An explicit sampler represents and samples the HMM parameters 0 and 0 in addition to the states t, while in a collapsed sampler the HMM parameters are integrated out, and only the states t are sampled.</S>
    <S sid="72" ssid="32">The difference between explicit and collapsed samplers corresponds exactly to the difference between the two PCFG sampling algorithms presented in Johnson et al. (2007).</S>
    <S sid="73" ssid="33">An iteration of the pointwise explicit Gibbs sampler consists of resampling 0 and 0 given the stateto-state transition counts n and state-to-word emission counts n0 using (5), and then resampling each state ti given the corresponding word wi and the neighboring states ti&#8722;1 and ti+1 using (6).</S>
    <S sid="74" ssid="34">The Dirichlet distributions in (5) are non-uniform; nt is the vector of state-to-state transition counts in t leaving state t in the current state vector t, while n0t is the vector of state-to-word emission counts for state t. See Johnson et al. (2007) for a more detailed explanation, as well as an algorithm for sampling from the Dirichlet distributions in (5).</S>
    <S sid="75" ssid="35">The samplers that Goldwater and Griffiths (2007) and Johnson (2007) describe are pointwise collapsed Gibbs samplers.</S>
    <S sid="76" ssid="36">Figure 1 gives the sampling distribution for this sampler.</S>
    <S sid="77" ssid="37">As Johnson et al. (2007) explains, samples of the HMM parameters 0 and 0 can be obtained using (5) if required.</S>
    <S sid="78" ssid="38">The blocked Gibbs samplers differ from the pointwise Gibbs samplers in that they resample the POS tags for an entire sentence at a time.</S>
    <S sid="79" ssid="39">Besag (2004) describes the well-known dynamic programming algorithm (based on the Forward-Backward algorithm) for sampling a state sequence t given the words w and the transition and emission probabilities 0 and 0.</S>
    <S sid="80" ssid="40">At each iteration the explicit blocked Gibbs sampler resamples 0 and 0 using (5), just as the explicit pointwise sampler does.</S>
    <S sid="81" ssid="41">Then it uses the new HMM parameters to resample the states t for the training corpus using the algorithm just mentioned.</S>
    <S sid="82" ssid="42">This can be done in parallel for each sentence in the training corpus.</S>
    <S sid="83" ssid="43">The collapsed blocked Gibbs sampler is a straight-forward application of the Metropoliswithin-Gibbs approach proposed by Johnson et al. (2007) for PCFGs, so we only sketch it here.</S>
    <S sid="84" ssid="44">We iterate through the sentences of the training data, resampling the states for each sentence conditioned on the state-to-state transition counts n and stateto-word emission counts n0 for the other sentences in the corpus.</S>
    <S sid="85" ssid="45">This is done by first computing the parameters 0* and 0* of a proposal HMM using (7). scribed above to produce a proposal state sequence t* for the words in the sentence.</S>
    <S sid="86" ssid="46">Finally, we use a Metropolis-Hastings accept-reject step to decide whether to update the current state sequence for the sentence with the proposal t*, or whether to keep the current state sequence.</S>
    <S sid="87" ssid="47">In practice, with all but the very smallest training corpora the acceptance rate is very high; the acceptance rate for all of our collapsed blocked Gibbs samplers was over 99%.</S>
  </SECTION>
  <SECTION title="3 Evaluation" number="3">
    <S sid="88" ssid="1">The previous section described six different unsupervised estimators for HMMs.</S>
    <S sid="89" ssid="2">In this section we compare their performance for English part-ofspeech tagging.</S>
    <S sid="90" ssid="3">One of the difficulties in evaluating unsupervised taggers such as these is mapping the system&#8217;s states to the gold-standard partsof-speech.</S>
    <S sid="91" ssid="4">Goldwater and Griffiths (2007) proposed an information-theoretic measure known as the Variation ofInformation (VI) described by Meil&#711;a (2003) as an evaluation of an unsupervised tagging.</S>
    <S sid="92" ssid="5">However as Goldwater (p.c.) points out, this may not be an ideal evaluation measure; e.g., a tagger which assigns all words the same single part-of-speech tag does disturbingly well under Variation of Information, suggesting that a poor tagger may score well under VI.</S>
    <S sid="93" ssid="6">In order to avoid this problem we focus here on evaluation measures that construct an explicit mapping between the gold-standard part-of-speech tags and the HMM&#8217;s states.</S>
    <S sid="94" ssid="7">Perhaps the most straightforward approach is to map each HMM state to the part-of-speech tag it co-occurs with most frequently, and use this mapping to map each HMM state sequence t to a sequence of part-of-speech tags.</S>
    <S sid="95" ssid="8">But as Clark (2003) observes, this approach has several defects.</S>
    <S sid="96" ssid="9">If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state.</S>
    <S sid="97" ssid="10">We can partially address this by cross-validation.</S>
    <S sid="98" ssid="11">We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech.</S>
    <S sid="99" ssid="12">We call the accuracy of the resulting tagging the crossvalidation accuracy.</S>
    <S sid="100" ssid="13">Finally, following Haghighi and Klein (2006) and Johnson (2007) we can instead insist that at most one HMM state can be mapped to any part-of-speech tag.</S>
    <S sid="101" ssid="14">Following these authors, we used a greedy algorithm to associate states with POS tags; the accuracy of the resulting tagging is called the greedy 1-to-1 Then we use the dynamic programming sampler deaccuracy.</S>
    <S sid="102" ssid="15">The studies presented by Goldwater and Griffiths (2007) and Johnson (2007) differed in the number of states that they used.</S>
    <S sid="103" ssid="16">Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.</S>
    <S sid="104" ssid="17">We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set).</S>
    <S sid="105" ssid="18">Also, the studies differed in the size of the corpora used.</S>
    <S sid="106" ssid="19">The largest corpus that Goldwater and Griffiths (2007) studied contained 96,000 words, while Johnson (2007) used all of the 1,173,766 words in the full Penn WSJ treebank.</S>
    <S sid="107" ssid="20">For that reason we ran all our estimators on corpora containing 24,000 words and 120,000 words as well as the full treebank.</S>
    <S sid="108" ssid="21">We ran each estimator with the eight different combinations of values for the hyperparameters &#945; and &#945;' listed below, which include the optimal values for the hyperparameters found by Johnson (2007), and report results for the best combination for each estimator below 1.</S>
    <S sid="109" ssid="22">Further, we ran each setting of each estimator at least 10 times (from randomly jittered initial starting points) for at least 1,000 iterations, as Johnson (2007) showed that some estimators require many iterations to converge.</S>
    <S sid="110" ssid="23">The results of our experiments are summarized in Figures 2&#8211;5.</S>
    <S sid="111" ssid="24">1We found that on some data sets the results are sensitive to the values of the hyperparameters.</S>
    <S sid="112" ssid="25">So, there is a bit uncertainty in our comparison results because it is possible that the values we tried were good for one estimator and bad for others.</S>
    <S sid="113" ssid="26">Unfortunately, we do not know any efficient way of searching the optimal hyperparameters in a much wider and more fine-grained space.</S>
    <S sid="114" ssid="27">We leave it to future work.</S>
  </SECTION>
  <SECTION title="4 Conclusion and future work" number="4">
    <S sid="115" ssid="1">As might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.</S>
    <S sid="116" ssid="2">On small data sets all of the Bayesian estimators strongly outperform EM (and, to a lesser extent, VB) with respect to all of our evaluation measures, confirming the results reported in Goldwater and Griffiths (2007).</S>
    <S sid="117" ssid="3">This is perhaps not too surprising, as the Bayesian prior plays a comparatively stronger role with a smaller training corpus (which makes the likelihood term smaller) and the approximation used by Variational Bayes is likely to be less accurate on smaller data sets.</S>
    <S sid="118" ssid="4">But on larger data sets, which Goldwater et al did not study, the results are much less clear, and depend on which evaluation measure is used.</S>
    <S sid="119" ssid="5">Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy, confirming the results reported by Johnson (2007).</S>
    <S sid="120" ssid="6">Variational Bayes converges faster than all of the other estimators we examined here.</S>
    <S sid="121" ssid="7">We found that the speed of convergence of our samplers depends to a large degree upon the values of the hyperparameters &#945; and &#945;', with larger values leading to much faster convergence.</S>
    <S sid="122" ssid="8">This is not surprising, as the &#945; and &#945;' specify how likely the samplers are to consider novel tags, and therefore directly influence the sampler&#8217;s mobility.</S>
    <S sid="123" ssid="9">However, in our experiments the best results are obtained in most settings with small values for &#945; and &#945;', usually between 0.1 and 0.0001.</S>
    <S sid="124" ssid="10">In terms of time to convergence, on larger data sets we found that the blocked samplers were generally faster than the pointwise samplers, and that the explicit samplers (which represented and sampled 0 and 0) were faster than the collapsed samplers, largely because the time saved in not computing probabilities on the fly overwhelmed the time spent resampling the parameters.</S>
    <S sid="125" ssid="11">Of course these experiments only scratch the surface of what is possible.</S>
    <S sid="126" ssid="12">Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.</S>
    <S sid="127" ssid="13">Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.</S>
  </SECTION>
</PAPER>
