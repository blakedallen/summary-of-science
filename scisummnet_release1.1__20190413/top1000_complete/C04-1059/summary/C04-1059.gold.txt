Language Model Adaptation For Statistical Machine Translation Via Structured Query Models
We explore unsupervised language model adaptation techniques for Statistical Machine Translation.
The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.
Specific language models are then build from the retrieved data and interpolated with a general background model.
Experiments show significant improvements when translating with these adapted language models.
We apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus.
We construct specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora.
We convert initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection.
