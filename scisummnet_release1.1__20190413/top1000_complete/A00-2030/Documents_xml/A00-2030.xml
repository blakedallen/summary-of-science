<PAPER>
  <S sid="0">A Novel Use Of Statistical Parsing To Extract Information From Text</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S>
    <S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
    <S sid="4" ssid="2">Yet, relatively few have embedded one of these algorithms in a task.</S>
    <S sid="5" ssid="3">Chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
    <S sid="7" ssid="5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S>
    <S sid="8" ssid="6">Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire, which includes dozens of newspapers?</S>
    <S sid="9" ssid="7">Manually creating sourcespecific training data for syntax was not required.</S>
    <S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
  </SECTION>
  <SECTION title="2 Information Extraction Tasks" number="2">
    <S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
    <S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S>
    <S sid="13" ssid="3">For each organization in an article, one must identify all of its names as used in the article, its type (corporation, government, or other), and any significant description of it.</S>
    <S sid="14" ssid="4">For each person, one must find all of the person's names within the document, his/her type (civilian or military), and any significant descriptions (e.g., titles).</S>
    <S sid="15" ssid="5">For each location, one must also give its type (city, province, county, body of water, etc.).</S>
    <S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
    <S sid="17" ssid="7">For the following example, the template relation in Figure 2 was to be generated: &amp;quot;Donald M. Goldstein, a historian at the University of Pittsburgh who helped write...&amp;quot;</S>
  </SECTION>
  <SECTION title="3 Integrated Sentential Processing" number="3">
    <S sid="18" ssid="1">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S>
    <S sid="19" ssid="2">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
    <S sid="20" ssid="3">However, pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.</S>
    <S sid="21" ssid="4">For example, an error made during part-of-speech-tagging may cause a future error in syntactic analysis, which may in turn cause a semantic interpretation failure.</S>
    <S sid="22" ssid="5">There is no opportunity for a later stage, such as parsing, to influence or correct an earlier stage such as part-of-speech tagging.</S>
    <S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
    <S sid="25" ssid="8">A second consideration influenced our decision toward an integrated model.</S>
    <S sid="26" ssid="9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.</S>
    <S sid="27" ssid="10">1997).</S>
    <S sid="28" ssid="11">Finally, our newly constructed parser, like that of (Collins 1997), was based on a generative statistical model.</S>
    <S sid="29" ssid="12">Thus, each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S>
    <S sid="30" ssid="13">Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model.</S>
    <S sid="31" ssid="14">If the single generalized model could then be extended to semantic analysis, all necessary sentence level processing would be contained in that model.</S>
    <S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
  </SECTION>
  <SECTION title="4 Representing Syntax and Semantics Jointly" number="4">
    <S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
    <S sid="35" ssid="3">An example of an augmented parse tree is shown in Figure 3.</S>
    <S sid="36" ssid="4">The five key facts in this example are: Here, each &amp;quot;reportable&amp;quot; name or description is identified by a &amp;quot;-r&amp;quot; suffix attached to its semantic label.</S>
    <S sid="37" ssid="5">For example, &amp;quot;per-r&amp;quot; identifies &amp;quot;Nance&amp;quot; as a named person, and &amp;quot;per-desc-r&amp;quot; identifies &amp;quot;a paid consultant to ABC News&amp;quot; as a person description.</S>
    <S sid="38" ssid="6">Other labels indicate relations among entities.</S>
    <S sid="39" ssid="7">For example, the coreference relation between &amp;quot;Nance&amp;quot; and &amp;quot;a paid consultant to ABC News&amp;quot; is indicated by &amp;quot;per-desc-of.&amp;quot; In this case, because the argument does not connect directly to the relation, the intervening nodes are labeled with semantics &amp;quot;-ptr&amp;quot; to indicate the connection.</S>
    <S sid="40" ssid="8">Further details are discussed in the section Tree Augmentation.</S>
  </SECTION>
  <SECTION title="5 Creating the Training Data" number="5">
    <S sid="41" ssid="1">To train our integrated model, we required a large corpus of augmented parse trees.</S>
    <S sid="42" ssid="2">Since it was known that the MUC-7 evaluation data would be drawn from a variety of newswire sources, and that the articles would focus on rocket launches, it was important that our training corpus be drawn from similar sources and that it cover similar events.</S>
    <S sid="43" ssid="3">Thus, we did not consider simply adding semantic labels to the existing Penn TREEBANK, which is drawn from a single source &#8212; the Wall Street Journal &#8212; and is impoverished in articles about rocket launches.</S>
    <S sid="44" ssid="4">Instead, we applied an information retrieval system to select a large number of articles from the desired sources, yielding a corpus rich in the desired types of events.</S>
    <S sid="45" ssid="5">The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.</S>
    <S sid="46" ssid="6">Initially, we tried to annotate the training corpus by hand marking, for each sentence, the entire augmented tree.</S>
    <S sid="47" ssid="7">It soon became painfully obvious that this task could not be performed in the available time.</S>
    <S sid="48" ssid="8">Our annotation staff found syntactic analysis particularly complex and slow going.</S>
    <S sid="49" ssid="9">By necessity, we adopted the strategy of hand marking only the semantics.</S>
    <S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
    <S sid="51" ssid="11">To produce a corpus of augmented parse trees, we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus, now annotated with complete augmented trees like that in Figure 3.</S>
  </SECTION>
  <SECTION title="6 Tree Augmentation" number="6">
    <S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
    <S sid="53" ssid="2">For each sentence, combining these two sources involved five steps.</S>
    <S sid="54" ssid="3">These steps are given below:</S>
  </SECTION>
  <SECTION title="Tree Augmentation Algorithm" number="7">
    <S sid="55" ssid="1">syntactic modifier of the other, the inserted node serves to indicate the relation as well as the argument.</S>
    <S sid="56" ssid="2">For example, in the phrase &amp;quot;Lt. Cmdr.</S>
    <S sid="57" ssid="3">David Edwin Lewis,&amp;quot; a node is inserted to indicate that &amp;quot;Lt. Cmdr.&amp;quot; is a descriptor for &amp;quot;David Edwin Lewis.&amp;quot; 5.</S>
    <S sid="58" ssid="4">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.</S>
    <S sid="59" ssid="5">These labels serve to form a continuous chain between the relation and its argument.</S>
  </SECTION>
  <SECTION title="7 Model Structure" number="8">
    <S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
    <S sid="62" ssid="3">For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward.</S>
    <S sid="63" ssid="4">Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created.</S>
    <S sid="64" ssid="5">Word features are introduced primarily to help with unknown words, as in (Weischedel et al. 1993).</S>
    <S sid="65" ssid="6">We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.</S>
    <S sid="66" ssid="7">At each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection dependent on particular features of previously generated elements.</S>
    <S sid="67" ssid="8">We pick up the derivation just after the topmost S and its head word, said, have been produced.</S>
    <S sid="68" ssid="9">The next steps are to generate in order: In this case, there are none.</S>
    <S sid="69" ssid="10">8.</S>
    <S sid="70" ssid="11">Post-modifier constituents for the PER/NP.</S>
    <S sid="71" ssid="12">First a comma, then an SBAR structure, and then a second comma are each generated in turn.</S>
    <S sid="72" ssid="13">This generation process is continued until the entire tree has been produced.</S>
    <S sid="73" ssid="14">We now briefly summarize the probability structure of the model.</S>
    <S sid="74" ssid="15">The categories for head constituents, cl&#8222; are predicted based solely on the category of the parent node, cp: Modifier constituent categories, cm, are predicted based on their parent node, cp, the head constituent of their parent node, chp, the previously generated modifier, c&#8222;,_1, and the head word of their parent, wp.</S>
    <S sid="75" ssid="16">Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags, t,,, for modifiers are predicted based on the modifier, cm, the partof-speech tag of the head word, th, and the head word itself, wh: Head words, w&#8222;&#8222; for modifiers are predicted based on the modifier, cm, the part-of-speech tag of the modifier word , t&#8222;&#8222; the part-ofspeech tag of the head word , th, and the head word itself, wh: lAwmicm,tm,th,wh), e.g.</S>
    <S sid="76" ssid="17">Finally, word features, fm, for modifiers are predicted based on the modifier, cm, the partof-speech tag of the modifier word , t&#8222;&#8222; the part-of-speech tag of the head word th, the head word itself, wh, and whether or not the modifier head word, w&#8222;&#8222; is known or unknown.</S>
    <S sid="77" ssid="18">The probability of a complete tree is the product of the probabilities of generating each element in the tree.</S>
    <S sid="78" ssid="19">If we generalize the tree components (constituent labels, words, tags, etc.) and treat them all as simply elements, e, and treat all the conditioning factors as the history, h, we can write:</S>
  </SECTION>
  <SECTION title="8 Training the Model" number="9">
    <S sid="79" ssid="1">Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.</S>
    <S sid="80" ssid="2">However, because these estimates are too sparse to be relied upon, we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).</S>
    <S sid="81" ssid="3">For modifier constituents, the mixture components are: For part-of-speech tags, the mixture components are: Finally, for word features, the mixture components are:</S>
  </SECTION>
  <SECTION title="9 Searching the Model" number="10">
    <S sid="82" ssid="1">Given a sentence to be analyzed, the search program must find the most likely semantic and syntactic interpretation.</S>
    <S sid="83" ssid="2">More precisely, it must find the most likely augmented parse tree.</S>
    <S sid="84" ssid="3">Although mathematically the model predicts tree elements in a top-down fashion, we search the space bottom-up using a chartbased search.</S>
    <S sid="85" ssid="4">The search is kept tractable through a combination of CKY-style dynamic programming and pruning of low probability elements.</S>
    <S sid="86" ssid="5">Whenever two or more constituents are equivalent relative to all possible later parsing decisions, we apply dynamic programming, keeping only the most likely constituent in the chart.</S>
    <S sid="87" ssid="6">Two constituents are considered equivalent if: threshold of the highest scoring constituent are maintained; all others are pruned.</S>
    <S sid="88" ssid="7">For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman, 1997).</S>
    <S sid="89" ssid="8">We can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node.</S>
    <S sid="90" ssid="9">Thus, the scores used in pruning can be considered as the product of: 1.</S>
    <S sid="91" ssid="10">The probability of generating a constituent of the specified category, starting at the topmost node.</S>
    <S sid="92" ssid="11">2.</S>
    <S sid="93" ssid="12">The probability of generating the structure beneath that constituent, having already generated a constituent of that category.</S>
    <S sid="94" ssid="13">Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
    <S sid="95" ssid="14">The semantics &#8212; that is, the entities and relations &#8212; can then be directly extracted from these sentential trees.</S>
  </SECTION>
  <SECTION title="10 Experimental Results" number="11">
    <S sid="96" ssid="1">Our system for MUC-7 consisted of the sentential model described in this paper, coupled with a simple probability model for cross-sentence merging.</S>
    <S sid="97" ssid="2">The evaluation results are summarized in Table 1.</S>
    <S sid="98" ssid="3">In both Template Entity (TE) and Template Relation (TR), our system finished in second place among all entrants.</S>
    <S sid="99" ssid="4">Nearly all of the work was done by the sentential model; disabling the cross-sentence model entirely reduced our overall F-Score by only 2 points.</S>
    <S sid="100" ssid="5">Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.</S>
    <S sid="101" ssid="6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97), and evaluated name finding accuracy on the MUC7 named entity test.</S>
    <S sid="102" ssid="7">The results are summarized in Table 2.</S>
    <S sid="103" ssid="8">While performance did not quite match the best previously reported results for any of these three tasks, we were pleased to observe that the scores were at or near state-of-the-art levels for all cases.</S>
  </SECTION>
  <SECTION title="11 Conclusions" number="12">
    <S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
    <S sid="105" ssid="2">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>
    <S sid="106" ssid="3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
    <S sid="107" ssid="4">The semantic training corpus was produced by students according to a simple set of guidelines.</S>
    <S sid="108" ssid="5">This simple semantic annotation was the only source of task knowledge used to configure the model.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="13">
    <S sid="109" ssid="1">The work reported here was supported in part by the Defense Advanced Research Projects Agency.</S>
    <S sid="110" ssid="2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062, F30602-97-C-0096, and 4132-BBN-001.</S>
    <S sid="111" ssid="3">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government.</S>
    <S sid="112" ssid="4">We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.</S>
  </SECTION>
</PAPER>
