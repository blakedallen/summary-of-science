<PAPER>
  <S sid="0">Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Current phrase-based SMT systems perform poorly when using small training sets.</S>
    <S sid="2" ssid="2">This is a consequence of unreliable translation estimates and low coverage over source and target phrases.</S>
    <S sid="3" ssid="3">This paper presents a method which alleviates this problem by exploiting multiple translations of the same source Central to our approach is triangulathe process of translating from a source to a target language via an intermediate third language.</S>
    <S sid="4" ssid="4">This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods.</S>
    <S sid="5" ssid="5">Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Statistical machine translation (Brown et al., 1993) has seen many improvements in recent years, most notably the transition from word- to phrase-based models (Koehn et al., 2003).</S>
    <S sid="7" ssid="2">Modern SMT systems are capable of producing high quality translations when provided with large quantities of training data.</S>
    <S sid="8" ssid="3">With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also &#8216;back-off&#8217; to smaller sized phrases.</S>
    <S sid="9" ssid="4">This often leads to poor choices of target phrases and reduces the coherence of the output.</S>
    <S sid="10" ssid="5">Unfortunately, parallel corpora are not readily available in large quantities, except for a small subset of the world&#8217;s languages (see Resnik and Smith (2003) for discussion), therefore limiting the potential use of current SMT systems.</S>
    <S sid="11" ssid="6">In this paper we provide a means for obtaining more reliable translation frequency estimates from small datasets.</S>
    <S sid="12" ssid="7">We make use of multi-parallel corpora (sentence aligned parallel texts over three or more languages).</S>
    <S sid="13" ssid="8">Such corpora are often created by international organisations, the United Nations (UN) being a prime example.</S>
    <S sid="14" ssid="9">They present a challenge for current SMT systems due to their relatively moderate size and domain variability (examples of UN texts include policy documents, proceedings of meetings, letters, etc.).</S>
    <S sid="15" ssid="10">Our method translates each target phrase, t, first to an intermediate language, i, and then into the source language, s. We call this two-stage translation process triangulation (Kay, 1997).</S>
    <S sid="16" ssid="11">We present a probabilistic formulation through which we can estimate the desired phrase translation distribution (phrase-table) by marginalisation, p(s|t) _ Ei p(s, i|t).</S>
    <S sid="17" ssid="12">As with conventional smoothing methods (Koehn et al., 2003; Foster et al., 2006), triangulation increases the robustness of phrase translation estimates.</S>
    <S sid="18" ssid="13">In contrast to smoothing, our method alleviates data sparseness by exploring additional multiparallel data rather than adjusting the probabilities of existing data.</S>
    <S sid="19" ssid="14">Importantly, triangulation provides us with separately estimated phrase-tables which could be further smoothed to provide more reliable distributions.</S>
    <S sid="20" ssid="15">Moreover, the triangulated phrase-tables can be easily combined with the standard sourcetarget phrase-table, thereby improving the coverage over unseen source phrases.</S>
    <S sid="21" ssid="16">As an example, consider Figure 1 which shows the coverage of unigrams and larger n-gram phrases when using a standard source target phrase-table, a triangulated phrase-table with one (it) and nine languages (all), and a combination of standard and triangulated phrase-tables (all+standard).</S>
    <S sid="22" ssid="17">The phrases were harvested from a small French-English bitext and evaluated against a test set.</S>
    <S sid="23" ssid="18">Although very few small phrases are unknown, the majority of larger phrases are unseen.</S>
    <S sid="24" ssid="19">The Italian and all results show that triangulation alone can provide similar or improved coverage compared to the standard sourcetarget model; further improvement is achieved by combining the triangulated and standard models (all+standard).</S>
    <S sid="25" ssid="20">These models and datasets will be described in detail in Section 3.</S>
    <S sid="26" ssid="21">We also demonstrate that triangulation can be used on its own, that is without a source-target distribution, and still yield acceptable translation output.</S>
    <S sid="27" ssid="22">This is particularly heartening, as it provides a means of translating between the many &#8220;low density&#8221; language pairs for which we don&#8217;t yet have a source-target bitext.</S>
    <S sid="28" ssid="23">This allows SMT to be applied to a much larger set of language pairs than was previously possible.</S>
    <S sid="29" ssid="24">In the following section we provide an overview of related work.</S>
    <S sid="30" ssid="25">Section 3 introduces a generative formulation of triangulation.</S>
    <S sid="31" ssid="26">We present our evaluation framework in Section 4 and results in Section 5.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="32" ssid="1">The idea of using multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available.</S>
    <S sid="33" ssid="2">Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages.</S>
    <S sid="34" ssid="3">A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003).</S>
    <S sid="35" ssid="4">This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation.</S>
    <S sid="36" ssid="5">Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001).</S>
    <S sid="37" ssid="6">Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases.</S>
    <S sid="38" ssid="7">Their method acquires paraphrases by identifying candidate phrases in the source language, translating them into multiple target languages, and then back to the source.</S>
    <S sid="39" ssid="8">Unknown source phrases are substituted by the back-translated paraphrases and translation proceeds on the paraphrases.</S>
    <S sid="40" ssid="9">In line with previous work, we exploit multiple source corpora to alleviate data sparseness and increase translation coverage.</S>
    <S sid="41" ssid="10">However, we differ in several important respects.</S>
    <S sid="42" ssid="11">Our method operates over phrases rather than sentences.</S>
    <S sid="43" ssid="12">We propose a generative formulation which treats triangulation not as a post-processing step but as part of the translation model itself.</S>
    <S sid="44" ssid="13">The induced phrase-table entries are fed directly into the decoder, thus avoiding the additional inefficiencies of merging the output of several translation systems.</S>
    <S sid="45" ssid="14">Although related to Callison-Burch et al. (2006) our method is conceptually simpler and more general.</S>
    <S sid="46" ssid="15">Phrase-table entries are created via multiple source languages without the intermediate step of paraphrase extraction, thereby reducing the exposure to compounding errors.</S>
    <S sid="47" ssid="16">Our phrase-tables may well contain paraphrases but these are naturally induced as part of our model, without extra processing effort.</S>
    <S sid="48" ssid="17">Furthermore, we improve the translation estimates for both seen and unseen phrase-table entries, whereas Callison-Burch et al. concentrate solely on unknown phrases.</S>
    <S sid="49" ssid="18">In contrast to Utiyama and Isahara (2007), we employ a large number of intermediate languages and demonstrate how triangulated phrase-tables can be combined with standard phrase-tables to improve translation output. une patate delicate une patate chaud une question delicate</S>
  </SECTION>
  <SECTION title="3 Triangulation" number="3">
    <S sid="50" ssid="1">We start with a motivating example before formalising the mechanics of triangulation.</S>
    <S sid="51" ssid="2">Consider translating the English phrase a hot potato1 into French, as shown in Figure 2.</S>
    <S sid="52" ssid="3">In our corpus this English phrase occurs only three times.</S>
    <S sid="53" ssid="4">Due to errors in the word alignment the phrase was not included in the English-French phrase-table.</S>
    <S sid="54" ssid="5">Triangulation first translates hot potato into a set of intermediate languages (Dutch, Danish and Portuguese are shown in the figure), and then these phrases are further translated into the target language (French).</S>
    <S sid="55" ssid="6">In the example, four different target phrases are obtained, all of which are useful phrase-table entries.</S>
    <S sid="56" ssid="7">We argue that the redundancy introduced by a large suite of other languages can correct for errors in the word alignments and also provide greater generalisation, since the translation distribution is estimated from a richer set of data-points.</S>
    <S sid="57" ssid="8">For example, instances of the Danish en varm kartoffel may be used to translate several English phrases, not only a hot potato.</S>
    <S sid="58" ssid="9">In general we expect that a wider range of possible translations are found for any source phrase, simply due to the extra layer of indirection.</S>
    <S sid="59" ssid="10">So, if a source phrase tends to align with two different target phrases, then we would also expect it to align with two phrases in the &#8216;intermediate&#8217; language.</S>
    <S sid="60" ssid="11">These intermediate phrases should then each align with two target phrases, yielding up to four target phrases.</S>
    <S sid="61" ssid="12">Consequently, triangulation will often produce more varied translation distributions than the standard source-target approach.</S>
    <S sid="62" ssid="13">We now formalise triangulation as a generative probabilistic process operating independently on phrase pairs.</S>
    <S sid="63" ssid="14">We start with the conditional distribution over three languages, p(s, i|t), where the arguments denote phrases in the source, intermediate and target language, respectively.</S>
    <S sid="64" ssid="15">From this distribution, we can find the desired conditional over the source-target pair by marginalising out the intermediate phrases:2 where (1) imposes a simplifying conditional independence assumption: the intermediate phrase fully represents the information (semantics, syntax, etc.) in the source phrase, rendering the target phrase redundant in p(s|i, t).</S>
    <S sid="65" ssid="16">Equation (1) requires that all phrases in the intermediate-target bitext must also be found in the source-intermediate bitext, such that p(s|i) is defined.</S>
    <S sid="66" ssid="17">Clearly this will often not be the case.</S>
    <S sid="67" ssid="18">In these situations we could back-off to another distribution (by discarding part, or all, of the conditioning context), however we take a more pragmatic approach and ignore the missing phrases.</S>
    <S sid="68" ssid="19">This problem of missing contexts is uncommon in multi-parallel corpora, but is more common when the two bitexts are drawn from different sources.</S>
    <S sid="69" ssid="20">While triangulation is intuitively appealing, it may suffer from a few problems.</S>
    <S sid="70" ssid="21">Firstly, as with any SMT approach, the translation estimates are based on noisy automatic word alignments.</S>
    <S sid="71" ssid="22">This leads to many errors and omissions in the phrase-table.</S>
    <S sid="72" ssid="23">With a standard source-target phrase-table these errors are only encountered once, however with triangulation they are encountered twice, and therefore the errors will compound.</S>
    <S sid="73" ssid="24">This leads to more noisy estimates than in the source-target phrase-table.</S>
    <S sid="74" ssid="25">Secondly, the increased exposure to noise means that triangulation will omit a greater proportion of large or rare phrases than the standard method.</S>
    <S sid="75" ssid="26">An alignment error in either of the source-intermediate or intermediate-target bitexts can prevent the extraction of a source-target phrase pair.</S>
    <S sid="76" ssid="27">This effect can be seen in Figure 1, where the coverage of the Italian triangulated phrase-table is worse than the standard source-target model, despite the two models using the same sized bitexts.</S>
    <S sid="77" ssid="28">As we explain in the next section, these problems can be ameliorated by using the triangulated phrase-table in conjunction with a standard phrase-table.</S>
    <S sid="78" ssid="29">Finally, another potential problem stems from the independence assumption in (1), which may be an oversimplification and lead to a loss of information.</S>
    <S sid="79" ssid="30">The experiments in Section 5 show that this effect is only mild.</S>
    <S sid="80" ssid="31">Once induced, the triangulated phrase-table can be usefully combined with the standard source-target phrase-table.</S>
    <S sid="81" ssid="32">The simplest approach is to use linear interpolation to combine the two (or more) distributions, as follows: where each joint distribution, pj, has a non-negative weight, Aj, and the weights sum to one.</S>
    <S sid="82" ssid="33">The joint distribution for triangulated phrase-tables is defined in an analogous way to Equation (1).</S>
    <S sid="83" ssid="34">We expect that the standard phrase-table should be allocated a higher weight than triangulated phrase-tables, as it will be less noisy.</S>
    <S sid="84" ssid="35">The joint distribution is now conditionalised to yield p(s|t) and p(t|s), which are both used as features in the decoder.</S>
    <S sid="85" ssid="36">Note that the resulting conditional distribution will be drawn solely from one input distribution when the conditioning context is unseen in the remaining distributions.</S>
    <S sid="86" ssid="37">This may lead to an over-reliance on unreliable distributions, which can be ameliorated by smoothing (e.g., Foster et al. (2006)).</S>
    <S sid="87" ssid="38">As an alternative to linear interpolation, we also employ a weighted product for phrase-table combination: This has the same form used for log-linear training of SMT decoders (Och, 2003), which allows us to treat each distribution as a feature, and learn the mixing weights automatically.</S>
    <S sid="88" ssid="39">Note that we must individually smooth the component distributions in (3) to stop zeros from propagating.</S>
    <S sid="89" ssid="40">For this we use Simple Good-Turing smoothing (Gale and Sampson, 1995) for each distribution, which provides estimates for zero count events.</S>
  </SECTION>
  <SECTION title="4 Experimental Design" number="4">
    <S sid="90" ssid="1">Corpora We used the Europarl corpus (Koehn, 2005) for experimentation.</S>
    <S sid="91" ssid="2">This corpus consists of about 700,000 sentences of parliamentary proceedings from the European Union in eleven European languages.</S>
    <S sid="92" ssid="3">We present results on the full corpus for a range of language pairs.</S>
    <S sid="93" ssid="4">In addition, we have created smaller parallel corpora by sub-sampling 10,000 sentence bitexts for each language pair.</S>
    <S sid="94" ssid="5">These corpora are likely to have minimal overlap &#8212; about 1.5% of the sentences will be shared between each pair.</S>
    <S sid="95" ssid="6">However, the phrasal overlap is much greater (10 to 20%), which allows for triangulation using these common phrases.</S>
    <S sid="96" ssid="7">This training setting was chosen to simulate translating to or from a &#8220;low density&#8221; language, where only a few small independently sourced parallel corpora are available.</S>
    <S sid="97" ssid="8">These bitexts were used for direct translation and triangulation.</S>
    <S sid="98" ssid="9">All experimental results were evaluated on the ACL/WMT 20053 set of 2,000 sentences, and are reported in BLEU percentage-points.</S>
    <S sid="99" ssid="10">Decoding Pharaoh (Koehn, 2003), a beamsearch decoder, was used to maximise: where T and S denote a target and source sentence respectively.</S>
    <S sid="100" ssid="11">The parameters, Aj, were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set.</S>
    <S sid="101" ssid="12">We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts.</S>
    <S sid="102" ssid="13">The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003).</S>
    <S sid="103" ssid="14">Lexical weights The lexical translation score is used for smoothing the phrase-table translation estimate.</S>
    <S sid="104" ssid="15">This represents the translation probability of a phrase when it is decomposed into a series of independent word-for-word translation steps (Koehn et al., 2003), and has proven a very effective feature (Zens and Ney, 2004; Foster et al., 2006).</S>
    <S sid="105" ssid="16">Pharaoh&#8217;s lexical weights require access to word-alignments; calculating these alignments between the source and target words in a phrase would prove difficult for a triangulated model.</S>
    <S sid="106" ssid="17">Therefore we use a modified lexical score, corresponding to the maximum IBM model 1 score for the phrase pair: where the maximisation4 ranges over all one-tomany alignments and Z normalises the score by the number of possible alignments.</S>
    <S sid="107" ssid="18">The lexical probability is obtained by interpolating a relative frequency estimate on the sourcetarget bitext with estimates from triangulation, in the same manner used for phrase translations in (1) and (2).</S>
    <S sid="108" ssid="19">The addition of the lexical probability feature yielded a substantial gain of up to two BLEU points over a basic feature set.</S>
  </SECTION>
  <SECTION title="5 Experimental Results" number="5">
    <S sid="109" ssid="1">The evaluation of our method was motivated by three questions: (1) How do different training requirements affect the performance of the triangulated models presented in this paper?</S>
    <S sid="110" ssid="2">We expect performance gains with triangulation on small and moderate datasets.</S>
    <S sid="111" ssid="3">(2) Is machine translation output influenced by the choice of the intermediate language/s?</S>
    <S sid="112" ssid="4">Here, we would like to evaluate whether the number and choice of intermediate languages matters.</S>
    <S sid="113" ssid="5">(3) What is the quality of the triangulated phrase-table?</S>
    <S sid="114" ssid="6">In particular, we are interested in the resulting distribution and whether it is sufficiently distinct from the standard phrase-table.</S>
    <S sid="115" ssid="7">Before reporting our results, we briefly discuss the specific choice of model for our experiments.</S>
    <S sid="116" ssid="8">As mentioned in Section 3, our method combines the with 0-1 indicator features, and separate phrase-tables, respectively. triangulated phrase-table with the standard sourcetarget one.</S>
    <S sid="117" ssid="9">This is desired in order to compensate for the noise incurred by the triangulation process.</S>
    <S sid="118" ssid="10">We used two combination methods, namely linear interpolation (see (2)) and a weighted geometric mean (see (3)).</S>
    <S sid="119" ssid="11">Table 1 reports the results for two translation tasks when triangulating with a single language (es) using three different feature sets, each with different translation features.</S>
    <S sid="120" ssid="12">The interpolation model uses uniform linear interpolation to merge the standard and triangulated phrase-tables.</S>
    <S sid="121" ssid="13">Non-uniform mixtures did not provide consistent gains, although, as expected, biasing towards the standard phrasetable was more effective than against.</S>
    <S sid="122" ssid="14">The indicator model uses the same interpolated distribution along with a series of 0-1 indicator features to identify the source of each event, i.e., if each (s, t) pair is present in phrase-table j.</S>
    <S sid="123" ssid="15">We also tried per-context features with similar results.</S>
    <S sid="124" ssid="16">The separate model has a separate feature for each phrase-table.</S>
    <S sid="125" ssid="17">All three feature sets improve over the standard source-target system, while the interpolated features provided the best overall performance.</S>
    <S sid="126" ssid="18">The relatively poorer performance of the separate model is perhaps surprising, as it is able to differentially weight the component distributions; this is probably due to MERT not properly handling the larger feature sets.</S>
    <S sid="127" ssid="19">In all subsequent experiments we report results using linear interpolation.</S>
    <S sid="128" ssid="20">As a proof of concept, we first assessed the effect of triangulation on corpora consisting of 10,000 sentence bitexts.</S>
    <S sid="129" ssid="21">We expect triangulation to deliver performance gains on small corpora, since a large number of phrase-table entries will be unseen.</S>
    <S sid="130" ssid="22">In Table 2 each entry shows the BLEU score when using the standard phrase-table and the absolute improvement when using triangulation.</S>
    <S sid="131" ssid="23">Here we have used three languages for triangulation (it U {de, en, es, fr}\{s, t}).</S>
    <S sid="132" ssid="24">The source-target languages were chosen so as to mirror the evaluation setup of NAACL/WMT.</S>
    <S sid="133" ssid="25">The translation tasks range from easy (es &#8594; fr) to very hard (de &#8594; en).</S>
    <S sid="134" ssid="26">In all cases triangulation resulted in an improvement in translation quality, with the highest gains observed for the most difficult tasks (to and from German).</S>
    <S sid="135" ssid="27">For these tasks the standard systems have poor coverage (due in part to the sizeable vocabulary of German phrases) and therefore the gain can be largely explained by the additional coverage afforded by the triangulated phrase-tables.</S>
    <S sid="136" ssid="28">To test whether triangulation can also improve performance of larger corpora we ran six separate translation tasks on the full Europarl corpus.</S>
    <S sid="137" ssid="29">The results are presented in Table 3, for a single triangulation language used alone (triang) or uniformly interpolated with the standard phrase-table (interp).</S>
    <S sid="138" ssid="30">These results show that triangulation can produce high quality translations on its own, which is noteworthy, as it allows for SMT between a much larger set of language pairs.</S>
    <S sid="139" ssid="31">Using triangulation in conjunction with the standard phrase-table improved over the standard system in most instances, and only degraded performance once.</S>
    <S sid="140" ssid="32">The improvement is largest for the German tasks which can be explained by triangulation providing better robustness to noisy alignments (which are often quite poor for German) and better estimates of low-count events.</S>
    <S sid="141" ssid="33">The difficulty of aligning German with the other languages is apparent from the Giza++ perplexity: the final Model 4 perplexities for German are quite high, as much as double the perplexity for more easily aligned language pairs (e.g., Spanish-French).</S>
    <S sid="142" ssid="34">Figure 3 shows the effect of triangulation on different sized corpora for the language pair fr &#8594; en.</S>
    <S sid="143" ssid="35">It presents learning curves for the standard system and a triangulated system using one language (es).</S>
    <S sid="144" ssid="36">As can be seen, gains from triangulation only diminish slightly for larger training corpora, and that the purely triangulated models have very competitive performance.</S>
    <S sid="145" ssid="37">The gain from interpolation with a triangulated model is roughly equivalent to having twice as much training data.</S>
    <S sid="146" ssid="38">Finally, notice that triangulation may benefit when the sentences in each bitext are drawn from the same source, in that there are no unseen &#8216;intermediate&#8217; phrases, and therefore (1) can be easily evaluated.</S>
    <S sid="147" ssid="39">We investigate this by examining the robustness of our method in the face of disjoint bitexts.</S>
    <S sid="148" ssid="40">The concepts contained in each bitext will be more varied, potentially leading to better coverage of the target language.</S>
    <S sid="149" ssid="41">In lieu of a study on different domain bitexts which we plan for the future, we bisected the Europarl corpus for fr &#8594; en, triangulating with Spanish.</S>
    <S sid="150" ssid="42">The triangulated models were presented with fr-es and es-en bitexts drawn from either the same half of the corpus or from different halves, resulting in scores of 28.37 and 28.13, respectively.5 These results indicate that triangulation is effective for disjoint bitexts, although ideally we would test this with independently sourced parallel texts.</S>
    <S sid="151" ssid="43">The previous experiments used an ad-hoc choice of &#8216;intermediate&#8217; language/s for triangulation, and we now examine which languages are most effective.</S>
    <S sid="152" ssid="44">Figure 4 shows the efficacy of the remaining nine languages when translating fr &#8212;* en.</S>
    <S sid="153" ssid="45">Minimum error-rate training was not used for this experiment, or the next shown in Figure 5, in order to highlight the effect of the changing translation estimates.</S>
    <S sid="154" ssid="46">Romance languages (es, it, pt) give the best results, both on their own and when used together with the standard phrase-table (using uniform interpolation); Germanic languages (de, nl, da, sv) are a distant second, with the less related Greek and Finnish the least useful.</S>
    <S sid="155" ssid="47">Interpolation yields an improvement for all &#8216;intermediate&#8217; languages, even Finnish, which has a very low score when used alone.</S>
    <S sid="156" ssid="48">The same experiment was repeated for en &#8212;* de translation with similar trends, except that the Germanic languages out-scored the Romance languages.</S>
    <S sid="157" ssid="49">These findings suggest that &#8216;intermediate&#8217; languages which exhibit a high degree of similarity with the source or target language are desirable.</S>
    <S sid="158" ssid="50">We conjecture that this is a consequence of better automatic word alignments and a generally easier translation task, as well as a better preservation of information between aligned phrases.</S>
    <S sid="159" ssid="51">Using a single language for triangulation clearly improves performance, but can we realise further improvements by using additional languages?</S>
    <S sid="160" ssid="52">Figure 5 shows the performance profile for fr &#8212;* en when adding languages in a fixed order.</S>
    <S sid="161" ssid="53">The languages were ordered by family, with Romance before Germanic before Greek and Finnish.</S>
    <S sid="162" ssid="54">Each addition results in an increase in performance, even for the final languages, from which we expect little information.</S>
    <S sid="163" ssid="55">The purely triangulated (triang) and interpolated scores (interp) are converging, suggesting that the source-target bitext is redundant given sufficient triangulated data.</S>
    <S sid="164" ssid="56">We obtained similar results for en &#8212;* de.</S>
    <S sid="165" ssid="57">Our experimental results so far have shown that triangulation is not a mere approximation of the source-target phrase-table, but that it extracts additional useful translation information.</S>
    <S sid="166" ssid="58">We now assess the phrase-table quality more directly.</S>
    <S sid="167" ssid="59">Comparative statistics of a standard and a triangulated phrase-table are given in Table 4.</S>
    <S sid="168" ssid="60">The coverage over source and target phrases is much higher in the standard table than the triangulated tables, which reflects the reduced ability of triangulation to extract large phrases &#8212; despite the large increase in the number of events.</S>
    <S sid="169" ssid="61">The table also shows the overlapping probability mass which measures the sum of probability in one table for which the events are present in the other.</S>
    <S sid="170" ssid="62">This shows that the majority of mass is shared by both tables (as joint distributions), although there are significant differences.</S>
    <S sid="171" ssid="63">The JensenShannon divergence is perhaps more appropriate for the comparison, giving a relatively high divergence of 0.3937.</S>
    <S sid="172" ssid="64">This augurs well for the combination of standard and triangulated phrase-tables, where diversity is valued.</S>
    <S sid="173" ssid="65">The decoding results (shown in Table 3 for fr &#8212;* en) indicate that the two methods have similar efficacy, and that their interpolated combination provides the best overall performance.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="6">
    <S sid="174" ssid="1">In this paper we have presented a novel method for obtaining more reliable translation estimates from small datasets.</S>
    <S sid="175" ssid="2">The key premise of our work is that multi-parallel data can be usefully exploited for improving the coverage and quality of phrase-based SMT.</S>
    <S sid="176" ssid="3">Our triangulation method translates from a source to a target via one or many intermediate languages.</S>
    <S sid="177" ssid="4">We present a generative formulation of this process and show how it can be used together with the entries of a standard source-target phrase-table.</S>
    <S sid="178" ssid="5">We observe large performance gains when translating with triangulated models trained on small datasets.</S>
    <S sid="179" ssid="6">Furthermore, when combined with a standard phrase-table, our models also yield performance improvements on larger datasets.</S>
    <S sid="180" ssid="7">Our experiments revealed that triangulation benefits from a large set of intermediate languages and that performance is increased when languages of the same family to the source or target are used as intermediates.</S>
    <S sid="181" ssid="8">We have just scratched the surface of the possibilities for the framework discussed here.</S>
    <S sid="182" ssid="9">Important future directions lie in combining triangulation with richer means of conventional smoothing and using triangulation to translate between low-density language pairs.</S>
    <S sid="183" ssid="10">Acknowledgements The authors acknowledge the support of EPSRC (grants GR/T04540/01 and GR/T04557/01).</S>
    <S sid="184" ssid="11">Special thanks to Markus Becker, Chris Callison-Burch, David Talbot and Miles Osborne for their helpful comments.</S>
  </SECTION>
</PAPER>
