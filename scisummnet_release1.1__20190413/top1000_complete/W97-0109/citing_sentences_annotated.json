[
  {
    "citance_No": 1, 
    "citing_paper_id": "P00-1014", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Patrick, Pantel | Dekang, Lin", 
    "raw_text": "The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997)", 
    "clean_text": "The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P00-1014", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Patrick, Pantel | Dekang, Lin", 
    "raw_text": "We describe the different classifiers below: cl base: the baseline described in Section 7.2clR1: uses a maximum entropy model (Ratnaparkhi et al, 1994 )clBR5: uses transformation-based learning (Brill and Resnik, 1994) cl CB: uses a backed-off model (Collins and Brooks, 1995 )clSN: induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 )clHR6: uses lexical preference (Hindle and Rooth, 1993 )clR2: uses a heuristic extraction of unambiguous attachments (Ratnaparkhi, 1998) cl Pl: uses the algorithm described in this paper Our classifier outperforms all previous unsupervised techniques and approaches the performance of supervised algorithm", 
    "clean_text": "clSN induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 ).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-3010", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Clayton, Greenberg", 
    "raw_text": "At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSDalgorithmas part of their decision tree system", 
    "clean_text": "At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P14-3010", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Clayton, Greenberg", 
    "raw_text": "Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet", 
    "clean_text": "Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P14-3010", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Clayton, Greenberg", 
    "raw_text": "The system achieved 88.1% accuracy on the entire test set and 90.8% accuracy on the subset of the test set in which all four of the words in the quadruple were present in WordNet.Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNetmorphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features", 
    "clean_text": "Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNet morphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P14-3010", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Clayton, Greenberg", 
    "raw_text": "We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997)", 
    "clean_text": "We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2 string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W06-2603", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Fabrizio, Costa | Sauro, Menchetti | Alessio, Ceroni | Andrea, Passerini | Paolo, Frasconi", 
    "raw_text": "Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997)", 
    "clean_text": "Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "I05-1017", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Daisuke, Kawahara | Sadao, Kurohashi", 
    "raw_text": "PP-Attachment Accuracies of Previous Work method accuracy our method SVM 87.25% supervised Ratnaphakhi et al, 1994 ME 81.6% Brill and Resnik, 1994 TBL 81.9% Collins and Brooks, 1995 back-off 84.5% Zavrel et al, 1997 NN 84.4% Stetina and Nagao, 1997 DT 88.1% Abney et al, 1999 boosting 84.6% Vanschoenwinkel and Manderick, 2003 SVM 84.8% Zhao and Lin, 2004 NN 86.5% unsupervised Ratnaparkhi, 1998 81.9% Pantel and Lin, 2000 84.3% ME: Maximum Entropy, TBL: Transformation-Based Learning, DT: Decision Tree, NN: Nearest Neighbor configurations (McNemar? s test; p &lt; 0.05)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W04-2410", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mark, McLauchlan", 
    "raw_text": "The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet", 
    "clean_text": "The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P08-1037", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Eneko, Agirre | Timothy, Baldwin | David, Martinez", 
    "raw_text": "The best published results over RRRare those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. Their work is particularly inspiring in that it significantly outperformed the plethora of lexicalised probabilistic models that had been proposed to that point, and has not been beaten in later attempts. In a recent paper, Atterer and Schu ?tze (2007 )criticised the RRR dataset because it assumes that an oracle parser provides the two hypothesised structures to choose between", 
    "clean_text": "The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P08-1037", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Eneko, Agirre | Timothy, Baldwin | David, Martinez", 
    "raw_text": "The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task", 
    "clean_text": "The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P06-2029", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Kilian A., Foth | Wolfgang, Menzel", 
    "raw_text": "Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998)", 
    "clean_text": "Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P06-2029", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Kilian A., Foth | Wolfgang, Menzel", 
    "raw_text": "The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy", 
    "clean_text": "The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "H05-1035", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Marian, Olteanu | Dan, Moldovan", 
    "raw_text": "The difference in accuracy between a SVM model applied to RRR dataset (RRR-basic experiment) and the same experiment applied to TB2 dataset (TB2 278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP Maximum entropy, words (Ratnaparkhi et al, 1994) 77.7 RRR Maximum entropy, words& amp; classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees& amp; WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavrel et al, 1997) 84.4 RRR LexSpace Maximum entropy, unsupervised (Ratnaparkhi, 1998) 81.9 Maximum entropy, supervised (Ratnaparkhi, 1998) 83.7 RRR Neural Nets (Alegre et al, 1999) 86.0 RRR WordNet Boosting (Abney et al, 1999) 84.4 RRR Semi-probabilistic (Pantel and Lin, 2000) 84.31 RRR Maximum entropy, ensemble (McLauchlan, 2001) 85.5 RRR LSA SVM (Vanschoenwinkel and Manderick, 2003) 84.8 RRR Nearest-neighbor (Zhao and Lin, 2004) 86.5 RRR DWS FN dataset ,w/o semantic features (FN-best-no-sem) 91.79 FN PR-WWW FN dataset ,w/ semantic features (FN-best-sem) 92.85 FN PR-WWW TB2 dataset, best feature set (TB2-best) 93.62 TB2 PR-WWW Table 5: Accuracy of PP-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]
