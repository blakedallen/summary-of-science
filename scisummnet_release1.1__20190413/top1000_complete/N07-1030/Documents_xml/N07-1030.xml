<PAPER>
  <S sid="0">Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.</S>
    <S sid="2" ssid="2">In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="3" ssid="1">The task of coreference resolution involves imposing a partition on a set of entity mentions in a document, where each partition corresponds to some entity in an underlying discourse model.</S>
    <S sid="4" ssid="2">Most work treats coreference resolution as a binary classification task in which each decision is made in a pairwise fashion, independently of the others (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004).</S>
    <S sid="5" ssid="3">There are two major drawbacks with most systems that make pairwise coreference decisions.</S>
    <S sid="6" ssid="4">The first is that identification of anaphora is done implicitly as part of the coreference resolution.</S>
    <S sid="7" ssid="5">Two common types of errors with these systems are cases where: (i) the system mistakenly identifies an antecedent for non-anaphoric mentions, and (ii) the system does not try to resolve an actual anaphoric mention.</S>
    <S sid="8" ssid="6">To reduce such errors, Ng and Cardie (2002a) and Ng (2004) use an anaphoricity classifier &#8211;which has the sole task of saying whether or not any antecedents should be identified for each mention&#8211; as a filter for their coreference system.</S>
    <S sid="9" ssid="7">They achieve higher performance by doing so; however, their setup uses the two classifiers in a cascade.</S>
    <S sid="10" ssid="8">This requires careful determination of an anaphoricity threshold in order to not remove too many mentions from consideration (Ng, 2004).</S>
    <S sid="11" ssid="9">This sensitivity is unsurprising, given that the tasks are codependent.</S>
    <S sid="12" ssid="10">The second problem is that most coreference systems make each decision independently of previous ones in a greedy fashion (McCallum and Wellner, 2004).</S>
    <S sid="13" ssid="11">Clearly, the determination of membership of a particular mention into a partition should be conditioned on how well it matches the entity as a whole.</S>
    <S sid="14" ssid="12">Since independence between decisions is an unwarranted assumption for the task, models that consider a more global context are likely to be more appropriate.</S>
    <S sid="15" ssid="13">Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers.</S>
    <S sid="16" ssid="14">In this paper, we propose to recast the task of coreference resolution as an optimization problem, namely an integer linear programming (ILP) problem.</S>
    <S sid="17" ssid="15">This framework has several properties that make it highly suitable for addressing the two aforementioned problems.</S>
    <S sid="18" ssid="16">The first is that it can utilize existing classifiers; ILP performs global inference based on their output rather than formulating a new inference procedure for solving the basic task.</S>
    <S sid="19" ssid="17">Second, the ILP approach supports inference over multiple classifiers, without having to fiddle with special parameterization.</S>
    <S sid="20" ssid="18">Third, it is much more efficient than conditional random fields, especially when long-distance features are utilized (Roth and Yih, 2005).</S>
    <S sid="21" ssid="19">Finally, it is straightforward to create categorical global constraints with ILP; this is done in a declarative manner using inequalities on the assignments to indicator variables.</S>
    <S sid="22" ssid="20">This paper focuses on the first problem, and proposes to model anaphoricity determination and coreference resolution as a joint task, wherein the decisions made by each locally trained model are mutually constrained.</S>
    <S sid="23" ssid="21">The presentation of the ILP model proceeds in two steps.</S>
    <S sid="24" ssid="22">In the first, intermediary step, we simply use ILP to find a global assignment based on decisions made by the coreference classifier alone.</S>
    <S sid="25" ssid="23">The resulting assignment is one that maximally agrees with the decisions of the classifier, that is, where all and only the links predicted to be coreferential are used for constructing the chains.</S>
    <S sid="26" ssid="24">This is in contrast with the usual clustering algorithms, in which a unique antecedent is typically picked for each anaphor (e.g., the most probable or the most recent).</S>
    <S sid="27" ssid="25">The second step provides the joint formulation: the coreference classifier is now combined with an anaphoricity classifier and constraints are added to ensure that the ultimate coreference and anaphoricity decisions are mutually consistent.</S>
    <S sid="28" ssid="26">Both of these formulations achieve significant performance gains over the base classifier.</S>
    <S sid="29" ssid="27">Specifically, the joint model achieves f-score improvements of 3.7-5.3% on three datasets.</S>
    <S sid="30" ssid="28">We begin by presenting the basic coreference classifier and anaphoricity classifier and their performance, including an upperbound that shows the limitation of using them in a cascade.</S>
    <S sid="31" ssid="29">We then give the details of our ILP formulations and evaluate their performance with respect to each other and the base classifier.</S>
  </SECTION>
  <SECTION title="2 Base models: coreference classifier" number="2">
    <S sid="32" ssid="1">The classification approach tackles coreference in two steps by: (i) estimating the probability, PC(COREF|hi, ji), of having a coreferential outcome given a pair of mentions hi, ji, and (ii) applying a selection algorithm that will single out a unique candidate out of the subset of candidates i for which the probability PC(COREF|hi, ji) reaches a particular value (typically .5).</S>
    <S sid="33" ssid="2">We use a maximum entropy model for the coreference classifier.</S>
    <S sid="34" ssid="3">Such models are well-suited for coreference, because they are able to handle many different, potentially overlapping learning features without making independence assumptions.</S>
    <S sid="35" ssid="4">Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000).</S>
    <S sid="36" ssid="5">The model is defined in a standard fashion as follows: comes (COREF and &#172;COREF).</S>
    <S sid="37" ssid="6">Model parameters are estimated using maximum entropy (Berger et al., 1996).</S>
    <S sid="38" ssid="7">Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002).</S>
    <S sid="39" ssid="8">We use a Gaussian prior with a variance of 1000 &#8212; no attempt was made to optimize this value.</S>
    <S sid="40" ssid="9">Training instances for the coreference classifier are constructed based on pairs of mentions of the form hi, ji, where j and i are the descriptions for an anaphor and one of its candidate antecedents, respectively.</S>
    <S sid="41" ssid="10">Each such pair is assigned either a label COREF (i.e. a positive instance) or a label &#172;COREF (i.e. a negative instance) depending on whether or not the two mentions corefer.</S>
    <S sid="42" ssid="11">In generating the training data, we followed the method of (Soon et al., 2001) creating for each anaphor: (i) a positive instance for the pair hi, ji where i is the closest antecedent for j, and (ii) a negative instance for each pair hi, ki where k intervenes between i and j.</S>
    <S sid="43" ssid="12">Once trained, the classifier is used to create a set of coreferential links for each test document; these links in turn define a partition over the entire set of mentions.</S>
    <S sid="44" ssid="13">In the system of Soon et. al.</S>
    <S sid="45" ssid="14">(2001) system, this is done by pairing each mention j with each preceding mention i.</S>
    <S sid="46" ssid="15">Each test instance hi, ji thus formed is then evaluated by the classifier, which returns a probability representing the likelihood that these two mentions are coreferential.</S>
    <S sid="47" ssid="16">Soon et. al.</S>
    <S sid="48" ssid="17">(2001) use &#8220;Closest-First&#8221; selection: that is, the process terminates as soon as an antecedent (i.e., a test instance with probability &gt; .5) is found or the beginning of the text is reached.</S>
    <S sid="49" ssid="18">Another option is to pick the antecedent with the best overall probability (Ng and Cardie, 2002b).</S>
    <S sid="50" ssid="19">Our features for the coreference classifier fall into three main categories: (i) features of the anaphor, (ii) features of antecedent mention, and (iii) relational features (i.e., features that describe properties which hold between the two mentions, e.g. distance).</S>
    <S sid="51" ssid="20">This feature set is similar (though not equivalent) to that used by Ng and Cardie (2002a).</S>
    <S sid="52" ssid="21">We omit details here for the sake of brevity &#8212; the ILP systems we employ here could be equally well applied to many different base classifiers using many different feature sets.</S>
  </SECTION>
  <SECTION title="3 Base models: anaphoricity classifier" number="3">
    <S sid="53" ssid="1">As mentioned in the introduction, coreference classifiers such as that presented in section 2 suffer from errors in which (a) they assign an antecedent to a non-anaphor mention or (b) they assign no antecedents to an anaphoric mention.</S>
    <S sid="54" ssid="2">Ng and Cardie (2002a) suggest overcoming such failings by augmenting their coreference classifier with an anaphoricity classifier which acts as a filter during model usage.</S>
    <S sid="55" ssid="3">Only the mentions that are deemed anaphoric are considered for coreference resolution.</S>
    <S sid="56" ssid="4">Interestingly, they find a degredation in performance.</S>
    <S sid="57" ssid="5">In particular, they obtain significant improvements in precision, but with larger losses in recall (especially for proper names and common nouns).</S>
    <S sid="58" ssid="6">To counteract this, they add ad hoc constraints based on string matching and extended mention matching which force certain mentions to be resolved as anaphors regardless of the anaphoricity classifier.</S>
    <S sid="59" ssid="7">This allows them to improve overall f-scores by 1-3%.</S>
    <S sid="60" ssid="8">Ng (2004) obtains f-score improvements of 2.8-4.5% by tuning the anaphoricity threshold on held-out data.</S>
    <S sid="61" ssid="9">The task for the anaphoricity determination component is the following: one wants to decide for each mention i in a document whether i is anaphoric or not.</S>
    <S sid="62" ssid="10">That is, this task can be performed using a simple binary classifier with two outcomes: ANAPH and ANAPH.</S>
    <S sid="63" ssid="11">The classifier estimates the conditional probabilities P(ANAPH|i) and predicts ANAPH for i when P(ANAPH|i) &gt; .5.</S>
    <S sid="64" ssid="12">We use the following model for our anaphoricity classifier: This model is trained in the same manner as the coreference classifier, also with a Gaussian prior with a variance of 1000.</S>
    <S sid="65" ssid="13">The features used for the anaphoricity classifier are quite simple.</S>
    <S sid="66" ssid="14">They include information regarding (1) the mention itself, such as the number of words and whether it is a pronoun, and (2) properties of the potential antecedent set, such as the number of preceding mentions and whether there is a previous mention with a matching string.</S>
  </SECTION>
  <SECTION title="4 Base model results" number="4">
    <S sid="67" ssid="1">This section provides the performance of the pairwise coreference classifier, both when used alone (COREF-PAIRWISE) and when used in a cascade where the anaphoricity classifier acts as a filter on which mentions should be resolved (AC-CASCADE).</S>
    <S sid="68" ssid="2">In both systems, antecedents are determined in the manner described in section 2.</S>
    <S sid="69" ssid="3">To demonstrate the inherent limitations of cascading, we also give results for an oracle system, ORACLE-LINK, which assumes perfect linkage.</S>
    <S sid="70" ssid="4">That is, it always picks the correct antecedent for an anaphor.</S>
    <S sid="71" ssid="5">Its only errors are due to being unable to resolve mentions which were marked as nonanaphoric (by the imperfect anaphoricity classifier) when in fact they were anaphoric.</S>
    <S sid="72" ssid="6">We evaluate these systems on the datasets from the ACE corpus (Phase 2).</S>
    <S sid="73" ssid="7">This corpus is divided into three parts, each corresponding to a different genre: newspaper texts (NPAPER), newswire texts (NWIRE), and broadcasted news transcripts (BNEWS).</S>
    <S sid="74" ssid="8">Each of these is split into a train part and a devtest part.</S>
    <S sid="75" ssid="9">Progress during the development phase was determined by using crossvalidation on only the training set for the NPAPER (COREF-PAIRWISE), the anaphoricity-coreference cascade system (AC-CASCADE), and the oracle which performs perfect linkage (ORACLE-LINK).</S>
    <S sid="76" ssid="10">The first two systems make strictly local pairwise coreference decisions. section.</S>
    <S sid="77" ssid="11">No human-annotated linguistic information is used in the input.</S>
    <S sid="78" ssid="12">The corpus text was preprocessed with the OpenNLP Toolkit2 (i.e., a sentence detector, a tokenizer, a POS tagger, and a Named Entity Recognizer).</S>
    <S sid="79" ssid="13">In our experiments, we consider only the true ACE mentions.</S>
    <S sid="80" ssid="14">This is because our focus is on evaluating pairwise local approaches versus the global ILP approach rather than on building a full coreference resolution system.</S>
    <S sid="81" ssid="15">It is worth noting that previous work tends to be vague in both these respects: details on mention filtering or providing performance figures for markable identification are rarely given.</S>
    <S sid="82" ssid="16">Following common practice, results are given in terms of recall and precision according to the standard model-theoretic metric (Vilain et al., 1995).</S>
    <S sid="83" ssid="17">This method operates by comparing the equivalence classes defined by the resolutions produced by the system with the gold standard classes: these are the two &#8220;models&#8221;.</S>
    <S sid="84" ssid="18">Roughly, the scores are obtained by determining the minimal perturbations brought to one model in order to map it onto the other model.</S>
    <S sid="85" ssid="19">Recall is computed by trying to map the predicted chains onto the true chains, while precision is computed the other way around.</S>
    <S sid="86" ssid="20">We test significant differences with paired t-tests (p &lt; .05).</S>
    <S sid="87" ssid="21">The anaphoricity classifier has an average accuracy of 80.2% on the three ACE datasets (using a threshold of .5).</S>
    <S sid="88" ssid="22">This score is slightly lower than the scores reported by Ng and Cardie (2002a) for another data set (MUC).</S>
    <S sid="89" ssid="23">Table 1 summarizes the results, in terms of recall (R), precision (P), and f-score (F) on the three ACE data sets.</S>
    <S sid="90" ssid="24">As can be seen, the AC-CASCADE system generally provides slightly better precision at the expense of recall than the COREF-PAIRWISE system, but the performance varies across the three datasets.</S>
    <S sid="91" ssid="25">The source of this variance is likely due to the fact that we applied a uniform anaphoricity threshold of .5 across all datasets; Ng (2004) optimizes this threshold for each of the datasets: .3 for BNEWS and NWIRE and .35 for NPAPER.</S>
    <S sid="92" ssid="26">This variance reinforces our argument for determining anaphoricity and coreference jointly.</S>
    <S sid="93" ssid="27">The limitations of the cascade approach are also shown by the oracle results.</S>
    <S sid="94" ssid="28">Even if we had a system that can pick the correct antecedents for all truly anaphoric mentions, it would have a maximum recall of roughly 70% for the different datasets.</S>
  </SECTION>
  <SECTION title="5 Integer programming formulations" number="5">
    <S sid="95" ssid="1">The results in the previous section demonstrate the limitations of a cascading approach for determining anaphoricity and coreference with separate models.</S>
    <S sid="96" ssid="2">The other thing to note is that the results in general provide a lot of room for improvement &#8212; this is true for other state-of-the-art systems as well.</S>
    <S sid="97" ssid="3">The integer programming formulation we provide here has qualities which address both of these issues.</S>
    <S sid="98" ssid="4">In particular, we define two objective functions for coreference resolution to be optimized with ILP.</S>
    <S sid="99" ssid="5">The first uses only information from the coreference classifier (COREF-ILP) and the second integrates both anaphoricity and coreference in a joint formulation (JOINT-ILP).</S>
    <S sid="100" ssid="6">Our problem formulation and use of ILP are based on both (Roth and Yih, 2004) and (Barzilay and Lapata, 2006).</S>
    <S sid="101" ssid="7">For solving the ILP problem, we use lp solve, an open-source linear programming solver which implements the simplex and the Branch-and-Bound methods.3 In practice, each test document is processed to define a distinct ILP problem that is then submitted to the solver.</S>
    <S sid="102" ssid="8">Barzilay and Lapata (2006) use ILP for the problem of aggregation in natural language generation: clustering sets of propositions together to create more concise texts.</S>
    <S sid="103" ssid="9">They cast it as a set partitioning problem.</S>
    <S sid="104" ssid="10">This is very much like coreference, where each partition corresponds to an entity in a discourse model.</S>
    <S sid="105" ssid="11">COREF-ILP uses an objective function that is based on only the coreference classifier and the probabilities it produces.</S>
    <S sid="106" ssid="12">Given that the classifier produces probabilities pC = PC(COREF|i, j), the assignment cost of commiting to a coreference link is cC(i,j) = &#8722;log(pC).</S>
    <S sid="107" ssid="13">A complement assignment cost cC(i,j) = &#8722;log(1&#8722;pC) is associated with choosing not to establish a link.</S>
    <S sid="108" ssid="14">In what follows, M denotes the set of mentions in the document, and P the set of possible coreference links over these mentions (i.e., P = {hi, ji|hi, ji &#8712; M &#215; M and i &lt; j}).</S>
    <S sid="109" ssid="15">Finally, we use indicator variables x(i,j) that are set to 1 if mentions i and j are coreferent, and 0 otherwise.</S>
    <S sid="110" ssid="16">The objective function takes the following form: This is essentially identical to Barzilay and Lapata&#8217;s objective function, except that we consider only pairs in which the i precedes the j (due to the structure of the problem).</S>
    <S sid="111" ssid="17">Also, we minimize rather than maximize due to the fact we transform the model probabilities with &#8722;log (like Roth and Yih (2004)).</S>
    <S sid="112" ssid="18">This preliminary objective function merely guarantees that ILP will find a global assignment that maximally agrees with the decisions made by the coreference classifier.</S>
    <S sid="113" ssid="19">Concretely, this amounts to taking all (and only) those links for which the classifier returns a probability above .5.</S>
    <S sid="114" ssid="20">This formulation does not yet take advantage of information from a classifier that specializes in anaphoricity; this is the subject of the next section.</S>
    <S sid="115" ssid="21">Roth and Yih (2004) use ILP to deal with the joint inference problem of named entity and relation identification.</S>
    <S sid="116" ssid="22">This requires labeling a set of named entities in a text with labels such as person and location, and identifying relations between them such as spouse of and work for.</S>
    <S sid="117" ssid="23">In theory, each of these tasks would likely benefit from utilizing the information produced by the other, but if done as a cascade will be subject to propogation of errors.</S>
    <S sid="118" ssid="24">Roth and Yih thus set this up as problem in which each task is performed separately; their output is used to assign costs associated with indicator variables in an objective function, which is then minimized subject to constraints that relate the two kinds of outputs.</S>
    <S sid="119" ssid="25">These constraints express qualities of what a global assignment of values for these tasks must respect, such as the fact that the arguments to the spouse of relation must be entities with person labels.</S>
    <S sid="120" ssid="26">Importantly, the ILP objective function encodes not only the best label produced by each classifier for each decision; it utilizes the probabilities (or scores) assigned to each label and attempts to find a global optimum (subject to the constraints).</S>
    <S sid="121" ssid="27">The parallels to our anaphoricity/coreference scenario are straightforward.</S>
    <S sid="122" ssid="28">The anaphoricity problem is like the problem of identifying the type of entity (where the labels are now ANAPH and &#172;ANAPH), and the coreference problem is like that of determining the relations between mentions (where the labels are now COREF or &#172;COREF).</S>
    <S sid="123" ssid="29">Based on these parallels, the JOINT-ILP system brings the two decisions of anaphoricity and coreference together by including both in a single objective function and including constraints that ensure the consistency of a solution for both tasks.</S>
    <S sid="124" ssid="30">Let cAj and cAj be defined analogously to the coreference classifier costs for pA = PA(ANAPH|j), the probability the anaphoricity classifier assigns to a mention j being anaphoric.</S>
    <S sid="125" ssid="31">Also, we have indicator variables yj that are set to 1 if mention j is anaphoric and 0 otherwise.</S>
    <S sid="126" ssid="32">The objective function takes the following form: The structure of this objective function is very similar to Roth and Yih&#8217;s, except that we do not utilize constraint costs in the objective function itself.</S>
    <S sid="127" ssid="33">Roth and Yih use these to make certain combinations impossible (like a location being an argument to a spouse of relation); we enforce such effects in the constraint equations instead.</S>
    <S sid="128" ssid="34">The joint objective function (5) does not constrain the assignment of the xhi,ji and yj variables to be consistent with one another.</S>
    <S sid="129" ssid="35">To enforce consistency, we add further constraints.</S>
    <S sid="130" ssid="36">In what follows, Mj is the set of all mentions preceding mention j in the document.</S>
    <S sid="131" ssid="37">Resolve only anaphors: if a pair of mentions hi, ji is coreferent (xhi,ji=1), then mention j must be anaphoric (yj=1).</S>
    <S sid="132" ssid="38">These constraints thus directly relate the two tasks.</S>
    <S sid="133" ssid="39">By formulating the problem this way, the decisions of the anaphoricity classifier are not taken on faith as they were with AC-CASCADE.</S>
    <S sid="134" ssid="40">Instead, we optimize over consideration of both possibilities in the objective function (relative to the probability output by the classifier) while ensuring that the final assignments respect the signifance of what it is to be anaphoric or non-anaphoric.</S>
  </SECTION>
  <SECTION title="6 Joint Results" number="6">
    <S sid="135" ssid="1">Table 2 summarizes the results for these different systems.</S>
    <S sid="136" ssid="2">Both ILP systems are significantly better than the baseline system COREF-PAIRWISE.</S>
    <S sid="137" ssid="3">Despite having lower precision than COREF-PAIRWISE, the COREF-ILP system obtains very large gains in recall to end up with overall f-score gains of 4.3%, 4.2%, and 3.0% across BNEWS, NPAPER, and NWIRE, respectively.</S>
    <S sid="138" ssid="4">The fundamental reason for the increase in recall and drop in precision is that COREF-ILP can posit multiple antecedents for each mention.</S>
    <S sid="139" ssid="5">This is an extra degree of freedom that allows COREFILP to cast a wider net, with a consequent risk of capturing incorrect antecedents.</S>
    <S sid="140" ssid="6">Precision is not completely degraded because the optimization performed by ILP utilizes the pairwise probabilities of mention pairs as weights in the objective function to make its assignments.</S>
    <S sid="141" ssid="7">Thus, highly improbable links are still heavily penalized and are not chosen as coreferential.</S>
    <S sid="142" ssid="8">The JOINT-ILP system demonstrates the benefit ILP&#8217;s ability to support joint task formulations.</S>
    <S sid="143" ssid="9">It produces significantly better f-scores by regaining some of the ground on precision lost by COREFILP.</S>
    <S sid="144" ssid="10">The most likely source of the improved precision of JOINT-ILP is that weights corresponding to the anaphoricity probabilities and constraints (8) and (10) reduce the number of occurrences of nonanaphors being assigned antecedents.</S>
    <S sid="145" ssid="11">There are also improvements in recall over COREF-ILP for NPAPER and NWIRE.</S>
    <S sid="146" ssid="12">A possible source of this difference is constraint (9), which ensures that mentions which are considered anaphoric must have at least one antecedent.</S>
    <S sid="147" ssid="13">Compared to COREF-PAIRWISE, JOINT-ILP dramatically improves recall with relatively small losses in precision, providing overall f-score gains of 5.3%, 4.9%, and 3.7% on the three datasets.</S>
  </SECTION>
  <SECTION title="7 Related Work" number="7">
    <S sid="148" ssid="1">As was just demonstrated, ILP provides a principled way to model dependencies between anaphoricity decisions and coreference decisions.</S>
    <S sid="149" ssid="2">In a similar manner, this framework could also be used to capture dependencies among coreference decisions themselves.</S>
    <S sid="150" ssid="3">This option &#8212;which we will leave for future work&#8212; would make such an approach akin to yj &#8805; Luo et al. (2004) use Bell trees to represent the search space of the coreference resolution problem (where each leaf is possible partition).</S>
    <S sid="151" ssid="4">The problem is thus recast as that of finding the &#8220;best&#8221; path through the tree.</S>
    <S sid="152" ssid="5">Given the rapidly growing size of Bell trees, Luo et al. resort to a beam search algorithm and various pruning strategies, potentially resulting in picking a non-optimal solution.</S>
    <S sid="153" ssid="6">The results provided by Luo et al. are difficult to compare with ours, since they use a different evaluation metric.</S>
    <S sid="154" ssid="7">Another global approach to coreference is the application of Conditional Random Fields (CRFs) (McCallum and Wellner, 2004).</S>
    <S sid="155" ssid="8">Although both are global approaches, CRFs and ILP have important differences.</S>
    <S sid="156" ssid="9">ILP uses separate local classifiers which are learned without knowledge of the output constraints and are then integrated into a larger inference task.</S>
    <S sid="157" ssid="10">CRFs estimate a global model that directly uses the constraints of the domain.</S>
    <S sid="158" ssid="11">This involves heavy computations which cause CRFs to generally be slow and inefficient (even using dynamic programming).</S>
    <S sid="159" ssid="12">Again, the results presented in McCallum and Wellner (2004) are hard to compare with our own results.</S>
    <S sid="160" ssid="13">They only consider proper names, and they only tackled the task of identifying the correct antecedent only for mentions which have a true antecedent.</S>
    <S sid="161" ssid="14">A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems.</S>
    <S sid="162" ssid="15">This approach proceeds by first generating 54 candidate partitions, which are each generated by a different system.</S>
    <S sid="163" ssid="16">These different coreference systems are obtained as combinations over three different learners (C4.5, Ripper, and Maxent), three sampling methods, two feature sets (Soon et al., 2001; Ng and Cardie, 2002b), and three clustering algorithms (Best-First, Closest-First, and aggressivemerge).</S>
    <S sid="164" ssid="17">The features used by the reranker are of two types: (i) partition-based features which are here simple functions of the local features, and (ii) method-based features which simply identify the coreference system used for generating the given partition.</S>
    <S sid="165" ssid="18">Although this approach leads to significant gains on the both the MUC and the ACE datasets, it has some weaknesses.</S>
    <S sid="166" ssid="19">Most importantly, the different systems employed for generating the different partitions are all instances of the local classification approach, and they all use very similar features.</S>
    <S sid="167" ssid="20">This renders them likely to make the same types of errors.</S>
    <S sid="168" ssid="21">The ILP approach could in fact be integrated with these other approaches, potentially realizing the advantages of multiple global systems, with ILP conducting their interactions.</S>
  </SECTION>
  <SECTION title="8 Conclusions" number="8">
    <S sid="169" ssid="1">We have provided two ILP formulations for resolving coreference and demonstrated their superiority to a pairwise classifier that makes its coreference assignments greedily.</S>
    <S sid="170" ssid="2">In particular, we have also shown that ILP provides a natural means to express the use of both anaphoricity classification and coreference classification in a single system, and that doing so provides even further performance improvements, specifically f-score improvements of 5.3%, 4.9%, and 3.7% over the base coreference classifier on the ACE datasets.</S>
    <S sid="171" ssid="3">With ILP, it is not necessary to carefully control the anaphoricity threshold.</S>
    <S sid="172" ssid="4">This is in stark contrast to systems which use the anaphoricity classifier as a filter for the coreference classifier in a cascade setup.</S>
    <S sid="173" ssid="5">The ILP objective function incorporates the probabilities produced by both classifiers as weights on variables that indicate the ILP assignments for those tasks.</S>
    <S sid="174" ssid="6">The indicator variables associated with those assignments allow several constraints between the tasks to be straightforwardly stated to ensure consistency to the assignments.</S>
    <S sid="175" ssid="7">We thus achieve large improvements with a simple formulation and no fuss.</S>
    <S sid="176" ssid="8">ILP solutions are also obtained very quickly for the objective functions and constraints we use.</S>
    <S sid="177" ssid="9">In future work, we will explore the use of global constraints, similar to those used by (Barzilay and Lapata, 2006) to improve both precision and recall.</S>
    <S sid="178" ssid="10">For example, we expect transitivity constraints over coreference pairs, as well as constraints on the entire partition (e.g., the number of entities in the document), to help considerably.</S>
    <S sid="179" ssid="11">We will also consider linguistic constraints (e.g., restrictions on pronouns) in order to improve precision.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="9">
    <S sid="180" ssid="1">We would like to thank Ray Mooney, Rohit Kate, and the three anonymous reviewers for their comments.</S>
    <S sid="181" ssid="2">This work was supported by NSF grant IIS0535154.</S>
  </SECTION>
</PAPER>
