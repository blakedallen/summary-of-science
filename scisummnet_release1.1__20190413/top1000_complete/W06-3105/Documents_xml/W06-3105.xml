<PAPER>
  <S sid="0">Why Generative Phrase Models Underperform Surface Heuristics</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We investigate why weights from generative models underperform heuristic estimates in phrasebased machine translation.</S>
    <S sid="2" ssid="2">We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics.</S>
    <S sid="3" ssid="3">The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM.</S>
    <S sid="4" ssid="4">In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can.</S>
    <S sid="5" ssid="5">Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score.</S>
    <S sid="6" ssid="6">We also show that interpolation of the two methods can result in a modest increase in BLEU score.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability.</S>
    <S sid="8" ssid="2">Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.</S>
    <S sid="9" ssid="3">One particularly surprising result is that a simple heuristic extraction algorithm based on surface statistics of a word-aligned training set outperformed the phrase-based generative model proposed by Marcu and Wong (2002).</S>
    <S sid="10" ssid="4">This result is surprising in light of the reverse situation for word-based statistical translation.</S>
    <S sid="11" ssid="5">Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IBM word alignment models (Brown et al., 1993).</S>
    <S sid="12" ssid="6">This well-known result is unsurprising: reestimation introduces an element of competition into the learning process.</S>
    <S sid="13" ssid="7">The key virtue of competition in word alignment is that, to a first approximation, only one source word should generate each target word.</S>
    <S sid="14" ssid="8">If a good alignment for a word token is found, other plausible alignments are explained away and should be discounted as incorrect for that token.</S>
    <S sid="15" ssid="9">As we show in this paper, this effect does not prevail for phrase-level alignments.</S>
    <S sid="16" ssid="10">The central difference is that phrase-based models, such as the ones presented in section 2 or Marcu and Wong (2002), contain an element of segmentation.</S>
    <S sid="17" ssid="11">That is, they do not merely learn correspondences between phrases, but also segmentations of the source and target sentences.</S>
    <S sid="18" ssid="12">However, while it is reasonable to suppose that if one alignment is right, others must be wrong, the situation is more complex for segmentations.</S>
    <S sid="19" ssid="13">For example, if one segmentation subsumes another, they are not necessarily incompatible: both may be equally valid.</S>
    <S sid="20" ssid="14">While in some cases, such as idiomatic vs. literal translations, two segmentations may be in true competition, we show that the most common result is for different segmentations to be recruited for different examples, overfitting the training data and overly determinizing the phrase translation estimates.</S>
    <S sid="21" ssid="15">In this work, we first define a novel (but not radical) generative phrase-based model analogous to IBM Model 3.</S>
    <S sid="22" ssid="16">While its exact training is intractable, we describe a training regime which uses wordlevel alignments to constrain the space of feasible segmentations down to a manageable number.</S>
    <S sid="23" ssid="17">We demonstrate that the phrase analogue of the Dice coefficient is superior to our generative model (a result also echoing previous work).</S>
    <S sid="24" ssid="18">In the primary contribution of the paper, we present a series of experiments designed to elucidate what re-estimation learns in this context.</S>
    <S sid="25" ssid="19">We show that estimates are overly determinized because segmentations are used in unintuitive ways for the sake of data likelihood.</S>
    <S sid="26" ssid="20">We comment on both the beneficial instances of segment competition (idioms) as well as the harmful ones (most everything else).</S>
    <S sid="27" ssid="21">Finally, we demonstrate that interpolation of the two estimates can provide a modest increase in BLEU score over the heuristic baseline.</S>
  </SECTION>
  <SECTION title="2 Approach and Evaluation Methodology" number="2">
    <S sid="28" ssid="1">The generative model defined below is evaluated based on the BLEU score it produces in an endto-end machine translation system from English to French.</S>
    <S sid="29" ssid="2">The top-performing diag-and extraction heuristic (Zens et al., 2002) serves as the baseline for evaluation.1 Each approach &#8211; the generative model and heuristic baseline &#8211; produces an estimated conditional distribution of English phrases given French phrases.</S>
    <S sid="30" ssid="3">We will refer to the distribution derived from the baseline heuristic as &#966;H.</S>
    <S sid="31" ssid="4">The distribution learned via the generative model, denoted &#966;EM, is described in detail below.</S>
    <S sid="32" ssid="5">While our model for computing &#966;EM is novel, it is meant to exemplify a class of models that are not only clear extensions to generative word alignment models, but also compatible with the statistical framework assumed during phrase-based decoding.</S>
    <S sid="33" ssid="6">The generative process we modeled produces a phrase-aligned English sentence from a French sentence where the former is a translation of the latter.</S>
    <S sid="34" ssid="7">Note that this generative process is opposite to the translation direction of the larger system because of the standard noisy-channel decomposition.</S>
    <S sid="35" ssid="8">The learned parameters from this model will be used to translate sentences from English to French.</S>
    <S sid="36" ssid="9">The generative process modeled has four steps:2 The corresponding probabilistic model for this generative process is: where P(e, &#175;fi , &#175;ei, a|f) factors into a segmentation model &#963;, a translation model &#966; and a distortion model d. The parameters for each component of this model are estimated differently: ing function based on absolute sentence position akin to the one used in IBM model 3.</S>
    <S sid="37" ssid="10">While similar to the joint model in Marcu and Wong (2002), our model takes a conditional form compatible with the statistical assumptions used by the Pharaoh decoder.</S>
    <S sid="38" ssid="11">Thus, after training, the parameters of the phrase translation model &#966;EM can be used directly for decoding.</S>
    <S sid="39" ssid="12">Significant approximation and pruning is required to train a generative phrase model and table &#8211; such as &#966;EM &#8211; with hidden segmentation and alignment variables using the expectation maximization algorithm (EM).</S>
    <S sid="40" ssid="13">Computing the likelihood of the data for a set of parameters (the e-step) involves summing over exponentially many possible segmentations for each training sentence.</S>
    <S sid="41" ssid="14">Unlike previous attempts to train a similar model (Marcu and Wong, 2002), we allow information from a word-alignment model to inform our approximation.</S>
    <S sid="42" ssid="15">This approach allowed us to directly estimate translation probabilities even for rare phrase pairs, which were estimated heuristically in previous work.</S>
    <S sid="43" ssid="16">In each iteration of EM, we re-estimate each phrase translation probability by summing fractional phrase counts (soft counts) from the data given the current model parameters.</S>
    <S sid="44" ssid="17">This training loop necessitates approximation because summing over all possible segmentations and alignments for each sentence is intractable, requiring time exponential in the length of the sentences.</S>
    <S sid="45" ssid="18">Additionally, the set of possible phrase pairs grows too large to fit in memory.</S>
    <S sid="46" ssid="19">Using word alignments, we can address both problems.4 In particular, we can determine for any aligned segmentation ( 1I1, eI1, a) whether it is compatible with the word-level alignment for the sentence pair.</S>
    <S sid="47" ssid="20">We define a phrase pair to be compatible with a word-alignment if no word in either phrase is aligned with a word outside the other phrase (Zens et al., 2002).</S>
    <S sid="48" ssid="21">Then, ( 1I1, eI1, a) is compatible with the word-alignment if each of its aligned phrases is a compatible phrase pair.</S>
    <S sid="49" ssid="22">The training process is then constrained such that, when evaluating the above sum, only compatible aligned segmentations are considered.</S>
    <S sid="50" ssid="23">That is, we allow P(e, &#65533;fI1 , eI1, aIf) &gt; 0 only for aligned segmentations ( 1I1, eI1, a) such that a provides a oneto-one mapping from &#65533;fI1 to eI1 where all phrase pairs (&#65533;faj, ej) are compatible with the word alignment.</S>
    <S sid="51" ssid="24">This constraint has two important effects.</S>
    <S sid="52" ssid="25">First, we force P(ej |li) = 0 for all phrase pairs not compatible with the word-level alignment for some sentence pair.</S>
    <S sid="53" ssid="26">This restriction successfully reduced the total legal phrase pair types from approximately 250 million to 17 million for 100,000 training sentences.</S>
    <S sid="54" ssid="27">However, some desirable phrases were eliminated because of errors in the word alignments.</S>
    <S sid="55" ssid="28">Second, the time to compute the e-step is reduced.</S>
    <S sid="56" ssid="29">While in principle it is still intractable, in practice we can compute most sentence pairs&#8217; contributions in under a second each.</S>
    <S sid="57" ssid="30">However, some spurious word alignments can disallow all segmentations for a sentence pair, rendering it unusable for training.</S>
    <S sid="58" ssid="31">Several factors including errors in the word-level alignments, sparse word alignments and non-literal translations cause our constraint to rule out approximately 54% of the training set.</S>
    <S sid="59" ssid="32">Thus, the reduced size of the usable training set accounts for some of the degraded performance of OEM relative to OH.</S>
    <S sid="60" ssid="33">However, the results in figure 1 of the following section show that OEM trained on twice as much data as OH still underperforms the heuristic, indicating a larger issue than decreased training set size.</S>
    <S sid="61" ssid="34">To test the relative performance of OEM and OH, we evaluated each using an end-to-end translation system from English to French.</S>
    <S sid="62" ssid="35">We chose this nonstandard translation direction so that the examples in this paper would be more accessible to a primarily English-speaking audience.</S>
    <S sid="63" ssid="36">All training and test data were drawn from the French/English section of the Europarl sentence-aligned corpus.</S>
    <S sid="64" ssid="37">We tested on the first 1,000 unique sentences of length 5 to 15 in the corpus and trained on sentences of length 1 to 60 starting after the first 10,000.</S>
    <S sid="65" ssid="38">The system follows the structure proposed in the documentation for the Pharaoh decoder and uses many publicly available components (Koehn, 2003b).</S>
    <S sid="66" ssid="39">The language model was generated from the Europarl corpus using the SRI Language Modeling Toolkit (Stolcke, 2002).</S>
    <S sid="67" ssid="40">Pharaoh performed decoding using a set of default parameters for weighting the relative influence of the language, translation and distortion models (Koehn, 2003b).</S>
    <S sid="68" ssid="41">A maximum phrase length of three was used for all experiments.</S>
    <S sid="69" ssid="42">To properly compare OEM to OH, all aspects of the translation pipeline were held constant except for the parameters of the phrase translation table.</S>
    <S sid="70" ssid="43">In particular, we did not tune the decoding hyperparameters for the different phrase tables. pe</S>
  </SECTION>
  <SECTION title="3 Results 8" number="3">
    <S sid="71" ssid="1">Having generated OH heuristically and OEM with EM, we now0compare their performance.</S>
    <S sid="72" ssid="2">While the model and training regimen for OEM differ from the model fromMarcu and Wong (2002), we achieved tion maximization algorithm for training OEM was initialized with the heuristic parameters OH, so the heuristic curve can be equivalently labeled as iteration 0.</S>
    <S sid="73" ssid="3">Thus, the first iteration of EM increases the observed likelihood of the training sentences while simultaneously degrading translation performance on the test set.</S>
    <S sid="74" ssid="4">As training proceeds, performance on the test set levels off after three iterations of EM.</S>
    <S sid="75" ssid="5">The system never achieves the performance of its initialization parameters.</S>
    <S sid="76" ssid="6">The pruning of our training regimen accounts for part of this degradation, but not all; augmenting OEM by adding back in all phrase pairs that were dropped during training does not close the performance gap between OEM and OH.</S>
    <S sid="77" ssid="7">Learning OEM degrades translation quality in large part because EM learns overly determinized segmentations and translation parameters, overfitting the training data and failing to generalize.</S>
    <S sid="78" ssid="8">The primary increase in richness from generative wordlevel models to generative phrase-level models is due to the additional latent segmentation variable.</S>
    <S sid="79" ssid="9">Although we impose a uniform distribution over segmentations, it nonetheless plays a crucial role during training.</S>
    <S sid="80" ssid="10">We will characterize this phenomenon through aggregate statistics and translation examples shortly, but begin by demonstrating the model&#8217;s capacity to overfit the training data.</S>
    <S sid="81" ssid="11">Let us first return to the motivation behind introducing and learning phrases in machine translation.</S>
    <S sid="82" ssid="12">For any language pair, there are contiguous strings of words whose collocational translation is non-compositional; that is, they translate together differently than they would in isolation.</S>
    <S sid="83" ssid="13">For instance, chat in French generally translates to cat in English, but appeler un chat un chat is an idiom which translates to call a spade a spade.</S>
    <S sid="84" ssid="14">Introducing phrases allows us to translate chat un chat atomically to spade a spade and vice versa.</S>
    <S sid="85" ssid="15">While introducing phrases and parameterizing their translation probabilities with a surface heuristic allows for this possibility, statistical re-estimation would be required to learn that chat should never be translated to spade in isolation.</S>
    <S sid="86" ssid="16">Hence, translating I have a spade with OH could yield an error.</S>
    <S sid="87" ssid="17">But enforcing competition among segmentations introduces a new problem: true translation ambiguity can also be spuriously explained by the segmentation.</S>
    <S sid="88" ssid="18">Consider the french fragment carte sur la table, which could translate to map on the table or notice on the chart.</S>
    <S sid="89" ssid="19">Using these two sentence pairs as training, one would hope to capture the ambiguity in the parameter table as: Assuming we only allow non-degenerate segmentations and disallow non-monotonic alignments, this parameter table yields a marginal likelihood P(fle) = 0.25 for both sentence pairs &#8211; the intuitive result given two independent lexical ambiguHence, a higher likelihood can be achieved by allocating some phrases to certain translations while reserving overlapping phrases for others, thereby failing to model the real ambiguity that exists across the language pair.</S>
    <S sid="90" ssid="20">Also, notice that the phrase sur la can take on an arbitrary distribution over any english phrases without affecting the likelihood of either sentence pair.</S>
    <S sid="91" ssid="21">Not only does this counterintuitive parameterization give a high data likelihood, but it is also a fixed point of the EM algorithm.</S>
    <S sid="92" ssid="22">The phenomenon demonstrated above poses a problem for generative phrase models in general.</S>
    <S sid="93" ssid="23">The ambiguous process of translation can be modeled either by the latent segmentation variable or the phrase translation probabilities.</S>
    <S sid="94" ssid="24">In some cases, optimizing the likelihood of the training corpus adjusts for the former when we would prefer the latter.</S>
    <S sid="95" ssid="25">We next investigate how this problem manifests in &#966;EM and its effect on translation quality.</S>
    <S sid="96" ssid="26">The parameters of &#966;EM differ from the heuristically extracted parameters &#966;H in that the conditional distributions over English translations for some French words are sharply peaked for &#966;EM compared to flatter distributions generated by &#966;H.</S>
    <S sid="97" ssid="27">This determinism &#8211; predicted by the previous section&#8217;s example &#8211; is not atypical of EM training for other tasks.</S>
    <S sid="98" ssid="28">To quantify the notion of peaked distributions over phrase translations, we compute the entropy of the distribution for each French phrase according to the standard definition.</S>
    <S sid="99" ssid="29">The average entropy, weighted by frequency, for the most common 10,000 phrases in the learned table was 1.55, comparable to 3.76 for the heuristic table.</S>
    <S sid="100" ssid="30">The difference between the tables becomes much more striking when we consider the histogram of entropies for phrases in figure 2.</S>
    <S sid="101" ssid="31">In particular, the learned table has many more phrases with entropy near zero.</S>
    <S sid="102" ssid="32">The most pronounced entropy differences often appear for common phrases.</S>
    <S sid="103" ssid="33">Ten of the most common phrases in the French corpus are shown in figure 3.</S>
    <S sid="104" ssid="34">As more probability mass is reserved for fewer translations, many of the alternative translations under &#966;H are assigned prohibitively small probabilities.</S>
    <S sid="105" ssid="35">In translating 1,000 test sentences, for example, no phrase translation with &#966;(e |f) less than 10&#8722;5 was used by the decoder.</S>
    <S sid="106" ssid="36">Given this empirical threshold, nearly 60% of entries in &#966;EM are unusable, compared with 1% in &#966;H.</S>
    <S sid="107" ssid="37">While this determinism of &#966;EM may be desirable in some circumstances, we found that the ambiguity in &#966;H is often preferable at decoding time.</S>
    <S sid="108" ssid="38">Several learned distributions have very low entropy.</S>
    <S sid="109" ssid="39">30 In particular, the pattern of translation-ambiguous 0 phrases receiving spuriously peaked distributions (as 0 - 01 01 - .5 5 - 1 1 described in section 3.1) introduces new traslation Entropy errors relative to the baseline.</S>
    <S sid="110" ssid="40">We now investigate both positive and negative effects of the learning process.</S>
    <S sid="111" ssid="41">The issue that motivated training a generative model is sometimes resolved correctly: for a word that translates differently alone than in the context of an idiom, the translation probabilities can more accurately reflect this.</S>
    <S sid="112" ssid="42">Returning to the previous example, the phrase table for chat has been corrected through the learning process.</S>
    <S sid="113" ssid="43">The heuristic process gives the incorrect translation spade with 61% probability, while the statistical learning approach gives cat with 95% probability.</S>
    <S sid="114" ssid="44">While such examples of improvement are encouraging, the trend of spurious determinism overwhelms this benefit by introducing errors in four related ways, each of which will be explored in turn.</S>
    <S sid="115" ssid="45">The first effect follows from our observation in section 3.2 that many phrase pairs are unusable due to vanishingly small probabilities.</S>
    <S sid="116" ssid="46">Some of the entries that are made unusable by re-estimation are helpful at decoding time, evidenced by the fact that pruning the set of OEM&#8217;s low-scoring learned phrases from the original heuristic table reduces BLEU score by 0.02 for 25k training sentences (below the score for OEM).</S>
    <S sid="117" ssid="47">The second effect is more subtle.</S>
    <S sid="118" ssid="48">Consider the sentence in figure 4, which to a first approximation can be translated as a series of cognates, as demonstrated by the decoding that follows from the Heuristic heuristic parameterization OH.6 Notice also that the Learned translation probabilities from heuristic extraction are non-deterministic.</S>
    <S sid="119" ssid="49">On the other hand, the translation system makes a significant lexical error on this sim&gt; 2 ple sentence when parameterized by OEM: the use of caract&#180;erise in this context is incorrect.</S>
    <S sid="120" ssid="50">This error arises from a sharply peaked distribution over English phrases for caract&#180;erise.</S>
    <S sid="121" ssid="51">This example illustrates a recurring problem: errors do not necessarily arise because a correct translation is not available.</S>
    <S sid="122" ssid="52">Notice that a preferable translation of degree as degr&#180;e is available under both parameterizations.</S>
    <S sid="123" ssid="53">Degr&#180;e is not used, however, because of the peaked distribution of a competing translation candidate.</S>
    <S sid="124" ssid="54">In this way, very high probability translations can effectively block the use of more appropriate translations at decoding time.</S>
    <S sid="125" ssid="55">What is furthermore surprising and noteworthy in this example is that the learned, near-deterministic translation for caract&#180;erise is not a common translation for the word.</S>
    <S sid="126" ssid="56">Not only does the statistical learning process yield low-entropy translation distributions, but occasionally the translation with undesirably high conditional probability does not have a strong surface correlation with the source phrase.</S>
    <S sid="127" ssid="57">This example is not unique; during different initializations of the EM algorithm, we noticed such patterns even for common French phrases such as de and ne.</S>
    <S sid="128" ssid="58">The third source of errors is closely related: common phrases that translate in many ways depending on the context can introduce errors if they have a spuriously peaked distribution.</S>
    <S sid="129" ssid="59">For instance, consider the lone apostrophe, which is treated as a single token in our data set (figure 5).</S>
    <S sid="130" ssid="60">The shape of the heuristic translation distribution for the phrase is intuitively appealing, showing a relatively flat distribution among many possible translations.</S>
    <S sid="131" ssid="61">Such a distribution has very high entropy.</S>
    <S sid="132" ssid="62">On the other hand, the learned table translates the apostrophe to the with probability very near 1. phe, the most common french phrase.</S>
    <S sid="133" ssid="63">The learned table contains a highly peaked distribution.</S>
    <S sid="134" ssid="64">Such common phrases whose translation depends highly on the context are ripe for producing translation errors.</S>
    <S sid="135" ssid="65">The flatness of the distribution of OH ensures that the single apostrophe will rarely be used during decoding because no one phrase table entry has high enough probability to promote its use.</S>
    <S sid="136" ssid="66">On the other hand, using the peaked entry OEM(the|') incurs virtually no cost to the score of a translation.</S>
    <S sid="137" ssid="67">The final kind of errors stems from interactions between the language and translation models.</S>
    <S sid="138" ssid="68">The selection among translation choices via a language model &#8211; a key virtue of the noisy channel framework &#8211; is hindered by the determinism of the translation model.</S>
    <S sid="139" ssid="69">This effect appears to be less significant than the previous three.</S>
    <S sid="140" ssid="70">We should note, however, that adjusting the language and translation model weights during decoding does not close the performance gap between OH and OEM.</S>
    <S sid="141" ssid="71">In light of the low entropy of OEM, we could hope to improve translations by retaining entropy.</S>
    <S sid="142" ssid="72">There are several strategies we have considered to achieve this.</S>
    <S sid="143" ssid="73">Broadly, we have tried two approaches: combining OEM and OH via heuristic interpolation methods and modifying the training loop to limit determinism.</S>
    <S sid="144" ssid="74">The simplest strategy to increase entropy is to interpolate the heuristic and learned phrase tables.</S>
    <S sid="145" ssid="75">Varying the weight of interpolation showed an improvement over the heuristic of up to 0.01 for 100k sentences.</S>
    <S sid="146" ssid="76">A more modest improvement of 0.003 for 25k training sentences appears in table 1.</S>
    <S sid="147" ssid="77">In another experiment, we interpolated the output of each iteration of EM with its input, thereby maintaining some entropy from the initialization parameters.</S>
    <S sid="148" ssid="78">BLEU score increased to a maximum of 0.394 using this technique with 100k training sentences, outperforming the heuristic by a slim margin of 0.005.</S>
    <S sid="149" ssid="79">We might address the determinization in OEM without resorting to interpolation by modifying the training procedure to retain entropy.</S>
    <S sid="150" ssid="80">By imposing a non-uniform segmentation model that favors shorter phrases over longer ones, we hope to prevent the error-causing effects of EM training outlined above.</S>
    <S sid="151" ssid="81">In principle, this change will encourage EM to explain training sentences with shorter sentences.</S>
    <S sid="152" ssid="82">In practice, however, this approach has not led to an improvement in BLEU.</S>
    <S sid="153" ssid="83">Another approach to maintaining entropy during the training process is to smooth the probabilities generated by EM.</S>
    <S sid="154" ssid="84">In particular, we can use the following smoothed update equation during the training loop, which reserves a portion of probability mass for unseen translations.</S>
  </SECTION>
  <SECTION title="5 Acknowledgments" number="4">
    <S sid="155" ssid="1">We would like to thank the anonymous reviewers for their valuable feedback on this paper.</S>
    <S sid="156" ssid="2">In the equation above, l is the length of the French phrase and k is a tuning parameter.</S>
    <S sid="157" ssid="3">This formulation not only serves to reduce very spiked probabilities in OEM, but also boosts the probability of short phrases to encourage their use.</S>
    <S sid="158" ssid="4">With k = 2.5, this smoothing approach improves BLEU by .007 using 25k training sentences, nearly equaling the heuristic (table 1).</S>
  </SECTION>
  <SECTION title="4 Conclusion" number="5">
    <S sid="159" ssid="1">Re-estimating phrase translation probabilities using a generative model holds the promise of improving upon heuristic techniques.</S>
    <S sid="160" ssid="2">However, the combinatorial properties of a phrase-based generative model have unfortunate side effects.</S>
    <S sid="161" ssid="3">In cases of true ambiguity in the language pair to be translated, parameter estimates that explain the ambiguity using segmentation variables can in some cases yield higher data likelihoods by determinizing phrase translation estimates.</S>
    <S sid="162" ssid="4">However, this behavior in turn leads to errors at decoding time.</S>
    <S sid="163" ssid="5">We have also shown that some modest benefit can be obtained from re-estimation through the blunt instrument of interpolation.</S>
    <S sid="164" ssid="6">A remaining challenge is to design more appropriate statistical models which tie segmentations together unless sufficient evidence of true non-compositionality is present; perhaps such models could properly combine the benefits of both current approaches.</S>
  </SECTION>
</PAPER>
