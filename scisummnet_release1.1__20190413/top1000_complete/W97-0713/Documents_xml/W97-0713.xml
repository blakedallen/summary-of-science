<PAPER>
  <S sid="0">From Discourse Structures To Text Summaries</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &amp;quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &amp;quot;greatest&amp;quot; and &amp;quot;significant&amp;quot; or indiphrases as &amp;quot;the main aim of this paper&amp;quot; and &amp;quot;the purpose of this article&amp;quot;, while non-important senuse words as &amp;quot;impossible&amp;quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries &#8212; an empirical view</S>
  </ABSTRACT>
  <SECTION title="1 Motivation" number="1">
    <S sid="2" ssid="1">The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &amp;quot;convincing&amp;quot; samples of the output In very few cases, the direct output of a summarization program is compared with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use baps words such as &amp;quot;greatest&amp;quot; and &amp;quot;significant&amp;quot; or indicator phrases such as &amp;quot;the main aim of this paper&amp;quot; and &amp;quot;the purpose of this article&amp;quot;, while non-important sentences use stigma words such as &amp;quot;hardly&amp;quot; and &amp;quot;impossible&amp;quot; (Edmundson, 1968, Rush, Salvador, and Zamora, 1971), (vi) important sentences and concepts are the highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries &#8212; an empirical view</S>
  </SECTION>
  <SECTION title="2.1 Introduction" number="2">
    <S sid="3" ssid="1">Researchers in computational linguistics (Mann and Thompson, 1988, Matthiessen and Thompson, 1988, Sparck Jones, 1993) have long speculated that the nuclei that pertain to a rhetorical structure tree (RS-tree) (Mann and Thompson, 1988) constitute an adequate summanzation of the text for which that RS-tree was built However, to our knowledge, there was no experiment to confirm how valid this speculation really is In what follows, we describe an experiment that shows that there exists a strong correlation between the nuclei of the RS-tree of a text and what readers perceive to be the most important units in a text We know from the results reported in the psychological literature on summarization (Johnson, 1970, Chou Hare and Borchardt, 1984, Sherrard, 1989) that there exists a certain degree of disagreement between readers with respect to the importance that they assign to various textual units and that the disagreement is dependent on the quality of the text and the comprehension and summarization skills of the readers (Winograd.</S>
    <S sid="4" ssid="2">1984) in an attempt to produce an adequate reference set of data, we selected for our experiment five texts from Scientific American that we considered to be well-written The texts ranged in size from 161 to 725 words We used square brackets to enclose the minimal textual units (essentially the clauses) of each text Overall, the five texts were broken into 160 textual units with the shortest text being broken into 18 textual units, and the longest into 70 The shortest text is given in (1), below (here, for the purpose of reference, the minimal units are not only enclosed by square brackets, but also are numbered) sun than Earth --21 [and slim atmospheric blanket,3] [Mars experiences frigid weather conditions 4] [Surface temperatures typically average about &#8212;60 degrees Celsius (-76 degrees Fahrenheit) at the equator5] [and can dip to &#8212;123 degrees C near the poles'] [Only the midday sun at tropical latitudes is warm enough to thaw ice on occasion,/ [but any hquid water formed in this way would evaporate almost nistantlyg] [because of the low atmospheric pressure 9] [Although the atmosphere holds a small amount of water,19] [and water-ice clouds sometimes develop.11] [most Martian weather involves blowing dust or carbon dioxide 12] [Each winter, for example, a blizzard of frozen carbon dioxide rages over one pole,I3] [and a few meters of this dry-ice snow accumulatell [as previously frozen carbon dioxide evaporates from the opposite polar cap Is] [Yet even on the summer pole,16] [where the sun remains in the sky all day 1ong,171 (ternperatures never warm enough to melt frozen water Is] We followed Garner's (1982) strategy and asked 13 independent Judges to rate each textual unit according to its importance to a potential summary The judges used a three-point scale and assigned a score of 2 to the units that they believed to be very important and should appear in a concise summary, I to those they considered moderately important, which should appear in a long summary, and 0 to those they considered unimportant, which should not appear in any summary The judges were instructed that there were no nght or wrong answers and no upper or lower bounds with respect to the number of textual units that they should select as being important or moderately important The Judges were all graduate students in computer science, we assumed that they had developed adequate comprehension and summanzauon skills on their own, so no training session was carried out Table 1 presents the scores that were assigned by each Judge to the units in text (1) The same texts were also given to two computational linguists with solid knowledge of rhetorical structure theory(RST) The analysts were asked to build one RS-tree for each text We took then the RS-trees built by the analysts and used our formalization of RST (Marcu, 1996, Marcu, 1997b) to associate with each node in a tree its salient units The salient units were computed recursively, associating with each leaf in an RS-tree the leaf itself, and to each internal node the salient units of the nucleus or nuclei of the rhetorical relation corresponding to that node We then computed for each textual unit a score, depending on the depth in the tree where it occurred as a salient unit the textual units that were salient units of the top nodes in a tree had a higher score than those that were salient units of the nodes found at the bottom of a tree Essentially, from a rhetorical structure tree, we derived an importance score for each textual unit the importance scores ranged from 0 to n where n was the depth of the RS-treet Table 1 presents the scores that were derived from the RS-trees that were built by each analyst for text (1)</S>
  </SECTION>
  <SECTION title="2.2.2 Results" number="3">
    <S sid="5" ssid="1">Overall agreement among judges.</S>
    <S sid="6" ssid="2">We measured the ability of judges to agree with one another, using the notion of percent agreement that was defined by Gale (1992) and used extensively in discourse segmentation studies (Passonneau and Litman, 1993, Hearst, 1994) Percent agreement reflects the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion The percent agreements computed for. each of the five texts and each level of importance are given in table 2 The agreements among judges for our experiment seem to follow the same pattern as those described by other researchers in summarization (Johnson, 1970) That is, the judges are quite consistent with respect to what they perceive as being very important and unimportant, but less consistent with respect to what they perceive as being less important In contrast with the agreement observed among judges, the percentage agreements computed for 1000 importance assignments that were randomly generated for the same texts followed a normal distribution with p = 47 31,o = 004 These results suggest that the agreement among judges is significant Agreement among judges with respect to the importance of each textual unit.</S>
    <S sid="7" ssid="3">We considered a textual unit to be labeled consistently if a simple majority of the judges (&gt; 7) assigned the same score to that unit Over'Section 32 gives an example of how the Importance scores were computed all, the judges labeled consistently 140 of the 160 textual units (87%) In contrast, a set of 1000 randomly generated importance scores showed agreement, on average, for only 50 of the 160 textual units p1ng=0 05 The judges consistently labeled 36 of the units as very important, 8 as less important, and 96 as unimportant They were inconsistent with respect to 20 textual units For example, for text (1), the judges consistently labeled units 4 and 12 as very important, units 5 and 6 as less Iraportant, units 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 17 as unimportant, and were inconsistent in labeling unit 18 If we compute percent agreement figures only for the textual units for which at least 7 judges agreed, we get 69% for the units considered very important, 63% for those considered less Important, and 77% for those considered unimportant The overall percent agreement in this case is 75% Statistical significance.</S>
    <S sid="8" ssid="4">It has often been emphasized that agreement figures of the kinds computed above could be misleading (1Crippendorff, 1980, Passormeau and Litman, 1993) Since the &amp;quot;true&amp;quot; set of important textual units cannot be independently .known, we cannot compute how valid the importance assignments of the judges were Moreover, although the agreement figures that would occur by chance offer a strong indication that our data are reliable, they do not provide a precise measurement of reliability To compute a reliability figure, we followed the same methodology as Passonneau and Litman (1993) and Hearst (1994) and applied the Cochran's Q summary statistics to our data (Cochran, 1950) Cochran's test assumes that a set of judges make binary decisions with respect to a dataset The null hypothesis is that the number of judges that take the same decision is randomly distributed Since Cochran's test is appropriate only for binary judgments and since our main goal was to determine a reliability figure for the agreement among judges with respect to what they believe to be important, we evaluated two versions of the data that reflected only one importance level In the first version we considered as being important the judgments with a Sarre of 2 and unimportant the judgments with a score of 0 and 1 In the second version, we considered as being important the judgments with a score of 2 and 1 and unimportant the judgments with a score of 0 Essentially, we mapped the judgment matrices of each of the five texts into matrices whose elements ranged over only two values 0 and 1 After these modifications were made, we computed for each version and each text the Cochran statistics Q, which approximates the )(2 distribution with n &#8212; 1 degrees of freedom, where ri is the number of elements in the dataset In all cases we obtained probabilities that were very low p &lt; 10-6 MIS means that the agreement among judges was extremely significant Although the probability was very low for both versions, it was lower for the first version of the modified data than for the second This means that it is more reliable to consider as Important only the units that were assigned a score of 2 by a majority of the judges As we have already mentioned, our ultimate goal was to determine whether there exists a correlation between the units that judges find important and the units that have nuclear status in the rhetorical structure trees of the same texts Since the percentage agreement for the units that were considered very important was higher than the percentage agreement for the units that were considered less important, and since the Cochran's significance computed for the first version of the modified data was higher that the one computed for the second, we decided to consider the set of 36 textual units labeled by a majority of judges with 2 as a reliable reference set of importance units for the five texts For example, units 4 and 12 from text (1) belong to this reference set Agreement between analysts.</S>
    <S sid="9" ssid="5">Once we determined the set of textual units that the judges believed to be important, we needed to determine the agreement between the analysts who built the discourse trees for the five texts Because we did not know the distribution of the importance scores derived from the discourse trees, we computed the correlation between the analysts by applying Spearman's correlation coefficient on the scores associated to each textual unit We interpreted these scores as ranks on a scale that measures the importance of the units in a text The Spearman rank correlation coefficient is an alternative to the usual correlation coefficient It is based on the ranks of the data, and not on the data itself, so is resistant to outliers The null hypothesis tested by the Spearman coefficient is that two variables are independent of each other, against the alternative hypothesis that the rank of a variable is correlated with the rank of another variable The. value of the statistics ranges from &#8212;1, indicating that high ranks of one variable occur with low ranks of the other variable, through 0, indicating no correlation between the vanables, to +1, indicating that high ranks of one variable occur with high ranks of the other callable The Spearman correlation coefficient between the ranks assigned for each textual unit on the bases of the RS-trees built by the two analysts was very high 0793, at the p &lt; 0 0001 level of significance The differences between the two analysts caine mainly from their interpretations of two of the texts the RS-trees of one analyst mirrored the paragraph structure of the texts, while the RS-trees of the other mirrored a logical organization of the text, which that analyst believed to be important Agreement between the analysts and the judges with respect to the most important textual units.</S>
    <S sid="10" ssid="6">In order to determine whether there exists any correspondence between what readers believe to be important and the nuclei of the RS-trees, we selected, from each of the five texts, the set of textual units that were labeled as &amp;quot;very important&amp;quot; by a majority of the judges For example, for text (1), we selected units 4 and 12, ic, 11% of the units Overall, the judges selected 36 units as being very important, which is approximately 22% of the units in a text The percentages of important units for the five texts were 11,36,35, 17, and 22 respectively We took the maximal scores computed for each textual unit from the RS-trees built by each analyst and selected a percentage of units that matched the percentage of important units selected by the judges In the cases in which there were ties, we selected a percentage of units that was closest to the one computed for the judges For example, we selected units 4 and 12, which represented the most important 11% of units as induced from the RS-tree btult by the first analyst However, we selected only unit 4, which represented 6% of the most important units as induced from the RS-tree built by the second analyst The reason for selecting only unit 4 for the second analyst was that units 1011, and 12 have the same score &#8212; 4 (see table 1) If we had selected units 10, 11 and 12 as well, we would have ended up selecting 22% of the units in text (1), which is farther from 11 than 6 Hence, we determined for each text the set of important units as labeled by judges and as derived from the RS-trees of those texts We calculated for each text the recall and precision of the important units derived from the RS-trees, with respect to the units labeled important by the judges The overall recall and precision was the same for both analysts 56% recall and 66% precision In contrast, the average recall and precision for the same percentages of units selected randomly 1000 times from the same five texts were both 25 7%, a = 0 059 In summarizing text, it is often useful to consider not only clauses, but full sentences To account for this, we considered to be important all the textual units that pertained to a sentence that was characterized by at least one important textual unit For example, we labeled as important textual units 1 to 4 in text (I), because they make up a full sentence and because unit 4 was labeled as Important For the adjusted data, we determined again the percentages of important units for the five texts and we re-calculated the mall and precision for both analysts the recall was 69% and 66% and the precision 82% and 75% respectively In contrast, the average recall and precision for the same percentages of units selected randomly 1000 times from the same five texts were 38 4%, a = 0 048 These results confirm that there exists a strong correlation between the nuclei of the RS-trees that pertain to a text and what readers perceive as being important in that text Given the values of recall and precision that we obtained, it Is plausible that an adequate computational treatment of discourse theories would provide most of what is needed for selecting accurately the important units in a text However, the results also suggest that RST by itself is not enough if one wants to strive for perfection &#8226; The above results not only provide strong evidence that discourse theories can be used effectively for text summarization, but also enable one to derive strategies that an automatic summarizer aught follow For example, the Spearman correlation coefficient between the judges and the first analyst, the one who did not follow the paragraph structure, was lower than the one between the judges and the second analyst It follows that most human judges are inclined to use the paragraph breaks as valuable sources of information when they interpret discourse If the atm of a summarization program is to MIMIC human behavior, it seems adequate for the program to take advantage of the paragraph structure of the texts that it analyzes Currently, the rank assignment for each textual unit in an RS-tree is done entirely on the basis of the maximal depth in the tree where that unit is salient (Marcu, 1996) Our data seem to support the fact that there exists a correlation also between the types of relations that are used to connect various textual units and the importance of those units in a text We plan to design other experiments that can provide clearcut evidence on the nature of this correlation</S>
  </SECTION>
  <SECTION title="3 An RST-based summarization program" number="4">
    <S sid="11" ssid="1">Our summarization program relies on a rhetorical parser that builds RS-trees for unrestricted texts The mathematical foundations of the rhetorical parsing algonthm rely on a firse-Order formalization of valid text structures (Marcu, 1997b) The assumptions of the formalization are the following 1 The elementary units of complex text structures are non-overlapping spans of text 2 Rhetorical, coherence, and cohesive relations hold between textual units of various sizes 3 Relations can be partitioned into two classes paratactic and hypotactic Paratactic relations are those that hold between spans of equal importance Hypotactic relations are those that hold between a span that is essential for the writer's purpose, i e, a nucleus, and a span that increases the understanding of the nucleus but is not essential for the writer's purpose, i a, a satellite 4 The abstract structure of most texts is a binary, tree-like structure 5 If a relation holds between two textual spans of the tree structure of a text, that relation also holds between the most important units of the constituent subspans The most important units of a textual span are determined recursively they correspond to the most important units of the unmediate subspans when the relation that holds between these subspans is paratactic, and to the most important units of the nucleus subspan when the relation that holds between the immediate subspans is hypotacuc The rhetoncal parsing algorithm, which is outlined. in figure 1, is based on a comprehensive corpus analysis of more than 450 discourse markers and 7900 text fragments (see (Marcu, 1997b) for details) When given a text, the rhetorical parser determines first the discourse markers and the elementary units that make up that text The parser uses then the information derived from the corpus analysis in order to hypothesize rhetoncal relations among the elementary units In the end, the parser applies a constraint-satisfaction procedure to detemune the text structures that are valid If more than one valid structure is found, the parser chooses one that is the &amp;quot;best&amp;quot; according to a given metric The details of the algorithms that INPUT a text T I Determine the set D of all discourse markers in T and the set UT of elementary textual units in T 2 Hypothesize a set of relations R between the elements of UT 3 Determine the set ValTrees of all valid RS-trees of T that can be built using relations from R 4 Determine the &amp;quot;best&amp;quot; RS-tree in ValTrees on the basis of a metric that assigns higher weights to the trees that are more skewed to the right are used by the rethoncal parser are discussed at length in (Marcu, 1997a, Marcu, 1997b) When the rhetorical parser takes text (I) as input, it produces the RS-tree in figure 2 The convention that we use is that nuclei are surrounded by solid boxes and satellites by dotted boxes, the links between a node and a subordinate nucleus or nuclei are represented by solid arrows, and the links between a node and a subordinate satellite by dotted Imes The nodes with only one satellite denote occurrences of parenthetical information for example, textual unit 2 is labeled as parenthetical to the textual unit that results from juxtaposing 1 and 3 The numbers associated with each leaf correspond to the numencal labels in text (1) The numbers associated with each internal node correspond to the salient units of that node and are explicitly represented in the RS-tree By inspecting the RS-tree in figure 2, one can nonce that the trees that are built by the program do not have the same granularity as the trees constructed by the analysts For example, the program treats units 13,14, and 15 as one elementary unit However, as we argue in (Marcie, 1997b), the corpus analysis on which our parser is built supports the observation that, in most cases, the global structure of the RS-tree is not affected by the inability of the rhetorical parser to uncover all clauses in a text &#8212; most of the clauses that are not uncovered are nuclei of Mir relations The summarization program takes the RS-tree produced by the rhetorical parser and selects the textual units that are most salient in that text If the aim of the program is to produce just a very short summary, only the salient units associated with the internal nodes found closer to the root are selected The longer the summary one wants to generate, the farther the selected salient units will be from the root In fact, one can see that the RS-trees built by the rhetoncal parser induce a partial order on the Importance of the textual units For text (1), the most important unit is 4 The textual units that are salient in the nodes found one level below represent the next level of importance (in this case, unit 12&#8212; unit 4 was already accounted for) The next level contains units 5,6, 16, and 18, and so on To evaluate our program, we associated with each textual unit in the RS-trees built by the rhetorical parser a score in the same way we did for the RS-trees built by the analysts For example, the RS-tree in figure 2 has a depth of 6 Because unit 4 is salient for the root, it gets a score of 6 Units 5,6 are salient for an internal node found two levels below the root therefore, their score is 4 Unit 9 is salient for a leaf found five levels below the root therefore, its score is 1 Table 1 presents the scores associated by our summarization program to each unit in text (1) We used the importance scores assigned by our program to compute statistics similar to those discussed in the previous section When the program selected only the textual units with the highest scores, in percentages that were equal to those of the judges, the recall was 53% and the precision was 50% When the program selected the full sentences that were associated with the most unportant units, in percentages that were equal to those of the judges, the recall was 66% and the precision 68% The lower recall and precision scores associated with clauses seem to be caused primarily by the difference in granularity with respect to the way the texts were broken into subunits the program does not recover all minimal textual units, and as a consequence, its assignment of Importance scores is coarser When full sentences are considered, the judges and the program work at the same level of granularity, and as a consequence, the summarization results improve significantly</S>
  </SECTION>
  <SECTION title="4 Comparison with other work" number="5">
    <S sid="12" ssid="1">We are not aware of any RST-based summarization program for English However, Ono et al (1994) discuss a summarization program for Japanese whose minimal textual units are sentences Due to the differences between English and Japanese, it was impossible for us to compare Ono's summarizer with ours Fundamental differences concerning the assumptions that underlie One's work and ours are discussed at length in (Marcu, 1997b) We were able to obtain only one other program that summarizes English text &#8212; the one included in the Microsoft Office97 package We run the Microsoft summanzation program on the five texts from Scientific American and selected the same percentages of textual units as those considered important by the judges When we selected percentages of text that corresponded only to the clauses considered important by the judges, the Microsoft program recalled 28% of the units, with a precision of 26% When we selected percentages of text that corresponded to sentences considered important by the judges, the Microsoft program recalled 41% of the units, with a precision of 39% Al] Microsoft figures are only slightly above those that correspond to the baseline algontlims that select important units randomly It follows that our program outperforms significantly the one found in the Office97 package We are not aware of any other summarization program that can build summaries with granularity as fine as a clause (as our program can)</S>
  </SECTION>
  <SECTION title="5 Conclusions" number="6">
    <S sid="13" ssid="1">We described the first experiment that shows that the concepts of rhetoncal analysis and nucleanty can be used effectively for summarizing text The experiment suggests that discourse-based methods can account for detemmimg the most important units in a text with a recall and precision as high as 70% We showed how the concepts of rhetorical analysis and nucleanty can be treated algorithmically and we compared recall and precision figures of a summarization program that implements these concepts with recall and precision figures that pertain to a baseline algorithm and to a commercial system, the Microsoft Office97 summarizer The discourse-based summanzation program that we propose outperforms both the baseline and the commercial summarizer (see table 3) However, since its results do not match yet the recall and precision figures that pertain to the manual discourse analyses, it is likely that improvements of the rhetorical parser algorithm will result in better performance of subsequent implemetations</S>
  </SECTION>
  <SECTION title="Acknowledgements." number="7">
    <S sid="14" ssid="1">the invaluable help he gave me during every stage of this work and to Marilyn Mantel, David Mitchell, Kevin Schlueter, and Melanie Baliko for their advice on expenmental design and statistics I am also grateful to Marzena Makuta for her help with the RST analyses and to my colleagues and friends who volunteered to act as judges in the experiments described here This reasearch was supported by the Natural Sciences and Engineenng Research Council of Canada</S>
  </SECTION>
</PAPER>
