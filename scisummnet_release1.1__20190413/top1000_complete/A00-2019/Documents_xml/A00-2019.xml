<PAPER>
  <S sid="0">An Unsupervised Method For Detecting Grammatical Errors</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.</S>
    <S sid="2" ssid="2">The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL).</S>
    <S sid="3" ssid="3">The errorrecognition system, ALEK, performs with about 80% precision and 20% recall.</S>
  </ABSTRACT>
  <SECTION title="Introduction" number="1">
    <S sid="4" ssid="1">A good indicator of whether a person knows the meaning of a word is the ability to use it appropriately in a sentence (Miller and Gildea, 1987).</S>
    <S sid="5" ssid="2">Much information about usage can be obtained from quite a limited context: Choueka and Lusignan (1985) found that people can typically recognize the intended sense of a polysemous word by looking at a narrow window of one or two words around it.</S>
    <S sid="6" ssid="3">Statistically-based computer programs have been able to do the same with a high level of accuracy (Kilgarriff and Palmer, 2000).</S>
    <S sid="7" ssid="4">The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word.</S>
    <S sid="8" ssid="5">We have developed a statistical system, ALEK (Assessing Lexical Knowledge), that uses statistical analysis for this purpose.</S>
    <S sid="9" ssid="6">A major objective of this research is to avoid the laborious and costly process of collecting errors (or negative evidence) for each word that we wish to evaluate.</S>
    <S sid="10" ssid="7">Instead, we train ALEK on a general corpus of English and on edited text containing example uses of the target word.</S>
    <S sid="11" ssid="8">The system identifies inappropriate usage based on differences between the word's local context cues in an essay and the models of context it has derived from the corpora of well-formed sentences.</S>
    <S sid="12" ssid="9">A requirement for ALEK has been that all steps in the process be automated, beyond choosing the words to be tested and assessing the results.</S>
    <S sid="13" ssid="10">Once a target word is chosen, preprocessing, building a model of the word's appropriate usage, and identifying usage errors in essays is performed without manual intervention.</S>
    <S sid="14" ssid="11">ALEK has been developed using the Test of English as a Foreign Language (TOEFL) administered by the Educational Testing Service.</S>
    <S sid="15" ssid="12">TOEFL is taken by foreign students who are applying to US undergraduate and graduate-level programs.</S>
  </SECTION>
  <SECTION title="1 Background" number="2">
    <S sid="16" ssid="1">Approaches to detecting errors by non-native writers typically produce grammars that look for specific expected error types (Schneider and McCoy, 1998; Park, Palmer and Washburn, 1997).</S>
    <S sid="17" ssid="2">Under this approach, essays written by ESL students are collected and examined for errors.</S>
    <S sid="18" ssid="3">Parsers are then adapted to identify those error types that were found in the essay collection.</S>
    <S sid="19" ssid="4">We take a different approach, initially viewing error detection as an extension of the word sense disambiguation (WSD) problem.</S>
    <S sid="20" ssid="5">Corpus-based WSD systems identify the intended sense of a polysemous word by (1) collecting a set of example sentences for each of its various senses and (2) extracting salient contextual cues from these sets to (3) build a statistical model for each sense.</S>
    <S sid="21" ssid="6">They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)).</S>
    <S sid="22" ssid="7">Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.</S>
    <S sid="23" ssid="8">He extracted contexts from correct usage of each confusable word in a training corpus and then identified a new occurrence as an error when it matched the wrong context.</S>
    <S sid="24" ssid="9">However, most grammatical errors are not the result of simple word confusions.</S>
    <S sid="25" ssid="10">This complicates the task of building a model of incorrect usage.</S>
    <S sid="26" ssid="11">One approach we considered was to proceed without such a model: represent appropriate word usage (across senses) in a single model and compare a novel example to that model.</S>
    <S sid="27" ssid="12">The most appealing part of this formulation was that we could bypass the knowledge acquisition bottleneck.</S>
    <S sid="28" ssid="13">All occurrences of the word in a collection of edited text could be automatically assigned to a single training set representing appropriate usage.</S>
    <S sid="29" ssid="14">Inappropriate usage would be signaled by contextual cues that do not occur in training.</S>
    <S sid="30" ssid="15">Unfortunately, this approach was not effective for error detection.</S>
    <S sid="31" ssid="16">An example of a word usage error is often very similar to the model of appropriate usage.</S>
    <S sid="32" ssid="17">An incorrect usage can contain two or three salient contextual elements as well as a single anomalous element.</S>
    <S sid="33" ssid="18">The problem of error detection does not entail finding similarities to appropriate usage, rather it requires identifying one element among the contextual cues that simply does not fit.</S>
  </SECTION>
  <SECTION title="2 ALEK Architecture" number="3">
    <S sid="34" ssid="1">What kinds of anomalous elements does ALEK identify?</S>
    <S sid="35" ssid="2">Writers sometimes produce errors that violate basic principles of English syntax (e.g., a desks), while other mistakes show a lack of information about a specific vocabulary item (e.g., a knowledge).</S>
    <S sid="36" ssid="3">In order to detect these two types of problems, ALEK uses a 30-million word general corpus of English from the San Jose Mercury News (hereafter referred to as the general corpus) and, for each target word, a set of 10,000 example sentences from North American newspaper text' (hereafter referred to as the word-specific corpus).</S>
    <S sid="37" ssid="4">The corpora are extracted from the ACL-DCI corpora.</S>
    <S sid="38" ssid="5">In selecting the sentences for the word ALEK infers negative evidence from the contextual cues that do not co-occur with the target word &#8212; either in the word specific corpus or in the general English one.</S>
    <S sid="39" ssid="6">It uses two kinds of contextual cues in a &#177;2 word window around the target word: function words (closed-class items) and part-of-speech tags (Brill, 1994).</S>
    <S sid="40" ssid="7">The Brill tagger output is post-processed to &amp;quot;enrich&amp;quot; some closed class categories of its tag set, such as subject versus object pronoun and definite versus indefinite determiner.</S>
    <S sid="41" ssid="8">The enriched tags were adapted from Francis and Kaera (1982).</S>
    <S sid="42" ssid="9">After the sentences have been preprocessed, ALEK counts sequences of adjacent part-ofspeech tags and function words (such as determiners, prepositions, and conjunctions).</S>
    <S sid="43" ssid="10">For example, the sequence a/AT full-time/I.</S>
    <S sid="44" ssid="11">1 jobINN contributes one occurrence each to the bigrams AT+JJ, JJ+NN, a+JJ, and to the part-of-speech tag trigram AT+JJ+NN.</S>
    <S sid="45" ssid="12">Each individual tag and function word also contributes to its own unigram count.</S>
    <S sid="46" ssid="13">These frequencies form the basis for the error detection measures.</S>
    <S sid="47" ssid="14">From the general corpus, ALEK computes a mutual information measure to determine which sequences of part-of-speech tags and function words are unusually rare and are, therefore, likely to be ungrammatical in English (e.g., singular determiner preceding plural noun, as in *a desks).</S>
    <S sid="48" ssid="15">Mutual information has often been used to detect combinations of words that occur more frequently than we would expect based on the assumption that the words are independent.</S>
    <S sid="49" ssid="16">Here we use this measure for the opposite purpose &#8212; to find combinations that occur less often than expected.</S>
    <S sid="50" ssid="17">ALEK also looks for sequences that are common in general but unusual in the word specific corpus (e.g., the singular determiner a preceding a singular noun is common in English but rare when the noun is specific corpora, we tried to minimize the mismatch between the domains of newspapers and TOEFL essays.</S>
    <S sid="51" ssid="18">For example, in the newspaper domain, concentrate is usually used as a noun, as in orange juice concentrate but in TOEFL essays it is a verb 91% of the time.</S>
    <S sid="52" ssid="19">Sentence selection for the word specific corpora was constrained to reflect the distribution of part-of-speech tags for the target word in a random sample of TOEFL essays. knowledge).</S>
    <S sid="53" ssid="20">These divergences between the two corpora reflect syntactic properties that are peculiar to the target word.</S>
    <S sid="54" ssid="21">The system computes mutual information comparing the proportion of observed occurrences of bigrams in the general corpus to the proportion expected based on the assumption of independence, as shown below: Here, P(AB) is the probability of the occurrence of the AB bigram, estimated from its frequency in the general corpus, and P(A) and P(B) are the probabilities of the first and second elements of the bigram, also estimated from the general corpus.</S>
    <S sid="55" ssid="22">Ungrammatical sequences should produce bigram probabilities that are much smaller than the product of the unigram probabilities (the value of MI will be negative).</S>
    <S sid="56" ssid="23">Trigram sequences are also used, but in this case the mutual information computation compares the co-occurrence of ABC to a model in which A and C are assumed to be conditionally independent given B (see Lin, 1998).</S>
    <S sid="57" ssid="24">Once again, a negative value is often indicative of a sequence that violates a rule of English.</S>
    <S sid="58" ssid="25">ALEK also uses mutual information to compare the distributions of tags and function words in the word-specific corpus to the distributions that are expected based on the general corpus.</S>
    <S sid="59" ssid="26">The measures for bigrams and trigrams are similar to those given above except that the probability in the numerator is estimated from the wordspecific corpus and the probabilities in the denominator come from the general corpus.</S>
    <S sid="60" ssid="27">To return to a previous example, the phrase a knowledge contains the tag bigram for singular determiner followed by singular noun (AT NN).</S>
    <S sid="61" ssid="28">This sequence is much less common in the word-specific corpus for knowledge than would be expected from the general corpus unigram probabilities of AT and NN.</S>
    <S sid="62" ssid="29">In addition to bigram and trigram measures, ALEK compares the target word's part-ofspeech tag in the word-specific corpus and in the general corpus.</S>
    <S sid="63" ssid="30">Specifically, it looks at the conditional probability of the part-of-speech tag given the major syntactic category (e.g., plural noun given noun) in both distributions, by computing the following value.</S>
    <S sid="64" ssid="31">For example, in the general corpus, about half of all noun tokens are plural, but in the training set for the noun knowledge, the plural knowledges occurs rarely, if at all.</S>
    <S sid="65" ssid="32">The mutual information measures provide candidate errors, but this approach overgenerates &#8212; it finds rare, but still quite grammatical, sequences.</S>
    <S sid="66" ssid="33">To reduce the number of false positives, no candidate found by the MI measures is considered an error if it appears in the word-specific corpus at least two times.</S>
    <S sid="67" ssid="34">This increases ALEK's precision at the price of reduced recall.</S>
    <S sid="68" ssid="35">For example, a knowledge will not be treated as an error because it appears in the training corpus as part of the longer a knowledge of sequence (as in a knowledge of mathematics).</S>
    <S sid="69" ssid="36">ALEK also uses another statistical technique for finding rare and possibly ungrammatical tag and function word bigrams by computing the x2 (chi square) statistic for the difference between the bigram proportions found in the word-specific and in the general corpus: = The x2 measure faces the same problem of overgenerating errors.</S>
    <S sid="70" ssid="37">Due to the large sample sizes, extreme values can be obtained even though effect size may be minuscule.</S>
    <S sid="71" ssid="38">To reduce false positives, ALEK requires that effect sizes be at least in the moderate-to-small range (Cohen and Cohen, 1983).</S>
    <S sid="72" ssid="39">Direct evidence from the word specific corpus can also be used to control the overgeneration of errors.</S>
    <S sid="73" ssid="40">For each candidate error, ALEK compares the larger context in which the bigram appears to the contexts that have been analyzed in the word-specific corpus.</S>
    <S sid="74" ssid="41">From the wordspecific corpus, ALEK forms templates, sequences of words and tags that represent the local context of the target.</S>
    <S sid="75" ssid="42">If a test sentence contains a low probability bigram (as measured by the x2 test), the local context of the target is compared to all the templates of which it is a part.</S>
    <S sid="76" ssid="43">Exceptions to the error, that is longer grammatical sequences that contain rare subsequences, are found by examining conditional probabilities.</S>
    <S sid="77" ssid="44">To illustrate this, consider the example of a knowledge and a knowledge of The conditional probability of of given a knowledge is high, as it accounts for almost all of the occurrences of a knowledge in the wordspecific corpus.</S>
    <S sid="78" ssid="45">Based on this high conditional probability, the system will use the template for a knowledge of to keep it from being marked as an error.</S>
    <S sid="79" ssid="46">Other function words and tags in the +1 position have much lower conditional probability, so for example, a knowledge is will not be treated as an exception to the error.</S>
    <S sid="80" ssid="47">TOEFL essays are graded on a 6 point scale, where 6 demonstrates &amp;quot;clear competence&amp;quot; in writing on rhetorical and syntactic levels and 1 demonstrates &amp;quot;incompetence in writing&amp;quot;.</S>
    <S sid="81" ssid="48">If low probability n-grams signal grammatical errors, then we would expect TOEFL essays that received lower scores to have more of these ngrams.</S>
    <S sid="82" ssid="49">To test this prediction, we randomly selected from the TOEFL pool 50 essays for each of the 6 score values from 1.0 to 6.0.</S>
    <S sid="83" ssid="50">For each score value, all 50 essays were concatenated to form a super-essay.</S>
    <S sid="84" ssid="51">In every super-essay, for each adjacent pair and triple of tags containing a noun, verb, or adjective, the bigram and trigram mutual information values were computed based on the general corpus.</S>
    <S sid="85" ssid="52">Table 1 shows the proportions of bigrams and trigrams with mutual information less than &#8212;3.60.</S>
    <S sid="86" ssid="53">As predicted, there is a significant negative correlation between the score and the proportion of low probability bigrams (r,= -.94, n=6, p&lt;.01, two-tailed) and trigrams (r,= -.84, n=6, p&lt;.05, two-tailed).</S>
    <S sid="87" ssid="54">ALEK was developed using three target words that were extracted from TOEFL essays: concentrate, interest, and knowledge.</S>
    <S sid="88" ssid="55">These words were chosen because they represent different parts of speech and varying degrees of polysemy.</S>
    <S sid="89" ssid="56">Each also occurred in at least 150 sentences in what was then a small pool of TOEFL essays.</S>
    <S sid="90" ssid="57">Before development began, each occurrence of these words was manually labeled as an appropriate or inappropriate usage &#8212; without taking into account grammatical errors that might have been present elsewhere in the sentence but which were not within the target word's scope.</S>
    <S sid="91" ssid="58">Critical values for the statistical measures were set during this development phase.</S>
    <S sid="92" ssid="59">The settings were based empirically on ALEK's performance so as to optimize precision and recall on the three development words.</S>
    <S sid="93" ssid="60">Candidate errors were those local context sequences that produced a mutual information value of less than &#8212;3.60 based on the general corpus; mutual information of less than &#8212;5.00 for the specific/general comparisons; or a x2 value greater than 12.82 with an effect size greater than 0.30.</S>
    <S sid="94" ssid="61">Precision and recall for the three words are shown below.</S>
  </SECTION>
  <SECTION title="3 Experimental Design and Results" number="4">
    <S sid="95" ssid="1">ALEK was tested on 20 words.</S>
    <S sid="96" ssid="2">These words were randomly selected from those which met two criteria: (1) They appear in a university word list (Nation, 1990) as words that a student in a US university will be expected to encounter and (2) there were at least 1,000 sentences containing the word in the TOEFL essay pool.</S>
    <S sid="97" ssid="3">To build the usage model for each target word, 10,000 sentences containing it were extracted from the North American News Corpus.</S>
    <S sid="98" ssid="4">Preprocessing included detecting sentence boundaries and part-of-speech tagging.</S>
    <S sid="99" ssid="5">As in the development system, the model of general English was based on bigram and trigram frequencies of function words and part-ofspeech tags from 30-million words of the San Jose Mercury News.</S>
    <S sid="100" ssid="6">For each test word, all of the test sentences were marked by ALEK as either containing an error or not containing an error.</S>
    <S sid="101" ssid="7">The size of the test set for each word ranged from 1,400 to 20,000 with a mean of 8,000 sentences.</S>
    <S sid="102" ssid="8">To evaluate the system, for each test word we randomly extracted 125 sentences that ALEK classified as containing no error (C-set) and 125 sentences which it labeled as containing an error (E-set).</S>
    <S sid="103" ssid="9">These 250 sentences were presented to a linguist in a random order for blind evaluation.</S>
    <S sid="104" ssid="10">The linguist, who had no part in ALEK's development, marked each usage of the target word as incorrect or correct and in the case of incorrect usage indicated how far from the target one would have to look in order to recognise that there was an error.</S>
    <S sid="105" ssid="11">For example, in the case of &amp;quot;an period&amp;quot; the error occurs at a distance of one word from period.</S>
    <S sid="106" ssid="12">When the error is an omission, as in &amp;quot;lived in Victorian period&amp;quot;, the distance is where the missing word should have appeared.</S>
    <S sid="107" ssid="13">In this case, the missing determiner is 2 positions away from the target.</S>
    <S sid="108" ssid="14">When more than one error occurred, the distance of the one closest to the target was marked.</S>
    <S sid="109" ssid="15">Table 3 lists the precision and recall for the 20 test words.</S>
    <S sid="110" ssid="16">The column labelled &amp;quot;Recall&amp;quot; is the proportion of human-judged errors in the 250sentence sample that were detected by ALEK.</S>
    <S sid="111" ssid="17">&amp;quot;Total Recall&amp;quot; is an estimate that extrapolates from the human judgements of the sample to the entire test set.</S>
    <S sid="112" ssid="18">We illustrate this with the results for pollution.</S>
    <S sid="113" ssid="19">The human judge marked as incorrect usage 91.2% of the sample from ALEK's E-set and 18.4% of the sample from its C-set.</S>
    <S sid="114" ssid="20">To estimate overall incorrect usage, we computed a weighted mean of these two rates, where the weights reflected the proportion of sentences that were in the E-set and C-set.</S>
    <S sid="115" ssid="21">The E-set contained 8.3% of the pollution sentences and the C-set had the remaining 91.7%.</S>
    <S sid="116" ssid="22">With the human judgements as the gold standard, the estimated overall rate of incorrect usage is (.083 x .912 + .917 x .184) = .245.</S>
    <S sid="117" ssid="23">ALEK's estimated recall is the proportion of sentences in the E-set times its precision, divided by the overall estimated error rate (.083 x .912) / .245 = .310.</S>
    <S sid="118" ssid="24">The precision results vary from word to word.</S>
    <S sid="119" ssid="25">Conclusion and pollution have precision in the low to middle 90's while individual's precision is 57%.</S>
    <S sid="120" ssid="26">Overall, ALEK's predictions are about 78% accurate.</S>
    <S sid="121" ssid="27">The recall is limited in part by the fact that the system only looks at syntactic information, while many of the errors are semantic.</S>
    <S sid="122" ssid="28">Nicholls (1999) identifies four error types: an unnecessary word (*affect to their emotions), a missing word (*opportunity of job.</S>
    <S sid="123" ssid="29">), a word or phrase that needs replacing (*every jobs), a word used in the wrong form (*pollutions).</S>
    <S sid="124" ssid="30">ALEK recognizes all of these types of errors.</S>
    <S sid="125" ssid="31">For closed class words, ALEK identified whether a word was missing, the wrong word was used (choice), and when an extra word was used.</S>
    <S sid="126" ssid="32">Open class words have a fourth error category, form, including inappropriate compounding and verb agreement.</S>
    <S sid="127" ssid="33">During the development stage, we found it useful to add additional error categories.</S>
    <S sid="128" ssid="34">Since TEOFL graders are not supposed to take punctuation into account, punctuation errors were only marked when they caused the judge to &amp;quot;garden path&amp;quot; or initially misinterpret the sentence.</S>
    <S sid="129" ssid="35">Spelling was marked either when a function word was misspelled, causing part-ofspeech tagging errors, or when the writer's intent was unclear.</S>
    <S sid="130" ssid="36">The distributions of categories for hits and misses, shown in Table 4, are not strikingly different.</S>
    <S sid="131" ssid="37">However, the hits are primarily syntactic in nature while the misses are both semantic (as in open-class:choice) and syntactic (as in closed-class:missing).</S>
    <S sid="132" ssid="38">ALEK is sensitive to open-class word confusions (affect vs effect) where the part of speech differs or where the target word is confused with another word (*In this aspect, ... instead of In this respect,...).</S>
    <S sid="133" ssid="39">In both cases, the system recognizes that the target is in the wrong syntactic environment.</S>
    <S sid="134" ssid="40">Misses can also be syntactic &#8212; when the target word is confused with another word but the syntactic environment fails to trigger an error.</S>
    <S sid="135" ssid="41">In addition, ALEK does not recognize semantic errors when the error involves the misuse of an open-class word in combination with the target (for example, make in &amp;quot;they make benefits&amp;quot;).</S>
    <S sid="136" ssid="42">Closed class words typically are either selected by or agree with a head word.</S>
    <S sid="137" ssid="43">So why are there so many misses, especially with prepositions?</S>
    <S sid="138" ssid="44">The problem is caused in part by polysemy &#8212; when one sense of the word selects a preposition that another sense does not.</S>
    <S sid="139" ssid="45">When concentrate is used spatially, it selects the preposition in, as &amp;quot;the stores were concentrated in the downtown area&amp;quot;.</S>
    <S sid="140" ssid="46">When it denotes mental activity, it selects the preposition on, as in &amp;quot;Susan concentrated on her studies&amp;quot;.</S>
    <S sid="141" ssid="47">Since ALEK trains on all senses of concentrate, it does not detect the error in &amp;quot;Susan concentrated in her studies&amp;quot;.</S>
    <S sid="142" ssid="48">Another cause is that adjuncts, especially temporal and locative adverbials, distribute freely in the wordspecific corpora, as in &amp;quot;Susan concentrated in her room.&amp;quot; This second problem is more tractable than the polysemy problem &#8212; and would involve training the system to recognize certain types of adjuncts.</S>
    <S sid="143" ssid="49">False positives, when ALEK &amp;quot;identifies&amp;quot; an error where none exists, fall into six major categories.</S>
    <S sid="144" ssid="50">The percentage of each false positive type in a random sample of 200 false positives is shown in Table 5.</S>
    <S sid="145" ssid="51">Domain mismatch: Mismatch of the newspaper-domain word-specific corpora and essay-domain test corpus.</S>
    <S sid="146" ssid="52">One notable difference is that some TOEFL essay prompts call for the writer's opinion.</S>
    <S sid="147" ssid="53">Consequently, TOEFL essays often contain first person references, whereas newspaper articles are written in the third person.</S>
    <S sid="148" ssid="54">We need to supplement the word-specific corpora with material that more closely resembles the test corpus.</S>
    <S sid="149" ssid="55">Tagger: Incorrect analysis by the part-of-speech tagger.</S>
    <S sid="150" ssid="56">When the part-of-speech tag is wrong, ALEK often recognizes the resulting n-gram as anomalous.</S>
    <S sid="151" ssid="57">Many of these errors are caused by training on the Brown corpus instead of a corpus of essays.</S>
    <S sid="152" ssid="58">Syntactic analysis: Errors resulting from using part-of-speech tags instead of supertags or a full parse, which would give syntactic relations between constituents.</S>
    <S sid="153" ssid="59">For example, ALEK false alarms on arguments of ditransitive verbs such as offer and flags as an error &amp;quot;you benefits&amp;quot; in &amp;quot;offers you benefits&amp;quot;.</S>
    <S sid="154" ssid="60">Free distribution: Elements that distribute freely, such as adverbs and conjunctions, as well as temporal and locative adverbial phrases, tend to be identified as errors when they occur in some positions.</S>
    <S sid="155" ssid="61">Punctuation: Most notably omission of periods and commas.</S>
    <S sid="156" ssid="62">Since these errors are not indicative of one's ability to use the target word, they were not considered as errors unless they caused the judge to misanalyze the sentence.</S>
    <S sid="157" ssid="63">Infrequent tags.</S>
    <S sid="158" ssid="64">An undesirable result of our &amp;quot;enriched&amp;quot; tag set is that some tags, e.g., the post-determiner last, occur too infrequently in the corpora to provide reliable statistics.</S>
    <S sid="159" ssid="65">Solutions to some of these problems will clearly be more tractable than to others.</S>
  </SECTION>
  <SECTION title="4 Comparison of Results" number="5">
    <S sid="160" ssid="1">Comparison of these results to those of other systems is difficult because there is no generally accepted test set or performance baseline.</S>
    <S sid="161" ssid="2">Given this limitation, we compared ALEK's performance to a widely used grammar checker, the one incorporated in Microsoft's Word97.</S>
    <S sid="162" ssid="3">We created files of sentences used for the three development words concentrate, interest, and knowledge, and manually corrected any errors outside the local context around the target before checking them with Word97.</S>
    <S sid="163" ssid="4">The performance for concentrate showed overall precision of 0.89 and recall of 0.07.</S>
    <S sid="164" ssid="5">For interest, precision was 0.85 with recall of 0.11.</S>
    <S sid="165" ssid="6">In sentences containing knowledge, precision was 0.99 and recall was 0.30.</S>
    <S sid="166" ssid="7">Word97 correctly detected the ungrammaticality of knowledges as well as a knowledge, while it avoided flagging a knowledge of.</S>
    <S sid="167" ssid="8">In summary, Word97's precision in error detection is impressive, but the lower recall values indicate that it is responding to fewer error types than does ALEK.</S>
    <S sid="168" ssid="9">In particular, Word97 is not sensitive to inappropriate selection of prepositions for these three words (e.g., *have knowledge on history, *to concentrate at science).</S>
    <S sid="169" ssid="10">Of course, Word97 detects many kinds of errors that ALEK does not.</S>
    <S sid="170" ssid="11">Research has been reported on grammar checkers specifically designed for an ESL population.</S>
    <S sid="171" ssid="12">These have been developed by hand, based on small training and test sets.</S>
    <S sid="172" ssid="13">Schneider and McCoy (1998) developed a system tailored to the error productions of American Sign Language signers.</S>
    <S sid="173" ssid="14">This system was tested on 79 sentences containing determiner and agreement errors, and 101 grammatical sentences.</S>
    <S sid="174" ssid="15">We calculate that their precision was 78% with 54% recall.</S>
    <S sid="175" ssid="16">Park, Palmer and Washburn (1997) adapted a categorial grammar to recognize &amp;quot;classes of errors [that] dominate&amp;quot; in the nine essays they inspected.</S>
    <S sid="176" ssid="17">This system was tested on eight essays, but precision and recall figures are not reported.</S>
  </SECTION>
  <SECTION title="5 Conclusion" number="6">
    <S sid="177" ssid="1">The unsupervised techniques that we have presented for inferring negative evidence are effective in recognizing grammatical errors in written text.</S>
    <S sid="178" ssid="2">Preliminary results indicate that ALEK's error detection is predictive of TOEFL scores.</S>
    <S sid="179" ssid="3">If ALEK accurately detects usage errors, then it should report more errors in essays with lower scores than in those with higher scores.</S>
    <S sid="180" ssid="4">We have already seen in Table 1 that there is a negative correlation between essay score and two of ALEK's component measures, the general corpus n-grams.</S>
    <S sid="181" ssid="5">However, the data in Table 1 were not based on specific vocabulary items and do not reflect overall system performance, which includes the other measures as well.</S>
    <S sid="182" ssid="6">Table 6 shows the proportion of test word occurrences that were classified by ALEK as containing errors within two positions of the target at each of 6 TOEFL score points.</S>
    <S sid="183" ssid="7">As predicted, the correlation is negative (F., = -1.00, n = 6,p &lt;.001, two-tailed).</S>
    <S sid="184" ssid="8">These data support the validity of the system as a detector of inappropriate usage, even when only a limited number of words are targeted and only the immediate context of each target is examined.</S>
    <S sid="185" ssid="9">ALEK and by a human judge For comparison, Table 6 also gives the estimated proportions of inappropriate usage by score point based on the human judge's classification.</S>
    <S sid="186" ssid="10">Here, too, there is a negative correlation: rs = &#8211;.90, n = 5, p &lt; .05, two-tailed.</S>
    <S sid="187" ssid="11">Although the system recognizes a wide range of error types, as Table 6 shows, it detects only about one-fifth as many errors as a human judge does.</S>
    <S sid="188" ssid="12">To improve recall, research needs to focus on the areas identified in section 3.2 and, to improve precision, efforts should be directed at reducing the false positives described in 3.3.</S>
    <S sid="189" ssid="13">ALEK is being developed as a diagnostic tool for students who are learning English as a foreign language.</S>
    <S sid="190" ssid="14">However, its techniques could be incorporated into a grammar checker for native speakers.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="191" ssid="1">We thank Susanne Wolff for evaluating the test sentences, and Robert Kantor, Ken Sheppard and 3 anonymous reviewers for their helpful suggestions.</S>
  </SECTION>
</PAPER>
