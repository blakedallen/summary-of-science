<PAPER>
  <S sid="0">Investigating GIS And Smoothing For Maximum Entropy Taggers</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing.</S>
    <S sid="2" ssid="2">We show analytically and empirically that the correction feature, assumed to be required for the correctof unnecessary.</S>
    <S sid="3" ssid="3">We also explore the use of a Gaussian prior and a simple cutoff for smoothing.</S>
    <S sid="4" ssid="4">The experiments are performed with two tagsets: standard Penn Treebank and the larger set of lexical types from</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">The use of maximum entropy (ME) models has become popular in Statistical NLP; some example applications include part-of-speech (Pos) tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996).</S>
    <S sid="6" ssid="2">Many tagging problems have been successfully modelled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), &amp;quot;supertagging&amp;quot; (Clark, 2002) and chunking (Koeling, 2000).</S>
    <S sid="7" ssid="3">Generalised Iterative Scaling (GIS) is a very simple algorithm for estimating the parameters of a ME model.</S>
    <S sid="8" ssid="4">The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant.</S>
    <S sid="9" ssid="5">Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm.</S>
    <S sid="10" ssid="6">However, the extra book keeping required for us means that GIS is often faster in practice (Malouf, 2002).</S>
    <S sid="11" ssid="7">This paper shows, by a simple adaptation of Berger's proof for the convergence of HS (Berger, 1997), that GIS does not require a correction feature.</S>
    <S sid="12" ssid="8">We also investigate how the use of a correction feature affects the performance of ME taggers.</S>
    <S sid="13" ssid="9">GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting.</S>
    <S sid="14" ssid="10">A simple technique used to avoid overfitting is a frequency cutoff, in which only frequently occurring features are included in the model (Ratnaparkhi, 1998).</S>
    <S sid="15" ssid="11">However, more sophisticated smoothing techniques exist, such as the use of a Gaussian prior on the parameters of the model (Chen and Rosenfeld, 1999).</S>
    <S sid="16" ssid="12">This technique has been applied to language modelling (Chen and Rosenfeld, 1999), text classification (Nigam et al., 1999) and parsing (Johnson et al., 1999), but to our knowledge it has not been compared with the use of a feature cutoff.</S>
    <S sid="17" ssid="13">We explore the combination of Gaussian smoothing and a simple cutoff for two tagging tasks.</S>
    <S sid="18" ssid="14">The two taggers used for the experiments are a POS tagger, trained on the WSJ Penn Treebank, and a &amp;quot;supertagger&amp;quot;, which assigns tags from the much larger set of lexical types from Combinatory Categorial Grammar (ccG) (Clark, 2002).</S>
    <S sid="19" ssid="15">Elimination of the correction feature and use of appropriate smoothing methods result in state of the art performance for both tagging tasks.</S>
  </SECTION>
  <SECTION title="2 Maximum Entropy Models" number="2">
    <S sid="20" ssid="1">A conditional ME model, also known as a loglinear model, has the following form: where the functions fi are the features of the model, the A, are the parameters, or weights, and Z(x) is a normalisation constant.</S>
    <S sid="21" ssid="2">This form can be derived by choosing the model with maximum entropy (i.e. the most uniform model) from a set of models that satisfy a certain set of constraints.</S>
    <S sid="22" ssid="3">The constraints are that the expected value of each feature fi according to the model p is equal to some value Ki (Rosenfeld, 1996): Calculating the expected value according to p requires summing over all contexts x, which is not possible in practice.</S>
    <S sid="23" ssid="4">Therefore we use the now standard approximation (Rosenfeld, 1996): where p(x) is the relative frequency of context x in the data.</S>
    <S sid="24" ssid="5">This is convenient because p(x) is zero for all those events not seen in the training data.</S>
    <S sid="25" ssid="6">Finding the maximum entropy model that satisfies these constraints is a constrained optimisation problem, which can be solved using the method of Lagrange multipliers, and leads to the form in (1) where the Ai are the Lagrange multipliers.</S>
    <S sid="26" ssid="7">A natural choice for Ki is the empirical expected value of the feature fi: xo, An alternative motivation for this model is that, starting with the log-linear form in (1) and deriving (conditional) MLES, we arrive at the same solution as the ME model which satisfies the constraints in (5).</S>
  </SECTION>
  <SECTION title="3 Generalised Iterative Scaling" number="3">
    <S sid="27" ssid="1">GIS is a very simple algorithm for estimating the parameters of a ME model.</S>
    <S sid="28" ssid="2">The algorithm is as follows, where E p f, is the empirical expected value of J and E p fi is the expected value according to model p: In practice C is maximised over the (x, y) pairs in the training data, although in theory C can be any constant greater than or equal to the figure in (8).</S>
    <S sid="29" ssid="3">However, since determines the rate of convergence of the algorithm, it is preferable to keep C as small as possible.</S>
    <S sid="30" ssid="4">The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant.</S>
    <S sid="31" ssid="5">Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event, defined as follows: For our tagging experiments, the use of a correction feature did not significantly affect the results.</S>
    <S sid="32" ssid="6">Moreover, we show in the Appendix, by a simple adaptation of Berger's proof for the convergence of HS (Berger, 1997), that GIS converges to the maximum likelihood model without a correction feature.1 The proof works by introducing a correction feature with fixed weight of 0 into the iis convergence proof.</S>
    <S sid="33" ssid="7">This feature does not contribute to the model and can be ignored during weight update.</S>
    <S sid="34" ssid="8">Introducing this null feature still satisfies Jensen's inequality, which is used to provide a lower bound on the change in likelihood between iterations, and the existing GIS weight update (7) can still be derived analytically.</S>
    <S sid="35" ssid="9">An advantage of GIS is that it is a very simple algorithm, made even simpler by the removal of the correction feature.</S>
    <S sid="36" ssid="10">This simplicity means that, although GIS requires more iterations than 11s to reach convergence, in practice it is significantly faster (Malouf, 2002).</S>
  </SECTION>
  <SECTION title="4 Smoothing Maximum Entropy Models" number="4">
    <S sid="37" ssid="1">Several methods have been proposed for smoothing ME models (see Chen and Rosenfeld (1999)).</S>
    <S sid="38" ssid="2">For taggers, a standard technique is to eliminate low frequency features, based on the assumption that they are unreliable or uninformative (Ratnaparkhi, 1998).</S>
    <S sid="39" ssid="3">Studies of infrequent features in other domains suggest this assumption may be incorrect (Daelemans et al., 1999).</S>
    <S sid="40" ssid="4">We test this for ME taggers by replacing the cutoff with the use of a Gaussian prior, a technique which works well for language models (Chen and Rosenfeld, 1999).</S>
    <S sid="41" ssid="5">When using a Gaussian prior, the objective function is no longer the likelihood, L(A), but has the form: 2oMaximising this function is a form of maximum a posteriori estimation, rather than maximum likelihood estimation.</S>
    <S sid="42" ssid="6">The effect of the prior is to penalise models that have very large positive or negative weights.</S>
    <S sid="43" ssid="7">This can be thought of as relaxing the constraints in (5), so that the model fits the data less exactly.</S>
    <S sid="44" ssid="8">The parameters o-, are usually collapsed into one parameter which can be set using heldout data.</S>
    <S sid="45" ssid="9">The new update rule for GIS with a Gaussian prior is found by solving the following equation for the Ai update values (denoted by S), which can easily be derived from (10) by analogy with the proof in the Appendix: This equation does not have an analytic solution for Si and can be solved using a numerical solver such as Newton-Raphson.</S>
    <S sid="46" ssid="10">Note that this new update rule is still significantly simpler than that required for 11s.</S>
  </SECTION>
  <SECTION title="5 Maximum Entropy Taggers" number="5">
    <S sid="47" ssid="1">We reimplemented Ratnaparkhi's publicly available POS tagger MXPOST (Ratnaparkhi, 1996; Ratnaparkhi, 1998) and Clark's CCG supertagger (Clark, 2002) as a starting point for our experiments.</S>
    <S sid="48" ssid="2">CCG supertagging is more difficult than POS tagging because the set of &amp;quot;tags&amp;quot; assigned by the supertagger is much larger (398 in this implementation, compared with 45 POS tags).</S>
    <S sid="49" ssid="3">The supertagger assigns CCG lexical categories (Steedman, 2000) which encode subcategorisation information.</S>
    <S sid="50" ssid="4">Table 1 gives some examples.</S>
    <S sid="51" ssid="5">The features used by each tagger are binary valued, and pair a tag with various elements of the context; for example: fi(x ) = { 1 if word(x)= the &amp; y = DT ,y</S>
  </SECTION>
  <SECTION title="0 otherwise" number="6">
    <S sid="52" ssid="1">(12) word(x) = the is an example of what Ratnaparkhi calls a contextual predicate.</S>
    <S sid="53" ssid="2">The contextual predicates used by the two taggers are given in Table 2, where w, is the ith word and ti is the ith tag.</S>
    <S sid="54" ssid="3">We insert a special end of sentence symbol at sentence boundaries so that the features looking forwards and backwards are always defined.</S>
    <S sid="55" ssid="4">The supertagger uses POS tags as additional features, which Clark (2002) found improved performance significantly, and does not use the morphological features, since the POS tags provide equivalent information.</S>
    <S sid="56" ssid="5">For the supertagger, t, is the lexical category of the ith word.</S>
    <S sid="57" ssid="6">The conditional probability of a tag sequence y ...y, given a sentence w wn is approximated as follows: where x; is the context of the ith word.</S>
    <S sid="58" ssid="7">The tagger returns the most probable sequence for the sentence.</S>
    <S sid="59" ssid="8">Following Ratnaparkhi, beam search is used to retain only the 20 most probable sequences during the tagging process;2 we also use a &amp;quot;tag dictionary&amp;quot;, so that words appearing 5 or more times in the data can only be assigned those tags previously seen with the word.</S>
  </SECTION>
  <SECTION title="6 POS Tagging Experiments" number="7">
    <S sid="60" ssid="1">We develop and test our improved POS tagger (c &amp;c) using the standard parser development methodology on the Penn Treebank WSJ corpus.</S>
    <S sid="61" ssid="2">Table 3 shows the number of sentences and words in the training, development and test datasets.</S>
    <S sid="62" ssid="3">As well as evaluating the overall accuracy of the taggers (Acc), we also calculate the accuracy on previously unseen words (UwoRD), previously unseen word-tag pairs (UTAG) and ambiguous words (AMB), that is, those with more than one tag over the testing, training and development datasets.</S>
    <S sid="63" ssid="4">Note that the unseen word-tag pairs do not include the previously unseen words.</S>
    <S sid="64" ssid="5">We first replicated the results of the MXPOST tagger.</S>
    <S sid="65" ssid="6">In doing so, we discovered a number of minor variations from Ratnaparkhi (1998): MXPOST uses a cutoff of 1 for the current word feature and 5 for other features.</S>
    <S sid="66" ssid="7">However, the current word must have appeared at least 5 times with any tag for the current word feature to be included; otherwise the word is considered rare and morphological features are included instead.</S>
  </SECTION>
  <SECTION title="7 POS Tagging Results" number="8">
    <S sid="67" ssid="1">Table 4 shows the performance of MXPOST and our reimplementation.3 The third row shows a minor improvement in performance when the correction feature is removed.</S>
    <S sid="68" ssid="2">We also experimented with the default contextual predicate but found it had little impact on the performance.</S>
    <S sid="69" ssid="3">For the remainder of the experiments we use neither the correction nor the default features.</S>
    <S sid="70" ssid="4">The rest of this section considers various combinations of feature cutoffs and Gaussian smoothing.</S>
    <S sid="71" ssid="5">We report optimal results with respect to the smoothing parameter a, where a = No-2 and N is the number of training instances.</S>
    <S sid="72" ssid="6">We found that using a 2 gave the most benefit to our basic tagger, improving performance by about 0.15% on the development set.</S>
    <S sid="73" ssid="7">This result is shown in the first row of Table 5.</S>
    <S sid="74" ssid="8">The remainder of Table 5 shows a minimal change in performance when the current word (w) and previous word (pw) cutoffs are varied.</S>
    <S sid="75" ssid="9">This led us to reduce the cutoffs for all features simultaneously.</S>
    <S sid="76" ssid="10">Table 6 gives results for cutoff values between 1 and 4.</S>
    <S sid="77" ssid="11">The best performance (in row 1) is obtained when the cutoffs are eliminated entirely.</S>
    <S sid="78" ssid="12">Gaussian smoothing has allowed us to retain all of the features extracted from the corpus and reduce overfitting.</S>
    <S sid="79" ssid="13">To get more information into the model, more features must be extracted, and so we investigated the addition of the current word feature for all words, including the rare ones.</S>
    <S sid="80" ssid="14">This resulted in a minor improvement, and gave the best performance on the development data: 96.83%.</S>
    <S sid="81" ssid="15">Table 7 shows the final performance on the test set, using the best configuration on the development data (which we call c&amp;c), compared with MXPOST.</S>
    <S sid="82" ssid="16">The improvement is 0.22% overall (a reduction in error rate of 7.5%) and 1.58% for unknown words (a reduction in error rate of 9.7%).</S>
    <S sid="83" ssid="17">The obvious cost associated with retaining all the features is the significant increase in model size, which slows down both the training and tagging and requires more memory.</S>
    <S sid="84" ssid="18">Table 8 shows the difference in the number of contextual predicates and features between the original and final taggers.</S>
  </SECTION>
  <SECTION title="8 POS Tagging Validation" number="9">
    <S sid="85" ssid="1">To ensure the robustness of our results, we performed 10-fold cross-validation using the whole of the WSJ Penn Treebank.</S>
    <S sid="86" ssid="2">The 24 sections were split into 10 equal components, with 9 used for training and 1 for testing.</S>
    <S sid="87" ssid="3">The final result is an average over the 10 different splits, given in Table 9, where o- is the standard deviation of the overall accuracy.</S>
    <S sid="88" ssid="4">We also performed 10-fold cross-validation using MXPOST and TNT, a publicly available Markov model PO S tagger (Brants, 2000).</S>
    <S sid="89" ssid="5">The difference between MXPOST and c&amp;c represents a reduction in error rate of 4.3%, and the difference between TNT and C&amp;C a reduction in error rate of 10.8%.</S>
    <S sid="90" ssid="6">We also compare our performance against other published results that use different training and testing sections.</S>
    <S sid="91" ssid="7">Collins (2002) uses WSJ 0018 for training and WSJ 22-24 for testing, and Toutanova and Manning (2000) use WSJ 00-20 for training and WSJ 23-24 for testing.</S>
    <S sid="92" ssid="8">Collins uses a linear perceptron, and Toutanova and Manning (T&amp;M) use a ME tagger, also based on MXPOST.</S>
    <S sid="93" ssid="9">Our performance (in Table 10) is slightly worse than Collins', but better than T&amp;M (except for unknown words).</S>
    <S sid="94" ssid="10">We noticed during development that unknown word performance improves with larger a values at the expense of overall accuracy - and so using separate cy's for different types of contextual predicates may improve performance.</S>
    <S sid="95" ssid="11">A similar approach has been shown to be successful for language modelling (Goodman, p.c.</S>
    <S sid="96" ssid="12">).</S>
  </SECTION>
  <SECTION title="9 Supertagging Experiments" number="10">
    <S sid="97" ssid="1">The lexical categories for the supertagging experiments were extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier and Steedman, 2002).</S>
    <S sid="98" ssid="2">Following Clark (2002), all categories that occurred at least 10 times in the training data were used, resulting in a tagset of 398 categories.</S>
    <S sid="99" ssid="3">Sections 02-21, section 00, and section 23 were used for training, development and testing, as before.</S>
    <S sid="100" ssid="4">Our supertagger used the same configuration as our best performing POS tagger, except that the a parameter was again optimised on the development set.</S>
    <S sid="101" ssid="5">The results on section 00 and section 23 are given in Tables 11 and 12.4 c&amp;c outperforms Clark's supertagger by 0.43% on the test set, a reduction in error rate of 4.9%.</S>
    <S sid="102" ssid="6">Supertagging has the potential to benefit more from Gaussian smoothing than POS tagging because the feature space is sparser by virtue of the much larger tagset.</S>
    <S sid="103" ssid="7">Gaussian smoothing would also allow us to incorporate rare longer range dependencies as features, without risk of overfitting.</S>
    <S sid="104" ssid="8">This may further boost supertagger performance.</S>
  </SECTION>
  <SECTION title="10 Conclusion" number="11">
    <S sid="105" ssid="1">This paper has demonstrated, both analytically and empirically, that GIS does not require a correction feature Eliminating the correction feature simplifies further the already very simple estimation algorithm.</S>
    <S sid="106" ssid="2">Although GIS is not as fast as some alternatives, such as conjugate gradient and limited memory variable metric methods (Malouf, 2002), our C&amp;C POS tagger takes less than 10 minutes to train, and the space requirements are modest, irrespective of the size of the tagset.</S>
    <S sid="107" ssid="3">We have also shown that using a Gaussian prior on the parameters of the ME model improves performance over a simple frequency cutoff.</S>
    <S sid="108" ssid="4">The Gaussian prior effectively relaxes the constraints on the ME model, which allows the model to use low frequency features without overfitting.</S>
    <S sid="109" ssid="5">Achieving optimal performance with Gaussian smoothing and without cutoffs demonstrates that low frequency features can contribute to good performance.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="12">
    <S sid="110" ssid="1">We would like to thank Joshua Goodman, Miles Osborne, Andrew Smith, Hanna Wallach, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.</S>
    <S sid="111" ssid="2">This research is supported by a Commonwealth scholarship and a Sydney University Travelling scholarship to the first author, and EPSRC grant GR/M96889.</S>
  </SECTION>
  <SECTION title="References" number="13">
    <S sid="112" ssid="1">Kamal Nigam, John Lafferty, and Andrew McCallum.</S>
    <S sid="113" ssid="2">1999.</S>
    <S sid="114" ssid="3">Using maximum entropy for text classification.</S>
    <S sid="115" ssid="4">In Proceedings of the IJCAI-99 Workshop on Machine Learning for Information Filtering, pages 61-67, Stockholm, Sweden.</S>
    <S sid="116" ssid="5">Adwait Ratnaparkhi.</S>
    <S sid="117" ssid="6">1996.</S>
    <S sid="118" ssid="7">A maximum entropy part-ofspeech tagger.</S>
    <S sid="119" ssid="8">In Proceedings of the EMNLP Conference, pages 133-142, Philadelphia, PA. Adwait Ratnaparkhi.</S>
    <S sid="120" ssid="9">1998.</S>
    <S sid="121" ssid="10">Maximum Entropy Models for Natural Language Ambiguity Resolution.</S>
    <S sid="122" ssid="11">Ph.D. thesis, University of Pennsylvania.</S>
    <S sid="123" ssid="12">Adwait Ratnaparkhi.</S>
    <S sid="124" ssid="13">1999.</S>
    <S sid="125" ssid="14">Learning to parse natural language with maximum entropy models.</S>
    <S sid="126" ssid="15">Machine Learning, 34(1-3):151-175.</S>
    <S sid="127" ssid="16">Ronald Rosenfeld.</S>
    <S sid="128" ssid="17">1996.</S>
    <S sid="129" ssid="18">A maximum entropy approach to adaptive statistical language modeling.</S>
    <S sid="130" ssid="19">Computer, Speech and Language, 10:187-228.</S>
    <S sid="131" ssid="20">Mark Steedman.</S>
    <S sid="132" ssid="21">2000.</S>
    <S sid="133" ssid="22">The Syntactic Process.</S>
    <S sid="134" ssid="23">The MIT Press, Cambridge, MA.</S>
    <S sid="135" ssid="24">Kristina Toutanova and Christopher D. Manning.</S>
    <S sid="136" ssid="25">2000.</S>
    <S sid="137" ssid="26">Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</S>
    <S sid="138" ssid="27">In Proceedings of the EMNLP conference, Hong Kong.</S>
    <S sid="139" ssid="28">Hans van Halteren, Jakub Zavrel, and Walter Daelemans.</S>
    <S sid="140" ssid="29">2001.</S>
    <S sid="141" ssid="30">Improving accuracy in wordclass tagging through combination of machine learning systems.</S>
    <S sid="142" ssid="31">Computational Linguistics, 27(2): 199-229.</S>
  </SECTION>
</PAPER>
