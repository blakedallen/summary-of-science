A00-1004@@  Parallel texts have been used in a number of studies in computational linguistics. Effect of  Stopwords We also found that stop-lists have significant effect on the translation model. Both systems were tested in E-C CLIR.
A00-1006@@If we want to make a conversation proceed smoothly using these translation systems, it is important o use not only linguistic information, which comes from the source language, but also extra-linguistic nformation, which does not come from the source language, but, is shared between the participants of the conversation. worse: Impression of a translation is worse. An architecture for dialogue management, context tracking, and pragmatic adaptation i  spoken dialogue system.
A00-1008@@ The purpose of the Atlas project is to enlarge the scope of student interaction in an intelligent tutoring system (ITS) to include coherent conversational sequences, including both written text and GUI actions. Cohen, J. Morgan and M. E. Pollack, Intentions in Communication. T: Thedirection of the acceleration vector is straight up.
A00-1009@@ In this paper we present a linguistically motivated framework for uniform lexicostructural processing. In Aspects of Automated Natural Language Generation, Dale, R., Hovy, E., Rosner, A disturbance will move north of Lake Superior.
A00-1011@@ One major goal of information extraction (IE) technology is to help users quickly identify a variety of relations and events and their key players in a large volume of documents. Text Task Templates R P F-M Set in keys Rel. ), and a template generation module (cf.
A00-1014@@ In recent years, speech and natural anguage technologies have matured enough to enable the development ofspoken dialogue systems in limited domains. 102 (1) S: Hello, this is MIMIC, the movie information system. Mixed initiative in dialogue: An investigation i to discourse segmentation.
A00-1016@@  The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. A procedural model of language understanding. The upshot is that a simple semantics can be assigned to rules like the following one, which defines the action of attempting to open a door which may already be open: procedure ( open_door (D), i f_then_else (status (D, open_closed, open), presupposi t ion_fa i lure (already_open(D)), change_status (D, open_closed, open) ) ) 4.
A00-1017@@  One of the distinctive properties of natural anguage generation when compared with other language ngineering applications i that it has to take seriously the full range of linguistic representation, from concepts to morphology, or even phonetics. In addition, it is largely standard in generation as elsewhere in language applications, to make extensive use of par t ia l  representations, often using a type system to capture grades of underspecification. In Search of a Reference Architecture for NLG Systems.
A00-1018@@ For the sake of argument, lets consider a translator to be a black box with source text in and target ext out. Ident i f i ca t ion  In order for TransCheck to evaluate a translation pair, their constitutive lements must first be identified. This includes highlighting only those type of errors of interest o the user and setting the alignment parameters.
A00-1019@@  TRANSTYPE is part of a project set up to explore an appealing solution to Interactive Machine Translation (IMT). ce -que ,  lepremier ministre, ad i t . We implemented a cascade filtering strategy based on the likelihood score p, the frequency f ,  the length l and the entropy value e of the sequences.
A00-1020@@  The recent availability of large bilingual corpora has spawned interest in several areas of multilingual text processing. We filter out all Sff that do not contain A E-R. WordNet: A Lexical Database.
A00-1021@@ Question Answering is a task that calls for a combination of techniques from Information Retrieval and Natural Language Processing. Building a generation knowledge source using internet-accessible newswire. 6 Wer lec t  The Werlect algorithm used many of the same features of phrases used by AnSel, but employed a different ranking scheme.
A00-1023@@ With the explosion of information in Internet, Natural language QA is recognized as a capability with great potential. Question  Processor i : :eXt P r~_~ . However, in the real world scenario such as a QA portal, it is conceived that the IE results based on the processing of the documents hould be complemented by other knowledge sources uch as e-copy of yellow pages or other manually maintained and updated ata bases.
A00-1024@@  In any real world use, a Natural Language Processing (NLP) system will encounter words that are not in its lexicon, what we term unknown words. S. Weiss and N. Indurkhya. However, capitalization i formation is not available in closed captions.
A00-1025@@  In this paper, we describe and evaluate an implemented system for general-knowledge question answering. In E. Voorhees, editor, Proceedings of the Eighth Text Retrieval Conference TREC 8. A vector space model for information retrieval.
A00-1026@@ Far more scientific information exists in the literature than in any structured atabase. Morin E. and Jacquemin C. Projecting corpus-based semantic links on a thesaurus. The SPECIALIST parser is based on the notion of barrier words (Tersmette t al.
A00-1028@@  Expressive grammar formalisms allow grammar developers to capture complex linguistic generalizations concisely and elegantly, thus greatly facilitating grammar development and maintenance. 8 First-order pruning no pruning on VPverb\[main\] no pruning on VPverb\[main\] and NP  Eng l i sh  First-order pruning 0.11 1. The  same f s t ructure  can  in fac t  be  ass igned by  more  than  one  parse ,  so that  in  some cases  a sentence  is con s idered  out  of  coverage  ven if  the  spec ia l i zed  grammar  ass igns  to  it  the  cor rect  f s t ruc ture .
A00-1030@@Many applications of natural language processing have a need for a large vocabulary lexicon. If it gets this fax, the rule will construct a category nmsp interpretation (a kind of noun), if the condition ( i s root-of-cat  root   (adj n))  is true (ie, if the root is a known adjective or noun). Conceptual indexing: A better way to organize knowledge.
A00-1031@@  A large number of current language processing systems use a part-of-speech tagger for pre-processing. Capitalization  Additional information that turned out to be useful for the disambiguation process for several corpora and tagsets is capitalization information. Syntactic annotation of a German newspaper corpus.
A00-1032@@ The first step in natural anguage processing is to identify words in a sentence. We examined an effect of applying the morphofragments o analysis. We use a trigram model.
A00-1033@@  Current information extraction (IE) systems are quite successful in efficient processing of large free text collections due to the fact that they can provide a partial understanding of specific types of text with a certain degree of partial accuracy using fast and robust language processing strategies (basically finite state technology). "Type Subj-Cl Subj wenn -Type Spannsatz Verb J "Type stelltenJ Verb .For m MF die Arbeitgeber Forderungen) Cont \[Type Iohfi\[l 1~ \] \[Type   mp e-Io: I II \[Verb rType Ve.b 1! 0 Base-C lause-Modu le  correctness B CFragments Recall cmterium total found correct in% Type 130 129 121 93.08 Par t ia l  130 129 125 96.
A00-1034@@ Named entity (NE) tagging is a task in which location names, person names, organization names, monetary amounts, time and percentage expressions are recognized and classified in unformatted text documents. There are a lot of common words that exist in the gazetteers, uch as T, "A", "Friday", "June", "Friendship", etc. Therefore, a conservative strategy has been applied.
A00-1038@@ The goal of the Entity Indexing R&D program at LEXIS-NEXIS is to add controlled vocabulary indexing for named entities to searchable fields in appropriate news documents across thousands of news publications, where documents include both incoming news articles as well as news articles already in the LEXIS-NEXIS archives. Using a list of all companies on the major U.S. stock exchanges, we randomly selected 89 companies for investigation. In T. Strzalkowski (ed.
A00-1039@@  The task of Information Extraction (I-E) is the selective extraction of meaning from free natural language text. Pre-proeess ing:  Normal i za t ion  Before applying the discovery procedure, we subject the corpus to several stages o f  preprocessing. 285 4 Resu l ts  1 An objective measure of goodness of a pattern o.
A00-1040@@  Named entity (NE) recognition is the process of identifying and categorising names in text. This is partially because names which are made up from initials (eg "C. N. N." and "B. A simple rule-based part of speech tagger.
A00-1041@@ In this paper, we describe and evaluate a questionanswering system based on passage retrieval and entity-extraction technology. Quant i t ies  Quantities include bare numbers and numeric expressions like The Three Stooges, 4 1//2 quarts, 27~o. Monetary amounts (eg, ~The classifier makes a three way distinction between Person, Location and Organization; names where the classifier makes no decision were classified as Other Named E~tity.
A00-1043@@Current automatic summarizers usually rely on sentence extraction to produce summaries. We can see that along five edges (they are D--+B, D--+E, D--+G, B--+A, B-+C), both the human and the program made decisions. The  a lgor i thm There are five steps in the reduction program: Step 1: Syntactic parsing.
A00-1044@@ Information extraction systems have traditionally been evaluated on online text with relatively few errors in the input. All of that data includes both the audio and a manual transcription. In order to learn how much degradation i performance is caused by the absence of these features from SNOR format, we performed the following experiment.
A00-2001@@  In this paper we describe a preliminary implementation of a middle-level dialogue management system. In J. P. Miiller, M. J. Wooldridge, and N. R. Jennings, editors, Intelligent Agents III -Proceedings of the Third International Workshop on Agent Theories, Architectures, and Languages (ATAL-96), Lecture Notes in Artificial Intelligence. 2 contains the dialogue moves observed in the most recent urn, la tes t_speaker ,  and next_moves, containing the dialogue moves to be performed by the system in the next turn.
A00-2002@@ Almost all current MT systems process text one sentence at a time. Rhetorical relations that end in the suffix "-e" denote relations that correspond to embedded syntactic constituents. We end with a discussion.
A00-2003@@ Generating adequate referring expressions i an active research topic in Natural Language Generation. Nevertheless, overall, the sortal class annotations were reliable (n ---0. Sortal classes provide information about the discourse entity that a referring expression evokes or accesses.
A00-2004@@They proposed that text segments with similar vocabulary are likely to be part of a coherent opic segment. In Proceedings of the 31st Annual Meeting of th.e ACL. Char_align: A t)rogram for aligning parallel texts at the character level.
A00-2011@@ Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Journal of the Royal Statistical Society, Series B, 39(I). A Statistical Approach to Machine Translation.
A00-2014@@However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal representation. The most generalizable of extracted grammars uses the D i rect  method on template-expanded s ntences. A theory of the learnable.
A00-2015@@  In the Japanese language, since word order in a sentence is relatively free compared with European languages, dependency analysis has been shown to be practical and effective in both rulebased and stochastic approaches to syntactic analysis. The (a) (b) (c) (d) (e) (f) (g) (h) content words part of the chunk (bunsetsu) is one of the following types: A predicate (ie, a verb or an adjective). Using decision trees to construct a practical parser.
A00-2016@@  Given the enormous complexity of natural anguage, parsing is hard enough as it is, but often unforeseen events like the crises in Bosnia or East-Timor create a sudden demand for parsers and machine translation systems for languages that have not benefited from major attention of the computational linguistics community up to that point. A sentence has a correct operat ing  sequence,  if the system fully predicts the logged parse action sequence, and a correct s t ruc ture  and  labe l ing,  if the structure and syntactic labeling of the final system parse of a sentence is 100% correct, regardless of the operations leading to it. A Theory of Syntactic Recognition for Natural Language.
A00-2017@@  The task of predicting the most likely word based on properties of its surrounding context is the archetypical prediction problem in natural language processing (NLP). will J  b~rd corped~%~ det pcomp det.. a-.. Vir~.n attr "moc l  tl~e director, Pierre years attr * non-eXecutive qnt 6t Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29. Def in i t ion 4 (Compos ing  features)  Let f l ,  f2 be feature definitions.
A00-2019@@The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word. % of the sample from ALEKs E-set and 18. When more than one error occurred, the distance of the one closest o the target was marked.
A00-2020@@  Manually marking corpora is a time consuming and expensive process. Our distribution for the data, D, is then: D -(1 A)M + AA (I) where M is the majority distribution, and A is the anomalous distribution. The process is subject o human error by the experts doing the marking.
A00-2021@@  "Unification-based" Grammars (UBGs) can capture a wide variety of linguistically important syntactic and semantic onstraints. Predicates with suffix : s indicate the subject slot of an intransitive or transitive verb; the suffix : o specifies the nouns in the corresponding row as objects of verbs or prepositions. Such a procedure stimates parameters )~m+l,.-., Am+k associated with the auxiliary features, so the estimated istributions take the form (6) (for simplicity we only discuss joint distributions here, but the treatment of conditional distributions i parallel).
A00-2023@@  Large textual corpora offer the possibility of a statistical approach to the task of sentence generation. E(cjJcontext(cl..Cj_l) ) where I stands for the internal score, E the external score, and cj for a child node of p. The specific formulation of I and E, and the precise definition of the context depends on the language model being used. A capped N-best heuristic search algorithm on the other hand has complexity O(vN1).
A00-2024@@  There is a big gap between the summaries produced by current automatic summarizers and the abstracts written by human professionals. (5) genera l i za t ion  or specification Replace phrases or clauses with more general or specific descriptions. In A. Manaster-Ramis, editor, Mathematics of Language.
A00-2025@@One way of compressing audio information is the  automatic creation of textual summaries which can be skimmed much faster and stored much more efficiently than the audio itself. 5For EXP, we define 0  2: 0 1 1 1 1 1 1 and here we have very  re levant  in fo rmat ion  and HE ** BEHAVES **** IRREVERENT FORMATION C S D S D S S 0. This paper addresses in particular this latter case and (a) explores means of making textual summaries less distorted (ie, reducing their word error rate (WElt)), and (b) assesses how the accuracy of the summaries changes when methods for word error rate reduction ar e applied.
A00-2026@@  This paper presents three trainable systems for surface natural language generation (NLG). Domains which require complex and lengthy phrase fragments to describe a single attribute will be more challenging to model with features that only look at word n-grams for n E {2, 3). is a special "stop" symbol.
A00-2027@@Although these systems interact with users in a manner more similar to human-human interactions than earlier systems employing system initiative strategies, their response strategies are typically selected using only local dialogue context, disregarding dialogue history. Therefore, their gain in naturalness and performance under optimal conditions i often overshadowed by their inability to cope with anomalies in dialogues by automatically adapting dialogue strategies. Performance Feature r p # of user turns 0,71 0 ASR rejection 0.
A00-2028@@  Spoken dialogue systems promise fficient and natural access to a large variety of information sources and services from any phone. SI: AT&T How may I help you UI: I need credit please. U3: It is a wrong number.
A00-2029@@  One of the central tasks of the dialogue manager in most current spoken dialogue systems (SDSs) is error handling. Our predicted classes correspond to correct recognition (T) or not (F). In Proceedings of E UR OSPEECH99.
A00-2030@@Yet, relatively few have embedded one of these algorithms in a task. t "ABC News" is the name of an organization. "A paid consultant to ABC News" describes a person.
A00-2031@@But to date, the end product of the parsing process has for the most part been a bracketing with simple constituent labels like NP, VP, or SBAR. In fact, we know of only 1There is a single exception i the corpus: one constituent is tagged with -LOC-I~R. If knowledge about feature fi makes S more likely than with just f l .
A00-2032@@ Because Japanese is written without delimiters between words) accurate word segmentation to recover the lexical items is a key step in Japanese text processing. Compatible and all-compatible brackets rates lOOT I g5 t 85 . 2%, respectively) with respect o Chasen.
A00-2033@@  A long-standing issue regarding algorithms that manipulate context-free grammars (CFGs) in a "topdown" left-to-right fashion is that left recursion can lead to nontermination. In Proceedings o/ the Spoken Language Technology Workshop, pages 3-8, Plainsboro, New Jersey. Replace the set of all productions A-+af t1 ,  .
A00-2034@@  Diathesis alternations are alternate ways in which the arguments of a verb are expressed syntactically. A generative perspective on verb alternations. The outcome for the individual verbs using the mean as a threshold was: True negatives: add admit answer borrow choose climb cost declare demand drink eat feel imagine notice outline pack paint perform plan practise prescribe proclaim read remain sing steal suck survive understand wash win write  True positives: bend boil burn change close cool dry end fly improve increase match melt open ring roll shut slam smash Mart stop tilt wake  False negatives: accelerate bang break cook crack decrease drop expand flood land march repeat rip rock shatter 260 _ ~ ~ ~Objects  of Subjects _~ / ~ "~ntransitive Intra/~i:e /silence ~ / / \ \ war.
A00-2035@@  Sentence boundary disambiguation (SBD) is an important aspect in developing virtually any practical text processing application syntactic parsing, Information Extraction, Machine Translation, Text Alignment, Document Summarization, etc. Instead, they applied a simplified method. A standard way to identify syntactic ategories for word-tokens i part-of-speech (POS) tagging.
A00-2036@@ Traditionally, algorithms for natural language parsing process the input string strictly from left to right. The first author is supported by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 V0, and was employed at AT&T Shannon Laboratory during a part of the period this paper was written. , an, qn be an accepting computation for w in M, and choose some symbol $  E. We can now encode the accepting computation as ($, q0)(al, ql)  (an, qa) where we pair alphabet symbols to states, prepending $ to make up for the difference in the number of alphabet symbols and states.
A00-2037@@ As our ability to build robust and flexible spokenlanguage interfaces increases, it is worthwhile to ask to what extent we should incorporate various human-human discourse phenomena into our dialogue models. Experiments with a Spoken Dialogue System for Taking the U.S. Census. Approach In this study we hypothesized that subjects will choose to use acknowledgments in human-computer interaction if they are given an interface that provides opportuni t ies  for and responds to acknowledgments.
A00-2039@@ In the past two decades computational morphology has been quite successful in dealing with the challenges posed by natural anguage word patterns. This allows us to move forward while suppressing the spellout of e. A generic recipe for infixation ensures that segmental material can be inserted anywhere within an existing morpheme FSA. Steven Bird and T. Mark Ellison.
A00-2040@@  Automatic grapheme to phoneme conversion (ie the conversion of a string of characters into a string of phonemes) is essential for applications of text to speech synthesis dealing with unrestricted text, where the input may contain words which do not occur in the system dictionary. macro (de fau l t ru les ,  %% museum %% moedig(st) %% mogelijkheid . % and a phoneme accuracy of 93.
A00-2043@@With the emergence of empirical methodologies in the early nineties, attention has almost completely shifted away from this topic. 2We current ly do not further interpret the information contained in tense or modal i ty  markers. In M. Bates and R. M. Weischedel, editors, Challenges in Natural Language Processing, pages 146-175.
A83-1002@@The EUFID system permits users to communicate with database management systems in natural English rather than formal query languages. Moreover, if some companies were only receivers and never shippers it is " "~e is the important dist inct ion between a "closed world" database in which the assumption is that the database covers the whole world (of the application) and an "open world" database in which it is understood that the database does not represent all there is to the real world of the application. Users  may make t ime compar i sons  that require more granular ity than is stored in the  database.
A83-1015@@  The EPISTLE  project has as its long-range goal the machine processing of natural language text in an office environment. Yet few grammarians would care to dignify, by describing it with rules of the core grammar,  a text string like: "Opt ions:  A l (T ransmi t ter  Clocked by Dataset) B3-(without the 605 Recall Unit) CS-(with ABC Ring Indicator) D8twi thout  Auto Answer) E I0  (Auto Ring Selective)." NOUN*"Commiss  ion"  I .
A83-1016@@ A large amount of natural language Input, whether to text processing or questlon-answerlng systems, conslsts of shortened sentence forms, sentence nfragmentsn. In Advances In 17 (N.C. Yov l ts ,  ed . ) For example, In type If fragment skin no ~ruot lons,  skin has the medical subclass BODYPART, and erunt lons  has +he medlcal subclass SIGN/SYMFrrOM.
A83-1028@@Thus,  our "app l ied"  sys tem remains  a research  veh ic le  that  serves  as an exce l lent  tes tbed  fo r  proposed new procedures . Paxton, W. H., "A Framework fo r  Speech Understanding," Tech. Our evaluation of the LRC ~E systems current status will be based on three points!
A88-1002@@ The large economic potential of automatic text processing is leading to an increasing interest in its commercial pplications. $d matches any determiner (a, the, this, etc. In particular, the words and phrases the system looks for and the context in which i t  looks for them are specified through a modified version of the powerful pattern matching language used in Carnegie Groups Language Craft TM product I \[3\].
A88-1003@@The parser p..roduces as !ts output a feature graph that descnoes the syntacticpropenies ov the constituents of the sen(ence. U-2 x4 e5 x6  x7 x8 I I x9 ANNOTATIONS: begin reso lv ing  anaphor  "it" l ist o f  cand idates  and their  init ial scores  evaluate f i rst  candidate, "himself" animate cand idate  cannot corefer with "it" next candidate to be evaluated is "f i le" no CS eva luator  wants  to reject or suppor~ this candidate  "fi le" surv ives  with orignal score of  ! Recency, for example, uses the following scoring: Sentence (score, confidence) contribution of this CS n (current) (1, 0. )
A88-1007@@ A major concern in designing a naturallanguage system is portability: It is advantageous to design a system in such a way that it can be ported to new domains with a minimum of effort. E.g., for the svo pattern \[Iou t instal\[, aac\], the query to the user would be something like "Can a loss install a sac". I  Logic Programming and its Applications, D.I-LD.
A88-1008@@ Natural anguages contain a variety of "logical operators" which interact with each other to give rise to different ypes of ambiguity. (36) Each person grabbed a chair . To appear in G. Chierchia, B. Partee & R. Turner (eds.
A88-1010@@This algorithm, which takes time proportional to rt 3 (rt -" length of input string) on a single processor, can operate in time n using n 2 processors (with shared memory and allowing concurrent writes). As a result, our procedure would not operate in l inear t ime for general (ambiguous) grammars . Both an interpreter and a compiler are available.
A88-1012@@Establishment of standard representations and access mechanisms for lexical information will help us to better meet hese requirements. with a hierarchical relationship among its internal components. The entire Collins English-German dictionary, consisting of 46,600 entries, was recently submitted to the parser and E-G grammar with a parsing rate of 80% of the entries.
A88-1014@@Since many queries are simply sets of lexemes or phrases, and all queries can be reduced to that form, we believe that focusing on lexical and phrasal issues may be the most appropriate strategy for applying computational linguistics to information retrieval. In Visible Language, W. Frawley and R. Smith (eds. "A Tool Kit for Lexicon Building."
A88-1018@@ The System for Conceptual Information Summarization, Organization and Retrieval (SCISOR) is an implemented system designed to extract information from naturally occurring texts in constrained omains. However, SCISOK is designed to benefit ultimately from an extended vocabulary (i. e. thousands of word roots) and increased omain knowledge. ACE, an apparel maker planning  a leveraged buyout, rose $2 I/2 to $3S 3/8, as a rumor spread that another buyer might appear.
A88-1020@@Work on applications such as information retrieval or machine translation has consistently focused on semantic analysis. DECL NP DET ADJ* "the" NOUN* "man" VERB* "visited" NP DET ADJ* "the" N O U N * "apart ment" ~ PP PREP "in" DET ADJ* "the" NOUN* "house" PUNC "." Thus 65% is a conservative estimate.
A88-1026@@And Motivation Introduction Luke is a knowledge base editor that has been enhanced to support entering and maintaining the semantic mappings needed by a natural language interface to a knowledge base. )*11 ~ USER-RGEHDR (e tasks) (m I~t,m,c~ofRCrt.H~) 42 tasks: Verify  napping M.OATR-i ( jb )  11,25a8 Ver i fy  nab uord -OATA( jb )  i lp25s8 Ver i fy  hem aord -COMPUTATION( jb )  Verify hem uord -COMPUTE( jh)  11s2 Verify napping H.FILE-I ( jb )  11s25s8 Ver i fy  hem uord -FILE( jb )  l la25sA Oeflne nouns FILE-STRUCT (jb)  11s25s 9ef ine nouns ORTR-STRUCT ( jb )  l l s ISs  6 .b A. Proceedings of the 24th Annual Meeting of the Association of Computational Linguistics.. R.J. Brachman, R.E.
A88-1027@@ CRITIQUE is a large-scale natural anguage text processing system that identifies grammar and style errors in English text. Lets contemplate how a president is selected. Heidom, George E., Karen Jensen, Lance Miller, Roy Byrd, and Martin Chodorow.
A88-1028@@ This paper describes enhancements made to current name search techniques used to access large databases of proper names. ,  Final Hidden Markov Model Parameters Two Sta!e, State Output Model Output Probabilities Symbol CR a b c d e State Transition State 1 2 1 0 0 0.33 0 0. Each state corresponds to exactly one output symbol (a letter or word delimiter).
A88-1030@@ The present paper describes the procedure that was followed in an extended experiment o reliably find basic surface clauses in unrestricted English text, using various combinations of finitary and stochastic methods. In the example above, \[the man\] would be a noun phrase fragment. The definition of basic clause that was used in this study has the following characteristics: a) it concentrates on certain defining characteristics present at the beginnings of clauses; b) it follows from a particular hypothesis about syntactic working memory: that it is limited to processing one clause at the time; and c) it assumes that the recognition of any beginning of a clause automatically eads to the syntactic losure of the previous clause.
A88-1031@@ Nabu is a multilingual Natural Language Processing system under development in the Human Interface Laboratory at MCC, for shareholder companies. R IGOROUS TEST ING In order to become practical, a system must admi t  and have undergone -rigorous testing. When (and if) this rule fires, it would match the suffix (ation) in the glosseme string derivation, and the feature (NOUN), and therefore transform that glosseme into pL)f.derive" (VERB (DERIVE NOUN +ION) Note the e restoration: rules can in principle remove and add any letter sequence, hence affix alternation is handled in a straightforward man232 ner.
A88-1032@@  Ambiguity is a problem in any natural anguage processing system. If g is the Grand Canyon, n is New York, and e is the seeing event, the neutral ogical form will include . Let o be the oil and c the compressor.
A88-1033@@Words are disambiguated up to part-of-speech, but word senses are not identified. "DIAGRAM: A Grammar for Dialogues, Comm. The efficiency of the method, ie, the speed of finding an optimal path, depends on the accuracy of tile estimates ll(n).
A88-1034@@The basic advantages of this design stem from the fact that it is inherently modular: control is simple, modules can be developed and debugged independently. take "for" as a marker. This we did by having prepositions, not verbs, always take the NP  as an argument if there was ~t preposition/particle intervening between the verb and the NP  and by having verbs take the NP  as an argument if there were no preposition/particle intervening.
A92-1001@@  The basic question addressed in this paper is that of how to connect a general NL interface and a back-end application in a principled way. So work onl(E,C,clare) can be expanded t DB_PROJECT_MEMBEK(clare,C), giving the desired r~ sult. where ski is a Skolem function.
A92-1008@@  The increasing amount of information to be communicated to users of complex technical systems nowadays makes it necessary to find new ways to present information. If 1 E YLOC, we take the seconc component. (4) "Object B is below Object A."
A92-1010@@ Natural anguage interfaces to information systems allow the user to converse with the system in a natural way without being restricted by the syntax of a formal query language. Pointing: A Way Toward Explanation Dialogue. The I-Domain concepts can be generated automatically flom the underlying SIC!
A92-1011@@  Over the last few years, the utilization of machine readable dictionaries (MRDs) in compiling lexical components for Natural Language Processing (NLP) systems has awakened the interest of an increasing number of researchers. OBJ and E-ItlJI~N are sorted variables for individual objects. A Taxonomy for English Nouns and Verbs.
A92-1012@@  The ACQUILEX LKB is designed to support representation of multilingual exical information extracted from machine readable dictionaries (MRDs) in such a way that it can be utilised by NLP systems. In E. Klein and F. Veltman (eds. 4 Defau l t  inher i tance  and  taxonomies  We extend the typed FS system with default inheritance.
A92-1013@@ One of the fundamental property of computational lexicons is an account of the relations between verbs and its arguments. In general word associations are collected by extracting word pairs in a +-5 window. A second problem with statistically collected word pairs is that an analysis based simply on surface distribution may produce data at a level of granular i ty too fine.
A92-1014@@  Quite a few grammatical formalisms have been proposed by computational linguists, which are claimed to be "good" (declarative, highly modular, etc.) CT,~ : Credit of instance-tuple T with identification number i. /9 is a constant parameter.
A92-1019@@  This paper eports on the development of DILEMMA-2*, a lemmatizer-tagger for the sublanguage of medical abstracts. In the case of errors concerning noun,  verb  or adj e ct ive assignment, we encountered similar context problems, and again, only in very strict contexts could a rule be applied. The numeral only has a predecessor, viz.
A92-1024@@ While a computer program that can provide complete understanding of arbitrary input text remains a distant dream, it is currently possible to construct natural anguage processing systems that provide a partial understanding of certain types of text with limited accuracy. B., and Schmandt, L. M. TCS: A Shell for Content-Based Text Categorization. This pattern would match in sentences like the following: e ABC Company announced profits of more than 50 million dollars last year.
A92-1027@@ Much of the research being done in parsing today is directed towards the problem of information extraction, sometimes referred to as "message processing" or "populating data-bases". As a program, Sparser is quite robust. To this end, Sparser parses opportunistically and bottom up rather than predicting that an S will be completed.
A92-1028@@ In all natural anguages, components hat can be easily deduced by the reader are frequently omitted f~om expressions in texts. Sentence Structure Control \] \] I(no,ledge base I I Rule~o~%a~r~eziin ~ \[ I Verbal Semantic Information  Knowledge Base Verbal Semantic Feature Syatea Rules for Determining L Verbal Relationships \[. Megumi Kameyama "A property-sharing constraint in centering."
A92-1040@@ The approach to dialogue managenaent described here has been developed as part of the Sundial project (Speech UNderstanding in DIALogue) whose goal is to build real-time integrated computer systems capable of maintaining co-operative dialogues with users over standard telephone lines. Towards a dynamic interopretation theory of utterances in dialogue. The language of spoken interaction and the application task can both be changed by instructing the relevant modules t.o consult different knowledge bases.
A94-1002@@In a collaborative effort between academics and industry, we have embarked on a project that uses text generation research in service of an industrial application. Text generation in COMET In Dale, R. and Mellish, C.S. Fig5written in FUF by J. Lira 6written in C by N. Morgan 9 RUNID: REG1 Run-ID REG1 started at the BASE plan.
A94-1003@@  Computational linguists work with texts. A more troubling problem is the instability of the model. The n most frequent tokens for each corpus are selected.
A94-1004@@ Documents are becoming increasingly available in machine-readable form. Note that a character shape code subset { , . We expect o be able to extend the technique to many other European languages that have similar characteristics.
A94-1007@@ This paper presents a model for analyzing English sentences including coordinate conjunctions such as "and" , "or" , "but" and equivalent words. Analysis cost: English coordinate conjunctions have a variety of linguistic functions. These gaps must be filled with elements from other conjunct for a proper interpretation, as in (1) and (1).
A94-1008@@ This paper combines knowledge-based and statistical methods for part-of-speech disambiguation, taking advantage of the best features of both approaches. Because of the limited size of the window, the choices made by a stochastic disambiguator a e often quite naive from the linguistic point of view. % of words received a correct analysis.
A94-1009@@Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus. Initial  maximum Highest accuracy on the first iteration, and falling thereafter. Tagging English Text with a Probabilistic Model.
A94-1010@@  In speech recognition and understanding systems, many kinds of language model may be used to choose between the word and sentence hypotheses for which there is evidence in the acoustic data. When all training utterances have been incorporated, find all the triples (u, ci,cj), i ~ j, such that u E ci but the probability of u is maximized by cj. The training corpus consisted of the 4,279 sentences in the 5,873-sentence such that were analysable and consisted of fifteen words or less.
A94-1012@@  Current technologies in natural language processing are not so mature as to make general purpose systems applicable to any domains; therefore rapid customization of linguistic knowledge to the sublanguage of an application domain is vital for the development of practical systems. A Probabilistic Parsing Method for Sentence Disambiguation. A hypothesis i either competing with or complemenlary to other hypotheses generated from the same sentence.
A94-1013@@End-of-sentence punctuation marks are ambiguous; for example, a period can denote an abbreviation, the end of a sentence, or both, as shown in the examples below: (1) The group included Dr. J.M. A status report on the ACL/DCI. This finds abbreviations like "J.R." and "Ph.D." Note that the final period is a punctuation mark which needs to be disambiguated, and is therefore not considered part of the word.
A94-1016@@ Current MT systems, whatever translation method they employ, do not reach an optimal output on free text. Statistically generating such a model is feasible, since it does not rely on knowing correspondences between source References  Brown, P., K. Cocke, S. Della Pietra, V.J. "The Pangloss Mark I MAT System."
A94-1018@@ It is important that natural anguage interface systems have the capability of composing the globally most plausible explanation if a given input can not be syntactically parsed. Another solution indicated by (e-9), which says that the fifth word of the input must be a conjunctive is derived from (e-7). 4 P re l iminary  Exper iments  In order to evaluate the technique described above, some preliminary experiments were conducted.
A94-1019@@ Large terminological databases are now available and can be used as lexicons in Natural Language Processing (NLP) systems aimed at terminology extraction. Such a limitation is specifically problematic during the development s age. A logical semantics for feature structures.
A94-1024@@  As a part of large scale project on natural anguage processing for Turkish, we have undertaken the development of a number of tools for analyzing Turkish text. The  Multi-word  Construct Processor As mentioned before, tagging text on lexical item basis may generate spurious or incorrect results when multiple lexical items act as single syntactic or semantic entity. 4 Per fo rmance  of  the  Tagger  We have performed some preliminary experiments to assess the effectiveness of our tagger.
A94-1025@@NLP in medic ine Medical patient reports consists mainly of free text, combined with results of various laboratories. Black, and S.G. Pulman. In the current segmentation program, the major part of this list contains medical suffixes, which constitute a clearly definable 2v for verbs, adj for adjectives and n for noun; others are nb \[sing or phr\] for number, pers \[1, 2, 3 or nil\] for person.
A94-1026@@Homophone errors are one of the error types that can be detected and corrected in REVISE. "fZ~" is detected as a homophone error. Because the semantic ategory \[nature\] for "t~l~" is included in this set, "~"  is indicated as a correct candidate for homophone error "f~-~--" in the compound noun "I~I ,~.. ~-~" based on formula (2).
A94-1027@@ Text categorization is the classification of documents with respect o a set of predefined categories. Usually, a set of categories i defined beforehand. S. E. Robertson and K. Sparck Jones.
A94-1028@@ The task of information retrieval is to extract relevant documents from a large collection of documents in response to user queries. s Including extra terms in documents changes the way other terms are weighted. the relevance of a document is established through full-text retrieval.
A94-1047@@Forty percent of the operational marine forecasts (roughly half of all marine forecast text) in Canada is now produced using FoG. FoG uses three major stages to compose forecasts: (1) graphically mediated content determination, (2) text planning resulting in interlingual forms, and (3) realization of English and French texts from the interlingua. Lexical Selection and Paraphrase in a MeaningText Generation Model.
A97-1003@@  In the area of machine translation, one important interface is that between the speech recognizer and the parser. A *   indicates the locat ion of the error. Any opinions, findings, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
A97-1004@@  The task of identifying sentence boundaries in text has not received as much attention as it deserves. It is theretbre asy and inexpensive to retrain this syst.em tbr different genres of text in English and text in ()tiler l:(.oma.n-a.lphabet languages. 3 Our  Approach  \e present two systems for identifying sentence boundaries.
A97-1006@@ Appointment scheduling is a problem faced daily by many individuals and organizations, and typically solved using communication in natural anguage (NL) by phone, fax or by mail. COSMA allows human and machine agents to participate in appointment scheduling dialogues via e-mall. A Diagnostic Tool for German Syntax.
A97-1007@@The central storage for dialogue information within the overall system is the dialogue module that exchanges data with 15 of the other modules. A turn is defined as one contribution of a dialogue participant. Basic notions within VERBMOBIL are tu~8 and  ut terances.
A97-1011@@ We are concerned with surface-syntactic parsing of running text. e Sometimes the context gives strong hints as to what the correct reading can not be. Parsing English with a link grammar.
A97-1012@@Previous work in finite-state parsing at sentence level falls into two categories: the constructive approach or the reductionist approach. Douglas E. Appelt, Jerry It. Each transducer performs a specific linguistic task.
A97-1013@@ The language models used by natural anguage analyzers are traditionally based on two approaches. Models are coded as follows: B stands for bigrams, T for trigrams and C for hand-written constraints. ,  v,~} be a set of variables.
A97-1014@@  The work reported in this paper aims at providing syntactically annotated corpora (treebanks) for stochastic grammar induction. focus particles such a.s only or also. The typical treebank architecture is as follows: S t ructures :  A context-free backboI~e is augmented with trace-filler epresentations of non-local dependencies.
A97-1015@@  A major concern in corpus based approaches i that the applicability of the acquired knowledge may be limited by some feature of the corpus. 1 in column A, row E shows the cross entropy of modeling by domain E and testing on domain A. of qualif ied partial trees) ra t io  frequency ru le  (Example) (domain/corpus) A. Johnson" "First Lady Jacqueline Kennedy" B.
A97-1016@@The language specific information of such a system is stored as 1. a morphotactic description of the words to be processed as well as 2. a set of two-level morphonological (or spelling) rules. Afr ikaans plurals are almost always derived with the addi t ion of a suffix (mostly -e  or s )  to the singular form. Andrew R. Golding and Henry S. Thompson.
A97-1018@@and the Related Issues In Chinese, there do not exist delimiters, such as spacing in English, to explicitly indicate boundaries between words. (ii) structural information # nature of characters absolute closure characters for CNs They will definitely belong to a Chinese surname once falling into the control domain of it:  open characters for CNs For this sort of characters, possibilities of being included in a name and excluded out of the name must be reserved: CNI read novel (CNI is reading a novel) CN2 like read novel (CN2 likes to read novels) # position in unknown words For instance, "~"  always occurs in the first position of given name of CNs, illustrated as "~:~l~E ~" . References \[1\] N.Y. Liang, "Automatic Chinese Text Word Segmentation System -CDWS", Journal of Chinese Information Processing, Vol.
A97-1020@@ GLOSSER applies natural anguage processing techniques, especially morphological processing and corpora analysis, to technology for intelligent computerassisted language learning (ICALL). Non que leurs urines de l~fea ion,  m~is e l l~ offri~ent des dimen*ions  in de arm treffen ~c. Morphological analysis (lemmatization) is robust and accurate and more than up to the task 137 DE LA TERRE A LA LUNE Trajet Direct en 97 Heures 20 Minutes  par Jules Veme I LE GUN-CLUB sueme f(~r ale de,, I~tats-Unis, un nouveau club tr~s influvnt tn~ la rifle de Bali/more, en ple~n Maryland.
A97-1021@@  A wide range of new capabilities in NLP applications such as foreign language tutoring (FLT) has been made possible by recent advances in lexica. The LC, S for this verb is in the class of Fill Verbs (9. ingly for a manner, !
A97-1022@@ Automatic grammar checking is one of the fields of natural anguage processing where simple means do not provide satisfactory results. ELENI~ ZVOLEN{~HOIERRNUMI n~o,t: Is.or \] E.o,p.,,: ~ ~ ~ \[ i l J 147 The user may get several types of messages about he correctness of  the text: a) The macro changes the color of  words in the text according to the type of the detected error the unknown words are marked blue, the pairs of words involved in a syntactic error are marked red. It is locally only a tiny bit simpler.
A97-1024@@  Like most other big corporations today, IBM is interested in cost-effective, yet high-quality information dissemination. V. Lux and E. Dauphin. 1 60 tion means a savings.
A97-1025@@  Spelling checkers are now available for all major word processing systems. They represent a scaling factor for each dimension in the T and D matrices. A principle axis transformation for non-hermitian matrices.
A97-1026@@ To replace the conventional multiple questions on standardized xaminations, choice ~Test i ems in this paper are copyrighted by Educational Testing Service (ETS). It was used in this study to distinguish t e Excellent essays with scores of 9 and l0 from essays with lower end scores in the 0 3 range. The mechanism of action o f r~ iet ion  enz3anes.
A97-1029@@ In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMMs to their problems. In particular, the firstWord feature arises from the fact that if a word is capitalized and is the first word of the sentence, we have no good information as to why it is capitalized (but note that allCaps and capPeriod are computed before f i r  s tWord,  and therefore take precedence). The top level model for generating all but the first word in a name-class iPr((w,f) I (w,f)_,, NO).
A97-1031@@  There is no doubt that the amount of textual information electronically available today has passed its critical mass leading to the emerging problem that the more electronic text data is available the more difficult it is to find or extract relevant information. :np))) 30 generic syntactic verb subcategorization frames are defined by fragment combination patterns (e.g, for transitive verb frame). 2 Par t -o f speech  d isambiguat ion  Morphological ambiguous readings are disambiguated wrt.
A97-1032@@  Real technical documents are full of text in tabular and other complex layout formats. We have a set of n v-v  cohesion functions {f~r-v, fv -v . t specifies whether the (sub)template is to be counted as a value (tv) or a label area ( t l ) .
A97-1033@@In this summarization paradigm, problems arise when information needed for the summary is either missing from the input article(s) or not extracted by the information extraction system. E.g., if an ontology contains information about the word "president" as being a realization of the concept "head of state", then under certain conditions, the description can be replaced by one referring to "head of state". Among these are n-grams such as "Prime Minister" or "Egyptian President" which were tagged as NP by POS.
A97-1034@@ The theme of this paper is the design of software and data architectures for natural language processing using corpora. edu/t ipst er D. McKelvie, H. Thompson and S. Finch. A. Mikheev and S. Finch.
A97-1035@@  This paper reviews the currently available design strategies for software infrastructure for NLP and presents an implementation of a system called GATE a General Architecture for Text Engineering. Add i t ive  Approaches  Additive architectures for managing information about text add markup to the original text at each successive phase of processing. A related issue is storage overhead.
A97-1036@@ The shift from Computational Linguistics to Language Engineering 1 is indicative of new trends in NLP. "Abstract Data Types for Multext Tool I/O". This multilingual machinetranslation system is built out of heterogeneous components, such as an English generator written in Lisp, a Spanish morphological nalyzer written in Prolog, a Glossary-Based Machine-Translation e gine written in C, etc.
A97-1038@@  CogentHelp isa prototype tool for authoring dynamically generated on-line help for applications with graphical user interfaces (GUIs). In A. Zampolli, N. Calzolari, and M. Palmer, editors, Current Issues in Computational Linguistics: In Honour of Don Walker. Next we determined that since no inference is required beyond checking for equality, these propositions and properties can be conflated with their linguistic rea l i za t ions  ie, the indexed, human-authored help snippets CogentHelp takes as input.
A97-1041@@  In a hospital setting it can be difficult for caregivers to obtain needed information about patients in a timely fashion. S. Pan and K. McKeown. (Currently, we are using AT&T Bell Laboratories Text To Speech System).
A97-1042@@ :  Top ic  Spot t ing  by  Pos i t ion  In an increasingly information-laden world, the problem of automatically finding the major topics of texts acquires new urgency. Morgan Kaufmann Publishing Co. D.E. In R. Merchant, editor, The Proceedings of the TIPSTER Text Program Phase I, San Mateo, California.
A97-1043@@ With increasing numbers of machine readable documents becoming available, automatic document summarisation has become one of the major research topics in IR and NLP studies. L imi ta t ions  o f  the  Method When the ratio of extraction was higher than 30%, the results was 72. Key  Paragraphs  Exper iment  E f fec t iveness  o f  the  Method In Key Paragraphs Experiment, the overall results were positive, especially when the ratio of extraction was 10,,,30%.
A97-1046@@  Information Retrieval (IR) is an increasingly important application area of Natural Language Processing (NLP). References Belkin, N., and Croft, B. In: Sag, I. and Szabolcsi, A.
A97-1049@@  More and more multilingual information is available on-line every day. This system is designed to expand to other lan332 guages besides English and Japanese and other domains beyond S&T terms. The user has the choice of selecting the sources (e.g, Washington Post, Nikkei Newspaper, Web pages), languages (eg, English, Japanese, or both), and specific date ranges of documents to constrain queries.
A97-1050@@  Reliable translation lexicons are useful in many applications, such as cross-language t xt retrieval. A scalable architecture for bilingual lexicography. Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons.
A97-1051@@  In the absence of complete and deep text understanding, implementing information extraction systems remains a delicate balance between general theories of language processing and domain-specific heuristics. A rule language for constructing task-specific phrase tagging and/or p e-tagging rule sets. A Corpus-Based Approach to Language Learning.
A97-1052@@ Predicate subcategorization is a key component of a lexical entry, because most, if not all, recent syntactic theories project syntactic structure from the lexicon. We compute this measure by calculating the percentage ofpairs of classes at positions (n, m) s.t. In Boguraev, B. and Briscoe, E. eds.
A97-1053@@ In corpus-based NLP, extraction of linguistic knowledge such as lexical/semantic collocation is one of the most important issues and has been intensively studied in recent years. Let F(e) be the set of tuples (fl . ,  fn are judged as independent if, for every subset f i l , .
A97-1055@@ It is well known that statistically-based approaches to lexical knowledge acquisition are faced with the problem of low counts. We now need a scoring function to evaluate these alternatives. The purpose of the method is to automatically select:  A domain-appropriate such tthat of categories, that well represent the semantics of the domain.
A97-1056@@  In this paper word-sense disambiguation is cast as a problem in supervised learning, where a classifier is induced from a corpus of sense-tagged text. A sentence with an ambiguous word is represented by a feature set with three types of contextual feature variables: 2 (1) The morphological feature (E) indicates if an ambiguous noun is plural or not. The measure of a model.
C00-1003@@  ~lPhe term selectional restrictions refers to semantic sortal constraints imposed on the 1)ar -ticipants of linguistic constructions. Among other things, it wouhl need to know that keyl)o~r(ts ~rc not edible. We have timnd the se(:ond at)l)roach t)articularly useful in the develolnnent of practical systems.
C00-1004@@On the other hand, there is still continuing demand for the iinprovement of learning lnodels when sufficient quantity of annotated corpora are not available in the users domains or languages. Brill proposed a transfbrmation-based method. The second and lower levels are th, e subdivision level.
C00-1007@@  For many apt)lications in natural anguage gen~ eration (NLG), the range of linguistic expressions that must be generated is quite restricted, and a grammar tbr generation can be fltlly specified by hand. N{)l;e that in an LTAG, I;here is no distinction betw{:en lexicon nnd grammar. %~gs as a grammatical formalism tbr generation.
C00-1010@@A DOP model provides linguistic representations lotan tmlimitcd set of sentences by generalizing from a given corptts of annotated exemphus, it operates by decomposing the given representations into (arbitrarily large) fiagments and recomposing those pieces to analyze new sentences. "An optimized algorithm for Data Oriented Parsing", in R. Mitkov and N. Nicolov (cds. Learning E\[ficient Disambiguation.
C00-1011@@ A well-known property of stochastic grammars is their prope,lsity to assign highe, probabil it ies to shorter derivations o1 a sentence (cf. "l,carning to Parse Natural 14mguage with Maximum linlropy Models", Machit~e /.earning 34. % while the Viterbi n-best method obtains 84.0%.
C00-1014@@ In this paper we consider two questions. I Numeral-classilier combinations are shown in bold, the noun phrases they quantify are underlined. Of course, we expect o add more classifters at the noun level.
C00-1018@@ The field of Computational Linguistics (CL) has 1ooth moved towards applications and towards large data sets. as a quick ru\]e-ofthunll) 1)rote dure detecting serious errors only. items are requhe(l (;o achieve full (lisjuncl;ion coverage on Lhe (-Oml)h!t(; rule.
C00-1020@@ This paper describes the implementation f an online lexical semantic disambiguation system for English within a client/server linguistic application. In Proceedings of the Workshop on atttomatic Information E:mztctiotz arid the Building of Lexical Semantic Resourees,, ACL, p71-77, Madrid, Spain. Lets disambiguate seize in the following example sentence: United Slates federal agents seized a smfaceto-air rocket launche~; a rocket motto; range-finders and a variety of milim O, manuals.
C00-1021@@; (olr(!lates of flmctional words (a.nd morph(;m(~s) on this leve, l ha v(~ the form of indices of lexi(al nod(, labels.l 1An except ion  concerns  coord inat ing  con junct ions ,  which,  in PDT,  are. (As a) sportsman he is goodl, see ex. In E. Hajif:ov{~, editor, Lssues of Valency and Meaning, Studies in Honour of Jarnlila Panevov5, pages 106 132.
C00-1022@@  Terminological resources have, been found useful in re;my Nl,l ) nl)t)li(-;ltionn , in(:huting Computer-Aided Translation and Information Retrieval. We tested several measures, including Component frequency, information gain and mutual information. total nmnber of N,(c) terms.
C00-1023@@  Word sense disambiguation (WSD) remains an open probleln in Natural Language Processing (NLP). The 1)rol)ahilities al each node at(, used i;o (lisanfl)iguat(! A perspective ell word sense disambiguation methods mid their evaluation.
C00-1024@@ Today many web sites on the lnternet provide online news services. ~; g  ,~a.rx & a_~-.-24 "~d :V4 o (Because I couldfft get a ticket, (so) 1 didnt see a movie) (3) Topic chains The topic of a clausal segment is usually deleted under the identity with a topic in its preceding segment. 161 -~.~ .e~ 4-/5,-" 5, (The protesters, carrying signs and chanting, opposed the global trade liberalization being worked on at a meeting of trade ministers flom the World Trade Organization.)
C00-1026@@ The occurrences of unknown words cause difficulties in natural language processing. Therefore the process of identifying semantic class of a compound boils down to find and to determine the semantic lass of its head morphen-~e. This thesaurus is a lattice structure of concept taxonomy.
C00-1027@@ Adaptive language models were introduced in the Speech Recognition literature to model repetition. The probability of "said" depends on many factors (e.g, genre, topic, style, author) that make the distributions broader than chance (Poisson). The line shows what woukt be expected under a Poisson.
C00-1029@@ Knowledge of which words are able to fill particular a.rgument slots of a. lredlca.te ca,n be used tbr structural disa.mbiguation. The formula.e for these est imates will I,e give,, shortly. %, whh;h is i, line with our results.
C00-1030@@  Ill the last few ye~trs there has t)een a great investment in molecula.r-l)iology resear(:h. This has yielded many results l;\]la. \c iml)lem(mt a I \ ]MM to est imate this using th(. Error l){mnds for {:onvolutions e{}{les and an asyml)totically optimum
C00-1031@@A COLLF.CT module determines a list of potential antecedents (LPA) for each anaphor (l~ronourl, delinile noun, proper name, etc.) We consider that the effort e(AJ, a, EDRAa.) A statistical approach to anaphora resolution.
C00-1034@@  Apl)lying Machine lx~arning t(.chniques to Natural Language Processing is a booming domain of rus(:~ar(h. One of the reasons is the. This flmction se\]e(ts a fist of elements w\]fich are eateg(}rised as mmIei. A lexicalized tree adjoining grammar fbr english.
C00-1036@@A l)octunent Type l)elinilion (DTD) is roughly similar to a coiitext-free grammar j with exactly one predelined terminal. This process is mediated througll text in the language of the author, showing the types t(5 be relined as specially highlighted textual units. This involves a generalization o1 function types with dependent types.
C00-1043@@The last decade has witnessed great advances and interest in the area of Information Extraction (IE) fiom real-world texts. Tim justification of this answer is t)rovided by the r~ ( following proof I;r~lee. Goto 3. until the Q/A performance improves.
C00-1044@@  In recent years, computalional tcchniqt,es for the determination of &:deal semantic features have been proposed and ewdualed. Any opinions, tindings, or recommendations a,e those of tile authors, and do not necessarily rellect the views of the above agencies. 3 0.0006 0.0070 Key: (s G ,5)+: one or more instances of members ofS.
C00-1045@@  Besides the well established problem of lnonoun resolution, pronoun generation is now attracting renewed attention. Finally, the cent;ering version suggest;ed \])y Stlul)e and Hahn (\]! The Cb is consid(:red as (;h(: \]o(al focus of ai;i;(:ntion.
C00-1047@@ Measuring the representativeness (ie, the informativeness or domain specificity) of a term ~ is essential to various tasks in natural language processing (NLP) and information retrieval (IR). E,isting measures of representative~kess 2. For termT,  ca lcu la te  M(D(T)), the va lue of the measure  for D(T).
C00-1051@@  There have been a number of attempts to use statistical techniques to improve parsing performance. This means that the al)t)roximation error is negligil)le if PR,, is sut\[iciently close to 1),R, which holds for a reasonably small mlmt)er n in lnOSt cases in practical statistical parsing. References Brill, E. and J. Wu.
C00-1052@@However, with a left-re(:ursive grammar such l)ars(as tyl)i(:ally fail to termim~te. BUP: A 1)otl;oIn-ll I) t)arser embedded in Prolog. \]n the~ (:ontext of the other transforms that Moore introduces, it seems to have the, sallle effect in his system as the s(Je(;tive lefl;-corll(W trailsform does lmre.
C00-1053@@ Multimodal interfaces allow content to be conveyed between humans and machines over multiple different channels uch speech, graphics, pen, and hand gesture. In this case, the integration coukt be kept at the level of here and there by introducing a rule which splits line gestures into their component start and end points (Gli,,e ---) Gi,oim Gl,,,i,,t). In Proceedings of 31 t Annual meeting of the Association for
C00-1054@@ Multimodal interfaces are systems that allow input and/or output o be conveyed over multiple different channels uch as speech, graphics, and gesture. In Proceedin,q,s o/ ICSLI, Sydney, Australia. 1 ~ is the set of productions of the form A -+ (~whereA E Nand,~, C (NUT)* .
C00-1056@@ As the amount of international communication increases, more foreign words arc flooding into the Korean language. that is a base model of Brown model-1 \[2\]. Precision ll\]e{isule is tile average number o1 relrieved correct words divided by the number of generated candidates (Eq.
C00-1059@@ A thesaurus plays essential roles in information retrieval systems. Acknowledgements: This research was st, pported in part by the Next-generation Digital Library System R&D Project of MITI (Ministry of International Trade and Industry), IPA (Inlbrmation-technology Promotion Agency), and JIPDEC (Japan Information Processing Development Center). So we extract cooccurrence in a window.
C00-1060@@One evident advantage of this apl)roaeh over lmrely statistical parsing techniques is that grammars can provide precise smnantie representations. Advantage  1 A new distance metric. The word yukkuri s an adverb modifying the verb h, ash&u.
C00-1061@@In Korean technical documents, many English words are used in their original forms. with Test Set I E-K trans. PCa gets a higher accuracy than PCp.
C00-1062@@  and  Pre l iminar ies  This 1}al)er exat\]liltes the generation t)\]{}t)leln for a {:erl;ain linguistically mol;ivate, d subclass {}f LFG grammars. Thus, in the LFG generation case the st)e(:ialized grammar turns out to be in a less l)owerful tbrmal class than the original. are (possibly t lnpty) sequences of attributes).
C00-1063@@  Syntactic analysis, or parsing has been a main objective in Natural Language Processing. Then, a thesaurus i employed to solve this problem. When a case COml)Onent does nol: have a CM, it can 1)e assigned to the 9a, we, or ni CM slot.
C00-1065@@ As a first step prior to parsing, traditional Part of Speech (POS) tagging assigns limited morpho-syntactic nformation to lexical items. In addition, elements of E and F are marked + if they are to the left of the anchor, if they are to the right. Bui ld ing hypertags : a detailed example Let us start with a simple exemple were we want "donner" to be assigned the supertags o~1 (J. dmme tree pomme D M.I J. gives an apple to M.) and o~2 (J donne h M. tree l)omme/J, gives M. an 449 apple).
C00-1066@@ With the rapid growth of the internet, the availability of on-line text information has been considerably increased. Computational Linguistics, Vol 24, No I, pp. If W is totally unrelated to S, the affinity is close to 0.
C00-1067@@An example for this effect is (1): The scope ambiguity that is perceived alter processing the first sentence is no longer present after the second one. The new rules (NonI1) ~md (NonI2) Mlow to derive dominan(:e infbrmation from nonintervention constraints. A tree domain A is a nonempty, t)refixed-closed subset of N* which is closed under the left-sil)ling relation.
C00-1068@@  In a st)oken dialogu(: system, it fr(:(tuently o(:cm:s that the system incorrectly rccogniz(:s user utterances and the user makes exl)ressions the system has not (~xt)ccted. But it ca.n 1)e adopted only in a simi)le task. Then, we compute a posteriori probability tbr a word.
C00-1069@@  This paper describes the Agile system I tbr the multilingual generation of instructional texts as found in soft;ware user-manuals in Bulgarian, Czech and Russian. Press ll,cturn t;o end the polyline. This architectm:e mirrors the reference architecture for generation diseusse(t in I/,eiter 8z Dale (1.
C00-1072@@  This t)aper describes the automated (:reation of what we call topic signatures, constructs that can I)lay a central role. Q5 What  measures  have  been  taken/p lanned/ recommended (e tc . ) Over  tile pas t  week .
C00-1073@@  Recent advances in spoken language understanding have made it 1)ossible to develop dialogue systems tbr many applications. An apparent limitation of these results is that EIC may not 1)e the best baseline strategy tbr coral)arisen to our learned strategy. "Times" tracks the number of times that N,lFun has aske(1 the user ~d)out he attribute.
C00-1077@@The increasing development of electronic lexieal resources, coupled with new methods for automatically creating and fine-tuning them from corpora, has begun to pave the way for a more dominant appearance of natural language processing techniques in the field of terminology. A context word whicll is also a term (whicll we call a context erm) is likely to 1)e a better indicator than one wlfich is not. Autoumtic natural a(:quisition of a terminology.
C00-1079@@ As a transfer problem in a machine translation (MT), lexical and structural differences exist between source and target languages, which requires l-n, m-n, or n-1 mapping strategies for machine translation system. In this reason, we need to consider just types with n-I and n-m mapping relations. This provides a good characteristic for simply representing MWTUs.
C00-1080@@But none of these approaches uses constraint programming techniques lo implement standard chart parsing algorithnls directly in a constraint system. A ,  ( side conditions on A \] . A deduction scheme R has t11o general form AI .
C00-1081@@  The stochastic language modeling, imported fl:om the speech recognition area, is one of the snccessflfl methodologies of natural language processing. As for predictive power, however, the completely lexica.lized model has the lowest cross e~/tropy. (b) Ir a.n illiproven\]ont in observed, the word is 10xica.lized definitively.
C00-1085@@Instead of using a model class that assumed independence, Abney suggested using Random Fields Models (RFMs) tbr attribute-value grmnmars. Compute the expectation of each feature w.r.t R. 4. (l as an CXl}Oncntial distribution) couh\] 1)e used instead.
C00-2086@@  Sentences with eoor(lination (:ontaill multil)le phrases of like syntacti(; tyl)e. When the given sentence shows structmal ambiglfit;y, tiler(; may 1)e multii)le pairs of candidates tbr l)ossil)le conjuncts/disjuncts, usually a single pair of which is identified as intended by human language understanders, larsing for coordination should thus find the exact syntactic t)oundaries of these "intended" conjuncts/(lisjuncts. The  .A lgor i thm Among the pairs of head nouns of candidal;e conjuncts/d is juncts,  al)out 88. Otherwise write them to n cell.
C00-2089@@  Part of Speech Tagging and Shallow Parsing are two well-known problems in Natural Language Processing. NPtool gave a precision ral, e of 95-98% and a recall ratK of 98. % and a rKcall rate of 92.%.
C00-2090@@ \[n Translation Memory (TM) or Example-Based lVlachine Translation (EBMT) systems, one of lhe decisive tasks is to retrieve from the database, the example that best approaches the input sentence. in a combined and uniform way. We then also eliminate these (m-l) last diagonals.
C00-2091@@ From the type logical grmnmars point of view, the parametric part of the language analysis is the lexicon, and the constant one is the logical rules. Geometry of interaction I: Interpretation of system F. In C. Bonotto, R. Ferro, S. Valentini, and A. Zanardo, editors, Logic Colloquium 88, pages 221-260. A note on turbo cut elimination.
C00-2092@@It has been shown that DOP has the ability to locate syntactic and semantic dependencies, both of which are quite important for machine translation. A statistical approach to machine translation. In the rest of this article, we will refer to this 3-tuple as tile pair (T,, g) .
C00-2093@@There has been less attention to a second problem in text plmming, that of realizing the RS by a te:rt struct.uTe (TS), in which the material in the RS is distributed among I)aragraphs, entences, vertical ists, etc., perhaps linked up by discourse connectives such as since and however. All terminal  nodes must have tilt: minimal  TIqXT-LEVEI, o f  0. We hot)e to gradually extend this frmnework to cover many phenonmna that are currently excluded:  Since its inlmt takes the form of a rhetorical structure tree, the text; strueturer inherits ally limitations of RST as a description of rhetorical organization.
C00-2094@@  Disambiguation of lexical ambiguities in naturally oceuring free text is considered a hard task for computational linguistics. Valence induction with a head-lexicalized PCFG. 92); tl~e objects of mobi l ize belong with prol)ability 0.
C00-2098@@Tile main domains are "appointment scheduling" and "travel planning". Brill, E. A Coqms-Based Applvach To Lmtguage Learning. 1. find the set (\] of all common subtrees of r\[) and 0.
C00-2099@@The first step when assigning a dependency description to an input string is to segment he input string into nuclei. A theory of stochastic grammars. lio(le, and the la.tter will a.lways 1)e a. dependent elthe fornier, directly, or indirectly tlirough the lists of n:iovcd dependents.
C00-2100@@ Tl-te subcategorization f verbs is an essential issue in parsing, because it helps disambiguate the attachment of arguments and recover the correct predicate-argument relations by a parser. Can subcategorisation probabilities help a statistical parser. In Proceedings of COLING-ACI, 98, Universitd e Montrdal, Montreal, pages 483-490.
C00-2102@@  It is widely a.greed that named entity recognition is an imt)ort;ant ste t) ti)r various al)pli(:ations of natural language 1)ro(:(.ssing such as intbnnation retrieval, maclfine translation, intbrmation extraction and natural language understanding. Minimally supervised ,\]almnese named e, ntity recognition: I{esource, s and evahmtion. % part-ofspeech accuracy against newspaper a ticles.
C00-2108@@(;(%orisation frames ;rod their seleetional i)refer(;nces tbr the, arguments within the frames. D:)r exalnple, mar~,n, er of Motion verbs 1)referably appeared with a subject only, sometimes with a following adverl). Recall was define(l by the I)ercentage of verbs (verb senses) within the correct clusters compared to the total munber of verbs (verb senses) to be clustered: I,,e,bs ......... , ,.,,.,, .....
C00-2109@@  Dependency analysis is regarded as one of the standard methods of Japanese syntactic analysis. \]2~,atllleS which arc thought to l)e uscflfl in del)ealden(:y analysis, and it: learns the weights of the R~atures flom training data. in the ME model, we define a set el!
C00-2110@@  Syntactic analysis or parsing based on traditional methods, like Chmt parsing or the GLR parsing algorithm, takes cubic or greater tin:e in the sentence length to analyze natural language sentences. l)llnsetsll 1)eing analyzed and the output of a.n edge. The training on the smaller training data (about 8000 sentences) t;akes M)out 10 seconds on Ultra SPAR.C-I.
C00-2111@@ The UNL project of network-oriented multilinguat communication has proposed a standard for encoding the meaning of natural language utterances as semantic hypergraphs intended to be used as pivots in multilingual information and communication systems. v" Ge I~ati0n her / \,4v French utterance Fig. Each decoration is a set of wlriable-value pairs.
C00-2116@@  in the Thai language, there is no explicit word boundary; this causes a lot of problems in Thai language processing including word segmentation, information retrieval, machine translation, and so on. Programs for Machine Learning.Morgan Publishers San Mated, California, 302 p. Shannon, C.E. of words extracted by the decision t ree  No.
C00-2117@@ The development of text databases via the Internet has given impetus to research in computational linguistics towards the automatic handling of this information. On a Distribution Law for Word Frequencies. Finally, all the documents containing one of the following strings in their headline: " In ternat iona l  : ", "Market ing  & Med ia : " ,  "Po l i t i cs  & Po l i cy : " ,  or "Wor ld  Markets  : " without inchlding a line starting with the string "@ By " were considered as reportage.
C00-2118@@  \])eta.ileal hfforma.tion a.I)out verbs is critical to a. broad ra.nge of NI,I ) and l i t  1;asks, yet; ils mau us. \e report here on experiments using a single holdout training and testing methodology. (lel;(rmina.tion for la.rge numl)ers o\[ verl)s is difficult aml resource intensive.
C00-2119@@ Word-breaking is an unavoidable and crucial first step toward sentence analysis in Japanese. Jensen, Karen, George E. Hektorn and Stephen D. Richardson (eds.). Processing speed on a 15,000-sentence corpus 5.
C00-2122@@For the language 1)airs EnglishGerman an(1 Gerinan-English, tbur different translation methods are applied in parallel, thus increasing the systems robustness and versatility. least nearly maximizes, the accord of t;h(; rescaled (:onfidence wflues with the judgment 1)rovided by human aunot;ators. An additional step needs to 1)e taken in order to make the confidence wflues comparable with one another.
C00-2123@@ The goal of machine translation is tile translation of a text given in some source language into a targel: language. more context o pre(lict a source word. This munber must be less than or equal to n 1.
C00-2126@@ Although it is said tha~ word order is free in Japanese, linguistic research shows that there art certain word order tendencies  adverbs of time, for example, tend to t)recede subjects, mM bunsetsus in a sentence that are modified by a long modifier tend to precede other bunsetsus in the sentence. moditiee, the probability is estimated as follows: I or ~t bunsetsus 231, 232, . 0.024 prol)ability assigned by the M.E.
C00-2127@@ Sulnmaries are used to select relevant information from information retrieval results. Re-scoring ojrelationu I f  the condition is not fll lfi l led, thes;e steps from selection of the core relation Must I. Then an important relation is selected as a "core" relation.
C00-2128@@The czplicit tc.~m is the name of one thing and the implicit t;c~"m is the name of something to which it; is related. Marl;i A. Hearst. (3) O We rank nouns 1)y at)plying the measure 214.
C00-2131@@ So far, a number of methodologies and systelns for machine trauslation using large corpora exist. 908 is apl)lied I;o 1)oth source a.nd (;arget trees. This ac(:macy level is not I)romising and it is not; useful for later 1)ro(:e, sses since it needs human (:he(:king ml(l (:orrecion.
C00-2133@@  Previous work on anaphora resolution has yieMed a rich basis of theories and heuristics for finding antecedenls. KTII Speech 7)ansmission Laborato O, Quarterly Progress and Status Report, 2: 1-83. In qhhny Given, editor, &,max and Semantics 12: Discomwe arid ,S),ntax, New York.
C00-2135@@In coN;rasl,, fewer resull;s are reported in phraselevel correspondence. while tile English counterpart follows S-V-O s~rucfiure. Consequently, nearer segments are likely to 1)e dependency-related.
C00-2139@@  Unsupervised learning of syntactic structure is one of the hardest 1)rol)lems in NLP. Using more Sl)e(:itic statistics generate better results. We will give a short overview here.
C00-2140@@Ilowever, very little a.ttention has been given so far to the summariza.tion of spol, r(~n language, even less of conversalions vs. monological texts. serious issues is the lack el senten(:e or clause boundaries in spoken dialogues whi(:h ix particularly problemati(: . A word-based annotation and evaluation scheme for summariza
C00-2148@@ The difficulty o1 achieving adequate hand-crafted semantic representations has limited the lield of natural language t)rocessing to applications that can be contained within well-deIined sub-domains. The event denoted by the transitive variant is a composition of two subevents: E1 refers to the event of av.ql running, and E2 refers to the event of an entity (argO) causing event E l . Each lexical entry corresponds to a tree.
C00-2152@@ Machine translation by analogy lo pairs of corresponding expressions in the source and target languages, or "example-based transhtlion", was firs! Maxwell ll, and A. Zacncn, cds. In our experiments with Japanese and English parsing, we found that l)el-rule garbage collection reduced the overall read/write memory requirements by as much as a factor of four to six.
C00-2155@@Bas ic  Idea  The intuitive idea underlying our approach is to generalize in a first step the set of all lexicon entries. Approximating an HPSG through a CFG ~ is interesting for the following practical reason: assuming that we have a CFG that comes close to an HPSG, we can use the CFG as a cheap filter (running time complexity is O(IGI 2 x n 3) for an arbitrary sentence of length n). Zero pronoun processing: Some requirements tbr a Verbmobil system.
C00-2157@@For example, one might want to retrieve verbs with their sentential complements, or specific fronting or extraposition phenomena. Th, e (new) LKB system. In computational linguistics, t~ature structures are used for that purpose.
C00-2159@@ A parallel corpus is an important resource for corpus-based approaches to CLIR. Information Mapping begins with a large word-by-word matrix. All characters in the English texts are l-byte characters and all characters, including alphabetical and numerical characters, in the Japanese texts are 2-byte, so there is no word which is shared by both English and Japanese texts.
C00-2160@@  The increasing number of on-line texts available has resulted in automatic text summarization becomiI,g a major research topic in the NLP community. One of them could be a lack of cohesion. Most of them have e-valuated well-formed texts produced by people, and used two measures: percentage of familiar words in the texts (word level) and the average length of the sentences in the texts (syntactic level).
C00-2161@@  Traditionally, database management systems were designed to store snapshot information, valid at a particular moment of time (state). 000\] (which in turn is n lappe(t to an element of 7)). \e I)egin our presentation of the.
C00-2162@@  In this pal)er, we address the question of how morl)hological and syntactic analysis can help statistical machine translation (SMT). is often trnnslnted 1)y "Th, e would suit me very well." The word "I~iichtetee" tbr example can not be translated although its coml)onents "Friichte" and "Tee" appear in the training set of EUTRANS.
C00-2163@@The different alignment models we present provide different decoInt)ositions of Pr(f~,a~le(). This is due to the restriction in all alignment models that a source language word can 1)e aligned to at most one target language word. and the remaining entries a vexy low weight.
C00-2167@@  The need for II1 and IE technologies i getting larger because of the improvements ill computer technology and the appearance of the Internet. In particulaI, the definition of the newly introduced NE tyl)e "artifact" was Colitroversial. This will 1)ecome complicated when we want to tag a sul)string of what ix generally considered a ,Japanes(~ wor(t, l/or (~xaml)le , il.t .Jal)allese there is a word "Ratnich" which means "Visil; 3apa.n" and consists of two Chinese eh.aracters, "Ra" Although mmly word segreenters identif~y it as a single, word, we expect to extrtmt only "Nichi" as a local;ion.
C00-2172@@For flflly aul;onlat;i(" l;ranslal;ion the main t ) ro t )  \ ]em is t )oor  COVela~e oi1 l leW data. Proceedings of th, e MT Summit IV, 1. In this way a translation graph is constructed.
C02-1002@@ The scientific and technological advancement in many domains is a constant source of new term coinage and therefore keeping up with multilingual lexicography in such areas is very difficult unless computational means are used. % (BASE with 10 iterations had a Rec* of 70. The threshold for the COGN(TS, TT) test was empirically set to 0.
C02-1011@@We address here the problem of Base NPtranslation, in which for a given Base NounPhrase in a source language (eg, informationage in English), we are to find out its possibletranslation(s) in a target language (eg,  in Chinese).We define a Base NP as a simple andnon-recursive noun phrase. Let c~ represent a randomvariable on C~ . Here, coverage is defined asthe percentage of phrases which have translationsselected, while top n accuracy is defined as thepercentage of phrases whose selected top ntranslations include correct translations.For EM-NBC-Ensemble, we set the  !in (4) tobe 5 on the basis of our preliminary experimentalresults.
C02-1013@@1Head-dependent relationships (possibly labelledwith a relation type) have been advocated as a use-ful level of representation for grammatical struc-ture in a number of different large-scale language-processing tasks. Theweighted precision and recall are then`tbucfevg ie*jhkwl465JxVO`;_P;respectively, expressed as percentages. In-formation is used about which grammar rules in-troduce subjects, complements, and modifiers, andwhich daughter(s) is/are the head(s), and which thedependents.
C02-1016@@Genetically related languages often exhibit recur-rent sound correspondences (henceforth referred tosimply as correspondences) in words with similarmeaning. On average, methods B, C, and D outper-form both comparison programs. However, because man-ual determination of correspondences is an ex-tremely time-consuming process, it has yet to be ac-complished for many proposed language families.A system able to perform this task automaticallyEnglish Latint E n d e k e tent u The correspondingphonemes shown in boldface originate from a sin-gle proto-phoneme.from unprocessed bilingual wordlists could be ofgreat assistance to historical linguists.
C02-1024@@The algorithm is extended to incor-porate a full set of logical operators into this calcu-lation so as to improve the accuracy of the resultingdenotations { and thereby improve the accuracy ofparsing { without increasing the complexity of theoverall algorithm beyond polynomial time (both interms of the length of the input and the number ofentities in the environment model). These cat-egories are drawn from a minimal set of symbols Csuch that:NP 2 C and S 2 C;if ;  : : : 2 Pg.and the following deductive parser,1which will beextended later to handle a richer set of semantic op-erations. Under this interpretation, aquantier like `no would denote a set of pairsfhA1; B1i; hA2; B2i; : : : g where each Aiand Biaredisjoint subsets of E , corresponding to an accept-able pair of restrictor and body sets satisfying thequantier `no.
C02-1025@@Considerable amount of work has been done in re-cent years on the named entity recognition task,partly due to the Message Understanding Confer-ences (MUC). Each name class  is subdividedinto 4 sub-classes, ie, N begin, N continue, N end,and N unique. An algorithm that learnswhats in a name.
C02-1026@@Research systems participating in TRECs and the coming QAC focused on the problem of answering closed-class questions that have short fact-based answers (factoids) from a large collection of text. from two online search engines indicate that e-coli is an abbreviation for the bacteria Escherichia. Results of 90 runs shown in (MRR, PCT5) score pair where A: Altavista, G: Google, M: MSNSearch, X: all three search engines, W: context window size, R: number of top ranked web paged used, T: web gloss weight cut-off threshold.
C02-1027@@The state of the art of todays full parsing andknowledge-based automatic analysis still fallsshort of providing a reliable processing frame-work for robust, real-world applications suchas automatic abstracting or information ex-traction. Morphological analysisBulgarian morphology is complex, for exam-ple the paradigm of a verb has over 50forms. For example one of the rules forsynthesis of NP phrases has the form:NP (def :Y full art :F ext :+ rex : )AP (gender :X def :Y full art:F number: L )NP (ext: )The features and values in the rules are notfixed sets and can be changed dynamically.
C02-1033@@Topic analysis, which aims at identifying the top-ics of a text, delimiting their extend and findingthe relations between the resulting segments, hasrecently raised an important interest. A specific threshold is used forthe decision. A segment is thefirst n sentences of a randomly selected documentfor the Brown corpus.
C02-1036@@ Sentence realization, the last stage in natural language generation, derives a surface string from a more abstract representation. Amalgam: A machine-learned generation module. In "A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text", R. Dale, H. Moisl, and H. Somers (ed.
C02-1040@@The classification of verbs based on their underlyingthematic structure involves distinguishing verbs thattake the same number and category of argumentsbut assign different thematic roles to these argu-ments. %.Our best performing classifier achieves a 33. Chicago University Press, Chicago, IL.Christopher D. Manning.
C02-1042@@Early QA systems used a simple filtering technique, question word density within a fixed n-word window, to pinpoint answers. An Algorithm that Learns Whats in a Name. Semantic types     locator types N/A N/A  0.0 0.  quantity types 22.
C02-1049@@ One of the most prominent problems in computer processing of Chinese language is identification of the word sequences of input sentences. 0% and a precision rate of 61. category   {T} {category} category {Dfa} Vh category {category} Na {Vcl} char category  {} VH category char  Na {} category category char Na Dfa {} Rule types and Examples  Rules of the 10 different types of patterns above were generated automatically by extracting each instance of monosyllabic words in the training corpus.
C02-1050@@The statistical approach to machine translation re-gards the machine translation problem as the maxi-mum likelihood solution of a translation target textgiven a translation source text. TheNULL symbol at index 0 is also a lexical entry inwhich no morpheme is aligned from the channeltarget morpheme, such as masu n(|e) : Representing theprobability of a source word e generating words. An efficient a* search algorithm for statis-tical machine translation.
C02-1051@@ Natural Language Processing (NLP) is an emerging technology with a variety of real-world applications. A Japanese Lexicon, 5 volumes, Iwanami Shoten, Tokyo. Argu-ments are classified into three types: Topic Phrase (TP), headed by a topic marker wa, Focus Phrase (FP), headed by focus particles mo, koso, dake, sae, shika, etc., and Kase Phrase (KP), headed by case particles ga, wo, ni, e, to, yori, de, kara, and made.
C02-1052@@Knowl-edge of countability is particularly important inmachine translation, because the closest trans- This research was done while the second author wasvisiting the NTT Communication Science Laboratorieslation equivalent may have different countabil-ity from the source noun. These are not, however di-rectly linked to a full ontology. Toward an MT system with-out pre-editing  effects of new methods in ALT-J/E.
C02-1053@@Extracting important sentences means extract-ing from a document only those sentences thathave important information. A JapaneseLexicon (in Japanese). tf(t, D(Si))).Here, T is the number of sentence in a docu-ment, and Vzis the number of words in sentenceSz D(Si) (repetitions are ignored).
C02-1056@@In spoken language translation one of the keyissues is how to deal with unrestricted expres-sions in spontaneous utterances. The sentences with the form S(subject)V(verb) O(object) C(complement) emphasizes theobject by moving it before the verb. [3 4] A generalized pattern.
C02-1058@@Word sense disambiguation (WSD) is an intermedi-ate task that is necessary for accomplishing mostnatural language processing tasks, especially machinetranslation and information retrieval. 1 0 24 0 25Total 25 0 66 9 100[Note]S1: a system or instrument for calculatingamount, size, weight, etc.S2: an action taken to gain a certain endS3: a law suggested in Parliament(b) Polysemous word promotion 2 1 0 0 3Total 79 17 0 4 100[Note]S1: an activity intended tohelp sell a productS2: advancement in rank orpositionS3: action to help somethingdevelop or succeed(c) Polysemous word race 0 0 0 0 0Total 29 50 0 21 100[Note]S1: any competition, or a contest of speedS2: one of the groups that humans can be divid-ed into according to physical features, his-tory, language, etc.S3: a channel for a current of water(d) Polysemous word tank 8 23 5 36Total 65 24 11 100[Note]S1: a large container for storing liquid or gasS2: an enclosed heavily armed, armored vehicle(e) Polysemous word title 3 3 0 6 0 12Total 53 31 0 8 8 100[Note]S1: a word or name given to a person tobe used before his/her name as a signrank, profession, etc.S2: a name given to a book, play, etc.S3: the legal right to own somethingS4: the position of being the winner of ansports competition(f) Polysemous word trial A comparison of our method withWSD using a second-language monolingual corpus isgiven below.First, our method performs alignment during theacquisition phase, and transforms word-word correla-tion data into sense-clue correlation data, which is farmore informative than the original word-word correla-tion data. The numeratorof the second factor is the maximum plausibility ofalignments that suggest the i-th sense of the polyse-mous word.
C02-1062@@: textual and differential semantics In textual analysis, each lexical item from a text is broken down in a list of semantic features. Most systems execute this task after a learning phase. Example: Lets calculate the semantic proximity between seller and florist: d We will then be able to see that the proximity between florist and seller does not produce the same result (antisymmetry).
C02-1064@@Text generation is an important technique usedfor applications like machine translation, sum-marization, and human/computer dialogue. The model represented by P (T ) inEq. Department ofInformatics, Kyoto University.S.
C02-1065@@Many electronic documents in various languagesare distributed via the Internet, CD-ROM, andother computer media. In order to intensifythe value of dependent words, f(t, r) in equa-tion(3) is replaced with the following f  We define a modifiedvector (context word vector 2, cw2), which is aversion of cw1.Similarly, another context vector is also de-fined for semantic attributes to which co-occurring belong by using the following featurevalue a instead of w (context attribute vector,ca). As a result each word has aunique attribute..
C02-1070@@In this re-search, we explore the idea of using an informationextraction system designed for one language to au-tomatically create a comparable information extrac-tion system for a different language.To achieve this goal, we rely on the idea of cross-language projection. Automatically Constructing a dictionary forinformation extraction tasks. In Proceedings of theInternational Conference on Recent Advances in NaturalLanguage Processing.E.
C02-1075@@This paper deals with the problem of how to dis-ambiguate the readings of sentences, analyzed by agiven unification-based grammar (UBG).Apparently, there are many different approachesfor almost as many different unification-basedgrammar formalisms on the market that tackle thisdifficult problem. A context-free approximation of Head-Driven Phrase StructureGrammar. A probabilisticcorpus-driven model for lexical-functional analysis.In Proceedings of COLING/ACL-98.Taylor L. Booth and Richard A. Thompson.
C02-1078@@Anaphora resolution is crucial in natural lan-guage processing (NLP), specifically, discourseanalysis. In addition, t isa parametric constant, which is experimentallyset to 0. In the case wherean antecedent occurs in n sentences previ-ous to the sentence including a zero pro-noun, its value takes n. Constraint related to relative clauses (r),which denotes whether an antecedent is in-cluded in a relative clause or not.
C02-1080@@ Named entity (NE) recognition is a fundamental step to many language processing tasks. A token e may have multiple Pr(e) values, each is dependent on the role token e plays in a possible NE, such as the probability of being a surname, first-name, prefix, suffix, general token or cue-word. We assume that the number of PER names in the corpus is N, and the rest of tokens is M. Because there are N surname and N first-name tokens in the corpus, the total number of tokens is M+2N.
C02-1088@@ Named entity extraction is an important step for various applications in natural language processing. (N:list)      (PP)  (V:announced)  PN : proper noun, N : noun, PP : postposition, V : verb In the above examples, KAIST has different categories although same postposition, e-seo, followed. "Automatic adaptation of Proper Noun Dictionaries through cooperation of machine learning and probabilistic methods", Proceedings of the 23rd ACM SIGIR Conference on R&D in IR (SIGIR).
C02-1089@@ More than 100 Chinese input methods have been created in the past [1-6]. A Touch-Typing Pin-yin Input System. It is an n-gram based approach.
C02-1090@@ Machine-readable thesauri are now an indispen-sable part for a wide range of NLP applications such as information extraction or semantics-sensitive information retrieval. NN, a unique root ROOT  N, one first determines the least common supercon-cept of a pair of concepts a,b being compared. A class was formed by words lexicaliz-ing all child concepts of a given concept.
C02-1094@@Normal spoken language is not delivered in an un-interrupted monotone; prosodic cues such as pausesor boundary tones greatly help the listener to un-derstand an utterance. Precision is the percentageof breaks assigned by the model which is correctaccording to the corpus: precision  		where S is the total number of breaks which ourmodel assigns to the corpus and I is the number ofinsertion errors (breaks that the model assigns eventhough no break occurs in the test corpus). Perfor-mance structures: A psycholinguistic and linguis-tic appraisal.
C02-1096@@N-gram language modeling techniques have beensuccessfully embedded in a number of natural lan-guage processing applications, including word pre-dictors for augmentative and alternative communi-cation (AAC). Spies, A Language Model for CompoundWords, Proc. M words (and with N+N com-pounds split) that we describe below was used tocollect MI values for noun pairs.
C02-1099@@ In Korean, many technical terms in a domain specific text, especially science and engineering are from foreign origin. Except it, all rules are applied to both classes (E-class and G-class). Moreover, our method shows higher performance in C.A.
C02-1101@@Corpora are widely used in natural languageprocessing today. A Maximum En-tropy Model for Part-of-Speech Tagging. We achievedprecision of 80100% and showed that manyannotation errors exist in widely used corpora.The performance seems to be high enough forpractical use in corpus refinement.ReferencesSteven Abney, Robert E. Schapire, and YoramSinger.
C02-1104@@ Some NLP applications make use of shallow parsing techniques (typically the ones treating large data), some others rely on deep analysis (eg machine translation). A constraint is specified as follows. Let R a symbol representing a constraint relation between two (sets of) categories.
C02-1112@@ Supervised learning has become the most successful paradigm for Word Sense Disambiguation (WSD). The decision-threshold e in performance  coverage. Parsing English with a Link Grammar.
C02-1114@@Semantic knowledge for particular domains isincreasingly important in NLP. In COLING-ACL, Montreal, August.Christopher D. Manning and Hinrich Schutze. Return to step 2 and iterate n times5.
C02-1115@@Word sense disambiguation (WSD) is an openresearch eld in natural language processing(NLP). Instead of training with the samekind of information for all words, which under-estimates which information is more relevant toeach word, our research shows that each word ismore eectively learned using a dierent set offeatures. A baseline methodologyfor word sense disambiguation.
C02-1117@@When log(f) is drawn against log(r) in a graph (which is often called a Zipf curve), a straight line is obtained with a slope of 1. To appear in Physica A. Paul, D. B. Other words, compound words, are built up by combining syllables together, similar to word n-grams in English.
C02-1122@@To understand a text, it is necessary to find outrelations between words in the text. Construction of a Japaneserelevance-tagged corpus. The accuracy ofcase change was 2/4.
C02-1125@@ Measuring the representativeness (ie, the informativeness or domain specificity) of a term is essential to various tasks in natural language processing (NLP) and information retrieval (IR). We denote this normalized value by NormM(D(T)). This paper solves this problem by giving a mathematically sound measure.
C02-1126@@The information included inthe node labels augmentations may include lexicalitems, or a node label suffix to indicate the node is anargument and not an adjunct; such extra informationmay be viewed as latent information, in that it is notdirectly present in the treebank parse trees, but maybe recovered by some means. ImplementationThe expectation step (E-step) of the Inside-Outsidealgorithm is performed by a parser that computes allpossible derivations for each parse tree in the train-ing data. But we may also use it for augment-ing trees, by computing the most likely T + for agiven sentence-tree pair (S ,T ).
C02-1130@@  There has been much interest in the recent past concerning automated categorization of named entities in text. of the Fifth Conference on Applied Natural Language Processing, Washington, D.C. Witten, I. An algorithm that learns whats in a name.
C02-1132@@Recent research into verb-argument structure hashas attempted to acquire the syntactic alternationbehavior of verbs directly from large corpora. Recogni-tion performance of a structured language model. In addition toan unobserved cluster variable c, we introduce a sec-ond unobserved variable r for the semantic role ofan argument.
C02-1136@@With the recent advances of the continuousspeech recognition technology, a considerablenumber of studies have been made on spokendialogue systems. j|B) (2)For a sequence of bunsetsus, B, the methodidentifies the dependency structure withargmaxSP (S|B) Dependencies do not cross each other. j, ri, dij, pij, li)C(ri, dij, pij, li)Here, C is a cooccurrence frequency functionand B is a sequence of bunsetsus (b1b2 bn).In the formula (1), the first term of the righthand side expresses the probability of cooccur-rence between the independent words, and thesecond term does that of the distance betweenbunsetsus.
C02-1138@@Systems that interact with the user via naturallanguage are in their infancy. or names of operations that in-crementally build the sentence plan, eg merge.Domain-dependent, task-independent featuresare those whose names include open class wordsFeatures Used Mean Score S.D.all 4. or thenames of the role slots, eg $DEST-CITY. Assessing the output quality of a generatoris a complex issue.
C02-1140@@Unknown words are a major bottleneck for learnersof any language, due to the high overhead involved inlooking them up in a dictionary. Calculate the corpus-based frequency F (s) ofeach dictionary entry s in the corpus and thenthe string probability P (s), according to equa-tion (3). Calculate the probability of a given segment be-ing realized with each reading (P (r|k)), andof phonological (Pphon(r)) or conjugational(Pconj(r)) alternation occurring.
C02-1143@@Word Sense Disambiguation (WSD) is a centralopen problem at the lexical level of Natural Lan-guage Processing (NLP). A maximum entropyapproach to natural language processing. whether there is a subject, direct object, indi-rect object, or clausal complement (a comple-ment whose node label is S in the parse tree)4. the words (if any) in the positions of subject,direct object, indirect object, particle, preposi-tional complement (and its object)5. a Named Entity tag (PERSON, ORGANIZA-TION, LOCATION) for proper nouns appear-ing in (4)6.
C02-1144@@However, they often include many rare senses while missing domain-specific senses. In D) the cluster containing e could have been merged with either set (we arbitrarily chose the second). For instance, U.S. state names can be clustered this way because they tend to appear in the following contexts: (List A) ___ appellate court campaign in ___  ___ capital governor of ___  ___ drivers license illegal in ___  ___ outlaws sth.
C02-1145@@The Penn Chinese Treebank (CTB) is anongoing project, with its objective being tocreate a segmented Chinese corpus annotatedwith POS tags and syntactic brackets. Experimental resultsPortion   Precision  Recall  Time         Accuracy      1            N/A          N/A       28h:01m   99. A Maximum Entropy Part-Of-Speech Tagger.
C02-1151@@In a typical IE applicationof constructing a jobs database from unstructured text,the system has to extract meaningful entities like title andsalary and, ideally, to determine whether the entities areassociated with the same position. A learning approach to shallow parsing. National Con-ference on Artificial Intelligence, pages 806813.E.
C02-1153@@The derivation tree, to theright, contains the history of the tree grafting pro-cess that generated the derived tree, to the left. A principle-basedhierarchical representation of LTAGs. ); the occurrence of the by phrase before sen-tential complements (I was told by Mary that ... );and wh-extraction of sentential complements and ofexhaustive PPs.
C02-1154@@This particular scenario requiresa comprehensive list of disease names. Forexample, names of agents regularly function asthe name of the disease they cause: \E. Candidate Acquisition: Compute ascore for each candidate type t 2 	, based on how many dierent patterns in AcceptPatmatch an instance of type t, how reliable these patterns are.To rank a candidate type t 2 	 consider the setof patterns in AcceptPat which match on someinstance of t; lets call this set Mt.
C02-1157@@Currently, the state-of-the-art speech recognizerscan take dictation with satisfactory accuracy. This means,however, our method is not able to eciently uselexical information about the content words at thisstage. A probabilistic parsing methodfor sentence disambiguation.
C02-1161@@The vocabulary of users of domain-specific retrievalsystems often differs from the vocabulary within aparticular resource, leading to retrieval failure. E+04 0 when be babe ruth pay 1. For each paraphraseof F ( G    # para F ), where CH is the lemmatized query:(a) Extract the content lemmas from   : I 	  I J, where K is the number ofcontent lemmas in paraphrase.
C02-1163@@Humans generally have language capability,mostly for their mother tongue and to a lesserextent for foreign languages. The2http://www-nagao.kuee.kyoto-u.ac.jp/nl-resource/juman-e.htmlprocess consists of two parts, ie, template re-trieval and template matching.The process first searches for templates sat-isfying similarity to the input expression. We havealready constructed a prototype for Japanese-Chinese.
C02-1165@@Information Extraction (IE) is a technologyused for locating and extracting specic piecesof information from texts. In M. T. Pazienza, editor,Information Extraction. Natu-ral Language Processing Pacic Rim Symposium(NLPRS97).D.
C02-1166@@ The growing availability of comparable corpora, through the Internet or via distribution agencies providing newspapers articles in different languages, has led researchers to develop methods to extract bilingual lexicons from such corpora, in order to enrich existing bilingual dictionaries, and help cross the language barrier for cross-language information retrieval. Term alignment in use: Machine-aided human T UVW(XfiY VTZ)[\W^]`_ Wbafi]dc U[\W(Z Xfe g h] ijlkmVUVY Y nYponLqSTProcessing Alignment and Use of Translation Corpora. Capturing the comparable: A system for querying comparable text corpora.
C02-2002@@ The quality of many NLP systems depends heavily on the completeness of the dictionary they use. However, like many other bound morphemes in Chinese, it can occasionally be used as an independent word, as in the following sentence:   he  at  I  home  chat LE  two CL  hour He chatted for two hours at my house. Wu A. and Jiang Z.
C02-2005@@ The detection of abbreviations in a text corpus forms one of the initial steps in tokenization (cf. Each scaling                                                    4 The use of e as a base for scaling factors S1 and S2 reflects that log  can also be expressed as HA being elog /2 more likely than H0 (cf. This can be illustrated with the pair (U.N, ) The application of the scaling factors does not change the value as the initial log  6 Obviously, this assumption is only valid if the abso-lute number of occurrence is not too small.
C02-2006@@Recently there has been increasing interest inapplying natural language processing (NLP) sys-tems, such as keyword extraction, automatic textsummarization, and machine translation, to Inter-net documents. We have also developed a Japanese pars-ing system with LAL output functionality. For instance, given a prepo-sitional phrase Prep N, ESGs head word of theprepositional phrase is Prep, but EtoJ MT engineshead is N. In most cases, we can make systematicconversion routines for dierent structures.
C02-2007@@ As the processing of content information has nowadays become the center of NLP, a bilingual concept MRD is of increasingly great significance for IE, MT, WSD and the like. The actual practices of the lexicographers are as follows:       (i) For each tree node in English, if there exists a corresponding Chinese concept, the lexicographers simply translate the English concept into Chinese. A comparatively high efficiency may be achieved.
C02-2019@@In recent years, systems developed for analyz-ing written-language texts have become consid-erably accurate. Weimplemented this model within an M.E. The output is a sequenceof morphemes with grammatical attributes.
C02-2020@@The presentwork addresses this issue in a specialized domain:medicine. C.I.D.The city-block metric is computed as the sum of the abso-lute differences of corresponding vectors positions.Pascale Fung and Kathleen McKeown. Another simplestemmer was used for French; it handles some -sand -x endings.The context of occurrence of each word is thenapproximated by the bag of words that occur withina window of N words around any occurrence of thatpivot In the experiments reported here, Nwas set to 3 (ie, a seven-word window) to approxi-mate syntactic dependencies.
C02-2021@@These views are, plausibly, familiar totypical users of English. This goes against a naiveassumption in most of the literature that targetinformation should automatically overridetransfers. Serial mixed metaphors createnested pretence cocoons where the metaphoricalview of B as C is nested within a pretencecocoon with the view of A as B.
C02-2022@@The aim of thiswork is to allow the type of generalisation that ispermitted by the use of phonemes with allophonicvariation to be taken one level higher, ie above thelevel of the single language. The next steps in this research willinvolve fully integrating the metaphonemes into amultilingual lexicon to enable testing on a speechsynthesis system.ReferencesBaayen, H., R. Piepenbrock and H. van Rijn. /s/, /S/and /z/ are the only sibilants that occur in all threelanguages.
C02-2024@@Our ISTFS is an indexing substrate that en-ables such knowledge-based systems to keep andretrieve TFSs, which can represent symbolic struc-tures such as quasi-logical forms or a taxonomy andthe output of parsing of unification-based grammarsfor a very large set of documents.The algorithm for our ISTFS is concise and effi-cient. We canalso find the second best index path by finding thepath pi s.t. Cambridge University Press, Cambridge, U.K.A.
C04-1002@@Efficiency in parsing as well as accuracy is one ofvery important issues in natural languages process-ing. Itachieves a dependency accuracy of 87. The former is a stack forkeeping IDs of modifier bunsetsus to be checked.The latter is an array of integers that stores head IDsthat have already been analyzed.Following the presented algorithm, let us parse a// Input: N: the number of bunsetsus in a sentence.// w[]: an array that keeps a sequence of bunsetsus in the sentence.// Output: outdep[]: an integer array that stores an analysis result, ie, dependencies between// the bunsetsus.
C04-1004@@ A Hidden Markov Model (HMM) is a model where a sequence of observations is generated in addition to the Markov state sequence. For convenience, we denote P  as the conditional state probability distribution of the states given E  and i)|( ii Espis iE)|( iEP  kNNEp ki as the conditional state probability of  given . (p)( s|oThere are several problems with this generative approach.
C04-1006@@Word-aligned bilingual corpora are an impor-tant knowledge source for many tasks in nat-ural language processing. Thusfor the linear interpolation, a word pair (f, e)obtains a large combined count, if the count inat least one direction is large. A probability modelto improve word alignment.
C04-1007@@For the interpretation of texts, it is not enough to un-derstand each sentence individually; one also needsto have an idea of how sentences relate to each other,ie one needs to know the discourse structure of thetext. For example, the expres-sions U.S. District Judge Peter Smith, Judge Smith,and Mr. Smith are treated as referring to the same en-tity and can therefore be placed in the same chain.When all proper noun chains have been built, thosethat contain only one element (ie one occurrence ofa term) are removed. In T. Sanders,J.
C04-1009@@Following directly on this work, I adopt thetype hierarchies of HPSG wholesale into TCCG, ex-ploring directly the theoretical advantages this hy-brid approach yields, with direct comparison to pre-vious CCG and HPSG work. (e) Barring morphosyntax, auxiliary andcontrol verbs share a category.While these generalizations are of course deriv-able meta-theoretically (from the categories in (6),there is no explicit mechanism in CCG for statingstatic relationships (there are mechanisms for de-riving categories, which I discuss below). (7) (a) All verbs share morphosyntactic features.
C04-1010@@There has been a steadily increasing interest in syn-tactic parsing based on dependency analysis in re-cent years. For thesetokens, we consider both the word form (T.LEX,N.LEX) and the part-of-speech (T.POS, N.POS), asassigned by an automatic part-of-speech tagger ina preprocessing phase. In Proceedings of LREC, pages 447454,Granada, Spain.E.
C04-1011@@Among the many formalisms used for descrip-tion and analysis of syntactic structure of natu-ral language, the class of context-free grammars(CFGs) is by far the best understood and mostwidely used. t;w) as the frequency (number of occur-rences) of s a7 t in the unique computation ofM , if it exists, that accepts w; this frequency is0 if w is not accepted by M . Then thechoice that minimizes H(Gp ||Mp) determinesthe choice that minimizes D(Gp ||Mp), irre-spective of H(Gp).
C04-1013@@Efficient learningalgorithms, that have some guarantees of cor-rectness, would clearly be useful. We conclude with a critical analysisof our results. , k, the hypothesis Hi willbe good, and that when the algorithm termi-nates the final hypothesis will have low error.We will do this by induction on the index i ofthe hypothesis Hi.
C04-1015@@In response to the ongoing expansion of bilingualcorpora, many machine translation (MT) meth-ods have been proposed that automatically ac-quire their knowledge or models from the cor-pora. NP node represented by a boldframe). Baseline (Example-based Transfer only)The best translation that had the same seman-tic distance was randomly selected from thethe bus TM: -0.07LM: -1.
C04-1018@@However, newswire does not offerdirect access to facts, events, and opinions; rather,journalists report what they have experienced, andreport on the experiences of others. A Comprehensive Grammarof the English Language. Ph.D. thesis,University of Pennsylvania, Philadelphia.Hamish Cunningham, Diana Maynard, Kalina Bont-cheva, and Valentin Tablan.
C04-1021@@ and MotivationOver the last fifteen years or so, much of the NLPcommunity has focused on the use of statisticaland machine learning techniques to solve a widerange of problems in parsing, machine translation,and more. It is possible to add implicit attributes to Etto get a set E t with exactly one compatibleattribute for every value. PRECISEchooses the first ancestor of p such that when n isattached to the new node, the modified parse treeagrees with PRECISEs semantic model.
C04-1022@@In spite of novel algorithmic developments andthe increased availability of large text corpora,statistical language modeling remains a diffi-cult problem, particularly for languages withrich morphology. Rule 1 indicatesa backoff that drops the third factor, Rule 2drops the second factor, etc. The following GA options gave goodresults: population size 30-50, crossover proba-bility 0. , mutation probability 0.01, StochasticUniversal Sampling as the selection operator, 2-point crossover.
C04-1024@@Large context-free grammars extracted from tree-banks achieve high coverage and accuracy, butthey are difficult to parse with because of theirmassive ambiguity. add-analysis(n,r,m) incrementsthe size of the child array by 2 and adds the in-dex of the first new element to the first-childarray. The function new-node(b,e,A) adds the number of A at the endof the catnum array.
C04-1025@@Within the framework of Head-Driven Phrase Struc-ture Grammar (HPSG), the so-called linearization-based approaches have argued that constraints onword order are best captured within domains thatextend beyond the local tree. Fi-nally, (8g) introduces a global LP constraint requir-ing an E to precede an F whenever both elementsoccur in the same domain.Now consider licensing the string efjekgikj withthe above grammar. ExamplesWe start with an example illustrating how a CFGrule is encoded in GIDLP format.
C04-1026@@e. that once we know the syntactic structure of asentence, we can deterministically compute its se-mantics.Unfortunately, this assumption is typically notjustified. M. Kaplan and J. T. Maxwell III. Principles of Constraint Programming.Cambridge University Press.N.
C04-1030@@In statistical machine translation, we are givena source language (French) fJ , which is to be translated intoa target language (English) It allows an independentmodeling of target language model Pr(eI1) andtranslation model Pr(fJ1 |eI1). Here, J is the length ofthe source sentence and E is the vocabularysize of the target language.. . The extension to higher-order n-gramlanguage models is straightforward.
C04-1031@@Word alignment is the task of identifying trans-lational relations between words in parallel cor-pora with the aim of re-using them in natu-ral language processing. Automatic evaluationand uniform filter cascades for inducing n-best translation lexicons. From Jerusalem and back:a personal account.
C04-1032@@However, all these models constrainthe alignments so that a source word can bealigned to at most one target word. First of all, in calculationof these probabilities with the models IBM-1,IBM-2 and HMM the EM-algorithm is per-formed exact, ie the summation over allalignments is efficiently performed in the E-step. A probability modelto improve word alignment.
C04-1035@@The phenomenon of sluicingbare wh-phrasesthat exhibit a sentential meaningconstitutesan empirically important construction whichhas been understudied from both theoreticaland computational perspectives. (11) Patrick: [...] then I realised that it was FenniteKatherine: Who [KCV, 4694]Which/Which N Both sorts of sluices ex-hibit a strong tendency to reprise. TiMBL yields a success rate of89%.
C04-1036@@Features typically corre-spond to other words that co-occur with the charac-terized word in the same context. In particular, a common practice is to filter out features by minimal frequency and weight thresholds. The value of each entry is determined by some weight function weight(w,f), which quantifies the degree of statisti-cal association between the feature and the corre-sponding word.
C04-1040@@Dependency AnalysisWord dependency is important in parsing technol-ogy. Such a tripletmeans that xi. MethodologyInstead of building a word dependency corpus fromscratch, we use the standard data set for comparison.Dependency Analyzer We use the one vs. allothers backward parsing method based on anIOB2 By the chunking,each word is tagged as O: Otherwise.Please see Kudos paper for more details.
C04-1041@@Lexicalised grammar formalisms such as Lexical-ized Tree Adjoining Grammar (LTAG) and Com-binatory Categorial Grammar (CCG) assign one ormore syntactic structures to each word in a sentencewhich are then manipulated by the parser. The estimation methodmaximises the following objective function:L() is the log-likelihood of model ,and G() is a Gaussian prior term used to avoidoverfitting (n is the number of features; i is theweight for feature fi; and  is a parameter of theGaussian). Ph.D. thesis, Univer-sity of Pennsylvania.James R. Curran and Stephen Clark.
C04-1044@@There is a complexity issue if one consider ex-act parsing with large scale lexicalized gram-mars. RulesF is a set of composition rules be-tween syntactic structures. SatA;(ii) S  , Sn are composed into a struc-ture S in C by means of rules of RulesC ,then f(S1), .
C04-1045@@In statistical machine translation, a translationmodel Pr(fJ1 |eI1) describes the correspondencesbetween the words in the source language sen-tence fJ1 and the words in the target languagesentence eI1. The obtained wordalignment is compared to a reference alignmentproduced by human experts. (e, eis)where f is the full form of the word, eggehe; , e)The training procedure for the other modelparameters remains unchanged.
C04-1047@@This paper addresses the challenging problem ofeliminating unsatisfactory outputs from machinetranslation (MT) systems, which are subsystems ofa speech-to-speech machine translation (S2SMT)system. The N-best list is expected to ap-proximate the total summation as closely as possi-ble. Each floating number in the first to third column ofeach MT system indicates the average performance of the proposed RSCM, the average difference of the performanceof the proposed RSCM from that of the existing RSCM, and the t-value of the left-next difference, respectively.
C04-1048@@The amount of research available indiscourse segmentation is considered small; indiscourse parsing it is even smaller.The difficulties in developing a discourseparser are (i) recognizing discourse relationsbetween text spans and (ii) deriving discoursestructures from these relations. A Rule Based Approach to Dis-course Parsing. ][because I told her yesterday..]The discourse segmenter in Step 1 outputs twosub-trees, one with two leaves She knows ; another with twoleaves She knows what time you will come andbecause I told her yesterday.
C04-1051@@While several different learning methods have been applied to this problem, all share a need for large amounts of data in the form of pairs or sets of strings that are likely to exhibit lexical and/or structural paraphrase alternations. R. Barzilay and  L. Lee. Spelling: British/American sources system-atically differ in spellings of common words (colour / color); other variants also appear (email / e-mail).
C04-1053@@More specifically, given the advances of machine learning statistical methods for NLP, with supervised training methods leading the way to major improvements in performance on different tasks, a particularly valuable resource is now represented by large linguistically annotated corpora. I. Dagan and A. Itai. F.J. Och and H. Ney.
C04-1055@@For example, inter-pretation in the TRIPS collaborative dialog assistantrelies on the representation produced by its parserfor word sense disambiguation, constituent depen-dencies, and semantic roles such as agent, theme,goal, etc. A Practical Se-mantic Representation for Natural LanguageParsing. The parser runsuntil it finds a complete analysis or hits this upper5The Collins parse time for the 309 utterances of 6 words orlonger was 30 seconds.label gold recall produced precision crossingADJ 2 0.0% 0 0.0% 0.0%ADJP 17 17.
C04-1057@@Many natural language processing tasks involve thecollection and assembling of pieces of informa-tion from multiple sources, such as different doc-uments or different parts of a document. Sentenceextraction as a classification task. First,we assume that there is a finite set T of textual unitst1, t2, .
C04-1059@@ Language models (LM) are applied in many natural language processing applications, such as speech recognition and machine translation, to encapsulate syntactic, semantic and pragmatic information. Each node corresponds to a source word. In either case, we get a set of target n-grams for each source word.
C04-1060@@In re-cent years, a number of syntactically motivated ap-proaches to statistical machine translation have beenproposed. The final step,a French translation of each original English word,at the leaves of the tree, is chosen according to adistribution Pt(f |e). The grammar is restricted to bi-nary rules, which can have the symbols in the righthand side appear in the same order in both lan-guages, represented with square brackets:X  [Y Z]or the symbols may appear in reverse order in thetwo languages, indicated by angle brackets:X  Y ZIndividual lexical translations between Englishwords e and French words f take place at the leavesof the tree, generated by grammar rules with a singleright hand side symbol in each language:X  e/fGiven a bilingual sentence pair, a synchronousparse can be built using a two-dimensional exten-sion of chart parsing, where chart items are indexedby their nonterminal Y and beginning and endingpositions l, m in the source language string, and be-ginning and ending positions i, j in the target lan-guage string.
C04-1061@@share the assumption thatthe lexical connectives or discourse markers arethe primary source of information for construct-ing a rhetorical tree automatically. Experiments in Constructing a Corpusof Discourse Trees. We seethe result of step 1 as a corpus resource in itsown right (it can be used for training statisticalclassifiers, for instance) and at the same timeas the input for step 2, which fills the gaps :now annotators have to decide how the set ofsmall trees produced in step 1 is best arrangedin one complete tree, which involves assigning2http://www.cis.upenn.edu/pdtb/relations to instances without any lexical sig-nals and also making more complicated scopejudgements across larger spans of text  themore subjective and also more time-consumingstep.Our approach is as follows.
C04-1064@@Many researchers who study automatic summariza-tion want to create systems that generate abstractsof documents rather than extracts. The algo-rithm consists of the following steps:Step 0 Transform all source sentences into DTPs.Step 1 For each sentence  in the abstract, applyStep 2 and Step 3.Step 2 Transform  Vs 3 e f l 8 9 6 .The above procedure allows us to derive many-to-many correspondences.. Similarity MetricsWe need a similarity metric to rank DTP similar-ity. A Japanese Lex-icon (in Japanese).
C04-1066@@Japanese and Chinese sentences are written withoutspaces between words. A Statistical Part-of-Speech Tagger In Proc. 5.method also uses the likelihood based on n-grammodel.
C04-1067@@Word segmentation in Chinese and Japanese isan important and difficult task. Given aword sequence W , its most likely POS sequence Tcan be found as follows:T can be found as follows where W ranges over thepossible segments of S (w1  However, the Markov model-based methodhas a difficulty in handling unknown words. A Maximum Entropy Approachto Natural Language Processing.
C04-1070@@Text categorization is the task of assigning atext1 to one or more of a set of predefined cat-egories. Thus, if we collect the random indexvectors into a random matrix R of order c Advantages of Random IndexingOne advantage of using Random Indexing isthat it is an incremental method, which meansthat we do not have to sample all the databefore we can start using the context vectors Random Indexing can provide intermediaryresults even after just a few vector additions.Other vector space models need to analyze theentire data before the context vectors are oper-ational.Another advantage is that Random Indexingavoids the huge matrix step, since the di-mensionality k of the vectors is much smallerthan, and not directly dependent on, the num-ber of contexts c in the data. The BoW representation ignores allsemantic or conceptual information; it simplylooks at the surface word forms.There have been attempts at deriving moresophisticated representations for text catego-rization, including the use of n-grams or phrases1In the remainder of this paper, we use the termstext The tfidf measure is a standard weighting scheme,where tf i is simply the frequency of word i in the doc-ument, and idf is the inverse document frequency, givenby Nni where N is the total number of documents in thedata, and ni is the number of documents in which wordi occurs.
C04-1071@@feelings as ex-pressed in positive or negative comments, questions,and requests, by analyzing large numbers of docu-ments. The pattern (e) matches with phrasessuch as X-wa yoi-to omowa-nai. and produces a sentiment unit with thenegation feature.
C04-1072@@The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric. This is a feature that many machine translation researchers look for. We compute the automatic scores for the n-best translations and their reference translations.
C04-1077@@It has been said that we have too much informa-tion on our hands, forcing us to read through a greatnumber of documents and extract relevant informa-tion from them. This is based on general ideas of a summarization systemand is not intended to impose any conditions on a summariza-tion system.Mainichi articlesYomiuri articlesabstract(a)(b)(c)(d)Doc. Multiple document summarization isnow a central issue for text summarization research.
C04-1078@@In a typical IE system, generalized pattern rules are usually represented as regular expressions and matched against test instances through exact matching for each slot, which we call hard matching. (e) If the termination condition is satisfied, the process ends with a set of learned soft and hard pattern rules. A test instance is assigned tagi if it has the highest conditional probability of having t given the soft pattern rule i (represented by vector Pai) which is greater than a pre-defined threshold  among all the soft pattern rules.
C04-1080@@ The empiricist revolution in computational linguistics has dramatically shifted the accepted boundary between what kinds of knowledge are best supplied by humans and what kinds are best learned from data, with much of the human-supplied knowledge now being in the form of annotations of data. 4 Unsupervised Tagging: A Comparison 4. We then used these n-grams and their counts to bias the initial estimates of state transitions in the HMM taggers.
C04-1088@@From a broader per-spective, issues of style, genre, and authorship are an interesting sub-area of text categorization. There are a total of 9377 such structures. In R. Dale, H. Moisl and H. Somers, eds., Handbook of Natural Language Processing.
C04-1089@@ New words such as person names, organization names, technical terms, etc. The context of each candidate translation e  is viewed as a document. Learning a translation lexicon from monolingual corpora.
C04-1090@@Phrase-based SMT approaches can be classified into two categories. 4(e) is a translation of Fig. The output sentence can then simply be read off T*.
C04-1091@@In the decoding problem we are given thelanguage and translation models and a sourcelanguage sentence and are asked to find themost probable translation for the sentence. Time is shown in seconds on a logscale. So, Al-ternatingSearch finds a better translation e forf by solving FIXED ALIGNMENT DECODING.For this purpose it employs NaiveDecode.
C04-1093@@Term descriptions, which have been carefully orga-nized in hand-crafted encyclopedias, are valuablelinguistic knowledge for human usage and compu-tational linguistics research. These sentences can be classified into a spe-cific group on the ground of the underlined expres-sions, excepting sentence (e). (a) XML is an extensible markup language.
C04-1099@@Cross Language Information Retrieval (CLIR)is increasingly relevant as network-based re-sources become commonplace. BL,RSVV S(t) otherwise.The value of the k parameter is set empir-ically. GLIMPSE: A toolto search through entire file systems.
C04-1104@@However, relevant theoretical researches on Chinese verbs are generally limited to case gram-mar, valency, some semantic computation theo-ries, and a few papers on manual acquisition or prescriptive designment of syntactic patterns. 2 A sample of the tree bank or relevant introduction could be found at http://mtlab.hit.edu.cn. c. Error-driven correction: Some key errors occurring in the former two parts are corrected according to manually ob-tained error-driven rules, which are gen-erally about words or POS in the corpus.
C04-1107@@The most popular methods of sentence reduc-tion for text summarization are corpus basedmethods. Suppose that we are givenl training examples (xi, yi), (1  l), wherexi is a feature vector in n dimensional featurespace, yi is the class label {-1, +1 } of xi. Assume that e belongs tothe group mi.
C04-1108@@The growth of computerized documents enablesus to find relevant information easily owing totechnological advances in Information Retrieval.Although it is convenient that we can obtain agreat number of documents with a search en-gine, this situation also presents the informationpollution problem: Who is willing to take thetedious burden of reading all those text docu-ments Afterwe select significant sentences as a material fora summary, we must find a proper arrangementof the sentences and edit each sentence by delet-ing unnecessary parts or inserting necessary ex-pressions. Choose a.Choose the rest, sentence f.The refined ordering isa-b-e-c-d-f.No precedent sentences before sentence b. We assume a newspaper article to bewritten about one topic.
C04-1110@@In comparison,speech summarization is a rather new research areawhich emerged only a few years ago. Note that the absolute utterance scores aretaken from the real data, ie they have been nor-malized w.r.t. Concluding RemarksWe introduced a new approach to spoken dialoguesummarization.
C04-1111@@ The Natural Language Processing (NLP) com-munity has recently seen a growth in corpus-based methods. Riloff, E. and Shepherd, J. 2) Molybdenum is a metal.
C04-1112@@: WSD for DutchA major problem in natural language processing(NLP) for which no satisfactory solution has beenfound to date is word sense disambiguation (WSD).WSD refers to the resolution of lexical semanticambiguity and its goal is to attribute the correctsense(s) to words in a certain context. % when compared to a model which was builtwithout smoothing. During test-ing, the sum of the weights i of all features i foundin the test instances is computed for each class c andthe class with the highest score is chosen.A big advantage of maximum entropy modelingis that the features include any information whichmight be useful for disambiguation.
C04-1119@@ In transliteration, a word in one language is con-verted into a character string of another language expressing how it is pronounced. (ki ru shu shu ta i n)." Method E gave the best ratio.
C04-1121@@ Software companies typically receive high volumes of electronic customer feedback every day, some of it in the form of elicited surveys, some of it in the form of unsolicited comments, suggestions, criticism. In R. Dale, H. Moisl and H. Somers, eds., Handbook of Natural Language Processing. Feature vectors We experimented with a range of different feature sets.
C04-1122@@Recently, Named Entity (NE) recognition has beengetting more attention as a basic building block forpractical natural language applications. A self-learning universal concept spotter. However there is a hugepeak near the score zero.0.
C04-1127@@Research in information extraction (IE) and its re-lated fields has led to a wide range of applicationsin many domains. This resultsin a diverse distribution of extraction patterns inthe target language. The matched NE instance is then extracted.The pattern candidates may be simple predicate-argument structures (eg (resign from   C-POST  )in business domain) or even a complicated subtreeof a sentence which commonly appears in the rel-evant documents (eg (   C-ORG  report personnelaffair (that   C-PERSON  resigns)) ).
C04-1128@@In this paper, we discuss work on the detection ofquestion and answer pairs in email threads, ie, co-herent exchanges of email messages among severalparticipants. Exploiting e-mail structureto improve summarization. For these type of email exchanges,a summary that can highlight the main question(s)asked and the response(s) given would be useful.Being able to distinguish questions pertaining to dif-ferent issues in an email thread and being able toassociate the answers with their questions is a ne-cessity in order to generate this type of summary.In this paper, we present our work on the detec-tion of question and answer pairs in email conver-sations.
C04-1129@@Syntactic simplification is an NLP task, the goal ofwhich is to rewrite sentences to reduce their gram-matical complexity while preserving their meaningand information content. References tonamed entities: a corpus study. Iowa State University Press,Ames, IA.N.
C04-1133@@This paper describes a new model for the acquisi-tion and exploitation of selectional preferences forpredicates from natural language corpora. Topical featurestrack open-class words that appear within a cer-tain window around a target word, and local fea-tures track small N-grams associated with the tar-get word. (1) CPA Pattern set for treat:I.
C04-1134@@ The merits of translation at the word level or the concept level have long been a cause for debate among linguists. is then linked to all HowNet concepts whose Chinese word/phrase is one of the translations and the part of speech is verb v. Only the concepts in the top N categories are considered as correctly linked to the lexical entries in the cause_harm We heuristically chose N to be three in our algorithm. Available at http://www.keenage.com/zhiwang/e_zhiwang.html Bonnie J. Dorr, Gina-Anne Levow, and Dekang Lin.
C04-1136@@Many tools in the area of natural-language process-ing involve the application of ranking methods tosets of candidates, in order to select the most use-ful items from an all too often overwhelming list.Examples of such tools range from syntactic parsers(where alternative analyses are ranked by their plau-sibility) to the extraction of collocations from textcorpora (where a ranking according to the scores as-signed by a lexical association measure is the essen-tial component of an extraction pipeline ).To this end, a scoring function g is applied to thecandidate set, which assigns a real number g(x) R to every candidate x.  Conventionally, higherscores are assigned to candidates that the scoringfunction considers more useful. 5 for the t-score measure corresponding to a nominal sig-nificance level of  This threshold is obtained from thelimiting distribution of the t statistic. Therefore, I use the following approachbased on the unconditional distribution P (A).
C04-1138@@ Most large organisations, companies and politi-cal parties have a department analysing the news on a daily basis. Data cluster-ing: a review. In C. Peters (ed.
C04-1139@@ Research in linguistics and language engineering thrives on the availability of data. In all, 15 text categories (A-R) are distinguished. In order to be able to decide whether the texts were native-written or not, we searched autobiographical material, as indicated by the phrase I was born in CITY, with CITY replaced by a name of a capital city.
C04-1141@@Natural language is an open and very flexible com-munication system. Until 50% (  	 ) of the rankedlist is considered, modifiability maintains a three tofive percentage point advantage in precision over t-test and frequency. A statisticalmethod for extracting uninterrupted and interruptedcollocations from very large corpora.
C04-1146@@Over recent years, many Natural Language Pro-cessing (NLP) techniques have been developedthat might benefit from knowledge of distribu-tionally similar words, ie, words that occur insimilar contexts. Detecting a continuum of compositionalityin phrasal verbs. rank if it is one of the k near-est neighbours of w using measure m and zerootherwise.
C04-1147@@For these applications, we are in-terested in terms that co-occur in close proxim-ity more often than expected by chance, for exam-ple,  NEW,YORK These pairs of termsrepresent distinct lexical-semantic phenomena, andas consequence the terms have an affinity for eachother. In Proceedings of the 19thCOLING.E. A general frameworkfor distributional similarity.
C04-1151@@ Sentence-aligned parallel corpus is an important resource for empirical natural language tasks such as statistical machine translation and cross-lingual information retrieval. This is a completely unsupervised method. Using N-best list for Named Entity Recognition from Chinese
C04-1152@@Although computational morphological analyzershave existed for many years for a number of lan-guages, there are still many languages for which nosuch analyzer exists, but for which there is an abun-dance of electronically-available text. Unsupervised learning of themorphology of a natural language. Related is the need to enable approximatematching of boundary characters due to ortho-graphic shifts such as -y to -i-, as well as incorpo-rating other orthographic filters on possible morphs(such as requiring prefixes to contain a vowel).
C04-1154@@tree fragmentsaligned at sentential and sub-sentential levels. All the sister nodes of s are aligned withsister nodes of t. We link pars and part. The two decomposition op-erators, which are similar to those used in Tree-DOPbut are refined to take the translational links into ac-count, are as follows: the root operator which takes any pair of linkednodes in a tree pair to be the roots of a subtree pairand deletes all nodes except these new roots and allnodes dominated by them; the frontier operator which selects a (possiblyempty) set of linked node pairs in the newly cre-ated subtree pairs, excluding the roots, and deletesall subtree pairs dominated by these nodes.Allowing the root operator to select the root nodesof the original treebank tree pair and then the fron-tier operator to select an empty set of node pairsensures that the original treebank tree pair is al-ways included in the fragment base  Fragments (c),(d) and (e) were derived by selecting all further pos-sible combinations of node pairs by root and fron-tier.
C04-1161@@The main hypothesis underlying the tasks in Lex-ical Acquisition is that it is possible to infer lexi-cal properties from distributional evidence, taken asa generalisation of a words linguistic behaviour incorpora. We expect object adjectives to have a rigid posi-tion, right after a noun (in Catalan). 1% of theadjectives in the corpus).
C04-1162@@Arguably, PageRank can be singledout as a key element of the paradigm-shift Googlehas triggered in the field of Web search technol-ogy, by providing a Web page ranking mechanismthat relies on the collective knowledge of Web ar-chitects rather than content analysis of individualWeb pages. Wordnet: A lexical database. jIn(Vi)S(Vj)|Out(Vj)|where d is a damping factor that can be set between0 and 1 2.Starting from arbitrary values assigned to eachnode in the graph, the PageRank computation it-erates until convergence below a given thresholdis achieved.
C04-1164@@ In recent years, considerable progress has been invested in developing the conceptual bases for building technology that allows knowledge reuse and sharing. O           (10) Step 5 Evaluation of the taxonomic hierarchy: The whole similarity between target ontology and benchmark ontology can be represented as: ( )( )11,1 argmax ,taxonomic T BpT Bi jj qiSim O OSim VL VLp  Evaluation of the non-taxonomic relation Some relations defined in the ontology are non-taxonomic set such as synonym. m and n are the numbers of the concepts in the vertex lists of the target ontology and the bench mark ontology, respectively.
C04-1173@@Various tasks dealing with natural languagedata have to cope with the numerous differentsenses possessed by every lexical item: ma-chine translation, information retrieval, infor-mation extraction ... In a HSW, this featureis typically high.I the distribution of incidence degrees (ie thenumber of neighbours) of vertices accord-ing to the frequency of nodes (how manynodes are there that have an incidence de-gree of 1, 2, ... n). Thisis the measure we will use to determine if anode s is closer to a node r than another nodet.
C04-1175@@In Thai language, characters are written withoutexplicit word boundaries. Proceedings of the NinthElectronics Engineering Conference.A. A StatisticalAp-proach to Thai Morphological Analyzer.Natural Lan-guage Processing and IntelligentInformation System Technology ResearchLaboratory.S.
C04-1176@@in Japanese are usually writ-ten by a phonogram type of Japanese charac-ter set, KATAKANA. The number of documents in the Cor-2http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.htmlWord  pus was 4,678,040 and the distinct numberof KATAKANA words in the Corpus was1,102,108.As for a test set, we collected candidatepairs whose string penalties range from 1 to12. SubstitutionReplace a character with another charac-ter.
C04-1177@@Much about the behaviour of words is most appro-priately expressed in terms of word senses ratherthan word forms. 21ffi" of eachneighbour ( 213-4 ) multiplied by a weight. Furthermore, they account for a relativelysmall percentage of tokens for the filtered words asshown by 5$CflSh$h PP .
C04-1179@@ To produce a semantic analysis has long been a goal of Computational Linguistics. is a weight for the feature function. has order 0, while over her hand Syntactic pattern (pat): The sentence level syntactic pattern is generated from the parse tree by looking at the phrase type and logical functions of each frame element in a sentence.
C04-1180@@However, the usefulness of this outputis limited, since the underlying meaning (as repre-sented in a predicate-argument structure or logicalform) is difficult to reconstruct from such skeletalparse trees.In this paper we demonstrate how a wide-coverage statistical parser using Combinatory Cat-egorial Grammar (CCG) can be used to generate se-mantic representations. -conversion turns the previous expression intoa first-order translation for A spokesman lied:x(spokesman(x)   e(lie(e)  agent(e,x))).The resulting semantic formalism is very sim-ilar to the type-theoretic language L However, we merely use the lambda-calculus as a tool for constructing semantic rep-resentations, rather as a formal tool for model-theoretic interpretation. The specific involvementin CCG of rules of functional composition (indexed B and  B in derivations) and type-raising (in-dexed  T and  T) allows very free derivation ofnon-standard constituents.
C04-1181@@We focus in particular onvague scalar predicates like small or long. number(X ,multiple)holds(result(A,now),shape(X ,circle))e. propose(A)We formulate these constraints in an expressiveontology. In order for anindividual e of type t to serve as part of a solutionto a constraint network like (5a-e), e must addition-ally meet the constraints associated with type t. Inthis way, FIGLET requires utterance interpretationsto respect domain knowledge.Solving many of the constraints appearing in (5a-e) requires contextual reasoning about domain ac-tions and their consequences.
C04-1185@@The main rationale for this type of underspeci-fication is to ensure monotonicity, and thus upwardscompatibility of the output of shallow parsing withsemantic representations obtained from full syntac-tic parsing. Instead of predicateswith fixed arity, eg l4:on(e ,e,y), predicates and ar-guments are represented as independent elementarypredications: on(l4,e Constraints forequality of variables in elementary predications areto be added incrementally, to accommodate forknowledge-poor systems like PoS taggers, wherethe identity of referential variables of, eg, adjec-tives and nouns in potential NPs cannot be estab-lished, or else chunkers, where the binding of argu-ments to predicates is only partially established.An example of corresponding MRS (1.a) andRMRS (1.b) representations illustrate these differ-ences, cf. (1) Every fat cat sat on a mata.
C04-1186@@ In semantic role labeling (SRL) the goal is to group sequences of words together and classify them by using semantic labels. A Semantic Chunking Model Based on Tagging. The dependency relation nodes are indicated by R: The lexical nodes are indicated by W:.
C04-1188@@Current retrieval systems allow us to locate docu-ments that might contain the pertinent information,but most of them leave it to the user to extract theuseful information from a ranked list of documents.Hence, the (often unwilling) user is left with a rel-atively large amount of text to consume. E.g.,for the sentence . Learning surfacetext patterns for a question answering system.
C04-1189@@ Our objective in HITIQA is to allow the user to submit exploratory, analytical questions, such as What has been Russias reaction to the U.S. bombing of Kosovo The distinguishing property of such questions is that one cannot generally anticipate what might constitute the answer. A self-learning Universal Concept Spotter. The groups must be at least groups of size N, where N is a user controlled setting.
C04-1193@@In this paper we describe a method of acquiringa thesaurus and other useful information froma machine-readable dictionary. driverhas a familiarity of 6. Sensesare linked to Goi-Taikei semantic classes by thefollowing heuristic: look up the semantic classesC for both the headword (wi) and the genusterm(s) (wg).
C04-1194@@But this work has also shown that these resources must be used with caution: they bring on an improvement of results only if word sense disambiguation is performed with a great accuracy. T. Pedersen and R. Bruce. In Text Mine01, Workshop of the 1st SIAM International Conference on Data Mining.S.
C04-1197@@Semantic parsing of sentences is believed to be animportant task toward natural language understand-ing, and has immediate applications in tasks suchinformation extraction and question answering. Textchunking based on a generalization of winnow. [A1 The pearls] , [A0 I] [V said] , [C-A1 were leftto my daughter-in-law].Moreover in some cases, an argument might be arelative pronoun that in fact refers to the actual agentoutside the clause.
C04-1199@@The goal is to return from the collection text snippets (eg, 50 or 250 characters long) or exact answers (eg, names, dates) that answer natural language questions submitted by users. Building a Question Answering Test Collection. The n-grams that are encountered at least 10 times are considered candidate patterns.
C04-1200@@Recent computational work either focuses on sentence subjectivity Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion. This is a very crude step. Two humans annotated the 100 sentences with three categories (positive, negative, and N/A).
C04-1204@@Recently, deep linguistic analysis has successfullybeen applied to real-world texts. Argument labelsin a predicate-argument structure are basically de-fined in a left-to-right order of syntactic realizations,while if we had a cue for a movement in the PennTreebank, arguments are put in its canonical posi-tion in a predicate-argument structure..  Disambiguation modelBy grammar extraction, we are able to obtain a largelexicon together with complete derivation trees ofHPSG, i.e, an HPSG treebank. 38th ACL, pages 456463.Stephen Clark and James R. Curran.
C08-1002@@Such an organization helpsin avoiding lexicon representation redundancy andenables generalizations across similar verbs. A SequentialModel for Multi-Class Classification. Let xidenote the featurevector of an instance i, and let X denote the spaceof all such feature vectors.
C08-1003@@In many Natural Language Processing (NLP)tasks we find that a large collection of manually-annotated text is used to train and test supervisedmachine learning models. In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics.Deerwester, S., S. Dumais, G. Furnas, T. Landauer, andR. In Proceedingsof the Second Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL-01).Pradhan, S., E. Loper, D. Dligach, and M. Palmer.
C08-1006@@  The ability to detect deceptive statements in text and speech has broad applications in law en-forcement and intelligence gathering. The T/F tags were later reviewed by at least one other technical researcher. "I cant do enough.
C08-1011@@Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). References Brants, T., and Franz, A. Indeed, an especially compelling paraphrase may be found in the Google n-grams themselves, without recourse to the web.
C08-1015@@Dependency parsing aims to build the dependencyrelations between words in a sentence. It can perform dependency parsing in O(n)time. Additionally, a large unlabeled corpus wasprovided.
C08-1019@@Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.machine-generated summaries, but the correlationof these automatic scores with human evaluationmetrics has not always been consistent.In this paper, we analyze the state of currenthuman and automatic evaluation of topic-focusedsummarization. ROUGE: A package forautomatic evaluation of summaries. These4000 correlations (and p-values) are then used toestimate the median correlation for a canonicalvariate.
C08-1021@@Even now,building large and rich enough knowledge basesfor broadcoverage semantic processing takes agreat deal of expensive manual effort involvinglarge research groups during long periods of de-velopment. Once a sense is selected, the wordsense is removed from P and included into I. Thealgorithm finishes when no more pending wordsremain in P.Algorithm 1 SSI-Dijkstra AlgorithmSSI (T: list of terms)for each {t  However, when dis-5If no monosemous words are found or if no initial sensesare provided, the algorithm could make an initial guess basedon the most probable sense of the less ambiguous word of W. 63ambiguating a TS of a word sense s (for instanceparty#n#1), the list I already includes s.In order to measure the proximity of one synsetto the rest of synsets of I, we use part of theknowledge already available to build a very largeconnected graph with 99,635 nodes (synsets) and636,077 edges. In Proceedings of LREC, Lisbon, Portugal.Agirre, E. and D. Martinez.
C08-1022@@The field of research in natural language process-ing (NLP) applications for L2 language is con-stantly growing. This gives a trainingdataset comprising 8,898,359 instances. Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.then present our proposed approach on both L1 andL2 data and discuss the results obtained so far.
C08-1026@@10), with the contexts generallyabstracted to POS tags. (7) of { birth-control/JJ , poison/NN } pillsOne limitation of the variation n-grammethod isthe fact that some distinctions often need non-localinformation (cf. Of these 18, (c) 8 cases canonly be found by extending the method to pairwiseclasses.
C08-1027@@Allowing translation ofword sequences (phrases) instead of single wordsprovides SMT with a robustness in word selectionand local word reordering.PSMT has two means of reordering the words.Either a phrase pair has been learned where the tar-get word order differs from the source (phrase in-ternal reordering), or distance penalized orderingsof target phrases are attempted in decoding (phrasec The first solution is strong,the second is weak.The second solution is necessary for reorderingswithin a previously unseen sequence or over dis-tances greater than the maximal phrase length. (4) mr president , i have three points . S is source, B is baseline, and P is the SPTO approach.
C08-1029@@In many languages, a concept can be expressedwith several different linguistic expressions. Grammaticality is assessed by P (t).) The proposed model consists of twocomponents: (i) a structured N -gram languagemodel that ensures grammaticality and (ii) a distri-butional similarity measure for estimating seman-tic equivalence and substitutability between twophrases.
C08-1031@@ In the past few years, there was a growing inter-est in mining opinions in the user-generated con-tent (UGC) on the Web, eg, customer reviews, forum posts, and blogs. Kim, S. and Hovy, E. Determining the Sentiment of Opinions. Turney, P. Thumbs Up or Thumbs Down Wiebe, J. and Mihalcea, R. Word Sense and Subjec-tivity.
C08-1032@@Shouldit be based on the derived or the derivation tree Many proposals have been putforward but only recently did sufficient consensusc Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.emerge to support the specification of a TAG basedcompositional semantics. In XMG, node names are by de-fault local to a class. Forinstance, the SEM dimension may include the fol-lowing semantic formula and schema1:(1) a. Binary Relation Schema: l1: P (E), l1:Theta1(E,X), l1: Theta2(E,Y )In (1a), the flat semantic formula associatedwith every underspecifies scope by stating that thescope handle h2scopes, directly or indirectly ( ),over (the label L2associated with) the scopal ar-gument.In (1b) on the other hand, underspecifica-tion bears on the predicate P and the theta rolesTheta1, Theta2which are unification variableswhose value will be provided by the lexicon.
C08-1034@@Semantic Web and knowledge management appli-cations require to populate the concepts of theirdomain ontologies with individuals and find theirrelationships from various data sources, includingdatabases and natural language texts. In this way, we tend to penalize the6A study of how the number of snippets N would impactthe performance of the IBOP algorithm has been deferred tofuture work. Ex-tending a lexical ontology by a combination of dis-tributional semantics signatures.
C08-1038@@Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved. Stochastic Realisation Ranking for a FreeWord Order Language. Wedo this in terms of n-gram models over GFs.
C08-1041@@TheLHS and RHS can be words, phrases, or even syn-tactic trees, depending on SMT models. ), f(Xk), e(Xk))]where hiis a binary feature function, iis the fea-ture weight of hi. However, the ruleselections are independent of context information,except the left neighboring n  1 target words forcomputing n-gram language model.
C08-1042@@Whilesome grammar induction systems operate on rawtext, many of the most successful ones presumeprior part-of-speech tagging. Training uses a simple variationof Forward-Backward. They reportthat switching to induced tags decreases the overallbracketing F-score from 71.  to 63. , although therecall of VP and S constituents actually improves.Additionally, they find that NP and PP recall de-creases substantially with induced tags.
C08-1043@@approacheshave shown much promise in RTE for entailmentpairs where the text and hypothesis remain short,we expect that performance of these types of sys-tems will ultimately degrade as longer and moresyntactically complex entailment pairs are consid-ered. In Proceedings of the Second PASCALChallenges Workshop.Zettlemoyer, L. S. and M. Collins. (Fig-ure 2 lists the set of commitments that were ex-tracted from a t-h pair included in the PASCALRTE-3 Test Set.
C08-1046@@Many language-independent parsing al-gorithms were proposed there. which is a moreplausible head. While for some dependents and theirheads whether the distance is 1 or not is impor-tant, absolute distance is not so important since2Contextual features are features neither in the dependentnor in the candidate head(s).Japanese is a free-order language.
C08-1049@@In the rest of the paper, wecall this operation mode Joint S&T. Because N is the limitation on the countAlgorithm 2 Lattice generation algorithm. % on segmentation, and 16.%on Joint S&T.
C08-1050@@The role-semantic paradigm has a long and richhistory in linguistics, and the NLP communityhas recently devoted much attention to develop-ing accurate and robust methods for performingc It is widelyconjectured that an increased SRL accuracy willlead to improvements in certain NLP applica-tions, especially template-filling systems. % with a training set of 25,000 in-stances. For in-stance, the path from gave to she is VP-S-NP.PHRASETYPE.
C08-1053@@Relation extraction is one of the tasks in thenatural language processing which is constantlyrevisited. A Comparison of String DistanceMetrics for Name-Matching Tasks. 7, a comparison of any two sequences ofdifferent lengths results in the 0-score.
C08-1054@@The interpretation of coordinate structures directlyaffects the meaning of the text. A discriminativelearning model for coordinate conjunctions. (population increase) and (air pollution)b. population (increase and (air pollution))c. ((population increase) and air) pollutiond.
C08-1055@@A Noun Phrase (np) is a referring expressionif its communicative purpose is to identify anobject to a hearer. However, if (s)he removesall the horses we count it as a narrow-scopereading. D, the proposed algorithm will: P into words; Lexi-calisation takes a property as input and438returns the set of possible realisations ofthat property.
C08-1056@@Addressed to relatives or peers, written on the spur of themoment, using interfaces, each with its specificconstraints (computer keyboards, PDAs, mobilephones keypads), these electronic messages arecharacterised by massive and systematic devia-tions from the orthographic norm, as well as bya non conventional use of alphabetical symbols.c Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.In fact, letter and punctuation marks are not onlyused to conventionally encode a phonetic content,but also to introduce meta-discourse, and to sig-nal emotions, verbal effects (eg laughters), or at-titudes (humor, derision, emphasis etc). For instance, to account for the erratic use ofaccentuated letters in SMS, the most general pro-nunciation rule for letter e (incidentally, e is themost frequent letter in French) predicts five pro-nunciations: /@/, /e/, /E/, //, //, plus the pos-sibility of being deleted. The dictionnaryused in the experiments reported above con-tains about 4,200 entries.This module is implemented as a finite-state transducer E which transduces lettersequences in  into mixed grapheme andphoneme sequences (in ( the second module converts the graphemicportions of the input message into a phone-mic string using a set of manually encodednon-deterministic letter-to-phone rules; theserules notably encode the possibility for eachsymbol to encode its spelling (eg u for /ju/ orR for /@r/).
C08-1057@@Othertasks, such as the extraction of factual information,remain a bigger challenge.Researchers have recently begun to use deeperNLP techniques (eg statistical parsing) for im-c Foroptimal performance, many of these techniquesrequire richer syntactic and semantic informa-tion than is provided by existing domain lexicons(eg UMLS metathesaurus and lexicon1). A number of experimentswere reported. I(Clusters; SCFs) ,where  is a parameter that balances the con-straints.
C08-1060@@The mass media can affect world events by sway-ing public opinion, officials and decision makers.Financial investors who evaluate the economic per-formance of a company can be swayed by positiveand negative perceptions about the company in themedia, directly impacting its economic position.The same is true of politics, where a candidatesperformance is impacted by media influenced pub-lic perception. Bulletin of Economic Research,51:130.Wolfers, J. and E. Zitzewitz. Also, combining the internal mar-ket baseline with a news system improved perfor-mance, suggesting that forecasting future publicopinions requires a combination of new information and continuing trends, neither of which can becaptured by the other.ReferencesDebnath, S., D. M. Pennock, C. L. Giles, andS.
C08-1064@@Translation model size is growing quickly due tothe use of larger training corpora and more com-plex models. Its size is 4|T |bytes and it enables lookup of any length-m sub-string of T in O(m+ log |T |) time.Fast extraction using sampling. Under the loosest interpretation ofthis capability, any subset of words in a sentenceThis research was conducted while I was at the Universityof Maryland.
C08-1066@@A necessary (if not sufficient) condition for truenatural language understanding is a mastery ofopen-domain natural language inference (NLI):the task of determining whether a natural-languagehypothesis can be inferred from a given premise.Indeed, NLI can enable more immediate applica-tions, such as semantic search and question an-c Regrettably, such ap-proaches tend to founder on the myriad complexi-ties of full semantic interpretation, including tense,aspect, causality, intensionality, modality, vague-ness, idioms, indexicals, ellipsis, and many otherissues. The entailment relation between x andxis found by projecting the entailment relationgenerated by e upward through xs semantic com-position tree. Dumbo is a small animal.
C08-1067@@This research has been carried out in the frame-work of a customer project for PSA PeugeotCitroen. In 36th Annual Meeting of the Associ-ation for Computational Linguistics and 17th International Conference on Computational Linguistics(Proceedings of COLING-ACL 98), pages 444450,Universite de Montreal, Montreal, Quebec, Canada.Gotti, F., P. Langlais, E. Macklovitch, D. Bourigault,B. In this way additional n:1candidates are constructed.. .
C08-1069@@They found that themanual development of such mapping rules is not atrivial task; their mapping rules covered only 85%of the GRs in a GR-annotated corpus; ie, 15% ofthe GRs in the corpus could not be covered by themapping from the gold-standard CCG analyses ofthose sentences.We propose another method for the cross-framework performance analysis of the parserswherein the output of parsers are first convertedto a CFG tree. In In Pro-ceedings First Conference on Linguistic Resources,pages 447455.Charniak, E. and M. Johnson. A program calledMayz is distributed with the grammar, which was1Version 2.
C08-1071@@Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.However, labeled training data is expensive toannotate. Distributions are L (left expansion), R(right expansion), H (head word), M (head phrasalcategory), and T (head POS tag). Let W and S repre-sent the n-best lists from the baseline WSJ and self-trained parsers, respectively.
C08-1072@@Disfluency is one obstacle preventing speechrecognition systems from being able to recog-nize spontaneous speech. D,T that are not covered by thetree are assigned label  Prob-abilities for HHMM models Q-Expand, Q-Trans,and F-Reduce can then be estimated from these val-ues directly. aflight to Boston, uh, I mean, to Denver.
C08-1074@@Ochs method performs a seriesof one-dimensional optimizations of the featureweight vector, using an innovative line search thattakes advantage of special properties of the map-ping from sets of feature weights to the resultingtranslation quality measurement. 86probabilities, E-to-F and F-to-E phrase translationlexical scores, French language model log proba-bilities, phrase pair count, French word count, anddistortion score. To en-sure this, each step progresses as follows:~d(i) u(i)~w(i) otherwise.With this update rule, we know that ~w(i+1) willnever go below m, since the initial value is not be-low m, and any step moving below m will resultin a negative ratio and therefore not be accepted.So far, 2 is left as a free parameter.
C08-1076@@Nevertheless, most of these works are con-fined to certain individual principles rather thanformulating a general theory describing the emer-gence of these regular patterns across the conso-nant inventories.The self-organization of the consonant inven-tories emerges due to an interaction of differentforces acting upon them. E. Shannon and W. Weaver. Let us de-note the feature distance between two consonantsCiand C iby D(Ci, Ci).
C08-1078@@Event detection is a core Natural Language Pro-cessing (NLP) task that focuses on the automaticidentification and classification of various eventtypes in text. Usingthe annotated data, a kappa score of 0. So instead we have usedprecision, recall and F1 to evaluate each technique.If a is the number of sentences correctly classi-fied by a system to class i, b is the total num-ber of sentences classified to class i by a system,and c is the total number of human-annotated sen-tences in class i.
C08-1079@@Pronoun resolution is the task of determining theantecedent of an anaphoric pronoun, or a pro-noun pointing back to some previously mentioneditem in a text. A ranking ap-proach to pronoun resolution. Moreexperiments should be designed to make the influ-ences of annotation schemes on the pronoun reso-lution process clearer.ReferencesBerger, Adam L., Vincent J. Della Pietra, and StephenA.
C08-1083@@Abbreviations present two major challenges in nat-ural language processing: term variation and am-biguity. A maximum entropy ap-proach to natural language processing. Abbreviation LIfter using Corpus-based Extraction:http://uvdb3.hgc.jp/ALICE/ALICE index.html10LIBSVM  A Library for Support Vector Machines:http://www.csientu.edu.tw/cjlin/libsvm/System P R F1Schwartz & Hearst (SH) .
C08-1088@@ Information extraction is one of the key tasks in natural language processing. As indicated as follows, each CFG rule has the form: P LnL1H R1RmHere, P is the parent node, H is the head child of the rule, LnL1 and R1Rm are left and right modifiers of H respectively, and both n and mmay be zero. For comparison purposes, the training parameters C (SVM) and  (tree kernel) are also set to 2.  and 0.  respectively.
C08-1091@@Unsupervised learning of grammar from text(grammar induction) is of great theoretical andpractical importance. LL mapping is used for evaluation.tion of constituents covered by the T (P ) most fre-quent labels before mapping with SC is 0. [1,M ]) is the number oftimes the i-th label is the parent of the representedlabel.
C08-1093@@In recent years, user logs of search engines have at-tracted considerable attention in research on queryclustering, query suggestions, query expansion, orgeneral web search. In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics (ACL07), Prague, Czech Republic.Robertson, Stephen E., Steve Walker, and MichelineHancock-Beaulieu. A technique for the measurementof attitudes.
C08-1094@@While there have been great advances in the statis-tical modeling of hierarchical syntactic structure inthe past 15 years, exact inference with such mod-els remains very costly, so that most rich syntactic modeling approaches involve heavy pruning,pipelining or both. Atline 7, we have identified the chart cell being pro-cessed, which is (t, e). Therefore the overall worst-casecomplexity of the algorithm under these conditionsis O(n
C08-1096@@ This paper describes the automatic extraction of linguistic event frames based on a corpus of MEDLINE abstracts that has been annotated with gene regulation events by a group of do                                                 Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Semantic Roles 762Our annotated corpus consists of 677 MED-LINE abstracts on E. Coli. required a large amount of discussion.
C08-1097@@Anaphora resolution is one of the most importanttechniques in discourse analysis. (e) For each possible case frame, estimateeach correspondence probabilistically,and select the most likely case frame andcorrespondence.In this paper, we concentrate on three case slotsfor zero anaphora resolution: ga (subjective), and ni (dative), which coverabout 90% of zero anaphora. but only a few types.
C08-1098@@(1)HMM taggers are fast and were successfully ap-plied to a wide range of languages and training cor-pora.c Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.POS taggers are usually trained on corpora withbetween 50 and 150 different POS tags. More precisely, if Tis the set ofPOS tags that occurred with suffix , |T | is thesize of the set T , fis the frequency of suffix ,and p (t) is the probability of POS tag t among thewords with suffix , then the following conditionmust hold:fa|Ta|tTapa The training of the SVMTool tookmore than a day. isalso a valid attribute specification, but 1:Nom 1) must have been used be-fore an attribute of the POS tag at position i-(k+1)may be examined.
C08-1099@@Psycholinguistic studies suggest a model of humanlanguage processing with three important proper-ties. Ph.D. thesis, Cognitive Science,The Johns Hopkins University.Hobbs, Jerry R., Douglas E. Appelt, John Bear,David Israel, Megumi Kameyama, Mark Stickel, andMabry Tyson. is a sentence delimiterbased on whether the words preceding it can bereduced as a sentence), or with n-gram languagemodels (if words are observations, this is simplyan autoregressive HMM topology).
C08-1101@@Opinions have been investigated at the phrase, sen-tence, and document levels. A support vector method for multi-variate performance measures. Finally, weshowed via our machine learning experiments thatthe presence of opinion frames can be automati-cally detected.ReferencesCarletta, J., S. Ashby, S. Bourban, M. Flynn,M.
C08-1102@@The resolution of lexical ambiguities has long beenconsidered an important part of the process ofunderstanding natural language. Text, Speech and Language Technology.Springer.E. In Proceedingsof the Second Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL-01), pages 7986, Pittsburgh, PA., June.S.
C08-1103@@Subjectivity analysis is concerned with extract-ing information about attitudes, beliefs, emotions,opinions, evaluations, sentiment and other privatestates expressed in texts. The annotator begins with a corpus of documents thathas been annotated w.r.t. Extracting opinions, opinionholders, and topics expressed in online news media text.In Proceedings of ACL/COLING Workshop on Sentimentand Subjectivity in Text.Kobayashi, N., K. Inui, Y. Matsumoto, K. Tateishi, and
C08-1104@@In recent years, subjectivity analysis and opinionmining have attracted considerable attention in the NLP community. If S is equal or higher than anagreed threshold T, then the synset is classified assubjective, otherwise as objective. Senti-WordNet: A Publicly Available Lexical Resource forOpinion Mining.
C08-1106@@Shallow parsing identifies the non-recursive coresof various phrase types in text. , the NP started by her isstill incomplete, so the label for  is likely to be I,which conveys the continuation of the phrase, eg, In contrast, for He gaveher  , the phrase started by her is normally self-complete, and makes the next label more likely tobe B, eg,  [He] gave [her] [flowers].In other words, latent-dynamics is an interme-diate representation between input features and la-bels, and explicitly modeling this can simplify theproblem. 9% by using a CRF model.
C08-1107@@In many NLP applications, such as Question An-swering (QA) and Information Extraction (IE), itis crucial to recognize whether a specific targetmeaning is inferred from a text. We manually classified each rule l as either: (a) Correct the rule is valid insome contexts of the event but extracted some in-correct mentions; (b) Partial Template l is only apart of a correct template that entails r. For exam-ple, learning X decide ; (e) Incorrect otherincorrect rules, eg charge X  First, rela-tive to BInc, it tends to learn incorrect rules forhigh frequency templates, and therefore extractedmany more incorrect mentions for the same num-ber of incorrect rules. r, by giving more weight to the cov-erage of the features of l by r (with  Such approach is notfeasible for non-comparable corpora where statis-tical measurement is required.
C08-1109@@The long-term goal of our work is to develop asystem which detects errors in grammar and us-age so that appropriate feedback can be given tonon-native English writers, a large and growingsegment of the worlds population. In COLING Workshop on Hu-man Judgments in Computational Linguistics.Turner, J. and E. Charniak. We use the preceding nounphrase (PN) and following head (FH) from theoriginal feature set for the N-p-N feature.
C08-1114@@A pair of words (petrify:stone) is analogous to an-other pair (vaporize:gas) when the semantic re-lations between the words in the first pair arehighly similar to the relations in the second pair.Two words (levied and imposed) are synonymousin a context (levied a tax) when they can be in-terchanged (imposed a tax), they are are antony-mous when they have opposite meanings (blackc We subsume synonyms, antonyms, andassociations under analogies. Using information contentto evaluate semantic similarity in a taxonomy. For example, consider the knowledge en-coded in WordNet: much of the knowledge inWordNet is embedded in the graph structure thatconnects words.Analogies of the form A:B::C:D are calledproportional analogies.
C08-1116@@Recent work on the task of acquiring attributesfor concept classes has focused on the use of pre-compiled lists of class representative instances,where attributes recognized as applying to multi-ple instances of the same class are inferred as be-ing likely to apply to most, or all, members ofthat class. Finding con-cept attributes in the web using a parser. and S.C. Shapiro, editors, Natural Language Processingand Knowledge Representation: Language for Knowledgeand Knowledge for Language.
C08-1119@@For instance, one mightdescribe a burqa (a full body covering for Muslimwomen) as a suit of armor, as a shield againstprying eyes or, depending on ones communi-cation goal, as a wearable cage. Norwood, NJ: Ablex.Hofstadter, D. R. and the Fluid Analogy ResearchGroup. Thisconceptualization can be viewed as an interme-diate stage in a slippage path from Governor toPresident as follows:Governor of California  governor of 12% of the United States leader of 12% of the United States president of 12% of the United States president of 100% of the United StatesPresident of the U.S.
C08-1125@@Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). 1 T ation result f using i in d onolingual corpora Using the in-domain translation dictionary im-used (from 29. A part of this ropean languages.
C08-1128@@Chinese sentences are written in the form of a se-quence of Chinese characters, and words are notseparated by white spaces. leads to M new possible alignments in the E-to-C direction b+k1, . A generativeconstituent-context model for improved grammar in-duction.
C08-1132@@ Case structures (ie predicate-argument struc-tures) represent what arguments can be attached to a predicate, which are very useful to recognize the meaning of natural language text. A Study in Multilingual Parser Optimization. 0% F-score on testing data set.
C08-1136@@Many recent syntax-based statistical machinetranslation systems fall into the general formalismof Synchronous Context-Free Grammars (SCFG),where the grammar rules are found by first align-ing parallel text at the word level. ), Sand Tmust besubsequences of S and T , respectively. (e) Finally, if y violates monotonicity, re-move it from X .
C08-1138@@The common explicit representations of this rela-tion are word alignments, phrase alignments and structure alignments between bilingual sentences. Given the source parse tree 1( )JT f , there are multiple derivations3 that could lead to the same target tree 1( )IT e , the mapping probability 1 1( ( ) | ( ))I JrP T e T f is ob-tained by summing over the probabilities of all derivations. A syntax-based statistical translation model.
C08-1139@@This has led to increased in-terest in the development of parallel treebanks as the source for such syntactic data. Building a resource for studying translation shifts. Ths results seem to point at the skip1_span1 and skip2_span1 configurations as the best-suited for further development.Unexpectedly, the results of the extrinsic evaluation do not strictly follow the trends found in the intrinsic evaluation.
C08-1142@@ Creating a large labeled training corpus is very expensive and time-consuming in some real-world applications. Augment L with these m new examples, and remove them from U 4. A stopping criterion for ac-tive learning.
C08-1144@@Probabilistic synchronous context-free grammar (PSCFG) models define weighted transductionrules that are automatically learned from paralleltraining data. fi1, andwhere k is an index for the nonterminal X thatindicates the one-to-one correspondence betweenthe new X tokens on the two sides (it is not inthe space of word indices like i, j, u, v,m, n). The n-gram LM implementation described in Brants etal.
C08-1145@@One way of dividing up such systemsis into those that take the whole output frommultiple systems and judge between them toselect the best candidate, and those that com-bine elements of the outputs to construct abest candidate.Deciding between whole sentence level out-puts looks like a classical classification prob-lem. does not have a higher ac-curacy either. 1)-gram we concentrateon the higher n-grams first.
C08-2002@@Computational approaches to sentiment analysiseschew a general theory of emotions and focuson extracting the affective content of a text fromthe detection of expressions of sentiment. Let Sa, Sbbe the FS associatedrespectively to a and b i.e Sa: [category : [groupa:subgroupa],modality : [polarity : pa, strength : sa]  bethe FS deduced from the combination of SaandSb. A Computational Semantic Lexiconof French Verbs of Emotion.
C08-2003@@Unlike written language, speechand hence, au-tomatic speech transcriptiondoes not come seg-mented into units. We trained n-gram modelsboth based on words and on words plus POS-information that was incrementally obtained fromthe parser. The workthat does treat intra-turn utterance segmentationdoes so in an offline context, namely the post-processing of automatic transcripts of recordedspeech such as meeting protocols (Fung et al,c Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.
C08-2013@@I(u) and O(u)respectively denote the projection of an observa-tion u into the input space and output space: ifu  For very high-dimensional input spaces(hundreds of thousand of elements), we proposea heuristic which reduces computation time with alimited impact on recall. The constrainton s leads to the frontier {(d, )} (since the countof t must be null). Computing analogies in very large inputspace (hundreds of thousand forms) however re-mains computationally demanding, as the retrievalalgorithm must be carried out o(I) times.
C08-2019@@Most participants considered atwo-phase re-ranking approach, where first topic-based relevancy search was employed, and thensome sort of filtering for subjectivity was applied;these filters were based on trained classifiers orsubjectivity lexicons.We propose an alternative approach to reviewsearch, one that is unsupervised and that doesnot rely on pre-existing dictionaries. Any opinions, findings, andconclusions or recommendations expressed are those of theauthors and do not necessarily reflect the views or officialpolicies, either expressed or implied, of any sponsoring in-stitutions, the U.S. government, or any other entity.ReferencesAllan, James, Margaret E. Connell, W. Bruce Croft, Fang-Fang Feng, David Fisher, and Xiaoyan Li. AlgorithmDefine a search set as the top n webpages returnedin response to a reviewor opinion-oriented queryby a high-quality initial search engine, in our case,the top 20 returned by Yahoo!.
C08-2022@@Discourse relations between textual units are con-sidered key for the ability to properly interpretor produce discourse. N-gram discourse relation modelsWe have shown above that some relations, such ascomparison, can be easily identified because theyare often explicit and are expressed by an unam-biguous connective. (I1) I took my umbrella this morning.
C08-2024@@However, simply iden-tifying fixed geospatial regions and specific facil-ities is not enough to achieve a complete repre-sentation of all the spatial phenomena present. We are currently evaluating the reliabilityof the FrameNet encoding of motion predicates,and are developing algorithms for translating lexi-cal structures to Spatio-temporal markup.ReferencesAsher, N. and P. Sablayrolles. For modeling mo-tion, however, we restrict our discussion to spatio-temporal regions occupied by physical matter de-noted by the type s  p.For this work, we performed several map-pings between Muller, Asher and Sablayrolles, andFrameNet.
C08-5001@@The basic idea ofDP is to solve a bigger problem by divide-and-conquer, but also reuses thesolutions of overlapping subproblems to avoid recalculation. with a setR, where V is the set of vertices, E is the set of hyperedges, and R is theset of weights. kreturns the ordered list of the top-k elements in a set.
C80-1008@@These systems promise to make major improvements in the ease-of-use of data base management and other computer systems. In t~fs~n,  we w~ll go through the basic options in these phases, giving a quick survey of systems exhibiting the options. In--\]$e~cal processing, (processing individual words), absolute ill-formedness can come from misspelling and mistyping; relative ill-formedness can arise from unexpected words.
C80-1012@@  When we look up l inguist ic  theor ies  of sentence or even of text -semant ics  to see what  they can offer  in respect  to word -mean ing ,  we wi l l  be conf ronted with bas ica l ly  two types F ILLMORE /3/ has referred to as check l i s t semant ics  and proto type-semant ics . ZADEH, L.A.:"PRUF a meaning representation language for natural languages. For the generat ion  of new mean ings  which denote poss ib le  but  not  yet labeled e lements  (or sets of elements) in U, it can wel l  be argued that the fo l lowing def in i t ions  should operate on both, re ferent ia l  meanings  M(x) and l inguis tic descr ip t ions  D(z) the former of which only are given here.
C80-1017@@ The semantic and syntactic interrelations between the lexical units of a human language are notoriously complex and intricate, whether considered from the individual language-users point of view or from the perspective of the collective competence of a language community. to whiff fuml-a to fumble h~-! Both to f ish and to angle "contain" an e lement  of catching.
C82-1013@@ For its theoretic and practical implications automatic abstracting has recently emerged as one of the most promising and interesting research topics in the field of natural language studies covered by computational linguistics. 83 84 D. FUM, G. GUIDA and C. TASSO SPECIFICATIONSAND BASIC METHODOLOGIES In defining SUSYs specifications we have tried to implement at a certain level of abstraction an important human feature: the capability to generate summaries of different content and extent depending on the users goals. the focus is on the s~ry  schema.
C86-1020@@ As the means of natural language processing axe gradually reaching a stage where the realisation of large-scale projects like EUROTRA becomes more and more feasible, the demand for lexical databases increases. Entries which are too i rregular  to fit into such a class should be defined as i r regular. A lexicographer interface for the editing of entries.
C86-1031@@  This paper describes parallel model for natural language parsing and gives a design for its implementation. lohn" as a candidate sentence head. Sinlultaneously, ti~e NP master sends the adjective noun pair, (tough, coach) to the AN-master.
C86-1040@@  English at its simplest is an SVO, Subject Verb Object, language. She also adds this utterance to the cunent discourse space, i,e. In Josh:, A.
C86-1045@@ The work on merging strategies from unification grammars and categorial grammars has its origins in several research efforst that have been pursued in parallel. /~  Left /~  agr ca~//pers / form cat/pers~nu m S Finite N 3 Sg It corresponds to the derived-category s mboh (9) S \ N form : Finite pers : 3 num: Sg (10a) and (10b) are the rules that combine constituents. in graph notation: val u e~J -~~~.
C86-1046@@ The recent development of grammar theory ehibits convergencies among various approaches, such as Government-Binding Theory, Generalized Phrase Structure Grammar, Definite Clause Grammar, Lexical Functional Grammar, Functional Unification G~ammar, and others. It is the means t:o formalize syntact:ic alternatives. The base lexicon creates the \].in\]{ between the segments of ti~e input language and the terms of DRL.
C86-1048@@  This paper discusses the relationstfip between Tree Adjoining Grammars (TAGs) and :Head Grammars (HGs). We now give a MHG generating L~. In general, the term f i (~-T, .
C86-1050@@  I Like most llngafistic theories, the theory of generalized phrase structure grammar (GPSG) has described language axiomatically, that is, as a set of universal and language-specific constraints on the we\[l-formedncss of linguistic elements of some sort. As in the previous definition, we will be quite lax with respcct to our notation for paths, using ((a b) c) and (a (b e) ) as synonymous with (ab  c) . However, a modification of these exemptions i necessary in the case of lexical defaults, ie, default values instantiated on lexical constituents.
C86-1063@@The so f tware  module GRAPHON (GRAPHemo-PHONome-conversion) has  boon deve loped  to conver t  any  g iven  German text  in to  i ts  phonet ic  t ranscr ip t ion  (I.P.A. \[2\] Allen, J.: "Synthes is  of Speech from Unrest r i c ted  Text". F1SX f LSX TX IN  F1SX "FLSX fX IV  + or" + werb  + s t  + At  + i g + en  + + er  + werb  + s t  4.
C86-1067@@ Ordinary Japanese sentences are written using a combination of Kana, which are Japanese phonogramic characters, and Kanji, which are ideographic Chinese characters. A double circled VTX can usually be shared. Since the complexity of the above process is on an oder of n, the quasi-best path can be obtained very efficiently.
C86-1085@@81) and the modality of linguistic communication (telephone vs. teletype, cf. E.g., tbr the representation i Fig. Instead, one might regard our environment asa study in vitro, eliminating a number of uncertainty t~tctors so that tile essential characteristics of spatial deixis become more salient.
C86-1086@@ Linguistic (and related) literature describes numerous forms of non-singular concept~ that can be found in discourse including intensional (or functional) concepts, mass concepts, generic (or general) concepts attributive concepts, abstract concepts, etc. Some new objects from L 0 different than N/s. Presumably Mary brings only a part of w but we can say that w is being brought by Mary every day This is the same w every day, although each time possibly a different part of it is in transit, which leads to the obvious translation (at L+I ) (i) 5a --, (br -e -d  M w) where br -e -d  stands for brings every day.
C86-1090@@ Computers were first used for the processing of natural language texts over 30 years ago. Porter, Probabilistic Models of Indexing and Searching, in Information Retrieval Research, R.N. l(b) shows the corresponding document-term graph where a line between term T. and document D. represents the correspondin~ term assignmen~ appears in Fig.
C86-1103@@ The intent of this paper is to show some aspects of a computer dictionary geared towards the natural anguage component of an expert system. The main objectives of the project Linguistics and Logic Based Legal Expert System, which is a Joint Research Project between the University of T/ibingen and the IBM Scientific Center Heidelberg, are to design and implement anatural language based knowledge acquisition and query system and to build a legal expert system on its basis. Morristown: Association for Computational Linguistics (order from Dr. D. E. Walker, Bell Communications Research, 445 South Street, Morristown NJ 07960), 275-282.
C86-1119@@To  complete d iscourse-connected conclusions, a number  of various linguistic phenomena must be resolved. V~e postulate the application of the nonmonoton ic  inference system for that purpose. A set  of fo rmulae  der ivab le  from a g iven  de faul t  theory  is; ca l led  an  extens ion  of the  theory  and is i n te rpreted  as  a set of beliefs about  the wor ld  being *h~delled.
C86-1130@@ The task of natural anguage generation is that of producing linguistic output to satisfy the communicative r quirements of a computer system. This inheritance is a representational tool which has been employed throughout the history of Artificial Intelligence (e.f.\[18,19,2,3\]). J. Moore and A. Newell.
C86-1141@@ A spoken message can be produced either to utter a written text (text-to-speech system), or to communicate orally the information given in a semantic representation (semantic-representation-to-speech system). 1. tn French, semantic features ore needed to distinguish only o few non-homophonic homographs, mostly technical words, I. The erasing of the auxiliary vocabulary leads to: Jean a offert des anemones ~ Mar/e pour la rendr~ iveureuse.
C86-1149@@ This paper introduces a new approach to knowledge-based machine translation for well-defined domains, integrating two recent advances in computational inguistics: entity-oriented parsing \[16\] and functional grammars \[4, 19\]. e Robustness -The recognition of ill-structured language is very important, especially for the shorttext domains we envision for our system (telex messages, banking transactions, doctor.patient dialogs, etc.). The Tomita algorithm can be viewed as an extended LR parsing algorithm \[t\].
C86-1152@@ The Science Institute of IBM Japan, Ltd. has been involved in Engllsh-Japanese machine translation for four years (I). at the IBM T.J. Watson Research Canter (3). .. E-J Translation of Simple Noun Phrases One of the issues in MT is how to create and maintain a large terminology dictionary.
C86-1154@@75 percent of them are published exclusively in Japanese, only a 5th of Japanese papers are currently evaluated from Western refereeing and information services. 652 The second -, and probably most important step is to decide in which way the content  o f  the  semant ic  represen ta t ion  should be uttered as German text. It supported experimenting w i th  the system and its s tep-by step improvement 3.
C86-1156@@The study of anaphora has been a central issue in both theoretical nd computational linguistics. References Bach, E. and Partce, B. Barwise, J. and Perry, J. PhD Thesis, University of Massachusetts. For example, the expression "N:syn:index" refers to the value of the Index attribute of the syn attribute of the variable N. We make heavy use of the attribute-value notation to represent feature bundles associated with constituents.
C88-1002@@A TAGs basic component is a finite set of elementary trees, each of which is a domain of locality, and can be viewed as a minimal inguistic structure. Corresponding to No V N1 , we have 1. elmer N Jean aires i I MJrle Merle NPs are substituted at the proper nodes in the trees. 34) Jean e:;t content que tout le monde le regalde.
C88-1003@@and  purpose  In knowledge-based natural language understanding systems the role of syntax is by no means self-evident. It should t,e observed that the isomorphy principle allows for both lexical and structural ambiguity, ie it does not require ~hat a given word, or grammatical relation can be interpreted in only one way. Impl ied  referents  a~d types.
C88-1006@@The theme of this paper is how to deal with the phonological or orthographic half of the problem of computational morphology, ie, how to handle the various problems associated with the spellings of morphemes. 108-109 Gemination rule: + -~ c l /C  V cl  _V ;  where cl is in {b,d,f,g,l,m,n,p,r,s,t} Words: refer (default is that it is consistent with all rules) bother -gemination (means that the gemination rule does not apply to this word) There are other sets of data for which this technique is usefnl. , ,a t lve  :~ ,u ie  Features There is a 9roblem with previous accounts of English that have been done in terms of two-level rules.
C88-1007@@and Introduction The aim of this paper is to explore how the linguistic theory known as Unification Categorial Grammar can be adapted to the general methodology ofMachine Translation using Isomorphic Grammars, as pioneered by Landsbergen and others in the ROSETTA team \[Landsbergen 87a, b\]. / v 8 The ~monohngual) UCG formalis~n Many recent grarmnar formalisms \[Shieber 86\] represent linguistic objects as t~ts of attribute-.value pairs. There remain many problems with realising this approach in s. practical I~ystem.
C88-1009@@Introduction Uni f icat ion-based grammar formalisms have become a popular f ield of research. .o~of~ e Features (} : ) *o  Note  that  the  notat ion  f . Rounds, W.C., Kasper, R.: A complete logical calculus for record structures represent ing l inguistic information.
C88-1016@@ In this paper we will outline an approach to automatic translation that utilizes techniques of statistical information extraction from large data bases. Then increment by 1 the counter C(e,,f). Stone: Classification and Regression Trees, Wadsworth alld t~rooks, M(mtcrey.
C88-1017@@Moreover, an MT system must be based on a linguistically justified theory of grammar. One of d~e most important questions for the constructive version is ir~ what order the components of GPSG have to be applied. \[GKPS:214\], which would not fit ittto a universal principle.
C88-1021@@ :  The  Complex i ty  of  Anaphora  Reso lu t ion  Anaphora is a pervasive phenomenon in natural language communication, whether it be complex multi-party human discourse or more constrained bilateral human-computer dialog. \[9\] Charniak, E. Towards a Model of Childrens Story Comprehension. References  \[1\] Bimbaum, L. and Selfridge, M. Conceptual Analysis in Natural Language.
C88-1026@@ Binding is a component subtheory of Governmentbinding which applies in the derivation of the logical form of utterances from their surface rcpresentation. u , l  P,C; iu~O~ bc dent  iu diltbronl ways, (hm ;iN,. The attribute node is associated with every node in an S-Structure tree, enumerating them in preorder.
C88-1027@@We shall call X-structures (middle level in the maximal projections) as "groups": a group contains an overt or an empty head, wrapped by its specifiers (denoted Specif) and modifiers (denoted Modif) but without argument(s) (ds~ noted Arg). e~ ing  (e:r,,bod0,i~g into ,U om NP) of tile VP. 5.t meanings of the verb  S "to be".
C88-1029@@ It is generally believed that a generation system can be modularized into a sequence of components, the first one making the "high level" decisions (ie the conceptual decisions), the following ones making the linguistic decisions (eg lexical and syntactic onstruction choices), the penultimate, one performing the "low level" operations (ie the syntactic operations), and the last one handling the morphological operations. The computation o f  L1 can also depend upon the synthesis of other elements, thereby involving non local dependencies. 3 Synthesis of personal  pronouns If a token refers to the speaker(s) or the hearer(s), it must be synthesized as a first or second person pronoun; the only operation to be performed is the computation of this "dialogue" pronoun.
C88-1031@@Current machine translation (MT) systems deal only superficially, if at all, with the translation of style. 2 A Def in i t ion of  Style for Mach ine  Translat ion Our approach to a definition of style is non-literary, groupbased, and, most important, goal-directed. For example, the concepts of detached adjcctival clause, piled-up adje.ctives, and adjectival phr~,~e are all .significant in stylistics.
C88-1033@@: The influence of the criterion "Aktionsart" with respect o the temporal relations of temporal entities often seems to be overemphasized. e) (e\] )  Then he bordered the t ra in . 32-843 VAN BENTItEM, I.
C88-1034@@ Our goal is to construct a morpho-syntactic analyzer for French which is capable of automatically detecting and of correcting (automatically or with help from the user) spelling mistakes, agreement errors and the most important syntax errors. We call these rrors "typographical" e~rors: we will not discuss them fresher in this paper. Word categorization At this point, a word can have been assigned a single lexical category, as for example cahier : N IF-, etc.\].
C88-1037@@ Natural language interfaces are being applied to various applications; most of them, especially data base and expert system interfaces, require the production of precise texts. A. N. De ttoeck 8z B. G. T. Lowden 86 "Generating English paraphrases from formal relational calculus expressions", Proc. (E~,,h man is in a same room.)
C88-1039@@ Much work in current research in the field of semantic pragmatic analysis has been concerned with the interpretation of ~Latural language utterances in the context of dialogs, eg, determining the speakers goals \[Allen 83\], deriving beliefs of one agent about another \[Wilks/Bien 83\], and planning speech acts \[Appelt 85\]. 2 shows the ru~e which applies to the idiom "I do not know whether X." References Allen 83: J. F. Allen: Recognizing Intentions from Natural Language Utterances, in: M. Brady and R. C. Berwick (Ed.
C88-1054@@  BIDIRECTIONALITY, or the ability to use a cormnon knowledge base for both language anMysis mid generation, is a desirable feature of a reM language proccssing system. Th$.,~e xamples, while only touching upon the lexical representation a of FLUSH, shows some of the characteristics of a birectional lexicon. 3 The  Bidirectional  Lex icon  Several characteristics are essential to a lexicon that can be used effectively in both analysis and generation: 267 1.
C88-1055@@  The ambiguity and imprecision of language are the key problems in building language understanding programs. same type as b, and merge Col~s roles with C,~etos roles. Concretion can a\]E:o result in deriving a non-literM interpretation, asin the "give a kiss" and "take argmnents" examples.
C88-1063@@ Many computational linguists have found systemic grammar (SG) to be quite useful, because it provides an explicit representation of features that determine how a sentence functions in the context of communication. t ions produced by the p~ser. Acknowledgements  I would like to thank Bill Mann for originally suggesting and eneouro aging this topic of research.
C88-1065@@  If asked "Did Sally eat" after having been told that Sally ate a pear, speakers of English would not hesitate to answer "Yes". STAt/T: I dont know. Thus: (63) Sally ate a pear.
C88-1070@@ Ill-formed input cannot be ignored when a natural language processing system such as a computer assisted instruction (CAD system or a machine translation system is built. The corrected seaten,:e, "Mr. Brown has eaten an apple. And then the ins~mnfiation mechanism and interpretat ion of Input sentence .................... Jnpu~\[q)ParsingPloce s sing .
C88-1072@@ This paper reports on a prototype news analysis system (NAS) which classifies and indexes news stories in real time. ~r i~a~ona l  forms and associations between words and affixes. Both eases often indicate insufficient info 13nat ion of a lexica\] item.
C88-1075@@  It is often necessary in practical situations to attempt parsing an incorrect or incomplete input. A.~I  Craxnxnar  o f  the  ana lyzed  language  l  i~ia grmr.m~ is taken flom \[28\]. B) is replaced by e~ then no stack symbol is popped from (resp.
C88-2085@@The author  is forced to concen trate on s ing le  teach ing  programs wi thout  a lmost  any chance to genera l i se  f rom his resu l ts  and to reuse parts of it in other  contexts  later. morpho-syntact ie  regu lar  i t ies  be longs  to ne i ther  the one nor  the o ther  category . This t ime the network  cons is ts  of f ive nodes and 16 edges.
C88-2099@@ Thi~; paper is concerned with heuristics for segmenting narratives into units that tb~t  the basic elements of discourse representations and that constrain the application of focusing algorithmr,. The most important temporal discontinuities are: a. First, ~t tile minimal level of segmentation, DSs are usttally smaller than typographical p ragraphs.
C88-2103@@The general set-up, the screen display and the system configuration are illustrated in Appendix. k PUS I /  PI~E-PIIRASE .~ (a) After the conversation we will have lunch. (b) I heard the lecture was Interesting.
C88-2110@@  Much of the work on polysemy has tended to confuse the nature of word meaning by labeling many different types of ambiguity as the same phenomenon. t We term these the L-system and U-system, respectively. Wilks, Yorick, "Preference Semantics," in Formal Semantics of Natural Language, Keenan, E.
C88-2120@@ Though a wealth of insights on the structure and meaning of discourse ha,,; been gathered by researchers in linguistics, psychology, ethnomethodelogy and artificial intelligence, these insights have not been integrated into formal grammars which display the breadth, depth and precision ef formal treatments of sentential syntax and semantics. Technical Report, Eindhoven: Philips Research Labs, M.S. We shall not attempt o specify this data structure in detail in the.
C88-2127@@ errors, but the err0r~ and the reasons for them have to be understood. In most applications, the, goal is to understand a sentence despite any errors, i. e. to somehow analyse the sentence. Complex features have been used by most schools ol linguistics \[KAPLAN R.M., BRESNAN J.
C88-2128@@ the use of a single grammar for both parsing and generation is an idea ~ith a certain elegance, the desirability of which several researchers nave noted. D,O, j\] This rule corresponds to the logically valid inference consisting of instantiating a rule of the grammar as a conditional statement. Experiments with a powerful parser.
C88-2132@@Deeodifying the vocal signal is a process that must take into account phenomena such as the eoarticulatory processes typical of continuous peech and the presence of many sources of variability of the signal (anatomic haracteristics of the speaker, emission speed, prosody and so on). References Barton, E.; Berwick, R., and Ristad, E. Computational Complexity and Natural Language. This yields a succesful recognition of the sentence.
C88-2142@@Although we had been engaged in developing an MT system of texts for several years (Mu project \[Nagao85, Nagao86\]), we were puzzle(| when we examined the data of dialogue translation gathered by the research group of ATR, which is a newly established research organization for translation of telephone dialogues and is now gathering dialogue translation data in various hypothetical situations. \[Structure bound translation\] A hotel near to Roppongi s good. For tile less important Parts I:ig.
C88-2150@@ Using the same grammar formalism, or even the same grammar, for both analysis and synthesis is usually regarded as an elegant, efficient and sometimes even as the psychologically most plausible approach to natural anguage parsing and generation. i~there is a sequence To. A decidable generation procedure presupposes the possibility of comparing the input structure with the partial f-structure of a derived partial e-structure.
C88-2153@@ Machine readable dictionaries (MRDs) contain knowledge about language and the world essential for tasks in natural language processing (NLP). logical Models, M!T Fmss/B~a0ibN Books: Cambddg% MA. These consumers need a variety of semantic information.
C88-2154@@  DLT (Distributed Language Translation). The DLT design offers a long. The content part is monolingual, ie the semantics i a question of IL-only.
C88-2155@@Under the Alvey Directorates research programme in natural language processing, an English-Japanese machine translation project was carried out at the Centre for Computational Linguistics, University of Manchester Institute of Science and Technology and the Centre for Japanese Studies, University of Sheffield. The user responds with the number of  any statement which is true, or T followed by the number of any statement which is false. "A Feature-based Categorial Morphosyntax for Japanese".
C88-2159@@ An approach is proposed to automatical ly  analyze Japanese dialogue containing zero pronouns, the most frequent type of anaphora which corresponds in fimction to personal pronouns in English. ;,ioned i~ Sect;ion 2 were with speakers being e.,_~itted ~ ubjects with zery few exceptions. To deal with all possible cases, ihrtt~er elaboration i  the inter-field omain of semantics, p~~t_~matic~, anddiscourse grammar is needed.
C90-1003@@Grammars pecifying the relationship between strings and semantic representations often have details of these representations embedded in them. In Logic Programming, K. L. Clark and S.-A. (1) Every man kicked a donkey.
C90-1005@@ Two text processing problems rely heavily on cooccurrence patterns the way that words appear together, possibly idiosyncraticly. "CONVINCING MANAGEMENT proved tough" is even harder since it presents a Necker cube situation (i. e. changing the interpretation of either word seems immediately to change the interpretation of the pair). HEIGHTENING OVER A 12-MONTH PERIOD.
C90-1018@@According to the goals of Translators Workbench as an integrated multilingual toolkit, the concept of the style checker includes multilinguality, and it uses other tools of Translators Workbench such as parser and lexicon, which provide SIMSA with more power than comparable approaches. are stored in a separate analysis file. G. Heyer, R. Kese, M. L0dtke and G. Winkelmann: Translators Workbench A toolkit for translators, in: ESPRIT 89.
C90-2002@@ Fl:e proliferation of on-line textual information has intensified the search for ctlieient automated in(h;x-. 8 2 what it can contain (C), what purpose it serves (T); and how it arises (A). It, renla ins an empirical question how well weak method,, can I;e employc(l to dis-criminate among thequa leo fano .n .
C90-2009@@ Chinese is a highly flexible language, The same meaning may be represented in many different Chinese patterns. Examples (d) attd (e) are legal interpretations. In such cases, a robnst parser for n:ttural hmguages can be designed.
C90-2016@@ The goal of the system, a part of which is described in this paper, was to synthesize speech utterances starting from a conceptual representation of the knowledge to be uttered (concept-to-speech system). His algorithm was designed in a bottom-up fashion. For each node with index i for each successor do, left to right: i f  the successor is a lexical item, synthesize it and append i as boundary marker if the successor is a significant node, assign index i+ l otherwise assign index i when all nodes on that level have been processed, overwrite the index that was written last with i The problem that a left-to-right process cannot know whether the following word is on the same level in the tree is solved by permitting to overwrite a marker already written.
C90-2022@@An interesting upshot of the strategy we propose is that it allows :\[or language independent generation. First, we could try to generate all sentences whose semantics are logically equivalent o the input semantics. First, a sign Sig~l is created whose semantics i as in (2b).
C90-2028@@  In any system for Natural Language Processing having a dictionary, the question arises which entries are included in it. (1) Given an lu X and two of its senses ocl and 52, is X ambiguous or vague with respect o S1 and $2  In (1) a sense of an lu is the meaning the 1ll has in a certain set of contexts. Every zoo ha~s an elephant.
C90-2029@@ iln the field of natural anguage analysis, Unification Grammars are a main research topic. E.g., the path "fset sem_role action" of node 01 has the value "zwemmen". If an efficient implementation f a parser for TAGs is desired (eg, in a natural anguage access ystem to an expert system), the existence of polynomial time acceptors for the word problem of TAGs becomes relevant (upper time bound O(n 4 log n), see \[Harbusch 89\]).
C90-2033@@The generation of strings from logical forms was studied intensively by \[Shi88,89\]\[Ca189\]. J o e NP n psr (Daughter,ttead,Mother); member(Daughter,Head.subcategorization). psv represents a Mother --, Daughter Head rule.
C90-2036@@ The correct program reads a list of misspelled words from the input stream (stdin) ,and prints a set of candidate corrections for each word on the output stream (stdout). *E* *S* " It is absurb and probably obscene for any person so engaged to und The first word of the triple was a spell reject; the other two were the candidates (in alphabetical order). (The system was trained on the AP wire liom 2/88 o 2/89; the results below were computed from AP wire during 3/89 9/89).
C90-2044@@  Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, CLUE WORDS,  DISCOURSE MAI:tKERS~ arid DISCOURSE PARTICLES \[3, 4, 14, 17, 19\]. The four shows provided approximately ten hours of conversation between expert(s) m~d callers. P i t ch  accents ,  peaks or valleys in the F0 contour that  fall on the stressed syllables of lexical items, signify intonat ional  prominence.
C90-2045@@  I)espite the long history of research and development, perfect or nearly perfect analysis of a fairly ,vide range of natural language sentences is still beyond the state of the art. Otherwise, the modifiee of the phr~e is ambiguous. Subject A is one of the authors who actually developed the grammar.
C90-2048@@ In this paper, we describe the structural characteristics of extended, planned 1 explanations involving complex physical devices and present a computational model for generating such explanations. Moore and W. R. Swartout. A reactive approach to explanation.
C90-2049@@ The bottleneck of sentence analysis, structural ambiguity, occurs when a sentence has several alternatives for modifier-modifiee relationships (dependencies) between words or phrases. Structural disambiguation f a sentence is done as follows. Tile node locations of ei and ej in t~.
C90-2050@@ Incremental generation (ie immediate verbalization of the parts of a stepwise computed conceptual structure often called "message") is an important and efficient property of human language use (\[DeSmedt&Kempen~7\], \[Levelt89\]). 1 this can graphically be represented asfollows: \[f,q s~mtcr: Fig. Incremental and Parallel Processing l~D-structures are processed at the DBS-level.
C90-2052@@ The notion of a rew:rsible MT system was first expressed by Landsbergen \[11\]. In B. J. Grosz and M. E. Stickel, editors, Research on Interactive Acquisition and Use o\] Knowledge. For a slightly more general definition, cf.
C90-2060@@ In this paper we describe the results obtained from the experiment with reversing a PROLOG parser for a substantial subset of English into an efficient generator. An argument X of literal pred(  1 347  pred l ,  and predt  precedes pred  on the rhs; 3 or (E) it is "in" in the head literal L on lhs of the same clause. We assume that symbols Xi in definitions (P) and (R) above represent terms, not just variables.
C90-2067@@ Automated language understanding requires the determination f the concept which a given use of a word represents, a process referred to as word sense disambiguation (WSD). References AMSLER, R. A. Each concept iin the network is linked, via bidirectional activatory or inhibitory links, to only a subset of the complete microfeature s t. A given concept theoretically shares everal microfeatures with concepts to which it is closely related, and will therefore activate the nodes corresponding to closely related concepts when it is activated :itself.
C90-2074@@  From the knowledge ngineering perspective in artificial intelligence, a natural language processJng (NLP) system can be seen as a knowledgebased system in which major concerns are the discovery, acqlfisition and maintenance of knowledge or rules \[2\]. \[10\] Michalski, R.S., A theory and methodology of inductive l axning, in: Machine Learning, Michalski et al (eds. 2 An  Overv iew o f  XMAS XMAS (Xpert in Morphological Analysis and Synthesis) is a learning system \[12,13,14\] which consists of a learning element (Meta-XMAS), a knowledge base (KB), and two inference ngines of a morphological nalyzer (MOA) and a morphological synthesizer (MOS).
C90-3002@@  Recently a number of formalisms for grammatical description have been proposed with the aim of overcoming the expressive deficiencies of simple unification based formalisms like PATR-II. A semantic theory of obligatory control. Acknowledgement This work has been carried out within the framework of the Eurotra R&D programme for machine translation financed by the European Communities.
C90-3003@@One question that arises fairly frequently however, at least in the context of discussion about two-level morphology, is roughly, "Why dont you use normal generative phonological rules" ie, rules of the type that are taught in elementary linguistics classes. The vowels \[e\] and \[~\] also provide complications for the revcrqal of the vowel deletion rule. 9" That is, if \[rj\] on the surface can come from e i ther /n /o r / ( I / ,  then the rule would necessarily be optional in the reverse direction.
C90-3008@@ Extraction and representation f text meaning is a central concern of natural language application developers. In KBMT-89 semantic interpretation occurs partly ~3his term is due to Yorick Wilks, and is distinct: from machinereadable dictionary, which is simply a printed dictic~laly stor~ electrcmically, L 112 NL Input 27 e ~inU cgl.m i;t~; fae o ) -\] ! In the partial case of a particular application area, the representation problems ate alleviated.
C90-3013@@This paper describes the approach taken to the unification of disjunctive feature structures in an experimental bottom-up shift-reduce Japanese aaalyser called Propane, for Prolog Parser using the Nadine Grammar. O~c0mplicati0ti  hat arises in parsing written Japanese ~s that wor d born}danes a.re not mdmated explic~\[ly. when many disjunctions remain, l.,e far more expensive Ihan looking for a single realizatiola.
C90-3014@@ Knowledge Base System Using Unification-based Formalism A Case Study of Korean Phonology Hee-Sung Chung Korea Academy of Industrial Technology(KAITECH) 70-6, Yangjae-Dong, Seocho-Gu, S6oul Korea syllable consists of all the segments that precede the neeleus and are tantosyllabic segments that follow the necleus. A language has its own sound patterns. 1 Korean syllable structure 3.
C90-3017@@There is a natural appeal to the attempt to characterize parsing and ge~;era~ion i a symmetrical way. Cons ider  now the fo l lowing e lementary def inite clause program (P0)t2: a(A) :a(B), ~(B.A). phrase_p and phrase g are both equivalent o phrase.
C90-3019@@The Prob lem The choice of target language realizations in machine translation or in multilingual generation is conditioned by constraints involving different levels of linguistic description for the individual anguages. of  an  *entity* and i~s syntactic realization variants, namely a relative clause or an embedded participle. Although most of these approaches allow for a description of linguistic phenomena t each individual level, it is hard for them to explicitly express interactions between levels without using directionality.
C90-3034@@This representation is successively modified by a recursive algorithm until all the quantifiers present in the input have been dealt with and given scope over some part of the output. I present a modified algorithm which avoids such intermediate forms. 191 The basic operation of H&S is similar.
C90-3035@@ It is customary to judge the success of scientific models by their agreement or otherwise with the observed data. We cast CFGs in a logical formalism. e. The same generalizations can be captured via transformations or metarules.
C90-3037@@ Coordi:uation is a particularly troublesome phenomenon to account for in theories of syntax based upon phrase structure rules. 210 4 Using the Sequencing Rule once more, we can prove the whole given a proof of r : l  i ( \ [~:~\ ] ) )  . The rule declares that if one string defines a transition from Category0 to Category1, and another defines a transition from Category1 to Category2, then the combined string defines a transition from Category0 to Category2 ie Co String0 Ca, CI String 1 C2 Co String 0  String 1 C~ (here e denotes concatenation of word strings eg "Ben" "sits" is equivalent to "Ben sits") For this example, we can instantiate the Sequencing Rule as follows: i j, \] " eo" cO, co \[ \] I t :  ,,B0o \[ \] 7At this stage no restr ict ions have been imposed upon the ordering of the rules, and more than  one proof  tree is possible.
C90-3038@@ For the realization of an interpreting telephony system, an accurate word recognition system is necessary. (a) Local syntax errors. \[al T.J.Sejnowski, C.R.Rosenberg, "NETtalk, A Parallel Network that l,earns to Read Aloud", Teeh.
C90-3044@@  Use of extracted information fiom examples or example-based translation is becoming the new wave of machine translation. S WD), score( T~I E, TW D ) ) For example, the score of the translation in  6 Examples The English verb eat corresponds to two Japanese verbs, tabcrv and okasu. MBT2 chooses htberu for he cat.s t~ota, toes and okasu for sulfuric acid cals i ron.
C90-3045@@The statements of dependencies and recursion possibilities in a tree are factored, the former following from primitive dependencies in elementary trees, the latter a consequence of an operatkm of adjunction of trees. The nonterminal names in the logical form grammar e mnemonic for Formula, Relation (or function) symbol, Term, and Quantifier. A uniform architecture for parsing and generation.
C90-3046@@ Sentence analyses are essentially reasoning processes which derive assumptions/expectations tom observed input sentences. "Taro goes to a park. I drank (something), too.
C90-3048@@ Machine Translation (Mf) or natural lang~lge translation in general is a typical example of the under-constrained problems which we often encounter in the field of artificial intelligence 1. In D. Maxwell, K. Schubert & T. Witkam (eds) New directions in machine translation, Dordrecht: Foris, 109-120. In a context such as business correspondenc e , it might be the case that much less information is necessary to identify the relevant triple than that conveyed by the actual linguistic expressions and that, because ach individual language usually has its own conventions which letters must follow, the actual informational contents of the two expressions might be different.
C90-3051@@and Introduction This paper focuses on the problem of incremental parsing (and to some extent interpretation); in particular, how reason-maintenance techniques can be used to achieve a strong notion of incrementality allowing for piecemeal construction, revision, and comp;~rison of partial analyses. An analysis (or interpretation) of a phr~e, sentence, etc. A characteristic of these fra.meworks i  that they only handle semantic-interpretation alternatiw,s and do not (attempt to) integrate this with parsing.
C90-3053@@ This p.qper deals with a general phenomenon of (machine) translation. In: Stelner, E.II., P. Sehmidt and C. Zel insky-WibbelL From Syntax to Semantics. /~The mat is t~nder the cat.
C90-3063@@The use of selectional constraints i one of the most popular methods in applying semantic information to the resolution of ambiguities in natural anguages. 1 The statistics were collected from part o\[ the corpus, of about 28 million words. Then, a postprocessing algorithm identifies the various relations in the parse tree.
C90-3068@@ Arabic has some features which lead to comlpex syntax different from those of the European languages. b. Verbal sentence: a sentence that contains a verb which precedes the subject. The few attempts to give a formal mode~ for Arabic sentences were based on transformational generative grammar \[111, \[2\], \[3\] and o thers -but  some linguists adopted more recent l inguist ic models such as Lexical Functional Grammar \[4\], dependency grammar \[5\] and functional grammar 16\].
C90-3079@@ Weather forecasts (WF) are the subject of various manipulations. Inconsistency e l iminat ion  TWO assertions (W~ , ~ ,R 4) and (~ ,~,R  z) are  said to be contradictory i f  they predict inconsistent weather characteristics (eg sunny weather and overcast) for overlapping regions and time periods. In terms of the conceptual model i t  means that i f  W i and are ~nconsistent, O,c~ and 4c~en the ~econd assertion is replaced by the assertion (W z ,~-~,~-~) ,  where &-5 and Rz-Riare calculated on the basis of the corresponding submodels.
C90-3088@@ A long English sentence, from the parsing point of view, is defined as a sentence which has complicated syntactic structure or has too many words in it. % e. Average parsing time for these long sentences : 2 min/sentence For ERSO-ECMT, the rate of correct matching is about one third of the total long sentence. .If the resultant segments are still with length greater than 40, does part i t ioning recursively on them, until no more pattern can be used.
C90-3101@@The translators previous output would be stored as hypertext, with the parallel texts as far as possible aligned. TUs English phrase French phrase 1-2-6 2-3 2-4 2-3-4 4-5 6-7 tmanimously con firm t~ard of PAC the board tx~ard of ILK|| dd \[C. 5tre unanime duns sa confirmation de coaseil du PAC le conseil conseil de mandat The rcmaini~g s~ep in BKB construction is the coding of references. Each ellipse corresponds to a subtree.
C92-1008@@ l)ufing tim last years it has become increasingly apparent that dialog and text understanding systems must account Ior connectivity relations that extend over sentence boundaries. E~:hframe identifier (in bold face) is assigned alist of slots (enclo~d by angular brackets). The underlying theoretical foundations are due to D. Hillis, a former M.I.T.
C92-1010@@  Morphological cbussilicat, ion of natttral languages according to their word StltI(ttlrt+s idaces languages like Turkish, Finnish, and lhmgar ian Io a class called "ag ghfl+inalive langua.ges". The allophones of {At  are A and E, where {It represents I, i, U, or {r. The vowels O and (} are only used in root inorl)hemes (especially in the first syllable) of Turkish words. of ruh,s for I.he two IIKLill root.
C92-1014@@ Pedagogical grmnmar Nalks typically organize their descriptinns of the inflectiomd morphology of a langtmge in terms of paradigms, groups of rnlas which characterize the inflectional behavior of some subset of the languages vocabulary. We have built t~firly complete representations h)r English, French, and Gennan, and have begun invcsfigating Spanish. Au Overview of KRI,, a Knowledge l,(epresentation l.auguage.
C92-1015@@While there is an element of truth in this, it is also true that the predilection of constraintbased grammarians for a string-based phonology has predisposed them towards egmental phonology, in which procedural thinking is at its peak. It specilies that there is a (possibly empty) sequence of nasal tlon-obstnmnts, tolluwed optitmally by a nasal 7Observe that m, n and  Notice that with this simple regular expression the notions of autosegruental SPREADING and BLOCKING are captured, This approach will therefore generalise to such phenomena s vowel harmony. It has the li31lowing segntent inventory: stops: p, t ,k ,   or the end of the word.
C92-1019@@ Chinese sentences arc cx)mposed with string of characters without blanks to mark words. A lexicon with such a s~e of course would still leave out many compounds and proper names. Fan and W. H. q.t;ai," Automatic Word Identification in Chinese Sentences by the Relaxation "/~chniqne," Computer Processing of Chinese and Oriental Languages, Vol.
C92-1024@@ Categorial Grammars (CGs) consist of two components: (i) a lexicon, which assigns syntactic types (plus an associated meaning) to words, (ii) a calculus which determines the set of admitted type combinations. A convention of left association is used, Bo that, eg ((t~np)/pp)/np may be written s\np/pp/np. The elimination ru le/E states that proofs of A/B and B may be combined to construct a proof of A.
C92-1026@@1,eature bundles have long been used l)y phonologists, and more recent work on so-called feature geonletry (e.~. One general motivation, which we shall not e.xplore here. Making use of the selectors, this is simply $2 !o ra l  !
C92-1027@@ The present finite-state approach to syntax should not be confused with eg attempts to characterize syntactic structures with regular 1. Then use the strategy E with the remaining rules. The following is one of them: @8 the DEF ART 8/ program V PRES NON-SG3 8FINV 8MAINV 0 run N NOM PL 8PREDC 8@ This one is very ungranmmtlcal,  though.
C92-1029@@ Analysis of a long Japanese sentence is one of many difficult problems which cannot be solved by the continuing efforts of many researchers and remain abaudoned. I f lWs  are inflected, infinitives are compared. In tile following, 1 indicates the number of bunsetsus and a(n, n) is a KB.
C92-1030@@  Uuifieation-based framework bins been an area of active research in natural anguage processing. Kernel verbs occur first in a predicate phrase sequence. Voice auxiliaries precede all other auxiliaries, and within this category, the causative auxiliary (sa)se,u precedes the passive auxiliary (ra)re~t.
C92-1032@@ One of our most robust observations about language  dating back at least to the seminal work of Miller and Chomsky \[MC63\]  is that rightand left-branching constructions such as ( la) and (lb) seem to cause no particular difficulty in processing, but that multiply center-embedded constructions such as (lc) are difficult to understand. In R. Luce, R. Bush, and E. Galanter, editors, Handbook of Math. Summary  o f  the  Argument  For expository purposes, we begin with tile discussion in \[AJ91\].
C92-1033@@A full sentential parser that produces complete mmlysis of input, may be considered reasonably fast if the average parsing time per sentence falls anywhere between 2 and 10 seconds. rn (SR, P) :t imed out, !, sk ip  (SR), s to re  (P) . "User manual of Fidditch, a deterministic parser."
C92-1034@@It combines elementary lexical trees with two operations, adjoining and substitution. We can now state the passive rule: PASSIVE input : LEI output : LEo passive E LEo.CLASSES CHANGE-AR1TY(LEi.CLASS) E LEo.CLASSES LE~.CLASS E LEo.CLASSES *I) v (pp) p np E LEo.tree-description by Suppose we let LEi.class to be DITRANS1. A lexicalized trec adjoining grammar for English.
C92-1037@@Previous work on the semantics of bare plural NPs has largely focussed on the contribution they make in habitual sentences like (1) Mar~ eaiJ peaches. The following analysis of (7) John a~e a peach. We propose to generalise u step further, making NP semantics map functions of type (e -4 t) --~ t to truth values (ie that they arc of type ((e --, t) --4 t) --, t)).
C92-1038@@E-mail address i  R. DaleQed.   In previous work \[Da189,DH91,Rei90a,Rei90b\] we have proposed algorithms for determining the content of referring expressions. A General Organization ofKnowledge for Natural Language Processing:.
C92-1049@@  Recognizing the roles that utterances play in a dialogue and how the utterances should be interpreted in the context of preceding dialogue is a crucial part of a robust model of understanding. tlowever, since this is an e-action, evidence is necessary to infer this action. In Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, editors, Intentions in Communication, pages 77-104.
C92-1051@@  Kuno said in \[7\] that in Japanese discourses we have to omit as many components in a sentence as possible unless we get any ambiguity. Every dement of Cf(U;_I) must be realized in Ui. In this case "you-rid used in the subordinated clause makes Johns fearing ("kowakatta") state t)e observed from not John but some other not yet specified person, say, X.
C92-1055@@ Ambiguity resolution has long been the focus in natural anguage processing. Then,  a discrimination-oriented function, namely g (. Moreover, the parameters e tinlated from the training corpus may be quite differ from that obtained from the real applications.
C92-1056@@ The problem of word-sense disambiguation is central to text processing. The configuration C is is constnacted by replacing the old sense of the ith word by the sense S o. It is E that we seek to minimize.
C92-1057@@ From the point of view of decidability, the parsing problem with a definite clause grammar is much more complex than the corresponding problem with a context-free grammar. ZFor the interested reader, the given system expresses (using "conventions of summation" familiar in tensor algebra) the Acknowledgments Thanks to C. \]3oitet, M. Boyer, F. Guenthner, F. Perrault, Y. Schabes, M. Simard, and G. van Noord for comments and suggestions. , X~ ) --+ \[term\] Af , where n E N is a co-unit nonterminal of arity k, k E N. where \[term\] E V and where .hi is a finite sequence of terminal goals of V, of nonterminal unit goals of U, of auxiliary predicates of Q, or of nonterminal co-unit goals of N. 6.
C92-1058@@  In this paper we present a general framework for parallel parsing algorithms. Therefore we redefine complete(v) d.~ yield(~) e U*. Let T(G) C J:bt be the class of trees that can be constructed from P; ie, if some node is labelled A and its children X1, .
C92-1061@@  Nowadays, two formalisms are very attractive in Natural Language Processing: Categorial Grammars, and Dependency Grammars. node is the target of a type-i edge. From now on, L and R will denote respectively: the left hand side and the right hand side of a Construction Net.
C92-2067@@ There are several reasons performance measure is required while building machine translation systems. Statistics summaa-y of distance measure (Edit ing cost scheme: Insertion 5, Deletion 1, Rep lacement  5 and swap 6) Origi~ad status Best case Cost per word Total t 312,266 289,290 Cost per sentence 51. Because the design goal of a system is to optimize some gain or minimiz~e some cost, a good performance measure is definitely an important factor on the improvement of the system.
C92-2068@@ Despite recent efforts in improving graph unification algorithms, graph unification renlains the most expensive part  of parsing, both in t ime and space. 21 I.e.. the cxisthig copy of the ilodc. Thus,  althtmgh the space and t ime reductioo attained by structure-sharing (Jail t)e significant.
C92-2069@@  A common need in natural language information retrieval is to identify the information in free-form texts using a selected set of canonical terms, so that the texts can be retrieved by conventional database techniques using these terms as keywords. The~e approaches hare a common weakness in that they can not capture the implicit meaning of words (or only captured a little), and this seems to be a crucial problem. A Statistical Approach to Machine "lYanslation.
C92-2072@@The central idea is that publishers tyle rules can be maintained as rules in a knowledge base, and a special inference engine that  encodes trategies for examining text can be used to apply these rules. h. *There are a dog. tAIso of the Department of Artificial Intelligence at the University of Edinburgh; e-mail address is R,Daleed.
C92-2082@@  Currently there is much interest in the automatic acquisition of lexiea\[ syntax and semantics, with the goal of building up large lexicons for natural lain guage processing. Discovery  o f  New Pat terns  How can these patterns be found Initially we discovered patterns (1 )  (3) 5y observation, looldug through text and noticing die patterns and tile relationships they indicate, lu order to find new patterns automatical ly, we sketch the following procedure: 1. l)ecide on a lexical relation, R, that is of interest, eg, "gro up/member"(iu our formulation this is a subset of the hypouylny relation). A more common problem is underspecification.
C92-2088@@  It has become widely accel)ted that developing a large scale semantic dictionary is indispensable to future natural language research. Example 1 E: I hung my coat on the hook. A translation example is given in Example 2.
C92-2096@@:  MOTIVAT ION Ceaain types of questions require in response a statement of a conclusion and arguments osupport i . An assertiou of the form E ( c las  s. p rogramaning  + ) is a. positive valuation of the course Oil tile programming  scale. It is also and over all a rhetorical decision.
C92-2097@@  Many applications dealing with spoken-language, such as automatic telephone interpreting system, need efficient and robust processing. When Eij is close,st o I, TEl is selected as file most plausible TE. 7, generates (2) with a total distance value of 0. .
C92-2102@@ Several focal issues have emerged in machine trans lation (MT) research as the result of recent intensive studies in the field. In terms of discourse representation theory, e is tim discourse referent of which the logical form is a description. (la) Site wears a bat and shoes.
C92-2105@@In many situations of communication a speaker need not to worry about the possible ambiguity of what she is saying because she can assume that the \]tearer will be able to disambiguate ile utter ance by means of contextual information or would otherwise ask for clarification. Finally, for the proposed strategy to i)e meaningful, it nmst bc the case that r*;versible grammars are being used. \[n his model parsing and generation are performed in an isolated way by means of two different granunars.
C92-2114@@ Lexical choice cannot be made dining text generation wifltottt taking into account he linguistic context, both the lexical context of inmmdiately surrounding words and rite larger textual context. This process obeys a principle of economy. On the Place of Words In lhe Generation Process in C.L.
C92-2115@@The transfer process in macbine translatiou systems is, in general, more complicated than the processes of analysis and generatimt. hl this (:~e, such a node is lexlcMized by c~L.dida.te tr~tnslation words. SimTran is RCT coupled with a similarity calculation method.
C92-3134@@  There are a great number of different dialogue styles which people use even in very restricted task.oriented domains. A Computer Model o.f Conversation. E laborat ion :  If an explanation has not been given in enough detail, the explainer may fill in the gaps.
C92-3136@@  In discourse processing, two major problems are understanding the underlying connections between successive dialog responses and deciding on the content of a coherent dialog response. uses the rule: O~e re.on wh~ a plan shouldn~ be ezecuted is that it conflicts with assenting a more desirable plan. Achieving G has an undesirable effect S. Reasons why achieving oal G i~ desirable: Achieving G in an enablement for another goal.
C92-3138@@  The current state of the art in natural language processing calls for components Callable of sophisticated deep semantic analysis, modular representation ofresources, and re-usability of those resources across different NLP applications. 5 Conc lus ions  and  l, Mture  Work  Tile (:lose fit between the linguistic description required in a TFS-based architecture and those I)eing pursued within SFG have motivated a detailed investigation of the nmtual compatibility of the representational nd formal mechanisms and linguistic ACTES DE COLlNG-92, NANi\]. A full specification would contain ...... y ....... details (cf.
C92-3161@@ Shalt2 is a research prototype of a knowledge-based, multi-domain/multi-lingual MT system. A generalization from e~ch ~ to z provides the opposite mapping. The Jap~mese gr~.nmax was originally written by Mitamura and T~keda\[2\].
C92-3162@@ The production of abstracts from input source texts, using computers, is a subject in natural language processing that has attracted much attention and investigative study. \[3\] Mann, W.C. and Thompson, S.A. "Rhetorical Structure Theory : Description and Construction of Text Structures." A single-PC demonstration version is also available.
C92-3165@@  It has been continuously mentioned thatt some kind of latnguage knowledge is essential in goodquality speech understanding. Entries "r n" indicate tile action "reduce constituents on the stack usiug rule n". Parsing spoken language: A semantic caseframc approach.
C92-4188@@ One of the major bottlenecks for large-scale NLP applications such as the METAL MT system 1 is the acquisition of their lexicons 2. distinctinns in verbs), we a~e mainly interested here in tile actual output of the lexicographic work, both quantitatively and for representatinn issues. Walker,  D., A. Zampoll i  & N. Calzolari (eds) (forthcoming) -Automating the Lexicon: Research and Practice in a Multilingual Environment.
C92-4189@@The procedure we nsed was based on the assumption that the semantic relationship between the headword and its genus should be reflected m their 1,1X)CE semantic categories. simihtr connection botwtam the ord*r in which word ~n~ Jro listed And the, fr~luoney with which they arm uJcd (in LDOCE) holds for the l i t  edition u well. In that schelne, file pragmatic cedes were arranged in a tree structure in which each node of the tree is a single pragmatic e(xle.
C92-4190@@ the semantic and syntactic properties of these different classes of adverbs. "Generating a Lexical Database for Adverbs." Thus the sentence adverb "evidently" can appear in positions uch as the following: S ADV NP VP A V PP Evidently John walked in the store.
C92-4195@@The system needs for each word of its reference vocabulary two types of reference patterns:  phonetic transcriptions of the main pronunciation variants of the word. The signiticant step to our current version was the classification of an extensive German morph list based on about 9,(100 nlorphs compiled by the Institut fi~r deutsche Sprachc m Mannheim (Germany). A file containing the orthographic words to be segmented into morphs.
C92-4199@@  Word Identification (WI, also known as Segmentation) has been an important and active issue ill Chinese Natural Language Processing. Just llke personal names appear with title, place names can be identified through the unit such as xian ~ (county), shi i~i (city), jie ff~ (street), lu t~ (road), etc. Furthermore, surnames are among a limited set.
C92-4201@@ It is widely known that the word formation mechanism of compounding is highly productive, in Ger man as well as in English, and that efficient strategies have to bc ,lcvelopcd to dcal with this linguistic phe nomenon in any kind of NI,1 ~ system. suits ill a l+Ittllre paper. on Theoretical and Methodological Issues in hil" o/ NL, pages 87 (.
C92-4202@@Therefore, the analysis and generation components ofa knowlodge-based MT system must have at least he following functional parts: a grammar for the language, a lexicon for the language, a shared set of domain concepts, and rules that map syntactic structures onto semantic structures (or viceversa for generation). Upper case frame names (e,g,, *BREAK) indicate conceptual frames. As a result, lexical mapping may also require semantic feature assignment.
C92-4203@@Although they are usefal to translate large amounts of texts roughly and rapidly, high quality translation is impossible. t Best Match ~ Retrieval J \[Retrieved EXaml)h, \] YOll~re ~z great actor. memsure, bat makes a convenient starting point.
C92-4207@@In this process, tree will use many kinds of infitrmaLion. Finally SPR INT  draw a world image (m the graphic display. The area of spa(e-language r lationstrip contains a lot of hmd issues, and some liroblems related to this work are mentioned tMow.
C92-4211@@ The traditional approaches of natural anguage parsing are based on rewriting rules. A(zrf, s DE COLING-92, NANTEs. If each word in a sentence can be found in our corpus and the corresponding dependence r lation can also be found in our knowtcdge base, it is also feasible for CBCP to perform syntactic parsing in an open corpus.
C94-1002@@ Correctly determining number is a difficult problem when translating from Japanese to English. The translations before the proposed processing was implemented are marked O1.D, the translations produced by AI;I-J/I,; using the proposed t)rocessing are marked NEW. They cannot take GEN a  because they cannot be moditied by a.
C94-1013@@  Machine Translation (MT) is considered the paradigm task of Natural Language Processing (NLP) hy some researchers because it combines almost all NLP research :treas: syntactic parsing, semantic disambigt, ation, knowledge rel)reseutation, language generation, lexical acquisition, and morphological analysis and synthesis. PAC, U.S.A., JEIDA Machine Translation System P,tsearch Commitlec, Tokyo. As illustrated in Figure 5, traditional transfer-based MT systems tart with general coverage, and gradt, ally seek to improve accuracy and later fluency.
C94-1014@@ EBMT is based on the idea of performing translation by imitating translation examples of similar sentences \[Nagao 84\]. T I lE  API"LICATION EVALUATION The development of the nmtching method presented in this paper was part of the research work conducted under the LRE I project TRANSLEARN. "Pilot Implementation of a Bilingual Knowledge Bank".
C94-1015@@However, in order to make tilt best use o1 the a(.lv:.lnlages of an example-based flamcwork, it is essential to effectively integrate an example-based method anti source language analysis. "leaves verb-propn Kyoto at eleven a .m." ;e vii\]ties were col//pu,ed based on Ihe present transfer knowledge of the T1)MT system.
C94-1017@@We have now completed the first imlflemcutalion of it mock-up, I,I1)IA-1. I.T\[a foment do . Example-Based Approach to Machine Translation.
C94-1023@@The perfonnance of a probabilistic model is affected by the estimation error due to insufficient raining data and the modeling error due to lacking complete knowledge of the problem to be conquered. As stated before, CAP, T regards all selected features Its jointly dependent. On the contrary, due to the conditional independent assumption for P ( f l ,  " , .
C94-1024@@The ta.ggels are all local hi the sense that they use inform~tion front a limited context in deciding which tag(s) to choose for each word. Parsing English will  a laid{ (hmuinar. As is well known, these taggers are quil;e, successful.
C94-1025@@Their features concern part-of-speech (POS), gel,der, number, etc. E.g., if gen:MAS appears in a certain (sub-)context the,, this (sub-)context will l,e preselected for gen:l:EM too. A Statistical Approach to Machine q~ranslation.
C94-1026@@  Real texts provide the alive phenomena, usages, and tendency of langnage in a parlictflar space and time. (s) lOwrev I,)mclion (E): Assume each sentence has a weight, which is measured by tile nmnber of critical POSes. 7he BDC Chinese Tagged Corpus, Taiwan, R.O.C.
C94-1027@@ Words are often ambiguous in their part of speech. We used a gain threshold of 10. Overlearning means  that  irrelevant features of the train ing set are learned.
C94-1029@@ This paper aims at presenting a computational morphology model which can handle the non-linear phenomenon of Semitic morphology. l{ule 3 sl:at.es the deletion of lexical \[e\] in {re(we} in the. They  conclude that their representation is closer I,o the linguist,ic mmlysis of lIarris tlmn McCarthy.
C94-1030@@Fhis clmin is judged t,o /)e wrongly deleted ~t the. they are written by many kinds of characters, especially thousands of "kanji" characters. rihis procedure detects that k characters a.rv.
C94-1032@@In recent years, we have seen a fair number of l)al)ers reporting accuracies ofmore than 95% for English part of speech tagging with statistical language modeling techniques \[2-4, 10, 11\]. : "Tagging Text with a Probabilistic Moder, ICASSP-9I, pp. That is, the top-N candidates are exact.
C94-1036@@ AND MOTIVATION An English sentence has several words and those words are separated with a space, it is e~usy to divide an English sentence into words. And e sentences have both morphemes it (the corpus and the ones not in the corpus. For example, I am Tom.
C94-1043@@  The acquisition and representation of lexical knowledge from machine-readable dictionaries and text corpora have increasingly become major concerns in Computational Lexicography/Lexicology. Feature Structure (I"S) descriptions of word senses such as tilt. \[IO (at)l: The ship docked (at Glasgow) Unfortunately, an indication of diatheses which relate the various occurrences of tt,e verb to one another is rarely provided.
C94-1044@@ Needs for large scale lexical resources for Natural Language Processing (NLP) in general and for Machine Translation (MT) in p:uticular, increase very day. I~I)R Electronic I)ictionary Technical Guide. Accept ion-basod lexical organization After studying and comparing different projecls of lexic:d dat;dmses, including I"J)R (E\[31~.
C94-1048@@  When vocabulary cmmot be found in bil ingual dictionaries, it is f reqnendy obt;ain(;d by using a l:hird language as an intermediary. Type E There are no ECs. does not always have a unique answer.
C94-1049@@  Word vectors reflecting word meanings are expected to enable numerical approaches to semantics. Portion of a reference network. Next, each coordinal.e is changed hire its deviation in thc ~ vector: where t The logarithm with -t- is dellned to be () for an arg;ument less than 1.
C94-1052@@ Recently, several approaches have been made to extend lexical unification-based formalisms to deal with multi l inguistie phenomena in order to be used in applications such ,as Machine Translation \[7\]. **  E~pec io  de  ora l  con o lg lmo5 #~tone io~ co lo r~tes ,  U que  ~e bate  co~ choco la te . ocepc i& l :5  ** m. *+ Reel  on de  bat le .
C94-1053@@  This pallor presents a, text planner for l,h, w,rlmliz~ttion of natural deduction (ND) st, yle proofs \[Gm,:Uq. It enforces also that certain PCAs be used to mediat.e between 1)arts of l)roofs. (Derive Reasons: (((ELE a U) explicit) ((SUBSET U F) omit)) Conclusion: (ELE a F) Method: (Dof-Subsot omit)) Our surface generator TAG-GI~N \[Ki194\] produces the ul, terance: "Since a is an element of U, a is an element of F." Notice, only the l\[~ason labeled as "explicit" is verbalized.
C94-1056@@Abstract generation is, like Machine Translation, one of the ultimate goal of Natural Language Processing. Abstr~t  generation is realized ~s a suitaMe application of the extracted rhetorical structure. As for the technical papers, tile average length ratio( abstract/original ) w;~s 24 %, and tile coverage of tl,e key sentence and the most important key sentence were 51% and 74% respectively.
C94-1058@@decisions are made on the basis of the specified input. An explicit parameter xpressing tile desired egree of fluency influences the time--limit.s. During simultaneous interpret a.tion requests are rather unusual.
C94-1061@@ In this paper, we propose a grammar model that combines lexical organization of grammatical knowledge with lexicalized conlrol of the corresponding parser in an objectoriented specification framework. Therefore, computationally and cognitively plausil)le models of nalural language tmde,slandin,e, should account for parallelism at the lheoretical evel of language description. The evolving technology of classificationbased knowledge t prcsenlallon systems.
C94-1063@@ Natural languages exhibit significant word order (WO) variation and intricate ordering rules. The following atomic we constraints have been defined: Precedence constraints:  We also allow atomic we constraints to combine into complex logical expressions, using the following operators with obvious semantics:  Ifthenelse ( i  f thene  1 s e) Our we  restriction language is, of course, partly logically redundant (eg immediately precedence may be expressed through precedence and adjacency, and so is tim case with the last two of the operators, etc.). First, the reflexive particle never occurs sentence-initially (information we cannot express in ID/LP); in EFOG we express this as: ip: not(f i rst(part(ref l ) ,s))  .
C94-1070@@lattice. N, is comptlted as the whiteboards third layer, using a C-based ou-tine J-l{ dictionary.  Speech translation systems must integrate components handling speech recognition, machine translation and speech synthesis. The whiteboard can be the central place where graphical interfaces are developed to allow for e~Lsy inspection, at v,uious leve ls  o f  det~fil.
C94-1071@@ Fhlite state devices have recently attracted a hit of interest in computational inguistics. Thus the rules we deal with c:tn roughly he seen as sentpnt e strllcLiires where itt least oi1(! }lore 3AO tWO dilferenl, ways of ol)t.ahiing new I)msing I)rogra.llls.
C94-1079@@Principles are constraints over X-bar structures. In d~e lexicon, a phrase "wl . {lhe result o\[ I~ixe colnbinM;ion is a.
C94-1080@@ In this paper, we propose a grammar model that combines lexical organization of grammatical knowledge with lexicalized control of the corresponding parser in a coherent object-oriented specification framework. Now, a relation between event types is delined hy causes! Turning to actual events now, we define an actor a/as being composed of an identity n (taken from lhe set of natural numbers, N), a state e Sand a behavior e 9{.
C94-1084@@A technical lexicon consists of simple as well as complex lexical units. But, we tat~e into account he cases where base-MWU structure is broken ms well as their variants. Ext rac t ion  of  base  MWU To extract and count the occurrences of base MWU (under their various forms) we use their morphosyntactic structures.
C94-1091@@ A classifier has a significant use in Thai language tbr construction of noun or verb to express quantity, determination, pronoun, etc. The roles of c lassi l ier in Tha i  hmguage in Thai language, we use classifiers ill wuious situations. (see examples in a. and b.)
C94-1092@@ Each rno,fl.h the (~OBUILI) tean-i supplies an apl)roximately 10 million word b;ttch of lw:trl~.tlp coded running text (see Appendix A) in ASCII format. Ideally, (:v0ry rill(: is tested against ~t represelltative SalYil)le frolri a, corpus. \] wish to thallk also l)r(fl .
C94-1099@@ The construction of a knowledge base related to a specific domain for a NL understanding system is time consuming. The  Argument  Rest r i c t ions  The argument restrictions are instantiated versions of the signatures for each predicate. The signature file, then, is m~t just domain-independe.nt.
C94-1100@@ Tile task of information retrieval is to extract relevant documents from large collection of documents ill response to a users query. Alleg~dions t~l lnIil|agelnelll railings, as well as Felofls Io StlCh charges ~ue relevanl. RI~IEI~J,;NCI(S Church, Kenneth Ward and flanks, Patrick.
C94-1101@@Iris theory included the co,lcept tlutt ~ hmgnage could be a,pproximated by an nth order Markov model by n to l)e extended t(~ infinity. \vllich C. E. Shtulnon esthrutte.d for English, it cotktimted to decrease to zero. 5 J (give eflbct) have relatively high frequencies and there are no othCw significant combinations in the n-gram tM)le with r J~gNI as the prefix, l-)kN~(I (ill ;I.ll(l ()lit Imspital) b~we ahnost all the t ime I -~@ b~9.
C94-2106@@ Various machine translation systems have approached the stage of being put to practical use. The third example shows a pattern pair indicating that the equiwdent of the Japanese expression "N 1 (FOG) g(l N 2(C()NCRETF~ OBJECIS or PLACES)we tsutxumu" is the English expression "N1 veil N2". Our machine translation system, AI,T-J/E, uses two types of Japanese to English transfer pattern dictionaries based on verbs: the semantic valence pattern transfer dictionary and the idiomatic expression transfer dictionary (Fig.
C94-2107@@S(Slllo{:Iic{ss ill :lal)an,se. \[n GIlIl j i s strllctlll:e, solne node (all have more than two (laughter nodes to make more coutl)lexsentence. \[Takubo 87\] divided level A into two levels.
C94-2112@@ Recently, work in computational semantics and lexical semantics has made an interesting shift. Thus, only Nl)s having associated tra.nsi+ tion events will allow coercion a,n(\[ control. Notice that one might expect there to l)e raising constructious involving coerced NP com\])lements.
C94-2113@@ The problem of word sense disambiguation is one which has received increased attention in recent work on Natural Language Processing (NLP) and hfformation Retrieval (IR). For example, consider the verb "mo(u)lt", whose single sense in the American Heritage Dictionary, Third Edition (AHD3) corresponds to two senses in Longmans Dictionary of Contemporary English (LDOCE): 2 AHD3 !y: part :or a!l of a coat or... covering; sucli:as feathers;cuticle or  skin  LDOCE (1) "(of a biid ) to !0se o r thro TM off  (tleatberS) at the seasoii when new feathers grow . S~nsc \])jsambkguatiQ!~ ve!sus l!fformation lmss Sense disambiguation algorithms arc l~equently faced with mnltiple "conect" choices, a siluation which increases their odds of choosing a reasonable sense, hut which also has bidden negative consequences for selnanlic processing.
C94-2114@@  Improvement of cow, rage in practical domains is one of the most important issues in the area of examplebased systems. It is an oi)en question how many e~xaml)les are required for disambiguating sentences in a specific domain. parse trees axe stored as a, contextbase.
C94-2119@@Such constraints are necessary for the accurate analysis of natural anguage text+ Accordingly, the acquisition of these constraints is an essential yet time-consuming part of porting a natural language system to a new domain. These i)mses were th(m regularized aim reduced to t,riph~s. A cquisitiott of seleetion~d i)aAA,erns.
C94-2121@@  A text is not a mere set of unrelated sentences. A new tool for discourse anal ltearst\[5\] independently proposes a similar measure for text segmentation a d evaluates the performance o\[ her method with precision and recall rates.
C94-2122@@Most of this work is based on similarity measures derived flom the distrilmtion of woMs in corpora. The following s(mtences show potysemous usages of t~rke. We used Lhesc ill E:,:l)erinwnt-II.
C94-2134@@  Reusability of existing linguistic knowledge is the most import,~mt requirement for the rapid development of pra.ctical nal, ural \]augtlage l)rocessing systems. The r(~sull, s of the t;hre(~ CXl)erimeul,s art. \[u fact, each word wil, hin the Saml)h, s(mt(~uces Ir()m IJw I ;N IX  mamIM has 1.
C94-2139@@  Analyzing compound nouns is one of the crucial issues for natural language processing systems, in particular for those systems that aim at a wide coverage of domains. A probabilistic parsing method for sentences disambiguation. If the argument of cat is a tree, cat returns the category of tl,e rightmost leaf of tile tree.
C94-2144@@ Over l;he last lb.w years, eonsl;raint-based grammar tbrmalisms have become the predominmtt t)ar;tdigm in natural  language processing ~uld (:oulptltal;iollal linguistics. ALE |;he al;tribu(:e logic engine users guide. Consider all expression like x~ A .
C94-2154@@A m~L,ior reason for adding types to ~ forma,lism is to express restrictions on fea.ture cooccurences a.s in (;ls(:: \[5\] in order to rule out nonexista.nt yl)es of objects. Loosely speaking, the appropriateness sl)eeifieations for type t encode the part of p that sta, tes that an object of tyl)e t deserves features f and g, both with boolean vahles. A normal form for typed feature structures.
C94-2164@@These studies have presumed that utterances can be analyzed syntactically and semantically and that the representation ofthe speech acts performed by those ntterances can be obtained. VP NP V I I N desu r etto 400-yen Phe filler etto modifies the following word 400yen and the logical form of the sentence is the same as that of 400-yen desn. Referring as a Collaborative Process.
C94-2165@@ In Alices Adventures in Wonderland by Lewis Carrel many of Alices friends have names that consists of two words, for example: the March Hare, the Mock Turtle, and the Cheshire Cat. "a good wine white"). Tab le  2. :  The last ten collocations by bt N word pair g___ 1," 1 __.
C94-2172@@In this note, we present results concerning the theory and practice of classification schemes t)ased on word frequencies. In an experiment the scheme was u~d to classify two types of documents, and was found to work very well indeed. rn  "\]kYil  ~i2 is the largest, it is only necessary to determine which of n l  n~ r.m the (Plr Pi2 " " "Pin, ) is largest, and that is an easy machine calculation.
C94-2175@@  Bilingnal (or parallel) texts are useful as resources of linguistic knowledge as well as in applications uch as machine translation. Given a bilingual text, content words are extracted from each sentence. Aligning sentences ill bilingual corpora, Proceedings of th.e 29th Annual Meeting of ACL, pp.
C94-2180@@made significant contributions at a theorelic~d level, the effectiveness of discourse processing in NLP systems h~s not been studied so Ira at a practical level (of. In this paper, we descril)e how we have customized our multilingual discourse module wilhin our tcxl understanding system for a pmticulm" I~mk (ie data exlaaction i  the joint venture domain) in two different lmlguagcs (ie English mid Japanese), mid report he cwduation rcsul|s. A Formal Approach to Discourse Anaphora.
C94-2182@@ In conversation, a person sometimes has to refer to an object that is not previously known to the other participant. In R R. Cohen, J. Morgan, and M. E. Pollack, editors, Intentions in Communication, pages 77-103. Now, the confidence value of each attribute isequivalent o its salience within the context of the referring expression.
C94-2183@@tree, mid cc, hereuce relat,ions. 3 Automat ic  Detect ion  of  Dis course  S t ructure  3. )e guessed without detailed knowledge.
C94-2187@@  In this paper, we describe a method for discovering coherent texts from the unstructured corpus. TextTiling: A Quantitative Approach to l)iseomse Segmentation. O ranks highest, whose average value, 0.
C94-2189@@As a linguistic phenomenon, the use of pronouns is treated a.s a coreferential prohlem in which both the antecedent and the pronoun co-refer to some object. By traversing th( surface parse tree of a sentence, a 1)referen(:e vMue can be provided for each candidate noun phrase according to its syntact ic position. Selecti(m of I;he correct antece(tent flom the candidates.
C94-2191@@ Anaphora resolution is a complicated problem in computational linguistics. ,  as the most  l i ke ly  antecedent. An estimation of the probability of a subject, (direct or indirect) object or verb phrase (the only possible centers in our texts) to be centers, can be represented asa predicate with arguments: center (X, PI, \[symptoml (weight factorl 1 weight factorl2 ) .... symptomN (weight factorN~, weight factolNz)\]  where center (X, I, list) represents the estimated probability of X to be the center of a sentence (clause), X E {subjec t,objectl, object2 .
C94-2192@@They also claim that the under: s\[)ecilieil,y of the rhel;orieal relal,ion delinil,ions causes problems, Our elaim is thai, the main cause ()17 I,he di\[liculties of applying l iST t;o t;exl; i)rocessing systems is that SOlile of the relat;ions are delined on the basis of l;he elfeets which they have OIL a reader, This is particularly the case for the relations classitied as prcseulatioual rclalion.s, the relal;ions whose intended etfects are to increase some inclination in a reader. Nucleus stalx~s an aetion which will I)e perforined by a reade, r, and the action becomes possible by presenting the situation in Satellite. @72 ~ ~ o (Disagreenmnt of iuternatioital policy is making the market unsettle.)
C94-2194@@The rise of spoken language NI, P applications has le.d to increased inte.rest in such issues as real t ime pro-cessing and on-line error recovery. Generalisatiou remains a hard problem, of course. Morgan, and M. E. Pollack, editors, Intentions in Communication.
C94-2195@@ Prel)ositioual phrase attachment disambiguation is a difficult problem. I:ithm begins with n,ore refiued input. I{e(;a.llSe these papers deseril)e results ol)tained on different corpora, however, it is (lifIicull; to II~,:tl,:.
C94-2196@@ A discourse slralegy is a strategy for communicaling with another agent. Rhetorical slrllClllre theory: l)escriplion and conslrllc|io\]t of text structures. Rationality and ils roles in re~e;oning.
C94-2199@@Definite clause grammars (DCGs) are one of the simplest and most widely used unification grammar formalisms. This type of simplification is generally impossible in a I)UG. Lexica\] flmctional grammar: a R)rmal system for grammatical representation.
C94-2203@@  Processing (7hincsc texts is spccifi(ally difficult in its computation because liolmally sentc.nces in Chinese texts arc rcp:rcscnt(;d as strings of Chiucse characters without spaccs to indica.t(: wor(| boundaries. l, he word sgrucLm:e rule gives l;he evident, ial st;rengLh (0. ) A Hybrid Approach ~o Unknown Word l)etection and Segmentation of Chinese, Apl)e~r in Prec.
C94-2204@@ Describing objects is one of several purposes for which l inguists use fe.at, ul:e structures. The denotations of the species partition U, and S assigns et*ch object iu 1 the ul|ique species whose denottttion contains the object. Appropriateness encodes ~t rcbttionship between l;he dcnotaLions of species and atl:ributes: ifa(cr, ,v) is deliued then the den()tt~tion of a.ttributc (v acts upoi~ each ol~jecl, il, the, denota.l;ion of species cr to yield at, olLiect in the dcnol, ation of type ~(o-, ,v), but ifa((r, ,,,) is undefined then the denotati(m of al.l.ribul.e ~v ~tc/.s upon no object in the deuotal ion of species or.
C94-2206@@  This paper presents an incremental construction method of a lex iea l  t ransducer  (LT) for Korean. For example, i,I formalizing (ii) as a t;wo-h.vcl rill(; we |nus|, take into aceoun\[, bol, h irregular eonjugw t, ion C  -p  v(rbs/n(ljtci, ives and vowel harmony. The intermediate l vel, (b), is eliminated in the cascade, thus the final lexical transducer maps (a) directly to (e).
C94-2209@@  Processing a Mandarin Chinese corpus needs to go through several stages. mailv le/u yitou/d niu/n ./w (btty -ed head cow correct result :bought a cow ) it). This is a multitag-oriented disambiguation.
C96-1003@@  Recently various methods for automatically constructing a thesaurus (hierarchically clustering words) based on corpus data. values extracted from a corpus (step (i)). Our experimental  tesult.s how t.hal: il.
C96-1004@@  We address the problem of automatically acquiring case frame patterns (selectional patterns) from large corpus data. For the sk)t-based too(M, sometimes case slots are found to I)e del)endent. \[ able I shows the average perplexit.y ()btmmd for some randomly  s(hctcd verbs.
C96-1008@@lhe complex tasks performed eg by systems with nmltimodal user interfaces or by systems tackling the processing of spontaneous speech often require more than one computer in order to run acceptably last. Component  s t ruc ture  The interior structure of a component (see Fig. 2 as an exa.mi)\[e for what purpose split channels were used.
C96-1009@@Recently, large scale textual corpora give the potential of working with the real data, (!ither fin grammar inferring, or for enriching the le.xicon. In Transmission of information: a statistical theory o.f communications, M.I.T. Only those n-grants of frequency 3 or more are considered.
C96-1011@@Since there was n() I)()S-t.agged Spanish c(,rt)us availabh 1;() us and since creating a large hand-l,;tgp;(xl corltus is both cosl, ly aud I)r()ne l,o inconsislamcy, Gc decision was also a l)ra, ci, ical one. For our t,esls, wc (h~litmd four set,s o\[" hat . The remaining unknowns are assigned a set of ambiguous open-class tags of N, V, ADJ, and ADV so that they can be disambiguated by the Learner.
C96-1012@@Various researches on word sense disamil)ignation have recently been utilized in (:orlms-based apt)roache.s, reflecting the growth in the numlmr of machine readable texts. As with Inost thesauruses, the length of the 1lath between two words in Bunruigoihyo is exl)e, eted tt) reflect the similarity be,tween them. Let a certain verb have n senses (sl, 32, .
C96-1023@@~.lal)mW~se does noI, \]ta,ve contrasting singular and l)hlr;fl forms of nouns. and English, we fomtd il; t;o }m strikingly similaa to the, annlysis for Thai 1)rol)OS(~d by Sornlcrtlamwmic, het  al. For example, many insects, a whole swarm, .
C96-1025@@  \[\]ii(ter the eomt)ositional assulnption, senmntie analysis relies on the combination of the meaning representations of parts to build the meaning representations of a whole. A model of a given type has 138 Spatial  ote unetion Physie  object Inte,,tional O,,an e . In the context of a predicate, one of the concepts in the reference model is selected as the incolning point of a link from the predicates inealfing representatk) n, The coneel)t-oriente, d domMmnlodel apl)roaeh advocated here hyI)othesizes that the behaviour of words is driven by their conceptuM ro|es in the domain.
C96-1026@@  Recently, work in computational semantics and lexical semantics has made an interesting shift. --~ *which causes somebody to be angry e. -~ which is a manifestation ofsomebodys anger (12) Un holnnle triste h voir "A sad man to see" a. We are now able to show how the head distinction is relevant o classify emotional states adjectives and explain their semantic selection.
C96-1030@@run in parallel to propose translations of various portions of the input, Dora which the final translation is selected by a statistical anguage model. An in{:{mtl}tel,e (licl;i{mary or rool,/synonym list m{wely causes Pan EB M\[ l;o miss son. index (n(}l, required, I}ul, im l}roves speed al, run time).
C96-1038@@ Prepositional phrases are usually ambiguous. Whether the prepositionM phre~se with a telescope modifies the hea.d noun girl or the verb watch are not resolved by using only one knowledge sour(:e. Many researchers observe text  corpora and learll some knowledge based on language model I,o determine the plausible attachment. Kevin watched the girl with a telescope.
C96-1039@@However, the resolution of unknown words, ie, those words not in the dictionaries, form the bottleneck. The company name ~\]~\]i)t is a typical example. Acknowledgments The research was supported in part by National Science Council, Taipei, Taiwan, R.O.C.
C96-1043@@Thctc arc many m{}re industrial proiccts in Analysis than in Natural l,anguagc (;cneration. E l iminatory  cr i ter ia and overal l  average  All the automatic and human letters met the eliminatory criteria standards. The quality of tile t+esulting style of reply varies widely.
C96-1046@@ Pronunciation-by-analogy (PbA) is an influential psychological model of the process of reading aloud. % for the D&N 70 test set with Websters database. The threshold used was e times the maximum product score found so far, with ~ set by at 10 -3.
C96-1049@@ Applications on the basis of unification-based grammars (UG) so far are rather rare to say the least. Analysis -Ent r ies  for   in  :  Prepositions \]nay occur in strongly bound PPs where they are functional elenrents (semantically empty, but syntactically relevant). This handles cases such as geb-e.
C96-1054@@This is one of the largest projects dealing with Machine Translation (MT) of spoken language. The rule not only identifies the event marker E, but unifies the instances X and Y of the relevant thematic roles. A. F rank  and  U. Reyle.
C96-1055@@  This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources. These verbs are taken from i e , translations provided in bilinEnglish "glosses" ( . In B. Boguraev and T. Briscoe, editor, Computational Lexicography for" Natural Language Processing.
C96-1057@@(Peter first pointed to the fourth lucky number. The relation R has to be understood as characterizing the e i as opportunities for Peter to point to (specific) numbers. We call this reading: the Retardation-reading (R).
C96-1058@@In this paper, we 1)resent a flexible l)robat)ilistic parser that simultaneously assigns both part-ofsl)eech tags and a bare-bones dependency structure (illustrate.d in l!igure 1). Markov sour(:e modeliug of text gener~Ltiou. (The link to i from its eth child is associated with the probability P r ( .
C96-1059@@ While interpreting instructions, an agent is continually faced with a number of possible actions to execute, the majority of which are not appropriate for the situation at hand. Example (6) ("Be careful not to burn the garlic") is coded as AW t)e(:ause the reader is well aware that burning things when cooking them is bad. A Natural History of Negation.
C96-1060@@First, I reanalyzc the hypotheses I proposed earlier with respect o a corl)uS of naturally occurring data} I show that those hypotheses are basically supported, iThe examples in (Di Euge.nio, 199{)) were constructed. Much work still remains to t)e done on (:entering. lapcrs from o, l)agstv, hl Seminar.
C96-1061@@A system which processes poken language must address all of the ambiguities arising when processing written language, plus other ambiguities specitie to the speech processing task. As we mentioned e, arlier, the disaanbiguation task benelits from both non (-ontexLand context-l)~sed methods. Preliminary experiments show that both t)roposals help reduce the adverse ffect of the cumulative error problem.
C96-1062@@ Interpreting nominal compounds consists in retrieving the predicative relation between the constituents. Automated hd, e,cp.cctal, ion of Nominal Compounds in a Technical Domain, Phi) Thesis. Sen:laaltic labels are also used I,o express seleetional restrictkms on arguments.
C96-1064@@The syste, n works on the I)a, sis of a set of straightforward a\]m.logy-lm.sed 1)tin -ciples whi(:h ha.ve I)e(m used for a wide range of NLI ) a.lfl)licatio,s (I)irrelli el: a.l., 1!). Note incidentally (;ha.t, for reasons of computationa. ioii is ( atr ie(I  (~til.
C96-1067@@ Machine translation is usually significantly inferior to human translation, and for most applications where high-quality results are needed it must be used in conjunction with a human translator. It; was trained on 47M words fiom the Canadian Hansard Corpus, with 750/o used to make relative-flequency I)arameter estintates and 25% used to reestimate interpolation coefticients. We took a number of steps in art effort to achieve a good compromise.
C96-1070@@ A system dealing with spoken language requires a quick response in order to provide smooth communication between humans or between a human and a computer. Through preliminary exl)erimentation , our new T I )MT has b(~e.n shown to be efficient and particularly promising for spokendangnage translation. 2, and "gozen X ~ j t   with the distance value 0.00.
C96-1072@@ Proper names represent a unique challenge for MT and IR systems. The first two, E-E-J and J-E-J, have been completed; J-J-J is in progress. The weighted average of the P&R for companies, persons, locations, and dates is 94.0%.
C96-1073@@  In this paper, we argue that Higher Order Unification (HOU) provides a linguistically adequate tool for modeling the semantics of focus. (Jon only~ read the letters that 5arah sent to PAUL1) b. Jon also~ onlgt read the letters lhat 5UE.e sent to PAUL:,. A theory of focus interpretation.
C96-1075@@  JANUS is a multi-lingual speech-to-speech translation system designed to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain. ;VE I,IPII{.ES TOI)O El, I)\[A PODR(AMOS 11{ 1)E MATINI;" O SEA t.;N I,A TAI{I)E VEI{ EL I,A I EL\[ f :UI ,A (I{oughly " Yes what  do you th ink \[ have Tuesday  the I teenth and Wednesday  the nznetccnth  f~cc all day we ,-ou~d qo see tlze matmde so zn the a f te rnoon see the the 7novzc." The translation component consists of a t)arsing module and a generation module.
C96-1076@@There are two facts that conspire to make tile treatment of disjunction an important consideration when building a natural language processing (NLP) system. (Not(; thai; wilful; kind (if consLrainl;s the /)s a.n(I l/~s are ix not iml)ortmfl; here.) An Overview of l)i%iuncl;iw.
C96-1077@@However, there is at least one serious rival two-level notation in existence, developed in response to practical difficulties encountered in writing large-scale morphological descriptions using Koskenniemis notation. \[-1 Def in i t ion 2. Descriptions of 9 EU languages arc t)eing develot)e(1.
C96-1080@@The h;xicon is available to members of the Linguistic Data Consortium for both research and commercial applications. zeroed %o go vs * "lie sugg(sl;e{1 dmt l should g(} and I wan~ed." The (:orI)us us(xl for this t;agging {:(resists (if Brown (all, ie 7 MB), Wall Sia{.
C96-1082@@ The German joint research project Verbmobil (VM) 1 aims at the development of a speech to speech translation system. E.actual is the last vertex added to tto in an operation. Increment T. Create VT 4.
C96-1084@@ Text phenomena, eg, textual forms of ellipsis and anaphora re a challenging issue for the design of parsers for text understanding systems, since lacking recognition lacilities either esult in referentially incoherent or invalid text knowledge representations. This work has been funded by LGFG Baden-Wiirttemberg (M. Stuff)e). A conceptual reasoning approach to the resolution of textual ellipses.
C96-1085@@ There are several arguments why computational linguists feel attracted by the appeal of parallelism for natural language understanding (for a survey, cf. The per fo rmSearchHead message triggers (via per fo rmSearchHead 2o  : messages) asynchronous searchHeadFor:  messages to be forwarded by each receiving ph r a s e Ac t o r to its right-most we rdAct  o r. From this origin, the search message can be distributed to all word actors at the right "rim" of the dependency Irec by simply forwarding it to Ihe respective heads. a message passing protocol).
C96-1088@@In Dynamic Semantics, the semantic structure of a discourse gives rise to constraints on the resolution of anaphoric expressions. Muskens shows that DR~I boxes can t)e viewed as abbreviations for expressions in ordinary type logic. , )  ) We n(,(~d two in(li(:e,s: n is the, index of h,c: this is ml individual deline, d in input (:ont0,xt.
C96-1089@@  In the field of machitm translation, there is a, growing interest in corl)llS-I)ased al)l)roa. It seems di\[\[icutt I.o extend these statistical l lethods to ~t I)roa. The English text was not a literal translation.
C96-1091@@l{,e(;enl; work has seen proposals tier a range of such system l\[l.q, differing in their resource sensitivity (and hence, implicitly , their underlying llo|;ion of linguistic sl.rllei;llre), in some eases combining differing resource sensibilities within a, single system. Some of these proposed methods a  labelled de(luei;ion methodology (G;d)b;~y, \]994), where he was involved in deriving the argument. Consequently, tile \[o-I\] rule is not required, and hypothetical reasoning excluded.
C96-1092@@In this paper we address neither of these two issues. However, the issue raised with respcct o the CEI,RG in this p~tper is orthogonal to overall assumptions of German obtuse structure. 13\[t is worth pointing out that the importauce of subsunlption tt~s been noted for other linguistic phenomena as well.
C96-1094@@  Languages uch as Catalan, Czech, Finnish, German, Hindi, Hungarian, Japanese, Polish, Russian, Turkish, etc. Discourse descro~tion: diwrse analyses of a \])rod raising t,e.vt, eds. 2 In fo rmat ion  St ructure  \]n the Information Structure (IS) that I use for Turkish, a sentence is first divided into a topic and a comment.
C96-1096@@  There are five principal factors in Bantu languages which contribute to ambiguous analysis of wordtbrms. Third, reduplication is a productive phenomenon. In this paper I shall discuss the points one and two by using Swahili as a test language.
C96-1097@@ In natural language processing, the importance of large volume corpus has been pointed out together with the need for technology of analyzing these linguistic data. Length and Number of Extracted Substrings t  p  Proposed Metlgod N-gram Statistics a: Extract b: Total c:Extmct d: Total (;ran Substring Frequency Substring Frequency 2 ~ 970,203 2. However the frequency of such a pair is limitted to 1.
C96-2098@@  Alignment of corpora is now being actively studied to support example-based automatic translation and dictionary refinement. The former showed a precision of 82. Calculate the distance F(T) for each candidate.
C96-2101@@Both regard natural anguage as purposeful behaviour, but differ in how this behaviour is to be described. Thematic relatedness i based on the types of relationships which occur in the domain. A. van der Linden, and G. O. thoe Sehwartzenl)erg.
C96-2102@@  Ititherto, the field of punctuation has been almost completely ignored within Natural Language Processing, with perhaps the single exception of the sentence-final full-stop (period). I{Xt{C (\]rcnoble Laboratory, FechnicM Report. Thus (29) represents l g represents a variable 2 +pco represents a comma 608 legal use of punctuation adjoining a I)hrasal item since it occurs adjacent o the AD.n within the NP.
C96-2104@@  Natural language parser/analyser is essential for allowing advanced functions in document processing systems, such as keyword extraction to characterize a text, key-sentence extraction to abstract a document, grammatiea\] style checker, information or knowledge retrieval, natura\] lmlguage understanding, naturai language interface and so on. A 100-word sentence c~n be analyzed in less than 1 second on PC. %, the accuracy of almlyzed uke bunsets,~s for each buusetsu except the last one is 95.
C96-2105@@A replacement expression specifies that a given symbol or a sequence of symbols hould be replaced by another one in a certain context or contexts. * x\], inside the string abe, i.e_ laetweena and b and between b and c, alL x will be ignored any number of times. \ [1E.. .mE\] if UPIEt{ is identical with the empty string and i ff \[\]...n\] if UPPEI-t does not contain the empty string.
C96-2106@@  Unification-based theories of grammar allow for an integration of different levels of linguistic descriptions in a common framework of typed feature structures. This t)e,haviour is all outcome of our compilation schema, namely, cutting reentrancy points. The measurements have been obtained w.r.t.
C96-2110@@ Recent advances in information infrastructure have made an enormous number of on-line documents accessible. Ambigu i ty  in Determhl lng  Character Coding System For historical reasons, documents on the current WWW are encoded in various coding systems. "ESC $ B" shows the beginning of J\[S character set.
C96-2112@@provides adequate means for describing non-linear phenomena such as infixation, reduplication and templatic morphology. (3) PARSING FUNCTION +(c, E) Let B be a base (ie stem or word). A t~aturebased formalisln for twodevel phonology: a description and implementation.
C96-2122@@ Dependency and constituency frameworks define different syntactic structures. 3); 4) the root is a unique symbol as such that As: as E L and As~S. V27 Position is bounded by O(n).
C96-2123@@For example, compare the following to sentences: (1) a. Fhe cat that the dog that the man bought chased died. structural COml)lexity of the senI.en(:e is reduced as a result. Ioc  sent \]12(; Izook he found in I)a.ris /o his pal b. Joe sent :o his pal I, he book lie found in I)3ris F, xtral)oS(~.d re la t ive  (4) a.
C96-2126@@The F, nglish part of it was carried out by the Belgian partner" (Office Line Engineering NV, Zonnegcm; RAM1T, Gent) , the Dutch part by the Dutch partner (Lexicology Research Group, Free University Amsterdam1). A syntactic tagger and lcmmatizer for l)utch medical language. It distinguishes the following concept types: CC_Surgical Deed (indicating the surgical intervention), CC_Anatomy (indicating anatomical concepts), CC Pathology (indicating pathological concepts), CC Interventional Equipment (indicating the instrument), CC Combi (a term which has a medical meaning only ira combination with another-medicalterm), r T. 1,rizzanin (syntax), A. Kramer (lexicon), 1.
C96-2129@@  Omissions in translations arise in several ways. Th(  .orein 1 Leg A be lh.e array of all minimal omitlcd segments, sorled by/lhe horizonlal posilion of the left end poinl. In ter  fe r lng  segnmnts  are std)segtuents of maximal omitted segments with a slope m~gle at)()v(, Lit(: chosen threshold.
C96-2130@@Part-of-speech (pos) taggers are programs which assign a single pos-tag to a word-token, provided that, it is known what parts-of-speech t is word can take on in principle. Score Score Lexicon Guessing strategy Pull standard: P+S+E )hfll with new: P+A+S+E Small standard: P+S+E Small with new: P+A+S+E Total Unkn. This rule, for example, will work for word pairs like ,pecify spec i f ied  o r  deny  denied.
C96-2132@@ From simple electrical appliances to complex computer systems, almost all machines are accompanied by instruction manuals. shows a universal causal relation. shows that the antecedent of the senl.e.nce is an ~ssumpi, i0n .
C96-2136@@However, for those languages that have a different morphology and writing system from English, spelling correction remMns one of the signillcant unsolved researcil problems in computational linguistics. (hmtinuous Speech I{e(:ognil, ion. {7 0 7 3 Word  Segmentat ion  A lgor i thm 3.
C96-2137@@This phenomenon causes considerable problems in natural language processing systems. In these 28 instances, ~he verbs that  governed these zero pronouns expressed the modal i t ies o f  subekida should  or -s i tehanaranai  nmst n(it. I t~l elements to deduce like.ly referents.
C96-2141@@ In this paper, we address the problem of word alignments for a bilingual corpus. lhe I)etinidon of a MI \[ask. The mixtm;e model can be interpreted as a zeroth-order model in contrast to the first-order tlMM model.
C96-2143@@  An automatic generation system that is to produce good quality texts has to include effective algorithms for choosing the linguistic expressions referring to the domain entities. The classification includes: Specif ic entities. e is a concept being defined through a listing of its components, use a definite singular noun phrase .
C96-2147@@lilt many natural languages, elements that can be easily deduced by tim reader are frequently omitted front the. The improvement of the I)e.rforma.nce in method 2 also ilntflies tit(! S. Takada and N. Doi, Centering in Japanese: A Step \]bwards Better Interpretation of Pronouns and Zero-Pronouns, Proe.
C96-2148@@ and Motivation Relaxation is a well-known technique used to solve consistent labelling problems. Any rehttionship between any subset of words and tags may be expressed as constraint and used l;o feed the algorithm. Searching a more specific support flLnction.
C96-2151@@Sparse data is a perennial problem when applying statistical techniques to natural anguage processing. The variance is the quadratic moment w.r.t, the mean, and is thus such a measure. This is equivalent o assigning a count of one to the occurring, and one third to the non-occurring outcomes.
C96-2153@@ One of the most inl;eresting problems comes about by the tendency of natural language discourse to be ambiguous and open to a wide variety of interpretations. E is a finite set of edges of the form (vl, {v2,...,  vn), {d l , . A resolution procedure derives the hillfledged semantic representations.
C96-2154@@  Statistical anguage models play ~ major role. The &;tails (if e, ach stet) will be described lat(~u. scores of the N-best hypothes(;s gen(.rnted l)y the speech recognizer.
C96-2155@@ This paper describes a new generation method that produces multiple paraphrases from a semantic input which may contain ambiguities. This explains the reason that expression of the fact \[loud(e)\[ is conditioned on the choice q2 (the 5th slot of the array in node 11). It may also enable different kinds of interactions between the translation system and the human expert who operates i t  tbr instance, disambiguation by a monolingual in the target language.
C96-2157@@  hlentifying concepts in natural language text is an important intbrmation extraction task. state-owncd electronics giant Thomson S.A . (m phrase, or an N-gram) its follows.
C96-2158@@  It is by now widely agreed upon that tile process of resolving anaphors in natural anguage text is supported by a w~riety of strategies employing different kinds of knowledge. neither of the t)ronouns is confined structurally to one of the intrasenttmtial ntece. But, afl, e ra  first decision, eg (Sb) The barberi told the clientj a story, while he| shaved himz.
C96-2159@@These algorithms are now getting keen attention from the natural anguage processing (NLP) research community since the huge text corpus is becoming widely available. Goodness  of  Genera l i za t ion  Another factor to consider is the measurement ofthe goodness of a generalization. Two methods for learning alt-j/e translation rules from examples and a semantic hierarchy.
C96-2161@@  Thesauruses are among the most useful knowledge resources for natural language processing. !pie c~mdida.te sets of con imcth)l!s. Suppose we want to extract he viewpoint o\[ the noun "HF, I{IKOPUTAA" (helicopter).
C96-2163@@  In corpus-based NLP, acquisition of lexical knowledge has become one of the major research topics. 4 C lass -based Assoc ia t ion  Score 4. 970 4..  Subsumption Relation Next, we introduce subsuraption relation ~/ o f  ~ a bilingual surface case structure  and a Japanese ease-class frame fa: e ~f f3 iff.
C96-2164@@  The rapid expansion of the Internet enables us to easily access a lot of information sources in the world. Type of a Sentence: Sentence types are fact, conjecture, or insistence. In this case, the three most important sentences selected by grou t) B were 0, 2, and 3.
C96-2182@@ Most texts are rich in multi-word expressions that cannot be properly understood let alne be processed in an NLP system, ff they are not recognized as complex lexical units. Lexieal  and  St ructura l  Var ia t ion  The formalism provides a set of RE operators to combine the descriptions of single words. the ears sharpen) can now both be expressed very simply according to the same schema s WOVlArg( f ixS( :den ( :schSnen ) :Schein) vahren ) WOVlhrg( fix2(:die :Ohren) spitzen ) Further macros are defined for German for MWLs with a reflexive or particle verb, to express scrambling of an idiom-external PP complement or topicalisation.
C96-2202@@ Dictionaries are indispensable in NLP in order to determine the grammatical functions and meanings of words, but the coutinuous increase of new words and technical terms make unknown words an ongoing problem. The application d:lscussed here is word extraction fron\] a Japanese corpus. Ca lcu la t ing  S t r ing  Env i ronments  The cMculation of the enviromnent of an arbitrary string (possible word) in a corpus is basically identical to tire POS algorithm above, except that because Japanese has no blank space between words arr(t a raw (unsegmented) corpus is used, the extent of the environment is ambiguous.
C96-2211@@It has suddenly OltCncd up a window to wtst amounts of data, on the Internet. Handling aml)iguous parses is a. difficult task. to  ha ,ve  several counterparts in a target language, the number of tr(:e-pairs in STAG would grow much la.rgo, r tha.n tha.t of sour(:(.
C96-2212@@We will consider i~ tree represent~tl, ion of MI the words in t,he vocM)uh~ry in which the root; node l:ei)resenl;s the whole vo(:i~l)uli~ly i~lltl ~ le~f llOde rel)rese\[lt;S a, word ill the voclJ)llli~ry. ~\]n the actuM implement~ttion, we only htwe to work on the bigr~ml t*Lble instead of tim whole text. Make a dendrogram out of this process.
D07-1001@@The computational treatment of sentence compres-sion has recently attracted much attention in theliterature. In Proceedings ofthe Intelligent Scalable Text Summarization Work-shop (ISTS), ACL-97.Briscoe, E. J. and J. Carroll. SUMMAC: A text summarization evalua-tion.
D07-1002@@Recent years have witnessed significant progress indeveloping methods for the automatic identificationand labeling of semantic roles conveyed by senten-tial constituents. log(1+ Nnr)(3)where fr is the frequency of r occurring in SR; N isthe total number of SRs evoked by a given frame;and nr is the number of SRs containing r.For each frame element we thus generate a setof semantic role assignments Set(SRA). [Lee]Seller sold a textbook [toAbby]Buyer.b.
D07-1003@@ and MotivationOpen-domain question answering (QA) is a widely-studied and fast-growing research problem. Selectively usingrelations to improve precision in question answering.In Proceedings of the 10th Text REtrieval Conference(TREC-10), Gaithersburg, MD, USA.Leonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss. be a candidate an-swer sentence.
D07-1005@@Word Alignment of parallel texts forms a cru-cial component of phrase-based statistical machinetranslation systems. BLEUpoints and provides a better baseline. {1, 2, ..., I}The parameter  controls the number of empty align-ments; a higher value favors more empty alignmentsand vice versa.
D07-1006@@Several generative models and a large number ofdiscriminatively trained models have been proposedin the literature to solve the problem of automaticword alignment of bitexts. Each featurefunction hm has an associated weight m. Givena vector of these weights , the alignment searchproblem, ie the search to return the best alignmenta Given the feature function param-eters estimated in the M-step and the feature func-tion weights  determined in the D-step, the E-stepsearches for the Viterbi alignment for the full train-ing corpus.We use 1  F-Measure as our error criterion. A maxi-mum entropy approach to combining word alignments.In Proceedings of HLT-NAACL, pages 96103, NewYork.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and R. L. Mercer.
D07-1007@@These results castdoubt on the assumption that sophisticated dedicatedWSD systems that were developed independentlyfrom any particular NLP application can easily beintegrated into a SMT system so as to improve trans-lation quality through stronger models of contextand rich linguistic information. Do I pay on the busSMT Please get on the busSMT+WSD I buy a ticket on the busInput   I want to reconfirm this ticket.SMT I would like to reconfirm a flight for this ticket.SMT+WSD I would like to reconfirm my reservation for this ticket.Input eL0 I have another appointment, so please hurry.SMT I have an appointment for a, so please hurry.SMT+WSD I have another appointment, so please hurry.Input w`J0~G Could you tell me the way to BroadwaySMT Could you tell me the way to Broadway I am sorry.SMT+WSD Excuse me, could you tell me the way to BroadwayInput w  *&7Ref. SMT Without any congressmen voted against him.SMT+WSD No congressmen voted against him.Input (fLVTS/ SMT Russias policy in Chechnya and CIS neighbors attitude is even more worried that theUnited States.SMT+WSD Russias policy in Chechnya and its attitude toward its CIS neighbors cause the UnitedStates still more anxiety.Input C bSMT As for the U.S. human rights conditionsSMT+WSD As for the human rights situation in the U.S.Input /:HB, sAcSMT The purpose of my visit to Japan is pray for peace and prosperity.SMT+WSD The purpose of my visit is to pray for peace and prosperity for Japan.Input : 2P; SMT In order to prevent terrorist activities Los Angeles, the police have taken unprecedentedtight security measures.SMT+WSD In order to prevent terrorist activities Los Angeles, the police to an unprecedented tightsecurity measures.
D07-1008@@Recent years have witnessed increasing interest intext-to-text generation methods for many naturallanguage processing applications ranging from textsummarisation to question answering and machinetranslation. (2)The goal of the training procedure is to find a param-eter vector w such that it satisfies the condition:i,y  0 (3)where xi,yi are the ith training source tree and tar-get derivation. Extracting paraphrasesfrom a parallel corpus.
D07-1011@@Generation of Referring Expressions (GRE) is awell-studied sub-task of microplanning in NaturalLanguage Generation. thesis, Univiersity of Tilburg.E. Gupta and A. J. Stent.
D07-1012@@This paper is concerned with the task of predict-ing whether a sentence contains a grammatical er-ror. Singular subject with pluralcopula errors (e. g. The man are) peak at a recall of91.0. A novel deep processing XLE-based approach2.
D07-1013@@In this work wefocus on a common parsing paradigm called data-driven dependency parsing. The PDT: A 3-level annotation scenario. L}Let D(Gx) represent the subgraphs of graph Gxthat are valid dependency graphs for the sentencex.
D07-1014@@Dependency parsing canbe used to provide a bare bones Conditional distribu-tions over outputs (here, trees) given inputs (here,sentences) have certain advantages. y will referto a directed, unlabeled dependency tree, which is amap y : {1, ..., n}  {0, ..., n} from child indicesto parent indices; x0 is the invisible wall symbol.Let Yx be the set of valid dependency trees for x. Inthis paper, Yx is equivalent to the set of all directedspanning trees over x. Wepropose that this model take a log-linear form:p~ (x)(1)where ~f is a feature vector function on parsed sen-tences and ~ inference algorithms in the non-projective case. (i, j),then, denotes the (multiplicative) contribution of theedge from child i to parent j to the total score ofthe tree, if the edge is present.
D07-1015@@Learning with structured data typically involvessearching or summing over a set with an exponen-tial number of structured elements, for example theset of all parse trees for a given sentence. The sets T mp (x) andT mnp (x) are defined analogously for multi-root struc-tures. Experiments with a higher-order projectivedependency parser.
D07-1016@@The status of English as a global language meansthat English words and phrases are frequently bor-rowed by other languages, especially in domainssuch as science and technology, commerce, adver-tising, and current affairs. % higherthan the baseline), a recall of 65. The frequency profiles of the two datasets are broadly similar (the difference in means ofboth groups is only 0.000676), albeit significantlydifferent according to a paired t-test (p Thisis one reason why the inclusion entity models per-formance on the inclusion set does not reach the up-per limit set by the random sample.
D07-1019@@ Nowadays more and more people are using Inter-net search engine to locate information on the web. Mayes E., Damerau F. and Mercer R. Context based spelling correction. Darroch J. N. and Ratcliff D. Generalized iterative scal-ing for long-linear models.
D07-1020@@ An ever-increasing number of question answering, summarization and information extraction systems are coming to rely on heterogeneous sets of documents returned by open-domain search en-gines from collections over which application developers have no control. References A. Bagga and B. Baldwin. Based on their relation-ship to the ambiguous personal name, the named entities identified in a text can be divided into three categories:  (i) Target entity: an entity that has a mention of the ambiguous personal name.
D07-1021@@ This paper will describe two methods of com-pressing trigram language models: HashTBO and ZipTBO. We choose m to be a power of two near  , where E[t] is the expected value of the interarrivals, de-fined below. Then the relative entropy D(P||P) where h is the history.
D07-1022@@As the field of statistical NLP expands to handlemore languages and domains, models appropriatefor standard benchmark tasks do not always workwell in new situations. T, where T is the set of syntactic treesfor the language. of EMNLP, pages 4956.A.
D07-1023@@ The availability of a high-quality lexicon is crucial to the development of fundamental text-processing components such as part-of-speech (POS) taggers and syntactic parsers. Unsupervised learning of the morphology of a natural language. y, and t is a threshold that we set to 0.  in all of our experiments.
D07-1024@@However, the amount of biomedicalliterature regarding protein interactions is increas-ing rapidly and it is difficult for interaction databasecurators to detect and curate protein interaction in-formation manually. In Proceedings of the19th Conference on Neural Information ProcessingSystems (NIPS), Vancouver, B.C, December.R. M. Phizicky and S. Fields.
D07-1025@@Sequence alignment is a problem that crops up inmany forms, both in computational linguistics (CL)and in other endeavors. Parker, A. Fern, and P Tadepalli. Elements of sequence e are character pairscs, ct, with cs  (s, t, e)be a feature vector, having the same dimensional-ity as , for a source, target, and corresponding editsequence.
D07-1028@@Sentence generation, or surface realisation, is thetask of generating meaningful, grammatically cor-rect and fluent text from some abstract semantic orsyntactic representation of the sentence. % coverage and a BLEU score of 75. Aselective unpacking algorithm allows the extractionof an n-best list of realisations where realisationranking is based on a maximum entropy model.
D07-1029@@Many machine translation (MT) frameworks havebeen developed, including rule-based transfer MT,corpus-based MT (statistical MT and example-basedMT), syntax-based MT and the hybrid, statisticalMT augmented with syntactic structures. Similarly, it boosts common phrasepairs that are selected by multiple decoders.S |C(f, e)|is the number of systems that use phrase pair (f, e)to translate the input sentence. Thesentence hypothesis selection based on N-gram lan-guage model further improves the translation qual-ity.
D07-1030@@ Within the Machine Translation (MT) field, by far the most dominant paradigm is SMT, but many existing commercial systems are rule-based. Construction of a Hierarchical Translation Memory. An interpolated translation model is built by linear interpolating the n synthetic models.
D07-1031@@We focus on first-order Hid-den Markov Models (HMMs) in which the hiddenstate is interpreted as a POS tag, also known as bitagmodels.In this setting we show that EM performs poorlywhen evaluated using a 1-to-1 accuracy evalua-tion, where each POS tag corresponds to at most onehidden state, but is more competitive when evaluatedusing a many-to-1 accuracy evaluation, where sev-eral hidden states may correspond to the same POStag. Here m is the number of possible observations (ie, the size of thevocabulary), s is the number of hidden states and I() is the indicator function (ie, equal to one if itsargument is true and zero otherwise), nx,y is the number of times observation x occurs with state y, ny,y isthe number of times state y follows y, and ny is the number of times state y occurs; these counts are from(xi,yi), ie, excluding xi and yi. This also explainswhy the many-to-1 accuracy is so much better thanthe one-to-one accuracy; presumably several hidden299Estimator 1-to-1 Many-to-1 VI H(T |Y ) H(Y |T )EM (50) 0.
D07-1032@@Coordinate structures are a potential source of syn-tactic ambiguity in natural language. The generativeprobability of a predicate was estimated from the2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html311collected coordinate bunsetsus using maximum like-lihood. Outline of the ModelThis model gives a probability to each possible de-pendency structure, T , and case structure, L, of theinput sentence, S, and outputs the syntactic, coordi-nate and case structure that have the highest proba-bility.
D07-1033@@Many NLP tasks such as POS tagging and namedentity recognition have recently been solved as se-quence labeling. A new approximate maximal marginclassification algorithm. http://chasen.org/taku/software/CRF++8We also replaced the optimization module in the originalpackage with that used in the Amis maximum entropy estima-tor (http://www-tsujii.is.s.u-tokyo.ac.jp/amis) since we encoun-tered problems with the provided module in some cases.
D07-1034@@ Large-scale semantic lexicons are important re-sources for many natural language processing (NLP) tasks. For each target word, we evaluated the automatic classification (and the baseline ranking) by matching the human decisions with the top N candidates. It is therefore even more important to devise robust ways for automatic acquisition of such a resource.
D07-1035@@However, due to the lack of editorial and quality control, reviews on products vary greatly in quality. 6.0 megapix is out, though only a few. Also the best price for a major brand.
D07-1043@@They are particularly appealing for tasksin which there is an abundance of language dataavailable, but manual annotation of this data isvery resource-intensive. In theperfectly homogeneous case, this value, H(C|K),is 0. V I is presented414as a distance measure for comparing partitions (orclusterings) of the same data.
D07-1046@@Morphological analysis and disambiguation are cru-cial pre-processing steps for a variety of natural lan-guage processing applications, from search and in-formation extraction to machine translation. A finite-statemorphological grammar of Hebrew. When the definitearticle h is prefixed by one of the prepositions b, kor l, it is assimilated with the preposition and theresulting form becomes ambiguous as to whetheror not it is definite.
D07-1047@@The sum-mary can be an abstraction or extraction. Such work requires a system to produceabstract summaries. In Proceedings ofthe Third International Conference on Language Re-sources and Evaluation (LREC).J.T.
D07-1048@@is a general term for customer ser-vice centers, help desks, and information phonelines. Wright, and A. Gorin. Yang and J. O. Pedersen.
D07-1049@@Language modelling (LM) is a crucial component instatistical machine translation (SMT). In 45th AnnualMeeting of the Association of Computational Linguists (Toappear).E. Network applicationsof Bloom filters: A survey.
D07-1051@@The annotation of corpora has become a crucial pre-requisite for NLP utilities which rely on (semi-) su-pervised machine learning (ML) techniques. In each AL round the com-mittees k classifiers are trained on the already an-notated data C, then a pool of unannotated data Pis predicted with each classifier resulting in n au-tomatically labeled versions of P . A maximum entropy approach tonatural language processing.
D07-1053@@: NLP for AAC systems Augmented and Alternative Communication (AAC) is a field of research which concerns natural language processing as well as human-machine interaction, and which aims at restoring the com-municative abilities of disabled people with severe speech and motion impairments. We measured a Pearson coefficient of -0. The last two components are a text editor (to write e-mails or other documents) and a speech synthesis module, which is used in case of spoken communication.
D07-1055@@the search problem, ie how to find the besttranslation candidate among all possible targetlanguage sentences; the modeling problem, ie how to structurethe dependencies of source and target languagesentences; the training problem, ie how to estimate thefree parameters of the models from the trainingdata.Here, the main focus is on the training problem. Workshop on Intrinsic andExtrinsic Evaluation Measures for MT and/or Summariza-tion at the 43th Annual Meeting of the Association of Com-putational Linguistics (ACL), pages 6572, Ann Arbor, MI,June.Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J.Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L.Mercer, and Paul S. Roossin. The de-nominators of the n-gram precisions evaluate to thenumber of n-grams in the hypothesis, ie I n+1.The n-gram counts for the Bleu score computa-tion are usually collected over a whole document.For our purposes, a sentence-level computation ispreferable.
D07-1057@@Coreference resolution is the task of determiningwhether two or more noun phrases refer to the sameentity in a text. For a node T in the parse tree, if1. We thentry tuning r and t at the same time.
D07-1060@@This is be-cause such knowledge sources capture semantic in-formation about concepts and, to some extent, worldknowledge. As German wordsin a phrase can be highly inflected, we lemmatizeall components. Ties Score P R FMonolingualHPG 222 174 11 171.  .
D07-1061@@Several kinds of Natural Language Processing systemsneed measures of semantic relatedness for arbitrary wordpairs. In both vari-ants, the wizard Tokens only neighbors are the wiz-ard#n TokenPOS nodes, and wizard#n584MarkovLink MarkovGlossNode Probability Node Probabilitywizard 1.0E-1 wizard 1.E-01wizard#n 2. as generated by the MarkovLinkmodel and the MarkovGloss model with return probabil-ity 0. .Formally, we define the probability n(t)i of finding theparticle at node ni  A synset is bestthought of as a concept evoked by one sense of one ormore words.
D07-1062@@Semantic Role Labeling (SRL) aims to identify andlabel all the arguments for each predicate occurringin a sentence. It is also worthnoting that the R and I category features help mostfor identification. A study on convolution kernels forshallow semantic parsing.
D07-1064@@Coordination, along with prepositional phrase at-tachment, is a major source of syntactic ambiguityin natural language. It cannot encode nested coordinations like(A, B, and (C and D)), however. Evaluation criteriaWe employed two evaluation criteria: (i) correctnessof the conjuncts output by the algorithm, and (ii) cor-rectness of the range of coordinations as a whole.For the correctness of conjuncts, we further usetwo evaluation criteria.
D07-1068@@Named and Numeric Entities (NEs) refer to propernouns (eg PERSON, LOCATION and ORGANI-ZATION), time expressions, date expressions and soon. Independent Among the modelsconstructed by combination of defined cliques, thebest F1-value is achieved by CR model, followed bySC, SCR, C, SR, S, R and I. A gaussianprior for smoothing maximum entropy models.
D07-1069@@In a conversation or debate between a group ofpeople, we can think of two remarks as interact-ing if they are both comments on the same topic.For example, if one speaker says taxes shouldbe lowered to help business, while another arguestaxes should be raised to support our schools, thespeeches are interacting with each other by describ-ing the same issue. In Machine Learning: Proceedings of theTwenty-Third International Conference (ICML).David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Department of Political Science, The Pennsylva-nia State University.Mark E. J. Newman.
D07-1072@@An important question whenlearning PCFGs is how many grammar symbolsto allocate to the learning algorithm based on theamount of available data.The question of how many clusters (symbols) Specifically, an HDP-PCFGis defined to have an infinite number of symbols;the Dirichlet process (DP) prior penalizes the useof more symbols than are supported by the trainingdata. Blei and M. I. Jordan. In Associationfor the Advancement of Artificial Intelligence (AAAI).E.
D07-1073@@It has been known that Gazetteers, or entity dic-tionaries, are important for improving the perfor-mance of named entity recognition. We assessed how the labelingfor these entities changed between (B) and (E). These IOB2tags were used in the same way as other features7http://www.cs.utah.edu/hal/TagChunk/8We use bare B, I, and O For example, we useda feature such as the Wikipedia tag is B-guitaristand the NE tag is B-PER.
D07-1074@@ and Related Work The ability to identify the named entities (such as people and locations) has been established as an important task in several areas, including topic de-tection and tracking, machine translation, and in-formation retrieval. References Bagga, A. and B. Baldwin. News Stories We downloaded the top two stories in the ten MSNBC news categories (Business, U.S. We then performed a post-hoc evalua-tion of the disambiguations hypothesized for the surface forms correctly identified by the system (ie if the boundaries of a surface form were not identified correctly then we disregarded it).
D07-1075@@can reliably identifya victim of a murder event. Next, we use the u prob-ability threshold to separate these N patterns intotwo subsets. In Proceedings of the36th Annual Meeting of the Association for Computa-tional Linguistics and 17th International Conferenceon Computational Linguistics, pages 404408, Mon-treal, Quebec, August.R.
D07-1077@@In reordering approaches, sentences in thesource language are first parsed, for example using aTreebank-trained parser. We decide to reorder DNPs of the NP+DEG for-mat, because they often can only map to the NP2 ofNP1 expression is more general and can replaceNP1s NP2 One exception is whenthe NP is a pronoun (PN), eg, (he) (s) i(name), in which case the DNP acts simply like apossessive pronoun. The reorderingapproach even achieved a 0.
D07-1078@@As a result, often no substructures corre-sponding to partial PTB constituents are extracted toform translation rules.Syntax translation models acquired by composingtreebank grammar rules assume that long rewritesare not decomposable into smaller steps. Binarizing Syntax TreesWe are going to binarize a tree node n that domi-nates r children n1, ..., nr. The input to the forest-based algorithm is a(e-forest, f, a)-triple.
D07-1079@@String models are popular in statistical machinetranslation. Composed rules are ex-tracted in addition to minimal rules, which meansthat a larger n limit always results in a superset ofthe rules extracted when a smaller n value is used.When n is set to 0, then only minimal rules are ex-tracted. Machine translationwith a stochastic grammatical channel.
D07-1082@@However, creating a large sense-tagged corpus is very expensive and time-consuming, because these data have to be an-notated by human experts. in i    (1) where U is the uncertainty measurement function H represents the entropy function. A maximum entropy approach to natural language processing.
D07-1086@@Billions of times every day, people around the worldcommunicate with Internet search engines via asmall text box on a web page. A maximum entropy model forpart-of-speech tagging. Attest time, the classifier chooses the segmentation forx that has the highest score according to the learnedparameterization: y A ranking approach was also used previously by Daume At test time, N 1segmentation decisions are made for the N -lengthquery and an output segmentation y is produced.Here, features depend only on the input query x andthe position in the query i.
D07-1087@@Over the past decades much textual data has be-come available in electronic form. We are grateful to Antal van den Bosch,Marieke van Erp, Steve Hunt, and the staff at Natu-ralis, the Dutch National Museum for Natural His-tory, for interesting discussions and help in prepar-ing the data.ReferencesLeonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss. The general form of a Hid-den Markov Models state-emission distribution isP (o|s), where s is the state, ie a field type in ourcase, and o is the observation.
D07-1088@@For example, webpages of products on Amazon may contain a list of data records such as books, watches, and electronics. 0 and a recall of 0. These features are (a) root form of every word, (b) the sub-ject within the sentence, (c) the object within the sentence, and (d) the governing verbs.
D07-1089@@The scientific literature of biomedicine, genomics,and other biosciences is a rich, complex, and con-tinually growing resource. The match-factor adds this functionality by rewarding MCAsthat align words to multiple word positions when (t,p)that maximizes the expected utility. A discriminative framework forbilingual word alignment.
D07-1090@@Given a source-language (eg, French) sentence f ,the problem of machine translation is to automati-cally produce a target-language (eg, English) trans-lation e. As a generalrule, more data tends to yield better language mod-els. A bit of progress in languagemodeling. After scoring, the decoder prunes this set asindicated by the four black disks at time t + 1, thenextends these to form five new nodes (one is shared)at time t + 2.
D07-1091@@The current state-of-the-art approach to statisticalmachine translation, so-called phrase-based models,is limited to the mapping of small text chunks with-out any explicit use of linguistic information, mayit be morphological, syntactic, or semantic. The translation of the inputsentence f into the output sentence e breaks down toa set of phrase translations {(fj , ej)}.For a translation step component, each featurefunction hT is defined over the phrase pairs (fj , ej)871given a scoring function  (fj , ej) (3)For a generation step component, each featurefunction hG given a scoring function  (ek) (4)The feature functions follow from the scoringfunctions ( For instance, re-call our earlier example: a scoring function for ageneration model component that is a conditionalprobability distribution between input and outputfactors, eg,  Efficient DecodingCompared to phrase-based models, the decomposi-tion of phrase translation into several mapping stepscreates additional computational complexity. In their ap-proaches, first, an n-best list with the best transla-tions is generated for each input sentence.
D07-1092@@It has been for a long time a faculty as-sessed in the so-called SAT Reasoning tests used inthe application process to colleges and universitiesin the United States. 84received (at least) a translation by ANALOG. , LN}gathers N observations.
D07-1096@@This defines a dependency graph, wherethe nodes are the words of the input sentence and thearcs are the binary relations from head to dependent.Often, but not always, it is assumed that all wordsexcept one have a syntactic head, which means thatthe graph will be a tree with the single independentword as the root. A fundamental algorithm fordependency parsing. on Computational Language Learning (CoNLL).S.
D07-1097@@Wecall this system Single Malt, to emphasize the factthat it consists of a single instance of MaltParser.The second parser is an ensemble system, whichcombines the output of six deterministic parsers,each of which is a variation of the Single Malt parserwith parameter settings extrapolated from the firststage of optimization. The names Top and Next refer to the token on top of thestack S and the first token in the remaining input I, respectively.selection. A fundamental algorithm fordependency parsing.
D07-1099@@It re-quires use of a single dependency parsing modelfor the entire set of languages; model parametersare estimated individually for each language on thebasis of provided training sets. Experiments were runon a standard 2. 42ndMeeting of Association for Computational Linguistics,Barcelona, Spain.R.
D07-1100@@Making use of as many informative features as pos-sible is crucial to obtain high performance in ma-chine learning based NLP. Construction of a Basque Dependency Tree-bank. The probabil-ity distribution P,M(h|w) is a joint distribution ofall the heads conditioned by a sentence, thereforewe call this model sentence-level model.
D07-1101@@Structured prediction problems usually involvemodels that work with factored representations ofstructures. Right:a closed structure for the chart entry [h, e, m]C ; the algorithmlooks for the l and cmo that yield the optimal score. n] is the index of the modifier token, andl  L] is the label of the dependency.
D07-1102@@This is primarily due to the ability toincorporate complex structural features that cannotbe modeled under a CFG. ThePDT: a 3-level annotation scenario. In Pro-ceedings of the 43nd Annual Meeting of the Association forComputational Linguistics.S.
D07-1103@@This has led toan impression that these pairs must contribute some-thing in the grand scheme of things and, certainly,more data is better than less.Nonetheless, this bulk comes at a cost. It can be smaller though becausemultiple co-occurrences can occur within a singlealigned sentence pair and be counted multiple timesin c(s, t). of source / tar-get n,m-grams observed in a word-aligned parallelcorpus.
D07-1104@@Current statistical machine translation systems relyon very large rule sets. This requires O(n log log |T |)time for n items. As-suming that A and E are roughly the size of F , thecost is 4|T |.
D07-1106@@ Increasingly, short passages or web pages are be-ing translated by desktop machine translation soft-ware or are submitted to machine translation ser-vices on the Web every day. ), which may not be handled properly by a machine translation system. Now, we formally state the problem we are deal-ing with:  While a proper name N is given.
D07-1107@@Perhaps the greatest obstacle isthe dynamic nature of sense definition: the correctgranularity for word senses depends on the appli-cation. AutomaticGeneration of a Coarse Grained WordNet. I dont believe in word senses.Computers and the Humanities, 31(1-2): 1-13.Adam Kilgarriff.
D07-1108@@Natural language tends to be ambiguous. A Maximum Entropy Model forPart-of-Speech Tagging. Note that L canbe different from G, which is the number of bag-of-words in baseline features.
D07-1111@@The dependency parsing approach pre-sented here extends the existing body of work mainly in four ways:  1. E. Briscoe and J. Carroll. A First Language: The Early Stages.
D07-1112@@Dependency parsing, an important NLP task, can bedone with high levels of accuracy. Multilingual dependency parsing with a two-stage discriminative parser. We trieda number of criteria to weigh sentences without suc-cess, including sentence length and number of verbs.Next, we trained a discriminative model on the pro-vided unlabeled data to predict the domain of eachsentence based on POS n-grams in the sentence.Training sentences with a higher probability of be-ing in the target domain received higher weights,also without success.
D07-1113@@ As a growing number of people use the Web as a medium for expressing their opinions, the Web is becoming a rich source of various opinions in the form of product reviews, travel advice, social issue discussions, consumer complaints, stock market predictions, real estate market predictions, etc. One consists of opinions such as I like/dislike it, and the other consists of opinions like It is likely/unlikely to happen. We proposed a novel technique which generalized n-gram feature patterns.
D07-1114@@The explosive increase in Web communication hasattracted increasing interest in technologies for auto-matically mining personal opinions from Web doc-uments such as product reviews and weblogs. Cambridge : Cambridge University Press.T. Associative anaphora resolution: a web-based approach.
D07-1115@@Sentiment analysis is a recent attempt to deal withevaluative aspects of text. One is based on a the-saurus. Experimental result demonstratedthe feasibility of our approach.Acknowledgement This work was supported bythe Comprehensive Development of e-Society Foun-dation Software program of the Ministry of Edu-cation, Culture, Sports, Science and Technology,Japan.
D07-1116@@In Modern Standard Arabic (MSA), all nouns andadjectives have one of three cases: nominative(NOM), accusative (ACC), or genitive (GEN). The corresponding nunated formsfor these three diacritics are:  +u for GEN. Nominals not ending withTa Marbuta (  h) receive anextra Alif in the accusative indefinite case (e.g,   fl  ffikitAbA ).Diptotes are like triptotes except that when theyare indefinite, they do not express nunation and theyuse the +a suffix for both ACC and GEN. Modern Literary Arabic: A Refer-ence Grammar.
D07-1117@@Part-of-speech (POS) tagging is potentially help-ful for many advanced natural language processingtasks, for example, named entity recognition, pars-ing, and sentence boundary detection. 101Michael Collins, Robert E. Schapire, and Yoram Singer. A maximum entropy modelfor part-of-speech tagging.
D07-1123@@This difference is a nat-ural consequence of their respective parsing strate-gies: CKY-style maximization of link score and in-cremental parsing.In this paper, we describe an attempt to unify thetwo approaches: an incremental parsing strategy thatis trained to maximize performance over sentencesrather than over individual parsing actions. and n, where nis the head and n the dependent.Actions Parser actions ConditionsInitialize nil, W,  Each parsing action was assigned ascore, and the beam search allows us to find a bet-ter overall score of the sequence of actions. CESS-ECE: A multilingual and multilevelannotated corpus.
D07-1124@@One of the main advantages of de-terministic parsing lies in the ability to use the sub-tree information in the features to decide the nextstep. Unigram and trigram combinations(with t and n) of the lexical items, POS tags, CPOStags of these words are part of this context featureset. For every sentence in the training data,starting with the initial configuration (nil,W,  ),weights are adjusted to satisfy the above constraintsbefore proceeding to the next correct configuration.This process is repeated till we reach the final con-figuration (S,nil,E).
D07-1125@@In this paper we focus on two things. n2 ) for an n-word sentence, where c issome constant time in which the LINK operationcan be performed. Alarger model used for the datasets of Catalan andHungarian adds the h(j/i) feature from every cate-gory.
D07-1126@@However, unlike maximum-marginparsing, it is not limited to parsing sentences of 15words or less due to computation time. CESS-ECE: A multilingual and multilevelannotated corpus. In Conference on Natural LanguageLearning.S.
D07-1127@@Dependency parsing is a topic that has engenderedincreasing interest in recent years. the last dependent 8assigned to a head  . In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL05).S.
D07-1128@@The Pro3Gres parser is a dependency parser thatcombines a hand-written grammar with probabilis-tic disambiguation. A First Language: The Early Stages.Harvard University Press.John D. Burger and Sam Bayer. Asso-ciation for Computational Linguistics.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and Mark John-son.
D07-1129@@We have recently seen growing popularity of depen-dency parsing. If not, there must be a cycle. (e))The input to the above classifiers is an edge e in-stead of a whole sentence x.  is a mapping froman edge to a feature vector.
D07-1130@@The CoNLL07 domain adaptation task was createdto explore how a parser trained in one domain mightbe adapted to a new one. We then usedModel D to rank the parses for the unannotatedin-domain data (PBIOTB unsupervised), and de-rived Model E from the combined counts from thehighest-ranked parses for all of the training and de-velopment data. is automatically derivedfrom a unification grammar.
D07-1131@@ The target of dependency parsing is to automatically recognize the head-modifier relationships between words in natural language sentences. There are three basic operation (action) types: Shift (S), Left (L), and Right (R). Construction of a Basque Dependency Treebank.
D08-1002@@Although RTE-3 contains a wide variety ofcontradictions, it does not reflect the prevalence ofseeming contradictions and the paucity of genuinecontradictions, which we have found in our corpus. Butother types of contradictions can only be detectedwith the help of a body of background knowledgeK: In these cases, T and H alone are mutually con-sistent. does not denote a functional relation.
D08-1007@@Selectional preferences (SPs) tell us which argu-ments are plausible for a particular predicate. While none occur with damage in6I.e. % for a final unseentest set.
D08-1008@@Automatic semantic role labeling (SRL), the taskof determining who does what to whom, is a use-ful intermediate step in NLP applications perform-ing semantic analysis. The weight vector w is updated incrementally,one example at a time. Features and SearchThe feature functionsyn is a factored represen-tation, meaning that we compute the score of thecomplete parse tree by summing the scores of itsparts, referred to as factors: This algorithm has a time complexity ofO(n4), where n is the number of words in the sen-tence.
D08-1009@@While most TI researchers havefocused on high-quality inferences from a smallsource text, we seek to utilize sizable chunks of theWeb corpus as our source text. I.e., most Horn clauseinference rules will be trivially satisfied since theirantecedents will be false, so we only need to worryabout ones where the antecedent is true. k. When the precisevariable and degree is irrelevant to discussion, wesimply say R is PF.
D08-1011@@* System combination has been applied successfully to various machine translation tasks. In our method, 2 ( | )t s ip null e  for all target words is simply a constant pnull, whose value is optimized on held-out data 1. A systematic comparison of various statistical alignment models.
D08-1012@@In the absence of an n-gram language model, decod-ing a synchronous CFG translation model is veryefficient, requiring only a variant of the CKY al-gorithm. This projection ex-tends to the whole search state in the obvious way:assuming a bigram language model, the state l-X-rprojects to c(l)-X-c(r), where c() is the determin-istic word-to-cluster mapping.In our multipass approach, we will want a se-quence c1 . 6th Workshop on VeryLarge Corpora.E.
D08-1014@@We have seen a surge in interest towards the ap-plication of automatic tools and techniques for theextraction of opinions, emotions, and sentiments intext (subjectivity). A newmethod for sentiment classification in text retrieval. By using machine translation, from131OpinionFinder classifier P R Fhigh-coverage 67.
D08-1016@@Computational linguists worry constantly about run-time. E.g., a given verbmay not like to have two noun children both to itsleft. The n-word input sentence W is fullyobserved (not a lattice).
D08-1018@@Binarization, which transforms an n-ary grammarinto an equivalent binary grammar, is essential forachieving an O(n3) time complexity in the context-free grammar parsing. We simplify the opti-mal binarization to be:pi argminpi E(pi(G), C) (2)where E(pi(G), C) is the number of constituentsgenerated when CKY parsing C with pi(G).We next discuss how binarizations affect the num-ber of constituents generated in parsing, and presentour algorithm for finding a good binarization. It leadsto a compact grammar.
D08-1019@@Automatic text summarization is a rapidly develop-ing field in computational linguistics. Subcategorization lexicons are not readilyavailable for many languages and cover only verbs.E.g. l isthe number of clause nodes above w and N is themaximum level of embedding of the sentence whichw belongs to.
D08-1020@@The quest for a precise definition of text qualitypinpointing the factors that make text flow and easyto readhas a long history and tradition. Automated essay scoringwith e-rater v. . t|s| |t|(3)4Other cohesion building devises discussed by Hallidayand Hansan include lexical reiteration and discourse relations,which we address next.
D08-1021@@Paraphrases are alternative ways of expressing thesame information. CCG-labels(ei+ni , P )}The function CCG-labels describes the set of CCG-labels for the phrase spanning positions i to i+ n increate equalVP/(NP/NNS) create equal . A parallel corpus for statisticalmachine translation.
D08-1023@@The goal of creating statistical machine translation(SMT) systems incorporating rich, sparse, featuresover syntax and morphology has consumed muchrecent research attention. As the training data only providessource and target sentences, the derivations are mod-elled as a latent variable.The conditional probability of a derivation, d, fora target translation, e, conditioned on the source, f ,is given by:p (e, f) is the set of all derivations of thetarget sentence e from the source f.Here k ranges over the models features, and The features can referencethe entire source sentence coupled with each rule, r,and its target context, in a derivation.By directly incorporating the language modelcontext q into the model formulation, we will not216be able to exactly compute the partition functionZ (f)This model formulation raises the questions ofwhat an appropriate large subset of derivations fortraining is, and how to efficiently calculate the sumover all derivations in decoding. (e|f) model wesuffer the same problem as in training and cannotbuild the full chart.
D08-1024@@Eachof their new features rewards or punishes a deriva-tion depending on how similar or dissimilar it isto a syntactic parse of the input sentence. A practical minimal perfect hashingmethod. Basic algorithmLet e, by abuse of notation, stand for both outputstrings and their derivations.
D08-1027@@The construction of these datasets, how-ever, is extremely expensive in both annotator-hoursand financial cost. For each tokent in our training set, we assign t a weight for eachemotion e equal to the average emotion score ob-served in each headline H that t participates in. Butwe also found that adding non-experts to the goldstandard (E vs. improves agreement, suggest-ing that non-expert annotations are good enough toincrease the overall quality of the gold labels.
D08-1029@@ Cross-document entity co-reference is the problem of identifying whether mentions from different documents refer to the same or distinct entities. A Testbed for People Searching Strategies. (A) Name Strings:  (B) Name String Pairs with Score:  0.
D08-1031@@Coreference resolution is the task of grouping all thementions of entities1 in a document into equivalenceclasses so that all the mentions in a given class referto the same discourse entity. Finally, drepresents a document from the set D, and N is thetotal number of mentions in D.B-Cubed F-Score has the advantage of being ableto measure the impact of singleton entities, and ofgiving more weight to the splitting or merging oflarger entities. This featureis a proxy for having similar semantic types..
D08-1033@@In phrase-based translation, statistical knowledgeof translation equivalence is primarily captured bycounts of how frequently various phrase pairs occurin training bitexts. This way, we discour-age unaligned phrases while focusing learning on J.For simplicity, we reuse Pf (f) and Pe(e) from theprior over J. A bayesian analysis of somenonparametric problems.
D08-1034@@The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. (the insurance company) is a good example. Window size [-m, n] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu-ments will be utilized for the classification of cur-rent semantic role.
D08-1035@@Topic segmentation is one of the fundamental prob-lems in discourse analysis, where the task is todivide a text into a linear sequence of topically-coherent segments. We define B(t) as thevalue of the objective function for the optimal seg-mentation up to sentence t. The contribution to theobjective function from a single segment betweensentences t Cue PhrasesOne of the key advantages of a Bayesian frameworkfor topic segmentation is that it permits the prin-cipled combination of multiple data sources, even3This assumes that the objective function for individual seg-ments can also be computed efficiently. Thanks to Aaron Adler, S. R. K. Branavan,Harr Chen, Michael Collins, Randall Davis, DanRoy, David Sontag and the anonymous reviewers forhelpful comments and suggestions.
D08-1037@@Named entity (NE) transliteration is the process oftranscribing a NE from a source language to sometarget language based on phonetic similarity be-tween the entities. R. Golding and D. Roth. A Winnow basedapproach to context-sensitive spelling correction.
D08-1038@@How can we identify and study the exploration ofideas in a scientific field over time, noting periods ofgradual development, major ruptures, and the wax-ing and waning of both topic areas and connectionswith applied topics and nearby fields ideas and vocabulary are con-strained by their paradigm, successive incommensu-rate paradigms will naturally have different vocabu-lary and framing.Kuhns model is intended to apply only to verylarge shifts in scientific thought rather than at themicro level of trends in research foci. (z|y) as the empirical probability that an arbitrarypaper d written in year y was about topic z:p (d|y) is set to a constant 1/C. A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text.
D08-1039@@Data-driven methods have been applied very suc-cessfully within the machine translation domainsince the early 90s. 0 with the constraint such thate, e will be referred to asthe first and the second trigger, respectively. The experiments were carried outon a 2.
D08-1043@@Community-driven question answering services,such as Yahoo! 1sNic(t|s;Ji) (5)where s is a normalization factor to make the sumof translation probabilities for the word s equal to 1,N is the number of parallel string pairs, and Ji is theith parallel string pair. IBM Translation Model 1Obviously, we need to build a translation model inadvance.
D08-1044@@Over the past couple of decades, the field of compu-tational linguistics (and more broadly, human lan-guage technologies) has seen the emergence andlater dominance of empirical techniques and data-driven research. approach forcomputing word co-occurrence matrices.partial results across a cluster. I would like to thank Ya-hoo!
D08-1047@@requiresthe distance between the source string s and a can-didate string t to be less than .The choice of dist(s, t) and  involves a tradeoffbetween the precision, recall, and training/taggingspeed of the scorer. For example, we find an expandedsubstitution rule (na, n to the head of the minimum substitu-tion rule (a,  ); similarly, we obtain an expandedsubstitution rule (ae, e to the tail of the rule (a,  Letters in blue, green, and red respectivelyrepresent the longest common prefixes, longest com-mon postfixes, and different portions. In line 4, the code obtains a set of positivesubstitution rules F that can rewrite substrings start-ing at offset #i in the source string s. For each rulef  The candidate string t is qualified to be includedin gen(s) when the classifier assigns a positive labelto strings s and t (lines 8 and 9).
D08-1048@@Most inference-based NLP tasks require a largeamount of semantic knowledge at the predicate-argument level. These casesare due to the low FrameNet coverage, i.e LUs arenot fully annotated and they appear only in a subsetof their potential frames. Sensenumbers #n refers to WordNet 2.0.Using this model, LU induction is performed asfollows.
D08-1049@@Opinions, sentiments and other types of subjectivecontent are an important part of any meeting. (5) Um POS-SUBJ its very easy to use.Um NEG-SUBJ but unfortunately it doeslack the advanced functions POS-SUBJwhich I I quite like having on the controls.The positive and negative subjective category is formarking cases of positive and negative subjectivitythat are so closely interconnected that it is difficultor impossible to separate the two. Selection of n-grams is performedby the learning algorithm.
D08-1050@@A re-lated question is how to improve the performanceof these parsers on constructions that are rare in thePenn Treebank, such as questions. a semantically an-notated corpus for bio-textmining. Style-based text categorization: Whatnewspaper am I reading Outside the cave of shadows: Using syntacticannotation to enhance authorship attribution.
D08-1052@@may appearself-contradictory at first sight, since incrementalparsing usually means left-to-right parsing in thecontext of conventional parsing. In International Work-shop on Parsing Technologies.E. Journal ofPhysics A, 20:745752.J.
D08-1054@@Topic models are probabilistic and generative mod-els representing contents of documents. The topic distribution of d is dand topic distributions of the cited documents arei, i  represents the topic distributions ofall the documents. As a resultcontents of documents can be propagated For example, suppose web page A citespage B and page B cites page C, then the content ofpage A is influenced by that of page B, and the con-tent of page B is further influenced by the contentof page C. Therefore, HTM is able to more accu-rately represent the contents of hypertexts, and thusis more useful for text processing such as topic dis-covery and document classification.. cor-respond to the posterior distributions of their asso-ciated random variables.
D08-1055@@in the sentence During my trip to Italy, Imet him. Semanticrole labeling of NomBank: A maximum entropy ap-proach. (financial services of Japan and U.S.) 0.
D08-1058@@ In recent years, sentiment analysis (including sub-jective/objective analysis, polarity identification, opinion extraction, etc.) T. Mullen and N. Collier. 7 http://www.keenage.com/html/e_index.html 8 http://www.cs.pitt.edu/mpqa/ 9 http://www.wjh.harvard.edu/~inquirer/homecat.htm 556The semantic orientation value f kSO(revk) for revk is computed by summing the polarity values of all words in the review, making use of both the word polarity defined in the positive and negative lexi-cons and the contextual valence shifters defined in the negation and intensifier lexicons.
D08-1059@@Given an input sentence, a graph-based algo-rithm finds the highest scoring parse tree from allpossible outputs, scoring each complete tree, whilea transition-based algorithm builds a parse by a se-quence of actions, scoring each action individually.The terms graph-based In this paper,we do not differentiate graph-based and transition-based parsers by their search algorithms: a graph-based parser can use an approximate decoder whilea transition-based parser is not necessarily determin-istic. (T, s) is the feature vector extracted from the ac-tion T and the context s, and ~w is the weight vec-tor. After all input words are pro-cessed, the best candidate output from the agenda istaken as the final output.The projectivity of the output dependency treesis guaranteed by the incremental Covington process.The time complexity of this algorithm is O(n2),where n is the length of the input sentence.During training, the early update To give more templates, fea-tures from templates 1  refers to the difference between word in-dexes.
D08-1060@@One of the main issues that a translator (human ormachine) must address during the translation pro-cess is how to match the different word orders be-tween the source language and the target language.Different language-pairs require different levels ofword reordering. Chart-based DecoderGiven the source sentence, with constituent parse-trees, the decoder is to find the best derivation Dwhich yield the English string e:e (D) is the cost for each of the derivationsthat lead to e from a given source-parsed f ;  (e)is for cost functions from the standard n-gram lan-guage models;  (f |e) is the cost for the standardtranslation models, including general blocks. a special case inour proposed rules.
D08-1061@@MotivationUsers of large document collections can readily ac-quire information about the instances, classes, andrelationships described in the documents. We are given a2In practice, we use two directed edges, from i to C andfrom C to i, both with weight w. 84bob dylanmusician0. T consists of one or more columns.
D08-1064@@to a searching criticism, with two real-world case studies of significant failures of corre-lation between B Both cases involve comparisons betweenstatistical MT systems and other translation meth-ods (human post-editing and a rule-based MT sys-tem), and they recommend that the use of B berestricted to comparisons between related systems ordifferent versions of the same systems. is not a sentence-level metric. The dynamic program for WRR can beformulated as a Viterbi search through a finite-stateautomaton: given a candidate sentence c and a refer-ence sentence r, find the highest-scoring path match-ing c through the automaton with states 0, .
D08-1065@@Statistical language processing systems for speechrecognition, machine translation or parsing typicallyemploy the Maximum A Posteriori (MAP) deci-sion rule which optimizes the 0-1 loss function. Z(E) andZ(Ew) represent the sums2 of weights of all pathsin the lattices Ew and E respectively. A Statistical Approach to MachineTranslation.
D08-1067@@Coreference resolution is the problem of identifyingwhich mentions (ie, noun phrases) refer to whichreal-world entities. in the M-step, we proceedto the E-step, where the goal is to find the condi-tional clustering probabilities. We identify the N most probable clusterings andcompute their probabilities as follows.
D08-1068@@The goal of coreference resolution is to identifymentions (typically noun phrases) that refer to thesame entities. Thus we introduce predicates thatrepresent entity type, number, and gender:Type(x, e! We detected pronouns using a list.
D08-1069@@Coreference resolution is the task of partitioning aset of entity mentions in a text, where each par-tition corresponds to some entity in an underlyingdiscourse model. A fewexceptions are worth noting. Journal ofLinguistics, pages 6587.N.
D08-1072@@Statistical classifiers routinely process millions ofwebsites, emails, blogs and other text every day.Variability across different data sources means thattraining a single classifier obscures differences andseparate classifiers ignore similarities. This strategy yields the fol-lowing objective solved on each round of learning:min DKL (N (,) The constraint ensures that the re-sulting parameters(i+1,i+1)will correctly clas-sify xi with probability at least . was tuned on a single randomized runfor each experiment.
D08-1073@@Being able to temporally order events is a neces-sary component for complete document understand-ing. Nodes rep-resent events and times (event nodes start with theletter e, times with t ), and edges represent tempo-ral relations. These can lead tosuboptimal configurations, although they are guar-anteed to find a solution.
D08-1074@@In a language like English, tense is an explicit (andmaybe imperfect) grammaticalization of the tempo-ral location of situations, and such temporal locationis either directly or indirectly defined in relation tothe moment of speech. Elements of Symbolic Logic.The MacMillan Company, New York.Carlota S. Smith and Mary Erbaugh. The training set used here is suffi-cient to show the efficacy of the features, but to im-prove the tense classification to a satisfactory levelof accuracy, more training data need to be annotated.Feature accuracy (w/o)Governing verb/tense 0.
D08-1075@@In this paper we present a machine learning systemthat finds the scope of negation in biomedical texts.The system consists of two classifiers, one that de-cides if the tokens in a sentence are negation sig-nals (ie, words indicating negation), and anotherthat finds the full scope of these negation signals.Finding the scope of a negation signal means deter-mining at sentence level which words in the sentenceare affected by the negation. Classes O-NEG and I-NEG are among the most frequent and get highscores. ofInternational Workshop Language and Speech Infras-tructure for Information Access in the Balkan Coun-tries, pages 18, Borovets, Bulgaria.S.
D08-1076@@Many statistical methods in natural language pro-cessing aim at minimizing the probability of sen-tence errors. Then we performan s-t cut and split G into two subgraphs G1 (leftsubgraph) and G2 (right subgraph). The runtime complexity is thereforedominated by the initial sorting and amounts toOpK logKqTopological sort on a phrase lattice G  pV, Eqcan be performed in time p|V|   |E |q.
D08-1078@@Statistical machine translation (SMT) has improvedover the last decade of intensive research, but forsome language pairs, translation quality is still low.Certain systematic differences between languagescan be used to predict this. The dependent variable is thetranslation performance metric, the BLEU score.We then use a t-test to determine whether the co-efficients for the independent variables are reliablydifferent from zero. A study of translation edit rate withtargeted human annotation.
D08-1080@@Topic-driven summarization reflects a user-basedsummarization task: from a set of documents de-rive a summary that contains information on a spe-cific topic of interest to a user. for the country, the bird, cities with this namein the U.S. among others. A spreading activation theoryof memory.
D08-1081@@Our lives are increasingly comprised of multimodalconversations with others. A weakness779System Pyramid Precision AUROCRambow 0. 5.Pyramid precision and weighted f-score are simi-lar evaluation schemes in that they are both sentencebased (as opposed to, for example, n-gram based)and that they score sentences based on multiple hu-man annotations.
D08-1083@@Determining the polarity of sentiment-bearing ex-pressions at or below the sentence level requiresmore than a simple bag-of-words approach. (n refers to the number of negators found in a given expression. JMLR3:951.David R. Dowty, Robert E. Wall and Stanley Peters.
D08-1084@@The problem of natural language inference (NLI) isto determine whether a natural-language hypothesisH can reasonably be inferred from a given premisetext P . After initializing w to 0, we perform N train-ing epochs. A Systematic Comparison ofVarious Statistical Alignment Models.
D08-1085@@In this paper,we introduce an exact decipherment method basedon integer programming. In Pro-ceedings of NLDB: 4th International Conference onApplications of Natural Language to Information Sys-tems.Ravi Ganesan and Alan T. Sherman. Given acipher length of n, the network has 27  Each link corresponds to a named variablein our integer program.
D08-1089@@The re-ordering models used in the original phrase-basedsystems penalize phrase displacements proportion-ally to the amount of nonmonotonicity, with no con-sideration of the fact that some words are far moreM M D S D! For both C-E andA-E, we manually removed documents of Gigawordthat were released during periods that overlap withthose of our development and test sets. A hierarchical phrase-based modelfor statistical machine translation.
D08-1090@@While the amount of parallel data available to train astatistical machine translation system is sharply lim-ited, vast amounts of monolingual data are generallyavailable, especially when translating to languagessuch as English. )ePr(e|D) Pr(f |e))This formulation favors longer English docu-ments over shorter English documents. % im-provement in BLEU and a 0.
D08-1091@@The parse trees represent the desired outputof the system, while the derivation trees representthe typically much more complex underlying syntac-tic processes. Forexample, NP might be split into NP1 through NP8.The parameters of the refined productionsAx  The probability of a derivationt of a sentence w is proportional to the product of theweights of its productions r:P (t|w)  If each categoryis split into k subcategories, each (binary) produc-tion will be split into k3. A. Smith and M. Johnson.
D08-1092@@Unfortunately, parsing gen-eral bitexts well can be a challenge for newswire-trained treebank parsers for many reasons, includingout-of-domain input and tokenization issues.On the other hand, the presence of translationpairs offers a new source of information: bilin-gual constraints. as the F1-optimal subset of T (T ). Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
D08-1093@@Ideally, one would want tohave available a recipe for precisely answering thisquestion: given a parser and a particular domain ofinterest, how accurate are the parse trees produced This recipe, albeit cheap, cannot provideany guarantee regarding the performance of a parseron a new domain, and, as experiments in this papershow, can give wrong indications regarding impor-tant decisions for the design of NLP systems thatuse a syntactic parser as an important component.This paper proposes another method for measur-ing the performance of a parser on a given domainthat is both cheap and effective. A clozetest authoring system and its automation. We create a rootSYN feature based onthe syntactic category found at the root of the out-put tree (is it S We also createa puncSYN feature based on the number of wordslabeled as punctuation tags (based on the intuitionthat heavy use of punctuation can be indicative ofthe difficulty of the input sentences), and a label-SYN feature in which we bundled together informa-tion regarding the number of internal nodes in theparse tree output that have particular labels (howmany nodes are labeled with PP).
D08-1095@@Graph-based similarity measures have been usedfor a variety of language processing applications.In this paper we assume directed graphs, wheretyped nodes denote entities and labelled directedand weighted edges denote the relations betweenthem. Let each path p be as-sociated with its count, within the paths leading tothe correct nodes, denoted as C+p . A graph-search framework for associating gene identifiers withdocuments.
D08-1097@@An important step in question answering (QA) andother dialog systems is to classify the question tothe anticipated type of the answer. Checks their definitions in a dictionary3. For example, as the headword walrus of question What is the proper namefor a female walrus has the highest similarity mea-sure to animals, which is a description word of cat-egory ENTY:animal, thus the ENTY:animal is in-serted into the feature set of the given question..  N-GramsAn N-gram is a sub-sequence of N words from agiven question.
D08-1103@@In itsstrictest sense, antonymy applies to gradable adjec-tives, such as hotcold and tallshort, where thetwo words represent the two ends of a semanticdimension. purify d. criticize e. correctHere the target word is adulterate. 4 with a standard deviation of 2.
D08-1104@@Some phrases like kick the bucket are ambiguouswith regard to whether they carry literal or idiomaticmeaning in a certain context. NEG represents a verbal negation morpheme. 7Passivization is indicated by the suffix (r)are in Japanese.But the same suffix is also used for honorification, potentialsand spontaneous potentials.
D08-1105@@In language, many words have multiple meanings.The process of identifying the correct meaning, orsense of a word in context, is known as word sensedisambiguation (WSD). Significancetests using one-tailed paired t-test reveal that theseaccuracy improvements are statistically significantat the level of significance 0.01 (all significance testsin the rest of this paper use the same level of signif-icance 0.01). Also, these prior research efforts onlyexperimented with a few word types.
D08-1109@@In this paper, we explore the application of multilin-gual learning to part-of-speech tagging when no an-notation is available. Note that in all cases we condi-tion on the other sampled variables as well as theobserved words and alignments, e, f and a, whichare kept fixed throughout. Thus the values of m and n dependon the draw.
D08-1112@@Traditional supervised learning algorithms usewhatever labeled data is provided to induce a model.By contrast, active learning gives the learner a de-gree of control by allowing it to select which in-stances are labeled and added to the training set. To study the effects of nor-malization, we also conduct experiments with non-normalized variants TV E and TKL.Additionally, we argue that these token-level dis-agreement measures may be less appropriate formost tasks than measuring the committees disagree-ment about the label sequence y as a whole. marginal probability that m is the labelat position t in the sequence.Both of these disagreement measures are normal-ized for sequence length T .
D08-1113@@A recurring problem in computational linguisticsand language processing is transduction of charac-ter strings, eg, words. Forthe most part, the 2 classes are separated based onwhether or not the correct output ends in e. Thisuse of latent classes helped address many errors likewronging / wronge or owed / ow). Thebox marks a trigram to be scored.
E06-1002@@MotivationThe de-facto web search paradigm defines the re-sult to a users query as roughly a set of links to thebest-matching documents selected out of billionsof items available. Its redirect pagescorrespond to acronyms (U.S.A., U.S., USA, US),Spanish translations (Los Estados Unidos, Esta-dos Unidos), misspellings (Untied States) or syn-onyms (Yankee land).For any given Wikipedia entity e2E, let e:R bethe set of all names that redirect to e. . Cross-documentcoreference on a large scale corpus.
E06-1003@@In addition, by definition, the set ofclasses in C changes as a new ontology is consid-ered, making the creation of annotated data almostimpossible practically. The Se-mantic Web: A New Opportunity and Challenge forHLT. In ourexperiments for ontology population we used thepatterns described in the Hearsts paper plus thepattern t is (a | the) c:1. t is (a | the) c2.
E06-1004@@Themodels are independent of the language pair andtherefore, can be used to build a translation sys-tem for any language pair as long as a parallelcorpus of texts is available for training. in polynomial time, thereexists a machine that can evaluate g(.) , n.This problem is the same as that of counting thenumber of perfect matchings in a bipartite graphand is known to be #P-Complete ().
E06-1005@@In this work we describe a novel technique forcomputing a consensus translation from the out-puts of multiple machine translation systems.Combining outputs from different systemswas shown to be quite successful in automaticspeech recognition (ASR). Fourth Conference on AppliedNatural Language Processing, Stuttgart, Germany.E. A Systematic Comparisonof Various Statistical Alignment Models.
E06-1009@@The goal of spoken dialogue systems (SDS) isto offer efficient and natural access to applica-tions and services, such as email and calendars,travel and entertainment booking, and product rec-ommendation. 507802), http://www.talk-project.org.The first author was supported by EvangelischesStudienwerk e.V. Developing a flexible spoken dialog systemusing simulation.
E06-1010@@Data-driven approaches to syntactic parsing hasuntil quite recently been limited to representationsthat do not capture non-local dependencies. The degree of an arc e  E is the number ofconnected components c in Ge such that theroot of c is not dominated by the head of e. . The node 0 is a root (ROOT).
E06-1011@@Thissentence is an example of a projective (or nested)tree representation, in which all edges can bedrawn in the plane with none crossing. IWPT.A 2nd-Order Non-projective MSTParsing is NP-hardProof by a reduction from 3-D matching (3DM).DM: Disjoint sets X,Y,Z each withm distinct elementsand a set T  XY Z occurs in exactlyone element of S.Reduction: Given an instance of 3DM we defi ne a graphin which the vertices are the elements from X  We insert edges from root toall xi  X as well as edges from all xi  We order the words s.t. A statistical parser for Czech.
E06-1012@@The availability of treebanks of various sorts havefostered the development of statistical parserstrained with the structural data in these tree-banks. i away|i)Here i represents the context around the depen-dent wi and H(i), represents the context aroundthe head word. ParserStatistical dependency parsers first compute theprobabilities of the unit-to-unit dependencies, andthen find the most probable dependency tree T among the set of possible dependency trees.
E06-1014@@In modeling a collection of documents for infor-mation access applications, the documents are of-ten represented as a bag of words, ie, as termvectors composed of the terms and correspondingcounts for each document. A similarity-based probabilitymodel for latent semantic indexing. LSA-based Initialization of PLSAThe EM algorithm for estimating the parametersof the PLSA model is initialized with estimates ofthe model parameters   H  $   JLfi H  $  Gfi H.
E06-1015@@Intuitively,an ST rooted in a node n of the target tree alwayscontains all ns descendants until the leaves. We present (a) an algorithm for the eval-uation of the ST and SST kernels which runs inlinear average time and (b) a study of the impactof diverse tree kernels on the accuracy of SupportVector Machines (SVMs).Our fast algorithm computes the kernels be-tween two syntactic parse trees in O(m + n) av-erage time, where m and n are the number ofnodes in the two trees. A subset tree (SST) is a more general struc-ture.
E06-1016@@The occurrences of the senses of a word usuallyhave skewed distribution in text. In Proceedingsof the Conference on Recent Advances on NaturalLanguage Processing (RANLP03), Bulgaria.J.R.L. Pseudo-thesaurus-sense-tagged (PTST)data for a non-monosemous target word t (forexample, brilliant) used in a particular sense orcategory c of the thesaurus (for example, intel-ligence) may be generated as follows.
E06-1019@@Bilingual word alignment finds word-level corre-spondences between parallel sentences. In particular, all of the methods we testwill use the same objective function to select thebest Let A be analignment for an English, Foreign sentence pair,(E,F ). The actualnumber of possible alignments lies between n!
E06-1022@@Addressing is an aspect of every form of commu-nication. of 42nd Meeting of the ACL.E. A speaker can also just think aloud or mum-ble to himself without really addressing anybody(egWhat else do I want to say (while try-ing to evoke more details about the issue that he ispresenting)).
E06-1025@@Opinion mining is a recent subdiscipline of com-putational linguistics which is concerned not withthe topic a document is about, but with the opinionit expresses. 21, with a relative deterioration of24. Note that, ateach iteration k, a given term t is added to Trkoonly if it does not already belong to either Trp orTrn.
E06-1029@@ Bilingual terminologies have been the center of much interest in computational linguistics. Finally, we build another thesaurus from a Japanese-English dictionary. I. Langkilde and K. Knight.
E06-1030@@Traditional written corpora for linguistics researchare created primarily from printed text, such asnewspaper articles and books. This is dueto the more accurate n-gram counts in the down-loaded text. PhD thesis, University of Edinburgh, UK.Andrew R. Golding and Dan Roth.
E06-1032@@Conference pa-pers routinely claim improvements in translationquality by reporting improved Bleu scores, whileneglecting to show any actual example transla-tions. rwhere c is the length of the corpus of hypothesistranslations, and r is the effective reference corpuslength. A parallel corpus for statisticalmachine translation.
E06-1034@@Annotated corpora serve as training material andas gold standard testing material for the devel-opment of tools in computational linguistics, andas a source of data for theoretical linguists search-ing for relevant language patterns. A lot of people think 0 I will giveaway/RP the store(4) a. Saturday s crash ... that *T* killed 132of the 146 people aboard/RBb. Using the variation n-gram output, weflagged every non-fringe variation nucleus (token)as a potential error, giving us 21,575 flagged po-sitions in the WSJ.
E06-1035@@In this paper, we focus on segmenta-tion of multiparty dialogues, in particular record-ings of small group meetings. Se-lect: a lexical cohesion based news story segmenta-tion system. In Proceedings of the IEEE InternationalConference on Acoustic, Speech, and Signal Pro-cessing, Philadelphia, USA.S.
E06-1037@@This paper presents initial research toward thelong-term goal of designing a tutoring system thatcan effectively adapt to the student. In IJCAI Wkshp.on K&R in Practical Dialogue Systems.T. Answers that involve a con-cept already introduced in the dialogue are calledShallow, answers that involve a novel concept arecalled Novel, I dont know type answers arecalled Assertions (As), and Deep answers refer toanswers that involve linking two concepts throughreasoning.
E06-1039@@Many organizations are faced with the challengeof summarizing large corpora of text data. Tracking andsummarizing news on a daily basis with ColumbiasNewsblaster. (ii) Classification: Thefeatures calculated during step (i) are combinedinto a single numerical score for each sentence.
E06-1040@@Evaluation is becoming an increasingly importanttopic in Natural Language Generation (NLG), asin other fields of computational linguistics. E. Foster and J. Oberlander. Riezler and J. T. Maxwell III.
E06-1041@@The problem of Generating Referring Expressions(GRE) can be summed up as a search for the prop-erties in a knowledge base (KB) whose combinationuniquely distinguishes a set of referents from their dis-tractors. By item In each of the eight test domains, anagreement score was calculated for each domainelement e (ie 13 scores in each domain). Techni-cal report, TUNA Project, University of Aberdeen.H.P.
E06-1042@@In this paper, we propose TroFi (Trope Finder),a nearly unsupervised clustering method for sep-arating literal and nonliteral usages of verbs. The kappastatistic: a second look. (S&C) on a random sam-ple of 200 annotated examples annotated by twodifferent annotators was found to be 0. refs therein), thestandard assessment for  values is that tentativeconclusions on agreement exists when .
E06-1043@@The term idiom has been applied to a fuzzy cat-egory with prototypical examples such as by andlarge, kick the bucket, and let the cat out of thebag. (e) The beans were spilled by Tim. Inter-estingly, although it is a PMI-based measure,KL M^_NQPR N`QSTSVUXWaYperforms slightly better when thedata is separated based on frequency.
E06-1045@@It has long been documented that there are char-acteristic facial displays that accompany the em-phasised parts of spoken utterances. In J. van Kuppevelt, L. Dy-bkjr, and N. Bernsen, editors, Advances in Natural, Mul-timodal Dialogue Systems. To address this question, weperformed a user evaluation.
E06-1047@@: Arabic DialectsThe Arabic language is a collection of spokendialects and a standard written language. Thisprocess also gives us MSA-only substitution prob-abilities P ( ).We then apply various transformation rules (de-scribed below) to the MSA elementary trees toproduce a dialect grammar, at the same time as-signing probabilities P ( The synchronous-substitution probabilities can then be estimated as:P (, is the equivalence class of modulo lexical anchors and their POS tags.P (w, t | w, t) is assigned as described in Sec-tion 3; P ( NEG is modified so that we simplyinsert a $ negation marker postverbally, as the pre-verbal markers are handled by MSA trees. It is typically followed by a possessive pro-noun.
E06-1052@@A crucial challenge for semantic NLP applica-tions is recognizing the many different ways forexpressing the same information. Learn-ing surface text patterns for a Question Answeringsystem. If a rule matches, the template sub-graph ismapped accordingly into the sentence graph.For example, to match the syntactic tem-plate X(N) subj (POStags are in parentheses) in the sentence Prot1detected and activated Prot2 Var1(V)to overcome the syntactic differences.
E06-2001@@One key issue for linguists and languagetechnologists is how to access it. The filter also works as a simple languageidentifier. Clear, and N. Ostler.
E06-2004@@: Business ProcessModelling and Contact CentresMany companies use business process models (BPMs) to specify communicative (andmany other) ac-tions that must be performed in order to complete vari-ous tasks (eg verify customer identity, pay a bill). This shows part of a BPM for acinema booking process. Black, and R. Caley.
E06-2007@@The goal of word sense discrimination is to clusterthe occurrences of a word in context based on itsunderlying meaning. 1) (2)When this ratio approaches 1, the clustering hasreached a plateau, and increasing k will have nobenefit. In Proceedings of the16th IEEE International Conference on Tools withAI, pages 576584.H.
E06-2009@@The in-car system described below has been con-structed primarily in order to be able to collect datafor Reinforcement Learning (RL) approaches to mul-timodal dialogue management, and also to test and fur-ther develop learnt dialogue strategies in a realistic ap-plication scenario. Lemon, and T. Oka. In 4th SIGdial Workshop on Discourse andDialogue, Sapporo.A.
E06-2015@@The motivation comes from the fact that cur-rent coreference resolution systems are mostly re-lying on rather shallow features, such as the dis-tance between the coreferent expressions, stringmatching, and linguistic form. Else if they both havea defined one and it is the same T; else F. Possible values are U(nknown), T(rue) and F(alse). for n semantic roles filled by aconstituent, where each semantic argument labelARGi is always defined with respect to a predicatelemma predi.
E06-2016@@GOD (General Ontology Discovery) is an un-supervised system to extract semantic relationsamong domain specific entities and concepts fromtexts. Words belongingto a limited number of domains are called domainwords. rectan-gular matrix D, containing the domain relevancefor each term with respect to each domain, wherek is the cardinality of the vocabulary, and k To this aim,term clustering algorithms can be adopted: eachcluster represents a Semantic Domain.
E06-2017@@The resultingdocument vectors reside in the space of latent se-mantic concepts which can be expressed using dif-ferent words. (1)Each document word w is a translation of a queryterm qi with probability t(qi|w). For the vocabulary words in V , obtain a ma-trix of pair-wise similarities, S, using thelarge corpus W3.
E06-2020@@New monitoring devices such as remote sensing sys-tems are generating vast amounts of spatio-temporaldata. An ability to vary the length of the text dependenton the context it was being used, i.e in a newspa-per or being read aloud.These issues will be dealt with subsequent releasesof the software. Trend: a system for generating in-telligent descriptions of time-series data.
E06-2025@@Stochastic Tree Substitution Grammars (hence-forth, STSGs) are a simple generalization of Prob-abilistic Context Free Grammars, where the pro-ductive elements are not rewrite rules but elemen-tary trees of arbitrary size. Fordisambiguation, the best parse tree is taken to bethe most probable parse according to the weightsof the grammar.Several methods have been proposed to decideon the weights based on observed tree frequencies1A subtree t either has no daughters orthe same daughter nodes as i.in a tree bank. I will refer to this test as the frequency-distribution test.
E06-2031@@Blogs, diary-like web pages containing highlyopinionated personal commentary, are becomingincreasingly popular. i Oi)/i Ni its expected frequencyin corpus i (where i takes values 1 and 2 for thestandard and sample corpus, respectively). Other moodsshow a weekly cycle.
E06-3005@@Until now, research in the field of automaticquestion answering (QA) has focused on factoid(closed-class) questions like who, what, whereand when questions. Building a QuestionAnswering Test Collection. The BNC Sampler.Oxford University Computing Services.Fellbaum, C. WordNet: An ElectronicLexical Database.
E06-3006@@ How-To questions constitute a large proportion of questions on the Web. We performed a pairwise t-test to test the significance of the difference between the results of the two systems with an integrated Adapted Naive Bayes classifier and of the two baseline systems. Question answering using a large NLP system.
E06-3009@@In example (1), for in-stance, China and Taiwan stand for the govern-ments of the respective countries:(1) China has always threatened to use forceif Taiwan declared independence. have a widervariety in heads. This fact, together3Precision, recall and F-score are given for the metonymi-cal class only, since this is the category that metonymy recog-nition is concerned with.Acc P R FTiMBL 74.
E83-1005@@From the linguistic point of view the FP is a model to simulate the operations carried out by an Italian speaker when reading aloud any text. To produce particular emphasis, i .e . Algorithm for the pbonematic transcription of 9raphems $ Z /z/ I YES NSJ YEa Y ~r ~I;N CXc ~TI~ ~ * ~  .~0 /~/  S Z /sl/z/ G o ~PR~:EDED~ Izzl IZ Ix /YES  /&/ 27 iv.
E83-1013@@ One of the most complicated phenomena in English is conjunction constructions. Nagao, M., Tsijii, J., Yada, K., and Kakimoto, T. "An English Japanese Machine Translation System of the Titles of Scientific and Engineering Papers". In (Blackwell 81), a WRD AND arc was proposed.
E83-1020@@ The problem of performing an accurate synta~ tic analysis of Natural Language sentences is still challenging for A.I. Charniak E.: Six Topics in Search of a Parser: An Overview of AI Language Research. people working in the field of N.L.
E85-1002@@Ne i ther  does it out l ine a work ing  computer  program,  nor  invest i  gate a linguistic problem with the help of computational methods. Determiners are some, e~e~,  each, all, both, no, neither,  many, fev;, most, a few, one, etc. A/though the subject may be pure ly  linguistic in character  it is par t i cu la r ly  re levant  to Computat ional  L ingu is t ics .
E85-1015@@ Our objective has been to build a parser for Finnish to work as a practical tool in real production applications. Nelimarkka, E, J~ppinen, H. and Lehtola A., Two-way Finite Automata and Dependency Theory: A Parsing Method for Inf lectional Free Word Order Languages. The working storage of DPL-parsers (*) SITRA Foundation P.O.
E85-1019@@The starting point for the present work is a set of familiar and, for the most part, uncontroversial c|~Lm~ s about the nature of grammatical description and of human parsing of natural language. In R. Luce, R. Bush and E. Galanter (eds) Handbook of Mathematical Psychology Vol II, New York: John Wiley. Unix on a Sun workstation.
E85-1021@@ It has been traditionally assumed by computational linguists and particularly by designers of large natural language processing systems such as machine translation systems that the lexicon should be limited to lexical information that cannot be derived by rules. This is far from being an isolate case, examples like the following are not hard to find: (5)a. comput-er b. trans-mission c. under-stand d. re-ply e. hard-ly The problem with these words is that they are morphologically composed of two or more morphemes, but their meaning is not derivable from the meaning of these morphemes. "KIMMO: A general morphological processor".
E85-1024@@ In this paper we present an overview of one part of the work currently being carried out at the Unit for Computer Research on the English Language (UCREL) in the University of Lancaster, under SERC research grant number GR/C/47700. A further stage was later inserted between the two stages described above. PROBLEMS AND CONCLUSIONS I have described the basic structure of the parsing system that we are currently developing at Lancaster.
E85-1025@@These developments also demonstrate that if natural language processing systems are to be able to handle the grammatical nd logical idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. A suite of programs to unscramble and restructure all the fields in LDOCE entries has been written which is capab|e of decoding all the fields except those providing cross-reference and usage information for complete homographs. Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information i  the dictionary via the dictionary head words.
E87-1003@@This paper describes work in a partic~dAr area of computational morphology, that of morphogra phemics. e * o  Z ""  Z which could be automatically generated f~om the in~t ion  of the surface and lexical alphabets. Note that there is a distinction between the /u,,e~_7!~ and its Imp~nentatlon.
E87-1006@@ About thirty years ago, the development of decripting tccniques made computer scientists be involved for the first time in the field of Linguistics, especially in automatic translation matters. * a morphosyntaetic analyzer ealized by three regular grammars, recognizing respectively compound tenses of verbs (e,g. A grammar has been written to describe the fragment of Italian consider.~l.
E87-1011@@  A growing mass of work at present, both within the narrower field of computational linguistics and in the wider context of building knowledge-based systems, is focussed on making use of the lexical resources to be found in a number  of (monolingual) dictionaries of the style exemplified by eg The Longman Dictionary of Contemporary English, The Collins English Dictionary, or Websters Seventh New Collegiate Dictionary. $TRE88 ONSET PEAK CODA I SYLLABLE ./ C---. The most e~cient search strategy involves using the most specific few constraints as lookup keys (more specific keys ultimately ielding fewer entries).
E87-1012@@In the worst case, a LKB has to be built up from scratch, and even if one is available, it often does not come up to the requirements of a particular application. an indexed-sequential fi e of phoneme representations could be derived from a dictionary containing this as well as oliver information, and stored in another way (eg as a sequential text file). The result is a spelling rhyme dictionary.
E87-1023@@ In this paper we address the problem of choosing the best solution(s) from a set of interpretations of the same text segment (For the sake of brevity, throughout this text we use the term interpretation, where in fact we should write representation of an interpretation). Anyway, maximal (minimal) elements can be defined in the fol lowing way: An object E is a maximal (minimal) element if no competing object is better (worse) than E. Thus an object in a cycle of the graph cannot be maximal (minimal). In the fol lowing we refer to a l-n translation for this general phenomenon.
E87-1024@@ The LINLIN-project is concerned with the development of general-purpose natural language interfaces (NLIs) to computer software with special emphasis on communication i Swedish. In the same vein I ta~e the notion of a discourse object to apply in the most general sense; the universe of discourse is in principle just a collection of discourse objects. The main ideas are t~e following: (1) The content of user inputs is preferably represented as a structure of Discourse Object Descriptions (DODs) which relate in various ways to objects of the universe of discourse.
E87-1037@@and Introduction An apparent tendency in computational linguistics during the last few years has been towards declarative grammar formalisms. S t ra tegy  6 (LCKt) (Same preconditions as above.) E~cient Parsing for Natural Language.
E89-1002@@  The phenomenon of derivational equivalence is most evident in work on generalised categorial grammars, where it has been referred to as ~spurious ambiguity. We provid e the following reasoning to the effect that SN holds. A form on the left is cal led a REDEX, the form on the r ight,  its CONTRACTUM.
E89-1005@@ Significant progress has been achieved recently in natural language (NL) understanding systems through the use of plan recognition and "plan tracking" schemes that maintain models of the agents domain plans and goals. (Variables are shown with a prefixed question mark.) The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government.
E89-1006@@On a Reichenbachian analysis tenses are interpreted as relating three kinds of temporal entities: the time Of the event talked about, the speech time (time of utterance) and the refer*The research was done in the context of ACORD (ESPRIT P393) and LILOG, and was also supported by the German Science Foundation (Ro245/13-3) ence time. e l  Le 6 octobre Pierre arriva ~ Paris. ce jour-ld introduces the location time t for the first state, sl ,  (pleuvoir).
E89-1008@@At the very least, it appears to have led to the view that morphological descriptions should be restricted to those with an immediate interpretation in terms of the operations of finite-state transducers. In Anderson, S. R. and Kiparsky, P. A Festschrift for Morris Halle. In Klein, E. and Benthem, J.
E89-1009@@ Inheritance networks ("semantic nets") provide an intuitively appealing way of thinking about the representation of various kinds of knowledge. Variables such as N, P, L, G and V will be assumed to be typed (as nodes, paths, lvalues, gvalues and values respectively). 70  Acknowledgements Evanss work was supported by a grant from the SERC.
E89-1013@@ Prolog is frequently used in implementing natural language parsers or generators based on unification based grammars. Then, 7~i+t is obtained by replacing C in :Pi with AO :-BO&L. A Constraint-Based View of Language.
E89-1014@@ In this paper we present a logical treatment of semifree word order and bounded d iscont inuous  constituency. (I.e., they follow the Prolog convention.) A model is a pair (~i~ 2.. .
E89-1019@@It is designed to be used as a major component of interactive advisor systems uch as interfaces to database management systems and diagnostic expert systems. Grosz, Barbara J., Douglas E. Appelt, Paul Martin, and Fernando C.N. After morphological information has been supplied, VEX presents the following list of sentences: I The thingummy used up.
E89-1020@@Within an Inflexional morphology environment (Alshawl, t985), we propose a procedural approach based on automatically acquired flexlonlng paradigms. Each line In an AS Is filled (except the last field corresponding to the WORD-FORM column) by the Information uniquely Identifying e point in P. 4. Let us consider a TF to be always lexicograpHIcally sorted.
E89-1022@@ Most approaches toprocessing anaphora concern themselves mainly with the case of singulars and deal only peripherally with the complications of plurals. However, it becomes important when the analysis is expanded to multiple sentence t xts. They had a great ime.
E89-1023@@ Recent work in text generation i cludes emphasis on producing textual presentations of the explanations of reasoning in knowledge-based systems. KNOWLEDGE REPRESENTATION and EXPLANATION Previous research in natural anguage generation from knowledge based systems has primarily focused on independent knowledge representation schemes (e.g rule, frame or conceptual dependency formalisms). Salient objects in the knowledge base are marked, including the parent and children of the object(s) in question (which are explicitly in focus) and the siblings or cousins of the global focus (which are implicitly in focus).
E89-1024@@One major point on which the methodology of unification grammars differs radically from that assumed by linguistic theories lies in the way they deal with generalizations that hold over the domain of description. Pollard, C. and Sag, I. : Center for the Study of Language and Information. In Klein, E. and van Benthem, J.
E89-1034@@Linear precedence is encoded by (a) ordering valencies in a list and (b) using directional slashes indicating whether the argument is to be found to the left or to the right of the functor. Basic categories are of the form HeadAFeatures where Head is one of the atomic symbols n(oun), np or s(entence) and Features is a list of feature values. Instead, features interact with combination rules to enforce the 252  constraints on word order described in (9).
E89-1035@@The motivation for this research is twofold. A parser for generalised phrase-structure grammars. The example he gives of an NP type is DT* *S , F which would be the analysis assigned to an NP consisting of a determiner, plural noun, comma and finite clause.
E89-1037@@ In this paper we sketch an approach to machine translation that offers several advantages compared to many of the other strategies currently being pursued. These structures are of different formal types--the c-structure is a phrase-structure t e while the f-structure is a hierarchical finite function--and they characterize different aspects of the information carried by the sentence. We treat the oblique preposition as a PRED that itself takes an object.
E89-1038@@ In this paper we describe a framework for research into translation that draws on a combination of two existing and independently constructed technologies: the analysis component developed for German by the EUROTRA-D (ET-D) group of IAI and the generation component developed for English by the Penman group at ISI. In Halliday, Michael A.K. Semant ic  feature  t rans fer  Semantic features of the ET-D IS representation may also be transferred into sets of Penman inquiry responses.
E91-1004@@  All natural language grammars are alnbiguous. This average likelihood is often a poor estimat;e of probability. Acquiring a Noun Classification from Predicate-Argument Structures.
E91-1006@@  Tree Adjoining Grammars (TAGs) are a formalism for expressing rammatical knowledge that extends the domain of locality of context-free grammars (CFGs). HeadDriven Bidirectional Parsing: A Tabular Method. The  A lgor i thm In the following any (elementary orderived) tree will be denoted by a pair (N, E), where N is a finite set of nodes and E is a set of ordered pairs of nodes, called arcs.
E91-1008@@A formalism for the treatment of the referential relationships among the NPs of a sentence will be presented that is more expressive than indexation and more effective as a computational tool. )e~ w ; i.eo quantifiers and syntactic variables cannot corefer;, therefore, they can only function as binders;l 1(vi) if (~ d ~)~9~ w then there are no paths, in ~w. Sets A, P, R, constitute a partition of set N. Finally, Q denotes the set of quantified expressions and syntactic variables.
E91-1010@@Their definition covers most cases of command relations that have been presented in linguistic literature. 1 if we take the set {ae N: LABEL(a)e MAX} as a property P, where MAX is any set of maximal projections. Below I will specify both of these definitions formally.
E91-1011@@ With the development of highly parameterized syntactic theories like Government and Binding theory and Head-Driven phrase structure grammars and with the development of theories where rewriting and unification plays a central role, like Categorial grammars and Unification Grammars, there is an increasing need for more appropriate and more efficient feature systems. Mgu 0 i is replaced here by the subsumption constraint. Informally, when a typet i corresponds toa terminal structure, attempt is maae to find a terminal type description t : .
E91-1013@@  The LR method is known to be a very efficient parsing algorithm that involves no searching or backtracking. All grammars that solicit this performance difference in e~Tor detection have one property in common. Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems.
E91-1015@@  The work presented here is a dialogue model for  oral task oriented dialogues. D ia logue  s t ra tegy  mode l ing  In one running cycle, more than one dialogue act can be a candidate, this is due to the nondeterministic nature of the dialogue which is preserved until this step. It is a kind of dis_ S3.
E91-1016@@ There is an increasingly widespread view that linguistic behaviour esults from the complex interaction of multiple sources of partial information. E L We define ---., ,---, and A as usual. Two syllables cannot share a mora.
E91-1018@@When a text-to-speech synthesis system is used, it is likely that the text being processed will contain a few words which do not appear in the lexicon as entries in their own right. A matching is carried out character by character between the lexical and surface forms, checking each match with respect o the spelling rules (and hypothesizing nulls where appropriate). Thus, in the case of preamble, the stripping of the prefix prewill allow for the correct pronunciation/p r i i a  m b @ 1/: if the entire word had been passed to the letter-tosound rules, the incorrect pronunciation /p r i In addition to affixes, known root morphemes could also be stripped to leave the remaining unknown material.
E91-1023@@ Computational models of sentence syntax are increasingly based on well-defined linguistic theories and implemented using general formalisms; by contrast, morphology and phonology in the lexicon tend to be handled with tailor-made hybrid formalisms selected for properties such as finite state compilability, object orientation, default inheritance, or procedural efficiency. Hund Hund-e_ h/.ind-isch hund-i.q Consequently, Umlaut conditions must be inherited from several sources. Morphophonological generalisations thus require three levels of abstraction: L 1 , Morphotactic ID: L 2, Morphotactic QLP: L 3, Phonological: Orthographic: \[telephone * ADJ-ic\] \[\[telephone o final-stress\] " ic\] /t E I @ I O n I k/(SAMPA computer phonetic notation) "telephonic" Details of phonological feature structure will not be dealt with here.
E91-1027@@ Parsing is a process by which an input sentence is not only recognized as belonging to the language, but is also assigned a structure. ; in grammars for l lebrew, for example, it is convenient J M. Rimons main atfiliafion is the IBM Scientific Center, i laifa, Israel, E-mail: rimon@haifasc3.iinusl.ibm.com 2 j. I Icrz was partly supported by the I.eihniz (enter for R.esearch in Computer Science, the ! SCr(t,2) would have rejected one of the wrong two.
E91-1028@@ In very simple language generation systems, there is typically a one-to-one relationship between entities known to the system and the linguistic forms available for describing those entities; in effect, each entity has a canonical name. Choose Property for each  Pi E P~ do: Cr, ~-C~ f3 {x\]pi(x)} Chosen property is pj, where Crj is the small(;st se t f  goto  Step 3. A Const ra in t -Based  A lgor i thm Data  St ructures  We assume three global kinds of data structure.
E91-1033@@ The task of providing informative natural language xplanations for illustrating the results produced by decision support systems has been gtven increased attention recently. \[8\] Reiter E.: Generating Descriptions that Exploit a UsersDomain Knowledge. The first hypothesis identical for the "novice, but a series of inferences i needed to prove its adequacy.
E91-1039@@  Young children and natural language processing programs face a common problem: everyone else knows a lot more about words. t ire verbs are ntis-classified as having non-stative senses. (6) a. go: What that means in this case is go.
E91-1040@@ In recent years, it has become increasingly clear that the limited size of existing computational lexicons and the poverty of the semantic information they contain represents one of the primary bottlenecks in the development of realistic natural anguage processing (NLP) systems. The structure of the Merriam.Webster Pocket Dictionary~ Ph.D. BROWN, R. W. 0958) llow shall a thing be called OW Centre for the New OED, Oxford. Therefore, several passes through the tab!e are required.
E91-1043@@ The aspect of bidirectionality has been gaining importance since the growing rate of research on natural anguage generation over the last years offers us deeper insights into this cognitive ability of humans. E.g., during the generation of an utterance partial structures are analysed to avoid unnecessary ambiguities. The advantage of a uniform architecture is that intermediate r sults of one direction can 4For example, the complete structure of a produced utte~mce is analysed during \[he anticipation-feedbackloop of the HAM-ANS system (see Hoeppner t al.
E91-1044@@ Natural anguage sentences like the notorious (1) Every man loves a woman, are usually regarded to be scope ambiguous. verges to the number e. This means that the total number of subscopings and hence the lower bound on space complexity is of order n!. By comparison, the Hobbs & Shieber algorithm is O(n!
E91-1048@@Research in transfer-based MT systems has focussed on discovering an appropriate l vel of linguistic description for translation, at which we can specify translation relations" (or transfer ules) in a simple manner. Therefore, step (b)i is a kind of relaxation process which tries to find the most accurate solutions satisfying all constraints. 3 We introduce a new notation.
E91-1050@@ Benefits arising from the adoption of unification as a tool in computational linguistics are well known: a declarative, monotonic method of combining partial information expressed in data structures convenient for linguistic applications permits the writing of sensible grammars that can be made independent from processing mechanisms, and a growing familiarity, in both theoretical nd computational circles, with the techniques of unification fosters fruitful interchange of ideas and experiences. 1 Clearly there is a sense in which such relations can be viewed as ternary: T(FI, R, F2), where 171 and 172 are  17Ss, and R is the rule set which relates them. ), to encode arguments in these FSs, the identification of elements on such a llst by e. 8.
E91-1051@@Mapping functions uch as V (from c to f structure) and o (from c to semantic structure) are familiar from the LFO literhture. 295 - Intuitively, the approach works by building target constructions without assigning them PRED values directly, then specifying the target PRED values in such a way that it is possible to switch the heads for the eases in question..LP In fact, though this works for cases such as (2c,d,e), it is limited to cases in which it is correct o raise all the dependents of a predicate to the same slot in the construction headed by the translation of the adverb. (c) I think that John just arrived.
E93-1006@@As soon as a formal grammar characterizes a nontrivial part of a natural language, .almost every input string of reasonable ngth gets an unmanageably large number of different analyses. As our test set, we took the same 75 p-o-s sequences as used in the previous experiments. From a classical result of probability theory (Chebyshevs inequality) it follows that the time complexity of achieving a maximum error e is given by O(e2).
E93-1007@@  Grapheme-to-phoneme conversion is a central task in any text-to-speech (reading aloud) system. 4All connect ion is t  s imulat ions were run on P laNet  5. , a network simulator written by Yoshiro Miyata (Chukyo U., Japan). MORPA, a lexicon-based MORphological PArser.
E93-1010@@Lexicalist grammar formalisms, uch as Head-driven Phrase Structure Grammar (HPSG) and Categorial Unification Grammar (CUG) have two characteristic properties. 7. right(Ds,PO,P,E) if Ds exist from 7. The parser is a bottom-up active chart parser without prediction, in which the addition of an active item based on a rule R is considered whenever an inactive item H is entered into the chart which matches the head of R. More precisely, if item(Cat, \[ \], B, E) is derived, and there is a rule LHS --* D1,...,Dh-1, Ca~,Dh+l, .... Dn and there are inactive items matching D1...Dh-1, ranging from B0 to B, an iIem(LHS, Dh+I...Dn,Bo, E) is added to the chart.
E93-1012@@  The use of inheritance mechanisms in computational linguistics has become wide-ranging, with applications in semantics, yntax, morphology and phonology. /s/, resulting in a / t / sound. The stem "bend" for example has a coda which consists of an "n" and a "d" (in conventional terms).
E93-1013@@This is because the derivation of the meaning of a phrase can often be viewed as mirroring the surface constituent structure of the English phrase. Also in E. Wehrli and T. Stowell, eds., Syntax and Semantics 26: Syntax and the Lexicon. 2 Theoret i ca l  p re l iminar ies  In the following, we describe two linguistic assumptions that underlie this work.
E93-1015@@  In the past several years, many researchers have started looking at bilingual corpora, as they implicitly contain much information eeded for various purposes that would otherwise have to be compiled manually. Tunab i l i ty  The threshold is defined in terms of the source language term frequency. A statistical approach to machine translation.
E93-1018@@The most intricate examples they give to support his claim involve what they dubbed mult iple VPE  and can be illustrated by the following discourses (square brackets urround antecedent VPs, 01 indicates VP ellipses and indices represent anaphoric dependencies) 1 : *The work reported here was partially carried out in the LRE Project 61-062, Towards a declarative theory of discourse. Note that in this case, discourse parallelism does hold between aand e-clanses. ~i represent the semantics of VPEs where i indicates urface ordering.
E93-1021@@ Predicates require that their arguments be of a given type. II a d~ sgchapper dun barns. Achcter does not allow a complement of type e, while combining easily with the above NP.
E93-1023@@An important step in text-to-speech onversion is the generation of the correct phonemic representation o  the basis of the input text. In E. Klein and F. Veltman, editors, Natural Language and Speech. CELEX, a guide for users.
E93-1024@@LFG formulates the syntactic dependencies and generalizations of natural anguages in terms of the properties of formal structures of different types: ordinary phrase-structure t es represent the surface constituency of sentences while hierarchical finite functions represent their underlying grammatical relations. The Kaplan et al solutions depend on monolingual representations of phrasal, functional, and semantic information related by the correspondences ~p and a, with translation 193 correspondences  and ~ mapping source to target structures, as shown in the configuration in (1): (1) Source " " Target  f_structur e o ~_..__.._..-------~ o di) 0 / ~ ~:  c-structure These solutions utilize the formal device of codescription to specify the target structure constraints in terms of simple compositions ofthe and z mappings with the monolingual source correspondences. The just f-structure, though not accessible from the S node, maps through t to the outermost target f-structure.
E93-1025@@ A simple example of verb phrase (VP) ellipsis is given in sentence (1): (1) John likes his mother, and Bill does too. As a result, reading (9) for sentence (8) is correctly predicted to exist. The following rule governs role linking in this case: (26) A referential element is linked to the most immediate coreferential e ement that c-commands it in the syntax.
E93-1026@@  The paper shows how the verbal lexicon can be formalised in a way that captures and exploits generalisations about the alternation behaviour of verb classes. 6 Summary  and  d i scuss ion  First, HPSG-style verbal lexical entries, and the mappings between them corresponding to alternations, were described. We access the lexical entries for the ergative forms of verbs with DATR queries with the path prefix a l t  erg, which work as follows.
E93-1027@@ While quite a number of useful grammar formalisms for natural language processing now exist, it still remains a time-consuming and hard task to develop grammars and dictionaries with comprehensive coverage. \[5\] D is t inc t ion  of  closed and open lexical categories:  We assume that the existing grammar has a complete list of function words. (2) Buy a new car.
E93-1028@@A text is not just a sequence of words, but it also has coherent structure. (Note that each wi E W is included in LDV or their derivations.) c. Desmond goes to a theatre.
E93-1029@@Historic Origin Early transformational grammar consisted of a rather complex generative component and an equally complex and equally imperspicuous transformational component. Let Sf(~) denote the set of subformulas of ~ and Sf(e) the set of subformulas of . Namely, if a --* bz .
E93-1030@@  In this paper, we investigate the impact of the pluperfect ense on the temporal and rhetorical structure of narrative discourse. e. We all felt very good about it. The pluperfect of a state, such as (11), therefore, is assumed to first undergo a transformation i to an event.
E93-1031@@ We focus on aspects of the discourse behaviour of the temporal connectives before, after and when. (2) e. They were pacified when Major launched a charm offensive. Discourse  a t tachment  w i th  coherence Consider text (1,2a): (1) The backbenchers were in revolt.
E93-1033@@  When agents participate in a dialogue, they bring to it different beliefs and goals. An askre f  by Russ would be the expected reply to a prete l l  by Mother: wouldEz( in,do( in,pretel l (m, r, w)) ,  do(r,askref(r, In, w))) It would be expected by Mother because:  The lezpectation relation suggests that she might try to pretell in order to get him to produce an askref: lezpec~ation( do( in,prete l l ( in , r ,w ) ), knowsBet  e rRef ( in , r ,w) ,  do( r ,askre f ( r ,m,w) ) )   Russ may abduce cred aousB(knowsnetterRef(in, r, w ) ) to explain believe ( in ,knowsBet terRef ( in ,  r w)) . A Prolog technology theorem prover.
E93-1035@@  Finite-state approaches to morphology provide ways of analyzing surface forms by appealing to the notion of a finite-state transducer which in turn mimics an ordered set of rewrite rules. If were in state 1 and an a or ! A request for bnt (girl) to be inflected with Singular Nominative produces the list \ [b,n,t ,+,o,n\]  which is then fed to the appropriate automaton.
E93-1036@@It has been regarded as the most efficient parsing technique for context-free grammars. We define the set GOAL to be the set consisting of S, the start symbol, and of all nonterminals A which occur in a rule of the form B--* t~ A fl where is not e (the empty sequence of grammar symbols). r The root of the parse forest.
E93-1037@@Their success derives from the utility they have in identifying salient discourse ntities uch as topic and thereby locating the antecedent for an anaphor. Then, since the quoted discourse consists of a set of discourse segments, it will be assigned to a T-structure. I often see her playing outdoors."
E93-1040@@ Most broad coverage natural language parsers have been designed by incorporating hand-crafted rules. First, each iteration of the algorithm on a gr,-unmar with n nonterminals requires O(n31wl 3)time per t~ning sentence w. Second, the inferred grammar imposes bracketings which do not agree with linguistic judgments of sentence structure. A probabilistic parsing method for sentence disarnbiguation.
E93-1042@@XGs have the power of Turing machines), whereas the increase of power by the stack mechanism is not even enough to generate all Type 1 languages (see below). Let ft be the first (or leflmos0 wi-index from above in the index-stack Ix, and let w t be the subword of \[Xm\] containing the terminal into which ft is discharged, i.e all other wi-indices in Ix are only accessible after ft has been consumed. Let al.a n be the input.
E93-1045@@ We study parsing of tree adjoining rammars (tag) with particular emphasis on the use of shared forests to represent all the parse trees deriving a wellformed string. An e lementary  tree is either an initial tree or an auxiliary tree. ,  n}, the number of nonterminals in Go is O(n4).
E93-1046@@ In this paper we are concerned with grammar-based surface-syntactic analysis of running text. ~Morphological, clause boundary, and syntactic ambiguities 3The ENGCG parser can currently be tested automatically via E-mail by sending texts of up to 300 words to engcg@ling.Helsinki.FI. A Comprehensive Grammar of the English Language.
E93-1047@@The structures are set in correspondence by a function from the c-structure nodes (constituents) into the substructures of the f-structure. If we start with the 3times modifier this set contains the typed variables ae and a(e,t). Lexical-Functional Grammar: A Formal System for Grammatical Representation.
E95-1001@@ Whenever we hear a sentence or read a text we build up mental representations i  which some aspects of the meaning of the sentence or text are left underspecified. e. Some problem was solved by everybody. l  t. 5For the construction of underspecified representations see \[2\], this volume.
E95-1002@@ The semantic analysis of standard HPSG deviates from the familiar Montegovian way to construct semantic representations mainly in that it uses unification to eliminate the need for 13-reduction. 2 Second, there is no underspecified representation f ambiguities that arise from the distributive/collective distinction of plural NPs (neither within theHPSG framework nor in the C(ore)L(anguage)E(ngine)3). / IREL ever~ / \[LABEL iT~Tll( / / b Es : Jf L t LSCOPE ll\[~JJ ) The entry for the indefinite singular determiner, (4), introduces a new individual type referent.
E95-1004@@  The Exoseme system \[6, 7\] is an operational application which continuously analyses the economic flow from Agence France Presse (AFP). We thus discover from time to time new organisations which share the acronym CDC with Caisse des D~p6ts  e t  Consignat ion. For example : ,, apposition of an individuals position : Peskine, d i rector  of the  group, * name complement typical of a company : the shareholders of Fibaly  name complement typical of a location : the  mayor of  Gisenyi.
E95-1006@@Accordingly, it casts its linguistic analyses in terms of a composite ontology: two independent domains  a domain of constituency information (c-structure), and a domain of grammatical function information (f-structure)  linked together in a mutually constraining manner. Thus we demand that the following conditional statement be valid: (e-struct A (down)true) --~ V ~" This says that if we are at a c-struct node which has at least one daughter (that is, a non-terminal node) then one of the subtree licensing disjuncts (or rules) must be satisfied there. Let t, t I be finite (possibly null) sequences of the modalities (up) and (down), and let f ,  f   be finite (possibly null) sequences of feature modalities.
E95-1007@@  Unification Grammars with a context-free skeleton, like Lexical Fhnctional Grammar (LFG) and PATRII (cf. That h e is well-defined results of course from 7(ii). Suppose there is a recursion of type (a-c) (cf.
E95-1009@@Defining dialects is one of the first tasks that linguists need to pursue when approaching a language. In the above technique, very small phonetic differences, such as that between a moderately palatalized and a very palatalized \[t\], count the same as major differences, such as that between a \[t\] and an \[e\]. This is therefore a O(N 3) algorithm, comparable in efficiency to agglomeration.
E95-1010@@ Given texts in two languages that are to some degree translations of one another, an alignment of the texts associates entences, paragraphs or phrases in one document with their translations in the other. Distribution of 51 for (a) hand aligned and (b) randomly aligned blocks 71 Dis t r ibut ion  of  4 -Gram Match  Scores  fo r  Hand A l igned  Set  55.00  . Given a set of English terms, e i, Spanish terms, sj, from two blocks, the translation operation, T(I), generates aset of terms in the opposite language by stemming each term and retrieving the terms that the stemmed word translates to in Collins.
E95-1011@@  Unification-based grammar formalisms can be viewed as generalizations of Context-Free Grammars (CFG) where the nonterminal symbols are replaced by an infinite domain of feature structures. For a given grammar G and input string al . Rather than giving such prominenc.e to the root feature structure, we suggest hat the entire derivation tree should be seen as the object that is derived from the input, ie, this is what the parser returns.
E95-1012@@The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures. The other difference is the use of le f t  and r ight  as models of the dominance relationships between nodes. 7 Acknowledgements I acknowledge the support of the Language Technology Group of the Human Communication Research Centre, which is a UK ESRC funded institution.
E95-1014@@Nominalizations are used for a variety of stylistic reasons: to avoid repetitions of a verb, to avoid awkward intransitive uses of transitive verbs, in technical descriptions where passive is commonly used, etc. The  rqomina l i zat ion  Cl ine The phenomenon of nominalization in English happens when a verb is replaced by a noun construction using a gerundive or nominal form of the verb. It remains to be seen whether these statistical results are more useful to lexicographers than their more traditional tools of key-wordin-context files and T-score measures.
E95-1016@@ In recent years there has been a common agreement in the NLP research community on the importance of having an extensive coverage of selectional restrictions (SRs) tuned to the domain to work with. Descr ip t ion  The technique functionality can be summarized as :  I nput  The training set, ie a list of complement co-occurrence triples, (verblemma, syntactic-relationship, noun-lemma) extracted from the corpus. Var ia t ions  on the  pr io r  p robab i l i ty  When considering the prior probability, the more independent of the context it is the better to measure actual associations.
E95-1017@@In the current computational nd psycholinguistic literature there are two main approaches to the incremental construction of logical forms. However there is a major problem. (R(Ax.thinks(x,P (john)))) (mary) The second representation is appropriate if the sentence finishes with a sentential modifier.
E95-1018@@ Categorial Grammar  formalisms consist of logics. The implication o (resp. Each connective has a Right \[R\] and Left \[L\] rule, showing, respectively, how to prove and how to use a type containing that connective.
E95-1022@@Part-of-speech analysis usually consists of (i) introduction of ambiguity (lexical analysis) and (ii) disambiguation (elimination of illegitimate alternatives). Defence Advanced Research Projects Agency, U.S. Govt. "Parsing English with a Link Grammar".
E95-1023@@Word order domains in Reapes approach are totally ordered sequences. (7) daft einen Mann in der Stral\]e r lanfen sah. Impos ing  the res t r i c t ion  We note that precedence can be restricted to nonatomic types such as HPSG signs without compromising the grammar in any way.
E95-1025@@ There are two key ingredients for building an NLP system:  a processing model (parser, generator etc.) The syntax of feature declarations i given in (3). They provide a compact notation.
E95-1026@@VERBMOBIL combines the two key technologies speech processing and machine translation. 1) where the layers differ with respect o the type of knowledge they use and the task they are responsible for. Sitter, S. and Stein, A.
E95-1027@@It is based on the identification of the sublanguage specific cooccurrence properties of words in the syntactic relations in which they occur in the texts. Firstly, we measure the contexts in which words w E W occur, and define a statistically motivated similarity measure between contexts of occurrence of words to infer a similarity between words, d(wl, w2), wl, w2 E W. In our case the context is defined to be a vector of word bigram statistics across the corpus for one and two words to the left and right, thus representing each word to be classified by a vector of bigram statistics. However, general and full scale parsing is not required for many tasks of knowledge acquisition but rather a robust identification of certain text segments i needed.
E95-1031@@I am sure this is what  he means. 2ain.ertion, Otdeletion , Ofmutation lEES st|\] strictly 1 in Lyon s  o r i~-~l  p~per  ~In fact, there axe cases that an inserted phrase cannot be constructed to form a nonterminal node. The error value e of an edge is calculated as follows.
E95-1032@@But it suffers a number of drawbacks, especially when viewed from a computational perspective. Pronouns typically refer to these functions, eg he-agent(e). In (24) I enjoyed it.
E95-1033@@The results we present rest upon two major assumptions: 1. In particular, improvements are due to discussions we had with S. Schacht, N. Br6ker, P. Neuhaus, and M. Klenner. ........ \[den\] ....................................................... "" Die  F i rma Cc~paq, The company Compaq, die den LTg-L i te  entwicke l t ,  bestfickt ihn mi t  einem PCI-Motherboard.
E95-1034@@Word order variation in relatively free word order languages, such as Czech, Finnish, German, Japanese, Korean, Turkish, is used to convey distinctions in meaning that go beyond traditional propositional semantics. The variables T, F, G1, G2 will be unified with the interpretations of the proper constituents in the sentence during the derivation. "Ay~e knows that yesterday, FATMA left."
E95-1037@@This paper adopts a corpus-based approach to process discourse information. Department of Linguistics, Brown University, Providence, R.I. F. Jelinek. Metrics for Performance 1 average #of candidates 2 average rank of assumed topic 3 frequency of candidates 4 frequency of assumed topic 5 frequency of computed topic 6 average rank of topic in previous paragraph E # of nouns in basic form in paragraph i / # of paragraphs E rank of assumed topic in paragraph i/# of paragraphs y.
E95-1041@@However none of these methods has considered the way of dealing both phenomena in the same concrete system. The sausage machine: A New Two-Stage Parsing Model, Cognition, 6. 3) If some anaphora.are l ft unresolved, apply the anaphora module again.
E95-1042@@ This paper describes the aggregation process in the natural language generator of the Visual and Natural language Specification Tool (VINST), and how the aggregation can be improved. USED_WORD_LIST is a list of previous used words. Aggregation i  Natural Language Generation: In the Proceedings of the Fourth European Workshop on Natural Language Generation, Pisa, Italy, 28-30 April.
E99-1001@@ Named Entity recognition involves processing atext and identifying certain occurrences of words or expressions as belonging to particular categories of Named Entities (NE). To begin with, the lists would be huge: it is estimated that there are 1.  million unique surnames just in the U.S. There is a similar problem with company names.
E99-1006@@ Most anaphora resolution algorithms are designed to deal with the co-indexing relation between anaphors and NP-antecedents. 6: \[The government dont tell you everything.\]i B. Theyre not even there yet A.2: Uh-huh.
E99-1007@@ Recent years have witnessed a shift in grammar development methodology, from crafting large grammars, to annotation of corpora. 4 Exper iments  in  C lus ter ing  and  C lass i f i ca t ion  Our goal was to determine whether statistical indicators can be automatically combined to determine the class of a verb from its distributional properties. A non-unified analysis of agentive verbs.
E99-1008@@ The notion of mild context-sensitivity originates in an attempt by \[Joshi 85\] to express the formal power needed to define the syntax of natural languages (NLs). We have seen that these phenomena c n both be defined by RCGs. ,ap)  is the complementary w.r.t T* of the language defined by A(ax , .
E99-1009@@ Consider the paraphrase: following examples of (1) a. b. C. Frodo lives in Bag End. Fairly uncontroversially, this class may be taken to include at least sentences S and names N; what the class is exactly is not fixed by the formalism. For instance, assuming the expected lexical type assignments o proper names and intransitive and transitive verbs, there are the following derivations: (9) N~N S~SkL  N,N~S ~ S john+runs: S (10) N~N N~N S~S~ N, NiS ~ S /L N, (NiS)/N, N ~ S john+finds+mary: S Ungrammaticality occurs when there is no validity of the sequents arising by lexical insertion, as in the following: (11) NiS, N ~ S runs+john: S 3 Ambiguity and spurious ambiguity The sentence (12) is structurally ambiguous.
E99-1010@@  Word classes are often used in language modelling to solve the problem of sparse data. (12) By using these two optimization processes we enforce that the classes E are mono-lingually good classes and that the classes ficorrespond to ~. Determine the word alignment a~.
E99-1011@@The goals of the SUMMAC evaluation were to judge individual summarization systems in terms of their usefulness in specific summarization tasks and to gain a better understanding of the issues involved in building and evaluating such systems. 2 SUMMAC Summar izat ion  Tasks  In order to address the goals of the evaluation, two main extrinsic evaluation tasks were defined, based on activities typically carried out by information analysts in the U.S. Government. The Reliability of a Dialogue Structure Coding Scheme.
E99-1015@@  Current approaches to automatic summarization cannot create coherent, flexible automatic summaries. 112 Proceedings of EACL 99 Does this sentence r fer to own work (excluding previous work of the same author) Does this sentence contain material that describes the specific aim described in the paper Does this sentence make reference to the structure of the paper I TEXTUAL \] Does the sentence describe general background, including phenomena to be explained or linguistic example sentences t\[ BACKGROUND 1 Does it describe anegative aspect J of the other work, or a contrast or comparison of the own work to it Y ~ N O  \[ CONTRAST I Does this sentence mention the other work as basis of or support for own work Studies I and II For Studies I and II, we used three highly trained annotators. In Inderjeet Mani and Mark T. Maybury, editors, Proceedings of the workshop on Intelligent Scalable Text Summarization, in association with A CL//BA CL97.
E99-1016@@ Partial parsing, often referred to as chunking, is used as a pre-processing step before deep analysis or as shallow processing for applications like information retrieval, messsage extraction and text summarization. We introduce a modification to this encoding. Cascaded Markov Models add left-to-right context-information t  context-free parsing.
E99-1017@@ Context sensitive rewrite rules have been widely used in several areas of natural language processing. As an example, suppose that T,c,. Since E is a recognizer, it is first coerced to identity(E).
E99-1018@@ Part-of-speech (POS) taggers are software devices that aim to assign unambiguous morphosyntactic tags to words of electronic texts. In addition, there is always the opportunity to bias empirical learning with linguistically motivated parameters, o as to 7 In this method, adataset ispartitioned 10 times into 90% training material and 10% testing material. The ambiguity is resolved by eliminating the tag(s) with different POS than the one returned by the decision tree.
E99-1019@@  The greater the amounts of text people can access and have to process, the more important efficient methods for text categorisation become. R.I. Forsyth and D. Holmes. There are three such domain categories in our experiments, politics (P), law (L), and economy (E).
E99-1023@@  The text corpus tasks parsing, information extraction and information retrieval can benefit from dividing sentences in chunks of words. IO Words inside a baseNP receive an I tag, others receive an O tag. A memory-based approach to learning shallow natural language patterns.
E99-1025@@ Many natural language applications are beginning to exploit some underlying structure of the language. In E. Roche and Schabes Y., editors, Finite State Devices for Natural Language Processing. Class-based n-gram models of natural language Computational Linguistics, 18. :467479.
E99-1026@@  Dependency structure analysis is one of the basic techniques in Japanese sentence analysis. Here /3 is an empirical probability and PME is the probability assigned by the M.E. T r ip le t  features:  basically consist of the twin features plus the features between bunsetsus.
E99-1028@@However, a major obstacle impedes the acquisition of lexical knowledge from corpora, ie the difficulties of manually sensetagging a training corpus, since this limits the applicability of many approaches to domains where this hard to acquire knowledge is already available. The results (Set~3) are passed to the function Reeogn i t ion-o f -Po lysemy,  which determines whether or not a verb is polysemous. R. Bruce and W. Janyce.
E99-1029@@This refers to the fact that the elementary trees that make up the grammar are larger than the corresponding units (the productions) that are used in phrase-structure ule-based frameworks. We will call this the extended domain  of  local i ty hypothes is . The value of the feature wh is set by the N Pcomplement, and percolated to the root of the PP.
E99-1031@@ When building natural language understanding systems, choosing the best technique for anaphora resolution is a challenging task. We created two different supervisor t See (Byron and Allen. George Ferguson and James E Allen.
E99-1032@@ Event structure is traditionally accounted for using two sets of notions : change-of-state / affectedness and incrementality. John has drunk a beer). t (5) Mona has already sailed.
E99-1034@@It was not until recently that a more elaborated use of this information resulted in consistent improvement of retrieval effectiveness. F i r s t -o rder  co -occur rence  First-order co-occurrence measures the degree to which two terms appear together in the same context. Term ti is represented here by (wil, wi2, ..., wire) T, where wij is the number of time that ti and tj occur in the same context.
E99-1035@@  This report describes a parsing system for fast and accurate analysis of large bodies of written Swedish. (i) uses the Mutual Information, statistics, based on the n-grams. Part-of-Speech Tagging and Partial Parsing, In Corpus-Based Methods in Language and Speech Processing, Young S. and Bloothooft G., editors, Kluwer Acad.
E99-1039@@Defaults have been used in the definition of inflectional morphology, specification of lexical semantics, analysis of gapping constructions and ellipsis among others. However, since these types subcategorise for 2 arguments, they need to override the default of exactly one argument, specified by the e-list value for TAIL, and add an extra argument: an NP object for trans, and a predicative complement for intrans-control. 2 Defau l t  Inher i tance  and  YADU In this work, a default multiple orthogonal inheritance network is used to represent lexical information.
E99-1043@@The vast repository of research papers which are the results of genome research are a natural environment in which to develop language ngineering tools and methods. In E. Roche and Y. Schabes, editors, Finite-Slate Language Processing. Designing a (finite-state) parsing grammar.
E99-1051@@ In this contribution we present some improvements on the design of a Dialogue Management System for the automatization of simple telephone tasks in a PABX environment (automatic name dialing, voice messaging, ... ). A pre-defined single fixed mixed-initiative strategy was used in all the cases. A first group, Group A, showed a "fluent" interaction with the system, similar to the one supposed by the mixed-initiative strategy (for example, as an answer to the question of the system "do you want to do any other task ", these users could answer something like "yes, I would like to send a message to John Smith").
H01-1006@@ Question Answering is gaining increased attention in both the commercial and academic arenas. [8] Rosch, E. et al Basic Objects in Natural Categories, Cognitive Psychology 8, pp. [2] Mihalcea, R. and Moldovan, D. A Method for Word Sense Disambiguation of Unrestricted Text.
H01-1009@@Information Extraction (IE) systems today are commonly basedon pattern matching. TBP can also represent a complicatedpattern for a node which is far from the root node in the depen-dency tree, like C!D!E, which is hard to represent without thesentence structure.For matching with TBP, the target sentence should be parsed intoa dependency tree. Sentence RetrievalThe system then calculates the TF/IDF-based score of relevanceto the scenario for each sentence in the relevant document set andretrieves the n most relevant sentences as the source of the patterns,where n is set to 300 for this experiment.
H01-1011@@To generate atitle for a spoken document becomes even more challengingbecause we have to deal with word errors generated by speechrecognition.Historically, the title generation task is strongly connected totraditional summarization because it can be thought of extremelyshort summarization. Title Generation for SpokenBroadcast News using a Training Corpus. To avoid struggling with organizing selected titlewords into human readable sentence, Hauptmann [2] used Knearest neighbour method for generating titles.
H01-1015@@Recent research on dialogue is based on the assumption that di-alogue acts provide a useful way of characterizing dialogue behav-iors in human-human dialogue, and potentially in human-computerdialogue as well [16, 27, 11, 7, 1]. We also appreciate the contribution of J. Aberdeen,E. A formal framework for linguisticannotation.
H01-1017@@Over the last five years, three technological advances havecooperated to push speech-enabled dialogue systems backinto the limelight: the availability of robust real-time speechrecognition tools, the explosion of Internet-accessibleinformation sources, and the proliferation of mobileinformation access devices such as cell phones. Galaxy-II: A Reference Architecture for The GCSI i sdistributed under a modified version of the MIT X Consortiumlicense, and we are reasonably certain that the licensesimplifies all these tasks.
H01-1028@@In studying the contrasts between human-computer (HC) andhuman-human (HH) dialogues [1] it is clear that many HCdialogues are plagued by disruptive errors that are rarely seenin HH dialogues. Forcalculations in the present analysis, we used E as the baselineof real errors, rather than E+Q. These40 false errors can be classified as follows:A.
H01-1029@@Current technology makes the automated capture, storage,indexing, and categorization of broadcast news feasible allowingfor the development of computational systems that provide for theintelligent browsing and retrieval of news stories [Maybury,Merlino & Morey 97; Kubula, et al, 00]. [Rabiner, `89] L. R. Rabiner, A tutorial on hidden Markovmodels and selected applications in speech recognition.Proceedings of the IEEE, vol. The trained model associates a highlikelihood of seeing the value .
H01-1030@@  The goal of TDT is to monitor and reorganize a stream of broadcast news stories in such a way as to help a user recognize and explore different news events that have occurred in the data set. [14] E. Fox, G. Nunn, W. Lee, Coefficients for combining concept classes in a collection, In the proceedings of the 11th ACM SIGIR Conference, pp. [2] Y. Yang, T. Ault, T. Pierce, Combining multiple learning strategies for effective cross validation, the Proceedings of the 17th International Conference on Machine Learning (ICML), pp.
H01-1033@@The effectiveness of a broad class of cross-language informationretrieval (CLIR) techniques that are based on term-by-term transla-tion depends on the coverage and accuracy of the available trans-lation lexicon(s). Match the stem of a document term to surface forms of Frenchterms in the tralex.. Hearst, F. Gey, and R. Tong, editors, Proceedings of the22nd Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval, pages7481, Aug.
H01-1043@@Syntactic analysis or parsing has been a main objective inNatural Language Processing. Automatic acquisition of a largesubcategorization dictionary from corpora. In IEICETransactions on Information and Systems, volumeE77-D No.
H01-1045@@Retrieving descriptions of the words and phrases, which are notoften found in dictionaries, has potential benefits for a number offields. [5] Srihari, R & Li, W. A Question Answering System Consequently a morestringent form of relevance was devised.
H01-1046@@ In natural language processing, syntactic and semantic knowledge are deeply intertwined with each other, both in their acquisition and usage. The top-50 most similar paths to X solves Y. Dagan I, Lee L, and Pereira F., Similarity-based Methods for Word Sense Disambiguation. Minipar works with a constituency grammar internally.
H01-1052@@A significant amount of work in empirical natural languageprocessing involves developing and refining machine learningtechniques to automatically extract linguistic knowledge from on-line text corpora. [9] Nigam, K, McCallum, A, Thrun, S and Mitchell, T. TextClassification from Labeled and Unlabeled Documents usingEM. A Bayesian hybrid method forcontext-sensitive spelling correction.
H01-1054@@ Although recent years has seen increased and successful research efforts in the areas of single-document summarization, multidocument summarization, and information extraction, very few investigations have explored the potential of merging summarization and information extraction techniques. Where the differences vs. the Baseline system are significant according to the t-test, the p-values are shown. But a U.N. relief coordinator says its a "scenario from hell".
H01-1060@@Our goal is to producesystems that allow interactive users to presentEnglish queries and retrieve documents in languages that they can-not read. ADDITIONAL AUTHORSClara I. Cabezas ( Department of Linguistics, top University ofMaryland, College Park, email: clarac@umiacs.umd.edu)7. We achieve a meanaverage precision of 0.
H01-1062@@In comparison with written language, speech and espe-cially spontaneous speech poses additional difficulties forthe task of automatic translation. [16] N. Reithinger, R. Engel: Robust content extraction fortranslation and dialog processing. Notice the large number of word singletons, i. e.words seen only once.
H01-1064@@Increasing amounts of public, corporate, and private audio presenta major challenge to speech, information retrieval, and human-computer interaction research: how can we help people to take ad-vantage of these resources when current techniques for navigatingthem fall far short of text-based search methods In this paper,we describe SCANMail, a system that employs automatic speechrecognition (ASR), information retrieval (IR), information extrac-tion (IE), and human computer interaction (HCI) technology to per-mit users to browse and search their voicemail messages by contentthrough a GUI interface. Details and a performance evaluation of the CallerId processare described in [7]. The system uses a 14k vocabulary, automatically generatedby the AT&T Labs NextGen Text To Speech system.
H01-1065@@Multidocument summarization poses a number of newchallenges over single document summarization. [7] V. Hatzivassiloglou, J. Klavans, and E. Eskin.Detecting text similarity over short passages:Exploring linguistic feature combinations via machinelearning. The weight of a linear order(Thi1; : : : ; Thik) is dened as the sum of the counts for everypair Cil;im, such that il imand l; m 2 f1 : : : kg.
H01-1069@@Several research projects have recently investigated theproblem of automatically answering simple questions that havebrief phrasal answers (factoids ), by identifying and extractingthe answer from a large collection of text. A Boolean query is formed. There are many ways to ask the same thing: What is the age o fthe Queen of Holland Howlong has the ruler of Holland been alive Such variations form a sort ofsemantic equivalence class of both questions and answers.Since the user may employ any version of his or her question,and the source documents may contain any version(s) of theanswer, an efficient system should group together equivalentquestion types and answer types.
H05-1001@@Many approaches to summarization can be verybroadly characterized as TERM-BASED: they at-tempt to identify the main topics, These ap-proaches can be divided again very broadly in lex-ical approaches, among which we would includeLSA-based approaches, and coreference-based We are not aware, however, of any attempt touse both lexical and anaphoric information to iden-tify the main terms. Baldwin and T. S. Morton. To compute relative utility, a number ofjudges, (N  1) are asked to assign utility scores toall n sentences in a document.
H05-1002@@Information Structure (IS) is a partitioning of thecontent of a sentence according to its relation tothe discourse context. The last threecolumns divide the contexts in three groups:1. all instances having these contexts areassigned t;2. all instances having these contextsare assigned f;3. Our best system achieves a 90.
H05-1003@@ Reference resolution has proven to be a major obstacle in building robust systems for information extraction, question answering, text summarization and a number of other natural language processing tasks. This paper does not necessarily reflect the position or the policy of the U.S. Gov-ernment. A model-theoretic coreference scoring scheme.
H05-1004@@For example, in the following text segment,(1): The American Medical Associationvoted yesterday to install the heir apparent asits president-elect, rejecting a strong, upstartchallenge by a district doctor who argued thatthe nations largest physicians group needsstronger ethics and new leadership. The worst case (ie,when all entries in ffik  nflff,m( are non-zeros) complexityof the Kuhn-Algorithm is ;3t p E  and p * needed in the F-measure computation at Line 7.The core of the F-measure computation is the Kuhn-Munkres algorithm at line 5. Symmetry is a desirable property.
H05-1006@@The proliferation of speech recognition (SR) sys-tems is hampered by the ever-presence of recogni-tion errors and the significant amount of effort in-volved in error correction. Combining knowledge sources toreorder n-best speech hypothesis lists. In Proceedings of the thirdinternational workshop on parsing technologies.Rong Zhang and Alexander I. Rudnicky.
H05-1008@@Information Extraction (IE) is a technology for find-ing facts in plain text, and coding them in a logicalrepresentation, such as, eg, a relational database.IE is typically viewed and implemented as a se-quence of stagesa pipeline:1. E.g., at the nameclassification stage, a disease name (especially if notin the disease dictionary) may be misclassified, andused as a filler for the location slot.We further group together the location fills bystripping lower-case words that are not part of theproper name, from the front and the end of the fill.E.g., we group together southern Mumbai as referring to the same name.After grouping and trimming insignificant words,the number of distinct names appearing in locationfills is, which covers a total ofrecords,or  of all extracted facts. Baseline: Raw MajorityWe begin with a simple recovery approach.
H05-1010@@This approach has two primaryadvantages and two primary drawbacks. In addition to aModel 2-style quadratic feature referencing rela-tive position, we threw in the following proxim-ity features: absolute difference in relative posi-tion abs(j/|e|k/|f |), and the square and squareroot of this value. The maximum weight bi-partite matching problem, arg maxyY s(y), canbe solved using well known combinatorial algo-rithms or the following linear program:maxzjksjkzjk(1)s.t.jzjk However, we usethe LP to develop the learning algorithm below.For a sentence pair x, we denote positionpairs by xjkand their scores as sjk.
H05-1012@@tM ]that renders the meaning of the source sequence intothe target sequence. N66001-99-2-8916.The views and findings contained in this material arethose of the authors and do not necessarily reflectthe position or policy of the U.S. government and noofficial endorsement should be inferred. e-6 over the HMM algorithm and similarly Max-Ent should be rejected with a probability of 0. e-6 over the GIZA++ algorithm.
H05-1013@@In many natural language applications, such as au-tomatic document summarization, machine transla-tion, question answering and information retrieval,it is advantageous to pre-process text documents toidentify references to entities. A new approximate maximal margin classifi-cation algorithm. Thereis then a rather large drop with max link to r1u3^an ,followed by another drop for last link to r1q3^a` andfirst link performs the poorest, scoring r_pc^a` .
H05-1014@@The problem of novelty detection has long been a sig-nificant one for retrieval systems. The first is identifying relevantsentences, which is essentially a passage retrievaltask. Let M be thenumber of matched sentences, ie, the number ofsentences selected by both the assessor and the sys-tem, A be the number of sentences selected by theassessor, and S be the number of sentences selectedby the system.
H05-1017@@Supervised classification is the task of assigning cat-egory labels, taken from a predefined set of cate-gories (classes), to instances in a data set. Under mild regularity condi-tions of the unknown density function, it can beshown that mixtures of gaussians converge, in a sta-tistical sense, to any distribution.More formally, let ti  T be an instance describedby a vector of features ~ti  ).For each category Ci, GM induces a mappingfrom the similarity scores between its ID and anyinstance tj , sim(idci , tj), into the probability of Cigiven the text tj , P (Ci|tj). Journal of the American Society of InformationScience.A.
H05-1018@@Many NLP tasks such as parse selection and tag-ging can be posed as the classification of labeledordered trees. The time wasmeasured using a computer with 2. Therefore, our method isbeneficial even if we train the SVM only once.To see how our method scales to large amountsof data, we plotted the time for the conversion andthe SVM training w.r.t.
H05-1020@@ Our work is motivated by the objective to bring closer numerous achievements in the domains of machine learning and classification to the classical task of ad-hoc information retrieval (IR), which is ordering documents by the estimated degree of relevance to a given query. The second stage converts preference relations into a rank order. It is motivated by a typical histogram of df(t) distribution, which looks much more uniform in a logarithmic scale.
H05-1021@@Therefore anytranslation scheme that incorporates reordering mustnecessarily balance model complexity against theability to realize the model without approximation.In this paper our goal is to formulate models of lo-cal phrase reordering in such a way that they can beembedded inside a generative phrase-based model Although thismodel of reordering is somewhat limited and can-not capture all possible phrase movement, it formsa proper parameterized probability distribution overreorderings of phrase sequences. M Chinese wordson C-E and 5. Translation is mod-eled via component distributions realized as WFSTs(Fig 1 and Eqn 1) : Source Language Model (G),Source Phrase Segmentation (W ), Phrase Transla-tion and Reordering (R), Target Phrase Insertion( ), and Target Phrase Segmentation () In contrast, we will show that the cur-rent model can be used for both phrase alignmentand translation.
H05-1022@@Describing word alignment is one of the fundamen-tal goals of Statistical Machine Translation (SMT).Alignment specifies how word order changes whena sentence is translated into another language, andgiven a sentence and its translation, alignment spec-ifies translation at the word level. We use a simple, single parameterdistribution, with  wordsin the target language; n( ; e) is defined for  The hallucination process is a simplei.i.d. Thesmaller system is built on the FBIS C-E bitext col-lection.
H05-1023@@Here,phrase pairs, or blocks are obtained automati-cally from parallel sentence pairs via the underlyingword alignments. A maximum posterior methodfor word alignment. Shown in the following are severalkey E-step computations of the posteriors.
H05-1026@@Language models play an important role in manyapplications like character and speech recognition,machine translation and information retrieval. In all cases the same neural network architecturewas used, i.e a 120 dimensional continuous wordrepresentation and 500 hidden units. A neural probabilistic lan-guage model.
H05-1027@@The traditional approach uses a paramet-ric model with maximum likelihood estimation (MLE), usually with smoothing methods to deal with data sparseness problems. Unlike the case of E(. Let A be the input pho-netic string.
H05-1028@@Despite recent advances, interpreting what users communicate to the system is still a significant challenge due to insufficient recognition (eg, speech recognition) and understanding (eg, language understanding) performance. ( | )itP e gWhen no gesture is involved in a given input, the salience distribution at any given time is a uniform distribution. Class-based n-gram models of natural language.
H05-1029@@ Over the last decade, improvements in speech rec-ognition and other component technologies have paved the way for the emergence of complex task-oriented spoken dialog systems. Sorry, I didnt Catch That! A sec-ond system, the Lets Go!
H05-1033@@The computational treatment of discourse phenom-ena has recently attracted much attention, partly dueto their increasing importance for potential appli-cations. In Proceedings of the 3rdACL Workshop on Very Large Corpora, 8294.S. The MIT Press, Cambridge, MA.A.
H05-1034@@ Language model (LM) adaptation attempts to ad-just the parameters of a LM so that it performs well on a particular (sub-)domain of data. Let dt,i be the 267value for the dth parameter after the ith training sample has been processed in pass t over the train-ing data. Training data is a set of input-output pairs.
H05-1036@@In this paper, we generalize some modern prob-abilistic parsing techniques to a broader class ofweighted deductive algorithms. E.g., the inside algorithm in Fig. Eisner and D. A. Smith.
H05-1039@@Due to the ever increasing large amounts of onlinetextual data, learning from textual data is becom-ing more and more important. Second, we also replaceNT (time noun, such as NO ), DT (determiner, suchas P , Q ), cardinals (CD, such as R , S , T ) and M310(measurement word such as   ) with their POStags. A Unified Framework ForAutomatic Evaluation Using N-Gram Co-occurrenceStatistics.
H05-1040@@An important step in factual question answering(QA) and other dialog systems is to classify thequestion (eg, Who painted Olympia) to the antic-ipated type of the answer (eg, person). NIST.R Khardon, D Roth, and L. G Valiant. E.g., What birds eat snakes have the same words but differentinformers.
H05-1041@@Question answering (QA) systems for document col-lections typically aim to identify in the collectionstext snippets (eg, 50 or 250 characters long) or ex-act answers (eg, names, dates) that answer natu-ral language questions submitted by their users. The kappa statistic:A second look. In the case of t, setting it to any valueless than 0.  leads to a negative precision above0.
H05-1042@@A practi-cal generation system typically operates over a largedatabase with multiple entries that could potentiallybe included in a text. An additional constraint is impliedby computational considerations: our optimizationframework, based on minimal cuts in graphs, sup-ports only pairwise links, so we restrict our attentionto binary relations.We generate a range of candidate link types us-ing the following template: For every pair of entitytypes Ei and E j, and for every attribute k that is asso-ciated with both of them, create a link of type Li, j,k.A pair of entities a,b Multiple pairs of entries can be con-nected by the same link type.If the database consists of n entity types, and thenumber of attribute types is bounded by m, thenthe number of link types constructed by this processdoes not exceed O(n2(m +(m2)+(m3)))  O(n2m3).In practice, this bound is much lower, since only afew attributes are shared among entity types. Design of a knowledge-based reportgenerator.
H05-1043@@The Web contains a wealth of opinions about products,politicians, and more, which are expressed in newsgroupposts, review sites, and elsewhere. In the following, we formalize this map-ping.Let T denote the type of a word relationship in R (syn-onym, antonym, etc.) OPINE then uses the datato find explicit product features (E).
H05-1044@@Sentiment analysis is the task of identifying positiveand negative opinions, emotions, and evaluations.Most work on sentiment analysis has been done atthe document level, for example distinguishing pos-itive from negative reviews. BoosTexter: A boosting-based system for text categorization. Exploring Attitude and Affect in Text(AAAI Spring Symposium Series).R.
H05-1045@@In recent years, there has been a great deal of in-terest in methods for automatically identifying opin-ions, emotions, and sentiments in text. In Proceedings of the3rd Workshop on Very Large Corpora.E. 1 edges forming a linear chain.
H05-1048@@Automatic extraction ofthis model from raw text is important for creat-ing a knowledge base (such as relational databases,marked-up text etc.) of the4th Conference on Very Large corpora, pages 142-144.Lance E. Ramhsaw and Mitchel P. Marcus. For example, organization has gov-ernment, commercial, educational, non-prot andother as its sub-types.
H05-1049@@A fundamental stumbling block for several NLP ap-plications is the lack of robust and accurate seman-tic inference. As in the vertex case, we have weightsfor each hypothesis edge, w(e), based upon theedges label; typically subject and object relationsare more important to match than others. Usually a single sentence, but occasionally longer.
H05-1050@@Some of NLPs most interesting problems have to do withunsupervised learning. Their seedconsisted of 7 simple rules (that New York, California,and U.S. are locations; that any name containing Incor-porated is an organization; and that I.B.M. e), to reflect thebest polarity.To evaluate a learned classifier, we measure its over-lap with the true classification.
H05-1051@@ Lexical ambiguity refers to words that share the same orthography but have different meanings (word senses). This is a relative decrease of 14.%. One explanation for this can be  R-Precision Precision @10 Avg.
H05-1052@@Many natural language processing tasks consist of la-beling sequences of words with linguistic annotations,eg word sense disambiguation, part-of-speech tag-ging, named entity recognition, and others. Dependenciesbetween pairs of labels are represented as directed orindirected edges e  Such label dependencies can be learnedfrom annotated data, or derived by other means, as il-lustrated later. Wordnet: A lexical database.
H05-1053@@To take advantage of this, a methodfor automatically determining the one sense givena discourse or document is required. We chose eight words thatare particularly salient in the Sport corpus (referredto as S sal), eight in the Finance corpus (F sal), andseven that had equal (not necessarily high) saliencein both, (eq sal). 19predominant sense of a word from raw text.
H05-1054@@ Named Entity Recognition (NER) is one of the key techniques in the fields of Information Extraction, Question Answering, Parsing, Metadata Tagging in Semantic Web, etc. Therefore, a single model cant capture all types of entities. S.Y.Yu, et al Description of the Kent Ridge Digital Labs System Used for MUC-7.
H05-1055@@ Named Entity (NE) translation and transliteration are very important to many multilingual natural language processing tasks, such as machine trans-lation, crosslingual information retrieval and ques-tion answering. Not knowing Fs translation E, we cannot apply the translation model and the target language model for name origin classification. Apply a beam search around the initial phrase alignment path, searching for the optimal alignment which minimizes the overall phrase alignment cost, defined as:  Here is the i th source phrase in F, is its tar-get candidate under alignment A.
H05-1056@@Named entity recognition (NER), the identificationof entity names in free text, is a well-studied prob-lem. Learn an extractor E from the training corpus Ctrain . (which often indicates proximity toa home phone number appearing in a signature)..
H05-1058@@All of these models makeuse of varying amounts of contextual information.In this paper, we present a new model which re-mains within the well understood framework of Dy-namic Bayesian Networks (DBNs), and we showthat it produces state-of-the-art results when ap-plied to the POS-tagging task. For knownwords, a small set of features is used, while a muchlarger set of features is used for unknown words.This switching increases the speed of the model atno cost: the additional features increase the taggingaccuracy for unknown words but are redundant forknown words.This model factorizes the joint probability as:P (c, s,o)  Both of these are second order models(connecting tags in triples), but with different setsof features. 62then trained using all n+1 versions of each sentence,thus inducing the desired dependence between si1and si.
H05-1059@@One is the efficiencyof training. A maximum entropy approachto natural language processing. Therefore,in the first iteration, we always use the local classi-fiers trained with no contextual tag information (ie(P (ti|o)).
H05-1060@@One of the great successes in computational linguisticshas been the construction of morphological analyzers fordiverse languages. Eachpolygon corresponds to a feature template. of ACL Workshop on Computa-tional Approaches to Semitic Languages.E.
H05-1061@@ Key phrases such as named entities (person, loca-tion and organization names), book and movie ti-tles, science, medical or military terms and others 1, are usually among the most information-bearing linguistic structures. Search any web pages containing f and hint words e, as proposed in this paper. We propose a new framework to mine key phrase translations from web corpora.
H05-1062@@Named Entity Recognition (NER) is a crucial stepin many Information Extraction (IE) tasks. E being an error type (insertion, dele-tion, type, extent, type+extent, multiple) and e itsweight (resp. An algorithm that learns whatsin a name.
H05-1064@@A number of recent approaches in statistical NLPhave focused on reranking algorithms. In Proceedings of EMNLP.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-nifer C. Lai, and Robert L. Mercer. A statistical model for parsing and wordsense disambiguation.
H05-1066@@How-ever, there are certain examples in which a non-projective tree is preferable. , vn} and set E  [1 : n] of pairs (i, j) of directed edges vi  [1 : n]}That is, Gx is a graph with the sentence words andthe dummy root symbol as vertices and a directededge between every pair of distinct words and fromthe root symbol to every word. Thereare at most O(n) recursive calls since we cannotcontract the graph more then n times.
H05-1067@@... pleasure has probably been the main goal all along. Wordnet: A lexical database. In Pro-ceedings of the European Conference on Machine Learning.D.E.
H05-1068@@Among the most successful natural language learn-ing techniques for a wide variety of linguistic phe-nomena are supervised inductive learning algo-rithms for classification. The best set of classifiers, w.r.t.the target metric, is then greedily selected. (Select-ing a set of size 1 is equivalent to parameter andalgorithm tuning.)
H05-1069@@% accuracy1on words for which manually sense-tagged data areavailable. A commercial electronic dictionary application. One first looks up the Chinesetranslations of this sense in an English-Chinese dic-tionary, and finds that |E is the right Chinesetranslation corresponding to this particular sense.Then, the next stage is to automatically build a col-lection of Chinese text snippets by either searchingin a large Chinese corpus or on the Web, using |E as query.
H05-1071@@Search engines are extremelyhelpful for several linguistic tasks, such as computing us-age statistics or finding a subset of web documents to an-alyze in depth; however, these engines were not designedas building blocks for NLP applications. This is a 134-fold speedup. the set of unique error labels; |E| is the numberof unique error labels in the urn.
H05-1073@@Text does not only communicate informative con-tents, but also attitudinal information, includingemotional states. she answered, have I not reason to weep (2b) N: Nevertheless, he wished to try him first, and took a stonein his hand and squeezed it together so that water dropped outof it.Cases (1a) and (1b) are from the well-known FOLKTALE Snowdrop, also called Snow White. (1a)and (1b) are also correctly classified by the sim-ple content BOW approach, although our approachhas higher prediction confidence for E/NE (1a); italso considers, eg direct speech, a fairly high verbcount, advanced story progress, connotative wordsand conjunctions thereof with story progress fea-tures, all of which the BOW misses.
H05-1077@@This paper presents an examination of a collectionof semantic associates evoked by German verbs ina web experiment. 6 associate responses with a range of 0-16. . These cases are due to (i) lemmatisation inthe empirical grammar dictionary, where noun com-pounds such as Autorennen car racing are lemma-tised by their lexical heads, creating a mismatch be-tween the full compound and its head; (ii) domainand size of training corpus, which underrepresentsslang responses like Grufties old people, dialectexpressions such as Ausstecherle cookie-cutter The remaining 50% of the nouns are repre-sented in the grammar but do not fill subcategorised-for linguistic functions; clearly the conceptual rolesof the noun associates are not restricted to the sub-categorisation of the target verbs.
H05-1078@@One of thechallenges ahead lies in moving from hand-craftedprograms of limited scope to robust systems inde-pendent of a given domain. Notice that M [n, n],that is the [SEM-NULL,SEM-NULL] cell in the matrix, isnever taken into account. Semantic tagging us-ing a probabilistic context-free grammar.
H05-1079@@All examples are from the corpus released as part ofthe RTE challenge. These tools rely on Dis-course Representation Structures for T and H as wellas lexical and world knowledge. An example isexy(event(e)agent(e,x)in(e,y)in(x,y))which states that if an event is located in y, then sois the agent of that event.Lexical knowledge is created automatically fromWordNet.
H05-1083@@A coreference resolution system aims to group togethermentions referring to the same entity, where a mention isan instance of reference to an object, and the collection ofmentions referring to the same object in a document forman entity. Thesefeatures help to link the PRO(onominal) mention  (hm) with the NAM(e) mention 	J @ (AlzqAqywn).Without syntactic features theses two mentions were splitinto different entities. A maximum entropy approach tonatural language processing.
H05-1085@@In a statistical machine translation task, the goal isto find the most probable translation of some foreignlanguage text f into the desired language e. That is,the system seeks to maximize P (e|f). A systematic compari-son of various statistical alignment models. J. Och and H. Ney.
H05-1086@@Sentence retrieval is the task of retrieving a rel-evant sentence in response to a users query.Tasks such as question answering, novelty de-tection and summarization often incorporate asentence retrieval module. A statistical approach to machinetranslation. %, which is statistically significant using atwo-tailed t-test at the .05 level.
H05-1087@@Log-linear models have been used in many areas ofNatural Language Processing (NLP) and InformationRetrieval (IR). I am especially grateful to698Sameer Maskey for allowing me to use his speechsummarization dataset for the evaluation in Sec-tion 7. Recall (R) and preci-sion (P) are defined in terms of the number of truepositives (A), misses (B), and false alarms (C) of theclassifier (cf.
H05-1088@@Event recognition is, after entity recognition, one ofthe major tasks within Information Extraction. In ad-dition, it is conceived from a highly modular per-spective. Event recog-nition is also at the core of Question Answering,1This work was supported by a grant from the AdvancedResearch and Development Activity in Information Technology(ARDA), a U.S. Government entity which sponsors and pro-motes research of import to the Intelligence Community whichincludes but is not limited to the CIA, DIA, NSA, NIMA, andNRO.since input questions touch on events and situationsin the world (states, actions, properties, etc.
H05-1091@@One of the key tasks in natural language process-ing is that of Information Extraction (IE), which istraditionally divided into three subproblems: coref-erence resolution, named entity recognition, andrelation extraction. The MIT Press,Cambridge, MA.Vladimir N. Vapnik. For example, the words from the sen-tence protesters seized several stations are mappedin the lexicon to the following categories:protesters : NPseized : (S\NP )/NPseveral : NP/NPstations : NPThe transitive verb seized expects two arguments:a noun phrase to the right (the object) and anothernoun phrase to the left (the subject).
H05-1092@@Identifying the interactions between proteins is oneof the most important challenges in modern ge-nomics, with applications throughout cell biology,including expression analysis, signaling, and ratio-nal drug design. A logistic sigmoid function is used in theoutput layer. InProceedings of the SIGIR04 workshop on Search and Dis-covery in Bioinformatics.T.
H05-1094@@Within information extrac-tion, for example, part-of-speech tagging and shallowparsing are often performed before the main extractiontask. Anyopinions, findings and conclusions or recommendations ex-pressed in this material are the author(s) and do not necessarilyreflect those of the sponsor.ReferencesByrd, R. H., Nocedal, J., & Schnabel, R. B. Repre-sentations of quasi-Newton matrices and their use in limitedmemory methods. We will denote weight k in an individual CRF i byik and a single feature by fik(sit1, sit, si1t ,x, t).
H05-1095@@We propose here a model designed to dealwith multi-word expressions that need not be con-tiguous in either or both the source and the targetside.The rest of this paper is organised as follows. Och proposes two workarounds to thisproblem: the first one relies on a direct optimiza-tion method derived from Powells algorithm; thesecond introduces a smoothed (continuous) versionof the error function E(T,R) and then relies on agradient-based optimization method.We have opted for this last approach. A decoder forsyntax-based statistical MT.
H05-1096@@The work presented in this paper deals with con-fidence estimation for machine translation (MT).Since sentences produced by a machine translationsystem are often incorrect but may contain correctparts, a method for identifying those correct partsand finding possible errors is desirable. Consider a target word e occurring in thesentence in position i1. N -best lists from the system (eq.
H05-1097@@The problem of distinguishing between multiplepossible senses of a word is an important subtask inmany NLP applications. Typically, strong independence assumptions arethen made about the distribution P (s|t). Our basic logistic regression model usesthe following features, which correspond to the fea-ture space for a standard Naive Bayes model: the part of speech of a (generated using theBrill tagger)5; variable for each word whichis 1 if that word is in a fixed context centeredat a (cr words to the right and cl words to theleft), and 0 otherwise.We also consider an extension to this model, whereinstead of the fixed context features above, we use: {l, r} and each possi-ble context size cd  {1, ..., Cd}, an occursvariable for each word.This is a true generalization of the previous con-text features, since it contains features for all pos-sible context sizes, not just one particular fixed size.This feature set is equivalent to having one featurefor each word in each context position, except thatit will have a different prior over parameters understandard L2 regularization.
H05-1098@@Hierarchical organization is a well known prop-erty of language, and yet the notion of hierarchi-cal structure has, for the last several years, beenabsent from the best performing machine transla-tion systems in community-wide evaluations. M. Lewis II and R. E. Stearns. The ba-sic model uses the following features, analogous toPharaohs default feature set: The second has weight one, and the firsthas weight w(S  g),the idea being that parameter g controls the modelspreference for hierarchical phrases over serial com-bination of phrases.Phrase translation probabilities are estimated byrelative-frequency estimation.
H05-1099@@Finite-state parsing (also called chunking or shallowparsing) has typically been motivated as a fast first-pass for  For many very-large-scale natural language processing tasks (eg open-domain question answering from the web), context-free parsing may be too expensive, whereas finite-state parsing is many orders of magnitude faster andcan also provide very useful syntactic annotationsfor large amounts of text. Textchunking based on a generalization of Winnow. In par-ticular, any S node that contains an empty subjectNP and a VP is reduced to just a VP node, andthen combined with any immediately-preceding VPnodes to create a single VP constituent.
H05-1101@@State of the art architectures for machine transla-tion are all based on mathematical models calledtranslation models. G [w1, w2], we write t,l andt,r to denote the left and the right parse trees, re-spectively, associated with . , cn} is a set of clauses.
H05-1102@@Because of the intro-duction of the adjoining operation, the TAG formal-ism is provably stronger than Context Free Gram-mar (CFG) both in the weak and the strong genera-tive power. Schabes and R. C. Waters. A neural network parser that handlessparse data.
H05-1103@@ The REAP system automatically provides users with individualized authentic texts to read. is a hypernym of person. Any opinions, findings and conclusions or recommen-dations expressed in this material are those of the authors and do not necessarily reflect the views of the U.S. Department of Education.
H05-1105@@Resolution of structural ambiguity problems suchas noun compound bracketing, prepositional phrase(PP) attachment, and noun phrase coordination re-quires using information about lexical items andtheir cooccurrences. % recall with a precision of 91. Search engine statisticsbeyond the n-gram: Application to noun compound bracket-ing.
H05-1107@@Natural language applications that use super-vised learning methods require annotated train-ing data, but annotated data is scarce for manyWe thank Stephen Clark, Roger Levy, Carol Nichols,and the three anonymous reviewers for their helpful com-ments.non-English languages. (a) Evaluation against the CoreTag gold standard. The confidence score is computed as:logP (T |W )length of the sentence .
H05-1110@@ and MotivationModern statistical natural language processing tech-niques require large amounts of human-annotateddata to work well. )||Pi(|i))where D is a reasonable measure of distance be-tween probability distributions. Pn(|n))where d measures the distance between P and all thePi under the parameter setting i.
H05-1111@@Intelligent language technologies capable of fullsemantic interpretation of domain-general text re-main an elusive goal. The more specificmodel, P (r|v, s), performs less well, and may beover-fitting on this relatively small amount of train-ing data. Class based con-struction of a verb lexicon.
H05-1112@@Problem DescriptionThe identification of semantic relations in open textis at the core of Natural Language Processing andmany of its applications. Lets define with xi the feature vector of an in-stance i and let X be the space of all instances; iexi  The multi-class classification is performedby a function that maps the feature space X into asemantic space SF : X  S)n where n isthe number of examples x each accompanied by itssemantic relation label r. The problem is to decidewhich semantic relation r to assign to a new, unseenexample xn+1. Generalizing caseframes using a thesaurus and the mdl principle.
H05-1113@@), idioms like kick the bucket ), phrasal verbs like find out and compounds like village commu-nity. In Advances in Kernel Methods SupportVector Learning.T. Ex-ample : take a look.
H05-1115@@Recent work has motivated the need for systemsthat support Information Synthesis Understanding such storiesis challenging for a number of reasons. Journal of the American So-ciety for Information Science and Technology, 56(3),March.Stephen E. Robertson, Steve Walker, MichelineHancock-Beaulieu, Aarron Gull, and Marianna Lau. (2:14)While the outer edge of the hurricane approached the North Carolina coast Wednesday, the center of the storm was still400 miles south-southeast of Cape Hatteras, N.C., late Wednesday morning.
H05-1116@@The bulk of the research in thisarea, however, addresses fact-based questions like:When did McDonalds open its first restaurant or What is the Kyoto Protocol. noun phrase (n), verb phrase (v), prepositionalphrase (p), and clause (c)  The exact match criterion is satisfied only by answer seg-ments whose spans exactly correspond to a constituent inthe CASS output. a Tokyo organization representing about 150Japanese groups).
H05-1118@@Improving information retrieval (IR) through natu-ral language processing (NLP) has been the goalfor many researchers. settings survive anywayusing our top-N selection approach. The result is usually a rankedlist of documents.
H05-1119@@She has shown that the audio recordings arevaluable to users if portions of interest can be accessedquickly and easily.Stifelman explored various techniques for this, includ-ing user-activity based techniques (most noteworthy time-stamping notes so they can serve as an index into therecording) and content-based ones (signal processing foraccelerated playback, snap-to-grid The latter are intended for sit-uations where the former fail, eg when the user has notime for taking notes, does not wish to pay attention to it,or cannot keep up with complex subject matter, and as aconsequence the audio is left without index. The expected value is takenw.r.t. A. James and S. J.
H05-1120@@There are several sources of error in written lan-guage. Crm (1)where F is the frequency, r is rank, C is a constant,and m is an exponent close to 1. log(r) (2)The frequency and rank of search query tokensapproximately follow the same distribution, withsome deviation at the high and low ends. The letter e is most com-monly mistyped as a, o, and i; the letter i is mostoften mistyped as a, u, and e. For the most part,vowel substitutions can be considered to be cogni-tive errors (except o  i may be a cognitive error ortypographic error).
H05-1121@@Query expansion is a simple but very useful tech-nique to improve search performance by addingsome terms to an initial query. pt)(2)where pt is the probability that a term t appears inrelevant documents. qt is the probability that a termt appears in non-relevant documents.
H05-1122@@Topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assess-ing student answers. The egg has a constant hori-zontal velocity. S2: I have an exam tomorrow.
H05-1124@@Text segmentation is a basic task in language pro-cessing, with applications such as tokenization, sen-tence splitting, named-entity extraction, and chunk-ing. A kernel method formulti-labeled classification. Ifwe can bound the largest gap in any non-contiguoussegment by a constant g  n, then the runtime canbe improved to O(ngkT ).
H05-1127@@The problem of developing a dialog manager can beexpressed as the task of building a specific dialogpolicy for the dialog system to follow as it interactswith the user. We chose to use e-greedy action selection inorder to achieve this goal. As highervalues for S and lower values for L indicate betterdialogs, we subtract w2L from w1S.
H05-2007@@All are predicated on the con-cept of n-gram matching between the sentence hypoth-esized by the translation system and one or more ref-erence translationsthat is, human translations for thetest sentence. A maximum entropy model forpart-of-speech tagging. t j) of every possi-ble tag sequence ti .
H05-2012@@Information Extraction (IE) is a technology for find-ing facts in plain text, and coding them in a logicalrepresentation, such as a relational database.Much published work on IE reports on closedexperiments; systems are built and evaluated basedon carefully annotated corpora, at most a few hun-dred documents. On the other hand, automaticerror reduction requires a critical mass of extractedfacts. In IJ-CAI03 Workshop on Learning Statistical Models fromRelational Data.U.
H05-2017@@The Web contains a wealth of customer reviews as aresult, the problem of review mining We decompose theproblem of review mining into the following subtasks:a) Identify product features, b) Identify opinions re-garding product features, c) Determine the polarity ofeach opinion and d) Rank opinions according to theirstrength (eg, abominable ).We introduce OPINE, an unsupervised information ex-traction system that embodies a solution to each of theabove subtasks. Finally, OPINE identifies the SO label ofword w in the context of feature f and sentence s. For ex-ample, some people like large scanners (I love this largescanner) and some do not (I hate this large scanner ).The phrases with non-neutral head words are retained asopinion phrases and their polarity is established accord-ingly. The system then finds explicitly men-tioned product features (E) using an extended versionof KNOWITALLs extract-and-assess strategy describedabove.
H05-2018@@OpinionFinder is a system that performs subjectivityanalysis, automatically identifying when opinions,sentiments, speculations, and other private states arepresent in text. A high-precision, rule-based clas-sifier is used to identify these expressions. .0 is used to tok-enize, sentence split, and part-of-speech tag the data,and the Abney stemmer2 is used to stem.
H86-1009@@  Considerable progress has been made in developing systems which understand short passages of technical text. This information is needed not only for some of the applications (e~g., message summarizat ion) but also to resolve some of the syntactic and semantic ambiguities in the messages. A diagnostic system, for example,  would then be able to accept initial observations in the form of a brief textual summary rather than force the user to go through an elaborate questionnaire; this may be a substantial advantage for broad-coverage diagnostic systems, which must be able to accept a wide variety of d i f ferent symptoms.
H86-1017@@\] In cooperative man-machine interaction, it is necessary but not sufficient for a system to respond truthfully and informatively to a users question. for a natural anguage utterance. Pollack, Martha E. Goal Inference in Expert Systesm.
H86-1020@@ Investigation of constrained grammatical systems from the point of view of their linguistic adequacy and thei computational tractability has been a major concern of computational linguists for the last several years. Step 15 is completed in O(n). (e) Case 5 corresponds to adjoining.
H89-1008@@Examples of the kind of information that may be available for a particular unit are: its home port, current location, current employments (an employment is a complex concept including destination, projected arrival time, purpose, etc. Show me the ships with a personnel resource readiness of C3. A domain model, which is a class-and-attribute r presentation of the concepts and relationships that the Parlance user might employ in quer ies .
H89-1012@@As our integration methodology, we use lallice parsing. 5 parses and a mode of 1. An example of these rules is given below: 3 S --~ NP  VP  OPTSADJUNCT (lambda (np vp oa) (oa (intension ( (q np) vp) ) ) ) This is the top-level declarative clause rule given earlier, with its corresponding semantic rule.
H89-1013@@: MOTIVATION Portability is measurable by the person-effort expended to achieve a pre-specified egree of coverage, given an application program. The most frequent examples where one does not have ~e coupling of those patterns i  the preposition of. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government.
H89-1018@@Spontaneous speech contains a number of phenomena that cause problems for current systems. restarts repeating a word or phrase. It is not sufficient o simply ignore unrecognized areas without classifying them.
H89-1022@@  Penman is designed as a domain-independent text generator that can be installed in an application system to generate text for the application on demand. Bateman, J., Kasper, R., Schfitz, J. and Steiner, E. Interfacing an English Text Generator with a German MachineTranslation Analysis. In terpretat ion  o f  Sentence  P lans  A sentence plan in the SPL notation is interpreted in two phases.
H89-1024@@ Our earlier development efforts \[2,3,4,5,6,7,8,9\] centered on improving the SD speaker-stress robustness for both IWR and CSR tasks. T IED MIXTURES A version of tied mixtures \[15,16\] has been tested and shown to provide a small improvement for the SI task. This is not sufficient o train an SI system.
H89-1027@@ For slightly over a year, we have focused our research effort on the development of a phonetically-based spoken language understanding system called SUMMIT. On the other hand, the same panel shows that the correct label for the first schwa in "assistance" is the third most likely candidate, beh ind /n /and/ r j / . Glass, M. Pfiillips, and S. Seneff, "Acoustic Segmentation a d Phonetic Classification in the SUMMIT system," Proc.
H89-1032@@ This project undertakes to provide meaningful measures of progress in the field of natural language processing (NLP). The events that will cause the system to fill in a template concern hostile or potentially hostile encounters between one or more members of the U.S. forces and one or more members of an enemy force -detecting the enemy, tracking it, targeting it, harassing it, or attacking it. A Sublanguage for Reporting and Analysis of Space Events.
H89-1043@@ We report on an innovative and highly effective architecture for integrating speech and natural-language (NL) processing. We have identified a number of factors that we believe are responsible for this. In fact, no information about word boundaries i  even passed to the parser.
H89-1049@@ Recent theories of focusing and reference rely crucially on discourse structure to constrain the availability of discourse ntities for reference, but deriving the structure of an arbitrary discourse has proved to be a significant problem (\[Webber 881). For each of the remaining segments (A-E), the processing is described below. Higgins, F. R. The Pseudo-Cleft Construction in English.
H89-1053@@-Context Sensitivity in Realizations Phonologists claim that the context in which a phoneme occurs leads to consistent differences in how it is pronounced. This tree is for the special case describing whether/t /  is glottalized. J.R. Quinlan, "Generating production rules from decision trees," IJCAI-87, pp.
H89-2002@@ Prosodic information can mark lexical stress, identify phrasing breaks, and provide information useful for semantic interpretation. Boundary  Tone  C lass i f i ca t ion  Boundary tones are another important cue for phrase boundary detection and for semantic interpretation. Laura ran away m/th ~e man wem/ni/a green robe.
H89-2006@@This is accomplished by adding semantic features to the grammar rules, placing them on the same footing as the existing syntactic features. A New Parsing Algorithm for Unification Grammar. These N hypotheses are then analyzed by the parser, using the combined syntactic/semantic grammar.
H89-2007@@These experiments are presented by the authors in these proceedings. WORDS and NOISE SPHX i PHNX % Reduction sent 47. A session represents approximately 100 utterances.
H89-2017@@ Spoken language systems must, obviously, deal with spontaneous speech. The number of "um"s includes a variety of different pause fillers used by the subjects. The data was accessed via a wizards interface.
H89-2018@@ One of the first tasks confronting researchers developing a spoken language system is the collection of data for analysis, system training, and evaluation. As a result, the quantitative differences were found to be statistically significant. \[3\] Seneff, S., "TINA: A Probabilistic Syntactic Parser for Speech Understanding Systems," Proceedings of the First DARPA Speech and Natural Language Workshop, pp.
H89-2019@@ The DARPA community has recently moved forward in beginning to define methods for common evaluation of spoken language systems. A similar point can be made concerning vague expressions (S-5). The two Boolean values are t rue  and fa l se .
H89-2020@@In recent DARPA speech community-wide r cognition system evaluations, the recognition systems have been tested using two grammatical conditions: no grammar (or null grammar), and the word-pair grammar. A word may be assigned to multiple classes. rice of Naval Research under N00014-85-C-.0279.
H89-2021@@ The ability to communicate by speech is known to enhance the quality of communication, asreflected in shorter problem-solving times and general user satisfaction \[2\]. We believe that grammaticality s an important component of a composite metric for the language habitability of an SLS and can provide a meaningful basis for comparing different SLS interfaces to a particular application 3. A comparison speech versus typed input.
H89-2027@@ In a spoken language system (SLS) we have a large search problem. Typically, alternatives must be generated using a fast match 201 Sp,.+h__l JOr+.r,+ Reorder L_. Top N Choices: Set charts which resolution to five.
H89-2031@@ A major problem that restricts the usefulness of natural language processing systems is the cost, in time and effort, of porting a system to a new domain. This result is remarkable for a number of reasons. We have also used the Learner output o acquire this information.
H89-2044@@ The acceptability of any voice interface depends on its ease of use. Some sample utterances are N-S-V-H-6-T-49, ENTER-4-5-8-2-1 and 312 P-I-T-T-S-B-U-R-G-H. A total of 106 vocabulary items appeared in the vocabulary, of which about 40 were rarely uttered. As a result, there were many confusions of silence or noise segments with weak phonetic events.
H90-1006@@ "Prosody," the suprasegmental information in speech, ie, information that cannot be localized to a specific sound segment, can mark lexical stress, identify phrasing breaks and provide information useful for semantic interpretation. 2 Steedman,  however, \[14\] had  made some interest ing arguments  concerning the appropr iateness  of categorial  g rammar  for reflecting prosodic s t ructure . These rules necessitate the incorporation into the modified grammar of a rule L ink  ~ e; otherwise, the sentence will not parse, because an empty node introduced by the grammar will either not be preceded by a link, or not followed by one.
H90-1017@@To achieve the goal of real-time recognition on a personal computer isa process that requires analysis of the computational requirements of the recognition algorithm along several dimensions, and the improving the recognizers performance along those dimensions. An acoustic luster consists of a mean vector and a variance vector. Recognition The recognition search to find the most likely sentence hypothesis i  based on the time-synchronous decoding algorithm that is used in almost all current CSR systems for this vocabulary size.
H90-1020@@Numbers are meaningless unless it is clear where they come from. For example, for the fare restriction code "VU/ i "  the prompt may appear as "V U slash one" or as "V U one", depending on what the subject said. References [1] L. Bates and S. Boisen, "Developing an Evaluation Methodology for Spoken Language Systems," this volume.
H90-1021@@ The ATIS corpus provides an opportunity to develop and evaluate speech systems that understand spontaneous speech. The speech collection occurs in an office environment rather than a sound booth. The evaluation methodology supported by ATIS depends on having a comparable representation f the answer for each utterance.
H90-1056@@Statistical language models were quite popular in the Previous work [5] led to the spelling correction program, correct. The performance of G/E is significantly better than the other four. This is probably a desirable property.
H90-1064@@This form of continuous observation pdf shares the generality of discrete observation pdfs (histograms) with the absence of quantization error found in continuous density pdfs. Paul and E. A. Martin, "Speaker StressResistant Continuous Speech Recognition," Proc. Finally, a third observation stream was tested.
H91-1009@@ In this paper we report on some preliminary work done at Dragon Systems on the Resource Management benchmark task. Other content word errors often involve homophones ( uch as "ships+s" --~ "ships"). This results in a speaker-dependent PEL spelling for each PIC.
H91-1013@@ While most successful systems to date have been based on hidden Markov models (HMMs), there may be utility in combining the HMM approach with some other very different approach. F. K. Soong and E.-F. Huang, "A Tree-Trellis Based Fast Search for Finding the N-Best Sentence Hypotheses in Continuous Speech Recognition," Proceedings o\] the Third DARPA Workshop on Speech and Natural Language, pp. M. Ostendoff and S. Roukos, "A Stochastic Segment Model for Phoneme-based Continuous Speech Recognition," IEEE Trans.
H91-1021@@ Improving the performance of spoken language systems requires addressing issues along several fxonts, including basic improvements in natural language processing and speech recognition as well as issues of integration of these components in spoken language systems. It will be important o lower the rate of such errors to a level well below S%. The application component was not able to rnal~e a sensible call for the remaining inputs.
H91-1025@@ An a,lluring a,spect of the staMstica,1 a,pproa,ch to ins,chine tra,nsla,tion rejuvena.ted by Brown, et al, \[_1\] is the systems.tic framework it provides for a.tta.cking the problem of lexicM dis~tmbigua.tion. For this E l  find the best 3: 3 . Clearly the probability distributions P r (E )  and Pr (F IE  ) over sentences are immensely complicated.
H91-1029@@ Speech recognition/understanding systems will ultimately establish their usefulness by working well under eal application conditions. D i rec tory  Ass i s tance  Ca l l  Complet ion :  Yes /No  Recognition The target response for the DACC trial was an isolated yes or no response. The recognizer that was trained and tested on laboratory speech performed almost 9% better than the recognizer t ained and tested on real user speech (95.
H91-1033@@ "Subcategorization" refers to the constraints hat a verb (or other syntactic head) places on the type and relative order of the phrases that serve as its arguments and (by some definitions) the effect of these argument phrases on the meaning of the whole clause. \[11\] Pereira, F. C. N. and Warren, D. H. D. "Definite Clause Grammars for Language Analysis--A Survey of the Formalism and a Comparison with Augmented Transition Networks". \[8\] Johnson, R. and Rosner, M. "A rich environment for experimentation with unification grammars".
H91-1034@@ One of the conclusions SRI has drawn from working with the ATIS common task data is that, even with a very constrained user task, there will always be unanticipated expressions and difficult constructions in the spoken language elicted by the task that will cause problems for a conventional, analytical approach to natural-language processing. Each template has a set of key words (or key phrases). and Hayes, P.J., Recovery Strategies for Parsing Extragrammatical L nguage, Technical Report CMU-CS-84-107, Carnegie-Mellon Univer
H91-1036@@ Elsewhere \[1\] we describe a change in our approach to NL processing to allow for more robust methods of interpretation. I f  so, it is added to the chart; otherwise, it is rejected. With this rationale, we have implemented several variants of a bottom-up arsing algorithm that allows us to use limited top-down constraints derived from the leftcontext o block the formation of just the phrases that implicitly contain gaps not licensed by the preceding context.
H91-1037@@ Traditionally natural language processing (NLP) has focussed on obtaining complete syntactic analyses of all input and on semantic analysis based on handcrafted knowledge. PERPETRATOR: ID OF INDIV(S): a string 6. CATEGORY OF INCIDENT: set element, e.g, TERRORIST ACT, STATE-SPONSORED VIOLENCE 5.
H91-1042@@ In a Graham/Harrison/Ruzzo (GHR) parser, the chart is used to maintain a record of syntactic onstituents hat have been found (terms) and grammatical rules that have been partially matched (dotted rules). Act ion Type N Pa i rs  A1 A4 Rlghtmost  Endpolnt N-1 . 9 seconds, on a Sun 4/280.
H91-1044@@ All natural language grammars are ambiguous. Acquiring a Noun Classification from PredicateArgument Structures. % Accuracy Rate for Prepositional Phrm~e Attachment, by high accuracy.
H91-1051@@ It is well known that the pronunciation of a word or subword unit such as a phone depends heavily on the context. of the troo,~ which ~p~eifie~ the model to I)e used. EXPERIMENTAL RESULTS We to~t~d tbi.
H91-1057@@ A language model is used in speech recognition systems and automatic translation systems to improve the performance of such systems. The resulting 3dynamic estimators are linearly smoothed together to obtain a dynamic trigram model denoted by pc,(w,+l I w,., w,-a). \[2\] Kuhn, R., Speech Recognition and the Frequency o\] Recently Used Words: a Modified Markov Model for Natural Language, Proceedings of COLING B,dapest, Vol.
H91-1061@@ Text classification systems, ie systems which can make distinctions between meaningful classes of texts, have been widely studied in information retrieval and natural  anguage processing. The absolute level of e/Fectiveness might be a lesser concern. A news story categorization system.
H91-1062@@ There is no single dialogue problem. Q2: Which is the cheapest one if I want o travel Monday. But both 1 and 3 are often reasonable r sponses.
H91-1064@@ The TRAINS project involves building an intelligent planning assistant hat is eonversationally-proficit in natural language. First, we are developing a database for studying discourse phenomena. Request (Req) A request involves one agent attempting to get the other agent o do something by direct means.
H91-1065@@ 1 Natural language processing, and AI in general, have focused mainly on building rule-based systems with carefully handcrafted rules and domain knowledge. Therefore, we tried removing all tags whose probability is more than e 2 less likely than the most likely tag. 3 Parlance is a trademark of BBN Systems and Technologies.
H91-1066@@ The interpretation of large volumes of text poses many control problems, including limiting the complexity of analysis and ensuring the production of valid interpretations without considering too many possibifities. Topic  ana lys is  and  f i l ter ing:  Patterns for topical keywords and phrases help to perform topic analysis and filtering of stories. Tempora l  phrase  recogn i t ion  and  reduct ion :  The pattern matcher picks out many temporal adverbial phrases, such as: II THE PAST FEW HOURS MORE TilhB 3 MOBTIIS AGO.
H91-1067@@ Accurate parsing requires knowing the subcategorization frames of verbs, as shown by (1). (1) a. oK I expected him to eat ice-cream b. In ~Sth Ann~tal Meeting oj r the Association .for Comp.
H91-1070@@ A key goal of spoken language systems is to provide support for interactive problem solving. The system then fills in the appropriate slots in the displayed ticket, including airline, flight number, departure and arrival times, far e category, and dollar amount. Can I help you with something else7 Subject: NO THANK YOU (9) ATIS: Have a nice day.
H92-1003@@support a common evaluation on speech, natural language and spoken language;  maximize the amount of data collected;  provide some diversity in data collection paradigms;  reduce cost to any one site by sharing the data collection activity across multiple participating sites. Shriberg, E., E. Wade, and P. Price, "Human-Machine Problem Solving Using Spoken Language Systems (SLS): Factors Affecting Performance and User Satisfaction" Proc. Co l lec t ing  the  Data  Data collection procedures were not standardized across sites.
H92-1005@@ For the first two years of the DARPA Spoken Language Program, common evaluation in the ATIS domain has been performed solely with the Common Answer Specification (CAS) protocol \[4\], whereby a systems performance is determined by comparing its output, expressed as a set of database tuples, with one or more predetermined reference answers \[1\]. Find a flight from Atlanta to Baltimore. Resu l t s  and  Ana lyses  From the collected data, we made a number of measurements for each scenario, and examined how the two systems differed in terms of these measures.
H92-1006@@ The use of a common task and a common set of evaluation metrics has been a comerstone ofDARPA-funded research in speech and spoken language systems. Appolt, D., Jackson, E., and R. Moore, "Integration of Two Complementary Approaches to Natural Language Understanding," Proc. Find a flight from Atlanta to Baltimore.
H92-1008@@ In support of our research toward developing telephonebased spoken language systems, we have joined BBN, CMU, MIT, and SRI in collecting speech and language data for the ATIS (Air Travel Information Service) domain. ATIS: lm ready to begin a scenario. AT~zT is in the middle of the pack in utterances per scenario, collection rate (number of utterances collected ivided by recording session duration), and human-machine interaction time /45 Scenar~\[o: "Determine the type of aircraft used on a flight from one city to another that leaves before (or after) a certain time of the day."
H92-1009@@ Data collection is a critical component of the DARPA Spoken Language Systems (SLS) program. DARPA Speech and Natural Language Workshop, E Price (ed. We hypothesized that one way users might adapt would be to conform to the language mode~s constraining recognition.
H92-1016@@We have also modified our natural language component o include a robust parsing strategy. We also introduced a number of specific allophones for certain phonemes in certain contexts, such as a retroflexed / f /  or a stop closure following a fricative, and a number of new diphone units, allowing a sequence of two phonemes to be treated as a diphthong, such as /e l /o r /a t / . In addition, it is likely that there are stronger II Input I Correct \[ Incorrect No Answer I Error H Text 187. contextual effects in a spontaneous speech corpus such as ATIS than in a more carefully spoken "read" corpus such as Resource Management.
H92-1017@@ In recent work on the Paramax spoken language understanding system we have focused on domain-independent capabilities whose relevance xtends beyond the ATIS application. 1 For example, the instantiated ecompositions produced for "flights leaving Boston" are: f l i ght_C( f l ight l ,  source(_ )  . If there are no such flights, the inference is retracted, leaving in the context a concept of (all) th.e flights from Boston to Denver.
H92-1020@@ Statistical n-gram language models are useful for speech recognition and language translation systems because they provide an a-priori probability of a word sequence; these language models improve the accuracy of recognition or translation by a significant amount. SI: the letter fire to to to  We can see that the trigram cache can make some rare bigrams (wl, w~) more likely if both wx and w2 have already occurred due to a term of the form d(wt)d(w2) whereas the ME cache still has the factor p,(wl, w2) which will tend to keep a rare bigram somewhat \]ess probM)\]e. This is particular\]y pronounced for $2, where we expect d(building) to be quite accurate after 10 sentences, the ME cache penalizes the unlikely bigram by a factor of about 13 over the trigram cache. The actuM reduction was a factor of 1.
H92-1021@@ Linguistic constraints are an important factor in human comprehension of speech. Jelinek, E, Medaldo, B., Roukos, S., and Strauss, M., "A Dynamic Language Model for Speech Recognition," Proceedings of the Speech and Natural Language DARPA Workshop, pp. 7, exceeded a given threshold.
H92-1022@@ There has been a dramatic increase in the application of probabilistic models to natural anguage processing over the last few years. This resulted in a 3. There are 192 tags in the tag set, 96 of which occur more than one hundred t imes in the corpus.
H92-1023@@  In this paper we describe work which uses decision trees to estimate probabilities of words appearing with various parts-of-speech, given the context in which the words appear. Given a training corpus E of events, the decision tree method proceeds by placing the observed histories into equivalence classes by asking binary questions about them. The set of histories is denoted by 7-/, and a pair (t, H) is called an event.
H92-1026@@  Almost any natural language sentence is ambiguous in structure, reference, or nuance of meaning. We denote by t~ the sentential form obtained just before we expand node i. Mot ivat ion  fo r  H is tory -based  Grammars  One goal of a parser is to produce a grammatical  interpretation of a sentence which represents the syntactic and semantic intent of the sentence.
H92-1028@@ This paper introduces the idea of N-gram constrained context-free language models. (7) Given this energy E, the following algorithm generates a sequence of samples, {W 1, W 2, W3,. The work presented here is a beginning.
H92-1030@@ This paper is an exploration into the possibility of automatically acquiring the phrase structure of a language. text, one would need a prohibitively large corpus. (John and Mary k i ssed /VBD in / IN  the car vs. John and Mary bought /VBD the car).
H92-1041@@ Text categorization-the automated assigning of natural language texts to  predefined categories based on their content-is a task of increasing importance. Results We first looked a t  effectiveness of proportional assignment with word-based indexing languages. Particular care was taken in assigning categories [I].
H92-1055@@ The need for speech recognition systems and spoken language systems to be robust with respect to their acoustical environment has become more widely appreciated in recent years (eg \[1\]). and lngebretsen, R. B., "Blind Deconvolution Through Digital Signal Processing", Proc. Boll, S. F. , "Suppression of Acoustic Noise in Speech Using Spectral Subtraction", ASSP, Vol.
H92-1060@@ Current approaches to the language understanding aspect of spoken language systems tend to fall into two categories. Re jec t ion  Cr i te r ion  Because the DARPA evaluation mechanism currently penalizes ystems for incorrect answers, we augmented the robust parser with a capability for detecting certain key words, such as "between," which, if not properly understood, would most likely lead to an incorrect answer. Seneff, S., "The MIT ATIS System: Preliminary Development, Spontaneous Speech Data Collection, and Performance Evaluation,"
H92-1061@@ We describe the fallback understanding component of the DELPHI Natural Language component of BBNs Spoken Language System. DELPHI s grammar roles incorporate semantic onstraint and interpretation components by associating with each element of the fighthand side a grammatical relation label which keys into an associated system of semantic rules. Each is tried in succession until a topic is chosen.
H92-1064@@The expense and time consumption are sizable and, as the port may be minimal or incomplete, the evaluation may be based on a demonstration f less than the full potential of the system. Basic Sentences 1 Declarative Sentences 2 Imperative Sentences ......kn.~.o. Who-questions a) with verb .
H92-1073@@ As spoken language technology progresses and goals expand, progressively larger, and more challenging corpora need to be created to support advanced research. Next sequences of letters are separated: U.S.---~U. D. B. Paul, "Experience with a Stack Decoder-Based HMM CSR and Back-Off N-Gram Language Models," Proc.
H92-1085@@ Spontaneous spoken language often includes peech that is not intended by the speaker to be part of the content of the utterance. While acoustics alone cannot tackle th e problem of locating repairs, since any prosodic patterns found in repairs will be found in fluent speech, acoustic information can be quite effective when combined with other sources of information, particularly, pattern matching. as opposed to 380 ms (s.d.
H92-1086@@ It  is natural to expect phrase structure to be important in predicting prosodic phrasing. In particular, if we consider only the relation licenses by immediate constituency, and excise the clausal node (S), the remaining connected pieces of phrase structure--which I call chunks---are Selkirks C-phrases. The second boundary is a chunk boundary.
H92-1087@@ Spoken language understanding is a difficult problem, in part because of the many ambiguities inherent in natural language. P robab i l i ty  Score  Use of the decision tree for break synthesis suggests an alternative approach to the correlation score, which is to compute the probability of the sequence of automatically labeled break indices conditioned on the hypothesized parse. Ostendoff, M. & Veilleux, N. A Hierarchical Stochastic Model for Automatic Prediction of Prosodic Boundary Location.
H92-1088@@ This work addresses two related questions. P rosod ic  Character i s t i cs  o f  Other  Words  Although we do not yet have quantitative analysis pecific to non-target speech, we do notice two consistent prosodic patterns in the remaining parts of the utterances outside of the city names. Silverrnan, K. E. A., The Structure and Processing of Fundamental Frequency Contours.
H92-1089@@ The hypothesis that discourse structure is signalled by variation in intonational features uch as pitch range, timing, and amplitude has been examined instudies uch as \[1, 2, 3, 4, 5, 6, 7\]. e. \[However, at least one passenger on the one thousand mile flight, which lasted one hour and twenty five minutes, said the plane had shuddered and passengers were tense.\] f. "It was a normal anding, there was no emergency," Stanton said. Silverman \[3\] found that manipulation of pitch range alone, or in conjunction with pausal duration between utterances, enabled subjects to disambiguate r liably potentially ambiguous topic structures.
H93-1004@@ Data availability and evaluation procedures structure research possibilities: the type and amount of training data affects the performance of existing algorithms and limits the development of new algorithms; and evaluation procedures document progress, and force research choices in a world of limited resources. Black, E., et al, "A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars," Proc. An  alternative would  be to perform the evaluation at a common site.
H93-1005@@This effort was made possible by funding from the Brit ish Economic and Social Research Council. Extract from a Map Task Corpus transcript 4. CONCLUSION The HCRC Map Task corpus has been designed to allow investigation of a range of issues relevant o both psychological models of human language production and comprehension and to speech technology, especially as the focus on effort switches to more natural, unconstrained speech.
H93-1007@@ An important problem for natural anguage interfaces, as well as for other NL applications such as message processing systems, is coping with input which cannot be handled by the systems grammar. Grammatical relations include the familar deep-structure complement relations of subject, direct-object e c., as well as other various adjunct relations, such as PP-COMP in the rule below: (NP etc.) Such a structure can obviously also be represented as a set of n semantic objects and n-1 triples consisting of a semantic relation and head and argument semantic objects.
H93-1017@@ Many advanced speech recognition techniques cannot be developed or used in practical speech recognition systems because of their extreme computational requirements. It should be noted that, although we refer to lattices as word lattices, they could be used at other linguistic level, such as the phoneme, syllable, e.t.c. Schwartz, R., BBN Systems and Technologies, Cambridge
H93-1025@@ As everyone who has tried it knows, the hardest part of building a broad-coverage parser is not simply covering all the constructions of the language, but dealing with ambiguity. Dagan, I. and Itai, A. A slot-scoring rule is of the form: Slot + E ( 4-Body).
H93-1036@@ The PANGLOSS project is a three-site collaborative effort to build a large-scale knowledge-based machine translation system. Class-based n-gram models of natural language. Ultra: A multilingual machine translator.
H93-1037@@ The DARPA initiative in machine translation supports three very different avenues of research, including CANDIDEs fully automatic system \[1,2\], the interactive, knowledge-based system of the PANGLOSS group \[3-6\], and LINGSTAT, also an interactive system. This can regarded as a kind of syntactic interlingua. E. Hovy and S. Nirenburg, "Approximating an Interlingun in a Principled Way," Proceedings ofthe Speech and Natural Language Workshop, pp.
H93-1040@@This is perhaps due in part to the subjectivity inherent in judging the quality of any translation output (human or machine). All three methodologies demonstrate higher adequacy, fluency and quality scores for 208 I 0o0 0o0 0 eoo o700 4 *OeO0 t i O.S~ o~ "I 0. Original Passages were retrieved in French, Spanish, and Japanese, for translation i to English.
H93-1042@@ From standard components and a suite of generalizable customization techniques, we have developed an English to Swedish speech translation system in the air travel planning (ATIS) domain. Our experience sug4The S-CLE and the adaptation process i described in detail in \[lo\]. Black, E., et al, "A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars," Proc.
H93-1047@@ There has been a great deal of interest of late in the automatic induction of natural anguage grammar. % was obtained in bracketing a test set. Currently, each transformation i the learned list is only applied once in each appropriate environment.
H93-1049@@Word association ratios are a promising tool for lexicography, but there seem to be at least two limitations to the method: 1) much data with low frequency words or word pairs cannot be used and 2) generalization of word usage still depends totally on lexicographers. Common noun categories 1 a. EVALUATION Using 280,000 words of Japanese source text from the T IPSTER joint ventures domain, we tried several variations of the initial submatrices (word groups) from which the search in step three of the method starts: a) complete bipartite subgraphs, b) pre-classified noun groups and c) significantly frequent word pairs.
H93-1050@@ Semantic (selectional) constraints are necessary for the accurate analysis of natural language t xt. Step 3 (the triples extraction) includes a number of special cases :  (a) (h) (c) (d) (e) if a verb has a separable particle (eg, "out" in "carry out"), this is attached to the head (to create the head carry-out) and not treated as a separate relation. Accordingly, the acquisition of these constraints i an essential yet time-consuming part of porting a natural anguage system to a new domain.
H93-1051@@ The goal of this study is to systematical ly explore the effects of such variables as the number of  senses per word and the number of training examples per sense on corpus-based statistical sense resolution methods. Voorhees, E. M., Leacock C., and Towell, G., Learning context o disambiguate word senses. We thank Kenneth Church of AT&T Bell
H93-1052@@ The use of collocations to resolve lexical ambiguities i certainly not a new idea. Models which give greater weight o immediate context would seem more appropriate in these circumstances. Sproat, R., J. Hirschberg and D. Yarowsky "A Corpus-based Synthesizer," in Proceedings, International Conference on Spoken Language Processing, Banff, Alberta.
H93-1053@@ A linguistic scale is a set of words, of the same grammatical category, which can be ordered by their semantic strength or degree of informativeness \[1\]. Such algorithms are in general stronger than hierarchical methods \[9\]. After the similarities h/ave been computed for any pair of adjectives, we utilize the knowledge offered by the observed adjective-adjective pairs; we know that the adjectives which appear in any such pair cannot be part of the same group, so we set their similarity to 0, overriding the similarity produced by "r. 2..
H93-1054@@ The problem of syntactic ambiguity is a pervasive one. (food (handling and storage)) procedures c. ((mail fraud) and bribery) charges d. Clorets (gum and (breath mints)) e. (baby food) and (puppy chow) These bracketings comprise two groups, those that conjoin noun2 and noun3 (a-c) and those that conjoin noun2 and noun4 (d-e). Conversely, we also hope to have shown the utility of knowledge-based semantic lasses in arriving at a statistical characterization f linguistic phenomena, s compared to purely distributional methods.
H93-1061@@ We wish to propose a new version of an old idea. Brill, E. A simple rule-based part of speech tagger. Search only for a specific sense.
H93-1064@@ Text-to-speech synthesis could profitably be used to automate or create many information services, if only it were of better quality. Firstly, using the same prosodic principles and features as above, it employs variation in pitch range, boundary tones, and pause durations to define the end of the spelling of one item from the start of the next (to avoid "Terrance C McKay Sr." from being spelled "T-E-R-R-A-N-C-E-C, M-C-K-A Why Senior"), and it breaks long strings of letters into groups, so that "Silverman" is spelled "S-I-L, V-E-R, M-A-N". and Fallside, E "Synthesis by rule of prosodic features inword concatenation synthesis", Int.
H93-1066@@ Disfluencies inspontaneous speech pose serious problems for spoken language Systems. B. and Beckman, M. E. Japanese Tone Structure. Lickley, R. J. and Bard, E. G. Processing disfluent speech: Recognising disfluency before lexical access.
H93-1070@@ Other than experimental results, the first part of this paper contains little new material. This is the class of tf*idf (term frequency times inverse document frequency) weights \[1, 6, 7\], that assigns weight wik to term Tk in document / ) i  in proportion to the frequency of occurrence of the term in D~, and in inverse proportion to the number of documents to which the term is assigned. All experiments were done with the SMART information retrieval system, most using the TREC/T IPSTER collections of documents, queries, and relevance judgements.
H93-1074@@ Multi-modal systems allow users to both tailor their input style to the task at hand and to use input strategies that combine several modes in a single transaction. The difference can actually be attributed to the additional incurred costs of non-real-time r cognition and error correction. Ins t rumentat ion  All applications were instrumented to generate a stream of time-stamped events corresponding to user and system actions.
H93-1076@@ Research on application of spoken language processing to document creation and information retrieval has focused on the use of speech as an interface to systems that operate primarily on text-based material. Wdcox, L., Smith, L and Bush, M. "Wordspotling for voice editing and indexing". This approach enables interactive t xt-image editing and reproduction, i dependent of font or writing system.
H94-1011@@As in tests conducted last year, the large-vocabulary continuous speech recognition technology tests made use of Wall Street Journal-based Continuous Speech Recognition (WSJ-CSR) corpus material which was collected at SRI International (SRI) under contract to the Linguistic Data Consortium (LDC). With unsupervised channel compensation e abled, the CMU system achieved an error rate of 15. % for Class A, and 6.
H94-1014@@ The overall performance of a large vocabulary continuous speech recognizer is greatly impacted by the constraints imposed by a language model, or the effective constraints of a stochastic language model that provides the a priori probability estimates of the word sequence P (wz , . L. R. Bah1, E E Brown, P. V. deSouza nd R. L. Mercer, "A Tree-Based Statistical Language Model for Natural Language Speech Recognition," IEEE Trans. H.Witten and T. C. Bell, Whe Zero Frequency Estimation of Probabilities of Novel Events in Adaptive Text Compression," IEEE Trans.lnformation Theory, VoL 1T-37, No.
H94-1016@@ Speech recognition accuracy is affected as much by the language model as by the acoustic model. The sample of only 8,000 sentences i clearly not sufficient o find all the new words that people might use. PIaceway, P., R. Schwartz, P. Fung, L. Nguyen, "The Estimation of Powerful Language Models from Small and Large Corpora", Proc.
H94-1018@@But evaluation practice is still uneven, and sound methodologies need to be developed, both for laboratory experiments and working system investigations. Eva luat ion  gauges  Apart from the distinctions between kinds of evaluation just mentioned, it is helpful to categorise performance evaluation according to whether it is concerned with effectiveness, e~iciency, or acceptability (to humans). Assessment  s t ra tegy  It is evident that serious evaluation requires a wellunderstood ecomposition of the whole, in terms of the evaluation aims, or remit, and design, and the precise 104 definition of the evaluation subject.
H94-1019@@ The TextEval  project aims to explore and develop a new approach to the automatic  eva luat ion  of computer -generated  texts, based on the use of  standard sets.We believe that  fast, accurate and automatic evaluation methods are vital to the development of any large piece of natural  anguage software, and note that  cur rent  methods,  which involve extensive intervention by human experts, are too cost ly to be a rout ine part  of the development cycle. "Accurate Methods for the Statistics of Surprise and Coincidence" Computational Linguistics 19(3) pp 61-74, March, 4 Hatch, E. and Lazaraton, A. The difference from the word-based metric is simply that we have used the tagger s  lexicon to col lapse across equivalence classes of similar words.
H94-1020@@All of this material has been hand corrected after processing by automatic tools. Report o the U.S Office of Education on Cooperative Research Project No. E-OOZ Brown University, Providence.
H94-1022@@The benefits hoped to be derived from such a shift include greater focus on underlying technology issues, rather than application issues, and lowering the overhead required to participate in evaluations in terms of developing application systems. Every blue block is tall and the complements are distinguished by the position they fill in the argument list: since t:he difference between these is structurally determined and unambiguous, but we will not give different representations to the two scopings of Some girl likes every boy. For example, for % flight on an airline", there is really only one salient relation between flights and airlines, so f l ight_a i r l ine  might as well be used to name that relation.
H94-1024@@Evaluation of Machine Translation (MT) has proven to be a particularly difficult challenge over the course of its history. SYSTRAN, a commercial system, produced FE. One method employed the same criteria used in the U.S. government to determine the competence of human translators.
H94-1025@@  This paper describes a semi-automatic method for associating a Japanese lexicon with a semantic oncept axonomy using a Japanese-English bilingual dictionary as a "bridge", in order to support semantic processing in a knowledge-based machine translation (MT) system. Aproximating an interlingua in a principled way. A bilingual concept JW i -k  is assigned to the kth correspondence pair.
H94-1026@@ A number of proposals have come up in recent years for hybridization of MT. Without dynamic programming, this would have a combinatorial time complexity. "The Pangloss Mark I MAT System."
H94-1027@@ Hieroglyphics remained undeciphered forcenturies until the discovery of the Rosetta Stone in the beginning of the 19th century in Rosetta, Egypt. Leed, R. L. and Nakhimovsky, A. D., "Lexical Functions and Language Learning ", Slavic and East European Journal, Vol. Smadja, E, Retrieving Collocational Knowledge from Textual Corpora.
H94-1034@@ Interactive spoken dialog provides many new challenges for spoken language systems. Note that he context for category Ci+l is both C~ and R~. % and a precision of 93.
H94-1035@@  Previous work in the area of intonation generation i cludes an early study by Young and Fallside (\[26\]), and studies by Terken (\[24\]), Houghton, Isard and Pearson (cf. Liberman, Mark and A.L. The  morb id  nature  of the examples,  for which we apologize, is due entirely to the special nature  of the  t rauma domain .
H94-1039@@ Understanding spontaneous speech presents everal problems that are not found either in recognizing read speech or in parsing written text. The views and conclusions contained in this document are those off the authors and should not be interpreted asrepresenting the official policies, either expressed or implied, of the U.S. Government. There is a heuristic procedure for resolving ties.
H94-1042@@ Language modeling for speech recognition has focused on robustness, using statistical techniques such as n-grams, whereas work in language understanding and information extraction has relied more on rule based techniques to leverage linguistic and domain information. (rn, Pn)) where r i are the rules and P i  are the positions within the rules. Each time a command is uttered, a new referent is created.
H94-1046@@ It is generally recognized that systems for automatic seine identification should be evaluated against a null hypothesis. First, compile a semantic o-occurrence matrix. However, Gale" Church, and Yarowsky \[I\] do not make clear how they determined what the most frequently occurring senses are.
H94-1047@@ Assigning sense tags to the words in a text can be viewed as a classification problem. A Method for Disambiguating Word Senses in a Large Corpus. AT~T Bell Laboratories Statistical Research Report No.
H94-1050@@For instance, in speech recognition some of the intermediate stages might correspond to sequences of units like phones or syllables. K.-E Lee, "Context dependentphonetic hidden Markov models for continuous speech recognition," IEEE Trans. ; we also wish to thank United Informaties, Inc., R.O.C.
H94-1051@@ Designing and refining a natural anguage grammar is a difficult and time-intensive task, often consuming months or even years of skilled effort. hard constraint, r must match | exactly. Upon completion, a list of shift and reduce operations is returned.
H94-1053@@ Hidden understanding models are an innovative application of statistical mechanisms that, given a string of words, determines the most likely meaning for the string. P. Placeway, R. Schwartz, P. Fung, L. Nguyen, "The Estimation of Powerful Language Models from Small and Large Corpora," IEEE ICASSP, 1I:33-36 A. J. Viterbi, "Error Bounds for Convolutional Codes and an Asympotically Optimum Decoding Algorithm," IEEE Transactions on Information Theory IT-13(2):260 if t in Lexieal Realization ModelJ / Thus far, we have discussed the need to search among all meanings for one with a maximal probability.
H94-1062@@ Hidden Markov Models (HMMs) have proved to be an effective basis for modelling time-varying sequences of speech spectra. T ree-based  C lus ter  ing  In order to compare top-down tree clustering with the bottom-up agglomerative approach used in previous ystems, an RM system was constructed using each of the two methods. through to singleton sets such as {l}, {m}, etc.
H94-1074@@ In applying speech recognition techniques to retrieve information from large unrestricted text corpora, several issues immediately arise. We use a stop list 1 containing approximately 100 words. For example, when a speaker intended the word "president" the recognizer output "P R EH S EH D EH N T" would incorrectly match the word "precedent".
I05-1007@@In the recent development of full parsing technology, semantic knowledge is sel-dom used, though it is known to be useful for resolving syntactic ambiguities.The reasons for this may be twofold. Resolving Ambiguities of Predicate-objectand Modifier-noun Structures for Chinese V-N Patterns. For a better recognition, one can define accurate regularexpressions.
I05-1010@@Knowledge about how we recognize objects is of great practical importance formany NLP tasks. ), a QA test (Sect. The effect of t(C, A), on theother hand, seems to have weakened greatly.
I05-1015@@ Text chunking is a process to identify non-recursive cores of various phrase types without conducting deep parsing of text [3]. is  a weight associated with feature kf . L. A. Ramashaw and M. P. Marcus: Text chunking using transformation-based learning.
I05-1018@@Natural language processing (NLP) is being demanded in various fields, suchas biomedical research, patent application, and WWW, because an unmanage-able amount of information is being published in unstructured data, ie, naturallanguage texts. : A maximum entropy approach tonatural language processing. Johnson, M., Riezler, S.: Exploiting auxiliary distributions in stochastic unification-based grammars.
I05-1034@@  The relation extraction task identifies various semantic relations such as location, affiliation, revival and so on between entities from text. Culotta, A. and Sorensen, J. A similarity function returns a normalized, symmetric similarity score in the range [0, 1].
I05-1040@@ Machine transliteration is an automatic method to generate characters or words in one alphabetical system for the corresponding characters in another alphabetical system. With the simi-lar manner, t based on MBL outputs the target grapheme o. 452 J.-H. Oh and K.-S. Choi their Japanese counterparts).
I05-1053@@ A Named Entity (NE) is essentially a proper noun phrase. In essence, our proposed model consists of two sub-models: a lexical mapping model (LMM), characterized by 1( | )xx x np  , that models the monotonic genera-tive process of phrase pairs; and a permutation model (PM), characterized by 1( | )xx x nkk kp e e  % % , that models the permutation process for reordering of the target language. Op en tes t NIST 7.
I05-1082@@A noun compound (NC) is an N made up of two or more nouns, such as golf club orpaper submission; we will refer to the rightmost noun as the head noun and the re-mainder of nouns in the NC as modifiers. While tagging the NCs, we got952 S.N. 7THRESHOLD(a) error rate with similarity 0.
I05-2004@@Algorithms for extractive summarization are typi-cally based on techniques for sentence extraction,and attempt to identify the set of sentences that aremost important for the overall understanding of agiven document. Sentence extractionas a classification task. InProceedings of the HLT/NAACL Workshop on Auto-matic Summarization, Edmonton, Canada, May.R.
I05-2011@@ Sophisticated language processing in recent years has made possible increasingly complex challenges for text analysis. Improved Backing-off for n-gram Language Modeling. We combined them to obtain a more reliable list.
I05-2012@@Technical terms can be clas-sified into single-word terms, and complex term                                                                                                                     * The first authors current affiliation is with Computational Linguistics Group, National Institute of Information and Communications Technology, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan (or multi-word term) according to the number of their constituents. For the give source text, S, it finds the most probable alignment set, A, and target text, T. t(fj|ei) is estimated by EM algorithm. Unlike IBM Model, our alignment model can deal with n:1 alignment.
I05-2013@@ A lot of research is dedicated to anaphora resolution since it is a crucial issue, for example, for Information Retrieval or Text Summarization. More precisely, I have worked on a corpus of 3. 4.  il wrongly tagged as [IMP] instead of [ANA]: 0,3\% Very few errors: 33.
I05-2018@@For example: an apple, two apples, three apples. This means they have only a singular form, such as water, rice, wine. To detect which class a compound noun is, we proposed some simple, viable n-gram models, such as freq(N) (the fre-quency of the singular form of the noun) whose parameters values (web hits of literal queries) can be obtained with the help of WWW search engine Google.
I05-2020@@This paper studies issues on compiling a bilinguallexicon for technical terms. 4Japanese entries are supposed to be segmented into asequence of words by the morphological analyzer JUMAN(http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html)5In our rough estimation, the upper bound of this rateis about 80%. Translation pairs in the bilingual constituentslexicons whose frequencies in P2are high.. in the existing bilin-gual lexicon Eijiro, where the term s consistsof exactly one constituent.
I05-2027@@ In this paper we present an approach to headline generation for a single document. C-Y Lin and E. Hovy. An Artificial Intelligence Approach edited by R.S.
I05-2030@@The explosive spread of communication on theWeb has attracted increasing interest in technolo-gies for automatically mining large numbers ofmessage boards and blog pages for opinions andrecommendations.Previous approaches to the task of mining alarge-scale document collection for opinions canbe classified into two groups: the document clas-sification approach and the information extrac-tion approach. In some cases, the attributemay be missing from a sentence. For example1,dezain-waa hen-daga watashi-wa -ga suki-davdesigna weird I [it] likev(The design is weird, but I like it.
I05-2038@@ Research and development for information ex-traction from biomedical literature (bio-textmining) has been rapidly advancing due to demands caused by information overload in the genome-related field. A noun phrase is gen-erally left unstructured. Only 15 cases were true ambiguities that needed knowledge of biology to solve, in which 5 in-volved coordination (eg, the scope of various in various T cell lines and peripheral blood cells) embedded in the sen-tence, since mathematical formulae were outside the scope of the original PTB scheme.
I05-2044@@ Dependency parsing describes syntactic struc-ture of a sentence in terms of links between in-dividual words rather than constituency trees. S:Shift LA:Left-arc RA:Right-arc R:reduce Fig. declare to teachers a  piece      exciting   of  news.
I05-2045@@A range of extraction mod-els have been used, including both symbolic rulesand statistical rules such as HMMs or Kernels.These methods have been particularly success-ful in some specific domains. (E) is the identified relation labels from our estimated clusters. Using syntactic dependency as a local contextto resolve word sense ambiguity.
I05-3002@@Currently, the most popular method for Chi-nese input is phonetic and pinyin based, because Chinese people are taught to write the corre-sponding phonetic and pinyin syllables of each Chinese character and word in primary school. Golden Mandarin (I) A Real-Time Mandarin Speech Dictation Machine for Chinese Language with Very Large Vocabu-lary, IEEE Transaction on Speech and Audio Processing, 1(2). To-ward a Unified Approach to Statistical Lan-guage Modeling for Chinese, ACM Transactions on Asian Language Information Processing, 1(1):3-33.Gu, H.Y., C.Y.
I05-3003@@However, implementation of Chinese syntactic structure analyzers is still lim-ited, since the structure of the Chinese language is quite different from other languages. A Grammar of Spoken Chinese. S and I are stacks, S keeps the words being in consideration, and I keeps the words to be processed.
I05-3005@@ Part-of-speech tagging is an important enabling task for natural language processing, and state-of-the-art taggers perform quite well, when training and test data are drawn from the same corpus. so a comparison is not informative. Most English suf-fixes are derivational and inflectional suffixes like   -able, -s and -ed.
I05-3007@@ The accessibility to large scale corpora, at one billion words or above, has become both a blessing and a challenge for NLP research. A Festschrift in Honour of B.T.S. `No, said Scarlet, `but then I dont notice much.
I05-3008@@However, thecaveat at this point should be carefully formu-lated, due to the fact that there are no unequiv-ocal opinions concerning with some basic theo-retical settings in Chinese morphology. Second,the ambiguity of characters is often shunned by57SC VV compounds Concept types of modifier componentEe 37  ; e-mail)is a kind of mail. Some testing datais not semantically transparent due totheir metaphorical uses, For instance, I (Aj) is assigned to the  The quality and coverage of CILIN and char-acter ontology:Since our SC systems test and training dataare gleaned from CILIN and the characterCompound types Our model Current bestmodelV-V 42.00% 39.
I05-3012@@ WSD tries to resolve lexical ambiguity which refers to the fact that a word may have multiple meanings such as the word walk in  Walk or Bike to school and BBC Education Walk Through Time, or the Chinese word   (He is also partly right). Towell, G. and Voorhees, E. M.: Disambiguating highly ambiguous words. Q. Lu, Y. Li, and R. F. Xu, Improving Xtract for Chi-nese Collocation Extraction.
I05-3013@@ The rapid global proliferation of Internet applica-tions has been showing no deceleration since the new millennium. hold homophony in a Chinese dialect. such as digit se-quence; capital word or letter sequence; mixed case word; abbreviation; Roman numeral; URL and e-mail address.
I05-3017@@Chinese is written without inter-word spaces, sofinding word-boundaries is an essential first stepin many natural language processing applica-tions including monoand cross-lingual infor-mation retrieval and text-to-speech systems. "!5 France Telecom R&D Beijing Heng LI ZH "! Open (italics indicate performance below baseline)128(1 p)/n) where n is the number of words.
I05-3019@@The Yahoo team participated in all four closed tasks and all four open tasks at the second inter-national Chinese word segmentation bakeoff. The increase of F-score from 0. Our system processes a sentence independ-ently.
I05-3020@@ In the research fields of Chinese natural lan-guage processing (NLP), a high-performance Chinese word segmentor (CWS) is a useful pre-processing stage to produce an intermediate re-sult for later processes, such as search engines, text mining and speech recognition, etc. The differences of F-measure and ROOVbetween near-by steps of our CWS Step F F(d) ROOV ROOV(d)1 0. A compression-based algo-rithm for Chinese word segmentation.
I05-3022@@The development of the Chinese word segmen-tation system presented in this bakeoff began in Feb. this year, and will last for one year with the support of the ILAB Beijing initial project within France Telecom R&D. Chinese word segmentation: a pragmatic approach. N-gram language model  In our system, Chinese words can be categorized into one of the following types: lexicon words, morphological words, factoids, name entities.
I05-3023@@We participated in the closed and open tests forall the four corpora, referred to as, cityu, msr andpku, respectively. means collapsing both Englishtexts and Arabic numbers.close test English E & Nas 95. Similar to the PAUM, SVMis a maximal margin algorithm.
I05-3024@@We participated in the closed track of the second Chinese word segmentation bakeoff for the training corpus of HK (City University of Hong Kong, PK (Beijing University), and MS (Microsoft Research). C. Rim, H. S. Lim, Automatic Word Spacing Using Hidden Markov Model for Refining Korean Text Corpora, Proc. is a non-space tag and 1 158segmentation with the higheset probability.
I05-3027@@Rather, systems tended to do well on corpora largely drawn from a set of similar Mandarin varieties to the one they were originally developed for. Our final system achieved a F-score of 0. Y is the label sequence for the sentence, X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled.A CRF allows us to utilize a large number of n-gram features and different state sequence 168based features and also provides an intuitive framework for the use of morphological features.
I05-3034@@The model treatsknowledge sources as feature functions, and al-lows the system to be extended easily by adding new feature functions. W* is the candidate set.Giving the direct maximuneglecting its renormalization, we can obtain the following decision rule: #m* 1arg m ax h ,#+ mw w mw w sO hi (W#sequence. Every word will belong to a class.
I05-3035@@ Word segmentation is the first step in Chinese NLP, but segmentation of the Chinese text into words is a nontrivial task. (a) Cn (n 2,1,0,1,2 ) (b) CnCn+1( n 2,1,0,1) (c) C1C1(d) Pu(C0 ) In addition to the character features, we came up with another type word context feature which was found very useful in our experiments. In the open tracks, we employed a dictionary of 134,458 entries.
I05-4002@@Parsing is one of the important processes for nat-ural language processing and, in general, a large-scale CFG is used to parse a wide variety of sen-tences. 9 bunsetsu in a sentence. )In the former case, the underlined part issegmented into 5 bunsetsu (3 gatsu, 31nichi, gogo, 9 ji, and 43 fun goro,) inthe Kyoto corpus, while it is not segmentedin our corpus.
I05-5001@@ Paraphrase detectionthe ability to determine whether or not two formally distinct strings are similar in meaningis increasingly recognized as crucial to future applications in multiple fields including Information Retrieval, Question Answering, and Summarization. WordNet Coverage in Word Associa-tion Output 4External_Matches_2_LED (i.e,, the ratio of total lexical matches to Levenshtein edit distance.) 3 Constructing a Classifier 3.
I05-5002@@ The Microsoft Research Paraphrase Corpus (MSRP), available for download at http://research.microsoft.com/research/nlp/msr_paraphrase.htm, consists of 5801 pairs of sen-tences, each accompanied by a binary judgment indicating whether human raters considered the pair of sentences to be similar enough in mean-ing to be considered close paraphrases. % that of the longer; andThe two sentences had a bag-of-words lexical distance of e  8 edits.This enabled us extract a set of 49,375 initial candidate sentence pairs whose author was known,  The purpose of these heuristics was two-fold: 1) to narrow the search space for sub-sequent application of the classifier algorithm and human evaluation, and 2) to ensure at least some diversity among the sentences. 104 Constructing a Classifier 4.
I05-5003@@Automatic machine translation evaluation is ameans of scoring the output from a machine trans-lation system with respect to a small corpus ofreference translations. A perfor-mance improvement of approximately 0. The value of thefeature vector ~f countt (w)|si|where countt (w) is the number of times wordw occurs in W with tag t.The feature vector defined above characterizesthe nature of the words in the sentences that donot match.
I05-5004@@Paraphrases are alternative ways of conveying thesame content. Given (a) a source sentence and(b) an automatically generated candidate para-phrase, human annotators are asked to (c) judgethe appropriateness of it and, if it is inappropri-ate, they are also asked to (d) classify the un-derlying errors into a predefined taxonomy, andmake (e) appropriate revisions (if possible) and(f) format-free comments. soyokaze-ga sentakumono-o yurasu.breeze-NOM laundry-ACC to sway-PRESThe breeze makes the laundry sways.
I05-5005@@Patient records are typically large collections ofdocuments that reflect the medical history ofa patient over a period of time. Although each documentin this collection will have a specified purpose,there tends to be a high degree of redundancybetween documents, but the sheer volume ofinformation makes access extremely difficult.The work presented in this paper is part of theClinical E-Science Framework project (CLEF),which aims at providing tools to facilitate easyaccess to a patients medical history. This feature is a directresult of the report requirements.
I05-5010@@Our interest in thispaper lies instead with variations at the level oftext structuring  The following is a simpleexample, containing one nucleus-satellite relation(REASON) and one multinuclear relation (CON-JUNCTION1):reasonNUCLEUS: recommend(doctors, elixir)SATELLITE: conjunction1: quick-results(elixir)2: few-side-effects(elixir)Ignoring variations in the wording of proposi-tions, ICONOCLAST generates over 20 texts re-alising this input (or many more if a larger reper-toire of discourse connectives is allowed). A costvalue is computed every time a proposition is re-alised by a clause for which the grammar allowspassivization. Technical Report RR-83-115, Information Sciences Institute.D.
I05-5011@@ One of the difficulties in Natural Language Processing is the fact that there are many ways to express the same thing or event. Extracting paraphrases from a parallel corpus. This paper does not necessarily reflect the position of the U.S. Gov-ernment.
I05-6001@@They are successfully exploited forthe induction of treebank grammars, train-ing of stochastic parsers, and for evaluat-ing and benchmarking competitive parsingand grammar models. a, University ofCambridge, University of Cambridge, UK.R. E.g.,pron type( ,expl) identifies an expletive pronoun.
I05-6008@@We aim at finding methods that facilitate the de-scription of the linguistic behavior of multiwordexpressions. Reflexives may instantiate either openargument slots or an NP within complement PPs;and (e), among modification, we explore pre-nominal adjectives, past participles, gerunds andother intervening material.In addition, some expressions allow relativeclauses and PP post-nominal modifiers. So far, an LVCis represented as a P N V triple, but we need toknow other syntactic requirements of the predic-ate.
I08-1004@@ It is well known that syntactic structured informa-tion plays a critical role in many critical NLP ap-plications, such as parsing, semantic role labeling, semantic relation extraction and co-reference reso-lution. Cambridge, MA Culotta A. and Sorensen J. To deal with the cases that an anaphor and an antece-dent candidate do not occur in the same sentence, we construct a pseudo parse tree for an entire text by attaching the parse trees of all its sentences to an upper S Given the parse tree of a text, the problem is how to choose a proper tree span to well cover syn-tactic structured information in the tree kernel computation.
I08-1005@@ Relation extraction is to detect and classify various predefined semantic relations between two entities from text and can be very useful in many NLP ap-plications such as question answering, eg to an-swer the query Who is the president of the United States , and information retrieval, eg to expand the query George W. Bush with the president of the United States via his relationship with the United States. Culotta A. and Sorensen J. LY corresponds to the l  labeled instances; UY : the bottom u  rows of 0Y .
I08-1007@@Spelling variations, such as center and centre,which have different spellings but identical mean-ings, are problematic for many NLP applicationsincluding information extraction (IE), question an-swering (QA), and machine transliteration (MT). tkfrequency of sk,where |K| is the number of characters in a termt, tk is the k-th character of a term t, sk is thek-th character sequence of a term s, frequencyof sk  is the occurrences of the alignments,frequency of sk We aligned in Japanese-to-English di-rection, and got 1 : m alignments (one Japanesecharacter : m alphabetical characters) to cal-culate P (tk|sk). A hybrid back-transliteration system for Japanese.
I08-1013@@Current research in the automatic compilation ofbilingual dictionaries from corpora uses of compara-ble corpora. @ medic-inal prescription), surveillance glycemique ( *"A!BC glycemic monitoring), bre alimentaire ( DEF!G dietary bre), produit laitier ( H:I1J dairyproduct), fonction renale ( K!L MN kidney func-tion).The problem of fertility could only be solvedthanks to a contextual analysis in contrast to theforeign name problem that could be solved by anheuristic. A Word-to-Word Model ofTranslational Equivalence.
I08-1016@@Two of the key components of effective summariza-tions are the ability to identify important points inthe text and to adequately reword the original textin order to convey these points. A compositional context sensitive multi-document summarizer: exploring the factors that influ-ence summarization. The summarizer versionthat uses NP-rewrite has overall better content selec-tion performance than the purely extractive system.The original pyramid score increased from 0.
I08-1020@@Finding information about people on huge text col-lections or on-line repositories on the Web is a com-mon activity. In Proceedings of the 40th AnniversaryMeeting of the Association for Computational Linguis-tics (ACL02).Douglass R. Cutting, Jan O. Pedersen, David Karger, andJohn W. Tukey. S. Mann and D. Yarowsky.
I08-1028@@The language understanding (LU) of spoken dia-logue systems in the early phases of their devel-opment should be trained with a small amount ofdata in their construction. in a grammar description in sentencetags. The w is the thresh-old for determining whether word W is accepted ornot.
I08-1029@@Prosody plays a crucial role in language understand-ing. We conclude with a brief discussionof future work. Journal ofthe Acoustical Society of America, 111.C.X.
I08-1033@@ Word alignment is an important preprocessing task for statistical machine translation. Richard O. Duda, Peter E. Hart, David G. Stork. Where i denotes a break point in f,  denotes first i characters of f, and  denotes last n-i characters of f. If the information gain of a breaking point is positive, the result substrings are considered to be better, ie more pure than original word.
I08-1038@@ With the rapid expansion of network application, more and more customer reviews are available on-line, which are beneficial for product merchants to track the viewpoint of old customers and to assist potential customers to purchase products. After that, a mapping function is designed to identify implicit features. /a  /w The seed opinion words employed in the itera-tive learning are:  Empirically, Thresholdfeature and Thresholdopword in Algorithm 1 is set to 0. , Mini-mum-Offset is set to 4.
I08-1047@@ Extraction of lexical knowledge from a large col-lection of text data with minimal supervision has become an active area of research in recent years. ], the point of dividing pmi(i,p) by maxpmi in Espresso is to normalize the reliability to [0, 1]. Analysis of a Very Large AltaVista Query Log.
I08-1048@@ Supervised learning models set their parameters using given labeled training data, and generally outperform unsupervised learning methods when trained on equal amount of training data. Procedure: Active Learning Process Input: initial small training set L, and pool of unlabeled data set U Use L to train the initial classifier C (ie a classi-fier for uncertainty sampling or a set of classifiers for committee-based sampling) Repeat  Use the current classifier C  to label all unlabeled examples in U  Based on active learning rules R such as un-certainty sampling or committee-based sam-pling, present m top-ranked unlabeled ex-amples to oracle H for labeling  Use L to retrain the current classifier C Until the predefined stopping criterion SC is met. (8) Assuming N unlabeled examples in the pool U, the total time is O(N) for automatically determin-ing whether the proposed stopping criterion SCMES is satisfied in the active learning.
I08-1059@@ English is today the de facto lingua franca for commerce around the globe. Amalgam: A machine-learned generation module. Golding, Andrew R. and Dan Roth.
I08-1064@@In recent years, supervised machine learning has be-come the standard approach to obtain robust andwide-coverage NLP tools. %and a recall of 52. In this case, the modelpredicts e to project to wt.
I08-1067@@Techniques for leveraging syntactic and morpholog-ical information for statistical machine translation(SMT) are receiving a fair amount of attention nowa-days. Phrase-Based SMT: the BaselineGiven a source sentence f , SMT chooses as its trans-lation e, which is the sentence with the highest prob-ability:e decision rule, this is writtenas:e The decodingprocess works by segmenting the input sentence finto a sequence of I phrases fI1. This reduces, andoften eliminates, the distortion load : Corresponding constituent in Hindi,where X is S, O, or VXm: modifier of XEssentially, the SVO order of English is changedto SOV order, and post-modifiers are converted topre-modifiers.
I08-1069@@Until fairly recently, most NLP research has focusedon the ten or so majority languages of the world, thecanonical high density languages. For instance, if afeature f5 in F2 checks whether the current linecontains a citation, f+15 checks whether the nextline contains a citation.After the lines in a document are tagged by thelearner, we identify IGT instances by finding all thespans in the document that match the B [I | BL]E pattern; that is, the span starts with a B, endswith an E, and has zero or more I or BL in between. Coreference or not: A twinmodel for coreference resolution.
I08-2077@@Named Entity Recognition (NER) is an impor-tant tool in almost all Natural Language Process-ing (NLP) application areas. fk(st1, st, o, t)),589where, fk(st1, st, o, t) is a feature function whoseweight k, is to be learned via training. Named Entities: Recognition, Classificationand Use, Special Issue of Lingvisticae InvestigationesJournal, 30(1):95114.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
I08-2080@@Named entity recognition (NER) is the task of iden-tifying and classifying phrases into certain classesof named entities (NEs), such as names of persons,organizations and locations.Japanese texts, which we focus on, are writtenwithout using blank spaces. In thispaper, we use the IOBES model, in which S 4 + 1) NE tags.For example, NE tags are assigned as following:(1) Kotoshi 4 gatsu Roma ni itta.this year April Rome to wentB-DATE I-DATE E-DATE S-LOCATION O O( went to Rome on April this year. Named entityrecognition: A maximum entropy approach using global in-formation.
I08-2082@@Proofing technology for native speakers of Englishhas been a focus of work for decades, and sometools like spell checkers and grammar checkers havebecome standard features of document processingsoftware products. A smaller t3 can reduce recall,but can increase GP. 20ID Pre-editing version Post-editing version1 Which team can take the champion Which team will win the championship2 I attend to Pyoung Taek University.
I08-2084@@ Terminology Extraction (TE) is a subtask of in-formation extraction. Our idea here is to combine the frequency with T-Score, a unithood feature. References C. Manning and H. Schuetze.
I08-2089@@Often more data is better data, and so it should comeas no surprise that recently statistical machine trans-lation (SMT) systems have been improved by theuse of large language models (LM). Distributed language modeling for n-best listre-ranking. A slightly bettervalue of 28.
I08-2097@@Dependency parsing has been utilized in a varietyof natural language processing (NLP) applications,such as paraphrase acquisition, relation extractionand machine translation. A maximum entropy model forpart-of-speech tagging. In this paper, we employed theMSTParser, which can process 3.  sentences/s on aXEON 3.0GHz machine in spite of the time com-plexity of O(n3).
I08-2105@@It has long been believed that being able to detectthe correct sense of a word in a given context  per-forming word sense disambiguation (WSD)  Eachnode in this graph represents a word sense, andweighted edges will connect every pair of senses(corresponding to different words). d) + dbIn(a)wbacOut(b) wbcWP (b)where:a, b, c are vertices in the graph;WP (a) is the weighted PageRank score of node a;d is the probability that there will be a jump from a given vertexto another in the graph. The anatomy of a large-scale hypertextualweb search engine.
I08-2112@@To correctly extract answers, modern question an-swering systems depend on matching words be-tween questions and retrieved passages containinganswers. Hyponym InductionWe review several approaches to learning is-a rela-tions. They reported precisionat 50 instances between 0.
I08-2126@@In this paper, hy-ponymy relation is defined as a relation between a hy-pernym and a hyponym when the hyponym is a (kindof) hypernym.1. (hyponym is a typical hypernym)hyponym wa. Using ency-clopedic knowledge for named entity disambiguation.In Proceedings of the 11th Conference of the EACL,pages 916.O.
I08-2127@@Human languages are rich enough to be able toexpress the same meaning through different dic-tion; we may produce different sentences to conveythe same information by choosing alternative wordsor syntactic structures. indicates whether the innerand outer elements of parenthetical expressions areinterchangeable.The first group acronym (I) reduces a full form toa shorter form by removing letters. SaRAD: A simple and robust abbre-viation dictionary.
I08-3008@@ Natural language parsing is one of the key areas of natural language processing, and its output is used in numerous end-user applications, eg machine translation or question answering. A Statistical Parser for Czech. s tagger is quite similar to the Danish Parole tags, but not identical.
I08-4003@@ Transliteration is a process of rewriting a word from one language into another by preserving its pronunciation in its original language, also known as translation-by-sound. Suppose that EW and CW form an E-C transliteration pair. Bayesian Learning of N-gram Statistical Language Modeling, In Proc.
I08-4014@@Performances of some systems varied significantly on different corpus and different standards, this kind of systems can not satisfy demands in practical applications. The boundary tags are B I and S, which B is the tag of the first character of a word which contains more than two characters, I is the other non-initial characters in a word, S is for the single character word. WS system keeps at a relatively steady performance.
I08-8003@@Dictionaries and corpora are only able to cover a certain proportion of language. Secondly, the annotators judged the quality of the transliteration and backt rans l i t e ra t ion sys tems to be approximately the same. Example 4 shows a contraction.
J03-2003@@When language is written, it appears as a collection of words set out on one or more(actual or virtual) pages. In this article,we will focus on the first of these methods (i.e, starting from a hierarchical rhetori-cal input). But we do re-quire a minimal standard of correctness.
L08-1023@@ Compared with traditional media such as online news and enterprise websites, weblogs have several unique characteristics, eg, containing abundant life experiences and public opinions toward different topics, highly sensitive to the events occurring in the real world, and associated with the personal information of bloggers. The weblog is parsed into a set of collocations. In: Internet Research 6.0: Internet Generations.
L08-1033@@ Compared with traditional media such as online news and enterprise websites, weblogs have several unique characteristics, eg, containing abundant life experiences and public opinions toward different topics, highly sensitive to the events occurring in the real world, and associated with the personal information of bloggers. The weblog is parsed into a set of collocations. When a new collocation is added, we compute the mutual information of the multiword collocations by the following formula, where n is the number of collocations in the network up to now.
L08-1053@@ Compared with traditional media such as online news and enterprise websites, weblogs have several unique characteristics, eg, containing abundant life experiences and public opinions toward different topics, highly sensitive to the events occurring in the real world, and associated with the personal information of bloggers. The weblog is parsed into a set of collocations. When a new collocation is added, we compute the mutual information of the multiword collocations by the following formula, where n is the number of collocations in the network up to now.
L08-1093@@ Compared with traditional media such as online news and enterprise websites, weblogs have several unique characteristics, eg, containing abundant life experiences and public opinions toward different topics, highly sensitive to the events occurring in the real world, and associated with the personal information of bloggers. When a new collocation is added, we compute the mutual information of the multiword collocations by the following formula, where n is the number of collocations in the network up to now. The weblog is parsed into a set of collocations.
L08-1403@@ Compared with traditional media such as online news and enterprise websites, weblogs have several unique characteristics, eg, containing abundant life experiences and public opinions toward different topics, highly sensitive to the events occurring in the real world, and associated with the personal information of bloggers. The weblog is parsed into a set of collocations. When a new collocation is added, we compute the mutual information of the multiword collocations by the following formula, where n is the number of collocations in the network up to now.
M92-1006@@In this adjunct test, we investigate the issues and effect of transforming the MUC-4 template design au-tomatically into an object-oriented design, with associated object-level matching conditions affecting th eoverall score .An object is simply a collection of slots that all refer to one item originating in the text . )* How the training of the system was doneWhat proportion of the training data was used (and how )Whether/Why/How the system improved over time, an dhow much of the training was automate d* What was successful and what wasnt, and what system moduleyou would most like to rewrit e* What portion of the system is reusable on a different applicatio n With a flat design, less-than-perfect templates must be examined t odetermine where the problems occurred .
M93-1011@@The GE-CMU TIPSTER/SHOGUN system is the result of a two-year research effort, part of the A1tPA -sponsored TIPSTER: data extraction program. SAID FRIDAY IT HAS SET UP A JOINT VENTURE113[IGNORE{41} : IN TAIWAN ] WITH A LOCAL CONCERN AND A JAPANESE TRADING HOUS ETO PRODUCE GOLF CLUBS TO BE SHIPPED TO JAPAN .JJV0002 Sentence 0 :[CNAME{24} : V	_Ek ` M ]	4A 73, 6* U IE L [MORPH{8} :	LZ ]L ` ~~t lie	1 [MORPH{4} : {eiJc ]	flJpa f x  v j[MORPH{5} :	EB L fc ]Where a company name is marked in pre-processing, this means that the name is "learned" rather tha nrecognized as a known name . Thesegoals seemed rather ambitious, but SHOGUN reached all of them .The following is a summary of SHOGUN s performance on all the official metrics .
M98-1009@@At the sentence level, the SIFT system (Statistics for Information From Text) employs a unifiedstatistical process to map from words to semantic structures. Weannotated full articles before discovering a more effective annotation strategy. A head constituent for the PER/NP, in this case a PER-R/NP.
M98-1017@@People, affairs, time, places and things are five basic entities in a document. We extract informationfrom World Wide Web documents, including proper nouns, E-mail addresses and home page URLs, and findthe relationship among these data. For example, f vs. vs. Kg,L vs. , and so on.A Chinese sentence is composed of a sequence of characters without any word boundary.
N01-1003@@Sentence planning is a set of inter-related but distincttasks, one of which is sentence scoping, ie the choiceof syntactic structure for elementary speech acts and thedecision of how to combine them into sentences. In Proceedings ofFirst North American ACL, Seattle, USA, May.Robert E. Schapire. More importantly, they were also not dis-tinguishable statistically from the current hand-craftedtemplate-based output of the AT&T Communicator sys-tem, which has been developed and fine-tuned over anextended period of time (whereas SPoT is based on judg-ments that took about three person-days to make).
N01-1006@@Much research in natural language processing hasgone into the development of rule-based machinelearning algorithms. Letr be such a rule. In Proceedings of the 38thAnnual Meeting of the ACL, pages 278285, HongKong, October.T.
N01-1007@@We present two methods for the unsupervisedlearning of the structure of personal names asfound in Wall Street Journal text. )Let ~l be a sequence of label assignments to thename ~n (a sequence of words). Riloff, E. and Shepherd, J.
N01-1009@@A fast programmer is typically a program-mer who programs quickly, a fast plane is typically aplane that flies quickly, a fast scientist can be a scien-tist who publishes papers quickly, who performs exper-iments quickly, who observes something quickly, whoreasons, thinks, or runs quickly. The plane went so fast it left its sound behind.e. a chair on which one sitscomfortablyb.
N01-1010@@Also, the currentversion of WordNet (1. ) This mea-sure also eectively takes into account a one-leveltransitivity in dij. Let nijdenote the number of in-stances where the judge 1 assigned sense i and thejudge 2 assigned sense j to the same instance, andni+and n+idenote the marginal totals of rows andcolumns respectively.
N01-1011@@Word sense disambiguation is the process of selectingthe most appropriate meaning for a word, based onthe context in which it occurs. While the accuracy ofthis approach was as good as any previously pub-lished results, the learned models were complex anddicult to interpret, in eect acting as very accurateblack boxes.Our experience has been that variations in learn-ing algorithms are far less signicant contributorsto disambiguation accuracy than are variations inthe feature set. Multinomial good-ness of t tests.
N01-1014@@In the narrow sense used in historical linguistics,cognates are words in related languages that havedeveloped from the same ancestor word. However, I found the effect ofconsidering paths longer than one link to be negligi-ble. A new algorithm for thealignment of phonetic sequences.
N01-1016@@While signicant eort has been expended onthe parsing of written text, parsing speechhas received relatively little attention. Nakatani, C. H. and Hirschberg, J. Acorpus-based study of repair cues in sponta-neous speech. Words that are edited out have an\E" above them.
N01-1017@@  Dictation applications of automatic speech recognition (ASR) require literal transcriptions of speech in order to train both speaker independent and speaker adapted acoustic models. Word Predictability after Hesitations: A Corpus-based Study. These results are statistically significant as evidenced by a t-test at 0.05 confidence level.
N01-1021@@What is the relation between a persons knowledge ofgrammar and that same persons application of thatknowledge in perceiving syntactic structure Theanswer to be proposed here observes three principles.Principle 1 The relation between the parser andgrammar is one of strong competence.Strong competence holds that the human sentenceprocessing mechanism directly uses rules of gram-mar in its operation, and that a bare minimum ofextragrammatical machinery is necessary. However,there is no clear consensus as to the size of the ele-ments over which exposure has clearest eect. In virtue of being a probabilistic parser itobserves principle 2.
N01-1023@@The current crop of statistical parsers share a similartraining methodology. Journal of Computer and System Sciences.A. Pick most probable n from H2 and add to labeled7.
N01-1024@@Many NLP tasks, such as building machine-readabledictionaries, are dependent on the results ofmorphological analysis. We also believethat some findings of this work can benefit otherareas of linguistic induction, such as part of speech.AcknowledgmentsThe authors wish to thank the anonymous reviewersfor their thorough review and insightful comments.ReferencesBaayen, R.H., R. Piepenbrock, and H. van Rijn. to mean truecircumfixes like the German ge-/-t as well ascombinations of prefixes and suffixes.
N01-1025@@Chunking is recognized as series of processes first identifying proper chunks from a sequence oftokens (such as words), and second classifying thesechunks into some grammatical classes. Namely, when the every re-moved support vector becomes error in Leave-One-Out procedure, |becomes the r.h.s. In other words, this problem becomes equiva-lent to solving the following optimization problem:LNMPOQMSRTMVUW9X Y-Z[3\I]-JI\^`_`aQbWcd8dfe9X	fgh-i.
N03-1001@@A major bottleneck in building data-driven speech pro-cessing applications is the need to manually transcribetraining utterances into words. )The n-gram phonotactic models used were representedas weighted finite state automata. To quantify this,we present empirical accuracy results from three differ-ent call-routing applications comparing our method withconventional utterance classification using word-trigramrecognition.Previous work at AT&T on utterance classificationwithout words used information theoretic metrics to dis-cover acoustic morphemes This led to a high false rejection rate (be-tween 40% and 50% for 1-best recognition output) whena word-based classification algorithm (the one describedby Wright et.
N03-1004@@Although atypical QA system classifies questions based on expectedanswer types, it adopts the same strategy for locating po-tential answers from the same corpus regardless of thequestion classification. Wordnet: A lexical database forEnglish. Thetop n candidate answers are then returned, each with anassociated confidence score.
N03-1010@@However, the quadratic component has such a small coefficient that it doesnot have any noticable effect on the translation speed for allreasonable inputs.a restricted stack search, to search errors. Therefore, the average complexity of  in practice is  , and the total complexity of  in practice is -/.-L. JD            .  3 50In   decoding, which combines up to two CHANGE operations or one CHANGE operation and one INSERT operation,  has a practical complexity of  , so that  -/.-L. JD    .    .    E  . We discuss below how  can be reduced to practically linear time for   decoding as well. In the original implementation of the algorithm, the entire alignment is evaluated after each searchstep (global evaluation, or  DFHGIJ8F ).
N03-1013@@This resource has al-ready been used effectively in a wide range of monolin-gual and multilingual NLP applications. Here, hunger  and hunger   arelinked in three ways, Naturally (N), by the Porter stem-mer (P), and in CatVar (C). Mapping WordNet Senses to a LexicalDatabase of Verbs.
N03-1014@@Unlike most problems addressed with machine learning,parsing natural language sentences requires choosing be-tween an unbounded (or even infinite) number of possi-ble phrase structure trees. TheChomsky adjunction is removed and replaced with a spe-cial modifier We also compiled some frequent chains of non-branching nodes (such as [S [VP &&& ]]) into a single nodewith a new label (becoming [S-VP &%&& ]). A neural network parser thathandles sparse data.
N03-1016@@PCFG parsing algorithms with worst-case cubic-timebounds are well-known. The I-tries were superior for the coarser estimates,while O-tries were superior for the finer estimates. SXL andSXR add the tags adjacent to e on the left and right re-spectively.
N03-1017@@Various researchers have improved the quality of statis-tical machine translation system with the use of phrasetranslation. This is a simple meansto optimize performance. Expectation Maximization learning in Marcuand Wongs framework yields both (i) a joint probabil-ity distribution	 , which reflects the probability thatphrases and are translation equivalents; (ii) and a jointdistribution  / , which reflects the probability that aphrase at position  is translated into a phrase at position.
N03-1018@@Although a great deal of text is now available in elec-tronic form, vast quantities of information still exist pri-marily (or only) in print. The probabilities are constructed for illustration, but realis-tic: notice how n is much more likely to be confused for c thank is.01b:h/0.056h:h/0. Their system is designed around a stratifiedalgorithm.
N03-1019@@The overallmodel is based on a two-level alignment between thesource and the target sentence: a phrase-level alignmentbetween source and target phrases and a word-levelalignment between words in these phrase pairs.The goal of this paper is to reformulate the ATTMso that the operations we intend to perform under a sta-tistical translation model, namely bitext word alignmentand translation, can be implementation using standardweighted finite state transducer (WFST) operations. Thisis implemented using AT&T FSM tools as followsfsmcompose O I D C # fsmproject -o #  fsmrmepsilon # fsmdeterminize  f .Given an alignment templateand a consistent sourcephrase, we note that the composition and determiniza-tion operations assign the probability  "	 #  fi (Equa-tion 16) to each consistent target phrase 	 . A finite-state ap-proach to machine translation.
N03-1020@@ Automated text summarization has drawn a lot of inter-est in the natural language processing and information retrieval communities in the recent years. If we ignore E (set it to 1), we obtain simple sentence recall score. ),  linear regression t-test (LRt, 11 degree of freedom for single document task and 13 degree of freedom for multi-document task), Pearson product moment coefficient of correlation (Pearson  ), and coefficient of determination (CD) for each Ngram(i,j) evaluation metric.
N03-1022@@MotivationIn spite of significant advances made recently in theQuestion Answering technology, there still remain manyproblems to be solved. WordNet glosses contain a source ofworld knowledge. Moldovan, M. Pasca, S. Harabagiu and M. Surdeanu.Performance issues and error analysis in an open-domain question answering system.
N03-1024@@In the past, paraphrases have come under the scrutinyof many research communities. Before squeezing  detroit a*e*  s*e* building  building reduced *e*was flattened blastedleveled levelled torazed leveled*e* into to  torubble in detroitdown ashesthe*e*groundb. for a morequantitative analysis.).
N03-1026@@Recent work in statistical text summarization has put for-ward systems that do not merely extract and concate-nate sentences, but learn how to generate new sentencesfrom Summary, Text Depending on the cho-sen task, such systems either generate single-sentenceheadlines The chal-lenge for such systems is to guarantee the grammatical-ity and summarization quality of the system output, iethe generated sentences need to be syntactically well-formed and need to retain the most salient information ofthe original document. Optimization of the function shownbelow was performed using a conjugate gradient opti-mization routine:L() f(s).At the core of the exponential probability model is a vec-tor of property-functions f to be weighted by parameters. "A prototype is ready.
N03-1027@@A fundamental concern for nearly all data-driven ap-proaches to language processing is the sparsity of la-beled training data. are assumed to be a random vector in the space. Model interpolation in this case per-System Training Heldout LR LPMAP Brown;T Brown;H 76.0 75.
N03-1031@@How-ever, the production of such corpora is expensive andlabor-intensive. During selection, oneparser first acts as the teacher and the other as the student,and then the roles are reversed.A and B are two different parsers.M iA and M iB are the models of A and B at step i.U is a large pool of unlabeled sentences.U i is a small cache holding a subset of U at step i.L is the manually labeled seed data.LiA and LiB are the labeled training examples for A and Bat step i.Initialize:L0A  Add unlabeled sentences from U .M iA and M iB parse the sentences in U i andassign scores to them according to their scoringfunctions fA and fB .Select new parses {PA} and {PB} according to someselection method S, which uses the scoresfrom fA and fB .Li+1A is LiA augmented with {PB}Li+1B is LiB augmented with {PA}M i+1A  Second, the central control uses some selection method,S, to choose a subset of these labeled sentences (based onthe scores assigned by f ) to add to the parsers The focus of this paper is on the selection phase, butto more fully investigate the effect of different selectionmethods we also consider two possible scoring functions.. labels disagree) for a human to label.
N03-1032@@These tests attemptto measure dependence between words by using statisticstaken from a large corpus. Thomas Nelson and Sons Ltd, second edition.T. Each dimen-sion represents a random discrete variable!
N03-1033@@Almost all approaches to sequence problems such as part-of-speech tagging take a unidirectional approach to con-ditioning inference along the sequence. A survey ofsmoothing techniques for maximum entropy models. The back-ward interaction between t0 and the next tag t+1 showsup implicitly later, when t+1 is generated in turn.
N03-1036@@The importance of automatic methods for enriching lex-icons, taxonomies and knowledge bases from free text iswell-recognized. H which subsumes as manyas possible of the members of S as closely as possiblein the hierarchy. In ACLSIGLEX Workshop, Columbus, Ohio.T.
N03-1037@@Typically, trainable summarization systems characterize each sentence according to a set of predefined features and then learn from training material which feature combinations are indicative of good extract sentences. With respect to the length restriction, we choose only the top n R-scored sentences. A vector space model for information retrieval.
N03-2002@@The art of statistical language modeling (LM) is to createprobability models over words and sentences that trade-off statistical prediction with parameter variance. Two features make an FLM distinct from a stan-dard language model: 1) the variables {F, F1, . In R. Rosenfeld, M. Osten-dorf, S. Khudanpur, and M. Johnson, editors, MathematicalFoundations of Speech and Language Processing.
N03-2003@@Language models constitute one of the key componentsin modern speech recognition systems. In such cases N-grams are typically pruned. A maximum entropy part-of-speech tag-ger.
N03-2005@@Topic Detection and Tracking (TDT) research is spon-sored by the DARPA TIDES program. A newevent detection (NED) system detects when a story dis-cusses a previously unseen event. The detection cost is normalizedby dividing by min HLONF!F3P-Q5RHUWV3PXZX-Q5R so that aperfect system scores 0, and a random baseline scores 1.Equal weight is given to each topic by accumulating errorprobabilities separately for each topic and then averaged.The minimum detection cost is the decision cost when thedecision threshold is set to the optimal confidence score.
N03-2008@@ The ability to develop automatic methods for semantic classification has been hampered by the lack of large semantically annotated corpora. % on a held out test set. This is reasonable 3  Using n-best lists of 50 and 100 showed no significant difference in performance.
N03-2011@@Not surprisingly, though, the task is very difficult. 3 and replace rule (4) with the following one, which is underspecified w.r.t to the cate-gory feature. A decision-based approach to rhe-torical parsing.
N03-2012@@Meetings are an integral component of life in most or-ganizations, and records of meetings are important forhelping people recall (or learn for the first time) whattook place in a meeting. A statistical model for discourse actrecognition in dialogue interactions. The features used in classificationinclude heuristic word types and counts, word-based fea-tures derived from n-gram scores, and prosodic features.Simple word-based features include: the total num-ber of words in a spurt, the number of positive keywords, and the class (positive, negative,backchannel, discourse marker, other) of the first wordbased on the keywords.
N03-2015@@To recognize a morpheme boundary, for example be-tween a root and a suffix, a learner must have seen atleast two roots with that suffix and at least two suffixeswith that root. Unsupervised Learning of theMorphology of a Natural Language. For example, suppose that, in a given corpus, theprefix friend occurs only with the suffixes NULL,s, and ly occurs only with thesame suffixes.
N03-2016@@In the context of machine translation, the term cognatesdenotes words in different languages that are similarin their orthographic or phonetic form and are possibletranslations of each other. %, and a corresponding improvementin both precision and recall. J. Och and H. Ney.
N03-2017@@The IBM statistical machine translation (SMT) modelshave been extremely influential in computational linguis-tics in the past decade. fm, an alignment is a set of linksbetween the words in E and F . A probabilitymodel to improve word alignment.
N03-2018@@Human tutors can respond to both the content ofstudent speech and the manner with which it is spoken(eg confidently Building spo-ken dialogue tutoring systems has great potential benefit,for speech is the most natural and easy to use form ofnatural language interaction, and it supplies a rich sourceof prosodic and acoustic information about the speakerscurrent mental state, which can be used to monitor thepedagogical effectiveness of student-computer interac-tions. The architecture of Why2-Atlas: A coachfor qualitative physics essay writing. Can you re-call itSTUDENT: Um no it was one of Newtons laws but I dontremember which one.
N03-2019@@ Practical NLP applications such as text summariza-tion and question-answering place increasing demands on the processing of temporal information. (3) U. N. SecretaryGeneral Boutros Boutros-Ghali Sunday opened a meeting of ....Boutros-Ghali ar-rived in Nairobi from South Africa,  Thus, the latest news is often pre-sented first, instead of events being described in order of occurrence (the latter ordering is called the narrative convention). This feature (not a machine feature) is called anchors.
N03-2022@@ Portability and domain independence are critical challenges for Natural Language Processing (NLP) systems. GJ present a comprehensive empirical approach to the problem of semantic role assignment. Fellbaum C., WordNet: an Electionic Lexical Database, Cambridge, MA, The MIT Press.
N03-2023@@In order to evaluate a word sense disambiguation (WSD)algorithm in a new language or domain, a sense-taggedevaluation corpus is needed, but this is expensive to pro-duce manually. Pessimistic refers to theevenly distributed category-based pseudowords, gener-ated by requiring the word frequency to fall in the interval[E/2;3E/2]. A SimpleAlgorithm for Identifying Abbreviation Definitions in
N03-2024@@Automatically generated summaries, and particularlymulti-document summaries, suffer from lack of coher-ence One explanation is that the most widespread sum-marization strategy is still sentence extraction, where sen-tences are extracted word for word from the original doc-uments and are strung together to form a summary. A Markov Chain ModelThe initial examination of the data showed that syntacticforms in coreference chains can be effectively modeledby Markov chains.Let   be random variables taking values in I. fi .These properties have very visible counterparts in thebehavior of coreference chains. A coreference chain consists of all the mentionsof an entity within a document.
N03-2026@@At least 23 people werereported dead, with more than 140 injured, and Pres-ident Arroyo of the Philippines characterized the blastas a terrorist act. % and a precision fora random sample of translation pairs of 87%. With the 13 hour time difference, itwas then 4:20 A.M on the same date in Washington, DC.Twenty-four hours later, at 4:13 A.M. on March 5, partic-ipants in the Translingual Information Detection, Extrac-tion and Summarization (TIDES) program were notifiedthat Cebuano had been chosen as the language of interestfor a surprise language practice exercise that had beenplanned quite independently to begin on that date.
N03-2029@@For example, for questions like When wasGandhi born These examples sug-gest that the text patterns such as   NAME  was born in BIRTHDATE   when formulated as regular expres-sions, can be used to select the answer phrase to ques-tions.  ANSWER  was a prolific  OCCUPATION QT 7. Technical Report,Department of Computer Science, Carnegie-MellonUniversity, CMU-CS-95144.D.
N03-2034@@This paper discusses the lexicon and ontology we builtand coupled with the parser LCFLEX (Rose We also report theexcellent parsing results we obtained.Our ultimate goal is to develop a (semi)automaticmethod to derive domain knowledge from instructionaltext, in the form of linguistically motivated actionschemes. Wemodified and augmented LCFLEXs existing lexicon andbuilt an ontology.To illustrate our work, we will refer to the lexical en-try for position, that can be both a noun (n) or a verb(vlex)  Eachpart of speech (POS) category is associated to a semtag,an index that links the POS entry to the corresponding se-mantic representation. Wiping the slateclean: a lexical semantic exploration.
N03-3001@@The goal of TDT consists ofbreaking the stream of news into individual news stories,to monitor the stories for events that have not been seenbefore and to gather stories into groups that each discussa single topic.Several approaches have been explored for compar-ing news stories in TDT. Weighted Perceptron approachNow, all that remains to be done is to com-bine the semantic class-specific log-likelihood scores[L1(D2|D1), .., L|C|(D2|D1)]T in a principled way toobtain the overall similarity score of D1 with respect toD2. otherwise.The linear discriminant function clearly constitutes a per-ceptron.
N03-3004@@Word sense discrimination is the process of grouping orclustering together instances of written text that includesimilar usages of a given target word. 0 AcknowledgmentsThis research is being conducted as a part of my M.S. Edmonds and S. Cotton, editors.
N03-3006@@Many extensions to text-based, data-intensive knowledgemanagement approaches, such as Information Retrievalor Data Mining, focus on integrating the impressive re-cent advances in language technology. The parser uses the CYK algorithm, whichhas parsing complexity of O(n3), where n is the numberof words in a word-based, but only chunks in a head-of-chunk-based model. Thispaper presents such a parsing system.
N03-3009@@ Text segmentation can be defined as the automatic iden-tification of boundaries between distinct textual units (segments) in a textual document. This mean value then acts as the minimum allowable boundary strength that must be exceeded if the end of textual unit n is to be classified as the boundary point between two news stories. Okumura M., T. Honda, Word sense disambiguation and text segmentation based on lexical cohesion.
N03-4011@@The output of these programs is a ranked list of similar words to each word. Phase I computes each elements top-k similar elements. Each cluster corresponds to a sense of the headword.
N04-1001@@Detecting entities, whether named, nominal or pronom-inal, in unrestricted text is a crucial step toward under-standing the text, as it identifies the important concep-tual objects in a discourse. Each prefix, stem orsuffix will be called a token in this discussion; any con-tiguous sequence of tokens can represent a mention.For example, the word trwmAn could be segmented in 3 tokens (for instance, ifthe word was not seen in the training data):trwmAn  t  rwm  Anwhich introduces ambiguity, as the three tokens form re-ally just one mention, and, in the case of the word tm-nEh, which has the segmentationtmnEh  t  mnE  hthe first and third tokens should both be labeled aspronominal mentions  but, to do this, they need to beseparated from the stem mnE.Pragmatically, we found segmenting Arabic text to be anecessary and beneficial process due mainly to two facts:1. some prefixes/suffixes can receive a different men-tion type than the stem they are glued to (for in-stance, in the case of pronouns);2. keeping words together results in significant datasparseness, because of the inflected nature of thelanguage. In the current implementation, the models use a history of2 tags.Feature Type Ar Zh EnToken in window of 5      Morph in window of 5   N/A  POS info      Text chunking info  theoutput of the system on the segmented data: the text isfirst segmented into tokens, and the classification is thenperformed on tokens.
N04-1002@@For example, consider the following short passage of text:  John Smith was appointed chair of the committee. The runtimes of incremental approaches are linear whereas the runtime of our agglomerative vector space approach is O(n). Note that it is not possible to have a precision or recall of zero since entity e is always in common between the two chains.
N04-1003@@Reading and understanding text is a task that requires theability to disambiguate at several levels, abstracting awaydetails and using background knowledge in a variety ofways. sup-plied in the previous Ior E-step, this amounts to themaximum likelihood estimation. Given a query name n, the rankingof the entities with regard to the value of P (e)  P (n|e)(shown in brackets) by Model II is as follows.Input: George Bush1.
N04-1004@@For example, it isimpossible to extract the semantic content of the verbalutterance Ill take this one without an accompanyingpointing gesture indicating the thing that is desired. This is the bicycle ridden by E.T.. (1)where ts, te describe the start and ending time of a gestureor reference, w  S is the word corresponding to r, andGtype is the type of gesture (eg deictic or trajectory).An alternative, useful description of the set B is as thefunction b(g) which returns for each gesture a set of cor-responding references.
N04-1005@@In the past four decades of speech and natural languageprocessing, both data-driven approaches and rule-basedapproaches have been prominent at different periods intime. Galescu, and A. Stent. The AT&T next-generation TTS.
N04-1007@@To date, research in question answering has concentratedon factoid questions such as Who was Abraham Lincolnmarried to Factoidquestions, however, represent but one facet of questionanswering, whose broader goal is to provide humans withintuitive information access using natural language.In contrast to factoid questions, the objective for defi-nition questions is to produce as many useful nuggetsof information as possible. Given n total responses, wecalculate the final number of responses to return as:n if n  Component EvaluationWe evaluated the performance of each individual surfacepattern and the dictionary lookup technique on 160 def-inition questions selected from the TREC-9 and TREC-10 QA Track testsets. nuggets fitsvery naturally into a binary classification task.
N04-1008@@The definition of the task, however, is generally restricted to answering factoid questions: questions for which a complete answer can be given in 50 bytes or less, which is roughly a few words. Chin-Yew Lin and E.H. Hovy. We implement this step as choosing to return the first N hits provided by the search engine.
N04-1011@@Sentences are generally demarcatedby a major fall (or rise) in f0, lengthening of thefinal syllable, and following pauses. For each of these wegenerated a training and test corpus. For example, as mentioned earlier theparser uses a mixture of n-gram models to predictthe sequence of categories on the right-hand sideof syntactic rules, backing off ultimately to a dis-tribution that includes just the head and the preced-ing siblings category.
N04-1012@@In particular, they enable rapidcreation of labeled datasets which can then be used fortrainable speech and language technologies. On building a more efficient grammar byexploiting types. Touretzky, and T. Leen, editors, Advances in Neural Infor-mation Processing Systems, volume 7, pages 705712.
N04-1013@@In applications that are sensitive to the meanings ex-pressed by natural language sentences, it has becomecommon in recent years simply to incorporate publiclyavailable statistical parsers. A noisy-channelmodel for document compression. Submitted for publication.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and Mark John-son.
N04-1014@@Much of natural language work over the past decade hasemployed probabilistic finite-state transducers (FSTs)operating on strings. Thesize of the derivation trees is at worst O(|Q||I||O||R|).For a corpus of K examples with average input/outputsize M , an iteration takes (at worst) O(|Q|  Tree-to-String Transducers (xRS)We now turn to tree-to-string transducers (xRS). So finite lookaheadis not a problem.
N04-1016@@However, they do not demonstrate thatrealistic NLP tasks can benefit from web counts. Then we investi-gate the generality of the web-based approach by apply-ing it to a range of analysis and generations tasks, involv-ing both syntactic and semantic knowledge: (c) orderingof prenominal adjectives, (d) compound noun bracketing,(e) compound noun interpretation, and (f) noun count-ability detection. In these cases, we set the webcount to a large constant (108).
N04-1018@@Repeated words, hesitations such as um anduh, and corrections to a sentence in mid-stream area normal part of conversational speech. In the second approach, referred to asthe linearly interpolated model, a decision is made basedon the combined posterior probabilityPtree(E|A,W ) + (1  )PLM (E|W ),where A corresponds to the acoustic-prosodic featuresand the weighting factor  can be chosen empirically tomaximize target performance, ie bias the prediction to-ward the more accurate model. For example, the discourse marker I mean serves asan explicit editing term in the following edit disfluency:I didnt tell her that, I mean, I couldnt tell her that hewas already gone.2.
N04-1019@@Evaluating content selection in summarization has provento be a difficult problem. Bleu: A method for automatic evalu-ation of machine translation. 1) should not be expressed if all the SCUs intier n have not been expressed.
N04-1020@@The ability to identify and analyse temporal informationis crucial for a variety of practical NLP applications suchas information extraction, question answering, and sum-marisation. Adjectives(A), nouns (N and NW) and temporal signatures (T), allseem to play more of a role in the fusion rather than theinterpretation task. The ensemble (E) classified correctly 70.
N04-1021@@Despite the enormous progress in machine translation(MT) due to the use of statistical techniques in recentyears, state-of-the-art statistical systems often producetranslations with obvious errors. Then, every phrase fproduces its translation e (using the corresponding align-ment template z). The total probability is the product over allalignment templates i, either P (ATi is right-continuous)or 1  P (ATi is right-continuous).In both models, the probabilities P have been esti-mated from the full training data (train).
N04-1022@@However, given the many factors that influencetranslation quality, it is unlikely that we will find a singletranslation metric that will be able to judge all these fac-tors. The BLEU score is zero if any of the n-gramprecisions E K  is zero for that sentence pair. Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
N04-1023@@In this paper, we introducetwo novel machine learning algorithms specialized forthe MT task.Discriminative reranking algorithms have also con-tributed to improvements in natural language parsingand tagging performance. It achieves a BLEU scoreof 31. Reranking enables rapidexperimentation with complex feature functions, becausethe complex decoding steps in SMT are done once to gen-erate the N-best list of translations..
N04-1025@@In the course of constructing a search engine for stu-dents, we wanted a method for retrieving Web pagesthat were not only relevant to a students query, but alsowell-matched to their reading ability. We derive L(Gi | T)from (1) via Bayes Rule, which is:However, we first make two further assumptions:1. %, with a standard deviationof 6.
N04-1026@@This paper investigates the automatic classification ofstudent emotional states using acoustic-prosodic, non-acoustic-prosodic, and contextual information, in a cor-pus of human-human spoken tutoring dialogues. H. Witten and E. Frank. Wealso ran cross-validations training on n-1 subjects and testingon the remaining subject, but found our results to be the same.
N04-1027@@Research on dialogue systems in the past has fo-cused on engineering the various processing stagesinvolved in dialogical human-computer interaction(HCI) e. g., robust automatic speech recognition,intention recognition, natural language genera-tion or speech synthesis (cf. The flip side, i. e.,computer-human interaction (CHI), has received verylittle attention as a research question by itself. In Proceedings of the 7th Inter-national Conference on Spoken Language Processing,Denver, U.S.A.J.
N04-1028@@Spoken Dialogue Systems and Non-NativeSpeakersSpoken dialogue systems rely on models of human lan-guage to understand users Such modelscover the acoustic and linguistic space of the commonlanguage used by the system and the user. AcknowledgmentsThe authors would like to thank Alan W Black, Dan Bo-hus and Brian Langner for their help with this research.This material is based upon work supported by the U.S.National Science Foundation under Grant No. Galaxy-II: A reference architecture forconversational system development.
N04-1030@@Automatic, accurate and wide-coverage techniques thatcan annotate naturally occurring text with semantic argu-ment structure can play a key role in NLP applicationssuch as Information Extraction, Question Answering andSummarization. We will refer to this as G&H SystemI.The Chen and Rambow (C&R) SystemChen and Rambow report on two different systems,also using a decision tree classifier. A generative model for semantic role labeling.
N04-1031@@Information can be provided in various forms, and one ofthem is speech form. If e is target and contains adverbs and nouns, it isdifficult to count the frequency because of the sparse dataproblem. Forexample:(4) watashi-wa eiga-wo mi masuI a movie to watch (politeness)I watch a movie(4) implies politeness using the postpositional particlemasu.Those two interpersonal expressions often appear inspoken language, and are easily recognized as such bya morphological analyzer and simple rules.
N04-1032@@  Thematic roles (AGENT, THEME,  LOCATION, etc) provide a natural level of shallow semantic representation for a sentence. A Statistical Parser for Czech. Experimental Results As in our Chinese experiments, we used our SVM-based classifier, using N one-versus-all classifiers.
N04-1033@@In statistical machine translation, we are given a sourcelanguage (French) fJ ,which is to be translated into a target language (English It allows anindependent modeling of target language model Pr(eI1)and translation model Pr(fJ1 |eI1)1. For the maximization problemin (11), we define the quantity Q(j, e) as the maximumprobability of a phrase sequence that ends with the lan-guage word e and covers positions 1 to j of the sourcesentence. We see a clearlinear dependency.
N04-1035@@One such modelembodies a restricted, linguistically-motivated notion ofword re-ordering. Inferring Complex RulesNow we have a precise problem statement: learn the setA(S, T ). A systematic comparison ofvarious statistical alignment models.
N04-1037@@These sys-tems typically rely on a variety of morphosyntacticfactors that have been posited in the literature toaffect the interpretation of pronouns in naturally-occurring discourse, including gender and numberagreement, the distance between the pronoun andantecedent, the grammatical positions of the pro-noun and antecedent, and the linguistic form of theantecedent, among others. 3%accuracy,6 provides a nontrivial baseline. Experiments with n-fold jackknifing forother values of n produced similar results.
N04-1038@@Most compu-tational models for coreference resolution rely on prop-erties of the anaphor and candidate antecedent, such aslexical matching, grammatical and syntactic features, se-mantic agreement, and positional information.The focus of our work is on the use of contextual roleknowledge for coreference resolution. In Proceedings of the Sixth Workshopon Very Large Corpora.E. A statistical approachto anaphora resolution.
N04-1039@@They are also some-times called logistic regression models, maximum likeli-hood exponential models, log-linear models, and are evenequivalent to a form of perceptrons/single layer neuralnetworks. In addition, the exponential prior inspires animproved version of Good Turing discounting with lowerperplexity.Conditional maxent models are of the formP )where x is an input vector, y is an output, the f i are the so-called indicator functions or feature values that are trueif a particular property of x, y is true, F is the numberof such features,  represents the parameter set 1...n,and i is a weight for the indicator fi. ); using a Gaussian prior (perplexity 183.
N04-1040@@Story link detection, as defined in the Topic Detection andTracking (TDT) competition sponsored by the DARPATIDES program, is the task of determining whether twostories, such as news articles and/or radio broadcasts, areabout the same event, or linked. as derived from the training corpus,and KL is the Kullback-Leibler divergence:  1#	 z 	t#s%.2fl.2ffIn computing the clarity measure, theterm frequencies were smoothed with the General Englishmodel using a weight of 0.01. In Multiple Classier Systems, Cagliari,Italy.Richard O. Duda and Peter E. Hart.
N04-1041@@Current manually constructed ontologies such as WordNet and Cyc have important limitations. % in a human evalua-tion. Riloff, E. and Shepherd, J.
N04-2002@@ Chemical names recognition is one of the first tasks needed for building an information extraction system in the biomedical domain. 4 Classification Using N-gram Models W e  can estimate probability of a string given class (chemical or non-chemical) as the probability of letters of the string based on a finite history. If the number of tokens containing s is n(s) and the number of chemicals containing s is c(s) , then the selection criterion becomes 95.
N04-2004@@The understanding of natural language text includes notonly analysis of syntactic structure, but also of semanticcontent. In E. Bachand R. Harms, editors, Universals in Linguistic The-ory, pages 188. English Verb Classes and Alter-nations: A Preliminary Investigation.
N04-2005@@ and Motivation American Sign Language (ASL) is a visual/spatial natural language used primarily by the half million Deaf individuals in the U.S. and Canada. Acknowledgements I would like to thank my advisors Mitch Marcus and Martha Palmer for their guidance, discussion, and revisions during the preparation of this work. A Survey of Current Paradigms in Machine Translation.
N04-2006@@An English noun phrase (NP) may contain a determiner,such as this, that, a, an or the, which specifies the refer-ence of its head. Head-Driven Statistical Modelsfor Natural Language Parsing, Ph.D. Thesis, Univer-sity of Pennsylvania, Philadelphia, PA.Julia E. Heine. It had, however,only a modest effect.
N04-3002@@There is reason to believe that speech-based tutorial dialogue systems could be even more ef-fective. A framework for robust sentence levelinterpretation. J. Litman and S. Pan.
N04-3003@@This demonstration will present a prototype system forbidirectional speech-to-speech translation within alimited semantic domain, that of first encountersbetween a patient and a medical professional. ThePashto speaker, playing the role of an injured patient,had received training in complaints consistent with aparticular injury scenario and had seen others use thesystem, but had not interacted with the system himself.Except as noted, the translations are understandable.Spoken input System resultI am a doctor, can I helpyou AyA t@ dard larezAyt of much [minormisrecognition; translationshould have been "much"]Do you take medications First,commercially nonviable languages, such as Pashto,often offer very few linguistic resources (such aslinguistic descriptions, acoustic data, texts, languageprocessing tools). Gemini:A natural language system for spoken languageunderstanding.
N04-3006@@The goal of the semantic parser is to analyze the semanticstructure of a natural language sentence. Wordnet: A lexical database. Ruppenhofer, and E. Wood.
N04-3012@@In version 2.0, there are nine separatenoun hierarchies that include 80,000 concepts, and 554verb hierarchies that are made up of 13,500 concepts.Isa relations in WordNet do not cross part of speechboundaries, so similarity measures are limited to mak-ing judgments between noun pairs (eg, cat and dog) andverb pairs (eg, run and walk). Technical ReportCSRP 569, University of Sussex, January.S. Command LineThe utility similarity.pl allows a user to measure specificpairs of concepts when given in word#pos#sense form.For example, car#n#3 refers to the third WordNet nounsense of car.
N04-4001@@ The explosion of available online text has made it necessary to be able to present information in a succinct, navigable manner. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics. Results On average, we found the length of a tile to be 4.
N04-4006@@In essence, these ap-proaches involve beginning from a smoothed languagemodel trained on out-of-domain observations, and adjust-ing the model parameters based on in-domain observa-tions. In a first scenario, weproduced a MAP estimated model on the first 4. Leth represent an n-gram history of zero or more words.
N04-4007@@In CMUs Project LISTEN, for example, the tutor oper-ates by prompting children to read individual sentences out loud. Word Error Rate (%) Experimental Configuration MFCC PMVDR (A) Baseline: single n-gram language model 17. This system has a real-time factor of 0.
N04-4008@@ Since the early 90s, automatic alignment of bilingual documents and sentences based on lexical and syntactic information has been a major focus of the statistical NLP community as their results are a valuable resource for statistical machine translation, cross-lingual question answering, and other bilingual or cross-lingual tasks. /make a phone call). is linked to all HowNet concepts that are translations of beat which are verbs, and which also belong to the HowNet category beat that are linked to the cause_harm Only the concepts in the top N categories are considered as correctly linked to the lexical entries in the cause_harm We heuristically chose N to be three in our algorithm.
N04-4009@@Reliable estimation of theweights in both paradigms requires a substantialmanually-annotated corpus of examples. A maximum entropyapproach to natural language processing. (d) Otherwise, with some probability (0. in our experiments1), create a pronoun-antecedent pair for each possible antecedentand label the coreference outcome as False.
N04-4010@@ Named Entity Recognition (NER) is the first step for many tasks in the fields of natural language processing and information retrieval. A Spoken Chinese NER Model 2. Finally, an NE output is considered valid if  1 3  1,( )0,iNE occurs in the i th hypothesisNEOtherwise Experimental setup We use the n-best hypothesis of 1,046 Broadcast News Chinese utterances from the BBN LVCSR system.
N04-4011@@One important aspect of building multimodal systems is for the system to understand the meanings of multimodal user inputs. For example, one task was to find e least expensive house in the most populated town. Gold, S. and Rangarajan, A.
N04-4015@@Rarely occurring inflected forms of a stem in Arabic often do not accurately translate due to the frequency imbalance with the corresponding translation word in English. A Syntax-Based Statistical Translation Model. Group I: At least one of  tagE|tagik occurs as one of the top 3 most probable translation pairs in Pr(tagE|tagA).
N04-4020@@Reliable annotated data are necessary for a wide varietyof natural language processing tasks. Theseannotation matrices, Mannotator, have N rows and M col-umns, where n is the number of messages and m is thenumber of labels. A total, , is setto the sum of all cells of Ag.
N04-4023@@Indexing and retrieving stories within a large collectionof video requires automatic detection of story bound-aries, and video story segmentation is an essential steptoward providing the means for finding, linking, summa-rizing, and visualizing related parts of multimedia col-lections. Hsu, S. Chang, A Statistical Framework for FusingMid-level Perceptual Features in News Story Segmen-tation, In addition we analyzed the ASR word se-quences in all broadcasts to automatically derive a set ofsource-dependent cue phrase n-gram features.
N04-4026@@1 shows an example block translation us-ing five Arabic-English blocks  . fl ) if it has a left adjacentpredecessor. fl )1We keep all blocks for which A@CBEDFHG and the phraselength is less or equal I .
N04-4027@@In this paper, we discuss work on summarizing emailthreads, ie, coherent exchanges of email messagesamong several participants. Exploiting e-mail structure toimprove summarization. msg num: The ordinality of m in t (ie, the absoluteposition of m in t).
N04-4028@@Information extraction usually consists of tagging a se-quence of words (eg a Web document) with semanticlabels (eg PERSONNAME, PHONENUMBER) and de-positing these extracted fields into a database. 2, except the max is replacedby a summation. t+1(si) are recursivelydefined similarly as in Eq.
N04-4032@@Parsing speech can be useful for a number of tasks, in-cluding information extraction and question answeringfrom audio transcripts. Speech repairs: A parsingperspective. The SLM parser uses binary trees,but the syntactic structures we are given as truth oftenbranch in N-ary ways, whereU  .
N04-4035@@Natural spoken discourse is composed a sequence of ut-terances, not independently generated or randomly strungtogether, but rather organized according to basic struc-tural principles. This effectiveness is a sub-stantial improvement over the sample baseline of 50%. Specifically, wordduration, normalized mean pitch, and normalized meanintensity all differed significantly for words in topic-finalposition relative to occurrences throughout the story (Fig-ure 1).
N04-4036@@The field of NLP had seen a resurgence of research inshallow semantic analysis. of the constituent corresponding to the semanticargument.Position  This is a binary feature identifying whetherthe constituent is before or after the predicate.Head word  The syntactic head of the constituent..  Classifier and Implementation We formulate theparsing problem as a multi-class classification problemand use a Support Vector Machine (SVM) classifier in theONE vs ALL (OVA) formalism, which involves trainingn classifiers for a n-class problem  Performance We evaluate our system on threetasks: i) Argument Identification: Identifying parse con-stituents that represent arguments of a given predicate, ii)Argument Classification: Labeling the constituents thatare known to represent arguments with the most likelyroles, and iii) Argument Identification and Classification:Finding constituents that represent arguments of a pred-icate, and labeling them with the most likely roles. A generative model for se-mantic role labeling.
N04-4037@@ Semantic representation, and, obviously, its extraction from an input text, are very important for several natural language processing tasks; namely, information extrac-tion, question answering, summarization, machine trans-lation and dialog management. In the following, each com-ent will be described along the dimensions of its (i)t, (ii) decision context, (ii) features, (iv) classifier (v) output. In the second e, the input is the sequence of word/tag pairs.
N04-4038@@Arabic is garnering attention in the NLP community dueto its socio-political importance and its linguistic differ-ences from Indo-European languages. In addition, we are tryingto extend the approach to semantic chunking by hand-labeling a part of Arabic TreeBank with argumentsor semantic roles for training.ReferencesErin L. Allwein, Robert E. Schapire, and Yoram Singer. Thus the task is a one of 19 classification task(since there are I and B tags for each chunk phrase type,and a single O tag).
N06-1001@@Capitalization is the process of recovering case in-formation for texts in lowercase. An edgebetween two words in E represents the dependencybetween them captured by monolingual n-gram lan-guage models. A Gaussian priorfor smoothing Maximum Entropy models.
N06-1002@@Unfortunately the use of phrases in SMT is beset by a number of difficult theoretical and practical problems, which we attempt to characterize below. A unigram model performs surprisingly well. Varying MTU n-gram model order.
N06-1003@@This is problematic forso called low density language pairs which do nothave very large parallel corpora. A parallel corpus for statisticalmachine translation. If we exclude 451 words worth of names, num-bers, and foreign language text in 2,000 sentencesthat comprise the Spanish portion of the Europarltest set, then the number of unique n-grams in textare: 7,331 unigrams, 28,890 bigrams, 44,194 tri-grams, and 48,259 4-grams.
N06-1004@@Distortion in phrase-based MT occurs when the order of phrases in the source-language sentence changes during translation, so the order of corresponding phrases in the target-language trans-lation is different. E.g., one could apply right-to-left SCMs, or distorted target SCMs which assume a target hypothesis generated the source sentence, instead of vice versa. With a 95% bootstrap confidence interval of 0.
N06-1005@@Recognizing the semantic equivalence of two frag-ments of text is a fundamental component of manyapplications in natural language processing. This heuristic allows us to pre-dict false entailment in the example Pertussis is notvery contagious and ...pertussis, is a highly conta-gious bacterial infection Modal auxiliary verb mismatchIf any two nodes (h, t) are aligned, and t is modifiedby a modal auxiliary verb (e.g, can, might, should,etc.) A Semantic Ap-proach to Recognizing Textual Entailment.
N06-1006@@During the last five years there has been a surge inwork which aims to provide robust textual inferencein arbitrary domains about which the system has noexpertise. A possible explanation runs as fol-lows. This work was supportedin part by the Advanced Research and DevelopmentActivity (ARDA)s Advanced Question Answeringfor Intelligence (AQUAINT) Program.ReferencesE.
N06-1007@@The entailment relations between verbs are a naturallanguage counterpart of the commonsense knowl-edge that certain events and states give rise to otherevents and states. In In Proceed-ings of Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP04).T.M. Selection and Information: A Class-BasedApproach to Lexical Relationships.
N06-1008@@Our goal is to develop an unsupervised method foracquiring inference rules that describe logical impli-cations between event occurrences. Let us write e : exp if event instance ecan be described by event description exp. It just works as a placeholder.
N06-1010@@Named Entity Recognition (NER) is the task ofidentifying and classifying phrases that denote cer-tain types of named entities (NEs), such as per-sons, organizations and locations in news articles,and genes, proteins and chemicals in biomedical lit-erature. The smaller the rank rT (f) is, the more impor-tant the feature f is in the training set T . Each tokenis assigned the tag I if it is part of an NE and the tagO otherwise.
N06-1011@@Named Entity recognition has been getting muchattention in NLP research in recent years, since itis seen as a significant component of higher levelNLP tasks such as information distillation and ques-tion answering, and an enabling technology for bet-ter information access. A discriminative framework for bilin-gual word alignment. This research is supported bythe Advanced Research and Development Activity(ARDA)s Advanced Question Answering for Intel-ligence (AQUAINT) Program and a DOI grant underthe Reflex program.ReferencesNasreen AbdulJaleel and Leah S. Larkey.
N06-1012@@It is not uncommon to havehundreds of thousands or even millions of features.But not all features, even ones that are carefully engi-neered, improve performance. For example, a feature functionfk(yt1, yt, x, t) could be a binary test that has value 1 ifand only if yt1 has the label adjective, yt has the labelproper noun, and xt begins with a capital letter.Linear-chain CRFs correspond to finite state machines,and can be roughly understood as conditionally-trainedhidden Markov models (HMMs). Thus, if n is large, each xicontributes weakly to the output y.
N06-1013@@The alignment-combination approach (calledACME) operates at the level of alignment links,rather than at the sentence level (as in previous MEapproaches). A discriminative framework for bilin-gual word alignment. Ak is an alignment link (i, j).
N06-1014@@First, even withthe highly optimized implementations in GIZA++,models 3 and above are still very slow to train. The E-step chooses q givena fixed  to maximize the lower bound. On the other hand, if the edge (i+2, j+2)were included, that penalty would be mitigated.
N06-1016@@ Corpus-based methods for word sense disambiguation (WSD) have gained popularity in recent years. David D. Lewis and William A. Gale. One method is to pick the example whose prediction vector q displays the greatest Shannon entropy:  Using either method of uncertainty sampling, the computational cost of picking an example from T candidates is: O(TD) where D is the number of model parameters.
N06-1018@@With increasing demand from ever more sophisti-cated NLP applications, interest in extracting andunderstanding temporal information from texts hasseen much growth in recent years. Lenders, W. Hess, and T. Portele, editors, Proceed-ings of the 4th Conference on Natural Language Pro-cessing KONVENS 98.N. E.g., tomorrow, last year, two weeksfrom today.
N06-1019@@State-of-the-art statistical parsers require largeamounts of hand-annotated training data, and aretypically based on the Penn Treebank, the largesttreebank available for English. , pim are the corresponding gold-standard de-pendency structures;  (S) is the set of possiblederivation, dependency-structure isa smoothing parameter; and n is the number of fea-tures. A comparison of algorithms for max-imum entropy parameter estimation.
N06-1020@@In parsing, we attempt to uncover the syntactic struc-ture from a string of words. Next, a discriminative reranker reorders then-best list. Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
N06-1021@@Dependency parsing is an alternative to constituencyanalysis with a venerable tradition going back atleast two millenia. Both algo-rithms are O(N3), where N is the number of wordsin a sentence. % and a sentence accuracy of63.
N06-1022@@Reasonably accurate constituent-based parsingis fairly quick these days, if fairly quick meansabout a second per sentence. In our version we keep track of only theparent, the head constituent and the constituentimmediately to the right or left, depending onwhich side of the constituent we are processing.With this scheme the above rules now look likethis:A Ad,c eAd,c Aa,c dAa,c a Ab,cAb,c b cSo, for example, the rule A Ad,c e wouldhave a high probability if constituents of typeA, with c as their head, often have d followedby e at their end.Lastly, we add parent annotation to phrasalcategories to improve parsing accuracy. A* parsing:Fast exact viterbi parse selection.
N06-1023@@Case structure (predicate-argument structure or log-ical form) represents what arguments are related toa predicate, and forms a basic unit for conveying themeaning of natural language text. Such optimization could be a way to improvethe system performance. For example, the left ar-gument of the predicate is the subject, and the rightargument of the predicate is the object in most cases.Blaheta and Charniak proposed a statistical methodCurrently, National Institute of Information and Communi-cations Technology, JAPAN, dk@nict.go.jpCurrently, Graduate School of Informatics, Kyoto Univer-sity, kuro@i.kyoto-u.ac.jpfor analyzing function tags in Penn Treebank, andachieved a really high accuracy of 95.
N06-1024@@categories that represent dis-placed constituents. Ph.D. thesis, De-partment of Computer and Information Sciences, Uni-versity of Pennsylvania.E. or The man (0) I saw.
N06-1025@@While machine learning has proved toyield performance rates fully competitive with rulebased systems, current coreference resolution sys-tems are mostly relying on rather shallow features,such as the distance between the coreferent expres-sions, string matching, and linguistic form. Else T if the first para-graph of text of PREi/j contains TREj/i ; else F.I/J RELATED CONTAINS U if no Wikipediapage titled as TREi/j is available. Conclusions and Future WorkThe results are somehow surprising, as one wouldnot expect a community-generated categorizationto be almost as informative as a well structuredFeature set F1baseline (Soon w/o DISTANCE) 58.
N06-1026@@ Recently, many researchers and companies have explored the area of opinion detection and analysis. hNand opinion expression e. The conditional probability P h h1h2. A Maximum Entropy Approach to Natural Language.
N06-1027@@ Threaded discussion is popular in virtual cyber communities and has applications in areas such as customer support, community development, inter-active reporting (blogging) and education. model of a web surfers behavior. Marom, Y. and Zukerman, I.
N06-1028@@Spoken lan-guage tests are still mostly scored by human rat-ers. Of these, 798 recordings were from one of five main test items, identified as P-A, P-C, P-T, P-E and P-W. They attempt to solve a multivariate discrete classifi-cation problem where an n-dimensional hyper-plane separates the input vectors into, in the simplest case, two distinct classes.
N06-1029@@Tone and intonation play a crucial role across manylanguages. A multi-level modelfor recognition of intonation labels. Journal ofthe Acoustical Society of America, 111.C.X.
N06-1030@@The  construction  of  speech-to-speech  translationsystems is difficult, complex, and prohibitively ex-pensive for all but handful of major languages. For  example,  inSpanish the letter c may generate K (65%), or THwhen followed by e or i (32%), or CH when fol-lowed by h (3%). It would be good if a tight ceiling could be estimat-ed from partial data in order to know (and report tothe lexicon builder) that  with  n rules defined thesystem is m percent complete.
N06-1031@@Syntax-based MT of-fers the potential advantages of enforcing syntax-motivated constraints in translation and capturinglong-distance/non-contiguous dependencies. In step 4, the output 2,500-sentence n-best list is reranked using an n-gram language modeltrained on 800M words of English news text. The third column givesthe BLEU score along with an indication whether itis a statistically significant increase (s), a statisti-cally significant decrease (t), or neither () internalannotation and external annotation..  Internal AnnotationInternal annotation reveals information about anode and its descendants to its surrounding nodes(ancestors, sisters, and other relatives) that is other-wise hidden.
N06-1032@@Phrase-based translationwith multi-word units excels at modeling local or-dering and short idiomatic expressions, however, itlacks a mechanism to learn long-distance dependen-cies and is unable to generalize to unseen phrasesthat share non-overt linguistic information. % overallin a hybrid system. Automatic evaluation of ma-chine translation quality using n-gram co-occurrence statis-tics.
N06-1034@@More for-mally, a set of interaction parameters are measuredin a spoken dialogue system corpus, then used ina multivariate linear regression to predict the targetperformance variable. Spoken Language Processing (ICSLP).A. 0Hereafter, predictors in a model are not highly correlated(R  Linear regression does not assume thatpredictors are independent, only that they are not highly corre-lated.
N06-1035@@A host of issues confront spoken dialogue systemdesigners, such as choosing the best system action toperform given any user state, and also selecting theright features to best represent the user state. onK&R in Practical Dialogue Systems.E. Informally, thedialogue follows a question-answer format.
N06-1037@@ Relation extraction is a subtask of information ex-traction that finds various predefined semantic re-lations, such as location, affiliation, rival, etc., between pairs of entities in text. Generally, we can represent a parse tree T by a vector of inte-ger counts of each sub-tree type (regardless of its ancestors):  ( )T Thus it is computational infeasible to directly use the feature vector ( )T Relation Feature Spaces of the Example Sentence  , where the phrase type E1-O-PER denotes that the current phrase is the 1st entity, its entity type is PERSON and its mention level is NOMIAL, and likewise for the other two phrase types E2-O-GPE 291tational issue, we introduce the tree kernel function which is able to calculate the dot product between the above high dimensional vectors efficiently. where N1 and N2 are the sets of all nodes in trees T1 and T2, respectively, and Ii(n) is the indicator func-tion that is 1 iff a sub-tree of type i occurs with root at node n and zero otherwise.
N06-1041@@Learning, broadly taken, involves choosing a goodmodel from a large space of possible models. (c) is a potential over a clique c, taking theform exp{T f(c)}, and f(c) is the vector of fea-tures active over c. In our sequence models, thecliques are over the edges/transitions (yi1, yi) andnodes/emissions (xi, yi). Tagging english text with a proba-bilistic model.
N06-1042@@Morphological disambiguation is the task of select-ing the correct morphological parse for a given wordin a given context. In Proceedings ofthe Workshop on Linguistically Interpreted Corpora,EACL 99, Bergen, Norway.Oflazer, K. and Kuruoz, I. Tagging and morpho-logical disambiguation of turkish text. A maximum entropy model forpart-of-speech tagging.
N06-1043@@Itis well-known that this method also minimizes thecross-entropy between the probability distributioninduced by the tree bank, also called the empiricaldistribution, and the tree probability distribution in-duced by the estimated grammar.In this paper we generalize the maximum like-lihood method, proposing an estimation techniquethat works on any unrestricted tree distribution de-fined over an infinite set of trees. N is the start symbol and R is afinite set of rules. log pG(t).
N06-1045@@A useful tool in natural language processing taskssuch as translation, speech recognition, parsing, etc.,is the ranked list of results. We remove cycles from each355Algorithm 1: Weighted Determinization of Tree AutomataInput: BOTTOM-UP TREE-TO-WEIGHT TRANSDUCER .Output: SUBSEQUENTIAL BOTTOM-UP TREE-TO-WEIGHT TRANSDUCER .begin123PRIORITY QUEUE456ENQUEUE7while do8v head9v10for each v such that do11if such that then12s.t. Interactivelyexploring a machine translation model.
N06-1046@@The task captures a mechanism for merg-ing together two or more linguistic structures intoa single sentence. Spot:A trainable sentence planner. In R. Dale,C.
N06-1048@@The TREC Definition and Relationship questionsare evaluated on the basis of information nuggets,abstract pieces of knowledge that, taken together,comprise an answer. Wedid not explore using n-gram idfs. A tf componentis not meaningful because the data are so sparse.
N06-1049@@The field of question answering has been movingaway from simple factoid questions such as Whoinvented the paper clip to more complex informa-tion needs such as Who is Aaron Copland andHow have South American drug cartels been usingbanks in Liechtenstein to launder money , whichcannot be answered by simple named-entities. Evalu-ating summaries and answers: Two sides of the samecoin Automaticevaluation of summaries using n-gram co-occurrencestatistics. :)subjected to a weight.
N06-1050@@We present a methodology for creating a test collec-tion of scientific papers that is based on the Cran-field 2 methodology but uses a current conference asthe main vehicle for eliciting relevance judgementsfrom users, ie, the authors.Building a test collection is a long and expensiveprocess but was necessary as no ready-made test col-lection existed on which the kinds of experimentswith citation information that we envisage could berun. In UK e-Science All Hands Meeting.Oliver McBryan. Similar trends were observed with otherevaluation measures, eg, MAP and R-precision in-creased to 0.
N06-1052@@In this approach, queries and documentsare assumed to be sampled from hidden generativemodels, and the similarity between a document anda query is then calculated through the similarity be-tween their underlying models.Clearly, good retrieval performance relies on theaccurate estimation of the query and document mod-els. Given a query and a docu-ment, we would first compute an estimate of the cor-responding query model ( Q) and document model( D), and then score the document w.r.t. serves to balance the confidence be-tween d and its neighborhood N(d) in the model es-timation step.
N06-1054@@Many tasks in natural language processing involvesequence labeling. FSAs are more powerful than n-gram mod-els. We thengive a brute-force decoding algorithm (3).
N06-1055@@So far, however, most of the research effortshave focused on analyzing the predicate-argumentstructure of verbs, largely due to absence of an-notated data for other predicate types. It proves to be a good feature for nominalpredicates as well. In other words, the best the system cando is to correctly label all arguments that have a con-stituent with the same text span in the parse tree.all corep (%) r(%) f(%) f(%)49.
N06-1056@@Recent work on natural language understanding hasmainly focused on shallow semantic analysis, suchas semantic role labeling and word-sense disam-biguation. To replenishthe removed links, links from the most probable re-verse alignment, a (obtained by treating the sourcelanguage as target, and vice versa), are added to a, aslong as a remains n-to-1, and v(a) is not increased. S. Zettlemoyer and M. Collins.
N06-1058@@The use of automatic methods for evaluatingmachine-generated text is quickly becoming main-stream in natural language processing. Boostexter: A boosting-based system for text categorization. 4) beginning at position i  2 and endingat position i + 2.
N06-1060@@ to the problem Personal names are problematic for all language technology that processes linguistic content, espe-cially in applications such as information retrieval, document clustering, entity extraction, and transla-tion. With two strings, string s of size m and string t of size n, the algorithm has O(nm) time and space complexity. It combines a Soundex style algo-rithm with Levenshtein by replacing the e(si,tj) function of Levenshtein with a function r(si,tj).
N06-1061@@General clustering algorithms are thenapplied to these vectors to cluster the given corpus. It requires e , the number of clus-ters, as input, and partitions the data set into exactly eclusters. Therefore eachsubtree represents a cluster.
N06-2001@@Their main drawbacks arecomputational complexity and the fact that only dis-tributional information (word context) is used togeneralize over words, whereas other word prop-erties (eg spelling, morphology etc.) Class-based n-gram modelsof natural language. We thus define a factoredneural language model (FNLM) (Fig.
N06-2002@@  Of the 6,000 living languages in the world only a handful have the necessary monolin-gual or bilingual resources to build a working statistical or example-based ma-chine translation system. Haspelmath, Martin and Matthew S. Dryer,       David Gil, Bernard Comrie, editors. Comrie, Bernard and N. Smith.
N06-2009@@This analysis of the input questionaffects the subset of documents that will be exam-ined and ultimately plays a key role in determiningthe answers the system chooses to produce. Exploiting paraphrases in a question answering system. We obtained a baseline MRR (top fiveanswers) of 0.
N06-2010@@Although the natural language processing community hastraditionally focused largely on text, face-to-face spokenlanguage is ubiquitous, and offers the potential for break-through applications in domains such as meetings, lec-tures, and presentations. Thus, we include two features each, eg, J isPRONOUN and I is PRONOUN, indicating respectivelywhether the candidate anaphor and candidate antecedentare pronouns. Towards a model of face-to-facegrounding.
N06-2013@@Approaches to statistical machine translation (SMT)are robust when it comes to the choice of their in-put representation: the only requirement is consis-tency between training and evaluation. Portage: A Phrase-based MachineTranslation System. ofthe Conference on Language Resources and Evaluation.S.
N06-2014@@In natural language understanding research withdata-driven techniques, data labeling is an essentialbut time-consuming and costly process. rj)hj(x) + rjg(x)(1)where hj(x) is the likelihood of x generated byclass j in the labeled data, g(x) is the distributionof unlabeled data, and rj is the relative proportionof unlabeled data compared to the labeled data forclass j. F is defined as 2prp+r where p ismacro precision and r is the macro recall.Method Acc F Neg recallSVM 85.
N06-2015@@ Many natural language processing applications could benefit from a richer model of text meaning than the bag-of-words and n-gram models that cur-rently predominate. I. Niles and A. Pease. Young, and R. Grishman.
N06-2017@@As most literature in text linguistics argues, afelicitous text should be coherent which meansthat the content has to be organised in a waythat makes the text easy to read and comprehend.The easiest way to demonstrate this claim isby arbitrarily reordering the sentences that anunderstandable text consists of. Centering: a parametrictheory and its instantiations. 4The classification rate is computed according to theformula Better(M,GSO) + Equal(M,GSO)/2.
N06-2018@@ Named-entity recognition is one of the most ele-mentary and core problems in biomedical text min-ing. Given an input se-quence o, the state sequence set S is a finite set. In-stead we implement N-best Viterbi search to find the N state sequences with the highest probabilities.
N06-2020@@LSA is based on the idea of as-sociation of elements (words) with contexts and similarity in word meaning is defined by similarity in shared contexts. The overall average sil-houette value is simply the average of the S(i) for all points in the whole dataset. In this paper we describe a first set of experiments investigating the tight-ness, separation and purity properties of sense-based clusters.
N06-2021@@More effective information access is beneficial to dealwith the increasing amount of broadcast news speech.Many attempts have been made in the past decade to buildnews browser, spoken document retrieval system, andsummarization or question answering system to effec-tively handle the large volume of news broadcast speech(eg, the recent DARPA GALE program). (O) is the normalization term, functionsgk(Ri, O) are indicator functions weighted by , and k isused to indicate different features. 1or not a particular anchor is talking.
N06-2023@@The goal of single document text or speech sum-marization is to identify information from a textor spoken document that summarizes, or conveysthe essence of a document. of ICSLP, Lisbon, Portugal.Ohtake K., Yamamoto K., Toma y., Sado S., Ma-suyama S. Newscast Speech Summarization via Sen-tence Shortening Based on Prosodic Features Proc. We built a12XN matrix using these features for N sentencesin the story where N was the total length of thestory.
N06-2024@@Named entity recognition (NER) is the task of iden-tifying named entities in free texttypically per-sonal names, organizations, gene-protein entities,and so on. However, in the context ofthis paper we focus on precision-recall trade-off in the generalcase, avoiding settings E.g, the token-level F2 curve peaks at  The y-axis gives F in terms of . that optimizes a learnedextractor with respect to a specific evaluation met-ric.
N06-2025@@ThePT kernel is a new function that we have designedto generate larger substructure spaces. In the testing phase,we selected the class associated with the maximumSVM score.For the ST, SST and PT kernels, we found that thebest  Kernel running time experimentsTo study the FTK running time, we extracted fromthe Penn Treebank several samples of 500 trees con-taining exactly n nodes. This means that the worst case com-plexity of the PT kernel is O(p2|NT1 ||NT2 |), where is the maximum branching factor of the two trees.Note that the average  in natural language parsetrees is very small and the overall complexity can bereduced by avoiding the computation of node pairswith different labels.
N06-2026@@Moving to-wards a shallow semantic level of representation hasimmediate applications in question-answering andinformation extraction. Because the his-tory representation computed for the move i  1is included in the inputs to the computation of therepresentation for the next move i, virtually any in-formation about the derivation history could flowfrom history representation to history representationand be used to estimate the probability of a deriva-tion move. First,we show that it is possible to build a single inte-grated system successfully.
N06-2027@@People who suffer from severe language impair-ments lack the ability to express themselves throughnatural usage of language and cannot achieve var-ious forms of communication. WORDNET: a lexical database forEnglish. Kipper, H. Trang Dang, and M. Palmer.
N06-2031@@Current dialogue systems overlook an interestingfact of language-based communication. 0.  and n + 0.  secondsbefore the target. Toward a mech-anistic psychology of dialogue.
N06-2038@@Simple target struc-tures define just a number of slots to be filledwith a string extracted from a text (slot filler).For this simple kind of information extraction,statistical approaches that model IE as a tokenclassification task have proved very successful.These systems split a text into a series of to-kens and invoke a trainable classifier to decidefor each token whether or not it is part of a slotfiller of a certain type. Begin/End (BE )tagging requires n + 1 classes for each of thetwo classifiers (B-type + O for the first, E-type+ O for the second). Thus moststrategies could already reach an accuracy of 93% by al-ways predicting the O class.
N06-2040@@In all these applications, the human partnerreceives navigation instructions from a system. In Proceedings of ISCA Tutorial and Re-search Workshop on Multi-Modal Dialogue in Mobile En-vironments.E. Multimodal dialog in a pedestrian navi-gation system.
N06-2046@@ Automated multi-document summarization has drawn much attention in recent years. 5.T~M e&Note that given a link between a sentence pair of si and sj, if si and sj comes from the same document,the link is an intra-document link; and if si and sjcomes from different documents, the link is an in-ter-document link. Automatic Evaluation of Summa-ries Using N-gram Co-occurrence Statistics.
N06-2049@@Under the scheme, each character of aword is labeled as B if it is the first character of amultiple-character word, or O if the character func-tions as an independent word, or I (whole)/O (north)/B (capital)/I (city)/I.We found that so far all the existing implementationswere using character-based IOB tagging. 2 the results of IOB tagging were re-evaluated.A confidence measure threshold, t, was defined for mak-ing a decision based on the value. A new OOV was thus created.
N06-2050@@ Spontaneous conversations are a very important type of speech data. Preliminaries to a Theory of Speech Disfluencies. We use f-score and the ROUGE score as evaluation metrics.
N06-4004@@Parallel text alignment procedures attempt to iden-tify translation equivalences within collections oftranslated documents. The entire parallel text is split into parts.During each E-step , statistics are collected paral-lel over each part, while in the M-steps, these statis-tics are merged together to update model parame-ters for next iteration. A program foraligning sentences in bilingual corpora.
N06-4006@@Such corpora consist of texts (eg documents, ab-stracts, or sentences) and annotations that associate structured information (eg POS tags, named enti-ties, shallow parses) with extents of the texts. Instances corre-spond to individuals of a class. 2 Related work There exists a plethora of manual text annotation tools for creating annotated corpora.
N06-4007@@Word sense and name discrimination are problemsin unsupervised learning that seek to cluster the oc-currences of a word (or name) found in multiple con-texts based on their underlying meaning (or iden-tity). 1) (2)When this ratio approaches 1, the clustering hasreached a plateau, and increasing k will have nobenefit. In Proceedings of the Conferenceon Computational Natural Language Learning, pages4148, Boston, MA.S.
N06-4009@@ We will demonstrate SconeEdit, a new tool for exploring and editing knowledge bases that inte-grates domain text. Consider the sentence Washington, D.C. is a city. We plan to leverage Scones rea-soning ability, along with SconeEdits document-driven design, to select which nodes are likely to be relevant to the user in the context of the loaded document(s).
N06-4010@@Although our Japanese ex-periments were applied retrospectively, the resultswould have placed us in the mid-range of partici-pating systems. In Proceedings ofthe 14th Text Retrieval Conference.E.W.D. A Speech Interface forOpen-domain Question-answering.
N07-1001@@Prosody refers to intonation, rhythm and lexicalstress patterns of spoken language that convey lin-guistic and paralinguistic information such as em-phasis, intent, attitude and emotion of a speaker.Prosodic information associated with a unit ofspeech, say, syllable, word, phrase or clause, influ-ence all the segments of the unit in an utterance. Bangalore and A. K. Joshi. The f0, RMSenergy (e) of the utterance along with features for5Pitch accent Boundary toneCorpus Speaker Set Model Acoustics Acoustics+syntax Acoustics Acoustics+syntaxEntire Set Maxent acoustic model 80.09 84.
N07-1004@@Making decisions is an important aspect of conver-sations in collaborative work. Stanford,and E. Tabassi. (10) A: Okay , thats a fair evaluation.
N07-1006@@Evaluation has long been a stumbling block in thedevelopment of machine translation systems, due tothe simple fact that there are many correct trans-lations for a given sentence. approach, where a a single metric evaluatesboth fluency and adequacy. Consider the Chinese-English translation example below:Source: wo bu neng zhe me zuoHypothesis: I must hardly not do thisReference: I must not do thisIt is clear that the word not in the MT output can-not co-exist with the word hardly while maintain-ing the meaning of the source sentence.
N07-1007@@ Generation of grammatical elements such as in-flectional endings and case markers is an impor-tant component technology for machine transla-tion (MT). S: proposed system is bet-ter; B: baseline is better; E: of equal quality  55we can see that the raw agreement rate between the two annotators (ie, number of agreed judg-ments over all judgments) is only 63% (27+9+27 /100) in fluency and 62% (17+9+36/100) in ade-quacy. These variations differ from t only in their case markers.
N07-1008@@tL], by finding the mostlikely translation given by:T  The inventory of blocks in cur-rent systems is highly redundant. (S|T ) (2)This is the simplest MaxEnt1 model that uses twofeature functions. A maximum entropy/minimumdivergence translation model.
N07-1010@@Coreference resolution aims to find multiple mentionsof an entity (eg, PERSON, ORGANIZATION) in adocument. Et} is non-zero; or counting how many non-zero members thereare in {g(1)k (e, mt) : e  Both are simple op-erations that can be carried out quickly. is also a binary indicatorfunction.
N07-1011@@Noun phrase coreference resolution is the problemof clustering noun phrases into anaphoric sets. In the Pairwise Model, thenumber of possible y variables is O(|x|2), wherex is the set of noun phrases. The algorithm is as follows: Given initial pa-rameters , perform greedy agglomerative cluster-ing on training document i until an incorrect clusteris formed.
N07-1014@@Since then, a significant amount of research in the area of quantitative lin-guistics has been devoted to the question how this property emerges and what kind of processes gen-erate such Zipfian distributions. With a new word probability (1-s), an existing edge is followed from the current vertex to the next vertex according to its weight: the probability of choosing endpoint X from the endpoints of all outgoing edges from the current vertex C is given by   (0,1), a new word is generated by the word generator model, and a next word is chosen from the word graph in proportion to its weighted indegree: the probability of choosing an existing vertex E as successor of a newly generated word N is given by . In Proceedings of the Symposium on Applications of Communications Theory, London M. Matsumoto and T. Nishimura.
N07-1015@@An important information extraction task is relationextraction, whose goal is to detect and characterizesemantic relations between entities in text. Given a set of rela-tion instances {ri}, each labeled with a type ti  T ,where T is the set of predefined relation types plusthe type nil, our goal is to learn a function that mapsa relation instance r to a type t  Note that wedo not specify the representation of s here. Thesubgraph on the left represents a bigram feature.
N07-1016@@Web Information Extraction (WIE) sys-tems extract assertions that describe a rela-tion and its arguments from Web text (eg,(is capital of,D.C.,United States)). Step 4 re-quires time O(N log N) to sort the comparisonscores and perform one iteration of merges. For example, a WIE system mightextract (is capital city of,Washington,U.S.
N07-1018@@This paper presents two Markov chain MonteCarlo (MCMC) algorithms for inferring PCFGs andtheir parses from strings alone. for all pro-ductions r. We found that for  was reduced below this t be-gan to exhibit nontrivial structure. However, in our experiments with theSesotho data above we found that for the small val-ues of  necessary to obtain a sparse solution,theexpected rule count E[fr] for many rules r was lessthan 1.
N07-1019@@Recent interest in syntax-based methods for statis-tical machine translation has lead to work in pars-ing algorithms for synchronous context-free gram-mars (SCFGs). The matching represents the edges(xi, ypi(i)) produced from the input permutation .Let H be the union of G1, G2, and M . Explicit construction of concentra-tors from generalized n-gons.
N07-1020@@ Morphological analysis is the task of segmenting a word into morphemes, the smallest meaning-bearing elements of natural languages. A simpler, intuitive approach to morpheme induction. S. Dasgupta and V. Ng.
N07-1021@@Some re-search has moved towards a more comprehensiveview, eg construing the generation task as a singleconstraint satisfaction problem. The experiments below focused on a.m.forecasts of wind characteristics. S. Paiva and R. Evans.
N07-1022@@This paper explores the use of statistical machinetranslation (SMT) methods in natural language gen-eration (NLG), specifically the task of mappingstatements in a formal meaning representation lan-guage (MRL) into a natural language (NL), ie tacti-cal generation. (e) is thenormalizing factor. Paired t-tests wereused to measure statistical significance.
N07-1023@@Sentence compression addresses the problem of re-moving words or phrases that are not necessaryin the generated output of, for instance, summa-rization and question answering systems. Hedge: A parse-and-trim approach to headline generation. 85assessed the usefulness of lexical and POS anno-tation (setting s and v to 0).
N07-1025@@Ambiguity is inherent to human language. In Proceedings ofthe EACL Workshop on New Text, Trento, Italy.E. The average length of a paragraph is 80 words.
N07-1026@@Natural Language Processing (NLP) applicationsbenefit from the availability of large amounts of an-notated data. This procedure is oftensuboptimal: Euclidean distance relies on a model ofnormally distributed i.i.d. To do so, itdefines the nC hard labels matrix Y and the NCsoft labels matrix f , whose first n rows are identicalto Y .
N07-1028@@Query expansion has long been a focus of infor-mation retrieval research. However, handling suchlong queries is a challenge. The profile usu-ally consists of documents previously viewed, websites recently visited, e-mail correspondence and soon.
N07-1029@@In recent years, machine translation systems basedon new paradigms have emerged. A hierarchical phrase-based modelfor statistical machine translation. The con-fidence score assigned to each word was chosen tobe + ! +  O  where the  O was based on therank of the aligned hypothesis in the systems  -best.
N07-1030@@The task of coreference resolution involves impos-ing a partition on a set of entity mentions in a docu-ment, where each partition corresponds to some en-tity in an underlying discourse model. A model-theoretic coreference scoring scheme. The model is defined in a standard fashionas follows:PC(COREF|i, j) is a normalization factor over both out-comes (COREF and COREF).
N07-1032@@The problem of combining labeled and unlabeledexamples in a learning task (semi-supervised learn-ing) has been studied in the literature under variousguises. To do this, we iteratethrough all pairs (X,G) where X is a test exampleand G is a training newsgroup, in increasing orderby the angle between X and G. If X is not yet asso-ciated and G is not yet full, , 1.0,and we collect supervised error results E( Now, instead of using the single best The esti-mated probabilities are given byp (ie, it is 1 at the position of E(j))2/2) isthe value of the kernel which expresses the sim-ilarity between two clusterings of the same train-ing dataset, in terms of their errors. A of documents is partitioned into clustersC1, .
N07-1033@@Annotation cost is a bottleneck for many natural lan-guage processing applications. Fi+T ) (7)where Fj denotes the fold numbered j mod 9, andacc(Z | , Y ) means classification accuracy on theset Z after training on Y with hyperparameters .To evaluate whether two different training meth-ods A and B gave significantly different average-accuracy values, we used a paired permutation test(generalizing a sign test). A sequential algorithm fortraining text classifiers.
N07-1034@@Two different approaches have become popular forbuilding spoken dialogue systems. In Eurospeech, Lisbon Portugal.S. Com-putational Intelligence: a logical approach.
N07-1036@@Psycholinguistic experiments have shown that eyegaze is tightly linked to human language process-ing. Starker and R. A. Bolt. An entity e on the graphi-cal display is fixated by gaze fixation f if the area ofe contains the fixation point of f .
N07-1038@@For example,a restaurant review may express judgment on foodquality as well as the service and ambience of therestaurant. % compared to only a 1. The ith component of yt is the rankfor the ith aspect, and will be denoted by y[i]t. Thegoal is to learn a mapping from instances to ranksets, H : X  Ym, which minimizes the distancebetween predicted ranks and true ranks..
N07-1039@@Sentiment analysis, which seeks to analyze opin-ion in natural language text, has grown in interestin recent years. reaction experience negative unmarked neg36.0 Agfa ePhoto Smile (2)  Product Name Attitude Target Orient. Wordnet: A lexical database forEnglish.
N07-1040@@In the recent past, there has been a focus oninformation management from scientific litera-ture. between NP and CIT measured in para-graphs(d) Is CIT after NP in the discourse (cataphor) (e) Distance between CIT and the closest firstperson pronoun or this paper Contextual:(a) Rank of CIT (how many other reference listitems are closer)(b) Number of times CIT is cited in the paragraph(c) Number of times CIT is cited in the wholepaper Distance measures:(a) Dist.
N07-1041@@From business people to the everyday person, e-mail plays an increasingly central role in a modernlifestyle. (1) Document detection: Classify an e-mailas to whether or not it contains an action-item. will produce a reliable prediction.
N07-1042@@This paperpresents a statistical method which picks relation-ships which violate few constraints as measured bya probabilistic database model. In Proceedings ofICDL, pages 8594.E. r1..n,where Z is the partition function and corresponds tothe total probability of all database configurations.
N07-1043@@The study of semantic similarity between words hasbeen an integral part of natural language processingand information retrieval for many years. Resnik and N. A. Smith. of 15th International World WideWeb Conference.E.
N07-1044@@Word sense disambiguation (WSD), the ability toidentify the intended meanings (senses) of wordsin context, is crucial for accomplishing many NLPtasks that require semantic processing. Determining wordsense dominance using a thesaurus. Wecan now approximate the frequency with which aword w1 occurs with the sense s by computing itssynonym frequencies: for each word w2  syns(s),the set of synonyms of s, we field a query of the formw1 AND w2.
N07-1048@@In these languages, the num-ber of possible word forms is very large becauseof many productive morphological processes; wordsare formed through extensive use of, eg, inflection,derivation and compounding (such as the Englishwords rooms, roomy, bedroom, which all stemfrom the noun room ).For some languages, language modeling based onsurface forms of words has proven successful, or atleast satisfactory. AT&T Labs Research. NAACL-HLT, New York, USA.O.-W. Kwon and J. Korean large vocabulary con-tinuous speech recognition with morpheme-based recogni-tion units.
N07-1049@@A dependency parse tree encodes useful semantic in-formation for several language processing tasks. Itsbehavior can be described as repeatedly selectingand applying some parsing rules to transform itsstate.The state of the parser is represented by a quadru-389ple S, I, T,A : S is the stack, I is the list of (re-maining) input tokens, T is a stack of saved to-kens and A is the arc relation for the dependencygraph, consisting of a set of labeled arcs (wi, r, wj),wi, wj  W (the set of tokens), and d  D (theset of dependencies). Given an input sentence s,the parser is initialized to  In the experiments presented below, we used asfeatures the lemma, Part-of-Speech, and dependencytype of the following items: 2 top items from S; Down to the leftmost childd + + Down to the rightmost childd Description of the atomic movements allowed onthe graph relatively to a token w. 2 leftmost and 2 rightmost children from thetop of S and I .
N07-1050@@This means that an approach that starts by de-riving the best projective approximation of the cor-rect dependency structure is likely to achieve highaccuracy, while an approach that instead attemptsto search the complete space of non-projective de-pendency structures runs the risk of finding struc-tures that depart too much from the near-projectivenorm. The graph G is projective, ie, if i  The degree of an arc e  E is the number ofconnected components (ie, weakly connectedsubgraphs) in G(min(e)+1,max(e)1) that are notdominated by the head of e in G(min(e),max(e)). Since arcs are usedto represent dependency relations, we will say that iis the head and j is the dependent of the arc (i, j).The function L assigns a dependency type (label)r  j for the reflexiveand transitive closure of the arc relation E and thecorresponding undirected relation, respectively.Definition 2 A dependency graph G is well-formedif and only if:1.
N07-1051@@Treebank parsing comprises two problems: learn-ing, in which we must select a model given a tree-bank, and inference, in which we must select aparse for a sentence given the learned model. While these three methods yield409Objective P R F1 EXBEST DERIVATIONViterbi Derivation 89. Withthis limitation, unary chains are not a problem.
N07-1052@@Inference tasks in NLP often involve searching foran optimal output from a large set of structured out-puts. and weighting them via optimized s(r) andt(r), respectively. A synchronoustree projects monolingual phrase structure trees ontoeach sentence.
N07-1053@@In order to fully understand a piece of text, wemust understand its temporal structure. Sinceverb tense alone is inherently deictic, it is not suffi-cient to decide the direction, but we do add both theclosest verb (w.r.t. Software available at http://www.csientu.edu.tw/cjlin/libsvm.E.
N07-1054@@Relations such as Cause and Contrast, which we callrhetorical-semantic relations (RSRs), may be sig-naled in text by cue phrases like because or how-ever which join clauses or sentences and explicitlyexpress the relation of constituents which they con-nect (Example 1). That is, we used the M&E-stylemethod for mining instances, but we gathered themfrom the PDTB corpus. A new statistical parser based on bigramlexical dependencies.
N07-1055@@How-ever, they have trouble finding the beginning or endof the document, or recovering from sudden shifts intopic (such as occur at paragraph boundaries). ~S(i1), the set of all entities whichhave appeared before sentence i. However, consider the case where an en-tity has a history ~r h, and then does not appear inthe next sentence.
N07-1057@@Developing natural language applications is generallydependent on the availability of annotated corpora.Building annotated resources, however, is a signif-icantly time consuming process involving consider-able human effort. In Proceedingsof the e-Humanities Workshop, Amsterdam. (2) Si is a strict subset of Sj: we remove yjfrom the PS and promote its children: yjschildren will become children of yj s parent.
N07-1058@@An automatic measure of read-ability that incorporated both lexical and gram-matical features was thus needed. T. Cover and P. Hart. A nine-fold cross-validation was employed.
N07-1061@@How-ever, large parallel corpora do not exist for manylanguage pairs. The webas a parallel corpus. Weuse English e as the pivot language.
N07-1062@@The majority will remain on disk.The on-demand loading is employed on a per sen-tence basis, ie we load only the phrase pairs thatare required for one sentence into memory. Wetake one item from the stack and update the transla-tion options E(j, j) in line 4. 2.We use a similar notation for the prefix tree T withnodes 1, ..., k, ...,K. The outgoing edges of a treenode k are numbered with 1, ...,m, ...,Mk, ie anedge in the prefix tree is identified by a pair (k,m).The source word labeling the mth outgoing edge oftree node k is denoted as fTk,m and the successornode of this edge is denoted as sTk,m  (1)Let k0 denote the root node of the prefix tree andlet fk denote the prefix that leads to tree node k.Furthermore, we define E(k) as the set of possibletranslations of the source phrase fk.
N07-1063@@This achievessearch errors comparable to a strong Cube Pruning Synchronous Parsing for SMTProbabilistic Synchronous Context Free Grammar(PSCFG) approaches to statistical machine transla-tion use a source terminal set (source vocabulary)TS , a target terminal set (target vocabulary) TT anda shared nonterminal set N and induce rules of theformX  is a sequence of nonterminals and source ter-minals, (iii)  is a sequence of nonter-minals and target terminals, (iv) the number cnt( is equal to the num-ber cnt() of nonterminal occurrences in , (v) is a one-to-one mapping from nonterminal occurrences in  tononterminal occurrences in , and (vi) w  )is a non-negative real-valued weight assigned to therule. i wordsin the source sentence f1  fn, starting at positioni + 1, and have weight w (equivalent to P (D)), ande  is a sequence of target terminals,with possible elided parts, marked by . E+06Number of LM MissesModel CostCPH.Search0.
N07-1064@@The quality of machine translation (MT) is gener-ally considered insufficient for use in the field with-out a significant amount of human correction. For languages such as French and English,the first of these phases (tokenization) is mostly astraightforward process; we do not describe it anyfurther here.Decoding is the central phase in SMT, involv-ing a search for the hypotheses t that have high-est probabilities of being translations of the cur-rent source sentence s according to a model forP (t|s). PORTAGE: A Phrase-Based Machine Trans-lation System.
N07-1066@@Question answering aims at finding exact answersto a users natural language question from a largecollection of documents. In Proceedings of Cross-Language EvaluationForum.E. as a similarity threshold, respectively.
N07-1067@@As question-answering systems advance from han-dling factoid questions to more complex requests,they must be able to determine how much informa-tion to include while making sure that the informa-tion selected is indeed relevant. A Markov randomfield model for term dependencies. In Pro-ceedings of the Thirteenth Text Retrieval Conference.T.
N07-1069@@Correctly identifying semantic entities and success-fully disambiguating the relations between them andtheir predicates is an important and necessary stepfor successful natural language processing applica-tions, such as text summarization, question answer-ing, and machine translation. Instead, researchers typicallybuild a single model for the numbered arguments(Arg0, Arg1, . Recent studies have shown that lobsters pri-marily feed on live fish, dig for clams, seaurchins, and feed on algae and eel-grass.c.
N07-1070@@Semantic Role Labeling (SRL) is the pro-cess of producing such a markup. In generating the Brown training and558SRL SRL Task P R F ATrain Test (%) (%) (%)WSJ WSJ Id. However, there is a considerable dropin Classification accuracy (86.
N07-1071@@ Semantic inference is a key component for ad-vanced natural language understanding. PARAMETERS SELECTED FROM DEV SET SYSTEM RANKING STRATEGY  Indicates statistically significant results (with 95% confidence) when compared with all baseline systems using pairwise t-test. Preference Semantics: a family history.
N07-2001@@Recently, user simulation has been used in the de-velopment of spoken dialog systems. A higher percentage in-dicates the policy might be better trained with moretraining instances. Cor-rectness (correct(c), incorrect(ic)) is automaticallyjudged by the system1 and kept in the systems logs.Percent incorrectness (ic%) is also automaticallycalculated and logged.
N07-2002@@ Our work aims to the acquisition of deep gram-matical information for nouns, because having in-formation such as countability and complementa-tion is necessary for different applications, espe-cially for deep analysis grammars, but also for question answering, topic detection and tracking, etc. 7 and a recall of 0. The positive or negative re-sults of the n pattern checking are stored as binary values of a n dimensional vector, one for each oc-currence.
N07-2003@@  Conducting applied spoken language interface re-search is generally a costly endeavor. A sample interaction with ConQuest 1 S: Welcome to ConQuest [] You can also cast a vote for the best paper award. DiSCoH is built using AT&T technology and a call-routing approach; ConQuest relies on a plan-based dialog manage-ment framework (RavenClaw) and an open-source infrastructure (Olympus).
N07-2010@@ Recent advances in digital broadcasting and re-cording allow fans access to an unprecedented amount of sports video. A General Model of Action and Time. The data represents video of 9 different teams, at 4 different stadiums, broadcast on 4 different stations.
N07-2011@@Detecting and adapting to user affect is being ex-plored by many researchers to improve dialogue sys-tem quality. E.g., the binary variable for LAQhas 2 values: LAQ We then compute the 2 value be-tween the binary variables. Thearchitecture of Why2-Atlas: A coach for qualitativephysics essay writing.
N07-2012@@While there have been improvements and a significantnumber of methods introduced into the realm of dialog-based systems, there are aspects of these methods whichcan be further improved upon. Clearly, if Ankcan be computed in constant time, then F (s, f, k) canalso be computed in constant time. In Proceedings of IEEE ICASSP.R.
N07-2014@@Arabic is written without certain orthographic sym-bols, called diacritics, which represent among otherthings short vowels. Shadda is a consonant doubling dia-critic:  b. For instance,the final diacritics inkatabtu I wrote distinguish the person ofthe subject of the verb.
N07-2015@@This paper investigates the properties of large n-best lists in the context of statistical machine trans-lation (SMT). The highest n that fit into the 16GB machinewas 60 000. A smorgasbord of features for statisti-cal machine translation.
N07-2016@@We would liketo point out that since neither syntactic structures independency treebanks, nor structures arising in de-pendency parsing need a priori fall into any formalsubclass of dependency trees, we need means of de-scribing any non-projective structure. Here is a pseudocode ofan algorithm for fully determining all ill-nested sets:1: for all edges i jIts time complexity is obviously O(n2), since thecheck on line 3 can be implemented so as to takeconstant time (by precompuing  on demand for subtrees of the processededges, we preserve worst-case quadratic complexity. models linguistic dependency, and sorepresents a directed, rooted tree on V .
N07-2021@@Semantic parsing is the task of mapping a natu-ral language (NL) sentence into a complete, for-mal meaning representation (MR) which a computerprogram can execute to perform some task, likeanswering database queries or controlling a robot.These MRs are expressed in domain-specific unam-biguous formal meaning representation languages(MRLs). ConclusionsThis paper has presented a semi-supervised ap-proach to semantic parsing. Borland International, Scotts Valley, CA.O.
N07-2027@@In this paper, we use a linguistic rule-based method(LRBM) and a data-driven method (DDM) for tagg-ing text in the morphologically complex Icelandiclanguage.We present a novel LRBM. In E. Knig and J. Auw-era, editors, The Germanic Languages. In Proceedings of the 6th Conference on Appliednatural language processing, Seattle, WA, USA.S.
N07-2028@@In addition tomaximizing conditional likelihood of the availablelabels, ER also aims to minimize the entropy of thepredicted label distribution on unlabeled data. )L( (Y(u)|x(u)).This negative entropy term increases as the decisionboundary is moved into sparsely-populated regionsof input space. In practice, they approxi-mate the entropy of the labels given the N-best la-bels.
N07-2031@@ The original goals of the RH parser were to obtain accurate parses where (a) application speed was needed, and (b) large amounts of annotated mate-rial for a subject idiom were not available. A major "augmentation" is a preference-scoring component. Addi-tional goals that evolved were (c) that parses for particular documents could be brought to an almost arbitrary level of correctness for research purposes, by grammar correction, and (d) that information collected during parsing could be modified for an application with a modest amount of effort.
N07-2032@@Wikipedia (www.wikipedia.org) has emerged as theworlds largest online encyclopedia. The expanded tree is defined as core treeof r because it attempts to capture the clues for r. Steps toextract the core treeC of a relationship r from a sentences are described as follows:(i)] Initialize the core tree C as blank. Learning surfacetext patterns for a question answering system.
N07-2037@@The SMT has been formulated as a noisy channel model in which the target language sentence, e is seen as dis-torted by the channel into the foreign language f :)()|(argmax)|(argmaxeeePefPfePe  The overwhelming proportion of the SMT research has been focusing on im-proving the translation model. A Systematic Comparison of Various Statistical Alignment Models. Notice that the denominator includes a sum over all possible outcomes, o, which is essentially a normalization factor for probabilities to sum to 1.
N07-2038@@Bootstrapping Statistical Dialogue ManagersOne of the key advantages of a statistical approach to Dia-logue Manager (DM) design is the ability to formalise de-sign criteria as objective reward functions and to learn anoptimal dialogue policy from real dialogue data. A probabilistic framework fordialog simulation and optimal strategy learning. At any time t, the user is in a state S, takes ac-tion au, transitions into the intermediate state S, receivesmachine action am, and transitions into the next state S (1)Assuming a Markovian state representation, user be-haviour can be decomposed into three models: P (au|S)for action selection, P (S|au, S) for the state transitioninto S, and P (S for the transition into S. 1).The user agenda A is a stack-like structure containingthe pending user dialogue acts that are needed to elicit theinformation specified in the goal.
N07-2039@@Spoken dialogue systems are emerging as an effec-tive means for humans to access information spacesthrough natural spoken interaction with comput-ers. Marchand and R. I. Damper. Name pronunciation with ajoint n-gram model for bi-directional grapheme-to-phonemeconversion.
N07-2041@@Relation extraction from text is a step beyondNamed-Entity Recognition (NER) and generally de-mands adequate domain knowledge to build rela-tions among domain-specific concepts. Psortdb: A database of sub-cellular localizations for bacteria. (noisy data R 84.
N07-2044@@Communication is a significant quality-of-life issuefor individuals with severe speech impairments. Comparative analysis shows a 17. Similarly, users spent 0.
N07-2045@@Determiner placement (choosing if a noun phraseneeds a determiner, and if so, which one) is anon-trivial problem in several language processingtasks. (3)Next we describe how we assign a probability tothe expansion e of a constituent. (Deciding between a and anis a trivial postprocessing step.
N07-2047@@1 illustrates an example of phrase alignmentfor statistical machine translation (SMT). A general re-gression technique for learning transductions. The time complexityof a naive implementation of the blended n-spectrumstring kernel between two sentences si and sj isO(n|si||sj|), where || denotes the length of the sen-tence.
N07-2052@@Semantic similarity (SS) is typically defined via thelexical relations of synonymy (automobile  car)and hypernymy (vehicle  car), while semantic re-latedness (SR) is defined to cover any kind of lexi-cal or functional association that may exist betweentwo words. As a second strategy, weaverage over all category pairs. For the Les measure, we givethe results for considering: (i) only the first para-graph (+First) and (ii) the full text (+Full).
N07-3001@@Compounds are semantic units containing at leasttwo content-bearing morphemes. The tagger tagsmaximal length strings from six subdomains of theSwedish MeSH: A: Anatomy, B: Organisms, C: Dis-eases, D: Chemicals and Drugs, E: Analytical, Diag-nostic, and Therapeutic Techniques and Equipment,and F: Psychiatry and Psychology. InSwedish, compounding is a very productive mor-1phological process.
N07-3002@@Subsequent research began tofocus more on conditional models of parse structuregiven the input sentence, which allowed discrimi-native training techniques such as maximum con-ditional likelihood (ie maximum entropy) Another unexploited connec-tion is that probabilistic approaches pay closer at-tention to the individual errors made by each compo-nent of a parse, whereas the training error minimizedin the large margin approachthe structured mar-gin loss I have addressed both of theseissues, as well as others in my work. of AAAI, pages 598603.E. A maximum entropy inspired parser.
N07-3003@@The last decade has seen statistical techniques forNatural Language Processing (NLP) gaining thestatus of standard approaches to most NLP tasks.While advances towards robust statistical inferencemethods (cf. They also have a limitedand arbitrary coverage. We showed (1) howto retrieve Wikipedia articles from textual queriesand resolve ambiguous queries based on the arti-cles link structure; (2) compute semantic related-ness as a function of the articles found and the pathsbetween them along the categorization graph (Fig-ure 1).
N07-3007@@At the present time, the Spoken Language Sys-tems Lab (L2F) integrates a project in the Houseof the Future helps the user in daily tasks thatdeal with devices and services, through speech com-mands. Usingmore relations is still a challenge as the complexityincreases. What Have On-tologies Ever Done For Us: Potential Applications ata National Mapping Agency, volume 188.Thomas R. Gruber.
P00-1001@@Language is shaped not only bygrammar, but also by the cognitive processing ofspeakers and addressees, and by the medium inwhich it is used. Move to the uh, orange squareUtterances c, d, and e were spontaneousdisfluencies, and f, g, and h were edited versionsthat replaced the removed material with pausesof equal length to control for timing. A model ofspeech repairs and other disruptions.
P00-1009@@Data-Oriented Parsing (DOP) models learn howto provide linguistic representations for anunlimited set of utterances by generalizing from agiven corpus of properly annotated exemplars.They operate by decomposing the givenrepresentations into (arbitrarily large) fragmentsand recomposing those pieces to analyze newutterances. "A Method forDisjunctive Constraint Satisfaction", in M.Tomita (ed. According to paired t-tests alldifferences in accuracy were statisticallysignificant.
P00-1010@@The extraction of temporal information fromnews offers many interesting linguisticchallenges in the coverage andrepresentation of temporal expressions. Wilson, I. Mani, B. Sundheim, and L. Ferro.Some Conventions for Temporal Annotation of A Logical Approach to NarrativeUnderstanding.
P00-1014@@Prepositional phrase attachment is a commonsource of ambiguity in natural languageprocessing. Mary ate the salad with a fork. Contextually Similar WordsThe contextually similar words of a word w arewords similar to the intended meaning of w i  itscontext.
P00-1015@@Finding simple and non-recursive base NounPhrase (baseNP) is an important subtask formany natural language processing applications,such as partial parsing, information retrieval andmachine translation. F, E and I mean respectivelythat the word is the left boundary, rightboundary of a baseNP, or at another positioninside a baseNP. Determining the N best POSsequencesThe goal of the algorithm in the 1st pass is tosearch for the N-best POS-sequences within thesearch space (POS lattice).
P00-1020@@ Empirical methods are critical to gauge the scalability and robustness of proposed approaches, to assess progress and to stimulate new research questions. This measure is proportional to two factors: (A) the weight of the objective  o is an objective, e is an entity, refo is an ancestor of o in the value tree  w(o,refo) is the product of the weights of all the links from o to refo  vo is the component value function for leaf objectives (ie, attributes), and it is the recursive evaluation over children(o) for nonleaf objectives Given a measure of an objectives strength, a predicate indicating whether an objective should be included in an argument (ie, worth mentioning) can be defined as follows:  s-notably-compelling The constant k In the definition of s-notably-compelling, the constant k determines the lower bound of s-compellingness for an objective to be included in an argument. Acknowledgements Our thanks go to the members of the Autobrief project: S. Roth, N. Green, S. Kerpedjiev and J. Mattis.
P00-1022@@In this paper, we focus specifically on theresolution of a linguistic problem for Spanishtexts, from the computational point of view:zero-pronouns in the subject Therefore, the aim of this paper is notto present a new theory regarding zero-pronouns, but to show that other algorithms,which have been previously applied to thecomputational resolution of other kinds ofpronoun, can also be applied to resolve zero-pronouns.The resolution of these pronouns isimplemented in the computational system calledSlot Unification Parser for Anaphora resolution(SUPAR). For example, it would include punctuation markssuch as a semicolon. For pronouns and zero-pronouns, theantecedents in the four previous sentences, areconsidered.The following restrictions are first applied tothe list of candidates: person and numberagreement, c-command4 constraints andsemantic consistency5.
P00-1029@@In this paper we present an approach to un-supervised learning and automatic detectionof syllable structure. For ex-ample, class #14 represents mostly stressedsyllables involving the vowels /eI, A:, e:, O:/and others, in a variety of syllable positions innouns, adjectives or verbs. Computer Speech andLanguage, 13:155176.A.
P00-1031@@Chinese input method is one of the mostdifficult problems for Chinese PC users. Statistical Methods forSpeech Recognition, The MIT Press, Cambridge,Massachusetts.William E. Cooper. For example people in the southernarea of China do not distinguish zh-z,sh-s, ch-c, ng-n, etc..
P00-1037@@These models consist of twocomponents: a source model and a channelmodel. When we allowgeneric edit operations, the complexityincreases to O(|s|2*|w|2). Ourapproach will be to return an n-best list ofcandidates according to the error model, andthen rescore these candidates by taking intoaccount the source probabilities.We are given a dictionary D and aset of parameters P, where each parameter is3   IRU VRPH * Forcomputing the Damerau-Levenshteindistance between two strings, this can bedone in O(|s|*|w|) time.
P00-1038@@An important distinction in document summarizationis between generic summaries, which capture the cen-tral ideas of the document in much the same way thatthe abstract of this paper was designed to distill itssalient points, and query-relevant summaries, whichreflect the relevance of a document to a user-specifiedquery. Journal of the Royal Statistical Society,39B:138.E. Ph.D. thesis, University ofMassachusetts at Amherst.S.
P00-1039@@This paper presents an algorithm for textsummarization using the thematic hierarchyof a long text, especially for use by readerswho want to skim an electronic book of sev-eral dozens of pages on a computer display.For those who want an outline to quicklyunderstand important parts of a long text,a one-page summary is more useful than aquarter-size summary, such as that gener-ated by a typical automatic text summa-rizer. Thematic hierarchy detec-tion of a text using lexical cohesion. (c) Among the sentences in the B.S.C.
P00-1041@@Generating effective summaries requires the abil-ity to select, evaluate, order and aggregate itemsof information according to their relevance toa particular subject or for a particular purpose.Most previous work on summarization has fo-cused on extractive summarization: selecting textspans either complete sentences or paragraphs e-mail: vmit-tal@parc.xerox.com; Michael Witbrocks initial work onthis system was performed whilst at Just Research.then arranged in a linear order (usually the sameorder as in the original document) to form a sum-mary document. (b) System generated output using a lexical + positionalmodel. Tipster text phase III 18-month workshop notes, May.Fairfax, VA.Michael Witbrock and Vibhu O. Mittal.
P00-1043@@Vast amounts of textual documents and data-bases are now accessible on the Internet and theWorld Wide Web. Co-reference resolution is impor-t nt for two reasons. Syntactic structure of a sentencegiven within parentheses.
P00-1045@@Both in terms of speed and memory consump-tion, graph unification remains the most ex-pensive component in unification-based gram-mar parsing. However,Pereiras mechanism incurs a log(n) overheadfor accessing the changes (where n is thenumber of nodes in a graph), resulting inan O(n logn) time algorithm. Unification is a well known algo-rithm.
P00-1052@@The task of evaluating students writ-ing ability has traditionally been a labor-intensive human endeavor. The extensive use of I in thesubject position produced an unwanted eectof high coherence. A Centering approach to pronouns.
P00-1054@@ Many machine translation systems have been developed and commercialized. Our proposal depends on a handcrafted thesaurus. T in the experiment was 1/2.
P00-1057@@An aim of one strand of research in gener-ative grammar is to nd a formalism thathas a restricted descriptive capacity sucientto describe natural language, but no morepowerful than necessary, so that the reasonssome constructions are not legal in any nat-ural language is explained by the formalismrather than stipulations in the linguistic the-ory. Fuse all the elementary tree sets of thegrammar by identifying the upper foot  i.with the lower root. In most TAG-basedanalyses, bridge verbs adjoin at S (or C0), andraising verbs adjoin at VP (or I0).
P00-1058@@Why use tree-adjoining grammar for statisti-cal parsing Given that statistical natural lan-guage processing is concerned with the proba-ble rather than the possible, it is not becauseTAG can describe constructions like arbitrar-ily large Dutch verb clusters. Eectively, a dependency structure is madeparasitic on the phrase structure so that theycan be generated together by a context-freemodel.However, this solution is not ideal. Inducing a stochastic grammarfrom the Treebank4.
P00-1061@@Stochastic parsing models capturing contex-tual constraints beyond the dependencies ofprobabilistic context-free grammars (PCFGs)are currently the subject of intensive research.An interesting feature common to most suchmodels is the incorporation of contextual de-pendencies on individual head words into rule-based probability models. Iterating fur-ther shows a clear overtraining eect. If we observe incomplete datay 2 Y, the expectation q[] is replaced by the condi-tional expectation ~p[k0[]] given the observed data yand the current parameter value 0..  Class-Based LexicalizationOur approach to grammar lexicalization isclass-based in the sense that we use class-based estimated frequencies fc(v; n) of head-verbs v and argument head-nouns n in-stead of pure frequency statistics or class-based probabilities of head word dependen-cies.
P00-1065@@Identifying the semantic roles lled by con-stituents of a sentence can provide a level ofshallow semantic analysis useful in solving anumber of natural language processing tasks.Semantic roles represent the participants inan action or relationship captured by a se-mantic frame. A shal-low semantic level of representation is a moredomain-independent, robust level of represen-tation. Cur-rent information extraction systems often usedomain-specic frame-and-slot templates toextract facts about, for example, nancialnews or interesting political events.
P00-1067@@With the rapid development of the Internet,writing English becomes daily work forcomputer users all over the world. A Unified Approach to StatisticalLanguage Modeling for Chinese. InProceedings of the 31st Annual Conference of theAssociation for Computational Linguistics, 1-8,Columbus, OH.Dagan, I., K.W.
P00-1069@@The objective of word sense disambiguation(WSD) is to identify the correct sense of aword in context. One advantageof this method is that it eectively performsWSD with only a small number of labeled ex-amples and thus shows possibility of buildingword sense disambiguators for the languageswhich have no sense-tagged corpus.The rest of this paper is organized as fol-lows. First of all, the trainingset L(1)j(1  j  M) of labeled examples isconstructed for each base classier Cj.
P00-1073@@Inparticular, n-gram language model (LM) hasbeen demonstrated to be highly effective forthese domains. Miller, T. Leek, R. M. Schwartz, A hiddenMarkov model information retrieval system, inProc. Itthen results in a relatively domain-independentlanguage model.
P01-1003@@Language modeling is an important component incomputational applications such as speech recog-nition, automatic translation, optical characterrecognition, information retrieval etc. Wesmoothed it using a gaussian prior technique. However, the n-gram modelonly uses the information provided by the last).
P01-1004@@Translation retrieval(TR) is a description of this process of selectingfrom the TM a set of translation records (TRecs)of maximum L1 similarity to a given input. A Case Study on Memory BasedMachine Translation Tools. of the 5th InternationalConference on Theoretical and Methodological Is-sues in Machine Translation (TMI-93), pages 4757.E.
P01-1005@@ Machine learning techniques, which automatically learn  linguistic information from online text corpora, have been applied to a number of natural language problems throughout the last decade. Brill, E. and Wu, J. Classifier combination for improved lexical disambiguation. After the N models have been trained and run on the same test set, their classifications for each test sentence can be compared for classification agreement.
P01-1006@@The evaluation of any NLP algorithm or systemshould indicate not only its efficiency orperformance, but should also help us discoverwhat a new approach brings to the current stateof play in the field. In Simon Botleyand Antony Mark McEnery, editors, Corpus-based and Computational Approaches to DiscourseAnaphora, Studies in Corpus Linguistics, chapter 8,pages 145  John Benjamins PublishingCompany.Niyu Ge, J. Hale, and E. Charniak. In R. Mitkov and B. Boguraev, editors,Operational factors in practical, robust anaphoraresolution for unrestricted texts, pages 38  Automaticprocessing of large corpora for the resolutionof anaphora references.
P01-1008@@Paraphrases are alternative ways to convey thesame information. This provides a basis fora bootstrapping mechanism. There are a num-ber of context representations that can be con-sidered as possible candidates: lexical n-grams,POS-ngrams and parse tree fragments.
P01-1009@@Each consists of a query, a re-sponse (not shown), and then a follow-up query. A Structure for Plansand Behavior. For sev-eral, I began by asking a question without analternative phrase.
P01-1010@@The set of subtrees that is used isthus very large and extremely redundant. Using an Annotated LanguageCorpus as a Virtual Stochastic Grammar,Proceedings AAAI93, Washington D.C.R. ), Advances inProbabilistic and Other Parsing Technolo-gies, Kluwer Academic Publishers.E.
P01-1014@@Unfortunately, providing students with just a score (grade) is insufficient for instruction. Using the multivariate Bernoulli formula, below, this gives us the log probability that a sentence (S) in an essay belongs to the class (T) of sentences that are thesis statements. For example, we trained on Topics A, C, D, and E, using the thesis statements selected manually.
P01-1015@@We would also like to acknowledge the contributionof colleagues who worked on the RICHES system previ-ously: Neil Tipper and Rodger Kibble. In R. Dale,E. Modules com-municate by accessing a shared database.
P01-1016@@ This paper provides empirical support for the relationship between posture shifts and discourse structure, and then derives an algorithm for generating posture shifts in an animated embodied conversational agent from discourse states produced by the middleware architecture known as Collagen [18]. REFERENCES  [1] Andre, E., Rist, T., & Muller, J., Employing AI methods to control the behavior of animated interface agents, Applied Artificial Intelligence, vol. If generated, a low energy  posture shift is chosen.
P01-1017@@All of the most accurate statistical parsers [1,3,6,7,12,14] are lexicalized in that they conditionprobabilities on the lexical content of the sen-tences being parsed. Sometimes the imme-diate head of a constituent occurs after it (e.g,in noun-phrases, where the head is typically therightmost noun) and thus is not available for con-ditioning by a strict left-to-right parser.There are two reasons why one might preferstrict left-to-right parsing for a language model(Roark [15] and Chelba, personal communica-tion). Building a largeannotated corpus of English: the Penn tree-bank.
P01-1018@@How much strong generative power can besqueezed out of a formal system without increas-ing its weak generative power But this question is difficult to answerwith much finality unless we pin its terms downmore precisely.First, what is meant by strong generativepower Similarly, if we want to talkabout squeezing strong generative power out ofa formal system, we need to do so in the contextof some larger space of structural descriptions.Second, why is preservation of weak generativepower important If we interpret this constraint tothe letter, it is almost vacuous. (m1 (()), n2(()))igwhereas any equivalent CFG must have a stronggenerative capacity of the formfhambncndm, f m(gn(e()))igThat is, in a CFG the n bs and cs must appear laterin the derivation than the m as and ds, whereas inour example they appear in parallel. If, during the computation of t, a node 0creates the node , we say that 0 is productiveand produces .
P01-1019@@Some constraint-based grammar formalisms in-corporate both syntactic and semantic representa-tions within the same structure. Thus op is a function. of Simple semantic Enti-ties (SSEMENT)s  s3 is a bag of SEPs(the lzt) s4 is a set of equalities between variables(the eqs).We write a SSEMENT as: [i1][i2][SEPs]{EQs}.Note for convenience we omit the set markers {}from the hook and hole when there is no possibleconfusion.
P01-1020@@Human evaluation of machine translation (MT)output is an expensive process, oftenprohibitively so when evaluations must beperformed quickly and frequently in order tomeasure progress. In Proceedings ofthe 36th Annual Meeting of the Association forComputational Linguistics, Montreal Canada, Vol.I: 41-47.Bangalore, S., O. Rambow, and S. Whittaker. In this paper we cast the problem ofMT evaluation as a machine learningclassification task that targets both linguisticfeatures and more abstract features such as n-gram perplexity.
P01-1022@@Language models to constrain speech recogni-tion are a crucial component of interactive spo-ken language systems. Pallet, C. Pao, A. Rudnicky, and E. Shriberg.Expanding the scope of the atis task: The atis-3 cor-pus. For each r in 6 , of the form f  fJ@@:fhg , instantiate the daughters f J @@if\g usingall combinations of unifiable feature structuresfrom  .
P01-1023@@In a language generation system, a content plan-ner typically uses one or more plans In all cases,constraints on application of rules (eg, plan op-erators), which determine content and order, areusually hand-crafted, sometimes through manualanalysis of target text.In this paper, we present a method for learn-ing the basic patterns contained within a plan andthe ordering among them. This gives us the listof maximal motifs (ie patterns) which aresupported by the training data.Since they match in exactly the same positions, weprune the less specific one, as it adds no new information.A B C D E F  Moreover,as they have fixed length, they tend to be prettysimilar. Each line corresponds to a different pattern.
P01-1024@@Linear precedence in so-called free word orderlanguages remains challenging for modern gram-mar formalisms. The SUNY Press, Albany, N.Y.Stefan Muller. Finally, cat andvalencyID assign a category and an R For example, for an edgew must be assignedthe same agreement.
P01-1025@@In computational linguistics, a variety of (statis-tical) measures have been proposed for identify-ing lexical associations between words in lexi-cal tuples extracted from text corpora. Here t-test outper-forms log-likelihood, and even precision gainedby frequency is better than or at least comparableto log-likelihood. A case study on extractingPP-verb collocations.
P01-1026@@Reflecting the growth in utilization of the World WideWeb, a number of Web-based language processingmethods have been proposed within the natural lan-guage processing (NLP), information retrieval (IR)and artificial intelligence (AI) communities. E. Robertson and S. Walker. To avoidthis problem, we normalize P (d) by the number ofwords contained in d.  Application4.
P01-1027@@Typically, the lexicon models used in statisticalmachine translation systems are only single-wordbased, that is one word in the source language cor-responds to only one word in the target language.Those lexicon models lack from context infor-mation that can be extracted from the same paral-lel corpus. A maximum entropyapproach to natural language processing. For example, if wewant to model the existence or absence of a spe-cific word R in the context of an English word which has the translation  we can express thisdependency using the following feature function:Q3TSBUV3MUO7XWif    R and  RZY O otherwise (1)The ME principle suggests that the optimalparametric form of a model fffi  OZ taking intoaccount only the feature functions Q\[ ^] W._ is given by:fffi OPW`OZfia2cbIed:[Dcf[gQ\[cO 5KHere`OP is a normalization factor.
P01-1028@@We take the axiomatic view of language and showthat it yields an interestingly new perspective onthe tactical generation task ie the task of produc-ing from a given semantics a string with seman-tics .As (Cornell and Rogers, To appear) clearlyshows, there has recently been a surge of interestin logic based grammars for natural language. In contrast, a free model nfor  (written, n o p F  ) is a model such that ev-ery node in that model interprets exactly one nodevariable in  .In DG, lexical tree descriptions must obey thefollowing conventions. A Uniform Architecture for Parsingand Generation.
P01-1029@@The aim of this article is to describe the wordorder of German verbs and their comple-ments. A phrase structure with emancipation for(1f,g)2. opens an embedded do-main, hat has The embedded domain can go into theVorfeld (1c), the Nachfeld (1d), or the Mit-telfeld (1a,e).Note that we obtain the word order (1a) a sec-ond time, giving us two phrase structures: (2) a.
P01-1030@@Ofcourse, it is possible to miss a good translationthis way. The glossis constructed by aligning each French word fwith its most likely English translation efl (eflnmargmax o t(e  f )). If e slisa word of fertility 1 and e  is NULL, thene slis deleted from the translation.
P01-1031@@Clarification ellipsis (CE), nonsentential ellipti-cal queries such as (1a(i),(ii)) are commonplacein human conversation. The common ground com-ponent of ISs is assumed to be structured as fol-lows:8(8)	FACTS set of factsLATEST-MOVE (illocutionary) factQUD p.o. A: Did Bo finagle a raiseB: (i) Bo/ (ii) finagleb.
P01-1032@@The databasespecifies a set of interpretations for drop, depend-ing on its context in the source-language (SL). { is an occurrence of tag 9 (for a particular synset)in SEMCOR and z e is an occurrence of any of a setof tags for verb g in SEMCOR, with 9 being oneof the senses possible for verb g . In C.A.Bean and R. Green, editors, Relationships in theOrganization of Knowledge, pages 8198.
P01-1033@@Type-logical grammars offer a clear cut betweensyntax and semantics. Consequently, EXISTS must be given ((e t) QuantifiersQuantifiers may also play a part. Natural Deduction, A Proof-Theoretical Study.
P01-1037@@Open-domain textual Question-Answering(Q&A), as defined by the TREC competitions1 ,is the task of identifying in large collections ofdocuments a text snippet where the answer toa natural language question lies. WordNet: A Lexical Database. In later loops starting at step 3 one ofthe following three possible relaxations of Lex-ical relation are allowed: (a) common morpho-logical root (eg owner and owns, from questionQ742: Who is the owner of CNN  and ques-tion Q417: Who owns CNN  respectively);(b) WordNet synonyms (eg gestation and preg-nancy from question Q763: How long is hu-man gestation  and question Q765: A nor-mal human pregnancy lasts how many months , respectively) or (c) WordNet hypernyms (egthe verbs erect and build from question Q814:When was Berlins Brandenburg gate erected  and question Q397: When was the BrandenburgGate in Berlin built  Performance evaluationTo evaluate the role of lexico-semantic feedbackloops in an open-domain textual Q&A systemwe have relied on the 890 questions employedin the TREC-8 and TREC-9 Q&A evaluations.In TREC, for each question the performance wascomputed by the reciprocal value of the rank(RAR) of the highest-ranked correct answer givenby the system.
P01-1038@@While there is a vast theoretical and computa-tional literature on the interpretation of ellipticalforms, there has been little study of the generationof ellipsis. Thus(4) would be generated as a VPE. For this fea-ture, inter-annotator agreement could not beachieved to a satisfactory degree ( & /0 ),but the feature was not identified as use-ful during machine learning anyway.
P01-1039@@In recent years, the task of automatically extract-ing information from data has grown in impor-tance, as a result of an increase in the number ofpublicly available archives and a realization of thecommercial value of the available data. On the heading line, Prefers to precision, R to recall, F to F-measure, Cto caller-identity, and N to phone number. Although performance degrades signifi-cantly in the presence of speech racognition er-rors, it is still possible to reliably determine thesound segments corresponding to phone num-bers.ReferencesDouglas E. Appelt and David Martin.
P01-1040@@It is widely recognized that the proliferation ofannotation schemes runs counter to the need tore-use language resources, and that standards forlinguistic annotation are becoming increasinglymandatory. The categories areorganized in a hierarchy, from general tospecific. We alsoshow how the  framework can contribute to                                                           1 http://www.ilc.pi.cnr.it/EAGLES/home.html2 http://www.loria.fr/projects/TMFcomparison and merging of diverse syntacticannotation schemes.
P01-1041@@Named entity (NE) recognition is a task inwhich proper nouns and numerical informa-tion in a document are detected and classi-fied into categories such as person, organiza-tion, location, and date. This obtains a largenumber of NE candidates (Fig.  is  s character type and  is ff s POS tag. is a boolean value that indicates whether it isa positive example.
P01-1042@@Many statistical NLP applications, such as tag-ging and parsing, involve finding the valueof some hidden variable Y (eg, a tag or aparse tree) which maximizes a conditional prob-ability distribution P (Y |X), where X is agiven word string. Jelinek notes thatthis algorithms running time is n6 (where n is thelength of sentence being parsed), and we foundexhaustive parsing to be computationally imprac-tical. A new formalization of probabilisticGLR parsing.
P01-1043@@Hence,shallowparsers are very practical tools. % and a precision of 95. The constituent(s) being built Also, focussing only onconstituent boundaries  ensures determinism :there is no need for determinizing norminimizing the automata we obtain from ourrules.
P01-1045@@Current research on natural language parsingtends to gravitate toward one of two extremes:robust, partial parsing with the goal of broaddata coverage versus more traditional parsers thataim at complete analysis for a narrowly definedset of data. E.g.the label SUBJ encodes that the NP that is thealrightUHandCCthatDTshouldMDgetVBusPPthereRBaboutRBnineCDinINtheDTeveningNN 1.subject of the whole sentence. Ph.D. thesis,University of Pennsylvania.
P01-1050@@Over the last decade, much progress has beenmade in the fields of example-based (EBMT) andstatistical machine translation (SMT). Machine trans-lation with a stochastic grammatical channel. In Proceedings of NewMethods in Natural Language Processing, Sofia,Bulgaria.S.
P01-1051@@There have been many theories of language acqui-sition proposing a stereotypical order of acquisi-tion of language elements followed by most learn-ers, and there has been empirical evidence of suchan order among morphological elements of lan-guage (cf. It is a representation of the users abil-ity to correctly use each of the grammatical fea-tures of English, which we define as incorpo-rating both morphological rules such as plural-izing a noun with +S and syntactic rules suchas the construction of prepositional phrases andS V O sentence patterns. American Sign Lan-guage: A Teachers Resource Text on Grammar andCulture.
P01-1053@@In this paper we present an approach to super-vised learning and automatic detection of sylla-ble boundaries. The last grammar is a positional syl-lable structure grammar which expresses that theconsonants of the onset and coda are restricted ac-cording to the position inside of a word (e.g, ini-tial, medial, final or monosyllabic). Thegrammar rules were read from a lexicon.
P01-1054@@These grammars are formulatedusing a vocabulary provided by a finite partiallyordered set of types and a set of features that mustbe specified for each grammar, and feature struc-tures in these grammars must respect certain con-straints that are also specified. There is a least element, so X and e havemore than one maximal lower bound, jSk&jml andothers. Otherwise, the algorithm added X be-cause of a bounded set +(w k xw l 8 , with minimal up-per bounds, yzk{J(J(J|yd} , which did not have a leastupper bound, ie, ~ H .
P01-1055@@Machine learning has recently been proposed asa promising solution to a major problem in lan-guage engineering: the construction of lexicalresources. A corpus-based approach to languagelearning. NYU: Description of the JapaneseNE System used for MET-2.
P01-1057@@There is increasing interest in techniques for eval-uating Natural Language Generation (NLG) sys-tems. Have a look on the scales. However, there was no statis-tically significant difference in perceptions of theusefulness and relevance of the tailored and non-tailored letters.Free-text comments on the tailored letters werevaried, ranging from I carried mine with me allthe time and looked at it whenever I felt likegiving in to I found it patronising .
P01-1059@@The explosion of the World Wide Web hasbrought with it a vast hoard of information, mostof it relatively unstructured. The 4relative clauses are duplicates of one another:who helped Lewinsky find a job. The 78appositives fall into just 2 groups: friend (orequivalent descriptions, such as confidant),adviser (or equivalent such as lawyer).
P01-1060@@We work with syntactic trees in whichterminals are in addition labeled with uninflectedword forms (lemmas) derived from the lexicon.By percolating lemmas up the chains of heads,each node in a headed tree may be labeled witha lexical head. *A@ for (CBED .that NP SC deprive . We instead calculate O in a parseforest representation of a set of tree analyses.
P01-1061@@ap-proach to semantics, in which sentences are mor-phologically and syntactically resolved to a singletree before being interpreted. Cambridge University Press, Cambridge,U.K.T. Recognition and parsing of context-free languages in time n cubed.
P01-1064@@Documents usually include various topics. A sample is characterised by therange  . Ph.D. thesis, Computerand Information Science, University of Pennsylva-nia.Jeffrey C. Reynar.
P01-1066@@The objective of the DARPA COMMUNICATORprogram is to support research on multi-modalspeech-enabled dialogue systems with advancedconversational capabilities. Utiliz-ing statistical speech act processing in verbmobil.In ACL 95.E. Date: A dia-logue act tagging scheme for evaluation.
P01-1067@@A statistical translation model (TM) is a mathe-matical model in which the process of human-language translation is statistically modeled.Model parameters are automatically estimated us-ing a corpus of translation pairs. A statistical approach tolanguage translation. Note that an English wordcan be translated into a French NULL word.The notation E stands for a setof values of is aset of values of random variables associated with is the set of all ran-dom variables associated with a parse tree E:333B is the sequence of leaf wordsof a tree transformed by  .The probability of having a particular set ofvalues of random variables in a parse tree isP  Then, we assume thata transform operation is independent from othertransform operations, and the random variables ofeach node are determined only by the node itself.So, we obtainP  are as-sumed to be independent of each other.
P01-1068@@Word N-grams have been widely used as a sta-tistical language model for language processing.Word N-grams are models that give the transitionprobability of the next word from the previousN  1 word sequence based on a statistical analy-sis of the huge text corpus. Assign one unique class per word.s. Find a word pair whose frequency is abovethe threshold..
P01-1069@@Recently there has been considerable interest inapplying machine learning techniques to prob-lems in natural language processing. As explained earlier, the constantterm is used to offset the effect of threshold  .Once a weight vector AB JJ RD isobtained, we let |J .The prediction with an incoming feature vector is then $Re$AAwfffi .Since Winnow only solves binary classificationproblems, we train one linear classifier for eachchunk type. Slot grammar: a system forsimple construction of practical natural languagegrammars.
P01-1070@@The growth in popularity of the Internet highlightsthe importance of developing machinery for gen-erating responses to queries targeted at large un-structured corpora. Inferring infor-mational goals from free-text queries: A Bayesianapproach. Two types of tags were col-lected for each question: (1) tags describing lin-guistic features, and (2) tags describing high-levelinformational goals of users.
P02-1001@@Despite bounded memory theyare well-suited to describe many linguistic and tex-tual processes, either exactly or approximately.A relation is a set of (input, output) pairs. 4 cuts throughthe difficulty with a surprisingly simple trick. (c) means that the weights in (a)can be altered by adjusting the fewer weights in (b) and (c).This paper aims to provide a remedy through anew paradigm, which we call parameterized finite-state machines.
P02-1002@@However, as wedescribe later, each of these has suffered from appli-cability to a limited number of applications. Tech-nical Report CMU-CS-99-108, Computer ScienceDepartment, Carnegie Mellon University.Michael Collins, Robert E. Schapire, and YoramSinger. One simple idea is to just addlog observed[i]/expected[i] to i.
P02-1003@@Existing algorithms for realization from a flat inputsemantics all have runtimes which are exponential inthe worst case. Lets say we want to verbalizethe semantics{name(m, mary), buy(e,m, c),car(c), indef(c), red(c)}The LTAG grammar we use contains the elemen-tary trees which are used in the tree in Fig. Finally, we assume thatevery lexical semantics  , n}, using thegrammar G. The input sentence problem we construct is the sequence {start}  S,where start is a special start symbol.
P02-1004@@ The last stage of natural language generation, sentence realization, creates the surface string from an abstract (typically semantic) representation. In A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text, R. Dale, H. Moisl, and H. Somers (ed. Amalgam: A machine-learned generation module.
P02-1006@@ Most of the recent open domain question-answering systems use external knowledge and tools for answer pinpointing. Select an example for a given question type. Acknowledgements This work was supported by the Advanced Research and Development Activity (ARDA)s Advanced Question Answering for Intelligence (AQUAINT) Program under contract number MDA908-02-C-0007.
P02-1012@@Pronominalization is an important element in the au-tomatic creation of multi-paragraph and multi-pagetexts using natural language generation (NLG). Support for thiswork was provided by ITC-irst and the IntelliMediaInitiative of North Carolina State University.ReferencesDouglas E. Appelt. A Formal Approach to DiscourseAnaphora.
P02-1013@@Recently however, several pa-pers made a step in that direction. In theProceedings of the ESSLLI workshop on The Genera-tion of Nominal Expression.E. Then l treasurer is selectedwhich restricts the distractor set to SUTmVKXYTjZXYT g XYT h \ .There is no other literal which could be used to fur-ther reduce the distractor set hence properties of theform ,/no,7p are used.
P02-1014@@Noun phrase coreference resolution refers to theproblem of determining which noun phrases (NPs)refer to each real-world entity mentioned in a doc-ument. else N.DEMONSTRATIVE 2 Y if NP starts with a demonstrative such as this, The feature set contains relational and non-relationalfeatures. content wordsonly) of the other; else I.WORDS SUBSTR C if both NPs are non-pronominal and one NP is a proper substring (w.r.t.
P02-1015@@Several applications in natural language processingrequire parsing of a large but finite set of candidatestrings. The algo-rithm manipulates items of the form j EXSTn , where is a nonterminal of   and S , T are stacks of thePDA encoding  . The shadedareas represent segments S Pqd~T .
P02-1016@@Acquiring such a corpus is expensive and time-consuming and is often the bottleneck to build a parserfor a new application or domain. Decision tree parsingusing a hidden derivation model. We calculate the entropy of the dis-tribution over all candidate parses as the sentence entropyto measure the intrinsic ambiguity.Given a sentence 	 , the existing model T could gener-ate the top  Without confusion, we drop  ) Word EntropyAs we can imagine, a long sentence tends to have morepossible parsing results not because it is difcult but sim-ply because it is long.
P02-1018@@One of the main motivations for research on pars-ing is that syntactic structure provides important in-formation for semantic interpretation; hence syntac-tic parsing is an important rst step in a variety of I would like to thank my colleages in the Brown Labora-tory for Linguistic Information Processing (BLLIP) as well asMichael Collins for their advice. We perform a pre-order traversal of the sub-trees of t (ie, visit parents before their children),and at each subtree we nd the set of patterns thatmatch the subtree. Weexperimented with several different ranking crite-ria, including pattern depth, success probability (ie,c/m) and discounted success probability.
P02-1020@@A topic of considerable theoretical and practicalinterest is that of text reuse: the reuse of existingwritten sources in the creation of a new text. A program for align-ing sentences in bilingual corpus. This measure prefers SSs which notonly contain many terms in the DS, but alsodo not contain many additional terms.. PSNG: The proportion of matching n-grams amongst the shared terms.
P02-1021@@ and BackgroundText normalization is an important aspect ofsuccessful information retrieval frommedical documents such as clinical notes,radiology reports and discharge summaries,to name a few. A maximum entropy part ofspeech tagger. that is used toautomatically label the training data.To disambiguate the meaning ofabbreviations I am using a MaximumEntropy (ME) classifier.
P02-1023@@ Backoff n-gram models for applications such as large vocabulary speech recognition are typically trained on very large text corpora. In practice, n is usually set to 2 (bigram), or 3 (trigram). Select a setting of threshold pair (12) 2.
P02-1024@@It is a stochastic model, which predicts the next word (predicted word) given the previous n-1 words (conditional words) in a word sequence. A statistical approach to machine translation. Class-based n-gram models of natural language.
P02-1025@@The structured language model uses hidden parsetrees to assign conditional word-level languagemodel probabilities. equivalent to a context-free production of the type ffflffflff, whereTffflffflffare NT labels orPOS tags (only for  Z )  is specified using a rule-based approach.Assuming that the index of the headword on theright-hand side of the rule is  , we binarize the con-stituent as follows: depending on the  identity weapply one of the two binarization schemes in Fig-ure 4. A maximum entropy approach to nat-ural language processing.
P02-1026@@It is well-known from Information Theory thatthe most ecient way to send informationthrough noisy channels is at a constant rate. Ifhumans try to communicate in the most ecientway, then they must obey this principle. We nd the largest k  n, such thatsequence of words w1.
P02-1029@@A long-standing linguistic hypothesis asserts a tightconnection between the meaning components of averb and its syntactic behaviour: To a certain ex-tent, the lexical meaning of a verb determines its be-haviour, particularly with respect to the choice of itsarguments. Deletingna or frames containing s from the verb de-scription destroys the coherent clusters.  Cluster (c) contains two sub-classes from As-pect and Insistence, polluted by the verb rud-ern to row. Clustering MethodologyClustering is a standard procedure in multivariatedata analysis.
P02-1030@@Context plays an important role in many natural lan-guage tasks. Kluwer Academic Publishers,Boston, USA.Marti A. Hearst. The first step ex-tracts the contexts from raw text and compiles theminto a vector-space statistical description of the con-texts each potential thesaurus term appears in.We define a context relation as a tuple (w, r,w Thetype can be grammatical or the position of w in acontext window: the relation (dog, direct-obj,walk) indicates that the term dog, was the direct ob-ject of the verb walk.
P02-1031@@Much of the evaluation ofthese systems has been conducted on extractingrelations for specic semantic domains such ascorporate acquisitions or terrorist events in theframework of the DARPA Message Understand-ing Conferences.Recently, attention has turned to creating cor-pora annotated for argument structure for abroader range of predicates. ComputationalLinguistics, in press.Jerry R. Hobbs, Douglas Appelt, John Bear,David Israel, Megumi Kameyama, Mark E.Stickel, and Mabry Tyson. FASTUS:A cascaded nite-state transducer for extract-ing information from natural-language text.
P02-1034@@It is an incredibly simple algorithm toimplement, and yet it has been shown to be com-petitive with more recent learning methods such assupport vector machines  Crucially, the algorithmscan be efficiently applied to exponential sized repre-sentations of parse trees, such as the all subtrees It might seem paradoxical to be able to ef-ficiently learn and apply a model with an exponentialnumber of features. In a similar way,a model trained on the 41,992 sentence set was usedto produce 20 hypotheses for each sentence in thedevelopment set.As in the parsing experiments, the final kernel in-corporates the probability from the maximum en-tropy tagger, ie,o, Grwhere is the log-likelihood ofunder the tagging model,S;is the taggingkernel described previously, and  is a parameterweighting the two terms. Say g is the size of the training set, ie,gN .
P02-1035@@Statistical parsing using combined systems of hand-coded linguistically fine-grained grammars andstochastic disambiguation components has seen con-siderable progress in recent years. They report an F-score5Gildea reports a decrease from 86. In Proceedings of the Seven-teenth International Conference on Machine Learning(ICML00), Stanford, CA.Richard Crouch, Ronald M. Kaplan, Tracy H. King, andStefan Riezler.
P02-1036@@Stochastic Unification-Based Grammars (SUBGs)use log-linear models (also known as exponential orMaxEnt models and Markov Random Fields) to de-fine probability distributions over the parses of a uni-fication grammar. Recall that the sets offunctions A and M can be both be partitioned intodisjoint subsets A1, . It is straight-forward to show:LD() [gk|yi]) .We have just described the calculation of LD( ),so if we can calculate E be a packed repre-sentation such that  First, note that(2) implies that:E }|yi) involves the sum ofweights over all x  Viewedabstractly, these algorithms simplify these expres-sions by moving common factors over the max orsum operators respectively.
P02-1038@@We are given a source (French) , fJ , which is to be translated into atarget (English) The notational convention will be as follows. Statisti-cal and discriminative methods for speech recognition.In A. J. R. Ayuso and J. M. L. Soler, editors, SpeechRecognition and Coding New Advances and Trends.Springer Verlag, Berlin, Germany.H. Typically, training is performed byapplying a maximum likelihood approach.
P02-1039@@A statistical machine translation system based on thenoisy channel model consists of three components:a language model (LM), a translation model (TM),and a decoder. A syntax-based statis-tical translation model. Weobtained better n-gram accuracy, but the lower LPscore penalized the overall score.
P02-1040@@More-over, they can take weeks or months to finish. Consequently, we introduce a multiplicativebrevity penalty factor. We performed four pairwise t-test compar-isons between adjacent systems as ordered by theiraggregate average score.
P02-1041@@In this pa-per, we situate HLDS in the computational contextby explicating its properties as a framework for com-putational semantics and linking it to CombinatoryCategorial Grammar (CCG).The structure of the paper is as follows. (4) [e][party(x);past(e); to(e;x);come(e;Ed)]InL thus flattens logical forms to some extent, usingthe indexes to spread a given entity or event throughmultiple predications. (28) I know what Ed READ.
P02-1042@@However, the de-pendencies are typically derived from a context-freephrase structure tree using simple head percolationheuristics. However,the dependency structures with high enough P  C"S to be among the highest probability structures arelikely to have similar category sequences. A maximum entropy part-of-speech tagger.
P02-1043@@CCGgrammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin-guishing for example between many classes of verbswith different subcategorization frames. Acquiring Com-pact Lexicalized Grammars from a Cleaner Treebank. In R. Levine, ed., FormalGrammar: Theory and Implementation.
P02-1044@@ We address here the problem of word translation disambiguation. Word Sense Disambiguation Using a Second Language Monolingual Corpus. represent words to be translated, e and c represent context words.
P02-1045@@A major obstacle for natural language processingsystems which analyze natural language texts orutterances is the need to identify the entities re-ferred to by means of referring expressions. The work presented here hasbeen partially funded by the German Ministry ofResearch and Technology as part of the EMBASSIproject (01 IL 904 D/2, 01 IL 904 S 8), by SonyInternational (Europe) GmbH and by the KlausTschira Foundation. Another im-portant factor is the ratio between p and n, ie, thenumber of positive and negative instances added ineach iteration.
P02-1047@@In the field of discourse research, it is now widelyagreed that sentences/clauses are usually not un-derstood in isolation, but in relation to other sen-tences/clauses. This work was supported bythe National Science Foundation under grant num-ber IIS-0097846 and by the Advanced Research andDevelopment Activity (ARDA)s Advanced Ques-tion Answering for Intelligence (AQUAINT) Pro-gram under contract number MDA908-02-C-0007.ReferencesMichele Banko and Eric Brill. Such standards would preclude arms sales tostates like Libya, which is also currently sub-ject to a U.N. embargo.b.
P02-1049@@Spoken dialogue systems promise efficient and nat-ural access to a large variety of information servicesfrom any phone. These applications can receivemillions of calls a month. In Interac-tive Voice Technology for Telecommunications Applications,IVTTA, pages 97102.L.
P02-1051@@Named entity phrases are being introduced in newsstories on a daily basis in the form of personalnames, organizations, locations, temporal phrases,and monetary expressions. Finite-state devices produce a lattice con-taining all possible transliterations for a given name.The candidate list is created by extracting the n-besttransliterations for a given name. An algorithm that learns whats in a name.
P02-1053@@ If you are considering a vacation in Akumal, Mex-ico, you might go to a search engine and enter the query Akumal travel review. I chose AltaVista because it has a NEAR operator. A summary of the corpus of reviews.
P02-1057@@Single document summarization systems proposedto date fall within one of the following three classes:Extractive summarizers simply select and presentto the user the most important sentences ina text  By repeatedly applying asentence-simplification algorithm one sentence at atime, one can compress a text; yet, the outputs gen-erated in this way are likely to be incoherent andto contain unimportant information. Automatic summarization.William C. Mann and Sandra A. Thompson. InProceedings of the AAAI-98 Workshop on IntegratingArtificial Intelligence and Assistive Technology.R.
P02-1058@@ In recent years, text summarization has been enjoying a period of revival. It also allow s assessors to step through each model unit, mark all system units sharing content with the current model unit, and specify that the marked system units                                               3 Does a summary follow the rule of English grammatical rules independent of its content 4 Do sentences in a summary fit in with their surrounding sentences 5 Is the content of a summary expressed and organized in an effectiv e way 100 word summary with explicit time annotation. This is demonstrated by the top 3 systems T, N, and Y.
P02-1059@@That appears somewhat con-tradictory given that a supervised approach shouldbe able to exploit human supplied information aboutwhich sentence to include in an extract and whichnot to, whereas an unsupervised approach blindlychooses sentences according to some selectionscheme. is a smoothing function,eg, Laplaces law; t(~u) is some leaf node assignedto ~u; and DT represents some decision tree used toclassify ~u. It is a discrete valuedfeature.
P02-1060@@ Named Entity (NE) Recognition (NER) is to classify every word in a document into some predefined categories and "none-of-the-above". In ngram modeling, each tag is assumed to be probabilistically dependent on the N-1 previous tags. An Algorithm that Learns Whats in a Name.
P02-1061@@In this paper, we propose using a mixed case namedentity recognizer (NER) that is trained on labeledtext, to further train an upper case NER. has initCaps, then a feature (initCaps,. Q signifies that the text is converted toupper case before processing.JK(GFGFGFN(KIO(PILJKLO(PRJKL KL1(where J K L  O (P is determined by the maximumentropy classifier.
P02-1062@@Oneappeal of these methods is their flexibility in incor-porating features into a model: essentially any fea-tures which might be useful in discriminating goodfrom bad structures can be included. th word.SRfi for .0& 0///!O is  if P fi begins with a lower-case letter,  otherwise.ffflfi for .T& 0///!O is a transformation of P fi ,where the transformation is applied in the sameway as the final feature type in the maximumentropy tagger. th tag in the taggedsequence.QPfi for .0& 0///!O is the .
P02-1063@@Recently, corpus-based approaches have beenwidely studied in many natural language pro-cessing tasks, such as part-of-speech (POS) tag-ging, syntactic analysis, text categorization andword sense disambiguation. As a result, positive andnegative examples for each class are generated.Suppose we have five candidate classes A, B, C,D and E , and the true class of x is B. Fig-ure 1 (left) shows the created training examples.Note that there are only two labels (positive andnegative) in contrast with the original problem.Then a binary classifier for each class is trainedusing the examples, and five classifiers are cre-ated for this problem. A SequentialModel for Multi-Class Classification.
P02-1064@@Corpus-based supervised learning is now a stan-dard approach to achieve high-performance in nat-ural language processing. Let us see how many la-beled examples are required to achieve 96.0 % ac-curacy. A sequentialalgorithm for training text classifiers.
P02-1065@@In this paper I present algorithms for learn-ing the finite-state transduction between pairs of un-inflected and inflected words. 4%) using a neural networkclassifier. Finite-state models in the alignment of macro-molecules.Journal of Molecular Evolution, 35:7789.L.
P03-1001@@ Many of the recent advances in Question Answering have followed from the insight that systems can benefit by exploiting the redundancy of information in large corpora. I. Witten and E. Frank. 5  Answers that unequivocally identify an instances celebrity (eg, Jennifer Capriati is a tennis star) Answers that provide some, but insufficient, evidence to identify the instances celebrity (eg, Jennifer Capriati is a defending champion) Answers that provide no information to identify the instances celebrity (eg, Jennifer Capriati is a daughter) State of the Art  Extraction  Answer Mark Answer Mark Who is Nadia Comaneci U.S. citizen P Romanian Gymnast C Who is Lilian Thuram News page I French defender P Who is the mayor of Wash., D.C.
P03-1002@@The goal of recent Information Extraction (IE)tasks was to provide event-level indexing into newsstories, including news wire, radio and televisionsources. 8% in the fifth iteration (a 0. The following three columns indicatethe precision (P), recall (R), and F-measure (   )2obtained for the task of identifying argument con-stituents.
P03-1003@@ Current state-of-the-art Question Answering (QA) systems are extremely complex. The fundamental insight of our approach, which departs significantly from the current architectures, is that, at its core, a QA system is a pipeline of only two modules:   An IR engine that retrieves a set of M documents/N sentences that may contain answers to a given question Q. 4-Jean-Paul Sartre died of a lung ailment.
P03-1004@@In the field of Natural Language Process-ing, many successes have been reported. (s) is a frequency of s,given by the original PrefixSpan. Since H is much larger than N , it re-quires heavy computation to evaluate the dot prod-ucts  (x) in an explicit form.
P03-1005@@In this case, since the orig-inal natural language data is symbolic, researchersconvert the symbolic data into numeric data. A Vector SpaceModel for Automatic Indexing. represents the cost of node skip~l where ~zhas a path to ~F .Attribute sequences for non-terminated nodesWe define the attributes of the non-terminatednode as the combinations of all attribute sequencesincluding the node skip.
P03-1008@@refers to the person occupying seat 19. Let S be the setof examples that relax I covers. Metonymy resolu-tion as a classification task.
P03-1009@@Earlier work has largely ignoredthis issue by assuming a single gold standard classfor each verb (whether polysemic or not). In Conference on EmpiricalMethods in Natural Language Processing, Philadel-phia, USA.E. thesis, University of Cambridge, UK.A.
P03-1011@@Only recently have hybridapproaches begun to emerge, which apply prob-abilistic models to a structured representation ofthe source text. A decoder forsyntax-based statistical MT. of the children 1...mof idofor all partitions of span k, l into k1, l1...km, lmdo By storing partiallycompleted arcs in the chart and interleaving the in-ner two loops, complexity of O(jT jn3m!2m) can beachieved.
P03-1012@@Since their intro-duction, many researchers have become interestedin word alignments as a knowledge source. Let E be an English sentence e1, e2, . A maximum entropy approach tonatural language processing.
P03-1013@@German has a number of syn-tactic properties that set it apart from English, andthe Negra annotation scheme differs in important re-spects from the Penn Treebank markup. Both lexicalized models (C&R andCollins) performed worse than the baseline. Valence induction with a head-lexicalizedPCFG.
P03-1014@@One of the strong points of deep processing (DNLP)technology such as HPSG or LFG parsers certainlylies with the high degree of precision as well asdetailed linguistic analysis these systems are ableto deliver. A possible interpretation isthat for high , wrong topological constraints andstrong negative priorities can mislead the parser.Use of confidence weights yields the best per-formance gains (with  12), in particular, thresholdedprecision of bracket types PT, and tree entropy+E, with comparable speed-up of factor 2. A Stochastic TopologicalParser of German.
P03-1015@@In several areas of Natural Language Processing, acombination of different approaches has been foundto give the best results. Even on T-tags, this combination achieved anF-score of . It should, however, bestressed that most of the dependencies correctly de-termined by the n-gram methods extend over nomore than 3 tokens.
P03-1016@@ This paper addresses the problem of automatically extracting English synonymous collocation pairs using translation information. N represents the total counts of all the Chinese collocations in the training cor-pus. Extracting Para-phrases from a Parallel Corpus.
P03-1020@@While it is true that large, high quality text corporaare becoming a reality, it is also true that the digitalworld is flooded with enormous collections of lowquality natural language text. Bleu: a method for automaticevaluation of machine translation. Brill and R. C. Moore.
P03-1023@@ Coreference resolution is the process of linking together multiple expressions of a given entity. algorithm and the class value takes the smoothed ration, 21++tp , where p is the number of positive instances and t is the total number of instances contained in the corresponding leaf node. } A Centering approach to pronouns.
P03-1026@@This paper addresses the cost of copying edgesin memoization-based, all-paths parsers for phrase-structure grammars. This fourth option is thefastest by a factor of approximately 2. Each pass through the firstclause of build/3 then decrements Right, andseeds the chart with every category for the lexicalitem that spans from R-1 to R. The predicate,add edge/4 actually adds the lexical edge to theasserted copy of the chart, and then closes the chartdepth-first under rule applications in a failure-drivenloop.
P03-1028@@The explosive growth of online texts written in natu-ral language has prompted much research into infor-mation extraction (IE), the task of automatically ex-tracting specific information items of interest fromnatural language texts. A ma-chine learning approach to coreference resolution ofnoun phrases. Stone, R. Weischedel, and the Annotation Group.
P03-1030@@Topic Detection and Tracking (TDT) research issponsored by the DARPA Translingual InformationDetection, Extraction, and Summarization (TIDES)program. when used as a verb. In DE-LOS Workshop: Personalisation and RecommenderSystems in Digital Libraries.Ted E. Dunning.
P03-1031@@Here, a dialogue state means allthe information that the system possesses concern-ing the dialogue. As a speech recognitionengine, Julius3. Robust speech understanding using incremen-tal understanding with n-best recognition hypothe-ses.
P03-1033@@A spoken dialogue system is one of the promisingapplications of the speech recognition and naturallanguage understanding technologies. A model for generating betterexplanations. and the explanation about the fol-lowing dialogue procedure like Now I will ask inorder.
P03-1034@@Historically, work on NLG architecture has focusedon integrating major disparate architectural modulessuch as discourse and sentence planners and sur-face realizers. E. Fox Tree and J. C. Schrock. Significantly,Reiter states that fully interconnecting every modulewould entail constructing N(N   1) interfaces be-tween them.
P03-1035@@ Chinese word segmentation is the initial step of many Chinese language processing tasks, and has attracted a lot of attention in the research commu-nity. + indicates a morpheme boundary. Class models                                                    2 In our system, we define ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, phone number, and WWW.
P03-1036@@or produce nat-ural language, a system presumably has to know theelementary building blocks, ie, the lexicon, of thelanguage. However, thefluctuations due to random behaviour are very smalland paired t-tests show significant differences at thesignificance level of 0.01 for all pair-wise compar-isons of the methods at all corpus sizes.For Finnish, all methods show a curve that mainlyincreases as a function of the corpus size. Morph stringsFor each morph i, we decide the character string itconsists of: We independently choose li charactersat random from the alphabet in use.
P03-1037@@Linguistic count data often violate the simplistic as-sumptions of standard probability models like thebinomial or Poisson distribution. +1).To avoid confusion, we will refer to the distributionparameterized in terms of p and  as BB:BB(p,) )(8)After reparameterization the expectation and vari-ance areE[x;BB(p, ).Comparing this with the expectation and variance ofthe standard binomial model, it is obvious that thebeta-binomial has greater variance when  Suppose one sampleconsists of observing x successes in n trials (x occur-rences of the target word in a document of length n),where the number of trials may vary across samples.Now we want to estimate parameters based on a se-quence of s samples x1,n1, . Ifwe apply this to word frequency in documents, whatthis is saying is, informally: whether a given wordappears at all in a document is one thing; how oftenit appears, if it does, is another thing.This is reminiscent of Churchs statement that [t]he first mention of a word obviously dependson frequency, but surprisingly, the second doesnot.
P03-1039@@This failure has been due tothe limited representation by word alignment andthe weak model structure for handling complicatedword correspondence.This paper provides a chunk-based statisticaltranslation as an alternative to the word alignmentbased statistical translation. jf req(EA j ,J j)in which Ptm(J|E) and Plm(E) are translationmodel and language model probability, respec-tively1, f req(EA j ,J j) is the frequency for thepair EA j and J j appearing in the training cor-pus, and weight is a tuning parameter. (a) Select the number of head words.
P03-1040@@Recent research in machine translation challengesus with the exciting problem of combining statisti-cal methods with prior linguistic knowledge. The first of these (compound-ing of words) is addressed by preprocessing, whilethe others motivate features which are used in n-bestlist reranking. As a consequence, initiallyonly 43.
P03-1041@@Statistical Machine Translation defines the taskof translating a source language sentence 		into a target language sentence 		. We extract candidate translations of phraselength ( to N , starting at offset O to NQPR( . Instead of simply count-ing the number of consistent/inconsistent f * ffi-g,we sum the probability estimates  i!
P03-1043@@ Named Entity (NE) tagging is a fundamental task for natural language processing and information extraction. This joint probability can be computed by bi-gram HMM as follows:    where V denotes the size of the vocabulary, the back-off coefficients  s are determined using the Witten-Bell smoothing algorithm. Suppose there are a total of PERN  PER instances, LOCN  LOC instances, ORGN  ORG instances, PRON  PRO instances, then in the process of rule accuracy evaluation, the involved instance count for any NE type will be adjusted by the coefficient  NEPRO,ORGLOCPERminN) N, N, N(N .
P03-1044@@The work described in this paper is motivated byresearch into automatic pattern acquisition. We introduce a cutoffthreshold r 	s on document relevance; if the systemhas internal confidence of more than r 	ffs that a doc-ument  is relevant, it labels  as relevant externally0. To account for this, we introduce a pa-rameter, n : rather than acquiring a single patternon each iteration, each learner may acquire up to npatterns (3 in this paper), as long as their scores arenear (within 5% of) the top-scoring pattern.
P03-1046@@However, this paperdemonstrates that such models would be inadequatefor languages with freer word order. Argument probabilities:P (wjc;hhc0; w0i; i; hc; wii)The probability of generating word w, giventhat its lexical category is c and that hc; wi ishead of the ith argument of hc0; w0i. A Statistical Parser for Czech.
P03-1047@@The common idea is to delay the enu-meration of all readings for as long as possible. Instead, we introduce the sublanguagesof MRS-nets and equivalent wnd-nets, and show thatthey can be intertranslated in quadratic time.solved-form() is weakly connectedchoose a node X satisfying (F1) and (F2) in  ,Yn be all nodes s.t. ,k be the weakly connected components of |V {X ,Y1,...,Yn}Let (Wj,E j) be the result of solve( is a weakly normal domi-nance constraint whose fragments all satisfy one ofthe three schemas in Fig.
P03-1048@@Automatic document summarization is a field thathas seen increasing attention from the NLP commu-nity in recent years. Sentence Ex-traction as a Classification Task. Their output is an n% extract of thedocument (or cluster).
P03-1050@@Stemming is the process of normalizing word vari-ations by removing prefixes and suffixes. However, thescoring model needs to be modified, since t(a|e) isno longer available. It also constitutes a surprisingly high base-line.
P03-1051@@  Morphologically rich languages like       Arabic present significant challenges to many natural language processing applications because a word often conveys complex meanings decomposable into several morphemes (ie prefix, stem, suffix). Step 3: Keep the top N highest scored segmentations. % segmentation accuracy on a 9,606 word evaluation corpus.
P03-1053@@ The exact process by which a child acquires the grammar of his or her native language is one of the most beguiling open problems of cognitive science. A user may specify multiple grammars. If Gcurr cannot license s, new treelets will be utilized to achieve a successful parse.
P03-1055@@In this paper, we explore a novel approach for find-ing long-distance dependencies. In Proceedings of the EmpiricalMethods in Natural Language Processing Conference.University of Pennsylvania.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and Mark John-son. When evaluating antecedent recovery,the EEs are regarded as four-tuples, consisting of thetype of the EE, its location, the type of its antecedentand the location(s) (beginning and end) of the an-tecedent.
P03-1057@@Along with the efforts made in accumulating bilin-gual corpora for many language pairs, quite a fewmachine translation (MT) systems that automati-cally acquire their knowledge from corpora havebeen proposed. the MT result set that T [r] is replaced from Dociterthe rule contribution contrib[r]  This al-gorithm can be summarized as follows. Each ruleset is a subset of the base rule set.
P03-1058@@ The task of word sense disambiguation (WSD) is to determine the correct meaning, or sense of a word in context. Different dictionaries define a different sense inventory. We rely on two sources to decide on the sense classes of w: (i) The sense definitions in WordNet 1. , which lists seven senses for the noun channel.
P03-1059@@This paper is concerned with the task of knowledge-rich lexical acquisition from unannotated corpora,focusing on the case of countability in English.Knowledge-rich lexical acquisition takes unstruc-tured text and extracts out linguistically-precise cat-egorisations of word and expression types. of the Fifth International Workshop on ComputationalSemantics (IWCS-5), Tilburg, the Netherlands.Lane O.B. There is also a language-specific knowl-edge requirement.
P03-1061@@The Spontaneous Speech: Corpus and Process-ing Technology The CSJ is a collection of mono-logues and dialogues, the majority being mono-logues such as academic presentations and simu-lated public speeches. Where do we Stand on Maximum EntropyIn R. D. Levine and M. Tribus, editors, The Maximum En-tropy Formalism, page 15. We also used as features, bun-setsu boundaries and the labels (M), (O), (R), and(A), which were assigned to particular morphemessuch as personal names and foreign words.
P03-1062@@Any text-to-speech (TTS) system that aims at pro-ducing understandable and natural-sounding out-put needs to have on-board methods for predict-ing prosody. Daelemans, A. van den Bosch, and J. Zavrel. In Proceedings of the 35th AnnualMeeting of the Association for Computational Linguistics,Hong Kong.S.
P03-1063@@Sincethe position of a word plays an important role as asyntactic constraint in English, the methods are suc-cessful even with local information.However, these methods are not appropriate forchunking Korean and Japanese, because such lan-guages have a characteristic of partially free word-order. All nouns appearing successivelywithout postpositions are not a single nounphrase. The learning component involves addingtraining examples to memory, where all examplesare assumed to be fixed-length vectors of n at-tributes.
P03-1064@@Supertagging is the process of assigning thecorrect supertag to each word of an input sentence.The following two facts make supertagging attrac-tive. A learning approach to shallow parsing. In G. Rozenberg and A. Salomaa, editors,Handbook of Formal Languages, volume 3, pages 69 Disambiguation of su-per parts of speech (or supertags): Almost parsing.
P03-1065@@This paper presents a proven approach to identifying English PVs based on pattern matching using a formalism called Expert Lexicon. (6b) She looked [for quite a while]. This processing only requires 0.  second using the indexed PV Expert Lexicon module.
P03-1066@@ In recent years, many efforts have been made to utilize linguistic structure in language modeling, which for practical reasons is still dominated by trigram-based language models. Given the above parsing model, we used an ap-proximation parsing algorithm that is O(n2). This method achieved a 98.
P03-1067@@The development of speaker-independent mixed-initiative speech interfaces, in which users not onlyanswer questions but also ask questions and giveinstructions, is currently limited by the perfor-mance of language models based largely on word co-occurrences. the faucet : x1:Faucet(x1)in which m and n are integers and 0  m  n. Eachlambda expression x1::: xn:  indicates a functionfrom a tuple of entities he1::: eni to a truth valuedened by the remainder of the expression  (sub-VP ! NP:+ PP:+ events,meaning a noun phrase that does not denote any-3Denotation of competing applications of parse rulescan be unioned (though this eectively treats ambiguityas a form of disjunction), or stored separately to somenitie beam (though some globally preferable but locallydispreferred structures would be lost).
P03-1068@@Corpus-based methods for syntactic learning andprocessing are well-established in computationallinguistics. In the frameCOMMERCIAL TRANSACTION (henceforth C T), aBUYER gives MONEY to a SELLER and receivesGOODS in exchange. This appears to be a question of prag-matics.
P03-1069@@The structured textis typically assumed to be a tree (ie, to have a hier-archical structure) whose leaves express the contentbeing communicated and whose nodes specify howthis content is grouped via rhetorical or discourse re-lations (eg, contrast, sequence, elaboration).For domains with large numbers of facts andrhetorical relations, there can be more than one pos-sible tree representing the intended content. 6 given that f (h,e) isone and f (e) is six (see the normalization in (5)). Building a large annotated corpusof english: The penn treebank.
P03-1070@@ An essential part of conversation is to ensure that the other participants share an understanding of what has been said, and what is meant. and E.F. Schaefer, Contributing to dis-course. 8  [1] U: How do I get to Room 309 [2] M: To get to Room 309, go to that door and make a right.
P03-1071@@Topic segmentation aims to automatically divide textdocuments, audio recordings, or video segments,into topically related units. A statistical modelfor domain-independent text segmentation. For each poten-tial boundary, we count for each speaker i the num-ber of words that are uttered before (Li) and after(Ri) the potential boundary (we limit our analysisto a window of fixed size).
P03-2006@@Non-local dependencies (also called long-distance,long-range or unbounded) appear in many fre-quent linguistic phenomena, such as passive, WH-movement, control and raising etc. E.g., a pattern consisting of3 nodes will have a feature vector with 15 elements.Id Count Match Pattern Dependency Dep. , where Aichi is a (non-local) depen-dent of five verbs.
P03-2012@@Most coreference resolution systems proceed inthe following way: they first identify all thepossible markables (for example, noun phrases)and then check one by one candidate pairsfiffffiflff "!#$fiffflff %!%&(, trying to find out whetherthe members of those pairs can be coreferent. Combining two approachesUnique and non-unique NPs demonstrate differentbehaviour w.r.t. Bean and E. Riloff achieved an accuracy for def-inite non-anaphoric NPs of about  5 (F-measure), with various combinations of precisionand recall.
P03-2013@@ (1) Zebras always need to watch out for lions. 70-paasento-o  70-percent-ACC   The extracted nouns under this definition are initial candidates for ATNs. from a corpus aligned with English.
P03-2017@@ Answering emails sent to a company by its cus-tomers  to take just one example among many similar text-processing tasks  requires a reli-able understanding of the content of incoming messages. ITU at this point is more a research program than a com-pleted realization. 2: Snapshot of the MDA system applied to the author-ing of drug leaflets.
P03-2020@@This study aims to realize an automatic method ofcollecting technical terms that are related to a givenseed term. The term is not a general term.. Wesend all four queries to Goo but only the query s A typical corpus size isabout 500 sentences.
P03-2021@@The goal of a good document summary is to providea user with a presentation of the substance of a bodyof material in a coherent and concise form. Conversely,sentences that blend into the background have a verylow sentence score. In Proceedings of the 40thAnniversary Meeting of the Association for Computa-tional Linguistics (ACL-02), Philadelphia, PA, USA.Kathleen R. McKeown, Regina Barzilay, David Evans,Vasileios Hatzivassiloglou, Barry Schiffman, and Si-mone Teufel.
P03-2025@@Although, corpora have been an object of study ofsome decades, recent years saw an increased inter-est in their use and construction. % and a difference of -32. 33rd Hawaii International Conference on Sys-tem Sciences.R.
P03-2026@@ One of the most important things in keeping up with our current information-driven society is the acquisition of foreign languages, especially English for international communications. This can be considered the same as deciding which of the two labels (E: There is a missing word. Method A * there is telephone and the books .
P03-2031@@Various supervised learning meth-ods for Named Entity (NE) tasks were successfullyapplied and have shown reasonably satisfiable per-formance. This can beaccomplished by using a web search engine queriedwith pre-compiled NE list.As queries to a search engine, we used the listof Korean Named Entities composed of 937 per-son names, 1,000 locations and 1,050 organizations.Using a Part-of-Speech dictionary, we removed am-biguous entries which are not proper nouns in othercontexts to reduce errors of automatic annotation.For example, E is filtered out because it means a lo-cation (proper noun) in one context, but also meansbusiness conditions or a game (common noun) inother contexts. Segmenting Korean Compound Nouns usingStatistical Information and a Preference Rule.
P03-2034@@This paper demonstrates a multimodal interface forasking questions and retrieving a set of likely an-swers. EACL Workshop on Language Modeling for TextEntry Methods, apr.E.M. A system for spoken query informationretrieval on mobile devices.
P03-2039@@Like many other Asian languages (Thai, Japanese,etc), written Chinese does not delimit words byspaces and there is no clue to tell where the wordboundaries are. The only resourceneeded is a large corpus. Please refer to the paper cited for de-tails.Basically we would like to classify the charactersinto 3 categories, B (beginning of a chunk), I (insidea chunk) and O (outside a chunk).
P03-2041@@: Tree-to-Tree MappingsStatistical machine translation systems are trained onpairs of sentences that are mutual translations. That isthe E step of the EM algorithm. (If dj is null, then t.qmust guarantee that tj is the special null tree.
P04-1001@@Since the first appearance of Put-That-There One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs. Gold, S. and Rangarajan, A. indicates the number of inputs in which the referringexpressions were correctly recognized by the speech recog-nizer; b indicates the number of inputs in which the refer-ring expressions were  correctly recognized and werecorrectly resolved; c indicates the number of inputs inwhich the referring expressions were not correctly recog-nized; d indicates the number of inputs in which the refer-ring expressions also were not correctly recognized, butwere correctly resolved.
P04-1002@@The constructivist approach to language learningproposes that children acquire linguistic compe-tence (...) only gradually, beginning with moreconcrete linguistic structures based on particularwords and morphemes, and then building up tomore abstract and productive structures based onvarious types of linguistic categories, schemas, andconstructions. The first critical step is toguess as well as possible the meaning of the un-known word(s). Here is a lexical entry for theword fall.
P04-1004@@However, little is known aboutthe use of natural language in dialog setting in for-mal domains, such as mathematics, due to the lackof empirical data. [A and B must be disjoint.]). of IJCAI03 Workshop on Knowledge Rep-resentation and Automated Reasoning for E-Learning Sys-tems, Acapulco, Mexico.C.
P04-1005@@), parenthetical asides and repairs. Thatis, the model is used to classify each word in thesentence as belonging to a reparandum or not,and all other additional structure produced bythe model is ignored.We measure model performance using stan-dard precision p, recall r and f-score f , mea-sures. is a distinguished beginning-of-sentence marker.
P04-1006@@Success in language modeling has been dominatedby the linear n-gram for the past few decades. plate with a fork and . Thismeans that the lexicalized language model, on av-erage, computes probabilities for over 6.  millionstates when provided with 30,000 local trees.Model # edge pops O-WER WERnbest (Charniak) 2.  million 7.
P04-1007@@A crucial component of any speech recognizer is the lan-guage model (LM), which assigns scores or probabilitiesto candidate output strings in a speech recognizer. is a label (in our case, words), p[e]  R is the weight of the transition. A wA[pi].
P04-1010@@ Recently there has been a great deal of interest in improving natural-language human-computer conversation. Yield: a set of coefficients for each task. The AT&T-DARPA Communicator Mixed-Initiative Spoken Dialog System.
P04-1011@@One very challenging problem for spoken dialogsystems is the design of the utterance genera-tion module. AcknowledgmentsWe thank AT&T for supporting this research,and the anonymous reviewers for their helpfulcomments on this paper.ReferencesI. Stent, M. Walker, S. Whittaker, and P. Maloor.User-tailored generation for spoken dialogue: Anexperiment.
P04-1013@@Much recent work has investigated the applica-tion of discriminative methods to NLP tasks,with mixed results. This repre-sentation is called a hidden representation. As long as the hidden repre-sentation for position i  1 is always used tocompute the hidden representation for positioni, any information about the entire sequencecould be passed from hidden representation tohidden representation and be included in thehidden representation of that sequence.
P04-1014@@We also showed how the completeWSJ Penn Treebank can be used for training by de-veloping a parallel version of Generalised IterativeScaling (GIS) to perform the estimation.This paper significantly extends our earlier workin a number of ways. A Gaussianprior for smoothing maximum entropy models. ; then the valuesin (5) can be obtained by calculating E j for each sentence S j in the train-ing data (the second sum in (5)), and also E fi) (8)The likelihood in (4) can be calculated as follows:L() is the normalisation constant for .
P04-1015@@In statistical approaches to NLP problems such as tag-ging or parsing, it seems clear that the representationused as input to a learning algorithm is central to the ac-curacy of an approach. that could be applied to a diverse rangeof tasks, including parsing and tagging. GEN(xi), U  (xi, yi)U  R.This theorem implies that if there is a parameter vec-tor U which makes zero errors on the training set, thenafter a finite number of iterations the training algorithmwill converge to parameter values with zero training er-ror.
P04-1016@@Over the past few years, many machine learn-ing methods have been successfully applied totasks in natural language processing (NLP). si and tj represent the ith andjth symbols in S and T , respectively. Cortes and V. N. Vapnik.
P04-1017@@Most learning-based pronoun res-olution systems determine the reference rela-tionship between an anaphor and its antecedentcandidate only from the properties of the pair.The knowledge about the context of anaphorand antecedent is nevertheless ignored. A centering approach to pronouns. In this case, we can replaceDTi+1pron by DTipron during the i+1(th) testingprocedure.
P04-1018@@Coreference resolution in this context is definedas partitioning mentions into entities. * indicates that the resultis significantly (pair-wise t-test) different from the lineabove at ff#" fffl . One may con-sider training STYCW OG+ directly and use it toscore paths in the Bell tree.
P04-1019@@the gap using common-sense inferencesare arguably the most interestingand, at the same time, the most challenging prob-lem in anaphora resolution. A. Gernsbacher and D. Hargreaves. be the number of hyper-nim links between concepts s and s. Get from WordNet al the senses of both NBDand NPA;2.
P04-1020@@Noun phrase coreference resolution, the task of de-termining which noun phrases (NPs) in a text referto the same real-world entity, has long been con-sidered an important and difficult problem in nat-ural language processing. First, we trainin a supervised fashion a probablistic model ofanaphoricity PA(c | i), where i is an instance rep-resenting an NP and c is one of the two possibleanaphoricity values. A maximum entropy approach tonatural language processing.
P04-1021@@ In applications such as cross-lingual information retrieval (CLIR) and machine translation, there is an increasing need to translate out-of-vocabulary words from one language to another, especially from alphabet language to Chinese, Japanese or Korean. The number of parameters in the bigram TM is potentially 2T , while in the noisy channel model (NCM) its 2CT + , where T  is the number of transliteration pairs and C is the number of Chinese transliteration units. E2C error rates for n-gram TM tests.
P04-1024@@ Multilingual processing in the real world often involves dealing with proper names. 6 on a test of surnames. This results in only 6 candidates for the segments ko-i-zu.
P04-1025@@ and related workA massive amount of information is buried inscientific publications (more than 500,000 pub-lications per year). GENIES: a natural-languageprocessing system for the extraction of molecularpathways from journal articles. The identity of the regulator (R) must beknown.
P04-1026@@ There are several situations in language research or language engineering where we are in need of a specific type of extra-linguistic information about a text (document) and we would like to determine this information on the basis of lin-guistic properties of the text. We did not use the full rewrites, but rather constituent N-grams. The bottom pane shows the area from 0.0 to 0. .
P04-1033@@ Text categorization is the task of classifying documents into a certain number of pre-defined categories. In general, since unlabeled data are much less expensive and easier to collect than labeled data, our method is useful for text categorization tasks including online data sources such as web pages, E-mails, and newsgroup postings. ;|(Vt itjtitjVtdwNjtjijijijdwPcwPdwPncPcwPcPdPcdPcPdcP i (11)   where i) n is the number of words in document di, ii) wt is the t-th word in the vocabulary, iii) N(wt,di) is the frequency of word wt in document di.
P04-1035@@The computational treatment of opinion, sentiment,and subjectivity has recently attracted a great dealof attention (see references), in part because of itspotential applications. , vn, s, t}; the last two are, respec-tively, the source and sink. For reviews with fewer than N sentences,the entire review will be returned.
P04-1036@@The first sense heuristic which is often used as abaseline for supervised WSD systems outperformsmany of these systems which take surrounding con-text into account. We also calculate the WSD accu-racy that would be obtained on SemCor, when usingour first sense in all contexts (  We repeated the experiment with the BNC data for jcn us-ing #3VE\E Both WordNet similarity measures beatthis baseline. The SFCcontains an economy label and a sports label.
P04-1038@@This paper addresses WSD in Chinese through developing an Expectation-Maximization (EM) clustering model to learn Chinese verb sense distinctions. In the maximization step (M-step), )(cp  and )|( cfp i  are re-computed by maximizing the log-likelihood of all the observed data which is calculated by using ),...,,|(~ 21 mfffcp  estimated in the E-step. /event /ASP           A big event happened in their county.
P04-1039@@Supervised Word Sense Disambiguation (WSD)systems perform better than unsupervised systems.But lack of training data is a severe bottleneckfor supervised systems due to the extensive la-bor and cost involved. There arethree options: French (FR), Spanish (SP), or,Merged languages (ML), where the results areobtained by merging the English output of FRand SP.% Threshold: Sense selection criterion, inSALAAM, is set to either MAX (M) orTHRESH (T).These factors result in 39 conditions. A method forWord Sense Disambiguation of unrestricted text.
P04-1040@@We describe a method to automatically enrich theoutput of parsers with information that is presentin existing treebanks but usually not produced bythe parsers themselves. E.g., one can think of simple heuris-tics to distinguish subject NPs, temporal PPs, etc.,thus introducing functional labels and improvingthe scores. Antecedent recov-ery: Experiments with a trace tagger.
P04-1041@@The determination of syntactic structure is an im-portant step in natural language processing as syn-tactic structure strongly determines semantic inter-pretation in the form of predicate-argument struc-ture, dependency relations or logical form. Semantic forms specify which grammaticalfunctions (GFs) a predicate requires locally. T. Maxwell III, and M. Johnson.
P04-1042@@While parsers are been used for other purposes, theprimary motivation for syntactic parsing is as anaid to semantic interpretation, in pursuit of broadergoals of natural language understanding. A dependency relation,commonly employed for evaluation in the statisticalparsing literature, is defined at a node N of a lexi-calized parse tree as a pair wi, wj We present here de-pendency evaluations where the gold-standard de-pendency set is defined by the remapped tree, typedgory of null insertions, whereas previous work has; as a result,the null complementizer class 0 and WH-t dislocation class areaggregates of classes used in previous work. The prefixes {,M,G,D,R} in-dicate that the feature value is calculated with re-spect to the node in question, its mother, grand-mother, daughter, or relative node respectively.
P04-1043@@Hence, to deal with natural lan-guage semantics, the learning algorithm shouldbe able to represent and process structureddata. This problem can be divided into twosubtasks: (a) the detection of the argumentboundaries, ie all its compounding words and(b) the classification of the argument type, egArg0 or ArgM in PropBank or Agent and Goalin FrameNet.The standard approach to learn both detec-tion and classification of predicate argumentsis summarized by the following steps:1. extract the feature representation set, Fp,a; if the subtree rooted in a covers exactly thewords of one argument of p, put Fp,a in T+(positive examples), otherwise put it in T  In case the node a exactly coversPaul, a lecture or in Rome, it will be a positiveinstance otherwise it will be a negative one, egFgive,IN.To learn the argument classifiers the T + setcan be re-organized as positive T +argi and neg-ative Targi examples for each argument i. Inthis way, an individual ONE-vs-ALL classifierfor each argument i can be trained. The Predicate Word is represented bythe fragment [V delivers] and the Head Wordis encoded in [N talk].
P04-1044@@A crucial problem in the design of spoken dialoguesystems is to decide for incoming recognition hy-potheses whether a system should accept (considercorrectly recognized), reject (assume misrecogni-tion), or ignore (classify as noise or speech not di-rected to the system) them. A Model of Dialogue Movesand Information State Revision. Our methodology consists oftwo steps: i) we automatically classify the n-bestrecognition hypotheses for an utterance as eithercorrectly or incorrectly recognized and ii) we use asimple selection procedure to choose the best hy-pothesis based on this classification.
P04-1045@@This paper explores the feasibility of automaticallypredicting student emotional states in a corpus ofcomputer-human spoken tutoring dialogues. H. Witten and E. Frank. Say anapple falls from a tree.
P04-1048@@Semantic content can almost always be expressedin a variety of ways. Second,classes of a FrameNet-like nature. The best balance for the SemFrameversion is based on a clustering threshold of 2.0and a minimum precision threshold of 0. , whichyields a recall of 83.
P04-1049@@ Automatic generation of text summaries is a natural language engineering application that has received considerable interest, particularly due to the ever-increasing volume of text information available through the internet. ]a [There is another train on Platform B. PRn is the PageRank of the current sentence, PRn-1 is the PageRank of the sentence that relates to sentence n, on-1 is the out-degree of sentence n-1, and  is a damping parameter that is set to a value between 0 and 1.
P04-1051@@One central problem in discourse generation andsummarisation is to structure the discourse in away that maximises coherence. {0, 1}We have a binary variable xe for each edge e ofthe graph. A centering approach to pronouns.
P04-1052@@Referring expression generation has historicallybeen treated as a part of the wider issue of gener-ating text from an underlying semantic representa-tion. A classification scheme for attributes exists.. Technical report,Princeton University, Princeton, N.J.Ehud Reiter.
P04-1053@@Although Internet search engines enable us to ac-cess a great deal of information, they cannot eas-ily give us answers to complicated queries, such asa list of recent mergers and acquisitions of com-panies or current leaders of nations from all overthe world. This paper does not necessarily reflectthe position of the U.S. Government.We would like to thank Dr. Yoshihiko Hayashiat Nippon Telegraph and Telephone Corporation,currently at Osaka University, who gave one of us(T.H.) Our approach is based on2A research and evaluation program in information extrac-tion organized by the U.S. Government.context based clustering of pairs of entities.
P04-1054@@The ability to detect complex patterns in data is lim-ited by the complexity of the datas representation.In the case of text, a more structured data source(eg a relational database) allows richer queriesthan does an unstructured data source (eg a col-lection of news articles). Much time is spent onthe task of feature engineering  Feature extraction andinduction may result in features such as part-of-speech, word n-grams, character n-grams, capital-ization, and conjunctions of these features. vd} consist of twopossibly overlapping subsets m(ti)  We use m(ti) in the matchingfunction and s(ti) in the similarity function.
P04-1057@@As we all know, hand-crafted linguistic descriptionssuch as wide-coverage grammars and large scaledictionaries contain mistakes, and are incomplete.In the context of parsing, people often construct setsof example sentences that the system should be ableto parse correctly. Another systematic mis-take is reflected by the last n-grams. In that case, the preposi-tional complement is R-pronominalized.
P04-1059@@ Chinese word segmentation has been a longstanding research topic in Chinese language proc-essing. Notice that Score(,S,W) is not less than Score(,S,R). 2 In our system, we define three types of named entity: person name (PN), location name (LN), organization (ON) and translit-eration name (TN); ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, phone number, and WWW; and five types of morphologically derived words (MDW): affixation, reduplication, merging, head particle and split.
P04-1060@@There have been a number of recent studies exploit-ing parallel corpora in bootstrapping of monolin-gual analysis tools. L and R are used for the begin-ning and end of blocks, when the adjacent boundaryzone is empty; l and r are used next to non-emptyboundary zones. Even examples of com-plicated structural divergence from the classical MT3I.e., an element of  	 (or   	 ) continues the  -string at the other end.
P04-1061@@We make no claims as to thecognitive plausibility of the induction mechanismswe present here; however, the ability of these sys-tems to recover substantial linguistic patterns fromsurface yields alone does speak to the strength ofsupport for these patterns in the data, and hence un-dermines arguments based on the poverty of thestimulus First, most state-of-the-art supervised parsers make use of specific lexi-cal information in addition to word-class level infor-mation  perhaps lexical information could be a use-ful source of information for unsupervised methods.Second, a central motivation for using tree struc-tures in computational linguistics is to enable theextraction of dependencies  and it might be more ad-vantageous to induce such structures directly. After an argument is generated, its subtree inthe dependency structure is recursively generated.Formally, for a dependency structure D, leteach word h have left dependents depsD(h, l)and right dependents depsD(h, r). For exam-ple, it is easy enough to discover that DET N andDET ADJ N are similar and that V PREP DET andV PREP DET ADJ are similar, but it is much lessclear how to discover that the former pair are gen-erally constituents while the latter pair are generallynot.
P04-1062@@Unlabeled data remains a tantalizing potential re-source for NLP researchers. Even so, the majority of time spentby EM for such models is on the E steps. In M. I.Jordan, editor, Learning in Graphical Models.
P04-1063@@As the Internet grows, an increasing number ofcommercial MT systems are getting on line readyto serve anyone anywhere on the earth. (e, j | l))return jkendup an LM voted for by the majority. At, a best performing OTS system, gets 0.
P04-1064@@Aligning words from mutually translated sentencesin two different languages is an important and dif-ficult problem. GeneralM-N alignments then correspond to M-1-N align-ments from e-words, to a cept, to f -words (fig. Matrix factorisationAlignments between source and target words maybe represented by a I  K scaling matrix S which may givedifferent weights to the different cepts.
P04-1065@@Finite-state automata (FSA) methods proved to el-egantly solve many difficult problems in the fieldof natural language processing. We will give a moredetailed analysis here. Experiment 2 deals with a translation prob-lem and splits words of a bilanguage T 1,201,718 3,572,601All experiments were performed on a PC with a1.
P04-1066@@It was originallydeveloped to provide reasonable initial parameterestimates for more complex word-alignment mod-els, but it has subsequently found a host of ad-ditional uses. Add-n smoothing is a way of smooth-ing with a uniform distribution, so it is not surpris-ing that it performs poorly in language modelingwhen it is compared to smoothing with higher or-der models; e.g, smoothing trigrams with bigramsor smoothing bigrams with unigrams. |V |(3)where C(t, s) is the expected count of s generatingt, C(s) is the corresponding marginal count for s,|V | is the hypothesized size of the target vocabularyV , and n is the added count for each target word inV .
P04-1067@@Comparable corpora contain texts written in differ-ent languages that, roughly speaking, talk aboutthe same thing. w ) consistingin the measure of association of each word e(resp. Textual similarities based on a distribu-tional approach.
P04-1069@@Context-free grammars (CFGs) are standardly usedin computational linguistics as formal models of thesyntax of natural language, associating sentenceswith all their possible derivations. and Nthe sets of terminals and nonterminals, respectively,S the start symbol and R the set of rules. Note that each dead computation has a uniqueprefix that is a shortest dead computation.
P04-1073@@ Traditionally, Question Answering (QA) has drawn on the fields of Information Retrieval, Natural Language Processing (NLP), Ontologies, Data Bases and Logical Inference, although it is at heart a prob-lem of NLP. Clarke, C., Cormack, G., Kisman, D.. and Lynam, T.  Question answering by passage selection (Multitext experiments for TREC-9) Hendrix, G., E. Sacerdoti, D. Sagalowicz, J. Slocum: Developing a Natural Language Interface to Com-plex Data. Two calculations of P/R/F are made, depending on whether the averaging is done over the whole set, or first by individual; the results are very similar.
P04-1075@@ In the machine learning approaches of natural lan-guage processing (NLP), models are generally trained on large annotated corpus. However these works just follow a single criterion. N is the number of the support vectors in current model.
P04-1076@@ Cross document name disambiguation is required for various tasks of knowledge discovery from textual documents, such as entity tracking, link discovery, information fusion and event tracking. Output { }MK ,  as a local optimal solution. For example, Bill Clinton exclusively stands for the previous U.S. president although in real life, although many other people may also share this name.
P04-1077@@ Using objective functions to automatically evalu-ate machine translation quality is not new. (P) correlation of ROUGE-S* with adequacy increases from 0. ROUGE: A Package for Automatic Evaluation of Summaries.
P04-1078@@An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations. BLEU: a Method for Automatic Evaluation of Machine Translation. For example, the ROUGE-like metric AEv(0,1) explains only 61.
P04-1079@@ Automatic methods for evaluating different as-pects of MT quality  such as Adequacy, Fluency and Informativeness  provide an alternative to an expensive and time-consuming process of human MT evaluation. Computers and Translation: a trans- Comparative stylistics of French and English : a methodology for translation / translated and edited by Juan C. Sager, M.-J.
P04-1080@@Almost all of sense disambigua-tion methods are heavily dependant on manuallycompiled lexical resources. Searching a mapping function to maximizethe accuracy of U can be formulated as: (19)In fact,i,j Qi,j is equal to N , the number ofoccurrences of target word in test set.. %a share in a company or business 20.
P04-1081@@Achieving higher precision in supervised wordsense disambiguation (WSD) tasks without resort-ing to ad hoc voting or similar ensemble techniqueshas become somewhat daunting in recent years,given the challenging benchmarks set by na We then discuss experimental re-sults confirming that this model outperforms state-of-the-art published models for Senseval-relatedlexical sample tasks as represented by (1) na veBayes models, as well as (2) maximum entropymodels. A topical/local classifier for word senseidentification. li is the lth element of  For each sentence, we show the fea-ture set associated with that occurrence of art The training and testing examples can berepresented as a set of binary vectors: each row shows the correct class c for an observed vector x of fivedimensions.TRAINING design/N media/N the/DT entertainment/N world/N Classx1 He studies art in London.
P04-1084@@Synchronous grammars have been proposed forthe formal description of parallel texts representingtranslations of the same document. In Proceedings ofthe 5th Meeting of the European Chapter of the Associationfor Computational Linguistics (EACL), Berlin, Germany.E. This capabilityenables a synchronous grammar to govern lower-dimensional sublanguages/translations.
P04-1085@@One of the main features of meetings is the occur-rence of agreement and disagreement among par-ticipants. of the ACL Workshopon Multilingual Summarization and Question An-swering.E. A. Schegloff and H Sacks.
P04-1087@@This paper is concerned with automatically acquir-ing the meaning of discourse markers. STRUCTURAL-SKELETON identi-fied the major constituents under the S or VP nodes,eg a simple double object construction gives NPVB NP NP. In Proceedings of the AAAISpring Symposium on Empirical Methods in Dis-course Interpretation and Generation.Ted J. M. Sanders, W. P. M. Spooren, and L. G. M. No-ordman.
P04-1088@@In this paper, we propose Feature Latent SemanticAnalysis (FLSA) as an extension to Latent Seman-tic Analysis (LSA). Suppose the data set S is classi-fied using n categories v1...vn, each with probabil-ity pi. Cell c(i, j) contains the frequency with whichwordi appears in documentj .
P04-2005@@Lexical knowledge is crucial for many NLP tasks.Huge efforts and investments have been made tobuild repositories with different types of knowl-edge. Thus, the Concept Spacewas formed by collecting a n-by-2, 500 matrix M ,such that element mij records the number of timesthat concept i and j co-occur in a window, wheren is the number of concept vectors that occur inthe corpus. At the end of this step, we haveproduced a set C, which consists of Chinese words{c1, c2, ..., cn}, where ci is the translation correspond-ing to sense si of w, and n is the number of senses thatw has.
P04-2007@@Lexical semantic classes group together words thathave a similar meaning. It givesa value of 1 if the two classifications agree com-No Named EntitiesTask Mean Sil Baseline Radj3-way 0. Clustering Verbs into ClassesWe use a bottom-up hierarchical clustering algo-rithm to group together 514 verbs into K classes.The algorithm starts by finding the similarities be-tween all the possible pairs of objects in the data ac-cording to a similarity measure S. After having es-tablished the distance between all the pairs, it linkstogether the closest pairs of objects by a linkagemethod L, forming a binary cluster.
P04-2012@@ Many natural language processing tasks, includ-ing parsing and machine translation, frequently require a morphological analysis of the language(s) at hand. In particular, two types of relations are defined: 1) C-suffix set inclusion relations relate pairs of CICs when the c-suffixes of one CIC are a superset of the c-suffixes of the other, and 2) Morpheme boundary relations occur be-tween CICs which propose different mor-Inflection Classes Verb Paradigm A B C Basic blame roam solve show sow saw sing ring 3rd Person Singular     Non-past -/z/ blames roams solves -/z/ shows sows saws -/z/ sings rings  Past -/d/ blamed roamed solved -/d/ showed sowed sawed V /eI/ sang rang  Perfective       or Passive -/d/ blamed roamed solved -/n/ shown sown sawn V The c-suffix set inclusion relations, represented vertically by solid lines, connect such CICs as e.es.ed and e.ed, both of which originate from the c-stem blam, since the first is a superset of the sec-ond. Morpheme boundary relations, drawn hori-zontally with dashed lines, connect such CICs as me.mes.med and e.es.ed, each derived from ex-actly the triple of word forms blame, blames, and blamed, but differing in the placement of the hy-pothesized morpheme boundary Hierarchical links, connect any given CIC to of-ten more than one parent and more than one child.
P04-3003@@ Machine transliteration plays an important role in machine translation. A better TSA would produce better results. /l/ is a syllable with only an isolated consonant.
P04-3009@@Most surface realiz-ers have been symbolic, grammar-based systemsusing syntactic linguistic theories like HPSG.These systems were often developed as eitherproof-of-concept implementations or to supportlarger end-to-end NLG systems which have pro-duced limited amounts of domain-specic texts.As such, determining the generic coverage ofa language has been substituted by the goal ofproducing the necessary syntactic coverage for aparticular project. This pa-per represents the analogous eort for a sym-bolic generation system using an enhanced ver-sion of the Fuf/Surge systemic realizer. Building a large annotated corpus ofEnglish: The PennTreeBank.
P04-3010@@Part-of-speech (POS) tagging is a job to assign aproper POS tag to each linguistic unit such as wordfor a given sentence. In English POS tagging, wordis used as a linguistic unit. Lee, and H.-C. Rim.
P04-3014@@Word-level alignment is a key infrastructural tech-nology for multilingual processing. F-measurelog(number of training sentences)E F-measurelog(number of training sentences)E Ineach case, our heuristics transform the English treesto achieve these same word orders. A cheap and fast way to builduseful translation lexicons.
P04-3018@@As systems improve, the availabilityof rich resources will be increasingly critical to QAperformance. The web as a resource for questionanswering: Perspectives and challenges. 0, and 100 documents retrieved via the Google APIfor each question, and extracted the most frequentfifty n-grams (up to trigrams).
P04-3019@@ Collocations are a phenomenon of word combination occurring together relatively often. Examples show as follows (additional clause tags will be attached):  (1) .the attitude (S* he has *S) toward the country (2) (S* I think (S* that the people are most concerned with the question of (S* when conditions may become ripe. "A Word-to-Word Model of Translational Equivalence".
P04-3020@@We showthat the results obtained with this new unsupervisedmethod are competitive with previously developedstate-of-the-art systems. Authoritative sources in a hyperlinked environ-ment. 1: On Saturday, Hurricane Florence was downgraded to a tropical storm, and its remnants       pushed inland from the U.S. Gulf Coast.
P04-3022@@Extraction of semantic relationships between en-tities can be very useful for applications such asbiography extraction and question answering, egto answer queries such as Where is the Taj Ma-hal. The addition of entity types, mention levelsand especially, the word proximity features (over-lap) boosts the recall at the expense of the very3The F-measure is the harmonic mean of the precision, de-fined as the percentage of extracted relations that are valid, andthe recall, defined as the percentage of valid relations that areextracted.Features P R F ValueWords 58. A statistical model for multilin-gual entity detection and tracking.
P04-3024@@Text classification is the assignment of predefinedcategories to text documents. , d|S|} be the set of training documents, anddenote the class of di with c(di). In order to avoid this additional com-plexity, instead of KLt(S) we use an approxima-tion KLt(S), which is based on the following twoassumptions: (i) the number of occurrences of wtis the same in all documents that contain wt, (ii)all documents in the same class cj have the samelength.
P04-3028@@ In this paper we investigate the automatic labeling of spoken dialogue data, in order to train a classifier that predicts students emotional states in a human-human speech-based tutoring corpus. References  A. Blum and T. Mitchell. 6 Acknowledgements Thanks to R. Pelikan, T. Singliar and M. Hauskrecht for their contribution with Feature Selection, and to the NLP group at University of Pittsburgh for their helpful comments.
P04-3032@@Computational linguistics has become a more experi-mental science. Also, EM can be applied whereappropriate, since it can be shown that EMs E counts canbe derived from the gradient. Instead ofbeing integer positions in an string, I, J and K can besymbols denoting states in a finite-state automaton.
P05-1001@@In supervised learning applications, one can oftenfind a large amount of unlabeled data without diffi-culty, while labeled data are costly to obtain. :L() is a loss function to quantify the differencebetween the prediction f(Xi) and the true outputYi, and r() is a regularization term to control themodel complexity. Named en-tity recognition with a maximum entropy approach.
P05-1002@@CRFs are undirected graphicalmodels that define a conditional distribution overlabel sequences given an observation sequence.They allow the use of arbitrary, overlapping,non-independent features as a result of their globalconditioning. The time complexity of a singleiteration is O(L2NTF ) where L is the numberof labels, N is the number of sequences, T isthe average length of the sequences, and F isthe average number of activated features of eachlabelled clique. A joint tagging accuracy of90.
P05-1003@@In general, this workhas demonstrated the susceptibility of CRFs to over-fit the training data during parameter estimation. The log-likelihood is given by:L ( ) (o) are empirical distributionsdefined by the training set. For NERan expert corresponding to label X consistsonly of features that involve labels B-X or I-X at the current or previous positions, while forPOS tagging an expert corresponding to labelX consists only of features that involve labelX at the current or previous positions.
P05-1004@@Lexicographers cannot possibly keep pacewith language evolution: sense distinctions are contin-ually made and merged, words are coined or becomeobsolete, and technical terms migrate into the vernacu-lar. Class-based probabilityestimation using a semantic hierarchy. The best per-formance on the 1.  test set was achieved with the SCOREvoting, without sharing or ranking penalties.The extracted synonyms are filtered before contribut-ing to the vote with their supersense(s).
P05-1005@@Word Sense Disambiguation (WSD) is the task ofdetermining the meaning of a word in a given con-text. University of Chicago Press, Chicago, IL.N Littlestone and M.K. Instance weighting explainedThe exemplar weights were derived from the fol-lowing method:1. pick a labelled example e, and extract its sensese and semantic class ce.
P05-1006@@A word can have different meanings dependingon the context in which it is used. English Verb Classes and Alterna-tions: A Preliminary Investigation. Ph.D. thesis,University of Pennsylvania Department of Computer
P05-1007@@We also employed taskefficacy, as we evaluated the learning that occursin students interacting with an Intelligent TutoringSystem (ITS) enhanced with NLG capabilities. In L. PytlikZillig, M. Bodvarsson,and R. Brunin, editors, Technology-based education:Bringing researchers and practitioners together. Natural languagegeneration for intelligent tutoring systems: a casestudy.
P05-1008@@ This paper1 is concerned with the problem of con-trolling the output of natural language generation (NLG) systems. producer output 1 output 2 output m output 3 output 4 ...  sorting dimension sorter output 3 output 1 output 14 output 10 output m ...  ...  ...  ... Cahill, Lynne; J. Carroll; R. Evans; D. Paiva; R. Power; D. Scott; and K. van Deemter From RAGS to RICHES: exploiting the potential of a flexible generation architecture. However, this approach suffers from a number of drawbacks: 1.
P05-1009@@Many of todays most popular natural language ap-plications  Machine Translation, Summarization,Question Answering  are text-to-text applications.That is, they produce textual outputs from inputs thatare also textual. Ph.D. thesis, Columbia University.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer. We control the unfolding using a prob-abilistic beam !
P05-1010@@In thoseparsers, the strong conditional independence as-sumption made in vanilla treebank PCFGs is weak-ened by annotating non-terminal symbols with manyfeatures Exam-ples of such features are head words of constituents,labels of ancestor and sibling nodes, and subcatego-rization frames of lexical heads. * a Viterbi complete tree. The heldout data was used for early stop-ping; ie, the estimation was stopped when the rate79H If-, is not a pre-terminal node, for each I --fi8-, let SH If-, is a pre-terminal node above word    //.H If-, is a root node, letbe the non-terminal symbol of-.
P05-1011@@A large treebank can be used astraining and test data for statistical models. A Gaussian prior forsmoothing maximum entropy models. Toutanova and C. D. Manning.
P05-1013@@However, this argumentis only plausible if the formal framework allowsnon-projective dependency structures, ie structureswhere a head and its dependents may correspondto a discontinuous constituent. Even this maybe nondeterministic, in case the graph contains sev-eral non-projective arcs whose lifts interact, but weuse the following algorithm to construct a minimalprojective transformation D Instead, we want to apply an in-101Lifted arc label Path labels Number of labelsBaseline d p nHead dh p n(n+ 1)Head+Path dh p In order to facilitatethis task, we extend the set of arc labels to encodeinformation about lifting operations. R,(c) for every wj W , there is at most one arc(wi, r, wj)  00(Only one of them concerns quality.
P05-1014@@ Distributional Similarity between words has been an active research area for more than a decade. A General Frame-work for Distributional Similarity. Thus, in virtually all the exam-ples tested in our experiment Hypothesis I was valid.
P05-1015@@There has recently been a dramatic surge of inter-est in sentiment analysis, as more and more peoplebecome aware of the scientific challenges posed andthe scope of new applications enabled by the pro-cessing of subjective language. Any opinions, findings, and conclusions or recommen-dations expressed are those of the authors and do not neces-sarily reflect the views or official policies, either expressed orimplied, of any sponsoring institutions, the U.S. government, orany other entity.ReferencesAtkeson, Christopher G., Andrew W. Moore, and Stefan Schaal. Can you do just aswell if you penalize all non-equal label assignmentsby the same amount, or does the distance betweenlabels really matterA: Youre asking for a comparison to the Pottsmodel, which setsTto the function T	Uffi l if   In the one set-ting in which there is a significant differencebetween the two, the Potts model does worse(ova I PSP  ).Q: Your datasets had many labeled reviews and onlyone author each.
P05-1016@@ Despite considerable effort, there is still today no commonly accepted semantic corpus, semantic framework, notation, or even agreement on pre-cisely which aspects of semantics are most useful (if at all). Riloff, E. and Shepherd, J. Chklovski, T., and Pantel, P. VERBOCEAN: Mining the Web for Fine-Grained Semantic Verb Relations.
P05-1017@@Identification of emotions (including opinions andattitudes) in text is an important task which has a va-riety of possible applications. DL)0 otherwise, (8)where lij denotes the link between word i and wordj, and d(i) denotes the degree of word i, whichmeans the number of words linked with word i. Twowords without connections are regarded as beingconnected by a link of weight 0. E(x,W )) is the nor-malization factor, which is called the partitionfunction and  is a constant called the inverse-temperature.
P05-1018@@A key requirement for any system that producestext is the coherence of its output. , pm(xi j)), where m is the num-ber of all predefined entity transitions, and pt(xi j)the probability of transition t in grid xi j . Centering: a parametric theory and its instan-tiations.
P05-1019@@Discourse coherence relations contribute to themeaning of texts, by specifying the relationships be-tween semantic objects such as events and propo-sitions. ExperimentsWe now describe our empirical experiments whichinvestigate the connections between a) subjects rat-ings of the similarity of discourse connectives, b)the substitutability of discourse connectives, and c)KL divergence and the new function V applied tothe distributions of connectives. Ph.D. thesis, University of Edinburgh.George A. Miller and William G. Charles.
P05-1020@@theproblem of determining which noun phrases (NPs)in a text or dialogue refer to which real-worldentity  Thecentral idea behind the majority of these learning-based approaches is to recast coreference resolutionas a binary classification task. As we can see,the N&C system outperforms the Duplicated Soonsystem by about 2-6% on the three ACE data sets. Space limitationspreclude a description of these features.
P05-1021@@Semantic compatibility is an important factor forpronoun resolution. A possible solution is the165web. Conference onComputational Linguistics, pages 869875.N.
P05-1022@@We describe a reranking parser which uses a reg-ularized MaxEnt reranker to select the best parsefrom the 50-best parses returned by a generativeparsing model. This insight extendsto n-best parsing as follows. A comparison of algorithms formaximum entropy parameter estimation.
P05-1023@@Kernel methods have been shown to be very ef-fective in many machine learning problems. R. The parse tree y with thelargest value for this discriminant function F (x, y)is the output parse tree for the sentence x. with smooth parameterization,the Fisher score of an example z is a vector of partialderivatives of the log-likelihood of the example withrespect to the model parameters: l ).This score can be regarded as specifying how themodel should be changed in order to maximize thelikelihood of the example z. An efficient implementation of a newDOP model.
P05-1025@@Automatic syntactic analysis of natural language hasbenefited greatly from statistical and corpus-basedapproaches in the past decade. (e) Before he told the story [,] he was cold.Once again, sentences (d) and (e) have identicalpart-of-speech sequences, but only sentence (e) fea-tures a fronted subordinate clause. %, with a mean of 92.
P05-1026@@In this paper, we propose a new architecture forinteractive question-answering based on predictivequestioning. The questions that aregenerated are based on question patterns asso-ciated with causal relations and therefore allowdifferent degrees for the specificity of the resul-tative, i.e obstacle or deterrent.We generated several questions for each answerpassage. Inthis case, if the QUAB question ( T U ) that wasdeemed to be most similar to a user question( T) under Similarity Metric 5 is containedin the cluster of QUAB questions deemed tobe most similar to Tunder Similarity Metric6, then TVU receives a cluster adjustment scorein order to boost its ranking within its QUABcluster.
P05-1027@@The question type is also used for selectinganswer candidates. 21Appendix: Analysis of Evaluation Results w.r.t.Question Type  Results of QBTE from the first-ranked paragraph (NB: No information about thesequestion types was used in the training phrase. right before and afteranswer a in d.. Morphologically analyze d. .
P05-1033@@Phrases, which can be any substring and notnecessarily phrases in any syntactic theory, allowthese models to learn local reorderings, translationof short idioms, or insertions and deletions that aresensitive to local context. Using thesettings described above, on a 2. wp|e|),the word penalty, gives some control over the lengthof the English output.We have separated these factors out from the ruleweights for notational convenience, but it is concep-tually cleaner (and necessary for polynomial-timedecoding) to integrate them into the rule weights,so that the whole model is a weighted synchronousCFG.
P05-1034@@ Over the past decade, we have witnessed a revolution in the field of machine translation (MT) toward statistical or corpus-based methods. The difference between Pharaoh and the Treelet system is significant at the 99% confidence level under a two-tailed paired t-test. A syntax-based statistical translation model.
P05-1036@@Summarization in general, and sentence compres-sion in particular, are popular topics. We even-tually truncate the list to the best 25, still based uponPexpand(l | s). trees A and Brespectivelyto make.
P05-1037@@The availability of many chat forums reflects theformation of globally dispersed virtual communi-ties. Lam and S. L. Rohall. Exploiting e-mailstructure to improve summarization.
P05-1038@@This paper brings together two strands of researchthat have recently emerged in the field of probabilis-tic parsing: crosslinguistic parsing and lexicalizedparsing. For example, themodel outputs:(6) (NCmp (N jours) (N commercants))But in the gold standard file, jours and commercantsare two distinct NPs. d(i) is a distance measure, afunction of the length of the surface string betweenthe head and the previously generated sister.Collins The generative pro-cess is enhanced to include a probabilistic choice ofleft and right subcategorization frames.
P05-1039@@A likely cause is the relative produc-tivity of German morphology compared to that ofEnglish: German has a higher type/token ratio forwords, making sparse data problems more severe.There are at least two solutions to this problem: first,to use better models of morphology or, second, tomake unlexicalized parsing more accurate.We investigate both approaches in this paper. Forboth the Witten-Bell and Kneser-Ney algorithms,the  s are a function of the context A  Bi  2  Bi  1. TnT: A statistical part-of-speechtagger.
P05-1040@@Annotated corpora have at least two kinds of uses:firstly, as training material and as gold standardtesting material for the development of tools in com-putational linguistics, and secondly, as a source ofdata for theoretical linguists searching for analyti-cally relevant language patterns.Annotation errors and why they are a problemThe high quality annotation present in gold stan-dard corpora is generally the result of a manualor semi-automatic mark-up process. Thus,there is no variation and no tractability problem inconstructing n-grams. A Comparison between Syn-tactic Analyses for Constituent Order and TheirProcessing Systems.
P05-1041@@It is important for an annotated corpus that the mark-up is both correct and, in cases where variant anal-yses could be considered correct, consistent. %.We did not include A and E, as there was variation indifficulty between test sets, and it is well known thatannotators improve (at least in speed of annotation)over time. Syntac-tic annotation of a German newspaper corpus.
P05-1044@@Even when theyavoid local maxima (eg, through clever initializa-tion) they typically deviate from human ideas ofwhat the right The views expressed are not nec-essarily endorsed by the sponsors. Because the features cantake any form and need not be orthogonal, log-linearmodels can capture arbitrary dependencies in thedata and cleanly incorporate them into a model.Z(~) (the partition function) is chosen so that All the ob-jective functions in this paper take the formi Ymarginal (a la` EM) {xi}  Bi (for each i). Rn specifies a weight for each of the n transi-tions in the automaton.
P05-1045@@Most statistical models currently used in natural lan-guage processing represent only local structure. Van Laarhoven and E. H. L. Arts. R. Curran and S. Clark.
P05-1046@@Information extraction is potentially one of the mostuseful applications enabled by current natural lan-guage processing technology. All results are averaged over 50 runs.annotated documents to the expected counts com-puted in the E-step. and a in quiet with unit building6 .
P05-1047@@Developing systems which can be easily adapted tonew domains with the minimum of human interven-tion is a major challenge in Information Extraction(IE). Assume that the set of pat-terns, P , consists of n element-filler pairs denotedby p1, p2, ...pn. MINIPAR: a minimalist parser.
P05-1049@@In this paper, we address the problem of word sensedisambiguation (WSD), which is to assign an appro-priate sense to an occurrence of a word in a givencontext. The distance measure can be automaticallyselected by minimizing the average value of functionH(D), H(W ) or H(YU ) over 20 trials.Let Q be the M N matrix. (a) Minimum spanning tree of this dataset.
P05-1050@@The main limitation of many supervised approachesfor Natural Language Processing (NLP) is the lackof available annotated training data. DVs for texts are estimated by ex-ploiting the formula 1, while the DV ~wi, correspond-ing to the word wi  V is the ith row of the domainmatrix D. To be a valid domain matrix such vectorsshould be normalized (i,e. k eigen-values of T, and all the remaining elements set to0.
P05-1051@@ Systems which extract relations or events from a document typically perform a number of types of linguistic analysis in preparation for information extraction. Using N-best Lists for Named Entity Recognition from Chinese Speech. In symbols:  Si  is the i-th sentence in the document.
P05-1052@@ Information extraction subsumes a broad range of tasks, including the extraction of entities, relations and events from various text sources, such as newswire documents and broadcast transcripts. A. Culotta and J. Sorensen. Illustration of a relation example R. The link sequence is generated from seq by removing some unimportant words based on syntax.
P05-1053@@ With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in tech-niques for automatically extracting information from text. Roth D. and Yih W.T. Culotta A. and Sorensen J.
P05-1054@@Linguistic and prosodic differences between gen-ders in American English have been studied fordecades. In text classification, the objective isto classify a document ~d to one (or more) of T pre-defined topics y. In our work, the worddude emerged as a male feature.
P05-1055@@Text search in particular is the most activearea, with applications that range from web and in-tranet search to searching for private information re-siding on ones hard-drive.Speech search has not received much attentiondue to the fact that large collections of untranscribedspoken material have not been available, mostlydue to storage constraints. (w,word(n))The Position Specific Posterior Lattice (PSPL) is arepresentation of the P (w, l|LAT ) distribution: foreach position bin l store the words w along with theirposterior probability P (w, l|LAT ). Of-ten they have a title, author and creation date.
P05-1056@@Standard speech recognizers output an unstructuredstream of words, in which the important structuralfeatures such as sentence boundaries are missing.Sentence segmentation information is crucial and as-sumed in most of the further processing steps thatone would want to apply to such output: taggingand parsing, information extraction, summarization,among others. E represent the statetags (ie, sentence boundary or not). Mallet: A machine learning for languagetoolkit.
P05-1057@@Instatistical machine translation, word alignment playsa crucial role as word-aligned corpora have beenfound to be an excellent source of translation-relatedknowledge.Various methods have been proposed for findingword alignments between parallel texts. , fTJ as POS tagsequences of the sentence pair e and f . Goto 2.The gain threshold t depends on the added linkl.
P05-1058@@In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. 0% and a recall of 75. ie1p  is the fertility probability for e , and .
P05-1059@@Thealgorithm builds a synchronous parse tree for bothsentences, and assumes that the trees have the sameunderlying structure but that the ordering of con-stituents may differ in the two languages.This probabilistic, syntax-based approach has in-spired much subsequent reasearch. For Expectation Max-imization training, we compute lexicalized insideprobabilities  (X, l,m, i, j), fromthe bottom up as outlined in Algorithm 1.The algorithm has a complexity of O(N4sN4t ),where Ns and Nt are the lengths of source and tar-get sentences respectively. The probabilitiesof the unary head generation rules are in the form ofP (X(e/f) | X).
P05-1060@@Such systems arewidely applicable, yet there remain many informa-tion extraction tasks that are not readily amenable tothese methods. Mallet: A machine learning for languagetoolkit.U. In Proceedings ofACL, pages 4147.E.
P05-1061@@Both kinds of mod-els have been developed for tagging entities suchas people, places and organizations in news mate-rial. One suchsentence is, John is C.E.O. MALLET: A machine learningfor language toolkit.D.M.
P05-1063@@The predominant approach within language model-ing for speech recognition has been to use an n-gram language model, within the source-channelor noisy-channel The language modelassigns a probability Pl(w) to each string w in thelanguage; the acoustic model assigns a conditionalprobability Pa(a|w) to each pair (a,w) where a is asequence of acoustic vectors, and w is a string. Precise n-gramprobabilities from stochastic context-free grammars. The AT&T 1xRTCTS system.
P05-1065@@The U.S. educational system is faced with the chal-lenging task of educating growing numbers of stu-dents for whom English is a second language (U.S.Dept. A maximum entropy part-of-speech tagger. Inthe same year, one quarter of all public school stu-dents in California and one in seven students inTexas were classified as LEP (U.S. Dept.
P05-1066@@in one language to be translated directly into phrases in another language. However, we believe it is a reasonable approximation to an ideal. The lack of per-sentence scores means that it is not possible to apply standard statistical tests such as the sign test or the t-test (which would test the hypothesis  	  0	 ,where   is the expected value under	 ). If  fi	   fi   If  fi   fi. Unfortunately Bleu scores do not give per-sentence measures E and    , and thus don ot allow a definition of    in this way.
P05-1067@@Such approaches, which are es-sentially stochastic string-to-string transducers, do not explicitly model natural language syntax or semantics. Let f  be the input sentence (foreign language), and e  be the output sentence (English). stand for individual and cumula-tive n-gram scores.
P05-1069@@In this paper, we present a block-based model for statis-tical machine translation. e isthe source phrase length and f is the target phrase length.Single source and target words are denoted by ^g anda respectively, where hHdi+i+ie and jHdi+ikiMf .We will also use a special single-word block set Z`lZwhich contains only blocks for which eHfH . 10, where the trainingtakes only a few seconds.
P05-1071@@Arabic is a morphologically complex language. He concludes that for highly inflectionallanguages the use of an independent morpholog-3Hajic For alllanguages, the use of a morphological analyzer re-sults in tagging error reductions of at least 50%.We depart from Hajic s work in several respects.First, we work on Arabic. is tagged as AV, N,or V (in the same syntactic contexts).
P05-1072@@The architec-ture underlying all of these systems introduces twodistinct sub-problems: the identification of syntacticconstituents that are semantic roles for a given pred-icate, and the labeling of the those constituents withthe correct semantic role.A detailed error analysis of our baseline systemindicates that the identification problem poses a sig-nificant bottleneck to improving overall system per-formance. To achieve this, weperformed a simple feature selection procedure. Argument Identification and Classification A combination of the above two tasks.ALL ARGs Task P R F1 A(%) (%) (%)HAND Id.
P05-1074@@Paraphrases are alternative ways of conveying thesame information. In Cecile L. Paris, William R.Swartout, and William C. Mann, editors, Natural Lan-guage Generation in Artificial Intelligence and Com-putational Linguistics. Extract-ing paraphrases from a parallel corpus.
P05-1075@@ The ordinary vocabulary of a language like English contains thousands of phrasal terms -multiword lexical units including compound nouns, technical terms, idioms, and fixed collocations. A Statistical Corpus-Based Term Extractor. Yet many of the best-performing lexical association measures, such as the t-test, assume normal distributions, (cf.
P05-1076@@Research into automatic acquisition of lexical in-formation from large repositories of unannotatedtext (such as the web, corpora of published text,etc.) In Proceedingsof the 5th Conference on Applied Natural LanguageProcessing, Washington DC, USA.E. Inheritanceand complementation: A case study of easy adjec-tives and related nouns.
P05-1077@@In the last decade, the field of Natural Language Pro-cessing (NLP), has seen a surge in the use of cor-pus motivated techniques. The above theorem states that the prob-ability that a random hyperplane separates two vec-tors is directly proportional to the angle between thetwo vectors (i,e.,  Thus, wehave Pr[u.r  Note that the above equa-tion is probabilistic in nature. (We can assume q to be a constant).
P05-2001@@Unknown words constitute a major source of diffi-culty for Chinese part-of-speech (POS) tagging, yetrelatively little work has been done on POS guess-ing of Chinese unknown words. Thefifth rule tags words such as d asverbs, and the additional constraints prevent nounssuch as fu-xia`ng lying-elephant The last rule tags words such as xue-be`i snow-quilt as nouns, but not zhai-shao pick-tippick the tips.One derivation rule for trisyllabic words has a spe-cial status. Precision for trisyllabic words isvery high, but recall is low.Chars Data R P F2 Training 24.05 96.
P05-2004@@Traditionally, various sequence labeling problems innatural language processing are solved by the cas-cading of well-defined subtasks, each extracting spe-cific knowledge. A maximum entropy model for part-of-speech tagging. Ghahramani and M. I. Jordan.
P05-2007@@ and Motivations American Sign Language (ASL) is a full natural language  with a linguistic structure distinct from English  Technology for the deaf rarely addresses this literacy issue; so, many deaf people find it dif-ficult to read text on electronic devices. A fluent, useful English-to-ASL MT system cannot ignore CPs. Acknowledgments  I would like to thank my advisors Mitch Marcus and Martha Palmer for their guidance, discussion, and revisions during the preparation of this work.
P05-2008@@Recent years have seen an increasing amount of re-search effort expended in the area of understandingsentiment in textual resources. % when predictingthe sentiment of articles from M&A. In B. Scholkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods Support VectorLearning.
P05-2010@@Consider a text starting with Mother died to-day. The kappastatistic: a second look. A random variable ranging between 0 and 20 says howmany random people marked an item as anchored.
P05-2012@@Fromoriginal word-based models, results were further im-proved by the appearance of phrase-based transla-tion models.However, many SMT systems still ignore anymorphological analysis and work at the surface levelof word forms. Byconsidering many different phrases as differentinstances of a single phrase class, we reduce the sizeof our phrase-based (now class-based) translationmodel and increase the number of occurrences ofeach unit, producing a model Pr(E|F ) Phrases not occurring inthe training data can still be classified into a class,and therefore be assigned a probability in the trans-lation model. Miller, and R. Tengi.
P05-2013@@Turkish is an agglutinating language, a single wordcan be a sentence with tense, modality, polarity, andvoice. We are also considering the use of un-labelled data to learn word-category pairs.ReferencesA.E. A quasi-arithmetic descrip-tion for syntactic description.
P05-2019@@The common de-nominator is the notion of topic as what an utter-ance is about. In reflexiveconstructions like (4), the non-reflexive NP, in thiscase jeg I, is considered the best representative. Centering: a framework for modeling the lo-cal coherence of discourse.
P05-2020@@Information Structure (IS) is a partitioning of thecontent of a sentence according to its relation tothe discourse context. I, ty you, ted now, tady here), weak pronouns, pronominal expressionswith a general meaning (nekdo somebody, jednou once) (unless they carry IC)t8. Consider, for example, the case in which the test set consistsof 70% f items and 30% t items.
P05-2021@@Large vocabulary continuous speech recognition ofinflective languages is a challenging task for mainlytwo reasons. Supplementary words form a loop in the back-offstate. Average length of a testimonyis 1.  hours.0 minutes from each testimony were transcribedand used as training data.
P05-2023@@The recognition of foreign words and foreign namedentities (NEs) in otherwise mono-lingual text is be-yond the capability of many existing approaches andis only starting to be addressed. as both data sets contain words like E-Mail orE-Business. The WWW as a resourcefor example-based machine translation tasks.
P05-3001@@People work together to make sure they understandone another. In any state, Dand M have agreed on a target variable T and a set ofconstraints that the value of T must satisfy. Linguistic RepresentationsThe way utterances signal task contributions isthrough a collection of presupposed constraints.
P05-3003@@One of the most exciting recent developments incomputational linguistics is that large-scale gram-mars which compute semantic representations arebecoming available. The evaluation wasdone using a 1. This datastructure maps each subgraph G to a set of splits,each of which records which fragment of G shouldbe placed at the root of the solution, what the sub-graphs after removal of this fragment are, and howtheir solutions should be plugged into the holes ofthe fragment.
P05-3006@@ Much of effort in Question Answering has focused on the short answers or factoid questions, which answer questions for which the correct response is a single word or short phrase from the answer sentence. Descriptive answer sentences generally have a particular syntactic structure. We obtained F-score of 0.
P05-3007@@ Until recently the NLP systems developed for processing clinical texts have been narrowly fo-cused on a specific type of document such as radi-ology reports [1], discharge summaries [2], medline abstracts [3], pathology reports [4]. The POS tagger annotator attaches a part of speech tag to each token. 9 for diabetes mellitus and 0.
P05-3012@@COMIC1 is an EU IST 5th Framework project com-bining fundamental research on human-human inter-action with advanced technology development formultimodal conversational systems. In the latter case, a newDAF is pushed onto the stack and executed. APML, amark-up language for believable behaviour generation.In H Prendinger, editor, Life-like Characters, Tools,Affective Functions and Applications, pages 6585.Springer.Mary Ellen Foster and Michael White.
P05-3014@@The task of word sense disambiguation consists ofassigning the most appropriate meaning to a polyse-mous word within a given context. Wordnet: A lexical database. Timbl: Tilburg memory based learner,version 4.0, reference guide.
P05-3020@@ Whereas most previous statistical work concerning parts of speech has been on tagging, this paper deals with part-of-speech induction. In this paper we use a straight-forward approach for evaluation. Class-based n-gram models of natural language.
P05-3021@@The TARSQI Project (Temporal Awareness andReasoning Systems for Question Interpretation)aims to enhance natural language question an-swering systems so that temporally-based questionsabout the events and entities in news articles can beaddressed appropriately. Forthcoming.TANGO: A Graphical Annotation Environment forOrdering Relations. It han-dles three different cases at present: (i) the eventis anchored without a signal to a time expressionwithin the same clause, (ii) the event is anchoredwithout a signal to the document date speech timeframe (as in the case of reporting verbs in news,which are often at or offset slightly from the speechtime), and (iii) the event in a main clause is anchoredwith a signal or tense/aspect cue to the event in themain clause of the previous sentence.
P05-3022@@Building a robust spoken dialogue system for a newtask currently requires considerable effort,  includ-ing  extensive  data  collection,  grammar  develop-ment, and building a dialogue manager that drivesthe  system using its  "back-end" application (egdatabase query, planning and scheduling). The  collaboration  managerqueries a domain-specific task component in orderto  make  decisions  about  interpretations  and  re-sponses. (Not  necessarily  as  discretestages; these elements may be interleaved as appro-priate for the specific item(s) and setting.)
P05-3023@@In this paper we give a brief description of atwo-way speech-to-speech translation system,which was created under a collaborative effortbetween three organizations within USC (theSpeech Analysis and Interpretation Lab of theElectrical Engineering department, the InformationSciences Institute, and the Institute for CreativeTechnologies) and the Information Sciences Lab ofHRL Laboratories. The Farsi acoustic models r e-quired an eclectic approach due to the lack of ex-isting labeled speech corpora. Modules communicate using a centralized mes-sage-passing system.
P05-3025@@There are many new approaches to statistical ma-chine translation, and more ideas are being sug-gested all the time. For example,if one has a noun-phrase, a verb-phrase, and a pe-riod, the user can search for the rule that connectsthem and builds an S on top, completing the sen-tence. The results of a search are presented in a list,again ordered by frequency.A few more features to note are: 1) loading andsaving your work at any point, 2) adding free-formnotes to the document (eg I couldnt find a rulethat... ), and 3) manually typing rules if one cannotbe found by the above methods.
P05-3026@@ A variety of different paradigms for machine translation (MT) have been developed over the years, ranging from statistical systems that learn mappings between words and phrases in the source language and their corresponding translations in the target language, to Interlingua-based systems that perform deep semantic analysis. Learning to Select a Good Translation. 405 Choosing best original translation 0.
P05-3027@@People, places, or companies often sharethe same name, and this can cause a considerableamount of confusion when carrying out Web searchor other information retrieval applications. Each row in this matrix forms a vector thatrepresents a context. The next column indicates thepercentage of the majority class (MAJ.) and count(N) of the total number of contexts for the namesor newsgroups.
P05-3029@@Society needs humor, not just for entertainment. The systemwon a jurys special prize. Among them there are: busi-ness world applications (such as advertisement, e-commerce, etc.
P06-1001@@Statistical machine translation (SMT) is quite ro-bust when it comes to the choice of input represen-tation. Martin, and A. Tikuisis. of NEMLAR Con-ference on Arabic Language Resources and Tools,Cairo, Egypt.E.
P06-1002@@Word alignments are a by-product of statisticalmachine translation (MT) and play a crucial rolein MT performance. A hierarchical phrase-based modelfor statistical machine translation. Manual align-ments are represented by two sets: Probable (P )alignments and Sure (S) alignments, where S P .
P06-1003@@division of a text or dis-course into topically coherent segments  classification of those seg-ments by subject matter  We then show that itssegmentation performance appears relatively ro-bust to speech recognition errors, giving us con-fidence that it can be successfully applied in a realspeech-processing system.The plan of the paper is as follows. In order to investigate the effect of thenumber of underlying topics T , we tested mod-els using 2, 5, 10 and 20 topics. A critique andimprovement of an evaluation metric for text seg-mentation.
P06-1004@@The development of computational models of textstructure is a central concern in natural languageprocessing. A statistical model fordomain-independent text segmentation. N (4)C [i, k] is the normalized cut value of the op-timal segmentation of the first k sentences into isegments.
P06-1005@@Pronoun resolution is a difficult but vital part of theoverall coreference resolution task. A statisti-cal approach to anaphora resolution. The correspond-ing information retrieval performance can now beevaluated along the Top-n / PR configurations.
P06-1006@@Pronoun resolution is the task of finding the cor-rect antecedent for a given pronominal anaphorin a document. Con-ference on Computational Linguistics, pages 869875.A. , the struc-tured feature for the instance i{him,theman} To incorporate such infor-mation, feature Simple-Expansion not onlycontains all the nodes in Min-Expansion, butalso includes the first-level children of thesenodes2.
P06-1007@@The main purpose of the current study is to inves-tigate the extent to which a probabilistic part-of-speech (POS) tagger can correctly model humansentence processing data. Bangalore and A. K. Joshi. This NP-complement bias was overcome bylexical information from high-frequency S-biasedverbs, meaning that if the S-biased verb was a highfrequency word, it was correctly tagged, but if theverb had low frequency, then it was more likely tobe tagged as NP-complement verb.
P06-1010@@As part of a more general project on multilin-gual named entity identification, we are interestedin the problem of name transliteration across lan-guages that use different scripts. A bootstrapping method for extracting bilin-gual text pairs.H.M. In general,we seek to estimate P (e|c), where e is a word inRoman script, and c is a word in Chinese script.Since Chinese transliteration is mostly based onpronunciation, we estimate P (e|c is the pronunciationof c. Again following standard practice, we de-compose the estimate of P (e|c) Here, ei is the ith subsequence ofthe English phone string, and ci is the ith subse-quence of the Chinese phone string.
P06-1013@@Word sense disambiguation (WSD), the task ofidentifying the intended meanings (senses) ofwords in context, holds promise for many NLPapplications requiring broad-coverage languageunderstanding. Determining wordsense dominance using a thesaurus. is the set of interconnec-tions between senses s and s. The contribution of asingle interconnection is given by the reciprocal ofits length, calculated as the number of edges con-necting its ends.
P06-1015@@With seemingly endless amounts of textual data at our disposal, we have a tremendous opportu-nity to automatically grow semantic term banks and ontological resources. where T is the sum of the reliability scores r ,,   An instance i is rejected if S(i) is smaller than some threshold . Exploiting Generic Patterns Generic patterns are high recall / low precision patterns (e.g, the pattern X of Y Using them blindly increases system recall while dramatically reducing precision.
P06-1016@@ With the dramatic increase in the amount of tex-tual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting informa-tion from text. 4 The training example sequence is feed N times for better performance. Roth D. and Yih W.T.
P06-1021@@Speech repairs, as in example (1), are one kindof disfluent element that complicates any sortof syntax-sensitive processing of conversationalspeech. A prosodicallylabelled database of spontaneous speech. In Proceedingsof the IEEE International Conference on Acoustics,Speech and Signal Processing, pages 405408, At-lanta, GA.R.
P06-1023@@Moved constituentsare co-indexed with a trace which is located atthe position where the moved constituent is to beinterpreted. S traces of type *T* occur in sentences with quotedspeech like the sentence Thats true! A verb with an NP and a PP argument, forinstance, is annotated with the feature np.Adjectives, adverbs, and nouns may also get asubcat feature which encodes a single argumentusing a less fine-grained encoding which maps PPto p, NP to n, S to s, and SBAR to b.
P06-1024@@Reinforcement Learning (RL) applied to the prob-lem of dialogue management attempts to find op-timal mappings from dialogue contexts to sys-tem actions. An open question eg How may I help you2. Explicitly confirm any one of slots 1...n. .
P06-1025@@ Designing a spoken dialogue system involves many non-trivial decisions. In this way, instead of at-tempting to acquire the actual student answer by asking to repeat, the system can skip these extra turn(s) and use the current hypothesis. CobotDS: A Spoken Dialogue System for Chat.
P06-1026@@As large amounts of language data have becomeavailable, approaches to sentence-level process-ing tasks such as parsing, language modeling,named-entity detection and machine translationhave become increasingly data-driven and empiri-cal. Bangalore and A. K. Joshi. Technical Report Research Note30, Center for Speech and Language Processing, JohnsHopkins University, Baltimore, MD.S.
P06-1027@@Semi-supervised learning is often touted as oneof the most natural forms of training for languageprocessing tasks, since unlabeled data is so plen-tiful whereas labeled data is usually quite limitedor expensive to obtain. Unfortunately, wewere not able to obtain any improvements over thestandard supervised CRF with self-learning, usingthe sets and O*   P . Learning fromlabeled and unlabeled data on a directed graph.
P06-1028@@CRFs arebasically defined as a discriminative model ofMarkov random fields conditioned on inputs (ob-servations) x. B-X, I-X and O indicate that the wordin question is the beginning of the tag X, insidethe tag X, and outside any target segment, re-spectively. Then, we can define asegment-wise discriminant function w.r.t.
P06-1030@@When giving an essay test, the examiner expects awritten essay to reflect the writing ability of the ex-aminee. of Characters Time (s)A 4 6. Here, a difference appears between e-raterand Jess, which uses the point-deduction systemfor scoring.
P06-1031@@This is perhaps because theJapanese language does not have a mass count dis-tinction system similar to that of English. Each leaf represents a result of the classi-fication. Variance is calculated by@;R_(6)where  denotes the number of samples used forcalculating the ratio.
P06-1032@@ Every day, in schools, universities and busi-nesses around the world, in email and on blogs and websites, people create texts in languages that are not their own, most notably English. Raw data came from a corpus of ~484. Replacement i especially like drinking tea .
P06-1033@@Dependency GraphsThe basic idea in dependency parsing is that thesyntactic analysis consists in establishing typed,binary relations, called dependencies, between thewords of a sentence. ExperimentsAll experiments are based on PDT 1.0, which isdivided into three data sets, a training set (t), adevelopment test set (d), and an evaluation testset (e). , we use t for training, d for tuning, ande for the final evaluation.
P06-1034@@One obstacle to the widespread deployment ofspoken dialogue systems is the cost involvedwith hand-crafting the spoken language generationmodule. I amfrom Spain and I had my 28th birthdaythere and we all had a great time. Extractingparaphrases from a parallel corpus.
P06-1035@@Two questions arise with thisapproach. Lexical MetricBringing this all together, we can define the lexicalmetric.A lexicon L is a mapping from a set of mean-ings M , such as DOG, TO RUN, GREEN,etc., onto a set F of forms such as /pies/, /biec/,/zielony/.The confusion probability P of m1 for m2 inlexical L is the normalised negative exponentialof the scaled edit-distance of the correspondingforms. Journal of Multivariate Analysis, 92(1):97115.Luay Nakleh, Tandy Warnow, Don Ringe, andSteven N. Evans.
P06-1036@@Obviously, readers and writers come to the dictionary with different mindsets, information and expectations concerning input and output. As one can see, associations are a very general and powerful mechanism. ), origin (Greek or Latin), gender (Vigliocco et al,  2 Of course, the input can also be hybrid, that is, it can be composed of a conceptual and a linguistic component.
P06-1037@@Supertagging is based on the combination of twopowerful and influential ideas of natural languageprocessing: On the one hand, parsing is (at leastpartially) reduced to a decision on the optimal se-quence of categories, a problem for which efficientand easily trainable procedures exist. Consider, for instance, thefollowing supertagger output for the previous ex-ample sentence:es: +EXPL/R+ mag: +S/N+AUX,SUBJCsein: PRED+AUX/L+ ...The supertagger correctly predicts that the firstthree labels are EXPL, S, and AUX. Second, the existinggrammar rules deal mainly with structural compat-ibility, while supertagging exploits patterns in thesequence of words in its input, i. e. both modelscontribute complementary information.
P06-1038@@Lexical resources are crucial in most NLP tasksand are extensively used by people. A graph modelfor unsupervised Lexical acquisition. Cate-gory merging is described below.This stage requires an O(1) computation foreach bidirectional arc of each node, so its com-plexity is O(|V |  Enhancing Category Quality: CategoryMerging and Corpus WindowingIn order to cover as many words as possible, weuse the smallest clique, a single symmetric arc.This creates redundant categories.
P06-1040@@can be used to mine large text corpora for word pairs YX :  in which X is a hyponym (type) of Y. WordNet: A lexical database for English. is also of rank r .
P06-1041@@There seems to be an upper limit for the levelof quality that can be achieved by a parser if itis confined to information drawn from a singlesource. The analysis isimperfect because the common noun Mark {e}{ Build initial analysis. This oracleparser achieves a structural and labelled accuracyof 84.
P06-1042@@Natural language parsing is a hard task, partly be-cause of the complexity and the volume of infor-mation that have to be taken into account aboutwords and syntactic constructions. In particular, it contains e-mail and oral transcriptions sub-corpora that in-troduce a lot of noise. Indeed, the confidence we can havein the estimation S(n)f is lower if the number of occurrencesof f is lower.
P06-1043@@Modern statistical parsers require treebanks totrain their parameters, but their performance de-clines when one parses genres more distant fromthe training datas domain. % on WSJ but a lowly 82. A relative weight of n is equivalent to using n copies ofthe corpus, ie an event that occurred x times in the corpuswould occur xn times in the weighted corpus.
P06-1044@@Such classes are useful for their ability tocapture generalizations about a range of linguis-tic properties. I(Clusters; SCFs) ,where  is a parameter that balances the con-straints. In Conference on Em-pirical Methods in Natural Language Processing,Philadelphia, USA.E.
P06-1045@@Lexical knowledge is one of the most important re-sources in natural language applications, making italmost indispensable for higher levels of syntacti-cal and semantic processing. 6DR is then given by:12( naNa +nbNb), (6)where Na and Nb are the numbers of pairs inhighly related and unrelated test sets, respectively.Since DR changes depending on threshold t, max-imum value is adopted by varying t.We used the reference similarity to create thesetwo test sets. 9mod, ie, the modifying word is a determiner, (2)ncmod-n, when the GR label is ncmod and themodifying word is a noun, (3) ncmod-j, when theGR label is ncmod and the modifying word is anadjective or number, (4) ncmod-p, when the GRlabel is ncmod Al-though some individual modification categoriessuch as detmod and ncmod-j outperform other cat-egories in some cases, the overall observation isthat all the modification categories contribute tosynonym acquisition to some extent, and the ef-fect of individual categories are accumulative.
P06-1046@@It is a general property of Machine Learning thatincreasing the volume of training data increasesthe accuracy of results. Each term corresponds to a unique node. We nowfind the closest two children of C and D. E is notchecked as it is only a child of B.
P06-1047@@ Extractive summarization selects sentences which contain the most salient concepts in documents. Event-based summarization which has e-merged recently attempts to select and organize sentences in a summary with respect to events or sub-events that the sentences describe. It is actually a kind of measure of lexical similarity.
P06-1048@@Automatic sentence compression has recently at-tracted much attention, in part because of its affin-ity with summarisation. It is a measure of thenumber of insertion (I), deletion (D) and substi-tution (S) errors between two strings. Then the compression was revealed bypressing a button.
P06-1049@@Among a numberof sub-tasks involved in MDS, eg, sentence ex-traction, topic detection, sentence ordering, infor-mation extraction, sentence generation, etc., mostMDS systems have been based on an extractionmethod, which identifies important textual seg-ments (eg, sentences or paragraphs) in source doc-uments. We define the as-sociation strength of arranging segments B after Ameasured by a chronology criterion fchro(A  B)in the following formula,fchro(A  (8)Here, am represents the last sentence in segmentA; b1 represents the first sentence in segment B;T (s) is the publication date of the sentence s;D(s) is the unique identifier of the document towhich sentence s belongs: and N(s) denotes theline number of sentence s in the original docu-ment. Rhetorical structuretheory: Toward a functional theory of text organiza-tion.
P06-1050@@ Consider the sentence from a news article: George W. Bush met with Vladimir Putin in Moscow. A simple rule-based part of speech tagger. The best n determined via cross validation turned out to be 0, ie, the event itself with no local context.
P06-1051@@Recently, textual entailment recognition has beenreceiving a lot of attention. These methods generallycombine a similarity measure with a set of possi-ble transformations T applied over syntactic andsemantic interpretations. The latter combines our newkernel with traditional approaches (i  , the 6th row refers to only the idf similaritymetric whereas the following two rows report thecross-pair similarity carried out with Eq.
P06-1052@@The basic idea be-hind it is to not enumerate all possible semanticrepresentations for each syntactic analysis, but toderive a single compact underspecified represen-tation (USR). We call thetrees in (V,E) the fragments of the graph. Theoverall runtime for the algorithm is O(n2S), whereS is the number of splits in Ch and n is the num-ber of nodes in the graph.
P06-1053@@They provide evidence from a series of cor-pus studies which show that parallelism is not lim-ited to co-ordination, but occurs in a wide rangeof syntactic structures, both within and betweensentences, as predicted if a general priming mech-anism is assumed. Note we assume there is only oneparameter pident applicable everywhere (ie, it hasthe same value for all rules).How is this used in a PCFG parser Let t1 and t2represent, respectively, the first and second coor-dinate sisters and let PPCFG(t) be the PCFG prob-ability of an arbitrary subtree t.Because of the independence assumptions ofthe PCFG, we know that pident  pident) PPCFG(t2) when structures do not match. Cognitive architectures in a ratio-nal analysis.
P06-1054@@ and BackgroundSyntactic parsing is one of the most fundamentaltasks in Natural Language Processing (NLP). % on a different test/developmentsplit. The new parse state is:(S): NP (NRY)NRY The next action is shift.
P06-1057@@In many language processing settings it is neededto recognize that a given word or term may be sub-stituted by a synonymous one. Estimating the supportof a high-dimensional distribution. In Proceedings of the 4thAnnual Symposium on Document Analysis and In-formation Retrieval, Las Vegas.E.
P06-1059@@The rapid increase of information in the biomedi-cal domain has emphasized the need for automatedinformation extraction techniques. High-recall protein entity recognitionusing a dictionary. The com-putational cost for semi-CRFs is O(KLN) whereL is the upper bound length of entities, N is thelength of sentence and K is the number of labelset.
P06-1062@@However, large scale parallel corpora are not readily available for most language pairs. But two major differences exist between STSG and our DOM tree alignment model: (i) Our DOM tree alignment model requires the alignment a sequential order invariant process, ie if node A is aligned with node B, then the sibling nodes following A have to be either de-leted or aligned with the sibling nodes following B. t.N Di refers to the text of nodeDiN ; l.NDi refers to the HTML tag of the node DiN ; jC.NDi  refers to the jth child of the node DiN ; [ ]nmC ,Di .N refers to the consecutive sequence of DiN s children nodes from mC.N Di to nC.NDi ; the sub-tree rooted at jC.NDi is represented as jTC.NDi  and the forest rooted at [ ]nmC ,Di .N  is represented as [ ]nmTC ,Di .N . To complete the alignment model, [ ]( )ATTr E jiF nm ,P ,],[  is to be estimated.
P06-1063@@This canlimit the applicability of grammatical resources in-duced from treebanks in that such resources un-derperform when used on a different type of textor for a specific task.In this paper we present work on creating Ques-tionBank, a treebank of parse-annotated questions,which can be used as a supplementary training re-source to allow parsers to accurately parse ques-tions (as well as other text). Design of a multi-lingual, parallel-processing statistical parsing engine. In all of the runs the coverage for both testsets is 100%, f-scores for the question test set de-crease as the amount of question data in the train-ing set is reduced (though they are still quite high.
P06-1064@@However, many corpora (Bohomva grammars has largely been confined to En-glish. (2) a. Gibt Peter Maria das Buchb. Natural Language Engineering.Stephen Clark and James R. Curran.
P06-1066@@Phrase reordering is of great importance forphrase-based SMT systems and becoming an ac-tive area of research recently. A block orientation model forstatistical machine translation. In International Work-shop on Spoken Language Translation.R.
P06-1067@@A language model is a statistical model that givesa probability distribution over possible sequences ofwords. The search is done in n timesteps. A Decoder forSyntax-based Statistical MT.
P06-1068@@Automatic text categorization is the task of assign-ing any of a set of predefined categories to a doc-ument. AT&T Labs Re-search.Cong Li, Ji-Rong Wen, and Hang Li. A re-examinationof text categorization methods.
P06-1069@@It is empiri-cally known that the indexing scheme is a non-trivial complication to system performance, es-pecially for some Asian languages in which there are no explicit word margins and even no natural semantic unit. This data collection has a moderate sparseness problem. Chinese Information Retrieval: Using Characters or Words On the Use of Words and N-grams for Chi-nese Information Retrieval, Proceedings of 5th In-ternational Workshop on Information Retrieval with Asian Languages Monica Rogati, Yiming Yang.
P06-1070@@In the worldwide scenario of the Web age, mul-tilinguality is a crucial issue to deal with andto investigate, leading us to reformulate most ofthe classical Natural Language Processing (NLP)problems into a multilingual setting. Learning a translationlexicon from monolingual corpora. k eigenval-ues of T, and all the remaining elements are setto 0.
P06-1071@@One of the main advantages of CME modeling is the ability to incorporate a variety of features in a uniform framework with a sound mathematical foundation. Preliminaries to a Theory of Speech Disfluencies. Ph.D. Thesis, University of California, Berkeley.
P06-1072@@Inducing a weighted context-free grammar fromflat text is a hard problem. Eisner and N. A. Smith. of Workshop onTreebanks and Linguistic Theories.E.
P06-1073@@This often leads to considerable ambigu-ity since several words that have different diacriticpatterns may appear identical in a diacritic-lesssetting. In COLING Workshop on Arabic-scriptBased Languages, Geneva, Switzerland.Tong Zhang, Fred Damerau, and David E. Johnson. A surveyof smoothing techniques for me models.
P06-1077@@, eI .Here, I is the length of the target string, and J is the lengthof the source string.substrings that are common enough to be observedon training data. Given a parse tree T , we find allusable TATs. Notethat Lynx made use of only 88, 066 TATs on testcorpus while 221, 453 bilingual phrases were usedfor Pharaoh.The feature weights obtained by minimum er-614FeaturesSystem d lm  (f |e) (ie h2) isnot a helpful feature for Lynx.
P06-1079@@Zero-anaphora is a gap in a sentence that has ananaphoric function similar to a pro-form (eg pro-noun) and is often described as referring backto an expression that supplies the information nec-essary for interpreting the sentence. ; otherwise 0.PRED AUX 1 if PRED contains auxiliaries such as  ; otherwise 0.DEMONSTRATIVE 1 if NP contains the article corresponding to DEMONSTRATIVE that ; otherwise 0.PARTICLE Particle followed by NP, such as wa (topic), ga (subject), o (object).Semantic NE Named entity of NP: PERSON, ORGANIZATION, LOCATION, ARTIFACT,DATE, TIME, MONEY, PERCENT or N/A.EDR HUMAN 1 if NP is included among the concept a human being PERSON, koko (here) LOCATION,sore (this) This result sup-ports that the former has an advantage of learn-ing non-anaphoric zero-pronouns (181 instances)as negative training instances in intra-sententialanaphoricity determination, which enables it to re-ject non-anaphoric zero-pronouns more accuratelythan the others. The Boosting-based algorithm pro-5To indicate which node belongs to which subtree, the la-bel of each node is prefixed either with L, R or I.
P06-1083@@  Reflecting the rapid growth in science and technology, new words and technical terms are being progressively created, and these words and terms are often transliterated when imported as loanwords in another language. The similarity ranges from 0 to 1. (e) A word beginning with the consonant .
P06-1086@@In this paper we present MAGEAD, a morphologi-cal analyzer and generator for the Arabic languagefamily, by which we mean both Modern StandardArabic (MSA) and the spoken dialects. The process of combiningmorphemes involves a number of phonological,morphemic and orthographic rules that modify theform of the created word so it is not a simple inter-leaving or concatenation of its morphemic compo-nents.An example of a phonological rewrite rule is thevoicing of the /t/ of the verbal pattern V1tV2V3(Form VIII) when the first root radical is /z/, /d/, or/*/ ( , ! The vocalismmorpheme specifies which short vowels to usewith a pattern.
P06-1088@@Here we give two reasons. A Gaussian priorfor smoothing maximum entropy models. Technical re-port, Carnegie Mellon University, Pittsburgh, PA.Stephen Clark and James R. Curran.
P06-1089@@Part-of-speech (POS) tagging is a fundamentallanguage analysis task. In the training phase,we need to estimate two types of parameters; localmodel (parameters), which is necessary to calcu-late p0(t|w), and global model (parameters), ie,i,j . Allthe corpora are POS tagged corpora in Chinese(C),English(E) or Japanese(J), and they are split intothree portions; training data, test data and unla-beled data.
P06-1091@@This paper presents a view of phrase-based SMTas a sequential process that generates block ori-entation sequences. QkqCNSscheme as a black box. The dimensionality of the fea-ture vector a CCL CEM depends on the numberof binary features.
P06-1093@@Call center is a general term for help desks, infor-mation lines and customer service centers. Theresultant transcriptions have a word error rate ofabout 40%. Tiling constructs longer n-grams fromsequences of overlapping shorter n-grams.
P06-1096@@At the same time, discriminative meth-ods have provided substantial improvements overgenerative models on a wide range of NLP tasks.They allow one to easily encode domain knowl-edge in the form of features. For that to happen, j  ai needs tobe aligned with I. A LocalizedPrediction Model for Statistical Machine Translation.
P06-1097@@The most widely applied training procedure forstatistical machine translation  We extend the IBM models with new(sub)models, which leads to additional in-creases in word-alignment accuracy. In M. I. Jordan, editor,Learning in Graphical Models. Allof these also used knowledge from one of theIBM Models in order to obtain competitive results774SYSTEM BLEU F-MEASUREA/E UNSUP.
P06-1098@@I1 , f J1 )) (3)where hm(eI1, f J1 ) is a feature function, such asa ngram language model or a translation model.When decoding, the denominator is dropped sinceit depends only on f J1 . A hierarchical phrase-basedmodel for statistical machine translation. The costof decoding with the proposed model is cubic toforeign language sentence length.Rules Stack[1, 11]X : [1, 11] (eI1| f J1 ,D) estimate the likelihood of twosentences f J1 and eI1 over a derivation tree D.We assume that the production rules in D areindependent of each other:h is estimated through the relative frequencyon a given bilingual corpus.
P06-1099@@Research on domain-specific automatic termrecognition (ATR) and on general-language collo-cation extraction (CE) has gone mostly separateways in the last decade although their underlyingprocedures and goals turn out to be rather simi-lar. Applied Statistics: A Handbookof Techniques. Taking n-gram term candidates, itdetermines the likelihood of precluding the ap-pearance of alternative tokens in various token slotcombinations, which results in higher scores formore constrained candidates.
P06-1100@@The output of most of these systems is flat lists of lexical semantic knowledge such as Italy is-a country For example, orange similar-to blue ontologizes in WordNet to orange#2 similar-to blue#1 In his framework, Pantel proposed a method of inducing ontologi-cal co-occurrence vectors 1  which are subse-quently used to ontologize unknown terms into WordNet with 74% accuracy. A taxonomy of part-whole relations. All possible permutations of senses are computed and scored by averaging r(sx) and r(sy).
P06-1101@@The goal of capturing structured relational knowl-edge about lexical terms has been the motivatingforce underlying many projects in lexical acquisi-tion, information extraction, and the constructionof semantic taxonomies. Taxonomies, Relations, and TaxonomicConstraintsWe define a taxonomy T as a set of pairwise re-lations R over some domain of objects DT. Given a set of fea-tures ERij  E, we assume we have some modelfor inferring P (Rij  T|ERij), ie, the posteriorprobability of the event Rij  T given the corre-sponding evidence ERij for that relation.
P06-1103@@Named Entity recognition has been getting muchattention in NLP research in recent years, since itis seen as significant component of higher levelNLP tasks such as information distillation andquestion answering. A discriminative frameworkfor bilingual word alignment. We build a feature vector fromthis example in the following manner: First, we split both words into all possiblesubstrings of up to size two:fiff Rffifl!
P06-1104@@ The goal of relation extraction is to find various predefined semantic relations between pairs of entities in text. The time complexity for computing this kernel is 1 2(| | | |)O N N In this paper, two composite kernels are de-fined by combing the above two individual ker-nels in the following ways:  1) Linear combination:  1 1 2 1 2 1 2 Evaluation on the development set shows that this composite kernel yields the best performance when  is the normalized ( , )K  is the polynomial expansion of ( , )K  Evaluation on the development set shows that this composite kernel yields the best performance when  The polynomial expansion aims to explore the entity bi-gram features, esp. Generally, we can represent a parse tree T  by a vector of integer counts of each sub-tree type (regardless of its ancestors):   ( )T (3)  where N1 and N2 are the sets of nodes in trees T1 and T2, respectively, and ( )isubtreeI n  is a function that is 1 iff the subtreei occurs with root at node n and zero otherwise, and 1 2( , )n n is the number of the common subtrees rooted at n1 and n2, ie  1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n can be computed by the following recur-sive rules:  (1) if the productions (CFP rules) at 1n  and 2n  are different, 1 2( , ) 0n n ,  where 1( )nc n is the child number of 1n , ch(n,j) is the jth child of node n  and In ad-dition, the recursive rule (3) holds because given two nodes with the same children, one can con-struct common sub-trees using these children and common sub-trees of further offspring.
P06-1105@@Dependency parsing is a basic technology for pro-cessing Japanese and has been the subject of muchresearch. a model in whichthe co-occurrence probability is defined asP (r|v), without the syntactic property syn)(e) one in which the co-occurrence probability ofthe case element, P (n|r, v), is simply added838Bunsetsu Sentenceaccuracy accuracyContext model 90. Fastexact inference with a factored model for naturallanguage parsing.
P06-1106@@Complex information needs can rarely be ad-dressed by single documents, but rather require theintegration of knowledge from multiple sources.This suggests that modern information retrievalsystems, which excel at producing ranked lists ofdocuments sorted by relevance, may not be suffi-cient to provide users with a good overview of theinformation landscape.Current question answering systems aspire toaddress this shortcoming by gathering relevantfacts from multiple documents in response toinformation needs. 0, N denotes sig. AcknowledgmentsThis work was supported in part by the U.S. Na-tional Library of Medicine.
P06-1107@@Natural Language Processing applications oftenneed to rely on large amount of lexical semanticknowledge to achieve good performances. The two methods are then fairly in line.The other approach we experiment is thequasi-pattern The idea isthat the implication T  The previ-ous relation between H and T probabilities thenholds also for word pairs. WordNet: A lexicaldatabase for English.
P06-1109@@While supervised parsers suffer fromshortage of hand-annotated data, unsupervisedparsers operate with unlabeled raw data of whichunlimited quantities are available. An efficient implementation of a newDOP model. Nevertheless we will see that our reestimation869procedure leads to significantly better accuracycompared to U-DOP (the latter would be equal toUML-DOP under 0 iterations).
P06-1110@@Discriminative machine learning methods haveimproved accuracy on many NLP tasks, includingPOS-tagging, shallow parsing, relation extraction,and machine translation. X(i) is a feature vector describingi, with each element in {0, 1}. 4: while dev set accuracy is increasing do5: t  tree with one (root) node6: while the root node cannot be split do7: decay `1 parameter 8: while some leaf in t can be split do9: split the leaf to maximize gain10: percolate every i  (n) to minimize R13: append t to ensembleListing 1 presents our training algorithm.
P06-1111@@We in-vestigate a prototype-driven approach to grammarinduction, in which one supplies canonical ex-amples of each target concept. However this exper-iment, labeled PROTO  (i,j)P (yij |Bij)P (cij |Bij)The distribution P (B) only places mass on brack-etings which correspond to binary trees. )where B(T ) corresponds to the bracketing ma-trix determined by T .
P06-1112@@Answer Extraction is one of basic modules in opendomain Question Answering (QA). R1 and R2 consistof N and M relations respectively. A maximum entropyapproach to natural language processing.
P06-1113@@In Question Answering the correct answer can beformulated with different but related words thanthe question. WordNet: a lexical database. Thesimilarity score is computed in the following way:if 2 represents the number of arguments in a pat-tern, each argument matched is defined to have acontribution of   2  , except for the subjectthat has a contribution if matched of 2/(N+1).
P06-1114@@Open-Domain Question Answering (Q/A) sys-tems return a textual expression, identified froma vast document collection, as a response to aquestion asked in natural language. corresponding entities, predicates, orphrases found in a pair of texts. In Proceed-ings of the 39th Meeting of the Association for Com-putational Linguistics.S.
P06-1115@@Computational systems that learn to transform nat-ural language sentences into formal meaning rep-resentations have important practical applicationsin enabling user-friendly natural language com-munication with computers. partition(s[i..j], t)will be selected in constructing the most probablepartial derivation. The procedure getMRreturns the MR for a semantic derivation.
P06-1116@@Cue phrases such as This paper proposes a novelapproach to. .e) We wanted to introduce a method. 8 and a ceiling of3.
P06-1120@@also called multi-word expressions, or id-iosyncratic interpretations that cross word bound-aries Hybrid Collocation ExtractionWe consider that syntactic analysis of source cor-pora is an inescapable precondition for colloca-tion extraction, and that the syntactic structure ofsource text has to be taken into account in order toensure the quality and interpretability of results. Extraction of V-N-collocationsfrom text corpora: A feasibility study for Ger-man. In Proceedings of the Workshop on VeryLarge Corpora: Academic and Industrial Perspec-tives, Columbus, U.S.A.Yaacov Choueka.
P06-1121@@We also usereal examples to show that our probability mod-els estimated from a large number of derivationsfavor phrasal re-orderings that are linguisticallywell motivated. A rule like (e) is particu-larly unfortunate, since it allows the word were tobe added without any other evidence that the VPshould be in passive voice. With a difference of 6.
P06-1122@@Data-driven machine translation (MT) relies onmodels that can be efficiently estimated from par-allel text. A greedy E-step iteratively re-assigns each source type to the cluster that max-imises Eq. A probabilistic framework for semi-supervisedclustering.
P06-1123@@Translational equivalence is a mathematical rela-tion that holds between linguistic expressions withthe same meaning. Automatic evaluation and uni-form filter cascades for inducing N -best translationlexicons. A syntax-based statistical translation model.
P06-1124@@Probabilistic language models are used exten-sively in a variety of linguistic applications, in-cluding speech recognition, handwriting recogni-tion, optical character recognition, and machinetranslation. to a separate se-quence of i.i.d. A neural probabilistic language model.
P06-1128@@Rapid expansion of text information has motivatedthe development of efficient methods of access-ing information in huge texts. We then provide a search engine for theseannotated sentences. Available at http://www.nlm.nih.gov/pubs/factsheets/medline.html.T.
P06-1129@@Such statistics indicate that a good query speller is crucial to search engine in improving web search relevance, because there is little op-portunity that a search engine can retrieve many relevant contents with misspelled terms. Instead, we use a method to approximate the sum over the n-best list (a list of most prob-able correction candidates). SC-SimCM achieves 26.% reduction in error rate over SC-EdCM, which is significant to the 0.001 level (paired t-test).
P06-1130@@Theyachieve robustness and coverage at a fraction ofthe development cost of hand-crafted grammars. % coverage, a BLEU score of 0. BLEU is the weighted averageof n-gram precision against the gold standard sen-tences.
P06-1131@@Our long-term goal is to develop conversationalrobots with whom we can interact through natural,fluent, visually situated dialog. However, Dale and Reiter do not de-fine how this set should be constructed, they onlywrite:  [w]e define the context set to be the set ofentities that the hearer is currently assumed to beattending to 236).Before applying the incremental algorithm wemust construct a context model in which we cancheck whether or not the description generateddistinguishes the target object. In H.L.Pick, editor, Spatial orientation.
P06-1132@@Generation of grammatical elements such as inflec-tional endings and case markers has become an impor-tant component technology, particularly in the context of machine translation (MT). All models are trained using a Gaussian prior. Each bunsetsu (or simply phrase in this paper) is defined as consisting of one content word (or n-content words in the case of compounds with n-components) plus any number of function words (including particles, auxiliaries and affixes).
P06-1133@@Conflicts arise when two groups of people takevery different perspectives on political, socio-economical, or cultural issues. Andthats why I support that. Ibelieve that choice is a womans choice.
P06-1134@@There is growing interest in the automatic extrac-tion of opinions, emotions, and sentiments in text(subjectivity), to provide tools and support for var-ious NLP applications. Senses are classified as S(ubjective),O(bjective), or B(oth). For verb assault there isa subjective sense:attack, round, assail, lash out, snipe, assault (attack in speechor writing) The editors of the left-leaning paper attacked thenew House SpeakerHowever, there is no corresponding sense for4I.e., the first three were labeled O by both annotators.
P06-1136@@Questions like Who is Colin Powell Statistics from 2,516 Fre-quently Asked Questions (FAQ) extracted from Internet FAQ Archives1 show that around 23. 1) Sparseness of search results derogated the learning of the ordered centroid: E.g. 8 T-Test has been performed.
P06-1139@@Machine Translation, Summa-rization, Question Answering  Webelieve two reasons explain this state of affairs.First, these generic NLG systems use input rep-resentation languages with complex syntax and se-mantics. In the next iteration, XZY5[^]o_la is a sin-gleton set containing the state POPed out from thetop of m . For a WIDL-expression  , we define a mapping, calledUNFOLD, between the WIDL-graph *  and apFSA 9.
P06-1140@@The idea isto take advantage of the generators periphrasticability:2 given a set of generated paraphrases thatsuitably express the desired content in the dialoguecontext, the system can select the specific para-phrase to use as its response according to the pre-dicted quality of the speech synthesized for thatparaphrase. We also performed seven post-hoc comparisons using two-tailed t-tests, as wedid not have an a priori expectation as to whichfeature set would work better. Ex-tracting paraphrases from a parallel corpus.
P06-1141@@Named entity recognition (NER) seeks to lo-cate and classify atomic elements in unstructuredtext into predefined entities such as the namesof persons, organizations, locations, expressionsof times, quantities, monetary values, percent-ages, etc. R. Curran and S. Clark. A Maximum Entropy Approach toNamed Entity Recognition.
P06-1142@@ In applications such as cross-lingual information retrieval (CLIR) and machine translation (MT), there is an increasing need to translate out-of-vocabulary (OOV) words, for example from an alphabetical language to Chinese. Using the PSM, we extract 137,711 distinct E-C pairs. S. Brin and L. Page.
P06-1143@@ Punjabi is the mother tongue of more than 110 million people of Pakistan (66 million), India (44 million) and many millions in America, Canada and Europe. For example, E} FZ ~  ~ha }Z wiX In the first sentence, the word ~ha is pronounced as [ i] and it conveys the meaning of wide. a process referred to as Transliteration.
P06-2002@@Wefirst introduce the general structure of a rote ex-tractor and its limitations. A typical entitykind for an author is person. E means that the corresponding tokens areequal, so no edition is required.
P06-2005@@SMS translation is a mobile Machine Translation (MT) application that translates a message from one language to another. Then we further decompose the probability 1 1( | )K KP s e  using a phrase alignment  as done in the previous word-based model. Phrase-based Model Given an English sentence e  and SMS sentence s , if we assume that e  can be decomposed into  phrases with a segmentation T , such that each phrase e  in  can be corresponded with one phrase s  in Kkkes , we have e e  and 1 1Nk Ke e   Assum-ing that an English sentence e, of length N is corrupted i i i    (3)  This is the basic function of the channel model for the phrase-based SMS normalization model, where we used the maximum approximation for the sum over all segmentations.
P06-2006@@Considerable progress has been made in accu-rate statistical parsing of realistic texts, yield-ing rooted, hierarchical and/or relational repre-sentations of full sentences. An introduction to tag sequence gram-mars and the RASP system parser, University of Cam-bridge, Computer Laboratory Technical Report 662.Briscoe, E.J. 5Relation Precision Recall F1 P R F1 Relationmod 75.
P06-2010@@In the last few years there has been increasing in-terest in Semantic Role Labeling (SRL). Recall (r) is the proportion of correct ar-guments which are predicted by a system. A study on convolutionkernels for shallow statistic parsing.
P06-2011@@ Translation of named entities (NE) attracts much attention due to its practical applications in World Wide Web. It results the C-E bilingual dictionary cannot be used in recognizing word sense similarity. This would result a problem.
P06-2012@@In this paper, we address the task of relation extrac-tion, which is to find relationships between name en-tities in a given context. As arelaxation of such NP-hard discrete graph partition-ing problem, spectral clustering technique computeseigenvalues and eigenvectors of a Laplacian matrixrelated to the given graph, and construct data clus-ters based on such spectral information.Thus the starting point of context clustering is toconstruct an affinity matrix A from the data, whichis an n  n matrix encoding the distances betweenthe various points. means a NP chunk asthe subject of the sentence.
P06-2013@@Chunking identifies the non-recursive cores ofvarious types of phrases in text, possibly as aprecursor to full parsing or information extrac-tion. The features are listed as follows: WORD: uni-gram and bi-grams of words inan n window. We extendthe tags with COO for Coordination: B-NP-COOand I-NP-COO.)
P06-2014@@Given a parallel sentence pair, or bitext, bilin-gual word alignment finds word-to-word connec-tions across languages. 105abs(j|E|k|F |)All three aligners link based on 2 correlationscores, breaking ties in favor of closer pairs. [AA] | AA.One feature indicates an inverted production A corresponding to unaligned tokens are givenblank feature vectors: T (r) vectors for thecorrect training structures.
P06-2017@@Sound systems of the worlds languages show re-markable regularities. The number of elements(edges) in the set E as computed from PlaNet is7022. Santa Fe work-ing paper 01-03-016.N.
P06-2019@@A mechanism for automatically compressing sen-tences while preserving their grammaticality andmost important information would greatly bene-fit a wide range of applications. l is the num-ber of clause constituents above wi, and N is thedeepest level of embedding. Sentence compres-sion for automated subtitling: A hybrid approach.
P06-2020@@We consider the problem of producing a multi-document summary given a collection of doc-uments. 5and a Pearson correlation of 0. Our goal is to approximate P (t| ), theprobability that a term will be used in a humanabstract.
P06-2025@@ In Natural Language Processing (NLP) application areas such as information retrieval, question answering systems and machine translation, there is an increasing need to translate OOV words from one language to another. The joint source-channel model with linguistic knowledge (Model D) has not performed well in the Bengali-English machine transliteration whereas the trigram model (Model E) needs further attention as its result are comparable to the modified joint source-channel model (Model F). Named entities hold a very important place in NLP applications.
P06-2027@@Open-ended question-answering (QA) systemstypically produce a response containing a vari-ety of specific facts proscribed by the questiontype. Answering DefinitionalQuestions: A Hybrid Approach. We4In our experiments we analyze TDT topics used inTDT-2 and TDT-4 evaluations.
P06-2029@@Most NLP applications are either data-driven(classification tasks are solved by comparing pos-sible solutions to previous problems and their so-lutions) or rule-based (general rules are formu-lated which must be applicable to all cases thatmight be encountered). occurs in one out of 2,000 sentences ofGerman and the word gegen If the LA scoreis higher than 1, i. e. we observe a much higherfrequency of co-occurrences in a large corpus, wecan assume that the two events are not statisti-cally independent  in other words, that there is apositive correlation between the two words. We therefore adopted his approach andartificially inflated all noun+preposition counts bya constant factor i.
P06-2033@@Algorithms for the Generation of Referring Ex-pressions (GRE) seek a set of properties that dis-tinguish an intended referent from its distractorsin a knowledge base. (A,B) between two clusters A andB is defined straightforwardly in terms of the dis-tance between their perspectives PA and PB: is created, where V is the set of clus-ters, and E is the set of edges with edge weightsdefined as the semantic distance between perspec-tives. Under the new definition, if D isthe only description for R, then it trivially satis-fies maximal coherence.
P06-2034@@A long-standing challenge within natural languageprocessing has been to understand the meaning ofnatural language sentences. Theaverage length of a natural language sentence inthis corpus is 6. Rerankingmodels trained in this way ensure that the n-bestSAPTs for each training example are not gener-ated by a baseline model that has already seen thatexample.
P06-2035@@Automatic processing of bilingual and multilingual corpora Processing bilingual and multilingual corpora constitutes a major area of investigation in natu-ral language processing. In Bouillon, P. & Clas A. : list of segments where the sequence appears 45, 46, 46, 48, 51, 51  Then we convert this list in a nL-dimension vec-tor vL, where nL is the number of textual seg-ments of the document of language L. Each di-mension contains the number of occurrences pre-sent in the segment.
P06-2036@@Synchronous Context-Free Grammars (SCFGs)are a generalization of the Context-Free Gram-mar (CFG) formalism to simultaneously producestrings in two languages. A hierarchical phrase-basedmodel for statistical machine translation. Forexample, the input rule:[ X  In-troducing a new grammar nonterminal Xi for eachinternal node of the tree yields an equivalent set ofsmaller rules:[ X  G(3)E(1)H(4)F (2) ]In the case of stochastic grammars, the rule cor-responding to the root of the permutation tree isassigned the original rules probability, while allother rules, associated with new grammar nonter-minals, are assigned probability 1.
P06-2038@@The cost of parsing quickly be-comes prohibitively expensive as the amount oftext to parse grows. A new statistical parserbased on bigram lexical dependencies. Wethen define a set S, where S is the set of spans296c1, .
P06-2039@@Example-based approaches for developing parsershave already been proposed in literature. ParsingEnglish with a link grammar. It projects thecorresponding relations in the target language sen-tence T .
P06-2041@@Thismethodology has emerged as an alternative tomore complex models, especially in dependency-based parsing. The node 0 is a root. {0}), and we use theterm token node for members of V +.The set E of arcs (or edges) is a set of orderedpairs (i, j), where i and j are nodes.
P06-2046@@Recognizing idioms in a sentence is important tosentence understanding. 0 Also, 167,268out of 220,684 idiom tokens in Mainichi newspa-per of 10 years (91 1Then we discuss what can be used to disam-biguate the verbal (N/P V) type. indicatesthose including a literal-usage idiom.
P06-2047@@The dependency graph (DG) is a packed shareddata structure which consists of the nodes corre-sponding to the words in a sentence and the arcsshowing dependency relations between the nodes.The scored DG has preference scores attached tothe arcs and is widely used as a basis of the opti-mum tree search method. (a) Dependency graph DGi(b) Constraint matrix CMi(c) Feasible solution value LBi(d) Upper bound value UBi(e) Inconsistent arc pair list IAPLiThe constraint matrix is common to all partial-problems, so one CM is shared by all partial-problems. Optimum Tree Search in a Scored DG2.
P06-2053@@ Almost since it became a global phenomenon, com-puters have been examining and reasoning about our email. For example, the NSA has had mas-sive parallel machines filtering e-mail traffic for at least ten years. A resource has been created to develop a system able to make these distinctions automatically.
P06-2054@@For most sequential labeling problems in naturallanguage processing (NLP), a decision is madebased on local information. Applying and extend-ing our approach to other natural language tasks(which are difficult to apply a parser to) such as in-formation extraction from e-mail data or biomed-ical named entity recognition is a topic of futurework.AcknowledgementsWe thank three anonymous reviewers for helpfulcomments. )Here, we introduce a second approximation.
P06-2055@@ High-performance named entity (NE) tagging is crucial in many natural language processing tasks, such as information extraction and machine translation. Using N-best Lists for Named Entity Recognition from Chinese Speech. Any opinions, findings and conclu-sions expressed in this material are those of the authors and do not necessarily reflect the views of the U. S. Government.
P06-2056@@The theme of this paper is the following as-sumption:The uncertainty of tokens comingafter a sequence helps determinewhether a given position is at aboundary. Then, fromall fragments, n-grams of less than 6 charac-ters were obtained. This can be intuitively under-stood: it is easy to guess that \e" will followafter \Hello!
P06-2059@@Recently, there has been an increasing interest insuch applications that deal with opinions (a.k.a.sentiment, reputation etc.). This is a task of classifying sen-tences into positive and negative. is positive and the other(I was not satisfied...) is negative.To get around this problem, we did not use suchitems.
P06-2060@@Anumber of these applications are characterized by adominance of a NONE class in the training exam-ples. As expected, the recall rises fastestfor At-Least-N when N is small, i.e when small mi-nority opinion or even a single dissenting opinion isbeing trusted. mem-bership in a hypothesized parse.
P06-2062@@Such descriptions are calledapplication grammars.A resource grammar (Ranta, to appear) is ageneral-purpose grammar that forms a basis forapplication grammars. (I need help)and e interesno iskusstvo (she is interestedin art). This is a simple case, where Rus-sian easily fits into the common API, although acorresponding phenomenon does not really exist.Sometimes, a problem does not arise until thejoining point, where agreement has to be made.For instance, in Russian, numeral modificationuses different cases to form a noun phrase in nom-inative case: tri tovariwa (three comrades),where the noun is in nominative, but pt to-variwe (five comrades), where the noun is ingenitive!
P06-2063@@  Many opinions are being expressed on the Web in such settings as product reviews, personal blogs, and news group message boards. Labeling each sentence is a time-consuming and costly task. (4) The only complaint that I have is that the French fries are usually cold.
P06-2064@@This makesit difficult to gauge the general-purpose utility ofthe different methods. Extended glossoverlaps as a measure of semantic relatedness. To map the verbs onto seedverbs, and hence estimate which semantic rela-tion(s) each is a predictor of, we experimentedwith two different methods.
P06-2065@@Unsupervised learning holds great promise for break-throughs in natural language processing. Journal of the Royal Statistical Society, 39(B).Finch, S. and N. Chater. Sonorous consonants are taken to be per-ceptually louder, and include n, m, l, and r. Addition-ally, vowels are more sonorous than consonants.
P06-2066@@Many practical implementa-tions of dependency parsing are restricted to pro-jective structures, where the projection of a headword has to form a continuous substring of thesentence. and E  V  V .Throughout this paper, we use standard terminol-ogy and notation from graph theory to talk aboutdependency graphs. In Eleventh Conference of theEuropean Chapter of the Association for Computa-tional Linguistics (EACL).T.
P06-2071@@ and problem clarificationSemantics extends beyond words. When using imagefeatures, grayscale images (no color histograms)and images without salient regions (no keypointsdetected) were also removed.Text features We used the following BOWs:(a) tokens in the page body; (b) tokens in a 10window around the target image (if multiple, thefirst was considered); (c) tokens in a 10 windowaround any instances of the query keyword (egsquash); (d) tokens of the target images alt at-tribute; (e) tokens of the title tag; (f) some metatokens. Note that thisis a nonlinear mapping of the original space.
P06-2072@@ As well known, the experiment conducted by George Miller on the mental lexicon properties in the early 80s pointed out that lexical meaning is derived from a set of lexical and conceptual relations among concepts. We previously mentioned that adjectives show a very particular semantic organization. (5) o prdio alto      the high building 2 Predicative contexts with relational adjectives are gener-ally ruled out in Portuguese.
P06-2074@@ Information Extraction (IE) is one of the funda-mental problems of natural language processing. Therefore, we decided to clas-sify sentences into 3 categories based on the com-plexity of dependency relations between the action cues (V) and the likely subject (S) and object cues (O). We notice that in the simple category, the perpetrator cue (terrorists) is always a subject, action cue (kidnapped) a verb, and vic-tim cue (peasants) For the average category, perpetrator and victim commonly appear under 3 relations: subject, object and pcomp-n.
P06-2075@@ Learning lexical semantic relationships is a fun-damental task needed for most text understand-ing applications. The judges were guided to annotate as Correct the pairs con-forming to the lexical entailment definition, which was reflected in two operational tests: i) Word meaning entailment: whether the meaning of the first (entailing) term implies the meaning of the second (entailed) term under some com-mon sense of the two terms; and ii) Substitutabil-ity: whether the first term can substitute the second term in some natural contexts, such that the meaning of the modified context entails the meaning of the original one. Weld, and A. Yates.
P06-2079@@Sentiment analysis involves the identification ofpositive and negative opinions from a text seg-ment. 6We use two-tailed paired t-tests when performing signif-icance testing, with p set to 0.05 unless otherwise stated.sionality of the feature space and may underminethe impact of the unigrams in the resulting clas-sifier. To seewhy this can be a problem, consider another sen-tence I like this long movie From this sentence,MINIPAR will also extract the VO relation (like,movie).
P06-2080@@For example, thenecessary of semantic parsing for most of NLP ap-plication and the ability to map natural language toa formal query or command language is critical fordeveloping more user-friendly interfaces.Recent approaches have focused on using struc-tured prediction for dealing with syntactic parsing(B. Taskar et. Let n be the number of commonbrackets in the two trees. This was because the output of eachSSVM is complex (i.e a logical form) so it is notsure that the voting method can select a correctedoutput.
P06-2081@@There is now considerable interest in affective lan-guage processing. Each factor gives a continu-ous dimension for personality scoring. Rating e-mail personality at zero acquain-tance.
P06-2083@@In the biomedical literature the amount of terms(names of genes, proteins, chemical compounds,drugs, organisms, etc) is increasing at an astound-ing rate. are combined into a node. The algorithm missed 28pairs because: 17 (10%) pairs in the corpus werenot acronyms but more generic aliases, eg, alphatocopherol (Vitamin E); 4 (2%) pairs in the cor-pus were incorrectly annotated (e.g, long form inthe corpus embryo fibroblasts lacks word mouse toform acronym MEFS); and 7 (4%) long forms aremissed by the algorithm, eg, the algorithm recog-nized pair protein kinase (PKR) while the correctpair in the corpus is RNA-activated protein kinase(PKR).
P06-2084@@Lexical association measures are mathematicalformulas determining the strength of associationbetween two or more words based on their occur-rences and cooccurrences in a text corpus. Kappa P (xy)+P (xy Laplace max[NP (xy)+1NP (x Collective strength P (xy)+P (xy stands for any word except w;  stands for any word; N is a total number of bi-grams. Tx as a ranker function.
P06-2085@@Good clarification strategies in dialogue systemshelp to ensure and maintain mutual understand-ing and thus play a crucial role in robust conversa-tional interaction. 6To find the context type which provides the rich-est information to a classifier, we compared the ac-curacy achieved in a 10-fold cross validation bya Na ve Bayes classifier (as a standard) on thesedata sets against the majority class baseline, us-ing a paired t-test, we found that that for context3 and context 4, Na We chose to use context 3, since thesefeatures will be available during system runtimeand the learnt strategy could be implemented in anactual system.. % improvement over a one rule-based baseline).
P06-2086@@ The most common preprocessing technique for text mining is information extraction (IE). "WordNet: A lexical database for English." Else                         Add s to the NegativeSet(P).
P06-2087@@Information retrieval (IR) is a challenging en-deavor due to problems caused by the underly-ing expressiveness of all natural languages. This could be handled interactivelythrough displaying a ranked list of retrieveditems returned by the first query. 33 using statistical methods and 0.
P06-2089@@Over the past decade, researchers have devel-oped several constituent parsers trained on an-notated data that achieve high levels of accu-racy. A maximum entropy approach to naturallanguage processing. The linear distance (number of words apart) between the head-words ofS(0) and S(1)9.
P06-2093@@The goal of statistical machine translation (SMT)is to produce a target sentence e from a source sen-tence f . [1, N ] (2)where N is the size of the vocabulary. A neural probabilistic lan-guage model.
P06-2094@@ Most of the worlds information is recorded, passed down, and transmitted between people in text form. usually not fulle data, the evaluation data are sele more useful and interesting e information is. Ex-tracting Paraphrases from a Parallel Corpus.
P06-2095@@There is no doubt that both professional andtrainee translators need access to authentic dataprovided by corpora. T (S(s0)).This reduces the class of experience to 128 words.This step crucially relies on a wide-coveragemachine readable dictionary. This means that the ex-isting method finds mostly NPs for NPs, verb-object pairs for verb-object pairs, etc, even if themost natural translation uses a different syntacticstructure, eg I like doing X instead of I do Xgladly (when translating from German ich macheX gerne).Second, suggestions are generated for the queryexpression independently from the context it isused in.
P06-2098@@The Viterbi algorithm and the CKY algorithms aretwo decoding algorithms essential to the area of nat-ural language processing. I, JADJ, 0 This part is called a transition. on Computational Natural Lan-guage Learning (CoNLL)T. Zhang.
P06-2100@@and Problem DefinitionPart-Of-Speech (POS) tagging is a complextask fraught with challenges like ambiguity ofparts of speech and handling of lexical ab-sence Theaccuracy of these taggers ranges from 93-98%approximately. A hybrid grammaticaltagger: CLAWS4 . D. Manning and H. Schutze.
P06-2102@@The ATBcomprises manually annotated morphological andsyntactic analyses of newswire text from differentArabic sources, while the AG is simply a huge col-lection of raw Arabic newswire text. As an example, the rootk t b,2 if interspersed with the pattern 1a2a3  thenumbers correspond to the positions of the first,second and third radicals in the root, respectively However, if the pat-tern were ma1A2i3, resulting in the word makAtib,it would mean offices/desks or correspondences.There are fifteen pattern forms for MSA verbs, ofwhich ten are commonly used. Fi-nally, we include sentential complements (SBARand S).
P06-2103@@Most of these efforts, how-ever, have limited applicability. Prentice Hall.Donia R. Scott and Clarisse S. de Souza. That is, E stands for theset of all possible order permutations of C ED , andF , with the additional information that any of theseorders are to appear between the beginning   andend of document   .
P06-2105@@While communicating, humans use different ex-pressions to convey the same meaning. were deter-mined using a grid search on each developmentset. In Proceedings of the PASCAL ChallengesWorkshop, Southampton, U.K., April.R.
P06-2107@@Computers have become an important tool to in-crease the translators productivity. Ininteraction-0, the system suggests a translation. Finally, sort the list maximising the diver-sity at the beginning of the suffixes and select thefirst N hypotheses.
P06-2108@@Currently, the most popular Chinese in-put system is phonetic and pinyin based ap-proach, because Chinese people are taught to write phonetic and pinyin syllables of each Chi-nese character in primary school. For this input syllables, we have a WSM-sentence . Golden Mandarin (I) A Real-Time Mandarin Speech Dictation Machine for Chinese Language with Very Large Vocabu-lary, IEEE Transaction on Speech and Audio Processing, 1(2).
P06-2109@@Although these methods have beensignificantly improved to extract good sentencesas summaries, they are not intended to shorten sen-tences; ie, the output often has redundant wordsor phrases. Note that our model does notuse Bayes D E F ).To represent all summary candidates, we cre-ate a compression forest as Knight and Marcu did.We select the tree assigned the highest probabilityfrom the forest.Features in the maximum entropy model are de-fined for a tree node and its surroundings. A Maximum Entropy Approach to NaturalLanguage Processing.
P06-2111@@People use multiple ways to express the same idea.These alternative ways of conveying the same in-formation in different ways are referred to by theterm paraphrase and in the case of single wordssharing the same meaning we speak of synonyms.Identification of synonyms is critical for manyNLP tasks. For bothapproaches, we used a cutoff n for each row in ourword-by-context matrix. Ex-tracting paraphrases from a parallel corpus.
P06-2112@@In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. And e represents an English sentence. A Probability Model to Improve Word Alignment.
P06-2123@@Many approaches have been proposed in Chineseword segmentation in the past decades. separates the results ofunigram, bigram and trigram.R P F R-oov R-ivAS 0. After the IOB tag-ging, each word is tagged with a B/I/O tag.
P06-2124@@Parallel data has been treated as sets of unre-lated sentence-pairs in state-of-the-art statisticalmachine translation (SMT) models. In BayesianStatistics 7.David Blei, Andrew NG, and M.I. Now e is generatedfrom a topic-based language model , instead of auniform distribution in BiTAM-1.
P06-3002@@Assigning syntactic categories to words is an important pre-processing step for most NLP applications. Proceedings of the HLT-NAACL-06 Workshop on Textgraphs-06, New York, USA E. Charniak, C. Hendrickson, N. Jacobson and M. Perkowitz. Combining Distributional and Morphological Information for Part of Speech Induction, Proceedings of EACL-03 T. Dunning.
P06-3003@@ An interesting and important problem in the Statistical Machine Translation (SMT) domain is the creation of sub-sentential alignment in a par-allel corpus (a bilingual corpus already aligned at the sentence level). pf and pl can be retrieved in O(|S|*log T) with a dichotomy search. So, the algorithm can be described as follow: 1Compute a correlation coefficient for all the substrings pairs in e and f  and mark all the ele-ments in e and f as free.
P06-3004@@He conducts experiments on theWSJ and the Brown Corpus, parsing one of thetreebanks while having trained on the other one.He draws the conclusion that a small amount ofmatched training data is better than a large amountof unmatched training data. The number of crossing branches isthe lowest in this treebank version.In the last modification that combines all mod-ifications made before (T U f NU NF), as ex-pected, all values drop dramatically. Treebanks and their annotation schemesrespectively are compared using a stepwise ap-proximation.
P06-3007@@ With the growing of online information, it is in-efficient for a computer user to browse a great number of individual news documents. Advances in Automatic Text Summarization, Inderjeet Mani and Mark T. Maybury (editors), 137-154. MEAD a platform for multidocument multilingual text summarization.
P06-3009@@Natural Language Processing is typically viewedas consisting of different layers,1 each of which ishandled separately. E.g., phonological, morphological, syntactic, semanticand pragmatic.Moreover, we show that morphological cues facil-itate syntactic disambiguation. Building a Tree-Bank forModern Hebrew Text.
P06-3010@@ Word Sense Disambiguation (WSD) is concerned with the identification of the correct sense of an ambiguous word given its context. References  E. Agirre and M. Stevenson. (1) If there is such a thing as reincarnation, I would not mind coming back as a squirrel.
P06-4001@@ Language testing, aimed to assess learners language ability, is an essential part of language teaching and learning. We define a notation scheme for the distractor designing. and I enjoy surfing on the Internet.
P06-4005@@Recently, biomedical researchers have been fac-ing the vast repository of research papers, egMEDLINE. Functions of Info-PubMedIn the Gene Searcher window, enter the nameof a gene or protein that you are interested in.For example, if you are interested in Raf1, typeraf1 Then, drag3http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/19one of the GeneBoxes from the Gene Searcherto the Interaction Viewer. Adapting a probabilistic disambiguationmodel of an HPSG parser to a new domain.
P06-4007@@are now being developed which al-low users to ask questions in the context of ex-tended dialogues in order to gather informationrelated to any number of complex scenarios. that anticipate a users in-formation needs. For ex-ample, two of the predictive question-answer pairsgenerated from the documents returned for ques-tion Q0, What has been the impact of job out-sourcing programs on Indias relationship with theU.S.
P06-4011@@In recent years, with the rapid development of globalization, English for Academic Purposes has drawn researchers attention and become the mainstream of English for Specific Purposes, particularly in the field of English of Academic Writing (EAW). In sum, the move progression generally follows the sequence of "B-P-M-R-C". denotes the beginning or the ending of a given abstract.)
P06-4015@@The SAMMIE system, developed in the TALKproject in cooperation between several academicand industrial partners, employs the InformationState Update paradigm, extended to model collab-orative problem solving, multimodal context andthe drivers attention state. It is based on some con-cepts of the ACT-R 4.0 system, in particular thegoal-oriented application of production rules, the2Short for (P)roduction rule system based on (A)ctivationand (T)yped feature structure (E)lements.activation of working memory elements, and theweighting of production rules. The lane-change-task as a toolfor driver distraction evaluation.
P06-4016@@As a consequence of globalization, a large and in-creasing number of people must cope with docu-ments in a language other than their own. [ TP [ DP the [ NP recordi [ CP thati [ TP [ DPJohn ] [ VP broke [ DP e]i ] ] ] ] ] [ VP was[ AP old ] ] ]In this analysis, notice that the noun record iscoindexed with the relative pronoun that, which inturn is coindexed with the empty direct object ofthe verb broke. It uses a bottom-up parsing al-gorithm with parallel treatment of alternatives, aswell as heuristics to rank alternatives (and cut theirnumbers when necessary).The syntactic structures built by Fips are allof the same pattern, that is : [ XP L X R ],where L stands for the possibly empty list of leftconstituents, X for the (possibly empty) head ofthe phrase and R for the (possibly empty) listof right constituents.
P06-4018@@Data types include tokens, tags, chunks,trees, and feature structures. For example, the following code readspart a of the Brown Corpus. If both are done indepen-dently, we need to be able to align the results.As task combinations multiply, managing the databecomes extremely difficult.To address this problem, NLTK 1.  introduceda blackboard architecture for tokens, unifyingmany data types, and permitting distinct tasksto be run independently.
P06-4020@@Firstly, all mod-ules have been incrementally improved to cover agreater range of text types. In cases where there is no parse rooted in S,the parser returns a connected sequence of partialparses covering the input. These rules are compiled into an efficientC program encoding a deterministic finite statetransducer.
P07-1001@@Statistical word alignment models learn word as-sociations between parallel sentences from statis-tics. ,M , S is aRR diagonal matrix of singular values s1  For each i, thescaled R-vector uiS may be viewed as representingwi, the i-th word in the vocabulary, and similarly thescaled R-vector vjS as representing dj , j-th docu-ment in the corpus. It con-siders all possible hidden tags of e and f and servesas constraint between the link.
P07-1002@@The machine translation task can be viewed as con-sisting of two subtasks: predicting the collection ofwords in a translation, and deciding the order of thepredicted words. Given anunordered sentence t and an unordered target de-pendency tree tree(t), we define two spaces of tar-get sentence orders. We define a set of feature functions fm(ol,n, spl)to describe a target word order ol,n of a given sen-tence pair spl.
P07-1003@@However, despite recentprogress, almost all syntactic MT systems, indeedstatistical MT systems in general, build upon crudelegacy models of word alignment. However, its distor-tion model considers only string distance, disregard-ing the constituent structure of the English sentence.To allow syntax-sensitive distortion, we considera new distortion model of the form pd(aj |aj , j, t).We condition on t via a generative process that tran-sitions between two English positions by traversingthe unique shortest path (aj , we choose whether to STOP orPOP from child to parent, conditioning on thetype of the parent node n. Upon choosingSTOP, we transition to stage 2.Stage 2 (MOVE(n, d)): Again, conditioning on thetype of the parent n After exactly one MOVE, wetransition to stage 3.Stage 3 (PUSH(n, n(n ))): Given the current noden, we select one of its children n, conditioningon the type of n and the position of the childn(n). Null emission probabilities were fixed to 1|e| , inversely pro-portional to the length of the English sentence.
P07-1004@@In statistical machine translation (SMT), translationis modeled as a decision process. Their weights are optimized w.r.t. The third methoduses a threshold-based selection method.
P07-1005@@Many words have multiple meanings, depending onthe context in which they are used. M. II Lewis and R. E. Stearns. LIBSVM: a library for sup-port vector machines.
P07-1006@@ Word Sense Disambiguation (WSD) is concerned with the identification of the meaning of ambi-guous words in context. Ea predicate p specifying the target relation to be learned knowledge  of the domain, described ac-cording to a language Lk, which specifies which predicates qi can be part of the definition of p. The goal is: to induce a hypothesis (or theory) h for p, with relation to E and , which covers most of the E+, without covering the E-, ie, K  The default inference engine induces a theory iteratively using the following steps: 1. A Maximum Entropy Part-Of-Speech Tagger.
P07-1007@@In natural language, a word often assumes differentmeanings, and the task of determining the correctmeaning, or sense, of a word in different contextsis known as word sense disambiguation (WSD). (s)(i) to the old priors pL(i). The likelihood ofthese N instances can be defined as:L(x1, .
P07-1008@@However, in a context in which the addictiveproperties of chocolate are very salient (eg, an on-line dieting forum), chocolate is more likely to becategorized as a drug and thus be considered moresimilar to heroin. We use over 2-gigabytes oftext from the online encyclopaedia Wikipediaas our corpus.E. A weaker correlation of 0.
P07-1009@@Linguistic typology aims to distinguish between log-ically possible languages and actually observed lan-guages. Essentially, all sampling steps are standardGibbs steps, except for sampling the error rates e.The Gibbs step is not available analytically for these.Hence, we use rejection sampling (drawing from theBeta prior and accepting according to the posterior).The sampling procedure for the HIER model isonly slightly more complicated. is alsogiven a Beta prior.
P07-1010@@Language models (LMs) are fundamental tools formany applications, such as speech recognition, ma-chine translation and spelling correction. We used DLM-train as a training set. Journal of Machine LearningResearch.Michael I. Jordan David M. Blei, Andrew Y. Ng.
P07-1011@@Detecting erroneous/correct sentences has the fol-lowing applications. p1 is a better indication of class E than p2. In our experiments, weempirically set minimum support at 0.
P07-1013@@Grapheme-to-Phoneme conversion (g2p) is the taskof converting a word from its spelling (eg Stern-anisol, Engl: star-anise oil) to its pronunciation(/"StERnPani:sP:l/). Morphological PreprocessingIn German, information about morphologicalboundaries is needed to correctly insert glottal stops[P] in complex words, to determine irregular pro-nunciation of affixes (v is pronounced [v] in ver-tikal but [f] in ver+ticker+n, and the suffix syllableheit is not stressed although superheavy and wordfinal) and to disambiguate letters (eg e is alwayspronounced /@/ when occurring in inflectional suf-fixes). Marchand and R. I. Damper.
P07-1014@@Redundancy is a strikingly common phenomenonthat is observed across many natural systems. In orderto test whether this difference is significant, we per-form a t-test comparing the distribution of the val-ues of RR that gives rise to such curves for the realand the random inventories. A linguistic system isalso not an exception.
P07-1015@@In this paper, we develop a multi-lingual transliteration system for named entities. For example, stop and fricative consonants such as /p, t, k, b, d, g, s, z/ are frequently deleted when they appear in the coda position. It means that t1  scores higher as the transliteration of e than t2.
P07-1016@@ The study of Chinese transliteration dates back to the seventh century when Buddhist scriptures were translated into Chinese. The E-C corpus also contains some entries without gender/surname labels, referred to as unclassified. (4) is in fact the n-gram likelihood of the token pair ,i it s (5) approximates this probability using a bigram language model.
P07-1019@@However, efficient de-coding under these paradigms, especially with inte-grated language models (LMs), remains a difficultproblem. For any item x that is not explored yet, thetrue cost c(x)  h(x), by the definition of h. Andthere exists an item y  bound by the definition ofbound. from cand to buf , and then line 12 pushesits successors {e, j + bi Cube GrowingAlthough much faster than full-integration, cubepruning still computes a fixed amount of +LM itemsat each node, many of which will not be useful forarriving at the 1-best hypothesis at the root.
P07-1020@@Machine translation can be viewed as consisting oftwo subproblems: (a) lexical selection, where appro-priate target language lexical items are chosen foreach source language lexical item and (b) lexical re-ordering, where the chosen target language lexicalitems are rearranged to produce a meaningful targetlanguage string. Bangalore and A. K. Joshi. In the sequentialmodel, we have a multiclass problem where eachclass ti is exclusive, therefore, all the classifier out-puts P (ti|) d, i+ d) : bag of n-grams BOgram(S, 0, |S|): bag of n-gramsin source sentence in the interval [i d, i+ d)) P (BOW (T )|BOgram(S, 0, |S|))Independence assumption between the labelsNumber of classes One per target word or phraseTraining samples One per source token One per sentencePreprocessing Source/Target word alignment Source/Target sentence alignmenti P (ti|) This can be problematic: withone classifier per word in the vocabulary, even allo-cating the memory during training may exceed thememory capacity of current computers.In the BOW model, each class can be detectedindependently, and two different classes can be de-tected at the same time.
P07-1021@@Syntactic representations based on word-to-word de-pendencies have a long tradition in descriptive lin-guistics. An order-annotated tree is atree labelled with pairs h; !i, where  is the labelproper, and ! A dependency structure is well-nested if and only if no annotation !.u/ containsa substring i    j    i    j , for i; j 2 N. Example.
P07-1022@@Projective Bilexical Dependency Grammars (PB-DGs) have attracted attention recently for two rea-sons. Kluwer Academic Publishers.Joshua T. Goodman. encod-ing of PBDGs as CFGs with an O(n5) parse time,where n is the length of the string to be parsed.
P07-1024@@Dependency approaches to language assume that ev-ery word in a sentence is the dependent of one otherword (except for one word, which is the global headof the sentence), so that the words of a sentence forman acyclic directed graph. To see this, consider the case whereboth child i (the longest child) and child i  1 (thesecond longest child) appear on the same side of thehead. Under this model the wordorder is determined by placing all dependents of aword in order of increasing weight from left to right.This reduces the number of parameters of the modelto T , if there are T dependency types, from T k ifa word may have up to k dependents.
P07-1026@@ Given a sentence, the task of Semantic Role Label-ing (SRL) consists of analyzing the logical forms expressed by some target verbs or nouns and some constituents of the sentence. 1) is a small penalty to penal                                                         3 Eq. It is defined as (1 )  (0 1)hybrid path csK K K We use a greedy strategy to fine-tune parameters.
P07-1027@@The task of Semantic Role Labeling (SRL) is toidentify predicate-argument relationships in naturallanguage texts in a domain-independent fashion. The desirablecharacteristics of such a feature are:1. However, for the best re-sults we should take advantage of gold parse treeswhenever possible, including those of the labeledtraining data.J&N maxent linear asoidentification 82.
P07-1028@@Selectional preferences, which characterize typ-ical arguments of predicates, are a very use-ful and versatile knowledge source. statistic hasapproximately a t distribution with 5 degrees offreedom. We write f for fre-quency, I for mutual information, and R(w) forthe set of arguments rp for which w occurs as aheadword.In this paper we only study corpus-based met-rics.
P07-1029@@While high-quality NLP corpora and tools are avail-able in English, such resources are difficult to obtainin most other languages. is a syntactic marker of defi-nite direct objects in Hebrew. yjjxj is an n dimen-sional weight vector assigning weight for each ofthe n features.
P07-1030@@The huge amount of information available on theweb has led to a flurry of research on methods forautomatic creation of structured information fromlarge unstructured text corpora. Mergethese into a single concept-word-independent clus-ter. We call the resultinggroups, S-groups.
P07-1031@@It is used as a standard train-ing and evaluation corpus in many syntactic analysistasks, ranging from part of speech (POS) tagging andchunking, to full parsing.Unfortunately, the Penn Treebank does not anno-tate the internal structure of base noun phrases, in-stead leaving them flat. Search engine statisticsbeyond the n-gram: Application to noun compound brack-eting. A Theory of Syntactic Recognition forNatural Language.
P07-1032@@This allows the many parsers based onthe Penn Treebank, for example, to be meaningfullycompared. F-score is the balanced harmonic mean of precision (P ) and recall (R):2PR/(P + R). A log-linearmodel scores the alternative parses.
P07-1033@@The task of domain adaptation is to develop learn-ing algorithms that can be easily ported from onedomain to anothersay, from newswire to biomed-ical documents. Forthis reason, we sketch the derivation here.Suppose that the data points x are drawn from areproducing kernel Hilbert space X with kernel K :X  Then,K can be written as the dot product (in X ) of two(perhaps infinite-dimensional) vectors: K(x, x) (x)Now, we can compute the kernel product be-tween s and t in the expanded RKHS by mak-ing use of the original kernel K. We denote the ex-panded kernel by K It is simplest to first de-scribe K are from the samedomain, then analyze the case when the domaindiffers. For aK-domain problem, we simply expand the featurespace to R(K+1)F in the obvious way (the +1 .K correspond to a single task).
P07-1034@@Many natural language processing (NLP) problemssuch as part-of-speech (POS) tagging, named entity(NE) recognition, relation extraction, and seman-tic role labeling, are currently solved by supervisedlearning from manually labeled data. The top k instancesthat are incorrectly predicted by  t,l (ranked by theirprediction confidence) are discarded. is no longer a constantbut is a function of .
P07-1035@@Model-based unsupervised learning techniques havehistorically lacked good methods for choosing thenumber of unseen components. A. Smith and J. Eisner. In Advances in Neural Infor-mation Processing Systems, pages 577584.E.
P07-1036@@Natural Language Processing (NLP) systems typi-cally require large amounts of knowledge to achievegood performance. The correct assignment was shown in (a). N is the numberof labeled samples.
P07-1037@@However, unlike in ruleandexample-based MT, it has proven difficult to date toincorporate linguistic, syntactic knowledge in orderto improve translation quality. BLEU points, or a 6. PST (t, ST ) is a supertagger assigning prob-abilities to sequences of wordsupertag pairs.
P07-1038@@Under a learn-ing framework, the input (ie, the sentence to beevaluated) is represented as a set of features. Our full featurevector consists of r  18 adequacy features, wherer is the number of reference systems used, and 26fluency features:Adequacy features: These include features de-rived from BLEU (eg, n-gram precision, where1  5, length ratios), PER, WER, fea-tures derived from METEOR (precision, recall,fragmentation), and ROUGE-related features (non-consecutive bigrams with a gap size of g, where1  5 and longest common subsequence).Fluency features: We consider both string-levelfeatures such as computing n-gram precision againsta target-language corpus as well as several syntax-based features. Training a sentence-level machinetranslation confidence measure.
P07-1039@@Automatic word alignment can be defined as theproblem of determining a translational correspon-dence at word level given a parallel corpus of alignedsentences. Translating collocations forbilingual lexicons: A statistical approach. We also assume that ACEand AEC contain 1:n alignments.
P07-1041@@Many natural languages allow variation in the wordorder. We trans-form each clause in the training set into a sequenceof such tags, plus three tags for the verb position (v),the beginning (b) and the end (e) of the clause. 4 tN(N1) , where t is the number of interchangesof consecutive elements to arrange N elements inthe right order.
P07-1042@@In these theories, asentence is associated not just with a semantic rep-resentation but with a semantic representation en-riched with additional syntactic, pragmatic and/ordiscourse information. Given a semantic formula, itmight produce several outputs. As a result, the semantics of John oftenruns is(1) {name(j,john),run(r,j),often(r)}The grammar used describes a core fragment ofFrench and contains around 6 000 elementary trees.It covers some 35 basic subcategorisation framesand for each of these frames, the set of argument re-distributions (active, passive, middle, neuter, reflex-ivisation, impersonal, passive impersonal) and of ar-gument realisations (cliticisation, extraction, omis-sion, permutations, etc.)
P07-1043@@Systems that produce natural language must synthe-size the primitives of linguistic structure into well-formed utterances that make desired contributions todiscourse. Foreach elementary tree t and possible step index i, weestablish the relationship between these parametersand the roles in two steps. The knowledgebase is some finite set of ground atoms; in the exam-ple, it could contain such entries as like(e,m,r) andrabbit(r).
P07-1044@@Intrinsic evaluation of the output of many languagetechnologies can be characterised as having at leasttwo aspects: how well the generated text reflectsthe source data, whether it be text in another lan-guage for machine translation (MT), a natural lan-guage generation (NLG) input representation, a doc-ument to be summarised, and so on; and how well itconforms to normal human language usage. We thengenerated a template of length n pre-terminal cate-gories using this CFG. For a metric, we normalise this bythe sentence length.
P07-1045@@For ex-ample, consider the following excerpt from a dia-logue in which the speaker describes a mechanicaldevice:So this moves up, and it  everything moves up.And this top one clears this area here, and goes allthe way up to the top. We next plan to investigate models with a tem-poral component, so that the behavior of the hiddenvariable is governed by a finite-state transducer.Acknowledgments We thank Aaron Adler, ReginaBarzilay, S. R. K. Branavan, Sonya Cates, Erdong Chen,Michael Collins, Lisa Guttentag, Michael Oltmans, and TomOuyang. Thus, conver-gence to a global minimum is not guaranteed.
P07-1047@@Here,we use acquisition to refer to the process of acquir-ing relevant vocabularies describing internal entities,and interpretation to refer to the process of automat-ically identifying internal entities given a particularword. To denote allthe word-object mappings in the i-th list pair, we in-troduce an alignment vector ai, whose element ai,jtakes the value k if the word wi,j is mapped to oi,k.Then, the likelihood of the observations given the371parameters can be computed as followsPr(D;) Therefore, the likelihoodcan be simplified asPr(D;) Similarly, wecan develop the model in the context of mappinggiven words to objects (for vocabulary interpreta-tion), whose solution leads to another set of prob-abilities {Pr(ok|wj),k} for each word wj indicat-ing how likely every object is mapped to it. Just and P. A. Carpenter.
P07-1048@@ As traditional entertainment channels and the internet converge through the advent of technolo-gies such as broadband access, movies-on-demand, and streaming video, an increasingly large range of content is available to consumers in the home. The Role of Natural Language in a Multimodal Interface. U.S. News and World Report.
P07-1049@@Grammar induction, the learning of the grammarof a language from unannotated example sentences,has long been of interest to linguists because of itsrelevance to language acquisition by children. I would like to thank Alexander Clark for suggesting thistest. This operation is a lexicon update.
P07-1050@@The dependency parsing problem is nat-urally a spanning tree problem; however, effi-cient spanning-tree optimization algorithms assumea cost function which assigns scores independentlyto edges of the graph. The third-best solution is eitherthe second-best solution to GY,{Ze} or the second-best solution to G{Y e},Z . In Conference on Natural Language Learning.Stefan Riezler, Tracy H. King, Ronald M. Kaplan, RichardCrouch, John T. III Maxwell, and Mark Johnson.
P07-1051@@  A major challenge in natural language parsing is the unsupervised induction of syntactic structure. TnT A Statistical Part-of-Speech Tagger. For a 2,000 sentence test set from a different part of the Europarl corpus we computed the most probable target sentence (using Viterbi n best).
P07-1054@@funding scheme.cipline that deals with the quantitative and qualita-tive analysis of text for the purpose of determiningits opinion-related properties (ORPs). We have experimented with several dif-ferent definitions of e, each for both positivity andnegativity. WordNet 2: A morphologically and semantically en-hanced resource.
P07-1055@@Extracting sentiment from text is a challenging prob-lem with applications throughout Natural LanguageProcessing and Information Retrieval. This algorithm is outlined in Fig-ure 2 and has a runtime of O(|Y(d)||Y(s)|2n), dueto running Viterbi |Y(d)| times over a label space ofsize |Y(s)|. Of all the weight vectors that sat-isfy these constraints, MIRA chooses the one that isas close as possible to the previous weight vector inorder to retain information about previous updates.The loss function L(y,y) Experiments with different loss functionsfor the joint sentence-document model on a develop-ment data set indicated that the hamming loss oversentence labels multiplied by the 0-1 loss over doc-ument labels worked best.An important modification that was made to thelearning algorithm deals with how the k constraintsare chosen for the optimization.
P07-1056@@Research results have beendeployed industrially in systems that gauge marketreaction and summarize opinion from Web pages,discussion boards, and blogs.With such widely-varying domains, researchersand engineers who build sentiment classificationsystems need to collect and curate data for each newdomain they encounter. We stress that ourmethod improves a supervised baseline. Inthat case, she can ask the question Which sourcesshould I label to obtain the best performance overall my domains Here we show how toselect source domains using only unlabeled data andthe SCL representation.
P07-1057@@The semantic relationship between words, andthe extraction of meaning from syntactic datahas been one of the main points of research inthe field of computational linguistics (see Sec-tion 5 and references therein). In the example notation, the anchoringword is the object of the first clause and thesubject of the second (O-S for short). n indicatesthe maximal distance between the two clauses.The terms SC, V C or OC with a subscriptedindex represent the cluster containing the sub-ject, verb or object (respectively) of the appro-priate clause.
P07-1059@@One of the fundamental problems in Question An-swering (QA) has been recognized to be the lexi-cal chasm This problem is mani-fested in a mismatch between question and answervocabularies, and is aggravated by the inherent am-biguity of natural language. Experimen-tal results show a significant improvement of SMT-based query expansion over both baselines. Synonyms for queryexpansion are read off from the n-best paraphrasesof full queries instead of from paraphrases of sep-arate words or phrases.
P07-1062@@These annotations include sentence segmen-tation into discourse units along with the linkingof discourse units, both within and across sentenceboundaries, into a labeled hierarchical structure. In Proceedings of the International Confer-ence in Computational Linguistics (COLING), pages 4349.E.F. Results for unlabeledbracketing are presented, along with results for la-beled bracketing, where the label is either Nucleusor Satellite, depending upon whether or not the nodeis more central (Nucleus) to the coherence of the textthan its sibling(s) (Satellite).
P07-1063@@Over the last fifty years, the Big Five Adistinct line of research has explored methods forautomatically generating language that varies alongpersonality dimensions, targeting applications suchas computer gaming and educational virtual worlds(Andre However, to date, (1) research in gener-ation has not systematically exploited the psycholin-guistic findings; and (2) there has been little evalua-tion showing that automatic generators can producelanguage with recognizable personality variation.Alt Realization Extra5 Err... it seems to me that Le Marais isnt as badas the others. H. Witten and E. Frank. A strategy for generatingevaluative arguments.
P07-1065@@Language modelling (LM) is a crucial component instatistical machine translation (SMT). Network applicationsof bloom filters: A survey. In our experiments we cre-ate a range of models referred to by the corpus used(EP or GW), the order of the n-gram(s) entered intothe filter (1 to 10), whether the model is Boolean(Bool-BF) or provides frequency information (Freq-BF), whether or not sub-sequence filtering was used(FTR) and whether it was used in conjunction withthe baseline trigram (+EP-KN-3).
P07-1067@@Semantic relatedness is a very important factor forcoreference resolution, as noun phrases used to re-fer to the same entity should have a certain semanticrelation. %as in NWire), with a similar or even higher preci-sion. For exam-ple, the following textAmerican || United States | Washington D.C. | .
P07-1068@@In the past decade, knowledge-lean approaches havesignificantly influenced research in noun phrase(NP) coreference resolution  the problem of deter-mining which NPs refer to the same real-world en-tity in a document. To represent i, we generateone feature from each non-empty subset of Li.. This indicates the proper noun is not a MUC NE.
P07-1069@@Current research in summarization focuses on pro-cessing short articles, primarily in the news domain.While in practice the existing summarization meth-ods are not limited to this material, they are notuniversal: texts in many domains and genres can-not be summarized using these techniques. We represent a tree of segments Spaired with titles T with the global feature vectorglob(S, T ). A discriminative model for tree-to-tree trans-lation.
P07-1070@@ Text summarization is the process of creating a compressed version of a given document that de-livers the main topic of the document. Authoritative sources in a hyperlinked environment. A nonnegative weight aff(si,tj) is specified on the edge, which is proportional to the importance of word tj in sentence si, computed as follows: ijjstttttji isftfisftf,tsaff )(  (4)where t represents a unique term in si and tft, isftare respectively the term frequency in the sentence and the inverse sentence frequency.
P07-1071@@Semantic understanding plays an important role inmany end-user applications involving text: for infor-mation extraction, web-crawling systems, questionand answer based systems, as well as machine trans-lation, summarization and search. In Advances in Neural Informa-tion Processing Systems, NIPS 13.B.E. N. Let us represent asentence of nw words to be analyzed as a functions().
P07-1073@@Presumably, none of the sentences for negativepairs state the targeted relation. A. Smith and J. Eisner. Then the sequence s will be represented in therelation example as a feature with weight computedas  controls the mag-nitude of the gap penalty, where g(s) We there-fore modified the kernel computation to optionallyignore subsequence patterns formed exclusively of578stop words and punctuation signs.
P07-1074@@ Information extraction (IE) has the task to discover n-tuples of relevant items (entities) belonging to an n-ary relation in natural language documents. 3 DARE Rule Representation  Our rule representation is designed to specify the location and the role of the arguments w.r.t. E.g., in the management succes-sion domain that concerns the identification of job changing events, a person can either move into a 584job (called Person_In) or leave a job (called Per-son_Out).
P07-1075@@ Information Extraction (IE) is the task of identify-ing information in texts and converting it into a predefined format. For each sentence, we compose a set of candidate tem-plates T using the extracted relation paths between each Ai and Aj. A. Culotta and J. Sorensen J.
P07-1076@@ Information Extraction (IE) is the task of extract-ing factual assertions from text. There is a significant practical difference between the two cases. On the other hand, if a word was missed from an entity (eg, Beverly O, instead of Beverly O  Neill ), the resulting sequence will be frequent.
P07-1077@@)We think that it is worth bearing in mind thatneither syntactic structures in dependency tree-banks, nor structures arising in machine-learning ap-proaches, such as MST dependency parsing, need apriori fall into any formal subclass of dependencytrees. The importance of such means becomes more evident fromthe asymptotically negligible proportion of projective trees toall dependency trees; there are super-exponentially many unre-stricted trees compared to exponentially many projective treeson n nodes. : a treebank for Portuguese.
P07-1078@@Therefore, enhanc-ing the performance of parsers when trained onsmall manually annotated datasets is of great impor-tance, both when the seed and test data are takenfrom the same domain (the in-domain scenario) andwhen they are taken from different domains (the out-of-domain or parser adaptation scenario). A two-stagemethod for active learning of statistical grammars. Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
P07-1080@@Instead, a labeling with these features canbe induced as part of the training process. of Pennsylvania, PA.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and Mark John-son. Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
P07-1082@@Translation of a text from a source language toa target language requires dealing with technicalterms and proper names. That is, if search in the rules ofpattern CVC was unsuccessful, it looks for e Similarly, segmentation for this word con-tinues with ll is an approximant, and therefore considered asa vowel when transliterating English to Persian). A joint source-channel model for machine transliteration.
P07-1083@@String similarity is often used as a means of quan-tifying the likelihood that two pairs of strings havethe same underlying meaning, based purely on thecharacter composition of the two words. In other similarity-driven applica-tions, E and F could be misspelled and correctlyspelled words, or the orthographic and the phoneticrepresentation of words, etc. Learning a transla-tion lexicon from monolingual corpora.
P07-1084@@Two main approaches exist for compiling corpora:Big is beautiful or Insecurity in large collec-tions. Workingwith Specialized Language: A Practical Guide to Us-ing Corpora. For example, theFrench term diabte de type 1 (Diabetes mellitustype I) extracted by the French term extraction pro-gram and found in UMLS was not extracted by theJapanese term extraction program although it ap-pears frequently in the Japanese corpus (     ! "
P07-1086@@Coordination disambiguation is a relatively littlestudied area, yet the correct bracketing of coordina-tion constructions is one of the most difficult prob-lems for natural language parsers. In cats and dogshowever there are two heads, though in the parsingmodel just one is chosen, somewhat arbitrarily, tohead the entire noun phrase.In the baseline model there is essentially one pa-rameter class for the estimation of word probabili-ties:Pword(wi|H(i)) (3)where wi is the lexical head of constituent i andH(i) is the history of the constituent. In other words: if anoracle were to choose from each set of n-best treesthe tree that maximised constituent precision and re-call, then the resulting set of oracle trees would havea NP coordination dependency f-score of 83.
P07-1088@@Information Extraction (IE) from text is far from in-fallible. We rank the elements of UR in ascending order off(e).HMM-T has two advantages over a more tradi-tional type checking approach of simply countingthe number of times in the corpus that each extrac-tion appears in a context in which a seed also ap-pears (cf. Each relation R has a setof types for its arguments.
P07-1089@@See http://www.nist.gov/speech/tests/mt/One major problem with linguistically syntax-based models, however, is that tree-to-string rulesfail to syntactify non-syntactic phrase pairs becausethey require a syntax tree fragment over the phraseto be syntactified. T , the rule r resorts to derivations fromsubcells to infer new derivations. A hierarchical phrase-based modelfor statistical machine translation.
P07-1090@@The focus of this paper is on function words, a classof words with little intrinsic meaning but is vital inexpressing grammatical relationships among wordswithin a sentence. Phrase Ordering around FunctionWordsWe use the following Chinese (c) to English (e)translation in Fig. As a result, Hiero produces grammarsof enormous size.
P07-1091@@The phrase-based approach has been considered thedefault strategy to Statistical Machine Translation(SMT) in recent years. last(t) in the TLsentence. A syntax-based statistical translation model.
P07-1095@@Log-linear models are a very popular tool in naturallanguage processing, and are often lauded for per-mitting the use of arbitrary fea-tures of the data by a model. We call this model l.u. A whole sentence maximum entropy lan-guage model.
P07-1096@@Many NLP tasks can be modeled as a sequence clas-sification problem, such as POS tagging, chunking,and incremental parsing. If p.S.T ,the top hypothesis of the selected span p, is com-patible with the gold standard, we update P and Qas in Algorithm 1. Ta-ble 3 shows the error rates on the development dataset with both left-to-right (L-to-R) and bidirectional(Bi-Dir) search methods.
P07-1098@@Question answering (QA) is as a form of informa-tion retrieval where one or more answers are re-turned to a question in natural language in the formof sentences or phrases. For thelatter, step 0 assigns  We address such problem779by defining a kernel on multiple PASs.Let Pt and Pt be the sets of PASs extracted fromthe text fragment t and t. We define:Kall(Pt, Pt) 4) the Kallkernel is used to handle predicate argument struc-tures, TK (Eq. A further answer re-ranking phase is op-tionally applied.
P07-1099@@Question answering (QA) systems aim at find-ing precise answers to natural language questionsfrom large document collections. In T. Strzalkowski and S. Harabagiu, editors,Advances in Textual Question Answering. Cogex: A logic prover for question answering.In Proceedings of HLT-NAACL.E.
P07-1102@@While humans are remarkably efficient, flexible andreliable communicators, we are far from perfect.Our dialogues differ in how successfully informa-tion is conveyed. Learning to predict problematic situations in a spokendialogue system: experiments with How may I help you To do so, we can make useof the fact that the priming effect decays over time.How strong that decay is gives us an indication ofhow much repetition probability we see shortly afterthe stimulus (prime) compared to the probability ofchance repetitionwithout ever explicitly calculatingsuch a prior.Thus we define the strength of priming as the de-cay rate of repetition probability, from shortly after2We use Generalized Linear Mixed Effects models fitted us-ing GlmmPQL in the MASS R library.
P07-1104@@ Parameter estimation is fundamental to many sta-tistical approaches to NLP. After initialization, Steps 2 and 3 are repeated T times; at each iteration, a feature is chosen and its weight is updated as follows. A maximum entropy part-of-speech tagger.
P07-1105@@There is considerable interest in learning computa-tional grammars. Let  _N   O _ ,~   R be the set ofgrammars specialized from _ .  is a set of constraint rules.
P07-1106@@Words are the basic units to process for most NLPtasks. The(balanced) F-measure is 2pr/(p + r).CWS systems are evaluated by two types of tests.The closed tests require that the system is trainedonly with a designated training corpus. Chinese word segmentation with maximum en-tropy and n-gram language model.
P07-1107@@Referring to an entity in natural language canbroadly be decomposed into two processes. I.e., allentities can be referred to by generic pronouns, thechoice of which depends on entity properties such asgender, not the specific entity.We therefore enrich an entitys parameters  tocontain not only a distribution over lexical headsh, but also distributions (t, g, n) over proper-ties, where t parametrizes a distribution over en-tity types (PER, LOC, ORG, MISC), and g for gen-der (MALE, FEMALE, NEUTER), and n for number(SG, PL). Asfar as notation, we assume a collection of I docu-ments, each with Ji mentions.
P07-1108@@These methods need large bilingual corpora. A Syntax Based Statistical Translation Model. In order to combine these n  pivot models with the standard model trained with the Lf-Le corpus, we use the linear interpolation method.
P07-1109@@Transliterations are words that are converted fromone writing script to another on the basis of their pro-nunciation, rather than being translated on the basisof their meaning. A new algorithm for the alignment ofphonetic sequences. 1)P (si, tj)B(i, j)F (I, J) (3)The major issue in porting the memoriless trans-ducer over to our task of transliteration extraction865is that its training is supervised.
P07-1110@@ Approaches to cross-language information retrieval (CLIR) fall generally into one of two types, or some combination thereof: the query translation The first of these, which is perhaps more common, in-volves translation of the query into the target lan-guage, for example using machine translation or on-line dictionaries. A Multilingual Ap-proach to Multilingual Information Retrieval. measure of the entropy of the term across all documents, and N is the number of  2 The text chunks generally had the same boundaries as the verses in the original text.
P07-1111@@As machine translation (MT) research advances, theimportance of its evaluation also grows. Training a sentence-level machinetranslation confidence measure. When training classifiers, assessment scoresare not used, and the training set is augmented withall available human reference translation sentences(4  To evaluate the rela-tive reliability of different metrics, we use boot-strapping re-sampling and paired t-test to determinewhether the difference between the metrics correla-tion scores has statistical significance (at 99.
P07-1112@@The work described in thispaper addresses this issue and presents an approachto automatically learning qualia structures for nounsfrom the Web. In Proceedings of LREC,pages 58996.E.M. weight and rank the returned qualia elements ac-cording to some measure.The patterns in our pattern library are actuallytuples (p, c) where p is a regular expression de-fined over part-of-speech tags and c a function c :string  Given a nomi-nal n and a clue c, the query c(n) is sent to the websearch engine and the abstracts of the first m docu-ments matching this query are downloaded.
P07-1115@@Research into automatic acquisition of lexical in-formation from large repositories of unannotatedtext (such as the web, corpora of published text,etc.) of the 5th ANLP,Washington DC, USA.E. Designing a dictionary of derived nominals.
P07-1116@@Segmenting a word into meaning-bearing units is particularly interesting for morpho-logically complex languages where words can becomposed of several morphemes through inflection,derivation and composition. Conditions 2 and 3 are met, becausethe transitional probability between auffuhr and thenext letter is low (there are a lot of different pos-sible continuations) and the transitional probabilityP (r|auffuh)  The stem candidate auffuhr is thenstored together with the suffix candidates {ender,ung, en, t, laune}.Step 2: Ranking candidate stemsThere are two types of affix candidates: type-1 affixcandidates are words that are contained in the database as full words (those are due to compounding);type-2 affix candidates are inflectional and deriva-tional suffixes. 5 min on a 2.
P07-1119@@A significant proportion of out-of-vocabulary wordsin machine translation models or cross language in-formation retrieval systems are named entities. Cheng, and E. Bart. A joint source-channelmodel for machine transliteration.
P07-1120@@system consists of a sequence of pro-cessing stages such that the output from one stageprovides the input to the next. A systematic comparison of variousstatistical alignment models. Let Tbe the union of the two n-best lists.
P07-1121@@Our algorithm, WASP, uses statisticalmodels developed for syntax-based SMT for lexicallearning and parse disambiguation. Let span(pi, pj) be the shortest substring of e that2includes e(pi)  from E.Add edges (p0, pi) to E if pi is not already connected to p0.For each edge (pi, pj) in E, set edge weight to the minimum word distance between e(pi) and e(pj). S. Zettlemoyer and M. Collins.
P07-1122@@Treebank parsers are trained on syntactically anno-tated sentences and a major part of their success canbe attributed to extensive manipulations of the train-ing data as well as the output of the parser, usuallyin the form of various tree transformations. The first rowcontains the result for Eisners algorithm using notransformation (N-Proj), projectivized training data(Proj), and pseudo-projective parsing (P-Proj). In Proceedings of ACL, pages 645652.E.
P07-1123@@There is growing interest in the automatic extractionof opinions, emotions, and sentiments in text (sub-jectivity), to provide tools and support for variousnatural language processing applications. A new method for sentiment classifi-cation in text retrieval. For the U tags, a class was also given;OU means, for instance, that the annotator is uncer-tain but she is leaning toward O. calcu-lated for the three annotators.
P07-1124@@Research in sentiment analysis has emerged to ad-dress the research questions: what is affect in textwhat features of text serve to convey it how canthese features be detected and measured automati-cally. Thiswork has been grounded in a strong human evalu-ative component. Ph.D. thesis, Trinity College Dublin.Paul Ekman and W. V. Friesen.
P07-1125@@The automatic processing of scientific papers usingNLP and machine learning (ML) techniques is anincreasingly important aspect of technical informat-ics. The baseline classi-fier achieves a BEP of 0. Statement of existence of proposed alterna-tives.Different models have been proposed to explain how en-docytosis of the ligand, which removes the ligand from thecell surface, results in N receptor activation.
P07-1126@@Our society deals with a growing bulk of un-structured information such as text, images andvideo, a situation witnessed in many domains (news,biomedical information, intelligence information,business documents, etc.). is a hyponym of bird). We choose this threshold to be 0. .
P07-2004@@Over the last decade, many of the main grammaticalframeworks used in computational linguistics wereextended to support semantic construction (ie, thecomputation of a meaning representation from syn-tax and word meanings). variables), let R be a set of n-ary relationsover Ic be a relation over H Lccalled the scope-over relation. , in) is a LU formula.
P07-2012@@The volume of chemical literature published has ex-ploded over the past few years. In Proceedingsof the 4th UK E-Science All Hands Meeting. In Proceed-ings of BioNLP in ACL (BioNLP07).Peter T. Corbett and Peter Murray-Rust.
P07-2017@@In particular, the use of polynomial kernel SVM implicitly takes the feature combinations into ac-count instead of explicitly combines features. However, directly adopt the algorithm is not a good idea. iDinn yxyxyxyx  where xi is a feature vector in D-dimension space of the i-th example, and yi is the label of xi either positive or negative.
P07-2026@@In recent years, statistical machine translation(SMT) systems have achieved substantial progressregarding their perfomance in international transla-tion tasks (TC-STAR, NIST, GALE).Statistical approaches to machine translation wereproposed at the beginning of the nineties and foundwidespread use in the last years. ConclusionsWe have shown that Minimum Bayes Risk decod-ing on N -best lists improves the BLEU score con-siderably. A statistical approach to machine translation.
P07-2027@@Besides identifying the task it-self, it is crucial to determine the owner, or personresponsible. my question I guess are they really your peers8 Initial Multi-Party ExperimentsThe experiments above used two-person dialog data:we expect that multi-party data is more complex. see example (4)).We allowed a separate class for genuinely ambigu-ous cases.
P07-2032@@Web 2.0 leads to the proliferation of user generatedcontent, such as blogs, wikis and forums. Automated essay scoringwith e-rater v. . Building a Large Annotated Corpusof English: The Penn Treebank.
P07-2034@@ Weblog (blog) is one of the most widely used cy-bermedia in our internet lives that captures and shares moments of our day-to-day experiences, anytime and anywhere. Method 2 achieves better precisions when using  I only memorized vowels today~ haha  Whats the hacker doing... darn it  Some Example Words in a Lexicon. 5 Emotion Classification Suppose a sentence S to be classified consists of n emotion words.
P07-2038@@Some noun pairs are more likely to be conjoinedthan others. A Graph Modelfor Unsupervised Lexical Acquisition. How-ever, before running the experiments we removedall pairs where the words in the pair were identical.This is because identical words occur more often incoordinate head words than in other lexical depen-dencies (there were 43 pairs with identical words inthe coordination set, compared to 3 such pairs in the150SimTest ncoord xcoord SDcoord nnonCoord xnonCoord SDnonCoord 95% CI p-valuecoordDistrib 503 0.
P07-2040@@Statistical and machine learning NLP techniques arenow so advanced that named entity (NE) taggers arein practical use. For exam-ple, the tuple [George Bush (person), the U.S. (loca-tion), president (Relation Label)] would be extractedfrom the sentence George Bush is the president ofthe U.S.. have a semantic relation umareta15(was born).
P07-2044@@Temporal information encoded in textual descrip-tions of events has been of interest since the earlydays of natural language processing. %, a gain of3% with our automatic features. We also extendprevious work and create bigram POS features of theevent and the token before it, as well as the bigramPOS of the first event and the second event.Event-Event Syntactic Properties: A phrase P issaid to dominate another phrase Q if Q is a daugh-ter node of P in the syntactic parse tree.
P07-2045@@How-ever, until now, most work in this field has been carried out on proprietary and in-house research systems. "A Systematic Comparison of Various Statistical Alignment Models." In order for the toolkit to be adopted by the community, and to make it easy for others to con-tribute to the project, we kept to the following principles when developing the decoder:  Portability It was developed in C++ for efficiency and fol-lowed modular, object-oriented design.
P07-2049@@Recently the task of multi-document summarizationin response to a complex user query has receivedconsiderable attention. -t 0 -d194compared to the GENERIC scenario. Rouge: a package for automatic evaluation ofsummaries.
P07-2052@@Accordingly, we cansay that the state-of-the-art lexicalized parsers aremainly based on unlexical (grammatical) informa-tion due to the sparse data problem. The context features are thetwo preceding nodes of node t (and t itself), the twosucceeding nodes of node n (and n itself), and their1We use nil to denote an empty list and a|A to denote alist with head a and tail A. They manu-ally split category tags from a linguistic view.
P07-2053@@Its greatest advantage is its speed, impor-tant both for a fast tuning cycle and when dealingwith large corpora, especially when the POS tag-ger is but one component in a larger information re-trieval, information extraction, or question answer-1http://mokk.bme.hu/resources/hunpos/ing system. A hybrid approach to part-of-speechtagging. From thetraining set TnT builds a trie from the endings ofwords appearing less than n times in the corpus, andmemorizes the tag distribution for each suffix.
P07-2054@@Research on SMT has been strongly boosted in the lastfew years, partially thanks to the relatively easy develop-ment of systems with enough competence as to achieverather competitive results. Crego, J.B. Marino, and A. de Gispert. Reordering experi-ments for n-gram-based smt.
P07-2055@@Part-of-speech (POS) tagging is an important taskin natural language processing, and is often neces-sary for other processing such as syntactic parsing.English POS tagging can be handled as a sequentiallabeling problem, and has been extensively studied.However, in Chinese and Japanese, words are notseparated by spaces, and word boundaries must beidentified before or during POS tagging. A large problem in word seg-mentation and POS tagging is the existence of un-known words. There are four POC tags, B, I , Eand S, each of which respectively indicates the be-ginning of a word, the middle of a word, the endof a word, and a single character word.
P07-2056@@ Part-of-Speech (POS) taggers for natural lan-guage texts have been developed using linguistic rules, stochastic models as well as a combination of both (hybrid taggers). E. Dermatas and K. George. A maximum entropy part-of-speech tagger.
P07-3002@@The discovery of grammars from unannotated ma-terial is an important problem which has receivedmuch recent research. The categoriesare, in the simplest case, formed by the atomic cate-gories s (for sentence), np (noun phrase), n (com-mon noun), etc., closed under the slash operators/, \. RANDOMASSIGNMENT()Loop:for i A can-didate solution genotype is an assignment of CCGcategories to the lexical items (types rather than to-kens) contained in the textual material.
P07-3006@@Accurately identifying events in unstructured text isan important goal for many applications that requirenatural language understanding. S is the set of final states, L is the set of edgelabels and T  (SL)S is the set of transitions.We note that it is the responsibility of the learningalgorithm to discover the correct number of states.We treat the task of discovering an event model asthat of learning a regular grammar from a set of pos-itive examples. Each sentencein the document set was rated by the two annotatorsand the assigned values were mapped into one of thefour label categories (N, C, X, and B).
P07-3008@@Grammar induction is a task within the field of nat-ural language processing that attempts to construct agrammar of a given language solely on the basis ofpositive examples of this language. In Proceedings of Computational Linguis-tics in the Netherlands (CLIN), pages 4559, Tilburg,the Netherlands.E. s Precision Recall F-score Hits/Pred.
P08-1001@@ Named Entity Recognition (NER) has long been a major task of natural language processing. Bunescu, R and M. Paca. An algorithm that learns whats in a name.
P08-1002@@The goal of coreference resolution is to determinewhich noun phrases in a document refer to the samereal-world entity. We also added a few simplerules to stem the irregular verbs be, have, do, andsaid, and convert the common contractions nt, s,m, re, ve, d, and ll to their most likely stem.We do the same processing to our n-gram corpus.We then find all n-grams matching our patterns, al-lowing any token to match the wildcard in place ofit. Anaphora in Natural LanguageUnderstanding: A Survey.
P08-1004@@Relation Extraction (RE) is the task of recognizingthe assertion of a particular relationship between twoor more entities in text. Ting and I. H. Witten. Statistical parsers are usually lexicalized(ie they make parsing decisions based on n-gramstatistics computed for specific lexemes).
P08-1006@@However, efforts to perform extensivecomparisons of syntactic parsers based on differentframeworks have been limited. Parsing Englishwith a Link Grammar. Clark and J. R. Curran.
P08-1007@@Recognizing that differ-ent concepts can be expressed in a variety of ways,we allow matching across synonyms and also com-pute a score between two matching items (such asbetween two n-grams or between two dependencyrelations), which indicates their degree of similaritywith each other.Having weighted matches between items meansthat there could be many possible ways to match, orlink items from a system translation sentence to areference translation sentence. During thisphase, we will construct three bipartite graphs, one58each for the remaining set of unigrams, bigrams, andtrigrams.Using bigrams to illustrate, we construct aweighted complete bipartite graph, where each edgee connecting a pair of system-reference bigrams hasa weight w(e), indicating the degree of similaritybetween the bigrams connected. ,MAXSIMn+d could achieve a correlation of 0.
P08-1009@@Equivalently, one can saythat phrases in the source, defined by subtrees inits parse, remain contiguous after translation. We can focus our interruption check onfh, the last phrase in fh1 , as any open subtree T (r)must contain at least one e  If this were not theAlgorithm 1 Interruption check. Next,we introduce our source dependency tree T .
P08-1012@@As these word-level align-ment models restrict the word alignment complex-ity by requiring each target word to align to zeroor one source words, results are improved by align-ing both source-to-target as well as target-to-source,then heuristically combining these alignments. C is our unique pre-terminal forgenerating terminal multi-word pairs:C  e/f .We parameterize our probabilistic model in themanner of a PCFG: we associate a multinomial dis-tribution with each nonterminal, where each out-come in this distribution corresponds to an expan-sion of that nonterminal. Ph.D. thesis, GatsbyComputational Neuroscience Unit, University CollegeLondon.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
P08-1016@@Words are essential to most models of language andspeech understanding. The wordsfragment ratio f is 2wp+s .Values of f are typically over 0.  for freely occur-ring words, under 0.  for fragments and strongly-attached affixes, and intermediate for clitics, someaffixes, and words with restricted usage. Ad-ditional phonetic features may not be easy to detect9The author does not read Arabic and, thus, is not in a posi-tion to explain why the annotaters did this.reliably, eg marking lexical stress in the presenceof contrastive stress and utterance-final lengthening.The actual phonology of fast speech may not bequite what we expect, eg performance on the pho-netic version of Buckeye was slightly improved bymerging nasal flap with n, and dental flap with d andglottal stop.
P08-1017@@Sincevarious terms can be used to describe a same con-cept, it is unlikely for a user to use a query term thatis exactly the same term as used in relevant docu-ments. A language modelingapproach to information retrieval. D(t2)|,where D(t) is the concatenation of the definitionsfor all the synsets containing term t and |D| is thenumber of words of the set D.Within a taxonomy, synsets are organized by theirlexical relations.
P08-1020@@We used the Big Five modelto develop PERSONAGE for several reasons. S. Paiva and R. Evans. H. Witten and E. Frank.
P08-1021@@In order to describe the nuances of an action, a verbmay be associated with various concepts such astense, aspect, voice, mood, person and number. A description of our data follows. to the parse trees.For disambiguation with n-grams (see 3.
P08-1022@@the assignment of informative syntactic cat-egories to linguistic objects such as words or lex-ical predicates  from the combinatory processesthat make use of such categories  such as pars-ing and surface realization. % of the test logical formshave a single root. ]makes use of n-gram language models over wordsrepresented as vectors of factors, including surfaceform, part of speech, supertag and semantic class.The search proceeds in one of two modes, anytimeor two-stage (packing/unpacking).
P08-1023@@Syntax-based machine translation has witnessedpromising improvements in recent years. e3|s|(5)where T is the 1-best parse, e1|d| is the penalty termon the number of rules in a derivation, Plm(s) is thelanguage model and e3|s| is the length penalty term196on target translation. Sharonwhich perform phrasal translations for the two re-maining subtrees, respectively, and get the Chinesetranslation in (e).
P08-1025@@Statistical machine translation systems basedon synchronous grammars have recently showngreat promise, but one stumbling block to theirwidespread adoption is that the decoding, or search,problem during translation is more computationallydemanding than in phrase-based systems. Formally, the rules in our gram-mar include preterminal unary rules:X  e/ffor pairing up words or phrases in the two languagesand binary production rules with straight or invertedorders that are responsible for building up upper-level synchronous structures. A hierarchical phrase-based modelfor statistical machine translation.
P08-1026@@The key idea behindunderspecification is that the parser avoids comput-ing all scope readings. A better n-best list: Prac-tical determinization of weighted finite tree automata.In Proceedings of HLT-NAACL.R. is a relabelling function from one terminal alpha-bet to another, we can write f (G) for the grammar(S,N,,R configurations that theseworst-case examples have.
P08-1027@@Automatic extraction and classification of seman-tic relationships is a major field of activity, of bothpractical and theoretical interest. Using the k-meansalgorithm, we obtained two nearly equal unlabeled233Method P R F AccUnsupervised clustering (4..) 64. Denote by ki the numberof training pairs in class i in training set T .
P08-1028@@In fact, the common-est method for combining the vectors is to averagethem. Mathematical Structures of Language.Wiley, New York.E. English as a formal language.
P08-1029@@Problem definitionConsider the task of named entity recognition(NER). Object correspondence as a machine learningproblem. An intuitivesolution might be to simply retrain the classifier, denovo, on the e-mail data.
P08-1030@@ Identifying events of a particular type within indi-vidual documents  Recognizing the different forms in which an event may be ex-pressed, distinguishing events of different types, and finding the arguments of an event are all chal-lenging tasks. These rules are ap-plied in the order (1) to (9) based on the principle of improving local information before global                                                           3 We tested different N  Although the rules may seem com-plex, they basically serve two functions:     to remove triggers and arguments with low (local or cluster-wide) confidence;  to adjust trigger and argument identification and classification to achieve (document-wide or cluster-wide) consistency. Within-Sentence Event Extraction For each event mention in a test document t , the baseline Maximum Entropy based classifiers pro-duce three types of confidence values:   LConf(trigger,etype): The probability of a string trigger indicating an event mention with type etype; if the event mention is produced by pattern matching then assign confidence 1.
P08-1031@@A central problem in language understanding istransforming raw text into structured representa-tions. In Advances in NIPS.David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Document d has Nd words; zd,n isthe topic for word wd,n.
P08-1033@@The highly accurate identification of several regu-larly occurring language phenomena like the specu-lative use of language, negation and past tense (tem-poral resolution) is a prerequisite for the efficientprocessing of biomedical texts. Thus given labeled sets Sspecand Snspec the task is to train a model that, for eachsentence s, is capable of deciding whether a previ-ously unseen s is speculative or not.The contributions of this paper are the following: We demonstrate that with a very limitedamount of expert supervision in finalising thefeature representation, it is possible to build ac-curate hedge classifiers from (semi-) automati-cally collected training data. A Guide to Health InsuranceBilling.
P08-1034@@One of the emerging directions in NLP is the de-velopment of machine learning methods that per-form well not only on the domain on which theywere trained, but also on other domains, for whichtraining data is not available or is not sufficient toensure adequate machine learning. The 58runs were then collapsed into a single set of 7,813unique words. 2 for LBS and CBS clas-sifiers respectively to 0.
P08-1036@@User generated content represents a unique source ofinformation in which user interface tools have facil-itated the creation of an abundance of labeled con-tent, eg, topics in blogs, numerical product and ser-vice ratings in user reviews, and helpfulness rank-ings in online discussion forums. A word can besampled using any window covering its sentence s,where the window is chosen according to a categor-ical distribution d,s. or you can get a bus for 1.
P08-1039@@However, CCG is a binary branch-ing grammar, and as such, cannot leave NP structureunderspecified. Melbourne, Australia.Stephen Clark and James R. Curran. Search enginestatistics beyond the n-gram: Application to nouncompound bracketing.
P08-1043@@Current state-of-the-art broad-coverage parsers as-sume a direct correspondence between the lexicalitems ingrained in the proposed syntactic analyses(the yields of syntactic parse-trees) and the space-delimited tokens (henceforth, tokens) that consti-tute the unanalyzed surface forms (utterances). We use v1.0mainly because previous studies on joint inferencereported results w.r.t. Lexical rules are estimatedin a similar manner.
P08-1044@@In order to improve the performance of automaticspeech recognition (ASR) systems on conversationalspeech, it is important to understand the factorsthat cause problems in recognizing words. Since all of the features arebinary, a coefficient of  indicates that the corre-sponding feature, when present, adds a weight of to the log odds (ie, multiplies the odds of an errorby a factor of e). The top 20 items on thislist include yup, yep, yes, buy, then, than, and r., allof which are acoustically similar to each other or toother high-frequency words, as well as the words af-ter, since, now, and though, which occur in manysyntactic contexts, making them difficult to predictbased on the language model.
P08-1045@@State-of-the-art statistical machine translation(SMT) is bad at translating names that are not verycommon, particularly across languages with differ-ent character sets and sound systems. The match fails if norule applies or the accumulated cost exceeds a presetlimit.Names may have n words on the English and m onthe Arabic side. % and a recall of 95.
P08-1046@@How humans acquire language is arguably the cen-tral issue in the scientific study of language. Adaptor Grammars: A frameworkfor specifying compositional nonparametric Bayesianmodels. Singer, and S. Roweis, editors, Ad-vances in Neural Information Processing Systems 20.MIT Press, Cambridge, MA.
P08-1047@@Gazetteers, or entity dictionaries, are important forperforming named entity recognition (NER) accu-rately. N is an MN thatdepends on verb v with relation r. A relation, r,is represented by Japanese postpositions attached ton. as the precisionof the output matches (ie, # e-matches/# matches),and rec.
P08-1049@@The modern Chinese language is a highly abbrevi-ated one due to the mixed use of ancient single-character words with modern multi-character wordsand compound words. We get a precisionof 97. P (full|abbr) (2)where e is an English translation, and j is the j-thmodel feature indexed as in the baseline system.. As will beshown in the experimental results, this is critical toobtain performance gain over the baseline system.
P08-1050@@Much research in lexical acquisition of verbs hasconcentrated on the relation between verbs and theirargument frames. [ACCOMPANIMENT]c. I sang with confidence. (1) a. I ate with a fork.
P08-1057@@In recent years, the computational linguistics com-munity has developed an impressive number of se-mantic verb classifications, ie, classifications thatgeneralise over verbs according to their semanticproperties. (3) For each verb v in cluster c, we add a ruleVc  (4) For each subcategorisation frame f of cluster cwith length n, we add a rule Ac  (6) For each terminal node r in the selectional pref-erence model, we add a rule Rc,f,i,r  With this rule, we jump (8) For each word node a in the a priori model, weadd a rule Ra  a whose probability is 1.Based on the above definitions, a partial parse MDL PrincipleA model with a large number of fine-grained con-cepts as selectional preferences assigns a higherlikelihood to the data than a model with a small num-ber of general concepts, because in general a largernumber of parameters is better in describing train-ing data. In Proceed-ings of the ACL Workshop on Unsupervised Learningin Natural Language Processing, pages 18, CollegePark, MD.Leonard E. Baum.
P08-1058@@Language models (LMs) are a core component instatistical machine translation, speech recognition,optical character recognition and many other areas.They distinguish plausible word sequences from aset of candidates. We can then encode each subset in a separatearray Ai, i  [t] in turn in memory. (3)Here f is the source sentence that we wish to trans-late, e is a translation in the target language, i, i  (Some features may not depend on f .
P08-1059@@One of the outstanding problems for further improv-ing machine translation (MT) systems is the diffi-culty of dividing the MT problem into sub-problemsand tackling each sub-problem in isolation to im-prove the overall quality of MT. Arabic does not use capitalization..  TaskMore formally, our task is as follows: given a sourcesentence e, a sequence of stems in the target lan-guage S1, . It,where It is the set of inflections corresponding to St,and xt refers to the context at position t. The con-text available to the task includes extensive morpho-logical and syntactic information obtained from thealigned source and target sentences.
P08-1061@@How-ever, a key drawback of supervised training algo-rithms is their dependence on labeled data, whichis usually very difficult to obtain. S. Mann and A. McCallum. In Proceedingsof the Tenth International Workshop on Artificial In-teligence and Statistics.E.
P08-1063@@Semantic Role Labeling is the problem of analyzingclause predicates in open text by identifying argu-ments and tagging them with semantic labels indi-cating the role they play with respect to the verb.Such sentencelevel semantic analysis allows to de-termine who andwhere, and, thus, characterize the participants andproperties of the events established by the predi-cates. Onthe other hand, VerbNet roles have a direct inter-pretation. If there is a R-X argument (reference), thenthere has to be a X argument before (referent)..
P08-1064@@Although good progress has been reported, the fundamental issues in applying linguistic syn-tax to SMT, such as non-isomorphic tree align-ment, structure reordering and non-syntactic phrase modeling, are still worth well studying. Given the source parse tree 1( )JT f , there are multiple derivations that could lead to the same target tree 1( )IT e , the mapping probability 1 1( ( ) | ( ))I JrP T e T f is obtained by summing over the probabilities of all deriva-tions. Given a pair of source and target parse trees 1( )JT f and 1( )IT e  in Fig.
P08-1065@@Pronouncing an unfamiliar word is a task that is of-ten accomplished by breaking the word down intosmaller components. In the first stage, all letters from theset {a, e, i, o, u} are marked as vowels, while the re-maining letters are marked as consonants. Silentletters are problematic, and some letters can behavedifferently depending on their context (in English,consonants such as m, y, and l can act as vowels incertain situations).
P08-1066@@In some languagepairs, ie Chinese-to-English translation, state-of-the-art hierarchical systems show significant advan-tage over phrasal systems in MT accuracy. In Proceedings ofthe 32nd IEEE International Conference on Acoustics,Speech, and Signal Processing (ICASSP).E. A systematic comparison ofvarious statistical alignment models.
P08-1067@@But despite itsapparent success, there remains a major drawback:this method suffers from the limited scope of the n-best list, which rules out many potentially good al-ternatives. V is the consequent node in the deduc-tive step, and tails(e)  is the list of antecedentnodes. There is also a distin-guished root node TOP in each forest, denoting thegoal item in parsing, which is simply S0,l where S isthe start symbol and l is the sentence length.
P08-1072@@Development of spoken dialog systems involves hu-man language technologies which must cooperateto answer user queries. Proceedings of the InternationalConference on User Modeling, 6374.Lochbaum, K.E. RTI04-02-06from the Regional Technology Innovation Programand by the Intelligent Robotics Development Pro-gram, one of the 21st Century Frontier R&D Pro-grams funded by the Ministry of Commerce, Indus-try and Energy (MOICE) of Korea.ReferencesBohus, B. and Rudnicky A. RavenClaw: Dia-log Management Using Hierarchical Task Decompo-sition and an Expectation Agenda.
P08-1073@@Designing a spoken dialogue system is a time-consuming and challenging task. E. Foster and J. Oberlander. For our domain, the usercan either add information (add), repeat or para-phrase information which was already provided atan earlier stage (repeat), give a simple yes-no an-swer (y/n), or change to a different topic by pro-viding a different slot value than the one asked for(change).
P08-1077@@Paraphrases are textual expressions that convey thesame meaning using different surface words. Extracting para-phrases from a parallel corpus. If n is the number of vec-tors and d is the dimensionality of the vector space,finding cosine similarity between each pair of vec-tors has time complexity O(n2d).
P08-1078@@Applied semantic inference is typically concernedwith inferring a target meaning from a given text.For example, to answer Who wrote Idomeneo ,Question Answering (QA) systems need to infer thetarget meaning Mozart wrote Idomeneo Generally, the tar-get meaning can be expressed in t in many differ-ent ways. Their clustering-based model is the onewe implemented for mv(r, t). The first list, denoted cpv:e, contains ex-amples for valid instantiations of that variable.
P08-1080@@Search results listings on the web have become stan-dardized as a list of information summarizing theretrieved documents. Pro-ceedings of the 21st annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 210.E.M. General information about a topic13.
P08-1082@@The problem of Question Answering (QA) has re-ceived considerable attention in the past few years.Nevertheless, most of the work has focused on thetask of factoid QA, where questions match short an-swers, usually in the form of named or numerical en-tities. Pml(q|A) is computed as the sum of the proba-bilities that the question term q is a translation of ananswer term a, T (q|a), weighted by the probabilitythat a is generated from A. Bymoving beyond the bag-of-word representation wehope to learn relevant transformations of structures,eg, from the squeaky We compute thePointwise Mutual Information (PMI) and Chi square(2) association measures between each question-answer word pair in the query-log corpus. A Noisy-Channel Ap-proach to Question Answering.
P08-1083@@The term unknowns denotes tokens in a text that can-not be resolved in a given lexicon. Thelinear-context model exploits information about thelexical context of the unknown words: to estimatethe probability for a tag t given a context c  We use the functionallow(t, w) to control the tags (ambiguity class) al-lowed for each word, as given by the lexicon.For a given word wi in a sentence, we examinetwo types of contexts: word context wi1, wi+1,and tag context ti1, ti+1. Our method generates a distribution ofpossible analyses for unknowns.
P08-1084@@The study of this con-nection has made possible major discoveries abouthuman communication: it has revealed the evolu-tion of languages, facilitated the reconstruction ofproto-languages, and led to understanding languageuniversals.The connection between languages should be apowerful source of information for automatic lin-guistic analysis as well. (b) Straymorphemes are then drawn from E and F (language-specific distributions) and abstract morphemes are drawn fromA. These three numbers, respectively denotedas m, n, and k, are drawn from a Poisson distribu-tion.
P08-1085@@The task of unsupervised (or semi-supervised) part-of-speech (POS) tagging is the following: given adictionary mapping words in a language to their pos-sible POS, and large quantities of unlabeled textdata, learn to predict the correct part of speech fora given word in context. Such a construction isgenerally unlikely in English. In Proceeding of ANLP-94.Sharon Goldwater and Thomas L. Griffiths.
P08-1086@@Parts of this research were conducted while the authorstudied at the Berlin Institute of Technologydo not differ in the last n Class-Based Language ModelingBy partitioning all Nv words of the vocabulary intoNc sets, with c(w) mapping a word onto its equiva-lence class and c(wji ) mapping a sequence of wordsonto the sequence of their respective equivalenceclasses, a typical class-based n-gram model approxi-mates P (wi|wi11 ) with the two following componentprobabilities:P (wi|wi11 )  p1(c(wi)|c(wi1in+1))(1)thus greatly reducing the number of parameters inthe model, since usually Nc is much smaller thanNv.In the following, we will call this type of model atwo-sided class-based model, as both the history ofeach n-gram, the sequence of words conditioned on,as well as the predicted word are replaced by theirclass.Once a partition of the words in the vocabulary isobtained, two-sided class-based models can be builtjust like word-based n-gram models using existinginfrastructure. Throughout one iteration of thealgorithm, in which for each word in the vocabularyeach possible move to another class is evaluated, this757amounts to the number of distinct bigrams in thetraining corpus B, times the number of clusters Nc.Thus the worst case complexity using the modifiedoptimization criterion is in:O(I Nc  (B +Nv))Using this optimization criterion has two effectson the complexity of the algorithm. Masters thesis, Universitat Karl-sruhe / Carnegie Mellon University.E.
P08-1087@@Traditional statistical machine translation methodsare based on mapping on the lexical level, whichtakes place in a local window of a few words. A simple rule-based part of speech tag-ger. The NIST metric clearly showsa significant improvement, because it mostly mea-sures difficult n-gram matches (eg due to the long-distance rules we have been dealing with).
P08-1089@@Paraphrases are different expressions that conveythe same meaning. An English-Chinese (E-C) bilingual parallel corpus is employed for train-ing. Note that, a subtree may contain several partial subtrees.
P08-1090@@This paper induces a new representation of struc-tured knowledge called narrative event chains (ornarrative chains). For-mally, the score is calculated as the following:E:x,y Therelation ij indicates that i is temporally before j. . This verb-only narrativemodel shows a 36.
P08-1091@@Shallow approaches to semantic processing are mak-ing large strides in the direction of efficiently andeffectively deriving tacit semantic information fromtext. Hence, weobserve sentences such as AKQ. To train the multi-class classifier, T+can be reorganized as positive T+argi and negativeTargi examples for each argument i.
P08-1092@@Producing biographies by hand is a labor-intensivetask, generally done only for famous individuals.The process is particularly difficult when persons ofinterest are not well known and when informationmust be gathered from a wide variety of sources. Answering definitional questions:A hybrid approach. Manual EvaluationROUGE evaluation is based on n-gram overlap be-tween the automatically produced summary and thehuman reference summaries.
P08-1093@@The volume of scientific literature has been growingrapidly. The taskof impact-based summarization is thus to 1) con-struct a representation of the impact of d, I , basedon d and C; 2) design a scoring function Score(. A hierarchical Dirich-let language model.
P08-1094@@In certain situations even the best automatic sum-marizers or professional writers can find it hard towrite a good summary of a set of articles. SIG is a classifier based on the six featuresidentified as significant in distinguishing easy fromdifficult inputs based on a t-test comparison (Ta-ble 4). A high overlap of topic words across twodocuments is indicative of shared topicality.
P08-1096@@Coreference resolution is the process of linking mul-tiple mentions that refer to the same entity. A predicatehas mention(e, m) is used for each mention ine 3. As a result, we only see0.
P08-1098@@Supervised machine learning methods have success-fully been applied to many NLP tasks in the last fewdecades. However, the goal of multi-task AL is to minimize the annotation effort over allannotation tasks and not just the effort for a singleannotation task.For a multi-task AL protocol to be valuable in aspecific multiple annotation scenario, the TQ for allconsidered learners should be1Of course, all selected examples would be annotated w.r.t.all annotation tasks. In the alter-nating selection protocol, the numbers of consecu-tive iterations si each single task protocol can betuned according to the  rXj (e) where the parameters 1 .
P08-1099@@A significant barrier to applying machine learningto new real world domains is the cost of obtainingthe necessary training data. A framework for learn-ing predictive structures from multiple tasks and unla-beled data. Incorporating prior knowledge into boosting.In ICML.N.
P08-1100@@The unsupervised induction of linguistic structurefrom raw text is an important problem both for un-derstanding language acquisition and for buildinglanguage processing systems such as parsers fromlimited resources. The size ofa configuration governs how much the meta-modelgeneralizes from individual examples.Let y(i,t) denote the model prediction on the i-thtraining example after t iterations of EM. 81ing expected counts computed in the E-step).
P08-1101@@Since Chinese sentences do not contain explicitlymarked word boundaries, word segmentation is anecessary step before POS tagging can be performed.Typically, a Chinese POS tagger takes segmented in-puts, which are produced by a separate word seg-mentor. For asentence with n characters, the number of possibleoutput sequences is O(2n1  Tn), where T is thesize of the tag set. those associated onlywith a limited number of Chinese words.
P08-1102@@Word segmentation and part-of-speech (POS) tag-ging are important tasks in computer processing ofChinese and other Asian languages. Experimentsshow that our cascaded model can utilize differentknowledge sources effectively and obtain accuracyimprovements on both segmentation and Joint S&T. b: the begin of the word m: the middle of the word e: the end of the word s: a single-character wordWe can extract segmentation result by splittingthe labelled result into subsequences of pattern s orbme which denote single-character word and multi-character word respectively.
P08-1105@@The growing amount of user generated content avail-able online creates new challenges for the informa-tion retrieval (IR) community, in terms of search andanalysis tasks for this type of content. Both authors were supported by the E.U. Articles arein English and come from a variety of sources.Statistical significance is tested using a two-tailedpaired t-test.
P08-1108@@One advantageof this representation is that it extends naturally todiscontinuous constructions, which arise due to longdistance dependencies or in languages where syntac-tic structure is encoded in morphology rather than inword order. , wn in O(n) time.To learn a scoring function on transitions, thesesystems rely on discriminative learning methods,such as memory-based learning or support vectormachines, using a strictly local learning procedurewhere only single transitions are scored (not com-plete transition sequences). For thegraph-based model, X is the set of possible depen-dency arcs (i, j, l); for the transition-based model,X is the set of possible configuration-transition pairs(c, t).
P08-1109@@Over the past decade, feature-based discriminativemodels have become the tool of choice for manynatural language processing tasks. N j can formcycles, leading to infinite unary chains with infinitemass. if right child is a PP then r,ws simplified rule: VP features:t,ds(w) base labels of states if some child is a verb tag, then rule,t,ds(w1) dist sim bigrams: with that child replaced by the wordt,ds(w+1) bigrams belowb(t),ds(w) rule, and base parent state Unaries which span one word:b(t),ds(w1) The length 40 models had a batch sizeof 30 and we allowed ten passes through the data.We used development data to decide when the mod-els had converged.
P08-1111@@Hand-built grammars are often dismissed as too ex-pensive to build on the one hand, and too brittleon the other. In E.M. Bender,D. is a modifier of ngabulu milk.
P08-1112@@The typical pipeline for a machine translation (MT)system starts with a parallel sentence-aligned cor-pus and proceeds to align the words in every sen-tence pair. A systematic comparisonof various statistical alignment models. For example, constraints might includeone word should not translate to many words orthat translation is approximately symmetric.The modification is to add a KL-projection stepafter the E-step of the EM algorithm.
P08-1114@@in a formal sense, go-ing beyond the finite-state underpinnings of phrase-based models, from approaches that are syntacticin a linguistic sense, ie taking advantage of apriori language knowledge in the form of annota-tions derived from human linguistic analysis or tree-banking. We map SBAR and S labels in Arabic parses to CP and IP,respectively, consistent with the Chinese parses. A con-ditional random field word segmenter.
P08-1115@@Today, virtually all statistical trans-lation systems seek the best hypothesis e for a giveninput f in the source language, according toe Even for text,there are often multiple ways to derive a sequenceof words from the input string. Thus,we make use of the following general decision rule:e is a directed acyclicgraph that formally is a weighted finite state automa-ton (FSA). To do so, the grammar is intersected withan n-gram LM.
P08-1116@@Paraphrases are alternative ways of conveying thesame meaning. I(ei, r, e) is themutual information between ei, r and e.For each word, we keep 20 most similar words asparaphrases. In this case, we assign hTM i(T, S) a minimum value.The decoder used here is a re-implementation of Pharaoh.
P08-1117@@Recognizing relations expressed in text sentences isa major topic in NLP, fundamental in applicationssuch as Textual Entailment (or Inference), QuestionAnswering and Text Mining. The instantiatedri replaces all the terminals in s that are covered bythe l-match. Authorities have arrested John Smith, a retiredpolice officer.
P08-1118@@In this paper, we seek to understand the ways con-tradictions occur across texts and describe a systemfor automatically detecting such constructions. (2) A government purchases food. Consider:T: Nike Inc. said that its profit grew 32 percent, as thecompany posted broad gains in sales and orders.H: Nike said orders for footwear totaled $4.
P08-1119@@Knowing the semantic classes of words (eg, troutis a kind of FISH) can be extremely valuable formany natural language processing tasks. of the Sixteenth National Conference on Arti-ficial Intelligence.E. A graph model forunsupervised lexical acquisition.
P08-2002@@When the transi-tion from previous word to current word is low-probability, from the parsers perspective, the sur-prisal is high and the psycholinguistic claim is thatbehavioral measures should register increased cog-nitive difficulty. Dependency theory: A formalism andsome observations. Technical report, IMS, Universitat Stuttgart,Germany.R.
P08-2003@@After having made substantial headway in factoidand list questions, researchers have turned their at-tention to more complex information needs that can-not be answered by simply extracting named en-tities like persons, organizations, locations, dates,etc. vm(T )), where the i-th elementvi(T ) is the number of occurrences of the i-th treefragment in tree T . It measures summaryquality by counting overlapping units such as then-gram (ROUGE-N), word sequences (ROUGE-Land ROUGE-W) and word pairs (ROUGE-S andROUGE-SU) between the candidate summary andthe reference summary.
P08-2004@@While most linguistic cues1We use the term non-objectivity to refer to the propertyof creating a bias from a speakers point of view that is not sup-ported by sufficient objective evidence. A Comprehensive Grammarof the English Language. For example, sentence (1)and sentence (2) are very similar in linguistic struc-tures, but only sentence (2) is non-objective.
P08-2008@@ The task of Verb Sense Disambiguation (VSD) consists in automatically assigning a sense to a verb (target verb) given its context. E.g., din-ner and breakfast have 34 DDNs in common, while dinner and child  only share 14. HR0011-06-C-0022, a subcontract from the BBN-AGILE Team.
P08-2009@@While part of speech (POS) tagging for English isvery accurate, languages with richer morphology de-mand complex tagsets that pose problems for datadriven taggers. The tagger uses a onevs. However, test evaluation is notpossible due to the prohibitive cost of training thetagger on nine splits; training took almost 4 days onan AMD Opteron 2.
P08-2010@@N-best list re-ranking is an important component inmany complex natural language processing applica-tions (eg machine translation, speech recognition,parsing). Ifeach boosting iteration t, MERT is called as as sub-procedure to find the best feature weights t on di. .M over the M N-best lists.
P08-2011@@Models of discourse coherence describe the relation-ships between nearby sentences, in which previoussentences help make their successors easier to un-derstand. Notes on CG and LM-BFGSoptimization of logistic regressio n. Paper availableat http://pub.hal3.name#daume04cg-bfgs, implemen-tation available at http://hal3.name/megam/, August.Peter Foltz, Walter Kintsch, and Thomas Landauer. %, but on a different corpus.
P08-2014@@This paper presents a new approach for constructinga discriminative transliteration model.Our approach is fully automated and requires littleknowledge of the source and target languages.Named entity (NE) transliteration is the process oftranscribing a NE from a source language to a targetlanguage based on phonetic similarity between theentities. Learning to resolve natural language am-biguities: A unified approach. We collected a set R, consisting54of 50,000 NE by crawling through Wikipedias arti-cles and using an English NER system available athttp://L2R.cs.uiuc.edu/ cogcomp.
P08-2015@@We present four techniques for online handling ofOut-of-Vocabulary (OOV) words in phrase-basedStatistical Machine Translation (SMT). In A. van den Bosch andA. English preprocessing simply in-cluded down-casing, separating punctuation fromwords and splitting off  s.OOV Handling Techniques and their Combina-tion We compare our baseline system (BASELINE)to each of our basic techniques and their full combi-nation (ALL).
P08-2016@@ An abbreviation is a letter or sequence of letters, which is a shortened form of a word or a sequence of words, which is called the sense of the abbreviation. H. Witten and E. Frank. Abbreviation disambiguation means to choose the correct sense for a specific context.
P08-2017@@Obtaining human annotations for linguistic data islabor intensive and typically the costliest part of theacquisition of an annotated corpus. Replacing the number of wordsneeding correction, c, with the product of l (the sen-tence length) and the accuracy p of the model, equa-tion 1 can be re-written as the estimate:h All algorithms shown have significant costsavings over the random baseline for accuracy lev-els above 92%. Enriching theknowledge sources used in a maximum entropy part-of-speech tagger.
P08-2018@@This paper presents a simple but high-performancemethod for text categorization. Extracting multi-word expressions with a semantic tagger. Though the Wikipedia-strict mod-ule showed the best performance, it was used not5Wiki-s, Wiki-l, Snippt and Cmpnt stand for Wikipedia-strict, Wikipedia-loose, Snippets and Components, respectively.
P08-2021@@Large improvements in machine translation (MT)may result from combining different approachesto MT with mutually complementary strengths.System-level combination of translation outputs isa promising path towards such improvements. A study of translation edit rate withtargeted human annotation. Specifically: (a) we showedthat invWER (computed using the ITG alignments)gives a better approximation to TER between ma-chine translation outputs and human references thantercom; and (b) in an oracle system combination ex-periment, we found that confusion networks gen-erated with ITG alignments contain better oracles,both in terms of tercomTER and in terms of BLEU.Future work will include rescoring results with alanguage model, as well as exploration of heuristics(eg, allowing only short block moves) that canreduce the ITG alignment complexity to O(n4).ReferencesS.
P08-2024@@ A Chinese sentence consists of a sequence of char-acters that are not separated by spaces. A sample character is placed below each of these layouts. Arrangements of components in Chinese  Layout Part 1 Part 2 Part 3 Elements that do not belong to the original Canjie codes of the characters are shown in smaller font.
P08-2026@@Parser self-training is the technique of taking anexisting parser, parsing extra data and then cre-ating a second parser by treating the extra dataas further training data. NANC is a news corpus,quite like WSJ data. ConclusionWe self-trained the standard C/J parser on270,000 sentences of Medline abstracts.
P08-2027@@Speech repairs occur when a speaker makes a mis-take and decides to partially retrace an utterance inorder to correct it. This systemachieves high accuracy in both parsing and detectingreparanda in text, by making use of transformationsthat create incomplete categories, which model thereparanda of speech repair well.ReferencesElizabeth R. Blackmer and Janet L. Mitton. andthe PRN category (I mean, that is, etc.)
P08-2028@@One of the first challenges in sentiment analysisis the vast lexical diversity of subjective language.Gaps in lexical coverage will be a problem for anysentiment classification algorithm that does not havesome way of intelligently guessing the polarity ofunknown words. controls (i) non-neutralsentiment propagation and (ii) polarity conflictresolution to calculate a global polarity for thecurrent composite construct. However, a syl-labic approach shrinks the search space and ensuresthe phonotactic well-formedness of the aliases.Shallow Parsing [E].
P08-2029@@In question processing, useful informationis gathered from the question and a query is created.This is submitted to an IR module, which providesa ranked list of relevant documents. In B. Scholkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods Support VectorLearning.P. Moreover, using the new POSSK and PAS-PTK5All adding kernels are normalized to have a similarity scorebetween 0 and 1, ie K Natural language processing for informa-tion retrieval.
P08-2039@@Arabic has a complex morphology compared toEnglish. For example, for the wordwlAwlAdh (and for his kids ), the factored words areAwlAd and w+l+N+P:3MS. T +Rmeans thatthe words seen in the training set were recombinedusing scheme T and the remainder were recombinedusing scheme R. In the remaining experiments weuse the scheme T + R. .
P08-2045@@Working out how events are tied together temporallyand causally is a crucial component for successfulnatural language understanding. A support vector method for multi-variate performance measures. F. Fu-ture work will investigate increasing the size of thecorpus and developing more statistical approacheslike the Google N-gram scores to take advantage oflarge-scale resources to characterize word meaning.AcknowledgmentsThis research was performed in part under an ap-pointment to the U.S. Department of Homeland Se-curity (DHS) Scholarship and Fellowship Program.ReferencesS.
P08-2049@@Consider, for example, the following pair ofsentences:1(1) Posttraumatic stress disorder (PTSD) is apsychological disorder which is classified asan anxiety disorder in the DSM-IV. 93However, there is a complication. Nonehad participated in Experiment I and none had abackground in sentence fusion research.
P08-2050@@In recent years, NLG evaluation has taken on a morecomparative character. Gatt, I. van der Sluis, and K. van Deemter. Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
P08-2052@@In this paper, we propose a simple method for effec-tively generating query-based multi-document sum-maries without any complex processing steps. We used a regression SVM. In a LARS graph, featuresare plotted on the x-axis and the corresponding co-efficients are shown on y-axis.
P08-2055@@However, these confidence scores are limited to relatively simple systems, such as command-n-control dialogue systems. % of a relative error reduction over the baseline. In real appli-cations, typically a threshold or cutoff t is needed:    {sp is correct, sp is incorrect}, x is the syntactic context of the parse sub-tree sp, fj are the features, j are the corresponding weights, and Z(x) is the normalization factor.
P08-2059@@Successful applications of supervised machinelearning to natural language rely on quality labeledtraining data, but annotation can be costly, slow anddifficult. In ACL Linguistic Anno-tation Workshop.N. ,where (i,i) are the parameters on round i and(i+1,i+1)are the new parameters after update.The constraint ensures that the resulting parameterswill correctly classify xi with probability at least .For convenience we write  Vi)4Vi.Each update changes the feature weights , and in-creases confidence (variance  Active Learning with ConfidenceWe consider pool based active learning.
P08-2060@@Over the last decade, many natural language pro-cessing tasks are being cast as classification prob-lems. The re-sult of the learning process is the set SV of Sup-237port Vectors, associated weights i, and a constantb. In con-trast to other methods, changing s is guaranteed notto change the classification accuracy, as it does notchange the computed decision function.
P08-2065@@This task has created a considerable interest due to its wide applications. The terms 1( ,..., )Nt t  are possibly words, word n-grams, or even phrases extracted from the training data, with N being the number of terms. Our first approach is using a common set of terms 1(  ,...,  )allNt t  to construct a uniform fea-ture vector x  and then train a predictor using all training data: m1 1arg min ( (  ), )kk kall all knmall i if k if L f X Y The common set of terms is the union of the term sets from multiple domains.
P08-2067@@Computing pairwise similarity on large documentcollections is a task common to a variety of prob-lems such as clustering and cross-document coref-erence resolution. More recent experiments suggest that a df-cut of 99. postings(t)4: for all di, dj  wt,djthat a term contributes to each pair that contains it.
P08-3003@@Many practical applications in NLP either requireor would greatly benefit from the use of temporalinformation. Future work will im-prove upon the majority criteria used in the inferencealgorithm, on creating more accurate event represen-tations, and on determining optimal model parame-ters automatically.AcknowledgementsWe wish to thank Kathleen McKeown and BarrySchiffman for invaluable discussions and comments.ReferencesDavid M. Blei, Andrew Y. Ng and Michael I. Jordan. Our proposed inference algorithmis a starting point for further work.
P08-4003@@Coreference resolution refers to the task of identify-ing noun phrases that refer to the same extralinguis-tic entity in a text. In Scholkopf, B., Burges, C., and Smola, A., editors,Advances in Kernel Methods Support Vector Learning.Kudoh, T. and Matsumoto, Y. A ranking approach to pro-noun resolution.
P80-1030@@ The problem of constructing a natural language ~rocessing system may be viewed as a problem oz constructing a knowledge-based system. For example, fixed expressions like "by and large , the Big Apple (meaning N.Y.C. * Wills gives me a headache.
P81-1030@@at  System Development Corporat ion produced machinereadable  copies of the Merriam-Webster New Pocke~ D ic t ionary  and the Sevent~ Co l leg ia te  D ic t ionary . Norman, Donald A., and David E. Rumelhart. SPACE SHAPES act ,d i scharge ,  bed , layer .
P81-1032@@ When people use language spontaneously, they o~ten do not respect grammatical niceties. e Transformations to Canonical Form prove useful both for domain-dependent and domain.independent constructs. This form Of eXl~"ts~n-drMm I~u~ing in restricted domains adds a two-fold effect to its rcbusmes Many smmous  parses are .ever generatod (bemnmo patterns yielding petentisfly spurious matches are never in inappropriate contexts,)  Additional knowledge (such as additional ~ grammar rules, etc.)
P82-1001@@ When contemporary linguists and philosophers speak of "semantics," they usually mean m0del-theoretic semantics-mathematical devices for associating truth conditions with Sentences. To appear in Jacobsen, O. and G. K. Pullum (eds.) APPENDIX C. Transcript of Sample Dialo~ue  ~ john is happy OK. ~ i s  john happy Yes.
P82-1014@@The research had three goals: (1) demonstrating the computational tractabi l i ty of Generalized Phrase Structure Grammar (GPSG), (2) implementing a GPSG system covering a large fragment of English, and (3) establishing the feasibi l i ty of using GPSG for interactions with an inferencing knowledge base. Linguist ic generalizations can be captured in a contextfree grammar with a metagrammor, i .e. !,  the result being a function expression to be applied to the translation of the f i rst  N!!
P82-1027@@ One of the mo~t important constituent processes of natural-language neration is the production of referring expressions, which occur in almost every utterance. Appelt, Douglas E., Planning Natural Language Utter For example, in requesting a hearer to remove the pump from the platform in an air-compressor assembly task, if the hearer knows that the pump is attached to the platform and nothing else, it is not necessary to mention the platform, since it is sufficient o say "Remove the pump," for the hearer to recognize the following propomtlon: Want(S, Do(H, Remove(pumpl, latforml))).
P83-1011@@  By viewing language generation as a planning process, one can not only account for the way people use language to satisfy different goals they have in mind, but also model the broad interaction between a speakers physical and linguistic actions. Conklin, E. deffery, and D. McDonald, Salience: The Key to the Selection Prob&m in Natural Language Generation. Cohen, Philip and C. R. Perrault, Elements of a PlanBased Theory of Speech Acts, Cognitive Science, vol.
P83-1017@@  For natural anguage processing systems to be useful, they must assign the same interpretation to a given sentence that a native speaker would, since that  is precisely the behavior users will expect.. 116 In the ca~e in which the verb is "positioned," however, the longer reduction does not yield the weak form of the verb; it will therefore be invoked, reslting in the structure: \[sthe woman \[vP positioned \[Npthe dress\]\[ppon that rackl\]\] 3. demonstrates resolution of a reduce-reduce conflict.
P83-1021@@  The aim of this paper is to explore the relationship between parsing and deduction. The proof of this fact is omitted for lack of spa~.e, but can be found elsewhere (Pereira and Warren, 1.q83). P is the pos i t ive  literal or head of the clause; Ql .
P84-1008@@  Most schools of linguistics use some type of feature notation in their phonological, morphological, syntactic, and semantic descriptions. Un i f i ca t ion  and  Genera l i za t ion  Several related grammar formalisms (UG, LFG, PATRII, and GPSG) now eist that are based on a very similar conception of features and use unification as their basic operation. In graph terms, this corresponds to a lattice where two vectors point to the same node: subject ~ I predicate agreement ~~agreement  numb~erson  sg 3rd 28 In a ca~~e like this, the values of the two paths have been "unified."
P84-1018@@A. Mach ine  Trans la t ion  A classical translat ing machine stands with one foot on the  input text and one on the output. A description is an expression over an essentially arbitrary basic vocabulary. I take it that  the French sentence has an essential ly isomorphic structure.
P84-1022@@ Recently, interest has focussed on computational architectures employing massively parallel processing lip2\]. \[10\] Fahlman, S,E., Hinton, G.E., and Sejnowski, T. Massively para l le l  arch i tectures  for AI: NETL, THISTLE, and BOLTZMANNmachines. The vocabulary, V, can be represented by the space I k, where I is some interval range defined within a chosen number system, N; IoN.
P84-1023@@Sublanguages differ from each other, and tron}. In $ublan~a&e: sl~lies of language in restricted semantic domains, ed. t is a set o.x s~ pauent documents (including patient his.tmy., eTam,n.ation, .and plan of treatment).
P84-1026@@ Parsing as standardly defined is a purely syntactic matter. Languages in which paratax is  i s  used much more than hypotaxis  ( i .e . ,  L inguist ic  Categories: Auxil ia r ie~ an___~dd Related Puzzles; vo__!.
P84-1027@@  I The design, implementation, and use of grammar formalisms for natural lang,age have constituted a major branch of computational linguistics throughout its development. First we add another statement to the specification of the entailment relation: 125  if e is inconsistent, {e} entails every element of P. That is, falsity entails anything. First, a precise defini!
P84-1034@@ We have been studying machine translation between Japanese and English for several years. Thanks are also due to Dr. J. Kawasaki, Dr. T. Mitsumaki and Dr. S. Mitsumori of 5DL Hitachi Ltd. for their constant encouragement to this work, and Mr. F. Yamano and Mr. A. Hirai for their enthusiastic assistance in programming. Case Marker E Body Code + Phase Code  V CONCLUSION We have found that there are some proper approaches to the treatment of syntax and semantics from the viewpoint of machine translation.
P84-1036@@ In previous papers it has been pointed out that ill a well-structured Lexical Data Has(. it becomes possible to detect automatical;y, an(l ~e evidence through interactlve queries a number Of morpho log ica \ ]  , syntact . Amsler, R.A., The Structure of the Herriam-Webster Pocket Dictionary, Ph.D, Thesis, Department of Computer Science~.
P84-1045@@ An utterance may be syntactically and semantically well-formed yet violate the prasmatlc rules of the world model. The function ORDERED-AVE appearing In the speakers plan operates upon non-numeric elements of such domains by calculating the average of the index numbers associated wlth each element instead of attempting to  ca lcu la te  the  average of the  e lements  themse lves . We employ a user model to constrain path expansion.
P84-1050@@Specialized vs generic software tools for MT Developing the software fo r  a spec i f i c  task or class of tasks requires that one knows the structure of the tasks involved. Note that a primitive (rule-)execution scheme ( i .e . And the in terpreter  embodies a genera l  p r inc ip le  about  the appropr ia te  way to  app ly  the exper t  knowledge coded in to  the  ru les .
P84-1053@@Some of the descriptions are s t imu la t ing  or conv inc ing . E.g., the (a)s in the following sentences are Gapping examples, but the (b)s are not: (9) (a) Max spoke f luently, and Albert  haltingly. *(b) Max wrote a novel, and Alex.
P84-1056@@By r, onserial  in te rac t ion  (NS1) we refer to communicat ion  which extends  beyond the normal,  serial, flow of in format ion entai led by the tasks  under taken  by each component . ), Natural I~nguage Processing, Cambridge University Press, Cambridge Selkwk,E. This, in turn,  suggests  that  the contro l  s t ruc ture  proposed  in the last sect ion is oversimpli f ied.
P84-1062@@ Modelling system structures of word meanings and/or world knowledge is to face the problem of their mutual and complex relatedness. Other than declarative knowledge that can be represented in pre-defined semantic network structures, meaning relations of lexical relevance and semantic dispositlons which are haevlly dependent on context and domain of knowledge concerned wi l l  more adequately be defined procedurally, i .e . ): Empirical Semantics I ,  Bo
P84-1063@@ One of the promising approaches to analyzing taskoriented ialogues has involved modeling the plans of the speakers in the task domain. Pollack, M.E., "Goal inference in expert systems," Ph.D. thesis proposal, U. Sidner, C.L., "Focusing in the comprehension of definite anaphora," in M. Brady (ed). Levy, D., "Communicative goals and strategies: Between discourse and syntax," in T. Givon (ed).
P84-1065@@ Two problems in natural anguage generation are deciding what to say and how to say it. John gave Mary a book. Sam, I like him.
P84-1068@@In light of these prospects, the prototype MT system described below I should be seen as an experiment in the ecnputational trea~nent of a particular sublanguage. Gr i s t ,  R., Hirsdnman, L. and Frieclman, C. "Natural Language Interfaces Using Limited Semantic Information." P4~ENCES Chevalier et al T/K94-~IbO, Description du syst/~re.
P84-1073@@ LR parsers\[ I ,  2\] have been developed originally for programming language of compilers. 4 10 State  *ad j  "be  "det  *n * that  $ NP S VP . O S | III "p rep  6 sh 3 A 0 HI 2 "v 1 ilP 12 4v  0 S t "prep  13 "det  3 sh 10 TELESCOPE 0 MP 2 "v  7 NP t2 d#" 0 S I I "p rep  6 "dot  3 "n )0 re 4 $ 0 NP 2 "v  7 NP t2 alP" Consider the word "that" in the sentence: That information is important is doubtful.
P84-1075@@  I The goal of natural-language processing research can be stated quite simply: to endow computers with human language capability. More extensive discussions of various aspects of the PATR-II formalism and systems can be found in papers by Shieber e t  a/., \[83\], Pereira and Shieber \[84\] and Karttunen \[84\]. 362 properties of PATR-II to give a flavor for our approach to the design of the language.
P84-1082@@Th is  theory  re  p resents  a major b reakthrough in that  it sys temat  ically accounts  for the context  dependent  in terpreta t ion  of sentences ,  in par t i cu la r  with re  gard  to anaphor ic  re lat ions . Nominal nodes  Nominal nodes  are  Argument  nodes  where  the  AR GUMENT conta ins  a nominal  e lement ,  i . Relation nodes and Argument  nodes  Nodes  of type  Relat ion conta in  the re la t ion  name and po in ters  to f i r s t  and  last  ARGUMENT.
P84-1088@@ For over a decade research has been ongoing into the diverse and complex issues involved in developing smart natural language interfaces to database systems. Let us call these tuples T,,,_o~. This results in a range specification.
P84-1089@@ Any robust natural language interface must be capable of processing input utterances that deviate from its grammatical and semantic expectations. S.C. and Sondheimer, N K., "Relaxalion Techniques for Parsing Grammatically IlI-Folmed Ioput in Natural Language Understanding Systems," American Journal o1 Computational Lmguistics. Clearly, the semantic distance and the type of relationship over which this kind of substitution is allowed needs to be controlled fairly carefully  m a restricted domain everything is eventually related to everything e!se.
P84-1093@@  Dictionaries constitute a unique resource for a broad range of research involving natural language, information, knowledge, and the analysis of contemporary culture. A number of dictionaries have now been prepared by computer typesetting, so the tapes used to drive the photocomposer a e available. Formulation of a comprehensive plan to coordinate research efforts in the field.
P84-1099@@This paper is relevant for a class of MT systems with the following characteristics: (i) The translation process is broken down into three stages: source text analysis, transfer, and target text synthesis. For translation systems of the analysistransfer-synthesis family, the following diagram is a useful description: *The research described here was done in the context of the Eurotra project; we are grateful to e~l the Eurotrans for their stimulation and he lp  (i) TRF R~ . ~ (ilke) A (emotyi, (swim) / \  (zwem) (swim) (graag) L M (empty) (sNim) Note that Dutch graag is not translated at all; it only serves as a basis for the complex transfer elementKC,l~.
P84-1100@@ In this paper, we assume some basic knowledge of CAT (Computer Aided Translation) terminology (MT, M.AHT, HAMT, etc.). N~dobejkine N. "Illustration sur le d~veloppement dun atelier de traduction automatis~e". In his recent thesis (9), R. Gerber has attempted to design such a system, and to propose an initial implementation.
P84-1104@@ The phenomenon we wish to model is the understanding ofgarden path sentences (GPs) by native speakers of English. This request constructs a noun phrase with "the" modifying "boat  . "~e have been rather conservative in giving rules to determine when "ed" indicates a past participle instead of the past tense.
P84-1107@@ To express in natural language the information given in a semantic representation, at least two kinds of decisions have to be made: "conceptual decisions" and "linguistic decisions". Thereby he assassinated him in a spectacular way. ACKNOWLEDGEMENTS I would like to thank Lawrence Birnbaum for many valuable
P84-1109@@Problem Statement As a first approximation, we define a "nominal compound (NC) as s string of two or more nouns having the same distribution as a singie noun, as in example (I): (1) aircraft bomb bay door actuating cylinders We will see below that provisions have to be made in some cases for intervening adjectives. Fillmore C., Types of Lexicsl Information, in D. Steinberg and L. 3akobovits (eds. Finally, we have seen that rules which interpret 515 NCs obey e number of constraints.
P84-1114@@ In the last years we have been involved in building a natural language (Italian) interface to ward a relational database. Winograd T.: Language as a Cognitive Process; Vol.l Syntax. Charniak E.:Six Topics in Search of a Parser: An Overview of AI Language Research.
P85-1007@@ \]he tlnifyin~ tiltme of m,wh c-trent pragmatics antl discourse re~earrh is that the c.herence . for brevity, the discussion of the.,e speech acts has been omitted. The  At t i tudes  Neither BEL ,  BMB.
P85-1008@@The real problem in natural language processing is the interpretation of discourse. "An Overview o( KRL, A Knowledge Representation Language, Cocmti~ Scie~e, vol. "~el of reilisals and Ihe r t .vr la i ion  /.,~.
P85-1009@@ Formal inductive inference methods have rarely been applied to actual natural language systems. F:.gure I compares a correct DFA for the English auxiliary system with an overgeneralized DFA. Both are shown in a minimized, canonical form.
P85-1010@@When modular ctmstraints ,xre involved, rule systems that multiply out their surface effects are large and clumsy (see Barton. In place ot the state involving S ~ {A, 13, C} . A~ in a UCFG G~ yields not k + I ordinary dotted rules, but but 2 ~ possible dotted UCFC rules tracking accumulation of set elements.
P85-1011@@ lnvestigatiou of constrained grammatical sys tem from the point of view of their linguistic &leqnary and their computational tractability has been a mnjor concern of computational linguists for the last several years. 1 left ,  sLblLnKg in A \ [ i . Therefore, the time complexity of the parsing algorithm is O(S).
P85-1013@@ lhe term logic grammar will be used here, in the context of natural language processing, to mean a logic programming system (implemented normally in Polog), which associates semantic representations Cnormally in some version of preaicate logic) with natural language text. of approximate parse, which is similar to Pereiras notzon of r igh~s~ normal form, but was developed independently. 7"no followin E five sentences were used for timing tests.
P85-1022@@ Someone who knows a natural language is able to use utterances of certain types to give and receive information about the world, flow can we explain this We take as our point of departure the assumption that members of a language community share a certain mental system  a grammar  that mediates the correspondence b tween utterance types and other things in the world, such as individu~ds, relations, and states of ~ffairs, to a large degree, this system i~ the language. In the second c,~e. This proposition is the sense of the unraised subformula  (TICKLE agt: I p int :  RON) .
P85-1023@@ It is not our intention to present here a comprehensive overview of the previous work on coordination, but just to describe a couple of recent studies On this topic and to specify the main differences between them and our approach. %T~ problem here is that the left conjunct is not a complete sentence. A second "syntactic" box appears in fig.l.
P85-1026@@ Cohen, Perrault and Allen showed in thelr paper "Beyond Question Answering" \[8~ that ",.. users of cluestlon-answerzng systems expect them to do more than just answer isolated questions  they expect systems to engage tn conversation. Excerpt  8 (Te lephone)  A: I. ~.e.. p lacmg ~t later ~n the list of things to relax.
P85-1032@@  The project we refer to as HPSG is the current phase of an ongoing effort at Hewiett-Pa~,axd Laboratories to develop an English language understanding system which implements current work in theoretical linguistics. This language is a descendant of FRL  and is currently under development at HP  Labs. We report here on the structure of our lexicon, the mechanisms used in its representation, and the resulting sharp decre~e in the number of phrase structure rules needed.
P85-1033@@ Natural language processing systems need much larger lexicons than those available today. Ahlswede, Thomas E., in press. (5) the text of a prompt.
P85-1034@@WordSmith allows the user to explore a multidimensional space of information about words. At one end of the spectrum is the long e sound in beet ( / i / ) . (d) Combining pronunciations From the substring matches, the pronunciations o f /b / , / r / , /n / , /g / ,  and /e /are  obtained in a straightforward manner, but pronunciation of the vowel a is not the same in the bran and ange substrings.
P85-1035@@  A language understanding program should be able to acquire new lexical items from context, forming for novel phrases their linguistic patterns and figuring out their conceptual meanings. I took on a, bard ~ffi,,.k. Clearly, the phrases in the following examples: ~sdce on am enemy Lake os  an o ld  name ~a~e on the shape of  a essdce deserve separate ntries in the phrasal lexicon.
P85-1037@@The goal of this research is to extract semantic information from standard dictionary definitions, for use in constructing lexicons for natural language processing systems. F i l te r ing  Filtering, like sprouting, results in lists of words bearing a certain property (eg, \[+human\]). It was our goal to automate the genus extraction and disambiguation processes o that semantic hierarchies could be generated from full-sized dictionaries.
P85-1038@@ I would like to describe some recent psychological research on the nature and organization of lexical knowledge, yet to introduce it that way, as research on the nature and organization of lexical knowledge, usually leaves the impression that it is abstract and not very practical. Chi ld ren s  Use o f  D ic t ionar ies  We have been looking at what happens when teachers send children to the dictionary to "look up a word and write a sentence using it." References Amsler, R. A.
P85-1039@@Current syntax-based processors tend to work only within a narrow semantic domain, s ince  they rely heav i ly  on word co -occur rence  pat terns  which hold on ly  w i th in  texts  from a par t This paper describes a syntactic approach to natural language processing which is not bound to a narrow semantic field, and which requires little or no world knowledge. (16:4) The election...was a stunning defeat for the Peronists .... E lect ion  re fers  to the main event  in t roduced in 16 : i . In T. Glvon (Ed.
P86-1003@@  English sentences contain many types of temporal information. Charniak, E. and McDermott, D. Introduction to Artificial Intelligence. Leech, Geoffrey N. Meaning and the English Verb.
P86-1004@@  This paper describes the SDC PUNDIT 2 system for processing natural language messages. In Universals in Linguistic Theory, E. Bach and R. T. Harms (ed. Like disk drive from sentence 1, motor is a dependent entity.
P86-1005@@we have sought to respond to problems of both an applied and a scientific nature. When defining a modifier, users may view all current modifiers of. Johnson, T. Natural Language Computing: The Commercial Applications.
P86-1006@@ An important goal of computational linguistics has been to use linguistic theory to guide the construction of computationally efficient real-world natural anguage processing systems. As is well known, the Turing machine for any arbitrary r.e. There are m metarules, o finite closure can create an ID rule grammar with O(n 2~) symbols.
P86-1008@@ A formal computational model of a linguistic process makes explicit a set of assumptions about the nature of the process and the kind of information that it fundamentally involves. Each reversal of direction begins a new line. The possibility x/T is eliminated everywhere it occurs along the way.
P86-1010@@ Basing a parser on Government-Binding theory has led to a design that is quite different from traditional algorithms. The parser assumes that the input sentence can l~e broken into its constituent words and morphemes. I wish to thank my thesis advisor, R~bert Berwick, for his helpful advice and criticisms.
P86-1011@@  Recent work \[9,3\] has revealed a very close formal relationship between the grammatical formalisms of Tree Adjoining Grammars (TAGs) and Head Grammars (HGs). 5 LC2 A/P VP ~_ct N V AlP 5 However, Pollard\[5\] gives another analysis which has the following derivation structure. For each node ~/in an elementary tree, we have two nonterminal X. and I".
P86-1013@@ In recent years, there has been an upsurge of interest in techniques for parsing sentences containing coordinate conjunctions (and, or and but) \[1,2,3,4,5,8,9\]. A partial-parse has three parts: 1. VERB, PREP, ADV, NOTE, and FUNCTION slots are i filled during segmentation, while CASES and MEASURE slots are added during recombination.
P86-1014@@ The claim that at least some human languages cannot be described by a Context-free Grammar no matter how large or complex has had an interesting career. English is not a Contextfree Language. Studies in Descriptive and Historical Linguistics: Festschri f t  for Winfred P. Lehmann (Paul Hopper, ed.
P86-1015@@ / Revision is a large part of the writing process for people. Glatt, Barabara S. (I 982) "Defining Thematic Progressions and Their Relationships to Reader Comprehension", in Nystrand, Martin, ed. Merlin competes with the T-6830.
P86-1016@@A study of transcripts of expert-user dialogues reveals that users often exhibit misconceptions about the objects modeled in a domain. In Proceedings of the ~1~ A . ROMPER is specifically concerned with responding to two kinds of misconceptions: those involving an objects classification (which I call misclassifications) and those involving an objects attributes (which I call n~attr ibut lons).
P86-1017@@ The phrasal approach to language processing \[Backer75, Pawley83, Fillmore86\] emphasizes the role of the lexicon as a knowledge source. The sentence she read him the r io t  act does not make sense in the context of debating food taste. He put his foot down yesterday when I asked him for a quarter.
P86-1018@@ Large natural  language process ing systems need lexicons much larger than those avai lab le  today with expl ic i t  in format ion about lex lca l -semant ic  re%ationships,  about usage, about forms, about morphology,  about case frames and se lect ion  rest r ic t ions  and other kinds of co l locat ional  information. "Computat iona l  Lexicology:  A Research Program." e. -ant one that per forms (a spec i f ied  act ion).
P86-1021@@  The role of prosody in discourse has been generally acknowledged but little understood. Sacks, H., Sehlegoff, E., and Jefferson, G., A simple systematies for the organization of turn-taking for conversation, Lanuage 50 pp. Clark, H. H., and Marshall, C. R., Definite reference and mutual knowledge, in Elements of discourse understanding, ed.
P86-1022@@ We describe an experimental text-to-speech system that uses a deterministic parser and prosody rules to generate phrase-level pitch and duration information for English input. LEFT-HAND POWER UNIT ON EACH SHELF IN FORTY-E IGHT CHANNEL MODULE POWERS ONLY ECHO CANCELLERS IN THAT SHELF. For sentences uch as (i!
P86-1029@@ It is widely acknowledged that Donnellans distinction \[7\] between referential and attributive uses of definite descriptions must be taken into account in any theory of reference. For example, for John, the police investigator, the set {Smithn murderer, the man who~e fingerprints these are} is an individuating set of Smiths murderer. In particular, I contend the following: 1.
P86-1030@@  Certain problems of ambiguity and inference failure in opaque contexts are well known, opaque contexts being those in which an expression can denote its intension or underlying concept rather than any particular extension or instance. For  example,  some opaque contexts forbid inferences from postmodi f ier  deletion, while others permi t  it. Ascribes a particular descriptor to an individual; "agent a would use the descriptor dl to describe the entity e".
P86-1032@@ The importance of plan inference (PI) in models of conversation has been widely noted in the computational-linguistics literature. A Theory of Human Action. There is a discrepancy here between the belief R ascribes to Q in satisfaction of Clause (ii) of (P l )  namely, that expressed in (15)--and Rs own beliefs about the domain.
P86-1033@@ In order to interpret a sequence of utterances fully, one must know how the utterances cohere; that is, one must be able to infer implicit relationships as well as non-relationships between the utterances. E. D. Sacerdoti, A Structure for Plans and Behavior. Litman \[10\] contains a full discussion of the implementation.
P86-1036@@Indeed, a need to res t r i c t  the  form of the  meaning representat ion  can be at odds with par t i cu la r  approaches  towards  produc ing  it as for example the  "compos i t iona l "  approach ,  which does not  seek  to cont ro l  express ion  complex i ty  by giving in terpreta t ions  for whole phrasa l  pat terns ,  but  s imply combines together  the meaning of ind iv idual  words in a manner  appropr ia te  to the  syntax  of the  u t te rance . N -ary  functions are treated as a N+I  ary predicates. Other spec ia l -purpose  techn iques  are  also presented ,  such as the  c reat ion  of "prox ies"  to s tand  in for e lements  of a se t  for which  only the  card ina l i ty  is known.
P86-1037@@  The representation f meaning, and the use of such a representation to draw inferences, is an issue of central concern in natural anguage understanding systems. H and E are examples of what are often called generalized quantifiers. To illustrate this let us consider the following set of clauses: pr imre l  fa ther ,  p r imre l  mother.
P87-1001@@ It has usually been assumed that the semantics of temporal expressions i directly related to the linear dimensional concaption of time familiar from high-school physics that is, to a model based on the number-line. Changing the context In E. Keenan, (ed. N.Y.: Comell University Press.
P87-1002@@ JANUS is a natural language understanding and generation system which allows the user to interface with several knowledge bases maintained by the US NAVY. t r & leave(Vincent)(t) \] The one in (22) I have already discussed. The notation IV/nNP, thus, stands for an IV followed by n slashed NPs.
P87-1005@@ The existence of commercial natural language interfaces (NLIs), such as INTELLECT from Artificial Intelligence Corporation and Q&A from Symantec, shows that NLI technology provides utility as an interface to computer systems. All vessels have a major weapon system. Using IRACQ, mappings 1The work presented here was supported under DARPA contract #N00014-85-C-0016.
P87-1006@@Designing, developing, and debugging a rich natural language interface (its parser, grammar, recovery strategies from unparsable input, etc.) Users in the typed user-adviser dialogues seem to expect the interface to be unable to handle f ragmentary input such ~s found in Informal Spoken language and p lanned or  ed l ted  the i r  l anguage  to be as complete  and  fo rmal  as in the Human-Computer  dialogues, and more complete and formal than the language in Typed Human-Human dialogues. In T. Givon (Ed.
P87-1010@@Important design features of this model include a set of morpheme lexicons and a set of parallel finite state transducers which implement phonological rules mapping surface strings to lexical representations. \[4\] The / i /o f  the verb stem is changed due to the following/u/ of the past tense morpheme. This nonisomorphism is illustrated when a morpheme which is phonologically an affix is syntactically a separate word n this is the case with cliticization.
P87-1011@@The property of Combinatory Categorial Grammars that has occasioned concerns about processing is spurious ambiguity: CCGs  that directly use functional composition and type raising admit alternative derivations that nevertheless result in fully equivalent parses from a semantic point of view. To appear in R. Oehrle, E. Bach, and D. Wheeler (eds. top NP S/FVP FVP/VP VP/S ~ S/S S/FVP FVP/VP VP/NP cakes I can be l ieve  that  she w i l l  ea t  We took note above of the fact that there were at least 132 distinct derivations for the sentence now appearing in (10) with CCGs using forward functional composition directly.
P87-1015@@ Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical forma~smLittle attention, however, has been paid to the structural descriptions that these formalisms can assign to strings, ie their strong generative capacity. 106 Each member of a set of trees can be adjoined into distinct nodes of trees in a single elementary tree set, i.e, derivations always involve the adjunction of a derived auxiliary tree set into an elementary tree set. , zn l  and C to derive ~./x , .
P87-1018@@ Japanese has many noun phrase patterns of the type A no B. For the sentence, (sl) Hanako  wa kyonen e o k.aita. Because the number of possible dependencies between the constituents is 2 "I (2n-3)l!
P87-1020@@For brevity, we wil l  call such expressions nominals. This generalization strengthens the argument that IN N\] constituents with left-hand stress are of parent category N O . SELF: The left-hand member is the word self (e.g~, self promotion, self analysis...).
P87-1021@@ My basic premise is that in processing a narrative text, a listener is building up a representation of the speakers view of the events and situations being described and of their relationship to one another. Hinrichs, E. "Temporal Ana~ohora in Discourses of English". In Studies in Linguistic Semantics, C. Fillmore & D.T.
P87-1022@@  In the approach to discourse structure developed in \[Sid83\] and \[GJW86\], a discourse xhibits both global and local coherence. (e) Create the proposed anchors. (Cb-O.f combinations from the cross-product of the previous two steps) 2.
P87-1023@@ Cue phrases are linguistic expressions -such as okay, but, now, anyway, by the way, in any case, that reminds me -which may, instead of making a semantic ontribution to an utterance (ie, affecting its truth conditions), be used to convey explicit information about the structure of a discourse \[4\], \[16\], \[5\]. Mann, W.C. and Thompson, S.A. Relational Propositions in Discourse. The value of X 2 itself for (n) degrees of freedom (d.f.)
P87-1024@@  In this paper we describe a computational model of concept acquisition for natural language making use of positive-only data, modelled on a theory of lexical semantics. The markedness conventions here are: (4) Lu -* S/B (s) L~ -C/E Within the causal subsystem there are three predicates, Cl, C2, and I. References  \[I\] Angluin, D. "Inductive Inference of formal Languages from positive data."
P87-1026@@  Natural language processing systems must use a broad range of lexical knowledge to account for the syntactic use and meaning of words and constructs. A Knowledge.Based Approach to Language Production. This allows the semantic interpreter to handle this example much in the same way as it would handle `Transrnit the job to the printer n, because the rood-tel relation class includes both postnominal modifiers and adverbial prepositional phrases.
P87-1030@@There is general agreement that a correct, direct response to a question may, under certain circumstances, be inadequate. The system then considers the background of the user (eg the courses taken), the background of the domain (eg what courses are offered) and a query from the user (e.g, "Can I enroll in CS572 "), and ensures that the goal of the query is compatible with the attainment of the overall domain goal. M. Brady and R. C. Berwick, Cambridge: MIT Press.
P87-1032@@Unlike feature structures and first-order terms, the atomic symbols of #/-terms are ordered in an IS-A taxonomy, a distinction that is useful in performing semantic type-class reasoning during grammatical analysis. Earley Deduction, Technical Report CS/E-86-002, Oregon Graduate Center, Beaverton, OR. Language aa a Cognitive Process, Vol.
P87-1034@@ and Motivation A linguistic theory specifies a computational process that assigns structural descriptions to utterances. Top ica l l za t ion  The rule 4a expands clauses and rule 4b introduces unbounded dependency constructions (UDCs) in English. I adopt this strongly falsifiable constraint in RGPSG.
P88-1003@@ Syntactically parallel utterances which contain plural noun phrases often require entirely different semantic treatments depending upon the particular verbs (or adjectives or prepositions) that these plural NPs are combined with. Every expression has a type. The truth-conditions which Bennetts approach would predict, however, are expressed by the formula: Vx e SQUARES: V.R CIRCLES: CONTAIN\[x,y\] which obviously does not correspond to the state of affairs pictured above.
P88-1005@@ One of the major sources of ambiguity in sentences results from the different scopes that can be assigned to the various quantified noun phrases in the sentence. (17) have equivalent logical forms quant(3,P, person(P),not(see(john,P))) not(quant(V,e, person(e),see(john,e))) Similarly, the preferred scopings of sentences Someone did not see John. Some student akes a tes~.
P88-1006@@TELI has produced queries for at least four different "backend" systems and has been adapted for over a dozen domains of data. DIAGRAM: a grammar for dialogues. Sager, N. Natural Language Information Processing: A Computer Grammar of English and Its Applications.
P88-1010@@  Compositional systems of semantic interpretation, while logically and computationally very attractive \[6,20,26\], seem unable to cope with the fact that the syntactic form of aa utterance is not the only source of information about its meaning. eOur analysis trees are closer to the functional s t ruc  78 r (fail0). Pereira and M. E. Pollack.
P88-1012@@ Abductive inference is inference to the best explanation. There is a problem, however. Or e may exist only in someones beliefs.
P88-1016@@A narrative is often told from the perspective of one or more of its characters; it cam also contain passages that are not told from the perspective of any character. Now, consider the refe~e4-c.e to David in (15a. le t  s  dont play that game anymore," he said. "
P88-1017@@  Japanese has a rich grammaticalized system of honorifics to express the speakers honorific attitudes toward discourse agents (ie persons who are related to the discourse). c. $ensei ga kyoositu e ko rare to. Here the expression "o-aw-i-suru" is analized.
P88-1018@@A crucial task in text generation is to be able to control linguktic resources o as to make what is generated conform to what is to be expre~ed. the  process SENDING (PROCESS) ob l l  gate the hearer, tn  any way (e.  g. to  car ry  fo r  so~fteo . *+ post LI ~-SOCl al -p l  1t ng favm,trs **  see~i a l  | exl  cai -p l  act n 9 post t t  v~ sot1 1 -p i  a.:t ng Vould the performance of  the fwocess ob l igate  the hearer In  any va)~ IS the process of  the k ind  that   This subordinates the semantic organizatlon to the grammatical organization and necessar;\]y obscures the unity of the politeness reasoning process.
P88-1019@@The system, which is interactive and designed for use by a translator, normally runs in an interactive mode, and includes a number of special bilingual editing functions. I can speak only Japanese. E: this is my name.
P88-1021@@ The general idea of utterance planning has been at the focus of much NL processing research for the last ten years. This means that, if a hearer initially believes -~p, the default theory will have (at least) one extension for the case in which his belief that -~p persists, and one extension in which he changes his mind and believes p. Perhaps it will even have an extension in which he suspends belief as to whether p. The source of the difficulties urrounding Perranlts theory is that the default logic h e adopts is unable to describe the attitude revision that occurs in consequence of a speech act. This theory of speech acts has been presented with respect o declarative sentences and representative speech acts.
P88-1022@@  PAULINE (Planning And Uttering Language In Natural Environments) is a language generation program that is able to realize a given input in a number of different ways, depending on how its pragmatic (interpersonal and situation-specific) This work was done while the author was at the Yale University Computer Science Departmentt New Haven This work was supported inpart by the Advanced Research Projects Agency monitored by the Office of Naval Research under contract N00014-82-K-0149. Tm\]o~.~ to a Users Lewd oi ~ e . Though many generators would be capable of producing the individual sentences, some of the pre-real~ation planning tasks have never been attempted, and others, though studied extensively (and in more detail than implemented in PAULINE) have not been integrated into a single planner under pragmatic ontrol This paper involves the questions: what are these pl~n-;-g tasks How can they all be integrated into one planner How can extralinguistic communicative goals be used to control the planning process What is the nature of the relation between text planner and text realiser With the integrated approach, planning and generation is one contlnuous process: the planner-realizer handles yntactic constraints the same way it treats treats all other constraints (such u focus or lack of requisite hearer knowledge), the only difference being that syntactic constraints tend to appear late in the planning-realisation process.
P88-1023@@was written to provide spoken directions for driving between any two points in the Boston areal7\] over the telephone. 3 Certain relationships 3It is possible that  the intermedla~e phrase my prove an even betty" u~t  for discourse segmentation. Note that commands to the speech synthesizer have been simplified for readability as follows: T   indicates the topline of the current intonational phrase; F   indicates the amount of final lowering; D   corresponds to the duration of pause between phrases; N*   indicates a pitch accent of type N; other words are not accented.
P88-1025@@ Book indexing is of wide practical interest o authors, publishers, and readers of printed materials. 5 shows a ranked phrase list in decreasing order according to a composite (tf  Using such an ordered list, a typical indexing policy consists in choosing the top n entries from the list, or choosing entries whose weight exceeds a given threshold T. When T is chosen as 0. , the 12 phrases listed at the bottom of Fig. Typical Book Index Entries Document 659 .T A Highly Associative Document Retrieval System .W This paper describes a document retrieval system implemented with a subset of the medical literature.
P88-1027@@ As natural language systems grow more sophisticated, they need larger and more d~led  lexicons. Having done this, we were able to add new relations in the com~e of developing the grammar. Consider the entries: (5) dodecahedron 0 0 n a solid having 12 plane faces (6) build 1 1 vt to form by ordering and uniting materials...
P88-1029@@  Functional Unification Grammar \[Kay79\] (FUG) and other grammatical formalisms that use feature structures and unification provide a general basis for the declarative r presentation of natural language grammars. If a clause has one of the feature alternatives described by the Indicativs~3ype system (either Declarative or/n terrooaties), then it must also have the feature Indieaties. Description (3) is merely compatible with Az (but not satisfied by Az), because Az has no value for the feature aubj : e~se.
P88-1030@@These parsers were constructed to illustrate the Parsing as Deduction approach, which views a parser as a specialized theorem-prover which uses knowledge of a language (ie its grammar) as a set of axioms from which information about the utterances of that language (eg their structural descriptions) can be deduced. CSLI Lecture Notes Series, d is t r ibuted by Chicago University Press. Inferences using the X, O, 245 Case, Move-a and LF-movement principles are immediately delayed since the relevant structures are uninstantiated.
P88-1031@@ A stack plays an important role in natural language parsing. For example, if "1" is to be shifted to F, G and H in the above example, then the stack will look like: /  r \  / \ / \ A  B  C . #, n  C  D  Z Suppose that the stack must be reduced in the following three different ways.
P89-1002@@ The problem of generating a well-formed naturallanguage xpression from an encoding of its meaning possesses certain properties which distinguish it from the converse problem of recovering a meaning encoding from a given natural-language expression. Jerry R. Hobbs and Stuart M. Shieber. In R. Oehrle, E. Bach, and D. Wheeler, editors, Categorial Grammars and Natural Language Structures.
P89-1003@@A number of linguistic theories and computational approaches to parsing natural anguage have employed the notion of associating informational dements, called feature structures, consisting of features and their values, with phrases. E -" Q is a finite partial/unction (the transition function), 5. qo ~ Q (the initial state), 6. A logical semantics for feature structures.
P89-1004@@The CLE has two main levels of semantic representation: quasi logical forms (QLFs), which may in turn be scoped or unscoped, and fully resolved logical forms (LFs). The full logical form for Every representative voted is as follows: quant( fora l l ,  x, Repr(x), past (quant (ex is ts ,  e, Ev(e), Vote(e,x)))). Here are some quantifier translations:  at least three but less than seven ,,~ )tm~n.
P89-1005@@It is less widely realized by computational linguists that unification can also be a powerful tool for specifying the semantic interpretation of natural languages. Taking the notion of interpretation i  this way, we will explain how our approach can be made to preserve compositionality. To solve this problem, we can use a technique called "gapthreading."
P89-1008@@All uses of definite descriptions for the purpose of referring are functional in the sense that they are supposed to identify the referent. I do not know why this is so. Moreover, if the bird did not enter through the attic, th e speaker uttering 14(a) would simply be wrong.
P89-1009@@In particular, the system generates natural language descriptions of cookery recipes. The p~nciple of adequacy requires that a referring expression should identify the intended referent unambiguously, and provide sufficient information to serve the purpose of the reference; and the principle of e~ciency, pulling in the opposite direction, requires that the referring expression used must not contain more information than is necessary for the task at hand. Suppose, also, that the domain includes a number of attributes (a I, a~, and so on), and that each attribute has a number of permissible values {v,,t, v,,2, and so on}; and that each entity is described by a set of attributevalue pairs.
P89-1015@@ One of the most serious obstacles to developing parsers to effectively analyze unrestricted English is the difficulty of creating sufllciently comprehensive grammars. E r ro rs  in  the  hand-wr i t ten  d i sam b iguat ion  ru les  Using the tagged Brown Corpus, we can ask how well the disambiguation rules of Fidditch perform in terms of the tagged Brown Corpus. Resolving Lexical Ambiguity in a Deterministic Parser.
P89-1016@@ Many basic issues need to be addresssed before technology will be able to leverage successfully from the natural advantages of speech. Composing letters with a simulated listening typewriter. IMPL ICAT IONS FOR INTERACTIVE  SPOKEN LANGUAGE SYSTEMS 1 A long-term goal for many spoken language systems i the development of fully interactive capabilities.
P89-1017@@  A major problem of computational linguistics is the inefficiency of parsing natural language. As an example, one has the generalized E~rley prediction. Alternatively, P~ may be seen as a finite state automaton with states I~.
P89-1020@@Recently there has been interest in the development of a general computational treatment of the comparative. Instead, the constraint component deals with this problem by means of special constraints that assign and pass up the comparativ e marker; other constraints test that the comparative clause is in the scope of the marker. A translation to logical form.
P89-1022@@ The problem of how language is learned is perhaps the most difficult puzzle in language understanding. This is a probl~n we discuss at the end of this paper. Give: Requires a terminus.
P89-1023@@ Semantic knowledge codification for language processing requires two important issues to be considered: 1. A third method lets the system propose the supertype. S. Nirenburg, V. Raskin 111e Subworld Concept Lexicon and the Lexicon Management System Computational Linguistics, special issue of the Lexicon D. Walker, ,4.
P89-1024@@ Hybrid representation systems have been explored before \[9, 24, 31\], but until now only one has been used in an extensive natural language processing system. Hinrichs, E.W., Ayuso, D.M., and Scha, R. The Syntax and Semantics of the JANUS Semantic Interpretation Language. Statements of user preference, eg, I want to leave in the afternoon, should be accommodated in interfaces to expert systems, as should statements of belief, I believe I must fly with a U.S. carrier.
P89-1025@@ Providing explanations in an advisory situation is a highly interactive process, requiring a dialogue between advice-giver and adviceseeker (Pollack eta/. W e  currently have over 60 plan operators and the system can answer tlie following types of (follow-up) questions: Why Why are you trying to achieve goal Why are you using method to achieve goal How did you achieve goal (in this case) What is the difference between concept1 and concept2 The text planning system described in this paper is being incorporated into two expert systems currently under development. (BR8 S II x) should be read as the speaker believes the speaker and hearer mutually believe x.
P89-1030@@  Discourse entities (DEs) are descriptions of objects, groups of objects, events, etc. In M. Brady and R. C. Berwick (Eds. Thus, the ID for "the cat" in "1 saw the cat" is (t x cats T).
P89-1031@@In the course of developing natural anguage interfaces, computational linguists are often in the position of evaluating different theoretical pproaches to the analysis of natural anguage (NL). A centering approach to pronouns. Wheels Newsweek Tasks N Hobbs 100 .
P89-1032@@ This paper describes an implemented mechanism for assigning antecedents to bound anaphors and personal pronouns, and for establishing disjoint reference between Noun Phrases. This set of indices indicates all the other NPs with which i t  is disjoint in reference. ) External NPs that meet this criterion are filtered, since not all NPs can be antecedents of an N anaphor.
P89-1033@@ Sophisticated techniques have been developed for the implementation f parsers for (augmented) context-free grammars. the rightmost leaf., s has one daughter proof which is determined by applying Proof Reconstruction to the daughter tree of g.  I f  g is a basic category and has two daughter trees tt and t~_, then A is the list of all leaves of t. s has two daughter proof trees Pt and P2C is the label of the leaf whose projection line ends at the root g. tl is the sister tree of this leaf. Right.Rule Preference: Complex categories on th.e righthand side of the arrow become current functors before complex categories on the lefthand side.
P90-1003@@  Prosodic information can mark lexical stress, identify phrasing breaks, and provide information useful for semantic interpretation. These rules necessitate the incorporation into the modified grammar of a rule Link --* e. Otherwise, a sentence such as a wh-question will not parse because an empty node introduced by the grammar will either not be preceded by a link, or not be followed by one. Each of these aspects of prosody can benefit a spoken language system (SLS).
P90-1006@@ The limited capacity of working memory is intrinsic to human sentence processing, and therefore must be addressed by any theory of human sentence processing. In R. Jacobs and P. Rosenbaum (eds. In another representation the NP her is the specifier of a hypothesized NP which is pushed onto a substack containing the other eading of the verb expected: (15){ { \[tp \[ueJohn\] [vpexpected \[tp e\]\]\] } { \[~p ~p her \] \] } } This representation is associated with at least xra PLUs since the verb expected has a thematic role to assign.
P90-1008@@They draw attention to a part of a sentence the focus of the focusing subjunct--which often represents new information. Re levant  propos i t ions  Our semantics employs a computational nalogue of Rooths p-sets for a frame representation. This says that ira frame X satisfies the frame description slot(X, agent, ross)( i .e,  its agent is ross), then it must be the frame ,ash l  (a ,ash frame whose pat ient  slot is filled by dogl).
P90-1010@@Conversation between two people has a number of characteristics that have yet to be modeled adequately in human-computer dialogue. (PROMPT E control) E: "Yeah" (PROMPT E abdicates control) CONTROL SHIFT TO C  C: "Ive got two in there. The example above illustrates the clustering of event anaphora t segment boundaries.
P90-1012@@  In the right situation, a speaker can use an unqualified indefinite description without being misunderstood. Let S, H, U, and C be constants denoting speaker, hearer, utterance, and context, respectively. Also, I have shown that the calculation of normal state implicature is based only on the third notion, i:e., that certain "defaults" are context-dependent.
P90-1014@@In the principles-and-parameters model of language, the principle known as free indexation plays an important part in the process of determining the referential properties of elements uch as anaphors and pronominals. Knuth, & O. Patashnik, Concrete Mathematics: A Foundation for Computer Science. Cross-indexing accounts for this by optionally merging indices i and j.
P90-1016@@  This paper proposes a set of representations for tenses and a set of constraints on how they can be combined. For example, the STS of the past tense is "E,R R.S." This program was linked to a simple feature-grammar parser, allowing it to take sentences as input, n In addition to building the CTS for a sentence, the program lists the apparent Emat Ea4/ relation for the CTSs it accepts, and the constraints violated by the CTSs it rejects.
P90-1017@@ There are a number of different divergence types that arise during the translation of a source language to a target language. sv, N, A, P, I, and C stand for Verb, Noun, Adjective, Preposition, Inflection, and Complementiser, respectively. Thus, the G routine activates steps (c) and (d) normally: the dominating head \[E,o., GOL.c\] is mapped into the phrasal head goes; and the modifier \[M .
P90-1019@@Many of these systems (in particular \[5, 7, 1\]) learn the new meanings based upon expectations arising from the morphological, syntactic, se*Supported by an AT&T Bell Laboratories Ph.D. scholarship. Second, instantiate this rule for each of the lexical categories N, V and P viewing NSPEC as DET, VSPEC as AUX and making PSpEC degenerate. In this paper, I present a new theory which can account for the language learning which a child exhibits.
P90-1020@@FUG enjoys uch popularity mainly because it allies expressiveness with a simple economical formalism. It is also an inefficient mechanism as the number of pairs processed uring unification is O (n ~) for a taxonomy of depth d with an average ofn branches at each level. Unification: a Multidisciplinary Survey.
P90-1021@@There a two, not quite unrelated, reasons  for  incorporat ing  de fau l ts  mechanisms into a linguistic formalism. The two formulae below, for example,  are equivalent w.r.t. In E. Klein & J.  van Benthem (eds.
P90-1022@@Many modern linguistic theories, such as Lexical-Functional Grammar \[1\], Functional Unification Grammar \[12\] Generalized PhraseStructure Grammar \[6\], Categorial Unification Grammar \[20\] and Head-dr iven PhraseStructure Grammar \[18\], replace the atomic categories of a context-free grammar with a "feature structure" that represents he.syntactic and semantic properties of the phrase. The complex element el has two attributes, subj and pred, the value of which are the complex elements e 2 and e 3 respectively. .~e 5 el0 a g r ~ a g r  number person singular third If constraints on attribute-value structures are restricted to conjunctions of equality constraints (ie requirements hat the value of a path of attributes is equal to a constant or the value of another path) then the set of satisfying attributevalue structures i the principal filter generated by the minimal structure that satisfies the constraints.
P90-1025@@Typically such formalisms use a single mechanism--variable substitution--for allpurposes, and the most widely used variable substitution mechanism is unification) Such complex-feature based grammars, then, axe viewed as systems in which partial feature structures are built up, by the process of unification, into successively more specified structures. In (44), the constitutent level at which neutralization is attempted is that of the phrase (N-P), whereas in (43) and (45) it is at the level of the pre-tenninal (N). e. *The sheep that is ready are there.
P90-1026@@  Parsing and generation me both concerned with the relation between texts and representations, and in so far as a grammar defines this relation without reference to direction, it may be regarded as reversible. Instead of generating firom the non-head aughters of each chaining rule as it is attached, the pivot is firm linked to the root, so that, if backtracking is forced, effort will not have been spent on processing StrU~h-e that must be discarded. The result is that S   is repeatedly added above S-gap, in a non-termlnating attempt o ensure completeness of the search.
P90-1027@@ The results reported in this paper are part of the ongoing research project o explore possibilities of an automated derivation of both an efficient parser and an efficient generator for natural anguage, such as English or Japanese, from a formal specification for this language. This means, in turn, that S is an essential argument in asser t ion . 9 The at most 1 requirement is the strictest possible, and it can be relaxed to at most n in specific applications.
P90-1028@@Next, we discuss two ways to implement a semantics-driven strategy. Linden, E. van der, and Minnen, G., (submitted) An account of Non-monotonous phenomena in bidirectional Lambek Theorem Proving. A prosodic operator connects prosodic elements.
P90-1031@@ We have implemented and tested a parsing system which is rapid and robust enough to apply to large bodies of unedited text. For all phrases p created (via accept -and-c lose)  by the rule chair, which led to R. perform lower(p). This rule is then popped off the queue, and accepts the \[N 1 2\] corresponding to "pastry", also closing and creating the phrase \[NP 0 2\].
P90-1033@@Information for COMPLEX is derived from multiple lexical sources o senses in one source need to be related to appropriate senses in the other sources. ban u e Sublanguage Analysis, m Su l, ~ ag : Description and Processing, eds. Speak with a burr USE(-OF-INSTRUMENT) based on burr(n,2,b,1)-"a small rotary cutting tool".
P90-1034@@ A variety of linguistic relations apply to sets of semantically similar words. REFERENCES Chodorow, Martin S., Roy J. Byrd, and George E. Heidom. (1) The clothes we wear, the food we eat, the air we breathe, the water we drink, the land that sustains us, and many of  the products we use are the result o f  agricultural research.
P90-1035@@It has been shown that they recognize xactly the set of languages recognized by deterministic push down automata. It applies iff in state i there is a dotted tree \[8, dot, rb, stars\], where dot E stars. A Study of Tree Adjoining Grammars.
P90-1036@@ Compared with the formalism of context-free grammars (CFC, s), the rules of Tree Adjoining Grammars (TAGs) can be imagined intuitively as parts of context-free derivation trees. This invariant is extended to represent a TAG derivation for t i+l. A Study of Tree Adjoining Grammars.
P90-1037@@Lexicalizing Phrase Structure rules In most current linguistic theories the information put in the lexicon has been increased in both amount and complexity. In the n0VDN1 Tree Family, for e~mmple, kick the bucket selects only al, and the trees corresponding to wh-movement on NO; bury the hatchet selects also the trees for passive (and possibly topic~liT.ation on N1). * Mary was cost a chance to win.
P91-1002@@Our inheritance networks are constructed from only the most rudimentary primitives: basic concepts and ISA and ISNOTA links. Note that the assumption that f ,  the most (} {d} {e} {.} CLASSIC: A structural data model for objects.
P91-1004@@In this paper, we present an enhanced plan-based model of dialogue understanding that provides a framework for computer processing of mixed-initiative dialogues. 3) Dialogue plans plans for establishing a dialogue construction. (1) What am I supposed to do (2) First, you must register for the conference.
P91-1005@@ To make sense of each others utterances, conversational participants must recognize the intentions behind those utterances. A Theory Of Human Action. (1) Joe: I want to lift the piano.
P91-1009@@  Referring expressions in discourse are multifunctional and dual-faced. Its that/* i t  that makes me sneeze. In Charles N. Li, editor, Subject and Topic, pages 149-188.
P91-1014@@We are also grateful to Aravind Joehi for his support of this research. By construction, in a given state s E S, non-kernel dotted rules have been introduced before run-time by the closure of kernel dotted rules. It  contains a set of actions.
P91-1015@@  Although most formalisms in computational linguistics assume that phrases are built by string concatenation (eg as in PATR II, GPSG, LFG and most versions of Categorial Grammar), this assumption is challenged in non-concatenative grammatical formalisms. The parser thus needs two grammar specific predicates: head/2 and s t r ing /2 . most word of a phrase.
P91-1016@@Although many papers report natural language processing systems based in part on syntactic analysis, their authors typically do not emphasize the complexity of the parsing and grammar acquisition processes that were involved. Gazdar, Gerald, Klein E., Pullum G., and Sag I., Generalized Phrase Structure Grammar, Harvard Univ. The symbol b stands for blank, ax-1; for article, adj for adjective, p for preposition, n for noun, and v for verb.
P91-1017@@  The resolution of hxical ambiguities innon-restricted text is one of the most difficult asks of natural language processing. Consider first a single relation. and Rossin P.S., A statistical approach to language translation, Computational Linguistics, vol.
P91-1019@@One of the tasks where the statistical data on associated words has been used with some success is lexical disambiguation. bank(l) : \[ \] : land along the side of a fiver, lake, etc. bank(9) : \[MD\] : a place where something is held ready for use, such as blood.
P91-1021@@The choice of languages for the prototype system is English and Swedish. : "wants to have") John is in a hur ry  John har  br  t tom (lit. is a hiring event by Mary Smith of e." In this notation, quantified formulae consist of a generalized quantifier, a variable, a restriction and a scope; square brackets are used for the application of predicates and operators to their arguments.
P91-1022@@The lynchpin of their approach is a. large collection of pairs of sentences that. In this example, we have an el-bead followed by an eft-bead followed by an e-bead followed by a ~l-bead. A statisticM approach to machine translation.
P91-1023@@The sentence alignment task is to identify correspondences between sentences in one language and sentences in the other language. The most embarrassing category is I-0, which was never 182 Extracting a Subcorpus with Lower Error Rate ~r e~ i t  o . Overall, there was a 4.
P91-1024@@Conventional machine translation systems use rules as the knowledge. However N~s are not similar to each other. Examples (a), (d) and (e) are idiomatic expressions.
P91-1025@@ The focus of machine translation (MT) technology has been on the translation of sentence structures out of context. Groenendijk, T. Jansaen, and M. Stokhof (eds), Formal Methods in the Study of Language. In the initial SUR are infons for 9 -~-b ":I ~ ~" (rimoofo sisutemu) remote system, 7 ~ :I i.
P91-1026@@ Sets of semantically similar words are very useful in natural anguage processing. "The case for case", in E. Bach & Harms (Eds.) The next 2 f ie lds are Japanese  verbs and the i r  corresponding English words.
P91-1027@@ This paper describes an implemented program that takes an untagged text corpus and generates a partial list of verbs occurring in it and the subcategorization frames (SFs) in which they occur. (3) a. OK I expected him to eat ice-cream b. In a Manner of Speaking.
P91-1028@@  The primary task of a computational lexicon is to associate character strings representing word forms with information able to constrain the distribution of those word forms within a sentence. More formally, we define default eztension, superclass eztension, and global ez~e~sion asfollows: 7(1) The default eztension of a FS ~ with respect to a set of FSs  We refer to this set as E(~b, c). ,  any FS F admitted by a class c~ subsumes every FS that can be created by applying to F the classes (c~ + I , .
P91-1030@@ Prepositional phrase attachment is the canonical case of structural ambiguity, as in the time worn example, (1) I saw the man with the telescope The existence of such ambiguity raises problems for understanding and for language models. Since 70 bigram frequencies f(soldier/N p) are incremented by 1/2, the unigram frequency for soldier/N is incremented by 70/2. Underlying Relations Our model takes frequency of cooccurrence as evidence of an underlying relationship, but makes no attempt o determine what sort of relationship is involved.
P91-1032@@Grammars for spoken language systems are subject to the conflicting requirements of language modeling for recognition and of language analysis for sentence interpretation. Thus the corresponding e-transition goes from state 4 to state 2. Therefore, every dotted rule A --* a .
P91-1034@@For example, the system they describe translates the French sentence Je vais prendre la ddcision as I will make the decision, correctly interpreting prendre as make. A statistical approach to language translation. We cM1 the model from which we compute Pr (E) the language model and that from which we compute Pr (F IE  ) the translation model.
P91-1036@@ In the past, several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data. S tep  3: RE JECT OR LABEL  COLLOCATION This last step consists o f  deciding on a label for the bigram from the set of label\[i~.s. MD make a takeover NN .
P91-1040@@ Unification is a central operation in recent computational inguistic research. Vq~n, where i includes no disjunctions. The atomic formula l s t_or_2nd(Per)  in (4) constrains the variable Per to be either 1st or hd .
P91-1042@@ Many modern linguistic theories are using feature structures (FS) to describe linguistic objects representing phonological, syntactic, and semantic properties. Compar i son  w i th  o ther  Approaches  Karttunens Reversible Unification \[Karttunen 86\] does not use structure sharing at M1. For every choice point with n possible continuations, n  1 new generation counters are created.
P92-1002@@ To understand an elliptical expression it is necessary to recover the missing material from surrounding context. A Formal Approach to Discourse Anaphora. Many of the remaining errors involve an antecedent that takes a VP or S as complement, often leading to subtle ambiguities.
P92-1003@@ Identification of the appropriate conjuncts of the coordinate conjunctions in a sentence is fundamental tothe understanding of the sentence. A__ comprehensive grammar ofthe English language. (e) If a pre-conjunct is still not found, then the post-conjunct is conjoined to the first word in the sentence.
P92-1005@@ The monotonicity property of unification based grammar formalisms is perhaps the most important factor in their widespread use for grammatical description and parsing. as a temporal form, whose category specifies tense and aspect information. S IMPL IF IED SEMANTICS  We will use the notation \[\[~.\]\]m for the truth value of an expression ~.
P92-1006@@  This paper addresses the question: Why should we use probabilistic models in natural anguage understanding There are many answers to this question, only a few of which are regularly addressed in the literature. Some of the techniques proposed are well-known ideas, such as compiling e-t, ra.nsitions (null gaps) out of the grammar and heuristically controlling the introduction of predictions. Pearl: A Probabilistic Chart Parser.
P92-1008@@ Spontaneous spoken language often includes speech that is not intended by the speaker to be part of the content of the utterance. as opposed to 380 msec (s.d. R indicates that the second of the two words was intended by the speaker to replace the first.
P92-1009@@  In this paper we present algorithms for the interpretation and generation of a certain kind of conversational implicature occurring in the following type of conversational exchange. (We are currently investigatin E what factors are involved in this choice. \[B\] The s tores  were closed.
P92-1010@@This forms the basis of a class of deterministic parsers which build partial descriptions of trees rather than the trees themselves. Thus ~ partitions T(S)  into equivalence classes. ~ (VT, tree)\[M E T ::~ T ~ \] The proof ollows from two lemmas.
P92-1012@@ Beginning with the late 70s, there has been a considerable interest within the computational linguistics field for rewriting systems that enlarge the generative power of context-free grammars (CFG) both from the weak and the strong perspective, still remaining far below the power of the class of contextsensitive grammars (CSG). In P. Sells, S. Shieber, and T. Wasow, editors, Foundational Issues in Natual Language Processing. Let G E LCFRS(2); in the general case, any nonterminal in G having fan-out two derives a set of pair of strings; these sets define a binary relation that is called here co-occurrence.
P92-1014@@ A typical information retrieval fiR) task is to select documents from a database in response to a users query, and rank these documents according to relevance. For example, the following definitions are excerpted from the Oxford Advanced Learners Dictionary (OALD): storage n \[U\] (space used for, money paid for) the storing of goods ... diversion n \[U\] diverting ... procession n \[C\] number of persons, vehicles, ete moving forward and following each other in an orderly way. "A mathematical theory of communication."
P92-1016@@ A speake~ (S) gives instructions to a hearer CrI) in order to affect Hs behavior. (b) If  there is no such direct relation, recursively look for possible connections between e and the components 7iof sequences that either generate or enable/5. Given the T-Box in fig.
P92-1018@@  In \[9\] a comparison was made of the generative capacity of a number of grammar formalisms. In particular, the ith node incident with e is merged with the ith node incident with e ~. Translations on a context-free grammar.
P92-1021@@ In many language processing systems, uncertainty in the boundaries of linguistic units, at various levels, means that data are represented not as a well-defined sequence of units but as a lattice of possibilities. For example, "special" forms such as telephone numbers or e-mail addresses, which are common in many domains, may contain spaces. Of these, only seven (5%) received a parse.
P92-1022@@  In a context-free grammar, the derivation of a string in the rewriting sense can be captured in a single canonical tree structure that abstracts all possible derivation orders. (In particular, the implementation does not achieve the theoretical O(n 6) bound on complexity, because of a lack of appropriate indexing.) The resultant tree is adjoined at the root node (address e) of initial tree ap~.
P92-1023@@ In a large natural language processing system, such as a machine translation system (MTS), ambiguity resolution is a critical problem. Hence, they are attractive for a large system. ,  fI,N), given that X1 ... XM are reduced to At in the context of a7 and fit.
P92-1025@@  Since negotiation is an integral part of multi-agent activity, a robust natural anguage understanding system must be able to handle subdialogues in which participants negotiate what has been claimed in order to try to come to some agreement about those claims. We will refer to such actions as e-actions ince we contend that there must be evidence to support he inference of these actions. We have not  invest igated how much evidence is needed for an  agent  to have a part icu lar  amount  of confidence in a belief; our  work has  concentrated on recogniz ing how the s t rength  of belief is communicated  in a discourse and the impact  that  the different belief s t rengths  have on the recognit ion of discourse acts.
P92-1026@@ Most contemporary grammar models employed in computational linguistics separate statements about dominance from those that determine linear precedence. LP constraints must be respected within a head domain. Because of the declarative specification of LP constraints, this encoding is neutral with respect o processing direction (parsing-generation).
P92-1028@@ State-of-the-art natural anguage processing (NLP) systems typically rely on heuristics to resolve many classes of ambiguities, eg, prepositional phrase attachment, part of speech disambiguation, word sense disambiguation, conjunction, pronoun resolution, and concept activation. Dagan, I. and Itai, A. In S 1, for example, the antecedent of"who" is "the judge."
P92-1030@@as the lowest-level discourse elements. c. She had{e3} spent{e,} five weeks in Alaska with two friends. (5) a. John went to the hospital.
P92-1031@@ Many natural language sentences have more than one possible reading with regard to quantifier scope. On the basis of this representation e may in a given context derive a representation with a determined scope. I now present the formalism.
P92-1033@@However, unlike previous work, the current approach examines aspectual considerations within the context of machine translation. $: Mar ls  me gusts a mf (Mary pleases me) (b) S t ruc tura l  divergence:  E: John entered the house 4. *-a Parameters singular  NP complements  preter i t  past progressive morpheme imperfect past progressive morpheme plural NP complements Span ish  Examples Juan le dio una pufialada a Marts   John s tabbed Mary (once) Juan conoci6 a Marts  John met  Mary (once)   Lee estaba p intando un cuadro Lee was painting a picture (~r  some t ime)   Lee conocfa a Maria Lee knew Mary (for some t ime)   Chris est Thus, if we are generating a surface sentence from an interlingual form that matches these three verbs but we know the value of the telic feature from the context of the source-language s ntence (ie, we are able to determine whether the activity reached a definite point of completion), then we will choose ransack, if the setting is +telic, or obliterate or destroy, if the setting is -telic.
P92-1034@@When combined with an attribute inheritance system, classification provides a general pattern-matching and unification capability that can be used to do much of the processing needed by NL generation systems, including content-determination, surfacerealisation, and portions of text planning. In E. Klein and F. Veltman, editors, Natural Language and Speech. Decompos i t ion  is an assertional role, whose value is a list of three templates.
P92-1047@@Sentence (1) is a typical Container-For-Content metonymy and "this glass" is replaced with "the liquid in this glass" in its metonymic reading. Producer for Product He bought a Ford. Information conveyer for information giver The T.V.
P92-1054@@  Machine-readable dictionaries contain a rich set of relationships between their senses, and indicate them in a variety of ways. For example, consider the definitions of the word appreciate and appreciation: * apprec ia te  I. to be thankful or grateful for 2. to understand and enjoy the good qualities of 3. to understand fully 4. to understand the high worth of 5. apprec ia t ion  I. judgment, as of the quality, worth, or facts of something 2. a written account of the worth of something 3. understanding of the qualities or worth of something 4. grateful feelings 5. rise in value, esp. \[3\] Krovetz R., "Lexical Acquisition and Information Retrieval", in Lezical Acquisition: Building the Lezicon Using On-Line Resources, U. Zernik (ed), pp.
P93-1001@@We have been most interested in the terminology application. Even when we did end up using the electronic format, much of  the markup had to be treated as noise since we haven  t  been able to bui ld interpreters to handle all of  the wor ld s  markup languages,  or even a large percentage of  them. The enlarged Bureau made a decision.
P93-1003@@ Areas of investigation using bilingual corpora have included the following:  The term "correspondence" is used here to signify a mapping between words in two aligned sentences. The weights C0(s, t) can be interpreted as the mean number of times that npF(t) corresponds to  apE(s) given the corpus and the initial assumption of equiprobable correspondences. Considered as a translation, this is lame.
P93-1005@@  Almost any natural anguage sentence is ambiguous in structure, reference, or nuance of meaning. The any-contsistentt ra e is a measure of the grammar s  coverage of linguistic phenomena. Pearl: A Probabilistic Chart Parser.
P93-1007@@ Disfluencies in spontaneous speech pose serious problems for spoken language systems. J. P. Gee and E Grosjean. Our "speech-first" investigation of repairs is aimed at determining the extent to which repair processing algorithms can rely on the edit signal hypothesis n practice.
P93-1009@@  There has been much debate concerning the appropriate level of language processing at which to treat VP-ellipsis resolution. Furthermore, she suggests that the function of the propositional register discussed by S&H is appropriate for accommodating her rules. Toward a theory of anaphoric processing.
P93-1012@@ The phenomenon f semantic oercion, or "metonymy", is quite a common one in natural language. Now, consider all the internal bindings of N which have R as their coercion relation. A labeled-argument notation is used for clause semantics.
P93-1014@@RGs multiple syntactic strata would seem to preclude its use in the kind of monotonic, unification-based parsing system many now consider standard (\[1\], \[11\]). The parser looks for arcs to complete that are Surface, S t ructura l  and  Re la t iona l  (SSR) . A logical version of functional grammar.
P93-1016@@A key issue in building a principle-based parser is how to procedurally interpret he principles. In (4), e is properly governed by tensed I, e I is properly governed by "think". The item i2 is sent to N, which propagates it to Nbar.
P93-1019@@and  Goa ls  ALLOMORPHY or MORPHOPHONEMICS describes the variation we find among the different forms of a morpheme. Therefore some allomorphic rules 97r* denotes the Kleene closure over alphabet 11" and A the complement of A with respect o ~r. Similarly, allomorphic variants covary in the style register they belong to: the German dative singular in -e, dera Kinde, belongs to a formal register.
P93-1020@@ A discourse consists not simply of a linear sequence of utterances, 1 hut of meaningful relations among the utterances. ACKNOWLEDGMENTS The authors wish to thank W. Chafe, K. Church, J. DuBois, B. Gale, V. Hatzivassiloglou, M. Hearst, J. Hirschberg, J. Klavans, D. Lewis, E. Levy, K. McKeown, E. Siegel, and anonymous reviewers for helpful comments, references and resources. In W. C. Mann and S. A. Thompson, editors, Discourse Description.
P93-1021@@  This paper describes a new discourse module within our multilingual natural anguage processing system which has been used for understanding texts in English, Spanish and Japanese (el. A discourse marker (cf. D iscourse  Domain  KB The discourse domain KB contains discourse domain objects each of which defines a set of discourse phenomena to handle \[n a particular domain.
P93-1022@@  Statistical data on word cooccurrence relations play a major role in many corpus based approaches for natural anguage processing. For th e non-occurring set, it correctly identified 126 pairs (84%). Wordnet and distributional analysis: A class-based approach to lexical discovery.
P93-1023@@ As natural language processing systems become more oriented towards solving real-world problems like machine translation or spoken language understanding in a limited domain, their need for access to vast amounts of knowledge increases. For other tasks, such as part-of-speech assignment to free text, the comparison techniques are sound, but very high levels of performance (e.g, 90%) can be obtained by a zeroparameter model which operates at random; clearly this makes the assessment of the significance of an improvement over the base line of the random algorithm much harder. A New Measure of Rank Correlation.
P93-1024@@ Methods for automatically classifying words according to their contexts of use have both scientific and practical interest. In Roger D. Rosenkrantz, editor, E. T. Jaynes: Papers on Probability, Statistics and Statistical Physics, number 158 in Synthese Library, chapter 4, pages 40-76. A parser for text corpora.
P93-1028@@Each of these proposes a non-commutative, non-associative default unification operation that combines one structure representing strict information with another epresenting default information. I f  t is a maximally specific explained sort, lhen ~ is called a solution of n. The solutions for {+en, {)) and {T, {+t}) are +en and +t respectively. That  is, all of our defaults will be of the form: :M~ We will write D E as an abbreviation for this default inference.
P93-1029@@Part of the motivation for doing so is to accommodate partially free word order and discontinuous constituency without the complication of passing along intermediate "threading" information within derivations. In E.M. Morgado and J.P. Martins (eds. Schmidt and A.A. Stogny (eds.
P93-1031@@ A language generation system should select words that its user knows. ; Instance def in i t ions for po la r i ty  (tellm (polarity polar ityl )  (polarity polar ity-2))  ; More detai l  for one instance: po la r i ty  is ; represented as two p lusses which  should ; be equivalent. Multimedia Disambiguation An accompanying picture often makes clear what the referent o f  a referring expression is.
P93-1032@@ Rule-based parsers use subcategorization nformation to constrain the number of analyses that are generated. 15The verb redesign does not appear in the OALD, so its subcategorization e try was determined by me, based on the entry in the OALD for design. SA sample of 100 uses of /n from the New York Times suggests that about 70% of uses are in postverbal contexts, but, of these, only about 15% are subcategorized complements (the rest being fairly evenly split between NP modifiers and time or place adjunct PPs).
P93-1033@@ Natural language processing (NLP) systems need various knowledge including syntactic, semantic, discourse, and pragmatic knowledge in different applications. Acknowledgement This research is supported in part by NSC (National Science Council of R.O.C.) The confirmation i formation required by Volition Heuristic, Imperative Heuristic.
P93-1035@@ There has been a great deal of interest of late in the automatic induction of natural anguage grammar. E. Brill and M. Marcus. A stochastic approach to parsing.
P93-1041@@ A text is not just a sequence of words, but it has coherent structure. ., e( SN ) } of lexic al cohesiveness e(Si ). If S/ is inside a segment, it tends to be cohesive and makes c(Si) high.
P94-1001@@An agent has certain goals, and communication results from a planning process to achieve these goals. 1 Specific obligations arise from a variety of sources. This might block the formation of an intention to inform, but what is it that inspires the agent o respond at all As these examples illustrate, an account of question answering must go beyond recognition of speaker intentions.
P94-1002@@The structure of expository texts can be characterized as a sequence ofsubtopical discussions that occur in the context of a few main topic discussions. (C) indicates the number of correctly placed boundaries, (I) indicates the number of inserted boundaries. However, for lengthy written expository texts, multi-paragraph segmentation has many potential uses, including the improvement ofcomputational t sks that make use of distributional information.
P94-1004@@ Hidden understanding models are an innovative class of statistical mechanisms that, given a string of words, determines the most likely meaning for the string. R. Pieraecini, E. Levin, and C. Lee, Stochastic Representation f Conceptual Structure in the ATIS Tasic. A Frame Based Meaning Representation.
P94-1007@@ Technical writers routinely employ a range of forms of expression for precondition expressions ininstructional text. As just mentioned, this text identical to the original with respect o the four lexical and grammatical issues addressed in the corpus study with 47 *IG-Text* I I I n I I Ready-to-use J New_~ Extend-Hands t Converse Precondition Form: Relational /Sentence Form: Imper. In Ventola, E.: editor: Functional and Systemic Linguistics Approaches and Uses: pages 81 106.
P94-1009@@ Imagine a discourse context for (1) in which Rs use of just (ld) is intended to convey a No, ie, that R is not going shopping tonight. 6 To explain our notation, s and h are constants denoting speaker (R) and hearer (Q), respectively. e. The rear axle is broken.
P94-1011@@ Probabilistic language modeling with n-gram grammars (particularly bigram and trigram) has proven extremely useful for such tasks as automated speech recognition, part-ofspeech tagging, and word-sense disambiguation, and lead to simple, efficient algorithms. If so, the grammar is consistent, otherwise it is not\] For the grammar in (9), E is the 1 x 1 matrix (2q). Compute ach bigram probability P (w2 \]wl ), by dividing the bigram expectation c(wlw2\[S) by the unigram expectation C(Wl IS).
P94-1012@@Such corpora contain the same material that has been translated by human experts into two languages. GALE, WILLIAM A. L: KENNETH W. CHURCH. February May August November M.B.E.
P94-1013@@The algorithm considers multiple types of evidence in the context of an ambiguous word, exploiting differences in collocational distribution as measured by log-likelihoods. In Spanish, decision lists are trained for the general ambiguity classes including -o/-6, -e/-d, -ara/-ard, and -aran/-ardn. This given list is searched for the highest ranking match in the words context, and a classification umber is returned, indicating the most likely of the words accent patterns given the context s .
P94-1014@@This study is motivated by the need for better dialogue models in spoken-language systems (SLSs). English speech act verbs: A semantic dictionary. S. Duncan and D.W. Fiske.
P94-1015@@  Human face-to-face onversation is an ideal nmdel for human-computer dialogue. Beginning of a story 8. Research  Overv iew o f  Mu l t imoda l  D ia logues  Multimodal dialogues that combine verbal and nonverbal communication have been pursued 102 mainly from the following three viewpoints.
P94-1017@@  Left-corner (LC) parsing is a parsing strategy which has been used in different guises in various areas of computer science. How to cover a grammar. We let e denote the empty string.
P94-1018@@  The aim of this paper is to work towards a computational model of how humans yntactically process the language that they hear and read. scan  get the next word from the input stream for each analysis a in the parsers memory empty the slot containing afor each lexical entry e of the word make a copy a ~ of a add the leaf derivation e to the right of a ~ add a ~ as a new analysis  It suffers from a standard problem of simple bottom-up arsers: it can only know when a certain substring has a derivation, but in case a substring does not have a derivation, the parser cannot yet know whether or not a larger string containing the substring will have a derivation. In Richard T. Oehrle, Emmon Bach, and sI am indebted to Henry Thompson for a discussion of monotonicity.
P94-1019@@ The task of lexical selection in machine translation (MT) is choosing the target lexical item which most closely carries the same meaning as the corresponding item in the source text. After defining the similarity measure in one domain, the similarity between two verb meanings, e. g, a target verb and a source verb, can be defined as a summation of weighted similarities between pairs of simpler concepts in each of the domains the two verbs are projected onto. In particular, the verb argument s ructure is used for selecting the target verb.
P94-1020@@  This paper presents a method for constructing probabilistic classifiers for word-sense disambiguation that offers advantages over previous approaches. The log-linear model expresses the logarithm of E\[x\] (the mean of x) as a linear sum of the contributions of the "effects" of the variables and the interactions among the variables. ,  n}, but is not thought of as a response to any one of the variables {1 .
P94-1022@@  We introduce the notion of Regular Form for Tree Adjoining ( ; rammars (TA(;s). Furthermore, the best known recognition algorithm for LTAGs runs in O(n 6) time. In substitution an instance of an initial tree in which the root is labeled X E NT is substituted for a frontier node (other than the foot) in an instance of either an initial or auxiliary tree that is also labeled X.
P94-1023@@Recently, researchers have shown a revived interest in variants of Marcus et als DTheory, most likely due to the availability of approaches and techniques developed in the study of feature structures and their underlying logics. An obvious solution is to enumerate the maximal extensions of F0 FI F N and feed them to the generators. (Atoms of the form e(x,31 ) are satisfiable if and only if x and 31 denote identical members of the domain of discourse.
P94-1027@@A formalism is said to be lexicalized if it is composed of structures or rules associated with each lexical item and operations to derive new structures from these elementary ones. Every time an anchoring to a unit m is completed, C is reduced from 0W(t~). I also gratefully acknowledge Chantal Enguehard and two anonymous reviewers for their remarks on earlier drafts.
P94-1033@@Therefore, the quality of their translations greatly affects the performance of a machine translation system. Hence, such an n-gram is a preferred compound candidate. It might therefore xtract many inappropriate t rminologies with high false alarm.
P94-1035@@Firstly there has not been any work on consistency checking techniques for feature terms augmented with set descriptions. if it is not the case that C~ I. At and for any relation f  ~" there exists no e  \]2 to an element a(x)  /41 such that for distinct constants Cl, c2 : a(cl) # a(c2) 3. every primitive concept C  The set-valued description of the subcategorisation principle can now be stated as given in example (3).
P94-1036@@Several mathematical models have been proposed which extend the formal power of CFGs, while still maintaining the formal properties that make CFGs attractive formalisms for formal and computational linguists, in particular, polynomial parsability and restricted weak generative capacity. Since G may contain e-productions, the algorithm is adapted by letting the indices of the matrix refer to positions between symbols in the input string, not the symbols themselves. We conclude with a brief summary.
P94-1038@@ Data sparseness is an inherent problem in statistical methods for natural language processing. D iscount ing  and  Red is t r ibut ion  Many low-probability bigrams will be missing from any finite sample. There were a total of 96 such disagreements.
P94-1039@@  For many natural anguages, a major problem for a language learner, whether human or machine, is the system of bound morphology of the language, which may carry much of the functional load of the grammar. The stimuli consisted of twosyllable words, where the initial consonant (the onset) of each syllable came from the set /p ,  b, f, v, m, t, d, s, z, n, k, g, x, 7, xj/, the vowel from the set / i ,  e, u, o, a/, and the final consonant, when there was one, from the set /n ,  s/. In James L. McClelland and David E. Rumelhart, editors, Parallel Distributed Processing, Volume 2, pages 216-271.
P94-1040@@I am grateful to Ted Briscoe for comments on an earlier version of this paper, to David Weir for valuable discussions, and to Hiyan Alshawi for assistance with the CLARE system. In Categories, Polymorphism and Unification, edited by E. Klein & J. van Benthem, Centre for Cognitive Science, Edinburgh University, UK. The number of categories i actually infinite in grammars that use a fully recursive feature system.
P94-1041@@ Interactive spoken dialog provides many new challenges for spoken language systems. gol tol oran-I uml gol tol Corning ml I m2 I x I int \[  et I ml I m2 I A speech repair can also be characterized by its repair pattern, which is a string that consists of the repair labels (word fragments are labeled as -, the interruption point by a period, and editing terms by e). % on a fair test.
P94-1044@@  Improved understanding of the nature of knowledge used in human language processing suggests the feasibility of interactive models in computational linguistics (CL). S t ructura l  Uni f icat ion Graded unification builds structure xactly as classical unification except in the case of atomic unification, where it deviates crucially. The perfect  compat ib i l i ty  is computed by a formula identical to this except hat UaSt rength  is set to 1.
P94-1046@@Specifically, to account for the forward progression of time induced by successive simple past tenses in a narrative, they treat the simple past as referring to a time evoked by a previous past tense. I would like to thank Stuart Shieber and Barbara Grosz for valuable discussions and comments on earlier drafts. He spilt a bucket of water.
P94-1050@@  In general, texts are "about" some topic. Skorochodko, E.F. Adaptive Method of Automatic Abstracting and Indexing. More formally, let n be the length of the concatenation of articles; let m be the number of unique tokens (after lemmatization and removal of words on the stop list); let B be a list of boundaries, initialized to contain only the boundary corresponding to the beginning of the series of articles, 0.
P94-1051@@ Parallel linguistically meaningful text units are indispensable in a number of NLP and lexicographic applications and recently in the so called Example-Based Machine Translation (EBMT). Having estimated the b i and 0 2, the probabilistic score assigned to the comparison of two sentences across languages i  just the area under the N(0,o 2) p.d.f., specified by the estimation error. \[Basili 92\] Basili R. Pazienza M. Velardi P. "Computational lexicons: The neat examples and the odd exemplars".
P94-1052@@ Compound Nouns: Compound nouns (CNs) are a commonly occurring construction in language consisting of a sequence of nouns, acting as a noun; pottery coffee mug, for example. The results show more success with left branching attachments, o it may be possible to get better overall accuracy by introducing a bias. A modifier should be associated with words it modifies.
P95-1002@@first observed that traditional phonological rewrite rules can be expressed as regular elations if one accepts the constraint that no rule may reapply directly to its own output. This transducer will flap a t after any odd number of stressed vowels, rather than simply after any stressed vowel. The next n symbols of the transductions output are now marked as having been used.
P95-1003@@ Linguistic descriptions in phonology, morphology, and syntax typically make use of an operation that replaces ome symbol or sequence of symbols by another sequence or symbol. Special thanks are also due to Kenneth R. Beesley for technical help on the definitions of the replace operators and for expert editorial advice. Final states are distinguished by a double circle.
P95-1004@@  Semitic is known amongst computational linguists, in particular computational morphologists, for its highly inflexional morphology. Forthcoming in Narayanan, A. and Ditters, E., editors, The Linguistic Computation o.f Arabic. Compilation of n:l two-level rules into finite state automata.
P95-1005@@There are two main contributions of the work we will discuss in this paper. 2 D iscourse  S t ructure  Our discourse model is based on an analysis of naturally occurring scheduling dialogues. L. Lambert and S. Carberry.
P95-1006@@  In order to develop a practical natural anguage processing (NLP) system, it is essential to deal with ill-formed sentences that cannot be parsed correctly according to the grammar ules in the system. S tep  2: J o in ing  par t ia l  parses  on  the  basis  of  the  d i scourse  in fo rmat ion  If the partial parses are not unified into a single structure in the previous step, they are joined together on the basis of the discourse information until a unified parse is obtained. (44th sentence) Therefore, these two partial parses are restructured by changing the part of speech of the word "side" to noun, and the modifiee of the noun "operator" to otric view (n)J ~.~ :~f.
P95-1007@@Compound Nouns If parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts. A training instance is any sequence of four words WlW2W3W 4 where wl, w4 ~ .h/and w2, w3 E A/. Conceptua l  Assoc ia t ion  One problem with applying lexical association to noun compounds is the enormous number of parameters required, one for every possible pair of nouns.
P95-1008@@The language lacks many of the constructs found in general purpose, knowledge representation formalisms, yet it has sufficient expressive power to capture concisely the structure of lexical information at a variety of levels of linguistic description. On the other hand, the meaning of each of the di is obtained with respect o the empty path e (ie path extension does not apply to subterms of inheritance descriptors). We can define a partial function A( f )  : U* --~ U* (the default interpretation of T/N)  as follows.
P95-1013@@  Head Driven Phrase Structure Grammar (HPSG) and Tree Adjoining Grammar (TAG) are two frameworks which so far have been largely pursued in parallel, taking little or no account of each other. Principles, such as the *We would like to thank A. Abeill6, D. Flickinger, A. Joshi, T. Kroch, O. Rambow, I. In P. Sells, S. Shieber, and T. Wasow, eds., Foundational Issues in Natural Language Processing.
P95-1015@@  Many have argued that discourse has a global structure above the level of individual utterances, and that linguistic phenomena like prosody, cue phrases, and nominal reference are partly conditioned by and reflect this structure (cf. In E. Hovy and D. Scott, editors, Interdisciplinary Perspectives on Discourse. 5\]\] you know the pears fal l i ,  and.
P95-1016@@The representation f intentions in the VERBMOBIL system serves two main purposes:  Utilizing the dialogue act of an utterance as an important knowledge source for translation yields a faster and often qualitative better translation than a method that depends on surface expressions only. Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, editors. 3 The  Stat i s t i ca l  P red ic t ion  Method and  i t s  Eva luat ion  In order to compute weighted dialogue act predictions we evaluated two methods: The first method is to attribute probabilities to the arcs of our network by training it with annotated ialogues from our corpus.
P95-1019@@  In collaborative consultation dialogues, the consultant and the executing agent collaborate on developing a plan to achieve the executing agents domain goal. R. Michael Young, Johanna D. Moore, and Martha E. Pollack. A tripartite plan-based model of dialogue.
P95-1021@@A salient feature of TAG is the extended omain of locality it provides. Get t ing  Word  Order  R ight :  Kashmir i  \[ twO~:: -~ NP VP ! The SA-tree for ~ E To(G) consists of a single node labelled by the elementary d-tree name for a.
P95-1022@@ In this paper we are concerned with the syntactic analysis phase of a natural language understanding system. For example, if a natural anguage understanding system is interfaced with a speech recognition component, chances are that this c o ~ t  is uncertain about the actual string of words that has been uttered, and thus produces a word lattice of the most promising hypotheses, rather than a single sequence of words. A yes-no problem is undecidable (cf.
P95-1023@@TAGs are tree generating systems, and are strictly more powerful than context-free grammars. By the hypothesis, it also computes all nodes ( i  , j ,k  , l  )within the last 2/3, i.e, { i ~, ! References D. Coppersmith and S. Winograd, Matrix Multiplication Via Arithmetic Progressions, in Proc.
P95-1025@@By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus (10 million words), the system can identify salient words for each category. The meaning or underlying concepts of a word are very difficult o capture accurately but dictionary definitions provide a reasonable representation a d are readily available. Moreover, the magnitude of mutual information is decreased ue to the noise of the spurious senses while the average magnitude of the occurrence probability is unaffected, e Inclusion of the occurrence probability term will lead to the dominance of this term over the mutual information term, resulting in the system flavouring the sense with the more frequently occurring defining concepts most of the time.
P95-1026@@  This paper presents an unsupervised algorithm that can accurately disambiguate word senses in a large, completely untagged corpus) The algorithm avoids the need for costly hand-tagged training data by exploiting two powerful properties of human language: 1. Slator, Brian, "Using Context for Sense Preference," in Text-Based Intelligent Systems: Current Research in Text Analysis, Information Extraction and Retrieval, P.S. In A. Zampoli, N. Calzolari and M. Palmer (eds.
P95-1027@@Such an opposition can occur at various linguistic levels. 5 Eva luat ion  o f  L ingu is t i c  Tests  For each of the variables, we measured how many pairs in each group it classified correctly. In classical inear modeling, the response variable y is modeled as y -bTx+e where b is a vector of weights, x is the vector of the values of the predictor variables and e is an error term which is assumed to be normally distributed with zero mean and constant variance, independent of the mean of y.
P95-1028@@  It is generally assumed that sentences with multiple quantified NPs are to be interpreted by one or more unambiguous logical forms in which the scope of traditional logical quantifiers determines the reading or readings. In S. Peters and E. Saarinen, editors, Processes, Beliefs, and Questions, pages 129 164. 2 T rad i t iona l  Approaches  All three paradigms of grammar formalisms introduced earlier share similar linguistic judgments for their grammaticality analyses.
P95-1029@@ Many theories of semantic interpretation use A-term manipulation tocompositionally compute the meaning of a sentence. Essentially, the logic variable X is meant o be interpreted as a bound variable, which requires an additional ayer of programming. 2 3 Each of the three operations have both a forward and backward variant.
P95-1030@@  Current approaches to automatic speech and handwriting transcription demand a strong language model with a small number of states and an even smaller number of parameters. RISTAD, E. S., AND THOMAS, R. G. Context models in the MDL framework. The fourth term represents the cost of encoding c(.Iw ) for E(w).
P95-1031@@  In applications uch as speech recognition, handwriting recognition, and spelling correction, performance is limited by the quality of the language model utilized (7; 7; 7; 7). , the parameter e is set to an arbitrary small constant. A formal theory of inductive inference.
P95-1033@@Our thesis in this paper is that he lexical information actually gives sufficient information toextract not merely word alignments, but also bracketing constraints for both parallel texts. As an abbreviation we write co.., for the sequence of words eo+l,e,+2,. A statistical approach to machine translation.
P95-1034@@  A large-scale natural language generation (NLG) system for unrestricted text should be able to operate in an environment of 50,000 conceptual terms and 100,000 words or phrases. The grammar assigns what we call an e-structure to each subexpression. A CL (student session).
P95-1035@@In this paper, we describe an alternative generation component which has polynomial time complexity. A Statistical Approach to Machine Translation. The  Generat ion  A lgor i thm The generator cycles through two phases: a test phase and a rewrite phase.
P95-1036@@  In this paper we present some novel applications of the so-called Explanation-Based Learning technique (EBL) to parsing Lexicalized Tree-Adjoining rammars (LTAG). Explanation-Based Generalization: A Unifying View. In Proceedings of the I~  h Interna.
P95-1037@@  Parsing a natural anguage sentence can be viewed as making a sequence of disambiguation decisions: determining the part-of-speech of the words, choosing between possible constituent structures, and selecting labels for the constituents. A commonly-used technique for smoothing is deleted interpolation. Model  Parameter i za t ion  First, lets be very clear on what we mean by an n-gram model.
P95-1039@@The states represent parts of speech (categories, tags), there is exactly one state for each category, and each state outputs words of a particular category. Formally, seeing tags as sets of words and clusters as sets of tags: VcEC, tl,t2Ec, t l~t2,wE}/Y: wEt l : :~w~t2 (2) If this condition holds, then for all words w tagged with a cluster e, exactly one tag two fulfills w E twe A t~.e E c, yielding fo . (d) Add the cluster which maximized the tagging accuracy to the tagset and remove the two tags previously used.
P95-1044@@  In this paper, we address the problem of modelling interactions between different levels of language analysis. Linguistics and Philosophy, 4:517-558. res op arg sere  }hon "\]H" res cat n r person none number none possessive none syn nprop |case none |relative none Lform common type property \] sere form h~ I~)j op ( / ,  free, concat) syn Lnprop \[ form com. Lexical elements may have (a) phonemes, (b) metephonemes such as H for high vowel, and D for a dental whose voicing is not yet determined, and (c) optional segments, eg, -(y)lA, to model vowel/consonant drops, in the phon feature.
P95-1047@@For the time being, we approximate the problem as induction from phone sequences rather than acoustic pressure, and assume that learning takes place in an environment where simple semantic representations of the speech intent are available to the acquisition mechanism. References  Timothy Andrew Cartwright and Michael R. Brent. Others, l ike / t / ,  /wo/, and /don/ demonstrate how the system compensates for the morphological irregularity of English contractions.
P95-1050@@Most of the proposed algorithms first conduct an alignment of sentences, i. e. those palxs of sentences axe located that are translations of each other. A statistical pproach to machine translation. Word n in the English matrix is then the translation of word n in the German matrix.
P95-1053@@ Aggregation is any syntactic process that allows the expression of concise and tightly constructed text such as coordination or subordination. S tep  h Sor t ing  The system first ranks the attributes to determine which are most similar across messages with the same action. 2 System Arch i tec ture  Fig.
P95-1055@@  Computer language learning is an area of much potential and recent research. Finally, for each word E T, if word and w appear in one or more sentences together, the sentence representations in words entry that correspond to such sentences are modified by eliminating the portion of the sentence representation that matches t, thus shortening that sentence representation for the next iteration. If t occurs n times in one of these sentence representations, the sentence representation is removed n times, since we add one copy of the representation to wR for each occurrence of w in a sentence.
P96-1001@@However, it is also well known that the HOU approach to NL semantics ystematically overgenerates and that some general theory of the interface between the interpretation process and other sources of linguistic information is needed in order to avoid this. For instance, the FSV of (4a) 2 is (4b), the set of formulae of the form l(j,x) where x is of type e, and the pragmatic effect of focus is to presuppose that the denotation of this set is under consideration. It turns out that for finding all g-unifiers it is sufficient o bind x to terms of the same type as x (otherwise the unifier would be ill-typed) and compatible colour (otherwise the unifier would not be a C-substitution) that either  have the same head as the right hand side; the so-called imi ta t ion  solution (.kz.a in our example) or  where the head is a bound variable that enables the head of one of the arguments of x to become head; the so-called pro jec t ion  binding ()~z.z).
P96-1002@@  Generative grammar and formal language theory share a common origin in a procedural notion of grammars: the grammar formalism provides a general mechanism for recognizing or generating languages while the grammar itself specializes that mechanism for a specific language. Johnson, David E. and Paul M. Postal. Note, though, in contrast o typical applications of default logics, a GPSG grammar is not an evolving theory.
P96-1003@@Information Retrieval Information retrieval (IR) is an important application area of naturaManguage processing (NLP). We believe the use of N-P-substructure analysis can lead to more effective information management, including more precise IR, text summarization, and concept clustering. Sag and A. Szabolcsi (eds.
P96-1004@@Some natural anguage processing (NLP) tasks can be performed with only coarse-grained semantic information about individual words. The method returns a set of normalized (i. e., uninflected) word/feature pairs. This is a labor intensive task.
P96-1005@@  This paper deals with the discovery, representation, and use of lexical rules (LRs) in the process of largescale semi-automatic computational lexicon acquisition. In P. Saint-Dizier and E.Viegas, Computational Lcxical Semantics. Approaches to LRs and  The i r  Types In reviewing the theoretical and computational linguistics literature on LRs, one notices a number of different delimitations ofLRs from morphology, syntax, lexicon, and processing.
P96-1008@@In this paper, we present such a system. Placeway, P., Schwartz, R., Fung, P., and Nguyen, L. "The Estimation of Powerful Language Models from Small and Large Corpora." The probability of each of these operations i  determined by a statistical decision tree model.
P96-1009@@ While there has been much research on natural dialogue, there have been few working systems because of the difficulties in obtaining robust behavior. A Statistical Approach to Machine Translation. We ran several experiments og5 T !
P96-1011@@This renders (2) syntactically ambiguous. In R. Oehrle, E. Bach and D. Wheeler, editors, Categorial Grammars and Natural Language Structures. The function Pre:ferableTo(~, r) (step 15) provides flexibility about which parse represents its class.
P96-1015@@1 1The regular expression formalism and other notational cdnventions used in the paper are explained in the Appendix at the end. 112 4 App l i ca t ions  The directed replacement operators have many useful applications. For example, we may define NP as \[(d) a* n+J.
P96-1016@@Similarly, many problems in natural language processing, in particular parsing and generation, can be expressed as transductions, which are calculations of such correspondences. We briefly discuss a sample derivation. 4 Formal  resu l t s  Theorem 1 SynchUVG-DL has the language preservation property.
P96-1018@@Most of these works assume voluminous aligned corpora. 7 organ compound water radioactive PET  spatial such metabol ism verb scientist wnter water mappin |  take university thousht compound label task radioactivity visual noun s i |na l  present I) 7"/L,~Z 4 .& time ~xY dan~6~e a.ut oradiogrsphy ability CT auditory mental MRI  CT ,b MR ! Let us consider a 3-1 matching.
P96-1019@@The LM is usually built by collecting statistics of words over a large set of text data. After segmenting the set T2, we add to our vocabulary all unseen words with its counter greater than a threshold e. Then we use the augmented vocabulary and construct another LMi+I using the segmented T2. We get a perplexity 188 for a general Chinese corpus with 2.  million characters 4 6 Acknowledgment  The first author would like to thank various members of the Human Language technologies Department at the IBM T.J Watson center for their encouragement and helpful advice.
P96-1021@@Assume, as we do throughout this paper, that the input language is Chinese and the task is to translate into English. Then the maximum size of E(s , t )  is proportional to t s. The asymptotic time complexity for the translation algorithm is thus bounded by O(T7). A statistical approach to machine translation.
P96-1022@@The past decade has seen a number of proposals for handling non-linear morphology; 2 however, none * Supported by a Benefactor Studentship from St Johns College This research was done under the supervision of Dr Stephen G. Pulman. Surf for all Setld(Var) e Variables % check variables there  is tl_set(SetId, Set) such that  Vat E Set & uni fy  Cats with  Features & fail. A prosodic theory of nonconcatenative morphology.
P96-1023@@  Until the advent of statistical methods in the mainstream of natural language processing, syntactic and semantic representations were becoming progressively more complex. "A Statistical Approach to Machine Translation". The second element, an event e, is an equivalence class of states after transitions.
P96-1024@@  In corpus-based approaches to parsing, one is given a treebank (a collection of text annotated with the "correct" parse tree) and attempts to find algorithms that, given unlabelled text from the treebank, produce as similar a parse as possible to the one in the treebank. ...." oo, ,%, / ,  t : " /  I 0 lO Labelled Tree Algorithm: Consistent Brackets Recall Bracketed Recall Algorithm: Consistent Brackets Recall . It is even less strict in that the observed (s , t ,X)  need not be in Tc-i t  must simply not be ruled out by any (q, r, Y) e To.
P96-1025@@  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). Probability estimates are based on counts of consecutive pairs of words in unreduced training data sentences, where baseNP boundaries define whether gaps fall into the S, C, E, B or N categories. I would also like to thank David Magerman for his help with testing SPATTER.
P96-1029@@  Much attention has been devoted recently to methods for inferring linguistic models from data. Let us assume that this /aa / i s  initial in the word, in which case we go left. We can represent his by setting p at this node to be E*, where E (conventionally) represents the entire alphabet: note that the alphabet is defined to be an alphabet of all : correspondence pairs that were determined empirically to be possible.
P96-1030@@  Suppose that we have a general grammar for English, or some other natural anguage; by this, we mean a grammar which encodes most of the important constructions in the language, and which is intended to be applicable to a large range of different domains and applications. Utterances may consist of more than one ut terance_un i t . Explanation-Based Generalization: a Unifying View.
P96-1032@@This has stimulated the computational linguistics community to develop extensions of these techniques to general context-free grammar parsing. We write e to denote the empty string. Formally, for a fixed PDA .
P96-1033@@  In natural anguage processing filtering is used to weed out those search paths that are redundant, ie, are not going to be used in the proof tree corresponding to the natural anguage xpression to be generated or parsed. E.g., in order to derive fact 13, magic fact 2 is unified with the magic literal in the modified version of rule 2 (in addition to the facts 12 and 10). I discuss two filter optimizations.
P96-1035@@  Intrasentential ntecedents, ie, antecedents occurring in the same sentence as the anaphor, are a crucial issue for the anaphora resolution method. M and Berwick R.C, editors, Computational Models of Discourse. Toward a computation of intrasentential coreference.
P96-1036@@Though several cross-linguistic studies have been carded out (cf. The Cb(U,), the most highly ranked element of C.t(Un-i) realized in \[In, corresponds to the element which represents the given information. (U,_ i ) (unbound iscourse dements); they express the new information in Un.
P96-1039@@ While research on generating coherent written text has flourished within the computational linguistics and artificial intelligence communities, research on the generation of spoken language, and particularly intonation, has received somewhat less attention. In Engdahl, E., editor, Integrating Information Structure into Constraint-Based and Categorial Approaches (DYANA-2 Report R. ..B). In fo rmat ion  S t ructure  and  In tonat ion  The relationship between intonational structure and information structure is illustrated by (3) and (4).
P96-1041@@Language models are used in speech recognition to resolve acoustically ambiguous utterances. In N. Oostdijk and P. de Haan, editors, Corpus-Based Research into Language. A hierarchical Dirichlet language model.
P96-1042@@  Many corpus-based methods for natural language processing (NLP) are based on supervised training-acquiring information from a manually annotated corpus. Tagging text with a probabilistic model. If cl ~ c~, select e for annotation; 4.
P96-1043@@ Words unknown to the lexicon present a substantial problem to part-of-speech (POS) tagging of realworld texts. Another important conclusion from the evaluation experiments i  that the morphological guessing rules do improve the guessing performance. For example, an ending-guessing rule A~: \[in s (JJ NN VBG) \ ]  says that if a word ends with "ing" it can be an adjective, a noun or a gerund.
P96-1044@@Language is a robust and necessarily redundant communication mechanism. It is capable of compressing a sequence of identical characters of length n to size O(log n). References  Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss.
P96-1052@@The field of punctuation has been almost completely ignored within Natural Language Processing, with perhaps the exception of the sentence-final full-stop (period). As a result, most contemporary systems imply strip out punctuation i input text, and do not put any marks into generated texts. Punctuation could occur adjacent o any complex structure.
P96-1057@@Since intra-sentential naphora occur at high rates in realworld texts, the model has to be extended for the resolution of anaphora t the sentence l vel. A centering approach to pronouns. Now, we are able to define the expression utterance in a satisfactory manner: An utterance U is a simple sentence, a complex sentence, or each full clause of a compound sentence 3. of an utterance is computed only with respect o the matrix clause.
P97-1001@@  A significant part of the development of formalisms for computational linguistics has been concerned with finding the appropriate data structures to model the linguistic entities. The other problem is that we can only specify delays on all constraints on t at once, and cannot delay individual principles. We extended this mechanism to the universal principles: the constraints on a certain type were only checked, once certain attributes were sufficiently instantiated (w.r.t.
P97-1003@@ Generative models of syntax have been central in linguistics since they were introduced in (Chomsky 57). Neither of these models is generative, instead they both estimate 7)(T\] S) directly. rather than Pz(Li, P, H I li, h, distancel), and that there are the additional probabilities of generating the head and the STOP symbols for each constituent.
P97-1004@@Terms cannot be retrieved properly with coarse text simplification techniques (eg stemming); their identification requires precise and efficient NLP techniques. Harris, Zellig S., Michael Gottfried, Thomas Ryckman, Paul Mattick Jr, Anne Daladier, T. N. Harris, and S. Harris. Recycling terms into a partial parser.
P97-1005@@  Computational linguists have been concerned for the most part with two aspects of texts: their structure and their content. For example, instead of estimating separate weights o, 3, and 3 for the ratios words per sentence (average sentence length), characters per word (average word length) and words per type (token/type ratio), respectively, we express this desired weighting: , I I + l  C+I  W+I  a log~+31og~+3,1og T+I  as follows: "(c~ /3  + 7) log(W + 1)a log(S + 1) + 31og(C + 1) ~. The log representation e sures that.
P97-1006@@  We are concerned here with the issue of classifying documents into categories. Lewis, David D., Robert E. Schapire, James P. Callan, and Ron Papka. Gale, Williams A. and Kenth W. Church.
P97-1007@@there are no available wide range lexicons for natural anguage processing (NLP) for other languages. Biblograf S.A. Barcelona, Spain. In Klein E. and Veltman F. eds.
P97-1008@@  The problem of data sparseness affects all statistical methods for natural language processing. 2 D is t r ibut iona l  S imi la r i ty  Mode ls  We wish to model conditional probability distributions arising from the coocurrence of linguistic objects, typically words, in certain configurations. c~(wl) is a normalization factor.
P97-1009@@  Given a word, its context and its possible meanings, the problem of word sense disambiguation (WSD) is to determine the meaning of the word in that context. The pos i t ion  indicates whether word is the head or the modifier in depen65 dency relation. We then explain how the similarity between a word and its selectors i  maximized.
P97-1010@@ Lexical ambiguity is a fundamental problem in natural language processing, but relatively little quantitative information is available about the extent of the problem, or about the impact that it has on specific applications. We examined all words in the dictionary in which a word ended in y, and in which the y could be replaced by e and still yield a word in the dictionary. Exper iments  with Par t  of  Speech Relatively little attention has been paid in IR to the differences in a words part of speech.
P97-1011@@  Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. C. D. E. 1. part2 is moved frequently and thus 2. is more susceptible to damage than part3. We compute 05% confidence intervals for the two error rates using a t-test.
P97-1013@@ Researchers of natural language have repeatedly acknowledged that texts are not just a sequence of words nor even a sequence of clauses and sentences. Assign a weight o each of the discourse trees and determine the tree(s) with maximal weight. op pc,~li t.. polar cap.
P97-1016@@ Achieving the goal of producing high quality machine translation output is hindered by lexica\] and syntactic ambiguity of the input sentences. Tl~e other is the parsing results on the same sets of data using the grammar which combines lexicalized semantic grammar rules and syntactic grammar rules. A number o f  the sentences we consider to be misparses are t svntacuc mksparses, but "semanucallv anomalous.
P97-1018@@  VVhen words have multiple senses, these may have very different frequencies. The Coherence Constra int  on E laborat ion states that an elaborating event must be temporally included in the elaborated event. Sag and A. Szabolsci (eds.
P97-1019@@In this approach, meaning assembly is guided not by a syntactic onstituent tree but rather by the flatter functional structure (the LFG f-structure) of the sentence. --.o fa,,~t rarely(P) The case of licensing quantifier phrases such as nobody and Jew students follows the same pattern. For example, nobody takes the form nobody:  ((g#"*e x  We can now derive a meaning for sentence (3), in which nobody and anyone play the roles of licenser and NPI, respectively.
P97-1020@@with aspectual properties of verbs clearly influencing the alternations of interest. If Top node of L E {ACT, BE. with a (thing 1) argument.
P97-1021@@  Data-oriented models of language processing embody the assumption that human language perception and production works with representations of concrete past language xperiences, rather than with abstract grammar rules. 7Experiments using fragments of maximal depth 6 and maximal depth 7 yielded the same results as maximal depth 5 166 E. Black, R. Garside, and G. Leech. nl/remko_b/dopsem/script ie html T. Briscoe.
P97-1023@@For example, some nearly synonymous words differ in orientation because one implies desirability and the other does not (eg, simple versus simplisfic). Michael R. Garey and David S. Johnson. 5 P red ic t ion  o f  L ink  Type  The analysis in the previous ection suggests a baseline method for classifying links between adjectives: since 77.
P97-1024@@  In this paper, we present an empirical argument in favor of a certain approach to statistical natural anguage modeling: we advocate statistical natural anguage models that account for the interactions between the explanatory statistical variables, rather than relying on independence a~ssumptions. In each experiment, performance IMutu,d Information provides an estimate of the magnitude of the ratio t)ctw(. We distinguish between a set of explanatory variames.
P97-1026@@Broadly speaking, it involves aggregating content into sentence-sized units, and then selecting the lexical and syntactic elements that are used in realizing each sentence. All are licensed, because of the poset relation R between book19 and books and the salient open proposition O. Ontological promiscuity offers a simple syntax-semantics interface.
P97-1027@@ Generating referential descriptions I requires electing aset of descriptors according to criteria which reflect humans preferences and verbalizing these descriptors while meeting natural anguage constraints. 6 E f fec ts  the  A lgor i thm Can Hand le  Space restrictions do not permit a detailed presentation of the new algorithm at work. In R. Dale, C. Mellish, M. Zock, editors, Current Issues in Natural Language Generation, pages 257-285, Academic Press, New York.
P97-1028@@The core idea of EBL is to transform the derivations (or explanations) computed by a problem solver (eg, a parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future. Explanation-based generalization: a unifying view. I would like to thank the HPSG people from CSLI, Stanford for their kind support and for providing the HPSG-based English grammar.
P97-1029@@  Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora. 5 E f f i c ient  Imp lementat ion  Techn iques  and  Extens ions  The current implementation f the voting approach is meant to be a proof of concept implementation and is rather inefficient. Thus, the outcome of the rule applications i independent of the order of rule applications.
P97-1030@@The stochastic approach generally attains 94 to 96% accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm. At ally node s of a context tree, let n(ats ) and /5(als ) be tile count of element a and its probability, respectively. Most theoretical work on re-sampling assumes i.i.d (identically, independently distributed) samples.
P97-1032@@  There are currently two main methods for automatic part-of-speech tagging. Tile corpus-annotation procedure allows us t.o perform a text-book statistical hypothesis test. "A syntax-based part of speech analyser".
P97-1033@@  Interactive spoken dialog provides many new challenges for natural anguage understanding systems. For Ri E {Mod, Can}, we define Oi to indicate the onset of the reparandum. Class-based n-gram models of natural anguage.
P97-1034@@ Naturally-occurring collaborative dialogues are very rarely, if ever, one-sided. A Mathematical Theory of Evidence. The subscript sub may be t-X or d-X, indicating that the function represents he task or dialogue bpa under scenario X. SThe initiative indices are represented as bpas.
P97-1035@@ Recent advances in dialogue modeling, speech recognition, and natural language processing have made it possible to build spoken dialogue agents for a wide variety of applications, n Potential benefits of such agents include remote or hands-free access, ease of use, naturalness, and greater efficiency of interaction. DR A8: Do you want o leave between 6 and 9 p.m. The mean performance of A is -.
P97-1036@@  By providing a number of channels through which information may pass between user and computer, multimodal interfaces promise to significantly increase the bandwidth and fluidity of the interface between humans and machines. 2 Qu ickset :  A Mu l t imoda l  In ter face  fo r  D is t r ibuted  In teract ive  S imulat ion  The initial application of our multimodal interface architecture has been in the development of the QuickSet system, an interface for setting up and interacting with distributed interactive simulations. In E. Klein and J. van Benthem, 287 editors, Categories, Polymorphisms, and Unification, pages 65-72.
P97-1038@@The authors propose using an adaptive Expectation and Maximization (EM) algorithm to estimate parameters for lexical translation probability (LTP) and distortion probability (DP), two factors in the TP, from an aligned bitext. Furthermore, the n arby connections @, O, and 0 ,  form a texture much like a permutation matrix with roughly one dot per row and per column. The one-to-one assumption leads to a textural pattern that can be categorized as  region of dense dots distributed much like the l s in a permutation matrix.
P97-1039@@  Texts that are available in two languages (bitexts) are immensely valuable for many natural language processing applications z. Bitexts are the raw material from which translation models are built. language number of number of RMS Error pair training TPCs genre test TPCs in characters French / English 598 parliamentary debates CITI technical reports other technical reports court transcripts U.N. annual report I.L.O. In geometric terms, the difference is a distance.
P97-1042@@  The past few years have witnessed an increased interest in applying finite-state methods to language and speech problems. We can define another form of Inser t  where the elements in rn are tuples of symbols as followS: Let R be a regular relation over the alphabet and let rn be a set of tuples of symbols not necessarily in E. Inser tm(R)  inserts a, for all a E m, freely throughout R. Def in i t ion  3. n}, denotes the tuple element si. A prosodic theory of nonconcatenative morphology.
P97-1043@@His conception addressed idactic goals and, thus, did not aim at formal precision, but rather at an intuitive understanding of semantically motivated ependency relations. \[\] Lemma 2 Let (V, E, k) be a possible instance of the vertex cover problem. This DG can characterize at least some context-sensitive languages such as anbnc n, ie, the increase in complexity corresponds to an increase of generative capacity.
P97-1047@@Stat is t ica l  Machine Trans lat ion Statistical machine translation is based on a channel model. Initialize the stack with a null hypothesis. 2 S tack  Decod ing  A lgor i thm Stack decoders are widely used in speech recognition systems.
P97-1048@@  One of the fundamental characteristics of language, viewed as a stochastic process, is that it is highly nonstationary. First is the decay of the probability of a word t as the distance k from the most recent occurrence of its mate s increases. A dynamic language model for speech recognition.
P97-1052@@Although f-structures are first and foremost syntactic representations they do encode some semantic information, namely basic predicate argument structure in the semantic form value of the PRED attribute. \ ]~\ ]andc~ dom(~\]) then ED n(...) ~1 e wl/-s The side condition in the second clause ensures that only identical substructures can have identical tags. 4 An  f s t ruc ture  UDRS re turn  t r ip  In order to illustrate the basic idea we will first give a simplified graphical definition of the translation r from f-structures to UDRSs.
P97-1053@@During the last few years, the phenomenon of semantic underspecification, ie the incomplete availability of semantic information in processing, has received increasing attention. 2 Context  Un i f i ca t ion  Context unification is the problem of solving context constraints over finite trees. Thus, this is a case where the additional expressive power of context constraints i crucial.
P97-1055@@These single-route models crucially suggest that the pronunciation of unknown words results from the parallel activation of similar lexical items (the lexical neighbours). The basic idea is to generate A(x), defined as {Ai(x), forAi E ,4, x E domain(Ai)}, which contains all the words that can be derived from x using a function in ,4. PRONOUNCE: a program for pronunciation by analogy.
P97-1056@@  Statistical approaches to disambiguation offer the advantage of making the most likely decision on the basis of available vidence. In E. Ejerhed and I. Dagan (eds.) If a particular pattern x~, .
P97-1057@@In this work we further elaborate on the errordriven learning paradigm. A space-economical suffix tree construction algorithm. ,  Ct be a partition of E (each Ci ~-O).
P97-1059@@  Finite-state automata have been successfully applied in many areas of computational linguistics. From the outside, an HMM tagger behaves like a sequential transducer that deterministically 1There is a different (unpublished) algorithm by Julian M. Kupiec and John T. Maxwell (p.c.). A Tutorial on HidR .o.
P97-1060@@  In recent years there has been a continuing interest in computational linguistics in both model theoretic syntax and finite state techniques. Since a quantifier prefix of the form 3. . The recognizable s ts are also closed under projections (mappings from one alphabet o another) and inverse projections, and again the construction is essentially that for finite state automata.
P97-1061@@ A collocation is a recurrent combination of words, ranging from word level to sentence level. The method comprises two stages: the first stage extracts sequences of words (or characters) t from a corpus as units of collocations and the second stage extracts recurrent combinations of units and constructs collocations by arranging them in accordance with word order in the corpus. In Proceedings o/ the 16th COLING, pages 525-530.
P97-1062@@  The parsing of unrestricted text, with its enormous lexical and structural ambiguity, still poses a great challenge in natural anguage processing. shift n It s-verb red uCe 2 . Machine Translation: A Knowledge-Based Approach.
P97-1063@@However, the IBM models, which attempt o capture a broad range of translation phenomena, are computationally expensive to apply. Note  that the frequencies are plotted on a log scale -the b imodal i ty  is quite sharp. (3) A can also be estimated empirically.
P97-1064@@  The main goal of the proposed project is to develop a language model(LM) that uses syntactic structure. The model will operate by means of two modules:  PREDICTOR predicts the next word wk+l given the word-parse k-prefix and then passes control to the PARSER;  PARSER grows the already existing binary branching structure by repeatedly generating the transitions ad jo in le f t  or ad jo in r ight  until it passes control to the PREDICTOR by taking a nu l l  transition. It can be brought o the full power of Spatter by changing the action of the adjoin operation so that it takes into account he terminal/nonterminal l bels of the constituent proposed by adjoin and it also predicts the nonterminal label of the newly created constituent; PREDICTOR will now predict he next word along with its POS tag.
P97-1065@@  &: Re la ted  Work  The applications of term recognition (specialised ictionary construction and maintenance, human and machine translation, text categorization, etc. Step 4: The previously created by C-value r list will now be re-ordered considering the weights obtained from step 3. (~ -~ + f(w) (3) where w is the noun/verb/adjective to be assigned a weight, n the number of the first candidate terms considered, t(w) the number of candidate terms the word w appears with, ft(w) ws total frequency appearing with candidate terms, f(w) ws total frequency in the corpus.
P97-1067@@When the same concept admits more than one lexicalization, it is often difficult to choose which of these synonyms is the most appropriate for achieving the desired pragmatic goals: but this is necessary for highquality machine translation and natural language generation. The approach relies on a generalization of lexical co-occurrence that allows for an implicit representation f the differences between two (or more) words with respect o any actual context. (1) However, such a move also would run the risk of cutting deeply into U.S. economic growth, which is why some economists think it would be a big {error I mistake \[ oversight}.
P97-1070@@In this framework, a paraphrase is a tool for modifying a text to fit a set of constraints like length or lexical density. A Lexicalised Tree Adjoining Grammar for English. Then, for example, the tree for (2a) can be defined as the adjunction of a flN0nx0VX tree (generic relative clause tree, standing for, eg, ~N0nx0Vnxlnx2) into an an0VY tree; the tree for (2b) can be defined as a conjoined S tree, having a parent Sm node and 2 child nodes an0VX and an0VY.
P97-1071@@ The placement of pitch accent plays an important role in the interpretation ofspoken messages. t (1) a Ill the 16th minute, the Ajax player Kluivert kicked the ball into the wrong goal. 4 Future  d i rec t ions  An open question which still remains, is at which level data structures hould be compared.
P99-1001@@  The nascent field of text data mining (TDM) has the peculiar distinction of having a name and a fair amount of hype but as yet alost no practitioners. Don R. Swanson and N. R. Smalheiser. The  L INDI  P ro jec t  The objectives of the LINDI project 3 are to investigate how researchers can use large text collections in the discovery of new important information, and to build software systems to help support this process.
P99-1004@@ An inherent problem for statistical methods in natural language processing is that of sparse data  the inaccurate representation i any training corpus of the probability of low frequency events. Thomas M. Cover and Joy A. Thomas. 2Strictly speaking, some of these funct ions are dissimilarity measures, but  each such funct ion f can be recast as a similarity funct ion via the s imple t rans format ion  C f ,  where C is an  appropr iate  constant .
P99-1005@@  In many statistical anguage-processing problems, it is necessary to estimate the joint probability or cooeeurrence probability of events drawn from two prescribed sets. For each test fold, (a) discard seen pairs and duplicates (b) discard pairs with unseen nouns or unseen verbs (e) for each remaining (n, vl), create (n, vl, v2) such that (n, v~) is less likely Step 3b is necessary because neither the similarity-based methods nor backoff handle novel unigrams gracefully. SankyhS: The Indian Journal of Statistics, 44(A):1-22.
P99-1008@@ We present a method of extracting parts of objects from wholes (eg "speedometer" f om "car"). Thus, say, "novel" might have "plot" as a part. E. parts NN-PL in PREP wholes NN-PL .
P99-1009@@  Machine learning has been very successful at solving many problems in the field of natural language processing. E. Brill and P. Resnik. Forgetting exceptions i harmful in language learning.
P99-1011@@  A grammar is, among other things, a device by which it is possible to express structure in a set of entities; a grammar formalism, the constraints on how a grammar is allowed to express this. S vP  Adv VP cunningly V NP defeated Det N J I the Sumerians or2 (1) ;35 (2) or3 (2. ) The diacritic E\] allows a sentential modifier for both trees (eg unfortunately / malheureusement).
P99-1013@@Viewing contemporary linguistic formalisms as very high level declarative programming languages, a grammar  for a natural language can be viewed as a program. a combination operator on denorations. 2Assmne that in all the example grammars, the types s, n, v and vp are maximal and (pairwise) inconsistent.
P99-1014@@ An important challenge in computational linguistics concerns the construction of large-scale computational lexicons for the numerous natural languages where very large samples of language use are now available. 5 L ingu is t ic  In terpreta t ion  In some linguistic accounts, multi-place verbs are decomposed into representations involving (at least) one predicate or relation per argument. References  Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss.
P99-1015@@  Aspectual classification maps clauses to a small set of primitive categories in order to reason about time. I f  a c lause can occur:  then  it is: with a temporal adverb Event  (eg, then) in progressive Extended Event  with a duration in-PP Cu lm Event  (eg, in an hour) in the perfect ense Cu lm Event  or S ta te  straints. Linguistic Stative Event T-test Indicator Mean Mean P-value frequency 932. clauses were rejected because of parsing problems.
P99-1017@@  When generating referring expressions (RE), it is generally considered necessary to provide sufficient information so that the reader/hearer is able to identify the intended referent. For example, the rightmost node shows that an RE with the content s ize&color(o l ) ,  eg, the small black thing, confuses ol and o3. In R. Dale, C. Mellish, and M. Zock, editors, Current Research in Natural Language Generation.
P99-1018@@  Sequential ordering among premodifiers affects the fluency of text, eg, "large foreign financial firms" or "zero-coupon global bonds" are desirable, while "foreign large financial firms" or "global zero-coupon bonds" sound odd. In E. L. Keenan, editor, Formal Semantics of Natural Language. Trans i t iv i ty  As we mentioned before, sparse data is a serious problem in our analysis.
P99-1020@@  Word Sense Disambiguation (WSD) is an open problem in Natural Language Processing. Vii ranks each pair vi -n j ,  for all i and j. Rat iona le  1. Using a semantic oncordance for sense identification.
P99-1021@@  Disambiguation of capitalized words in mixedcase texts has hardly received much attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks. Thus the disambiguation of capitalized words in the ambiguous positions leads to the identification of proper names I and in this paper we will * Also at HCRC, University of Edinburgh 1This is not entirely true adjectives derived from locations such as American, French, etc., are always writuse these two terms interchangeably. In Proceedings of the Fifth A CL Conference on Applied Natural Language Processing (ANLP97), Washington D.C., ACL.
P99-1022@@The most common language modeling paradigm used today is based on n-grams, local word sequences. ~ most topic  sens i t ive  fixed over the nodes of the tree. However, considering a linear contribution 172 Language Perplexity on Perplexity on Mode l  the  ent i re  the  target vocabulary vocabulary Standard  B igram Mode l  215 584 H is tory  s ize Sca led  100 5OO0 .
P99-1023@@  Part-of-speech tagging is the act of assigning each word in a sentence a tag that describes how that word is used in the sentence. State Transit ion Probabi l i t ies To estimate the state transition probabilities, we want to use the most specific information. A maximum entropy model for part-of-speech tagging.
P99-1025@@ The dialog manager described in this paper implements a novel approach to the problem of dialog management. Generating semantically consistent inputs to a dialog manager. G. Riccardi and S. Bangalore.
P99-1026@@ Building a real-time, interactive spoken dialogue system has long been a dream of researchers, and the recent progress in hardware technology and speech and language processing technologies i  making this dream a reality. James E Allen, Bradford W. Miller, Eric K. Ringger, and Teresa Sikorski. GUS, a frame driven dialog system.
P99-1027@@Should we translate the documents or the queries in cross-language information retrieval The question is more subtle than the implied two alternatives. Farwell., E. Hovy, and L. Gerber, editors, Machine Translation and the Information Soup, page 150. Translingual information retrieval : A comparative evaluation.
P99-1029@@ Cross-language information retrieval (IR) enables a user to retrieve documents written in diverse languages using queries expressed in his or her own language. The De~ree of Ambiguities I \[Wrds I Wrd Pairs # in S. # in T. Average # in S. # in T. Average Lan. Once the value for W b is calculated, the weight for the rest o f  the candidates are calculated as follows: Wr _ 1 W h (3) n -1  where n is the number of candidates.
P99-1032@@Our approach is data driven: we refine our understanding and presentation of the classification scheme guided by the results of the intercoder analysis. We are grateful to Matthew T. Bell and Richard A. Wiebe for participating in the annotation study, and to the anonymous reviewers for their comments and suggestions. A coefficient of agreement for nominal scales.
P99-1033@@There have also been a number of approaches to natural anguage parsing using extended finite state approaches in which a finite state engine is applied multiple times to the input, or various derivatives thereof, until some stopping condition is reached. 8. o. denotes the t ransducer  composit ion operator. Parsing English with a link grammar.
P99-1035@@The present paper describes an experiment where a slightly modified version of Carroll and Rooths model was applied in a systematic experiment on German, which is a language with rich inflectional morphology and free word order (or rather, compared to English, free-er phrase order). Manual pages for supar, u l t ra ,  hypar, and genDists. Possible arguments in the frames are nominative (n), dative (d) and accusative (a) NPs, reflexive pronouns (r), PPs (p), and infinitival VPs (i).
P99-1036@@But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. e -~p k (5) where p is the inverse of the number of characters in the character set. Automatic construction of a Chinese electronic dictionary.
P99-1037@@  Morphological analysis is an essential component in language ngineering applications ranging from spelling error correction to machine translation. In E. Ejerhed and I. Dagan, editors, Proc. Van den Bosch, and A. Weijters.
P99-1038@@  The phenomenon of quantifier scope ambiguity has been discussed extensively within computational and theoretical linguistics. An assignment function, I\[-~/, maps constants of the constraint language onto 298 elements which occur in S and wffs of the constraint language onto one of the pair of values {t , f} . So for example, consider again the sentence Every representative of a company saw most samples, and assume that terms in the underspecified representation representing the the grammatical objects every, exists, most, rep.of and see map onto the elements e, a, m, o and s respectively, where {e, a, m, o, s} C CON.
P99-1040@@ Builders of spoken dialogue systems face a number of fundamental design choices that strongly influence both performance and user satisfaction. Therefore, in this work, we focus on the task of automatically detecting poor speech recognition performance in several spoken dialogue systems developed at AT&T Labs. Kim sent me a message on it.
P99-1042@@ This paper describes our initial work exploring reading comprehension tests as a research problem and an evaluation method for language understanding systems. One of the oldest o be found dates back to about 800 years B.C. "Episodic Logic Meets Little Red Riding Hood: A Comprehensive, Natural Representation for Language Understanding", in L. Iwanska and S.C. Shapiro (eds.
P99-1043@@  Online information retrieval is now prevalent because of the ubiquitous World Wide Web. I  Proceedings of AAAI 92. The 1-best feature is most effective for disambiguating secondary language words in a mixed-language s ntence.
P99-1044@@  In the classical approach to text retrieval, terms are assigned to queries and documents. P rec i s ion  Because of the large volumes of data, only experiments on the French corpus are evaluated. Let / :  be the set of lemmas, morphological roots define a binary relation M from  to / :  that associates each lemma with its root(s): M E  M is not a function because compound lemmas have more than one root.
P99-1045@@This paper shows that NLP can benefit information retrieval in a very different way: rather than increasing the size and complexity of an IR index, linguistic information can make it possible to store less information i  the index. % with only a minor impact (-0. To appear in Dale, R., H. Moisl and H. Somers (eds.
P99-1046@@Despite this fact, most IR systems treat documents as indivisible units and index them in their entirety. We view segmentation as a labeling task. I Our Approach We treat the process of creating documents as an instance of the noisy channel model.
P99-1048@@  Most automated approaches to coreference r solution attempt o locate an antecedent for every potentially coreferent discourse ntity (DE) in a text. We propose a corpusbased mechanism toidentify non-anaphoric NPs automatically. Exhaustively examining all possible combinations i  expensive and, we believe, unnecessary.
P99-1049@@  This paper describes a statistical speech act type tagging system that utilizes linguistic, acoustic and situational features. Parameter Recall rate % Precision rate % A 89. We again admit a t  score that is greater than 2.01 as significant (twosided significance level of 5 %).
P99-1051@@The phenomenon i English is illustrated in (1)-(2) below. Leave a note for her. Cannot decide if cues surface structure is V MOD* N N+ (eg, offer a free bus service).
P99-1052@@ This paper describes a new method of dealing robustly with deficient speech or text input, which may be due to recognition errors, spontaneous speech phenomena, orungrammatical constructions. Using this graph, we then compute the shortest paths w.r.t. E.g., a prepositional phrase can be a complete utterance xpressing an answer to a question (On Monday.)
P99-1053@@The parsers used in most dialog systems have not evolved much past their origins in handling written text even though they may have to deal with speech repairs, speakers collaborating to form utterances, and speakers interrupting each other. 80 u: so the total is 81 s: five 255. Peter A. Heeman and James F. Allen.
P99-1054@@This is a very commonly held view among psycholinguists today. Journal o/ Memory and Language, 33:251-283. 426 x lO  4 Average States Considered per Sentence 98 96 94 14 i i RB0  .
P99-1059@@Lexicalized grammar formalisms are of both theoretical and practical interest o the computational inguistics community. However, S --+ e is not allowed in our bilexical CFGs. A quasi-arithmetical notation for syntactic description.
P99-1063@@  Natural anguage processing must disambiguate polysemous constituents in the input sentences. A Reference Grammar of Japanese. The Case in which New E lements  must be Infered in the Semant ic  Representat ion  There are cases in which we have to  infer new elements in the semantic representation so as to represent semantic relations between adnominal constituents and their modified nouns.
P99-1064@@After having demonstrated that punctuality actually breaks up into two, distinct notions, namely non-durativity and atomicity, I will argue here for a compositional semantic account of the latter. If x possesses proper subparts, then e will be incremental ; the whole point remains that incrementality is lexically licensed but structurally construed. Paper presented at the 29 t~ Linguistic Symposium on Romance Languages, University of Michigan, Ann Arbor, MI, April.
P99-1065@@ Much of the recent research on statistical parsing has focused on English; languages other than English are likely to pose new problems for statistical methods. Pt (NP ( IBM, NNP) I S, VP, bought, VBD) x Pt(NP (yesterday, NN) I S ,VP, bought ,VBD)  e~ (STOP I s, vP, bought, VBD)  Pr (STOP I S, VP, bought . .n+l  where L0 is defined as a special NULL symbol.
P99-1066@@ Sentence parsing is a task which is traditionMly rather computationally intensive. ), which encompasses words j through k  1 of the sentence, and t0,~ represents all n part-of-speech tags, from 0 to n 1. We also tried a hybrid of the two.
P99-1067@@Nevertheless, the results achieved with these algorithms have been found useful for the cornpilation of dictionaries, for checking the consistency of terminological usage in translations, for assisting the terminological work of translators and interpreters, and for example-based machine translation. A statistical approach to machine translation. I In cases in which an ambiguous word can be both a content and a function word (eg, can), preference was given to those interpretations that appeared to occur more frequently.
P99-1068@@  Text in parallel translation is a valuable resource in natural language processing. A statistical approach to machine translation. This likely to be an upper bound on the canComparison N Pr(Agree) J1, J2: 267 0.
P99-1069@@Probabilistic methods have revolutionized computational inguistics. The idea is to generate random samples of feature structures from the distribution P~i(w), where 0 is the current parameter estimate, and to use these to estimate E~(fj), and hence the gradient of the likelihood. A method for disjunctive constraint satisfaction.
P99-1070@@  Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. Reduce transitions are e-transitions that combine two stack symbols. We say that ~s nonterminal label is X and its weight is u.
P99-1071@@  Information overload has created an acute need for summarization. Serbs were holding over 250 U.N. personnel. As a first pass, we found paraphrasing rules manually.
P99-1075@@  Efficient treatment of syntactic/semantic ambiguity is a key to making efficient parsers for wide-coverage rammars. Hence, we unify E\] with this result. ,9 and S" denote sets of segments, and 7) and 7)" denote sets of dependency functions.
P99-1077@@The notion of text cohesion rests on the intuition that a text is "held together" by a variety of internal forces. This yields a \[C\]-dimensionalvector for each w E W. The direction that the vector has in the resulting ICI-dimensional space then represents the collocational behavior of w in the training corpus. The paragraph as a grammatical unit.
P99-1079@@A revised algorithm (Left-Right Centering) was motivated by the fact that the BFP algorithm did not allow for incremental processing of an utterance and hence of its pronouns, and also by the fact that it occasionally imposes a high computational load, detracting from its psycholinguistic plausibility. The "N" group consists of algorithms that search intersententially through all Cf-lists for an antecedent. A centering approach to pronouns.
P99-1081@@  The coordinate phrase (CP) is a source of structural ambiguity in natural anguage. It is analogous to n l  and n2 in the ambiguous CP. For example, when the heuristic came upon a noun phrase consisting of more than one consecutive noun the noun closest o the cc was extracted.
P99-1083@@ Filled pauses (FPs), false starts, repetitions, fragments, etc. Bates, R. and Stolcke, A. Ph.D. thesis, University of California t Berkely.
T75-2001@@ An augmented phrase structure grammar (APSG) consists of a col lection of phrase structure rules which are augmented by arbitrary condit ions and structure bui lding actions. Heidorn, G.E., "English as a very high level language for simulation programming," Proc. Simmons, R.F., "Semantic networks: their computation and use for understanding English sentences," in COMPUTER MODELS OF THOUGHT AND LANGUAGE, R.C.
T75-2036@@ AND MOTIVATION It is becoming increasingly evident to human intel l igence model bui lders and theorists that, in order to character ize human knowledge and bel ief as computer data structures and processes, it is necessary to deal with very large, expl ic i t ly  unif ied structures rather than smaller, ununif ied fragments. Origins of wants can be expl ic i t ly  represented via this link. As the basis for the conceptual representat ion o_~f language.
T78-1027@@ Understanding every day discourse requires making inferences from a very large base of common sense knowledge. Charniak, E., On the use of framed knowledge in language comprehension, forthcomming. If i t  were a MOVE with the EARTH as the thing moved then EARTHQUAKE.
W00-0103@@The introduction of such classes can reduce the amount of lexical semantic processing, because the number of disambiguation decisions can be restricted more clearly to those cases that involve real ambiguity (homonymy). Threshold Number of Ambiguos Clusters Nouns in (Systematic Clusters Polysemous (Systematic Classe)s Polysemous Nouns) 0,70 1. Comparison of sense distributions i now performed over synsets on all levels, not just over a small set on the top levels.
W00-0104@@These lexicons have been used as a domainindependent semantic resource as well as an evaluation criteria in various Natural Language Processing (NLP) tasks, such as Information Retrieval (IR), Information Extraction (IE) and Word Sense Disambiguation (WSD). I ~  inhalant dose / l ~  foot VESSEL spoon spoon dish plate k .o t  / \  bottle bucket 0. A cousin relation in WordNet is defined between two synsets (currently in the noun trees only), and it indicates that senses of a word that appear in both of the (sub)trees rooted by those synsets are related, s The cousins were manuMly extracted by the WordNet lexicographers.
W00-0203@@ IF (Interchange Format) is an interlingua used by the C-STAR consortium 1 for task-oriented ialogues. For example, acknowledg e and aff irm together account for 19.  percent of the data. ~ AGENT: a double room costs $150 a night.
W00-0207@@One of the big challenges in Natural Language processing efforts is to be able to make use of existing resources, a big difficulty being the sometimes large differences in syntax, semantics, and ontologies of such resources. E.g., using the words thousand, million, and billion. Machine Translation: A View from the Lexicon.
W00-0302@@GoDiS is a prototype dialogue system for information-seeking dialogue, capable of accommodating questions and tasks to enable the user to present information in any desired order, without explicitly naming the dialogue task. Scorekeeping in a language game. In addition to the control module, which wires together the other modules, there are six modules in  GoDiS: input, which receives input3from the user; interpret, which interprets utterances as dialogue moves with some content; generate, which generates natural language from dialogue moves; output, which produces output to the user; update, which updates the information state based on interpreted moves; and select, which selects the next move(s) to perform 4.
W00-0303@@  Dialogue modeling is a critical and challenging aspect of conversational systems, particularly when users are permitted flexibility with regard to defining the constraints of the task. This E-form is delivered to the turn manager, and provides the initial settings of the dialogue state. U: no thank you S: Thank you for calling!
W00-0304@@We have adapted the methods of RL to the problem of automatically learning a good dialogue strategy in a fielded spoken dialogue system. (ASR output: I d like to f ind out wineries the in the Lambertville in the morning.) U6: Visit a histor ic .
W00-0309@@ Spoken language interaction can take many forms. Except for the simplest tasks, graph systems have several limitations: Unless the graph is carefully designed, users will find themselves unable to switch to a topic that is coded in a different sub-tree without going through the common par~e~t of the two. References \[1\] James F. Allen, Lenhart K. Schubert, George Ferguson, Peter Heeman, Chung Hee Hwang, Tsuneaki Kato, Marc Light, Nathaniel G. Martin, Bradford W. Miller, Massimo Poesio, and David R. Traum, "The TRAINS Project: A case study in building a conversational p anning agent" Journal of Experimental nd Theoretical AI, 7(I 995), 7-48.
W00-0310@@  Conversational systems that use speech as the input and output modality are often realized by architectures that decouple speech processing components from language processing components. The influence of pitch range, duration, amplitude, and spectral features on the interpretation f l*+h I h%. Intonat ional  Meaning Theoretical work on intonational meaning has attempted to relate the grammatical e ements of Pierrehumberts system  pitch accent, phrase accent and boundary tone, to interpretive processes at different levels of discourse and dialogue structure.
W00-0401@@  We have specified a method of text summarization which produces indicative-informative abstracts for technical documents. The information about the s i tuat ion,  the  problem, the need for research,  etc. We are grateful to Professor Mich~le Hudon from Universit~ de Montreal for fruitful discussion and to Professor John E. Leide from McGill University and to Mme Gracia Pagola from Universit~ de Montreal for their help in recruiting informants for the experiments.
W00-0402@@ Discourse is understood to refer to any form of language-based communication involving multiple sentences or utterances. She examines discourse markers in interview data, looking specifically at their distribution and their particular interpretation(s). (2) Even though a child, (because) John is tall, so he has problem getting half-fare.
W00-0403@@However, all world agencies would be actively reporting on the major events that were to happen in Pakistan in the following days: Prime Minister Nawaz Sharif announced that in Gen. Musharrafs absence, the Defense Minister had been -sacked and replaced by General Zia Addin. By varying E between 0 and 1, the evaluation may favor or ignore subsumption. We have implemented CBS in a system, named MEAD.
W00-0405@@ With the continuing growth of online information, it has become increasingly important to provide improved mechanisms to find and present extual information effectively. A robust practical text summarization system. t ime line ordering: Text passages ordered based on the occurrence of events in time.
W00-0407@@ Summaries are often used to select relevant documents from information retrieval results. Extrinsic methods measure a systems performance in a particular task. , , ,~~e~ As you can see each summary is generic, ie .
W00-0408@@Yet understanding how to evaluate their output has received less attention. Measures which do not depend on ground truth compute the summarydocument similarity sire(s, d). Often, a fixed summary length K is used.
W00-0409@@: multi-document summarization as an enabling technology for IR  The rapid growth of electronic documents has created a great demand for a navigation tool to traverse a large corpus. .S addam Huss e i~. A briefing is represented asa tree.
W00-0503@@ .-, The foreign language material to be handled by the government is increasingly diverse and problematic. As can sometimes be the case, the prototype started being used in 18 S Translation g ~lr~nstl at\]n ~ Broker N,,,,...~._~" inr I continuous operation, causing a demand for improvement concurrent with ongoing operation. The clients remain e-mail, Web and FrameMaker.
W00-0507@@  TRANSTYPE is a project set up to explore an appealing solution to the problem of using Interactive Machine Translation (IMT) as a tool for professional or other highly-skilled translators. c/e pro jet de loi ce pro jet de Ioi est tr~s semblable au pro jet de Ioi que nous avons examin4 hier la chambre des communes ce+ / lOi" C/ p+ /est p/rojet d+ /tr~s d/e I+ /tr~s I/oi e+ /de e/st t+ /de  The first column indicates the target words the user is expected to produce. O(t~,s) stands for any function which maps t~,s into a set of equivalence classes.
W00-0508@@Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing. We call this a bilanguage TB. state recognizes asymbol wi E lZU {e}, where e is the empty string.
W00-0601@@ CS241, the graduate course in statistical language processing at Brown University, had as its class project the creation of programs to answer reading-comprehension tests. AND CHARNIAK, E. A statistical approach to anaphora resolution. are given a boost by the weighting scheme.
W00-0603@@  In the United States, we evaluate the reading ability of children by giving them reading comprehension tests. I fSeBEST Then Score(S) +-clue 2. E. Riloff and J. Shepherd.
W00-0605@@ This paper describes a preliminary reading comprehension system constructed as a semester-long project for a natural language processing course. N_o.te that the PP NeedSeraType and VP SemType appear to be less capable of guiding the selection of the correct answer. Deep Read: A reading comprehension system.
W00-0701@@  Many important natural language inferences can be viewed as problems of resolving phonetic, syntactic, semantics or pragmatics ambiguities, based on properties of the surrounding context. E. Brill and P. Resnik. A. Grove and D. Roth.
W00-0702@@  Expressive grammar formalisms allow grammar developers to capture complex linguistic generalizations concisely and elegantly, thus greatly facilitating grammar development and maintenance. 2 Unfo ld ing  and  Spec ia l i za t ion  The initial grammar is the grammar underlying the subset of correct parses in the training set. In the experiments, a simple hill-climbing algor ithm was adopted.
W00-0706@@  Word Sense Disambiguation (WSD) is the problem of assigning the appropriate meaning (or sense) to a given word in a text or discourse. A Coefficient of Agreement for Nominal Scales. The topical context is formed by c l , .
W00-0707@@In particular, if hi is known, finding the best word at the current position requires only a straightforward search through the target 1This ignores the issue of normalization over target texts of all possible lengths, which can be easily enforced when desired by using a stop token or a prior distribution over lengths. On the other hand, features like fstijl, indicating the presence of a specific pair (s, t) at position (i, j , /) ,  would cause severe data sparseness problems. Linear interpolation is designated with a + sign; and the MEMD2B position parameters are given as rex, where m and n are the numbers of position partitions and word-pair partitions respectively.
W00-0708@@  Article choice can pose difficult problems in natural language applications. We experimented with a range of features: 1. Forgetting exceptions i harmful in language learning.
W00-0712@@However, development of these analyzers typically begins with human intervention requiring time spans from days to weeks. In Survey of the State of the Art in Human Language Technology, R. Cole, Ed., Giardini Editori e Stampatori, Italy. Since the U and V matrices of an SVD are orthogonal matrices, then uuT:vvT : I .
W00-0717@@  In this paper I present a novel program that induces syntactic ategories from comparatively small corpora of unlabelled text, using only distributional information. S. Finch and N. Chater. For each word w, I then calculated the optimal coefficents c~ w).
W00-0726@@Text chunking is a useful preprocessing step for parsing. Consequently it does not belong to any VP chunk: ((S (SINV (CONJP Not only) does (NP-SBJ-1 your product) (VP have (S IE.g. We think that PPs behave sufficiently differently from NPs in a sentence for not wanting to group them into one class (as Ramshaw and Marcus did in their N-type chunks), and that on the other hand tagging all NP chunks inside a PP as I -PP would only confuse the chunker.
W00-0737@@Our approach follows Skut and Brants way by employing HMM-based tagging method to model the chunking process. A computational model of language performance: Data-oriented parsing. Normally, each tag is assumed to be probabilistic dependent on the N-1 previous tags.
W00-0801@@ With the term "globalization" becoming the theme of cuxrent political and economic discourse, communications technology exemplified by the World Wide Web OVWW) has become a source of an abundance of languages. In effect, we have assigned sense tags to rfi, e in its respective alignments, inthe appropriate contexts. The system was not penalized if it assigned more than one sense to the noun in the tag set i f  the correct sense was among the senses assigned.
W00-0802@@It is commonly assumed that sense distinctions in these lex~cal databases are too fine-grained for a majority of applications. E r roneous  cluster steer 1,2: castrated bull/ hint, indication off potential opportunity. A. Kilgarriff and M. Palmer.
W00-0804@@  This work describes some preliminary results about Word Domain Disambignation (WDD), a variant of Word Sense Disambiguation (WSD) where for each word in a text a domain label (among those allowed by the word) has to be chosen instead of a sense label. We expected a better esult for the bilingual extensions. dal;e#3 appointment, engagement  Stop Senses ynsets which appear frequently in different contexts, such as numbers, week days, colors, etc.
W00-0901@@Corpus-based techniques have increasingly been used to compare language usage in recent years. The ACAMRIT semantic tagging system: progress report, In L. J. Evett, and T. G. Rose (eds.) kinds, groups (Sector; Sectors) With the exception of Y I (an anomaly caused by an interviewees initials being mistaken for the PH unit of acidity), all of these semantic categories include important objects, roles, functions, etc.
W00-0902@@KeyWords compares a word list extracted from what has been called the study corpus (the corpus which the researcher is interested indescribing) with a word list made from a reference corpus. (Available online at www.direct.f2s.com) 3 This could be done in WordSmith itselftl~ough t e consistency list function. LAEL, Catholic University of S~o Paulo, Brazil.
W00-0904@@  With the advent of the information society and increasing availability of large mounts  of information in electronic form, new technologies such as information extraction are emerging to meet users information access needs. A statistical profile of the named entity task. i ser :  NEHMM HMMs are a widely u~d class of learning algorithms and can be considered to be stochastic finite state machines.
W00-0905@@Previous research, however, has shown that subcategorization probabilities vary widely in different corpora. The contributions of verb bias and p!a_u~ibility o the comprehension ftemporarily ambiguous sentences. Statistical parsing with a context-free grammar and word statistics.
W00-0906@@ The identification of the language style characterising the constituent parts of a corpus is very important o several appfieations. a given level of absolute correlation. La Pens~,e Sauvage, Grenoble.
W00-1007@@  Many natural language processing applications involve the complex task of resolving anaphora. Consider the fi)llowing two examples: Peter  boede ie t  r~dt hus. D. Byron and A. Stent.
W00-1011@@Close-world and static approaches have tremendous limitations and often fail when the task becomes complex and the application environment and knowledge changes. Two variables (i,e., local expertise l vel and accumulated expertise level) are maintained by the Level-Adjusting Agent for the automated level updating. If the confirmation isnegative i.
W00-1012@@ Several researches have modelled the process of argument negotiation in cooperative dialogue where one participant makes a proposal to another participant and as the result of negotiation this is accepted or rejected. I  should be mentioned that adding obligations to the standard BDI model is not new. If yes then f inish (the communicat ive goal has not been reached).
W00-1104@@  The main problem with the traditional boolean word-based approach to Information Retrieval (IR) is that it usually returns too many results or wrong results to be useful. A simple rule-based part of speech tagger. What parameters can seriously influence natural transition from laminar to turbulent f low on a model in a wind tunnel Can a satisfactory e~perimental technique be developed for measuring oscillatory derivatives on slender stingmounted models in supersonic wind tunnels Recent data on shock-induced boundary-layer separation.
W00-1107@@  Traditional information retrieval (IR) has been built on the "bag-of-words" assumption, which equates the weighted component keywords of a document with its semantic ontent. The relations view of natural language documents i highly amenable to integration with information retrieval systems. Ext rac t ion  Ru les  Extraction rules are used to extract arbitrary patterns of text according to a grammar specification.
W00-1201@@This increase has of course been due to their proven efficacy in many tasks, but also to their engineering effiCacy. Our variant adds another set of parameters: 3 ~2 (O~2) S l ~. tight limit was changed to e -9.
W00-1202@@ Tagging task, which adds lexical, syntactic or semantic information to raw text, makes materials more valuable. % the Firs t-n and E M  High Middle and High 31. ,  Middle and High 36. For N and V, there are some high ambiguous words.
W00-1203@@ The occurrences of unknown words cause difficulties in natural language processing. E~ERIME~ ~S~TS The knowledge xtraction processes for Chinese org~t ion  names are carried out by different stages. In practice, the threshold value n was set to 2.
W00-1207@@ In this paper, new words refer to newly coined words, occasional words and other rarely used words that are neither found in the dictionary of a natural language processing system nor recognized by the derivational rules or proper name identification rules of the system. Lua, K T. Experiments on the use of bigram mutual information i Chinese natural language processing. A two-character verb, for example, can have a V-V, V-N, V-N or A(dv)-V internal structure.
W00-1208@@  The comparison of the grammars extracted from annotated corpora (ie, Treebanks) is important on both theoretical and engineering grounds. their heads, and the positions of modifiers w.r.t, their modifiees. To achieve that, we first create a new tagset that includes all the tags 2If a template occurs n times in the corpus, it is counted as one template type but n template tokens.
W00-1209@@the most difficult problems in NLP. \[31 In this method, a new knowledge base. And then in the following decades researchers adopted many methods to solve the problem of automatic word sense disambiguation, including:l) AI-based method, 2) knowledgebased method and 3) corpus-based method.
W00-1210@@ Entity names and their relations form the main content of a document. Both the previous word -,~,~.~:.~.~e~ ~:rm,, and the next word "$.,k~" are positive evidences that make it certain that ""~!~..~_" shduld be classified as a person name, therefore our algorithm correctly extracted it from this sentence. The task i s  to extract the word sequences that are person name components.
W00-1212@@However, very limited work has been done with Chinese. For instance: vhv r,rl (\[i igCN  V\] NP) Chinese linguistics literature insists that those words are verbs, and should be marked as "V  ,  regardless of what context hey are in. For all block k after blocks ,builds dependency relations of ( block k , blocks ,COMP),( block k , b lock i ,OBJ l), ( block k , block~ ,OBJ2), etc.
W00-1213@@ Corpora are essential resources to any research in language ngineering. In "Universals in Linguistic Theory", E. Bach, R. Harms, eds., New York, Holt, Rinehart and Winston. The function of the part " t : l~S I~"  is the heart of the whole computer.
W00-1219@@ Almost all techniques to statistical anguage processing, including speech recognition, machine translation and information retrieval, are based on words. For example, ~,~-~-~t~J(missi le defense plan) is a complete word, and -~.~0-1~J~ (missile defense) isnot, although both have relatively high value of mutual information. "Identification of Unknown Words From a Corpus".
W00-1301@@ Many natural language tasks are essentially nway classification problems, where classification decisions are made from a small set of choices, based upon the linguistic context in which the ambiguity site occurs. I In all of our formulations, we ignore expressions denoting the empty set O and the set {e}. Let R be the RRE obtained by following the edges from the root to S, outputting each edge label as the edge is traversed.
W00-1302@@  When writing an article, one does not normally go straight to presenting the innovative scientific claim. In Robert Norman Oddy, Stephen E. Robertson, Cornelis Joost van Pdjsbergen, and P. W. Williams, eds., Information Retrieval Research, 172-191. In Inderjeet Mani and Mark T. Maybury, eds., Advances in Automatic Text Summarization, 155-171.
W00-1303@@  Dependency structure analysis has been recognized as a basic technique in Japanese sentence analysis, and a number of studies have been proposed for years. fit is an n dimensional feature vector that represents various kinds of linguistic features related with the chunks bi and b t. We obtain Dbest aking into all the combination of these probabilities. Experiments with a new Boosting algoritm.
W00-1304@@  In natural language processing, a great amount of work has gone into the development of machine learning algorithms which extract useful linguistic information from resources such as dictionaries, newswire feeds, manually annotated corpora and web pages. L. Mangu and E. Brill. I. Dagan and S. Engelson.
W00-1305@@  -:We consider here the issue of topic analysis, by which is determined a texts topic structure, which indicates what topics are included in a text and how topics change within the text. T. M. Cover and J. John Wiley & Sons Inc., New York. Let w m" be the sequence of all wis (wi E w rn) such that its corresponding si is 1, where ms denotes the number of l s in s ~.
W00-1306@@ :  Many learning problems in the domain of natural anguage processing need supervised training. Therefore, we define the evaluation function, fte(s, G) to be the tree entropy divided by the sentence l ngth. When the training corpus consists of a larg e reservoir of fully annotated parse trees, it is possible to directly extract a grammar based on these parse trees.
W00-1307@@  There are various grammar frameworks proposed for natural languages. It then checks each node e on the path. In total, there are at least 2 n decompositions of the ttree.
W00-1308@@ I There are now numerous systems for automatic assignment of parts of speech ("tagging"), employing many different machine learning methods. TnT A Statistical Part-ofSpeech Tagger. However, in the maximum entropy framework it is possible to easily define and incorporate much more complex statistics, not restricted to n-gram sequences.
W00-1312@@ Cross-language information retrieval (CLIR) can serve both those users with a smattering of knowledge of other languages and also those fluent in them. For the user who is fluent in two or more languages, even though e/she may be able to formulate good queries in each of the source languages, CLIR relieves the user from having to do so. If a Chinese word c has n translations el, e2, ...en.
W00-1314@@ With the easier access to bilingual corpora, there is a tendency in NLP community to process and refine the bilingual corpora, which can serve as the knowledge base in support of many NLP applications, such as automatic or human-aid translation, multilingual terminology and lexicography, multilingual information retrieval system, etc. We have available a synonymy Chinese dictionary, also. System Architecture English corpus Chunk L Identifying ~ lSentence  Aligned ingual Corp~ !
W00-1315@@  An empirical method for estimating term weights directly from relevance judgements is proposed. The performance of query expansion (fit-E) is particularly encouraging. A is estimated separately for each bin and each t f  value, based on the labeled relevance judgements.
W00-1316@@  The advent of the Internet has resulted in a massive information explosion. Mr. Robin then wrote a book. The value of this feature DMWM for the questionsentence pair q and s is M rn.
W00-1317@@  We use the term semantic parsing to refer to the process of mapping a natural anguage sentence to a structured meaning representation. CHILL has also been applied to a restaurant database in which the logical form resembles SQL, and is translated 133 Damba~ QUERY YOU PO~TED: all a goo~ ~ z~caL~ ~m ~o ~.t~oP RE~UI.T: ~ o o a ~ e ~  I,~ p .~. 7 Ack~nowledgements  This research was supported by a grant from the Daimler-Chrysler Research and Technology Center and by the National Science Foundation under grant n~I-9704943.
W00-1320@@f rom examples  Consider the following examples: 1. E.g., U.S.-Soviet gets the word sense of country_2 (a state or nation). The "out-of-the-box" BBN program uses values of-5 and 25 for k and n, respectively.
W00-1321@@ Long sentence analysis has been a critical problem in machine translation because of high complexity. Finding p. E C that maximizes H(p) is a problem in constrained optimization, which cannot be explicitly written in general. Our work is motivated by the fact that parsing becomes more efficient, if n becomes horter.
W00-1322@@  Word Sense Disambiguation (WSD) is the problem of assigning the appropriate meaning (sense) to a given word in a text or discourse. R. Mihalcea and I. Moldovan. 2 Learn ing  A lgor i thms Tested  2.
W00-1326@@These hypotheses were shown to hold for some particular corpora (totaling 380 Mwords) on words with 2-way ambiguity. Press: Editorial C. Press: Reviews (theatre, books, music, dance) D. Religion E. Skills and Hobbies F. Popular Lore G. Belles Lettres, Biography, Memoirs, etc. At least one of those words needs to be a content word.
W00-1327@@  Manual development of large subcategorised lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Briscoe, E. and Carroll, J. Generalised probabilistic Lt~ parsing for unificationbased grammars. Comlex syntax: building a computational lexicon.
W00-1401@@  For many applications in natural language generation (NLG), the range of linguistic expressions that must be generated is quite restricted, and a grammar for a surface realization component can be fully specified by hand. o Qual i ty:  How well-written is this sentence intermediate numeric values can also t)e chosen, but have no description associated with them.) A lexicalized Tree Adjoining Gralnmar for English.
W00-1402@@ Empirical methods are fundamental in any scientific endeavour to assess progress and to stimulate new research questions. House 3-17 has e reasonable location. Young, M. R. Using Grices McLrim of Quantity to Selecr the Corrtent o f  Plan Descriptions: Artificial Intelligence Journal, to appear.
W00-1403@@These strengths concern the reuse of knowledge, the support for early drafts in several anguages, the support for maintaining consistency when making changes, the support for producing alternative formulations, and the potential for producing higher quality outputs than machine translation. (The Ministry ~ (popu alien X -E~. Some of the differences at the sentence level can be explained by differences between the syntactic structures of Japanese 20 Level Sentence Paragraph Text Weighted Average All .Sentence Paragraph Text Weighted Average All Units P-D R P-D P 29.
W00-1404@@This language is based on grammatical specifications, called DTDs, which are roughly similar to context-free grammars 1 with an arbitrary number of non-terminals and exactly one predefined terminal called pcdata. Fernando C. N. Pereira and David H. D. Warren. the (ollowing French andEng l i sh  gi-annmkrs a/e pai~allel to the  : "  previous abstract  g rammar : l  This view of real ization is essentially the one we have adopted in the prototype at the t ime of writing, with some straighforward addit ions permit t ing  the handl ing of agreement constraints and morphological variants.
W00-1407@@ Arguing involves an intentional communicative act that attempts to create, change or reinforce the beliefs and attitudes of another person. (e) Addressing and ordering the counterarguments (opposing evidence) There ........  mention any counterarguments, to acknowledge them without directly refuting them, to acknowledge them and directly refuting them. All of  these values are expressed as a number in the interval \[0, i \].
W00-1408@@  During argumentation, people persuade their audience using a variety of strategies, eg, hypothetical reasoning, reasoning by cases and premise to goal. This explosion may have been:caused.b~.a tar.ge.iridium~rich~astexoid~.~trikiag~.E~.th~65,,million.~years BC.~ ... ...... \ [  ~ r o i d  had not struck Earth 65 million years BC, there wouldnt be a I large explosion that up throws material. P r ~  to goaT: "65 million years ago, dinosaurs became xtinctand giant sequoias proliferated.
W00-1410@@ The RAGS project ~ aims to define a reference architecture for natural anguage generation systems. In this exercise it was important o choose a system that had been developed by people outside the RAGS project. The point of this exercise was to lem-n: 1.
W00-1411@@Issues in pronoun generat ion The appropriate r alisation of anaphoric expressions is a long-standing problem in NLG research. both hold; same center (o  r Cb(Un) undefined), realised as Subject in Un+l; RETAIN." CONTINUE e. Mike cancelled last weeks project meeting at short notice.
W00-1416@@ Natural anguage interaction lends itself to tasks like generalization, abstraction, comparison, and summarization which call for SETS of objects to be picked out using definite referring expressions. Generating Referring E~pressions: Constructing Descriptions in a Domain of Objects and Processes. :T.he cover semantics offers an elegant, and convenient, definition of what it means for the set distinguished in(4c) to be characterized asthe squares clustered somewhere.
W00-1422@@  Typically, a text realization system requires a great deal of syntactic information from an application in order to generate a high quality text; however, an application might not have this information (unless it has been built with text generation in mind). A finite set of at t r ibutes  is associated with each non-terminal  symbol. ture for a given plan.
W00-1423@@ When we are face-to-face with another human, no matter what our language, cultural background, or age, we virtually all use our faces and hands as an integral part of our dialogue with others. In REAS dialogue, open questions are always general questions about some entity raised by a recent urn; although in principle such an open question ought o be formalized as theme(XP.Pe), REA can use the simpler theme(e). In S. Weitz, editor, Nonverbal Communication.
W00-1424@@ :  Vague proper t ies  and  Gradab le  Ad jec t ives  Some properties can apply to an object to a greater or lesser degree. R. Dale and E. Reiter. "A Grammar of Contemporary English".
W00-1426@@Text planning (or more broadly, document planning) can be divided into two stages. tation should already.imply a linear order. Most researchers have followed Scott and Souza in assuming that linear order should be left unspecified; i t  is during the transition to the document representation that the material is distributed among linguistic units (or perhaps diagrams, in a multimedia document) arranged in a specific order.
W00-1427@@  Most approaches to natural anguage generation (NLG) ignore morphological variation during word choice, postponing the computation of the actual word forms to be output to a final stage, sometimes termed linearisation. 8% with respect o the 14,825,661 relevant .tokens .i.mthe BNC . In Pwceedings of the 9th Conference of th, e European Chapter of the Association for Computational Ling.uistics (EACL), Bergen, Norway.
W00-1428@@  Natural language generation requires lexical, syntactic, and semantic knowledge in order to produce meaningful and fluent output. The lexical chooser links conceptual e ements to lexical items. I. Langkilde and K. Knight.
W00-1434@@  This paper describes the RSTTool, a graphical interface for marking up the structure of text. ..... he author hereby gra, t permission to use, copy, modify, ~d is t r ibute ,  and license this software and t6  documentation for ~ a n y  purpose, ~ provided that existing copyright notices are ~ ~reta ined  in all copies; and that this notice is iscluded verbat im ~ | i n  any distributions.~ No written agree~aent, license, or royalty  fee is required for any of the authorized use~s.i ~.~,~,~,~,~IModtflcations . A boundary marker is inserted.
W01-0502@@A large number of important natural language infer-ences can be viewed as problems of resolving ambi-guity, either semantic or syntactic, based on proper-ties of the surrounding context. Simple classifier are usedsince they are more likely to be accurate; they arechosen so that, with high probability (w.h.p. It worksby sequentially applying simpler classifiers, eachof which outputs a probability distribution over thecandidate labels.
W01-0507@@The task of text categorization has been exten-sively studied in Natural Language Processing.Most successful works rely on a large numberof classied data. Unsuper-vised Topic Separation and Keyword Identi-cation in Document Collections: A ProjectionApproach Technical Report.Kolenda, T, Hansen, L., K. and Sigurdsson, S. 000. This leads to a relatively small com-putational overhead.
W01-0511@@We are exploring empirical methods of determin-ing semantic relationships between constituents innatural language. In I.l Sag and A. Szabolsci, editors, Lex-ical Matters. AAAI Press / MIT Press.Thomas Rindflesch, Lorraine Tanabe, John N. We-instein, and Lawrence Hunter.
W01-0512@@Morphological analysis is one of the basic tech-niques used in Japanese sentence analysis. Weimplemented this model within an M.E. modelcorresponds to consulting a dictionary constructed fromthe training corpus.
W01-0513@@A multiword unit (MWU) is a connectedcollocation: a sequence of neighboring wordswhose exact and unambiguous meaning orconnotation cannot be derived from the meaning orconnotation of its components Inother words, MWUs are typically non-compositionalat some linguistic level. For example, suppose X isbachelor__s_degree. This suggests that in a language withMWUs, we do show modest performance gains.
W01-0516@@Many organizations have a large number ofon-line documents -such as manuals, technicalreports, transcriptions of customer service callsor telephone conferences, and electronic mail --which contain information of great potentialvalue. Aformation method represents how a character (orcharacters) in the word takes part in theabbreviation.We have defined five kinds of formationmethods:  F, I, L, E meansthat the first character of a word occurs in theabbreviation. TermExtraction using a Similarity-based Approach.
W01-0521@@The past several years have seen great progress inthe eld of natural language parsing, through the useof statistical methods trained using large corpora ofhand-parsed training data. In the case of n-gram lan-guage modeling, e is the next word to be predicted,and the conditioning events are the n  1 precedingwords. BO is the back-o set of events for which no data are present in thespecic distribution P1.
W01-0703@@ Previous literature on selectional preference has usually learned preferences for words in the form of classes, eg, the object of eat is an edible entity. In the case  whole documents, they were withdrawn from e training corpus and tested in turn. Incorrect Semcor tag B. Wrongly extracted verb-object relations C. Scarcity of data D. Misleading verb senses The class-to-class model should help to mitigate the effects of errors type C and D. We would specially hope for the class-to-class model to discard misleading verb senses.
W01-0706@@Shallow parsing is studied as an alternative tofull-sentence parsing. A learning approach to shallow parsing. Under thisassumption, the full parser tries to make it a com-plete sentence and decides that processing O O4 ConclusionFull parsing and shallow parsing are two differentstrategies for parsing natural languages.
W01-0713@@In this paper I present an algorithm using con-text distribution clustering (CDC) for the un-supervised induction of stochastic context-freegrammars (SCFGs) from tagged text. Each of these distributions will havezero mutual information, and the mutual informa-tion of the linear combination will be less thanor equal to the entropy of the variable combiningthem, MN	G .In particular if we haveK	O+4PQ$H+:RSD+UTSVFWVYXEV	PZ[V	R\ (4)using Jensens inequality we can prove that]	_^$9`aT b.WVficedgfWV(5)We will have equality when the context distribu-tions are sufficiently distinct. It consists primarilyof short questions and imperatives, and many se-quences of letters and numbers such as T W A, AP 5 7 and so on.For instance, a simple sentence like Show methe meal has the gold standard parse:(S (VP (VB Show)(NP (PRP me))(NP (DT the)(NN meal))))and is parsed by this algorithm as(ROOT (VVB Show)(PNP me)(NP (AT0 the)(NN1 meal)))According to this evaluation scheme its recallis only 33%, because of the presence of the non-branching rules, though intuitively it has correctlyidentified the bracketing.
W01-0720@@Computational learning of natural language canbe considered from two common perspectives.Firstly, there is the psychological perspective,which leads to the investigation of learning prob-lems similar to those faced by people and thebuilding of systems that seek to model human lan-guage learning faculties. Below we describethe basic (AB) CG.There is a set of atomic categories in CG, whichare usually nouns (n), noun phrases (np) and sen-tences (s). Cambridge University Press.Catherine E. Snow and Charles A. Ferguson, editors.
W01-0802@@This paper addresses the problem of contentdetermination in data summarisation. Below is an excerpt from one of thetranscripts of a KA session : The first thing I have got to do is to readthrough the questionaire just to get some idea ofwhere he is at with his smoking. Humans form a qualitative overview of theinput data set.
W01-0803@@(1)1As far as we know, Roussarie is the first author who hasadopted SDRT for text generation. (5) N If R and R3S are DRS the main eventuali-ties of which are not states,N and if the main event of R occurs beforethe main event of R S ,N then Narration**Sis a valid relation,where * and *Srespectively label R andR3S .This paper is organized as follows. Mary burst into a fit of tears.
W01-0805@@As a result, there aremany different algorithms for the generation ofreferring expressions, each with its own object-ives: some aim at producing the shortest possibledescription, others focus on efficiency or realisticoutput. As a first approxima-tion, we could define the costs of adding an edgec e thatoc-curs in a distinguishing description (estimated bycounting occurrences):cei eeThus, properties which occur frequently arecheap, properties which are relatively rare areexpensive. The graphperspective has a number of attractive proper-ties.
W01-0812@@This paper reports on new work in that direction, but with an emphasis on reusing resources originally produced for analysis purposes. Bangalore S. and Rambow O. In Reversible Grammar in Natural Language Processing, Strzalkowski T., ed., Kluwer Academic Publishers, Dordrecht, The Netherlands.
W01-0813@@Automatic summarization techniques havemostly neglected the indicative summary, whichcharacterizes what the documents are about. The corpuscontained 82 summaries, averaging a short 2. sentences per summary. For example, considerthe document-derived feature, length: if a doc-ument in the set to be summarized is of signifi-cantly short length, this fact should be brought tothe users attention.We determine a document features normvalue(s) based on all similar documents in the cor-pus collection.
W01-0814@@In NLP tasks such as translation, summarizationand text generation, where a system producestexts as its output, the cohesiveness of output textsis an important criterion for assessing the sys-tems performance. They would also like to thankthe reviewers for their helpful and encouragingcomments.ReferencesCristea, D., Ide, N., Marcu, D., and Tablan, V. Anempirical investigation of the relation between dis-course structure and co-reference. A pair of passages (1t. )
W01-0902@@In evaluating the performance of dialog systems,designers face a number of complicated issues.On the one hand, dialog systems are ultimatelycreated for the user, so usability factors such assatisfaction or likelihood of future use should bethe final criteria. Non-participants may resort to average marginal costto optimize their own expenditure.ReferencesBell, D. E., Raiffa, H., & Tversky, A. PARADISE: A framework for evaluating
W01-0904@@Annotated corpora have become a vital tool forNatural Language Processing (NLP) systems, asthey provide both a standard against which resultscan be evaluated and a resource from which to ex-tract linguistic information eg lexicons. Building a large annotatedcorpus of English: the Penn Treebank. Its syn-tactic role can be that of a simple noun (n) or anoun phrase (np), so we need a mechanism forchoosing between these two possibilities.The most obvious mechanism is to use the sur-rounding subtree to provide the context to selectthe correct rule.
W01-1009@@A tremendous amount of heterogenous informa-tion exists in electronic format (the most promi-nent example being the World Wide Web), but thepotential of this large body of knowledge remainsunrealized due to the lack of an eective informa-tion access method. English Verb Classes and Al-ternations: A Preliminary Investigation. The Open Source move-ment rst demonstrated the eectiveness and sus-tainability of programming computer systems ina distributed manner.
W01-1011@@ The volume of email messages is huge and growing. Whittaker, S. and Sidner, C. Email overload: Exploring personal information management of email. This combined method allows us to generate an informative, generic, at-a-glance In this paper, we show: (a) the efficiency of the linguistic approach for phrase extraction in comparing results with and without filtering techniques,  (b) the usefulness of vector representation in determining proper features to identify contentful information, (c) the benefit of using a new measure of TF*IDF for the noun phrase and its constituents, (d) the power of machine learning systems in evaluating several classifiers in order to select the one performing the best for this task.
W01-1013@@Modern Information Technologies are facedwith the problem of selecting, filtering andmanaging growing amounts of multilingualinformation to which access is usually criti-cal. Available at http://www.saic.com.M.T. It results in a very complex (but realis-tic) NLP architecture.
W01-1202@@Information Retrieval (IR) systems have been applied successfully to a large scale of search area in which indexing and searching speed is important. ), N OP Q R (Yahoo Korea) has the higher score than S T U (service) because it has much more strong clue to www.yahoo.co.kr. We call the score a term score.
W01-1203@@There has recently been a strong increase in the re-search of question answering, which identifies and ex-tracts answers from a large collection of text. Here is an example of a(slightly simplified) question parse tree. Question 2 has a complex Qtarget,giving first preference to a date or a temporal locationwith a year and second preference to a generaltemporal location, such as six years after she wasfirst elected to the House of Representatives.
W01-1301@@This paper describes a prototype system to visu-alize and animate a 3D scene from a written de-scription. AT&T Research Lab.C. Artificial Intelligence, a NewSynthesis.
W01-1305@@Information Extraction (IE) is an upcoming chal-lenging research area to cope with the increas-ing volume of unwieldy distributed information re-sources, such as information over WWW. Impact Coecients of TemporalIndicators (R0)The combined eect of all the temporal indica-tors in a sentence determines its temporal rela-tion. Only the adverb t, determinesthe temporal reference of the event described bythe verb.
W01-1306@@In this paper, Portuguese and English  Nevertheless, theadvocated hypotheses are expected to apply tocomparable expressions in other languages aswell. Four of suchcontexts are described as follows: (i) argumentsof temporal nominal predicates like period,year, or month  or of non-temporalpredicates (like terrible or disaster) representingproperties that can be predicated of timestretches  (8d-e); (iv)complements of temporal prepositions like sinceor until: The elections are scheduled for (*in)June.e. Among thesearguments, I underline the possibility ofanaphoric reference to the intervals denoted (inadverbial or non-adverbial contexts)8:(24) a.
W01-1309@@This paper describes a semantic tagging sys-tem that extracts temporal information from newsmessages. The expressed tempo-ral relation is represented by a PROLOG list(ie [incl,[E,T]]). The overallperformance showed a precision and recall rateof 84.
W01-1313@@noticed that narratives1 areabout more than one event and these events aretemporally ordered. We use a set ofrules to perform this selection. When I left early this morning,2.
W01-1315@@In interpreting narratives the most essentialinformation to be extracted is who did whatwhere, when and why, the classic journalisticimperatives. A General Model ofAction and Time. This is accomplished in accordancewith the following conventions:(i) temporal relations are encoded withdirected secondary edges Annotators are instructed to annotate thesentences as they naturally understand them.When the treebank is made up of a sequence ofconnected text, the annotators are encouraged tomake use of contextual information.The annotation scheme is simple, explicit andtheory neutral.
W01-1401@@ Knowledge acquisition from corpora is viable for machine translation. (1-e) I do not like the color. A Statistical Approach to Machine Translation.
W01-1402@@Commercially available machine translation(MT) systems have long been limited in theircost effectiveness and overall utility by the needfor domain customization. Integrating translations frommultiple sources within the Pangloss MarkIII machine translation system, In R. Dale, H. Moisl and H.Somers (eds) Handbook of Natural LanguageProcessing. Surprisingly, themeans that our comparisons have a certainasymmetry.
W01-1405@@The use of statistics in computational linguisticshas been extremely controversial for more thanthree decades. The speech recognizersused had a word error rate of about 25%. To classifyan observation vector y into one out of severalclasses c, the Bayes decision rule is:c Due to this factorization, we havetwo separate probability distributions which canbe modelled and trained independently of eachother.Fig.
W01-1406@@A machine translation system requires asubstantial amount of translation knowledgetypically embodied in bilingual dictionaries,transfer rules, example bases, or a statisticalmodel. A set of alignment grammarrules licenses only linguistically meaningfulalignments. Translation + Parent: A source node S anda target node T that have a lexicalcorrespondence, such that a parent Ps of Shas already been aligned to a parent Pt of T.Align S and T to each other.
W01-1407@@The statistical approach to machine translationhas become widely accepted in the last few years.It has been successfully applied to realistic tasksin various national and international research pro-grams. is a verb in the indicative present firstperson plural form. In Pro-ceedings of the 36th Annual Meeting of the Associa-tion for Computational Linguistics and the 17th In-ternational Conference on Computational Linguis-tics, pages 960967, Montreal, P.Q., Canada, Au-gust.Sonja Nieen, Franz Josef Och, Gregor Leusch, andHermann Ney.
W01-1408@@The goal of machine translation is the transla-tion of a text given in some source language intoa target language. For the sentences of lengths 12 and 14,we performed an A* search (E+) with 2, 4 and8 million possible hypotheses. A comparison of alignmentmodels for statistical machine translation.
W01-1409@@Crises and disasters frequently attract international at-tention to regions of the world that have previouslybeen largely ignored by the international community.While it is possible to stock up on emergency reliefsupplies and, for the worst case, weapons, regardlessof where exactly they are eventually going to be used,this cannot be done with multilingual information pro-cessing technology. Test subjects were asked to identify the document(s)containing the answers to 15 lead questions. Statistical machine translation.Final report, Center for Language and Speech Pro-cessing, John Hopkins University.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
W01-1411@@This paper is a report on work in progress aimedat learning word translation relationships auto-matically from parallel bilingual corpora. Automatic evaluationand uniform filter cascades for inucing N -best translation lexicons. Learning an English-Chinese lexicon from a parallel corpus.
W01-1505@@In recent years there has been a growing inter-est in the commercial deployment of NLP tech-nologies and in infrastructures for sharing NLPtools and resources. First, they store the sta-tus of a user session. 0 Finally, s/he composes them into aproject.
W01-1506@@Language technology and the linguistic sciencesare confronted with a vast array of languageresources, richly structured, large and diverse.Multiple communities depend on languageresources, including linguists, engineers,teachers and actual speakers. (A live demonstrationaccompanies this presentation. OLAC-OS: A vocabulary for identifying theoperating system(s) for which the softwareis available: Unix, MacOS, OS2, MSDOS,MSWindows.
W01-1507@@ISLE stands forInternational Standards for LanguageEngineering, and is carried out in collaborationbetween American and European groups in theframework of the EU-US International ResearchCo-operation, supported by NSF and EC. ), i) to allowfor agreement on a minimal level of specificityespecially in cases where we cannot reach wideragreement, and/or ii) enable mappability andcomparability of different lexicons, withdifferent granularity, at the minimal commonlevel of specificity (or maximal generality). Results will be widely disseminated andpublished, after due validation in collaborationwith EU and US HLT R&D projects, Nationalprojects, and industry.
W01-1508@@We succeeded in reaching a consensus within arepresentative part of the linguistic communityin Europe about a standard for such metadatadescriptions. This would be a logical extension tothe local configurable mapping of tools onresource types that is needed anyway for non-networked situations.References[1] http://www.mpi.nl/ISLE[2] P. Wittenburg, D. Broeder & B. Sloman:Meta-Descriptions for Language ResourcesEAGLES/ISLE A Proposal for a Meta-Description Standard for LanguageResources.http://www.mpi.nl/ISLE/documents/papers/white_paper_11.pdf. The relevant elements forcharacterising the resources in a way that isimportant to tools (a discussion that we willcome to later) are:o Reference to the resource itselfo Size (of media file, if the tool has alimit)o Format (for media files somewhat moresimple then for annotation units)o Type (for annotation unit the type ofanalysis result eg morphology,phonetics  For IMDI theneeds of the creators are the start and end pointsince the creators are also the major consumergroup of language resources.
W01-1511@@Applications using annotated corpora are often,by design, limited by the information found inthose corpora. InGLARF, conjoined phrases are explicitly markedwith the attribute value (CONJOINED T). Nomlex: A lexicon of nominal-izations.
W01-1514@@ Despite different methodologies, goals and traditions, researchers in a variety of specialties in linguistics and computational linguistics share a core of assumptions and needs. We can illustrate the advantages of AG with a example of the annotation of the Switchboard corpus for t/d deletion. A single utterance is written:  274.
W01-1515@@In the past, standardized file formats and codingpractices have greatly facilitated data sharing andsoftware reuse. A file I/O libraryA file I/O library (AG-FIO) supports input andoutput of AG data to existing formats. A formalframework for linguistic annotation.
W01-1603@@In dialogue research communities, the need ofdialogue corpora with various level of anno-tation is recognized. (Are you mentioning the seminar room )[1: small-sized meeting room: clarification]39 B: heya wa shou-kaigishitsu wa aite masu ka (Can I use the small-sized meeting room )40 A: {F to} kayoubi no {F e} 14 ji han karawa {F e} shou-kaigisitsu wa aite imasen(The small meeting room is not availablefrom 14:30 on Tuesday. RelevanceDialogue act tag can be regarded as a functionof utterance.
W01-1605@@The advent of large-scale collections ofannotated data has marked a paradigm shift inthe research community for natural languageprocessing. In Corpus annotation:Linguistic information from computer textcorpora, edited by R. Garside, G. Leech, and T.McEnery. % growth of Xerox Corp.s third-quarter net income on 7.% higher revenueearned mixed reviews from Wall Streetanalysts.
W01-1607@@In our initial experiments comparing human-human (HH) and human-computer (HC) inter-action we have annotated dialogues from the airtravel domain with several sets of tags: dialogueact, initiative and unsolicited information. A. Andernach, S. P. van de Burgt, and G. F.van der Hoeven, editors, Proceedings of the TwenteWorkshop on Language Technology: Corpus-basedapproaches to dialogue modelling, Enschede, TheNetherlands. In the HC dialogues, sys-Site Expert User TotalHC A 3 (0.
W01-1609@@Shipboard damage control refers to the task ofcontaining the effects of fire, explosions, hullbreaches, flooding, and other critical eventsthat can occur aboard Naval vessels. A multi-modal dialogue system forhuman-robot conversation. In Proceedings of theARPA Workshop on Human Language Technol-ogy.O.
W01-1611@@When describing the desired characteristics ofhuman-computer interaction, the common key-words are cooperation and naturalness. Predict-ing and adapting to poor speech recognitionin a spoken dialogue system. 0pm(c) The next bus to Malmi leaves at2.
W01-1612@@Anaphoric and bridging relations between discourseentities are of major importance for establishing andmaintaining textual coherence. Since there is no con-straint as to how many different markables can pointto another one, n:1 relations can be represented. ClinkA a coreferential links annotator.
W01-1616@@Clarication requests (CRs) are common inhuman conversation. (6)3Cassie: You did get o with himCatherine: Twice, but it was totallynon-existent kissing soCassie: What do you meanCatherine: I was sort of falling asleep. SCoRE: A tool for search-ing the BNC.
W01-1623@@Concretely, they provide real phonetic data and empirical data-driven knowledge on linguistic features of spoken language. However, they were told that the person who asked route questions had to make sure that s/he has completely understood the described routes. Each subject was recorded on a separate channel on a DAT tape.
W02-0101@@ Experience is the best teacher, as proverbial wisdom tells us. Fairfield, IA: WFF N PROOF Learning Games Associates. A Grammar of Contemporary English.
W02-0103@@Feature structures have been used prolifically at ev-ery level of linguistic theory, and they form themathematical foundation of our most comprehen-sive and rigorous schools of syntactic theory, includ-ing Lexical-Functional Grammar and Head-drivenPhrase Structure Grammar. The three main functions of the MoMo toolallow one to check (1) whether a feature structurecomplies with a given signature, (2) whether a well-formed feature structure satisfies a description or aset of descriptions, and (3) whether a well-formedfeature structure is a model of a description or a setof descriptions. 0 It offers hands-on ex-perience to linguists interested in the formalizationof linguistic knowledge in a constraint-based gram-mar formalism.
W02-0104@@How The Program Came AboutOur program is hosted by the Department ofComputing at Macquarie, which offers a typicalrange of computer science courses. With regard to the first of theseobservations, we take a strong position. Central to our proposalwas the identification of the twin streams of (a)spoken language interaction and (b) smart textprocessing, particularly with regard to the Web;we took the view that these two major areaswould define the future of commercial NLPactivities over the next five years.
W02-0105@@Algorithmic concepts and programming tech-niques from computer science are very useful toresearchers in natural language processing. By the end, I was approachedby four students asking if there were research op-portunities available in the topics we had cov-ered; interestingly, one of these students hadoriginally intended to major in electrical engi-neering. rebuttal that fluent lan-guage behavior is not a sufficient indication ofintelligence.
W02-0108@@When students learn programming, they havethe benefit of integrated development environ-ments, which support them throughout the en-tire application development process: from writ-ing the code, through testing, to documenta-tion. Universities will-ing to use GATE as a teaching tool will benefitfrom the comprehensive documentation, severaltutorials, and online demonstrations.ReferencesD.E. A Formal Frame-work for Linguistic Annotation.
W02-0109@@Teachers of introductory courses on compu-tational linguistics are often faced with thechallenge of setting up a practical programmingcomponent for student assignments andprojects. This work has foundwidespread pedagogical application.Other Researchers and Developers.A variety of toolkits have been created forresearch or R&D purposes. A number of considerationsinfluenced our choice.
W02-0111@@This paper is particularly addressed to readers at in-stitutions whose resources and organization rule outextensive formal course-work in natural languageprocessing (NLP). Specifically,I outline a simple and versatile lexicalized formal-ism for natural language syntax, semantics and prag-matics, called TAGLET, and draw on my experi-ence with CS 533 (NLP) at Rutgers to motivate thepotential role for TAGLET in a broad NLP classwhose emphasis is to introduce topics of current re-search. OverviewLike LTAG, TAGLET analyzes sentences as a com-plex of atomic elements combined by two kinds ofoperations, complementation and modification.
W02-0112@@ Computational Linguistics as a special subject can in Estonia only be studied at the University of Tartu (UT). The R&D at the University of Tartu has focused on the computer analyses of Estonian texts and compiling the text corpora of Estonian underlying that research. Quite a few of the present computational linguists received their first knowledge of CL from that group.
W02-0201@@More and more people are building dia-logue systems. (2)Address obligto AckSystem understood (1);CPS Act: evaluate-actionInterpretReplyIM RefTM BAGMNo, you are taking (0)U: Can I take an aspirin (a) Grounding Level,(b) Discourse Level, (c) and (d) Problem-Solving LevelThe back-end components report that theuser has a prescription for Celebrex, and thatCelebrex interacts with aspirin. Galescu, and A. Stent.
W02-0207@@ The complete understanding of naturally oc-curring discourse is still an unsolved task in computational linguistics. A Survey of Knowledge Sources in Dialogue Systems. The main task of intention recognition in SmartKom is to select the best hypothesis from the N-best list produced by the parser.
W02-0210@@The need for flexible interaction is apparent notonly in everyday computer use, but also in vari-ous situations and services where interactive sys-tems can diminish routine work on the part ofthe service provider, and also cater for the userswith fast and tailored access to digital infor-mation (call centers, help systems, interactivebanking and booking facilities, routing systems,information retrieval, etc. IEEE Transac-tions on Neural Networks, Special Issue on NeuralNetworks for Data Mining and Knowledge Discov-ery, 11(3):574585, May.T. Organizationof a massive document collection.
W02-0211@@Whereas most explanations are produced andadapted to benefit or inform a hearer, a self-explanation is produced for the benefit of thespeaker. In Proceedings of the First Meetingof the North American Chapter of the Association forComputational Linguistics.James D. Slotta, Michelene T.H. A neat theory of marker pass-ing.
W02-0214@@The analysis of the topic of a sentence or adocument is an important task for many nat-ural language applications. We approximate the proba-bility P (XN |S) to be equal for each map vec-tor inXN . Disregarding positioninformationFurthermore, to study the importance ofthe position information, we calculated theprobabilities using only ML estimates forP (T |W ), ie, disregarding the position of theword.. .
W02-0216@@This paper describes implemented multi-modal dia-logue systems1 which support collaboration with au-tonomous devices in their execution of multiple con-current tasks. Missiongoals are provided by a human operator, and an on-board planning system then responds. Report (Current Activity)O: Stop that.
W02-0217@@Probabilistic meth-ods look promising when modelling systems wherethere is uncertainty, and simple true/false judge-ments obscure some of the subtleties of represen-tation and processing that are required of an accu-rate model. In R. Cooper, K. Mukai, and J. Perry, editors, Situ-ation Theory and its Applications I. CSLI Publications.Prashant Parikh. A probabilistic approach to dia-logue act clarification.
W02-0218@@Since  the end of that work this term has been used in many other ways that are not technically related to the ISI usage. o 31    CCLooking at TD&E, you expended 65 poinds or Stand by 55 pounds, correction on that. Cooperation: A Philosophical Study.
W02-0221@@Thehypothesis underlying the use of dialogue act tag-ging in spoken dialogue evaluation is that a systemsdialogue behaviors have a strong effect on its usabil-ity. Spot:A trainable sentence planner. what city do you want to fly toU: chicagoA: on what dateU: on the twenty second of may there will be two peopletravellingA: what time do you need to departU: as soon as possible after five p.m.A: the first flight after five p.m. on that date is at five thiryfive p.m. arriving in chicago at six oh six p.m. on u.s. airU: is that ohareA: yes it isU: okay.
W02-0227@@Discourse interpretation is at the cornerstone ofhuman-computer communication, and an essentialcomponent of any dialogue system. Produces one    or more node "clusters"(c) Eliminate nodes that    arent in a sortest path(d) Candidates are all the subgraphs of (c) that connect the users nodes. A computational ar-chitecture for conversation.
W02-0301@@Application of natural language processing (NLP) isnow a key research topic in bioinformatics. 1 in11C, s, r, and d12Features that occur less than 10 times are removed.F-score. Thus, such a noise reduction method isalso important.
W02-0302@@ The application of large-scale genomics and proteomics technologies towards a wide variety of biological questions has resulted in a continuous stream of information regarding thousands of genes and gene products into the Medline database of biomedical abstracts. McCray, A.T., Srinivasan, S. and Browne, A. C. Lexical methods for managing variation in biomedical terminologies. References Blaschke, C. and Valencia, A.
W02-0303@@ Almost all current approaches to entity identification are actually not tackling the identification per se, but rather merely the (still difficult) location of named entities in text. For example, optional hyphenation heuristic allowed the official gene name alpha-2-macroglobulin to find a match in Moreover, C5a also enhanced transcription of the gene for the type-2 acute phase protein alpha 2-macroglobulin n HC indirectly by increasing LPS-dependent IL-6 release from KC. The first flaw is that it is computationally expensive, since it is a O(n2)-complex problem.
W02-0305@@ In the field of medical informatics, computerized tools are being developed that depend on databases of clinical information. A training case for a BN is a list of node / value assignments. The M+ Semantic Model 2.
W02-0307@@Language Technology (LT) resources are time-consuming and expensive to develop, and appli-cations rarely have the luxury of calling upon re-sources specially designed for the task at hand. So appearing to finda structure of interest by a tree-walk, in addition tobeing tedious, doesnt by itself guarantee the userthat s/he has found it all.In contrast, access by sub-string matching on in-dividual component terms has problems of both re-call and precision. Unsupervised, corpus-based method forextending a biomedical terminology.
W02-0308@@In other words, our objec-tive is to acquire hyponyms for terms in an originalvocabulary that appear in the literature but are notpresent in the original vocabulary. A secondary ob-jective of this evaluation was to gain insights abouthow these methods could be tuned in order to pre-vent inaccurate mappings and select the most use-ful candidate terms.The following classification was used to de-scribe the quality of the hyponymic relationshipbetween the candidate term and the Metathesaurusconcept(s) selected: relevant means that thehooking of the candidate term to the terminologywas relevant, even if a more specific concept wasavailable; non relevant means that none of theMetathesaurus concepts selected was a correcthook for the candidate term; more or less rele-vant means that the Metathesaurus concepts se-lected were not irrelevant as hooks, but weredistant ancestors, i. e., too general for the relation-ship to be fully informative. Creating demodified termsWhen adjectival modifiers are identified in a termO, a set of demodified terms {T1, T2,,Tn} is cre-ated by removing from term O any combinationsof adjectival modifiers found in it.
W02-0401@@Sentence extraction |the recovery of a given setof sentences from some document| is useful fortasks such as document summarisation (where theextracted sentences can form the basis of a summary)or question-answering (where the extracted sentencescan form the basis of an answer). Results are also presentedusing a at prior. Naive Bayes did not benet from afrequency-based cuto.
W02-0404@@ With the increasing availability of online news sources, interest in automatic summarization has continued to grow in recent years. As a result, the summary is unintelligi-ble. Explanations of the specific pragmatic concerns in each category, as well as their corresponding op-erator(s), are detailed in the appendix.
W02-0406@@DUC aims to compile standard training and test collections that can be shared among researchers and to provide common and large scale evaluations in single and multiple document summarization for their participants. Both of them report a 6. Only content words were used in forming n-grams.
W02-0506@@  Due to the morphological complexity of the Arabic language, Arabic morphology has become an integral part of many Arabic Information Retrieval (IR) systems. are a part of the stem template. For example, for the Arabic word  m  AymAn the possible prefixes are #,   A, and q  Ay, and the possible suffixes are  #,   n, and h  An.
W02-0602@@There are numerous languages for which no anno-tated corpora exist but for which there exists anabundance of unannotated orthographic text. The fscores for the Directed Search and Lin-guistica in English are very close, and the DirectedSearch appears to clearly outperform Linguistica inPolish.Suffixes Stems-a -e -ego -ej -ie -o -y dziwn% -a -ami -y -e The first paradigm shownis for the single adjective stem meaning strangewith numerous inflections for gender, number andcase, as well as one derivational suffix, -ie whichchanges it into an adverb, strangely. For example it is unclear where thebreak in the word, location is combined with thesuffix tion, but in terms of simple concatenationit is unclear if the break should be placed before orafter the t. is combined with thesuffix s, simple concatenation seems to work fine,though a different stem is found from location One solution is todevelop an evaluation technique which incorporatesthe adjustment or spelling change rules, such as theone that deletes the e when combiningwith tion.None of the systems being evaluated attempt tolearn adjustment rules, and thus it would be diffi-cult to analyze them using such a measure.
W02-0603@@According to linguistic theory, morphemes are con-sidered to be the smallest meaning-bearing ele-ments of language, and they can be defined in alanguage-independent manner. The training set is then used for estimat-ing the distance values d(M,L). For instance, accept the segmentationhalua + n (Engl.
W02-0605@@ This paper describes some results of our efforts to develop statistical techniques for unsupervised learning of syntactic word-behavior, with two specific goals: (1) the development of visualization tools displaying syntactic behavior of words, and (2) the development of quantitative techniques to test whether a given candidate set of words acts in a syntactically uniform way, in a given corpus. On the other hand, the suffix e appears as the last suffix in a syntactically heterogeneous set of words: nouns, verbs, and adjectives. Projection of nearest-neighbor graph by spectral decomposition In the canonical matrix representation of a (unweighted) graph, an entry M(i,j), with i distinct from j, is 1 if the graph includes an edge (i,j) and 0 otherwise.
W02-0606@@In recent years, there has been much interest in com-putational models that learn aspects of the morphol-ogy of a natural language from raw or structureddata. A Bayesian model formorpheme and paradigm identification. However, if it turned out that all of thesepairs were examples of the same morphological pat-tern (say, nominal plural formation in -s), the al-gorithm would not be of much use.
W02-0607@@In recent years, linguists have explored theoreticalmodels of how speakers discover the rules of theirlanguage. In D. E.Rumelhart, J. L. McClelland, and The PDP ResearchGroup, eds., Parallel Distributed Processing: Explo-rations in the Microstructure of Cognition: Vol. stands for a word boundary.
W02-0701@@Consequently, the translation quality is not good1 enough for S2S purposes. ;;; the English output  (1-e) I do not like the color. Lets review the process with a simple sample below.
W02-0703@@ Interlingua-based approaches to Machine Translation (MT) are highly attractive in systems that support a large number of languages. A neural network identified the concept sequences. : a Speech Translation System for E-commerce Applications.
W02-0706@@Present finite-state technology allows us to buildspeech-to-speech translation (ST) systems usingideas very similar to those of automatic speechrecognition (ASR). A Transla-tion Word Error Rate of 7. Follow-ing this idea, the search can be formulated as:Initialization:Let PrN ,t(s) be approximated by a source lan-guage model PrN (s).while not convergence1.
W02-0708@@Several speech translation projects have choseninterlingua-based approaches because of its con-venience (especially in adding new languages)in multi-lingual projects. Machine Translation: A Viewfrom the Lexicon. For example, the sentence I want totake a vacation has a predicate want with twoarguments I and to take a vacation, which inturn has a predicate take and two arguments, Iand a vacation.
W02-0712@@A speech-to-speech translation system must inte-grate at least three components  speech recogni-tion, machine translation, and speech synthesis. A conversa                                           Association for Computational Linguistics. When the user clicks thehonyaku (translate) (This is notwhat I ordered.
W02-0717@@Nespole!1 is a speech-to-speech machine tran-slation project designed to provide fully func-tional speech-to-speech capabilities within real-world settings of common users involved in e-commerce applications. { NEgotiation through SPOken Lan-guage in E-commerce. A key component in the Ne-spole!
W02-0718@@ In the last ten years, many projects addressed the speech to speech translation problem, S2ST, ie VERBMOBIL [1], C-STAR [2], NESPOLE! Most research groups are small and work only on some research themes, i.e prosody, acoustic model-ing, language modeling, speech synthesis. Such sentences feed a speech synthesizer.
W02-0801@@ This paper presents some preliminary experiments in the use of translation equivalences to disambiguate the interpretations of case suffixes in Basque. The triple contains a conjunction (X): these were tagged as incorrect. Ansa O., Arregi X., Lersundi M., A Conceptual Schema for a Basque Lexical-Semantic Framework TnT A Statistical Part-of-Speech Tagger.
W02-0802@@Prepositions have generally been viewed asfunction words to be discarded in many naturallanguage processing applications. Definitions of ofType Definition (Subsense(s))1. A comprehensive grammar of theEnglish language.
W02-0805@@Our goal is to characterize sense inventories, bothqualitatively and quantitatively, so that the followingquestions can be answered:  Given a pair of senses of the same word, arethey related If so, in what way and howclosely  How well are individual senses defined Should itbe split into subsenses  How do these issues affect the evaluation ofautomatic Word Sense Disambiguation (WSD)systems using the sense inventory Can the sense inventory be im-proved for evaluation purposes, for instance,splitting senses into finer-grained distinctionsor collapsing close senses into coarser clustersIn particular, we are interested in characterizingWordNet 1.  as sense inventory for the Senseval-2WSD comparative evaluation. Young Duffy was in S tag T fine S /tag T form whenhe defeated B. Valdimarsson of Iceland 402. A perspective on wordsense disambiguation methods and their evaluation.
W02-0806@@This paper presents a post-mortem analysis ofthe English and Spanish lexical sample tasks ofSENSEVAL-2. A coefficient of agreement for nominalscales. 9ehu all lesk corp 0.
W02-0807@@CL Researchs official submission for SENSEVAL-2used WordNet as the lexical inventory. Towards a Meaning-FullComparison of Lexical Resources. 8 at the fine-grained level and 0.
W02-0808@@It is well known that the most nagging issue forword sense disambiguation (WSD) is the definitionof just what a word sense is. Assessing Agreement onClassification Tasks: The Kappa Statistic.Computational Linguistics, 22:2, 249-254.Dagan, I. and Itai, A. sense assignments, we normalized thedata as follows: for each annotator and thealgorithm, each of the 33 words was represented asa vector of length n(n-1)/2, where n is the numberof occurrences of the word in the corpus.
W02-0809@@Much like syntactic word-class dis-ambiguation, it is not a end in itself, but rather a sub-task of other natural language processing tasks. Hendrickx and A. van den Bosch. Sense tags consist of the words lemma and asense description of one or two words (berg stapel )or a reference of the grammatical category (fiets N,fietsen V).
W02-0812@@There were eight Duluth systems that participatedin the English and Spanish lexical sample tasks ofSENSEVAL-2. A baseline methodology for wordsense disambiguation. Edmonds and S. Cotton, editors.
W02-0813@@Highly ambiguous words pose continuing problemsfor Natural Language Processing (NLP) applica-tions. an instancewith a closely related sense. whether there is a subject, direct object, indi-rect object, or clausal complement (a comple-ment whose node label is S in the parse tree)4. the words (if any) in the positions of subject,direct object, indirect object, particle, preposi-tional complement (and its object)5. a Named Entity tag (PERSON, ORGANIZA-TION, LOCATION) for proper nouns appear-ing in (4)6.
W02-0814@@The task of word sense disambiguation (WSD) isto assign a sense label to a word in context. E.g., the following is a training in-stance: American JJ history NN and CC mostmost JJS American JJ literature NN is VBZmost%3:00:01::. The second classifier in a word-expert is trainedwith information about possible disambiguat-ing content keywords in a context of three sen-tences (focus sentence and one sentence to theleft and to the right). :for every combination of a word form and a POS,WordNet1.
W02-0817@@Most of the efforts in the Word Sense Disam-biguation (WSD) field have concentrated on su-pervised learning algorithms. In Proceedings of the34th Annual Meeting of the Association for Com-putational Linguistics (ACL-96), Santa Cruz.H.T. Designing a task forSenseval-2, May.
W02-0902@@Recently, there has been a surge in researchin machine translation that is based on em-pirical methods. The number of possible mappingshas complexity O(n! Thismeasurement can be formalized as the numberof letters common in sequence between the twowords, divided by the length of the longer word.The example word pair friend and freundshares 5 letters (fr-e-nd), and both words havelength 6, hence there spelling similarity is 5/6, or0.
W02-0903@@The incompleteness of the available lexical re-sources is a major bottleneck in natural languageprocessing (NLP). One way to look at this setof classes is from the perspective of named-entityrecognition tasks, where there are a few classes ofa similar level of generality, e.g, PERSON, LOCA-TION, ORGANIZATION, OTHER. Adictionary1 in this context simply associates lexicalI would like to thank for their input everybody in the BrownLaboratory for Linguistic Information Processing (BLLIP) andInformation Retrieval and Machine Learning Group at Brown(IRML), and particularly Mark Johnson and Thomas Hofmann.I also thank Brian Roark and Jesse Hochstadt.
W02-0906@@ In recent years a considerable effort has been done on the acquisition of lexical information. Yesterday I talked with Mary. % over a baseline of 74%.
W02-0908@@In addition, thesaurus compilers cannotkeep up with constantly evolving language use andcannot afford to build new thesauri for the many sub-domains that NLP techniques are being applied to.There is a clear need for methods to extract thesauriautomatically or tools that assist in the manual cre-ation and updating of these semantic resources.Much of the existing work on thesaurus extractionand word clustering is based on the observation thatrelated terms will appear in similar contexts. However, what is needed issome algorithmic reduction that bounds the numberof full O(m) vector comparisons performed. term is modified by a noun or adjective4.
W02-0909@@The lexical choice processhas to choose between clusters of near-synonyms (toconvey the basic meaning), and then to choose be-tween the near-synonyms in each cluster. In R. Dale, H. Moisl, and H. Somers, edi-tors, Handbook of Natural Language Processing. I-Saurusdoes not include such knowledge.
W02-1001@@In response to these problems,they describe alternative parameter estimationmethods based on Conditional Markov RandomFields (CRFs). For both methods, we tried the algorithmswith feature count cut-os set at 0 and 5 (ie,we ran experiments with all features in trainingdata included, or with all features occurring 5times or more included { (Ratnaparkhi 96) usesa count cut-o of 5). Then the prob-ability (over the choice of all n + 1 examples) that thevoted-perceptron algorithm does not predict yn+1on in-put xn+1is at most2n + 1En+1minU; )22where En+1[] is an expected value taken over n + 1 ex-amples, R and DU;are as dened above, and the min istaken over  Data SetsWe ran experiments on two data sets: part-of-speech tagging on the Penn Wall Street Journaltreebank (Marcus et al 93), and base noun-phrase recognition on the data sets originally in-troduced by (Ramshaw and Marcus 95).
W02-1002@@The success and widespread adoption of probabilis-tic models in NLP has led to numerous variant meth-ods for any given task, and it can be difficult to tellwhat aspects of a system have led to its relative suc-cesses or failures. The graphicalmodel shown gives a joint distribution over (s, o),just like an HMM. and sense 2 was a phone line.
W02-1003@@We show a problem with thestandard algorithm for learning probabilistic deci-sion lists, and we introduce an incremental algorithmthat consistently works better. Let N(qi) be thenumber of non-zero ys for a given question. For instance, an actual question mightbe Is word before left Thus,the total number of questions can be very large, andrunning all the data through the possible new decisionlists for each question would be extremely slow.Fortunately, we can precompute entropyReduce(i)and incrementally update it.
W02-1004@@There are several reasons whyclassifier combination is useful. Daelemans, A. van den Bosch, and J. Zavrel. On one extreme, if the classifier outputs are0.0 0.
W02-1005@@word (in a dictionary or a morpholog-ical variant), typically ignoring context. R. Golding and D. Roth. .The contexts  are represented as a collectionof features.
W02-1006@@Natural language is inherently ambiguous. Thenine columns correspond to: (i) using only POSof neighboring words (ii) using only single wordsin the surrounding context with feature selection( , H-I. ) ,where denotes a null token.
W02-1008@@Noun phrase coreference resolution refers to theproblem of determining which noun phrases (NPs)refer to each real-world entity mentioned in a doc-ument. Relational features test whether some property P holds for the NP pair under consideration and indicate whetherthe NPs are COMPATIBLE or INCOMPATIBLE w.r.t. Aone and S. W. Bennett.
W02-1011@@Today, very large amounts of information are avail-able in on-line documents. A surveyof smoothing techniques for ME models. Okay, Im really ashamed of it, but I enjoyedit.
W02-1012@@The main task in statistical machine translation isto model the string translation probability   	 where the stringin one language is translatedinto another language as string. Basically, the new alignment probabilitiesflR	W R	T(ffstate that a jump width of zero de-pends on the English word. We use ,or vector notation e, f to de-note English and French strings.
W02-1013@@Such models are learned auto-matically from data, typically parallel corpora:texts in two or more languages that are mutualtranslations. V2),1I use the term text E such that each ver-tex has at most one edge in M , andeM ci,jis maximized. Ido not seek a solution to the generalized prob-lem here; one way of approximating it is by tak-ing the top k tsim-scored elements from the setM (the MWBM).If k is not known, it can be estimated (viasampling and human evaluation); I take the ap-proach of varying the estimate of k by applyinga threshold  on the tsim scores, then comput-ing precision and recall for those pairs in Mwhose score is above  T |k(5)where T is the set of k true translation pairs.Performance results are presented as (precision,recall) pairs as  (Tiesare broken at random.)
W02-1016@@But oneof the problems with multivariate clustering isthat it is something of a black art when appliedto high-dimensional natural language data. Possible arguments in the framesare nominative (n), dative (d) and accusative(a) noun phrases, reflexive pronouns (r), prepo-sitional phrases (p), expletive es (x), non-finiteclauses (i), finite clauses (s-2 for verb secondclauses, s-dass for dass-clauses, s-ob for ob-clauses, s-w for indirect wh-questions), and cop                                           Association for Computational Linguistics. , sn} in a highdimensional space.
W02-1017@@Syntactic structure helps us understand the seman-tic relationships between words. Deep Read: A reading comprehension sys-tem. Aone and S. W. Bennett.
W02-1020@@Text prediction can be seen as a form of in-teractive MT that is well suited to skilled transla-tors. Unlike the TransType prototype,which proposes a set of single-word (or single-unit)suggestions, we assume that each prediction consistsof only a single proposal, but one that may span anarbitrary number of words.As described above, the goal of the predictor isto find the prediction x that maximizes the expecteds: Let us return to serious matters.t:h s is the source sentence, h is thepart of its translation that has already been typed,x is what the translator wants to type, and x is theprediction.benefit to the user:x Thisobviously depends on how much of x is correct, andhow long it would take to edit it into the desired text.A major simplifying assumption we make is that theuser edits only by erasing wrong characters from theend of a proposal. A Maximum Entropy / MinimumDivergence translation model.
W02-1022@@One or two homologous sequences whisper . Extract-ing paraphrases from a parallel corpus. To do so, we need to supply a scoring functioncapturing the similarity in meaning between words.Since such similarity can be domain-dependent, weuse the data to induce  a paraphrase thesaurus T that lists linguis-tic items with similar meanings.
W02-1028@@These results suggest that auto-matic semantic lexicon acquisition could be used toenhance existing resources such as WordNet, or toproduce semantic lexicons for specialized domains.We have developed a weakly supervised bootstrap-ping algorithm called Basilisk that automaticallygenerates semantic lexicons. InProceedings of the Sixth Workshop on Very Large Cor-pora, pages 49{56.E. Deep Read: A reading comprehensionsystem.
W02-1029@@Ensemble learning is a machine learning techniquethat combines the output of several different classi-fiers with the goal of improving classification per-formance. Given n terms and up to m at-tributes for each term, the asymptotic time complex-ity of k-nearest-neighbour algorithm is O(n2m). term is modified by a noun or adjective4.
W02-1031@@The purpose of a language model (LM) is to de-termine the a priori probability of a word sequencew1; : : : ; wn, P (w1; : : : ; wn). Eec-tiveness of corpus-induced dependency grammarsfor post-processing speech. A new statistical parser basedon bigram lexical dependencies.
W02-1033@@The goal of a question answering system is to retrieve answers to questions rather than full documents or best-matching passages, as most information re-trieval systems currently do. 50 which represents a drop of 11. For example, "A B C" and "B C D" is tiled into "A B C D." The algorithm proceeds greedily from the top-scoring candidate all sub-sequent candidates (up to a certain cutoff) are checked to see if they can be tiled with the current candidate answer.
W02-1036@@The aim of system combination is tocombine portions of the individual systems outputswhich are partial but can be regarded as highly ac-curate. Bagging and boost-ing a treebank parser. C. Henderson and E. Brill.
W02-1039@@Statistical machine translation (SMT) seeks to de-velop mathematical models of the translation pro-cess whose parameters can be automatically esti-mated from a parallel corpus. Even in the most complex of1Though usually a simple word n-gram model is used for thelanguage model.the five IBM models, the reordering operation payslittle attention to context and none at all to higher-level syntactic structures. and has a span of [2,2].
W02-1111@@Most ofthese noun forms are common nouns (nouns describ-ing non-specific members of a general class, egdetective). This is a trendwhich requires further study. In Pro-ceedings of the 37th Annual Meeting of the Associationfor Computational Linguistics.N.
W02-1208@@Automatic word spacing is a process to de-cide correct boundaries between words in a sen-tence containing spacing errors. The proposedmodel can eectively solve the word spacingproblem by using only syllable statistics auto-matically extracted from raw corpora. The larger the values of K, J ,L, and I are, the more context can be consid-ered.
W02-1210@@ Natural language processing technology has recently reached a point where applications that rely on deep linguistic processing are becoming feasible. buy give-past The teacher bought me a book. In the above example, the inflectional rule changes the mu character to the n character and assigns the value nd-morph to the morphological feature RMORPH-BIND-TYPE.
W02-1303@@ With new applications, NLP sees new challenges and has to develop additional functionalities. This is much easier to do offline, of course, but there is also an increasing need in inconspicuous Raskin et alPage 3 interception and sanitizing of e-mail online. In other words, for each classified text T there must be generated a sanitized, downgraded text T, from which all sensitive data are removed according to a certain list of criteria.
W02-1304@@ The field of Information Society Technologies (IST) is one of the main thematic priorities of the European Commission for the 6th Framework programme. Agirre E., Lersundi M. and Martnez D. A Multilingual Approach to Disambiguate Prepositions and Case Suffixes. Blum A. and Mitchel T. Combining labelled and unlabeled data with co-training.
W02-1401@@Noun compounds are a frequently encounteredconstruction in natural language processing(NLP), consisting of a sequence of two or morenouns which together function syntactically as anoun. By retainingonly the k largest singular values1 and settingthe remaining smaller ones to zero, a new diag-onal matrix k is obtained; then the product ofUkV T is the m  n matrix Ak which is onlyapproximately equal to A. A. Evans and C. Zhai.
W02-1407@@ Automatic term recognition, ATR in short, aims at extracting domain specific terms from a corpus of a certain academic or technical domain. We assume the following about compound nouns or collocations:  Assumption   Terms having complex structure a e t  be made of xisting simple terms r o e The structure of complex terms is another important factor for automatic term candidates extraction. A Method of Measuring Term Representativeness.
W02-1411@@Paraphrasing research has attracted increasedattention, and the work in this field has becomemore active recently. Extracting paraphrases from a parallelcorpus. Each article is segmented and part-of-speech tagged by the morphological analyzerJUMAN1 and then parsed by the KNP2 parser.We then obtained a relation triplet (c1, r, c2)from each article, where a word c1depends ona word c2with the relation r. A complete listof r types and their examples is shown below: (U.S. army)In this list, rican be a particle, such as acase particle (r3) or an associative particle (r1).
W02-1502@@The past decade has seen the development ofwide-coverage implemented grammars represent-ing deep linguistic analysis of several languagesin several frameworks, including Head-DrivenPhrase Structure Grammar (HPSG), Lexical-Functional Grammar (LFG), and Lexicalized TreeAdjoining Grammar (LTAG). Further, the second argument of the on-rel(e) is the event variable of the study-rel. A case study in efficientgrammar-based processing.
W02-1503@@Large-scale grammar development platforms are ex-pensive and time consuming to produce. Finally, weprovide a summary and discussion. In the ParGram analysis,the c-structure analysis is left relatively free accord-ing to language particular needs and slightly vary-ing theoretical assumptions.
W02-1506@@Large-scale grammar development platforms shouldbe able to be used to develop grammars for a widevariety of purposes. de Paiva, C. Condoravdi, M. van den Berg,and L. Polanyi. Thus, a label-bracketed NP is assignedthe structure in (14).
W02-1507@@: Four grammardevelopment strategiesThere are several potential strategies to build wide-coverage grammars, therefore there is a need forclassifying these various strategies. Engineering a wide-coverage lexical-ized grammar. In Computational Linguistics, Vol 19.R.
W02-1509@@Much linguistic research is oriented to findinggeneral principles for natural language, classify-ing linguistic phenomena, building regular mod-els (eg, grammars) for the well-behaved (or well-understood) part of languages and studying remain-ing interesting Suddenly, grammarsstarted being extracted with an attempt to havefull coverage of the constructions in a certain lan-guage (of course, to the extent that the used corporarepresents the language) and that immediately posesa question: If we do not know how to model manyphenomena grammatically how can that be that weare extracting such a wide-coverage grammar.To answer that question we have to start a newthread at the edge of linguistics and computationallinguistics. I know ((the problem)and (that there is no solution to it)), where the con-juncts are licensed by the subcategorizations of theverb know). Thematching effects in free relatives: a parameter ofcore grammar.
W02-1510@@ The goal of the grammar development at Microsoft Research is to build robust, broad-coverage analysis and generation systems for multiple languages. Jensen, Karen, George E. Heidorn and Stephen D. Richardson (eds.). In this paper, we describe the tools and methods for the cross-linguistic development of analysis components of our system, which consists of three major modules: (i) the tokenization component, which performs word segmentation (in the case of Chinese and Japanese) and morphological analysis; (ii) the parsing component, which performs phrase-structure analysis and creates parse tree(s)1; (iii) the Logical Form (LF) component, which computes the basic predicate-argument structure from parse tree(s)2.
W02-1607@@Nowadays more and more people are interestedin word sense disambiguation (WSD). We have to combine withDTSim(s, t) because we are partially basing ondictionary. Eachgroup has a unique name called class-code.
W02-1610@@In this paper, we describe the design of an MTsystem that employs transfer rules induced fromparsed bitexts and present evaluation results for Ko-rean to English translation. A Wide CoveragePublic Domain Morphological Analyzer for English. Parses for the BitextsIn our experiments, we used a parallel corpus de-rived from bilingual training manuals provided bythe U.S. Defense Language Institute.
W02-1705@@Also, the DiComicrostructure of each monolingual dictionary isdefined by an XML schema, containing a largecommon core and a small specialization part(morphosyntactic categories, language usage). A tool has beenprogrammed at NII for that task... Axies relating them are specialbecause they cant in general relate them t oexternal semantic systems such as WordNet.
W02-1713@@we were not able to appropriately handle theissue of morphology, we could not parameterize the generation pro-cess at the desired level and we had no possibility to generate alternativesor recover from dead ends during generationsince XSL is lacking a backtracking mecha-nism.Therefore we decided to develop our own naturallanguage generation system that incorporates on theone hand many ideas found in XSL but on the otherhand tries to give a solution the above describedproblems. YAG: A Natural Lan-guage Generator for Real-Time Systems. XML PathLanguage (XPath) Version 1.0. http://www.w3.org/TR/xpath, November.World Wide Web Consortium.
W02-1809@@ It is important for many applications to be able to identify and extract person names in text. being a legitimate surname in Wade-Giles convention. The capitalized word(s), and the monosyllabic nature of words immediately after (or before) the surname give further support of its existence.
W02-1811@@ This is the first attempt to design a morphological analyzer to automatically analyze the morphological structures of Chinese compound words1. We cannot apply the same methods, such as n-gram language models, to resolve the ambiguities. Li, Charles and Sandra A. Thompson.
W02-1815@@It is generally agreed among researchers thatword segmentation is a necessary first step inChinese language processing. One of the three preceding (following)charactersistagged z.e.Theprecedingcharacteristagged z andthefollowingcharacteristagged w.f. The preceding (following) character istagged z andthecharactertwobefore(after)wastagged w.g.Thepreceding(following)characteris c.h.Thecharactertwobefore(after)is c.i.
W02-1817@@ It is well known that word segmentation is a prerequisite to Chinese information processing. ..... A Others tokens not mentioned above. In "Natural Language Understanding and Machine Translation", C. N. Huang & P. Zhang, ed., Tsinghua Univ.
W02-1904@@ The World Wide Web has become a fertile area, storing a vast amount of data and information. where  returns the root tag of an MPT, ()rootn denotes a relational weight of a parent to its children at level , and  is a function of estimating the similarity between the two MPTs based on their sub-trees similarities, which is defined as follows: n ()sub MPTSim  (3) where  is one of the two MPTs  and T  such that it subsums fewer sub-trees, AT 1T 2BT  is another one, and g  denotes a one-to-one function from  to AT BT . Transform the RTCS into a tag string.
W02-1906@@ Information Retrieval (IR) systems receive as input a users query, and they have to return a set of documents sorted by their relevance to the query. [3] Clarke, C.; Cormack, g, Kisman, D and Lynam, T. Question Answering by Passage Selection(Multitext Experiments for TREC-9) Proceedings of the Tenth Text REtrieval Conference, TREC-9. 1 It is a modification of the cosine model.
W02-2001@@Ideally, therefore, wewould like to have some means of automatically ex-tracting MWEs from a given domain or corpus, al-lowing us to pre-tune our grammar prior to deploy-ment. On building a more efficientgrammar by exploiting types. We additionally added in featuresdescribing: (a) the number of letters in the verblemma, (b) the verb lemma, and (c) the particlelemma.
W02-2007@@Our aim has been to build a maximally language-independent system for named-entity recognitionusing minimal supervision or knowledge of thesource language. Another advantageof this method is that single and multiple-word en-tities can be handled in the same way.The boundaries of entity candidates are deter-mined by a few simple rules incorporated into threediscriminators: is_B_candidate tests if a word canrepresent the beginning of an entity, is_I_candidatetests if a word can be the end of an entity, andis_E_candidate tests if a word can be an internalpart of an entity. Mbt: A memory-based part of speech tagger-generator.
W02-2008@@The quantity and reliability of linguistic informa-tion is primarily determined by the size of the train-ing corpus: with limited data available, extractingstatistics for any given language phenomenon andits surrounding context is unreliable. Without bet-ter models, all that training upon large corpora canachieve is better estimates of words which are ap-proximately i.i.d.To fully leverage the information in very largecorpora, we need to introduce more dependenciesinto the models to capture the non-stationary natureof language data. Duxbury Press, Belmont, CAUSA.Stanley F. Chen and Joshua T. Goodman.
W02-2009@@ This paper addresses the problem of detecting corresponding subtopics, or themes, within related bodies of text. References Cover, T. M. and Thomas, J. Structure-mapping: a theoretical framework for analogy.
W02-2016@@Dependency analysis has been recognized as a basicprocess in Japanese sentence analysis, and a num-ber of studies have been proposed. Japanese is a head-final language. 1)/2 trainingexamples (where n is the number of segments in asentence) must be produced per sentence.
W02-2024@@Named entities are phrases that contain thenames of persons, organizations, locations,times and quantities. The system uses tries as wellas character n-grams for encoding word-internaland contextual information. The B-XXX tag is used for the rstword in a named entity of type XXX and I-XXX is used for all other words in named en-tities of type XXX.
W02-2030@@Hand-built NLP grammars frequently have a depthof linguistic representation and constraints notpresent in current treebanks, giving them poten-tial importance for tasks requiring deeper process-ing. For a tree t, this feature has asvalue the number of time the expansion e occurredin t with the history h.  ExperimentsWe present experimental results comparing theparse ranking performance of different models. Exper-iments with a new boosting algorithm.
W02-2033@@Words differ in the subcategorisation framesthat realise their semantic arguments, and agiven word may have several different subcate-gorisation frames. Then, for instance, inthe sentence: (3)John talks to Marywith logical form (4) talk-communicative-act(e,x,y), john(x),comm-to(y), mary(y)the verb talks has two arguments, the NP sub-ject John, and the PP to Mary, as representedin the logical form associated with the verb,where the PP is the second argument and assuch should be included in the subcategorisationframe of the verb: (S\NP)/PP. Wefinish with some conclusions and a discussion offuture work.
W03-0103@@Reasoning about locations is essential for many NLPtasks, such as, for example, Information Extraction.Knowledge on place names comes normally from aNamed Entity Recognition module. For each classwe constructed a set of patterns. bellow)for the patterns associated with a given candidate, thenfiltered most of them out using the t-test.
W03-0105@@But many applications also re-quire grounding  ie, associating each classified textspan with a referent in the world or some model thereof.The current paper discusses spatial grounding of namedentities that may be referentially ambiguous, using a min-imality heuristic that is informed by external geographicknowledge sources. If the text also con-tains terms that ground unambiguously to I, J, and K, weassume the referent of A is Arather than A because theformer leads to a smaller spatial context.To use this spatial minimality heuristic, we start byextracting all place names using a named entity recog-nizer. (a) In New York, (b) In the U.S.A, (c) On 34thStreet and 3rd Avenue); the map generation task dependson such levels of granularity in the sense that to create auseful map, entities that belong to the same level of gran-ularity or scale should be marked (eg citycity ratherthan villagecontinent).
W03-0106@@ The task of location normalization is to decode geographic references for extracted location  Named Entities (NE). Ambiguity is a very serious problem for location NEs. In the Tipster Location Gazetteer used by InfoXtract, there are many common words, such as I, A, June, Friendship, etc.
W03-0203@@ Multiple-choice tests have proved to be an efficient tool for measuring students achievement. A Web-based System for Automatic Language Skill Assessment: EVALING. Here again T is the total number of students included in the item analysis.
W03-0205@@This paper describes preliminary work in exploring therelative effectiveness of speech versus text based tuto-rial dialogue systems. A framework for robust sentencelevel interpretation. Can you recall itStudent1: Um no it was one of Newtons laws-Tutor2: Right, right-Student2: but I dont remember which one.
W03-0209@@Rating constructed response items, particularly es-says, is a time-consuming effort. Lex-ical items had to be added to the systems lexiconto cover terms frequently used by students, suchas acronyms: E.L.C. Parsing En-glish with a Link Grammar.
W03-0210@@In this paper we describe CarmelTC, a novel hybridtext classification approach for analyzing essay answersto qualitative physics questions. Furnkranz, T. Mitchell Mitchell, and E. Riloff. You throw a pumpkinstraight up.
W03-0303@@Bilingual parsing based word alignment is promisingbut still difficult. null/e (5)Where f and e are words in the target vocabulary Vf andsource vocabulary Ve respectively. A is the alignmentof texts.
W03-0304@@We first take a closerlook at the standard whose prob-ability is maximal under some translation model:a In general, the size of A grows exponentiallywith the sizes of S and T , and so there is no efficient wayof computing a However, under the indepen-dence hypotheses of IBM Model 2, the Viterbi alignmentcan be obtained by simply picking for each position i inS, the alignment that maximizes t(si|tj)a(j, i,m, n), theproduct of the models lexical This procedure can trivially be carriedout in O(mn) operations. A parallel partition of S and T into m pairs of seg-ments si, tkj , where each tkj is a (possibly null)contiguous sub-sequence of T ; this partition can ofcourse be viewed as an alignment on the words of Sand T . A Statistical Approach to MachineTranslation.
W03-0305@@We participated the workshop shared task for English-French and Romanian-English word alignment. The sequence of updates will eventually convergeand the EAFs are then normalized (by dividing each ele-ment  $&% by the row marginal  $7@ ), so as to yield proba-bilistic translation lexicons, in which each source word isassociated with a target word through a score.Using the bilingual lexicon thus obtained, we use acorpus vocab Trial Testsize(E) lem  Ffi  GH Mem AER AER AERnnolem-ef-1. We useIBM Model 4 as a baseline.
W03-0306@@Simple baselines provide insights into the value of scor-ing functions and give starting points for measuring theperformance improvements of technological advances.This paper presents baseline unsupervised techniques forperforming word alignment based on geometric and wordedit distances as well as supervised fusion of the resultsof these techniques using the nearest neighbor rule. L(rj)(L(li) + L(rj))2(1)where L(li) is the length of the token at position i on theLHS. Anyof n source language words to align with any of m targetlanguage words, resulting in 2nm possible alignment con-figurations.
W03-0308@@It was aimed at building translation dictionaries from parallel corpora. Tufi, D. Tiered Tagging and Combined Classifiers In F. Jelinek, E. Nth (eds) Text, Speech and Dialogue, Lecture Notes in Artificial loafer) than as a separator.
W03-0309@@Word alignment is a crucial part of any Machine Transla-tion system, since it is the process of determining whichwords in a given source and target language sentencepair are translations of each other. A statisticalapproach to machine translation. AcknowledgmentsThis system is being implemented by Bridget ThomsonMcInnes as a part of her M.S.
W03-0310@@The bilingual data used for constructingtranslation models is often gathered from governmentdocuments produced in multiple languages. of German to English by a Frenchto English translation model43. It should beobserved, however, that our co-training algorithm is en-tirely general and may be applied to any formulation ofstatistical machine translation which relies on parallelRound NumberTranslation Pair 0 1 2 3FrenchEnglish 55.
W03-0317@@ Automatic bilingual lexicon construction based on bi-lingual corpora has become an important first step for many studies and applications of natural language proc-essing (NLP), such as machine translation (MT), cross-language information retrieval (CLIR), and bilingual text alignment. nlkqhpandnlqpvud ji (13) where l and n are the length of the source word E and the target word C, respectively. Linguistic Processing Rule 2 (R2): From error analysis of the aligned results of the training set, the proposed approach suffers from the fluid TUs, such as t, d, tt, dd, te, and de.
W03-0318@@ To achieve translation technology that is adequate for speech translation, the possibilities of several corpus-based approaches are being investigated. The arrangement of the English translations of the por-tions (1-e) is an adequate translation. Though method-N is better in sentence splitting quality, method-T is better in split-and-translate quality.
W03-0320@@We present an aligned parallel corpus of Inuktitut andEnglish from the Nunavut Hansards. For example, I deal with him.Of the 100 pairs, 43 were deemed exact matches and44 were deemed good matches. % and a recall rate of 92.%.
W03-0402@@Support Vector Machines (SVMs) have been successfullyused in many machine learning tasks. Intuitively, it is the local normalization thatresults in the label bias problem.One way of using discriminative machine learning al-gorithms in sequential models is to rerank the n-best out-puts of a generative system. It provides an alternative way of using a largemargin classifier for sequential models.
W03-0403@@Even with significant resources such as the Penn Tree-bank, a major bottleneck for improving statistical parsershas been the lack of sufficient annotated material fromwhich to estimate their parameters. A sequentialalgorithm for training text classifiers. The node labels are the names of theERG rules used to build the analysis.tion tree, out of three ERG analyses, for what can I dofor you.
W03-0404@@Many natural language processing applications couldbenefit from being able to distinguish between factualand subjective information. Learning Dictionaries forInformation Extraction by Multi-Level Bootstrapping.In Proceedings of the 16th National Conference on Ar-tificial Intelligence.E. The percentage agreement is 88%, and the  The annotated data will be available to U.S. governmentcontractors this summer.
W03-0405@@One open problem in natural language ambiguityresolution is the task of proper noun disambigua-tion1. A. Smith and G. Crane. Ta-ble 6 shows the performance on the four O(60) ex-ample hand-labeled test sets for naturally occurringpolysemous person names.
W03-0407@@In each case, co-training wasused successfully to bootstrap a model from only a smallamount of labelled data and a much larger pool of un-labelled data. A maximum entropy part-of-speech tagger. In Proceedings of the 6th Workshop onComputational Language Learning, Taipei, Taiwan.James R. Curran and Stephen Clark.
W03-0410@@Computational linguists face a lexical acquisition bot-tleneck, as vast amounts of knowledge about individualwords are required for language technologies. , with a mean of . For example, the locationargument, the truck, is direct object in I loaded the truckwith hay, and object of a preposition in I loaded hay ontothe truck.
W03-0411@@English prepositions convey important relations intext. For example, hereis a simple parse tree with the new annotation for-mat:(S (NP-TPC-5 This)(NP-SBJ every man)(VP contains(NP *T*-5)(PP-LOC within(NP him))))This shows that the prepositional phrase (PP) is pro-viding the location for the state described by the verbphrase. 32 workers at a factorytemporal .
W03-0414@@ and motivationThe Machine Translation community has recently under-gone a major shift of focus towards data-driven tech-niques. Note that no morphemes for the imparfait were extracted.This is an artifact of the training data which contains very fewinstances of imparfait.dimension&  (dimension&e Only thelast two dimensions are shown. A Decoder forSyntax-Based Statistical MT.
W03-0418@@The concept of a script was introducedin this research, to explain how people understand thesesituations and make inferences about them. SPANIEL uses a statistical model to identifycorrelations. The reverse traversal is anal-ogous, with the loop proceeding from n  Note that the first (respectively last) event doesnot need to be checked, as AUGMENT ensures the InSeqcriterion for this event with its single neighbor.REVISE can have one additional effect.
W03-0419@@Named entities are phrases that contain the namesof persons, organizations and locations. Example:[ORG U.N. ] official [PER Ekeus ] heads for[LOC Baghdad ] .This sentence contains three named entities: Ekeusis a person, U.N. is a organization and Baghdad isa location. A Simple Named Entity Extractor usingAdaBoost.
W03-0420@@#%$&)(+* ,.-The argmax operation denotes the search problem, iethe generation of the sequence of named entities. A gaussian priorfor smoothing maximum entropy models. Given a threshold i , we only include thosefeatures that have been observed on the training data atleast i times.
W03-0424@@Named Entity Recognition1 (NER) can be treated as atagging problem where each word in a sentence is as-signed a label indicating whether it is part of a namedentity and the entity type. yn given a sentence w1 . InvestigatingGIS and Smoothing for Maximum Entropy Taggers.In Proceedings of the 11th Meeting of the EuropeanChapter of the ACL, Budapest, Hungary.James R. Curran and Miles Osborne.
W03-0426@@2 on the test data where other systems However, not all train-ing data was used in training the LSTM networks. SARDNET:A Self-0rganizing Feature Map for Sequences, pages577584. s when used as target patterns.
W03-0428@@For most sequence-modeling tasks with word-level eval-uation, including named-entity recognition and part-of-speech tagging, it has seemed natural to use entire wordsas the basic input features. The smaller space of characters allows us to obtain denseestimates for longer E -grams than is possible with word-levelmodels. In practice, we have a special stop sym-bol in our n-gram counts, and the probability of emittinga space from a final state is the probability of the n-gramhaving chosen the stop character.1We index characters, and other vector elements by relativelocation subscripts: AB is the current character, AC is the follow-ing character, and AD C is the previous character.
W03-0429@@Language independence is difficult to achieve in namedentity recognition (NER) because different languages ap-pear to require different features. The vectors produced by the featur-izer for input to the SVM package are voluminous, lead-ing to significant I/O costs, and slowing tag assignment.Two methods might ameliorate this problem. Tnt + subcat Test A 91.
W03-0432@@Language independent NER requires the development ofa metalinguistic model that is sufficiently broad to ac-commodate all languages, yet can be trained to exploitthe specific features of the target language. and A from the training set. Illegal assignments, suchas an I-PER without a preceding B-PER, cannot arise dueto the restrictions of the transition matrix.The datasets for both languages contained extra infor-mation including chunk and part-of-speech information,as well as lemmas for the German data.
W03-0434@@An important research area in the field of information ex-traction is Named Entity Recognition. The performance wereported for the German data is achieved by using thefollowing features: B+C+D+E+F+G+H+I+J (with somesmall modifications), plus the German word lemma fea-ture provided by the task.The additional German dictionaries were provided tous by Radu Florian. An algorithm that learns whatsin a name.
W03-0501@@  In this paper we present Hedge Trimmer, a HEaD-line GEneration system that creates a headline for a newspaper story by removing constituents from a parse tree of the first sentence until a length threshold has been reached. GLEANS: A Generator of Logical Extracts and Using a t-score, the difference is significant with greater than 99.
W03-0502@@ Multiple articles on a particular topic tend to contain redundant information as well as information that is unique to each article. SimFinder: A Flexible Clustering Tool for Summarization. In the first step, we sum the judges If a summarizer has to pick a 2 sentence summary, and it picks A and C, its utility score is 30.
W03-0505@@Law reports form the most important part of a lawyers orlaw students reading matter. A maximum entropy approach to natural languageprocessing. 3 in respect of postage and I wouldallow the appeal.
W03-0507@@ As research on automatic text summarization is being a hot topic in NLP, we also see the needs to discuss and clarify the issues on how to evaluate text summarization systems. This task is the same as task A-2 in TSC1. We used recall, precision and F-measure for the evaluation of the extracts, and content-based as well as subjective methods for the evaluation of the free summaries.
W03-0508@@It is an understatement to say that measuring thequality of summaries is hard. 0) would be reached somewhere between 30 and40 summaries.. between rankingsfor 50 summaries on the basis of two consensus sum-maries, each based on a size N base summary collec-tion, for N between 5 and 200viz. The police laterarrested a white Dutch man.
W03-0509@@ Automatic Multi-Document summarization is still hard to realize. In other words, a systematic evaluation may be possible. This paper does not neces-sarily reflect the position of the U.S. Government.
W03-0608@@In writing this paper we hope to promote a discussion onthe design of an autonomous agent that learns semanticassociations in its environment or, more precisely, thatlearns to associate regions of images with discrete con-cepts. A statisticaltranslation model for contextual object recognition. Note that some words do not appear in both thetraining and test sets, hence the n/a.
W03-0609@@We are interested in how robots might learn languagegiven qualitatively the same inputs available to children -natural language utterances paired with sensory access tothe environment. It is the only one with a statistically significantmutual information value. A conditional probability field assigns to each  ,which corresponds to a point in an  -dimensional sen-sor group, a conditional probability of the form E   ,where E denotes the occurrence of some event.
W03-0611@@NLP systems that interact with the world often need mod-els of what words mean in terms of the non-linguisticworld. This could be be-cause the forecaster decided that the numerical forecastwas underestimating the speed at which the wind wasshifting, and hence he believed that the wind would beNE at 12 or 15, even though the data file predicted E andENE for these times. A new model of lexical choice fornouns.
W03-0707@@Spoken dialogue systems are emerging as an effectivemeans for humans to access information spaces throughnatural spoken interaction with computers. Chung and S. Seneff, Integrating speech with keypad in-put for automatic entry of spelling and pronunciation of newwords, Chung, S. Seneff, and C. Wang, Automatic acquisition ofnames using speak and spell mode in spoken dialogue sys-tems, Polifroni, G. Chung, and S. Seneff, Towards automatic gen-eration of mixed-initiative dialogue systems from web con-tent, Schalkwyk, L. Hetherington, and E. Story, Speech recogni-tion with dynamic grammars, Seneff and J. Polifroni, Dialogue management in the MER-CURY flight reservation system, The system parses the name within a completeparse, but with a generic unknown word as a stand-infor the restaurant name.
W03-0802@@The idea was to combine bothin order to benefit from their advantages. There arebasically three kinds of results: lists of unique identifiers denoting references tonodes in the XML input document XML documentsIn other words, if we formulate queries as functions, weget the following query signatures: Dwhere C is the component, D an XML document, P* a(possibly empty) sequence of parameters, S* a sequenceof strings, and N* a sequence of node identifiers.We now give examples for each of the query types.. . E.g., a fastindex-sequential storage and retrieval mechanism basedon XML is encapsulated through the shallowprogramming interface.
W03-0804@@Language resources are bodies of electronic languagedata used to support research and applications in thearea of natural language processing. We make severalobservations concerning this assumption:o the model assumes a fundamental linearity of ob-jects in the base,2  eg, as a time line (speech); a s e-quence of characters, words, sentences, etc. A formal framework forlinguistic annotation.
W03-0809@@Interface specifications are an important part of the com-putational infrastructure in engineering language technol-ogy (LT) systems. To reduce the cost of ontology designand maintenance, it is necessary to construct ontologieswhich are re-usable, i. e., support multiple applicationsand domains. A comparison of(semantic) markup languages.
W03-0902@@: deriving generalknowledge from textsWe have been exploring a new method of gaining gen-eral world knowledge from texts, including fiction. A report can be favorable. Individual -s may have a world.
W03-0903@@The ways in which knowledge has been represented inmulti-modal dialogue systems (MMDS) show that indi-vidual representations with different semantics and het-erogeneously structured content can be found in vari-ous formats within single natural language processing(NLP) systems and applications. Then, theexisting ontology was adopted in the SMARTKOM projectand modified to cover a number of new domains, e. g.,new media and program guides. The domain conceptsemerged through a comprehensive corpus analysis.
W03-0904@@  This paper discusses selected issues in ontological se-mantics (OS), an implemented computational-semantic theory that deals with the extraction, representation and use of meaning in natural language texts. Be-yond Basic Semantic Dependencies I: The Microtheory of Modality in Ontological Semantics. If Alex Patrick were the name of a company (cf.
W03-0905@@A spate of advanced new applications has called for amassive effort in script acquisition. It is desirablefrom the point of view of nonproliferation of elementsof metalanguage to avoid introducing a concept of, sayD R I V E R  if it could always be referred to asDRIVE.AGENT. Stock, C. Strapparava, and A.
W03-0906@@What are the appropriate metrics for evaluating perfor-mance in text understanding There is probably no oneuniversal measure that suffices, leading to a collectionof metrics for evaluating different facets of text under-standing. are a challenge for ECD. I know howthings would have to be for it to be true that There is no greatestpair of prime numbers,   and   , such that   	 .
W03-0907@@Story understanding is a fundamental unsolved problemin artificial intelligence and computational linguistics. The event cal-culus predicates important for this paper are as follows: Terminates(e, f, t) represents that if event e occurs at t thenfluent f stops holding after t.Reasoning using the event calculus is carried out asfollows: If 1 and 2 are conjunctions of Happens andtemporal ordering formulas,  is a conjunction of Initi-ates, Terminates, and Releases axioms,  is a conjunction of state constraints,  isa conjunction of trajectory axioms,  is a conjunction ofuniqueness-of-names axioms, and  is a conjunction ofHoldsAt formulas, then we are interested in the follow-ing:CIRC[1  Abduction and planning are performed by taking1, , , , , , and  as input, and producing 2 asoutput. A model or interpretation of a lan-guage maps constant symbols of the language to elementsof a domain D, n-ary function symbols to functions fromDn to D, and n-ary predicate symbols to a subset of Dn.We confine our attention to finite domains.
W03-0908@@Modern Question Answering (QA) systems aim at pro-viding answers to natural language questions in an open-domain context. A multi-strategy and multi-source approach to question an-swering. Learningsurface text patterns for a Question Answering system.In Proceedings of the ACL Conference.Martin M. Soubbotin.
W03-0909@@Generally speaking, language understanding for somecognitive agent means reconstructing the presumedspeakers goals in communicating with her/him/it. In: I. Mani and M. Maybury (eds. A discourse structure analyzer for Japanese text.Proc.
W03-1002@@In this work we define, implement and evaluate a novelmodel for statistical machine translation (SMT).Our goal was to produce a SMT system for translat-ing foreign languages into English which utilizes somesyntactic information in both the foreign language andEnglish without, however, requiring a full parse in eitherlanguage. are included as a benchmark. Learning depen-dency translation models as collections of finite state head transducersComputational Linguistics, 26(1), 4560.S.
W03-1004@@Unlike in traditional concept-to-text generation, text-to-text generation applica-tions take a text as input and transform it into a newtext satisfying specific constraints, such as length insummarization or style in text simplification. Text alignment in a tool for translatingrevised documents. Macro Alignment: Find CandidateParagraph(s)At this stage, the clustering and training are com-pleted.
W03-1005@@Our main claim is that that thepre-processing approach, coupled with a lexical-ized parser outperforms both state-of-the-art post-processing and in-processing. The unlexicalized CYKparser we use has a worst-case asymptotic runtimeof O   n3N3  where n is the number of words and Nis the number of nonterminals. In Proceedings of the 33rd Annual Meet-ing of the Association for Computational Linguistics,Cambridge, MA.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and Mark John-son.
W03-1006@@Syntax mediates between surface word order andmeaning. Also,we treat a PropBank argument (ARG0 . Ph.D. thesis, Univer-sity of Pennsylvania.Daniel Gildea and Martha Palmer.
W03-1007@@ Recent work in the development of FrameNet, a large database of semantically annotated sentences, has laid the foundation for statistical approaches to the task of automatic semantic classification. A Gaussian prior for smoothing maximum entropy models. The previous role feature indicates the classification that the n-previous frame ele-ment received.
W03-1008@@Correctly identifying the semantic roles of sentenceconstituents is a crucial part of interpreting text, andin addition to forming an important part of the infor-mation extraction problem, can serve as an interme-diate step in machine translation or automatic sum-marization. Acquir-ing Compact Lexicalized Grammars from a CleanerTreebank. Adjuncts are representedas functor categories such as S/S which expectand return the same type.
W03-1009@@ and Related WorkIn many natural language processing applications,such as parsing or language modeling, sentences aretreated as natural self-contained units. I(Xi, Ci|Li)where H(Xi|Li) is the conditional entropy of theith word given local context, and I(Xi, Ci|Li) isthe conditional mutual information between the ithword and out-of-sentence context, given the localcontext. Entropy Rate ConstancyEntropy, as a measure of information, is often usedin the communication theory.
W03-1010@@Lexical acquisition can be described as the processof populating a grammar skeleton with lexical items,through a process of mapping word lemmata ontolexical types described in the grammar. effects of new methods in ALT-J/E. of the Fifth International Workshop on ComputationalSemantics (IWCS-5), Tilburg, the Netherlands.Lane O.B.
W03-1012@@Recent work in statistical parsing has explored al-ternatives to the use of (smoothed) maximum likeli-hood estimation for parameters of the model. Sister adjunction isused in generating modifier sub-trees as sisters to the head, e.gin basal NPs. 7.A lexicalized sub-tree rooted on node n is splitinto two parts.
W03-1013@@Anattraction of CCG is its elegant treatment of coor-dination and extraction, allowing recovery of thelong-range dependencies inherent in these construc-tions. In Proceedings of the 18th International Con-ference on Computational Linguistics, pages 586592,Saarbrucken, Germany.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell III, and Mark John-son. A Gaussianprior for smoothing maximum entropy models.
W03-1015@@To ensure provable performance guaran-tees, the co-training algorithm assumes as input aset of views that satisfies two fairly strict condi-tions. a smoothing parameter, and k the numberof classes. Overall, our ranking methoddoes not exhibit the performance trend observedwith the B&M method: except for the spike betweeniterations 0 and 100, F-measure does not deteriorateas bootstrapping progresses.
W03-1016@@CONTENT SELECTION is the task of choosing theright information to communicate in the output of aNatural Language Generation (NLG) system, givensemantic input and a communicative goal. Cambridge UniversityPress, Cambridge, England.Johanna D. Moore and Cecile L. Paris. A two-stage model for content determina-tion.
W03-1018@@[fi] represents the expectationof feature fi in the training data (empirical expec-tation), and Ep[fi] is the expectation with respectto the model being estimated. C2i 2i ,subject to Ep i, (19)5It is also possible to impose 1-norm penalties in the objec-tive function. A maximum entropy approach to namedentity recognition.
W03-1019@@Until recent years, generative models were the mostcommon approach for many NLP tasks. Gradient Based OptimizationThe gradients of the four loss function can be com-puted as follows: Sequential Log-loss function:   "2ff*"4 3"ff" (3)where expectations are taken w.r.t.ff+*.Thus at the optimum the empirical and ex-pected values of the sufficient statistics areequal. This is a very desirable property for aclassifier.
W03-1020@@One of the main advantagesusing ME modeling is the ability to incorporatevarious features in the same framework with asound mathematical foundation. A Maximum Entropy Approachto Natural Language Processing. A Maximum En-tropy Approach to Identifying Sentence Boundaries.In: Proceedings of the Fifth Conference on AppliedNatural Language Processing, Washington D.C., 16-19.Zhou Ya-qian, Guo Yi-kun, Huang Xuan-jing, and WuLi-de.
W03-1021@@Most of thestate-of-the-art systems use n-gram language mod-els, which are simple and effective most of thetime. In practice, we use theSLM with all components modeled by neural net-works to generate N-best parses in the E step, and forthe M step, we use the modified back-propagationalgorithm to estimate the parameters of the neuralnetwork models based on the weights calculated inthe E step.We should be aware that there is no proof that thisEM procedure can actually increase the likelihoodof the training data. A neural probabilistic language model.
W03-1022@@This material is based upon work supportedby the National Science Foundation under Grant No. Sincefor training and testing we used only unambiguouswords there is always exactly one label per instance.ThusZsummarizes  word tokens that belong to thedictionary, where each instance ff is represented as avector of features   extracted from the context inwhich the noun occurred;  is the total number offeatures; anda is the true label of fi .In general, a multiclass classifier for the dictio-nary is a function y|j ld In the multiclass perceptron, one intro-duces a weight vector }hpj lm for everyah0qanddefines y implicitly by the so-called winner-take-allruley]zsb[u;S~}Q~ }_x (1)Here shj ln#m refers to the matrix of weights,with every column corresponding to one of theweight vectors}.The learning algorithm works as follows: Train-ing patterns are presented one at a time inthe standard on-line learning setting. Eachvector is a training instance.
W03-1023@@Other-anaphors are referential NPs with the mod-ifiers other and non-structural an-tecedents:1(1) An exhibition of American design and architec-ture opened in September in Moscow and willtravel to eight other Soviet cities. Problem examples canroughly be classified into five partially overlappinggroups: (a) examples that suffer from gaps in Word-Net, eg, (2); (b) examples that require domain-,situation-specific, or general world knowledge, eg,(3); (c) examples involving bridging phenomena(sometimes triggered by a metonymic or metaphoricantecedent or anaphor), eg, (6); (d) redescriptionsand paraphrases, often involving semantically vagueanaphors and/or antecedents, eg, (7) and (3); and(e) examples with ellipsis, eg, (8). Word-net 2 a morphologically and semantically enhancedresource.
W03-1024@@Anaphora resolution is an important research topicin Natural Language Processing. A Japanese sentence is a sequenceof bunsetsus:  	. is better than firstand e +svm1.
W03-1026@@Named entity (NE) recognition has drawn much at-tention in recent years. Combination algorithmsFor the first part of the experimental setup, weconsider the following classification framework:given the (probabilistic) output (RQ	TSUE of V classi-fiers W  !#!#!#1 WYX , the classifier combination problemcan be viewed a probability interpolation problem compute the class probability distribution condi-tioned on the joint classifier output:Z\[^]`_ acb]5degfihkj lZ`m!n! Information retrieval and extraction exercise.http://nlp.cs.nyu.edu/irex/index-e.html.MUC-6.
W03-1028@@Automatic keyword assignment is a research topicthat has received less attention than it deserves, con-sidering keywords Keywordsmay, for example, serve as a dense summary for adocument, lead to improved information retrieval, orbe the entrance to a document collection. The highest F-score is obtained by one ofthe n-gram runs. In case of a draw, thefirst occurring one is assigned.
W03-1101@@Our study fo-cuses on the effectiveness of applying sentence compression techniques to improve the perform-ance of extraction-based automatic text summariza-tion systems. Given a compressed sentence s, the channel model assigns the probability of an original sentence, t, which could have been generated by s.   We used K&Ms sentence compression algorithm as it was and did not retrain on new corpus. We used key n-grams to rerank com-pressions in our experiments.
W03-1107@@Recent methodologies of text categorization as ap-plied to Question-Answering(QA) and user naviga-tion on the Web address new types of problems, suchas the categorization of texts based on the questiontype in addition to one based on domain and genre.For good performance in a shallow approach, whichexploits the shallow specification of texts to cate-gorize them, requires a great deal of knowledge ofthe expressions in the answers corresponding to thequestions. Weobtained a kappa value of 0. e) At the end of sentences andimmediately before punctuation marks, the same ex-pressions appear repeatedly.
W03-1201@@Inparticular, a number of researchers have attemptedto use the English WordNet to bridge the gap Question answering (QA)Unlike IR systems which return a list of documentsin response to a query, from which the user mustextract the answer manually, the goal of QA is toextract from the corpus direct answers to questionsposed in a natural language.An important step before answer extraction isto identify and rate candidate passages from docu-ments which might contain the answer. COLING-ACL 98 Workshop on the Usage of Word-Net in Natural Language Processing Systems..P. Dempster, N.M. Laird and D.B. A Tutorial on Learning BayesianNetworks.
W03-1202@@ Ours is an age where many documents are archived electronically and are available whenever needed. Thus the frequency of word t in sentence s is stored in the cell  Ats. In addition, we used a beam size of 20.
W03-1206@@ HITIQA project is part of the ARDA AQUAINT program that aims to make significant advances in the state of the art of automated question answer-ing. WordNet: A Lexical Database. Visualization alternatives: non-pixel based images, of IS&T 46th Annual Conf.
W03-1208@@Open-domain Question Answering (ODQA) in-volves the extraction of correct answer(s) to a givenfree-form factual question from a large collectionof texts. ConclusionsThis paper presents a machine learning approach toquestion classification. Cortes and V. N. Vapnik.
W03-1209@@ Open-Domain factoid Question Answering (QA) is defined as the task of answering fact-based questions phrased in Natural Language. This is a binary-valued feature. (Yet Another Small Maximum Entropy Toolkit) http://www-i6.informatik.rwth-aachen.de/ Colleagues/och/software/YASMET.html   Classifier Re-Ranker Modeling Equa-tion  Hence, we evaluate our IR by only the precision measure at top N segments.
W03-1303@@ Basic notions used when describing a specific problem domain are concepts, classes and attrib-utes (or features). A class with the highest C(t, ci,j) score is used to classify the term t. Alternatively, multiple classes may be suggested by setting a threshold for C(t, ci,j). n) by a learning algorithm based on the information found in the corpus and the training ontology.
W03-1304@@ Named entities are basic constituents in a document. Type A: totally separated The above integration strategies put together all the possible protein candidates except the ambiguous cases (ie, types C, D and E). (confidence level) is equal to 0.005, the value of t is 2.
W03-1305@@Knowledge discovery in the rapidly growing area ofbiomedicine is very important. The com-plicated approach with the B/I/O notation requires(2N + 1)O(Lwords) (L is number of total wordsin a training corpus). It is a considerable reduction in the trainingcost.
W03-1306@@This kind of prob-lem has been studied in the field of natural languageprocessing as named entity recognition tasks. For example, we can make the cost of thesubstitution between a space and a hyphen muchlower than that of the substitution between E Both insertion and deletion costs are 100except for spaces and hyphens. % and a precision of71.
W03-1307@@ As the research in biomedical domain has grown rapidly in recent years, a huge amount of nature language resources have been developed and be-come a rich knowledge base. An Algorithm that Learns Whats in a Name. of Genome Informatics, Universal Academy Press, Inc.  K. Takeuchi and N. Collier.
W03-1308@@IE can benefit the medical sciencesby enabling the automatic extraction of facts relatedto prototypical events such as those contained in pa-tient records or research articles regarding molecularprocesses and their affect on human health. training data on a -2+2 win-dow. A significant proportion of the terms inour corpus undergo a local syntactic transforma-tions such as coordination which introduces ambi-guity that needs to be resolved by shallow parsing.For example the cand v-rel (proto) oncogenes andNF-kappaB and I kappa B protein families.
W03-1315@@In this paper, we investigate the extent to whichdifferent sources of information contribute towardsthe task of classifying the type of biological en-tity a phrase might refer to. A couple of surprisingwords were selected. These include dropping a plural s, substi-tuting one Greek character by another, changing anuppercase character by the same character in lowercase, changing an Arabic/Roman single digit by an-other, changing a Roman numeral by an Arabic one,and dropping digits.
W03-1501@@ Named entities are major components of a document. Cj should be associated with some foreign segment e  (hai xia) to Mountain and Strait, respectively, from these examples. Assume there are N foreign segments.
W03-1508@@Translation of proper names is generally recognizedas a significant problem in many multi-lingual textand speech processing applications. This list, in our case, is-i, e, u, o, r, u,ou, c, iu, ie.The second translation system, for converting pin-yin sequences to character sequences, has a one-to-one mapping between symbols and therefore has nowords with zero fertility. 15 is statistically significantat a   -value of 0.084.
W03-1601@@In natural language generation, producing somerealization of the input semantics is not the onlygoal. N. Pereira, and Robert C. Moore. A se-mantic side matches if it can be overlaid againstthe input.
W03-1602@@This paper reports on our ongoing research intotext simplification for reading assistance. Given the degree of readability respondent t as-signed to si(sj), map it to real value dor(t; s) 2[0; 1] so that the lowest degree maps to 0 and thehighest degree maps to 1. Tom bought a Honda from John.
W03-1604@@The problem of paraphrases conceals a number ofdifferent linguistic problems, which in our opinionneed to be treated in separate ways. (2) evt(install,A,[B,C]),object(D,E,[B]),object(s stowage compartment,G,[C])This means that a term (belonging to the samesynset as stowage compartment) is involved in an in-stall event with an anonymous object. In A. Zampolli, editor, Linguistic Struc-
W03-1605@@The phenomenon of paraphrase in human languagesis essentially the inverse of ambiguity  a givensentence could ambiguously have several meanings,while any given meaning could be formulated intoseveral paraphrases using various words and syntac-tic constructions. is matchedagainst the Q&A pairs in drink tea faq. Where is the best place to find English tea in the U.S.e.
W03-1608@@The richness of human language allows people toexpress the same idea in many different ways; theymay use different words to refer to the same entity oremploy different phrases to describe the same con-cept. We obtained a precisionof 0. The initial default score of any paraphraseis one (assuming perfect anchor matches), but foreach additional occurrence the score is incrementedby 12n, where n is the number of times the currentset of anchors has been seen.
W03-1609@@We are trying to obtain paraphrases which can beused for Information Extraction (IE) systems. This paperdoes not necessarily reflect the position or the pol-icy of the U.S. Government.ReferencesRegina Barzilay and Kathleen R. McKeown. Topic Detection & Tracking: A
W03-1610@@ This paper addresses the problem of extracting synonymous English words (synonyms) from multiple resources: a monolingual dictionary, a parallel bilingual corpus, and a monolingual cor-pus. N: number of triples in the corpus. However, the thesaurus coverage is a problem.
W03-1706@@  Syntactic research indicates that prosodic features, including stress, rhythm, intonation, and others, have an impact on syntactic structure. 8 percent and the unlabeled F-measure 4. 1 67.0 PCFG +RF in all the phrases 56.
W03-1707@@Linguistically interpreted corpora are instrumentalin supervised machine learning paradigms of natu-ral language processing. is also a possi-ble translation. In Sec-tion 2, we will discuss the annotation model in de-tail and describe our representation scheme.
W03-1708@@The research for Chinese information extraction isone of the topics in the project COLLATE1 (Com-putational Linguistics and Language Technologyfor Real World Applications). / (upperthree paths)|N||WIn the above examples, A, N, N5, and W repre-sent an adjective, a general noun, a Chinese LN,and a punctuation respectively. The error repairing rules for word segmen-tation and POS tagging are defined as follows:rectify_segmentation_error ( operator,old_word(s)_and_tag(s), repairing_mode,new_tag(s), preceding_context_constraint, fol-lowing_context_constraint)rectify_tag_error (old_word_and_tag, new_tag,preceding_context_constraint, follow-ing_context_constraint)Using these rules, we can move the word seg-mentation position newly and replace an error tagwith a correct tag.
W03-1717@@ Computer analysis of natural language sentences is a challenging task largely because of   the ambigui-ties in natural language syntax. Intelligent writing assistance, in A Handbook of Natural Language Processing: Tech-niques and Applications for the Processing of Lan-guage as Text, Dale R., Moisl H., and Somers H. Pairs with association scores below a certain threshold were then thrown out.
W03-1719@@The problem with this liter-ature has always been that it is very hard to comparesystems, due to the lack of any common standard testset. There is clearly no singlebest system, insofar as there is no system that con-Site word count R c  P c F OOV R    R  S08 34,955 0. Andi Wu and Aitao Chen provideduseful feedback on errors in some of the corpora.The first author wishes to thank Bill DuMouchel ofAT&T Labs for advice on the statistics.
W03-1721@@At the first Chinese word segmentation bakeoff, we partici-pated in the closed track using the Academia Sinica corpus(   for short) and the Beijing University corpus (  forshort). For example, theword E@F is segmented into E / F when it is not in thedictionary. After the initial segmentation, the text is segmentedinto `D` / afb / c / d / e /, which is subsequentlychanged into `D` / agb / cDdDe after combining thethree consecutive characters.
W03-1722@@  On behalf of the Institute of Computational Linguistics, Peking University, we would like to thank ACL-SIGHAN for sponsoring the First International Chinese Word Segmentation Bakeoff, which provides us an opportunity to present our achievement of the past decade. The Grammatical Knowledge-base of Contemporary Chinese  A Complete Specification (Second Version). The main difference between the work of U. Penn and that of ours is notion of word.
W03-1723@@ Word segmentation is very important for Chinese language processing, which aims to recognize the implicit word boundaries in Chinese text. Measures In the evaluation program of the First International Chinese Word Segmentation Bakeoff, six measures are employed to score the performance of a word segmentation system, namely test recall (R), test precision (denoted by P), the balanced F-measure (F), the out-of-vocabulary (OOV) rate for the test corpus, the recall on OOV words (ROOV), and the recall on in-vocabulary (Riv) words. + niww ii  is a word juncture.
W03-1726@@The same segmentation algorithm was applied to process these two corpora, except that character code conversion from GB to BIG5 for PK corpus and few modifications due to different segmentation standards had been made. 9% accuracy and a high applicability of 93. 4 Evaluation Results There are several evaluation indexes provided by SIGHAN, ie test recall (R), test precision (P), F score2, the out-of-vocabulary (OOV) rate for the test corpus, the recall on OOV words (Roov), and the recall on in-vocabulary (Riv) words.
W03-1801@@Nominal compounds are inherently ambiguous onboth the syntactic and semantic fronts. Head substitution(H-Sub) relates terms which share the same modi-ers but dierent heads : \eect of xanthan gum"and \addition of xanthan gum". Strictly synonymousMWTs coreference a single object/concept.
W03-1802@@Term extraction systems are now an integral partof the compiling of specialized dictionaries and up-dating of term banks. The result of this deletion is thestem R;M is the mutative segment to be concatenated to Rin order to form a noun.For example, the rule [ -e +e ] says that if there isan adjective which ends with e, we should strip thisending from it and append the string e to the stem.The algorithm below resumes the successive stepsfor identifying relational adjectives:1. When theredoes not exist a lexical function to label a conceptualrelationship, we introduce a new lexical function (i.ea non standard one).
W03-1804@@ Many models have been proposed to evaluate word de-pendencies. Second, it shows O(N log N) time complexity in our specific case. English Lexical Collocations: A study in computational linguistics.
W03-1805@@In many real world deployments of text mining tech-nologies, analysts are required to deal with large col-lections of documents from unfamiliar domains. Ranking n-length phrasesThe next example is ranking  -length phrases. A statisti-cal corpus-based term extractor.
W03-1806@@ Multiword units (MWUs) include a large range of lin-guistic phenomena, such as compound nouns (eg inte-rior designer), phrasal verbs (eg run through), adverbial locutions (eg on purpose), compound deter-minants (eg an amount of), prepositional locutions (eg in front of) and institutionalized phrases (eg con carne). As a consequence, computa-tion is hard. Let cam be the combined association measure, W a positional word-tag ngram, n-1 the set of all the positional word-tag (n-1)-grams contained in W, n+1 the set of all the positional word-tag (n+1)-grams containing W and sizeof(.)
W03-1807@@ 2 Automatic extraction of Multiword ex-pressions (MWE) is an important issue in the NLP community and corpus linguistics. A hybrid grammatical tagger: CLAWS4. In R. Garside, G. Leech and A. McEnery (eds.
W03-1808@@In this paper we discuss verb-particle constructions(VPCs) in English and analyse some of the avail-able sources of information about them for use inNLP systems. In relation to the A+C+E-VPCs, we foundthat 64% of these combinations are not listed. Verb-particle constructions in a computational grammar of
W03-1809@@Such items cause con-siderable problems for any semantically-groundedNLP application (including applications where se-mantic information is implicit, such as informationretrieval) because their meaning is often not sim-ply a function of the meaning of the constituentparts. Building a large annotated corpusof English: the Penn treebank. and approaching moderate (0.
W03-1812@@This paper is concerned with an empirical model ofmultiword expression decomposability. The scores are cosine similaritiessubsumes a sense of the MWE. The functionhyponym(word i,mwe) thus returns a value of 1 ifsome sense of word i subsumes a sense of mwe , anda value of 0 otherwise.A more proactive means of utilising the WordNethierarchy is to derive a semantic distance based onanalysis of the relative location of senses in Word-Net.
W03-1901@@Over the past 15-20 years, increasingly large bod-ies of language resources have been created andannotated by the language engineering community.Certain fundamental representation principles havebeen widely adopted, such as the use of stand-offannotation, use of XML, etc., and several attemptsto provide generalized annotation mechanisms andformats have been developed (eg, XCES, annota-tion graphs). Includingo  continuous segments (appear contiguouslyin the primary data)o  superand sub-segments, where groups ofsegments will comprise the parts of alarger segment (eg, a contiguous wordsegments typically comprise a sentencesegment)o  discontinuous segments (linked continuoussegments)o  landmarks (eg time stamps) that note apoint in the primary dataIn current practice, segmental information mayor may not appear in the document containingthe primary data itself. A formalframework for linguistic annotation.
W03-1905@@The eventual vision for computational lexicons isto enable universal access to sophisticatedlinguistic information, which in turn will serve as acentral component for content-based informationmanagement on the web. A portion of the DCR forConstructions is given in Appendix B. Entry 2 refers to instances of phrase objectsin the LDCR rather than including them inthe entry; this enables referring to a complexphrase (Vauxhave in the example) ratherthan including it directly in the entry, andprovides the potential to reuse the sameinstance by reference in the same or otherentries (this is done with N P  in theexample)..
W03-2102@@In this paper we present a detailed scheme forannotating expressions of opinions, beliefs, emo-tions, sentiment, speculation and other privatestates in newspaper articles. 9was achieved on a test set. For example, most of the writers sen-tences do not include a phrase such as I say.
W03-2118@@ The NESPOLE and C-STAR machine translation projects use an interlingua representation based on speaker intention rather than literal meaning. In Dialogue Processing in Spoken Language Systems: Revised Papers from ECAI-96 Workshop, E. Maier, M. Mast and S. LuperFoy (eds. Thus, we tested for signifi-cance using two-tailed matched pair t-tests.
W04-0103@@ Linguists propose new models for languages in order to explain language acquisition and processing by humans. CCR (s))  However, we might have to forcefully convert some values to 0 due to accidental gaps. Vg is a special symbol that represents schwa.
W04-0105@@Unsupervised learning presents unusual challengesto the field of computational linguistics. Some transformations,such as e  , will be suggested by more than onepair of signatures, while others, such as y  o,will occur with only one pair. This is dueto the fact that the stem-final e deletes before suf-fixes beginning in e or i.
W04-0107@@ Many natural language processing tasks, such as morphological analysis and parsing, have mature solutions when applied to resource-rich European and Asian languages. Compare the CICs me.mes.med and e.es.ed; each is de-rived from exactly the triple of word forms blame, blames, and blamed, but differ in the placement of the hypothesized morpheme boundary. We define a candidate inflection class (CIC) to be a set of c-suffixes for which there exists at least one c-stem, t, such that each c-suffix in the CIC concatenated to t produces a word form in the vocabulary.
W04-0109@@This paper presents the WordFrame model, a novelalgorithm capable of inducing morphological anal-yses for a large number of the worlds languages.The WordFrame model learns a set of string trans-ductions from inflection-root pairs and uses these totransform unseen inflections into their correspond-ing root forms. Snover and M. R. Brent. Unsupervised learning of themorphology of a natural language.
W04-0206@@The goal of this paper is to present a discourse-level annotation scheme developed for the pur-pose of investigating information distribution intext from a cross-linguistic perspective, with aparticular focus on the interplay of various fac-tors pertaining to the realization of informationstructure. Thereare two crucial technical requirements that must besatisfied to make this possible: (i) stand-off anno-tation at each level and (ii) alignment of base dataacross the levels. Journal of Language and Computation(JLAC), Special Issue.E.
W04-0208@@ Getting a machine to understand human narratives has been a classic challenge for NLP and AI. e. He gave these gifts to his family. W. Mann and S. Thompson.
W04-0210@@In this paper we discuss themethods used to identify possible utterances, theproperties of NPs and discourse entities that wereannotated, and (very briefly) anaphoric information.. A. Walker and E. Prince. (I would like (to be able to travel))b.
W04-0211@@In particular, we describe the relationship between the output of sentential pars-ing and discourse processing. If the antecedent f is more specific than the anaphoric element e, two cases are possible. Critically important, too, is to unify the semantically motivated structural analyses pre-sented here with an explicit S-DRT type formal semantic account of discourse semantics.
W04-0212@@We have been developingyet another annotation layer above these both. (7) INIT-SS: Rep. John LaFalce (D., N.Y.) said Mr.Johnson refused [to testify jointly with Mr. Mul-ford] and instead [asked to appear after the Trea-sury official had completed his testimony]. Example 6illustrates a case of higher verb disagreement.
W04-0213@@A corpus of German newspaper commentarieshas been assembled at Potsdam University, andannotated with different linguistic information,to different degrees. RhetoricalStructure Theory: A Theory of Text Orga-nization. of SouthernCalifornia / Information Sciences Institute.C.
W04-0214@@Discourse information plays an important part innatural language systems performing tasks suchas text summarization, question-answering systemsand collaborative planning. A statisticalapproach to anaphora resolution. Technical Report TRAINS TN 94-2, U.Rochester.W.
W04-0215@@The U-LDM is a theory of discourse structure and semantics that has as its goal assigning the correct inter-pretation to natural language utterances. Bernsen, N. O., Dybkjr, L. and Kolodnytsky, M.: The NITE Workbench A Tool for An-notation of Natural Interactivity and Multi-modal Data. A Rule Based Approach to Discourse
W04-0216@@It has long been known that animacy is animportant category in syntactic and morphologicalnatural language analysis. "How to Choose a PossessiveNoun Phrase Construction in Four EasySteps." The fact that itpermeates the choice between constructions inlanguages such as English changes this perception.The category is of obvious importance for highquality generation and translation.For instance, if one is faced with the task ofgenerating a sentence from a three place predicate,P (a,b,c), and one has the choice of rendering eitherb or c as the direct object, knowing that c is humanand b is abstract would lead one to choose c ceterisparibus.
W04-0304@@As wellas being psycholinguistically motivated, continuousunderstanding models offer potential computationaladvantages, including accuracy and efficiency im-provements for real-time spoken language under-standing and better support for the spontaneities ofnatural human speech. of the FifthDARPA Speech and Natural Language Workshop.E. % or 9.% on the test dialogue, althoughthe amount of work the parser performed was re-duced by only 4.0% and 3.
W04-0307@@They utilizenon-terminals that go beyond the level of a sin-gle word and do not explicitly use lexical fea-tures. Ad-ditionally, we will compare with Collins Note L denotes Loose coupling and T de-notes Tight coupling. Language model-ing using a statistical dependency grammar parser.
W04-0308@@Incrementality in parsing has been advocatedfor at least two different reasons. Ide-ally, we would like to require that the graph(W  I, A) is connected at all times. By contrast, the threeremaining trees all require that three tokens areInitialization nil,W,  In (6) they are sisters,both being dependents on the third token; in(7) the first is the grandparent of the second.And in pure dependency parsing without non-terminal symbols, every reduction requires thatone of the tokens reduced is the head of theother(s).
W04-0403@@bezcarja v golove (to have a screw loose, lit. At the same time the word temas the component of s tem chtoby (in order to,lit. Someprepositions like u or iz occur almost exclusivelyin fully compositional patterns, for example, ex-pressing location: u okna, morja (by the window,by the sea), or possession: u menja, u Ivana (Ihave, Ivan has).
W04-0404@@Noun-noun (NN) compounds (eg web server, kikaihoNyaku machine translation,1 theelements of which we will refer to as N 	 and N  inlinear order of occurrence) are a very real problemfor both machine translation (MT) systems and hu-man translators due to:constructional variability in the translations:kikaihoNyaku machine transla-tion well-sidemeeting, which translates most naturally intoEnglish as idle gossip ;high productivity and frequencyIn order to quantify the high productivity andfrequency of NN compounds, we carried out a1With all Japanese NN compound examples, we segmentthe compound into its component nouns through the use of the Combin-ing these observations, we see that a translator orMT system attempting to translate one of these cor-pora will run across NN compounds with high fre-quency, but that each individual NN compound willoccur only a few times (with around 45-60% occur-ing only once). 2, at a recall of 0. of the36th Annual Meeting of the ACL and 17th InternationalConference on Computational Linguistics (COLING/ACL-98), pages 704710, Montreal, Canada.Judith N. Levi.
W04-0405@@ While proper treatment of the Propositional Content (PC) of a sentence is undoubtedly important in natural language processing (NLP), the Non-propositional Content (NPC) also plays a critical role in tasks such as discourse understanding, dialogue modeling, detecting speakers intension. It produced a recall of 97. Application to J/E Machine Translation We introduce here another experimental system, referred to as ENGL, whose input is the NPS of a sentence and whose output is its English forms, to demonstrate the usefulness of our formalism.
W04-0407@@ 2 Most texts are rich in multiword expressions, which must be necessarily processed if we want any NLP tool to perform accurately. A cascaded syntactic analyser for Basque. In loak hartu to fall asleep there is a subject-verb relation as in loak hartu nau I have fallen asleep, literally sleep has caught me; therefore subject-auxiliary verb agreement would fade if both components were analyzed as one.
W04-0408@@Core conceptssuch as grammatical functions, valency and thehead-dependent asymmetry have now found theirway into most grammar formalisms, includingphrase structure-based ones such as HPSG, LFGand TAG. licenses an in-coming edge labeled l. I.e. Then for XDG solving,we introduce precisely n nodes for literal s. Usinggroups for generation thus comes at the expense ofhaving to introduce additional nodes.
W04-0409@@Multi-word expression extraction is an importantcomponent in language processing that aims toidentify segments of input text where the syntacticstructure and the semantics of a sequence of words(possibly not contiguous) are usually not composi-tional. %and a precision of 98. )the first part of the collocation, the accusativemarked noun kafay, is the fixed part and the partstarting with the verb yeis the variable part whichmay be inflected and/or derived in myriads of ways.For example the following are some possible formsof the collocation: yiyenler those who got mentally de-ranged yedigi the fact that (s/he) got mentallyderangedUnder certain circumstances, the fixed part mayactually vary in a rather controlled manner subjectto certain morphosyntactic constraints, as in the id-iomatic verb:(4) kafa(y) ek- (but literally to pull the head (but literally to pull theheads )where the fixed part can be in the nominative or theaccusative case, and if it is in the accusative case, itmay be marked plural, in which case the verb has tohave some kind of plural agreement (ie, first, sec-ond or third person plural), but no possessive agree-ment markers are allowed.In their simplest forms, it is sufficient to recognizea sequence of tokens one of whose morphologi-cal analyses matches the corresponding pattern, andthen coalesce these into a single multi-word expres-sion representation.
W04-0411@@Theycomprise a wide-range of distinct but related phe-nomena like idioms, phrasal verbs, noun-noun com-pounds and many others, that due to their flexiblenature, are considered to be a challenge for manyareas of current language technology. In B. Boguraev and E. Briscoe,editors, Computational Lexicography for NaturalLanguage Processing. Thus, whatlinks all the components of an MWE together, spec-ified each as an entry, is that they have the sameMWE identifier (eg i spill beans 1).
W04-0502@@While the tasks and answer requirements have varied slightly from year to year, the purpose behind QA evaluations remains the same: to move from the traditional document retrieval to actual information retrieval by providing an answer to a question rather than a ranked list of relevant documents. Voorhees, E.M. Overview of the TREC-8 Question Answering Track Report. Implementing a Question Answering Evaluation.
W04-0505@@Although most current research in question answer-ing (QA) is oriented towards open domains, aswitnessed by evaluation exercises such as TREC,CLEF, and NTCIR, various significant applicationsconcern restricted domains, eg, software manuals.In restricted domains, a QA system faces questionsand documents that exhibit less variation in lan-guage use (eg, words and fixed phrases, more spe-cific terminology) than in an open domain, and itcould access high-quality knowledge sources thatcover the entire domain. The Levenshtein measure is a measure of the similarity be-tween two strings, which are refered to as the source string sand the target string t. The distance is the number of deletions,insertions, or substitutions required to transform s into t1 Sir Malcolm Bradbury (writer/teacher) Dead.Heart trouble. Such resources,however, have a limited coverage.
W04-0506@@The current trend in Question Answering is to-wards the processing of large volumes of open-domain texts (eg documents extracted fromthe World Wide Web). In Proceedings of 7th EuropeanWorkshop on Natural Language Generation,Leiden, The Netherlands.E. Then, ourknowledge extractor transforms each relevantparagraphs into a logical representation.
W04-0507@@The 70% of accuracyis, of course, high enough to surprise the researchersof this field, but, on the other hand, the accuracy isnot enough to satisfy the normal users in the realworld, who expect more precise answers.The difficulty of constructing open-domainknowledge base is one reason for the difficulties ofopen-domain question answering. In Proceedings of the 5thRIAO conference on Computer Assisted Informa-tion Searching on the Internet.N. We use a rule basedanswer generation method.
W04-0508@@One of the core problems in exploiting scientificpapers in research and clinical settings is that theknowledge that they contain is not easily acces-sible. In Mark May-bury, editor, New Directions in Question Answering.AAAI Press.T.C. A Workbenchfor Finding Structure in Texts.
W04-0601@@XSLT is used to per-form many text-planning tasks, including structur-ing and aggregating the content, performing lexicalchoice via the selection of logical-form templates,and generating multiple alternative realizations formessages where possible.Using an external realizer at the end of the plan-ning process provides two advantages. The re-alizer will then give (4) a higher n-gram score than(3), and will therefore choose the desired structure.In addition to simplifying the implementation, re-taining multiple alternatives through the planningprocess also increases the robustness of the system,and provides a substitute for backtracking. EXEM-PLARS: A practical, extensible framework fordynamic text generation.
W04-0602@@ Metadata is a key source of information towards realization of the Semantic Web that could be exploited in many different ways. Maps of Rome small self-defined non val XML export from a database HoS  Berlin Collection large close to DC validated XML export from a database HoS  NECEP database small self defined validated XML export from a database L  Collection of Texts small self defined non val XML XML texts History of arts (HoA), History of Science (HoS), Linguistics (L), Ethnology (E), Phylosophy (P) Also the way in which the content of resources is described differs substantially. is a sub-class of OLAC:Creator resource value property I:Participant O:Creator isSubClassOf I:Genre F:Iconography mapsTo In the ECHO project we can identify a semantic overlap between IMDI:Genre We can imagine that RDF will be used by some projects, initiatives and institutions to establish widely recognized and used repositories with mapping relations.
W04-0705@@However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ. + Mentioni   All Formal(i, j) Formal and informal ways of referring to the same entity (Ex. 2 This class is used in the U.S.
W04-0706@@The next sec-tion explain our notion of indirect anaphora. It leads to a precision of52. Hana is the head-noun of the anaphor Hcani is the head-noun of the antecedent can-didate i Lana is the anaphors list of similar nouns Lcani is the list of similar nouns for the candi-date i Thecandidate with the lowest weight wins.
W04-0707@@(1) a. Toni Johnson pulls a tape measure acrossthe front of what was once a stately Victorianhome.b. L. Bean and E. Riloff. Tworesults are shown for the hand-coded decision tree:in one version, the system doesnt attempt to clas-sify all DDs; in the other, all unclassified DDs areclassified as discourse-new.Version of the System P R FBaseline 50.
W04-0711@@The need for identifying the antecedents ofanaphoric expressions in the literature is well-recognized. interaction of A with B2. For example, the protein domain namethe RNA subunit of RNase P can be grounded withRPM1 in the SUBUNIT field of RPM2 YEAST, ieConsists of a RNA moiety (RPM1) and the proteincomponent (RPM2).
W04-0713@@Most intersentential anaphor resolution al-gorithms exclusively account for pronominalanaphors with individual nominal antecedents(henceforth IPAs) in texts. A Centering Approach to Pro-nouns. 1 In the lastcolum the overall performance of the two algo-rithms is given as f-measure (F) which is definedas 1 is the weight of P and R. We have as-signed the same weight to P and R ( The results of the tests in-dicate that dar resolves IPAs significantly bet-ter than es00 (which uses str98).
W04-0802@@The goal of the MultiLingual lexical sample taskis to create a framework for the evaluation of sys-tems that perform Machine Translation, with a fo-cus on the translation of ambiguous words. For example, the training/test instancesfor arm.n do not include examples for contact arm,2We have made available transcriptions of the entries forapproximately 70 Hippocrene nouns, verbs, and adjectivesat http://www.d.umn.edu/pura0010/hindi.html, although thesewere not used in this task.pickup arm, etc., but only examples that refer to armas a single lexical unit (not part of a collocation). Foreach target word the system extracts a set of sen-tences from a large textual corpus.
W04-0804@@WordNet wasnot designed to serve as a lexical resource, but itspublic availability and reasonable comprehensivenesshave been dominant factors in its selection as thelexical resource of choice for Senseval and for manyapplications. These factors have led to furtherfunding by U.S. government agencies and manyimprovements are currently underway. An exact match received a score of 1.0.
W04-0807@@We describe in this paper the task definition, re-sources, participating systems, and comparative re-sults for the English lexical sample task, which wasorganized as part of the SENSEVAL-3 evaluation ex-ercise. Wordnet: A lexical database. The objective of this task wasto: (1) Determine feasibility of reliably finding theFine CoarseSystem/Team Description P R P Rhtsa3 A Naive Bayes system, with correction of the a-priori frequencies, byU.Bucharest (Grozea) dividing the output confidence of the senses by  		 (   ) 72.
W04-0813@@Our group (BCU, Basque Country University),participated in the Basque and English lexicalsample tasks in Senseval-3. A word-grammar based morphologicalanalyzer for agglutinative languages. Linear ker-nels were applied, and the soft margin (C) wasestimated per each word (cf.
W04-0814@@The out-put of a system is a list of frame elements, with theirnames and character positions in the sentence. Verbs withparticles were a particular problem. Statistical Methods in Linguistics, 12:529.D.
W04-0817@@We concentrated on two questions: first,whether role assignment can be improved by gener-alisation over training instances using different sim-ilarity measures; and second, the impact of EM-based clustering, both in deriving more informativeselectional preference features and in the generali-sations mentioned above. A comparison of algorithms formaximum entropy parameter estimation. 1. mother phrase type -0.
W04-0818@@The output ignores el-ements like determiners and negation, and featuressuch as plurals and verb tenses.This evaluation is of interest to the MITRE Cor-poration because it has a long-standing interest intext processing and understanding, in all its variousdimensions. Rule (2) createsan analogous dependency for the O link, making theright element the object of the left element. Pars-ing English with a Link Grammar.
W04-0819@@Each semantic frame is character-ized by a set of target words which can be nouns,verbs or adjectives. These sentenceswere parsed with the Collins Unrestricted Task ExperimentsFor this task we devised four different experimentsthat used four different combination of features: (1)FS1 indicates using only Feature Set 1; (2) +H in-dicates that we added the heuristics; (3) +FS2+FS3indicates that we add the feature Set 2 and 3; and(4) +E indicates that the extended data has also beenused. ConclusionsIn this paper we describe a method for automaticallylabeling semantic roles based on support vector ma-chines (SVMs).
W04-0820@@The foundation of this measure is the Con-ceptual Distance, defined as the length of the short-est path which connects two concepts in a hierar-chical semantic net. This has been done in a fully automatedway, by using the WordNet mappings for nouns and1Istituto per la Ricerca Scientifica e Tecnologica, Trento,Italyverbs, and by checking the similarity of synset termsand glosses for adjectives and adverbs. EAW: englishall-words task, scores are both with U and w/o U. LS: LexicalSample task.The integration has been made by means of a vot-ing technique.
W04-0822@@Classifier combination has become a standard ar-chitecture for shared task evaluations in word sensedisambiguation (WSD), named entity recognition,and similar problems that can naturally be cast asclassification problems. ve Bayes in a subsequent comparison ofWSD performance. 5%more accurate on the Multilingual (t) task.
W04-0824@@As a consequence train-ing instances are often too few in number to cap-ture extremely fine-grained semantic distinctions.Word senses, however, are not just independent enti-ties but are connected by several semantic relations;eg, the is-a, which specifies a relation of inclusionamong classes such as car is-a vehicle. MIT Press, Cambridge, MA.K.L Yoong and T.N. , thus for eachset of labels 8; & / we induce a set of pseudo-labels|8:&/ .For each synset in E\F we compiled a train-ing instance from the Wordnet glosses.
W04-0831@@RLSC algorithm, inputand output.This paper is not self-contained. Here the performance onSENSEVAL-2 as a function of ) .0. 4. test the model (using ) in the post-processing phase and then the scoring pythonscript)At this point we were worried by the lack of anyexplanation (and therefore the lack of any guaran-tee about performance on SENSEVAL-3).
W04-0832@@ The Automatic Labeling of Semantic Roles track in SensEval-3 focuses on identifying frame elements in sentences and tagging them with their appropriate semantic roles based on FrameNet1. Overlap Recall Attempted Test A 80. During training, this information is provided by simply looking at the true class of the constituent occurring n-positions before the target element.
W04-0833@@In this paper, we present the two systems webuilt for our first participation in the Englishlexical sample task at Senseval-3. It includes a total of7860 tagged samples for 57 ambiguous words. A large context window pro-vides domain information which increases theaccuracy for some target words such as bank.n,but not others like different.a or use.v (see Sec-tion 3).
W04-0834@@This paper describes the approach adopted by oursystems which participated in the English lexicalsample task and the multilingual lexical sample taskof SENSEVAL-3. # ,where # denotes a null token. Based on the official test keysreleased, the micro-averaged recall drops to 0.
W04-0835@@CL Research participated in four SENSEVAL-3tasks: English all-words, English lexical sample,disambiguation of WordNet glosses, and automaticlabeling of semantic roles. Thelow recall is a reflection of the small percentage ofitems attempted. Lexical Sample Recall (Training)Run Items Fine CoarseAdjectives 314 0.
W04-0837@@The highperformance of the first sense baseline is due to theskewed frequency distribution of word senses. A method for disambiguatingword senses in a large corpus. Results from SENSEVAL-3For SENSEVAL-3 we used the predominant sensesfrom the automatic rankings for i) all PoS (autoPS)and ii) all PoS except verbs (autoPSNVs).
W04-0838@@The task of word sense disambiguation consistsof assigning the most appropriate meaning to apolysemous word within a given context. is la-beled with a predicted word and sense. InProceedings of the Association for Computa-tional Linguistics, Madrid, Spain.R.
W04-0839@@The SyntaLex systems are supervised learnersthat identify the intended sense of a word (targetword) given its context. % and a coarse grained accuracy of 69. In Pro-ceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 4148.S.
W04-0849@@Supervised systems for word-sense disambigua-tion (WSD) often rely upon word collocations(ie, sense-specific keywords) to provide clueson the most likely sense for a word given thecontext. All collocational features are binary indi-cators for sense s, except for WordColl.ity is 20% or higher:(P (s|w)  0.This threshold was determined to be effectivevia an optimization search over the Senseval-2data. These five fea-tures are POSi for i from -2 to +2 ), wherePOS+1, for example, represents the POS of theword immediately following the target word.Five other local-context features representthe word tokens immediately surrounding thetarget word (Wordi for i from 2 to +2).Each Wordi feature is multi-valued; its valuescorrespond to all possible word tokens.There is a collocation feature WordCollsde-fined for each sense s of the target word.
W04-0851@@Word sense disambiguation can be viewed asa classification problem and one way to ob-tain a classifier is by machine learning methods.Unfortunately, there is no single one universalgood learning procedure. {1, 1} for all i.The hypothesis space H of RLSC is the set offunctions f : Rd  f2KIn spite of the complex mathematical toolsused, the resulted learning algorithm is a very1We didnt use any kind of smoothing. We trained a different binary classi-fier for each sense.
W04-0857@@The goal in the SENSEVAL-3 semantic role labelingtask is to identify roles and optionally constituentboundaries for each role, given a natural languagesentence, target, and frame. We call each nodein this tree a constituent. 9 and its recall was 0.
W04-0858@@The Senseval-3 English Lexical Sample (ELS) taskrequires disambiguating 57 words, with an averageof roughly 140 training examples and 70 testingexamples of each word. We then experimented withcombining the base classifiers, using a variety ofmeta-learning algorithms. An algebra for structured textsearch and a framework for its implementation.The Computer Journal, 38(1):4356.Egidio L. Terra and Charles L.A. Clarke.
W04-0860@@Designing a system for Natural Language Process-ing (NLP) requires a large knowledge on languagestructure, morphology, syntax, semantics and prag-matic nuances. (t) continues decreasingto a given threshold, u, very close to 0. . Each sense of aword is represented as a vector in an n-dimensionalspace where n is the number of words in all its con-texts.We use the LVQ algorithm to adjust the wordweights.
W04-0862@@The Swarthmore College system consisted of threesupervised classifiers which were used to performlexical ambiguity resolution in five languages. A lemmatized form of the col-location was found in 96. Obviously, these threeclassifiers, based solely on part-of-speech n-gramsaround the target word, had a high rate of agree-ment and were therefore over-weighted in the finalvoting.
W04-0863@@This paper describes the two joint component mod-els of the Swat-HK systems entered into four ofthe word sense disambiguation lexical sample tasksin Senseval-3: Basque, Catalan, Italian and Roma-nian, as well as a combination model for each lan-guage. A model was thentrained for each ambiguous word type. In addition to the boosting and maximum en-tropy models mentioned earlier, three other modelswere included: a nearest-neighbor clustering model,a decision list, and a Na The decrease in per-formance caused by the error ranged from 0.
W04-0902@@It is, however, difficult to scale thesemethods to unrestricted, general-domain natu-ral language input because of the overwhelmingproblems of grammar coverage, unknown words,unresolvable ambiguities, and incomplete do-main knowledge. More-over, it is possible to express scope islands by3In our case, DL formulas  Enumerating all scopings and usinga theorem-prover to determine logical equiva-lences requires O(n2) comparisons for n scop-ings. Deep Read: A readingcomprehension system.
W04-0904@@ Ontological Semantics (OntoSem) is a multi-lingual text processing environment that takes as input unrestricted text and, using a suite of static resources and processors, automatically creates text-meaning representations (TMRs) which can then be used as the basis for any NLP application, including MT, question answering, summarization, etc. The analysis of sentences (a)-(e) in OntoSem is carried out as follows. for pudding is a hard constraint.
W04-0905@@ In this paper we describe the evaluation regimen for a general-purpose syntactic-semantic analyzer, OntoSem, under continuous development at the Institute for Language and Information Technologies (ILIT) of the University of Maryland Baltimore County. The quality of semantic dependency determination is computed using the standard (m/n) measur We will now exemplify the evaluation of the semantic analysis of the sample sentence in 1:  D) e. Each TMR element in the gold standard is compared to the corresponding  1. Attachment is also measured as (m/n).
W04-0907@@Japanese relative clause constructions have the gen-eral structure [[S][NP]], and constitute a nounphrase. %, a very strong result. and mi-e-ru see-POT-PRES, of which we would (correctly) selectthe first.
W04-0911@@Our goal is(i) to empirically investigate the use of flexible natu-ral language dialog in tutoring mathematics, and (ii)to develop a dialog-based tutoring system for teach-ing mathematical theorem proving. of IJCAI03 Workshop on Knowledge Rep-resentation and Automated Reasoning for E-Learning Sys-tems, Acapulco, Mexico.C. We interfaceto the domain ontology through an upper-level on-tology of concepts at the lexical-semantics level.Domain specializations of conceptual relationsare encoded in the domain ontology, while a seman-tic lexicon assigns conceptually-oriented semanticsin terms of linguistic meaning frames and provides alink to the domain interpretation(s) through the do-main ontology.
W04-1003@@Research in summarization was one of the first ef-forts to use computers to understand The interest in automatic summariza-tion of text has continued, and currently is enjoy-ing increased emphasis as demonstrated by the nu-merous summarization workshops held during thelast five years. 00, but again a widevariation in overlap across the different documentsets. Document set 14, Judge B U.S. military air-craft crashes occur throughout the world moreoften than one might suspect.
W04-1006@@In this research, we focused on aproblem referred to as legal text summarization.As ever larger amounts of legal documents becomeavailable electronically, interest in automatic sum-marization has continued to grow in recent years. Input to the sys-tem is a legal judgment in English. Sec-tion 2 introduces the motivation of the research andthe context of the work.
W04-1007@@Law reports form an interesting domain for auto-matic summarisation. %) things unrelated to a case.E.g. A workbenchfor finding structure in texts.
W04-1008@@ Email for many users has evolved from a mere communication system to a means of organizing workflow, storing information and tracking tasks (ie to do Tools available in email clients for managing this information are often cumbersome or even so difficult to discover that users are not aware that the functionality exists. Example 1: On the H-1 visa issue, I am positive that you need to go to the Embassy in London to get your visa stamped into your passport. 2 Data We collected a corpus of 15,741 email messages.
W04-1009@@ In this paper, we present algorithms to address the shortcomings of both purely structural and purely statistical methods of sentence extraction summa-rization. 4 The expression for T(l) was chosen to assign the top node relative depth 1.  coordinations and n-aries are considered equally relevant and scored equally, whereas subordinated children are less relevant then subordinating ones. A rule based approach to discourse parsing.
W04-1013@@This is very expensive and difficult to conduct in a frequent basis. In the implementation, we use a Jackknifing procedure. This effectively gives more weight to matching n-grams occurring in multiple references.
W04-1015@@A sentence compression tool has been built withthe purpose of automating subtitle generation forthe deaf and hard-of-hearing. The database can also contain the tagof the abbreviated part (E.g. This is a verytime consuming task.
W04-1016@@ and MotivationThe practices of automatic summarization varywidely across many dimensions, including sourcelength, summary length, style, source, topic, lan-guage, and structure. A noisy-channelmodel for document compression. Automatic evaluationof summaries using n-gram co-occurrence statis-tics.
W04-1017@@The main goal of extractive summarization can beconcisely formulated as extracting from the inputpieces of text which contain the information aboutthe most important concepts mentioned in the inputtext or texts. This algorithm iterativelyadds to the solution S the set ti  T that locallymaximizes the increase in the total weight of ele-ments covered by S  The algorithm gives a so-lution with weight at least 1/(1  e) of the optimalsolutions total weight. Sentenceextraction as a classification task.
W04-1102@@The abbreviated form and root form are used interchangeably everywhere in the current Chinese articles. Unfortunately, a large Chineseabbreviation dictionary is not available. Also, we will assume that i jwshould at least contain the character that is aligned to it.
W04-1106@@ Sense tagging is an important task in NLP. In CILIN the verbs are put under the major classes from E to J, designating the concepts of attributes (E), actions (F), mental activities (G), activities (H), physical states (I), and relations (J). WW-asso(Wi,Wj) is therefore taken as the measure of S-template similarity (denoted as T-similarity).
W04-1107@@Their work has inspired many others to study chunking for other human languages. For example, one inputs a POS pattern: a_n_n, and an expected annotation result: B-NP_I-NP_E-NP3, the tool will list all the consistent and inconsistent sentences in the annotated text respectively. There is an example of a_n_n English also has such problem.
W04-1114@@ A Treebank can be defined as a syntactically processed corpus. Shallow parsing (or partial parsing) is usually defined as a parsing process aiming to provide a limited amount of local syntactic information such as non-recursive noun phrases, V-O structures and S-V structures etc. A Treebank specification was then documented.
W04-1116@@    For natural language understanding, the process of fine-grain semantic role assignment is one of the prominent steps, which provides semantic relations between constituents. Information-based Case Grammar: A Unification-based Formalism for Parsing Chinese. Taipei  speed-up  the  investments   in  Indonesia.DummyNPlocationPPSHeadVC31agentNPHeadNcaHeadP21HeadNcamannerDh b) Structure-dependent semantic roles assignments    Complex structures are always the hardest part of semantic roles assignments.
W04-1122@@ The unique feature of Chinese writing system is that it is character-based, not word-based. The specific n-grams for this category will not emerge on RFR values. In this phase, we segment each sentence, where each candidate ap-pears, in the source text with GPWS and eliminate the candidates cross word boundary; (d) Output the final terms on relative frequency ratios.
W04-1202@@ Numerous techniques help researchers locate relevant documents in an ever-growing mountain of scientific publications. Argumentation in biomedical abstracts Scientific research is often described as a problem solving activity. 2 Background Digital libraries aim at structuring their records to facilitate user navigation.
W04-1203@@The challenges of processing the vast amounts ofbiomedical publications available in databasessuch as MEDLINE have recently attracted aconsiderable interest in the Natural LanguageProcessing (NLP) research community. American Medical Informatics As-sociation, Bethesda, MD.Joshua M. Temkin and Mark R. Gilder. Pars-ing english with a link grammar.
W04-1205@@ Information extraction (IE) in the biomedical domain is now regarded as an essential technique for utilizing information contained in archived journal articles and abstract collections such as MEDLINE. We aim to provide a basis for this purpose. 2 Overview of the framework 2.
W04-1207@@ Developments in biology and biomedicine are reported in large bibliographical databases either focused on a specific species (eg Flybase, specialized on Drosophilia Menogaster) or not (eg Medline). Practically, each annotation aims at highlighting the set of words in the sentence describing:  Agents (A): the entities activating or controlling the interaction  Targets (T): the entities that are produced or controlled  Interaction (I): the kind of control performed during the interaction  Confidence (C): the confidence level in this interaction. Parsing English with a Link Grammar.
W04-1212@@The accelerating growth in biomedical literature isstimulating efforts both to screen individual papersquickly for useful information and to use aggrega-tions of papers for the collective information theyprovide. In this work:(i) articles were associated with GO codes; and then(ii) GO codes were assigned to new genes on the ba-sis of the GO-code associations with articles aboutrelated genes. The GeneOntology Consortiums aim is for gene products tobe described in a consistent manner across indepen-dent databases and species.
W04-1213@@Bio-entity recognition aims to identify and clas-sify technical terms in the domain of molecu-lar biology that correspond to instances of con-cepts that are of interest to biologists. This fact suggests that globaloptimization over whole sequence (e.g, Viterbioptimization) is crucial in this type of tasks. It also corre-sponds to the fraction in the whole MEDLINEdatabase: among 9,362 abstracts that can beretrieved with MeSH terms, blood cells andtranscription factors, 6,297 abstracts (67%)can also be retrieved with MeSH terms human,blood cells Consequently, a group of coordinatedentities involving ellipsis are annotated as onestructure like in the following example:... in [lymphocytes] and [Tand B-lymphocyte] count in ...In the example, Tand B-lymphocyte is an-notated as one structure but involves two entitynames, T-lymphocyte is annotated as one andinvolves as many entity names.prot.
W04-1215@@ The volumn of on-line material in the biomedical field has been growing steadily for more than 20 years. The frequency and t-test score were normalized to [0, 1]. A Practical Guide to Support Vector Classification.
W04-1216@@ In the Message Understanding Conference (MUC), Named entity Recognition aims to classify proper nouns, dates, time, measures and locations, etc. Estimation of Component of a Speech Recognizer. Explorations in Automatic Thesaurus Discovery, Kluwer Academic Publishers, Boston  Harris, Z.S.
W04-1217@@The explosion of information in the fields of molec-ular biology and genetics has provided a uniqueopportunity for natural language processing tech-niques to aid researchers and curators of databasesin the biomedical field by providing text miningservices. If nohits were returned by any pattern, a value O-webwas assigned. The final gazetteer contained 1,731,496 en-tries.
W04-1218@@German NER shares some char-acteristics with bio-entity recognition such as theunreliable capitalization of names, the resultingdifficulties of boundary detection and the entailedtreatment of homonymic and polysemic items. According to this, a label is revisable if thecompeting label is among the three best labels andhas a decision value higher than 0. , or if the valueof the outside-classifier is lower than 0. , ie thelabel OUTSIDE is not that confident. The local level describes a single occurrenceof a word form.
W04-1220@@ Recently, with the rapid growth in the number of published papers in biomedical domain, many NLP (Natural Language Processing) researchers have been interested in a task of automatic extraction of facts from biomedical articles. To simplify the classification problem, we assign each token only with I-[entity class]/O. First a list of high frequency keywords with class information is collected.
W04-1302@@ In recent years there has been a growing amount of work focusing on the computational modeling of language processing and acquisition, implying a cognitive and theoretical relevance both of the models as such, as well as of the language properties extracted from raw linguistic data. % and a recall of 13. We are able to identify phases in the generation of rules that turn out to be for English: a. initially inflectional morphology on verbs, with the plural s on nouns, and b. subsequently other types of morphemes.
W04-1501@@Different treebanks use different annotationschemes which make explicit two distinct butinterrelated aspects of the structure of the sen-tence, ie the function of the syntactic unitsand their organization according to a part-wholeparadigm. These elements allow for the annota-tion of a variety of phenomena: from the equideletion which affects the subject of infinitiveVerb depending on a tensed Verb (eg John(1)vuole T(1) andare a casa John(1) want toT(1) go home), to the various forms of gap-ping that can affect parts of the structure ofthe sentence (eg John va(1) a casa e MarioT(1) al cinema John goes(1) home and Mario7Referring to the current TUT corpus, we see thataround 7,7% words are amalgamated. A dependency tree-bank for English.
W04-1502@@(ibid) suggest that PGF is ex-tensible to other languages, including fixed wordorder languages such as English. Lexical-functional grammar: A formal system forgrammatical representation. The syntactic structure of 2(b) and(c) are isomorphous modulo word order.
W04-1510@@We introduce the new grammar formalism of Exten-sible Dependency Grammar (XDG). E.g.suppose there are two candidate c-structures in LFGparsing, but one is ill-formed semantically. A principle-based hi-erarchical representation of LTAG.
W04-1513@@ Dependency grammars have a long history and have played an important role in machine translation (MT). (e) stands for an empty node trace. Please note that the Bleu/NIST scorers, while based on n-gram matching, do not model syntax dur-ing evaluation, which means a direct comparison between a syntax based MT system and a string based statistical MT system using the above scorer would favor the string based systems.
W04-1602@@ Treebanks are language resources that provide annotations of natural languages at various levels of structure: at the word level, the phrase level, and the sentence level. 0 and a precision of 90. Matrix clause (S) coordination is possible and frequent.
W04-1612@@Inparticular, almost all additional text data thatcan easily be obtained (eg broadcast news cor-pora) is represented in standard script form. Thus, in thecase of a taa marbuwta ending with a followingcase vowel /i/, for instance, both the /t/ andthe /i/ need to be present. This entirely knowledge-free approach achieved a 16.
W04-1613@@ Text-to-speech synthesis is logically divided into two stages. As an example, adding this character adds aspiration to the phoneme /p/:  LZ Finally, there is also a single character in category (e), the Alef Madda (). This is done through a series of processes.
W04-1702@@ This paper is not a theoretical paper. Smart search engines strengthen the personalization aspect of e-learning. Bayer V., Farah J., Apprentissage des langues en Tandem sur Internet, ELA, n113, pp.
W04-1809@@Reflecting the rapid growth in science and tech-nology, new words have progressively been created.However, due to the limitation of manual compila-tion, new words are often out-of-dictionary wordsand decrease the quality of human language tech-nology, such as natural language processing, infor-mation retrieval, machine translation, and speechrecognition. Translating collocations forbilingual lexicons: A statistical approach. While we varied a threshold of a sim-ilarity, we also varied the number of Korean wordscorresponded to a single Katakana word (N).
W04-1901@@In this paper we describe the current state of a newlexical resource: the Hinoki treebank. Someone who drives a car . University of Chicago Press,Chicago.Melanie Siegel and Emily M. Bender.
W04-1906@@There is a growing insight that high-quality NLPapplications for information access are in need ofdeeper, in particular, semantic analysis. a relative pronoun of come thatcontrols the SUBJect of sell, (b.) In a given context, a predicatemay evoke alternative frames (ie word senses),where it is impossible to decide between them.E.g.
W04-2005@@In this paper we will present the parser used by thesystem GETARUN and discuss its performance withthe test suite called GREVAL set up by Carroll &Briscoe. [a,c,r]ambiguity class when preceded by a governingnoun check for verbs of sayingThis is used to disambiguate verbs preceded orfollowed by punctuation marks3. I am now referring to the class ofdeterminers which is used to tell apart wordsbelonging to the ambiguity class [verb,noun], themost frequent in occurrence in English.
W04-2007@@UNL is a project of multilingual personalnetworking communication initiated by the Uni-versity of United Nations based in Tokyo. In Natural Language Processing :the PLNLP approach, edited by K. Jensen, G.Heidorn and S. Richardson. A Robustand Flexible Platform for Dependency Ex-traction.
W04-2009@@Consider the task faced by a child trying to learnlanguage. (the GiveAction and coindexed GivingFrame) corre-sponds to the I will give you one phrase in whichthe Giving frames roles are filled by the speaker,the addressee and an Entity schema correspondingto the word one. New York: Academic Press.Charles Fillmore, Paul Kay, and M. C. OConnor.
W04-2103@@With the fast development of text miningtechnologies, automated management of lexicalresources is presently an important research issue.A particular text mining task often requires alexical database (eg, a thesaurus, dictionary, or aterminology) with a specific size, topic coverage,and granularity of encoded meaning. Different kinds of features derived fromprepositional phrases involving target nouns.On both classifiers and for both types D and E,the performance is noticeably higher when thecollocation of the noun with the preposition is usedas one single feature (D1 and E1). Thestatistical significance of differences betweenperformance of particular preprocessing methodsreported below was estimated by means of the one-tailed paired t-test..  DataThe set of nouns each provided with a classlabel to be used in the experiments was obtained asfollows.
W04-2206@@In particular, machinetranslation systems still mainly rely on largehand-crafted lexicons. ALT-J/E already has a very high coverageof native Japanese verbs. The translations were placed into threecategories: (i) A is better than B, (ii) A and B areequivalent in quality, and (iii) A is worse than B.For example in (2), the evaluation was (iii).
W04-2207@@ It is now an aknowledged fact that parallel corpora, ie corpora made of texts in one language and their translation in another language, are well suited in particular to cope with the problem of the construction of bilingual resources such as bilingual lexicons or terminologies. % for a linguistic knowledge based one. For instance, the pattern N-SUJ-V corresponds to the propagation going from a noun anchor pair to the verbs through the subject relation.
W04-2208@@One of the main reasons forthis is that we had many problems that had tobe solved by one-sentence to one-sentence ma-chine translation before we could solve the con-textual problem. (ni, case marker); (E)the UNCommittee on the Rights of the Child /(J) was extracted as atranslation pair in the third step. Exploiting a Probabilis-tic Hierarchical Model for Generation.
W04-2209@@  The JMdict project has as its primary goal the compilation of a Japanese-multilingual dictionary, ie a dictionary in which the headwords are from the Japanese lexicon, and the translations are in several other languages. Also it did not support the case where a sense or nuance was tied to a particular pronunciation, as occurs occasionally in Japanese;  e. provision for the inclusion of translational equivalents from several languages. (The two words derive from a common source.)
W04-2210@@In order to cope with information available inmany languages, modern information systemsneed large, high quality and multilingual lexi-cal resources. Papillon acception basedmultilingual databaseThe Papillon multilingual database has been de-signed independently of its usage(s). JMdict: a Japanese-Multilingual Dictionary.
W04-2214@@ The continuous expansion of the multilingual information society with a growing number of new languages present on the Web has led in recent years to a pressing demand for multilingual applications. MEANING: a Roadmap to Knowledge Technologies. (i) A series of non-consecutive codes is listed separated by a comma (see DANCE).
W04-2302@@This paper takes steps toward three surface genera-tion goals in dialogue systems; to create a domain-independent surface generator, to create a surface gen-erator that reduces dependence on large and/or domain-specific resources by using out of domain language mod-els, and to create an effective human-like surface genera-tor.Natural Language systems are relatively young andmost of todays architectures are designed and tested onspecific domains. The second is a domain dependentknowledge representation (KR). The languagemodels were standard n-gram approaches that dependedon a tagged air travel corpus for the attribute types.
W04-2305@@An important task in designing spoken dialogue systemsis to decide whether a system should accept (considercorrectly recognised) or reject (assume misrecognition)a user utterance. Instead of selecting correct interpretations,we imagine that one could also use the proposed setup todecide which of a finite set of dialogue moves was per-formed by a speaker.ReferencesAnanlada Chotimongkol and Alexander I. Rudnicky. N-best Speech Hypotheses Reordering UsingLinear Regression.
W04-2306@@This work was partly supported by the European Com-missions Information Society Technologies Programme undercontract no. The multi-lingual application isaccessible via a cellular or fixed network telephone.A manually refined version of the generated applica-tion is installed at Egnatia Bank in Greece and is used asa commercial product for phone banking..  CitizenCareCitizenCare is an e-government dialogue system forcitizen-to-administration interaction (via multiple chan-nels like internet and public terminals), filled with con-tent for an exemplary community. It provides a user-friendly interface where thedesign process is accelerated.
W04-2315@@the Universal Speech Interface project) is an attempt to create a standardized, speech-based interface for interacting with simple machines and information servers. %, with a mean of 80. One of the main differences between the NL pat-terns in the two systems was the lack of conversational phrases like can you give me and I would like to hear about in the Speech Graffiti system.
W04-2317@@Communication, between humans or between humansand conversational computer agents, involves address-ing. Contextualfeatures are used at each level of processing.Given all available multi-modal information E abouta conversational situation a statistical addressee identifi-cation method should classify the addressee for each dia-logue act in the conversation. Rather, it is asked in a broadsense.
W04-2318@@Contemporary theories of discourse, both computationaland descriptive, postulate a tree-structured hierarchicalmodel of discourse. A sample interaction ap-pears below. In Proceedings of the 32nd Annual Meet-ing of the Association for Computational Linguistics.G.-A.
W04-2319@@ Natural meetings offer rich opportunities for studying a variety of complex discourse phenomena. a well you actually are . 0, representing good agreement for this type of task.
W04-2322@@ In this paper we present an overview of recent theoretical and computational developments in discourse theory and parsing under the Linguistic Discourse Model (LDM) framework, a semantic account of discourse structure. Auxiliary and modal verbs [I might have succeeded.] c. Available at the S-node is information specific only to the subordinating or dominant constituent (usually the left child).
W04-2324@@Within a multi-level approach to discourse processing,this paper focuses on the semantic level. Secondly,among the DAGs which satisfy the left1-right2 principle,some are not instantiated, eg (E), and also (F). (S)DRS stands for (Segmented) Discourse Repre-sentation Structure.
W04-2325@@It is widely accepted that it would be desirable for dia-logue systems to be able to produce and understand thewhole range of Clarification Requests (CRs) that can befound in human-human dialogue, as exemplified in thefollowing:(1) a. Note that a confirmation request can be re-alised as an alternative question, as in (5-b-ii) and (4) b,or as a y/n-question, as in (1-c).Lastly, we also distinguish between CRs that point outa problematic element in the original utterance, and thosethat dont. (i) A: Can you pass me the salt B: The saltAs utterance is of course an example of an indirect speechact.
W04-2326@@This paper describes a coding scheme for annotating stu-dent emotional states in spoken dialogue tutoring cor-pora, and analyzes the scheme not only for its reliabil-ity, but also for its utility in developing a spoken dia-logue tutoring system that can model and respond to stu-dent emotions. In ISCA Workshop on Speech andEmotion.A. 0 .tutor it does not mean that uh uh I mean it will um274.
W04-2327@@As a result of this work, many aspectsof the proposals concerning anaphoric annotation madein MATE and GNOME have been subjected to a thoroughtest. Extending the range of re-lations to include, for example, attributes (e.g, I am notgoing to buy that. A Formal Approach to DiscourseAnaphora.
W04-2401@@Natural language decisions often depend on the out-comes of several different but mutually dependent predic-tions. ofUncertainty in Artificial Intelligence, pages 485492.E. We use N 1 and N 2 to denote the entityvariables of a relation Rij .
W04-2403@@Hence, to deal with nat-ural language semantics, the learning algorithm should beable to represent and process structured data. D N is used in its generation.However, this constraint does not apply to the productionVP V NP PP along with the fragment [VP [V NP]] asthe subtree [VP [PP [...]]] is not considered part of thesemantic structure.Even if the cardinality of F  will be very large the eval-uation of the kernel function is polynomial in the numberof parse-tree nodes.More precisely, a semantic structure ~x is mapped in 1 can be computed in O(|Nx|  if the production at nx and nz are different then In fact twosentences such as:(1) [Arg0 Paul ][predicate delivers ] [Arg1 a lecture] and(2) [Arg0 Paul ][predicate delivers ][Arg1 a plan on the de-tection of theorist groups active in the North Iraq]have the same argument type with a very different size.To overcome this problem we can scale the relative im-portance of the tree fragments with their size. This problem can be divided in two sub-tasks: (a) detection of the target argument boundaries,ie all its compounding words, and (b) classification ofthe argument type, eg Arg0 or ArgM.A direct approach to learn both detection and classifi-cation of predicate arguments is summarized by the fol-lowing steps:1. extract the feature representation set, Fp,a; if the subtree rooted in a covers exactly thewords of one argument of p, put Fp,a in T+(positive examples), otherwise put it in T In case the nodea exactly covers Paul, a lecture or in Rome, it will be apositive instance otherwise it will be a negative one, egFgive,IN.The above T+ and T sets can be re-organized as posi-tive T+argi and negative Targi examples for each argumenti.
W04-2405@@The task of word sense disambiguation consists in assign-ing the most appropriate meaning to a polysemous wordwithin a given context. The valueof this feature is either 0 or 1, depending if the currentexample contains one of the determined keywords or not.B (T) Maximum of M bigrams occurring at least N times are deter-mined for all training examples. However,with empirical settings, the error reduction is significantlysmaller, with a 9.
W04-2406@@Most words in natural language have multiple possiblemeanings that can only be determined by consideringthe context in which they occur. The SENSEVAL-2 words have a relatively smallnumber of training and test instances (around 50-200).However, the Line, Hard and Serve data is much larger,word.pos TRN TST S PB1 SC1 PB2 SC2 PB3 SC3 MAJart.n 159 83 4 37. Thereafter weshow the number of training (TRN) and test instances(TST) that remain after filtering, and the number ofsenses found in the test data (S).
W04-2407@@De-terministic parsing means that we always derive a singleanalysis for each input string. A Statistical Parser of Czech. wj to say that there is an arc from wi to wj(regardless of the label); we use  to denote thereflexive and transitive closure of the unlabeled arcrelation; and we use  for the correspond-ing undirected relations, ie wi  Given an inputstring W , the parser is initialized to nil,W,  (for anylist S and set of arcs A).
W04-2413@@First, we compute chunk sequences for all sentences(Sec. We found theoptimal category-specific F-score for * l^S m, increas-ing the recall at the cost of precision.Optimising Step 2 (Argument Labelling). The result of thebeam search are the N most probable (according to O#)chains that cover the whole sentence.
W04-2416@@In semantic role labeling the goal is to group sequencesof words together and classify them by using semantic la-bels. (e.g *S)*S) marksa position that two clauses end) Named entities: The IOB tags of named entities.There are four categories; LOC, ORG, PERSONand MISC.Using available information we have created the fol-lowing token level features: Token Position: The position of the phrase with re-spect to the predicate. SVMs were trained forbegin (B) and inside (I) classes of all arguments and oneoutside (O) class for a total of 78 one-vs-all classifiers(some arguments do not have an I-tag).
W04-2419@@The semantic role represents the relationship between apredicate and an argument. A Gaussian prior forsmoothing maximum entropy models . More specifically, we use some rules to attach theV, AM-MOD, and AM-NEG, and extend the boundaryof core roles to include to infinitive of the VP chunk likeexpect/B-VP (A1 to/I-VP take/I-VP dive/B-NP).
W04-2420@@Especially, for each target verb, allconstituents in a sentence which fill semantic roles of theverb have to be recognized. (A means accuracy. That is, since we find only the bound-ary of an argument in the identification phase, the num-ber of classes is decreased into two (ARG, NON-ARG)or three (B-ARG, I-ARG, O).
W04-2422@@The space of allowable transformations3. For example, a rule that the systemmight produce from template e In this paper, we use the term region are meant to capture structural rela-tionships among arguments, such as the fact that A1 re-gions usually follow V regions, or that arguments mayconsist of several NP chunks joined by PP chunks. A Corpus-Based Approach to LanguageLearning.
W04-2501@@ Our fundamental premise is that progress in Q/A cannot be achieved only by enhancing the processing compo-nents, but it also requires generating the best strategies for processing each individual question. Consider the fol-lowing examples:  a. However, examples (b) and (c) are ambiguous with respect to the scope of negation.
W04-2502@@  Current Question Answering systems extract an-swers from large text collections by (1) classifying questions by the answer type they expect; (2) using question keywords or patterns associated with questions to identify candidate answer passages and (3) ranking the candidate answers to decide which passage contains the exact answer. A Noisy-Channel Approach to Question Answering. The quality of the inferred an-swer is measured by (a) the judges; (b) the judge types; (c) judgment manner and (d) judgment stage.
W04-2504@@ In a real-world setting, questions are not asked in isolation, but rather in a cohesive manner that involves a sequence of related questions to meet users information needs. E. Hovy and D. Scott), volume 151 of NATO ASI Series, Series F: Computer and Systems Sciences, pp 139-157. As context question answering proceeds, the semantic network(s) for discourse grows, with different pointers of Topic and Focus.
W04-2508@@Over the past five years, much research has focused onthe different challenges question-answering systems facewhen answering questions in isolation as opposed toquestions presented as part of a contextualized interac-tion with a user.The domain of interactive question answering is typ-ically concerned with two tasks: the decomposition ofcomplex questions into questions that can be processedby current Question/Answering (Q/A) systems and thedynamic representation of dialogue between a user anda Q/A system. CO-GEX: A Logic Prover for Question Answering. Scenarios and QuestionsSince complex questions represent such diverse infor-mational goals, it should not be assumed that even thedecompositions produced by expert users will be suffi-ciently simple enough to be processed by current Q/Asystems.COMPLEX QUESTION:What is the current status of Indias Prithvi ballistic missile projectDECOMPOSITION(1) (a)  How should India (2) (b) What does Prithvi mean (2) (c) What class of missiles does Prithvi belong to (2) (d) What is its range/payload, and other technical details (3) (a)  What is the meaning of status Three arediscussed below:Clarification Questions.
W04-2509@@Open-domain question answering technologies allowusers to ask a question in natural language and obtainthe answer itself rather than a list of documents that con-tain the answer. Series 20 shown in Fig-ure 2 is such a series. Thefirst concerns the F measure.
W04-2510@@Generally, they are open-domain systems, and do not rely on specialised concep-tual knowledge as they use a mixture of statistical tech-niques and shallow linguistic analysis. This is unfeasible in a multilingual environment. Let us consider the case of a student questioning   not only the Danish but also the Italian site (by selecting specific modalities for entering questions):   (3) Hvem er lektor i fransk (Who is associate professor of French) As the question is in Danish, it has to be analysed by the Danish analysis component, which will produce a semantic interpretation roughly corresponding to the following term:  all(x) (lektor(x) & CourseOffer(x,y) & Course(y) & Name(y, French))4  Since all concepts and relations come from the Dan-ish ontology, it is not a problem to query the Danish knowledge base for all relevant examples.
W04-2602@@A lexicon is a key resource for natural language process-ing, providing the link between the terms of a languageand the semantic and syntactic properties with whichthey are associated. It is large or small depending on whether   has manyor few probable values.The mutual information between random variables  and  can be written:`KaPbX$flZ+R	fiE/]\^Z+%	fiE/Z+L/Z+fiE/ (2)This quantifies the amount that one expects to learn in-directly about   upon learning the value of  , or viceversa. The Hellinger distance between twodistributions A and p is defined as:qOrA	ptsWufVkX,bv8,v w, (4)The candidate MWU is then tentatively assigned tothe cluster for which this quantity is minimized andits distance to this cluster is noted (call this distanceScore Band % in Wordnetx 0.
W04-2604@@There is currently much interest in training super-vised systems to perform shallow semantic annota-tion tasks such as word sense tagging and semanticrole labeling. (4) wsj/05/wsj 0568.mrg 12 4:The tax payments will leave Unisys with $ 225million *U* in loss carry-forwards that *T*-1 willcut tax payments in future quarters . Extending a verb-lexicon using asemantically annotated corpus.
W04-2606@@Fundamentally, such classes definethe mapping from surface realization of arguments topredicate-argument structure and are therefore a criticalcomponent of any NLP system which needs to recoverpredicate-argument structure. In Conference on EmpiricalMethods in Natural Language Processing, Philadel-phia, USA.E. State University of New York Press, Albany.N.
W04-2607@@These relations, whichhave been widely studied and applied, are characterizedby a sharing of the same individual defining propertiesbetween the words and a requirement that the words beof the same syntactic class. A couple of exceptions to the above mentionedrelation types have been noted. As an example, consider thefollowing two sentences taken from a Readers Digestarticle:I attended a funeral service recently.
W04-2608@@ Prepositions have been studied from a variety of per-spectives. d.  It probably began over the weekend          of March 27.  e.  His father had died over the weekend. He quit over a bribery scandal.
W04-2611@@Several approaches to text-based information manage-ment applications are being pursued, including word-based statistical processing and those depending on string matching, syntax, or semantics. J Am Med In-form Assoc, 5(1):1-11. cquelinet C, Burgun A, Delamarre D, Strang N, Djab-bour S, Boutin B, Le Bontological foundations of a terminological system for end-stage diseases, organ failure, dialysis and transplantation. WordNets synsets), which constitutes a concept.
W04-2703@@Large scale annotated corpora have played a critical rolein speech and natural language research. The total num-ber of tokens in this class is therefore twice the numberof connective tokens, i.e, 5434. (1) Because [the drought reduced U.S. stockpiles], [theyhave more than enough storage space for their newcrop], and that permits them to wait for prices to rise.. .
W04-2704@@ An important question is the degree to which current statistical NLP systems can be made more domain-independent without prohibitive costs, either in terms of engineering or annotation. Bill served him a beer. For example, in the sentence in (8a) the temporal ArgM for the past five years does not modify the event variable e introduced by the verb manage, as our auto-matic translation would predict.
W04-2709@@ This paper describes a multi-site National Science Foundation project focusing on the annotation of six sizable bilingual parallel corpora for interlingual content with the goal of providing a significant data set for im-proving knowledge-based approaches to machine trans-lation (MT) and a range of other Natural Language Processing (NLP) applications. In E. Bach and R. Harms, editors, Universals in Linguistic Theory, pages 1--88. We considered him a fool.
W04-2802@@Fur-thermore no general computational method or frameworkfor measuring the difficulty of natural language under-standing tasks have been proposed so far. A methodfor evaluating incremental utterance understandingin spoken dialogue systems. can be set to reflect the respective importanceof p versus r, if  For evaluat-ing discourse understanding systems, however, such goldstandards and annotated training corpora will continue tobe needed.
W04-2805@@Within itsunification-based framework, constructions, frames andmental spaces are combined compositionally, yielding anetwork of entwined semantic and pragmatic structuresrepresenting the overall interpretation. In E, we see a completely matched Caused-Motionconstruction with a complete Caused-Motion scene andan empty constituent graph indicating that a complete in-stance of this construction has been found.In short, the construction recognizer builds up a graphdata structure to keep track of the constituents and an in-progress semspec to keep track of the semantics. Since this processing model is bottom up, first the level 0 reductionsare performed, then the level 1 reductions, then level 2 and finally level 3.
W04-2807@@ The difficulty of finding consistent criteria for making sense distinctions has been thoroughly attested to in the literature (Kilgarriff, 97, Hanks, 00). Tips on Being a Successful Movie Vampire ... I shall call the police.
W04-2902@@In the past decade, we have seen a dramatic increasein the availability of on-line academic lecture material.These educational resources can potentially change theway people learn  students with disabilities can en-hance their educational experience, professionals cankeep up with recent advancements in their field, and peo-ple of all ages can satisfy their thirst for knowledge. The remainingcurves were all computed from subject-specific material.Curve (D) was computed from a companion textbook,while curve (E) was computed from the first half of thecomputer science lectures. A probabilistic framework for segment-based speech recognition.
W04-2905@@Proper names are often key to our understanding of theinformation conveyed by a document. An algorithm that learns whatsin a name. Any opinions, findings and conclusions or recom-mendations expressed in this material are the author(s)and do not necessarily reflect those of the sponsor.ReferencesJames Allan.
W04-2906@@Natural spoken discourse is composed of a sequenceof utterances, not independently generated or randomlystrung together, but rather organized according to basicstructural principles. Durational fea-tures play a lesser role in the classifier. Specifically, do we find intona-tional cues in tone languagesWe have found highly significant differences basedon paired t-test two-tailed, ( ffflfiffi "!
W04-3101@@ This paper describes a methodology and data for the testing of molecular biology entity identification (EI) systems by developers and end users. A catalogue of false positives. The system will have low recall on entities that have numerals in initial position, followed by a dash, eg 825-Oak, 12-LOX, and 18-wheeler (/^\d+-/ in Perl).
W04-3102@@ With the increasing number of biomedical papers, and their electronic publication in NCBI-PUBMED, there is a growing focus on information retrieval from texts. In considering syntactic variations, some GATA-4/5/6 expression constructs ...  G A T 4 5 / / 6 A If next word is not stop word, the ID is outputted. The hierarchy has a directed acyclic graph structure.
W04-3103@@The scientific process involves making hypotheses,gathering evidence, using inductive reasoning toreach a conclusion based on the data, and then mak-ing new hypotheses. 400 crohns abstracts additional (last 2 sen-tences) by Qiu,e. The definite sentences below vary in topic andform.Affinity chromatography and coimmunoprecipi-tation assays demonstrated that c-Jun and T-Agphysically interact with each other.
W04-3111@@In this work we describe an ap-proach to two areas of biomedical information extraction,drug development and cancer genomics, that is based ondeveloping a corpus that integrates different levels of se-mantic and syntactic annotation. We would also be able to la-bel adjuncts consistently, such as the with prepositionalphrase in CYP3A4 activity was decreased by L, S and Fwith IC(50) values of about 200 mM. A fully statistical approach tonatural language interfaces.
W04-3201@@Recent work has shown that discriminativetechniques frequently achieve classification ac-curacy that is superior to generative techniques,over a wide range of tasks. Inthis representation, indices s and e refer to po-sitions between words, rather than to wordsthemselves. This changealone results in a small improvement (88.
W04-3202@@However, ex-periments using AL assume a model that is fixedahead of time: the model used in AL is the sameone we are currently developing training materialfor. to the needs of a particu-lar model. Touretzky, and T. Leen, editors, Advances in Neural Infor-mation Processing Systems, volume 7, pages 705712.
W04-3203@@A fundamental result of formal language the-ory is that the languages defined by context-freegrammars are the same as those accepted bypush-down automata. over trees and strings.By contrast, a parsing model defines P (t| ),conditioned on the input string. Eachparser also has a stack.
W04-3204@@However, the main draw-back for supervised WSD is the knowledge acqui-sition bottleneck: the systems need large amountsof costly hand-tagged data. Publicly availabletopic signatures for all wordnet nominal senses.In Proceedings of the 4rd International Con-ference on Language Resources and Evaluation(LREC), Lisbon, Portugal.E. Wasp-bench:a lexicographic tool supporting word sense dis-ambiguation.
W04-3206@@Modeling semantic variability in language hasdrawn a lot of attention in recent years. Create a pivot template, Tp2. The set of such sentences is defined as the sen-tence set of v (e), and is represented through the setof index numbers of related sentences (eg  The algorithm starts with an empty aggregategraph G0 and then merges the sentence graphs fromS one at a time into the aggregate structure.Lets denote the current aggregate graph withGi1(Vg, Eg) and let Pi(Vp, Ep) be the parse graphwhich will be merged next.
W04-3207@@Consider the problem of parsing a language L for whichannotated resources like treebanks are scarce. |E| + |F | Training Unlabeled Unlabeled Labeled Labeled CrossingTest Sen. A statistical approach to machine trans-lation.
W04-3209@@Since standard speech recog-nizers output an unstructured stream of words, im-proving transcription means not only that word ac-curacy must be improved, but also that commonlyused structural features such as sentence boundariesneed to be recognized. Universityof Stuttgart, Internal Report.E. A maximum entropy approach tonatural language processing.
W04-3210@@Written texts are usually broken up into sentencesand paragraphs. In Proceedings of the FifthConference on Applied Natural Language Pro-cessing, Washington, DC.Robert E. Schapire and Yoram Singer. The paragraph as a gram-matical unit.
W04-3213@@In particular, la-belling the semantic roles of the arguments of a verb(or any predicate), as in (1) and (2), provides crucialinformation about the relations among event partic-ipants. Probability estimationusing a semantic hierarchy. of the 6th Workshop on Very LargeCorpora.S.
W04-3215@@One motivation for usingCCG is the recovery of the long-range dependenciesinherent in phenomena such as coordination and ex-traction. What kind of a sports team is the Wisconsin Badgers12. Object Extraction in QuestionsFor the object extraction evaluation we consideredthe 36 questions in the development data whichhave the category (S[wq]/(S[q]/NP))/N assigned toWhat.
W04-3216@@There are a wealth of document/abstract pairs thatstatistical summarization systems could leverage tolearn how to create novel abstracts. 56 and a recall nearing0. However,if one were to build a decoder based on this model,one would need to account for this issue to avoiddegenerate summaries from being produced.The formal mathematical model behind the align-ments is as follows: An alignment  defines botha segmentation of the summary S and a mappingfrom the segments of S to the segments of the doc-ument D. We write si to refer to the ith segment ofS, and M to refer to the total number of segments2We write xba for the subsequence xa .
W04-3217@@Writingis viewed as a skill-based task, with skills beingelements of writing such as spelling, diction, andplot development. In the pseudo-code, E of size h and R of sizej are the event structures of the exemplar story andrewritten story respectively, with the names of eachof their events denoted as e and r. The set of entitiesof each event are denoted as Ne and Nr respectively.T is the lemmatized tokens of the rewritten storysraw text. WordNet(x) denotes the synset of x. of the rewritten story is t, and featureset is f , which has an index of i.
W04-3218@@Traditionally, this data isused for supervised system evaluation. As can be seen, in theExample dialog from C1: System: How may I help you System: Do you want to arrange a billpayment System Call-type: YesExample dialog from C2: System: How may I help you User: Yes somebody just called me fromI dont know its collect or something likethat and it had to do with A T and T andwhen... System: Do you have a question about aspecific charge on your bill User: It just said call you guys for helpabout trying to get through to a long dis-tant number and its not getting throughwhen I called the long distant ... Each cluster is identified by variousfeatures computed, characterizing the languageand the interaction. ), and with noevidence of a problem.
W04-3219@@ The ability to categorize distinct word sequences as meaning the same thing is vital to applications as diverse as search, summarization, dialog, and question answering. We view the source and target sentences S and T as word sequences s1..sm and t1..tn. R. Barzilay and L. Lee.
W04-3220@@Natural language processing research has tradition-ally been divided into a number of separate tasks,each of which is believed to be an important sub-task of the larger language comprehension or gener-ation problem. Thenwe iterate over the following steps: E-step: Using the current model parameters,for each datum in the sense-annotated corpus,compute expectations over the possible SCFs,and for each datum in the SCF-annotated cor-pus, compute expectations over the possiblesenses. AcknowledgementsThis paper is based on work supported in part bythe Advanced Research and Development Activity(ARDA)s Advanced Question Answering for Intel-ligence (AQUAINT) Program, and by the NationalScience Foundation under Grant No.
W04-3221@@(We will use the term values here to refer to any modifier.) In determining the weight, we performed the t test5 on boolean values instead of the original                                                      5 We consider only positive values of t. frequencies6, treating all positive frequencies as 1 and everything else as 0. However, these data were used in a different way.
W04-3222@@In this work we are concerned with building sta-tistical models for parse disambiguation  choos-ing a correct analysis out of the possible analysesfor a sentence. The le-types of the words let, plan, on, and thatare, with abbreviations, v sorb, v e p, p reg, andn deic pro sg respectively. Any character matches a wildcard.
W04-3223@@The maximum-entropy (ME) principle, which pre-scribes choosing the model that maximizes the en-tropy out of all models that satisfy given featureconstraints, can be seen as a built-in regularizationmechanism that avoids overfitting the training data.However, it is only a weak regularizer that cannotavoid overfitting in situations where the number oftraining examples is significantly smaller than thenumber of features. In Pro-ceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL99),College Park, MD.Ronald M. Kaplan, Stefan Riezler, Tracy H. King,John T. Maxwell III, and Alexander Vasserman. A comparison of algorithmsfor maximum entropy parameter estimation.
W04-3224@@Their success has often beenattributed to their sensitivity to individual lexicalitems, and it is precisely this incorporation of lexicalitems into features or parameter schemata that givesrise to their complexity. % overall, with a rate of 22. Thanks also to FernandoPereira, with whom I had invaluable discussionsabout distributional similarity.
W04-3228@@In re-cent years, a number of syntactically motivated ap-proaches to statistical machine translation have beenproposed. children(ta))insertion  symbollexical translation Pt(f |e)cloning Pmakeclone() of the children of ta andtb do Because we restrict our ele-mentary trees to include at most one child of the rootnode on either side, choosing elementary trees for anode pair is O(m2), where m refers to the maxi-mum number of children of a node. Thus overall complexity ofthe algorithm is O(|T |2m242m(2m)!
W04-3229@@Morphological processing and part-of-speech tag-ging are essential for many NLP tasks, includingmachine translation, information retrieval and pars-ing. For example, the word vidjela is assignedthe tag VpFS-XR-AA-, because it is a verb (V),past participle (p), feminine (F), singular (S), doesnot distinguish case (-), possessive gender (-), pos-sessive number (-), can be any person (X), is pasttense (R), is not gradable (-), affirmative (A), activevoice (A), and does not have any stylistic variants(the final hyphen).No. We admitwe expected a larger improvement.
W04-3233@@ and Prior WorkNoun Phrase Bracketing (NP Bracketing) is the taskof identifying any and all noun phrases in a sen-tence. It is always thecase that t  The time and space complexityof this decoding problem is thus O(d2n). The value Ai,d,t stores theprobability of being at position i and depth d af-ter applying tag t at that position.
W04-3234@@The problem of named entity recognition (NER) hasrecently received increasing attention. Class-based n-grammodels of natural language. Each part is a list of zeroor more elements.
W04-3236@@ Most corpus-based language processing research has focused on the English language. To determine the boundary tag assignment to the last word W with K characters, the first character of W is assigned boundary tag b, the last character of W is assigned tag e, and the intervening characters are assigned tag m. (If W is a single-character word, then the single character is assigned s.) In this way, the dynamic programming algorithm only considers valid tag sequences, and we are also able to make use of the 00CW feature during testing. R. Sproat and T. Emerson.
W04-3237@@Automatic capitalization is a practically relevantproblem: speech recognition output needs to becapitalized; also, modern word processors performcapitalization among other text proofing algorithmssuch as spelling correction and grammar checking.Capitalization can be also used as a preprocessingstep in named entity extraction or machine trans-lation. A Maximum Entropy Approachto Natural Language Processing. For example, one may wish to use a capi-talization engine developed on newswire text for e-mail or office documents.
W04-3238@@Very few spell checkers attempt to detect and correct word substitution errors, which refer to the use of in-lexicon words in inap-propriate contexts and can also be the result of both typographical mistakes (such as typing coed instead of cord) and cognitive mistakes (eg prin-cipal and principle). References Brill, E. and R. Moore. A contextual post-processing system for error correction using binary n-grams.
W04-3239@@Text classification plays an important role in orga-nizing the online texts available on the World WideWeb, Internet news, and E-mails. from nodes int to u, satisfying the conditions: (1)  preserves the labels.We denote the number of nodes in t as |t|. The complexity ofSVMs with tree kernel is O(L|N1||N2|), where N1and N2 are trees, and L is the number of supportvectors, which is too heavy to realize real applica-tions.
W04-3240@@ In this paper we discuss using machine learn-ing methods to classify email according to the intent of the sender. Proceedings of the 9th ACM SIGKDD, Washington, D.C.  V. Bellotti, N. Ducheneaut, M. Howard and I. Smith. Learning Rules that Classify E-Mail.
W04-3241@@The motivation for this comes from in-formation theory: the most efficient way of trans-mitting information through a noisy channel is at aconstant rate. Sapporo, pages 6572.Just, Marcel A. and Patricia A. Carpenter. The correlations forthe other n-gram models are lower.
W04-3242@@In many systems dealing with natural speech or lan-guage, such as Automatic Speech Recognition andStatistical Machine Translation, a language modelis a crucial component for searching in the oftenprohibitively large hypothesis space. A study of n-gram and decision tree letterlanguage modeling methods. is stillbased on a greedy approach.
W04-3243@@Nevertheless, its use remainscontroversial on the grounds that it may be unreli-able when applied to rare events. Significant Lexical Relationships.In Proceedings of the 13th National Conferenceon Artificial Intelligence, Portland, Oregon.William H. Press, Saul A. Teukolsky, William T.Vetterling, and Brian P. Flannery. ), based onthe identity n!
W04-3244@@Natural language processing involves many kinds oflinguistic expressions, such as sentences, phrases,documents and the collection of documents. Salton and C. S. Yang. http://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html.Eric P. Xing, Andrew Y. Ng, Michael I. Jordan,and Stuart Russell.
W04-3245@@State-of-the-art machine translation techniques arestill far from producing high quality translations.This drawback leads us to introduce an alternativeapproach to the translation problem that bringshuman expertise into the machine translation sce-nario. In practice, however, it may happenthat t is not present in the word graph   . Transforming the inferred regular grammarinto a transducer.
W04-3246@@The standard account of word-formation processesin Semitic languages describes words as combina-tions of two morphemes: a root and a pattern. It does notexplicate a and e vowels, does not distinguish be-tween o and u vowels and leaves many of the ivowels unspecified. We therefore trained the classifiersto consider as targets only letters that occurred inthe observed word, plus w, i, n and l, rather thanany of the alphabet letters.
W04-3247@@Text summarization is the process of automaticallycreating a compressed version of a given text thatprovides useful information for the user. This is a totally democraticmethod where each vote counts the same. Automatic evalu-ation of summaries using n-gram co-occurrence.
W04-3248@@ This paper addresses the Named Entity (NE) alignment of a bilingual corpus, which means building an alignment between each source NE and its translation NE in the target language. (For simplicity of denotation, we here use e and c to represent English NE and Chinese NE instead of ene and cne ). S is the alignment set we compute with our models based on S, and T is the set consisting of all the true alignments based on S. We define the evaluation metrics of precision, recall, and F-score as follows:                                                      3 http://www.isi.edu/~och/GIZA++.html [China] hopes to further economic  as uniform distribution; 2.
W04-3250@@Recently, the field of machine translation has beenchanged by the emergence both of effective statisti-cal methods to automatically train machine transla-tion systems from translated text sources (so-calledparallel corpora) and of reliable automatic evalua-tion methods.Machine translation systems can now be built andevaluated from black box tools and parallel corpora,with no human involvement at all.The evaluation of machine translation systemshas changed dramatically in the last few years. How-ever, in case of Students t-distribution, the solutionto this does not exist in closed form, but we can usenumerical methods.The size of the confidence interval can be com-puted byffi	 (5)The factor  depends on the desired p-level of sta-tistical significance and the sample size. Given a test set, we can compute aBLEU score.
W04-3251@@Although the field is still domi-nated by knowledge-intensive approaches, compo-nents such as question classification, answer extrac-tion, and answer verification are beginning to be ad-dressed through statistical methods. From the retrieved documents a setof content features (n-grams and paraphrases) wereselected through average mutual information. Each feature has also a corresponding averagemutual information score.
W04-3252@@Arguably, these algorithms can be singled outas key elements of the paradigm-shift triggered inthe field of Web search technology, by providing aWeb page ranking mechanism that relies on the col-lective knowledge of Web architects rather than in-dividual content analysis of Web pages. Technical report, Yale University.A. 1: On Saturday, Hurricane Florence was downgraded to a tropical storm, and its remnants       pushed inland from the U.S. Gulf Coast.
W04-3253@@Recently an increasing amount of research has beendevoted to investigating methods of recognizing fa-vorable and unfavorable sentiments towards specificsubjects within natural language texts. All others were obtained with a linear kernel.ness. InProceedings of the 27th Annual Conference of theACL, New Brunswick, NJ.N.
W04-3254@@However, extrinsic eval-uations are time-consuming to set up and canthus not be used for the day-to-day evaluationneeded during system development. For exam-ple, given 5 summaries a, b, c, d, e, AnnotatorA1 assigns P30 to summaries a, c and e. An-notator A2 (who has split P30 into F9. In Proceedings of theTwelfth International Conference on Informationand Knowledge Management , 508511.Rath, G.J, A. Resnick, and T. R. Savage.
W04-3255@@Therefore, with this model it is al-most impossible to search for optimal solutions inthe decoding process. First, T model(D ) trans-lates the Japanese word to an English word. These trimmed paths maycause a slight difference in NIST scores.
W04-3256@@Automatic text summarization is one form ofinformation management. We found 9 commonelements: bio  (info on birth and death), f a m efactor, personality, personal, social, education,nationality, scandal, and w o r k . WordNet: a lexical databasefor English.
W05-0101@@Thegoal of this course was to acquaint students with thestate-of-the-art of the field of NLP with an empha-sis on applications. A companion graduate course on Statis-tical NLP was taught by Dan Klein in the Com-puter Science department. (I alsowanted support for entity extraction, which NLTKdoes not supply.)
W05-0110@@ A new undergraduate teaching program, BA Lan-guage Technology, was recently introduced at the Potchefstroom Campus of the North-West Univer-sity (NWU). At any given time, there are a number of ongoing projects at the university. 2 Program Structure  After wide consultation with international and lo-cal role players and experts, a program was de-signed that combines language subjects and natural sciences (mainly computer science, mathematics and statistics) with a core group of computational linguistic and language technology subjects.
W05-0111@@ This paper presents both an overview and some of the details regarding audience, assignments, tech-nology, and projects in an interdisciplinary course on Natural Language Processing that has evolved over time and been successful along multiple di-mensions  and the fac-ultys perspective in terms of accomplishments and enjoyment. In a more extensive curriculum, courses provide a greater depth than is possible in our single course. The course has been taught every 1 to 2 years for the last 18 years.
W05-0201@@Ap-plying techniques for natural language processing(NLP), CAIG offers the possibility of creating alarge number of items of different challenging lev-els, thereby paving a way to make computers moreadaptive to students of different competence. A cloze look at place-ment testing. has two or more signals in T and isa verb, noun, adjective, or adverb.
W05-0202@@  Our aim is to investigate computational linguistics techniques in marking short free text responses automatically. If the student writes Y and D s/he will get only 1 mark. Automatically constructing a dictionary for information extraction tasks.
W05-0203@@Multiple-choice question exams are widely used andare effective to assess students But to the best of our knowledge, no attempthas been made to generate this kind of questions ina totally automatic way.This paper presents a novel approach to generatemultiple-choice questions using machine learningtechniques. Naive Bayes tends to choose the5A blank on a verbs or a part of idioms (as [according] to)was evaluated as E, most of the blanks on an adverbs, and (as[now]) were D and a blank on a punctuation or a quotation markwas NG. In this way, a training setwith 113 true instances was obtained..
W05-0206@@The main motivations behind developing automatedessay assessment systems are to decrease the time inwhich students get feedback for their writings, andto reduce the costs of grading. Page and N. S. Petersen. The e-rater scoring engine: Auto-mated essay scoring with natural language process-ing.
W05-0207@@To plagiarize is to steal and pass off (the ideasor words of another) as ones own; [to] use (an-others production) without crediting the source; [or]to commit literary theft [by] presenting as new andoriginal an idea or product derived from an exist-ing source. H. Witten and E. Frank. English Verb Classes and Alternations.A Preliminary Investigation.
W05-0208@@ This paper reports work in progress towards developing TuTalk, an authoring environment developed with the long term goal of enabling the authoring of effective tutorial dialogue agents. Towards Easier Creation of Tutorial Dialogue Systems: Integration of Authoring Environments for Tutoring and Dia-logue Systems, Proceedings of the ITS Workshop on Tutorial Dialogue Systems  Allen, J., Byron, D., Dzikovska, M., Ferguson, G., Galescu, L., & Stent, A. A framework for robust semantic interpretation.
W05-0302@@However, the creation of semantically annotated corpora has lagged dramatically behind the creation of other linguistic resources: in part due to the perceived cost, in part due to an assumed lack of theoretical agreement on basic semantic judgments, in part, finally, due to the understandable unwillingness of  research groups to get involved in such an undertaking. of the ACL Workshop on Sharing Tools and Resources for Research  and Education, 23-30 D. Day,  L. Ferro, R. Gaizauskas, P. Hanks, M.  Lazo, J. Pustejovsky, R. of the First International Conference on Language Resources and Evaluation. In I. Mani, J. Pustejovsky, and R. Gaizauskas, editors, The Language of Time: A Reader.
W05-0303@@We definereferential relations as a cover-term for all contex-tually dependent reference relations. He is in a bad way. The subordinate clause is again divided intothree topological fields: C (for: Komplementierer complementizer Edge labels are renderedin boxes and indicate grammatical functions.
W05-0304@@A great deal of annotation effort for many differentcorpora has been devoted to annotation for entitiesand syntactic structure (treebanks). For ex-ample, in Kand N-ras there are two entities, K-rasand N-ras, of which only the second is a solid blockof text. A novel use of statistical parsing toextract information from text.
W05-0305@@These ascrip-tions need not be the same. Let T be the set of nodes in the tree.Definition 4. . xConn+Arg1+Arg2is the lowest node with label S orSBAR such that:xConn+Arg1+Arg22 Ancestorparent(xConn+Arg2)5.
W05-0306@@For many applications of natural language tech-niques such as question-answering systems and di-alogue systems, acquiring knowledge about causalrelations is one central issue. Thus, we set a constraint on the ranges ofjudgements. of the HLT/NAACL Workshop on Frontiers inCorpus Annotation.S.
W05-0307@@We present a framework for the integrated analysisof the textual and prosodic characteristics of infor-mation structure in a corpus of conversational En-glish. Upstep and embedded register levels.Phonology, 19:77120.E. Both schemes have a hierarchicalstructure.
W05-0308@@This paper describes a fine-grained annotation schemefor key components and properties of opinions, emo-tions, sentiments, speculations, evaluations, and otherprivate states in text. (11) I think people are happy because Chavezhas fallen. In sentence (8), the U.S. reports crit-icism is the target of Chinas criticism.
W05-0309@@We believe that shallow semantics expressedas a dependency structure, ie, predicate-argumentstructure, for verbs, participial modifiers, and nom-inalizations provides a feasible level of annotationthat would be of great benefit. Bush) &Arg1(e, him) & MNR (e, privately) & LOC(e, in theWhite House) & TMP (e, on Thursday).Annotation of event variables starts by auto-matically associating all Propbank I annotationswith potential event ids. Building a Large Annotated Corpus of English:the Penn Treebank.
W05-0310@@ Corpus tagging is a prerequisite for many machine learning methods in NLP but has the drawbacks of high cost, inter-annotator inconsistency and the insufficient treatment of meaning. Inter-annotator agreement for a German newspaper corpus. The Iraqi government has agreed to let U.S. Representative Tony Hall visit the country to assess the humanitarian crisis.
W05-0311@@We tackle three limitations with the current state ofthe art in the annotation of anaphoric relations. E.g., a markable Emarked as pointing both to antecedent A, belongingto anaphoric chain {A,B}, and to antecedent C, be-longing to anaphoric chain {C,D}, would be treatedby our scripts as being interpreted as referring toanaphoric chain {A,B,C,D}. A Formal Approach to DiscourseAnaphora.
W05-0312@@The goal of the Chinese Discourse Treebank(CDTB) Project is to add a layer of discourse anno-tation to the Penn Chinese Treebank (Xue et al, Toappear), the bulk of which has also been annotatedwith predicate-argument structures. In Proceedings of the4th International Conference on Language Resourcesand Evaluation, Lisbon, Portugal.E. 7 billion, which is a 34.  percent increase.
W05-0401@@ The investigation for Chinese information extrac-tion is one of the topics of the project COLLATE dedicated to building up the German Competence Center for Language Technology. k; m is the total number of the relation R(i) in the NER pattern library. Here, it should be clarified that the negative case we mean is a case in which two or more NEs do not stand in any relationships with each other, i.e, they bear non-relationships which are also in-vestigated objects in which we are interested.
W05-0406@@The automatic classification of it as either referen-tial or non-referential is a topic that has been rel-atively ignored in the computational linguistics lit-erature, with only a handful of papers mentioningapproaches to the problem. Consider the example: It is a drearyday. (prepositional phrase)c. Tis glad I am to ear it, me lord.
W05-0407@@The design of features for natural language process-ing tasks is, in general, a critical problem. InProceedings of the 40th Annual Conference of theAssociation for Computational Linguistics (ACL-02),Philadelphia, PA, USA.T. A study on convolution ker-nels for shallow semantic parsing.
W05-0408@@The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classifi-cation. (E-Step) (3) The labeled and unlabeled data are then used to estimate parameters for a new classifier. We annotated a randomly-selected sample of 3,000 sentences for sentiment.
W05-0503@@Our specific and primary focus is on morphology, and on how knowledge of morphology can be a useful step towards a more complete knowledge of a languages linguistic structure. In the experiments we describe below, we explore four settings  for this threshold: 0. in allowing greater signature transform collapse, and hence greater signature collapse), 1.0, 1. , and 1. . Unsupervised learning of the morphology of a natural language.
W05-0504@@The heuristic finds 3and 4-state finite state automata (FSAs) from untagged corpora. Unsupervised Learning of the Morphology of a Natural Language. We compute the optimal alignment of S and T using the SED algorithm, where alignment between two identical letters (which we call twins) is assigned a cost of 0, alignment between two different letters (which we call siblings) is assigned a cost of 1. , and a letter in one string not aligned with a segment on the other string (which we call an orphan) is assigned a cost of 1.
W05-0601@@The large literature on term clustering, term sim-ilarity and weighting schemes shows that docu-ment similarity is a central topic in Information Re-trieval (IR). Class-based probabilityestimation using a semantic hierarchy. Such approach has two advantages: (a)we obtain a well defined space which supports thesimilarity between terms of different surface formsbased on external knowledge and (b) we avoid toexplicitly define term or sense clusters which in-evitably introduce noise.The class of spaces which embeds the above pairinformation may be composed by O(|V |2) dimen-sions.
W05-0602@@Most recent work in learning for semantic parsinghas focused on shallow In thispaper, we address the more ambitious task of learn-ing to map sentences to a complete formal meaning-representation language (MRL). The probability of choosing a head constituentlabel H: Ph(HjP; h). GEOQUERY: a DB Query LanguageGEOQUERY is a logical query language for a smalldatabase of U.S. geography containing about 800facts.
W05-0603@@An important but understudied language analy-sis problem is that of noun compound bracketing,which is generally viewed as a necessary step to-wards noun compound interpretation. http://opennlp.sourceforge.net/8Two NCs can appear more than once but with a differentinflection or with a different word variant, e.g,. They do not use tax-onomies and work with the word n-grams directly,achieving 78.
W05-0604@@Many text applications are predicated on the ideathat shallow lexical semantics can be acquiredthrough corpus analysis. This material is based onwork funded in whole or in part by the U.S. Govern-ment. Mathematical Structures of Language.Interscience Publishers, New York.T.K.
W05-0605@@ Word Sense Disambiguation (WSD) is one of the central problems in Natural Language Processing. First, an intermediate solution { }0, MK  is com-puted by a greedy algorithm. , , Z is the normaliza-tion factor, ftw ,  is the weight associated with tag t and feature f .
W05-0606@@The computation of surface similarity between pairsof words is an important task in many areas of nat-ural language processing. A new algorithm for thealignment of phonetic sequences. 6AcknowledgmentsThis research was funded in part by the Natural Sci-ences and Engineering Research Council of Canada(NSERC), and the Alberta Informatics Circle of Re-search Excellence (iCORE).ReferencesLeonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss.
W05-0608@@Text Categorization (TC) deals with the problem ofassigning a set of category labels to documents. Journal of the American Society of InformationScience.A. k eigen-values of T, and all the remaining elements set to0.
W05-0609@@Clustering approaches have been widely applied to nat-ural language processing (NLP) problems. E.g., labels are not taken into account whenmeasuring the quality of the partition. T (typically T is large) : the number ofiterations allowed.Output:  : the parameters in the distance function d. .
W05-0610@@Information Extraction (IE) is the process of auto-matic extraction of information about pre-specifiedtypes of events, entities or relations from text suchas newswire articles or Web pages. Manualannotation is a time-consuming process. The best system had ex-tremely good results on the two slots, C-acronymand C-homepage.
W05-0612@@Coreference resolution is the process of determin-ing which expressions in text refer to the same real-world entity. This estimationprocess is effectively the E-step in EM.The M-step is conducted by redefining our mod-els according to these fractional counts. A statisticalapproach to anaphora resolution.
W05-0613@@Achieving a model of discourse interpretation that isboth robust and deep is a major challenge. The baseline also appropriately handlesignorable utterances (i.e, those with the mood labelsirrelevant, pause, or pleasantry).The baseline performs poorly on labelled rela-tions (7. A: How about one pmc.
W05-0615@@The use of statistical methods in computational lin-guistics has produced advances in tasks such as pars-ing, information retrieval, and machine translation.However, most of the successful work to date hasused supervised learning techniques. In this positional model, each syllableis labeled as initial (I), medial (M), final (F), or asthe one syllable in a monosyllabic word (O). Forexample, this model predicts /E.gzE.kju.tIv/ for ex-ecutive, just as the categorical parser does, although/gz/ is never attested in word-initial position.
W05-0616@@The first step con-sists in the construction of a structural mapping be-tween a new instance of a problem and solved in-stances of the same problem. CELEX: a guide for users. , w(i) denotes the ith symbol inw.
W05-0617@@This paper presents a completely unsupervisedmethod for inducing morphological knowledge di-rectly from a large monolingual text corpus. We attempt both possible expan-sions (with or without the e). A number of researchers usesuch statistics directly.
W05-0618@@NLP applications involve mappings between com-plex representations. Assignments aremodeled as variables of a linear cost function. {1, ..., n}We also require that if two simple variables x(lij)and x(lkp), modeling respectively labels lij and lkpare set to 1, then the compound variable x(lij , lkp),which models co-occurrence of these labels, is alsoset to 1.
W05-0622@@The semantic role labelling task (SRL) involvesidentifying which groups of words act as argumentsto a given predicate. is the nor-malising function, which ensures that p is a validprobability distribution. We have defined node and pairwise clique fea-tures using data local to the corresponding syntacticnode(s), as well as some features on the predicateitself.Each feature type has been made into binary fea-ture functions g and h by combining (feature type,value) pairs with a label, or label pair, where thiscombination was seen at least once in the trainingdata.
W05-0623@@It is evident that there are strong statistical patternsin the syntactic realization and ordering of the argu-ments of verbs; for instance, if an active predicatehas an A0 argument it is very likely to come beforean A1 argument. Path Sequence of phrase types between the predicate andnode, with ,  Head-POS POS tag of head word Sub-Cat CFG expansion of predicates parent Ordinal Tree Distance Phrase-type concatenated with thelength of the Path feature Node-LCA Partial Path Path from the node to the lowestcommon ancestor of the predicate and the node PP Parent Head-Word If the parent of the node is a PP, theparents head-word PP NP Head-Word/Head-POS For a PP, retrieve the head-word /head-POS of its rightmost NP Temporal Keywords* Is the head of the node a temporalword e.g February Missing subject* Is the predicate missing a subject inthestandard Projected path* Path from the maximal extended projectionof the predicate to the node Predicate Lemma & PP Parent Head-Word Projected path & Missing subject*2. Also a variant that adds the pred-icate stem.
W05-0624@@This isespecially the case in the presence of noise (sinceeach misclassified example has to be stored as abound support vector). For an NP, this is either S or VP. A binary feature that describes if thenode is before or after the predicate token.
W05-0626@@ Semantic role labeling is to find all arguments for all predicates in a sentence, and classify them by semantic roles such as A0, A1, AM-TMP and so on. GT-PARA then converts the parse tree into a flat representation with all predi-cates and arguments expressed in [GPLVR] for-mat; where G: Grammatical function  5 denotes subject, 3 object, and 2 others; P: Phrase type of this boundary  00 denotes ADJP, 01 ADVP, 02 NP, 03 PP, 04 S, 05 SBAR, 06 SBARQ, 07 SINV, 08 SQ, 09 VP, 10 WHADVP, 11 WHNP, 12 WHPP, and 13 Others L: Distance (and position) of the argument with respect to the predicate that follows V: Voice of the predicate, 0: active 1: passive R: Distance (and position) of the argument with respect to the preceding predicate (n.b. The input is a full parse tree for each sentence.
W05-0627@@The semantic role labeling (SRL) is to assign syn-tactic constituents with semantic roles (arguments)of predicates (most frequently verbs) in sentences.A semantic role is the relationship that a syntacticconstituent has with a predicate. A maximum entropy approach tonatural language processing. The reason is easy to be understoodfor the dropping of automatic syntactic parser per-formance on new corpus but WSJ corpus.The training time on PIV 2.
W05-0629@@The semantic roles of constituents of thesentence are extracted for each target verb. Weadopted a default value (1.0). The BIOclass is learned as (B vs. other), (I vs. other),and (O vs. other).  Cost of constraint violation (floating number):There is a trade-off between the training errorand the soft margin for the hyper plane.
W05-0630@@Given the parse tree of aninput sentence, the basic system applies (1) a bound-ary classifier to select the nodes associated with cor-rect arguments and (2) a multi-class labeler to assignthe role type. Given a sentence from the training-set, generatea full syntactic parse-tree;2. P A:extract the feature representation set, Fp,a;if the subtree rooted in a covers exactly thewords of one argument of p, put Fp,a in T+(positive examples), otherwise put it in T (negative examples).We trained the SVM boundary classifier on T+ andT Ti , according to the ONE-vs.-ALL scheme.
W05-0631@@The SRL system described here depends on a full syntactic parse from the Charniak parser, and investigates aspects of using Support Vector Machines (SVMs) as the machine learning technique for the SRL problem, using the libSVM package. In the SRL problem, the features are nominal, and we followed the standard practice of represent-ing a nominal feature with n discrete values as n binary features. Within each sentence, we first map the predicate to a constituent in the syntax tree.
W05-0633@@They also provide the clas-sifier with enough information. This precludes a priori the output to align to thegold standard PropBank annotation and we use therefore prun-ing as a recovery strategy.those labels which account for at least 0. For each unseen word we back offby using the probability distribution P (r|posi)of the PoS posi of filling a semantic role r4.Named entities: the label of the named entitywhich spans the same words as the constituent,as well as the label of the largest named en-tity embedded within the constituent.
W05-0634@@The task of Semantic Role Labeling (SRL) involvestagging groups of words in a sentence with the se-mantic roles that they play with respect to a particu-lar predicate in that sentence. Thechunking system for combining all features wastrained using a 4-fold paradigm. Chunk based systemsclassify each base phrase as being the B(eginning)of a semantic role, I(nside) a semantic role, orO(utside) any semantic role (ie NULL).
W05-0635@@knowledge, a clear anal-ysis of the benefits of using full syntactic analysisversus partial analysis is not yet available. AcknowledgementsThis research has been partially funded by the Euro-pean Union project Computers in the Human Inter-action Loop s Ma`rquez andXavi Carreras for the help with the AdaBoost classi-fier, for providing the set of temporal cue words, andfor the many motivating discussions.ReferencesX. This value is 0 if the two phrases share thesame parent.The governing category, which indicates if NParguments are dominated by a sentence (typical forsubjects) or a verb phrase (typical for objects).We generalize syntactic paths with more than 3elements using two templates:(a) Arg  For simplicity we do not la-bel predicates.
W05-0636@@Although much effort has gone into developingstatistical parsing models and they have improvedsteadily over the years, in many applications thatuse parse trees errors made by the parser are a ma-jor source of errors in the final output. This results in a list {(F i, ti)} of parsetree / SRL frame pairs, from which the rerankerchooses. Mallet: A machine learn-ing for language toolkit.
W05-0638@@ The Semantic Role Labeling problem can be for-mulated as a sentence tagging problem. Denote the target constituent by t. The following features are the most common baseline features of ts parent and sibling constitu-ents. The goal of this ILP is to find a set of assignments for all zia that maxi-mizes the following function:  S should have one of these argument types, or no type (null).
W05-0639@@Semantic parsing, identifying and classifying the se-mantic entities in context and the relations betweenthem, potentially has great impact on its downstreamapplications, such as text summarization, questionanswering, and machine translation. The label -ANC means the constituentis a discontinuous core argument. The algorithm is straight-forward: Suppose there are N participating systems,we pick arguments with N votes, N-1 votes ..., andfinally 1 vote.
W05-0703@@In this paper we present initial work on MAGEAD, amorphological analyzer and generator for the Arabiclanguage family, by which we mean both ModernStandard Arabic (MSA) and the spoken dialects. Arabic morphological analysis techniques:A comprehensive survey. The opposite is true for whenradical symbol (C,1,2,3,4,5) appears in the template,a 0 is inserted in the vocalism tier (as no vowel fromthe vocalism can be inserted here) and a wild cardin the radical tier.
W05-0704@@ Due to the morphological complexity of the Arabic language, much research has focused on the effect of morphology on Arabic Information Retrieval (IR). Lee et al report a 2. Antworth, E. PC-KIMMO: a two-level processor for morphological analysis.
W05-0705@@We present here lessons learned from the extension of our NLP system that was originally implemented for Romance and Germanic European1 languages to a member of the Semitic language family, Arabic. Information retrieval application: a. Subgraph indexing. These are e then compiled into a finite-state automaton.
W05-0706@@Texts in Semitic languages like Modern Hebrew(henceforth Hebrew) and Modern Standard Ara-bic (henceforth Arabic), are based on writing sys-tems that allow the concatenation of different lexi-cal units, called morphemes. 9%, with a tag set of 24 POStags. We used two-tailed paired t-test for testing thesignificance of the difference between the averageresults of different systems.
W05-0707@@Part-of-speech (POS) tagging is often consideredas the first phase of a more complex natural lan-guage processing application. Thefollowing five tags are used for tagging the char-acters; B(egin), C(ontinue), E(nd), U(nique) andN(egative). Eachcharacter constitutes a training (test) instance.
W05-0708@@Part-of-speech (POS) tagging is a core natural lan-guage processing task that can benefit a wide rangeof downstream processing applications. Dialectal Arabic is a spoken language. of 6th Applied Natural Language Processing Conf.E.
W05-0709@@Information extraction is a crucial step toward un-derstanding and processing language. N66001-99-2-8916.The views and findings contained in this material arethose of the authors and do not necessarily reflectthe position of policy of the U.S. government and noofficial endorsement should be inferred.ReferencesPeter F. Abbou and Ernest N. McCarus, editors. Arabic is a highly inflected andderived language.
W05-0712@@ Named Entities (NEs) translation is crucial for ef-fective cross-language information retrieval (CLIR) and for Machine Translation. Each bi-gram of letters on the source side is aligned to an n-gram of letters sequence on the tar-get side, such that vowels have very low cost to be aligned to NULL. BLEU: a Method for Automatic Evaluation of machine translation.
W05-0802@@Text categorization (TC) is the task of assigning cat-egory labels to documents. Inter-lingual domain relations are captured by placing dif-ferent terms of different languages in the same se-mantic field (as for example HIV e/i, AIDSe/i,hospitale, and clinicai). Learning a translationlexicon from monolingual corpora.
W05-0804@@Statistical natural language processing usually suf-fers from the sparse data problem. 644 and a NIST scoreof 6. Initialize bilingual co-occurrence matrixC{F,E} with rows representing French words,and columns English words.
W05-0807@@Most prior research in part-of-speech (POS) tag-ging has focused on supervised learning over atagset such as the Penn Treebank tagset for En-glish, which is restricted to features that are mor-phologically distinguished in the focus language.Thus the only verb person/number distinction madein the Brown Corpus/Penn Treebank tagset is VBZ(3rd-person-singular-present), with no correspond-ing person/number distinction in other tenses. Bangalore and A. K. Joshi. J. Och and H. Ney.
W05-0808@@  Text alignment is not only used for the tasks such as bilingual lexicography or machine translation but also in other language processing applications such as multilingual information retrieval and word sense disambiguation. Group name or a part-of-speech category. Expected English word(s) (EEW) that this Hindi word group may align to.
W05-0810@@Word alignment is an important step in exploitingparallel corpora. )pibm2(s|E)where the summation is carried over all sub-strings s of I of 3 characters or more. A Program forAligning Sentences in Bilingual Corpora.
W05-0811@@Conventional word-alignment methods have been suc-cessful at treating many language pairs, but may be lim-ited in their ability to generalize beyond the Western Eu-ropean language pairs for which they were originallydeveloped, to pairs which exhibit more complex diver-gences in word order, morphology and lexical granular-ity. InProceedings of the 38th Annual Meeting of the Association for Compu-tational Linguistics, pages 440447.E. To save space, only a representative portion of each ma-chine is drawn.
W05-0812@@In practice, a sequenceof simpler models such as IBM Model 1 and an HMMModel are used to generate initial parameter estimatesand to enumerate a partial search space which can be ex-panded using hill-climbing heuristics. By conventionwe refer to e as the English sentence and f as the Frenchsentence. t( fi|eai) (1)We refer to d(ai|ai1) as the distortion model and t( fi|eai)as the translation model.
W05-0815@@The aim of the taskwas to align the words of sentence pairs in differ-ent language pairs. A string orsequence of words is indicated by a bar like in x, individualwords from the sequence carry a subindex and no bar like in xi,substrings are indicated with the first and last position like in xji .Finally, when the final position of the substring is also the lastof the string, a dot is used like in x.iDeciding the concatenation direction The direc-tion of the concatenation is also decided as a func-tion of the two words adjacent to the cut point, thatis:Pr(D | b, x) DI(xb, xb+1),where D stands for direct concatenation (iethe translation of xb1 will precede the transla-tion of x.b+1) and I stands for inverse. 834 and a recall of 0.
W05-0817@@The translation equivalence extractor has been also incorporated into a WSD system (Tufi A detailed description of TREQ&TREQ-AL is given in (Tufi We used the translation probabilities generated by GIZA++ for implementing a second aligner, MEBA, described in a                                                  1 http://www.cs.unt.edu/~rada/wpt/index.html#shared  2 http://www.fjoch.com/GIZA++. Another possible improvement (not implemented yet) was revealed by observing that the final result contained several incomplete n-m (phrasal) alignments. The hypotheses are filtered by a loglikelihood score threshold.
W05-0818@@of thesource text and segments of its translation (the tar-get text). By doing this, LIHLAcan deal with small changes in possible trans-lations such as different forms of the same verb,changes in gender and/or number of nouns,adjectives, and so on.Then, LIHLA selects the best target candidateword ti for sj the best candidate word accor-ding to BS among those in a position whichis favorably situated in relation to sj 3 as theirlongest common subsequence is a-l-i-n-a-m-e-n-t-o.ti (for target multiword units) and are not pos-sible translations for other words in T and S,respectively. Foreach source token sj in source sentence S, LIHLAwill look for the best token ti in the target parallelsentence T applying these heuristics in sequence:1.
W05-0819@@Participants in the shared task were provided with common sets of training data, consisting of English-Inuktitut, Romanian-English, and English-Hindi parallel texts and the participating teams could choose to evaluate their system on one, two, or all three language pairs. We come up with an m x n matrix, where m and n refer to the number of words in Hindi and English respectively. Sometimes a phrase is aligned with another phrase.
W05-0821@@Statistical machine translation (SMT) makes use ofa noisy channel model where a sentence e in the de-sired language can be conceived of as originating asa sentence f The goal is tofind, for every input utterance f is the translation model expressing proba-bilistic constraints on the association of source andtarget strings. A simplex method for functionminimization. The 4-gram model uses modified Kneser-Ney smoothing and interpolation of higher-orderand lower-order n-gram probabilities.
W05-0822@@ The rapid growth of the Internet has led to a rapid growth in the need for information exchange among different languages. Method E is based on the Europarl corpus as training data, 2. A Smor-gasbord of Features for tion.
W05-0823@@Sec-tion 3 presents a brief overview of the whole SMTprocedure. A. R. Fonol-losa and Marta Ruiz Costa-jussa` for their participa-tion in discussions related to this work.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer. A Ngram-based Statistical Machine Transla-tion Decoder.
W05-0824@@Machine translation is nowadays mature enoughthat it is possible without too much effort to de-vise automatically a statistical translation systemfrom just a parallel corpus. The results were however disappoint-ing for both the G-E and S-E translation direc-tions we tested. [i+2,j2] |Fci |split(F jc+1) otherwiseThis approach yielded a significant degradationin performance that we still have to analyze.
W05-0825@@Phrase extraction becomes a key component in to-days state-of-the-art statistical machine translationsystems. using thelexicalized position score as follows:ei+ki (fj) )(3)where |E| is the English sentence length. A systematiccomparison of various statistical alignment models.
W05-0828@@and Long-term PerspectiveThe problem of robust, efficient and reliablespeech-to-speech translation can only be crackedby the combined muscle of deep and shallow pro-cessing approaches. In ANLP, pages 95100.Christopher Hogan and Robert E. Frederking. BLEU: a method for automatic eval-uation of machine translation.
W05-0829@@Many of these phrase alignment strategiesrely on the pre-calculated word alignment and usedifferent heuristics to extract the phrase pairs fromthe Viterbi word alignment path. These co-occurrence frequencies willbe accumulated over the whole corpus to calculatethe initial L(f, e). Since ourphrase alignment is constrained by the locality as-sumption and we can only extract phrase pairs ofadjacent words, lower n-gram coverage will result inlower translation scores.
W05-0830@@ The use of structural and syntactic information in language processing implementations in recent years has been producing contradictory results. A Syntax-based Statistical Translation Model. Ph.D. thesis, Carnegie Melon University.
W05-0831@@Reordering is of crucial importance for machinetranslation. The two scores measure accuracy, i. e. largerscores are better. The implementation using a bit vector isstraightforward.
W05-0834@@A statistical machine translation system usually pro-duces the single-best translation hypotheses for asource sentence. An efficient algorithm for the n-best-strings problem. A finite-state approach tomachine translation.
W05-0835@@Suppose you were to find an English translation fora Spanish sentence. It has two possible outcomes:D (for direct) and I (for inverse). Europarl: A multilingualcorpus for evaluation of machine translation.
W05-0836@@hk(e, f))(1)where e is a single candidate translation for ffrom the set of all English translations E,  is theparameter vector for the model, and each hk is afeature function of e and f . The methods discussed below make useof Gen(f), the approximation to the complete can-didate translation space E, referred to as an n-bestlist. More precisely, let Sc(i)be theestimate of Score in iteration i (we will explain howto obtain this estimate below).
W05-0902@@Subjectivity plays an important role when remov-ing the unwanted or redundant information for sum-marising a document. Who sees this move as a threat3. What are the factors affecting the particular event (R)Questions by subject D:1.
W05-0903@@Machine translation (MT), as any other natural lan-guage processing (NLP) research subject, dependson the evaluation of its results. Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics. Orange: a method forevaluation automatic evaluation metrics for machinetranslation.
W05-0904@@Evaluation has long been a stumbling block in thedevelopment of machine translation systems, due tothe simple fact that there are many correct transla-tions for a given sentence. A path-based transfer model formachine translation. Forthe 1-depth subtrees, we get S, NP, VP, PRON, V,NP which also appear in the reference syntax tree.Since PRON only occurs once in the reference, itsclipped count should be 1 rather than 2.
W05-0905@@In the field of automatic summarization, it is widelyagreed upon that more attention needs to be paidto the development of standardized approaches tosummarization evaluation. Lin and E. H. Hovy. It is based on thesingular value decomposition (SVD) of an m  nterm-document matrix A, whose elements Aij rep-resent the weighted term frequency of term i in doc-ument j. n matrix of right-singular vectors.The rows of V T may be regarded as defining top-ics, with the columns representing sentences fromthe document.
W05-0906@@Recent developments in question answering (QA)and multi-document summarization point to manyinteresting convergences that present exciting oppor-tunities for collaboration and cross-fertilization be-tween these largely independent communities. A definition of relevance forinformation retrieval. This measurewill hopefully spur progress in definition questionanswering systems.The development of automatic evaluation metricsbased on n-gram co-occurrence for question answer-ing is an example of successful knowledge transferfrom summarization to question answering evalua-tion.
W05-0908@@Rapid and accurate detection of result differences iscrucial in system development and system bench-marking. For a restrictive significancelevel of 0. Assuming equivalence of thecompared system variants, these assessments wouldcount as type-I errors.
W05-0909@@The utility and attractiveness of automatic metrics for MT evaluation has consequently been widely rec-ognized by the MT community. We use a harmonic mean of P and 9R. Automatic Evaluation of Machine Translation Quality using N-gram Co-occurrence Statistics.
W05-1003@@This being the case, it is surprising how little attention has been devoted to this aspect of lexical representation in work on large-scale lexical semantics in Computa-tional Linguistics. (E.g., the color of the car.) A comprehensive grammar of the English language.
W05-1004@@The work de-scribed in this paper addresses this issue and presentsan approach to automatically learning qualia structuresfor nominals from the Web. In Proceedings of the FirstInternational Conference on Language Resources andEvaluation (LREC), pages 58996.E.M. Our aim is toautomatically acquire Qualia Structures from the Web fornominals, looking for (i) nominals describing the type ofthe object, (ii) verbs defining its agentive role, (iii) nomi-nals describing its parts or components and (iv) nouns orverbs describing its intended purpose.
W05-1005@@Many such verbs refer to states or acts thatare central to human experience (eg, sit, put, give);hence, they are often both highly polysemous andhighly frequent. (e) Which speech did Azin giveOthers have little or no syntactic freedom:3. un livre a` Sam.
W05-1006@@Natural language is full of idiomatic and metaphor-ical uses. A graphmodel for unsupervised lexical acquisition. For example,consider the following sentence from the British Na-tional Corpus (BNC).Ships laden with nutmeg, cinnamon,cloves or coriander once battled theSeven Seas to bring home their preciouscargo.Since the BNC is tagged for parts-of-speech, weknow that the words highlighted in bold are nouns.Since the phrase nutmeg, cinnamon, cloves or co-riander fits the pattern A, B, C or D, we createnodes for each of these nouns and create links be-tween them all.
W05-1007@@The intuition that semantic analysis can make a posi-tive contribution to language-based applications hasmotivated the development of a number of lexical-semantic resources. The benefit of the semanticannotation is constrained by the presence and qualityof semantic roles in the lexical-semantic resource(s)used. Lexical units aresaid to evoke a frame.
W05-1008@@In the case of a deep grammar, for exam-ple, system design encompasses the construction ofthe system of lexical types, templates, and/or phrasestructure rules, and data classification correspondsto the determination of the lexical type(s) each in-dividual lexeme conforms to. For each sister lex-eme where such a correspondence is found to ex-ist, we output the nature of the character transforma-tion and the word classes of the lexemes involved.E.g., the sister lexemes for achievementN in CAT-VAR are achieveV, achieverN, achievableAdj andachievabilityN; the mapping between achievementNand achieverN, eg, would be analysed as:N ment$  N +r$Each such transformation is treated as a single fea-ture.We exhaustively generate all such transformationsfor each lexeme, and filter the feature space as forcharacter n-grams above.Clearly, LRs which document derivational mor-phology are typically only available for high-densitylanguages. A general featurespace for automatic verb classification.
W05-1009@@This paper fits into a broader effort addressing theautomatic acquisition of semantic classes for Cata-lan adjectives. T-tests for the difference in means with7Alternatives not equal Alternatives: basic greater than event for prenominalmodification, event greater than basic Decision trees built with this feature set use the in-formation consistent with the observations just out-lined. as in I expectMary to be punctual for this meeting.
W05-1011@@In addition, lexicographers cannot keepup with constantly evolving language use and can-not afford to build new resources for the many sub-domains that NLP techniques are being applied to.There is a clear need for methods to extract lexicalsemantic resources automatically or tools that assistin their manual creation and maintenance.Much of the existing work on automatically ex-tracting resources is based on the distributional hy-pothesis that similar words appear in similar con-texts. Class-based probabilityestimation using a semantic hierarchy. In Proceedings of theWorkshop of the ACL Special Interest Group on the Lexicon(SIGLEX), pages 5966, Philadelphia, USA, 12 July.James R. Curran and Marc Moens.
W05-1201@@Asummarizer, for instance, could use it to extractthe most informative sentences, while a question-answering system  Our working hypothesis is thatsemantic overlap at the word and phrase levels mayprovide a good basis for deciding the semantic re-lation between sentences. Herrera, A. Pe nas, and F. Verdejo. Thus have I in the course of my life verymany contacts had with very many serious persons) In that way came I in the life with mass-of weighty/important peoplein touch).
W05-1202@@An expression is said to textually entail another ex-pression if the meaning of the second expression canbe inferred from the meaning of the first. A general frame-work for distributional similarity. and the lowest overlap is 0.
W05-1203@@Measures of text similarity have been used for along time in applications in natural language pro-cessing and related areas. All similarity scores have a value between 0 and 1. In Proceedings of the NAACL Workshop on Word-Net and Other Lexical Resources, Pittsburgh, June.I.
W05-1205@@Ordinary SDTGs allowany permutation of the symbols on the right-hand side tobe specified when translating from the input language tothe output language. es..t/cu..v]as the maximum probability of any derivation from i thatsuccessfully parses both es..t and cu..v . Second, ITGs can also be defined as the restrictedsubset of SDTGs where all rules are of rank 3.Polynomial-time algorithms are possible for varioustasks including translation using ITGs, as well as bilin-gual parsing or biparsing, where the task is to build thehighest-scored parse tree given an input bi-sentence.For present purposes we can employ the special case ofBracketing ITGs, where the grammar employs only onesingle, undistinguished dummy Designating this category A, aBracketing ITG has the following form (where, as usual,lexical transductions of the form A  e/f may possiblybe singletons of the form A  ei/fjThe simplest class of ITGs, Bracketing ITGs, areparticularly interesting in applications like paraphras-ing, because they impose ITG constraints in language-independent fashion, and in the simplest case do not re-quire any language-specific linguistic grammar or train-ing.
W05-1208@@ Many Natural Language Processing (NLP) applications need to recognize when the meaning of one text can be expressed by, or inferred from, another text. Representing and Reasoning with Probabilistic Knowledge, M.I.T. A Language Mod-eling Approach to Information Retrieval.
W05-1210@@Textual entailment has been proposed recently asa generic framework for modeling semantic vari-ability in many Natural Language Processing ap-plications, such as Question Answering, Informa-tion Extraction, Information Retrieval and Docu-ment Summarization. We define that entailment holds be-tween T and H if the relations within H can becovered by the relations in T . Our study extendsthis work by considering a broader range of infer-ence levels and inference mechanisms and providinga more detailed view.
W05-1301@@ A primary set of tasks facing biomedical text proc-essing systems is that of categorizing, identifying and classifying entities within the literature. We start with a robust, machine learning-based baseline system: a reimplementation of the system in [1]. 2 Background and Related Work 2.
W05-1302@@String Similarity and Reference ResolutionString similarity/matching algorithms are used as acomponent in reference resolution algorithms. is punctuation and state uni-gram HPQJ O IKJ.LObs. Also it can be noticedthat some of the mappings include subcases (eg,R.
W05-1303@@ In the genomics era, the field of biomedical research finds itself in the ironic situation of generating new information more rapidly than ever before, while at the same time individual researchers are having more difficulty getting the specific information they need. % at a recall of 66. In our system we chose an n of 6 as a good balance between performance and memory requirements.
W05-1305@@Annotated text corpora are used in modern computa-tional linguistics research and development to fine-tune computer algorithms for analyzing and classi-fying texts and textual components. put all text and annotations into a commondatabase format4. To il-lustrate, the two gerunds in the previous example,retarding and reducing both have direct objects (re-tarding processes and reducing risk), and the gerundtag is entered as o. GENIA provides detailed coverage ofa large number of semantic entities related to a spe-cific subset of human molecular biology, whereasGENETAG provides gene and protein name anno-tations only, for a wide range of organisms andbiomedical contexts (molecular biology, genetics,biochemistry, clinical medicine, etc.
W05-1307@@Proteins are often considered in terms of their net-works of interactions, a view that has spurred con-siderable effort in mapping large-scale protein in-teraction networks. AcknowledgementsThis work was supported by grants from the N.S.F. A CRF Tagger for Protein NamesThe task of identifying protein names is made diffi-cult by the fact that unlike other organisms, such asyeast or E. coli, the human genes have no standard-ized naming convention, and thus present one of thehardest sets of gene/protein names to extract.
W05-1308@@ Genomic research in the last decade has resulted in the production of a large amount of data in the form of micro-array experiments, sequence infor-mation and publications discussing the discoveries. c) Each row represents a simple sentence, d) for each constituent, role type is resolved and interaction words are tagged, e) Protein-Protein interaction is extracted. Parsing English with a Link Grammar.
W05-1501@@In order to tackle the algorithmic difficulties ofparsers when applied to real-life corpora, it is nowa-days usual to apply robust and efficient methodssuch as Markovian techniques or finite automata.These methods are perfectly suited for a large num-ber of applications that do not rely on a complex rep-resentation of the sentence. If the CF backbone G is cyclic (ie, A s.t. Amidthe sentences covered by such a system, 94.
W05-1503@@the termthat(x.read(x)(j))(book) and type CN, andperson that read the book Logically, concatenation isconjunction and the divisions are directed implica-tions. In P. Scott, C. Casadio, and R. Seely,editors, Language and Grammar: Studies in Math.Ling. A literal is a po-lar type with an atomic syntactic type.
W05-1504@@Many modern parsers identify the head word ofeach constituent they find. insider along the correct path, the FSAstate must record (at least) that insider has no parentyet and that R$ and Rcut are in particular states that19The full runtime is O(nE), where E is the number of FSAedges, or for a tighter estimate, the number of FSA edges thatcan be traversed by reading . t bounds the number of au-tomaton transitions from a state that emit the sameword.
W05-1506@@Many problems in natural language processing (NLP) in-volve optimizing some objective function over a set ofpossible analyses of an input string. The k-best derivationsproblem for hypergraphs, then, is to find D(t) given a hy-pergraph V, E, t,R.With the derivations thus ranked, we can introduce anonrecursive representation for derivations that is analo-gous to the use of back-pointers in parser implementa-tion.Definition 7. It is initialized to {e, 1}.
W05-1507@@In a number of recently proposed synchronousgrammar formalisms, machine translation of newsentences can be thought of as a form of parsing onthe input sentence. The interaction ofthe hook technique with pruning is an interesting71Algorithm 1 ITGDecode(Nt)for all s, t such that 0  G dofor all (Y, u1, v1) possible for the span of (s, S) do a hook who is on (S, t), nonterminal as Z, and outside expectation being v1 is requiredif not exist hooks(S, t, Z, v1) thenbuild hooks(S, t, Z, v1)end iffor all v2 possible for the hooks in (S, t, Z, v1) do combining a hook and a hypothesis, using straight rule [Y Z])}end forend forend for inverted rulefor all rules X  G dofor all (Z, u2, v2) possible for the span of (S, t) do a hook who is on (s, S), nonterminal as Y , and outside expectation being v2 is requiredif not exist hooks(s, S, Y, v2) thenbuild hooks(s, S, Y, v2)end iffor all v1 possible for the hooks in (s, S, Y, v2) do combining a hook and a hypothesis, using inverted rule Building the chart items withhooks may take more time than it saves if many ofthe hooks are never combined with complete con-stituents due to aggressive pruning. In practice it is beneficialto combine the probability given by ITG with a localm-gram language model for English:e Thelanguage model will lead to more fluent output byinfluencing both the choice of English words and thereordering, through the choice of straight or invertedrules.
W05-1508@@Annotated corpora are valuable resources for Natu-ral Language Processing (NLP) which often requiresignificant effort to create. The subject and object are noun phrases consist-ing of an adjective (a) and a noun (n). Directed edges are said to go from aparent to a child node.
W05-1509@@Natural language processing methods producingshallow semantic output are starting to emerge as thenext step towards successful developments in naturallanguage understanding. Notice that M [n, n], that isthe [SEM-NULL,SEM-NULL] cell in the matrix, is nevertaken into account.Syntactic labels are recovered with very high ac-curacy (F 96. The history representation of a parse historyd1, .
W05-1510@@Usually a singlesemantic representation can be realized as severalsentences. Exploiting a proba-bilistic hierarchical model for generation. In Proceedings of the IJCNLP-04.T.
W05-1511@@We investigated the performance efficacy of beamsearch parsing and deep parsing techniques inprobabilistic head-driven phrase structure grammar(HPSG) parsing for the Penn treebank. The pi[i, j] represents the set ofpartial parse results that cover words wi+1, . A maximum entropyapproach to natural language processing.
W05-1512@@State-of-the-art statistical parsers for natural lan-guage are based on probabilistic grammars acquiredfrom transformed tree-banks. Here is a characterization ofboth estimation methods:Estimation from latent-head distributions: Thekey steps of the EM algorithm produce a lexicalizedtree-bank TLEX, consisting of all lexicalized versionsof the original trees (E-step), and calculate the prob-abilities for the rules of GLEX on the basis of TLEX(M-step). Because the probabil-ities occurring in Charniaks definition are alreadyso specific that there is no real chance of obtainingthe data empirically, they are smoothed by deletedinterpolation:116S:roseNP:protsADJ:CorporateCorporateN:protsprotsVP:roseV:roserosePUNC:..Internal Rules:S:rose  ADJ:Corporate N:protsVP:rose  Intheir approach, a grammar transformation is usedto lexicalize a manually written grammar.
W05-1513@@Although they use different parsing algo-rithms, and differ on whether or not dependencies are labeled, they share the idea of greedily pursu-ing a single path, following parsing decisions made by a classifier. Veenstra, J., van den Bosch, A. When parsing a sentence with n words, the parser takes n shift actions (exactly one for each word in the sentence).
W05-1514@@The parser first performschunking by identifying base phrases, and convertthe identified phrases to non-terminal symbols. One is done by a rule dictionary. 2 when the margin was 0..
W05-1515@@There are strong reasons to believethe same would be true of parsing. , is proportional to K. Inour experiment, there were roughly |E|  1.  mil-lion training examples for each classifier..  BaselineIn the baseline setting, context item features (Sec-tion 2. . ) Aggregating ConfidencesTo get the cumulative score of a parse path P, we ap-ply aggregatorA over the confidences Q(I) in Equa-tion 4.
W05-1516@@A common charac-teristic of these parsers is their use of lexicalized statistics. We have taken a different approach. Suppose the dependency tree T is constructed in steps G1, , GN in the canonical order of the de-pendency links, where N is the number of words in the sentence.
W05-1517@@The weighted GRs fora sentence comprise the set of grammatical relationsin all parses licensed for that sentence, each GR isweighted based on the probabilities of the parsesin which it occurs. 4%,10In fact, in these experiments we use a threshold of A 3 (with   ( E EEREGA ) instead of a threshold of A E to reduce theinfluence of very low ranked parses. ImplementationThree processing stages are required to determineweighted GRs over the parse forest, calculating(1) filled GRs and corresponding inside probabili-164 28.0201     (ncsubj see+ed_VVD I_PPIS1 _)  35.
W05-1519@@ Repairs, hesitations, and restarts are common in spoken language, and understanding spoken language requires accurate methods for identifying such disfluent phenomena. This step gives a 2. The patterns have been extracted from development and training data, to deal with certain sequence-related errors, eg,  E N E  E E E, which means that if the neighbors on both sides of a word are classified into EDITED, it should be classified into EDITED as well.
W05-1522@@Large coverage Tree Adjoining Grammars (TAGs)tend to be very large, with several thousands oftree schemata, ie, trees with at least one anchornode. (between the verb arguments and a possible post-verbal subject). When generating trees, underspecified prece-dences between sibling nodes are handled by the in-terleaving operator.Positive and negative guards may be attached tonodes and are accumulated in a conjunctive way dur-ing the crossing phase, ie N  The remaining guardsare emitted as DYALOG guards in the trees.
W05-1526@@Unification parsers have problems with efficiencyand selecting the best parse. A robust system for naturalspoken dialogue. In addition, however,their baseline system is a classic HPSG parser withno efficiency features, while our baseline, TRIPS, isdesigned as a real-time dialogue parser which useshand-tuned weights to guide its search and imposesa maximum chart size.Acknowledgements Our thanks to Will DeBeau-mont and four anonymous reviewers.ReferencesJames F. Allen, Bradford W. Miller, Eric K. Ringger, andTeresa Sikorski.
W05-1528@@We restricted the training sets by making use of constraint features whereby the training set is restricted to only those examples which have the same value for the constraint feature as the query instance. We re-estimated the probability of each parse using our own baseline model, which is a replication of Collins Model 1. We compared the results of our k-NN model against the Bikel 1-best parser results using the paired T test where the data points being compared were the scores of each parse in the two different sets of parses.
W05-1601@@Traditionally, NLG systems have been built as deterministicdecision-makers, that is to say, they generate one string ofwords for any given input, in a sequence of decisions that in-creasingly specify the word string. , andD2 with a probability of . The margin is important con-sidering the far greater expense of n-gram generation.
W05-1605@@Yet notall paraphrases are appropriate for all contexts. The adjunction at some node X with top features tXand bottom features bX , of an auxiliary tree with roottop features r and foot bottom features f entails theunification of tX with r and of bX with f . Mary lent a book to Johnb.
W05-1606@@Generating referring expressions is a traditional, standard task in natural language generation. In E. Rosch and B. Llyod  (eds.) A Computational Model of Referring .
W05-1607@@Our long-term goal is to develop embodied conversationalrobots that are capable of natural, fluent visually situated di-alog with one or more interlocutors. A landmark can be: the speaker (3)a, the hearer(3)b, the scene (3)c, an object in the scene (3)d, or a group ofobjects in the scene (3)e. (3) a. the ball on my right [speaker]b. the ball to your left [hearer]c. the ball on the right [scene]d. the ball to the left of the box [an object in thescene]e. the ball in the middle [group of objects]Currently, new empirical research is required to see ifthere is a preference order between these landmark cate-gories. In H.L.Pick, editor, Spatial orientation.
W05-1615@@Each of these techniques is effective under different ap-plication contexts in which NLG systems operate. Document Planning: Text structure is specified by Weathernews, via a control file. WIND(KTS) 50M W 10-15 backing SW by mid after-noon and S 13-18 by midnight.
W05-1616@@ Most existing NLG systems assume that generated texts are read by proficient readers with good literacy levels. In R Dale et al (eds), Current Research in Natural Language Generation. A related text type is school reports.
W05-1621@@By contrast, large-scale automatic corpus-based experimentation takes placemuch more easily. A centering approach to pro-nouns. 2This could have been argued, if the value of T (E3RB) had beenmuch closer to 1.
W05-1625@@is rather difficult to ob-tain, we assume that all web pages are equally reliable. and Brittany is a partof France. For this purpose, we define a coherencerate of answers.Let us assume that there are N candidate answers comingfrom N different web pages.
W05-1626@@A general question faced by the NLG practitioner iswhether to use an off-the-shelf generator or develop onefrom scratch. All activatedrules are collected in a conflict set. Case study 1: realization withautomatically derived rulesThe first project involves overgeneration (and ranking)with rules that were extracted from a semantically an-notated subset of the Penn Treebank II.
W06-0101@@ Thesaurus is one of the most useful linguistic resources. E.M. Voorhees, Implement agglomerative hierarchi-cal clustering algorithm for use in document re-trieval, Information Processing & Management. Assume there are n testing words distributed in R synonyms sets.
W06-0115@@Many important natural language processing tasksranging from part of speech tagging to parsingto reference resolution and machine translationassume the ready availability of a tokenizationinto words. A total of 144 official runswere scored: 101 for word segmentation and 43for named entity recognition. 12, with ten other scores for MSRA andCITYU Open Track above 0.
W06-0116@@In the close track, we can not use anyexternal resources. In addition, we performed a post-processing according to n-best results generatedby the CRFs model. 1edges forming a linear chain.
W06-0120@@ Unlike Western languages, Chinese does not have explicit word delimiters. First, we test the matching accuracy of each template t on the development set. Each non-Chinese character c is replaced by a cluster rep-resentative symbol M, where c is in the cluster M. We refer to the string composed of all M as F. If the length of F is more than that of W, it will be shortened to W. The normalized sentence is then placed in one file, and the non-Chinese character sequence is placed in another.
W06-0121@@Chinese word segmentation is one of the core tech-niques in Chinese language processing and attractslots of research interests in recent years. System ID adds the long or-ganization name processing strategy.For the open track of MSRA, an external dictio-nary is utilized to extract the e and f features. For MSRA corpusand other three corpora, we build System I andSystem II respectively.
W06-0124@@(2) There is no space between words in Chi-nese, so ambiguous segmentation interacts withNER decisions. In this representation, eachcharacter is tagged as either the beginning of anamed entity (B tag), a character inside a namedentity (I tag), or a character outside a named entity(O tag).When we used conjunction features, we foundthat they helped the NER performance signifi-cantly. , g.Output: A final hypothesis  pass distribution Ds(dj , ci)to the weak classifier; derive the weak hypothesis s from the weakclassifier; We have chosen to use a simple classifiercalled a decision stump in the algorithm.
W06-0126@@ Word Segmentation (WS) and Named Entity Recognition (NER) are two basic tasks for Chinese Processing. (5) BE: E: the first given name and the last given name are composed of a word. One is a basic wordlists with about 60 thousands words.
W06-0130@@ At the third International Chinese Language Processing Bakeoff, we participated in the closed test in the Named Entity Recognition (NER) task using the MSRA corpus and the CITYU corpus. In the OBIE scheme, the last character of a named entity is labeled as E. The other characters are labeled in the same way as in OBIE scheme. Run ID Precision Recall F-Score cityu_a 92.
W06-0132@@ The system NER@ISCAS is designed under the Conditional Random Fields (CRFs. For instance, the forward vari-able t(si) defines the probability that state at time t being si at time t given the observation sequence o. o (2)In similar ways, we can obtain the backward variables and Baum-Welch algorithm. The lists altogether make up a list feature (SVCL).
W06-0139@@ With the rapid expansion of text media sources such as news articles, technical reports, there is an increasing demand for text mining and processing. .A Hybrid Approach to Chinese Word Segmentation around CRFs. For the WS task, there are only two word types, B-CP (Begin of Chinese phrase) and I-CP (Interior of Chinese phrase).
W06-0202@@A common approach to Information Extraction(IE) is to use patterns which match against textand identify items of interest. Pattern Enumeration and ComplexityIn addition to encoding different parts of the de-pendency analysis, each pattern model will alsogenerate a different number of potential patterns.A dependency tree, T , can be viewed as a setof N connected nodes. would be[V/make](subj[N/Smith]+obj[N/chairman])which covers the relation between Smith andchairman but not the relations between Smithand Acme or chairman and Acme.
W06-0203@@Methods like ontology based knowledge man-agement and information access through concep-tual search have become active research topics in the general research community, with practical applications in many areas. A functional ontology of law. E.g., soil is defined in article 2 of the German soil protection law as follows:  (2) Soil within the meaning of this Act is the upper layer of the earths crust () including its liquid components (soil solution) and gaseous components (soil air), except groundwater and beds of bodies of water.
W06-0204@@A recent approach to Information Extraction (IE)is to make use of machine learning algorithmswhich allow systems to be rapidly developed oradapted to new extraction problems. 94(with a precision of 0. The systemsare evaluated using the widely used precision (P)and recall (R) metrics which are combined usingthe F-measure (F).The texts used for these experiments have beenpreviously annotated with named entities.
W06-0205@@Lexical Chains are powerful representations of doc-uments compared to broadly used bag-of-words rep-resentations. Because Gs(V,E) is builtsuch that two vertices xi and xj are connected if andonly if they are similar11, a clique has the requiredproperties to be a good cluster. xi is connected to each vertex in P (with P theclique/Pole in construction), among the connected vertices, xi is the nearestone in average (s(xi, P )).
W06-0301@@   The challenge of automatically identifying opin-ions in text automatically has been the focus of attention in recent years in many different do-mains such as news articles and product reviews. Precision (P), Recall (R), and F-score (F) of Topic and Holder identification for opinion verbs (V) and adjectives (A) on Testset 1. For adjectives, the baseline marked the subject of a predicate adjective as a holder (eg  [holder I] was happy).
W06-0305@@News articles typically contain a mixture of infor-mation presented from several different perspec-tives, and often in complex ways. The schemecaptures four salient properties of attribution: (a)source, distinguishing between different types ofagents to whom AOs are attributed, (b) type, re-flecting the degree of factuality of the AO, (c) sco-pal polarity of attribution, indicating polarity re-versals of attributed AOs due to surface negatedattributions, and (d) determinacy of attribution, in-dicating the presence of contexts canceling the en-tailment of attribution. The corpus alsoincludes a broadly defined sense classification forthe implicit relations, and attribution annotationwith the earlier core scheme.
W06-0405@@However, these constraints are often limited to purely linguistic features, such as linearity or dependency relations be-tween categories within a given syntactic tree. nodeset(POS) returns a vector of node indices corresponding to a POS: node-set(noun) dependencyset(POS) returns a vector of dependency indices corresponding to a dependency name: dependen-cyset(SUBJECT) dependencyonfirstnode(n) returns a vector of dependency indices, whose first parameter is the node index n: depend-encyonfirstnode(12) These basic instructions make it possible for a Python script to access all internal XIP data at any stages. denotes a longest match strategy.
W06-0503@@MotivationSearch engines, question answering systems andclassification systems alike can greatly profit fromformalized world knowledge. number of output pairs #D  number of documents in the hand-processed sub-corpus#C  number of correct output pairs %E  proportion of example pairs among the correct output pairs#I  number of ideal pairs Recall and Precision with confidence interval at  For the birth-date relation, we used Edward Morykwas list offamous birthdays5 as examples. 6%Lonely Planet R instanceOf CV-system  Metric (S: Standard, I: Ideal Metric, R: Relaxed Ideal Metric).
W06-0504@@Mentions are portions of text which refer to enti-ties1. Currently, mentions referringto PE R S O N , ORGANIZATION and GEO-POLITICAL_ ENTITY have been annotated and co-references among such mentions have been es-tablished. In addition, the values of theSEX attribute can be gendered words (eg Mis-ter and wordsfrom grammatical categories carrying informa-tion about gender (eg adjectives).The attributes A CTIVITY, RO L E , AF -FILIATION are three strictly connected attributes.ACTIVITY refers to the actual activity performedby a person, while ROLE refers to the positionthey occupy.
W06-0505@@The work described in this paper was carried outduring the CLASS project1. The to-tal complexity of running the forward-backwardalgorithm is O(T (q logqM)2). In S. C. Shapiro,editor, Encyclopedia of Artificial Intelligence, NewYork.
W06-0507@@With the large growth of the information stored inthe web, it is necessary to have available automaticor semi-automatic tools so as to be able to processall this web content. Select a pair of related elements to be usedas seed. If the target pt appears in rs seed listwith a different hook, and the cardi-nality is 1:1 or 1:n, incorrect.
W06-0602@@ The volume of biomedical literature available on the Web has grown enormously in recent years, a trend that will probably continue indefinitely. However, it means a segment of DNA. In addition, trained on BioProp, BIOSMILEs F-score increases by 22.
W06-0605@@The process for selecting a target language for re-search activity in corpus linguistics, natural lan-guage processing or computational linguistics islargely arbitrary. 0 AcknowledgementsThe authors are grateful to Kathryn L. Baker forher comments on earlier drafts of this paper.Portions of the research in this paper were sup-ported by the Australian Research Council Spe-cial Research Initiative (E-Research) grant num-ber SR0567353 An Intelligent Search Infrastruc-ture for Language Resources on the Web. The webas a parallel corpus.
W06-0606@@ Different corpus annotation projects are driven by different goals, are applied to different types of data (different genres, different languages, etc.) In E. Bach and R. T. Harms, eds, Universals in Linguistic Theory. The first POS layers include eight main syntactic categories, ie N (noun), V (verb), D (adverb), A (adjective), C (conjunction), I (interjection), T (particles) and P (preposition).
W06-0609@@ The PropBank corpus annotates the entire Penn Treebank with predicate argument structures by adding semantic role labels to the syntactic constituents of the Penn Treebank. (17) She wrote a letter for Mary. No distinc-tion is made between VP-level modification and S-level modification.
W06-0705@@Users of todays automatic question-answering(Q/A) systems generally have complex informa-tion needs that cannot be satisfied by asking singlequestions in isolation. AcknowledgmentsThis material is based upon work funded in wholeor in part by the U.S. Government and any opin-ions, findings, conclusions, or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect the views of the U.S.Government.ReferencesIdo Dagan, Oren Glickman, and Bernardo Magnini. By identifying contextual entailmentrelationships between answers and elements in ascenario, systems could perform valuable forms ofanswer validation that could be used to select onlythe most relevant answers for a users considera-tion.Finally, in Case 3, entailment relationships existbetween the scenario and both the users questionand the returned answer, as saving $25 million canbe considered to be both an economic advantageand one of the ways that companies prot fromoutsourcing.
W06-0707@@The Document Understanding Conference (DUC)is a series of evaluations of automatic text sum-marization systems. Rouge: A package for automaticevaluation of summaries. The correlation ishigh even when the human summaries are ignored.Metric Spearman PearsonROUGE-2 (all) 0.
W06-0804@@Information Retrieval (IR) is an established fieldand, today, the conventional IR task is embodiedby web searching. As a result,ibm is boosted eight places to rank 20. In both cases, the sentenceboundary (c.f.
W06-0901@@Events are undeniably temporal entities, but theyalso possess a rich non-temporal structure that isimportant for intelligent information access sys-tems (information retrieval, question answering,summarization, etc.). Lexical features: full word, lowercase word,lemmatized word, POS tag, depth of word inparse tree WordNet features: for each WordNet POScategory c (from N, V, ADJ, ADV): If the word is in catgory c and there is acorresponding WordNet entry, the ID ofthe synset of first sense is a feature value Otherwise, if the word has an entry inWordNet that is morphologically relatedto a synset of category c, the ID of therelated synset is a feature value Left context (3 words): lowercase, POS tag Right context (3 words): lowercase, POS tag Dependency features: if the candidate wordis the dependent in a dependency relation, thelabel of the relation is a feature value, as arethe dependency head word, its POS tag, andits entity type Related entity features: for each en-tity/timex/value type t: Number of dependents of candidateword of type t Number of entity mentions of type treachable by some dependency path(ie, in same sentence) Length of path to closest entity mentionof type t4. We train a separate classi-fier for each attribute.
W06-0902@@The first of these, which werefer to here as the local semantics of a tempo-ral expression, should be derivable in a composi-tional manner from the components of the expres-sion; determining the value of the second, whichwe refer to as the global semantics of the expres-sion, may require arbitrary inference and reason-1Note that with TimeML one can annotate not only tem-poral expressions, but also events and relations betweenevents and temporal expressions. In IWCS-5, Fifth International Work-shop on Computational Semantics.E. R. Hobbs and F. Pan.
W06-1006@@Collocations are understood in this paper as id-iosyncratic syntagmatic combination of lexicalitems Unlike idioms (kick the bucket, lend ahand, pull someones leg), their meaning is fairlytransparent and easy to decode. In Proceedings of the Ninth Machine Trans-lation Summit, New Orleans, U.S.A.Heike Zinsmeister and Ulrich Heid. A Dictionary of English Collo-cations.
W06-1009@@This paper describes an empirical evaluation ofthe Bible as a resource for cross-language infor-mation retrieval (CLIR). 39, and a precision of 0. Now, each set of n results fora particular document can itself be thought of ann-dimensional vector.
W06-1104@@Linguistic distance plays an important role inmany applications like information retrieval, wordsense disambiguation, text summarization orspelling correction. Automatic Gen-eration of a Coarse Grained WordNet. Word pairs are filtered if they contain atleast one word that a) has less than three letters b)contains only uppercase letters (mostly acronyms)or c) can be found in a stoplist.
W06-1106@@Comparison between sentences is required formany NLP applications, including questionanswering, paraphrasing, text summarizationand entailment tasks. E-cient deep processing of Japanese. A semantic network of En-glish verbs.
W06-1107@@A number ofdifferent schemes for computing word similarityhave been proposed. of alternative similarityschemes can be objectively measured on a test setcontaining both cognates and unrelated pairs fromvarious languages.A perusal of individual distances in Mielkes met-ric reveals that some of them seem quite unintuitive.For example, [t] is closer to [j] than it is to [   ], [  ]is closer to [n] than to [i], [  ] is closer to [e] thanto [g]. A linguistically-motivated metricPhonetically natural classes such as /p b m/ are muchmore common among worlds languages than unnat-ural classes such as /o z g/.
W06-1108@@We compare string distance measures for theirvalue in modeling dialect distances. The two pronunciations sharefour unigrams: [m, E, l] and [@]. Two n-grams [x1...xn] and [y1...yn] can only match if atleast one pair (xi, yi) matches linguistically.
W06-1109@@Many kinds of models in NLP can be seen as dis-tributions of a variable. Technical Report CRL MCCS-94-273, Com-puting Research Lab, New Mexico State University,March.E. Number t of character based models to bedisambiguated by word based models6.
W06-1111@@Language contact is a common phenomenonwhich may even be growing due to the increasedmobility of recent years. It is applied after comparingall the results of all the Monte Carlo re-shufflings.The BETWEEN-PERMUTATIONS NORMALIZA-TION is similar to the last step of the within-permutation normalization, except that the lineartransformation is applied across permutations, in-stead of across groups (sub-corpora): for eachPOS trigram type i in each group (sub-corpora)g  {o, y}, the redistributed count Cgi is dividedby the average redistributed count for that type inthat group (across all permutations)Cgi . A special case of this are n-gramsthat occur only in one sub-corpus.
W06-1202@@ Over the past few years, compositionality and decomposability of MWEs have become impor-tant issues in NLP research. A semantic tagger for the Finnish language. As either L or N increases, the -value decreases.
W06-1203@@For us the discrimination task involvesdetermining for a given expression whether it hasa non-compositional interpretation in addition toits compositional interpretation, and the selec-tion task involves determining in a given context,whether a given expression is being used compo-sitionally or non-compostionally. A statistical approach to the seman-tics of verb-particles. 12).To summarize Experiment I, which is a vari-ant of a supervised phrase sense disambiguationtask, demonstrates that we can use LSA to distin-guish between literal and the idiomatic usage of anMWE by using local linguistic context.
W06-1206@@However, dueto the open-ended and dynamic nature of lan-guages, and the difficulties of grammar engineer-ing, such grammars are likely to contain errorsand be incomplete. tan :t[F an]and modify the lexical entries to use these newtypes, respectively. The parsability of a sequence wi .
W06-1208@@Compound nouns are a class of multiword expres-sion (MWE) that have been of interest in recentcomputational linguistic work, as any task with alexical semantic dimension (like machine transla-tion or information extraction) must take into ac-count their semantic markedness. A categorialvariation database for English. Search en-gine statistics beyond the n-gram: Application tonoun compound bracketing.
W06-1302@@Since these systems only handle a sin-gle domain, users must be aware of the limita-tions of these domains, which were defined bythe system developer. This situation isrepresented as choice (I). For a task catego-rized as a database search type, we defined thedialogue states as one of the following two types:specifying query conditions R1considers the N-best candidates of speech recogni-tion results that can be interpreted in the previousdomain.
W06-1303@@In the recent Hollywood movie iRobot The detective finds a small device thatprojects a holographic image of the deceased. NY, USA.David D. Lewis, Robert E. Schapire, James P. Callan,and Ron Papka. I have onemission: to make better soldiers, make bet-ter warfighters.
W06-1311@@Bridging anaphora represents a special part of thegeneral problem of anaphor resolution. Associative anaphora reso-lution: A web-based approach. For instance,establishing a relationship between os professores78[the teachers], with the semantic tags H and Hprof,and os politicos [the politicians], with the seman-tic tags H and Hprof, when the correct antecedentwas os docentes [the docents], with the semantictags HH (group of humans) and Hprof.
W06-1317@@The area of modeling discourse has arguably seenless success than other areas in NLP. In Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning, Ann Arbor, USA.T. In The Language of Word Meaning.Cambridge University Press.R.
W06-1318@@As it is oftenthought that more elaborate and fine-grained anno-tation schemes are difficult for annotators to applyconsistently, we decided to address this issue in anannotation experiment on which we report in thispaper. This is for in-stance obvious in the following example of the an-notations of two utterances by two annotators:1 S what do you want to know We therefore shouldnot use a black-and-white measure of agreement,like the standard , but we should have a measurefor partial annotator agreement.In order to measure partial (dis-)agreement be-tween annotators in an adequate way, we shouldnot just take into account whether two tags are hi-erarchically related or not, but also how far theyare apart in the hierarchy, to reflect that two tagswhich are only one level apart are semanticallymore closely related than tags that are several lev-els apart. The magnitude of disagreement between c1and c2 being located in two different levels ofdepths n and n+1 might be considered to bemore different than that between to levels ofdepth n + 1 and n + 2.
W06-1320@@The goal of thematic segmentation is to iden-tify boundaries of topically coherent segmentsin text documents. We illustrate belowwhat we consider the main problems of the Pkmetric, based on two examples.Let r(i, k) be the number of boundaries be-tween positions i and i + k in the gold standardsegmentation and h(i, k) be the number of bound-aries between positions i and i+k in the automatichypothesized segmentation. The criterion for division of thecurrent segment B into b1, ...bm subsegments isbased on the maximisation of a density logPr(W|S)Pr(S),1Occasionally within this document we employ the termutterance to denote either a sentence or an utterance in itsproper sense.
W06-1403@@In recent years, the generate-and-select paradigmof natural language generation has attracted in-creasing attention, particularly for the task of sur-face realization. In (e), V&B partially covers both alt1,0and alt1,1, and thus both alternatives are active..  DerivationFollowing lexical instantiation, the lexical edgesare added to the agenda, as is usual practice withchart algorithms, and the main loop is initiated.During each iteration of the main loop, an edgeis moved from the agenda to the chart. In (c) and (d), by and spartially cover alt1,0 and alt1,1, respectively, andthus these alternatives are active for their respec-tive edges.
W06-1405@@A computer agent should be individual. A frameworkfor stylistically controlled generation. In Pro-ceedings of the 10th European Workshop on NaturalLanguage Generation.Marilyn A. Walker, Janet E. Cahn, and Steve J. Whit-taker.
W06-1408@@Over the past two decades, a number of algo-rithms for generating referring expressions have been proposed. In E. Rosch and B. Llyod (eds.) In contrast, L1-items, which comprise the items one level below the T-group-items, are not all in one group.
W06-1410@@The generation of referring expressions (hence-forth GRE)  that is, the process of workingout what properties of an entity should be usedto describe it in such a way as to distinguishit from other entities in the context  is a re-current theme in the natural language generationliterature. Structuring knowledge for referencegeneration: A clustering algorithm. thesis, Massachusetts Institute of Technology.T.
W06-1412@@In todays world of mobile, context-aware com-puting, intelligent software agents are being de-ployed in a wide variety of domains to aid hu-mans in performing navigation tasks. Visual grounding ofroute descriptions in dynamic environments.R. Smartsight: a tourist assistant system.
W06-1414@@Natural Language interfaces to databases (here-after NLIDBs ) have long held an appeal to boththe databases and NLP communities. (the availability ofan insurance plan is a boolean value). Finally, it implements a mod-ule that translates the user-composed query intoSQL .The components highlighted in grey are thosethat are constructed by the current system.The T-box describes the high-level componentsof the queries.
W06-1419@@ For this special session, a specific question was asked: what would a shared task and shared cor-pus be that would enable us to perform compara-tive evaluations of alternative techniques in natu-ral language generation (NLG) In this position paper, we question the appropriateness of this specific question and suggest that the community might be better served by (1) looking at a differ-ent question: what are the dimensions and met-rics that would allow us to compare various techniques and systems and (2) not forgetting but encouraging usability evaluations of specific ap-plications. These requirements, of course, come at a cost. 2 Enlarging the view of system-oriented evaluations The comparison of NLG systems should not be limited to a particular task in a specific con-text.
W06-1506@@In relative clauses, the wh relative pronoun can beembedded in a larger phrase, as in (1) and (2). In the semantic trees, F stands for formulas,R for predicates and T for terms. (8) John saw a boy whose brother Mary hit.
W06-1509@@An appropriateformulation of Binding Theory (BT) is needed toexplain the pattern exhibited in (13). This is obtained by strengthening thelink between NP and himself in the lexical en-try s.t. (14) a. I spoke to [John and Bill]i about eachotheri.b.
W06-1511@@More precisely, NPIs seek to be placedwithin the scope of a negative operator at the levelof semantics. I.e., they statewhich elments are contributed as possible argu-ments for other semantic expressions and whicharguments need to be filled. The features NEG, MINP and N-SCOPEwork in the same way as in the case of nicht.
W06-1513@@Surface realisation consists in producing all thesentences associated by a grammar with a givensemantic formula. Specifically,a combined SBA/polarity strategy divides by 3. the space used for cases involving between 1 and12 auxiliary trees; and by 18.  the space used forcases involving between 14 and 16 auxiliary trees.literals w/o pol. Given a set of n modifiers allmodifying the same structure, all possible inter-mediate structures will be constructed ie 2n+1.A second reason for the exponential complexityof surface realisation is lexical ambiguity.
W06-1520@@Generative grammars that we commonly hearabout in computational linguistics are usuallybased on syntactic categories. A lexicalist and construction-based approach to coordinations. Statistical parsing with anautomatically-extracted Tree Adjoining Grammar.In Proceedings of the 38th Annual Meeting of theAssociation for Computational Linguistics, HongKong, China.N.
W06-1601@@An important source of ambiguity that must beresolved by any natural language understandingsystem is the mapping between syntactic depen-dents of a predicate and the semantic roles1 thatthey each express. A maximum entropy inspired parser. So to represent uncertaintyover these variables, we need only to represent adistribution over possible role vectors r. Thoughin the worst case the set of possible role vectors isstill exponential, we only need role vectors that areconsistent with both the observed list of syntacticrelations and a linking that can be generated bythe construction operations.
W06-1602@@Although superlative noun phrases (the nationslargest milk producer, the most complex arms-control talks ever attempted, etc.) To deal with this, we anal-yse possessives that interact with the superlative asfollows:Rome s oldest churchNP ((NP/N)/(N/N)\NP N/N N(NP/N)/(N/N)NP/NNPThis analysis yields the correct comparison set forsuperlative that follow a possessive noun phrase,given the following lexical semantics for the geni-tive:n.S.p.q.(u;S(x. 2for a theory of semantics.
W06-1603@@The task of sentence-level paraphrase recognition(PR) is to identify whether a set of sentences (typ-ically, a pair) are semantically equivalent. A trivialcase is when both sentences are identical, wordfor word. For the second, which tuple(s) (if any)are responsible for a pp instance.
W06-1604@@Semantic processing of text in applications suchas question answering or information extractionfrequently relies on statistical parsers. We call thisrepresentation a relational conjunction (RC). Con-sider the Treebank sentence: The S&P pit stayedlocked at its 30-point trading limit as the Dow av-erage ground to its final 190.
W06-1606@@To our knowledge though, no previous re-search has demonstrated that a syntax-based sta-tistical translation system could produce better re-sults than a phrase-based system on a large-scale,well-established, open domain translation task. lm(e) is the language model probability ofthe target translation under an ngram lan-guage model. A formal introduction to SPMT2.
W06-1607@@Smoothing is an important technique in statisticalNLP, used to deal with perennial data sparsenessand empirical distributions that overfit the trainingcorpus. International Association forMachine Translation.Stanley F. Chen and Joshua T. Goodman. Any similar strategy that does not split up{s|c(s, t) can be applied toany smoothing scheme.
W06-1608@@Thisfinding has generally been cast in favorable terms:such systems are robust to poor quality word align-ment. A parser that achieves 90. E. Frederking and K. B. Taylor, editors, Machinetranslation: From real users to research.
W06-1610@@Both evaluation tasks employ a compari-son strategy for comparing textual units from machine-generated and gold-standard texts. The BL E U-esque Matching Philosophy The primary task that a BLEU-esque procedure performs is to compare n-grams from the peer translation with the n-grams from one or more reference translations and count the number of matches. A systematic comparison of various statistical alignment models.
W06-1611@@ Predictive models of spoken dialogue system (SDS) performance are an important tool for re-searchers and practitioners in the SDS domain. The remediation subdialogue is specified in an-other question segment and corresponds to a new discourse segment (e.g DS2). For each trend or significant correla-tion we report the unigram/bigram, its average and standard deviation over all students, the Pearsons Correlation Coefficient (R) and the statistical significance of R (p).
W06-1612@@Information structure is the way a speaker orwriter organises known and new information intext or dialogue. A coefficient of agreementfor nominal scales. %, with high f-scores forboth classes.
W06-1613@@Why do researchers cite a particular paper However, re-searchers from the field of discourse studies havelong criticised purely quantitative citation analy-sis, pointing out that many citations are done outof politeness, policy or piety They found,for example, that 40% of the citations were per-functory, which casts further doubt on the citation-counting approach.Based on such annotation schemes and hand-analyzed data, different influences on citation be-haviour can be determined. In Proc.of the Third ACM Conference on Digital Libraries, pages8998.T.L. A re-examination of textcategorization methods.
W06-1615@@Discriminative learning methods are ubiquitous innatural language processing. Herewe outline a few recent advances. Fortraining instance t, the augmented feature vectorwill contain all the original features xt plus thenew shared features xt.
W06-1616@@Many inference algorithms require models tomake strong assumptions of conditional indepen-dence between variables. bestk(i, j)where n is the number of tokens and the index 0represents the root token. Ox is the objective function and Vxis a set of variables including integer declarations.solve(C, O, V ) maximises the objective functionO with respect to the set of constraints C and vari-ables V .
W06-1617@@is the object(labeled as ARG1): as the subject(ARG0) and Greenspan as the object (ARG1)of the noun predicate replacement. (NONETree(child))(2)Max(T ) is the maximum log-probability of atree T , NONE(T ) and ARG(T ) are respectivelythe log-probability of assigning label NONEand ARG by our argument identification modelto tree node T , child ranges through each ofT s children, and NONETree(child) is the log-probability of each node that is dominated by nodechild being labeled as NONE. Zx is a normalization factor.
W06-1618@@ Research in question answering, machine transla-tion and other fields has shown that being able to recognize the important entities in a text is often a critical component of these systems. Our model achieves a recall of 70. TimeBank annotates a word or phrase as an EVENT if it describes a situation that can hap-pen or occur, or if it describes a state that participate[s] in an opposi-tion structure in a given text Note that the TimeBank events are not restricted to verbs; nouns and adjectives denote events as well.
W06-1619@@For the last decade, accurate and wide-coverageparsing for real-world text has been intensivelyand extensively pursued. A comparison of algorithms formaximum entropy parameter estimation. )),where u is a model parameter, fu is a featurefunction that represents a characteristic of parsetree T , and Zw is the sum over the set of all pos-sible parse trees for the sentence.
W06-1620@@Pre-cision grammars are defined as implementedgrammars of natural language which capture fine-grained linguistic distinctions, and are generativein the sense of distinguishing between grammat-ical and ungrammatical inputs (or at least havesome in-built notion of linguistic markedness ).Additional characteristics of precision grammarsare that they are frequently bidirectional, and out-put a rich semantic abstraction for each span-ning parse of the input string. A general featurespace for automatic verb classification. The University of Chicago Press,Chicago, USA.Stefan Riezler, Tracy H. King, Ronald M. Kaplan, RichardCrouch, John T. Maxwell III, and Mark Johnson.
W06-1621@@A fundamental task for text understanding ap-plications is to identify semantically equivalentpieces of text. All mod-els assign a [0, 1] score to a given pair of text tand target word u which can be interpreted as theconfidence that u is lexically referenced in t. 75ID TEXT HYPOTHESIS ENTAIL-MENTREFER-ENCE1 Iran is said to give up al Qaeda members. Extracting paraphrasesfrom a parallel corpus.
W06-1623@@Understanding the temporal flow of discourse isa significant aspect of text comprehension. Elements of Symbolic Logic.Macmillan, New York, NY.Robert E. Schapire and Yoram Singer. Temporalconnectives in a discourse context.
W06-1626@@In this example, alternative hypoth-esis e(2) is a better translations than e(1) accordingto the reference (Ref) although its model score islower.SMT models are not perfect, it is unavoidableto have a sub-optimal translation output as themodel-best by the decoder. Given a test set with Tsentences, N hypotheses are generated for eachsource sentence ft. Denote e(r)t as the r-th rankedhypothesis for ft. e(1)t is the model-best hypoth-esis for this sentence. Calculate n-gram frequency using suffixarrayFor a corpus D with N words, locating all the oc-currences of wiin+1 takes O(logN ).
W06-1627@@The algorithm builds a synchronous parsetree for both sentences, and assumes that the treeshave the same underlying structure but that the or-dering of constituents may differ in the two lan-guages. After factorization, counting thefree variables enclosed in the innermost max oper-ator, we get five: i, k, u, v1, and u2. Inversion Transduction GrammarAn Inversion Transduction Grammar can generatepairs of sentences in two languages by recursivelyapplying context-free bilingual production rules.Most work on ITG has focused on the 2-normalform, which consists of unary production rulesthat are responsible for generating word pairs:X  e/f224and binary production rules in two forms that areresponsible for generating syntactic subtree pairs:X  Y ZThe rules with square brackets enclosing theright hand side expand the left hand side symbolinto the two symbols on the right hand side in thesame order in the two languages, whereas the ruleswith pointed brackets expand the left hand sidesymbol into the two right hand side symbols in re-verse order in the two languages.
W06-1628@@However,phrase-based models lack a direct representationof syntactic information in the source or target lan-guages; this has prompted several researchers toconsider various approaches that make use of syn-tactic information.This paper describes a framework for tree-to-tree based statistical translation. We then discard any clause pairs (e, g)which are inconsistent with the NP/PP alignmentsfor that sentence. di1 to a subset ofDi.
W06-1629@@Reflecting the rapid growth of science, technology,and economies, new technical terms and productnames have progressively been created. As a result, 16 943 word typeswere extracted. In probabilis-tic natural language processing, P (K) is usuallyrealized by a word or character N-gram model, andtherefore a K that appears frequently in a corpusis assigned a high probability.However, because our purpose is to generatenew words, the use of statistics obtained from ex-isting corpora is not effective.
W06-1630@@As a part of a on-going project on multilingualnamed entity identification, we investigate unsu-pervised methods for transliteration across lan-guages that use different scripts. Similarly, stop and frica-tive consonants such as /p, t, k, b, d, g, s, z/ arefrequently deleted when they appear in the codaposition. When a name is rare in a collection,253Source Target Cost Target Costg g 0 r 40. kh 2.  e 44. cCh 5.  del 24tsh 17.  ins 20N 26.
W06-1631@@Arabic words are derived from roots having three,four, or, in rare instances, five characters. A listof 285 482 Arabic words was extracted. To classify a text t, we build its n-gram frequency profile, and compute the distancebetween each n-gram in the text and in each lan-guage profile lj .
W06-1632@@Our work is concerned with multi-document sum-marization, namely with the merging of multipledocuments about the same topic taken from theweb. A paragraph bound-ary detection system. Being anaphors, proadverbials, such asdeswegen (because of that), daruber (aboutthat) explicitly link a sentence to the preced-ing one(s).
W06-1633@@Recent coreference resolution algorithms tacklethe problem of identifying coreferent mentions ofthe same entity in text as a two step procedure: (1)a classification phase that decides whether pairs ofnoun phrases corefer or not; and (2) a clusteriza-tion phase that groups together all mentions thatrefer to the same entity. S.E is the set of edges from S,while T.E is the set of edges from T . For maximum entropy classification weused a maxent4 tool.
W06-1634@@Although such IE attempts have demon-strated near-practical performance, the same setsof patterns cannot be applied to different kinds ofinformation. , pi) is a candidateof the raw pattern. There are three types of division ofraw patterns for generating combination patterns.These are:(a) Two-entity Division(a-1) Entity-Main-Entity Division(a-2) Main-Entity-Entity Division(b) Single-entity Division, and(c) No Division (Naive Patterns).Most raw patterns, where entities are at bothends of the patterns, are divided into Entity-Main-Entity.
W06-1637@@In psycholinguistics, priming refers to the fact thatspeakers prefer to reuse recently encountered lin-guistic material. Nonlo-cal dependencies that arise through wh-movementand right node raising (*T* and *RNR* traces) arecaptured in the resulting derivation. Syntactic priming: A corpus-based approach.
W06-1639@@One ought to recognize that the presentpolitical chaos is connected with the de-cay of language, and that one can prob-ably bring about some improvement bystarting at the verbal end. The1It is worth pointing out that the United States [alter] the citizen-government relationship E.g., Internet injects sweeping change into U.S. In Proceedings ofDigital Government Research (dg.o).W.
W06-1640@@Sentiment analysis is concerned with extractingattitudes, opinions, evaluations, and sentimentfrom text. We com-pute transitive closure by using a Union-Findstructure, which runs in time O(logn), which forpractical purposes can be considered linear (O(n))3. E-rulemaking: Information technologyand regulatory policy: New directions in digital govern-ment research.
W06-1641@@The recent rapid expansion of access to informa-tion has significantly increased the demands on re-trieval or classification of sentiment informationfrom a large amount of textual data. In the second task, we split the corpusinto two parts: (i) the training set, which was usedfor tuning the model parameters; and (ii) the test-ing set, which was used for constructing RsandRtand from which we retrieved sentences in re-sponse to queries. A translationmodel for sentence retrieval.
W06-1642@@feelings as expressed in positive ornegative comments, by analyzing unreadablylarge numbers of documents. Its light and has a zoom lens. I think it is beautiful.
W06-1643@@Summarization of meetings faces many challengesnot found in texts, ie, high word error rates, ab-sence of punctuation, and sometimes lack of gram-maticality and coherent ordering. of COLING,pages 889895.E. We show that there is a strong correlation be-tween the two elements of an AP in summariza-tion, and that one is unlikely to be included if theother element is not present in the summary.Most current statistical sequence models in nat-ural language processing (NLP), such as hiddenThis material is based on research supported in part bythe U.S. National Science Foundation (NSF) under GrantsNo.
W06-1644@@ With the rapid growth of audio-visual materials available over the web, effective language mod-eling of the diverse content, both in style and topic, becomes essential for efficient access and management of this information. T. Griffiths and M. Steyvers. Initial experiments demonstrate a 16.
W06-1646@@N -gram models have long been the stronghold ofstatistical language modeling approaches. Inflectional morphology works as aproxy for structured syntax in a language. (y), offeatures extracted from the hypothesis fj(y) andmodel parameters j , ie, v (yi|Ys) is modeledby a maximum entropy distribution of the form,P Thischoice simplifies the numerical estimation proce-dure since the gradient of the log-likelihood withrespect to a parameter, say j , reduces to differ-ence in expected counts of the associated feature,E In a large vocab-ulary system, this could be an enormous space.However, in a discriminative maximum entropyframework, only the observed features are consid-ered.
W06-1647@@Due to the rising importance of globalization andmultilingualism, there is a need to build natu-ral language processing (NLP) systems for an in-creasingly wider range of languages, includingthose languages that have traditionally not beenthe focus of NLP research. We achieve a POS tagging accuracyof 63. (3) Unsupervised trainingof HMM Tagger using both raw text and inferred lexicon.N Type Token0 23.
W06-1650@@ Unbiased user-supplied reviews are solicited ubiquitously by online retailers like Ama-zon.com, Overstock.com, Apple.com and Epin-ions.com, movie sites like imdb.com, traveling sites like citysearch.com, open source software distributors like cpanratings.perl.org, and count-less others. A practical guide to SVM classification. Task Definition Formally, given a set of reviews R for a particu-lar product, our task is to rank the reviews ac-cording to their helpfulness.
W06-1652@@Sentiment analysis and opinion recognition are ac-tive research areas that have many potential ap-plications, including review mining, product rep-utation analysis, multi-document summarization,and multi-perspective question answering. MIT Press,Cambridge, MA.S-M. Kim and E. Hovy. We used a list of 281stopwords.
W06-1655@@The problem of segmenting sequence data intochunks arises in many natural language applica-tions, such as named-entity recognition, shallowparsing, and word segmentation in East Asian lan-guages. Weconclude that it is not a practical alternative to omitthe C and CC label features. This type of fea-ture (specifically, a feature indicating the absenceas opposed to the presence of a chunk boundary)is a bit less natural in a semi-CRF, since in thatcase local features S(yj , yj+1,x) are defined onpairs of adjacent boundaries.
W06-1656@@Most IE systems rely on knowledge engineering or on machine learning to generate extraction patterns  the mechanism that extracts entities and relation instances from text. The final confidence estimates c(E) for the extraction E is set to the maximum of L(f(M)) over all matches M that produced E. 4   Experimental Evaluation Our experiments aim to answer three questions:   1. For example, assuming there is a seed instance Acquisition(Oracle, PeopleSoft), the sentence The Antitrust Division of the U.S. Department of Justice evaluated the likely competitive effects of Oracles proposed acquisition of PeopleSoft.
W06-1659@@ In this paper we propose a novel, and completely unsupervised approach for information extrac-tion. An example of a pattern is:      GPE(E2)  POS   (PERSON)+(E1)  This pattern indicates that the word(s) with the tag GPE in the sentence represents the second en-tity (Entity 2) in the relation, while the word(s) tagged PERSON represents the first en-tity (Entity 1) in this relation, the + symbol means that the (PERSON) entity is repetitive (ie may consist of several tokens). A Cluster Algorithm for Graphs.
W06-1660@@Named entities (NE) are phrases that containnames of persons, organizations, locations, etc.Named entity recognition (NER) is an importanttask in many natural language processing appli-cations, such as information extraction and ma-chine translation. An algorithm that learns whatsin a name. Experimental results show that the per-formance is significantly enhanced by using infor-mative training samples.In the future, wed like to focus on furtherexploring more effective methods to adapt NERmodel to a new domain with much less efforts,time and performance degrading.ReferencesDaniel M. Bikel, Richard L. Schwartz, and Ralph M.Weischedel.
W06-1661@@This paper describes the application of several dif-ferent statistical models for the task of realiza-tion ranking in tactical generation, ie the problemof choosing among multiple paraphrases that aregenerated for a given meaning representation. A smorgasbord of features for statisticalmachine translation. In R. Dale & K. fai Wong (Eds.
W06-1662@@Since summary sentences generally come from different sources in multi-document summarization, an optimal ordering is crucial to make summaries coherent and readable. For comparison between different matrices, E(M) needs to be averaged over nm. After classification, each sentence is assigned a label according to 7).
W06-1663@@Even now,building large and rich enough knowledge basesfor broadcoverage semantic processing takes agreat deal of expensive manual effort involvinglarge research groups during long periods of de-velopment. This baselinecan be considered as a lower-bound. In Proceedings ofCoNLL, Toulouse, France.E.
W06-1664@@The web is a good source of linguistic informa-tion for several natural language techniques suchas question answering, language modeling, andmultilingual lexicon acquisition. For (i), the results are identical: Bycutting edge c-d, which is a minimum edge cut, wecan obtain two clusters. A search engine fornatural language applications.
W06-1666@@For structured classification tasks,where labels are complex and have an internalstructure of interdependency, the 0-1 loss consid-ered in classical formulation of classification al-gorithms is not a natural choice and different lossfunctions are normally employed. We wouldexpect even better results with MBR-decoding iflarger n-best lists are used. A study on convolutionalkernels for shallow semantic parsing.
W06-1669@@Word sense disambiguation (WSD) is a keyenabling-technology. Each word in the context receivesa set of scores s, with one score per hub, where allscores are 0 except the one corresponding to thehub where it is placed. As G is undirected, the in-degree of a vertex v is equalto its out-degree.The MST is then used to perform word sensedisambiguation, in the following way.
W06-1670@@Named entity recognition (NER) is the most stud-ied information extraction (IE) task. Hierarchical Preferences in a Broad-CoverageLexical Taxonomy. The four lower rows reportthe average polysemy of nouns (N) ), in each dataset, both at the synset level(WS) The averagenumber of senses decreases significantly when themore general sense inventory is considered.We substituted the corresponding supersense toeach noun and verb synset in all three data-sets:SEM, SEMv and SE3.
W06-1671@@Information extraction (IE) algorithms populate adatabase with facts discovered from unstructuredtext. Note that R may contain mul-tiple fields with the same attribute but differentvalues (eg a person may have multiple job ti-tles). An algorithm that learns whatsin a name.
W06-1672@@ Name transliteration is an important task of tran-scribing a name from alphabet to another. A. Klementiev and D. Roth. 2 Preliminaries Let E and F be two finite alphabets.
W06-1673@@Almost any system for natural language under-standing must recover hidden linguistic structureat many different levels: parts of speech, syntac-tic dependencies, named entities, etc. Let A bethe set of n annotators A1, A2, ..., An (eg, partof speech tagger, named entity recognizer, parser).These are the variables in the network. Eachstage in the pipeline corresponds to a variable inthe network.
W06-1709@@ Because the field of phonology studies sound patterns of languages, corpus-based phonology typically relies on audio corpora. intervocalic tapping (d can become the tap sound [], spelled r, when it is be-tween two vowels): dumi dirt vowel-height alternations (o in final syl-lables can alternate with u in non-final syllables; there is a similar but more complicated i/e alternation): halo mix nasal assimilation (a nasal consonant can take on the place of articulation of a fol-lowing consonant): pam-butas borer (ng represents the velar nasal [])  nasal substitution (stem-initial obstruents can turn into nasals when certain pre-fixes are added): pili choosing syncope (the vowel of a stems final syl-lable can be deleted when a suffix is added, and the consonants that conse-quently become adjacent can undergo changes): gawa act gaw_-in to be done, tingin look partial reduplication (when foreign stems that begin with consonant sequences and/or foreign consonants such as f un-59dergo copying of the first syllable, the consonant sequence can be simplified and the foreign consonant can be nativ-ized): nag-fri-friendster ~ nag-pi-friendster using Friendster infix location (in foreign stems begin-ning with consonant sequences, an infix can go inside or after the consonant se-quence): g-um-raduate ~ gr-um-aduate graduated infix in vs. prefix ni: l-in-uto ~ ni-luto to be cooked location of reduplication in prefixed words: pa-pag-lagy-an ~ pag-la-lagy-an will place Cor-pus frequencies of the variants, however, or even basic word frequencies, have not previously been available. The histograms below show how many pre-fixed words display each range of tapping rates in the corpus, from 0 (always d) to 1 (always r).
W06-1710@@In order to reliably judge the collocative affinity oflinguistic items, it has to be considered that judge-ments of this kind depend on the scope of certaingenres or registers. )on grounds of a corpus of Wikipedia articles. one against all: First we ap-ply a one against all strategy, that is, we useX \ Yi as the set of negative examples forlearning category Ci where X is the set of alltraining examples and Yi is the set of positiveexamples of Ci.
W06-1803@@One current research hypothesis is that thisdifference is accounted for by interactive dialogue,which allows students to ask questions freely, andtutors to adapt their direct feedback and presenta-tion style to the individual students needs.Adding natural language dialogue to a tutorialsystem is a complex task. This corresponds tothe query variant (a) in the system. A question-answeringsystem for AP chemistry: Assessing KR&R tech-nologies.
W06-1805@@Understanding a text is one of the ultimate goalsof computational linguistics. An, the following ax-ioms schemas will be licensed:i, j s.t. Adjectives inClass 1 can be related to both nouns and verbs.Thus, for example the adjective afloat in WordNetis related to the noun floating which is related tothe verb float, by assuming that the semantics as-signed to the verb float is float(e), theme(e,a), theadjective afloat is assigned the following seman-tics:afloat  Theme1.floatThis is encoded in the following axiom schemas:MDR 1.
W06-1807@@Many NLP applications would benefit from theavailability of broad-coverage knowledge extrac-tion from natural language text. The goal ofsuch a translation would be to solve a problemP (such as paraphrasing or question-answering)where F allows P to be solved by some reason-ing process, or else the domain exhibits a type ofstructure easily represented in the formalism F .If we accept that current parsing technologycannot reliably combine accurate semantic anal-ysis with robustness, then the question ariseswhether noisy semantics can be ameliorated us-ing some other techniques. Wordnet: A lexical database forenglish.
W06-1808@@Search engines on the web and most existingquestion-answering systems provide the user witha set of hyperlinks and/or web page extracts con-taining answer(s) to a question. On the contrary, if r is low (0. drop) indicates a faster growth(resp.
W06-1907@@Much of the research into automatic question an-swering (QA) has understandably concentrated onthe English language with little regard to portabil-ity or efficacy in other languages. In Proceed-ings of the 14th Text Retrieval Conference.E.W.D. %contained a correct answer.
W06-2001@@Recently, the Natural Language Processing com-munity has become more and more interestedin developing language independent systems,in the effort of breaking the language barrierhampering their application in real use scenar-ios. Data andKnowledge Engineering Journal, page (To be pub-lished).E. (a day before)Implicit Dates reference!
W06-2002@@One way to decom-pose this parse tree is to view it as a sequenceof applications of CFG rules. Choose a positive integer n. . null denotes an unlabeledspan.i, j).
W06-2003@@ Studying various relationships between lan-guages is a central task in computational linguis-tics, with many application areas. 3); and (5) inserted Span-ish letters include the vowels o, e, a and i, at that order, where o overwhelms the others. The Eng-lish o maps exclusively to the Spanish o and not to other vowels.
W06-2006@@In this paper we describe a method which uses one ormore intermediary languages to automatically generatea dictionary to translate from one language, , to an-other,  . The papillon project: Cooperatively build-ing a multilingual lexical data-base to derive opensource dictionaries and lexicons. The method relies on using dictionar-ies that can connectto  and back tovia the in-termediary language(s).
W06-2008@@For Pol-ish, this has been made possible on account of itsaccessing to the European Union (EU) which hasresulted in the construction of a large multilingualcorpus of EU legislative texts and a growing inter-est for new Member States languages.This paper presents a direct projection of vari-ous morpho-syntactic informations from Englishand French to Polish. Result analysisThere are various reasons for the failure of thePOS tags and dependencies projection: a) wordalignment, b) lexical density, c) tokenization, d)POS tagging/parsing errors and e) insertion (fordependencies). A system-atic comparison of various statisical alignment mod-els.
W06-2105@@Advanced NLP applications such as question an-swering require deep semantic interpretation. .situation [si]dynamic situation [dy]action [da] write, sing, sellhappening [dn] rain, decaystatic situation [st] stand, be illsituational descriptor [sd]time [t] yesterday, Mondaylocation [l] here, there. (2) a. the building on the hillb.
W06-2106@@To date, 13 outof 373 prepositions (among the most frequent inEnglish) have been analyzed. Each sense isalso characterized by its syntactic function and itsmeaning, identifying the relevant paragraph(s)where it is discussed in Quirk et alTPP then makes available the sense analysis(including the lexicographers overview) and the setof instances for each preposition that is analyzed. Alexicographer then assigns a sense from theinventory to each instance.
W06-2110@@In the case of English,MWEs are conventionally categorised syntactico-semantically into classes such as compound nom-inals (eg New York, apple juice, GM car), verbparticle constructions (eg hand in, battle on),non-decomposable idioms (eg a piece of cake,kick the bucket) and light-verb constructions (egmake a mistake). In general, wewould expect the data set containing the high fre-quency and both positive and negative examples70Freq Type # P R Ff1 V 4WS . What do I hand inb.
W06-2112@@Any computer system for natural languageprocessing has to struggle with the problem of am-biguities. Whereas1The V-N-P-N quadruples also contain the head noun ofthe NP within the PP. allow for a more detailed investiga-tion.
W06-2201@@Their method, based ona training set, identies natural language surfacetext patterns that express some relation betweentwo instances. Snowball: Ex-tracting relations from large plain-text collections.In Proceedings of the Fifth ACM International Con-ference on Digital Libraries.E. If relation R is not functional, the patternshould be wide-spread, ie among the searchresults when querying a combination of thepattern and an instance in Iq there must be asmany distinct R-related instances from ca aspossible.To measure these criteria, we use the followingscoring functions for relation patterns s. .
W06-2204@@Information extraction is a form of shallow text anal-ysis that involves identifying domain-specific frag-ments within natural language text. More precisely, the children of state (G,P )are:{(G, P ) doesnt overlap e}}.The search proceeds as follows. To achieve this, we initializethe scores, and then iterate through the scoring process(ie calculate scores at step t+1 from scores at step t).This process repeats until convergence.Initialization.
W06-2207@@Traditionally, Information Extraction (IE) identi-fies domain-specific events, entities, and relationsamong entities and/or events with the goals of:populating relational databases, providing event-level indexing in news stories, feeding link discov-ery applications, etcetera.By and large the identification and selective ex-traction of relevant information is built around aset of domain-specific linguistic patterns. ), and the probability of seeing a word givena category, P (w|c; ). A semantic ap-proach to ie pattern induction.
W06-2303@@Moving towards a shal-low semantic level of representation is a first ini-tial step towards the distant goal of natural lan-guage understanding and has immediate applica-tions in question-answering and information ex-traction. The VPnode is assumed to be on the top of the parsersstack, and the S one is supposed to be its left-corner ancestor. , di1) include hand-crafted features of the derivation history that aremeant to be relevant to the move to be chosenat step i.
W06-2305@@Traditionally, broad coverage has always been consid-ered to be a desirable property of a grammar: the morelinguistic phenomena are treated properly by the gram-mar, the better results can be expected when applyingit to unrestricted text (c.f. leads to a greater increase of inaccuracy of 0. Such an approach is mainly used to either ex-clude rare phenomena in grammar induction (c.f.
W06-2404@@As in the case of other languages, the Japaneselanguage has various types of functional wordssuch as post-positional particles and auxiliaryverbs. mk includingmi at the current position i constitutes a candidatefunctional expression E as below:mj2mj1mj. Suppose that a se-quence of morphemes mj .
W06-2405@@We assumethat literal expressions can be distinguished fromidiomatic expressions provided we know how theirmeaning is derived. Multiword expressions: a painin the neck for NLP. P (t|s) is estimated asthe proportion of alignment t among all align-ments of word s found in the corpus in the con-text of the given triple.
W06-2501@@Humans are able to quickly judge the relative se-mantic relatedness of pairs of concepts. Extended glossoverlaps as a measure of semantic relatedness. Resnik 0.0Leacock & Chodorow 0.0evaluation of the measures of semantic related-ness.
W06-2503@@It is likely that accurate word-level semantic dis-ambiguation would benefit a number of differenttypes of NLP application; however it is gener-ally acknowledged by word sense disambiguation(WSD) researchers that current levels of accuracyneed to be improved before WSD technology canusefully be integrated into applications (Ide andWilks, in press). Automaticgeneration of a coarse grained WordNet. 0)] where the sensenumber is indicated before the bracketed r score.
W06-2505@@ Word Sense Disambiguation (WSD) is con-cerned with the choice of the most appropriate sense of an ambiguous word given its context. I Dont Believe in Word Senses. (e) n senses  n translations: different senses of a word being translated as different Portuguese words.
W06-2602@@In machine learning for natural language process-ing, many diverse tasks somehow involve pro-cessing of sequentially-structured data. They will be consideredgiven in the following, so we will refer to thisspecific instantiation simply as N(x). In I. Dagan and D. Gildea, editors, Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning.A.
W06-2604@@Document Categorization, the assignment of nat-ural language texts to one or more predefinedcategories based on their content, is an impor-tant component in many information organizationand management tasks. , 70,where ti is the total number of training documentsin category i. and Harshman, R.: Indexing by Latent Seman-tic Analysis.
W06-2605@@The availability of corpora annotated with syntac-tic information have facilitated the use of prob-abilistic models on tasks such as syntactic pars-ing. #is used to denote a constant. As the numberof examples covered (m) grow, the requirementson g(H) become even stricter.
W06-2606@@Statistically driven machine translation systemsare currently the dominant type of system in theMT community. ParsingEnglish with a link grammar. Yamamoto, and S. Yamamoto.
W06-2607@@In general, given a sen-tence in natural language, the annotation of a pred-icates semantic roles requires (1) the detection ofthe target word that embodies the predicate and(2) the detection and classification of the word se-quences constituting the predicates arguments. The fact that it is more accu-rate than CPAF reveals that we need to distin-27 for the core arguments (A0...AA), 13 for the adjunctarguments (AM-*), 19 for the argument references (R-*) and20 for the continuations (C-*). This prevents us to extract a treerepresentation for them.
W06-2608@@In computational linguistics, it is usual to deal withsequences: words are sequences of letters and syn-tagmatic relations are established by sequences ofwords. Note that s[i] does not nec-essarily form a contiguous subsequence of s. Forexample, if s is the sequence Ronaldo scored thegoal n are auxiliary functions with a sim-ilar definition as Kn used to facilitate the compu-tation. Domain ProximityThe approach described above requires a large scalelexical resource.
W06-2609@@Answering definition questions is a challenge forquestion answering systems. medicijn (a pain alleviating. In IJCAI-99 Workshop on Machine Learning for InformationFiltering, pages 6167.Juan C. Sager and M.C.
W06-2703@@This can be accom-plished by enabling actions such as click and snap-ping to the ends of word tokens. Forexample, a POS tagger trained on a more conven-tionally tokenised dataset might have no problemassigning a propernoun tag to Met-tRNA/eIF2   in... and facilitates loading of the Met-tRNA/eIF2   GTP ternary complex ...however, it would find it harder to assign tags tomembers of the 10 token sequence M et t RNA/ e IF 2   .Similarly, a statistical NER tagger typically usesinformation about left and right context looking ata number of tokens (typically one or two) on eitherside. 7 GHz, COladder from (2-1) up to (7-6), non-LTE line.In the Text Mining programme (TXM) we havecollected a corpus of abstracts and full texts ofbiomedical papers taken from PubMed Central,the U.S. National Institutes of Health (NIH) freedigital archive of biomedical and life sciencesjournal literature3 .
W06-2704@@Computational linguistics increasingly requiresdata sets which have been annotated for many dif-ferent phenomena which relate independently tothe base text or set of signals, segmenting the datain different, conflicting ways. n.d. Linguistic inter-pretation of a German corpus. Timings which are purely in secondsare given to 2 decimal places; those which extendinto the minutes are given to the nearest second.NXT is the condi-tion with XQuery using stand-off data; and XQ-K is the condition with XQuery using the redun-dant knitted data.QueryImplQ1 Q2 Q3NXT 3. s 1m24 18.
W06-2705@@ In translation studies the question of how trans-lated texts differ systematically from original texts has been an issue for quite some time with a surge of research in the last ten or so years. TnT A Statistical Part-of-Speech Tagger. Furthermore, the respective language as well as the n attribute or-ganising the order of the aligned texts are given.
W06-2711@@In the TALK project2 we are developing a mul-timodal dialogue system for an MP3 applicationfor in-car and in-home use. The annotated corpuswill be used (i) to investigate various aspects of1Our demonstration results from the efforts of a largerteam including also N. Blaylock, B. Fromkorth, M. Grac,M. A corpus collection and annotationframework for learning multimodal clarification strate-gies.
W06-2713@@ It is well known that when using XML-based annotation schemes to represent multi layer an-notations, it can be difficult to handle partially overlapping annotations. As a conse-quence, all AIF annotations make reference to character positions. In the following, we are going to demonstrate how I-CAB texts and their annotations can be accessed through the MEANING Browser.
W06-2714@@ and MotivationHeart of Gold is a middleware architecture for cre-ating and combining markup produced by mul-tiple natural language processing components inmultilingual environments. Mappings need only be defined for theimmediately depending annotations (see next sec-tion) which is by far not an n-to-n mapping in prac-tical applications.However, the fact that a specific DTD orSchema is not imposed by the middleware doesnot mean that there are no minimal requirements.Linking between different standoff annotations isonly possible on the basis of a least common en-tity, which we propose to be the character spans inthe original text2. The sce-nario is equally a good example for XSLT-basedannotation integration.
W06-2717@@The combined research on treebanks and paral-lel corpora has recently led to parallel treebanks.A parallel treebank consists of syntactically anno-tated sentences in two or more languages, takenfrom translated (ie parallel) documents. TheTreeAligner allows for m:n sentence alignment,word alignment and node alignment. of the Treebank-Workshop at Nodalida, Joen-suu, May.Noah A. Smith and Michael E. Jahr.
W06-2801@@With the advent of web-based communication,more and more corpora are accessible which man-ifest complex networks based on intertextual rela-tions. The cluster values areconfirmed by the wikis of technical docu-mentation (also w.r.t their numerical order).Thus, these wikis tend to be small worlds ac-cording to the model of Watts & Strogatz,but also prove disassortative mixing  compa-rable to technical networks but in departurefrom social networks. C1 C2 rWikipedia variant I undirected 19.
W06-2810@@Parallel corpora form the basis of much multilin-gual research in natural language processing, rang-ing from developing multilingual lexicons to sta-tistical machine translation systems. E.g., in describ-ing a particular person, reference will be made tosuch entities as country, organization and other im-portant entities which are related to it and whichthemselves have entries in Wikipedia. Hypertext functionality:A theoretical framework.
W06-2902@@In recent years, significant progress has been madein the area of natural language parsing. These results represent a 2. , ys) suggested by aprobabilistic model P (x, y| is a vector ofmodel parameters learned during training the prob-abilistic model.
W06-2904@@Subsequent researchbegan to focus more on conditional models of parsestructure given the input sentence, which alloweddiscriminative training techniques such as maximumconditional likelihood (ie maximum entropy However, as we demonstrate,not only can smoothing be applied in a large mar-gin training framework, it leads to generalization im-provements in much the same way as probabilisticapproaches. A probability model toimprove word alignment. Cur-rently, we only use undirected forms of these fea-tures, where, for example, Q RS  Q S,R for all pairs(or, put another way, we tie the parameters [ RS [S,R together for all YK	Z ).
W06-2905@@Many current theories of language use and acquisi-tion assume that language users store and use muchlarger fragments of language than the single wordsand rules of combination of traditional linguisticmodels. However, forunconstrained STSGs we need all sums and products in (8).will, in general, be computationally extremely ex-pensive to calculate E[f(t)] . In this paper I explore an alter-native formal and computational approach, wheremulti-word constructions have no special status,but emerge in a general procedure to find the beststatistical grammar to describe a training corpus.Crucially, I use a formalism known as StochasticTree Substitution Grammars The crucial difference is in the estimationprocedure for choosing the weights of the STSGbased on observed frequencies in a corpus.
W06-2906@@Successful resolution and generation of definiteanaphora requires knowledge of hypernym and hy-ponym relationships. A ma-chine learning approach to anaphoric reference. In Proceedings of the 37th AnnualMeeting of the Association for Computational Linguis-tics, pages 5764.S.
W06-2907@@Lexical substitution the task of replacing a wordwith another one that conveys the same meaning -is a prominent task in many Natural Language Pro-cessing (NLP) applications. A lower bound of 0. The targettraining value for the output of the system is 1 if v isindeed in C and -1 otherwise.
W06-2909@@For this purpose, we need to extract fea-tures from the sentences syntactic parse tree thatencodes the target semantic structure. , f|F|}, the indicator function Ii(n)is equal to 1 if the target fi is rooted atnode n and equal to 0 otherwise. P A:extract the feature representation set, Fp,a;if the subtree rooted in a covers exactly thewords of one argument of p, put Fp,a in the T+set (positive examples), otherwise put it in theT set (negative examples).The outputs of the above algorithm are the T+ andT These sets can be directly used to train aboundary classifier (eg an SVM).
W06-2910@@Depending on the types of verb classesto be induced, the automatic approaches vary theirchoice of verbs and classification/clustering algo-rithm. A GeneralFeature Space for Automatic Verb Classification. 3frames grammar relationsf-pp f-pp-pref n na na NP PP NP&PP ADVAssoc 37.
W06-2911@@Word sense disambiguation (WSD) is the task ofassigning pre-defined senses to words occurring insome context. Nonlinear component analysis as a kerneleigenvalue problem. In Proceed-ings of Senseval-3 Workshop.Upali S. Kohomban and Wee Sun Lee.
W06-2912@@How can we learn syntactic structure from unlabeleddata in an unsupervised way The importance ofunsupervised parsing is nowadays widely acknow-ledged. As this is the largest PCFGwe have ever attempted to parse with, it wasprohibitive to estimate the most probable parse treefrom 100 most probable derivations using Viterbi n-best. The probability of a derivationt1...
W06-2914@@and discourse topics (of more extendedstretches of discourse). LIBSVM:a library for support vector machines. 1 and fi,j is the weightedfrequency (determined in the previous step) ofthe j-th word from the vocabulary in the i-th ut-terance.
W06-2915@@In this paper we investigate a new problem of au-tomatically identifying the perspective from whicha document is written. The superscript t indicates that a sampleis from the t-th iteration. However, there is a key difference.
W06-2917@@For some years, a particular set of examples hasbeen used to provide support for nativist theoriesof first language acquisition (FLA). Clearly the algorithm has a strongbias. They claim that local statis-tics, effectively n-grams, can be sufficient to indi-cate to the learner which alternative should be pre-ferred.
W06-2918@@In these cases, a gazetteer or dic-tionary of possible entity identifiers is often useful.Such identifiers could be names of people, places,companies or other organisations. At the maximum likelihood so-lution the model satisfies a set of feature constraints,whereby the expected count of each feature underthe model is equal to its empirical count on the train-ing data:E p o  s  fk  Ep  s  o  fk   0 	 kIn general this cannot be solved for the k in closedform, so numerical optimisation must be used. The functions fk are feature functions rep-resenting the occurrence of different events in thesequences s and o.The parameters k can be estimated by maximis-ing the conditional log-likelihood of a set of labelledtraining sequences.
W06-2919@@Partial entity lists and massive amounts of unla-beled data are becoming available with the growthof the Web as well as the increased availability ofspecialized corpora and entity lists. Use induced patterns P to extract more entitiesE If needed, add high scoring entities in E to Eand return to step 2. Trigger words mark thebeginning of a pattern.
W06-2920@@Thisshared task on full (dependency) parsing is the log-ical next step. Parsing English with a linkgrammar. Experiments with a multilanguage non-projective dependency parser.E.
W06-2923@@Training treebanks and test data were provided for  13 different  languages: Arabic (Smr A number of these treebanks were not originally annotated in dependency style, but transformed from constituent tree style for the task, and all differ widely in terms of tag granularity (21-302  part-of-speech tags, 7-82 function labels). In the long term, a notational harmonization of the treebanks  should  allow  the  learner  to  be  seeded with existing hand-written dependency rules.References Afonso, S., E. Bick, R. Haber and D. Santos. (c)tica: A treebank of Portuguese.
W06-2925@@The parser uses a learning functionthat scores all possible labeled dependencies. In a given span, from word s to word e, itcompletes two partial dependency trees that coverall words within the span: one rooted at s and theother rooted at e. This is done in two steps. to a small numberof intervals.
W06-2927@@A dependency attachment matrix is con-structed, in which each element corresponds to a pair of tokens. We inves-tigated several preprocessing methods on a Chi-nese Treebank. In our method, the parser considers the depend-ency attachment of two nodes (n,t).
W06-2928@@The second stage is amaximum entropy classifier that labels the directeddependencies. The score d(i, j) of each dependencylink (i,j) is further decomposed as the weighted sumof its features f(i, j).To set w, we trained twenty averaged perceptronson different shuffles of the training data, using thedevelopment test set to determine when the percep-trons had converged. Ofcourse, this is a linguistic oversimplification.
W06-2929@@First, a parser produces a list of the mostlikely n parse trees under a generative, probabilisticmodel (usually some flavor of PCFG). Eisner and N. A. Smith. A dependency tree yis defined by three functions: yleft and yright (both{0, 1, 2, ..., n}  D, which labels the relation-ship between word i and its parent from label set D.In this work, the graph is constrained to be a pro-jective tree rooted at $: each word except $ has a sin-gle parent, and there are no cycles or crossing depen-dencies.
W06-2932@@Often in language processing we require a deep syn-tactic representation of a sentence in order to assistfurther processing. N/P: Allow non-projective/Force pro-jective, S/A: Sequential labeling/Atomic labeling,M/B: Include morphology features/No morphologyfeatures.assignment of edge labels instead of individual as-signment, and a rich feature set that incorporatesmorphological properties when available. For instance, if weconsider a head xi with dependents xj1 , .
W06-2933@@The CoNLL-X shared task consists in parsing textsin multiple languages using a single dependencyparser that has the capacity to learn from treebankdata. : a treebank for Portuguese. To avoidtoo small training sets, we pool together categoriesthat have a frequency below a certain threshold t. .
W06-2934@@Intheir original framework, features are only definedover single attachment decisions. A set of features per depen-dency is extracted using this information. of the 11thAnnual Meeting of the EACL.R.
W06-2935@@In a series of post-submission experi-ments, we investigated how much the parse resultscan help a machine learner. A grammar with this configuration was usedto produce the results submitted (cf. To findthe position of trace and antecedent we assume threeconstraints: The antecedent should c-command itstrace.
W06-2936@@Recently, we have seen dependency parsing growmore popular. The hardware used is anIntel Pentinum D at 3.0 Ghz with 4 GB of memory,and the software was written in C++. : a treebank for Portuguese.
W06-2937@@ The target of dependency parsing is to automati-cally recognize the head-modifier relationships between words in natural language sentences. Yu C. Wu, Chia H. Chang, and Yue S. Lee. A future work on the deterministic parsing strategy is to convert the existing model toward N-best pars-ing.References  S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
W06-3001@@Often users want toresearch about a particular topic or event or solvea specific task. 6%) open a new topic. Aswe have observed, in these cases the interveningsegment/s shift the focus of attention to an entity(maybe provided in some previous answer) closelyrelated to the one in focus of attention in the pre-ceding segment.
W06-3002@@Open-domain question answering (QA) technolo-gies allow users to ask a question using natural lan-guage and obtain the answer itself rather than a listof documents that contain the answer (Voorhees etal. Philip R. Cohen, Jerry Morgan and Martha E.Pollack eds. It is, however, a practical compromisefor an objective evaluation.
W06-3005@@Question Answering (QA) is an interactivehuman-machine process that aims to respondto users natural language questions with exactanswers rather than a list of documents. For instance, the AT&T AskAllier agent (http://www.allieatt.com/) isable to answer questions about the AT&T plansand services; and the Ikea Just Ask Anna! as beginning a new topic.
W06-3102@@Statis-tical machine translation views the translation pro-cess as a noisy-channel signal recovery process inwhich one tries to recover the input signal )In the context of the agglutinative languages sim-ilar to Turkish (in at least morphological aspects) ,there has been some recent work on translating fromand to Finnish with the significant amount of datain the Europarl corpus. AcknowledgementsThis work was supported by T UBITAK (TurkishScientific and Technological Research Foundation)project 105E020 Building a Statistical MachineTranslation for Turkish and English.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, John D. Lafferty, and Robert L. Mercer. birkonu ol+acak+trSelective Morpheme Concatenation: ulus+lararas terorizm de onem+li birsorun ol+ma+ya devam et+mek+te+dir .Input: the initiation of negotiations will represent thebeginning of a next phase in the process of accessionBaseline: muzakere+ler+in gor+us+me+ler yap+l+acak birder+ken asama+nn hasar+ ,katlm surec+i+nin bir sonra+ki asama+s+nn baslangc++ntemsil ed+ecek+tirgenerate possible legitimate surface words by tak-ing into account morphotactic constraints and mor-phographemic constraints, possibly (and ambigu-ously) filling in any morphemes missing in the trans-lation but actually required by the morphotacticparadigm.
W06-3103@@Arabic is a highly inflected language compared toEnglish which has very little morphology. In E. Hinrichs and D. Roth, editors, Proc.of the 41st Annual Meeting of the Association for Com-putational Linguistics.Y. We add the prefix s ( S: the starting point of the automaton.
W06-3104@@Sloppy Syntactic AlignmentThis paper proposes a new type of syntax-basedmodel for machine translation and alignment. may be related in thesource tree as, among other things, (a) parentchild, (b) childparent, (c) identical nodes, (d) siblings, (e) grandparentgrandchild,(f) c-commanderc-commandee, (g) none of the above. Analysischooses a parse T1 given S1.
W06-3105@@This well-known result is unsurprising: re-estimation introduces an element of competition intothe learning process. In translating 1,000 test sentences, for example,no phrase translation with (e|f) less than 105 wasused by the decoder. Our notation matches the literature for phrase-based trans-lation: e is an English word, e is an English phrase, and eI1 is asequence of I English phrases, and e is an English sentence.
W06-3108@@In recent evaluations, phrase-based statistical ma-chine translation systems have achieved good per-formance. A statistical approach to machinetranslation. We use the sourceposition j which is aligned to the last word of thetarget phrase in target position i.
W06-3110@@The (simplified) idea is that the con-fidence measure is used to decide if the machine-generated prediction should be suggested to the hu-man translator or not.There is only few work on how to improvemachine translation performance using confidencemeasures. Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics. ARPA Workshop on Human LanguageTechnology.S.
W06-3112@@They both score candidate translations on the basis of the number of n-grams it shares with one or more reference translations provided. A few examples are given in (1). Automatic Evaluation of MT Quality using N-gram Co-occurrence Statistics.
W06-3113@@In several natural language processing tasks, such asautomatic speech recognition and machine transla-tion, state-of-the-art systems rely on the statisticalapproach.Statistical machine translation (SMT) is basedon parametric models incorporating a large num-ber of observations and probabilities estimated frommonolingual and parallel texts. The conditional distribution iscomputed with the log-linear model:p R are real valued featurefunctions.The log-linear model is used to score translationhypotheses (e,a) built in terms of strings of phrases,which are simple sequences of words. Finally, quantization does not seems to beaffected by data sparseness and behaves similarly ondifferent translation tasks.ReferencesM Cettolo, M. Federico, N. Bertoldi, R. Cattoni, and B.Chen.
W06-3116@@As a mat-ter of fact, deriving such an engine from a bitext con-sists in (more or less) gluing together dedicated soft-ware modules, often freely available. Pharaoh: a Beam Search Decoder forPhrase-Based SMT. pn is the n-gram precision and BP is the brevity penalty usedwhen computing BLEU.in test (1), he stated whether the best translation wassatisfactory while the other was not.
W06-3117@@SITGs can be used to carry out a simulta-neous parsing of both the input string and the outputstring. A systematic comparison ofvarious statistical alignment models. Then, two additional rules of the form ff &  * and  ff -+0 were added.
W06-3118@@Briefly,Portage is a research vehicle and development pro-totype system exploiting the state-of-the-art in sta-tistical machine translation (SMT). A comparison of the en-hanced Good-Turing and deleted estimation methodsfor estimating probabilities of English bigrams. Sadat, G. Foster and R. Kuhn.
W06-3120@@Nowadays most Statistical Machine Translation(SMT) systems use phrases as translation units. A simplex methodfor function minimization. Each featurefunction models the probability that a sentence e inthe target language is a translation of a given sen-tence f in the source language.
W06-3122@@Statistical machine translation (SMT) systems com-bine a number of translation models with one ormore language models. Each 2 picks a preferred hypoth-esis. to cluster different numbers into thesame unigram entry, filtered noisy sentences and wecollected n-gram counts (up to 4-grams).
W06-3123@@Machine translation is a hard problem because ofthe highly complex, irregular and diverse natureof natural languages. Instead, it extracts phrase pairs(f i, ei) in the following manner. A word-to-word model of translationalequivalence.
W06-3124@@The  dependency  treelet  translation  systemdeveloped at MSR is a statistical MT system thattakes  advantage  of  linguistic  tools,  namely  asource language dependency parser,  as well as aword alignment component. DecodingWe use a tree-based decoder, inspired by dynamicprogramming. Furthermore,n-gram  channel  models  may  provide  greaterrobustness.
W06-3125@@Baseline N-gram-based SMT SystemAs already mentioned, the translation model usedhere is based on bilingual n-grams. of theACL Workshop on Building and Using Parallel Texts(ACL05/Wkshp), pages 6772, June.T. on Language Resources and Evaluation,LREC04, May.M.R.
W06-3202@@In language technology applications, unknownwords are a continuous problem. In German, we observe that /R/ ispreferred over /l/ and /v/ over /n/ and /j/. It is a combination of gram-mar 2. .
W06-3206@@The fields of computational phonology and mor-phology were among the earlier fields in compu-tational linguistics to adopt machine learning algo-rithms as a means to automatically construct pro-cessing systems from data. At the sametime, a possible sequence is not necessarily the cor-rect sequence, so this method can be expected to stillmake errors on the identity of labels in the output se-quence.In future work we plan to test a range of n-gramwidths exceeding the current trigrams. Language and Speech, 40:123.E.
W06-3207@@The function of the grammar is to map underlying or lexical forms to valid surface forms. In other words, it defines the conditional probability Pr(Ok | I), shown in the fourth column, of the kth output candidate given the input /I/ under the ranking r. The last column shows the total conditional prob-ability for each candidate after summing across rankings. 52The likelihood of the data, or set of overt surface forms, PH(O | M) depends on the parameter set-tings, the probability distributions over rankings and underlying forms, under the hypothesis H.  It is also conditional on M, the set of observed mor-phemes, which are annotated in the data provided to the algorithm.
W06-3307@@Information Extraction (IE) is a natural languageprocessing task in which text documents are ana-lyzed with the aim of finding mentions of relevantentities and important relationships between them.In many cases, the subtask of relation extraction re-duces to deciding whether a sentence asserts a par-ticular relationship between two entities, which isstill a difficult, unsolved problem. Us-ing avg instead would answer a different question -whether R(p1; p2) is true in most of the sentencescontaining p1and p2. To compute the degree of inter-action N : the total number of protein pairs co-occurring in the same sentence in the corpus.
W06-3308@@ The volume of biomedical literature available has experienced unprecedented growth in recent years. Take the sentence IL4 and IL13 recep-tors activate STAT6, STAT3 and STAT5 proteins in the human B cells, The NE the human B cells is found in two constituents (the                                                           1 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/ genia-ontology.html  60human B cells and in the human B cells) Yet only in the human B cells is an AM-LOC because here human B cells is preceded by the preposition in We believe such templates composed of NEs, real words, and POS tags may be helpful in identifying constitu-ents Template Generation (TG) Our template generation (TG) algorithm extracts general patterns for all argument types using the local alignment algorithm. We adopted a semi-automatic strategy to anno-tate BioProp.
W06-3312@@The biomedical sciences suffer from an overwhelm-ing volume of information that is growing at explo-sive rates. modulation of activity in B cells by human T-cellleukemia virus type I tax gene. was expressed by E. coliwith a C-terminal His tag from the vector pET-29b.
W06-3316@@The number of articles being published in biomedi-cal journals per year is increasing exponentially. CDS is part-of a gene. For linkinganaphors to their antecedents we look at: modan: set of anaphor pre-modifiers moda: set of antecedent pre-modifiers d: distance in sentences from the anaphorThe pseudo-code to find the antecedent for theDDs and PNs is given below: Input: a set A with all the anaphoric expres-sions (DDs and PNs); a set C with all the possi-ble antecedents (all NPs with biotype informa-tion) For each anaphoric expression Ai: Take the closest candidate as antecedent,if 1 and/or 2 are found; if none is found,the DD/PN is treated as non-anaphoric Output: The resolved anaphoric expressions inA linked to their antecedents.As naming conventions usually recommend genenames to be lower-cased and protein names to beupper-cased, our matching among heads and modi-fiers is case-insensitive, allowing, for example, mslgene to be related to MSL protein due to theircommon modifiers.Antecedent 1, if found, is considered coreferentto Ai, and antecedent 2, associative.
W06-3318@@ Named Entity Recognition (NER) is a key task in biomedical text mining, as biomedical named entities usually represent biomedical concepts of research interest (eg, protein/gene/virus, etc). We run 5-fold cross-validation, and measure performance (P/R/F) of exact match, left/right boundary match w.r.t. We therefore treat it as a binary-class problem, using one-vs-rest scheme.
W06-3328@@The biomedical domain is of great interest to in-formation extraction, due to the explosion in theamount of available information. In Pro-ceedings of the 11th Annual Meeting of the EuropeanChapter of the Association for Computational Linguis-tics.S. of Computer Science, University ofGlasgow.A.
W06-3403@@ While document similarity has been a concern in computational linguistics for some time, less atten-tion has been paid to change in similarity across time. Computational Measures of Language Similarity The unit of analysis in online communities is the (e-mail or chat) message. Conversation and Community: Chat in a Virtual World.
W06-3404@@A multitude of meetings are organized every dayaround the world to discuss and exchange impor-tant information, to make decisions and to collab-oratively solve problems. At each stagewe consider all participants not yet assigned roles,and pick that participantrole pair, say (p, r), thathas the highest probability value among all pairs un-der consideration. Our chosen strategy is tosample the speech output by each participant multi-ple times over the course of the meeting sequence,classify each such sample, and then aggregate theevidence over all the samples to arrive at the overalllikelihood that a participant is playing a certain role.To perform the sampling, we split each meetingin the meeting sequence into a sequence of contigu-ous windows each n seconds long, and then computeone set of features from each participants speechduring each window.
W06-3505@@To let a computer system understand natural lan-guage one needs knowledge about objects and theirrelations in the real world. In H.C. Bunt and W.J. The mapping of a linguistic term to an ontolog-ical concept..
W06-3506@@style questions, questionanswering systems must rely on inferential mecha-nisms. Our work is a first step to remedy thissituation. Foundations of CognitiveGrammar I: Theoretical Prerequisites.
W06-3601@@The relationshipamong these concepts is illustrated in Fig. |c| (9)where Pr(c) is the language model and e |c| is thelength penalty term based on |c|, the length of thetranslation. We use a standard trigram modelfor Pr(c).
W06-3602@@Here, the decoding algorithms are de-scribed as shortest path finding algorithms in regularlystructured search graphs or search grids. i b is theinitial decoder state, where no source position is covered:Om . For such a weighted graph,the shortest path from a single source can be computedin efT is the number of ver-texes and  number of edges in the graph.
W06-3603@@Discriminative machine learning methods have im-proved accuracy on many NLP tasks, includingPOS-tagging, shallow parsing, relation extraction,and machine translation. (Samplingwill be explained in 3.. Until then, assume thatthe sample S is the entire training set I.) An item is a (span, label) pair.
W06-3606@@Theyprovide a method to instantiate Markov networksfrom a set of constants and first-order formulae.While MLNs have the power to specify Markovnetworks with complex, finely-tuned dependencies,the difficulty of instantiating these networks growswith the complexity of the formulae. The procedure re-turns the predicate F i such that setting Fi to Trueresults in the greatest increase in the scoring functionS(yt, x).Let (ci . In line 3, the procedure FindMostLike-lyPredicate iterates through each query predicatein F t, setting each to true in turn and calculating itsimpact on the scoring function.
W06-3607@@ In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. Ranking with a p-Norm Push. The margin also determines the number of hypotheses (N) that we will store.
W06-3701@@ Increasing globalization and immigration have led to growing demands on US institutions for health-care and government services in languages other than English. Clicking once will select and highlight a Shortcut. We will consider the search facility first, as shown in Fig-ure 2.
W06-3702@@ Medical applications have emerged as one of the most promising application areas for spoken lan-guage translation, but there is still little agreement about the question of architectures. I would use this system again in a simi-lar situation. I was often unable to ask the questions I wanted.
W06-3706@@ Spoken Translation, Inc. (STI) of Berkeley, CA has developed a commercial system for interactive speech-to-speech machine translation designed for both high accuracy and broad linguistic and topical coverage. Clicking once will select and highlight a Shortcut. T-hus, a seamless transition is provided between Shortcuts and full translation.
W06-3708@@ Speech translation technology has the potential to give nurses and other clinicians immediate access to consistent, easy-to-use, and accurate medical interpretation for routine patient encounters. Most other systems depend on a moderate amount of domain-specific data being available. Sehdas speech translation system, S-MINDS, focuses on translating in such situations with extremely high accuracy.
W06-3711@@ Automatic speech-to-speech (S2S) translation breaks down com-munication barriers between people who do not share a common language and hence enable instant oral cross-lingual communica-tion for many critical applications such as emergency medical care. [6] R. Sarikaya, et al  Rapid Language Model Development Using External Resources for New Spoken Dialog Domains, [7] L. Gu et al Concept-based Speech-to-Speech Translation using Maximum Entropy Models for Statistical Natural Concept Genera-tion, Speech and Audio Processing, vol. The context-dependent states are generated using a decision-tree classifier.
W06-3804@@ In many information processing tasks one is inter-ested in measuring how much a text or passage is about a certain entity. rm, iifloorn22*100 where n is the number of items in the 2 rankings and rx,i and rm,i denote the position of the ith item in Rx and Rm. When computing a reference relation with an entity term, we only consider biographical terms found as described in (2. entity_score is zero when E does not occur in D. Otherwise we compute the entity score as follows.
W06-3810@@Document indexing and representation of term-document relations are very important issues fordocument clustering and retrieval. In our experiments we used a binary ad-jacency matrix W . For the vocabulary in V , obtain a matrix ofpair-wise similarities, S, using the corpus W2.
W06-3811@@Thesaurus are an important resource in many naturallanguage processing tasks. the Markovian matrix of G,such that[G This is thusthe probability that a random particle leaving node rwill be in node s after i time steps. For instance, the A.
W06-3907@@[9] Lauri Karttunen, 29, wrote:It is evident that logical relations between main sentences and their comple-ments are of great significance in any system of automatic data processingthat depends on natural language. (2) a. Ed forgot that the door was closed.b. One important thingto note is that the assertion instantiable(leave ev57) in not58 is lifted asuninstantiable(leave ev57) to the top level context t, thus capturing theintuitive meaning that the event type of Ed leaving Paris was not instantiated.
W06-3909@@ Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources. Similarly, PR04 has a relative recall of 0. To address this, we multiply pmi(i, p) with the discounting factor suggested in [16].
W06-3913@@Time structures our world, and the questions we ask reflect that. Otherwise, a new event and relation are added to the KB. We also mine the web to build fuzzy sets for vagueevents and to find events for which we can only get qualitative information.Acknowledgements The first author was supported by the NetherlandsOrganization for Scientific Research (NWO), under project number 612.066.02.The second author was supported by a PhD grant from the Research Foun-dation  [8] Schlobach, S., D. Ahn, M. de Rijke and V. Jijkoun, Data-driven type checkingin open domain question answering, Journal of Applied Logic (to appear).
W07-0103@@Metaphoric usages enhance theattributes of the source concept by comparing itwith the attributes of the target concept. Metonymyresolution as a classification task. V/A-N matrix stands forVerb/Adjective-Noun matrix, which is a two dimen-sional matrix with verbs or adjectives along one di-mension, and nouns along the other.
W07-0201@@Wikipedia1 is a free multi-lingual online encyclo-pedia that is constructed in a collaborative effortof voluntary contributors and still grows exponen-tially. E is a set of unordered pairs ofdistinct vertices, called edges. In a fullyconnected graph, the cluster coefficient is 1.
W07-0203@@ Usually, automatic summarization involves produc-ing a condensed version of a source text through selecting or generalizing its relevant content. This comprises, thus, a rewriting task. o de um Thesaurus Ele-trnico para o Portugus do Brasil.
W07-0204@@These algorithms essentially decide the weights of graph nodes based on global topological information. Third, the system gives similar performance when e is set 30greater than 10. t controls the type of text unit that represents  nodes.
W07-0301@@In the troubleshooting domain, a spoken dialog sys-tem (SDS) helps a user to restore a malfunction-ing product such as a DSL connection to a work-ing state. The goal of thecontrol algorithm is to choose actions that maximizethe expected return E[V ] given b and the POMDPparameters P, and the process of searching for sucha control algorithm is called optimization. The environmentthen generates an observation o The process of maintaining b at eachtime step is called belief monitoring.
W07-0304@@What differentiates each generation is not only the increase of complexity, but also the dif-ferent architectures used. Lewis, C., Di Fabbrizio, G., Prompt Selection with Rein-forcement Learning in an AT&T Call Routing Applica-tion, Proc. That would also result in a limited grammar or vo-cabulary at each turn.
W07-0401@@In machine translation, reordering is one of the ma-jor problems, since different languages have differ-ent word order requirements. A statistical approach to machine translation. Lattice WeightingAll reorderings of an input sentence S are com-pressed and stored in a lattice.
W07-0403@@Statistical machine translation benefits greatly fromconsidering more than one word at a time. M. Vilar and E. Vidal. ITG Viterbi and inside-outside algorithmshave polynomial complexity, but that polynomial isO(n6), where n is the length of the longer sentencein the pair.
W07-0404@@A number of recent syntax-based approaches tostatistical machine translation make use of Syn-chronous Context Free Grammar (SCFG) as the un-derlying model of translational equivalence. A hierarchical phrase-based modelfor statistical machine translation. The algorithm processes each rulein the input grammar independently, and determineswhether the rule can be factored into smaller SCFGrules by analyzing the rules permutation pi.As an example, given the input rule:[X  E(2)G(4)D(1)F (3) ]where X1, X2 and X3 are new nonterminals used torepresent the intermediate states in which the syn-chronous nodes are combined.
W07-0405@@In thisframework, decoding can be thought of as pars-ing problems, whose complexity is in general expo-nential in the number of nonterminals on the righthand side of a grammar rule. This definitionextends to a binarization b of an SCFG rule of rankn, where arities s(b) and t(b) are defined as themaximum source and target arities over all virtualnonterminals in b, respectively. A hierarchical phrase-based modelfor statistical machine translation.
W07-0412@@In this paper, we provide a conceptual basis forthinking of machine translation in terms of syn-chronous grammars in general, and probabilisticsynchronous tree-adjoining grammars in particular.The basis is conceptual in that the arguments arebased on generalizations about the translation re-lation at a conceptual level, and not on empiricalresults at an engineering level. Using lexicalized tags for machine translation.In Proceedings of the 13th International Conferenceon Computational Linguistics.Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-cent J. Della Pietra, Frederick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin. An example has alreadybeen seen in the example I like to take several longbubble baths every day.
W07-0413@@The problem of machine translation can be viewedas consisting of two subproblems: (a) lexical se-lection, where appropriate target language lexi-cal items are chosen for each source languagelexical item and (b) lexical reordering, wherethe chosen target language lexical items are rear-ranged to produce a meaningful target languagestring. Ideally, P (T |S) should be estimated directly to maximize theconditional likelihood on the training data (dis-criminant model). A syntax-based sta-tistical translation model.
W07-0612@@People frequently encounter noun-noun compoundssuch as MEMORY STICK and AUCTION POLITICSin everyday discourse. N. Kim and T. Baldwin. In the context of ourexperiment, the extensional, relational informationabout beetle and plant exemplars participants held inmemory is revealed in how they rated relational like-lihood during the exemplar transfer stage of the ex-1This is not significant if Bonferroni correction is used tocontrol the familywise Type I error rate amongst the multiplecomparisonsperiment.
W07-0702@@In large-scale machine translation evaluations,phrase-based models generally outperform syntax-based models1. influenceis also within a local range. An ensemble of log-linearmodels was combined using a multiplicative con-stant  which we train manually using held out data.t This is the first work sug-gesting the application of LOPs to decoding in ma-chine translation.
W07-0704@@Statistical machine translation (SMT) from English-to-Turkish poses a number of difficulties. E: the implementation3 of the acces-sion1 partnership2 will be monitor7+vvn in the framework6 of theassociation4 agreement5 .Note that when the morphemes/tags (starting witha +) are concatenated, we get the word-basedversion of the corpus, since surface words are di-rectly recoverable from the concatenated represen-tation. gives a BLEU scoreof 13.00.
W07-0712@@Nowadays, theavailability of larger and larger text corpora is stress-ing the need for efficient data structures and algo-rithms to estimate, store and access LMs. How does IRSTLM compare with SRILMw.r.t. SQELWorkshop on Multi-Lingual Information Retrieval Dialogs,pages 5559, Pilsen, Czech Republic.E.
W07-0714@@As string based-metrics, they are limited to superficial comparison of word sequences between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references. fluency  accuracy  average  d_50+WN 0. Europarl: A Parallel Corpus for Statistical Machine Translation.
W07-0715@@Estimates of conditional phrase translation probabil-ities provide a major source of translation knowl-edge in phrase-based statistical machine translation(SMT) systems. E step: Compute the expected phrase alignmentcounts according to the model, deriving the se-lection probabilities from the current estimatesof the translation probabilities as described.. M step: Re-estimate the phrase translationprobabilities according to the expected phrasealignment counts as described. Begin with a source sentence a. .
W07-0716@@This has become much more standard thanoptimizing the conditional probability of the train-ing data given the model (ie, a maximum likelihoodcriterion), as was common previously. The problem can thenbe thought of as defining a function g(m,n) whichcomputes the number of sentences in a hypotheti-cal English-to-English parallel corpus wherein thephrases e1 and e1 co-occur. A survey of statistical machine transla-tion.
W07-0717@@Language varies significantly across different gen-res, topics, styles, etc. that is the mostlikely translation of s, using the Viterbi approxima-tion:t In both cases, the forward are not used as features, but onlyas a filter on the set of possible translations: for eachsource phrase s that matches some ngram in s, onlythe 30 top-ranked translations t Mixture-Model AdaptationOur approach to mixture-model adaptation can besummarized by the following general algorithm:1. Given a set of metrics{D1, .
W07-0718@@The goals of this paper aretwofold: First, we evaluate the shared task entriesin order to determine which systems produce trans-lations with the highest quality. We define chanceagreement for fluency and adequacy as 15 , since theyare based on five point scales, and for ranking as 13Evaluation type P (A) P (E) KFluency (absolute) . A parallel corpus for statisticalmachine translation.
W07-0722@@One of the most interest-ing properties of mixture modelling is its capabilityto model multimodal datasets by defining soft parti-tions on these datasets, and learning specific proba-bility distributions for each partition, that better ex-plains the general data generation process. A systematic comparison ofvarious statistical alignment models. This model possesses appealing proper-ties among which are worth mentioning, the simplic-ity of the first-order word alignment distribution thatcan be made independent of absolute positions while177taking advantage of the localization phenomenonof word alignment in European languages, and theefficient and exact computation of the E-step andViterbi alignment by using a dynamic-programmingapproach.
W07-0723@@  One of the stated goals for the shared task of this workshop is to offer newcomers a smooth start with hands-on experience in state-of-the-art statis-tical machine translation methods. Category Example C-correct Regelungsentwurf Draft regulation  Ref: Draft regulation V-variant Schlachthfen Abattoirs  Ref: Slaughter houses  P-partly correct Anpassungsdruck Pressure Ref: Pressure for adaption F-wrong form Lnderberichte Country report  Ref: Country reports W-wrong Erbonkel Uncle dna  Ref: Sugar daddy U-untranslated Schlussentwurf Schlussentwurf  Ref: Final draft  Baseline system  C V P W U F Tot C 36 1 3  3 1 44 V 1 9 2 1 5  18 P   3  2  5 W    1 2  3 U       0 F 1     4 5 Second system Word order varies a great deal between German and English.
W07-0724@@The statistical machine translation (SMT) sys-tem PORTAGE was developed at the NationalResearch Council Canada and has recently beenmade available to Canadian universities andresearch institutions. This approach considers all phrasepairs (s, t) )of all sentence pairs containing (s, t) is deter-mined, as well as the count of all source/targetsentences containing s/t. The others are determined on the levelof words, phrases, and n-grams, and then com-bined into a value for the whole sentence.
W07-0725@@Due to time constraints, we only consideredthe translation between French and English. Architecture of the systemThe goal of statistical machine translation (SMT) isto produce a target sentence e from a source sen-tence f . A neural probabilistic lan-guage model.
W07-0726@@Recent work on statistical machine translation hasled to significant progress in coverage and quality oftranslation technology, but so far, most of this workfocuses on translation into English, where relativelysimple morphological structure and abundance ofmonolingual training data helped to compensate forthe relative lack of linguistic sophistication of theunderlying models. In ANLP, pages 95100.Christopher Hogan and Robert E. Frederking. Europarl: A parallel corpus for sta-tistical machine translation.
W07-0728@@Thestatistical system views the output of the rule-basedsystem as the source language, and reference hu-man translations as the target language. Martin, and A. Tikuisis. These are all direc-tions that we are currently pursuing.AcknowledgementsThis work was done as part of a collaboration withSYSTRAN S.A.
W07-0730@@Modern Statistical Machine Translation (SMT) sys-tems are trained on aligned sentences of bilingualcorpora, typically from one domain. Third, we used a maximum phrase lengthlimit of 10 (rather than 7, as typically done). Search engine statisticsbeyond the n-gram: Application to noun compound bracket-ing.
W07-0732@@ The evolution of SYSTRANs architecture over the last years has been to  the system to enable interaction between the internal systems rules and the external input  Based on this architecture, several directions are explored to introduce the use of  approaches at several levels of the process: use of corpus-based tools to validate and enrich linguistic resources (detection of forbidden se-quences, bilingual terminology extraction), auto-matic recognition of the text domain, use of a corpus-based decision mechanism within  use of word sense disambiguation techniques  and the use of a  language model in the generation phase to select alternative transla-tions, prepositions, and local reordering (adjective positioning). word not translated by SYSTRAN generating a translation with SPE. changes related to grammar o gram_det  change in determiner (on political com-mitmentssur des engagements politiques vs. sur les engagements politiques) o gram_prep  change in preposition (across the Atlantic change in tense (should not be hid-denne devraient often reflecting lack of agreement o gram_other  change in punctuation, case, or numbers  change in word order (long distance)  As is, this basic integration does not meet the acceptance criterion 8 improv.
W07-0734@@Automatic Metrics for MT evaluation have been re-ceiving significant attention in recent years. Multiple judgmentsare combined into a single number by taking theiraverage. determines the maximum penalty(0  FmeanIn all previous versions of Meteor , the values ofthe three parameters mentioned above were set to be: Meteor Implementations forSpanish, French and GermanWe have recently expanded the implementation ofMeteor to support evaluation of translations inSpanish, French and German, in addition to English.Two main language-specific issues required adapta-tion within the metric: (1) language-specific word-matching modules; and (2) language-specific param-eter tuning.
W07-0735@@Statistical phrase-based machine translation (SMT)systems currently achieve top performing results. If more steps contribute to thesame output factor, they have to agree on the out-come, ie partial hypotheses where two decodingsteps produce conflicting values in an output factorare discarded.A MAPPING step from a subset of source fac-tors S  They are included to smooththe relative frequencies used as estimates of thephrase probabilities.A GENERATION step maps a subset of target fac-tors T1 to a disjoint subset of target factors T2,T1,2  Again, the conditional probabil-ity is included in the log-linear combination in bothdirections.In addition to features for decoding steps, we in-clude arbitrary number of target language modelsover subsets of target factors, T  cTin+1) (6)While generation steps are used to enforce verti-cal Thanks to the synchronous-phrases assump-tion, all the decoding steps can be performed duringa preparatory phase. T+C full tags NC NC 13.
W07-0736@@In recent decades, alongside the growing researchon Machine Translation (MT), automatic MT evalu-ation has become a critical problem for MT systemdevelopers, who are interested in quick turnarounddevelopment cycles. 456398, with a standard deviation of 0. The n-gram order isthe default trigram.
W07-0737@@ For translators, not all source sentences are created equal. Our classifier achieves a promising 71. In other words it s an indication of the level of error that is intro-duced in the early parts of the phrase translation.
W07-0808@@In this paper we address one linguistic issue that wasraised while tagging a Hebrew corpus for part ofspeech (POS) and morphological information. Building a tree-bank of modern Hebrew text.Journal Traitement Automatique des Langues (t.a.l.). Rav Milim: A Comprehensive Dictio-nary of Modern Hebrew.
W07-0812@@Base Phrase Chunking (BPC), also known as shal-low syntactic parsing, is the process by which ad-jacent words are grouped together to form non-recursive chunks in a sentence. Any opinions, findings and conclusions or recommenda-tions expressed in this material are those of the author and donot necessarily reflect the views of DARPA.ReferencesErin L. Allwein, Robert E. Schapire, and Yoram Singer. Theyuse a degree 2 polynomial kernel.
W07-0814@@Stemming is the process of reducing morphologi-cal variants of a word into a common form. This result corresponds to a precisionof 95% and a recall of 90%. Instead, for the dictionary lookup to succeed,we first need to add the vowel e at the end of thestem.
W07-1001@@Natural language processing (NLP) techniques areoften applied to electronic health records and otherclinical datasets. Cambridge University Press, Cambridge, UK.E. A model and an hypothesis for languagestructure.
W07-1003@@Today, the Internet is a powerful tool for discov-ering novel information via news feed providers.This is becoming increasingly important for thepublic health domain because it can help to de-tect emerging and re-emerging diseases. A re-examination of textcategorization methods. In Working Notes of the AAAI/ICMLWorkshop on Learning for Text Categorization, pages513.N.
W07-1004@@One of the main challenges in biomedical infor-mation extraction (IE) targeting entity relationshipssuch as protein-protein interactions arises from thecomplexity and variability of the natural languagestatements used to express such relationships. Angelova, K. Bontcheva, R. Mitkov, N. Nicolov,and N. Nikolov, editors, Proceedings of the Interna-tional Conference on Recent Advances in Natural Lan-guage Processing (RANLP 05), Borovets, Bulgaria,pages 8993. In R. Dale, K. F. Wong, J. Su,and O. Y. Kwong, editors, Proceedings of the Sec-ond International Joint Conference on Natural Lan-gage Processing, Jeju Island, Korea, pages 5869.Eugene Charniak.
W07-1008@@Recent efforts in applying natural language pro-cessing to natural science texts have focused onthe recognition of genes and proteins in biomedi-cal text. Subject C was aPhD student with a chemistry degree. TheCharLmHmmChunker already has a mecha-nism for spotting distinctive substrings such asN,N- ConclusionWe have produced annotation guidelines that enablethe annotation of chemicals and related entities inscientific texts in a highly consistent manner.
W07-1009@@Traditionally, named entity recognition (NER) hasfocussed on entities which are continuous, non-nested and non-overlapping. denotes a DNA and the en-tire string CIITA mRNA ), where t represents the text.containing nested structures focus on recognisingthe outermost (non-embedded) entities (eg Kim etal. Language indepen-dent NER using a maximum entropy tagger.
W07-1011@@ Natural language processing (NLP) techniques can extract variables from free-text clinical records important for medical informatics applications per-forming decision support, quality assurance, and biosurveillance [1-6]. Friedman C. A broad-coverage natural language processing system. Hripcsak G, Zhou L, Parsons S, Das AK, Johnson SB.
W07-1012@@With the ever-increasing number of publishedbiomedical research articles and the dependencyof new research and previously published research,medical researchers and practitioners are faced withthe daunting prospect of reading through hundredsor possibly thousands of research articles to sur-vey advances in areas of interest. Proceedings of Em-pirical Methods for Natural Language Processing (EMNLP ).Humphreys B. L. and D. A. TheUMLS project: making the conceptual connection be-tween users and the information they need. % precision and a 98.
W07-1013@@More importantly, theconfidentiality requirements take time and effort toaddress, so it is not surprising that much work inthe biomedical domain has focused on edited jour-nal articles (and the genomics domain) rather thanclinical free text in medical records. Approximately 150 individu-als registered from 22 countries and six continents.Upon completing registration, an automated e-mailwas sent with the location of the training data. Each point represents a system.
W07-1014@@While these methods have shown reasonable performance on indexing and retrieval tasks of biomedical articles, it remains to be determined how they would perform on a different biomedical corpus (eg, clinical text) and on a different task (eg, coding to a different controlled vocabulary). We also added a further post-processing compo-nent to filter our results. The primary measure for the Challenge was defined as the balanced F-score, with a secondary measure being cost-sensitive ac-curacy.
W07-1017@@The modern hospital generates tremendous amountsof data: medical records, lab reports, doctor notes,and numerous other sources of information. A testcorpus contains 976 unlabeled documents. 0%, beating our trainingdata performance and exceeding the average inter-annotator score.
W07-1018@@ As natural language processing (NLP) is increasingly able to support advanced information management techniques for research in medicine and biology, it is being incrementally improved to provide extended coverage and more accurate results. A typical example is (1). McCray AT, Srinivasan S, and Browne AC.
W07-1022@@Base noun phrases (baseNPs), broadly the initialportions of non-recursive noun phrases up to thehead We present a number of experiments ex-ploring different ways of integrating NER into gen-eral purpose linguistic processing. The n-best parses are already scored accord-ing to a probabilistic model trained on general text.Our strategy is to re-score them using the additionalknowledge source of domain specific NER. In this data-set,noun phrases that contain gene names (excludingpost-modifiers) of 3 words or more comprise morethan 40% of the data and exhibit primarily: strings ofpremodifiers tudor mutant females, zygotic Dnop5expression; genitives: Robo s cytoplasmic domain,the rdgB protein s amino terminal 281 residues; co-ordination the copia and mdg-1 elements and par-enthetical apposition the female-specic gene Sexlethal ( Sxl ), and the SuUR (suppressor of under-replication) gene.
W07-1031@@The domain of biomedical text mining has becomeof importance for the natural language processing(NLP) community. For a n-thorder model, y becomes yt, yt1..., ytn. It is alsoworth noting that xt is the feature representation ofthe token in position t, which can include featuresextracted by taking the whole input sequence intoaccount, not just the token in question.
W07-1033@@Difficulty and potential application of biomedicalnamed-entity recognition has attracted many re-searchers of both natural language processing andbioinformatics. GENIA tagger 2, a biomed-ical text processing tool which automatically anno-2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/. A Maximum Entropy Approach215to Natural Language Processing.
W07-1101@@A multi-word expression (henceforth MWE) is usu-ally taken to be any word combination (adjacent orotherwise) that has some feature (syntactic, semanticor purely statistical) that cannot be predicted on thebasis of its component words and/or the combinato-rial processes of the language. For the Longman dictionary, theaccuracy achieved by the syntactic variation mea-sure employing the three best performing features(P, I and D) By contrast, none of the collocationscores perform significantly better than frequency. The item overlap be-tween t-score items and those extracted using thethe best-performing syntactic variation measure is116.
W07-1103@@This paper describes the design and implementa-tion of a lexicon of Dutch multiword expressions(MWEs). to make a blun-der). (7) bok schieten[.VP [.obj1:N1 [.hd:N (1) ]] [.hd:V (2) ]]Expressions of which one part is fixed and theother part is selected from a list of one or more co-occuring lexemes are represented with a so-calledLIST-index in the pattern.
W07-1104@@Some MWEs showproductive morphology and/or syntactic flexibility.Therefore, these two aspects are not sufficient con-ditions to discriminate actual MWEs from productiveexpressions. Dur-ing qualitative evaluation, we found many actualMWEs found by our algorithm, that were not con-sidered correct by our resources (eg [iemand] inde gordijnen jagen to drive s.o. Lawrence Erlbaum Associates, New Jersey.A.
W07-1106@@MostMWEs need to be treated as single units of mean-ing, eg, make a decision roughly means decide.Nonetheless, the components of an MWE can beseparated, making it hard for an NLP system to iden-tify the expression as a whole. Given thisobservation, the overall better-than-baseline perfor-mance of DiI-CF;L-Compmight seem unjustified ata first glance. A case study on extracting PP-verbcollocations.
W07-1202@@Maximising the likelihood involvescalculating feature expectations, which is computa-tionally expensive. A log-linear modelscores the alternative parses. InProceedings of HLT/NAACL-06, New York.James R. Curran, Stephen Clark, and David Vadas.
W07-1203@@In the development of stochastic disambiguationmodules for deep Forsentences out of coverage, it employs the robust-ness techniques (fragment parsing, skimming) 2% on all gram-17matical relations and morphosyntactic features (or72. In Proceedings of the Workshop on Treebanksand Linguistic Theories, Sozopol, Bulgaria.Stephen Clark and James R. Curran. Disambiguation for a LinguisticallyPrecise German Parser.
W07-1204@@In this paper we investigate the use of semantic in-formation in parse selection.Recently, significant improvements have beenmade in combining symbolic and statistical ap-proaches to various natural language processingtasks. Construction of a Japanese semantic lexicon:Lexeed. n-grams of lexical types with or withoutlexicalization.
W07-1206@@A large part of the work done in NLP deals withexploring how different tools and resources can beused to improve performance on a task. (a) only question head verb 0. Thesecond strategy uses key words from the question asqueries, and looks for frequently occuring n-gramsin the snippets returned by the search engine.
W07-1207@@Deep linguistic processing is essential for spokendialogue systems designed to collaborate with us-ers to perform collaborative tasks. First, we assign a different handle toeach term. The first step toward converting LF to MRS is toexpress LF terms as n-ary relationships.
W07-1210@@MRSs canbe converted into RMRSs: RMRS output from shal-lower systems is less fully specified than the out-put from deeper systems, but in principle fully com-patible. This requires specification ofthe syntax-semantics interface so that the checkercan extract the slots from the TFSs and determinethe slot operation(s) corresponding to a rule.Unfortunately, CLF are imprecise about the alge-bra in several respects. up to one argument of the relationHooks also include anchors: {[l, a, i]} is a hook.Instead of the rels list only containing EPs, suchas l1:chase(e,x,y), it contains a mixture of EPsand ARGs, with associated anchors, such asl1:a1:chase(e), l1:a1:ARG1(x), l1:a1:ARG2(y).
W07-1216@@This papers describes the Fips project, whichaims at developing a robust, multilingual deeplinguistic parsing system efficient enough for awide-range of NLP applications. ).The coordination procedure is triggered by thepresence of a conjunction. linguistic parser which as-signs to an input sentence an enriched S-structuretype of representation, along with a predicate-argument representation.
W07-1217@@Linguistically deep processing is of high theoret-ical and application interest because of its abilityto deliver fine-grained accurate analyses of natu-ral language sentences. Then for the graph in 2, the best par-tial parses are {e, g} and {f, g}, both of which have130the path length of 2. [i think you know it it s][court]4).
W07-1218@@Such test suites typically contain hand-constructed examples illustrating the grammaticalphenomena treated by the grammar as well as rep-resentative examples taken from texts from the tar-get domain. Kim and I should handle this ourselves.c. Consider Polish, a free word order language thatnonetheless has prepositions.
W07-1219@@Thus, identical edges withdifferent internal derivation history can be packedinto a single representative for further processing,thereby effectively solving the complexity issue. ), followed by a dis-cussion of the main results (3. C Moore and H. Alshawi.
W07-1306@@Crosslingual and multilingual processing is acquir-ing importance in the computational linguisticscommunity. India as a linguistic area. But first we describe a simple n-grambased measure.
W07-1307@@The most successful techniques distinguishconsonants and vowels, but treat eg all the voweldifferences as the same. For comparison we chose onlyvowels that are pronounced as monophthongs instandard Dutch, in order to exclude interference ofchanging diphthong vowel quality with the results.Nine vowels were used: /i, I, y, Y, E, a, A, O, u/.We calculated the acoustic distances between allvowel pairs as a Euclidean distance of the formantvalues. Vowel normalization a perceptual-acoustic study of Dutch vowels.
W07-1315@@ Words in natural language (NL) have internal structure. For example, the box containing the suffixes e, er, ieron, and i all occurred in the corpus. Sever Institute of Tech-nology, Computer Science Saint Louis, Mis-souri: Washington University, M.S.
W07-1402@@This paper reports on the system we used in the thirdPASCAL challenge on Recognizing Textual Entail-ment. Diploma Thesis (in preparation).Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. III Maxwell, and Mark John-son. 00 items) does not make a qualitativedifference.
W07-1404@@Our system transforms the1www.pascal-network.org/Challenges/RTE3two text snippets into three-layered semantically-rich logic form representations, generates an abun-dant set of lexical, syntactic, semantic, and worldknowledge axioms and, iteratively, searches for aproof for the entailment between the text T and apossibly relaxed version of the hypothesis H . Therefore, wecreated a proofs score adjustment module whichdeducts points for each pair whose H contains atleast one named entity not-derivable from T . For in-stance, the chain oil company#n#1 agent For this years challenge, we changed thesense selection mechanism and we used the clusterof WordNet senses to which the fine-grained senseassigned by the Word Sense Disambiguation sys-tem corresponds.
W07-1406@@More precisely, textual entailment is defined as  H, if the meaning of H, as interpreted in the context of T, can be in-ferred from the meaning of T. entails The name of [PN1]s wife is [PN2], although T contains much more (irrelevant) information. Witten, I. H. and Frank, E. Weka: Practical Machine Learning Tools and Techniques with Java Implemen-tations. 40On the other hand, accuracy varies with T-H pairs from different tasks.
W07-1407@@Textual Entailment is desirable in many natural lan-guage processing areas, such as question answer-ing, information extraction, information retrieval,and multi-document summarization. We are notaware of exactly how the T s were selected by the 3where an operation is an insertion, deletion, or sub-stitution of a single character.
W07-1409@@ The Pascal RTE site defines entailment between two texts T and H as holding "if, typically, a hu-man reading T would infer that H is most likely true" assuming "common human understanding of language as well as common background knowl-edge." 178.T "... people in Saskatchewan succumbed to the storm." "A Framework for Representing Knowledge".
W07-1412@@Theonly problem seems to be the size of the knowledgebases. It heavily depends on the size of C. We de-fine p as the placeholders of, respectively,(T ,H ) As C is combinatorial withrespect to |p| and |p Assigningplaceholders only to chunks helps controlling their73number. This suggeststhat there is a clear difference between the contentof RTE3d and the RTE3 test set.
W07-1414@@A drawback of this approachis that it is hard to cope with discontinuous para-phrases containing one or more gaps. Paraphrase substitutionConceptually, our paraphrase substitution algorithmtakes a straightforward approach. In Proceedings ofthe Second PASCAL Challenges Workshop on Recog-nising Textual Entailment, pages 4449, Venice, Italy.R.
W07-1420@@Textual entailment is the task of taking a pair of pas-sages, referred to as the text and the hypothesis, andlabeling whether or not the hypothesis (H) can befully inferred from the text (T), as is illustrated inPair 1. ScoreBASE if n is odd.Task The task domain used for evaluating entail-ment (ie IE, IR, QA or SUM) was also used as afeature to allow different thresholds among the do-mains. The first step is a tokenization processthat applies to the content words of the text andhypothesis.
W07-1421@@ Many NLP applications need to recognize when the meaning of one text can be expressed by, or inferred from, another text. According to the search results, we have the fol-lowing situations:                                                           3 http://www.acronym-guide.com direct (V) Le_Beau_Serge (N) be (be) Chabrol Le_Beau_Serge (N) Le (U) Beau (U) s be by obj lex-mod node lemma edge label father lemma lex-mod126a) left  left relations similarity This case is described by the following two tem-plates for the hypothesis and the text: relation1 HypothesisVerb relation2 relation1 TextVerb relation3  This is the most frequent case, in which a verb is replaced by one of its synonyms or equivalent ex-pressions  The transformation of the hypothesis tree is done in two steps:  1. If such a relation is found, we save it to an out-put file.
W07-1422@@According to the traditional formal semantics ap-proach, inference is conducted at the logical level.However, practical text understanding systems usu-ally employ shallower lexical and lexical-syntacticrepresentations, augmented with partial semanticannotations. The plaintiffs filed a lawsuit last year in U.S.District Court in Miami. Annotation rules areapplied to t and to each inferred premise prior toany entailment rule application and these featuresmay block inappropriate subsequent rule applica-tions, such as for negated predicates.
W07-1423@@However, instead ofdefining some distance based on edits, we will gen-erate derivations in some calculus that is able totransform dependency parse trees. H which we in-formally introduce next.Suppose we are given n transformationsTF1, . Generating derivationsThe heuristic described in the following generates aderivation that transforms T into H .
W07-1424@@A central tenet of statistical natural language pro-cessing (NLP) is theres no data like more data.One method for generating more data is to restateeach phrase in a corpus, keeping similar seman-tics while changing both the words and the wordsequence. Sleepiness appears a commonplace sign. The MIT Press, May.T.
W07-1425@@Linguistic expressions that convey the same mean-ing are called paraphrases. decrease sharply / show a sharp decreased. Dependency structure: To enable flexiblematching, such as Adv : N : C : V type inputand transformation pattern for N : C : Adv :V type phrases.
W07-1427@@Among the many approaches to textual inference,alignment of dependency graphs has shown utilityin determining entailment without the use of deepunderstanding. Our goal is to build a model that maximizesthe total alignment score of the full datasetD, whichwe take to be the sum of the alignment scores for allindividual text/hypothesis pairs (t, h).Each of the text and hypothesis is a semantic de-pendency graph; n(h) is the set of nodes (words)and e(h) is the set of edges (grammatical relations)in a hypothesis h. An alignment a : n(h) 7 {null} maps each hypothesis word to a text wordor to a null symbol, much like an IBM-style ma-chine translation model. f((hi, hj), (tk, t`)).Recent results in machine learning show the ef-fectiveness of online learning algorithms for struc-ture prediction tasks.
W07-1428@@approaches haveshown much promise in RTE for entailment pairswhere the text and hypothesis remain short, we ex-pect that performance of these types of systems willultimately degrade as longer and more syntacticallycomplex entailment pairs are considered. James Buchanan had a niece.T22. By explicitly enumerating the set of infer-ences that can be drawn from a t or h, our approachis able to reduce the task of RTE to the identificationof the set of commitments that support the inferenceof each corresponding commitment extracted from ahypothesis.
W07-1429@@Sentence Compression can be seen as the removalof redundant words or phrases from an input sen-tence by creating a new sentence in which the gistof the original meaning of the sentence remains un-changed. :::The::::::::president:::::::praised::::IBM::::::::research, during hisspeech.If a global alignment is applied for such a pair, thenweird alignments will be generated, like the one thatis shown in the next representation (we use charactersequences for space convenience and try to preservethe word first letter, from the previous example):D H M S T P R Q I S _ _ __ _ _ _ T P _ Q I S D H SHere it would be more adequate to apply local align-ment and extract all relevant sub-alignments. They were asked to clas-sify a cluster as a wrong cluster column is the baseline, where theSumo-Metric was applied rather than clustering.Columns S-HAC Only the QT achieves betterresults, but if we take the average of the four cluster-ing algorithms it is equal to 0.
W07-1431@@The last five years have seen a surge of interest inthe problem of textual inference, that is, automat-ically determining whether a natural-language hy-pothesis can be inferred from a given premise. Investigating a genericparaphrase-based approach for relation extraction. of the2nd PASCAL RTE Challenge Workshop.Jerry R. Hobbs.
W07-1503@@Manually annotated treebanks have played an im-portant role in the development of robust and ac-curate syntactic analysers. A very frequent issueis the proper handling of index nodes. De Rooy,and M.C.
W07-1504@@An increasing number of systems designed to au-tomatically generate linguistic and multimodal out-put now make use of corpora to help in decision-making (cf. E. Foster and J. Oberlander. In T. Nishida, edi-tor, Engineering Approaches to Conversational Informatics.Wiley.
W07-1506@@In order to use the Ne-gra or Tiger Corpus data to train a PCFG parser, itis necessary to convert the syntactic annotation intocontext-free syntax trees. Statistical Parsing for German: Mod-eling Syntactic Properties and Annotation Differences.Ph.D. Each subset of the originalchildren with a continuous terminal yield becomes apartial node.
W07-1509@@Semi-automated text annotation has been the subjectof several previous studies. Thus Y (x)for x of length n is the set of all sequences of lengthn matching the regular expression (O|(BI)). MethodsThroughout this work, we use a linear sequencemodel.
W07-1515@@These examples,while useful in demonstrating the various aspectsof Appraisal, can only be employed in a qualitativeanalysis and would bring about inconsistenciesif analysed collectively  one can expect thewriting style to depend upon the genre, resulting insignificantly different syntactic constructions andlexical choices.We therefore need to examine Appraisal acrossdocuments in the same genre and investigate pat-terns within that particular register. Note, however, that in this case itdoes not necessarily follow that REC (a w.r.t. Recognizingsubjectivity: a case study in manual tagging.
W07-1516@@ We are operating (as many do) on a fixed budget and need annotated text in the context of a larger project. An alternative approach that eliminates the overhead of entropy computations entirely is to estimate per-sentence uncertainty with 1 ( )P t is the Viterbi (best) tag sequence. References Anderson, B., and Moore, A.
W07-1522@@Coreference resolution and predicate-argumentstructure analysis has recently been a growing fieldof research due to the demands from NLP appli-cation such as information extraction and machinetranslation. These relations arecategorized into five cases: (a) a predicate and itsargument appear in the same phrase, (b) the argu-ment syntactically depends on its predicate or viceversa, (c) the predicate and its argument have anintra-sentential zero-anaphora relation, (d) the pred-icate and its argument have an inter-sentential zero-anaphora relation and (e) the argument does not ex-plicitly appear in the text (ie exophoric). Class-based con-struction of a verb lexicon.
W07-1525@@ Anaphoric annotation is notoriously problematic because of ambiguity and subjectivity issues. A communicative grammar of English. A principled approach In order to develop an annotation scheme which is maximally consistent, we initially identified a set of axiomatic requirements:  CONSTITUENCY o a primary or secondary markable must be an independent syntactic constituent  COMPLETENESS o neither sub-tokens nor non-phrasal nomi-nals are subject to annotation, only syntac-tic words (tokens) and phrases are  CONSISTENCY o corresponding features have to be analyzed in a corresponding way CONSTITUENCY and COMPLETENESS are necessary pre-conditions for an alignment between syntactic and anaphoric annotation, CONSISTENCY implies that annotation principles must be formulated in a way that allows for inter-subjective and cross-linguistically valid annotation decisions.
W07-1528@@Much previous work on linguistic annotation hasnecessarily focused on resource-rich languages, as itis these languages for which we have large corporain need of linguistic annotation. The language is Qanjobal,a Mayan language of Guatemala. At thestart of the annotation process the documentary lin-guist likely has a transcription and a translation, buthe or she may or may not have determined the mor-photactics of the language or even how to identifyword boundaries.So, in order (a) not to commit the annotator to onesingle order of annotation, or the presence of anyparticular annotation level besides the plain text, and(b) to allow annotation to refer to each of the levelsidentified in the BHB model  text, phrase, word,and morpheme , we allow annotation levels to referto each other via unique IDs.Requirements for IGT formats.
W07-1529@@Our hope is that a wide variety of anno-tation will be undertaken on the same corpora, whichwould facilitate:1. ; (3) A compilation of parallel sentences ex-tracted from comparable articles. (c) The use of underlying models for repre-senting annotation content that facilitatemerging, comparison, and analysis.
W07-1604@@% of the stu-dents in the US public school population speak alanguage other than English and have limited En-glish proficiency. Eventhough they reported a precision of 0. This usually took the form of missing com-mas, as in I disagree because from my point of viewthere is no evidence.
W07-1607@@This is because prepositions play a key rolein determining the meaning of a phrase or sentence,and their correct interpretation is crucial for manyNLP applications: AI entities which require spatialawareness, natural language generation (eg for au-tomatic summarisation, QA, MT, to avoid generat-ing sentences such as *I study at England), auto-matic error detection, especially for non-native En-glish speakers. In Nodalida-03, Reykjavik, Iceland.Yoav Freund and Robert E. Schapire. Each vec-tor therefore represents a corpus-seen occurrence ofa preposition and its context.
W07-1801@@Speech recognition grammars are used for guid-ing speech recognizers in many applications. We would like to thank Nuance Com-munications, Inc., OptimSys, s.r.o., and Opera Soft-ware ASA for software licenses and technical sup-port. Polytechnic Press,Polytechnic Institute of Brooklyn, N.Y.John Dowding, Beth A. Hockey, Jean M. Gawron, andChristopher Culy.
W07-1805@@ In command and control (C&C) speech interaction, users interact with a system by speaking com-mands or asking questions. That requires the developer to write better code for proper name normalization (e.g, Rich as well as breaking down the slot value into further components for better partial matching of names. A dynamic vocabulary spoken dialogue interface.
W07-1901@@There are two main implemen-tation techniques that have been used for making thisdecision. E. Foster and J. Oberlander. (2) Here is a family design.
W07-2001@@Since the start of Senseval, the evaluation of WordSense Disambiguation (WSD) as a separate task is amature field, with both lexical-sample and all-wordstasks. Please refer to the system description papersfor a more complete description. This was rather ex-Senseval-2 all wordsprecision recall coverageORGANIZERS 0.
W07-2002@@Word Sense Disambiguation (WSD) is a keyenabling-technology. This proba-bility can be computed counting the times an occur-rence with sense sj has been assigned cluster hi inthe train corpus.The mapping matrix is used to transform anycluster score vector h , hm) returned bythe WSID algorithm into a sense score vector s It suffices to multiply the score vectorby M , ie, s Finally, the result-ing test corpus is evaluated according to the usualprecision and recall measures for supervised wordsense disambiguation systems. However, none of the participating systems la-beled any instance with more than one sense.system All nouns verbsI2R 3.08 3. is the system submitted by the organizers of the task.System R. All Nouns VerbsFSc.
W07-2004@@for the ambiguous Chinese target words are given in the form of their English translations. SRCB-WSD system exploited maximum entropy model as the classifier from OpenNLP2 The fol-lowing features are used in this WSD system:   All the verbs and nouns in the context, that is, the words with tags n, nr, ns, nt, nz, v, vd, vn PoS of the left word and the right word noun phrase, verb phrase, adjective phrase, time phrase, place phrase and quantity phrase. A Chinese Corpus with Word Sense Annota-tion.
W07-2005@@We describe how the task was performedand how it was evaluated (essentially using the samescoring methods as previous Senseval lexical sampletasks). 4; the most frequent sense heuristichas a correlation of -0. Because of this, a simple Perl scriptwas used to score the results, giving precision, recall,and F-score.
W07-2006@@It is commonly thought that one of the major obsta-cles to high-performance Word Sense Disambigua-tion (WSD) is the fine granularity of sense inven-tories. However, given the novelty of the task webelieve that systems can achieve even better perfor-32System A P R F1NUS-PT 100.0 82. Building a sensetagged corpus with open mind word expert.
W07-2007@@Similar campaigns havenot been developed for the resolution of figurativelanguage, such as metaphor, metonymy, idioms andirony. In A. Stefanowitsch, editor, Corporain Cognitive Linguistics. In Workshop on the Usage of WordNet in NaturalLanguage Processing Systems, COLING-ACL 98, 142-148,Montreal, Canada.J.R.
W07-2009@@Word sense disambiguation (WSD) has been de-scribed as a task in need of an application. Lexical substitution as a task forwsd evaluation. T we calculate the mode (mi) iethe most frequent response provided that there is aresponse more frequent than the others.
W07-2010@@Two tracks were organized for this task, eachgathering data from a different corpus. The webas a parallel corpus. For comparison purposes, we alsoshow the corresponding MFS score of each word.Paired t-test on the results of the top two systemsshow no significant difference between them.
W07-2012@@Finding information about people in the World WideWeb is one of the most common activities of Internetusers. A couple of differences make our prob-lem different. In Proceedings ofACL-42, Reference Resolution Workshop.Gideon S. Mann.
W07-2013@@All words can potentially convey affective mean-ing. A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts. Theobtained results were normalized in the 0-100 range.SWAT: SWAT is a supervised system using an u-nigram model trained to annotate emotional content.Synonym expansion on the emotion label words wasalso performed, using the Roget Thesaurus.
W07-2014@@Newspaper texts, narratives and other texts describeevents that occur in time and specify the temporallocation and order of these events. This is a simplified version of theTimeML TLINK tag. Inter-sentence tem-poral relations are discovered by applying severalheuristics and by using statistical data extractedfrom the training corpus.XRCE-T used a rule-based system that relies ona deep syntactic analyzer that was extended to treattemporal expressions.
W07-2015@@For example, in more than ten yearsof manual construction (from version 1.  to 2. In Proceedings of CoNLL,Toulouse, France.E. Build-ing a semantic concordance of english.
W07-2016@@Correctly disambiguating words (WSD), and cor-rectly identifying the semantic relationships be-tween those words (SRL), is an important step forbuilding successful natural language processing ap-plications, such as text summarization, question an-swering, and machine translation. 9 Most Frequent WordNet Sense Baseline N/A 51. In those cases, the PropBank role label wasused instead.We proposed two levels of participation in thistask: i) Closed  the systems could use only the an-notated data provided and nothing else.
W07-2017@@Recent years have witnessed a surge in available re-sources for the Arabic language. The Arabic lan-guage is interesting from a computational linguisticperspective. The problem is not the ex-istence of data, but rather the existence of data an-notated with the relevant level of information that1Author 1 is supported by DARPA contract Contract No.HR0011-06-C-0023.
W07-2018@@The task of labeling frame-evoking words with ap-propriate frames is similar to WSD, while the task ofassigning frame elements is called Semantic RoleLabeling (SRL), and has been the subject of severalshared tasks at ACL and CoNLL. (The preposition in serves only as a marker of theframe element UNDERTAKING. In the Inges-tion frame, I is the INGESTOR and rutabaga fills theINGESTIBLES role.
W07-2024@@ As the sheer amount of web information expands at an ever more rapid pace, the named-entity am-biguity problem becomes more and more serious in many fields, such as information integration, cross-document co-reference, and question an-swering. A. Bagga and B. Baldwin. G. Mann and D. Yarowsky.
W07-2025@@In recent years, the temporal structure of text has be-come a popular area of natural language processingresearch. For Task C, 1indicates that the feature was used only for the firstevent and not the second, and 2 indicates the reverse.Strict RelaxedTask P R F P R FA 0. In the text:(2) Finally today, we [EVENT learned] thatthe space agency has taken a giant leapforward.
W07-2026@@There is a widely held belief in the computationallinguistics field that identifying and defining theroles of predicate arguments, semantic role label-ing (SRL), in a sentence has a lot of potential forand is a significant step towards the improvement ofimportant applications such as document retrieval,machine translation, question answering and infor-mation extraction. To train the multi-class classifier, T+can be reorganized as positive T+argi and negativeTargi examples for each argument i. As a consequence the classifier outputmay generate overlapping arguments.
W07-2033@@In linguistics metonymy means using one term, orone specific sense of a term, to refer to another,related term or sense. Metonymyresolution as a classification task. Weassumed that only PMWs ending with letter s mightbe in plural form, and for themwe compared the websearch result numbers obtained by the Google API.We ran two queries for each PMWs, one for the fullname, and one for the name without its last charac-ter.
W07-2041@@Deciding which particular person a name refers to within a text document depends mainly on the capacity to extract the relevant information out of texts when it is present1. We wrote Perl scripts for identify-ing the e-mail addresses, phone and fax numbers and extract them if they were in the same para-graph with the name of interest. The terms the N-grams contain are crucial.
W07-2044@@A typical word sense disambiguation system istrained on a corpus of manually sense tagged text.Machine learning algorithms are then employed tofind the best sense for a word in a novel contextby generalizing from the training examples. In Proceedings of Senseval-3: The Third Inter-national Workshop on the Evaluation of Systems forthe Semantic Analysis of Text.Eduard H. Hovy, M. Marcus, M. Palmer, S. Pradhan,L. Short paper.Claudia Leacock, Martin Chodorow, and George A.Miller.
W07-2046@@ There has been a growing interest in temporal information extraction in recent years, as more and more operational NLP systems demands dealing with time-related issues in natural languages. This value may be approximate for a number of reasons. For machine learning, the feature set for the three tasks A, B and C we engineered consist of what we call 1) first-class features; 2) derived features; 3) extended features, and 4) merged features.
W07-2047@@The performance of a Word Sense Disambiguation(WSD) system using a finite set of senses dependsgreatly on the definition of the word senses. The submitted answerhas a coverage of 100% and a precision of 81. For each value we created amapping between the feature value and a dimensionin the N-dimensional classification space and we as-signed the number 1.0 to that dimension if the fea-ture had the corresponding value or 0.0 otherwise.We first performed experiments with our existingset of features used at Senseval 3 All Words task.
W07-2048@@Gram-matical relations may be covered by many defini-tions but it is probably easier to use them as an exten-sion of dependency grammars, where relations takethe form of arc labels. The third column showswhether named entities were used (Y) or not (N).Interestingly, the scores are higher for the seman-tic dependency graphs than for flat labels, while thetwo other teams generally had higher scores for flatlabels. Malt-Parser: A data-driven parser generator for dependencyparsing.
W07-2049@@A key contribution ofthis research is that we examine the compatibility ofnoun compound (NC) interpretation methods overthe extended task of nominal classification, to gainempirical insight into the relative complexity of thetwo tasks.The goal of the nominal classification task is toidentify the compatibility of a given semantic re-lation with each of a set of test nominal pairs,eg between climate and forest in the fragment theclimate in the forest with respect to the CONTENT-CONTAINER relation. Our results compare favourablywith the established baselines, and demonstrate thatNC interpretation methods are compatible with themore general task of nominal classification.I T tagged accumulated untaggedi1 . Whencar is replaced by a similar word, the new nouncompound(s) (ie automobile/vehicle/truck factory)share the same SR as the original car factory.
W07-2054@@In the coarse-grained English all-words task,systems have to perform word sense disambiguation(WSD) of all content words (noun, adjective, verb,and adverb) occurring in five documents, using acoarse-grained version of the WordNet sense inven-tory. We obtained a scoreof 0. Forlocal collocations, we use 11 features: C1,1, C1,1,C2,2, C2,2, C2,1, C1,1, C1,2, C3,1, C2,1,C1,2, and C1,3, where Ci,j refers to the orderedsequence of tokens in the local context of an am-biguous word w. Offsets i and j denote the startingand ending position (relative to w) of the sequence,where a negative (positive) offset refers to a tokento its left (right).
W07-2057@@Accurate  word  sense  disambiguation  (WSD)  can support  many  natural  language  processing  and knowledge management  tasks. AcknowledgementsWe would like to thank Upali  S. Kohomban and Wee Sun Lee for  providing us with their  SENSE-VAL-3 English all-words task results file for Simil-Prime. FeaturesWe use a rich set of features to predict individual word senses.
W07-2063@@Cross-document coreference resolution is the task ofidentifying if two mentions of the same (or similar)name in different sources refer to the same individ-ual. System ConfigurationsFour system configurations were prepared for Se-mEval: System I: vector representations were createdfor full documents. GATE: A Framework and GraphicalDevelopment Environment for Robust NLP Tools andApplications.
W07-2069@@Personal names are often submitted to search en-gines as query keywords, as described in a report1indicating that about 10% of the English queriesfrom the search engine ALLTheWeb2 contain per-sonal names. S. Mann and David Yarowsky. #"T  between the centroid vector of clusterT and the feature vector    of search-result Webpage  is defined as follows:flffi!
W07-2074@@We applied a combination of differentk-Nearest Neighbor (k-NN) classifiers. In Proceedingsof the Conference on Recent Advances on Natural Lan-guage Processing (RANLP05), Borovets, Bulgaria.E. Proceedings of the SecondConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 40-47,Pittsburgh, PA, USA.T.
W07-2075@@This paper describes a graph-based unsupervisedsystem for induction and classification. Associationfor Computational Linguistics, June.E. Authoritative sources in a hyperlinkedenvironment.
W07-2077@@In Semantic Role Labeling (SRL) the goal is to iden-tify word sequences or arguments accompanying thepredicate and assign them labels depending on theirsemantic relation. Given a predicate and its Verbnet class, onlysome thematic roles are allowed. If there is a R-X argument (reference), thenthere has to be a X argument before (refer-enced)..
W07-2079@@ Name queries account for a substantial part of Web queries in commercial search engines. From all the bags of terms, we finally selected to compound the contexts b, c, d, e, f, g and j as in the training set they contributed to obtain the best re-sult. References A. Bagga and B. Baldwin.
W07-2081@@This system is an adaptation of anexisting system for deducing the correct semanticrelation between the pair of words in a noun-nouncompound. N. Kim and T. Baldwin. WordNet: A lexical database.
W07-2083@@The  SemEval  task  for  classifying  inter-noun semantic  relations  employs  seven  semantic relations  that  are  not  exhaustive:  Cause-Effect, Instrument-Agency,  Product-Producer  Origin-Entity,  Theme-Tool,  Part-Whole  and  Content-Container. The  most  frequent  syntactic  patterns  that immediately precede N1 in a corpus2. Set 2 is the subset obtained with the  top  n  features  as  ranked  by the  InfoGainAt-trEval filter  (n is  determined using 10-fold cross validation on the training data).
W07-2085@@The dataset provided consists of adefinition file and 140 training and about 70 testsentences for each of the seven relations consid-ered: Cause-Effect, Instrument-Agency, Product-Producer, Origin-Entity, Theme-Tool, Part-Whole,and Content-Container. If there is a direct match, then theweight is set to 1. In the Proceedings of the HLT/NAACL Workshopon Computational Lexical Semantics.R.
W07-2090@@The task of word sense disambiguation consists ofassigning the most appropriate meaning to a poly-semous word within a given context. Wordnet: A lexical database. In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing.M.
W07-2094@@Objectives The detection of emotional connotations in texts is a recent task in computational linguistics. WordNet: A lexical database. SentiWordNet assigns to each synset of WordNet three sentiment scores 4 : positivity, negativity, objectivity, the sum of which always equals 1.0.
W07-2097@@and Thethermometer hit 100 degrees, in the first case thesport domain helps in determining the right sense for1http://www.senseval.orghit, whereas in the latter the disambiguation is car-ried out mostly depending on the fact that the subjectof the sentence is an object.The combination of distinct methods representsitself a major problem. Then the values are multipliedin order to obtain the sense with a maximumprobability. , Rm be the fuzzy prefer-ence relations of m experts over n alternativesx1, x2, .
W07-2098@@The Sheffield team were involved in TempEval asco-proposers/co-organisers of the task. 40Task A Task B Task CFS FR FS FR FS FRUSFD 0. Witten and E. Frank, editors.
W07-2100@@The unifying characteristic of these sys-tems is that they use the same measure of substi-tutability for a given word and a surrounding con-text to perform the tasks. We calculated a baselineof 0. To formfeatures, the system uses the supertags obtainedfrom the parser as binary features, with a slightsimplification: by removing distinctions betweenthe argument types of the main S component,generalisation is facilitated among instances ofverbs which differ slightly on a local level butcombine with other parts of the sentence similarly.
W07-2101@@Based on this observation, we have designedand implemented a semantic architecture that wasused in both tasks. Metonymyresolution as a classification task. As can be observed, the readings457Base type Reading P R F BALocations literal 87.
W07-2102@@Inframe semantics, the meaning of words or word ex-pressions, also called target words (TW), comprisesaspects of conceptual structures, or frames, that de-scribe specific situations. 1% accuracy com-pared to a baseline of 60. We showed that a pipeline architecture of theSVM and ME classifiers as well as an adequate se-lection of the classification models can improve theperformance measures of each sub-task.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.
W07-2103@@Classification of semantic relations is important toNLP as it would benefit many NLP applications,such as machine translation and information re-trieval.Researchers have already proposed variousschemes. A book isalso smaller than its container, library. First, we used Google to conduct Websearches using queries such as book (*cm x*cm)and library (*m x*m).
W07-2108@@TempEval comprises novel tasks concerned with theidentification of temporal relations between eventsand temporal expressions (TEs). In the caseof task A the most frequent temporal relation presentin the training data is OVERLAP, in the case of taskB BEFORE and for task C OVERLAP.TASK A STRICT SCORE RELAXED SCOREP R F P R FWVALI 0. The underlying hypothesisis that the clause binding elements and the tensesof the two VPs provide a natural way to establishtemporal relations between two syntactically relatedclauses.
W07-2202@@Domain portability is an important aspect of the ap-plicability of NLP tools to practical tasks. B. Clegg and A. Shepherd. Clark and J. R. Curran.
W07-2204@@The highest rankedparse trees are added to the training set of the parserand the parser is retrained. They report a per-formance boost of 4. %of the 6 million BNC sentences obtained a parse,with an average parsing speed of 1. s per sentence.A gold standard set of 1,000 BNC sentences wasconstructed by one annotator by correcting the out-put of the first stage of Charniak and Johnsonsreranking parser.
W07-2208@@They are preferredbecause they give precise and in-depth analysesfor explaining linguistic phenomena, such as pas-sivization, control verbs and relative clauses. A comparison of algorithms formaximum entropy parameter estimation. Once the parser acquires thesupertaggers outputs, the n-gram information nevergoes in and out of the kernel.
W07-2216@@Dependency representations of natural language area simple yet flexible mechanism for encoding wordsand their syntactic dependencies through directedgraphs. The 3DM problem asks if there is a setS The set E contains mul-tiple edges between ever pair of nodes, each edgetaking on a label representing a single element ofthe set A  S. All other weights are set to zero.We show below that there exists a bijection be-tween the set of valid 3DMs for S and the set of non-zero weighted dependency graphs in T (G). ETkhiwkijwhere khiwkij is the weight of including both edges(h, i)k and (i, j)kin the dependency graph.
W07-2217@@A dependency tree represents a sentence as a labeleddirected graph encoding syntactic and semantic in-formation. A dependency tree from the Penn Treebank, with additional entity annotation from the BBN corpus.relation for the dependency graph, which consists ofa set of labeled arcs (wi, r, wj), where wi, wj  D (the set of dependencies).Given an input sentence s, the parser is initializedto  (3)The Shift rule advances on the input; each Leftr andRightr rule creates a link r between the next inputtoken n and the top token on the stack s. For produc-ing labeled dependencies the rules Leftr and Rightrare instantiated several times once for each depen-dency label.Additional parsing actions (cf. However, theyare not needed in the experiments reported here,because in the Penn Treebank used in our experi-ments dependencies are extracted without consider-ing empty nodes and the resulting trees are all pro-jective2.The pseudo code in Algorithm 1 reproducesschematically the parsing process.The function getContext() extracts a vector offeatures x relative to the structure built up to thatpoint from the context of the current token, ie, froma subset of I , S and A.
W07-2218@@Dependency parsing has been a topic of active re-search in natural language processing during the lastseveral years. Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing. Input Context: the last previous state with thesame queue I .
W07-2220@@The annual Conference on Computational NaturalLanguage Learning (CoNLL) has for the past nineyears organized a shared task, where participantstrain and test their learning systems on the samedata sets. A study in multilingual parser optimization.In Proc. I amalso indebted to all the people who have contributedto the shared task by providing data or participating.ReferencesS.
W08-0101@@Responsiveness in DialogueAlthough the quality of speech technologies has im-proved drastically and spoken interaction with ma-chines is becoming a part of the everyday life ofmany people, dialogues with artificial agents stillfall far short of their human counterpart in terms ofboth comfort and efficiency. Let (Kn) be a set of n silence clusters, the goal is to set the thresholds (n) that minimize overall meanlatency, while yielding a fixed, given number of false alarms E. let us define Gn the number of gaps amongthe silences of Kn. Let us assume (n) is a set of thresholds thatminimizesn Ln, and  Since their slopes differ, it is possible tofind a small  such that the decrease in FA yielded by p +  is exactly compensated by the increase yieldedby q  , but the reduction in latency in Kq is bigger than the increase in Kp, which contradicts the factthat (n) minimizes L.From Theorem 1, we get As.t.
W08-0109@@In this paper, we argue that the ability to store andquery large amounts of data is a key requirementfor data-driven dialog systems, in which the data isgenerated by the spoken dialog system (SDS) com-ponents (spoken language understanding (SLU), di-alog management (DM), natural language genera-tion (NLG) etc.) A coefficient of agreement for nominalscales. Also at the dialoglevel, the  statistic gave good results (0.
W08-0114@@As much attention is recently paid to autonomousagents such as robots and animated agents, spokendialogue is expected to be a natural interface be-tween users and such agents. A markup language for describ-ing interactive humanoid robot presentations. Battle, and S. Peters.
W08-0120@@ A robust dialog manager is an essential part of spoken dialog systems, because many such sys-tems have failed in practice due to errors in speech recognition. The utility function u(a,h) can be specific to each application. At each time instant t, the system chooses an action aA, and this causes the system to move from current state s to next state s The system computes the belief state in the next time instance b(s) This process is referred as belief update.
W08-0122@@There has been a great deal of research in recentyears on opinions and subjectivity. (1) D:: And I thought not too edgy and like a box, morekind of hand-held not as computery, yeah, more or-ganic shape I think. .Speaker D expresses an opinion in favor of a de-sign that is simple and organic in shape, and againstan alternative design which is not.
W08-0130@@This paper gives an overview of a new example-based generation technique that is designed to makegrammar-based generation easier to deploy in dia-logue systems. The distribution of rat-ings across utterances is not normal; to validateour results we accompanied each t-test by a non-parametric Wilcoxon rank sum test, and signifi-cance always fell in the same general range. The human trainee who talksto the doctor plays the role of a U.S. Army captainnamed Captain Kirk.
W08-0210@@Spoken Dialogue Systems model end-to-end ex-ecution of conversation and consequently requireknowledge of many areas of computational linguis-tics, speech technology and linguistics. Part of the incentivefor doing this was to protect Project Partners IP, sothat materials provided by (particularly commercial)Project Partners would not be housed at the Univer-sity, and would only be accessible to relevant stu-dent(s), the Project Partner, the instructor and thecourse assistant. Many other top-ics are possible depending on the available time.The Output Management unit looks at various as-pects of generation, timing of actions and could alsodiscuss paraphrase generation or prosodic mark up.Other topics of a system wide nature such as N-best processing or help systems can be discussed atthe end of the course if time allows.
W08-0301@@It is widely known that translation is by nomeans word-to-word conversion. is conditioned on f itself.This probability, P (|f), is in the same form asthe probability of a normal phrase pair, P (E|F as some special phrase of the tar-get language and f as a source language phraseon its own. Such a SWD mechanismfails when data sparseness becomes a problem.
W08-0302@@Machine translation (MT) by statistical modeling ofbilingual phrases is one of the most successful ap-proaches in the past few years. Al-though this same advantage was already obtainedin statistical MT through the transition from noisychannel count(e,f ,f context)Since we expect that adding conditioning vari-ables will lead to sparser counts and therefore morezero estimates, we compute features for many dif-ferent types of context. A statistical approach to machinetranslation.
W08-0303@@In machine translation parallel corpora are one veryimportant knowledge source. In the translationexperiments it was shown that the improvement issignificant at a significance level of 5%.ReferencesAtserias, J., B. Casas, E. Comelles, M. Gonzalez, L.Padro FreeLing 1.: Syntacticand semantic services in an open-source NLP library.In LREC06. The factored nodefor the direction (s, t) is connected to the variablenodes yji and y(j+s)(i+t).
W08-0304@@This is accomplished by either directly walk-ing the error surface provided by an evaluation met-ric w.r.t. Let-ting Ew denote the result of the translation argmaxw.r.t. The vertical lines indicate the surrogateloss values used for the center region under each of the schemes (i-iii).is distributed according to a Gaussian8, d s.t.di  This allows the procedure to mini-mize along diagonal search directions, while makingessentially no assumptions regarding the character-istics of the objective or the relationship between aseries of sequential line minimizations.
W08-0305@@ and BackgroundThe performance of every learning system is the re-sult of (at least) two combined effects: the repre-sentation power of the hypothesis class, determin-ing how well the system can approximate the targetbehaviour; and statistical effects, determining howwell the system can approximate the best element ofthe hypothesis class, based on finite and noisy train-ing information. This is a classical machine learningquestion, that however comes with the need for fur-ther theoretical work, since it breaks the traditionali.i.d. In preparation.Learning analysis of a machine translation system.N.
W08-0306@@)alignment models, is the most widely-used align-ment system. This researchwas supported by DARPA (contract HR0011-06-C-0022) and by a fellowship from AT&T Labs. A Hierarchical Phrase-Based Model forStatistical Machine Translation.
W08-0307@@Much research has been done on using syntactic in-formation in statistical machine translation (SMT).In this paper we use chunks (shallow syntax infor-mation) to improve an N -gram-based SMT system.We tackle both the alignment and reordering prob-lems of a language pair with important differencesin word order (Arabic-English). English preprocessingsimply included down-casing, separating punctua-tion from words and splitting off s. the Union alignment setof both translation directions (U); (b.) A simplex methodfor function minimization.
W08-0309@@Therewere two shared tasks this year: a translation taskwhich evaluated translation between 10 pairs of Eu-ropean languages, and an evaluation task which ex-amines automatic evaluation metrics.There were a number of differences between thisyears workshop and last years workshop: Instead of creating our testset by reserving a portion of the training data,we instead hired translators to translate a set ofnewspaper articles from a number of differentsources. A constraint grammar-based parserfor Spanish. three times for each automatic met-ric, comparing it to each type of human evaluation.Since there were no ties  1)where di is the difference between the rank forsystemi and n is the number of systems.
W08-0312@@Automatic Metrics for MT evaluation have been re-ceiving significant attention in recent years. BLEU: a Method for Automatic Eval-uation of Machine Translation. The morphemes in these languages117Exact Match Flexible MatchEnglish: Bleu 0.
W08-0313@@Statistical machine translation (SMT) is today con-sidered as a serious alternative to rule-based ma-chine translation (RBMT). Architecture of the systemThe goal of statistical machine translation (SMT) isto produce a target sentence e from a source sen-tence f . A neural probabilistic lan-guage model.
W08-0316@@In addition, no special-purposedecoder is required since standard WFST operationscan be used to obtain the 1-best translation or a lat-tice of alternative hypotheses. T (1)in which T is an acceptor for the target languagesentence and L is the word lattice of translations ob-tained during decoding. BLEU: a method for automaticevaluation of machine translation.
W08-0318@@We recently added minimum Bayes risk de-coding and reordering constraints to the decoder. We tried other n-best list sizes and scalingfactors, with very similar outcomes. ).A final modification to the data preparation istruecasing.
W08-0319@@Czech is a Slavic language with very rich morphol-ogy and relatively free word order. MT with a Deep Syntactic Transfer3. The t-layer by itselfdoes not bring any reduction in vocabulary.
W08-0320@@Modern Statistical Machine Translation (SMT) sys-tems are trained on sentence-aligned bilingual cor-pora, typically from a single domain. Column 1 provides a brief description of the model used. Note however that this year we had more training data com-pared to last year: 1.M vs. 1M words for News Commentary,
W08-0321@@Word alignment models are a crucial component in sta-tistical machine translation systems. denote the set ofsentences pairs from which the phrase pair (e, f) We calculate then a phrase alignment confidencescore pc as:pc(e, f) log sc(ek, fk)|S(e, f )| (6)This score is used as an additional feature of the phrasepair. A systematic compar-ison of various statistical alignment models.
W08-0324@@At its core is a transfer en-gine using two language-pair-dependent resources:a grammar of weighted synchronous context-freerules (possibly augmented with unification-style fea-ture constraints), and a probabilistic bilingual lexi-con of syntax-based wordand phrase-level transla-tions. Fast exactinference with a factored model for natural languageparsing. Theyields of the aligned parse tree nodes are extractedas constituent-level translation equivalents.Each entry in the lexicon is assigned a rule score,r, based on its source-side part of speech cs, source-side text ws, target-side part of speech ct, and target-side text wt.
W08-0329@@Confusion network decoding has been applied incombining outputs from multiple machine transla-tion systems. Each shifted block is con-sidered a single edit. HR0011-06-C-0022 under the GALE program.ReferencesS.
W08-0333@@Like many other NLP problems, output quality ofstatistical machine translation (SMT) systems in-creases with the amount of training data. After collecting counts, theconditional probability P (f |e) is computed by sum-ming over all columns for each f and dividing. The development of parallelalgorithms involves a number of tradeoffs.
W08-0334@@Our approach is in many ways a generalization of this work. An ME model is an exponential model with the following form:where: t is the class being predicted; c is the context of t;  The corpus statistics of the target language corpus (en). Interestingly Dutch, a relative of Malay, also improved substantially.
W08-0335@@Chinese word segmentation (CWS) is a necessarystep in Chinese-English statistical machine transla-tion (SMT). A compositeword (numbers, dates, times, etc.) Instead, we used equal s for allthe features.
W08-0401@@Statistical methods are widely used for machinetranslation. The probabil-ity is assigned to left and right sides as ps(t|fi, ei),where, s is left (l) or right (r), t is monotone (m),swap (s), or discontinuous (d). Japanese-to-English TranslationNext, we conducted J-E translation experimentsusing the same corpus.
W08-0402@@Large-scale parsing-based statistical machine trans-lation (MT) has made remarkable progress in thelast few years. However, one canimage a single goal item identified by the span [0, n] and thegoal nonterminal S, but not by the LM contexts. of the target-language word ei.Since the p-probability of e does not include theLM probability for the partial m-grams (ie, the first(m ek1), (5)an estimate of the LM probability of the m1-gramprefix of e. This estimated probability is taken intoaccount for pruning purposes (only).The function q(e1 .
W08-0405@@In order to apply the rules, the inputsentences are POS-tagged. 4, for a given rule id rand coverage position s we have to be able to look-up all possible edge extensions efficiently. [i, j] is the coverage interval for the lastsource phrase translated (the same as in Eq.
W08-0406@@Allowing translation ofword sequences (phrases) instead of single wordsprovides SMT with a robustness in word selectionand local word reordering.PSMT has two means of reordering the words. On average, a sentence contains0. 21 S based on this viewpoint , every small port and every ferry port which handlesa great deal of tourist traffic should feature on the european list .B baseret pa S is source, B is baseline, and P is the SPTO approach.
W08-0411@@These are commonly ac-quired from large volumes of automatically word-aligned sentence-parallel text corpora. Wellformedness constraintsGiven a pair of word-aligned sentences and theircorresponding parse trees S and T , represented assets of constituent nodes, our PFA node alignmentalgorithm produces a collection of aligned node-pairs (Si, Tj). 749, and a recall of 0.
W08-0502@@However,many formalisms do not require type checking. The regression testing is then rerun and, if thetype checking passes, the commit proceeds.In sum, introducing type checking at multiple lev-els provides a better development environment forgrammar engineers as well as documentation for thedevelopers and for applications.ReferencesButt, M., Forst, M., King, T.H. InGrammar Engineering Across Frameworks.Copestake, A.
W08-0506@@That year, a crucial compo-nent of his code was discovered to have a simpleerror with large consequences for his research. A variety of codecoverage metrics exist. Corpora also are valuable evalua-tion resources: the combination of a structured testsuite and a naturalistic corpus provides a powerfulset of tools for finding bugs in NLP applications.AcknowledgmentsThe authors thank James Firby, who wrote the func-tional tests, and Helen L. Johnson, who wrote therules that were used for the BioCreative data.
W08-0508@@This paper describes a fast, reliable and scalableframework for developing applications supportingtactical generation  by which we mean applicationswhich take as their input some semantic structurethat has already been organised at a high level, andchoose the syntactic structures and words required toexpress it. Com-lex Syntax: Building a Computational Lexicon. Techni-cal report, Information Sciences Institute, Marina delRey, California.T.
W08-0509@@Training state-of-the-art phrase-based statistical ma-chine translation (SMT) systems requires severalsteps. Theoretically the E-step requires sum-ming over all the alignments of one sentence pair,which could be (I + 1)J alignments in total. Thiscould reduce the I/O significantly when using thesame number of CPUs.
W08-0510@@In phrase-based translation, contiguous segments of words in the input sentence are mapped to contigu-ous segments of words in the output sentence. In SMT, we are given a source language sen-tence, s, which is to be translated into a target lan-guage sentence, t. The goal of machine translation is to find the translation, t , which is defined as:  The argmax implies a search for the best translation t in the space of possible translations t. This search is the task of the decoder, which we will concentrate on in this paper. The de-coder also produces diverse types of output, rang-ing from 1-best, n-best lists and word lattices.
W08-0601@@Automated protein-protein interaction (PPI) extrac-tion from scientific literature is a task of significantinterest in the BioNLP field. A short-est path dependency kernel for relation extraction. Thus, from each sen-tence,(n2)examples are generated, where n is thenumber of occurrences of protein names in the sen-tence.
W08-0607@@Science involves making hypotheses, experiment-ing, and reasoning to reach conclusions, which areoften tentative and provisional. For instance, negation of unhedg-ers was used as a syntactic pattern; the patternwas able to recognize know as an unhedger inthe following sentence, but not the negative quanti-fier (l i t t le), labeling the sentence as non-speculative. In the future, we willmove in this direction with the goal of character-izing the semantics of speculative language.AcknowledgementsWe would like to thank Thomas C. Rindflesch forhis suggestions and comments on the first draft ofthis paper.ReferencesdeMarneffe, M. C., MacCartney B., Manning C.D.
W08-0704@@isperhaps misleading here: it does not mean that themodels have no parameters, rather it means that thelearning process considers models with different setsof parameters). An adaptor grammar con-sists of terminals V , nonterminals N (including astart symbol S), initial rules R and rule probabilitiesp, just as in a PCFG. Suppose we havea node A to expand.
W08-0804@@Statistical NLP learning systems are used for manyapplications but have large memory requirements, aserious problem for mobile platforms. Practically, this means storingevery observed feature string in memory, a pro-hibitive cost for systems with constrained resources.Offline feature selection is a possible solution, butstill requires an alphabet and eliminates the poten-tial for learning new features after deployment, animportant property for adaptive e-mail or SMS pre-diction and personalization tasks.We propose a simple and effective approach toeliminate the alphabet and reduce the problem of di-mensionality through random feature mixing. Rcv1:A new benchmark collection for text categorization re-search.
W08-0909@@A reading difficulty, or readability, measure can bedescribed as a function or model that maps a textto a numerical value corresponding to a difficulty orgrade level. S. Chall and E. Dale. SMOG grading: A new read-ability formula.
W08-0912@@However, past approaches have focused typi-cally only on one type of spoken language, or on a range of types similar in linguistic entropy. 9 Pearson r Correlation  0. to respond with a proposed solution or answer.
W08-1002@@The literature on HPSG parsing of German has al-most exclusively been concerned with issues of the-oretical adequacy and parsing efficiency. With grandparenting added, however, therelative gains of the n-gram models greatly dimin-ishes. This corresponds to a baseline of 21.
W08-1005@@Thisnaive grammar is a poor one because its context-freedom assumptions are too strong in some ways(eg it assumes that subject and object NPs sharethe same distribution) and too weak in others (egit assumes that long rewrites do not decompose intosmaller steps). 6th Workshop on VeryLarge Corpora.E. Adding initial structural annota-tion results in a higher baseline performance.
W08-1006@@These techniquesare not independent, and we thus examine howlexicalization and Markovization interact, sincelexicalization for German has been the mostcontentious area in the literature. Fastexact inference with a factored model for natural lan-guage parsing. These results are41TuBa-D/Z Tiger NegraHoriz.
W08-1007@@Is it really that difficult to parse German We present adependency-driven parser that parses both depen-dency structures and constituent structures using anextended version of MaltParser 1.0. Previously, MaltParser combinedthe prediction of the transition with the prediction ofthe arc label r into one complex prediction with onefeature model. A Fundamental Algorithmfor Dependency Parsing.
W90-0104@@ Lexical choice for open-class words has typically been regarded as a matching or classification problem. In E. Rosch and B. Lloyd (Eds. For example, consider the following utterances: 4) A: "I want to flood room 16 with carbon dioxide" 5a) B: "Wail  there is an animal in the room" 5b) B: "Wail there is a dog in the room" 5c) B: "Wait, there is a Pekingese in the room" Assume the object in question is Fido, and As communicative goal is simply to inform B that Fido has the attributes {An/ma/, Breathes:Air}, and hence would be adversely affected if the room was flooded with carbon dioxide.
W90-0108@@ :  in ter fac ing  w i th  a text  generat ion  sys tem Consider the task of interfacing a domain-independent, reusable, general text generation system with a particular application domain, in order to allow that application to express ystem-internal information in one or more natural anguages. The removal of this kind of information allows upper model specifications to be invariant with respect o their particular occasions of use in texts and the adoption of textually motivated perspectives, such as, eg, theme/rheme s lections, definiteness, anaphora, etc. The anatomy of a systemic hoice.
W90-0109@@Natural language generation is the deliberate production of text to meet the communicative goals of some underlying application program. AIRLAND BATrLE MANAGEMENT PROJECT 9 Conduct covering force operations along avenues Band C to defeat he lead regiments of the first tactical echelon in the CFA in assigned sector. 7 Note that I will not attempt aformal definition.
W90-0111@@ Most models of natural language generation, be they computational or psychological, recognize that the task of text planning (also called conceptnalization\[14\], \[10\]), comprises the following essential subtasks: (1) content selection and (2) content organization. Pattabhiraman, T. and Cercone, N. Generation of Speech-Like Monologue: The Case of Bus Route Descriptions. Kempen, G. and Hoenkamp, E. "An Incremental Procedural Grammar for Sentence Formulation".
W90-0114@@ It is a well known fact that just as we cant string together a collection of words in an arbitrary order and achieve a well-formed sentence, we cant string sentences together in an arbitrary fashion and achieve a meaningful paragraph. E.  (1) Pushing the launp Test Switch should be the inst thing you should do. (2) Otherwise you dont execute a Lamp Test.
W91-0104@@  From the linguists point of view, a grammar is a formal device which defines a recursively enumerable set of well-formed linguistic structures, each having, among other aspects, a phonological content (or, when dealing with written text, a string content) and a semantic ontent. The hypotheses made on string compositionality in Lexical Grammars are simply that sister constituents concatenate heir strings; they entail that parsing is finitely e~numerable. This is unessential, but permits us to use the concepts of the previous ection, developed for unary relations, without having to generalize them to n-ary relations.
W91-0105@@  In general, the goal of parsing is the derivation of all possible grammatical structures defined by a grammar of a given string a (ie especially the determination of all possible logical forms of o~) and the goal of the corresponding generation task is the computation, of all possible strings defined by a grammar of a! If during parsing eg, two readings LF t I . In M. Brady and C. Berwiek, editors, Computational Models of Discourse.
W91-0109@@  Machine translation is an obvious application for reversible natural language systems, since both understanding and generation are important parts of the process. There are several arguments for this view (for e:kample, \[Isabelle, 89\]), including reducing the total cost of adding a new language and making it easier to maintain and validate the resulting system. fail, the system must have a default.
W91-0208@@: Lexical Implication Rules It is a truism that people, in interpreting and producing language, make use of both linguistic and real-world knowledge. 11Th e verb calve instead of the expected *calf is a mild case of pre-emption i Modem English. Nonetheless, this latter is not pre-empted eg he drank the whole bottle(ful); the pail holds eight bottle(ful)s of milk.
W91-0215@@A system with the capability of natural anguage understanding typically relies on knowledge about a restricted domain of application. (2b) The non-l inguist ic const i tuent  of  lexical meaning conc75 SITUITIO| E . Then I saw a hole in the fence."
W91-0222@@This paper proposes a two-level model that integrates tense and aspect information, based on theories by both Hornstein (in the spirit of l:teichenbach) and Allen, with lexicalsemantic information based on an extended version of Jackendoffs theory that includes a verb classification system proposed by Dowty and Vendler. 5 A similar scheme has been suggested by 4The empty location denoted by e corresponds to an unrealized argument of the predicate arrive. Thus, the full type entry under the matrix clause for the word al would be \[:kd,+t,:ka\].
W93-0104@@: Internal versus External Evidence It has long been appreciated by people working with proper names in unrestricted written texts that any adequate treatment requires the use of a grammar. External evidence is a necessity for high accuracy performance. embedded references to cities or countries, eg "Cambridge Savings Bank"  open class keywords like "Church" or "Bank" (following Coates-Stephens terminology), and the incorporation-terms used by companies of various countries when giving their full legal names ("Inc." in the U.S.A., "P.T."
W93-0105@@The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text. Appos i t ives  Appositives are important linguistic devices for introducing new mentions. 2 Proper  Names  Syntact i c  Forms and  Semant ic  At  t r ibutes  We first need to describe more precisely what we mean by proper names.
W93-0106@@However, a lexicon as given may not suit the requirements of a part icular computat ional  task. We will at t imes refer to it as a hierarchy nonetheless. A framework for representing knowledge.
W93-0107@@ The design of word-sense taxonornies iacknowledged as one of the most difficult (and frustrating) tasks in NLP systems. 4) None of the aforementioned studies provide a method to describe and evaluate the derived categories. These studies, though valuable, leave several open problems: 70 1) A metric of conceptual c oseness based on mere syntactic similarity is questionable, particularly if applied to verbs.
W93-0108@@ There is a widespread belief among linguists that a predicates subcategorization frames are largely determined by its lexical-semantic properties \[23, 11, 12\]. For example, only the entry for the refuse sense of deny in LLOCE includes the grammar code D1 which signals a ditransitive subcategorization frame: ( l l )  C193 verbs: not lett ing or al lowing deny \[D1;T1\] ... G127 verbs: re jec t ing . For example, the verb accord can be used in either one of two senses: agree or give, e,g.
W93-0109@@ When we construct a grammar, there is always a trade-off between the coverage of the grammar and the ambiguity of the grammar. "...gives current management  enough t ime to work on..." "...tel_.._l the people in the hall that..." ; "...tol__.d h im the man would..." "...expected the impact from the restructur ing to make..." ". "A Simple Rule-Based Part of Speech Tagger".
W93-0113@@ As more text becomes available lectronically, it is tempting to imagine the development of automatic  filters able to screen these tremendous flows of text extracting usefill bits of information. D ic t ionary  We also use an online dictionary as a gold standard following a slightly different procedure. 144 Roget  s entry Topic , ,  For each word, take the word appearing as most similar.
W93-0114@@ In information retrieval, proper nouns, group proper nouns, and group common nouns present unique problems. Use o f  P roper  Nouns  in Match ing  Both the lexical entry for the proper noun or the category code may be used for matching documents o queries. We reserve the last category as a miscellaneous category.
W93-0301@@The information retrieval application may be of particular elevance to this audience. Norm a system for translators. Except for Cf.e, most of the notation is borrowed from Brown ctal..
W93-0302@@ A typical information retrieval OR) task is to select documents from a d~!ahase in response to a users query, and rank these documents according to relevance. This is accomplished by removing a standard suffix, eg "stor+age", replacing it with a standard root ending C+e"), and checking the newly created word against the dictionary, ie, we check whether the new root ("store") is indeed a legal word, and whether the original root ("storage") is defined using the new root ("store") or one of its standard inflectional forms (eg, "storing"). In order to deal with structure, parsers t Standard IR benchmark collectiot~s are statistically too  small and the experiments can easily produce cotm~rinmitive results.
W93-0306@@  This paper outlines NPiool, a noun phrase detector. Here is a flow chart of the system: Preprocess ing V Morphological analysis V Constra int  Grammar pars ing %/ V NP-hostile finite IP-friendly finite state pars ing state pars ing V V IP extraction liP extraction V V I n te rsect ion  of noun phrase sets  SSee eg \[VoutLla/aen, Heikkil Preprocessing and morphological analysis The input ASCII text, preferably SGML-annotated, is subjected to a preprocessor that eg determines sentence boundaries, recognises fixed syntagms 9, normalises certain typographical conventions, and verticalises the text. ), we reach a recall of 98.
W93-0309@@ Collocations present an area that is important both for lexicography to improve their coverage in modern dictionaries as well as for lexical acquisition in computational linguistics, where the goal is to build either large reusable lexical databases (LDBs) or specific lexica for specialized NLP-applications. Although our main interest lies in SVCs we will in the following not distinguish between i) SVCs (eg to take into consideration), ii) lexicalized combinations with support verbs where the noun has lost its original meaning and which belong to phrasemes (eg to take a \[ancy), and iii) collocational combinations of support verbs with concrete or non-predicative nouns (eg to ta/,e a seat); we will refer to all these cases as V-N collocations. least 3 times, MI was calculated together with a t-score.
W93-0310@@These associative responses have been explained in psychology by the principle of learning by contiguity: "Objects once experienced together tend to become associated in the imagination, so that when any one of them is thought of, the others are likely to be thought of also, in the same order of sequence or coexistence as before. IThis research was supported by the Heinz-Nixdorf-lnstitute (project 536) 84 2 Mode l  According to the law of association by contiguity the learning of associations can be described as follows: If two words i and j occur together, the association strength aid(t) between i and j is increased by a constant fraction of the difference between the maximum and the actual association strength. The following selection of some 33 million words of machine readable English texts used in this study is a modest attempt o achieve this goal:  Brown corpus of present day American English (1 million words)  LOB corpus of present day British English (1 million words)  Belletristic literature from Project Gutenberg (1 million words)  Articles from the New Scientist from Oxford Text Archive (1 million words)  Wall Street Journal from the ACL/DCI (selection of 6 million words)  Proceedings of the Canadian Parliament (selection of 5 million words from the ACL/DCI-corpus)
W94-0113@@ Natural language processing of spontaneous speech is particularly difficult because it contains false starts, out of vocabulary words, and ungrammatical constructions. Introduce a mechanism for enforcing global constralnts, i. e. agreement, and other selectional restrictions. Ills solution is a general one.
W94-0201@@ Tim wealth of literature on tone and intonation has amply demonstrated that voice pitch (F0) in sp,~ech is umier independent linguistic ontrol. Re ferences  Bird, S. & Stegen, O. RP 57, University of Edinburgh, Centre for Cognitive Science. Phonologists usually d,~scribe a pitclf contour nmch as they describe sp~ech more generally, namely as a sequence of discrete units (i,e.
W94-0204@@ Two-level phonology combines the computational advantages of finite state technology with a formalism that permits phenomena to be described with familiar-looking rules. O Since there exist fixed context-sensitive grammars for which the acceptance problem is NPhard 9, the NP-hardness of FIXED DFSM RECOGNITION with nulls follows as a corollary. Follow arc 3 to s, recognizing n:n. (This is the only applicable arc.)
W94-0208@@ I l ifants lUllSt Icarni I,o recognize ccrtain sound seqllellCl.s ;IS I)(~illg words; this is a dillicult i)robIcn~ I)ecausc norll ial speech contains no obvious acoustic divisions between words. The length of the representation f the iuteger m is given by I.h(~ fimction e(~)(,.) Minimal generative xplanations: A middle ground between eurons and triggers.
W94-0302@@  Research in discourse processing has identified two representational requirements for discourse planning systems. 6 DPOCL s  P roper t ies  Plan structures in DPOCL represent three critical components. and the corresponding effect e,~ of sp is intended.
W94-0306@@* Dr. Paris is on leave from USC/Infonnation Sciences Institute, 4676 A&niralty Way, Marina del Rey, CA 90292 and the Computer Science Department ofthe University of Southern California. I n fo rmat ion  prov is ion and eulogy On taking these two stances, writers show a strong preference for the use of simple acdve declaratives. Var ia t ion  in ins t ruct ions  Instructions are aimed at conveying directions to perform a (set of) task(s).
W94-0307@@A standard problem in text generation is to determine what to include in the text and how to structure it. In E. Rosch and B. Lloyd, editors, Cognition and Categorization, pages 27-48. La g~n~ration i teractive de langage: comment visualiser le passage de lid@e k la phrase.
W94-0308@@ In user instructions, it is common to find expressions like: (1) Pull down and remove the white plastic tray that holds the video cable and unpack the cable. (42) Recoloque o filtro de p6 e monte Re-put the filter of dust and mount novamente o aparelho. This expression of Generation on its own would be extremely awkward: (41)  N~o sobrecarregue o filtro Do not overload the filter" resultando em conseqii6ncia em melhor resulting "in consequence in better funcionamento do motor.
W94-0311@@  Whether we talk of monolingual or multilingual generation, it is not surprising that there has been very little focus on the area of lexical choice. In P. StDizier and E. Viegas (Ed) Computational Lexical Semantics. (physical object) b. I read an angry book.
W94-0312@@ In Text Generation, some of the most interesting issues lie at the interface between the conceptual model (the underlying program) and the generator. e) they found themselves surrounded by strangers. c) they were nearly hit by a car d) they reached the other side stricken with fear.
W94-0315@@Whatever the approach, the system is divided into two main components: a message planning component and a linguistic realization component. In a context of discourse, the message planning component is a text planner and the message is a text  p lan *This work was realized at the lstituto per la Ricerca Scientifica e Tecnolo#ica (IRST) in Trento (Italy). \[Hovy 92\] Hovy E., Sentence planning requirements for automated explanation generation, DIAMOND Bericht n  \[Joshi 87\] Joshi A.K., The relevance of Tree Adjoining Grammar to generation, In Kempen G. \[Maier & Not 93\] Maier E. & Not E., Increasing cohesion in automatically generated natural anguage texts, In Torasso P.
W94-0316@@However, a text planning process which does not take into account linguistic resources that are available to express a particular meaning suffers from two major shortcomings: (i) it is not sensitive to variant discourse organizations at the sentence level; and (ii) it cannot guarantee that its text plan is always verbalizable by the linguistic module. how discourse structure relations are defined in lexical resources, Inters i tuat lonal  Layer. A Typed-Feature Structure Unification-Based Approach to Generation.
W94-0317@@This paper describes how reference decisions are made in PROVERB, a system that verbalizes machine-found natural deduction (ND) proofs. Acts: a PCA that conveys the fact that the conclusion Noden+l is derived from the premises Node1, ..., Noden by applying the method M. Features: (bottom-up general explicit detailed) If the conclusion, the premises and the method are instantiated to a E $1, (a E $2, $1 E $2), and def-subset respectively, the following verbalization can be produced: "Since a is an element of $1, and S1 is a subset of $2, a is an element of $2 according to the definition of subset." If impossible, a bottom-up operator will be chosen.
W94-0319@@ In this paper I survey some recently-developed NL generation systems that (a) cover the complete generation process and (b) are designed to be used by application programs, as well as (or even instead of) making some theoretical point. The main engineering argument for arranging modules into a pipeline instead of a more complex structure 165 7th International Generation Workshop  In a one-way pipeline of N modules there are only N-1 interfaces between modules, while a pipeline with two-way information flow has 2(N-l) interfaces, and a system that fully connects each module with every.other module will have N(N-1) interfaces. Are well enough known that I could easily obtain information about them.
W94-0327@@ In a monolingual dialogue system, strong arguments are needed for generation ot to reversibly use the same linguistic resources as parsing. OLF term: subj (e ,x ) . A Formalism for Incremental Sentence Generation" In Eds.
W95-0101@@Once trained, a sentence can be tagged by searching for the tag sequence that maximizes the product of lexical and contextual probabilities. In i t ia l  S ta te  Annotator  The unsupervised learner begins with an unannotated text corpus, and a dictionary listing words and the allowable part of speech tags for each word. The objective function for this transformation measures this by computing the difference between the number of unambiguous instances of tag Y in context C and the number of unambiguous instances of the most likely tag R in context C, where R E X, R ~ Y, adjusting for relative frequency.
W95-0102@@ Researchers investigating the acquisition of phrase-structure grammars from raw text have had only mixed success. Therefore, if movement and conjunction is of single constituents, phrase-structures (A-D) explain this evidence but (E-H) do not. Let us look one final time at the sequence V P N. There are only three words here, and therefore three heads.
W95-0104@@Two classes of methods have been shown useful for resolving lexical ambiguity. Since the number of collocations grows exponentially with e, it was only practical to vary g from 1 to 3. For instance, for {I, me}, the reliability metric did better than U(xly) (0.
W95-0105@@However, for many tasks, one is interested inrelationships among word senses, not words. 280 cable, line, transmission li e 0. Assuming that nura sen s e s (w \[ 5_ \] ) and sen  s e \[ 5_, k \] are reinterpreted accordingly, the algorithm will compute qo not only for the synsets directly including words in W, but also for any higher-level abstractions of them.
W95-0106@@However, at present bracketed corpora for Chinese are unknown, as is the case for many other languages. As an abbreviation we write e~..t for the sequence of words e~+~, e~+2,.. . A statistical approach to machine translation.
W95-0111@@ As we are facing the growing amount of on-line text, the use of text analysis techniques to access information from electronic sources has become more popular and, at the same time, more difficult. 138 I A total of 36 topical terms were manually determined based on the UPI focused sample. C. Gierl and D. Frost.
W95-0114@@ Building a domain-specific bilingual lexicon is one significant component in machine translation and machine-aided translation systems. 8 Exper iment  2: F ind ing  the Word  Trans lat ion Among a C luster  o f  Words  The above experiment showed to some extent he clustering ability of context heterogeneity. Kvec: A new approach for aligning parallel texts.
W95-0115@@ A machine translation system must be able to choose among possible translations based on context. An alignment filter is based on the relative positions of S and T in their respective texts\[Dag93\]. 197 References \[Bri92\] E. Brill, "A Simple Rule-Based Part of Speech Tagger," Proceedings of the 3rd Conference on Applied Natural Language PTvcessing, pp.
W96-0101@@Since, for statistical taggers, 90% of texts can be disambiguated solely applying lexical probabilities, it is, in fact, tempting to think that with more data and more accurate lexical estimates, more text could be better disambiguated. The 9 bigram in the example, \[p r\] \[jmp nmp\], occurs 306 times in the training corpus. This approach is often used for smoothing probabilities, but, considering the high ambiguity of some French suffixes, such as "e", "es", etc, it is doubtful that basing the estimates on the suffixes alone would give good results.
W96-0102@@  Part of Speech (POS) tagging is a process in which syntactic ategories are assigned to words. IGTree Complexity The asymptotic omplexity of IGTree (i.e, in the worst case) is extremely favorable. Output: A (sub)tree.
W96-0103@@One of the fundamental issues concerning corpus-based NLP is that we can never expect to know from the training data all the necessary quantitative information for the words that might occur in the test data if the vocabulary is large enough to cope with a real world domain. (e) The music sent Mary to sleep. Repeat I. and 2. until C classes remain.
W96-0107@@A high quality translation dictionary is indispensable for machine translation systems with good performance, specially for domains of expertise. There are still a 80 . PENSI~E is a trademark of Osaka Gas corporation, OGIS-RI, and Oki Electric Industry Co.,Ltd.
W96-0111@@It accomplishes this by maintaining a large corpus of analyses of previously occurring utterances. DOP4 puts all lexical categories (p-o-s tags) of the sentence words, as found in a dictionary, in the chart. The combination of subtree t and subtree u, written as t o u, yields a copy of t in which its leftmost nonterminal leaf node has been identified with the root node of u (ie, u is substituted on the leftmost nonterminal leaf node of t).
W96-0113@@  Stochastic language models are useful for many language processing applications such as speech recognition, natural anguage processing and so on. Tagging English text with a probabilistic model. pre(s): The set of morpheme numbers that connect o the s-th morpheme.
W96-0114@@Designing and refining a natural language grammar is a diiBcult and time-consuming task and requires a large amount of skilled effort. Repeat (3) until a termination condition is detected. Pereira, F., N. Tishby, and L. Lee: Distributional Clustering of English Words, in Pro.
W96-0201@@  The first step in most corpus-based multilingual NLP work is to construct a detailed map of the correspondence between a text and its translation (a b i text  map). So, for example, sentence E corresponds with sentence d. The aligned blocks are outlined with solid lines. Def in i t ions  Several key terms will help to explain SIMR.
W96-0204@@ In language modeling for speech recognition the goal is to constrain the search of the speech recognizer by providing a model which can, given a context, indicate what the next most likely word will be. A. Stolcke and E. Shriberg, "Statistical Language Modeling M. Ostendorf, A. Kannan, S. Austin, O. Kimball, R. Schwartz and J. R. Rohlicek, "Integration of Diverse Recognition Methodologies Through Reevaluation of NBest Sentence Hypotheses", Proc.
W96-0205@@  Segmentation f sentences into words is trivial in English because words are delimited by spaces. Moreover, sequence of characters may constitute a different word. Note that the system correctly tokenized -~fbJ~E~, although it is not registered in the dictionary.
W96-0208@@  Recent research in empirical (corpus-based) natural language processing has explored a number of different methods for learning from data. A conservation law for generalization performance. Reprinted in Readings in Knowledge Acquisition and Learning, B. G. Buchanan and D. C. Wilkins (eds.
W96-0209@@ This work is part of an effort to develop a robust, domain-independent syntactic parser capable of yielding the unique correct analysis for unrestricted naturally-occurring input. In Oostdijk, N L: de Haan, P. eds. Comlex syntax: building a computational lexicon.
W96-0210@@ This paper presents techniques that can be used to analyze the formulation of a probabilistic classifter. Parameter  Es t imat ion  In these experiments, we use maximum-likelihood estimates (M.. estimates) of the model parameters. P ro toco l  fo r  the  D isambiguat ion  Exper iments  There are three parameters that define a wordsense disambiguation experiment: (1 ) the  choice of words and word meanings (their number and type), (2) the method used to identify the "correct" word meaning, and (3) the choice of text from which the data is taken.
W96-0211@@ Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). E.g., ...the lamps near the paintings of the house that was damaged in the flood. % correct) CBL Default i ~  Algorithm Strategy Heuristics w/o  feature set selection 76.
W96-0212@@  Chart parsing is a commonly-used algorithm for parsing natural anguage texts. While chart parsing and calculations of j3 can be done in O(n 3) time, we have been unable to find an algorithm to compute the o~ L terms faster than O(nS). We refer to this model as the t r ig ram est imate .
W96-0213@@  Many natural anguage tasks require the accurate assignment of Part-Of-Speech (POS) tags to previously unseen text. Given a sequence of words {wl , . Tes t ing  the  Model The test corpus is tagged one sentence at a time.
W96-0214@@  The Data-Oriented Parsing (DOP) model has a short, interesting, and controversial history. Examining Bods data, we find he removed e productions. We performed a statistical analysis using a t-test on the paired differences between DOP and Pereira nd Schabes performance on each run.
W96-0304@@ There is a movement in testing to augment the conventional multiple-choice items (ie, test questions) with short-answer f ee-response items. Kaplan, Randy M. and Randy E. Bennett. In J. Pustejovsky and S. Bergler (Eds), Lexical Semantics and Knowledge Representation, Springer-Verlag, New York, NY.
W96-0306@@ This paper addresses the problem of large-scale acquisition of computational-semantic lexicons from machine-readable resources. In E. Bach and R.T. Harms, editor, Universals in Linguistic Theory. McDonald, and T. Plate.
W96-0309@@In this paper, we argue that composition in compound constructions involves specification of the arguments of predicate structures within the qualia structure of the head noun. E. Stickel, Douglas E. Appelt, and Paul Martin. It is important o note that systems utilizing compositional apparatus for the analysis of complex nominals need not treat all compounds compositionally.
W96-0310@@ The general toPic of this paper is the information about adjectival meaning which should be included in a computational lexicon. Similarly, LR E, the event-itself sub-LR places ^ $varl in the event position. Acknowledgment The research reported in this paper was supported by Contract MDA904-92-C-5189 with the U.S. Department ofDefense.
W96-0401@@  Most current models of text generation include a phase of content selection and organization, usually performed by a text planner or schema application engine, followed by a phase of grammatical surface-form rendering, performed by a sentence generator. Content  De l imi ta t ion  Modu le  The Content Delimitation module determines the material to be included into each separate SPL expression. Controlling a Language Generation Planner.
W96-0403@@  Many of the first NLG systems link their information structure to the corresponding linguistic resources either through predefined templates or via careful engineering for a specific application. For instance, the derivation of B from A can be verbalized as: "Since A, B." 3 Text Structure in P R 0 VERB 3.
W96-0405@@  This paper describes the input specification language of the WAG Sentence Generation system. The realiser operates directly on the KB, using the information within the sentence-specification t  tailor the expression. My sense of relevance is derived from relevance in generation what information has been selected as relevant o the speakers unfolding discourse goals.
W96-0406@@Graphics and text are very different media. Re la t iona l  keys  Relational keys are similar to the notion of the same name in relational databases \[7\] and help determine which variables depend on which others. A study of graph comprehension difficulties.
W96-0410@@Words come in a variety of conventional combinations; these units range from short expressions with idiosyncratic meanings, like the call number of a book, to full sentences with compositionallyderived, yet frozen, meanings, like You cant teach an old dog new tricks. A Study of Tree Adjoining Grammars. Finally, we extend SPUDs evaluation of alternatives, o that it describes the most salient entities possible, and uses basic-level terms wherever possible.
W96-0411@@ Current work in surface realization concentrates on the use of general, abstract algorithms that interpret declaratively defined, non-directional grammars. In E. W. Elcock and D. Michie editors, Machine Intelligence 8, pages 300-332. We call such a rule c-rule.
W96-0512@@  One of the major problems with the Internet is the abundance of information and the difficulty for the average computer user to read everything existing on a specific topic. E.g., the user will receive an initial announcement about an event and only updates after that point. Summar izer (s ) :  agents that are concerned with summarizing the data that they have collected over the network from different sources and producing natural-language r ports for the end-user.
W97-0104@@ 1 Since the large-scale annotated corpora, such as Penn Treebank\[MSM93\], have been built in English, statistical knowledge xtracted from them has been shown to be more and more crucial for natural language parsing and disambiguation. Reference \[ZQ~i96-\]describ.e.s the automatic dentification methods for some Chinese MRRs. 3) to disambiguate parse trees using stochastic !1 tl I: !
W97-0105@@ This article describes a grammar-based probabilistic parser, and presents experimental results for the parser as trained and tested on a large, highly varied treebank of unrestricted English text. E. Eyes and G. Leech. TOWAI~DS RADICALLY EXPANDING TRAIN ING-SET  SIZE VIA  T I : tEEBANK CONVERSION 5. .
W97-0108@@  The semantic classi~cation f words refers to the abstraction of ambiguous (surface) words to unambiguous concepts. 9 We compared our approach with supervised methods o contrast their reliance on annotated corpora with our r~nce  on WordNet. a Metric on S~m~tic Nets.
W97-0109@@ The problem with successful resolution of ambiguous prepositional phrase attachment is that we need to employ various types of knowledge. In natural language 3Also a kind of  context. % success of this method on 500 randomly-selected s ntences.
W97-0113@@ The traditional approach to automatic abstracting aims at providing a reader with fast access to documents by facilitating a judgement on their relevance to his or her information needs. The possible values are "C" for COLUMN, "E" for EDITORIAL and "N" for NEWS REPORT. Content Analysis: An Introduction to Its Methodology, volume 5 of The Sage COMMTF-~T series.
W97-0114@@This phenomenon causes considerable problems in natural language processing systems. Toward MT system without Pre-Editing -Effects of New Methods in ALT-J/E-. A property-sharing constraint in centering.
W97-0117@@ In this paper we describe a method of improving the accuracy of automated speech recognition through text-based linguistic post-processing. The remaining occurrences of L constitute the refute set, ~(L ~ R). "Error Correction via a Post-Processor for Continuous Speech Recognition."
W97-0119@@It ks unlikely that one can find parallel corpora in any given domain in electronic form. Compete correlation2(WoRM1, WORM2); if it is high, e and c are considered as a translation pair. Their output is in SET  A.
W97-0120@@  Word segmentation is an important problem for Japanese because word boundaries are not marked in its writing system. This is a popular heuristics in Japanese word segmentation. Longest  Match  S t r ing  F requency  The estimates of word frequencies by the above string frequency method tend to inflate a lot especially in short words, because of double counts.
W97-0122@@The question arises on many occasions. e l  Secondly, our concern is with language corpora, not with texts. I then computed corpus similarity.
W97-0123@@  In corpus-based NLP, extraction of linguistic knowledge such as lexical/semantic collocation is one of the most important issues and has been intensively studied in recent years. Finally, the conditional probability distribution p$(e~ Iv) is estimated. In generating y, the process may be influenced by some conteztual information z, a member of a finite set t~.
W97-0124@@ One of the problems facing natural anguage parsing (NLP) systems is the appearance of unknown words; words that appear in sentences, but are not contained within the lexicon for the system. An ngram tagger concentrates on the n neighbors of a word (where n tends to be 2 or 3), ignoring the global sentence structure. Tagging systems make only limited use of the syntactic knowledge inherent in the sentence, in contrast o parsers.
W97-0201@@The Penn Treebank corpus contains asufficient number of partof-speech tagged and syntactically parsed sentences to serve as adequate training material for building broad coverage part-of-speech taggers and parsers. I only focus on content word disambiguation (ie, words in the part of speech noun t, verb, adjective and adverb). A stochastic parts program and noun phrase parser for unrestricted text.
W97-0202@@The purpose of this work is to support related work in automatic word-sense disambiguation. COMPLEX syntax: building a computational lexicon. The third line represents he second word in the idiom (take-place-T), which is a noun (NN).
W97-0204@@ The research present here is to be conducted under the FrameNet research product at the University of California. In A. Lehrer and E. F. Kittay, editors, Prames, Fields and Contrasts, pages 75102. Charles J. Fillmore and B.T.S.
W97-0206@@  Our present understanding of how the meanings of polysemous words are represented in speakers minds and accessed uring language use is poor. In S. Small, G. Cottrell, and Tanenhaus, editors, Lexical .Ambiguity Resolution. m n m m m m m mm m m 2 Sources of  diff icult ies in a semantic annotation task We predicted that three properties of the words that were to be matched with specific WordNet senses would result in differences among the individual taggers annotations and between those of the taggers and the more experienced lexicographers.
W97-0207@@  Semantic entropy is a measure of semantic ambignity and uninformativeness. (1) tET Note that H(T\[s) is not the same as the conditional entropy H(TIS ). frequency E \[ do !
W97-0209@@  It has long been observed that selectional constraints and word sense disambiguation are closely linked. In ARPA Workshop on human Language Technology, Plainsboro, N J, March. Let n be a noun that stands in relationship R to predicate p, and let {sl, ..., st} be its possible senses.
W97-0210@@Such observations inform the increasing trend towards analysis of homogeneous corpora to identify linguistic constraints for use in systems intended to understand or generate coherent discourse. * For each I~ 6 Y, traverse the WordNet hierarchy and locate the (set of) senses of z, Si, that are connected with some sense of ~. For the same verb and subcal~.egorization pattern, the cluster-based method rejects four of the thirteen senses with error rate 5% (i.e, 3 out of 58 occurrences in the Y part of the Brown Corpus will be assigned wrong tags).
W97-0211@@ Lexical Acquisition (LA) processes strongly rely on basic assumptions embodied by the source information and training examples. E. Brill, P. Resnik, "A ItuleBased Approach to Prepositional Phrase Attachment Disambiguation, in Proc. x T.(C) x S.(C) (3) where OAw are the absolute occurrences of w in the corpus.
W97-0212@@ This paper describes a working sense tagger, which attempts to automatically ink each word in a text corpus to its corresponding sub-sense in the Cambridge International Dictionary of English (CIDE). As a learner dictionary, CIDE contains much examples text. Here, the CIDE database gives the possible selectionai classes for head as body part, state, object, human or device; for pupil as human or body part; for question as communication r abstract.
W97-0213@@ Word sense disambiguation (WSD) is perhaps the great open problem at the lexical level of natural language processing. Unsupervised algorithms 83 Target WordNet English Word Sense # description Spanish French German Italian 1 inter&, in t~t  interesse r~dito interest (noun) 3,4 drug la (noun) bank (noun) lb fire 1 (t. verb) 5 monetary (eg on loan) stake/share intellectual curiosity benefit, advantage medicine narcotic shoreline embankment financial inst. In Proceedings o.f the 34th Annual Meeting o.f the Society for Computational Linguistics, pp.
W97-0301@@ This paper presents a statistical parser for natural language that finds one or more scored syntactic parse trees for a given input sentence. A Maximum Entropy Part of Speech Tagger. .w ,  have all been assigned Jo in  X. Th i rd  Pass The third pass always alternates between the use of  BUILD and CHECK, and completes any remaining constituent structure.
W97-0302@@ In this paper, we examine thresholding techniques for statistical parsers. Furthermore, their algorithm requires time O(n a) to run just once. Mu l t ip le -Pass  Pars ing  We can use an analogous algorithm for multiple-pass parsing.
W97-0303@@ The correct interpretation of spontaneous poken language poses challenges that continue to fall outside of the reach of state-of-the-art technology. The avenues of exploration made available here CD .E F.c 8 2 Q. Substitutions and transpositions are not allowed in this version of the parser, nor is it possible to set a separate maximum penalty for skipping and for inserting. App ly ing  the  Genet ic  P rogramming Parad igm to Repa i r  There are five steps involved in applying the genetic programming paradigm to a particular problem: determining a set of terminals, determining a set of functions, determining a fitness measure, determining the parameters and variables to control the run, and determining the method for deciding when to stop the evolution process.
W97-0304@@  The task we address in this paper might seem on the face of it rather elementary: identify where one region of text ends and another begins. A separate set of (s, t) pairs were extracted from the WSJ  corpus. This shows the word or relevance score for each feature together with the value of e x for the feature after iterative scaling is complete for the final model.
W97-0305@@  Automatic detection of subject divisions within a text is considered to be a very difficult task even for humans, let alne machines. He also claims o that, unlike function words, the number of instances E of a specific content word is not directly associated with the document length, but is rather a function ~ of how much the document is about the concept exi ~5 pressed by that word. Now let A t be the subset of A that contains only those words that occur somewhere in B.
W97-0307@@  The aim of the work reported here is to construct acorpus of German annotated with syntactic structures (treebank). Here automatic annotation and human supervision are combined interactively whereby annotators are asked to confirm the local 65 G_enma l :  Corpus: IRefCorpus Tes~,ople I J~\] Editor: IThorsten J~\] I -~I i  _,,oa, li E.,t i i O~,o. This uniquely identifies a phrase category.
W97-0308@@  We present here a general design for, and modular implementation of, an algorithm for computing areas of agreement between structurally annotated corpora. Two trees in C t and C r are aligned, if they share the same yield (under the image of ~), ie: (W\[, W;) e Subtrees(C r) and (WE(i), W~t(j)/E Subtrees(C t) Two subtrees are strictly aligned if the above conditions hold and neither tree is a unary branch. Then, no terminal element intervenes between Wi and ts LSD, or between Wj and ts RSD, and the same condition holds of t ~.
W97-0309@@  The purpose of a statistical language model is to assign high probabilities to likely word sequences and low probabilities to unlikely ones. The perplexity V* is related to the log-likelihood by V* : e -~/N ,  where N is the total number of words processed. (6) as a hidden variable model.
W97-0311@@ The optimal way to  analyze linguistic data into its primitive elements is rarely obvious but often crucial. Induce a trial translation model between E and F. 9. First, a bit of notation.
W97-0313@@  Semantic information can be helpful in almost all aspects of natural language understanding, including word sense disambiguation, selectional restrictions, attachment decisions, and discourse processing. For example, consider the sentence: I bought an AK-~7 gun and an M-16 rifle. Some applications might require strict cat3Note that some of these words are not nouns, such as boardedand U.S.-made.
W97-0316@@ It is well known that a sentence in Chinese (or several other oriental languages) consists of a continuous string of characters without delimiting white spaces to identify words. We believe the rules we use for Step B, though 142 Rule E Examp Ies: czz  . 147 (a) TREC-5 Chinese Query #Z8: The Spread of Cel lu la r  Phones in China ~ J ~ ~ N ~
W97-0317@@  Ambiguity is the most specific feature of natural anguages, which sets them aside from programming languages, and which is at the root of the difficulty of the parsing enterprise, pervading languages at all levels: lexical, morphological, syntactic, semantic and pragmatic. 2 Ex is t ing  Mode ls  o f  A t tachment  Attempts to resolve the problem of PP attachment in computational linguistics are numerous, but the problem is hard and success rate typically depends on the domain of application. A database of quadruples of the form (configuration, v,n,p) was then created.
W97-0318@@Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause. 2 S ta t ive  and  Event  Verbs  Stativity must be identified to detect temporal constraints between clauses attached with when. For example, show appears primarily as a state.
W97-0319@@  Natural language information extraction (IE) systems take texts containing natural anguage as input and produce database templates populated with information that is relevant to a particular application. Hobbs, Jerry R., Douglas E. Appelt, John Bear, David Israel, Megumi Kameyama, Mark Stickel, and Mabry Tyson. 171 \[I Test Set 1 Test Set2 Test Sets land2 Uniform I 2.
W97-0320@@  Temporal information is often a significant part of the meaning communicated in dialogs and texts, but is often left implicit, to be recovered by the listener or reader from the surrounding context. TodaysDate is a representation of the dialog date. resolve_deictic(DT, RF): resolves the deictic term DT with respect o the reference frame RF.
W97-0322@@  Statistical methods for natural language processing are often dependent on the availability of costly knowledge sources uch as manually annotated text or semantic networks. The M-step makes maximum likelihood estimates of the model parameters using the sufficient statistics from the E-step. Suppose that we have N observations in a sample where each observation has q features.
W97-0323@@ Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. George A. Miller, Ed. Ron Kohavi and George H. John.
W97-0403@@ Traditional grammar, at its origin highly prescriptive, was aimed at written sentences, and completely ignored all characteristics of spoken language. D iscourse  S t ructure  One type of pragmatic information relates to signaling discourse structure. I gave it to you on Monday, yeah, probably on Monday the 27th.
W97-0407@@These systems accept speech and text input, and are trained using an example based approach. In ference  of  Subsequent ia l  T ransducers  The use of SSTs to model limited domain translation tasks has the distinctive advantage of allowing an automatic and efficient learning of the translation models from sets of examples. A statistical approach to machine translation.
W97-0408@@  In this paper we describe the head transducer model used for translation in an experimental English-to-Mandarin speech translation system. r~) of w into left and right relations 2 2 2 2 ( r l . "A Statistical Approach to Machine Translation".
W97-0503@@  Some people have disabilities which make it difficult for them to speak in an understandable fashion. E.g., verb frame information isassociated with each verb. A lexical database for English.
W97-0506@@  The work described in this paper concerns the communication eeds of people who cannot speak because of motor disabilities. We have discussed the limitations of the e~ciency of prediction, and introduced the idea of cogeneration which combines free text entry with fixed text associated with templates. % at a menu size of 8.
W97-0508@@  This paper briefly overviews a project whose longterm goal is the development of a "writing tutor" for deaf people who use American Sign Language (ASL). 5 Account ing  for  the  L2  Acqu is i t ion  Process  There are several reasons why a model of second language acquisition is necessary. In C. Baker and R. Battison, editors, Sign Language and the Deaf Community.
W97-0601@@ Interactive spoken dialogue systems are based on many component technologies: speech recognition, text-tospeech, natural language understanding, natural language generation, and database query languages. DR A8: Do you want o leave between 6 and 9 p.m. Each scenario execution has a corresponding AVM instantiation i dicating the task information requirements for the scenario, where each attribute is paired with the attribute value obtained via the dialogue. Thus n provides a basis for comparisons across agents that are performing different tasks.
W97-0801@@  EuroWordNet is an EC-funded project (LE2-4003) that aims at building a multilingual database consisting of wordnets in several European languages (English, Dutch, Italian, and Spanish). it should be possible to store domain-labels for non-English meanings, e.g: all Spanish bullfightmg terms should be linked to ILl-records with the domain-label bull-fighting. While the equivalence r lations match, the hyponymystructure does not (situation 2 above).
W97-0803@@  For most natural language processing (NLP) systems, thesauri comprise indispensable linguistic knowledge. For example, if the class codes of Kanzi Ks and K s are ~1,  c~2} and {c31 , c32 ,c~3} respectively, then a word including K, and K~ is assigned the codes {Ctl,Cs2,C31,C32,C33 }. In Proceedings of A CM SIGIR95, pp.
W97-0806@@  Text categorization (TC) is the classification ofdocuments with respect to a set of one or more pre-existing categories. In a closer look,and given three sets of N terms, M documents and Lcategories, the weight vector for document j is (wdl.l,Wd2j ..... wdNl) and the weight vector for category k is (WC-lk, WC2k-,. Let us take a look to one example.
W97-0808@@The selectional preferences that predicates have for their arguments provide useful information that can help with resolution of both lexical and structural ambiguity and anaphora s well as being important for identifying the underlying semantic roles in argument slots. Additionally a third constraint was added (D). .~  ~xmple  nouns undw lhe node In the cut ~,,.~t, .
W97-0809@@ Customizing information extraction systems across different domains has become an important issue in Natural Language Processing. The Tokenizer and the Preprocessor a e designed to identify some special categories such as e-mail address, phone number, state and city etc. ADD.RELATION is to add a relationship between two objects.
W97-0811@@ Any natural language processing system treating anything beyond the most restricted omains is confronted with the problem of distinguishing between uses of polysemous words. Indeed looking back at the example my own parents stuck together even if we retrieved the multi-word expression meaning it will be difficult o decide which translation to choose with existing dictionary indications s. 3 Multi-words expressions i clude ldtomattc expression (to sweep something under the rug), phrasal verbs (to spa up), or compounds (warmng hght) 4 Or some other derwed tag set. A simple Rule-Bases Part of Speech Tagger.
W97-0813@@ Sense disambiguation of a given word occurrence in a specific context (hereafter WSD) requires appeal to a wide typology of cues, ranging from syntactic subcategorization to lexico-semantic nformation and subject domain. Const ra in ts  on d i s t r ibut iona i ly -based  WSD In the example illustrated above, nouns stand in the same syntactic relation to the verbs. Consider the target affluire-acqua_l/S flow-water.
W97-0901@@  Fast and accurate name recognition products are only now coming onto the market. A Survey of Multilingual Tezt Retrieval. For example, among the errors we have encountered, an MT system failed to recognize a person name "Mori Hanae" in kanji characters, segmented it into three words "mori," "hana," and "e" and translated them into "forest," "England," and "blessing," respectively.
W97-0907@@ In the 1.  release of the Java Developers Kit(TM), a wide selection of text processing and internationalization interfaces have been added to the base Java package 1 making the package usable for multilingual 1A few pointers to online Java resources. Kb modem connection as a low end client.) 3 C l ient /Server  Arch i tec ture  In designing a distributed application several decisions can be built into the architecture of the product or left as runtime decisions.
W97-0910@@ The basic idea of this paper is simple and uncontroversial. 6 Conc lus ions  and  fur ther  d i rec t ions  We have demonstrated that it is practically feasible in the case of sufficiently close languages to generalize an existing grammar for one language to produce a grammar which, through the setting of a single parameter, becokes valid for either language. (Note that in general a word will have more than one lexical entry).
W97-1002@@  An increasing amount of information is available in the form of electronic documents. In S. Wermter, E. Riloff, and G. Scheler, editors, Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing. In Proceedings of the Eleventh International Conference on Machine Learning, pages 343-351, New Brunswick, N J, July.
W97-1008@@ What defines a word when there are no spaces in a written language Words, as they are known in English and other western languages, are the basic units of recognition i  most CSR systems, but when a language is written as a string of characters with no white space, how does one go about specifying the fundamental units that must be recognized Mapping onto English-style words is one solution, but an artificial one, and may hide natural characteristics of Japanese that can be important in recognition. 5 Emergence  o f  un i t s  One of the exciting things about this study was the emergence of units that are contracted in fast and casual speech. (C) ukagawashite $ itadakereba $ cause to humbly go if I could receive the favor of If you would allow me to go ...  omoimasu $\[I\] think (D) renraku shinakereba to $ omotte contact if \[I\] dont COMP thinking Ive been meaning to get in touch \[with you/him...\]  (E) sorosoro $ kimenakereba naranai $ soon if \[I\] dont decide it wont do \[I\] have to decide soon .
W97-1010@@Unfortunately, from a Natural Language Processing perspective, SCFGs are not appropriate grammars to learn. K. Lari and S. J. In future work we will atOsborne 64 Briscoe 86 tempt o rival the state-of-the-art through full lexicalisation and utilising bracketed training material.
W97-1011@@ In this paper, we explore the concept of a Differential Grammar  and apply it to the problem of grammar checking in the sense that it is used on the box your WYSIWYG word processor came in! Another factor which causes evere problems with  i ts/ i t s   is the extreme sensitivity of its differential grammar to contexts. University of Pennsylvania B. E. Wampler.
W97-1013@@Therefore, it is necessary to develop methods that, on the one hand, reduce the space of analyses, as much as possible prior to disambiguation, and on the other hand, reduce the sizes of grammars used for disambiguation. ExplanationBased Generalization: A Alternative View. And 2) the set of subtrees associated with a learned PA-SSF is assumed complete, i,e.
W97-1016@@A central issue in natural language analysis is structural ambiguity resolution. This software can be obtained from f tp : / / cs l i . This poses a serious problem for the MVD metric.
W97-1102@@ Dialectologists frequently speak of the range of dialects they describe as a "continuum", 1 which suggests a need t0 supersede the inherently discrete method of isogl0sses. In Christopher Moseley and R.E. 6 Future  D i rec t ions  We should like to extend this work in several directions.
W97-1107@@ In standard models of phonology, the phonological representation f a word is understood to be a hierarchical structure in which the phonological material (features and/or phonemes) is organized into syllables, which are in turn organized into feet, prosodic Words and intonation phrases. 1) We included every single form 1 193 in the dictionary, including proper nouns, no r 164 matter how foreign or anomalous it might appear p 157 to be, because we have the working hypothesis m 152 that low probabilities can explain the poor v 152 productivity of anomalous patterns. The sequencing constraints described by the original gramn~ar (for example, the requirement that an onset be followed by a rhyme and not by another onset) are enforced by tagging some nodes for the type of e~lement which must succeed it, in a fashion reminiscent of categorial grammar.
W97-1304@@This phenomenon causes considerable problems in natural language processing systems. Ext rac t ion  o f  Japanese  Zero P ronouns  and  The i r  Antecedents  The method to extract Japanese zero pronouns and their English equivalents consists of the following steps 1. of referential elements of \[ \[ \[ \[lntrasentential \]l \] I Deiitic I I zero ha \[ ga \] o \[ n,  \[ misc  Psve I you huit mi pron .
W97-1305@@ The interpretation f anaphoric expressions is known to be a difficult problem. Hence, the overall time complexity of the approach amounts to O(q2(n + log(q)) + s). The computation of the vectors t~d(vi,vo), which characterize the readings in which vi arbitrarily dominates vo, denotes the basic case.
W97-1307@@ Anaphora resolution is a component technology of an overall discourse understanding system. In E. Roche and Y. Schabes, editors, Finite State Devices \]for Natural Language Processing. In J. Groenendijk, T. Janssen, and M. Stokhof, editors, Formal Methods in the Study of Language.
W97-1311@@But coreference involving events, expressed via verbs or nominalised verb forms, is also common, and can play an important role in practical applications of natural language processing (NLP) systems. In S. Botley and T. McEnery, editors, Discourse Anaphora and Anaphor Resolution. They may be referred to through nominalised forms (resignation i  3 above) or through infinitival forms in control sentences (second sentence in 3).
W97-1312@@ Annotated anaphoric links in language corpora play an important role in teaching and research. The candidate with the highest overall score after stages d) and e) will be picked up as the most likely antecedent. In S. Botley, T. McEnery (Eds) "Discourse Anaphora nd Anaphor Resolution".
W97-1502@@  In a language understanding system where full, linguistically-motivated analyses of utterances are desired, the linguistic analyser needs to generate possible semantic representations and then choose the one most likely to be correct. 3 D isc r iminant -Based  Tra in ing  Many of the properties extracted from QLFs can be presented to non-expert users in a form they can easily understand. 2 Representat iona l  Issue.,~ In the version of QLF output by the CLEs analyser, content word senses are represented as predicates and predicate-argument relations are shown, so that selecting a single QLF during disambiguation e tails resolving content word senses and many structural ambiguities.
W97-1503@@  The growing number of applications exploiting NLP techniques i bringing about a shift from an artisan attitude with respect o NLP towards the need for more sophisticated solutions and tools (Linguistic Engineering, LE). Par t i c ipants  The testing group consisted of eight people from our department. Donald A. Norman and Stephen W. Draper.
W97-1506@@We begin with a brief overview of the ConTroll architecture as shown in Fig. 1 before focusing on the aspects relevant o large scale grammar development. ru le  ( loc ,  ca t  ,cat) .
W97-1509@@ Large-scale development systems for typed feature~ based grammars have typically oriented themselves towards parsing, either ignoring generation entirely (the usual case), or assuming that generation can be achieved for free by using a bi-directional control strategy with a semantically, rather than phonologically, instantiated query. The semantic head of chain rule vp matches (c), to produce a mother, (d), which must be further linked, and a non-head daughter, (e), which is recursively generated by using the lexical entry for "john." where MorphW is the result of applying N to W. For most grammars, this code can be heavily optimized by peephole filtering.
W97-1511@@  One of the essential tasks to realize an efficient natural anguage processing system is to construct a broad-coverage and high-accurate grammar. That is, how easy the category Cat  appears under a certain environment (l, r). 78 2 The  F ramework  o f  Grammar  Deve lopment  The proposed framework is composed of two phases: partial grammar acquisition and grammar refinement.
W97-1513@@  Hdrug is an environment to develop grammars, parsers and generators for natural anguages. In R. Tamassia nd I.G. Adding other ALE grammars i trivial.
W97-1514@@Genera l i t ies  Natural language processing is nowadays trongly related to Cognitive Science, since linguistics, psychology and computer science have to collaborate to produce systems that are useful for man-machine communication. V isua l i s ing  the Chart In the Chart interface, words are separated by nodes, numbered from 1 to numberOfWords + 1. On the  use of  the  Envy /Manager  source  code manager  to mainta in  the  syntact i c  ru les  base.
W98-0203@@Coreference is in some sense natures own hyperlink. F iner  Gra ined  Ana lys i s  o f  the  Documents  The fact that two entities coexisted in the same sentence in a document is noteworthy for correlational analysis. CogNIAC: A Discourse Processing Engine.
W98-0301@@The automatic identification of discourse segments and discourse markers in unrestricted texts is crucial for solving many outstanding problems in natural language processing, which range from syntactic and semantic analysis, to anaphora resolution and text summarization. (deal with parenthetical information) 5. if COMMA E status A markerTextEqual(i,",") A 6. As example (7) shows, taking the first comma s boundary of the parenthetical unit would be inappropriate.
W98-0303@@ The development of Electronic Essay Rater (e-rater). Gournay S,M, France: ilTT International. In addition, from a practical standpoint.
W98-0304@@ In text, discourse markers signal the kind of coherence relation holding between adjacent ext spans. For instance, als in its simultaneous meaning imposes the constraint E(e~),E(e~)_S, which can be realized by all grammatical tenses that meet this constraint. 1 Markers  and  Akt ionsar t /aspect  Aspect is traditionally taken to have two components, the noninherent grammatical features, and the inherent lexical features.
W98-0307@@ We are interested in a formal description of the document structure of scientific articles from different disciplines. Ex-C Ex-O-C Ex-E Ex-R Own conclusions/ claims Other conclusions/ claims Own evaluation methodology Own (numerical) results In our corpus study, we found that  three types of utterances (prompts, repetitions and summaries) were consistently used to signa/control shifts. I t  has often been s ta ted  that  discourse is an inherently collaborative process.
W98-0319@@ The structure of a discourse is reflected in many aspects of its linguistic realization. George,own University Press, Washington, D.C. J. R Searle. Predicting dialogue acts for a speech-to-speech translation system.
W98-0501@@ The aim of this paper is to define a dependency grammar framework which is both linguistically motivated and computationally parsable. A dependency parser for English. Currently, the speed of the parsing system I is several hundred words per second.
W98-0502@@ Coordination is a long standing problem for linguistic theories, because of its particular aspects which do not fit well with the dominance-based character of the vast majority of paradigms. Ro~ J. R., Gapping and the order of constituents, inBierwi~h M. and Heidolph M. So, we have CONJ-V for verbs, CONJ-N for nouns, and so on.
W98-0503@@997\] we have introduced aclass of formal grammars. Ng(Tr) denotes the maximum from {Ng(u, Tr);u E Tr}. Let Tr  be a D\ ] : I  tree.
W98-0511@@ Our team has been working with dependency grammars for more than twenty-five years (Courtin 73). Grammar example: We use the following categories: Determiner (D), Noun (N), Adjective (A), Verb (V). Parsing with dependency re lat ions  The linguistic model we use for dependency is inspired by the Tesni&e model (Tesni~re 59), which we will recall shortly in order to define precisely our terminology.
W98-0604@@  Although, nominalizationQ are very common in written text, the computational linguistics literature provides few systematic accounts of how to deal with phrases containing these words. David E. Johnson and Paul M. Postal. Therefore, the possibility of filling temporal slots from DET-POSS and N-N-MOD positions hould cause no conflicts for appointment.
W98-0605@@  The role of dictionary is undoubtedly important in Natural Language Processing (NLP). Collect many example phrases of ~A NO B", 2. Consequently, we select T (A), and split the node D into three subnodes.
W98-0606@@Yet this means that they must be able to handle vagueness, incompleteness or even ungrammaticality as these phenomena tend to be associated with language use under specific external constraints as eg in situations concerned with database access. Third, the query and its underlying concepts have to be compared to related queries and concepts: I.e. Compos i t iona l i ty  and non-compos i t iona l i ty  Generally, the parsing process in the TAMICP query interpretation component for the Austrian scenario relies strongly on the hypothesis that the semantics of a phrase (represented for example in quasi-logical form) can be derived by composition of the QLFs of the parts the phrase is composed of.
W98-0608@@The idea of  WYSIWYM editing is to provide an interface to a knowledge base that can be used easily by an author who is a domain expert but not necessarily an expert in knowledge representation. Power, R. and Scott, D. Multilingual Authoring using Feedback Texts, Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 98), Montreal, Canada. name (name I).
W98-0610@@ This paper describes a methodology for identifying significant topics in edited documents such as newspaper articles. In Information e.~traction: A multidisciplinary approach to an emerging information technology, edited by Maria Teresa Pazienza, pp. The finding probably will support hose who argue that the U.S. should regulate the class of asbestos including crocidolite more stringently than the common kind of asbestos, chrysotile, found in most schools and other buildings, Dr. Talcott said.
W98-0701@@  Identification of the right sense of a word in a sentence is crucial to any successful Natural Language Processing system. sentences, o we cannot make a practical comparison. Ifno match is found, ie the word has not previously occurred in the discourse, e(POS,SN) is set to 1 for all senses.
W98-0702@@ The verb have is semantically ambiguous. For e~Ample, The patient had an enema describes an event, but WordNet lists enema as artifacl; before act. C. Cardie and N. Howe.
W98-0704@@A team lead by Prof. George Miller aimed to create a source of lexical knowledge whose organization would reflect some of the recent findings of psycholinguistic research into the human lexicon. Expanded terms are generally taken from a thesaurus. Disambiguating oun grouping with respect o wordnet senses.
W98-0705@@  Text retrieval deals with the problem of finding all the relevant documents in a text collection for a given users query. We are indebted to Ren~e Pohlmann for giving us good pointers at an early stage of this work, and to Anselmo Pefias and David FernAndez for their help finishing up the test collection. Indexing by synsets o 2.
W98-0706@@ The task of Supervised Machine Learning can be stated as follows: given a set of classification labels C, and set of training examples E, each of which has been assigned one of the class labels from C, the system must use E to form a hypothesis that can be ~used to predict he class labels of previously unseen examples of the same type \[Mitchell 97\]. Text classification systems are used in a variety of contexts, including e-mail and news filtering, personal information agents and assistants, information retrieval, and automatic ndexing. A simple rule-based part of speech tagger.
W98-0709@@and Introduction One of the main issues in last years as regards NLP activities is the increas ingly  fast development of generic language resources. Biblograf S.A. Barcelona, Spain. and b) the limitation of the genus sense disambiguation techniques applied (ie, \[Bruce t al.
W98-0715@@However, like other large-scale knowledge-base systems or machine readable dictionaries (MRDs), WordNet contains massive ambiguity and redundancy. First, domain categories are mixed in with ontological categories (eg co , ,pet i t ion  and body verb categories). FASTUS: A Cascaded Finite-state Transducer for Extracting Information from Natural-language T xt, In Finite-state Language Processing, E. Roche and Y. Schabes (eds.
W98-0718@@We demonstrate in this paper that, as a rich semantic net, WordNet is indeed a valuable resource for generation. Moore, and R.A. Whitney. The main problem we discussed is adapting WordNet o a specific domain.
W98-0802@@  The availability of adequate tools for the creation, maintenance and use of multimodal spoken language corpora is an important instrumental goal for spoken language research, whether this research is motivated primarily by the desire to gain a better understanding of the mechanisms of spoken communication r by the wish to develop practical applications such as multimodal interfaces for humanmachine interaction. Speaker  Pane, t ranscr ipt ion in score format  : I Ikameron ~n I 2 o. F igure  11 . The Full Transcript Window displays the transcription i original format.
W98-0905@@Phonotact ic  Descr ipt ion In recent years, phonology  partly under the influence of computational models  has moved away from procedural, rule-based approaches towards explicitly declarative statements of the constraints that hold on possible phonological forms. Although only experiments for l-level transducers with labels a E I have been carried out so far, the approach will be extended to the general case, permitting multi-tier phonological description. Ordinarily, the S-notation (sometimes ~) is also used for the input of strings x E I*, such that ~ maps S  The term FSA is taken to refer to n-level transducers, where the input alphabet consists either of individual symbols a E I or of strings x E I*.
W98-1002@@Development of such a capabi l i ty for Arabic involved meeting some new challenges. A token cannot be both simultaneously. This allowed us to efficiently 9 I ISO 8859-6 Text I "~.
W98-1005@@ It is not trivial to write an algorithm for turning Translators must deal with many problems, and one of the most frequent is translating proper names and technical terms. They required that each English sound 36 ((AE N T OW N IY ON) (! We pursued a general solution, replacing each in37 stance of an English vowel in our training data with e II one of three symbols, depending on its position in AA the word.
W98-1007@@  In Arabic, as in other natural languages, the two challenges of morphological nalysis are the description of 1) the morphotactics and 2) the variation rules. Thus the English morphemes , ed and ing represent the concatenations \[s\], \[e d\] and \[i n g\] respectively. A prosodic theory of nonconcatenative morphology.
W98-1009@@ This paper describes a computer system for Arabic morphology that employs a new, faster algorithm to find roots and patterns for verb forms and for nouns and adjectives derived from verbs. Here the "S" in the word "T(a)Sryf" stands for the letter"o,," since there is no corresponding letter in English for this letter. the root of~.ak, is ~.~ 69 word root pattern stem Present 0 b_l_-_.. a:-J.: O J ;  ot, ja.
W98-1011@@  Modem Hebrew (MH) poses some interesting problems for the grammar designer. Preferring either of the two analyses, in the context of HPSG, boils down to deciding whether it is the determiner o  the noun that heads a nominal phrase. Since definiteness i a feature of phrases, inherited from the lexical head, DEF is a head feature, appropriate for all nominals.
W98-1012@@  A text generation system generally includes two main components: content determination (that decides what to say) and surface realization (that decides how to say it). Using argumentation to control lezical choice: a unification-based implementation. We keep the feature de f in i te  yes for this purpose.
W98-1102@@ Today, corpora are considered to be indispensable to NLP work: they provide information for the creation of other resources (eg, lexicons), enable the gathering of statistics on real-language use to inform theories and algorithms, and provide the raw materials for testing and training. References Hovy, E., and Ide, N. Proceedings of the Work.shop on Translingual Information Management: Current Levels and Future Abilities. 3 Leve ls  of  C o n f o r m a n c e  The CES provides a TEI-conformant Document Type Definition (DTD) for three levels of encoding for primary data together with its documentation (the "cesDoc DTD"): Level 1 : the minimum encoding level required for CES conformance, requiring markup for gross document structure (major text divisions), down to the level o f  the paragraph.
W98-1106@@Conceptual natural anguage processing typically involves case frame instantiation to recognize vents and role objects in text. In other words, the pattern (or case f rame)  did the r ight  th ing . CRYSTAL: Inducing a conceptual dictionary.
W98-1110@@  Part-of-speech (POS) tagging has many difficult problems to attack such as insufficient training data, inherent POS ambiguities: and most seriously unknown words. 2 L ingu is t ic  character i s t i cs  of  Korean  Korean is classified as an agglutinative language in which ~.n eojeol consists of several number of morphemes that have clear-cut morpheme boundaries. For example, ~L.. _-l--g\]" (thanks), which is a morpheme and an eojeol at the same time, is matched "(ZV*N)" (shown in Fig.
W98-1111@@  Language identification is an example of a general class of problems in which we want to assign an input data stream to one of several categories as quickly and accurately as possible. 8 9.3 Words Correct i Incorrect 1. To outline the approach: 1.
W98-1114@@Some of the work was carried out while the first author was a visitor at the Tanaka Laboratory, Department of Computer Science, Tokyo Institute of Technology, and at CSLI, Stanford University; the author wishes to thank researchers at these institutions for many stimulating conversations. A general architecture for text engineering (GATE) a new approach to language R~D. Comlex syntax: building a computational lexicon.
W98-1115@@  Finding one (or all) parses for a sentence according to a context-free grammar equires earch. Towards a uniform formal framework for parsing. We make the same assumption i this paper.
W98-1116@@which required attention to detail and exceptions, and led to the development of data-driven theories and to the use of corpora to model naturally occurring language. A probability space is a triple ~, .T, P, where f2 is the sample space, .T" is the event space and P is a function P : F ~ \[0, 1\]. Second, active and passive uses of the verbs were counted: cases in which usage could not 1)e determined by a simple pattern search w,;re classified by hand.
W98-1117@@  The maximum entropy framework has proved to be a powerful modelling tool in many areas of natural language processing. tags: the percentage of structural tags with the correct value r~ of the REL attribute, bracket ing:  the percentage of correctly recognised nodes, label led bracket ing:  like bracketing, but including the syntactic ategory of the nodes, s t ruc tura l  match :  the percentage ofcorrectly recognised tree structures (top-level chunks only, labelling is ignored). Class-based n-gram models of natural language.
W98-1118@@ Named entity recognition is one of the simplest of the common message understanding tasks. More complete discussions of M.E. A maximum entropy approach to natural anguage processing.
W98-1119@@  We present a statistical method for determining pronoun anaphora. The second half of the paper describes a method for using (portions of) t~e aforementioned program to learn automatically the typical gender of English words, information that is itself used in the pronoun resolution program. 8 Conc lus t ion  and  Future  Research  We have presented a statistical method for pronominal anaphora that achieves an accuracy of 84.
W98-1120@@  For some NLP applications, it is important o identify, "named entities" (NE), such as person names, organization ames, time, date, or money expressions in the text. So, we add a new entity class, position. The result from JUMAN is created based on JUMAN version 3.s output alone 1.
W98-1121@@ For recognizing spontaneous speech, the acoustic signal is to weak to narrow down the number of word candidates. T. Niesler and P. Woodland. Consider the sequence of words "hello can I help you".
W98-1122@@  Traditionally, n-gram language models implicitly assume words as the basic lexical unit. D is tance  Computat ion  and  H ierarch ica l  c lus ter ing  Having set up a feature vector for each word, the similarity between two words is measured using the Manhattan distance metric between their feature vectors. In the first step of the procedure, a set of candidate phrases (unit pairs) o is drawn out of a training corpus T and ranked according to a correlation coefficient.
W98-1123@@ Identification of discourse structure can be extremely useful to natural language processing applications uch as automatic text summarization or information retrieval (IR). Total Precision avg I S.D. If we allow the number of segments, r, to be given, we can apply this to segmentation to pick r segments from N paragraphs.
W98-1124@@Current approaches to automatic summarization employ techniques that assume that textual salience correlates with a wide range of linguistic phenomena. The score s(D) is computed as shown in (3), where wct, , t , . Sentence xtraction as a classification task.
W98-1126@@This paper investigates interactions between collocational properties and methods for organizing them into features. E.g., for both CO and SP collocations, there is one feature for adjectives and one for verbs. AT&T Bell Laboratories Statistical Research Report No.
W98-1207@@ The emergence of new statistical NLP methods increases the demand for corpora nnotated with syntactic structures. Therefore, we normalize the probabilities w.r.t. defines a separate Markov model.
W98-1213@@Of these online newspapers, 73% are in North America. Here is a link that will,.. A recent promotion 13 by A T & T and Residence 2 Inns 7 in the United States 6, for example 3, suggests that business 3 travellers I with young j children use video 3 and audio tapes ~,  voice 3 mail 3, videophones and E-mail to stay 3 connected, including kissing ~ the kids I good night 21 by phone 22.
W98-1218@@ and Motivation In this paper we wish to revisit Zipfs study of the relationship between rank and frequency of various linguistic and social units and constructions. The word frequency effect and lexical access, Neuropsychologia 20:6 615-627 Shannon, C.E. Powers 151 Applications and Explanations of Zipf s Law David M.W.
W98-1223@@ Learning word pronunciation can be a hard task when the relation between the spelling of a language and its corresponding pronunciation is many-tomany. In I6TR~E, information gain is used as a guiding function to compress a data base of instances of a certain task into a decision tree 1. 2 A lgor i thm,  Data ,  Methodo logy  2.
W98-1224@@Rather, they defer investing effort until new instances axe presented. Friendly-neighbouthood sizeand class-prediction strength a~e related functions, but differ in thei~ treatment of class ambiguity. Machine Learning, 10:57-78. van den Bosch and Daelemans 203 Cover, T. M. and P. E. Hart.
W98-1226@@ Statistics are frequently bandied around in NLP, and would seem to be the obvious way to compare competing systems and methodologies. Ambiguity resolution in a reductionistic parser. This parser then attempts only to give the more likely reading(s), but it does not necessarily offer a legal parse.
W98-1231@@ In NLP applications an input text undergoes anumber of transformations until the desired information can be extracted from it. In our calculation of complexity, only length and number  of lexical elements Lre taken into 8,ccoux l t . r replaces the values of the features in the marked WDs by those of FB.
W98-1232@@ Morphological analysis is the basis for most natural language processing tasks. The current system contains 102 two-level rules, accounting for plural formation, eg, cristal (crystal) cristais (crystals), diminutive and augmentative formation, eg, casa (house) casinha (houseDIM), feminine formation, eg, alemao (GermanMASC) alema (German-FEM), superlative formation pag~o (pagan) pananissimo (paganSUP), verbal stem alternation, eg, dormir (to sleep) durmo (sleep-IP-SG-PRES), and derivational forms, eg, forum (forum) \]orense (forensic). A closed-class word lexicon would eliminate these errors.
W98-1239@@This approach is characterized by two facts: (a) the use of corpora nd (b) the use of the notion of distribution instead of the sense of elements. For instance, after the English sequence direc, we only find, in our corpus, one letter t. After direct, we find four letters: /, l, o, and e (directly, director, directed, direction). The structures generated by these elements correspond to a lexical element (the nucleus of the chunk) surrounded by grammatical e ements (words or morphemes, generally a combination of both).
W98-1301@@ It has been recognized for some time that Optimality Theory (OT), introduced by Prince and Smolensky \[24\], is from a computational point of view closely related to classical phonological rewrite systems (Chomsky and Halle \[11) and to two-level descriptions (Kosksnniemi \[21\]). The first application produces a mapping from u:t+hln to u:t+hun. and Fi l l0ns constraints forbid empty nucleus S\[ \] and onset 0\[ \] brackets.
W98-1302@@  A recent publication \[15\] presented a novel way of transforming a context-free grammar into a new grammar that generates a regular language. an is recognized by T when F   is nonempty. For example, for --t the rule D -r aC~.
W98-1306@@Moreover, the automata contain many e-moves (jumps). 2 FSA Ut i l i t ies The FSA Utilities tool-box is a collection of tools to manipulate r gular expressions, finite-state automata and finite-state ransducers (both string-to-string and string-to-weight transducers). Note that we make sure that the transitive closure computation is only performed once for each input state, by memorising the closure/unctior~funct epsilon_dosure(U) ret.m U~u me~o( dos~e( {,,} )) end variant 2: per state In the case of the per subset approach the closure algorithm isapplied to each subset.
W98-1308@@ Regular expressions are a mathematical formalism widely used for many computational tasks, ranging from simple search-and-replace procedures to full-scale implementations of grammars of natural languages. 3 The regular expressions of XFST The standard set of regular expressions, originally introduced by Kleene, has just six syntactic forms: the one-symbol expression c, the empty string expression 0,the empty language expression e, the concatenation A B, the union A I B, and the Kleene closure A*. This 80 mm i i mm nm Bn i i nm i mR U i nm mm i n n mm mm mm mi n njm interface to XFST should be easy to integrate with the generic proof text editor so that, for instance, one could prove properties of regular expressions and edit XFST code by starting from mathematical specifications: A natural language interface also exists for the proof editor Coq (see \[1\]).
W98-1402@@of producing effectively designed pages of information analogous to overviews found in print-based publications such as encyclopediae or magazines. Given all concepts, we may construct the Concept lattice starting from the top concept (th e one that has no superconceptS) ~ and proceed topdown recursively. Since function~ and set-valued functions are binary relations, the encoding of a structured n-tuple i s composed of a set of binary encodings.
W98-1403@@  This paper describes a media-independent, compositional, plan-based approach to representing attributiv e descriptions for use in integrated text and graphics generat!on. Also, if task (A) is not required, it is not necessary t O provide numeric values on the horizontal axis in (d) . A computational model of referring.
W98-1406@@Microplanning involves low-level discourse structuring and marking, sentence boundary planning, clauseinternal structuring and all of the varied subtasks involved in lexical choice, These complex tasks are often modularized and treated separately. Special situations uch as coreference an be easily identified because the lexicon entries ar e grounded in their inputs. Controlling a language generation planner.
W98-1410@@A )erson who explains to another person a technical device or a logical line of reasoning adapts his explanations to the addressees knowledge. The  net utility E of the  application of a production is defined as . Dec la ra t ive  Knowledge Declarative knowledge is represented in terms of chunks in the declar: fe .atFsubsetG ative memory.
W98-1411@@The  Task  This paper presents ome initial experiments using stochastic search methods for aspects of text planning. As Marcu points out, the constraints on linear order only indirectly reflect requirements on the tree (becaus e related facts need not appear consecutively). shiltredge,was,a british designer,fT).
W98-1414@@The set of words from these grammatically heterogeneous classes that can signal coherence relations we call discourse markers. E.g., but can signal a general CONTRAST or a more specific CONCESSION. N is a shorthand for nucleus in the RST sense, S for satellite.
W98-1415@@  An expression is more concise than another expression if it conveys the same amount of information in fewer words. The term, hypotaxis, describes the relation between a dependent element and its dominant element. !1 il I I The patients past medical history is significant for bladder carcinoma1, status post cystectomy with a urostomy tube insertion2, left nephrolithiasis~, status post surgery4, recurrent syncopes, questionable vagovagal6, a neurological workup was negativer, and the EPS was negatives, abdominal aortic aneurysm approximately 5 cmg, high cholesterol10, exertional nginan, past tobacco smoker, quit about one year ago12.
W98-1417@@ The question that we will investigate is the starting point of generation, and we argue that this is NewInfo, the piece of new information exchanged in interaction, with which the mutual context gets updated. s t rateg ies  D ia logue  Manager  mJ~tyse ~luate  re  8 l~om:d  query  In form  i n fo rm Col laborat ive st rateg ies  Task  Manager  "l~oblem solver" | ,, ! " E. Hovy and L. Wanner.
W98-1419@@ Th e problem we address is that of producing efficient descriptions of objects, collections, acti0ns, events, etc. Each entity, e, comes with a context set D(e) including it and its distractors. For a more extensive discussion, see \[25\].
W98-1421@@Additionally, more than two people should be able to participate in the dialogue, a setting we call multi-party. important parts of the dialogue module are the sequence memory, which contains data structures for turns and segments (see figur e 3). S ta tus  P ro toco l  The current status of the negotiation is summarized.
W98-1425@@in order to Support the efficient development of NL generation systems, two orthogonal methods are currently pursued with emphasis: (1) reusable, general, and linguistically motivated surface realization components, and (2) simple, task-oriented template-based techniques. He also wants a confirmation of some of his choices. 3 Shal low Generat ion in TEMSIS  I n  order to tailor-the design of a generation system towards an application, we must account for different levels of granularity.
W98-1427@@ It Is generally agreed that the technology of natural language generation has evolved to a stage where it can feasibly be expected to be found in ,real world, applied systems. In this way, text generation technology isused as a means to presen t complex data entry tasks in a way which is both easy to understand and practical to work with. WYSIWYM editing is a new idea that requires practical testing.
W99-0105@@ This paper explores the idea that the operations of positing and resolving bridging anaphom in NPs with deverbal heads can be successfully guided by considering as possible implicit relations those suggested by the argument s ructures of the correspondink verbs. ._lT~e__nt Advances in Natural Language Processing,. We want t~ identify the texture of a text.
W99-0106@@A COLLECT module determines a list of potential antecedents (LPA) for each anaphor (pronoun, definite noun, proper name, etc.) The use of referential expressions instructuring discourse, l.anguag e and For example, for k -3, t -3.
W99-0108@@ Anaphoric expressions are an important component to generating coherent discourses. The corresponding sentences (also broken into clause, s t) are contained in Example 2. e/se use a definite description; 2.
W99-0202@@ The need to identify and extract important concepts in online text documents i  by now commonly acknowledged by researchers and practitioners in the fields of information retrieval, knowledge management and digital libraries. In relating names to entities, the Zunaid KAZI T. J. Watson Research Center, IBM P.O. A. Bagga and B. Baldwin.
W99-0204@@ A presentation of information content must be adapted to the context. A zero anaphora is encoded by a relational attribute. (e) Replace non-topical anaphoric elements with their antecedents.
W99-0205@@ A noun phrase can indirectly refer to an entity that has already been mentioned. Discussion o f  E r rors  Even if we had a noun case frame dictionary, there are certain pairs of nouns in indirect anaphoric relationship that could not be resolved using our framework. The Old Man with a Wen.
W99-0210@@It is essential to solve the anaphoric relation when a language istranslated into one that marks the pronoun gender. From this structure, a similar one in the target language is generated. The cat i was drinking milk.
W99-0211@@  In this paper we report preliminary work which explores the use of coreference chains to construct text summaries. A. Bagga and B. Baldwin. Alternatively, for event-based topics, chains of event coreference r lations could be used.
W99-0212@@  Search engines have become ubiquitous as a means for accessing information. This relationship is given with a weight of I(wl, w2)/N where N is a normalization constant. In this example one of the entity-based relationships of interest is the identity relationship between U.S. and United States.
W99-0302@@ Many people wish to annotate spoken (li~dogu(~ (:or-I)ora with co(led information. Note that this still leaves coding scheme and tool developers in a better situation than they have been previously, since, given th e willingness to learn XML and XSL, software developers will already be able to use the workbench to implement 16 new coding schemes, coding tools, and display functions. enough to jznak,~ it,  :r:,:h sip, ual is available.
W99-0304@@ Linguistic corpora are now indispensable of speech and language research communities. Surface forms: "hai (yes, yeah, right)", "eto (well, aah, urn)", "e (mnun, yeah)" English corresponding expressions are shown in bracket for reference. M. Meteer and A. Taylor.
W99-0306@@ The Clarity project is devoted to automatic detection and classification of discourse structures in casual, non-task-oriented conversation using shallow, corpus-based methods of analysis. Intracoder agreement of over 90% was tested by having the tagger e-tag three dialogues everal weeks after first tagging them. A discourse coding scheme for conversational spanish.
W99-0307@@ Empirical studies of discourse structure have primarily focused on identifying discourse segment boundaries and their linguistic correlates. The lower panel displays the text read by the annotator up to time t and only the first sentence that immediately follows the labeled edus. Then judges met and agreed on a particular segmentation.
W99-0311@@ Work on summarisation has suffered from a lack of appropriately annotated corpora that can be used for building, training and evaluating summarisation systems. In Robert Norman Oddy, S. E. Robertson, C. J. van Rijsbergen, and P. W. Williams, editors, Information Retrieval Research, pages 172-191. In G. Kempen, editor, Natural Langua9 e Generation: New Results in Artificial Intelligence, Psychology and Linguistics, pages 85-95, Dordrecht.
W99-0408@@  In order for any human language tutor to be effective, he or she must have an accurate picture of the students language acquisition status. 3 Mode l ing  Theor ies  o f  Cogn i t ive  Skill and Second Language Acqu is i t ion  We see our model as representing the users location along the path toward acquiring written English as a second language. In Wayne D. Gray, William E. Hefley, and Dianne Murray, editors, Proceedings of the International Workshop on Intelligent User Interlaces, pages 175-182, Orlando, Florida, January 4-7.
W99-0410@@  EVALING is a Leonardo da Vinci project funded by the European Union, involving four European laboratories. TAYLOR C., JAMIESON J., EIGNOR D. and KIRSCH I. Each partner is working on his own language and building specific tests (at the present: i Association pour le traitement automatique des langues (ASSTRIL) for French, Consorzio Lexicon Ricerche from the University of Salerno, for Italian & Piidagogische Hochschule Karlsruhe and Universit~t Mtinehen for German.
W99-0411@@Research at Educational Testing Service (ETS) has led to the recent development of e-rater, an operational automated essay scoring system. 65 GDF E Score Score Mean S.D. For one group, prompt made a difference.
W99-0602@@Access to this type of corpora raises a number of questions: Do they make new applications possible Can methods developed for handling bilingual texts be applied to multilingual texts More generally: is there anything to gain in viewing multilingual documents as more than just multiple pairs of translations However, the case for trilingual and multilingual alignments i not as clear. Although this explanation for the failure of the t r ia l  approach clearly contradicts our initial intuitions, we cannot entirely reject it. In practice, this will amount to aligning the elements of C (in our case, sentences) with individual "couples" of the XAB relation: whenever we align some sentence c E C with a sentence a E A, then this implies that c must also be aligned with all other sentences to which a is related within the transitive closure of XAB.
W99-0606@@ Boosting is a machine learning algorithm that has been applied successfully to a variety of problems, but is almost unknown in computational linguistics. References E. Brill and E Resnik. In Proceedings of COLING-A CL.
W99-0609@@However, there are circumstances, particularly involving domainspecific text, where WordNet does not have sufficient coverage. mlnlvans car  compact limousines jeep wagon cab sedan coupe hatchback trailer campers i craft ~sse l  yacht b~arges motorcycle I motorbike wagon I cart Occupations: person worker I editor technician riter journalist columnist commentator novelist biographer intellectual scientist I sociologist t--7 chemist \[____L_~ biochemist physicist  scholar  . A corpus-based approach for building semantic lexicons.
W99-0611@@  Many natural language processing (NLP) applications require accurate noun phrase coreference r solution: They require a means for determining which noun phrases in a text or dialogue refer to the same real-world entity. I. Dagan and A. Itai. S. Lappin and H. Leass.
W99-0612@@  The ability to determine the named entities in a text has been established as an important task for several natural language processing areas, including information retrieval, machine translation, information extraction and language understanding. Configuration (e) shows contrastive performance when using standard continuous EM smoothing on the same data and data structures. In this case there is a possible bootstrapping path using alternating left and right context o expand coverage to new contexts, but this tends to be not robust and wa s not pursued.
W99-0613@@ Many statistical or machine-learning approaches for natural anguage problems require a relatively large amount of supervision, in the form of labeled training examples. A. Blum and T. Mitchell. E. Riloff and J. Shepherd.
W99-0614@@ The problem of prepositional phrase (PP) attachment ambigu!ty is one of the most famous problems in natural anguage processing (NLP). If one adds even more information to attachment statistics (e. g., the position of NP candidate mothers like np2 for the second closest NP) the attachment data for the annotations in this paper becomes too sparse. c c, Ic, I where Ci is the set of all subsets of I with ni~t elements that contain ri.
W99-0615@@The models are based on Markov assumptions, which make it possible to view the language prediction as a Markov process. As we know, it is not always true and this first-order Markov assumption restricts the disambiguation i formation witlfin the first-order context. 5 Exper imenta l  Resu l t  We have tested our technique through part-ofspeech tagging eXperiments with the Hidden Markov Models which are variously lexicalized.
W99-0617@@ For recognizing spontaneous speech, the acoustic signal is to weak to narrow down the number of word candidates. E Heeman and J. Allen. T. Niesler and P. Woodland.
W99-0619@@ The production of natural, intelligible speech remains a major challenge for speech synthesis research. Development of a stemming algorithm. Speech Communicatio n, 11:513-546, January.
W99-0621@@  Shallow parsing is studied as an alternative to full-sentence parsers. In ACL93 workshop on the Acquisition of Lexical Knowledge from TexL Z. S. Harris. Let ty be the activation value for the yes-bracket target and t N for the no-bracket target.
W99-0623@@  The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems. The score for rri is ~ IS# N Sil, where j ranges #i over the candidate parses for the sentence. F-measure is the harmonic mean of precision and recall, 2PR/(P + R).
W99-0626@@One important task is the identification of so-called cognates, token pairs with a significant similarity between them, in bilingual text. The relation between e and a can be seen for example in pairs like (sida,side) and (lina,line). They include comparisons at the level of characters and n-grams with dynamic length.
W99-0627@@  and  Prob lem Statement  Routing submitted papers, abstracts or grant proposals to qualified reviewers i a central task of the academic enterprise, and a remarkably difficult one. D. Lewis and W. Gale. (Coauthor +Coauthor T) (4) where A is a weighting factor between the contributing sources of similarity.
W99-0628@@ Structural ambiguity is one of the most serious problems that Natural Language Processing (NLP) systems face. (1.a) John has a telescope. for input spaces of dimension d, where n is the number of units of the network.
W99-0629@@  When dealing with large amounts of text, finding structure in sentences i  often a useful preprocessing step. In E. Ejerhed and I. Dagan, editors, Proc. the preposition of a PP  chunk.
W99-0631@@ Knowledge of the constraints a verb places on the semantic types of its arguments (variously called selectional restrictions, selectional preferences, electional constraints) is of use in many areas of natural anguage processing, particularly structural disambiguation. estimated frequency treq (c, v, r) is given as follows. Hiding a Semantic Class Hierarchy in a Markov Model.
W99-0632@@Verbs which display the same diathesis alternations-alternations in the realization of their argument structure-are assumed to share certain meaning components and are organized into a semantically coherent class. % for category A verbs. This resulted in 154 verbs which take the NP-V-NP-NP frame, 135 verbs which take the NP-V-NP-PPw frame and 84 verbs which take the NP-V-NP-PPj~,r frame.
W99-0633@@The tagger does not use hand-crafted rules or prespecified language information, nor does the tagger use external lexicons. A Simple Rule-Based Part of Speech Tagger. Acknowledgements Above all, I would like to thank my advisor, Prof. Gunnel K~illgren, who always helped and encouraged me.
W99-0702@@The paper presents an approach to segment a corpus into words, based on entropy. Alias Separator Recall r io.n,  t flo.~t t0 BL 88. Detect ion  o f  Separators  To find a separator, we are looking for a strong boundary to serve as a beginning or end of the separator.
W99-0703@@ The Expedition project is devoted to fast "ramp-up" of machine translation systems from less studied, so-called "low-density" languages into English. 1 I Morphological Analyzer Generation 1 I Content for Morphological Analyzer Engine (lexicons, morphographemic rules) 1 Lrco_o.w. \] I I  _ e t e # \] Note that since we require that the analyses contain the verbal lemmas, a number of rules deal with the lemma marker +at  .
W99-0704@@Successful attribute elimination leads to compact datasets, which possibly increase classification speed. Resu l t s  The results show, first of all, that the compression rates obtained with BSE (average 34. Feature subset selection using a genetic algorithm.
W99-0705@@Thus, TBL has proved very useful, in many different ways, and is likely to continue to do so in the future. The use of the un ique/1 wrapper in the conditions of the rules has the effect hat a rule will trigger only if the assignments of tags to words in the relevant surroundings ar e non-mnbiguous. De le t ion  rules dictate when a feature value should be removed from a word.
W99-0706@@ An important level of natural language processing is the finding of grammatical relationships uch as subject, object, modifier, etc. i t /  spark le /spark le ,  htm. Direct performance comparisons, however, are elusive, since parsers are evaluated on an incommensurate true bracketing task.
W99-0707@@Shallow parsing is an important component of most text analysis systems in applications uch as information extraction and summary generation. In E. Ejerhed and I. Dagan. A simple rule-based part-ofspeech tagger.
W99-0708@@Identification of Noun Phrases (NPs) in free text has been tackled in a number of ways (for example, \[25, 9, 2\]). Although we do not go into the issues here, estimation of rules missing from such g-rammars i  different from estimating rammars 65 ab initio. C is a constant ensuring that the prior sums to one; F is the set of features used to describe categories: \] r \] is the length of a DCG rule r seen f(r) times.
W99-0807@@In this paper, we describe work in progress on a corpus-based grammar tutor. Otherwise, the students may get confused, and as a consequence frustrated. The reasons for this is that both CALL and our prese~!t aims are about computer-based language-related training and that CALL is a large and growing research area.
W99-0901@@ We describe here an approach to inducing selectional preferences from text corpora. Crucially, we make the mixing parameter, e, a function of the total count for the state. Generalizing case frames using a thesaurus and the MDL principle.
W99-0902@@ The objective of this paper is to analyse the applicability of statistical and learning methods to automated grapheme-phoneme alignment in Japanese, without reliance on pre-annotated training data or any form of supervision. Consideration of lexical context for a given tuple (g,Pl is four-fold, made up of the single character immediately adjacent o g in the graphe~ s t~ and single syllable immediately adjacent o p in the phoneme string, for both the left and right directions. Const ra in t -based  a l ignment  prun ing  The final step in alignment is to disallow all alignments (PSseg)-(GSseg) which contravene any of the following phonological constraints, applicable to grapheme segmentation ("G"), phoneme segmentation ("e"), and/or grapheme-phoneme alignment ("G-P"), respectively: (Pl) A demarkation i script form indicates a segment boundary, except for the case of kanjihiragana boundaries.
W99-0904@@ Development of electronic morphological resources has undergone several decades of research. Acknowledgements I thank two anonymous reviewers for useful comments on a first version of this paper. M. Hafer and S. Weiss.
W99-0905@@ Choosing the correct translation of a content word in context, referred to as "translation disambiguation (of content word)", is a key task in machine translation. I. Dagan and A. Itai. For simplicity we employ the sliding window approach where neighboring n words are judged to be the context.
W99-0909@@  In this paper we discuss a potential solution to two problems in Natural Language Processing (NLP), using a combination of statistical and symbolic machine learning techniques. In Daniel N. Oshershon and Howard Lasnik, editors, An Invitation to Cognitive Science: Language, volume 1, pages 199-241. ex ( \ [a ,  dog, called,  the,  fish ,  a,  small ,  ugly ,  desk \ ] ) .
X93-1018@@ In evaluating the state of technology for extracting information from natural language text by machine, it is valuable to compare the performance of machine xtraction systems with that achieved by humans performing the same task. Because by the time of the 24 month test the later starting analysts had as much or more practice coding templates as did the early starting analysts at the time of the 18 month test, it is reasonable to assume that their O CC CC UJ t  Z UJ o tU a . 70 60 i I  ALL ANALYSTS 5TH ANALYST B~)~ X , "  1E V OTHER ANALYSTS INDEP.
X96-1031@@ While the current MatchPlus learning law has proven to be effective in encoding relationships between words, it is computationally intensive and requires multiple passes through the training corpus. As a consequence, they share a common context vector. In practice, however, this summation is performed only over words that cooccur with the target word stem i.

