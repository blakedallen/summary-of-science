{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FunSumm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning Pegasus with Scisummnet data (NOT what we're planning to do in the project, but trying to run to see what happens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the dataset\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# For regular expressions\n",
    "import re\n",
    "# For handling string\n",
    "import string\n",
    "# For performing mathematical operations\n",
    "import math\n",
    "# Importing spacy\n",
    "import spacy\n",
    "# Importing json to read input\n",
    "import json\n",
    "# Importing rouge for evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import html\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "plot_dims = (16, 16)\n",
    "\n",
    "\n",
    "# url = \"https://cs.stanford.edu/~myasu/projects/scisumm_net/scisummnet_release1.1__20190413.zip\"\n",
    "# response = requests.get(url)\n",
    "# with zipfile.ZipFile(io.BytesIO(response.content)) as zipObj:\n",
    "#     # Extract all the contents of zip file in different directory\n",
    "#     zipObj.extractall(\"nlp_data\")\n",
    "#     print(\"File is unzipped in nlp_data folder\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pip\n",
    "# from pip._internal import main as pipmain\n",
    "\n",
    "# pipmain(['install', 'transformers'])\n",
    "# pipmain(['install', 'SentencePiece'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all raw text, break all papers into two parts -- Abstract and rest of document\n",
    "#first get all filepaths\n",
    "xmlfiles = []\n",
    "citations = []\n",
    "for subdir, dirs, files in os.walk(r'nlp_data'):\n",
    "    for filename in files:\n",
    "        filepath = subdir + os.sep + filename\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            xmlfiles.append(filepath)\n",
    "        if filepath.endswith(\".json\"):\n",
    "            citations.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next parse all XML documents\n",
    "\n",
    "def parse_xml_abstract(fp):\n",
    "    \"\"\" parse an XML journal article into an abstract and the rest of the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(fp)\n",
    "    except Exception as e:\n",
    "        return \"\",\"\",str(e)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    ab = []\n",
    "    bod = []\n",
    "    \n",
    "    for child in root:\n",
    "        if child.tag == \"ABSTRACT\":\n",
    "            for block in child:\n",
    "                ab.append(block.text)\n",
    "        else:\n",
    "            for block in child:\n",
    "                bod.append(block.text)\n",
    "                \n",
    "    #convert from list --> string\n",
    "    abstract = \"\\n\".join(ab)\n",
    "    body = \"\\n\".join(bod)\n",
    "    \n",
    "    #decode html entities\n",
    "    abstract = html.unescape(abstract)\n",
    "    body = html.unescape(body)\n",
    "    \n",
    "    return abstract,body,\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present a method for extracting parts of ob...</td>\n",
       "      <td>We present a method of extracting parts of obj...</td>\n",
       "      <td>[Berland and Charniak (1999) use Hearst style ...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>[The program takes the output of char_align (C...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Previous work has shown that Chinese word segm...</td>\n",
       "      <td>Word segmentation is considered an important f...</td>\n",
       "      <td>[Chinese word segmentation is done by the Stan...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We examine the viability of building large pol...</td>\n",
       "      <td>Polarity lexicons are large lists of phrases t...</td>\n",
       "      <td>[Recent work in this area includes Velikovich ...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extracting semantic relationships between enti...</td>\n",
       "      <td>Extraction of semantic relationships between e...</td>\n",
       "      <td>[They use two kinds of features: syntactic one...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>[In addition, Niessen and Ney (2004) decompose...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>We have developed a new program called alignin...</td>\n",
       "      <td>Aligning parallel texts has recently received ...</td>\n",
       "      <td>[There have been quite a number of recent pape...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>We present an approach to pronoun resolution b...</td>\n",
       "      <td>Pronoun resolution is a difficult but vital pa...</td>\n",
       "      <td>[, We follow the closed track setting where sy...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>We use logical inference techniques for recogn...</td>\n",
       "      <td>Recognising textual entailment (RTE) is the ta...</td>\n",
       "      <td>[However, this method does not work for realwo...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>This paper deals with two important ambiguitie...</td>\n",
       "      <td>The problem with successful resolution of ambi...</td>\n",
       "      <td>[The state of the art is a supervised algorith...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1009 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     We present a method for extracting parts of ob...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Previous work has shown that Chinese word segm...   \n",
       "3     We examine the viability of building large pol...   \n",
       "4     Extracting semantic relationships between enti...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  We have developed a new program called alignin...   \n",
       "1006  We present an approach to pronoun resolution b...   \n",
       "1007  We use logical inference techniques for recogn...   \n",
       "1008  This paper deals with two important ambiguitie...   \n",
       "\n",
       "                                                   body  \\\n",
       "0     We present a method of extracting parts of obj...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Word segmentation is considered an important f...   \n",
       "3     Polarity lexicons are large lists of phrases t...   \n",
       "4     Extraction of semantic relationships between e...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  Aligning parallel texts has recently received ...   \n",
       "1006  Pronoun resolution is a difficult but vital pa...   \n",
       "1007  Recognising textual entailment (RTE) is the ta...   \n",
       "1008  The problem with successful resolution of ambi...   \n",
       "\n",
       "                                              citations  \\\n",
       "0     [Berland and Charniak (1999) use Hearst style ...   \n",
       "1     [The program takes the output of char_align (C...   \n",
       "2     [Chinese word segmentation is done by the Stan...   \n",
       "3     [Recent work in this area includes Velikovich ...   \n",
       "4     [They use two kinds of features: syntactic one...   \n",
       "...                                                 ...   \n",
       "1004  [In addition, Niessen and Ney (2004) decompose...   \n",
       "1005  [There have been quite a number of recent pape...   \n",
       "1006  [, We follow the closed track setting where sy...   \n",
       "1007  [However, this method does not work for realwo...   \n",
       "1008  [The state of the art is a supervised algorith...   \n",
       "\n",
       "                                               filepath  \n",
       "0     nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "1     nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "2     nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "3     nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "4     nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "...                                                 ...  \n",
       "1004  nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "1005  nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "1006  nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "1007  nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "1008  nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "\n",
       "[1009 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_cols = []\n",
    "for fpn in range(len(xmlfiles)):\n",
    "    ab,bod,err = parse_xml_abstract(xmlfiles[fpn])\n",
    "    if err:\n",
    "        #print(fp, err)\n",
    "        continue\n",
    "    f = open(citations[fpn]) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    only_text = []\n",
    "    for entry in data:\n",
    "        only_text.append(entry['clean_text'])\n",
    "#     print(only_text)\n",
    "        \n",
    "    raw_cols.append([ab,bod,only_text,xmlfiles[fpn]])\n",
    "\n",
    "df = pd.DataFrame(raw_cols, columns=[\"abstract\",\"body\",\"citations\", \"filepath\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-arxiv')\n",
    "# The PEGASUS Model with a language modeling head. Can be used for summarization. \n",
    "# This model inherits from PreTrainedModel. \n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-arxiv')\n",
    "\n",
    "# ARTICLE_TO_SUMMARIZE = (\n",
    "# \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "# \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "# \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we present a method of extracting parts of objects from wholes (e.g. \" carometers from single word \" ) given some entity that has recognizable parts of other entity that depends on rank-orders of other words that may rank the entity in question . <n> this paper we use more \"part-of\" terminology that produces with 55% accuracy for top 50 words ranked by the system accuracy given a very large corpus . <n> we use the majority judgment of five human subjects to decide which proposed parts are correct and programed by an enduser and added to its output by taking two words that are in a part-whole relation by which to find existing patterns that are in a part-whole relation , or used as a part of rough semantic lexicon .']\n"
     ]
    }
   ],
   "source": [
    "## Sample with no fine-tunning \n",
    "\n",
    "sample_input = df.body[0]\n",
    "inputs = tokenizer([sample_input], max_length=1024, return_tensors='pt')\n",
    "\n",
    "#'max_length': Pad to a maximum length specified with the argument max_length \n",
    "# or to the maximum acceptable input length for the model if that argument is not provided.\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'])\n",
    "# print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
    "\n",
    "sample_output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.9692307692307692, recall=0.04542177361211247, fmeasure=0.08677685950413223), 'rougeL': Score(precision=0.8384615384615385, recall=0.039293439077144915, fmeasure=0.07506887052341599)}\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(str(sample_input),\n",
    "                      str(sample_output))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune with custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summaries</th>\n",
       "      <th>allTextReprocess</th>\n",
       "      <th>citations</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present a method for extracting parts of ob...</td>\n",
       "      <td>We present a method of extracting parts of obj...</td>\n",
       "      <td>[Berland and Charniak (1999) use Hearst style ...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>[The program takes the output of char_align (C...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Previous work has shown that Chinese word segm...</td>\n",
       "      <td>Word segmentation is considered an important f...</td>\n",
       "      <td>[Chinese word segmentation is done by the Stan...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We examine the viability of building large pol...</td>\n",
       "      <td>Polarity lexicons are large lists of phrases t...</td>\n",
       "      <td>[Recent work in this area includes Velikovich ...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extracting semantic relationships between enti...</td>\n",
       "      <td>Extraction of semantic relationships between e...</td>\n",
       "      <td>[They use two kinds of features: syntactic one...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>[In addition, Niessen and Ney (2004) decompose...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>We have developed a new program called alignin...</td>\n",
       "      <td>Aligning parallel texts has recently received ...</td>\n",
       "      <td>[There have been quite a number of recent pape...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>We present an approach to pronoun resolution b...</td>\n",
       "      <td>Pronoun resolution is a difficult but vital pa...</td>\n",
       "      <td>[, We follow the closed track setting where sy...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>We use logical inference techniques for recogn...</td>\n",
       "      <td>Recognising textual entailment (RTE) is the ta...</td>\n",
       "      <td>[However, this method does not work for realwo...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>This paper deals with two important ambiguitie...</td>\n",
       "      <td>The problem with successful resolution of ambi...</td>\n",
       "      <td>[The state of the art is a supervised algorith...</td>\n",
       "      <td>nlp_data/scisummnet_release1.1__20190413/top10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1009 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              summaries  \\\n",
       "0     We present a method for extracting parts of ob...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Previous work has shown that Chinese word segm...   \n",
       "3     We examine the viability of building large pol...   \n",
       "4     Extracting semantic relationships between enti...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  We have developed a new program called alignin...   \n",
       "1006  We present an approach to pronoun resolution b...   \n",
       "1007  We use logical inference techniques for recogn...   \n",
       "1008  This paper deals with two important ambiguitie...   \n",
       "\n",
       "                                       allTextReprocess  \\\n",
       "0     We present a method of extracting parts of obj...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Word segmentation is considered an important f...   \n",
       "3     Polarity lexicons are large lists of phrases t...   \n",
       "4     Extraction of semantic relationships between e...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  Aligning parallel texts has recently received ...   \n",
       "1006  Pronoun resolution is a difficult but vital pa...   \n",
       "1007  Recognising textual entailment (RTE) is the ta...   \n",
       "1008  The problem with successful resolution of ambi...   \n",
       "\n",
       "                                              citations  \\\n",
       "0     [Berland and Charniak (1999) use Hearst style ...   \n",
       "1     [The program takes the output of char_align (C...   \n",
       "2     [Chinese word segmentation is done by the Stan...   \n",
       "3     [Recent work in this area includes Velikovich ...   \n",
       "4     [They use two kinds of features: syntactic one...   \n",
       "...                                                 ...   \n",
       "1004  [In addition, Niessen and Ney (2004) decompose...   \n",
       "1005  [There have been quite a number of recent pape...   \n",
       "1006  [, We follow the closed track setting where sy...   \n",
       "1007  [However, this method does not work for realwo...   \n",
       "1008  [The state of the art is a supervised algorith...   \n",
       "\n",
       "                                               filepath  \n",
       "0     nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "1     nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "2     nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "3     nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "4     nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "...                                                 ...  \n",
       "1004  nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "1005  nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "1006  nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "1007  nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "1008  nlp_data/scisummnet_release1.1__20190413/top10...  \n",
       "\n",
       "[1009 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# in_df = pd.read_csv('/content/drive/My Drive/summaries_sample.csv')\n",
    "\n",
    "df.rename(columns={\"body\": \"allTextReprocess\"}, inplace = True)\n",
    "df.rename(columns={\"abstract\": \"summaries\"}, inplace = True)\n",
    "\n",
    "in_df = df\n",
    "in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Train Test Split\n",
    "train_pct = 0.6\n",
    "test_pct = 0.2\n",
    "\n",
    "in_df = in_df.sample(len(in_df), random_state=20)\n",
    "train_sub = int(len(in_df) * train_pct)\n",
    "test_sub = int(len(in_df) * test_pct) + train_sub\n",
    "\n",
    "train_df = in_df[0:train_sub]\n",
    "test_df = in_df[train_sub:test_sub]\n",
    "val_df = in_df[test_sub:]\n",
    "\n",
    "train_texts = list(train_df['allTextReprocess'])\n",
    "test_texts = list(test_df['allTextReprocess'])\n",
    "val_texts = list(val_df['allTextReprocess'])\n",
    "\n",
    "train_decode = list(train_df['summaries'])\n",
    "test_decode = list(test_df['summaries'])\n",
    "val_decode = list(val_df['summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decodings = tokenizer(train_decode, truncation=True, padding=True)\n",
    "val_decodings = tokenizer(val_decode, truncation=True, padding=True)\n",
    "test_decodings = tokenizer(test_decode, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "attention_mask\n"
     ]
    }
   ],
   "source": [
    "for key, val in test_decodings.items():\n",
    "    print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ourDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, decodings):\n",
    "        self.encodings = encodings\n",
    "        self.decodings = decodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.decodings['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "#         print(item)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "train_dataset = ourDataset(train_encodings, train_decodings)\n",
    "val_dataset = ourDataset(val_encodings, val_decodings)\n",
    "test_dataset = ourDataset(test_encodings, test_decodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 07:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=11.685738881429037, metrics={'train_runtime': 673.1462, 'train_samples_per_second': 0.004, 'total_flos': 21041862672384, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    eval_dataset=val_dataset         # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 15:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 12.366962432861328,\n",
       " 'eval_runtime': 39.42,\n",
       " 'eval_samples_per_second': 0.051,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('fine-tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tuned = PegasusForConditionalGeneration.from_pretrained('fine-tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we present a method of extracting parts of objects from wholes (e.g. \" carometers from single word \" ) given some entity that has recognizable parts of other entity that depends on rank-orders of other words that may rank the entity in question . <n> this paper we use more \"part-of\" terminology that produces with 55% accuracy for top 50 words ranked by the system accuracy given a very large corpus . <n> we use the majority judgment of five human subjects to decide which proposed parts are correct and programed by an enduser and added to its output by taking two words that are in a part-whole relation by which to find existing patterns that are in a part-whole relation , or used as a part of rough semantic lexicon .']\n"
     ]
    }
   ],
   "source": [
    "## Sample with fine-tunning \n",
    "\n",
    "sample_input = df.allTextReprocess[0]\n",
    "inputs = tokenizer([sample_input], max_length=1024, return_tensors='pt')\n",
    "\n",
    "#'max_length': Pad to a maximum length specified with the argument max_length \n",
    "# or to the maximum acceptable input length for the model if that argument is not provided.\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model_fine_tuned.generate(inputs['input_ids'])\n",
    "# print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
    "\n",
    "sample_output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.9692307692307692, recall=0.04542177361211247, fmeasure=0.08677685950413223), 'rougeL': Score(precision=0.8384615384615385, recall=0.039293439077144915, fmeasure=0.07506887052341599)}\n",
      "{'rouge1': Score(precision=0.9692307692307692, recall=0.04542177361211247, fmeasure=0.08677685950413223), 'rougeL': Score(precision=0.8384615384615385, recall=0.039293439077144915, fmeasure=0.07506887052341599)}\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores_fine_tuned = scorer.score(str(sample_input),\n",
    "                      str(sample_output))\n",
    "print(scores_fine_tuned)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\r\n"
     ]
    }
   ],
   "source": [
    "# !python3 -c 'import tensorflow as tf; print(tf.__version__)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
