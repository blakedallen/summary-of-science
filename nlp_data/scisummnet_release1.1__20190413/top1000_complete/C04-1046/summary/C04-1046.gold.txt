Confidence Estimation For Machine Translation
We present a detailed study of confidence estimation for machine translation.
Various methods for determining whether MT output is correct are investigated, for both whole sentences and words.
Since the notion of correctness is not intuitively clear in this context, different ways of defining it are proposed.
We present results on data from the NIST 2003 Chinese-to-English MT evaluation.
We introduce a sentence level QE system where an arbitrary threshold is used to classify the MT output as good or bad.
We study sentence and word level features for translation error prediction.
