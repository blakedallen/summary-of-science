Integrating Graph-Based and Transition-Based Dependency Parsers
Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference.
In this paper, we show how these results can be exploited to improve parsing
accuracy by integrating a graph-based and a transition-based model.
By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.
We first show how the MST Parser (McDonald et al, 2005) and the Malt Parser (Nivre et al, 2007) could be improved by stacking each parser on the predictions of the other.
In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other.
