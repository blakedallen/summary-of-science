<PAPER>
  <S sid="0">A Boosting Algorithm For Classification Of Semi-Structured Text</S>
  <ABSTRACT>
    <S sid="1" ssid="1">The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion/modality identification.</S>
    <S sid="2" ssid="2">Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required.</S>
    <S sid="3" ssid="3">Accordingly, learning algorithms must be created that can handle the structures observed in texts.</S>
    <S sid="4" ssid="4">In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts.</S>
    <S sid="5" ssid="5">The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners.</S>
    <S sid="6" ssid="6">We also discuss the relation between our algorithm and SVMs with tree kernel.</S>
    <S sid="7" ssid="7">Two experiments on opinion/modality classification confirm that subtree features are important.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="8" ssid="1">Text classification plays an important role in organizing the online texts available on the World Wide Web, Internet news, and E-mails.</S>
    <S sid="9" ssid="2">Until recently, a number of machine learning algorithms have been applied to this problem and have been proven successful in many domains (Sebastiani, 2002).</S>
    <S sid="10" ssid="3">In the traditional text classification tasks, one has to identify predefined text &#8220;topics&#8221;, such as politics, finance, sports or entertainment.</S>
    <S sid="11" ssid="4">For learning algorithms to identify these topics, a text is usually represented as a bag-of-words, where a text is regarded as a multi-set (i.e., a bag) of words and the word order or syntactic relations appearing in the original text is ignored.</S>
    <S sid="12" ssid="5">Even though the bag-of-words representation is naive and does not convey the meaning of the original text, reasonable accuracy can be obtained.</S>
    <S sid="13" ssid="6">This is because each word occurring in the text is highly relevant to the predefined &#8220;topics&#8221; to be identified.</S>
    <S sid="14" ssid="7">&#8727;At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp Given that a number of successes have been reported in the field of traditional text classification, the focus of recent research has expanded from simple topic identification to more challenging tasks such as opinion/modality identification.</S>
    <S sid="15" ssid="8">Example includes categorization of customer E-mails and reviews by types of claims, modalities or subjectivities (Turney, 2002; Wiebe, 2000).</S>
    <S sid="16" ssid="9">For the latter, the traditional bag-of-words representation is not sufficient, and a richer, structural representation is required.</S>
    <S sid="17" ssid="10">A straightforward way to extend the traditional bag-of-words representation is to heuristically add new types of features to the original bag-of-words features, such as fixed-length n-grams (e.g., word bi-gram or tri-gram) or fixedlength syntactic relations (e.g., modifier-head relations).</S>
    <S sid="18" ssid="11">These ad-hoc solutions might give us reasonable performance, however, they are highly taskdependent and require careful design to create the &#8220;optimal&#8221; feature set for each task.</S>
    <S sid="19" ssid="12">Generally speaking, by using text processing systems, a text can be converted into a semi-structured text annotated with parts-of-speech, base-phrase information or syntactic relations.</S>
    <S sid="20" ssid="13">This information is useful in identifying opinions or modalities contained in the text.</S>
    <S sid="21" ssid="14">We think that it is more useful to propose a learning algorithm that can automatically capture relevant structural information observed in text, rather than to heuristically add this information as new features.</S>
    <S sid="22" ssid="15">From these points of view, this paper proposes a classification algorithm that captures sub-structures embedded in text.</S>
    <S sid="23" ssid="16">To simplify the problem, we first assume that a text to be classified is represented as a labeled ordered tree, which is a general data structure and a simple abstraction of text.</S>
    <S sid="24" ssid="17">Note that word sequence, base-phrase annotation, dependency tree and an XML document can be modeled as a labeled ordered tree.</S>
    <S sid="25" ssid="18">The algorithm proposed here has the following characteristics: i) It performs learning and classification using structural information of text. ii) It uses a set of all subtrees (bag-of-subtrees) for the feature set without any constraints. iii) Even though the size of the candidate feature set becomes quite large, it automatically selects a compact and relevant feature set based on Boosting.</S>
    <S sid="26" ssid="19">This paper is organized as follows.</S>
    <S sid="27" ssid="20">First, we describe the details of our Boosting algorithm in which the subtree-based decision stumps are applied as weak learners.</S>
    <S sid="28" ssid="21">Second, we show an implementation issue related to constructing an efficient learning algorithm.</S>
    <S sid="29" ssid="22">We also discuss the relation between our algorithm and SVMs (Boser et al., 1992) with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002).</S>
    <S sid="30" ssid="23">Two experiments on the opinion and modality classification tasks are employed to confirm that subtree features are important.</S>
  </SECTION>
  <SECTION title="2 Classifier for Trees" number="2">
    <S sid="31" ssid="1">We first assume that a text to be classified is represented as a labeled ordered tree.</S>
    <S sid="32" ssid="2">The focused problem can be formalized as a general problem, called the tree classification problem.</S>
    <S sid="33" ssid="3">The tree classification problem is to induce a mapping f(x) : X &#8594; {&#177;1}, from given training examples T = {hxi, yii}Li=1, where xi &#8712; X is a labeled ordered tree and yi &#8712; {&#177;1} is a class label associated with each training data (we focus here on the problem of binary classification.).</S>
    <S sid="34" ssid="4">The important characteristic is that the input example xi is represented not as a numerical feature vector (bagof-words) but a labeled ordered tree.</S>
    <S sid="35" ssid="5">Let us introduce a labeled ordered tree (or simply tree), its definition and notations, first.</S>
    <S sid="36" ssid="6">We denote the number of nodes in t as |t|.</S>
    <S sid="37" ssid="7">Figure 1 shows an example of a labeled ordered tree and its subtree and non-subtree.</S>
    <S sid="38" ssid="8">Decision stumps are simple classifiers, where the final decision is made by only a single hypothesis or feature.</S>
    <S sid="39" ssid="9">Boostexter (Schapire and Singer, 2000) uses word-based decision stumps for topic-based text classification.</S>
    <S sid="40" ssid="10">To classify trees, we here extend the decision stump definition as follows.</S>
    <S sid="41" ssid="11">Definition 3 Decision Stumps for Trees Let t and x be labeled ordered trees, and y be a class label (y &#8712; {&#177;1}), a decision stump classifier for trees is given by The parameter for classification is the tuple ht, yi, hereafter referred to as the rule of the decision stumps.</S>
    <S sid="42" ssid="12">The decision stumps are trained to find rule h&#710;t, &#710;yi that minimizes the error rate for the given training data T = {hxi, yii}Li=1: In this paper, we will use gain instead of error rate for clarity.</S>
    <S sid="43" ssid="13">The decision stumps classifiers for trees are too inaccurate to be applied to real applications, since the final decision relies on the existence of a single tree.</S>
    <S sid="44" ssid="14">However, accuracies can be boosted by the Boosting algorithm (Freund and Schapire, 1996; Schapire and Singer, 2000).</S>
    <S sid="45" ssid="15">Boosting repeatedly calls a given weak learner to finally produce hypothesis f, which is a linear combination of K hypotheses produced by the prior weak learners, i,e.</S>
    <S sid="46" ssid="16">: A weak learner is built at each iteration k with different distributions or weights d(k) = (d(k) The weights are calculated in such a way that hard examples are focused on more than easier examples.</S>
    <S sid="47" ssid="17">To use the decision stumps as the weak learner of Boosting, we redefine the gain function (2) as follows: There exist many Boosting algorithm variants, however, the original and the best known algorithm is AdaBoost (Freund and Schapire, 1996).</S>
    <S sid="48" ssid="18">We here use Arc-GV (Breiman, 1999) instead of AdaBoost, since Arc-GV asymptotically maximizes the margin and shows faster convergence to the optimal solution than AdaBoost.</S>
  </SECTION>
  <SECTION title="3 Efficient Computation" number="3">
    <S sid="49" ssid="1">In this section, we introduce an efficient and practical algorithm to find the optimal rule (&#710;t, &#710;y) from given training data.</S>
    <S sid="50" ssid="2">This problem is formally defined as follows.</S>
    <S sid="51" ssid="3">Problem 1 Find Optimal Rule Let T = {(x1, y1, d1), ..., (xL, yL, dL)} be training data, where, xi is a labeled ordered tree, yi E {+1} is a class label associated with xi and di (EL i=1 di = 1, di &gt; 0) is a normalized weight assigned to xi.</S>
    <S sid="52" ssid="4">Given T, find the optimal rule (&#710;t, &#710;y) that maximizes the gain, i.e., The most naive and exhaustive method, in which we first enumerate all subtrees F and then calculate the gains for all subtrees, is usually impractical, since the number of subtrees is exponential to its size.</S>
    <S sid="53" ssid="5">We thus adopt an alternative strategy to avoid such exhaustive enumeration.</S>
    <S sid="54" ssid="6">The method to find the optimal rule is modeled as a variant of the branch-and-bound algorithm, and is summarized in the following strategies: We will describe these steps more precisely in the following subsections.</S>
    <S sid="55" ssid="7">Abe and Zaki independently proposed an efficient method, rightmost-extension, to enumerate all subtrees from a given tree (Abe et al., 2002; Zaki, 2002).</S>
    <S sid="56" ssid="8">First, the algorithm starts with a set of trees consisting of single nodes, and then expands a given tree of size (k &#8722; 1) by attaching a new node to this tree to obtain trees of size k. However, it would be inefficient to expand nodes at arbitrary positions of the tree, as duplicated enumeration is inevitable.</S>
    <S sid="57" ssid="9">The algorithm, rightmost extension, avoids such duplicated enumerations by restricting the position of attachment.</S>
    <S sid="58" ssid="10">We here give the definition of rightmost extension to describe this restriction in detail.</S>
    <S sid="59" ssid="11">Definition 4 Rightmost Extension (Abe et al., 2002; Zaki, 2002) Let t and t' be labeled ordered trees.</S>
    <S sid="60" ssid="12">We say t' is a rightmost extension of t, if and only if t and t' satisfy the following three conditions: Consider Figure 2, which illustrates example tree t with the labels drawn from the set G = {a, b, c}.</S>
    <S sid="61" ssid="13">For the sake of convenience, each node in this figure has its original number (depth-first enumeration).</S>
    <S sid="62" ssid="14">The rightmost-path of the tree t is (a(c(b))), and occurs at positions 1, 4 and 6 respectively.</S>
    <S sid="63" ssid="15">The set of rightmost extended trees is then enumerated by simply adding a single node to a node on the rightmost path.</S>
    <S sid="64" ssid="16">Since there are three nodes on the rightmost path and the size of the label set is 3 (= |G|), a total of 9 trees are enumerated from the original tree t. Note that rightmost extension preserves the prefix ordering of nodes in t (i.e., nodes at positions 1..|t |are preserved).</S>
    <S sid="65" ssid="17">By repeating the process of rightmost-extension recursively, we can create a search space in which all trees drawn from the set G are enumerated.</S>
    <S sid="66" ssid="18">Figure 3 shows a snapshot of such a search space.</S>
    <S sid="67" ssid="19">Rightmost extension defines a canonical search space in which one can enumerate all subtrees from a given set of trees.</S>
    <S sid="68" ssid="20">We here consider an upper bound of the gain that allows subspace pruning in this canonical search space.</S>
    <S sid="69" ssid="21">The following theorem, an extension of Morhishita (Morhishita, 2002), gives a convenient way of computing a tight upper bound on gain(ht', yi) for any super-tree t' of t. We can efficiently prune the search space spanned by right most extension using the upper bound of gain u(t).</S>
    <S sid="70" ssid="22">During the traverse of the subtree lattice built by the recursive process of rightmost extension, we always maintain the temporally suboptimal gain &#964; among all gains calculated previously.</S>
    <S sid="71" ssid="23">If &#181;(t) &lt; &#964;, the gain of any super-tree t' &#8839; t is no greater than &#964;, and therefore we can safely prune the search space spanned from the subtree t. If &#181;(t) &#8805; &#964;, in contrast, we cannot prune this space, since there might exist a super-tree t' &#8839; t such that gain(t') &#8805; &#964;.</S>
    <S sid="72" ssid="24">We can also prune the space with respect to the expanded single node s. Even if &#181;(t) &#8805; &#964; and a node s is attached to the tree t, we can ignore the space spanned from the tree t' if &#181;(s) &lt; &#964;, since no super-tree of s can yield optimal gain.</S>
    <S sid="73" ssid="25">Figure 4 presents a pseudo code of the algorithm Find Optimal Rule.</S>
    <S sid="74" ssid="26">The two pruning are marked with (1) and (2) respectively.</S>
  </SECTION>
  <SECTION title="4 Relation to SVMs with Tree Kernel" number="4">
    <S sid="75" ssid="1">Recent studies (Breiman, 1999; Schapire et al., 1997; R&#168;atsch et al., 2001) have shown that both Boosting and SVMs (Boser et al., 1992) have a similar strategy; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples.</S>
    <S sid="76" ssid="2">We here describe a connection between our Boosting algorithm and SVMs with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002).</S>
    <S sid="77" ssid="3">Tree kernel is one of the convolution kernels, and implicitly maps the example represented in a labeled ordered tree into all subtree spaces.</S>
    <S sid="78" ssid="4">The implicit mapping defined by tree kernel is given as: &#934;(x)=(I(t1 &#8838; x), ... , I(t|F |&#8838; x)), where tj&#8712;F, x &#8712; X and I(&#183;) is the indicator function 1.</S>
    <S sid="79" ssid="5">The final hypothesis of SVMs with tree kernel can be given by Similarly, the final hypothesis of our boosting algorithm can be reformulated as a linear classifier: 1Strictly speaking, tree kernel uses the cardinality of each substructure.</S>
    <S sid="80" ssid="6">However, it makes little difference since a given tree is often sparse in NLP and the cardinality of substructures will be approximated by their existence.</S>
  </SECTION>
  <SECTION title="Algorithm: Find Optimal Rule" number="5">
    <S sid="81" ssid="1">argument: T = {hx1, y1, d1i ..., hxL, yL, dLi} (xi a tree, yi &#8712; {&#177;1} is a class, and di (PLi=1 di = 1, di &#8805; 0) is a weight) returns: Optimal rule h&#710;t, &#710;yi begin We can thus see that both algorithms are essentially the same in terms of their feature space.</S>
    <S sid="82" ssid="2">The difference between them is the metric of margin; the margin of Boosting is measured in l1-norm, while, that of SVMs is measured in l2-norm.</S>
    <S sid="83" ssid="3">The question one might ask is how the difference is expressed in practice.</S>
    <S sid="84" ssid="4">The difference between them can be explained by sparseness.</S>
    <S sid="85" ssid="5">It is well known that the solution or separating hyperplane of SVMs is expressed as a linear combination of the training examples using some coefficients A, (i.e., w = PL i=1 Ai&#934;(xi)).</S>
    <S sid="86" ssid="6">Maximizing l2norm margin gives a sparse solution in the example space, (i.e., most of Ai becomes 0).</S>
    <S sid="87" ssid="7">Examples that have non-zero coefficient are called support vectors that form the final solution.</S>
    <S sid="88" ssid="8">Boosting, in contrast, performs the computation explicitly in the feature space.</S>
    <S sid="89" ssid="9">The concept behind Boosting is that only a few hypotheses are needed to express the final solution.</S>
    <S sid="90" ssid="10">The l1-norm margin allows us to realize this property.</S>
    <S sid="91" ssid="11">Boosting thus finds a sparse solution in the feature space.</S>
    <S sid="92" ssid="12">The accuracies of these two methods depends on the given training data.</S>
    <S sid="93" ssid="13">However, we argue that Boosting has the following practical advantages.</S>
    <S sid="94" ssid="14">First, sparse hypotheses allow us to build an efficient classification algorithm.</S>
    <S sid="95" ssid="15">The complexity of SVMs with tree kernel is O(L'|N1||N2|), where N1 and N2 are trees, and L' is the number of support vectors, which is too heavy to realize real applications.</S>
    <S sid="96" ssid="16">Boosting, in contrast, runs faster, since the complexity depends only on the small number of decision stumps.</S>
    <S sid="97" ssid="17">Second, sparse hypotheses are useful in practice as they provide &#8220;transparent&#8221; models with which we can analyze how the model performs or what kind of features are useful.</S>
    <S sid="98" ssid="18">It is difficult to give such analysis with kernel methods, since they define the feature space implicitly.</S>
  </SECTION>
  <SECTION title="5 Experiments" number="6">
    <S sid="99" ssid="1">We conducted two experiments in sentence classification.</S>
    <S sid="100" ssid="2">The goal of this task is to classify reviews (in Japanese) for PHS2 as positive reviews or negative reviews.</S>
    <S sid="101" ssid="3">A total of 5,741 sentences were collected from a Web-based discussion BBS on PHS, in which users are directed to submit positive reviews separately from negative reviews.</S>
    <S sid="102" ssid="4">The unit of classification is a sentence.</S>
    <S sid="103" ssid="5">The categories to be identified are &#8220;positive&#8221; or &#8220;negative&#8221; with the numbers 2,679 and 3,062 respectively.</S>
    <S sid="104" ssid="6">This task is to classify sentences (in Japanese) by modality.</S>
    <S sid="105" ssid="7">A total of 1,710 sentences from a Japanese newspaper were manually annotated according to Tamura&#8217;s taxonomy (Tamura and Wada, 1996).</S>
    <S sid="106" ssid="8">The unit of classification is a sentence.</S>
    <S sid="107" ssid="9">The categories to be identified are &#8220;opinion&#8221;, &#8220;assertion&#8221; or &#8220;description&#8221; with the numbers 159, 540, and 1,011 respectively.</S>
    <S sid="108" ssid="10">To employ learning and classification, we have to represent a given sentence as a labeled ordered tree.</S>
    <S sid="109" ssid="11">In this paper, we use the following three representation forms.</S>
    <S sid="110" ssid="12">Ignoring structural information embedded in text, we simply represent a text as a set of words.</S>
    <S sid="111" ssid="13">This is exactly the same setting as Boostexter.</S>
    <S sid="112" ssid="14">Word boundaries are identified using a Japanese morphological analyzer, ChaSen3.</S>
    <S sid="113" ssid="15">We represent a text in a word-based dependency tree.</S>
    <S sid="114" ssid="16">We first use CaboCha4 to obtain a chunk-based dependency tree of the text.</S>
    <S sid="115" ssid="17">The chunk approximately corresponds to the basephrase in English.</S>
    <S sid="116" ssid="18">By identifying the head word in the chunk, a chunk-based dependency tree is converted into a word-based dependency tree.</S>
    <S sid="117" ssid="19">It is the word-based dependency tree that assumes that each word simply modifies the next word.</S>
    <S sid="118" ssid="20">Any subtree of this structure becomes a word n-gram.</S>
    <S sid="119" ssid="21">We compared the performance of our Boosting algorithm and support vector machines (SVMs) with bag-of-words kernel and tree kernel according to their F-measure in 5-fold cross validation.</S>
    <S sid="120" ssid="22">Although there exist some extensions for tree kernel (Kashima and Koyanagi, 2002), we use the original tree kernel by Collins (Collins and Duffy, 2002), where all subtrees of a tree are used as distinct features.</S>
    <S sid="121" ssid="23">This setting yields a fair comparison in terms of feature space.</S>
    <S sid="122" ssid="24">To extend a binary classifier to a multi-class classifier, we use the one-vs-rest method.</S>
    <S sid="123" ssid="25">Hyperparameters, such as number of iterations K in Boosting and soft-margin parameter C in SVMs were selected by using cross-validation.</S>
    <S sid="124" ssid="26">We implemented SVMs with tree kernel based on TinySVM5 with custom kernels incorporated therein.</S>
    <S sid="125" ssid="27">Table 1 summarizes the results of PHS and MOD tasks.</S>
    <S sid="126" ssid="28">To examine the statistical significance of the results, we employed a McNemar&#8217;s paired test, a variant of the sign test, on the labeling disagreements.</S>
    <S sid="127" ssid="29">This table also includes the results of significance tests.</S>
    <S sid="128" ssid="30">In all tasks and categories, our subtree-based Boosting algorithm (dep/ngram) performs better than the baseline method (bow).</S>
    <S sid="129" ssid="31">This result supports our first intuition that structural information within texts is important when classifying a text by opinions or modalities, not by topics.</S>
    <S sid="130" ssid="32">We also find that there are no significant differences in accuracy between dependency and n-gram (in all cases, p &gt; 0.2).</S>
    <S sid="131" ssid="33">When using the bag-of-words feature, no significant differences in accuracy are observed between Boosting and SVMs.</S>
    <S sid="132" ssid="34">When structural information is used in training and classification, Boosting performs slightly better than SVMs with tree kernel.</S>
    <S sid="133" ssid="35">The differences are significant when we use dependency features in the MOD task.</S>
    <S sid="134" ssid="36">SVMs show worse performance depending on tasks and categories, (e.g., 24.2 F-measure in the smallest category &#8220;opinion&#8221; in the MOD task).</S>
    <S sid="135" ssid="37">When a convolution kernel is applied to sparse data, kernel dot products between almost the same instances become much larger than those between different instances.</S>
    <S sid="136" ssid="38">This is because the number of common features between similar instances exponentially increases with size.</S>
    <S sid="137" ssid="39">This sometimes leads to overfitting in training , where a test instance very close to an instance in training data is correctly classified, and other instances are classified as a default class.</S>
    <S sid="138" ssid="40">This problem can be tackled by several heuristic approaches: i) employing a decay factor to reduce the weights of large sub-structures (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). ii) substituting kernel dot products for the Gaussian function to smooth the original kernel dot products (Haussler, 1999).</S>
    <S sid="139" ssid="41">These approaches may achieve better accuracy, however, they yield neither the fast classification nor the interpretable feature space targeted by this paper.</S>
    <S sid="140" ssid="42">Moreover, we cannot give a fair comparison in terms of the same feature space.</S>
    <S sid="141" ssid="43">The selection of optimal hyperparameters, such as decay factors in the first approach and smoothing parameters in the second approach, is also left to as an open question.</S>
    <S sid="142" ssid="44">We employed a McNemar&#8217;s paired test on the labeling disagreements.</S>
    <S sid="143" ssid="45">Underlined results indicate that there is a significant difference (p &lt; 0.01) against the baseline (bow).</S>
    <S sid="144" ssid="46">If there is a statistical difference (p &lt; 0.01) between Boosting and SVMs with the same feature representation (bow / dep / n-gram), better results are asterisked.</S>
    <S sid="145" ssid="47">In the previous section, we described the merits of our Boosting algorithm.</S>
    <S sid="146" ssid="48">We experimentally verified these merits from the results of the PHS task.</S>
    <S sid="147" ssid="49">As illustrated in section 4, our method can automatically select relevant and compact features from a number of feature candidates.</S>
    <S sid="148" ssid="50">In the PHS task, a total 1,793 features (rules) were selected, while the set sizes of distinct uni-gram, bi-gram and trigram appearing in the data were 4,211, 24,206, and 43,658 respectively.</S>
    <S sid="149" ssid="51">Even though all subtrees are used as feature candidates, Boosting selects a small and highly relevant subset of features.</S>
    <S sid="150" ssid="52">When we explicitly enumerate the subtrees used in tree kernel, the number of active (non-zero) features might amount to ten thousand or more.</S>
    <S sid="151" ssid="53">Table 2 shows examples of extracted support features (pairs of feature (tree) t and weight wt in (Eq.</S>
    <S sid="152" ssid="54">5)) in the PHS task.</S>
    <S sid="153" ssid="55">A.</S>
    <S sid="154" ssid="56">Features including the word &#8220;&#12395;&#12367;&#12356; (hard, difficult)&#8221; In general, &#8220;&#12395;&#12367;&#12356; (hard, difficult)&#8221; is an adjective expressing negative opinions.</S>
    <S sid="155" ssid="57">Most of features including &#8220;&#12395;&#12367;&#12356;&#8221; are assigned a negative weight (negative opinion).</S>
    <S sid="156" ssid="58">However, only one feature &#8220;&#20999;&#12428;&#12395; &#12367;&#12356; (hard to cut off)&#8221; has a positive weight.</S>
    <S sid="157" ssid="59">This result strongly reflects the domain knowledge, PHS (cell phone reviews).</S>
    <S sid="158" ssid="60">B.</S>
    <S sid="159" ssid="61">Features including the word &#8220;&#20351;&#12358; (use)&#8221; &#8220;&#20351;&#12358; (use)&#8221; is a neutral expression for opinion classifications.</S>
    <S sid="160" ssid="62">However, the weight varies according to the surrounding context: 1) &#8220;&#20351;&#12356; &#12383;&#12356; (want to use)&#8221; &#8594; positive, 2) &#8220;&#20351;&#12356; &#12420;&#12377; &#12356; (be easy to use)&#8221; &#8594; positive, 3) &#8220;&#20351;&#12356; &#12420;&#12377; &#12363; &#12387;&#12383; (was easy to use)&#8221; (past form) &#8594; negative, 4) &#8220;&#12398; &#12411;&#12358;&#12364; &#20351;&#12356; &#12420;&#12377;&#12356; (... is easier to use than ..)&#8221; (comparative) &#8594; negative.</S>
    <S sid="161" ssid="63">C. Features including the word &#8220;&#20805;&#38651; (recharge)&#8221; Features reflecting the domain knowledge are extracted: 1) &#8220;&#20805;&#38651; &#26178;&#38291; &#12364; &#30701;&#12356; (recharging time is short)&#8221; &#8594; positive, 2) &#8220;&#20805;&#38651; &#26178;&#38291; &#38263;&#12356; (recharging time is long)&#8221; &#8594; negative.</S>
    <S sid="162" ssid="64">These features are interesting, since we cannot determine the correct label (positive/negative) by using just the bag-of-words features, such as &#8220;recharge&#8221;, &#8220;short&#8221; or &#8220;long&#8221; alone.</S>
    <S sid="163" ssid="65">Table 3 illustrates an example of actual classification.</S>
    <S sid="164" ssid="66">For the input sentence &#8220;&#28082;&#26230;&#12364;&#22823;&#12365;&#12367;&#12390;, &#32186;&#40599;, &#35211;&#12420;&#12377;&#12356; (The LCD is large, beautiful, and easy to see.</S>
    <S sid="165" ssid="67">)&#8221;, the system outputs the features applied to this classification along with their weights wt.</S>
    <S sid="166" ssid="68">This information allows us to analyze how the system classifies the input sentence in a category and what kind of features are used in the classification.</S>
    <S sid="167" ssid="69">We cannot perform these analyses with tree kernel, since it defines their feature space implicitly.</S>
    <S sid="168" ssid="70">The testing speed of our Boosting algorithm is much higher than that of SVMs with tree kernel.</S>
    <S sid="169" ssid="71">In the PHS task, the speeds of Boosting and SVMs are 0.531 sec./5,741 instances and 255.42 sec./5,741 instances respectively 6.</S>
    <S sid="170" ssid="72">We can say that Boosting is about 480 times faster than SVMs with tree kernel.</S>
    <S sid="171" ssid="73">Even though the potential size of search space is huge, the pruning criterion proposed in this paper effectively prunes the search space.</S>
    <S sid="172" ssid="74">The pruning conditions in Fig.4 are fulfilled with more than 90% probabitity.</S>
    <S sid="173" ssid="75">The training speed of our method is 1,384 sec./5,741 instances when we set K = 60, 000 (# of iterations for Boosting).</S>
    <S sid="174" ssid="76">It takes only 0.023 (=1,384/60,000) sec. to invoke the weak learner, Find Optimal Rule.</S>
  </SECTION>
  <SECTION title="6 Conclusions and Future Work" number="7">
    <S sid="175" ssid="1">In this paper, we focused on an algorithm for the classification of semi-structured text in which a sentence is represented as a labeled ordered tree7.</S>
    <S sid="176" ssid="2">Our proposal consists of i) decision stumps that use subtrees as features and ii) Boosting algorithm in which the subtree-based decision stumps are applied as weak learners.</S>
    <S sid="177" ssid="3">Two experiments on opinion/modality classification tasks confirmed that subtree features are important.</S>
    <S sid="178" ssid="4">One natural extension is to adopt confidence rated predictions to the subtree-based weak learners.</S>
    <S sid="179" ssid="5">This extension is also found in BoosTexter and shows better performance than binary-valued learners.</S>
    <S sid="180" ssid="6">In our experiments, n-gram features showed comparable performance to dependency features.</S>
    <S sid="181" ssid="7">We would like to apply our method to other applications where instances are represented in a tree and their subtrees play an important role in classifications (e.g., parse re-ranking (Collins and Duffy, 2002) and information extraction).</S>
  </SECTION>
</PAPER>
