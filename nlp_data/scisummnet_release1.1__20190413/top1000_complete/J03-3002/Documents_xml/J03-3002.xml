<PAPER>
  <S sid="0">The Web As A Parallel Corpus</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Parallel corpora have become an essential resource for work in multilingual natural language processing.</S>
    <S sid="2" ssid="2">In this article, we report on our work using the STRAND system for mining parallel text on the World Wide Web,first reviewing the original algorithm and results and then presenting a set of significant enhancements.</S>
    <S sid="3" ssid="3">These enhancements include the use of supervised learning based on structural features of documents to improve classification performance, a new contentbased measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale.</S>
    <S sid="4" ssid="4">Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="5" ssid="1">Parallel corpora have become an essential resource for work in multilingual natural language processing.</S>
    <S sid="6" ssid="2">In this article, we report on our work using the STRAND system for mining parallel text on the World Wide Web,first reviewing the original algorithm and results and then presenting a set of significant enhancements.</S>
    <S sid="7" ssid="3">These enhancements include the use of supervised learning based on structural features of documents to improve classification performance, a new contentbased measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale.</S>
    <S sid="8" ssid="4">Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="9" ssid="1">Parallel corpora&#8212;bodies of text in parallel translation, also known as bitexts&#8212;have taken on an important role in machine translation and multilingual natural language processing.</S>
    <S sid="10" ssid="2">They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997).</S>
    <S sid="11" ssid="3">More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English and a less resource-rich language, we project English annotations across the parallel corpus to the second language, using word-level alignments as the bridge, and then use robust statistical techniques in learning from the resulting noisy annotations (Cabezas, Dorr, and Resnik 2001; Diab and Resnik 2002; Hwa et al. 2002; Lopez et al.</S>
    <S sid="12" ssid="4">2002; Yarowsky, Ngai, and Wicentowski 2001; Yarowsky and Ngai 2001; Riloff, Schafer, and Yarowsky 2002).</S>
    <S sid="13" ssid="5">For these reasons, parallel corpora can be thought of as a critical resource.</S>
    <S sid="14" ssid="6">Unfortunately, they are not readily available in the necessary quantities.</S>
    <S sid="15" ssid="7">Until very recently, for example, statistical work in machine translation focused heavily on French-English translation because the Canadian parliamentary proceedings (Hansards) in English and French were the only large bitext available.</S>
    <S sid="16" ssid="8">Things have improved somewhat, but it is still fair to say that for all but a relatively few language pairs, parallel corpora tend to be accessible only in specialized forms such as United Nations proceedings (e.g., via the Linguistic Data Consortium, (http://www.ldc.upenn.edu)), religious texts (Resnik, Olsen, and Diab 1999), localized versions of software manuals (Resnik and Melamed 1997; Menezes and Richardson 2001), and the like.</S>
    <S sid="17" ssid="9">Even for the top handful of majority languages, the available parallel corpora tend to be unbalanced, representing primarily governmental or newswire-style texts.</S>
    <S sid="18" ssid="10">In addition, like other language resources, parallel corpora are often encumbered by fees or licensing restrictions.</S>
    <S sid="19" ssid="11">For all these reasons, it is difficult to follow the &#8220;more data are better data&#8221; advice of Church and Mercer (1993), abandoning balance in favor of volume, with respect to parallel text.</S>
    <S sid="20" ssid="12">Then there is the World Wide Web.</S>
    <S sid="21" ssid="13">People tend to see the Web as a reflection of their own way of viewing the world&#8212;as a huge semantic network, or an enormous historical archive, or a grand social experiment.</S>
    <S sid="22" ssid="14">We are no different: As computational linguists working on multilingual issues, we view the Web as a great big body of text waiting to be mined, a huge fabric of linguistic data often interwoven with parallel threads.</S>
    <S sid="23" ssid="15">This article describes our techniques for mining the Web in order to extract the parallel text it contains.</S>
    <S sid="24" ssid="16">It presents, in revised and considerably extended form, our early work on mining the Web for bilingual text (STRAND) (Resnik 1998, 1999), incorporating new work on content-based detection of translations (Smith 2001, 2002), and efficient exploitation of the Internet Archive.</S>
    <S sid="25" ssid="17">In Section 2 we lay out the STRAND architecture, which is based on the insight that translated Web pages tend quite strongly to exhibit parallel structure, permitting them to be identified even without looking at content; we also show how we have improved STRAND&#8217;s performance by training a supervised classifier using structural parameters rather than relying on manually tuned thresholds.</S>
    <S sid="26" ssid="18">In Section 3 we present an approach to detecting translations that relies entirely on content rather than structure, demonstrating performance comparable to STRAND&#8217;s using this orthogonal source of information.</S>
    <S sid="27" ssid="19">In Section 4 we describe how we have adapted the STRAND approach to the Internet Archive, dramatically improving our ability to identify parallel Web pages on a large scale.</S>
    <S sid="28" ssid="20">Section 5 puts all the pieces together, using structural and combined content-structure matching of pages on the Internet Archive in order to obtain a sizable corpus of English-Arabic Web document pairs.</S>
    <S sid="29" ssid="21">Finally we present our thoughts on future work and conclusions.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="30" ssid="1">STRAND (Resnik 1998, 1999) is an architecture for structural translation recognition, acquiring natural data.</S>
    <S sid="31" ssid="2">Its goal is to identify pairs of Web pages that are mutual translations.</S>
    <S sid="32" ssid="3">In order to do this, it exploits an observation about the way that Web page authors disseminate information in multiple languages: When presenting the same content in two different languages, authors exhibit a very strong tendency to use the same document structure (e.g., Figure 1).</S>
    <S sid="33" ssid="4">STRAND therefore locates pages that might be translations of each other, via a number of different strategies, and filters out page pairs whose page structures diverge by too much.</S>
    <S sid="34" ssid="5">In this section we describe how STRAND works, and we also discuss several related Web-mining methods, focusing on the overall architecture these systems have in common and the important system-specific variations.</S>
    <S sid="35" ssid="6">We then show how tuning STRAND&#8217;s structural parameters using supervised training can significantly increase its performance.</S>
    <S sid="36" ssid="7">Finding parallel text on the Web consists of three main steps: Example of a candidate pair.</S>
    <S sid="37" ssid="8">We consider each of these steps in turn. using the AltaVista search engine&#8217;s (http://www.av.com) advanced search to search for two types of Web pages: parents and siblings.</S>
    <S sid="38" ssid="9">A parent page is one that contains hypertext links to different-language versions of a document; for example, if we were looking for English and French bitexts, the page at the left in Figure 2 would lead us to one such candidate pair.</S>
    <S sid="39" ssid="10">To perform this search for the English-French language pair, we ask AltaVista for pages in any language that satisfy this Boolean expression: (anchor:&amp;quot;english&amp;quot; OR anchor:&amp;quot;anglais&amp;quot;) AND (anchor:&amp;quot;french&amp;quot; OR anchor:&amp;quot;fran&#184;cais&amp;quot;).</S>
    <S sid="40" ssid="11">A 10-line distance filter is used to restrict attention to pages on which the English and French pointers occur reasonably close to one another&#8212;specifically, those for which the regular expression (in Perl) /(english|anglais)/ is satisfied within 10 lines of the Perl regular expression /(french|fran\w+ais)/ in the HTML source.</S>
    <S sid="41" ssid="12">This helps filter out a page that contained, for example, a link to &#8220;English literature courses&#8221; and also contained an unrelated link to &#8220;French version&#8221; at the top.</S>
    <S sid="42" ssid="13">A sibling page is a page in one language that itself contains a link to a version of the same page in another language; for example, the page at the right of Figure 2 contains a link on the left that says &#8220;This page in english.&#8221; To perform this search for English pages matching a given French page, we request pages in French that match the Boolean expression anchor:&amp;quot;english&amp;quot; OR anchor:&amp;quot;anglais&amp;quot;.</S>
    <S sid="43" ssid="14">More recent versions of STRAND (unpublished) have added a &#8220;spider&#8221; component for locating pages that might have translations.</S>
    <S sid="44" ssid="15">Given a list of Web sites thought to contain bilingual text for a given language pair (e.g., sites identified using the AltaVista-based search), it is possible to download all the pages on each site, any Excerpts from a parent page (left) and a sibling page (right).</S>
    <S sid="45" ssid="16">The parent page is in Italian and contains links marked &#8220;Italiano/Italian,&#8221; &#8220;Francese/French,&#8221; and &#8220;Inglese/English.&#8221; The sibling page is in Dutch and contains a link marked &#8220;This page in english&#8221; in the leftmost column. of which might have a translation on that site.</S>
    <S sid="46" ssid="17">Although simple to implement, this method of locating pages shifts the burden of narrowing down the possibilities to the process of generating candidate document pairs.</S>
    <S sid="47" ssid="18">The results reported here do not make use of the spider.</S>
    <S sid="48" ssid="19">2.1.2 Generating Candidate Pairs.</S>
    <S sid="49" ssid="20">Pairing up potentially translated pages is simple when a search engine has been used to generate parent or sibling pages: One simply pairs the two child pages to which the parent links, or the sibling page together with the page to which it links.</S>
    <S sid="50" ssid="21">When all the pages on a site are under consideration, the process is rather different.</S>
    <S sid="51" ssid="22">The simplest possibility is to separate the pages on a site into the two languages of interest using automatic language identification (Ingle 1976; Beesley 1988; Cavnar and Trenkle 1994; Dunning 1994), throwing away any pages that are not in either language, and then generate the cross product.</S>
    <S sid="52" ssid="23">This potentially leads to a very large number of candidate page pairs, and there is no particular reason to believe that most of them are parallel translations, other than the fact that they appear on the same Web site.</S>
    <S sid="53" ssid="24">The spider component of STRAND adds a URL-matching stage, exploiting the fact that the directory structure on many Web sites reflects parallel organization when pages are translations of each other.</S>
    <S sid="54" ssid="25">Matching is performed by manually creating a list of substitution rules (e.g., english &#8594; big5),1 and for each English URL, applying all possible rules to generate URLs that might appear on the list of pages for the other language.</S>
    <S sid="55" ssid="26">If such a URL is found, the pair with similar URLs is added to the list of candidate document pairs.</S>
    <S sid="56" ssid="27">For example, suppose an English-Chinese site contains a page with URL (http://mysite.com/english/home en.html), on which one combination of substitutions might produce the URL (http://mysite.com/big5/home ch.html).</S>
    <S sid="57" ssid="28">The original page and the produced URL are probably worth considering as a likely candidate pair.</S>
    <S sid="58" ssid="29">Owing to the combinatorics (an exponential number of possible substitutions), only a fixed number of substitution combinations can be tried per English URL; however, in Section 4.3 we describe a more scalable URL-matching algorithm.</S>
    <S sid="59" ssid="30">Another possible criterion for matching is the use of document lengths.</S>
    <S sid="60" ssid="31">Texts that are translations of one another tend to be similar in length, and it is reasonable to assume that for text E in language 1 and text F in language 2, length(E) &#8776; C&#183;length(F), where C is a constant tuned for the language pair.</S>
    <S sid="61" ssid="32">The use of a document length filter is described in Smith (2001), in which such a filter is shown, at the sentence level, to reduce the size of the search space exponentially in the confidence p in a (1 &#8722; p) confidence interval for a linear regression model with only linear loss of good pairs. relies on analysis of the pages&#8217; underlying HTML to determine a set of pair-specific structural values, and then uses those values to decide whether the pages are translations of one another.</S>
    <S sid="62" ssid="33">The first step in this process is to linearize the HTML structure and ignore the actual linguistic content of the documents.</S>
    <S sid="63" ssid="34">We do not attempt to exploit nonlinear structure (e.g., embedded chunks), for two reasons.</S>
    <S sid="64" ssid="35">First, we suspect that many HTML authors use tags for formatting text rather than for indicating document structure; therefore any &#8220;tree&#8221; structure is likely to be inconsistent or poorly matched.</S>
    <S sid="65" ssid="36">Second, we required the matching algorithm to be fast, and algorithms for aligning tree structures are more demanding than those for linear structures.</S>
    <S sid="66" ssid="37">Both documents in the candidate pair are run through a markup analyzer that acts as a transducer, producing a linear sequence containing three kinds of token: The chunk length is measured in nonwhitespace bytes, and the HTML tags are normalized for case.</S>
    <S sid="67" ssid="38">Attribute-value pairs within the tags are treated as nonmarkup text (e.g., &lt;FONT COLOR=&amp;quot;BLUE&amp;quot;&gt; produces [START:FONT] followed by [Chunk:12]).</S>
    <S sid="68" ssid="39">The second step is to align the linearized sequences using a standard dynamic programming technique (Hunt and McIlroy 1975).</S>
    <S sid="69" ssid="40">For example, consider two documents that begin as follows: Using this alignment, we compute four scalar values that characterize the quality of the alignment: dp The difference percentage, indicating nonshared material (i.e., alignment tokens that are in one linearized file but not the other). n The number of aligned nonmarkup text chunks of unequal length. r The correlation of lengths of the aligned nonmarkup chunks. p The significance level of the correlation r. The difference percentage (dp) quantifies the extent to which there are mismatches in the alignment: sequence tokens on one side that have no corresponding token on the other side.</S>
    <S sid="70" ssid="41">In the example above, one document contains an H1 header that is missing from the second document.</S>
    <S sid="71" ssid="42">Large numbers of such mismatches can indicate that the two documents do not present the same material to a great enough extent to be considered translations.</S>
    <S sid="72" ssid="43">This can happen, for example, when two documents are translations up to a point (e.g., an introduction), but one document goes on to include a great deal more content than another.</S>
    <S sid="73" ssid="44">Even more frequently, the difference percentage is high when two documents are prima facie bad candidates for a translation pair.</S>
    <S sid="74" ssid="45">The number of aligned nonmarkup text chunks (n) helps characterize the quality of the alignment.</S>
    <S sid="75" ssid="46">The dynamic programming algorithm tries to optimize the correspondence of identical tokens, which represent markup.2 As a side effect, the nonmarkup text chunks are placed in correspondence with one another (e.g., the &#8220;Emergency Exit&#8221; and &#8220;Sortie de Secours&#8221; chunks in the above example).</S>
    <S sid="76" ssid="47">The more such pairings are found, the more likely the candidate documents are to represent a valid translation pair.</S>
    <S sid="77" ssid="48">The remaining two parameters (r and p) quantify the extent to which the corresponding nonmarkup chunks are correlated in length.</S>
    <S sid="78" ssid="49">When two documents are aligned with one another and are valid translations, there is a reliably linear relationship in the length of translated chunks of text: short pieces correspond with short pieces, medium with medium, and long with long.</S>
    <S sid="79" ssid="50">The Pearson correlation coefficient r for the lengths will be closer to one when the alignment has succeeded in lining up translated pieces of text, and the p value quantifies the reliability of the correlation; for example, the standard threshold of p &lt; .05 indicates 95% confidence that the correlation was not obtained by chance.</S>
    <S sid="80" ssid="51">In our original work, we used fixed thresholds, determined manually by inspection of development (nontest) data for English-Spanish, to decide whether a candidate pair should be kept or filtered out.</S>
    <S sid="81" ssid="52">Thresholds of dp &lt; 20% and p &lt; 0.05 were used.</S>
    <S sid="82" ssid="53">As with most search tasks, performance at finding parallel Web pages can be evaluated using standard measures of precision and recall and by combining those figures using the F-measure.</S>
    <S sid="83" ssid="54">It is not possible for us to measure recall relative to the entire set of document pairs that should have been found; this would require exhaustive evaluation using the entire Web, or pooling results from a large number of different systems, as is done in the TREC information retrieval evaluations.</S>
    <S sid="84" ssid="55">Therefore, recall in this setting is measured relative to the set of candidate pairs that was generated.</S>
    <S sid="85" ssid="56">Since the &#8220;truth&#8221; in this task is a matter for human judgment, we rely on bilingual speakers to judge independently whether page pairs are actually translations of each other for any given test set.</S>
    <S sid="86" ssid="57">In our experience no bilingual speaker is completely comfortable saying that another person&#8217;s translation is a good translation, so in creating the gold standard, we instead ask, &#8220;Was this pair of pages intended to provide the same content in the two different languages?&#8221; Asking the question in this way leads to high rates of interjudge agreement, as measured using Cohen&#8217;s &#954; measure.</S>
    <S sid="87" ssid="58">2.2.1 Using Manually Set Parameters.</S>
    <S sid="88" ssid="59">Using the manually set thresholds for dp and n, we have obtained 100% precision and 68.6% recall in an experiment using STRAND to find English-French Web pages (Resnik 1999).</S>
    <S sid="89" ssid="60">In that experiment, 326 candidate pairs, randomly selected from a larger set of 16,763 candidates, were judged by two human annotators.</S>
    <S sid="90" ssid="61">The humans agreed (i.e., both marked a page &#8220;good&#8221; or both marked a page &#8220;bad&#8221;) on 261 page pairs (86 &#8220;good&#8221; and 175 &#8220;bad&#8221;), and it is relative to those 261 that we compute recall.</S>
    <S sid="91" ssid="62">A modified version of STRAND was used to obtain English-Chinese pairs (see related work, below), and in a similar formal evaluation, we found that the resulting set had 98% precision and 61% recall for Chinese (http://umiacs.umd.edu/&#8212;resnik/strand/).</S>
    <S sid="92" ssid="63">Both these results are consistent with our preliminary findings for English-Spanish using a less rigorous evaluation (using the judgments of the first author rather than independent bilingual evaluators) and a very small test set; precision in this preliminary experiment was near ceiling and recall was in the vicinity of 60% (Resnik 1998).</S>
    <S sid="93" ssid="64">2.2.2 Assessing the STRAND Data.</S>
    <S sid="94" ssid="65">Although our focus here is finding parallel text, not using it, a natural question is whether parallel text from the Web is in fact of value.</S>
    <S sid="95" ssid="66">Two sources of evidence suggest that it is.</S>
    <S sid="96" ssid="67">First, Web-based parallel corpora have already demonstrated their utility in crosslanguage information retrieval experiments.</S>
    <S sid="97" ssid="68">Resnik, Oard, and Levow (2001) showed that a translation lexicon automatically extracted from the French-English STRAND data could be combined productively with a bilingual French-English dictionary in order to improve retrieval results using a standard cross-language IR test collection (English queries against the CLEF-2000 French collection, which contains approximately 21 million words from articles in Le Monde).</S>
    <S sid="98" ssid="69">During document translation, backing off from the dictionary to the STRAND translation lexicon accounted for over 8% of the lexicon matches (by token), reducing the number of untranslatable terms by a third and producing a statistically significant 12% relative improvement in mean average precision as compared to using the dictionary alone.</S>
    <S sid="99" ssid="70">Similarly, Nie and Cai (2001) have demonstrated improved cross-language IR results for English and Chinese using data gathered by the PTMiner system (Chen and Nie 2000), a related approach that we discuss in Section 2.3.</S>
    <S sid="100" ssid="71">Second, since bag-of-words IR experiments are not very illuminating with respect to fluency and translation quality, we conducted a ratings-based assessment of EnglishChinese data, asking two native Chinese speakers (who are fluent in English) to assign ratings to a set of English-Chinese items.</S>
    <S sid="101" ssid="72">The set contained: The human-translated and machine-translated pairs were included in order to provide upper-bound and lower-bound comparisons.</S>
    <S sid="102" ssid="73">The items were presented to one judge in a random order, and to the other judge in the reverse order.</S>
    <S sid="103" ssid="74">Chinese-English Web data were those collected by Jinxi Xu using a modified version of STRAND (see Section 2.3), excluding those that did not pass STRAND&#8217;s structural filter with the manually set thresholds.</S>
    <S sid="104" ssid="75">Sentence-like chunk pairs were defined as those in which the English side was 5&#8211;50 whitespace-delimited tokens long and that began with an uppercase alphabetic character and contained at least one token from an English stop list.3 This fairly strict filter provided a set of approximately 7,000 pairs from which the 30 test items were sampled.</S>
    <S sid="105" ssid="76">Participants were asked to provide each pair of items with three ratings, assessing English fluency, Chinese fluency, and adequacy of the translation.</S>
    <S sid="106" ssid="77">The choice and wording of the ratings criteria were derived from the human evaluation measures proposed by Dabbadie et al. (2002), with the wording of the translation assessment criterion modified to eliminate references to the direction of translation.</S>
    <S sid="107" ssid="78">(See Appendix B.)</S>
    <S sid="108" ssid="79">For all three measures, the two judges&#8217; ratings were significantly correlated (p &lt; 0.0001).</S>
    <S sid="109" ssid="80">Figure 3 shows additional quantitative results of the assessment, comparing judgments among human-translated, Web-generated, and machine-translated data.</S>
    <S sid="110" ssid="81">The ratings indicate that pairs from the Web contain on average somewhere between &#8220;mostly the same meaning&#8221; and &#8220;entirely the same meaning&#8221; (median 3.25).4 In comparison, current commercial-quality machine translation output achieves performance only between &#8220;much of the same meaning&#8221; and &#8220;mostly the same meaning&#8221; (median 2.5).</S>
    <S sid="111" ssid="82">Moreover, it is very likely that the Web translation quality is an underestimate: Some of the low-scoring outliers within the Web data could be eliminated by using state-of-the-art sentence alignment techniques and automatic detection and Translation adequacy ratings: Distribution over scores for human-translated (Human), Web-generated (WWW), and machine-translated (MT) data.</S>
    <S sid="112" ssid="83">The left plot provides results for judge 1, the right plot for judge 2. elimination of noisy pairs at the sentence level (cf.</S>
    <S sid="113" ssid="84">Nie and Cai [2001]).</S>
    <S sid="114" ssid="85">We observe that the distribution of scores for Web data peaks at the highest rating and that the data are in both cases modestly bimodally distributed.</S>
    <S sid="115" ssid="86">Machine-translated pairs, on the other hand, have generally lower quality.</S>
    <S sid="116" ssid="87">This suggests that high-quality parallel translations are present in this corpus and that poor-quality parallel translations are very poor (whether because of misalignment or simply because of poor translation quality at the document level) and might therefore be easily distinguishable from the better material.</S>
    <S sid="117" ssid="88">We plan to address this in future work.</S>
    <S sid="118" ssid="89">Qualitatively, the results are a source of optimism about parallel data from the Web.</S>
    <S sid="119" ssid="90">Looking monolingually, fluency of the English side is statistically comparable to that of English sentences from the FBIS parallel corpus (essentially at ceiling), and on average the Chinese Web data are judged somewhere between &#8220;fairly fluent&#8221; and &#8220;very fluent,&#8221; with the median at &#8220;very fluent&#8221; (only the second judge found the fluency of the Chinese Web sentences to be significantly worse than the human-generated Chinese sentences, Mann-Whitney test, p &lt; 0.02). several language pairs, it appears that STRAND&#8217;s structure-based filter consistently throws out around one-third of the candidate document pairs it has found in order to maintain its precision in the 98&#8211;100% range.</S>
    <S sid="120" ssid="91">It does so by respecting parameter thresholds that were determined manually using English-Spanish development data; the same parameters seem to have worked reasonably well not only for EnglishSpanish, but also for English-French and English-Chinese pairs.</S>
    <S sid="121" ssid="92">It is possible, however, that classification can be tuned for better performance.</S>
    <S sid="122" ssid="93">In order to investigate this possibility, we took a machine-learning approach: We used the four structural values (dp, n, r, and p) as features characterizing each document pair and treated the problem as a binary decision task, using supervised learning to make an attempt at better predicting human judgments.</S>
    <S sid="123" ssid="94">Using the English-French data, we constructed a ninefold cross-validation experiment using decision tree induction to predict the class assigned by the human judges.</S>
    <S sid="124" ssid="95">The decision tree software was the widely used C5.0 (http://www.rulequest.com/ demoeula.html).</S>
    <S sid="125" ssid="96">We used a decision tree learner because it is transparent (it is easy to see which features are being used to classify page pairs).</S>
    <S sid="126" ssid="97">In addition, a decision tree produced by C5.0 can be translated into a fast C program that is a rapid classifier of document pairs.</S>
    <S sid="127" ssid="98">Each fold had 87 test items and 174 training items; the fraction of good and bad pairs in each fold&#8217;s test and training sets was roughly equal to the overall division (33% to 67%, respectively).</S>
    <S sid="128" ssid="99">Precision and recall results are reported in Table 1, together with baseline results from STRAND&#8217;s untuned classifier as reported above.</S>
    <S sid="129" ssid="100">Looking at the decision trees learned, we see that they are very similar to one another.</S>
    <S sid="130" ssid="101">In every case a tree that looked like the following was learned: where the remaining branch involved various additional partitionings of the candidate pairs that had few aligned text chunks (small n) and relatively low (but perhaps unreliable) difference percentage (dp).</S>
    <S sid="131" ssid="102">That branch handled around 10% of the document set and was prone to overtraining.</S>
    <S sid="132" ssid="103">(The documents handled by this branch were mostly marked &#8220;bad&#8221; by the judges but appear to have been difficult to classify based on the structural features.)</S>
    <S sid="133" ssid="104">Note that the learned classifiers were substantially different from the heuristic threshold used earlier.</S>
    <S sid="134" ssid="105">Without tuning, the manually set parameters result in good document pairs&#8217; being discarded 31% of the time.</S>
    <S sid="135" ssid="106">Our cross-validation results indicate that tuning the parameters cuts that figure in half: Only 16% of the good pairs will be discarded, at a cost of admitting 4 false positives from every 100 candidate pairs.</S>
    <S sid="136" ssid="107">This approach is quite general and uses only a minimum of language-dependent knowledge.</S>
    <S sid="137" ssid="108">The features we are using are the same for any language pair.</S>
    <S sid="138" ssid="109">The tuning process needs to be done only once per language pair and requires only a few hours of annotation from untrained speakers of both languages to obtain the small labeled sample.</S>
    <S sid="139" ssid="110">Several other systems for discovering parallel text, developed independently, can be described as operating within the same three-stage framework as STRAND.</S>
    <S sid="140" ssid="111">Parallel Text Miner (PTMiner) (Chen and Nie 2000) exploits already-existing Web search engines to locate pages by querying for pages in a given language that contain links to pages that are likely to be in the other language of interest.</S>
    <S sid="141" ssid="112">Once bilingual sites are located, they are crawled exhaustively.</S>
    <S sid="142" ssid="113">In order to generate candidate pairs, PTMiner uses a URL-matching process similar to the one described above; for example, the French translation of a URL like (http://www.foo.ca/english-index.html) might be (http://www.foo.ca/french-index.html).</S>
    <S sid="143" ssid="114">PTMiner&#8217;s matching process uses a mapping of language-specific prefixes and suffixes and does not handle cases in which URL matching requires multiple substitutions.</S>
    <S sid="144" ssid="115">PTMiner also applies a length filter and automatic language identification to verify that the pages are in the appropriate languages.</S>
    <S sid="145" ssid="116">Chen and Nie report a 95% precise English-French corpus of 118MB/135MB of text and a 90% precise English-Chinese corpus of 137MB/117MB of text, based on inspection.</S>
    <S sid="146" ssid="117">Later versions of PTMiner include a final filtering stage to clean the extracted corpus; Nie and Cai (2001) independently used features similar to those described here to eliminate noisy pairs.</S>
    <S sid="147" ssid="118">Specifically, they used a file length ratio filter, the proportion of sentences that are aligned to empty (after a sentence alignment algorithm is applied), and a criterion that rewards sentence pairs that contain elements from a bilingual dictionary.</S>
    <S sid="148" ssid="119">Nie and Cai showed that hand-tuned combination of these criteria improved the quality of their parallel English-Chinese corpus by 8% (F-measure) at the text level.</S>
    <S sid="149" ssid="120">Bilingual Internet Text Search (BITS) (Ma and Liberman 1999) starts with a given list of domains to search for parallel text.</S>
    <S sid="150" ssid="121">It operates by sampling pages from each domain and identifying their languages; if a domain is deemed to be multilingual, all pages on the site are crawled exhaustively.</S>
    <S sid="151" ssid="122">BITS appears to consider all possible combinations of Web page pairs in the two languages (i.e., the full cross product within each site) and filters out bad pairs by using a large bilingual dictionary to compute a content-based similarity score and comparing that score to a threshold.</S>
    <S sid="152" ssid="123">For each page pair, the similarity score is number of translation token pairs number of tokens in A Translation token pairs are considered within a fixed window (i.e., a distance-based measure of co-occurrence is used).5 In addition to cross-lingual lexical matching, BITS filters out candidate pairs that do not match well in terms of file size, anchors (numbers, acronyms, and some named entities), or paragraph counts.</S>
    <S sid="153" ssid="124">Using an EnglishGerman bilingual lexicon of 117,793 entries, Ma and Liberman report 99.1% precision and 97.1% recall on a hand-picked set of 600 documents (half in each language) containing 240 translation pairs (as judged by humans).</S>
    <S sid="154" ssid="125">This technique yielded a 63MB parallel corpus of English-German.</S>
    <S sid="155" ssid="126">Other work on Web mining has been done by Jinxi Xu of BBN (personal communication), who began with our STRAND implementation and added a module for automatically learning string substitution patterns for URLs and also implemented a different dynamic programming algorithm for assessing structural alignment.</S>
    <S sid="156" ssid="127">Xu used the modified STRAND to obtain 3,376 Chinese-English document pairs, which we evaluated formally (see above), determining that the set has 98% precision and 61% recall.</S>
    <S sid="157" ssid="128">In addition, STRAND has been reimplemented by David Martinez and colleagues at Informatika Fakultatea in the Basque Country (personal communication), in order to perform exploratory experiments for discovering English-Basque document pairs.</S>
    <S sid="158" ssid="129">It is worth noting that STRAND, PTMiner, and BITS are all largely independent of linguistic knowledge about the particular languages, and therefore very easily ported to new language pairs.</S>
    <S sid="159" ssid="130">With the exception of the use of a bilingual dictionary (in BITS and later versions of PTMiner), these systems require, at most, a set of URL substring patterns for the URL pattern-matching stage (e.g., big5 &#8764; english in the example above; see further discussion in Section 4.3), and a modest amount of monolingual data for training n-gram-based language identifiers (typically 50,000 to 100,000 characters of text per language).</S>
    <S sid="160" ssid="131">Word-level translations are worth exploiting when they are available.</S>
    <S sid="161" ssid="132">In Section 3 we describe a bitext-matching process using a content-based similarity score grounded in information theory, and in Section 5 we show how structural and content-based criteria can be combined in order to obtain performance superior to that obtained using either method alone.</S>
    <S sid="162" ssid="133">5 Many details of this technique are left unspecified in Ma and Liberman (1999), such as the threshold for the similarity score, the distance threshold, and matching of non-one-word to one-word entries in the dictionary.</S>
    <S sid="163" ssid="134">An example of two texts with links shown.</S>
    <S sid="164" ssid="135">There are seven link tokens, five of which are lexical (non-NULL) in X (the English side), six in Y (French).</S>
  </SECTION>
  <SECTION title="3." number="4">
    <S sid="165" ssid="1">The approach discussed thus far relies heavily on document structure.</S>
    <S sid="166" ssid="2">However, as Ma and Liberman (1999) point out, not all translators create translated pages that look like the original page.</S>
    <S sid="167" ssid="3">Moreover, structure-based matching is applicable only in corpora that include markup, and there are certainly multilingual collections on the Web and elsewhere that contain parallel text without structural tags.</S>
    <S sid="168" ssid="4">Finally, other applications for translation detection exist, such as subdocument text alignment and cross-lingual duplicate detection (i.e., location of already-existing translations in a multilingual corpus).</S>
    <S sid="169" ssid="5">All these considerations motivate an approach to matching translations that pays attention to similarity of content, whether or not similarities of structure exist.</S>
    <S sid="170" ssid="6">We present here a generic score of translational similarity that is based upon any word-to-word translation lexicon (hand-crafted or automatically generated, or a combination, and possibly highly noisy).</S>
    <S sid="171" ssid="7">The technique is shown to perform competitively to the structure-based approach of STRAND on the task of identifying EnglishFrench document translations.</S>
    <S sid="172" ssid="8">We define a cross-language similarity score, tsim, for two texts by starting with a generative, symmetric word-to-word model of parallel texts (Melamed&#8217;s [2000] Method A).</S>
    <S sid="173" ssid="9">Let a link be a pair (x, y) in which x is a word in language L1 and y is a word in L2.</S>
    <S sid="174" ssid="10">The model consists of a bilingual dictionary that gives a probability distribution p over all possible link types.</S>
    <S sid="175" ssid="11">Within a particular link, one of the words may be NULL, but not both.</S>
    <S sid="176" ssid="12">In the generative process, a sequence of independent link tokens is sampled from the distribution.</S>
    <S sid="177" ssid="13">The model does not account for word order.</S>
    <S sid="178" ssid="14">An example of two texts with links is illustrated in Figure 4.</S>
    <S sid="179" ssid="15">Next, we desire to compute the probability of the most probable link sequence that could have accounted for the two texts.6 The probability of a link sequence is simply the product of the probabilities p of the links it contains.</S>
    <S sid="180" ssid="16">As noted by Melamed (2000), the problem of finding the best set of links is the maximum-weighted bipartite matching (MWBM) problem: Given a weighted bipartite graph G = (V1 U V2, E) with edge weights ci,j(i E V1,j E V2), find a matching M C_ E such that each vertex has at most one edge in M and Ee&#8712;M ci,j is maximized.</S>
    <S sid="181" ssid="17">The fastest known MWBM algorithm runs in O(ve+v2 log v) time (Ahuja, Magnati, and Orlin 1993).</S>
    <S sid="182" ssid="18">Applied to this problem, that is O(max(|X|, |Y|)3), where X and Y are the text lengths in words.</S>
    <S sid="183" ssid="19">To use MWBM to find the most probable link sequence, let the L1 words be V1 and the L2 words be V2.</S>
    <S sid="184" ssid="20">If two words x, y have p(x, y) &gt; 0, an edge exists between them with weight logp(x,y).</S>
    <S sid="185" ssid="21">If a word x (or y) may link to NULL with nonzero probability, then that potential link is added as an additional edge in the graph between x (or y) and a NULL vertex added to V2 (or V1).</S>
    <S sid="186" ssid="22">Each such x (or y) gets its own NULL vertex, so that multiple words may ultimately link to NULL.</S>
    <S sid="187" ssid="23">A sum of weights of links in a matching will be the log-probability of the (unordered) link sequence, and maximizing that sum maximizes the probability.</S>
    <S sid="188" ssid="24">The similarity score should be high when many of the link tokens in the best sequence do not involve NULL tokens.</S>
    <S sid="189" ssid="25">Further, it should normalize for text length.</S>
    <S sid="190" ssid="26">Specifically, the score is This score is an application of Lin&#8217;s (1998) information-theoretic definition of similarity.</S>
    <S sid="191" ssid="27">Starting with a set of axioms relating intuitions about similarity to the mathematical notion of mutual information (Shannon 1948), Lin derives the measure where X and Y are any objects generated by a probabilistic model.</S>
    <S sid="192" ssid="28">This technique of using a translation model to define translational similarity is generic to different sources of lexical translation information.</S>
    <S sid="193" ssid="29">An important feature is that it can be used with any symmetric translation model in which events can be divided into those that both sides of a bitext have in common and those that affect only one side.</S>
    <S sid="194" ssid="30">The measure is simplified by assuming that all links in a given translation lexicon are equiprobable.</S>
    <S sid="195" ssid="31">The assumption reduces the formula for tsim to number of two-word links in best matching (4) number of links in best matching The key reason to compute tsim under the equiprobability assumption is that we need not compute the MWBM, but may find just the maximum cardinality bipartite matching (MCBM), since all potential links have the same weight.</S>
    <S sid="196" ssid="32">An O(e.V/v) (or &#65533; O(|X |- |Y |- |X |+ |Y|) for this purpose) algorithm exists for MCBM (Ahuja, Magnati, and Orlin 1993).</S>
    <S sid="197" ssid="33">For example, if the matching shown in Figure 4 is the MCBM (for some translation lexicon), then tsim(X, Y) = 47 under the simplifying assumption.</S>
    <S sid="198" ssid="34">In earlier work (Smith 2002), we sought to show how multiple linguistic resources could be exploited in combination to recognize translation, and how the equiprobability assumption allowed straightforward combination of resources (i.e., set union of translation lexicon entries).</S>
    <S sid="199" ssid="35">In Section 3.2.1 we provide a clean solution to the problem of using unweighted translation lexicons along with probabilistic ones that improves performance over the earlier result.</S>
    <S sid="200" ssid="36">This would appear to make the equiprobability assumption unnecessary (apart from concerns about computational expense).</S>
    <S sid="201" ssid="37">However, we found that, if p(x,y) is set to the empirically estimated joint probability of the lexical link type (x, y), then performance turns out to be dismal.</S>
    <S sid="202" ssid="38">This is understandable: Using parameter estimation techniques like the one we used, a great deal of probability mass in the distribution p tends to go to frequent words, which are relatively uninformative with regard to whether texts are mutual translations.</S>
    <S sid="203" ssid="39">The equiprobability assumption helps to counteract this; in fact one could apply scoring techniques from information retrieval and cross-lingual information retrieval in weighting the lexicon.</S>
    <S sid="204" ssid="40">We leave this area of exploration to future work.</S>
    <S sid="205" ssid="41">Melamed (2000) used a greedy approximation to MWBM called competitive linking.</S>
    <S sid="206" ssid="42">Competitive linking iteratively selects the edge with the highest weight, links the two vertices of the edge, then removes them from the graph.</S>
    <S sid="207" ssid="43">(Ties are broken at random.)</S>
    <S sid="208" ssid="44">A heap-based implementation of competitive linking runs in O(max(|X|, |Y|) log max(|X|, |Y|)).</S>
    <S sid="209" ssid="45">Under the equiprobability assumption, all the weights are the same, so that competitive linking proceeds simply by randomly making legal links (those allowed by the translation lexicon) until no more can be made.</S>
    <S sid="210" ssid="46">If definition (4) is applied to pairs of documents in the same language, with a &#8220;translation lexicon&#8221; defined by the identity relation, then tsim is a variant of resemblance (r), as defined by Broder et al. (1997) for the problem of monolingual duplicate detection, except that tsim has the advantage of being token-based rather than typebased, incorporating word frequency.</S>
    <S sid="211" ssid="47">We have demonstrated that the tsim score can be used to extract translationally equivalent English-Chinese sentence pairs from even a noisy space with high precision (Smith 2002).</S>
    <S sid="212" ssid="48">It was also shown that combining multiple sources of word-level translation information (dictionaries, word-to-word translation models, cognates) had positive effects on performance on the sentence-matching task.</S>
    <S sid="213" ssid="49">These information sources were presumed to be extremely noisy (they are so presumed here, as well), though no independent evaluation was carried out on them.</S>
    <S sid="214" ssid="50">If the ad hoc translation lexicon induction techniques used here give good performance, then better techniques might lead to further improvement.</S>
    <S sid="215" ssid="51">In addition, the competitive linking approximation was shown to perform nearly as well as MCBM.</S>
    <S sid="216" ssid="52">We now apply our content-based similarity measure to the candidate pair classification task presented by STRAND.</S>
    <S sid="217" ssid="53">Recall that both the original STRAND classifier and those learned using decision tree methods, described in Section 2.2.3, employ only structural features of the documents to determine whether they are translations.</S>
    <S sid="218" ssid="54">Here we apply the tsim score to the same task and compare the results with those of the original STRAND classifier. sources.</S>
    <S sid="219" ssid="55">We begin with an English-French dictionary (a total of 34,808 entries, 4,021 of which are not one-to-one).7 Next, a word-to-word translation model (Melamed 2000) was trained on the dictionary.</S>
    <S sid="220" ssid="56">Note that the parameter estimation task here is very simple; in most cases the pairs are one-word to one-word, making the hidden link structure unambiguous (modulo NULLs).</S>
    <S sid="221" ssid="57">The training primarily served the purpose of breaking down multiword entries, informed by the rest of the entries, so as to obtain a fully one-word-to-one-word dictionary.</S>
    <S sid="222" ssid="58">The training procedure was an expectationmaximization (EM) procedure like that used by Melamed (2000), except that maximum weighted bipartite matching was used instead of competitive linking.</S>
    <S sid="223" ssid="59">Any entry containing a NULL was then removed.</S>
    <S sid="224" ssid="60">We add to this dictionary a list of English-French cognate pairs, identified using the method of Tiedemann (1999).</S>
    <S sid="225" ssid="61">Tiedemann&#8217;s approach involved learning languagespecific character weights for the computation of weighted edit distance to measure cognateness.</S>
    <S sid="226" ssid="62">He used a list of known cognates to train the weights.</S>
    <S sid="227" ssid="63">We instead used the weighted translation pairs in a translation model lexicon built from the Bible.8 The result is 35,513 word pairs from the corpus of Web pages under consideration.</S>
    <S sid="228" ssid="64">An additional set of 11,264 exact string matches were added (also from the corpus of Web pages).</S>
    <S sid="229" ssid="65">Qualitatively, these entries were highly noisy; a random selection of the cognate pairs is shown in Table 2.</S>
    <S sid="230" ssid="66">All of these word pairs were added to the dictionary, each with a count of one.</S>
    <S sid="231" ssid="67">We took the enhanced dictionary with counts to define a Dirichlet prior, which is the conjugate prior to a multinomial distribution over discrete events (like the distribution p over link types we seek to estimate) (MacKay and Peto 1995).</S>
    <S sid="232" ssid="68">Such a prior is characterized by counts of all such events; when it is used in an EM procedure, these prior counts are added to those produced by the E step on every iteration.</S>
    <S sid="233" ssid="69">Intuitively, if a word pair (x, y) is expected to be a likely lexical word pair in the dictionary and cognate set, then models that make (x, y) probable are more likely (according to the prior).</S>
    <S sid="234" ssid="70">Therefore the expected count of (x, y) is increased at each iteration of training to the extent that the prior favors it.</S>
    <S sid="235" ssid="71">Using the enhanced, weighted lexicon as a Dirichlet prior (containing 77,699 entries and a total count of 85,332), a word-to-word translation model (Melamed 2000) was trained on a verse-aligned Bible (15,548 verses, averaging 25.5 English words, 23.4 French words after tokenization).</S>
    <S sid="236" ssid="72">As before, we used the maximum weighted bipartite matching algorithm.</S>
    <S sid="237" ssid="73">The final lexicon consists of all word pairs with nonzero probability and contains 132,155 entries.</S>
    <S sid="238" ssid="74">Note that all word pairs in the enhanced dictionary are included; we have merely added to that dictionary by bootstrapping additional entries from the Bible.</S>
    <S sid="239" ssid="75">3.2.2 Results.</S>
    <S sid="240" ssid="76">In order to compare tsim with structural similarity scoring, we applied it to 325 English-French Web document pairs for which human evaluations were carried out in Section 2.</S>
    <S sid="241" ssid="77">As there is only one feature under consideration (tsim), the classifier must be a threshold on that value.</S>
    <S sid="242" ssid="78">At different thresholds, Cohen&#8217;s &#954; score of agreement (with each of Resnik&#8217;s (1999) two judges and their intersection) may be computed for comparison with STRAND, along with recall and precision against a gold standard (for which we use the intersection of the judges: the set of examples for which the judges agreed).</S>
    <S sid="243" ssid="79">The gold standard contained 86 page pairs marked as &#8220;good&#8221; by both judges and 174 page pairs marked as &#8220;bad&#8221; by both judges.9 Computing tsim (MCBM on the words in the document pair) is not tractable for very large documents and translation lexicons.</S>
    <S sid="244" ssid="80">However, in preliminary comparisons, we found that representing tsim for long documents by as few as their first 500 words results in excellent performance on the r. measure.10 This allows O(1) estimation of tsim for two documents.</S>
    <S sid="245" ssid="81">Further, the competitive linking algorithm appears to be as reliable as MCBM, and it runs significantly faster in practice.</S>
    <S sid="246" ssid="82">The results reported here approximated tsim in this way.</S>
    <S sid="247" ssid="83">Of the 325 pairs, 32 were randomly selected as a development set, which we used to select manually a threshold T = 0.44.</S>
    <S sid="248" ssid="84">This value maximized the r. score against goldstandard human judgments on the development set.11 r. scores against each judge and their intersection were then computed at that threshold on the test set (the remaining 293 pairs).</S>
    <S sid="249" ssid="85">These are compared to r. scores of the STRAND system (with original, untuned parameters), on the same test set, in Table 3.</S>
    <S sid="250" ssid="86">In every case, the tsim classifier agreed more strongly with the human evaluations, and its F score is higher than that of STRAND.</S>
    <S sid="251" ssid="87">Figure 5 shows r. and the F measure plotted against T. In this application, the content-based classifier (at its approximate best performance, thresholding at 0.44) complements the structural classifier&#8217;s high precision.</S>
    <S sid="252" ssid="88">Given two high-performing methods that use orthogonal information for identifying good candidate pairs (one using only structure, the other using only content), the natural question is whether the techniques can be combined for even better performance.</S>
    <S sid="253" ssid="89">We repeated the experiment presented in Section 2.2.3, adding the tsim score as a feature.</S>
    <S sid="254" ssid="90">The same cross-validation setup was used, with the same division into folds.</S>
    <S sid="255" ssid="91">Precision and recall results are reported in Table 4.</S>
    <S sid="256" ssid="92">The decision trees learned were once again all quite similar, with eight of the nine rooted as follows (a few of the trees differed slightly in the numerical values used as thresholds): The remainder of each tree varied, and there was some evidence of overtraining.</S>
    <S sid="257" ssid="93">These results clearly demonstrate the benefit of combining structural and contentbased approaches.</S>
    <S sid="258" ssid="94">We next describe how we have adapted the STRAND architecture to the Internet Archive, in order to generate the candidate pairs on a scale that has previously been unattainable.</S>
  </SECTION>
  <SECTION title="4." number="5">
    <S sid="259" ssid="1">One of the difficulties with doing research on Web mining is that large-scale crawling of the Web is a significant enterprise.</S>
    <S sid="260" ssid="2">Chen and Nie&#8217;s (2000) PTMiner represents one solution, a carefully thought-out architecture for mining on a large scale.</S>
    <S sid="261" ssid="3">Here we present a different solution, taking advantage of an existing large-scale repository of Web pages maintained on an ongoing basis by an organization known as the Internet Archive.</S>
    <S sid="262" ssid="4">The Internet Archive (http://www.archive.org/web/researcher/) is a nonprofit organization attempting to archive the entire publicly available Web, preserving the content and providing free access to researchers, historians, scholars, and the general public.</S>
    <S sid="263" ssid="5">Data come from crawls done by Alexa Internet, and hence they represent an industry-level resource of the sort not easily constructed within academia.</S>
    <S sid="264" ssid="6">At present, the Archive contains 120TB (terabytes) of data, by a conservative estimate, and it is growing at approximately 8TB per month.</S>
    <S sid="265" ssid="7">Text on the archive comprises over 10 billion Web pages, and the estimated duplicate rate is a factor of two (i.e., two copies of everything).12 The Internet Archive provides public access to the data via the Wayback Machine Web interface.</S>
    <S sid="266" ssid="8">As of this writing, a search for the ACL home page brings up links to 72 snapshots of that page dating back to June 7, 1997.13 The reader can get to that page directly on the Wayback Machine using a URL that points to the Internet Archive and provides both the desired page and the time stamp indicating which snapshot to retrieve.14 The Archive also provides researchers with free, direct access to its data via accounts on their cluster.</S>
    <S sid="267" ssid="9">The data are stored on the disk drives of approximately 300 machines, each running some variety of UNIX, creating what is in essence one huge file system.</S>
    <S sid="268" ssid="10">This provides a researcher with the remarkable sensation of having the entire Web on his or her hard drive.</S>
    <S sid="269" ssid="11">Mining terabytes on the Archive presents a number of challenges: The last of these, the cluster computing tools,15 is turned out to reduce drastically the time needed to port STRAND to the Archive, as well as the size of the STRAND code base.</S>
    <S sid="270" ssid="12">The centerpiece in Archive cluster computing is a parallelization tool called p2, which offers a UNIX command-line interface that allows one to specify (1) a parallelizable task, (2) a way to split it up, (3) a way to combine the results, and (4) a set of processors among which to divide the task.</S>
    <S sid="271" ssid="13">The p2 tool divides up tasks intelligently, invoking each parallel computation on the local machine where the data reside.</S>
    <S sid="272" ssid="14">In adapting STRAND&#8217;s three-stage process to the Internet Archive, the primary challenge was in the first two steps, locating possible translations and matching them up to produce candidate document pairs.</S>
    <S sid="273" ssid="15">Structural filtering remained essentially unchanged.</S>
    <S sid="274" ssid="16">Generating candidate pairs on the Archive involves the following steps: Steps 1 and 2 are performed via a parallel search operation plus combination of results; for example, extracting all URLs in the Hong Kong, Taiwan, or China domains (and their associated bookkeeping data) using a pattern like /(.hk|.tw|.cn)/.16 Step 3 is potentially tricky owing to computational complexity issues.</S>
    <S sid="275" ssid="17">As noted in Section 2.1.2, examining the cross product of a site&#8217;s page sets in two different languages is potentially very expensive, and matching documents by similarity of URLs can represent a combinatoric process in the general case.</S>
    <S sid="276" ssid="18">Example of LSS subtraction.</S>
    <S sid="277" ssid="19">We arrived at an algorithmically simple solution that avoids this problem but is still based on the idea of language-specific substrings (LSSs).</S>
    <S sid="278" ssid="20">The idea is to identify a set of language-specific URL substrings that pertain to the two languages of interest, (e.g., based on language names, countries, character codeset labels, abbreviations, etc.).</S>
    <S sid="279" ssid="21">For example, a set of LSSs for English-Arabic might be as follows (those containing numerals correspond to character code sets): 1256, 437, 864, 8859-1, 8859-6, a, ar, ara, arab, arabic, cp1256, cp437, cp864, e, en, eng, english, gb, iso, iso-8859-1, iso-8859-6, latin, latin-1, latin1, uk, us, usa For each URL, we form a &#8220;handle&#8221; by subtracting any substrings that match (insensitive to case) any item on the LSS pattern list.</S>
    <S sid="280" ssid="22">The subtraction process is implemented reasonably efficiently: If there are p patterns with maximum length l, and the URL&#8217;s length in characters is u, then the current implementation will do at most p &#215; u string matches of length no more than l.17 (Currently we use C strcmp for string matching.)</S>
    <S sid="281" ssid="23">In practice, this is extremely fast: We can generate handles for nearly 5,000 URLs per second on a six-year-old Sun Ultra 1 workstation.</S>
    <S sid="282" ssid="24">Figure 6 illustrates handle generation on two real URLs.</S>
    <S sid="283" ssid="25">As one would hope, these two URLs produce the same handle, and as a result, they wind up in the same bucket in step 3.18 In step 4, the URLs in each bucket are used to generate candidate pairs by taking the cross product and keeping those URL pairs for which the URL bookkeeping data indicate pages that are in the correct languages.</S>
    <S sid="284" ssid="26">For example, given the bucket containing the two URLs in Figure 6, this step would generate a single pair consisting of be more efficient to create buckets by doing a parallel sort of the entire URL set using the handle as the key, and then creating buckets based on identical handles&#8217; being on adjacent lines. the URL for the English page and the URL for the Arabic page, assuming the language ID information associated with each URL confirmed it was in the proper language.19 At this point, the candidate generation process is complete.</S>
    <S sid="285" ssid="27">The final step is to apply STRAND&#8217;s filtering step to each candidate pair, an operation that can itself be parallelized, since each candidate pair can be processed independently.</S>
    <S sid="286" ssid="28">The filtering pass will eliminate those page pairs (roughly 10% in our experience) whose URLs show similarity to each other but whose content and/or structure do not.</S>
    <S sid="287" ssid="29">It is interesting to note that by taking advantage of the Archive&#8217;s p2 cluster computing tool, together with its simple flat-text representations, adapting STRAND&#8217;s candidate generation process resulted in a dramatic reduction in the size of the program, cutting it literally in half, as measured in lines of code.</S>
  </SECTION>
  <SECTION title="5." number="6">
    <S sid="288" ssid="1">In the previous sections, we have described methods and results for structural matching, for content-based matching, and for dramatically scaling up the number of candidate pairs that can be generated for any given language pair by using the industrialstrength Web crawls stored on the Internet Archive.</S>
    <S sid="289" ssid="2">In this section we put all these pieces together, describing an experiment in mining the Internet Archive to find English-Arabic parallel text.</S>
    <S sid="290" ssid="3">The language pair English-Arabic is of particular global importance, but resources for it, particularly bilingual text, have generally not been easy to obtain.</S>
    <S sid="291" ssid="4">Moreover, Arabic is far behind on the Web&#8217;s exponential growth curve: Arabic text (as opposed to images) did not really start emerging on the Web until the release of Microsoft Windows 98, which provided Arabic support in its version of Internet Explorer.20 The input resources for our search for English-Arabic candidate pairs were a list of Internet domains likely to contain Arabic text.21 The list included 24 top-level national domains for countries where Arabic is spoken by a significant portion of the population: Egypt (.eg), Saudi Arabia (.sa), Kuwait (.kw), etc.</S>
    <S sid="292" ssid="5">In addition, we used a list of .com domains known to originate in Arabic-speaking countries.</S>
    <S sid="293" ssid="6">This list provided an additional 21 specific domains (e.g., (emiratesbank.com), (checkpoint.com)); note that the list is by no means exhaustive.</S>
    <S sid="294" ssid="7">In the experiments we report here, we mined two crawls from 2001, comprising 8TB and 12TB (i.e., less than one-sixth of the Archive as it existed at the time of the mining effort in early 2002) spread over 27 machines.</S>
    <S sid="295" ssid="8">Our list of URLs with relevant domains, obtained through pattern matching in Archive index files, numbers 19,917,923 pages.22 The language-specific substrings given earlier were subtracted from these URLs to generate handles, resulting in 786,880 buckets with an average of 25 pages per bucket.</S>
    <S sid="296" ssid="9">When all possible English-Arabic page pairs were generated from all 19 The Internet Archive tags its data for language using standard n-gram language identification techniques. buckets, the result was 8,294 candidate pairs.</S>
    <S sid="297" ssid="10">This number is lower than what might be expected, given the huge number of buckets, because many buckets were monolingual; note that only pairs of one English and one Arabic document are deemed to be candidates.</S>
    <S sid="298" ssid="11">A random sample of two hundred candidate pairs was given to two human evaluators, bilingual in English and Arabic, who were asked (independently) to answer, for each pair, the question &#8220;Is this pair of pages intended to show the same material to two different users, one a reader of English and the other a reader of Arabic?&#8221; The judges&#8217; answers showed a Cohen&#8217;s &#954; agreement of 0.6955, which is generally considered fair to good reliability.</S>
    <S sid="299" ssid="12">(Qualitatively, one judge was rather more strict than the other; when the stricter judge identified a page pair as valid translations, the less strict judge virtually always agreed.)</S>
    <S sid="300" ssid="13">Taking the set of 149 labeled pairs on which the two judges agreed (134 were marked &#8220;good,&#8221; 15 &#8220;bad&#8221;), we carried out an evaluation of the full candidate set similar to the one for English-French discussed in Section 2.2.3.</S>
    <S sid="301" ssid="14">This was a threefold cross-validation experiment in which decision tree classifiers were tuned on the features extracted for each candidate pair by structure-based classification.23 In addition to the four structural scores, we included two language identification confidence scores (one for the English page, one for the Arabic page); these were available as part of the Internet Archive&#8217;s bookkeeping information for each URL and required no additional computation on our part.</S>
    <S sid="302" ssid="15">Table 5 shows precision and recall of each fold&#8217;s classifier applied to the corresponding test set of page pairs.</S>
    <S sid="303" ssid="16">The value of the parameter-tuning process is dramatically confirmed by comparing the learned parameters with STRAND&#8217;s default parameters (manually determined by Resnik [1999]).</S>
    <S sid="304" ssid="17">Note, however, that the candidate generation system is highly precise to begin with; only around 10% of the pairs in the random sample of candidates were considered &#8220;bad&#8221; by both judges.</S>
    <S sid="305" ssid="18">A baseline system in which no filtering is done at all achieves 89.93% precision on the full labeled set (with 100% recall).</S>
    <S sid="306" ssid="19">Depending on the relative importance of precision and recall, these structure-based classifiers might be considered worse than that baseline.</S>
    <S sid="307" ssid="20">Upon inspection, we discovered that nearly 5,000 of the pairs in our candidate set were from a single domain, (maktoob.com).</S>
    <S sid="308" ssid="21">This site supports an online marketplace, and many of the pages discovered by our search were dedicated to specific merchandise categories within that service; a large portion of these were simply &#8220;no items available&#8221; and one or two similar messages.</S>
    <S sid="309" ssid="22">We ignored this domain completely in order to be conservative about the yield of page pairs, though we note that many of the pages within it are legitimate parallel text that could be extracted if a good duplicates filter were applied.24 In order to construct a final classifier, we trained a decision tree on all 149 of the manually judged examples on which both judges agreed.</S>
    <S sid="310" ssid="23">This was then applied to the candidate pairs, producing a set of 1,741 HTML document pairs hypothesized to be valid translations of one another.</S>
    <S sid="311" ssid="24">By way of simple duplicate detection, if a pair of URLs appeared multiple times (under case-insensitive matching), it was counted only once.</S>
    <S sid="312" ssid="25">(Note that when this occurs, the duplicate pair will differ by at least one time stamp, and therefore a more sophisticated technique for eliminating duplication might extract more text.)</S>
    <S sid="313" ssid="26">The remaining set contained 1,399 pairs.25 Converting from HTML to plain text and tokenizing, the English documents in this corpus total approximately 673,108 tokens, with an average of 481 tokens per document; the Arabic side contains 845,891 tokens, averaging 605 tokens per document.26 We combined the structural and content-based approaches to detecting translations by adding the tsim score to the set of structural features associated with each candidate pair, and then training a new decision tree classifier.</S>
    <S sid="314" ssid="27">Because Arabic is a highly inflected language with many surface forms, we found it necessary to use morphological preprocessing in order to make effective use of a dictionary.</S>
    <S sid="315" ssid="28">For English, we tokenized the text and used the WordNet lemmatizer to strip suffixes.</S>
    <S sid="316" ssid="29">The Arabic texts were tokenized at punctuation, then romanized and converted to root forms using a morphological analysis tool (Darwish 2002).</S>
    <S sid="317" ssid="30">This approximately halved the vocabulary size for the Arabic texts (from 89,047 types to 48,212 types).</S>
    <S sid="318" ssid="31">The translation lexicon used to compute tsim contained 52,211 entries, each containing one English lemma and one Arabic root.27 Of these, 16,944 contained two items that were both present in the candidate set of 8,294 Web page pairs.</S>
    <S sid="319" ssid="32">The approximations discussed in Section 3.2.2 were employed: Competitive linking on the first 500 words in each document was used to compute the score.</S>
    <S sid="320" ssid="33">Carrying out the same cross-validation experiment (on the same random split of data), the combined structural and content-based classifier produced the results in Table 6.</S>
    <S sid="321" ssid="34">Also shown is the performance of the tsim-only classifier, assuming an optimal threshold is chosen.</S>
    <S sid="322" ssid="35">Averaged over three folds, the classifier achieved 95.06% precision and 98.48% recall (1.8% and 5.24% better than without tsim, respectively).</S>
    <S sid="323" ssid="36">After building a single classifier on all 149 test pairs (the set on which both human judges agreed), we reclassified the entire candidate set.</S>
    <S sid="324" ssid="37">Ignoring again pages from the (maktoob.com) domain, 2,206 pairs were marked as translations.</S>
    <S sid="325" ssid="38">The same crude duplicate filter was applied, cutting the set back to 1,821 pairs.28 Table 7 shows word counts for various tokenization schemes: the morphological analysis used for computing tsim, the Egypt tokenizer (which is aggressive), and counting only tokens with some alphabetic character from the Egypt tokenizer (a conservative approximation).</S>
    <S sid="326" ssid="39">The analogous results, using the classifier from Section 5.2, are shown for comparison.</S>
    <S sid="327" ssid="40">To summarize the results, using the content-based similarity score as a feature not only improved precision, it increased the size of the corpus (in words) by 51&#8211;63%, depending on the tokenization scheme.29</S>
  </SECTION>
  <SECTION title="6." number="7">
    <S sid="328" ssid="1">A number of the techniques we have used to mine parallel data from the Web can be improved, and we suggest here some directions.</S>
    <S sid="329" ssid="2">28 There were 1,796 unique English URLs and 1,779 unique Arabic URLs, giving document duplication rates of 1.4% and 2.4%, respectively.</S>
    <S sid="330" ssid="3">29 A list of Wayback Machine URLs is available at (http://umiacs.umd.edu/&#8764;resnik/strand/); a sample of the document pairs is included in Appendix A.</S>
    <S sid="331" ssid="4">With respect to classifying document pairs as translations, the reader will notice that our approach to content-based cross-lingual similarity essentially boils down to a greedy matching of some of the words in a document pair using a dictionary.</S>
    <S sid="332" ssid="5">It remains to be seen whether weights in the dictionary can be exploited (Smith [2001] suggests that empirically estimated joint translation probabilities for word pairs are not useful).</S>
    <S sid="333" ssid="6">We suggest that the incorporation of scores from information retrieval (e.g., inverse document frequency) might be useful in discerning which lexicon entries are the strongest cues of translational equivalence.</S>
    <S sid="334" ssid="7">We also have not explored any filtering on the noisy translation lexicon; doing so might improve the quality of the tsim score.</S>
    <S sid="335" ssid="8">The competitive linking approximation (which, without weights, is essentially random matching of word pairs) and the use of only the initial portion of each document provide significant computational savings.</S>
    <S sid="336" ssid="9">In our experience, neither of these has significantly hurt the performance of tsim-based classifiers (as compared to finding the maximum cardinality bipartite matching and/or using the full documents), and in some cases competitive linking seems to improve performance.</S>
    <S sid="337" ssid="10">It is possible that some sample selection of words from document candidates might be profitable (e.g., creating a sample of size proportional to the document length, as opposed to a fixed size, or sampling only from content words, or sampling only words present in the dictionary).</S>
    <S sid="338" ssid="11">Smith (2002) suggested a bootstrapping paradigm for the construction of parallel corpora.</S>
    <S sid="339" ssid="12">Beginning with a seed set of translation information (either parallel corpora or a bilingual dictionary), high-precision initial classifiers might be constructed using content and/or structural features (whichever are available).</S>
    <S sid="340" ssid="13">We might then iteratively select additional page pairs in which the current classifier has high confidence of translational equivalence, gradually increasing the pool of parallel data and at the same time expanding the bilingual lexicon.</S>
    <S sid="341" ssid="14">This approach to minimally supervised classifier construction has been widely studied (Yarowsky 1995), especially in cases in which the features of interest are orthogonal in some sense (e.g., Blum and Mitchell 1998; Abney 2002).</S>
    <S sid="342" ssid="15">With respect to the generation of candidate pairs, we have described a progression from index-based searches on AltaVista to exhaustive matching of URLs on the Internet Archive.</S>
    <S sid="343" ssid="16">The combination of these approaches may be profitable, particularly for languages that are represented only very sparsely on the Web.</S>
    <S sid="344" ssid="17">For such languages, index-based searches on words from a language of interest might be used to identify sites potentially containing parallel text.</S>
    <S sid="345" ssid="18">Within such sites, it would likely be profitable to look for parallel documents in the full cross product of documents in the two languages of interest, obtained both on the Internet Archive and via crawling all pages on relevant sites.</S>
    <S sid="346" ssid="19">Finally, we plan to utilize parallel texts mined from the Web in our work on machine translation and acquisition of bilingual lexicons, and in the creation of resources for new languages via projection of annotations from English.</S>
  </SECTION>
  <SECTION title="7." number="8">
    <S sid="347" ssid="1">Although efforts at discovering parallel text on the Web were first reported in 1998, Web-based parallel corpora appear to have had only a limited impact on the community.</S>
    <S sid="348" ssid="2">Three reasons for this suggest themselves.</S>
    <S sid="349" ssid="3">Too few languages.</S>
    <S sid="350" ssid="4">Parallel text from the Web has been made available to the community in only a few pairs of languages.</S>
    <S sid="351" ssid="5">As of this writing, the STRAND Web site (http://umiacs.umd.edu/&#8764;resnik/strand/), presenting URL pairs discovered via STRAND runs, contains collections only for English-French, English-Chinese, EnglishBasque, and now English-Arabic, and we are not aware of any other efforts to disseminate Web-based parallel data publicly.</S>
    <S sid="352" ssid="6">Up to this point, it simply has not been easy to search the Web for parallel text in new language pairs.</S>
    <S sid="353" ssid="7">The most difficult part is finding the candidates: A year or two ago, we attempted to apply the original Webbased STRAND to the problem of finding English-Arabic text, and we were unable to locate enough search engine hits or sites to yield useful results.</S>
    <S sid="354" ssid="8">Too little data.</S>
    <S sid="355" ssid="9">Very large Web-based parallel text collections are not available to the community.</S>
    <S sid="356" ssid="10">The largest appear to have been obtained by Chen and Nie (2000), who acquired collections on the order of 15,000 document pairs for English-French, English-German, English-Dutch, English-Italian, and English-Chinese using the PTMiner system.</S>
    <S sid="357" ssid="11">However, these collections have not been made available to the general community.30 In contrast, the STRAND collections, which are available to the community in the form of URL pairs, are modest in size: The English-Chinese collection contains fewer than 3,500 document pairs, and the English-French fewer than 2,500.</S>
    <S sid="358" ssid="12">Difficulty with dissemination.</S>
    <S sid="359" ssid="13">Web-based collections are difficult to distribute.</S>
    <S sid="360" ssid="14">Standard mechanisms of the sort used by the LDC (a CD or downloadable file) are fraught with difficult legal issues, since, technically speaking, redistributing the actual content of Web pages could require permission from the author of every page.</S>
    <S sid="361" ssid="15">For example, presumably as a risk reduction strategy, the Web track for TREC-2002 (Text Retrieval Conference) limited its attention to the .gov domain and required the recipient of the data to sign a form that reduced the distributor&#8217;s liability.31 Similarly, the Google Programming Contest data set arrived with a limited-use license, indemnification from third-party claims, and a collection limited to the .edu domain, from which, presumably, authors are less likely to bring expensive lawsuits (http://www.google.com/programming-contest/).</S>
    <S sid="362" ssid="16">A possible fourth reason may have to do with questions about the utility of the data.</S>
    <S sid="363" ssid="17">For example, a Web-based parallel collection may be unpredictable in terms of its coverage, and the community is well aware of the dangers of using training data that are not representative of the test domain.</S>
    <S sid="364" ssid="18">A solution to this problem might be to extract topically relevant subsets of the collection for particular domains or applications, but of course this requires a &#8220;more is better&#8221; approach in order to obtain subsets that are large enough to be useful.</S>
    <S sid="365" ssid="19">The work reported in this article addresses each of these major problems.</S>
    <S sid="366" ssid="20">With respect to the number of language pairs, the Internet Archive offers us a huge sample of pages on the Web, and our techniques make it easy to explore that collection in an efficient way.</S>
    <S sid="367" ssid="21">Although it is probably impossible to crawl more than a small fraction of the Web, the Internet Archive is storing the results of commercial-scale Web crawling and has as its explicit mission the permanent storage of everything that can be found.</S>
    <S sid="368" ssid="22">The fact that we were able to find a substantial quantity of English-Arabic text (on the order of a million words per side, looking at less than a sixth of the Archive in 2002) offers the hope that it will be possible to find data for the less well-represented language pairs, if and when those data actually exist.</S>
    <S sid="369" ssid="23">Moreover, the final implementation we described here retains the almost entirely language-independent character of the original STRAND system, adding only the requirement of a reasonable translation lexicon.</S>
    <S sid="370" ssid="24">Therefore success in mining for parallel text in other languages depends primarily on whether the data exist in the Archive.</S>
    <S sid="371" ssid="25">With regard to corpus size, we demonstrated that the recall of structural matching, and hence its yield, can be significantly improved by simple and automatic classifier construction, requiring only a few hours&#8217; work from a bilingual annotator to create the training material.</S>
    <S sid="372" ssid="26">These results are further improved by adding content-based similarity as a feature.</S>
    <S sid="373" ssid="27">Our success with English-Arabic, a language pair that is not one of those usually considered well represented on the Web, encourages us to believe that for other languages of interest, we will be similarly successful.</S>
    <S sid="374" ssid="28">We have also done a bit of exploration to gauge the potential of the Archive for better-represented language pairs, using English-Chinese as an example.</S>
    <S sid="375" ssid="29">By way of context, Chen and Nie (2000) reported that PTMiner found around 15,000 English-Chinese document pairs by crawling 185 sites in the .hk (Hong Kong) domain, with the run taking about a week.</S>
    <S sid="376" ssid="30">We did a STRAND search of the two Internet Archive crawls used in the English-Arabic study, seeking English-Chinese parallel text in multiple domains where Chinese is a dominant language (e.g., .hk, .tw, .cn).</S>
    <S sid="377" ssid="31">Our initial candidate pair set was generated in approximately 30 hours, and contains over 70,000 candidate page pairs.</S>
    <S sid="378" ssid="32">We are optimistic that this can be improved still further by expanding the search to include all sites that contain at least one Chinese document, regardless of the domain.</S>
    <S sid="379" ssid="33">In terms of dissemination, the STRAND distribution mechanism models itself after Web search engines, distributing the URLs rather than the pages themselves.</S>
    <S sid="380" ssid="34">This places the legal burden on individual users, who are presumably safe under fair use provisions if they download pages for their individual use.</S>
    <S sid="381" ssid="35">Until recently the difficulty with this solution has been that the collection of URLs deteriorates over time as sites disappear, pages are reorganized, and underlying content changes: For example, in April 2002, we attempted to download the documents in the STRAND EnglishFrench, English-Chinese, and English-Basque collections, and we were able to access successfully only around 67%, 43%, and 40% of the URL pairs, respectively.</S>
    <S sid="382" ssid="36">However, the Internet Archive&#8217;s Wayback Machine provides a way to distribute persistent URLs.</S>
    <S sid="383" ssid="37">With regard to the quality of the data, in Section 2.2.2 we discussed two studies that demonstrate the utility of parallel Web data in acquiring translation lexicons for cross-language information retrieval.</S>
    <S sid="384" ssid="38">We also reported on the results of a human ratings study, which provided evidence that English-Chinese data mined from the Web contain reasonably fluent, reasonably translated sentence pairs.</S>
    <S sid="385" ssid="39">It is worth pointing out that, because STRAND expects pages to be very similar in structural terms, the resulting document collections are particularly amenable to sentence- or segment-level alignment.</S>
    <S sid="386" ssid="40">Indeed, just using dynamic programming to align the markup, ignoring the text, produces reasonable first-pass alignments of the intervening text as a side effect.</S>
    <S sid="387" ssid="41">We are currently adapting statistical text-based sentence alignment techniques to take advantage of the markup available in Web-based document pairs.</S>
    <S sid="388" ssid="42">Ultimately, the utility of parallel data from the Web is a question that will need to be addressed in practice.</S>
    <S sid="389" ssid="43">The potential, of course, is as rich and diverse as the Web itself, and what we as researchers can do with it is an exciting question that remains to be answered.</S>
  </SECTION>
  <SECTION title="Appendix A: Examples" number="9">
    <S sid="390" ssid="1">The following page pairs (Figures 7&#8211;8) are representative of English-Arabic parallel corpus extracted from the Internet Archive.</S>
    <S sid="391" ssid="2">The text from the pages is shown in full.</S>
    <S sid="392" ssid="3">Note that the full corpus is available as a list of Wayback Machine URLs at (http://umiacs.umd.edu/&#8764;resnik/strand).</S>
    <S sid="393" ssid="4">These pages show the generally high quality of the corpus and also illustrate some of the potential difficulties with parallel Web data.</S>
    <S sid="394" ssid="5">For example, the Arabic page in the first pair includes an additional caption not present in the English side.</S>
    <S sid="395" ssid="6">These kinds of problems are expected to be overcome during sentence alignment processing.</S>
  </SECTION>
  <SECTION title="Appendix B: Translation Ratings Criteria" number="10">
    <S sid="396" ssid="1">For each item, participants were instructed to provide three ratings.</S>
    <S sid="397" ssid="2">Quality of the English:</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="11">
    <S sid="398" ssid="1">Keezer for permitting and facilitating our use of the Internet Archive.</S>
    <S sid="399" ssid="2">Finally, we are indebted to several Computational Linguistics reviewers, whose comments helped us to greatly improve this article.</S>
  </SECTION>
</PAPER>
