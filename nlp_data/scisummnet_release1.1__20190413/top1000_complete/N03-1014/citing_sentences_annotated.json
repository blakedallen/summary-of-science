[
  {
    "citance_No": 1, 
    "citing_paper_id": "W04-0305", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "James B., Henderson", 
    "raw_text": "As has been previously proposed by Brants and Crocker (2000), we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003)", 
    "clean_text": "we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W04-0305", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "James B., Henderson", 
    "raw_text": "P (di|d1, ... ,di? 1)? P (di|h (d1, ... ,di? 1)) Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003)", 
    "clean_text": "Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W04-0305", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "James B., Henderson", 
    "raw_text": "In this way, the number of representations which information needs to pass through in order to flow from history representation i to history representation j is determined by the structural distance between i? s node and j? s node, and not just the distance between i and j in the parse sequence. This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003)", 
    "clean_text": "This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W04-0305", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "James B., Henderson", 
    "raw_text": "The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di? 1), as explained in more detail in (Henderson, 2003) .As with many other machine learning methods, training a Simple Synchrony Network involves first defining an appropriate learning criteria and then performing some form of gradient descent learning to search for the optimum values of the network? s parameters according to this criteria", 
    "clean_text": "The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di), as explained in more detail in (Henderson, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W07-2218", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Ivan, Titov | James B., Henderson", 
    "raw_text": "In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model", 
    "clean_text": "In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W07-2218", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Ivan, Titov | James B., Henderson", 
    "raw_text": "Unlike (Nivre et al, 2006), we can not use a lookahead in our generative model, as was discussed in section 3, so a greedy method is unlikely to lead to a good approximation. Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. We apply fixed beam pruning after each decision Shiftwj, because knowledge of the next word in the queue I helps distinguish unlikely decision sequences", 
    "clean_text": "Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "N06-2026", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Gabriele, Musillo | Paola, Merlo", 
    "raw_text": "If the internal semantics of a predicate determines the syntactic expressions of constituents bearing a semantic role, it is then reasonable to expect that knowledge about semantic roles in a sentence will be informative of its syntactic structure, and that learning semantic role labels at the same time as parsing will be beneficial to parsing accuracy. We present work to test the hypothesis that a cur rent statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser? s accuracy on the original parsing task", 
    "clean_text": "We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N09-2032", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Lonneke, van der Plas | James B., Henderson | Paola, Merlo", 
    "raw_text": "The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Net work called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007)", 
    "clean_text": "The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P05-1010", 
    "citing_paper_authority": 78, 
    "citing_paper_authors": "Takuya, Matsuzaki | Yusuke, Miyao | Jun'ichi, Tsujii", 
    "raw_text": "Henderson? s parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours", 
    "clean_text": "Henderson's parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "H05-1078", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Paola, Merlo | Gabriele, Musillo", 
    "raw_text": "Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks", 
    "clean_text": "Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "H05-1078", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Paola, Merlo | Gabriele, Musillo", 
    "raw_text": "Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system", 
    "clean_text": "Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "H05-1078", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Paola, Merlo | Gabriele, Musillo", 
    "raw_text": "We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations", 
    "clean_text": "We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "H05-1078", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Paola, Merlo | Gabriele, Musillo", 
    "raw_text": "1 is included in the inputs to the represen tion of the next move i, as explained in more detail in (Henderson, 2003)", 
    "clean_text": "SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i \u2212 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "H05-1078", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Paola, Merlo | Gabriele, Musillo", 
    "raw_text": "H03 indicates the model illustrated in (Henderson, 2003)", 
    "clean_text": "H03 indicates the model illustrated in (Henderson, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "H05-1078", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Paola, Merlo | Gabriele, Musillo", 
    "raw_text": "(Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs", 
    "clean_text": "(Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D07-1099", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Ivan, Titov | James B., Henderson", 
    "raw_text": "In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing", 
    "clean_text": "In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D07-1099", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Ivan, Titov | James B., Henderson", 
    "raw_text": "Unlike (Titov and Hender son, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003)", 
    "clean_text": "Unlike (Titov and Henderson, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W06-2303", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Gabriele, Musillo | Paola, Merlo", 
    "raw_text": "We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser? s ac curacy on the original parsing task, by explicitlymodelling semantic role labels as the interface be tween syntax and semantics", 
    "clean_text": "We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W06-2303", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Gabriele, Musillo | Paola, Merlo", 
    "raw_text": "To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem", 
    "clean_text": "To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W06-2303", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Gabriele, Musillo | Paola, Merlo", 
    "raw_text": "(Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step", 
    "clean_text": "(Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.", 
    "keep_for_gold": 0
  }
]
