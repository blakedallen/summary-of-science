[
  {
    "citance_No": 1, 
    "citing_paper_id": "H89-2012", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Kenneth Ward, Church | William A., Gale | Patrick, Hanks | Donald, Hindle", 
    "raw_text": "Church and Hanks (1989) discussed the use of the mutual information statistic in order to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntacfic co-occurrence onstraints between verbs and prepositions (content word/function word)", 
    "clean_text": "Church and Hanks (1989) discussed the use of the mutual information statistic in order to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntacfic co-occurrence onstraints between verbs and prepositions (content word/function word).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "N12-1094", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Xufeng, Han | Kota, Yamaguchi | Karl, Stratos | Alyssa, Mensch | Jesse, Dodge | Tamara L., Berg | Alexander C., Berg | Amit, Goyal | Margaret, Mitchell | Yejin, Choi | Hal, Daum&eacute; III", 
    "raw_text": "We use Pointwise Mutual Information (PMI) (Church and Hanks, 1989) to weight the contexts, and select the top 1000 PMI contexts for each adjective.3 Next, we apply cosine similarity to find the top 10 distribution ally similar adjectives with respect toeach target adjective based on our large generic corpus (Large-Data from Section 2.1)", 
    "clean_text": "We use Pointwise Mutual Information (PMI) (Church and Hanks, 1989) to weight the contexts, and select the top 1000 PMI contexts for each adjective.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "H05-1113", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Sriram, Venkatapathy | Aravind K., Joshi", 
    "raw_text": "Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998)", 
    "clean_text": "Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "H05-1113", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Sriram, Venkatapathy | Aravind K., Joshi", 
    "raw_text": "(Breidt, 1995) has evaluated the usefulness of the Point-wise Mutual Information measure (as suggested by (Church and Hanks, 1989)) for the extraction of V-N collocations from German text corpora", 
    "clean_text": "(Breidt, 1995) has evaluated the usefulness of the Point-wise Mutual Information measure (as suggested by (Church and Hanks, 1989)) for the extraction of V-N collocations from German text corpora.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P06-1005", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Shane, Bergsma | Dekang, Lin", 
    "raw_text": "We encode the semantic compatibility between a noun and its parse tree parent (and grammatical relationship with the parent) using mutual information (MI) (Church and Hanks, 1989)", 
    "clean_text": "We encode the semantic compatibility between a noun and its parse tree parent (and grammatical relationship with the parent) using mutual information (MI) (Church and Hanks, 1989).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W06-1665", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jing, Bai | Jian-Yun, Nie | Guihong, Cao", 
    "raw_text": "We use the following point wise MI (Church and Hanks 1989):) () (), (log), (kjkjkjwPwPwwPwwMI= We only keep meaningful combinations such that 0), (& gt ;kjwwMI. By these filtering criteria, we are able to reduce considerably the number of biterms and triter ms", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D09-1098", 
    "citing_paper_authority": 42, 
    "citing_paper_authors": "Patrick, Pantel | Eric, Crestan | Arkady, Borkovsky | Ana-Maria, Popescu | Vishnu, Vyas", 
    "raw_text": "We weigh each context f using point wise mutual information (Church and Hanks 1989)", 
    "clean_text": "We weigh each context f using point wise mutual information (Church and Hanks 1989).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "C96-2099", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Shiho, Nobesawa | Junya, Tsutsumi | Sun Da, Jiang | Tomohisa, Sano | Kengo, Sato | Masakazu, Nakanishi", 
    "raw_text": "Z (2) d:: l j= i (d -1 )dmax: max distance used wl: the i-th letter in the sentence w g (d): a certain weight for iV// concerning distance between letters The information between two remote words has less nmaning in a sentence when it comes to the semantic analysis (Church and Hanks, 1989)", 
    "clean_text": "Z (2) d:: l j= i (d -1 )dmax: max distance used wl: the i-th letter in the sentence w g (d): a certain weight for iV// concerning distance between letters The information between two remote words has less nmaning in a sentence when it comes to the semantic analysis (Church and Hanks, 1989).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W02-0606", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Marco, Baroni | Johannes, Matiasek | Harald, Trost", 
    "raw_text": "pairs Measuring the semantic similarity of words on the basis of raw corpus data is obviously a much harder task than measuring the orthographic similarity of words. Mutual information (first introduced to computational linguistics by Church and Hanks (1989)) is one of many measures that seems to be roughly correlated to the degree of semantic relatedness be tween words", 
    "clean_text": "Mutual information (first introduced to computational linguistics by Church and Hanks (1989)) is one of many measures that seems to be roughly correlated to the degree of semantic relatedness be tween words.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W10-0307", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Burr, Settles", 
    "raw_text": "by concatenating all the lines of text in which s appears. I also experimented with the co-occurence frequency c (s, w) and point-wise mutual information (Church and Hanks, 1989) as similarity functions", 
    "clean_text": "I also experimented with the co-occurence frequency c (s, w) and point-wise mutual information (Church and Hanks, 1989) as similarity functions.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "H05-1058", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Sheila M., Reynolds | Jeff A., Bilmes", 
    "raw_text": "to p? [0, 1) .Beginning with (Church and Hanks, 1989), numerous authors have used the point wise mutual in formation between pairs of words to analyze word co-locations and associations", 
    "clean_text": "Beginning with (Church and Hanks, 1989), numerous authors have used the point wise mutual in formation between pairs of words to analyze word co-locations and associations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "S10-1031", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Kathrin, Eichler | G&uuml;nter, Neumann", 
    "raw_text": "Point-wise mutual information (PMI, Church and Hanks (1989)) is used to capture the semantic relatedness of the candidate to the topic of the document", 
    "clean_text": "Point-wise mutual information (PMI, Church and Hanks (1989)) is used to capture the semantic relatedness of the candidate to the topic of the document.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "C10-2144", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Yulia, Tsvetkov | Shuly, Wintner", 
    "raw_text": "Early approaches to identifying MWEsconcentrated on their col locational behavior (Church and Hanks, 1989)", 
    "clean_text": "Early approaches to identifying MWEs concentrated on their collocational behavior (Church and Hanks, 1989).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P06-1107", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Fabio Massimo, Zanzotto | Marco, Pennacchiotti | Maria Teresa, Pazienza", 
    "raw_text": "Specifically, the measure Snom (vt ,vh) is derived from point-wise mutual information (Church and Hanks, 1989): Snom (vt ,vh)= log p (vt ,vh|nom) p (vt) p (vh|pers) (3) where nom is the event of having a nominalized textual entailment pattern and pers is the event of having an agentive nominalization of verbs", 
    "clean_text": "Specifically, the measure Snom (vt ,vh) is derived from point-wise mutual information (Church and Hanks, 1989): Snom (vt ,vh)= log p (vt, vh|nom) p (vt) p (vh|pers) (3) where nom is the event of having a nominalized textual entailment pattern and pers is the event of having an agentive nominalization of verbs.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W06-1204", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Sriram, Venkatapathy | Aravind K., Joshi", 
    "raw_text": "Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al, 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al, 2003)", 
    "clean_text": "Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al, 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W06-1204", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Sriram, Venkatapathy | Aravind K., Joshi", 
    "raw_text": "Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006)", 
    "clean_text": "Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "N04-1041", 
    "citing_paper_authority": 43, 
    "citing_paper_authors": "Patrick, Pantel | Deepak, Ravichandran", 
    "raw_text": "Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989)", 
    "clean_text": "Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "I05-1049", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Sriram, Venkatapathy | Aravind K., Joshi", 
    "raw_text": "Church and Hanks (1989) proposed a measure of association called Mutual In formation [9]", 
    "clean_text": "Church and Hanks (1989) proposed a measure of association called Mutual Information.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W09-1706", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Lonneke, van der Plas", 
    "raw_text": "We have used POINTWISE MUTUAL INFORMATION (PMI, Church and Hanks (1989)) to account for the differences in information value be tween the several headwords and attributes", 
    "clean_text": "We have used POINTWISE MUTUAL INFORMATION (PMI, Church and Hanks (1989)) to account for the differences in information value between the several headwords and attributes.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D11-1023", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Amit, Goyal", 
    "raw_text": "# Cs 20M 50M AS PMI LLR PMI LLR TopK R? R? R? R? 50 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 100 .98 .94 1.0 1.0 1.0 1.0 1.0 1.0 500 .80 .98 1.0 1.0 .98 1.0 1.0 1.0 1000 .56 .99 1.0 1.0 .96 .99 1.0 1.0 5000 .35 .90 1.0 1.0 .85 .99 1.0 1.0 10000 .38 .55 1.0 1.0 .81 .95 1.0 1.0 Table 1: Evaluating the PMI and LLR rankings obtained using CM sketch with conservative update (CU) and Exact counts not throwing away any infrequent word pairs, PMI will rank pairs with low frequency counts higher (Church and Hanks, 1989)", 
    "clean_text": "The explanation for such a behavior is: since we are not throwing away any infrequent word pairs, PMI will rank pairs with low frequency counts higher (Church and Hanks, 1989).", 
    "keep_for_gold": 0
  }
]
