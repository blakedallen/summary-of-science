<PAPER>
	<S sid="0">A Discriminative Matching Approach To Word Alignment</S><ABSTRACT>
		<S sid="1" ssid="1">We present a discriminative, large margin approach to feature-based matching for word alignment.</S>
		<S sid="2" ssid="2">In thisframework, pairs of word tokens re ceive a matching score, which is basedon features of that pair, including mea sures of association between the words,distortion between their positions, sim ilarity of the orthographic form, and soon.</S>
		<S sid="3" ssid="3">Even with only 100 labeled train ing examples and simple features whichincorporate counts from a large unlabeled corpus, we achieve AER perfor mance close to IBM Model 4, in muchless time.</S>
		<S sid="4" ssid="4">Including Model 4 predic tions as features, we achieve a relativeAER reduction of 22% in over inter sected Model 4 alignments.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">The standard approach to word alignment from sentence-aligned bitexts has been to constructmodels which generate sentences of one language from the other, then fitting those genera tive models with EM (Brown et al, 1990; Och and Ney, 2003).</S>
			<S sid="6" ssid="6">This approach has two primary advantages and two primary drawbacks.</S>
			<S sid="7" ssid="7">In itsfavor, generative models of alignment are wellsuited for use in a noisy-channel translation system.</S>
			<S sid="8" ssid="8">In addition, they can be trained in an un supervised fashion, though in practice they do require labeled validation alignments for tuning model hyper-parameters, such as null counts orsmoothing amounts, which are crucial to pro ducing alignments of good quality.</S>
			<S sid="9" ssid="9">A primarydrawback of the generative approach to alignment is that, as in all generative models, explicitly incorporating arbitrary features of the in put is difficult.</S>
			<S sid="10" ssid="10">For example, when considering whether to align two words in the IBM models (Brown et al, 1990), one cannot easily include information about such features as orthographic similarity (for detecting cognates), presence of the pair in various dictionaries, similarity of the frequency of the two words, choices made by other alignment systems on this sentence pair, and so on.</S>
			<S sid="11" ssid="11">While clever models can implicitly capture some of these information sources, ittakes considerable work, and can make the resulting models quite complex.</S>
			<S sid="12" ssid="12">A second draw back of generative translation models is that, since they are learned with EM, they require extensive processing of large amounts of data to achieve good performance.</S>
			<S sid="13" ssid="13">While tools likeGIZA++ (Och and Ney, 2003) do make it eas ier to build on the long history of the generativeIBM approach, they also underscore how com plex high-performance generative models can, and have, become.In this paper, we present a discriminative ap proach to word alignment.</S>
			<S sid="14" ssid="14">Word alignment is cast as a maximum weighted matching problem (Cormen et al, 1990) in which each pair of words (e j , f k ) in a sentence pair (e, f) is associated with a score s jk (e, f) reflecting the desirability of the alignment of that pair.</S>
			<S sid="15" ssid="15">The alignment 73 for the sentence pair is then the highest scoring matching under some constraints, for example the requirement that matchings be one-to-one.</S>
			<S sid="16" ssid="16">This view of alignment as graph matching isnot, in itself, new: Melamed (2000) uses com petitive linking to greedily construct matchingswhere the pair score is a measure of word to-word association, and Matusov et al (2004) find exact maximum matchings where the pair scores come from the alignment posteriors of generative models.</S>
			<S sid="17" ssid="17">Tiedemann (2003) proposes incorporating a variety of word association ?clues?</S>
			<S sid="18" ssid="18">into a greedy linking algorithm.What we contribute here is a principled ap proach for tractable and efficient learning of the alignment score s jk(e, f) as a function of arbitrary features of that token pair.</S>
			<S sid="19" ssid="19">This con tribution opens up the possibility of doing the kind of feature engineering for alignment that has been so successful for other NLP tasks.</S>
			<S sid="20" ssid="20">Wefirst present the algorithm for large margin es timation of the scoring function.</S>
			<S sid="21" ssid="21">We then showthat our method can achieve AER rates com parable to unsymmetrized IBM Model 4, usingextremely little labeled data (as few as 100 sen tences) and a simple feature set.</S>
			<S sid="22" ssid="22">Remarkably,by including bi-directional IBM Model 4 predic tions as features, we achieve an absolute AER of 5.4 on the English-French Hansards alignmenttask, a relative reduction of 22% in AER over intersected Model 4 alignments and, to our knowl edge, the best AER result published on this task.</S>
	</SECTION>
	<SECTION title="Algorithm. " number="2">
			<S sid="23" ssid="1">We model the alignment prediction task as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.</S>
			<S sid="24" ssid="2">For simplicity, we assume here that each word aligns to one or zero words in the other sentence.</S>
			<S sid="25" ssid="3">The edge weight s jkrepre sents the degree to which word j in one sentencecan translate into the word k in the other sen tence.</S>
			<S sid="26" ssid="4">Our goal is to find an alignment that maximizes the sum of edge scores.</S>
			<S sid="27" ssid="5">We represent a matching using a set of binary variables y jk that are set to 1 if word j is assigned to word k in the other sentence, and 0 otherwise.</S>
			<S sid="28" ssid="6">The score of an assignment is the sum of edge scores: s(y) = ? jk s jk y jk . The maximum weight bi-.</S>
			<S sid="29" ssid="7">partite matching problem, arg maxy?Y s(y), canbe solved using well known combinatorial algo rithms or the following linear program: max z ? jk s jk z jk (1) s.t. ? j z jk ? 1, ? k z jk ? 1, 0 ? z jk ? 1, where the continuous variables z jk correspond to the binary variables y jk . This LP is guaranteed.</S>
			<S sid="30" ssid="8">to have integral (and hence optimal) solutions for any scoring function s(y) (Schrijver, 2003).</S>
			<S sid="31" ssid="9">Note that although the above LP can be used to compute alignments, combinatorial algorithms are generally more efficient.</S>
			<S sid="32" ssid="10">However, we use the LP to develop the learning algorithm below.</S>
			<S sid="33" ssid="11">For a sentence pair x, we denote position pairs by x jk and their scores as s jk . We let.</S>
			<S sid="34" ssid="12">s jk = wf(x jk) for some user provided fea ture mapping f and abbreviate wf(x,y) = ? jk y jk wf(x jk).</S>
			<S sid="35" ssid="13">We can include in the fea ture vector the identity of the two words, their relative positions in their respective sentences, their part-of-speech tags, their string similarity (for detecting cognates), and so on.</S>
			<S sid="36" ssid="14">At this point, one can imagine estimating alinear matching model in multiple ways, includ ing using conditional likelihood estimation, anaveraged perceptron update (see which matchings are proposed and adjust the weights ac cording to the difference between the guessed and target structures (Collins, 2002)), or inlarge-margin fashion.</S>
			<S sid="37" ssid="15">Conditional likelihood es timation using a log-linear model P (y | x) = 1 Z w (x) exp{wf(x,y)} requires summing over all matchings to compute the normalization Zw(x), which is #P-complete (Valiant, 1979).</S>
			<S sid="38" ssid="16">In ourexperiments, we therefore investigated the aver aged perceptron in addition to the large-margin method outlined below.</S>
			<S sid="39" ssid="17">2.1 Large-margin estimation.</S>
			<S sid="40" ssid="18">We follow the large-margin formulation of Taskar et al (2005a).</S>
			<S sid="41" ssid="19">Our input is a set of training instances {(x i ,y i )}m i=1, where each in stance consists of a sentence pair x i and a target 74 alignment y i . We would like to find parameters.</S>
			<S sid="42" ssid="20">w that predict correct alignments on the train ing data: y i = arg max ?y i ?Y i wf(x i , y?</S>
			<S sid="43" ssid="21">i ), ?i, where Y i is the space of matchings appropriate for the sentence pair i.In standard classification problems, we typi cally measure the error of prediction,</S>
			<S sid="44" ssid="22">(y i , y?</S>
			<S sid="45" ssid="23">i ),using the simple 0-1 loss.</S>
			<S sid="46" ssid="24">In structured prob lems, where we are jointly predicting multiple variables, the loss is often more complex.</S>
			<S sid="47" ssid="25">While the F-measure is a natural loss function for this task, we instead chose a sensible surrogate that fits better in our framework: Hamming distance between y i and y?</S>
			<S sid="48" ssid="26">i , which simply counts the number of edges predicted incorrectly.</S>
			<S sid="49" ssid="27">We use an SVM-like hinge upper bound on the loss</S>
			<S sid="50" ssid="28">(y i , y?</S>
			<S sid="51" ssid="29">i ), given by max ?y i ?Y i [wf i (y? i ) +</S>
			<S sid="52" ssid="30">i (y? i ) ? wf i (y i )], where</S>
			<S sid="53" ssid="31">i (y? i ) =</S>
			<S sid="54" ssid="32">(y i , y?</S>
			<S sid="55" ssid="33">i ), and f i (y? i ) = f(x i , y?</S>
			<S sid="56" ssid="34">i ).</S>
			<S sid="57" ssid="35">Minimizing this upper bound encourages the true alignment y i to be optimal with respect to w for each instance i: min ||w||??</S>
			<S sid="58" ssid="36">i max ?y i ?Y i [wf i (y? i ) +</S>
			<S sid="59" ssid="37">i (y? i )] ? wf i (y i ), where ? is a regularization parameter.In this form, the estimation problem is a mixture of continuous optimization over w and com binatorial optimization over y i . In order to.</S>
			<S sid="60" ssid="38">transform it into a more standard optimization problem, we need a way to efficiently handle the loss-augmented inference, max ?y i ?Y i [wf i (y? i ) +</S>
			<S sid="61" ssid="39">i (y? i)].</S>
			<S sid="62" ssid="40">This optimization problem has precisely the same form as the prediction prob lem whose parameters we are trying to learn ? max ?y i ?Y i wf i (y? i ) ? but with an additionalterm corresponding to the loss function.</S>
			<S sid="63" ssid="41">Our as sumption that the loss function decomposes over the edges is crucial to solving this problem.</S>
			<S sid="64" ssid="42">In particular, we use weighted Hamming distance, which counts the number of variables in which a candidate solution y?</S>
			<S sid="65" ssid="43">i differs from the target output y i , with different cost for false positives (c+) and false negatives (c-):</S>
			<S sid="66" ssid="44">i (y? i ) = ? jk [ c-y i,jk (1 ? y? i,jk ) + c+y? i,jk (1 ? y i,jk ) ] = ? jk c-y i,jk + ? jk [c+ ?</S>
			<S sid="67" ssid="45">(c- + c+)y i,jk ]y? i,jk . The loss-augmented matching problem can thenbe written as an LP similar to Equation 1 (with out the constant term ? jk c-y i,jk ): max z ? jk z i,jk [wf(x i,jk ) + c+ ?</S>
			<S sid="68" ssid="46">(c- + c+)y i,jk ] s.t. ? j z i,jk ? 1, ? k z i,jk ? 1, 0 ? z i,jk ? 1.</S>
			<S sid="69" ssid="47">Hence, without any approximations, we have a continuous optimization problem instead of a combinatorial one: max ?y i ?Y i wf i (y? i )+</S>
			<S sid="70" ssid="48">i (y? i ) = d i +max z i ?Z i (wF i +c i )z i , where d i = ? jk c-y i,jk is the constant term, F i is the appropriate matrix that has a column of features f(x i,jk ) for each edge jk, c i is the vector of the loss terms c+ ?</S>
			<S sid="71" ssid="49">(c- + c+)y i,jk and finally Z i = {z i : ? j z i,jk ? 1, ? k z i,jk ? 1, 0 ? z i,jk ? 1}.</S>
			<S sid="72" ssid="50">Plugging this LP back into our estimation problem, we have min ||w||??</S>
			<S sid="73" ssid="51">max z?Z ? i wF i z i + c i z i ? wF i y i , (2) where z = {z 1 , . . .</S>
			<S sid="74" ssid="52">, z m }, Z = Z 1 ? .</S>
			<S sid="75" ssid="53">.?Z m . In-.</S>
			<S sid="76" ssid="54">stead of the derivation in Taskar et al (2005a), which produces a joint convex optimization problem using Lagrangian duality, here we tackle the problem in its natural saddle-point form.</S>
			<S sid="77" ssid="55">2.2 The extragradient method.</S>
			<S sid="78" ssid="56">For saddle-point problems, a well-known solution strategy is the extragradient method (Ko rpelevich, 1976), which is closely related to projected-gradient methods.</S>
			<S sid="79" ssid="57">The gradient of the objective in Equation 2 is given by: ? i F i (z i ? y i ) (with respect to w) and F i w + c i (with respect to each z i).</S>
			<S sid="80" ssid="58">We de note the Euclidean projection of a vector onto Z i as P Z i (v) = arg minu?Z i||v ? u|| and pro jection onto the ball ||w|| ? ?</S>
			<S sid="81" ssid="59">as P ?</S>
			<S sid="82" ssid="60">(w) = ?w/max(?, ||w||).</S>
			<S sid="83" ssid="61">75An iteration of the extragradient method con sists of two very simple steps, prediction: w?t+1 = P ?</S>
			<S sid="84" ssid="62">(wt + ? k ? i F i (y i ? zt i )); z?t+1 i = P Z i (zt i + ? k (F i wt + c i )); and correction: wt+1 = P ?</S>
			<S sid="85" ssid="63">(wt + ? k ? i F i (y i ? z?t+1 i )); zt+1 i = P Z i (zt i + ? k (F i w?t+1 + c i )), where ? k are appropriately chosen step sizes.</S>
			<S sid="86" ssid="64">The method is guaranteed to converge linearly to a solution w?, z?</S>
			<S sid="87" ssid="65">(Korpelevich, 1976; He and Liao, 2002; Taskar et al, 2005b).</S>
			<S sid="88" ssid="66">Please see www.cs.berkeley.edu/~taskar/extragradient.pdf for more details.The key subroutine of the algorithm is Eu clidean projection onto the feasible sets Z i . In.</S>
			<S sid="89" ssid="67">case of word alignment, Z i is the convex hull of bipartite matchings and the problem reduces to the much-studied minimum cost quadratic flow problem (Bertsekas et al, 1997).</S>
			<S sid="90" ssid="68">The projection problem P Z i (z? i ) is given by min z ? jk 1 2 (z? i,jk ? z i,jk )2 s.t. ? j z i,jk ? 1, ? k z i,jk ? 1, 0 ? z i,jk ? 1.We can now use a standard reduction of bipar tite matching to min cost flow by introducing a source node connected to all the words in one sentence and a sink node connected to all thewords in the other sentence, using edges of ca pacity 1 and cost 0.</S>
			<S sid="91" ssid="69">The original edges jk have a quadratic cost 1 2 (z? i,jk ? z i,jk )2 and capacity 1.</S>
			<S sid="92" ssid="70">Now the minimum cost flow from the source to the sink computes projection of z?</S>
			<S sid="93" ssid="71">i onto Z i We use standard, publicly-available code for solving this problem (Guerriero and Tseng, 2002).</S>
	</SECTION>
	<SECTION title="Experiments. " number="3">
			<S sid="94" ssid="1">We applied this matching algorithm to word level alignment using the English-French Hansards data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003).</S>
			<S sid="95" ssid="2">This corpus consists of 1.1M automatically aligned sentences, and comes with a validation set of 39 sentence pairs and a test set of 447 sentences.</S>
			<S sid="96" ssid="3">The validation and test sentences have been hand-aligned (see Och and Ney (2003)) and are marked with both sure and possible alignments.</S>
			<S sid="97" ssid="4">Using these alignments, alignment error rate (AER) is calculated as: AER(A,S, P ) = 1 ? |A ? S| + |A ? P | |A| + |S| Here, A is a set of proposed index pairs, S is the sure gold pairs, and P is the possible goldpairs.</S>
			<S sid="98" ssid="5">For example, in Figure 1, proposed align ments are shown against gold alignments, with open squares for sure alignments, rounded open squares for possible alignments, and filled black squares for proposed alignments.</S>
			<S sid="99" ssid="6">Since our method is a supervised algorithm, we need labeled examples.</S>
			<S sid="100" ssid="7">For the training data, we split the original test set into 100 trainingexamples and 347 test examples.</S>
			<S sid="101" ssid="8">In all our ex periments, we used a structured loss function</S>
			<S sid="102" ssid="9">(y i , y?</S>
			<S sid="103" ssid="10">i ) that penalized false negatives 3 times more than false positives, where 3 was picked bytesting several values on the validation set.</S>
			<S sid="104" ssid="11">In stead of selecting a regularization parameter ?and running to convergence, we used early stopping as a cheap regularization method, by set ting ? to a very large value (10000) and running the algorithm for 500 iterations.</S>
			<S sid="105" ssid="12">We selected a stopping point using the validation set by simply picking the best iteration on the validation set in terms of AER (ignoring the initial ten iterations, which were very noisy in our experiments).</S>
			<S sid="106" ssid="13">All selected iterations turned out to be in the first 50 iterations, as the algorithm converged fairly rapidly.</S>
			<S sid="107" ssid="14">3.1 Features and Results.</S>
			<S sid="108" ssid="15">Very broadly speaking, the classic IBM mod els of word-level translation exploit four primary sources of knowledge and constraint: association of words (all IBM models), competition betweenalignments (all models), zero- or first-order preferences of alignment positions (2,4+), and fer tility (3+).</S>
			<S sid="109" ssid="16">We model all of these in some way, 76 on e of th e ma jo r ob je ct iv es of th es e co ns ul ta ti on s is to ma ke su re th at th e re co ve ry be ne fi ts al l . le un de les grands objectifs de les consultations est de faire en sorte que la relance profite e?galement a` tous . on e of th e ma jo r ob je ct iv es of th es e co ns ul ta ti on s is to ma ke su re th at th e re co ve ry be ne fi ts al l . le un de les grands objectifs de les consultations est de faire en sorte que la relance profite e?galement a` tous .</S>
			<S sid="110" ssid="17">(a) Dice only (b) Dice and Distance on e of th e ma jo r ob je ct iv es of th es e co ns ul ta ti on s is to ma ke su re th at th e re co ve ry be ne fi ts al l . le un de les grands objectifs de les consultations est de faire en sorte que la relance profite e?galement a` tous . on e of th e ma jo r ob je ct iv es of th es e co ns ul ta ti on s is to ma ke su re th at th e re co ve ry be ne fi ts al l . le un de les grands objectifs de les consultations est de faire en sorte que la relance profite e?galement a` tous .</S>
			<S sid="111" ssid="18">(c) Dice, Distance, Orthographic, and BothShort (d) All features Figure 1: Example alignments for each successive feature set.</S>
			<S sid="112" ssid="19">except fertility.1First, and, most importantly, we want to include information about word association; trans lation pairs are likely to co-occur together in a bitext.</S>
			<S sid="113" ssid="20">This information can be captured, among many other ways, using a feature whose 1In principle, we can model also model fertility, by allowing 0-k matches for each word rather than 0-1, and having bias features on each word.</S>
			<S sid="114" ssid="21">However, we did not explore this possibility.</S>
			<S sid="115" ssid="22">value is the Dice coefficient (Dice, 1945): Dice(e, f) = 2CEF (e, f)C E (e)C F (f) Here, C E and C F are counts of word occurrences in each language, while C EF is the number of co-occurrences of the two words.</S>
			<S sid="116" ssid="23">With just this feature on a pair of word tokens (which depends only on their types), we can already make a stab 77 at word alignment, aligning, say, each English word with the French word (or null) with thehighest Dice value (see (Melamed, 2000)), sim ply as a matching-free heuristic model.</S>
			<S sid="117" ssid="24">With Dice counts taken from the 1.1M sentences, thisgives and AER of 38.7 with English as the tar get, and 36.0 with French as the target (in line with the numbers from Och and Ney (2003)).</S>
			<S sid="118" ssid="25">As observed in Melamed (2000), this use ofDice misses the crucial constraint of competition: a candidate source word with high asso ciation to a target word may be unavailable for alignment because some other target has an even better affinity for that source word.</S>
			<S sid="119" ssid="26">Melameduses competitive linking to incorporate this con straint explicitly, while the IBM-style models get this effect via explaining-away effects in EM training.</S>
			<S sid="120" ssid="27">We can get something much like the combination of Dice and competitive linking by running with just one feature on each pair: the Dice value of that pair?s words.2 With just a Dice feature ? meaning no learning is needed yet ? we achieve an AER of 29.8, between the Dice with competitive linking result of 34.0 and Model 1 of 25.9 given in Och and Ney (2003).</S>
			<S sid="121" ssid="28">An example of the alignment at this stage is shown in Figure 1(a).</S>
			<S sid="122" ssid="29">Note that most errors lie off the diagonal, for example the often-correct to-a` match.</S>
			<S sid="123" ssid="30">IBM Model 2, as usually implemented, addsthe preference of alignments to lie near the di agonal.</S>
			<S sid="124" ssid="31">Model 2 is driven by the product of a word-to-word measure and a (usually) Gaussian distribution which penalizes distortion from thediagonal.</S>
			<S sid="125" ssid="32">We can capture the same effect using features which reference the relative posi tions j and k of a pair (e j , f k ).</S>
			<S sid="126" ssid="33">In addition to aModel 2-style quadratic feature referencing relative position, we threw in the following proximity features: absolute difference in relative posi tion abs(j/|e|?k/|f |), and the square and squareroot of this value.</S>
			<S sid="127" ssid="34">In addition, we used a con junction feature of the dice coefficient times the proximity.</S>
			<S sid="128" ssid="35">Finally, we added a bias feature on each edge, which acts as a threshold that allows 2This isn?t quite competitive linking, because we use a non-greedy matching.</S>
			<S sid="129" ssid="36">in 19 78 Am er ic an s di vo rc ed 1, 12 2, 00 0 ti me s . en 1978 , on a enregistre?</S>
			<S sid="130" ssid="37">1,122,000 divorces sur le continent . in 19 78 Am er ic an s di vo rc ed 1, 12 2, 00 0 ti me s . en 1978 , on a enregistre?</S>
			<S sid="131" ssid="38">1,122,000 divorces sur le continent .</S>
			<S sid="132" ssid="39">(a) (b)Figure 2: Example alignments showing the ef fects of orthographic cognate features.</S>
			<S sid="133" ssid="40">(a) Dice and Distance, (b) With Orthographic Features.</S>
			<S sid="134" ssid="41">sparser, higher precision alignments.</S>
			<S sid="135" ssid="42">With these features, we got an AER of 15.5 (compare to 19.5 for Model 2 in (Och and Ney, 2003)).</S>
			<S sid="136" ssid="43">Note that we already have a capacity that Model 2 does not: we can learn a non-quadratic penalty with linear mixtures of our various components ? this gives a similar effect to learning the variance of the Gaussian for Model 2, but is, at least in principle, more flexible.3 These features fix the to-a` error in Figure 1(a), giving the alignment in Figure 1(b).</S>
			<S sid="137" ssid="44">On top of these features, we included other kinds of information, such as word-similarityfeatures designed to capture cognate (and ex act match) information.</S>
			<S sid="138" ssid="45">We added a feature forexact match of words, exact match ignoring accents, exact matching ignoring vowels, and frac tion overlap of the longest common subsequence.</S>
			<S sid="139" ssid="46">Since these measures were only useful for long words, we also added a feature which indicatesthat both words in a pair are short.</S>
			<S sid="140" ssid="47">These or thographic and other features improved AER to14.4.</S>
			<S sid="141" ssid="48">The running example now has the align ment in Figure 1(c), where one improvement may be attributable to the short pair feature ? it has stopped proposing the-de, partially because the short pair feature downweights the score of that pair.</S>
			<S sid="142" ssid="49">A clearer example of these features making a difference is shown in Figure 2, whereboth the exact-match and character overlap fea 3The learned response was in fact close to a Gaussian, but harsher near zero displacement.</S>
			<S sid="143" ssid="50">78 tures are used.</S>
			<S sid="144" ssid="51">One source of constraint which our model stilldoes not explicitly capture is the first-order de pendency between alignment positions, as in theHMM model (Vogel et al, 1996) and IBM models 4+.</S>
			<S sid="145" ssid="52">The the-le error in Figure 1(c) is symp tomatic of this lack.</S>
			<S sid="146" ssid="53">In particular, it is a slightly better pair according to the Dice value than the correct the-les.</S>
			<S sid="147" ssid="54">However, the latter alignment has the advantage that major-grands follows it.</S>
			<S sid="148" ssid="55">To use this information source, we included a feature which gives the Dice value of the wordsfollowing the pair.4 We also added a word frequency feature whose value is the absolutedifference in log rank of the words, discourag ing very common words from translating to very rare ones.</S>
			<S sid="149" ssid="56">Finally, we threw in bilexical features of the pairs of top 5 non-punctuation words ineach language.5 This helped by removing spe cific common errors like the residual tendency for French de to mistakenly align to English the (the two most common words).</S>
			<S sid="150" ssid="57">The resulting model produces the alignment in Figure 1(d).</S>
			<S sid="151" ssid="58">It has sorted out the the-le / the-les confusion, and is also able to guess to-de, which is not the most common translation for either word, but which is supported by the good Dice value on the following pair (make-faire).</S>
			<S sid="152" ssid="59">With all these features, we got a final AER of 10.7, broadly similar to the 8.9 or 9.7 AERs of unsymmetrized IBM Model 4 trained on the same data that the Dice counts were takenfrom.6 Of course, symmetrizing Model 4 by in tersecting alignments from both directions does yield an improved AER of 6.9, so, while ourmodel does do surprisingly well with cheaply ob tained count-based features, Model 4 does still outperform it so far.</S>
			<S sid="153" ssid="60">However, our model can4It is important to note that while our matching algo rithm has no first-order effects, the features can encode such effects in this way, or in better ways ? e.g. using as features posteriors from the HMM model in the style of Matusov et al (2004).</S>
			<S sid="154" ssid="61">5The number of such features which can be learned depends on the number of training examples, and since some of our experiments used only a few dozen training examples we did not make heavy use of this feature.</S>
			<S sid="155" ssid="62">6Note that the common word pair features affectedcommon errors and therefore had a particularly large im pact on AER.</S>
			<S sid="156" ssid="63">Model AER Dice (without matching) 38.7 / 36.0 Model 4 (E-F, F-E, intersected) 8.9 / 9.7/ 6.9 Discriminative Matching Dice Feature Only 29.8 + Distance Features 15.5 + Word Shape and Frequency 14.4 + Common Words and Next-Dice 10.7 + Model 4 Predictions 5.4 Figure 3: AER on the Hansards task.</S>
			<S sid="157" ssid="64">also easily incorporate the predictions of Model 4 as additional features.</S>
			<S sid="158" ssid="65">We therefore added three new features for each edge: the prediction of Model 4 in the English-French direction, the prediction in the French-English direction, and the intersection of the two predictions.</S>
			<S sid="159" ssid="66">With these powerful new features, our AER dropped dramatically to 5.4, a 22% improvement over the intersected Model 4 performance.Another way of doing the parameter estima tion for this matching task would have been to use an averaged perceptron method, as in Collins (2002).</S>
			<S sid="160" ssid="67">In this method, we merely run our matching algorithm and update weights based on the difference between the predictedand target matchings.</S>
			<S sid="161" ssid="68">However, the perfor mance of the average perceptron learner on the same feature set is much lower, only 8.1, not even breaking the AER of its best single feature (the intersected Model 4 predictions).</S>
			<S sid="162" ssid="69">3.2 Scaling Experiments.</S>
			<S sid="163" ssid="70">We explored the scaling of our method by learn ing on a larger training set, which we created by using GIZA++ intersected bi-directional Model 4 alignments for the unlabeled sentence pairs.</S>
			<S sid="164" ssid="71">We then took the first 5K sentence pairs from these 1.1M Model 4 alignments.</S>
			<S sid="165" ssid="72">This gave us more training data, albeit with noisier labels.</S>
			<S sid="166" ssid="73">On a 3.4GHz Intel Xeon CPU, GIZA++ took 18 hours to align the 1.1M words, while ourmethod learned its weights in between 6 min utes (100 training sentences) and three hours (5K sentences).</S>
			<S sid="167" ssid="74">79</S>
	</SECTION>
	<SECTION title="Conclusions. " number="4">
			<S sid="168" ssid="1">We have presented a novel discriminative, large margin method for learning word-alignment models on the basis of arbitrary features of wordpairs.</S>
			<S sid="169" ssid="2">We have shown that our method is suitable for the common situation where a moder ate number of good, fairly general features must be balanced on the basis of a small amount of labeled data.</S>
			<S sid="170" ssid="3">It is also likely that the method will be useful in conjunction with a large labeled alignment corpus (should such a set be created).</S>
			<S sid="171" ssid="4">We presented features capturing a few separate sources of information, producing alignments on the order of those given by unsymmetrized IBM Model 4 (using labeled training data of about the size others have used to tune generative models).</S>
			<S sid="172" ssid="5">In addition, when given bi-directional Model 4 predictions as features, our method provides a 22% AER reduction over intersected Model 4 predictions alone.</S>
			<S sid="173" ssid="6">The resulting 5.4 AER on the English-French Hansarks task is,to our knowledge, the best published AER fig ure for this training scenario (though since we use a subset of the test set, evaluations are not problem-free).</S>
			<S sid="174" ssid="7">Finally, our method scales to large numbers of training sentences and trains in minutes rather than hours or days for thehigher-numbered IBM models, a particular ad vantage when not using features derived from those slower models.</S>
	</SECTION>
</PAPER>
