<PAPER>
  <S sid="0">Nymble: A High-Performance Learning Name-Finder</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model.</S>
    <S sid="2" ssid="2">We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.</S>
  </ABSTRACT>
  <SECTION title="1." number="1">
    <S sid="3" ssid="1">In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM's to their problems.</S>
    <S sid="4" ssid="2">More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993).</S>
    <S sid="5" ssid="3">We would now propose that HMM's have successfully been applied to the problem of name-finding.</S>
    <S sid="6" ssid="4">We have built a named-entity (NE) recognition system using a slightly-modified version of an HMM; we call our system &amp;quot;Nymble&amp;quot;.</S>
    <S sid="7" ssid="5">To our knowledge, Nymble out-performs the best published results of any other learning name-finder.</S>
    <S sid="8" ssid="6">Furthermore, it performs at or above the 90% accuracy level, often considered &amp;quot;near-human performance&amp;quot;.</S>
    <S sid="9" ssid="7">The system arose from the NE task as specified in the last Message Understanding Conference (MUC), where organization names, person names, location names, times, dates, percentages and money amounts were to be delimited in text using SGML-markup.</S>
    <S sid="10" ssid="8">We will describe the various models employed, the methods for training these models and the method for &amp;quot;decoding&amp;quot; on test data (the term &amp;quot;decoding&amp;quot; borrowed from the speech recognition community, since one goal of traversing an HMM is to recover the hidden state sequence).</S>
    <S sid="11" ssid="9">To date, we have successfully trained and used the model on both English and Spanish, the latter for MET, the multi-lingual entity task.</S>
  </SECTION>
  <SECTION title="2." number="2">
    <S sid="12" ssid="1">The basic premise of the approach is to consider the raw text encountered when decoding as though it had passed through a noisy channel, where it had been originally marked with named entities.'</S>
    <S sid="13" ssid="2">The job of the generative model is to model the original process which generated the name-class&#8211;annotated words, before they went through the noisy channel.</S>
    <S sid="14" ssid="3">More formally, we must find the most likely sequence of name-classes (NC) given a sequence of words (W): Pr(NC I W) (2.1) In order to treat this as a generative model (where it generates the original, name-class&#8211;annotated words), and since the a priori probability of the word sequence&#8212;the denominator&#8212;is constant for any given sentence, we can maxi-mize Equation 2.2 by maximizing the numerator alone.</S>
    <S sid="15" ssid="4">I See (Cover and Thomas, 1991), ch.</S>
    <S sid="16" ssid="5">2, for an excellent overview of the principles of information theory.</S>
    <S sid="17" ssid="6">(2.2) Previous approaches have typically used manually constructed finite state patterns (Weischodel, 1995, Appelt et al., 1995).</S>
    <S sid="18" ssid="7">For every new language and every new class of new information to spot, one has to write a new set of rules to cover the new language and to cover the new class of information.</S>
    <S sid="19" ssid="8">A finite-state pattern rule attempts to match against a sequence of tokens (words), in much the same way as a general regular expression matcher.</S>
    <S sid="20" ssid="9">In addition to these finitestate pattern approaches, a variant of Brill rules has been applied to the problem, as outlined in (Aberdeen et al., 1995).</S>
    <S sid="21" ssid="10">The atomic elements of information extraction&#8212; indeed, of language as a whole&#8212;could be considered the who, where, when and how much in a sentence.</S>
    <S sid="22" ssid="11">A name-finder performs what is known as surface- or lightweight-parsing, delimiting sequences of tokens that answer these important questions.</S>
    <S sid="23" ssid="12">It can be used as the first step in a chain of processors: a next level of processing could relate two or more named entities, or perhaps even give semantics to that relationship using a verb.</S>
    <S sid="24" ssid="13">In this way, further processing could discover the &amp;quot;what&amp;quot; and &amp;quot;how&amp;quot; of a sentence or body of text.</S>
    <S sid="25" ssid="14">Furthermore, name-finding can be useful in its own right: an Internet query system might use namefinding to construct more appropriately-formed queries: &amp;quot;When was Bill Gates born?&amp;quot; could yield the query &amp;quot;Bill Gates&amp;quot;+born.</S>
    <S sid="26" ssid="15">Also, name-finding can be directly employed for link analysis and other information retrieval problems.</S>
  </SECTION>
  <SECTION title="3." number="3">
    <S sid="27" ssid="1">We will present the model twice, first in a conceptual and informal overview, then in a moredetailed, formal description of it as a type of HMM.</S>
    <S sid="28" ssid="2">The model bears resemblance to Scott Miller's novel work in the Air Traffic Information System (ATIS) task, as documented in (Miller et al., 1994).</S>
    <S sid="29" ssid="3">Figure 3.1 is a pictorial overview of our model.</S>
    <S sid="30" ssid="4">Informally, we have an ergodic HMM with only eight internal states (the name classes, including the NOT-A-NAME class), with two special states, the START- and END-OF-SENTENCE states.</S>
    <S sid="31" ssid="5">Within each of the name-class states, we use a statistical bigram language model, with the usual one-word-per-state emission.</S>
    <S sid="32" ssid="6">This means that the number of states in each of the name-class states is equal to the vocabulary size, I VI .</S>
    <S sid="33" ssid="7">The generation of words and name-classes proceeds in three steps: These three steps are repeated until the entire observed word sequence is generated.</S>
    <S sid="34" ssid="8">Using the Viterbi algorithm, we efficiently search the entire space of all possible name-class assignments, maximizing the numerator of Equation 2.2, Pr(W, NC).</S>
    <S sid="35" ssid="9">Informally, the construction of the model in this manner indicates that we view each type of &amp;quot;name&amp;quot; to be its own language, with separate bigram probabilities for generating its words.</S>
    <S sid="36" ssid="10">While the number of word-states within each name-class is equal to I VI , this &amp;quot;interior&amp;quot; bigram language model is ergodic, i.e., there is a probability associated with every one of the 1V12transitions.</S>
    <S sid="37" ssid="11">As a parameterized, trained model, if such a transition were never observed, the model &amp;quot;backs off' to a less-powerful model, as described below, in &#167;3.3.3 on p. 4.</S>
    <S sid="38" ssid="12">Throughout most of the model, we consider words to be ordered pairs (or two-element vectors), composed of word and word-feature, denoted (w, f).</S>
    <S sid="39" ssid="13">The word feature is a simple, deterministic computation performed on each word as it is added to or feature computation is an extremely small part of the implementation, at roughly ten lines of code.</S>
    <S sid="40" ssid="14">Also, most of the word features are used to distinguish types of numbers, which are language-independent.2 The rationale for having such features is clear: in Roman languages, capitalization gives good evidence of names.3 This section describes the model formally, discussing the transition probabilities to the wordstates, which &amp;quot;generate&amp;quot; the words of each name-class.</S>
    <S sid="41" ssid="15">As with most trained, probabilistic models, we looked up in the vocabulary.</S>
    <S sid="42" ssid="16">It produces one of the fourteen values in Table 3.1.</S>
    <S sid="43" ssid="17">These values are computed in the order listed, so that in the case of non-disjoint feature-classes, such as containsDigitAndAlpha and containsDigitAndDash, the former will take precedence.</S>
    <S sid="44" ssid="18">The first eight features arise from the need to distinguish and annotate monetary amounts, percentages, times and dates.</S>
    <S sid="45" ssid="19">The rest of the features distinguish types of capitalization and all other words (such as punctuation marks, which are separate tokens).</S>
    <S sid="46" ssid="20">In particular, the f irstWord feature arises from the fact that if a word is capitalized and is the first word of the sentence, we have no good information as to why it is capitalized (but note that allCaps and capPeriod are computed before f irstWord, and therefore take precedence).</S>
    <S sid="47" ssid="21">The word feature is the one part of this model which is language-dependent.</S>
    <S sid="48" ssid="22">Fortunately, the word have a most accurate, most powerful model, which will &amp;quot;back off' to a less-powerful model when there is insufficient training, and ultimately back-off to unigram probabilities.</S>
    <S sid="49" ssid="23">In order to generate the first word, we must make a transition from one name-class to another, as well as calculate the likelihood of that word.</S>
    <S sid="50" ssid="24">Our intuition was that a word preceding the start of a name-class (such as &amp;quot;Mr.&amp;quot;, &amp;quot;President&amp;quot; or other titles preceding the PERSON name-class) and the word following a name-class would be strong indicators of the subsequent and preceding name-classes, respectively.</S>
    <S sid="51" ssid="25">2 Non-english languages tend to use the comma and period in the reverse way in which English does, i.e., the comma is a decimal point and the period separates groups of three digits in large numbers.</S>
    <S sid="52" ssid="26">However, the re-ordering of the precedence of the two relevant word-features had little effect when decoding Spanish, so they were left as is.</S>
    <S sid="53" ssid="27">3 Although Spanish has many lower-case words in organization names.</S>
    <S sid="54" ssid="28">See &#167;4.1 on p. 6 for more details.</S>
    <S sid="55" ssid="29">Accordingly, the probabilitiy for generating the first word of a name-class is factored into two parts: Pr(NC I NC_,, w_1) Pr((w,f)firs, I NC, NC_,).</S>
    <S sid="56" ssid="30">(3.1) The top level model for generating all but the first word in a name-class is Pr((w, NC).</S>
    <S sid="57" ssid="31">(3.2) There is also a magical &amp;quot;+end+&amp;quot; word, so that the probability may be computed for any current word to be the final word of its name-class, i.e., Pr((+end+, o the r) I(w, f)find' NC).</S>
    <S sid="58" ssid="32">(3.3) As one might imagine, it would be useless to have the first factor in Equation 3.1 be conditioned off of the +end+ word, so the probability is conditioned on the previous real word of the previous name-class, i.e., we compute W-1 = last observed word otherwise NC , = START - OF - SENTENCE (3.4) Note that the above probability is not conditioned on the word-feature of w_1, the intuition of which is that in the cases where the previous word would help the model predict the next name-class, the world feature&#8212;capitalization in particular&#8212;is not important: &amp;quot;Mr.&amp;quot; is a good indicator of the next word beginning the PERSON name-class, regardless of capitalization, especially since it is almost never seen as &amp;quot;mr.&amp;quot;.</S>
    <S sid="59" ssid="33">The calculation of the above probabilities is straightforward, using events/sample-size: where c() represents the number of times the events occurred in the training data (the count).</S>
    <S sid="60" ssid="34">Ideally, we would have sufficient training (or at least one observation of!) every event whose conditional probability we wish to calculate.</S>
    <S sid="61" ssid="35">Also, ideally, we would have sufficient samples of that upon which each conditional probability is conditioned, e.g., for Pr(NC I NC 1, w_,), we would like to have seen sufficient numbers of NC_,, w1.</S>
    <S sid="62" ssid="36">Unfortunately, there is rarely enough training data to compute accurate probabilities when &amp;quot;decoding&amp;quot; on new data.</S>
    <S sid="63" ssid="37">3.</S>
    <S sid="64" ssid="38">3.</S>
    <S sid="65" ssid="39">3.1 Unknown Words The vocabulary of the system is built as it trains.</S>
    <S sid="66" ssid="40">Necessarily, then, the system knows about all words for which it stores bigram counts in order to compute the probabilities in Equations 3.1 &#8211; 3.3.</S>
    <S sid="67" ssid="41">The question arises how the system should deal with unknown words, since there are three ways in which they can appear in a bigram: as the current word, as the previous word or as both.</S>
    <S sid="68" ssid="42">A good answer is to train a separate, unknown word&#8211;model off of held-out data, to gather statistics of unknown words occurring in the midst of known words.</S>
    <S sid="69" ssid="43">Typically, one holds out 10-20% of one's training for smoothing or unknown word&#8211;training.</S>
    <S sid="70" ssid="44">In order to overcome the limitations of a small amount of training data&#8212;particularly in Spanish&#8212;we hold out 50% of our data to train the unknown word&#8211; model (the vocabulary is built up on the first 50%), save these counts in training data file, then hold out the other 50% and concatentate these bigram counts with the first unknown word&#8211;training file.</S>
    <S sid="71" ssid="45">This way, we can gather likelihoods of an unknown word appearing in the bigram using all available training data.</S>
    <S sid="72" ssid="46">This approach is perfectly valid, as we art trying to estimate that which we have not legitimately seen in training.</S>
    <S sid="73" ssid="47">When decoding, if either word of the bigram is unknown, the model used to estimate the probabilities of Equations 3.1-3 is the unknown word model, otherwise it is the model from the normal training.</S>
    <S sid="74" ssid="48">The unknown word&#8211;model can be viewed as a first level of back-off, therefore, since it is used as a backup model when an unknown word is encountered, and is necessarily not as accurate as the bigram model formed from the actual training.</S>
    <S sid="75" ssid="49">3.</S>
    <S sid="76" ssid="50">3.</S>
    <S sid="77" ssid="51">3.2 Further Back-off Models and Smoothing Whether a bigram contains an unknown word or not, it is possible that either model may not have seen this bigram, in which case the model backs off to a less-powerful, less-descriptive model.</S>
    <S sid="78" ssid="52">Table 3.2 shows a graphic illustration of the back-off scheme: The weight for each back-off model is computed onthe-fly, using the following formula: If computing Pr(XIY), assign weight of A to the direct computation (using one of the formulae of &#167;3.3.2) and a weight of (1 &#8212; A) to the back-off model, where (3.8) where &amp;quot;old c(Y)&amp;quot; is the sample size of the model from which we are backing off.</S>
    <S sid="79" ssid="53">This is a rather simple method of smoothing, which tends to work well when there are only three or four levels of back-off.4 This method also overcomes the problem when a back-off model has roughly the same amount of training as the current model, via the first factor of Equation 3.8, which essentially ignores the back-off model and puts all the weight on the primary model, in such an equi-trained situation.</S>
    <S sid="80" ssid="54">As an example&#8212;disregarding the first factor&#8212;if we saw the bigram &amp;quot;come hither&amp;quot; once in training and we saw &amp;quot;come here&amp;quot; three times, and nowhere else did we see the word &amp;quot;come&amp;quot; in the NOT-A-NAME class, when computing Pr(&amp;quot;hither&amp;quot; I &amp;quot;come&amp;quot;, NOT-A-NAME), we would back off to the unigram probability Pr(&amp;quot;hither&amp;quot; I NOT-A-NAME) with a weight of , since the number of unique outcomes for the word-state for &amp;quot;come&amp;quot; would be two, and the total number of times &amp;quot;come&amp;quot; had been the preceding word in a bigram would be four (a 4 Any more levels of back-off might require a more sophisticated smoothing technique, such as deleted interpolation.</S>
    <S sid="81" ssid="55">No matter what smoothing technique is used, one must remember that smoothing is the art of estimating the probability of that which is unknown (i.e., not seen in training).</S>
    <S sid="82" ssid="56">Unlike a traditional HMM, the probability of generating a particular word is 1 for each word-state inside each of the name-class states.</S>
    <S sid="83" ssid="57">An alternative&#8212; and more traditional&#8212;model would have a small number of states within each name-class, each having, perhaps, some semantic signficance, e.g., three states in the PERSON name-class, representing a first, middle and last name, where each of these three states would have some probability associated with emitting any word from the vocabulary.</S>
    <S sid="84" ssid="58">We chose to use a bigram language model because, while less semantically appealing, such n-gram language models work remarkably well in practice.</S>
    <S sid="85" ssid="59">Also, as a first research attempt, an n-gram model captures the most general significance of the words in each name-class, without presupposing any specifics of the structure of names, a la the PERSON name-class example, above.</S>
    <S sid="86" ssid="60">More important, either approach is mathematically valid, as long as all transitions out of a given state sum to one.</S>
    <S sid="87" ssid="61">All of this modeling would be for naught were it not for the existence of an efficient algorithm for finding the optimal state sequence, thereby &amp;quot;decoding&amp;quot; the original sequence of name-classes.</S>
    <S sid="88" ssid="62">The number of possible state sequences for N states in an ergodic model for a sentence of m words is Alm, but, using dynamic programming and an appropriate merging of multiple theories when they converge on a particular state&#8212;the Viterbi decoding algorithm&#8212;a sentence can be &amp;quot;decoded&amp;quot; in time linear to the number of tokens in the sentence, 0(m) (Viterbi, 1967).</S>
    <S sid="89" ssid="63">Since we are interested in recovering the name-class state sequence, we pursue eight theories at every given step of the algorithm.</S>
  </SECTION>
  <SECTION title="4." number="4">
    <S sid="90" ssid="1">Initially, the word-feature was not in the model; instead the system relied on a third-level back-off partof-speech tag, which in turn was computed by our stochastic part-of-speech tagger.</S>
    <S sid="91" ssid="2">The tags were taken at face value: there were not k-best tags; the system treated the part-of-speech tagger as a &amp;quot;black box&amp;quot;.</S>
    <S sid="92" ssid="3">Although the part-of-speech tagger used capitalization to help it determine proper-noun tags, this feature was only implicit in the model, and then only after two levels of back-off!</S>
    <S sid="93" ssid="4">Also, the capitalization of a word was submerged in the muddiness of part-of-speech tags, which can &amp;quot;smear&amp;quot; the capitalization probability mass over several tags.</S>
    <S sid="94" ssid="5">Because it seemed that capitalization would be a good name-predicting feature, and that it should appear earlier in the model, we eliminated the reliance on part-of-speech altogether, and opted for the more direct, word-feature model described above, in &#167;3.</S>
    <S sid="95" ssid="6">Originally, we had a very small number of features, indicating whether the word was a number, the first word of a sentence, all uppercase, inital-capitalized or lower-case.</S>
    <S sid="96" ssid="7">We then expanded the feature set to its current state in order to capture more subtleties related mostly to numbers; due to increased performance (although not entirely dramatic) on every test, we kept the enlarged feature set.</S>
    <S sid="97" ssid="8">Contrary to our expectations (which were based on our experience with English), Spanish contained many examples of lower-case words in organization and location names.</S>
    <S sid="98" ssid="9">For example, departamento (&amp;quot;Department&amp;quot;) could often start an organization name, and adjectival place-names, such as coreana (&amp;quot;Korean&amp;quot;) could appear in locations and by convention are not capitalized.</S>
    <S sid="99" ssid="10">The entire system is implemented in C++, atop a &amp;quot;home-brewed&amp;quot;, general-purpose class library, providing a rapid code-compile-train-test cycle.</S>
    <S sid="100" ssid="11">In fact, many NLP systems suffer from a lack of software and computer-science engineering effort: runtime efficiency is key to performing numerous experiments, which, in turn, is key to improving performance.</S>
    <S sid="101" ssid="12">A system may have excellent performance on a given task, but if it takes long to compile and/or run on test data, the rate of improvement of that system will be miniscule compared to that which can run very efficiently.</S>
    <S sid="102" ssid="13">On a Sparc20 or SGI Indy with an appropritae amount of RAM, Nymble can compile in 10 minutes, train in 5 minutes and run at 6MB/hr.</S>
    <S sid="103" ssid="14">There were days in which we had as much as a 15% reduction in error rate, to borrow the performance measure used by the speech community, where error rate = 100% &#8212; Fmeasure.</S>
    <S sid="104" ssid="15">(See &#167;4.3 for the definition of F-measure.)</S>
    <S sid="105" ssid="16">In this section we report the results of evaluating the final version of the learning software.</S>
    <S sid="106" ssid="17">We report the results for English and for Spanish and then the results of a set of experiments to determine the impact of the training set size on the algorithm's performance in both English and Spanish.</S>
    <S sid="107" ssid="18">For each language, we have a held-out development test set and a held-out, blind test set.</S>
    <S sid="108" ssid="19">We only report results on the blind test set for each respective language.</S>
    <S sid="109" ssid="20">The scoring program measures both precision =I recall, terms borrowed from the information-retrieval community, where number of correct responses and number responses number of correct responses number correct in key Put informally, recall measures the number of &amp;quot;hits&amp;quot; vs. the number of possible correct answers as specified in the key file, whereas precision measures how many answers were correct ones compared to the number of answers delivered.</S>
    <S sid="110" ssid="21">These two measures of performance combine to form one measure of performance, the F-measure, which is computed by the weighted harmonic mean of precision and recall: (/32 +1)RP where if represents the relative weight of recall to precision (and typically has the value 1).</S>
    <S sid="111" ssid="22">To our knowledge, our learned name-finding system has achieved a higher F-measure than any other learned system when compared to state-of-the-art manual (rule-based) systems on similar data.</S>
    <S sid="112" ssid="23">Our test set of English data for reporting results is that of the MUC-6 test set, a collection of 30 WSJ documents (we used a different test set during development).</S>
    <S sid="113" ssid="24">Our Spanish test set is that used for MET, comprised of articles from the news agency AFP.</S>
    <S sid="114" ssid="25">Table 4.1 illustrates Nymble's performance as compared to the best reported scores for each category.</S>
    <S sid="115" ssid="26">(4.2)</S>
  </SECTION>
  <SECTION title="4.3.3 The Amount of Training Data Required" number="5">
    <S sid="116" ssid="1">With any learning technique one of the important questions is how much training data is required to get acceptable performance.</S>
    <S sid="117" ssid="2">More generally how does performance vary as the training set size is increased or decreased?</S>
    <S sid="118" ssid="3">We ran a sequence of experiments in English and in Spanish to try to answer this question for the final model that was implemented.</S>
    <S sid="119" ssid="4">For English, there were 450,000 words of training data.</S>
    <S sid="120" ssid="5">By that we mean that the text of the document itself (including headlines but not including SGML tags) was 450,000 words long.</S>
    <S sid="121" ssid="6">Given this maximum size of training available to us, we successfully divided the training material in half until we were using only one eighth of the original training set size or a training set of 50,000 words for the smallest experiment.</S>
    <S sid="122" ssid="7">To give a sense of the size of 450,000 words, that is roughly half the length of one edition of the Wall Street Journal.</S>
    <S sid="123" ssid="8">The results are shown in a histogram in Figure 4.1 below.</S>
    <S sid="124" ssid="9">The positive outcome of the experiment is that half as much training data would have given almost equivalent performance.</S>
    <S sid="125" ssid="10">Had we used only one quarter of the data or approximately 100,000 words, performance would have degraded slightly, only about 1-2 percent.</S>
    <S sid="126" ssid="11">Reducing the training set size to 50,000 words would have had a more significant decrease in the performance of the system; however, the performance is still impressive even with such a small training set.</S>
    <S sid="127" ssid="12">On the other hand, the result also shows that merely annotating more data will not yield dramatic improvement in the performance.</S>
    <S sid="128" ssid="13">With increased training data it would be possible to use even more detailed models that require more data and could achieve significantly improved overall system performance with those more detailed models.</S>
    <S sid="129" ssid="14">For Spanish we had only 223,000 words of training data.</S>
    <S sid="130" ssid="15">We also measured the performance of the system with half the training data or slightly more than 100,000 words of text.</S>
    <S sid="131" ssid="16">Figure 4.2 shows the results.</S>
    <S sid="132" ssid="17">There is almost no change in performance by using as little as 100,000 words of training data.</S>
    <S sid="133" ssid="18">Therefore the results in both languages were comparable.</S>
    <S sid="134" ssid="19">As little as 100,000 words of training data produces performance nearly comparable to handcrafted systems.</S>
  </SECTION>
  <SECTION title="5." number="6">
    <S sid="135" ssid="1">While our initial results have been quite favorable, there is still much that can be done potentially to improve performance and completely close the gap between learned and rule-based name-finding systems.</S>
    <S sid="136" ssid="2">We would like to incorporate the following into the current model:</S>
  </SECTION>
  <SECTION title="6." number="7">
    <S sid="137" ssid="1">We have shown that using a fairly simple probabilistic model, finding names and other numerical entities as specified by the MUC tasks can be performed with &amp;quot;near-human performance&amp;quot;, often likened to an F of 90 or above.</S>
    <S sid="138" ssid="2">We have also shown that such a system can be trained efficiently and that, given appropriately and consistently marked answer keys, it can be trained on languages foreign to the trainer of the system; for example, we do not speak Spanish, but trained Nymble on answer keys marked by native speakers.</S>
    <S sid="139" ssid="3">None of the formalisms or techniques presented in this paper is new; rather, the approach to this task&#8212;the model itself&#8212;is wherein lies the novelty.</S>
    <S sid="140" ssid="4">Given the incredibly difficult nature of many NLP tasks, this example of a learned, stochastic approach to name-finding lends credence to the argument that the NLP community ought to push these approaches, to find the limit of phenomena that may be captured by probabilistic, finite-state methods.</S>
  </SECTION>
</PAPER>
