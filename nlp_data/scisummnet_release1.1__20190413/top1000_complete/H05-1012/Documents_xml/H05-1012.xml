<PAPER>
	<S sid="0">A Maximum Entropy Word Aligner For Arabic-English Machine Translation</S><ABSTRACT>
		<S sid="1" ssid="1">This paper presents a maximum entropyword alignment algorithm for Arabic English based on supervised training data.We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of su pervised and unsupervised methods yields superior performance.</S>
		<S sid="2" ssid="2">The probabilisticmodel used in the alignment directly models the link decisions.</S>
		<S sid="3" ssid="3">Significant improvement over traditional word alignment tech niques is shown as well as improvement onseveral machine translation tests.</S>
		<S sid="4" ssid="4">Perfor mance of the algorithm is contrasted with human annotation performance.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">Machine translation takes a source sequence, S = [s1 s2 . . .</S>
			<S sid="6" ssid="6">sK ] and generates a target sequence, T = [t1 t2 . . .</S>
			<S sid="7" ssid="7">tM ] that renders the meaning of the source sequence into the target sequence.</S>
			<S sid="8" ssid="8">Typically, algorithms operate on sentences.</S>
			<S sid="9" ssid="9">In the most general setup, one or more source words can generate 0, 1 or more target words.</S>
			<S sid="10" ssid="10">Current state of the art machine translation systems (Och, 2003) use phrasal (n-gram) features extracted automatically from parallel corpora.</S>
			<S sid="11" ssid="11">These phrases are extracted using word alignment algorithms that are trained on parallel corpora.</S>
			<S sid="12" ssid="12">Phrases, or phrasal features, represent a mapping of source sequences into a target sequences which are typically a few words long.In this paper, we investigate the feasibility of training alignment algorithms based on supervised alignment data.</S>
			<S sid="13" ssid="13">Although there is a modest cost associ ated with annotating data, we show that a reduction of 40% relative in alignment error (AER) is possible over the GIZA++ aligner (Och and Ney, 2003).</S>
			<S sid="14" ssid="14">Although there are a number of other applications for word alignment, for example in creating bilingual dictionaries, the primary application continues to be as a component in a machine translation system.</S>
			<S sid="15" ssid="15">We test our aligner on several machine translation tests and show encouraging improvements.</S>
	</SECTION>
	<SECTION title="Related Work. " number="2">
			<S sid="16" ssid="1">Most of the prior work on word alignments has been done on parallel corpora where the alignment at the sentence level is also done automatically.</S>
			<S sid="17" ssid="2">The IBMmodels 1-5 (Brown et al, 1993) produce word align ments with increasing algorithmic complexity and performance.</S>
			<S sid="18" ssid="3">These IBM models and more recent refinements (Moore, 2004) as well as algorithms thatbootstrap from these models like the HMM algorithm described in (Vogel et al, 1996) are unsuper vised algorithms.</S>
			<S sid="19" ssid="4">The relative success of these automatic techniques together with the human annotation cost has delayed the collection of supervised word-aligned corpora for more than a decade.(Cherry and Lin, 2003) recently proposed a di rect alignment formulation and state that it would be straightforward to estimate the parameters givena supervised alignment corpus.</S>
			<S sid="20" ssid="5">In this paper, we ex tend their work and show that with a small amountof annotated data, together with a modeling strat egy and search algorithm yield significant gains in alignment F-measure.</S>
			<S sid="21" ssid="6">89 show vAny +pAl# AlvAnyp secondWords WordNet the 2nd 2d pointed +pwvyqAl#+tA$Arw# wA$Art AlwvyqpWords Segm.</S>
			<S sid="22" ssid="7">to Aly Aly Source Target papers document indicate point Figure 1: Alignment example.</S>
	</SECTION>
	<SECTION title="Algorithm. " number="3">
			<S sid="23" ssid="1">In order to describe the algorithm, we will need to first describe the direct link model.</S>
			<S sid="24" ssid="2">Figure 1 shows two sequences where the top sequence is considered the source sequence and the bottom sequence the target sequence.</S>
			<S sid="25" ssid="3">Each sequence can have auxilliary information such as Arabic segmentation or English WordNet (Miller, 1990) information as shown.</S>
			<S sid="26" ssid="4">Given the source and target sequences, there are a number of different ways to link each target word to a sourceword.</S>
			<S sid="27" ssid="5">Each target word has a link li which indi cates which source position it links to.</S>
			<S sid="28" ssid="6">The range of li is from 0 to K and there are M of these links.</S>
			<S sid="29" ssid="7">The source word position 0 is used to indicate NULL which we imagine gives rise to unaligned Englishwords.</S>
			<S sid="30" ssid="8">In this paper, we refer to these words as be ing spontaneous.</S>
			<S sid="31" ssid="9">A valid link configuration has M links.</S>
			<S sid="32" ssid="10">Define L to be the set of all possible valid link configurations, and L to be a member of that set.</S>
			<S sid="33" ssid="11">We seek to maximize the alignment probability by finding the optimum link configuration Lopt, p(Lopt|S, T ) = argmax L?L p(L|S, T ) = p(lMi |tM1 , sK1 ) = M ? i=0 p(li|tM1 , sK1 , li?11 ).We factor this into a transition model and an obser vation model, p(L|S, T ) = 1Z M ? i=0 p(li|li?1)?p(li|tM1 , sK1 , li?11 )1??.</S>
			<S sid="34" ssid="12">where Z is the normalizing constant.We factor the model as above so that the tran sition model computation, which uses information available on the search hypotheses, is reduced during the search process.</S>
			<S sid="35" ssid="13">In the aligner presented here, ?is always set to 0.5.</S>
			<S sid="36" ssid="14">Next we will describe the tran sition model, then the observation model and finallythe experiments in alignment and machine transla tion.In the IBM Model 1 aligner, the choice of the lan guage to serve as states of the search algorithm is not prescribed, but practically the choice is important asit affects performance.</S>
			<S sid="37" ssid="15">To see this, note that in gen erative models an input word can only be aligned toa single state in the search.</S>
			<S sid="38" ssid="16">In our current situation, we are interested in aligning unsegmented Ara bic words and typical words have a few affixes toindicate for example pronouns, definiteness, prepositions and conjunctions.</S>
			<S sid="39" ssid="17">In English these are sepa rate words, and therefore to maximize performance the unsegmented Arabic words serve as states in the search algorithm and we align English words to these states.</S>
			<S sid="40" ssid="18">3.1 Transition Model.</S>
			<S sid="41" ssid="19">The transition model tends to keep the alignmentsclose together and penalizes alignments in which ad jacent words in the target language come from very distant words in the source language.</S>
			<S sid="42" ssid="20">Also, we would like to penalize many English words coming from the same Arabic state; we call this the state visit penalty and will be described later.</S>
			<S sid="43" ssid="21">In this paper, we use a parametric form for the transition model, p(li|li?1) = 1 Z(li?1) [ 1 dist(li, li?1) + 1ns(li) ] (1) 90 where ns(i) represents the state visit penalty for state i, Z(li?1) is the normalization constant and dist(li, li?1) = min(|li ? li?1|, |li ? fi|) + a. Here a is a penalty for a zero distance transition andis set to 1 in the experiments below.</S>
			<S sid="44" ssid="22">The min operator chooses the lowest cost transition distance ei ther from the previous state or the frontier state, fi, which is the right most state that has been visited (even though Arabic is normally displayed right to left, we make our Arabic state graphs from left toright).</S>
			<S sid="45" ssid="23">This is a language specific criteria and in tended to model the adjective noun reversal between English and Arabic.</S>
			<S sid="46" ssid="24">Once the current noun phrase is completed, the next word often aligns to the statejust beyond frontier state.</S>
			<S sid="47" ssid="25">As an example, in Fig ure 1, the verb ?pointed?</S>
			<S sid="48" ssid="26">aligns to the first Arabic word ?wA$Art?, and aligning the ?to?</S>
			<S sid="49" ssid="27">to its Arabic counterpart ?Aly?</S>
			<S sid="50" ssid="28">would incur normally a distance of 3 but with the frontier notion it incurs only a penalty of 1 on the hypothesis that aligns the word ?second?to ?AlvAnyp?.</S>
			<S sid="51" ssid="29">In this alignment with the frontier no tion, there are only distance 1 transitions, whereas the traditional shapes would incur a penalty of 2 for alignment of ?pointed?</S>
			<S sid="52" ssid="30">and a penalty of 3 for the word ?to?.The state visit penalty, ns(i) is the distance be tween the English words aligned to this state times the number of state visits1.</S>
			<S sid="53" ssid="31">This penalty controls the fertility of the Arabic words.</S>
			<S sid="54" ssid="32">To determine the English words that aligned to the Arabic position,the search path is traced back for each hypothe sis and a sufficiently large beam is maintained sothat alignments in the future can correct past alignment decisions.</S>
			<S sid="55" ssid="33">This penalty allows English determiners and prepositions to align to the Arabic content word while penalizing distant words from align ing to the state.</S>
			<S sid="56" ssid="34">In terms of alignment F-measureto be described below, the state visit penalty, if re moved makes the performance degrade from F=87.8 to F=84.0 compared to removing the frontier notion which only degrades performance to F=86.9.</S>
			<S sid="57" ssid="35">3.2 Observation Model.</S>
			<S sid="58" ssid="36">The observation model measures the linkage of the source and target using a set of feature functions defined on the words and their context.</S>
			<S sid="59" ssid="37">In Figure 1, an event is a single link from an English word to an Arabic state and the event space is the sentence pair.</S>
			<S sid="60" ssid="38">We use the maximum entropy formulation (e.g.</S>
			<S sid="61" ssid="39">(Berger et al, 1996)), 1We are overloading the word ?state?</S>
			<S sid="62" ssid="40">to mean Arabic word position.</S>
			<S sid="63" ssid="41">f = ?(li) h = [ ti?11 , sK1 ] p(f |h) = 1Z(h) exp ? i ?i?i(h, f), where Z(h) is the normalizing constant, Z(h) = ? f exp ? i ?i?i(h, f).</S>
			<S sid="64" ssid="42">and ?i(h, f) are binary valued feature functions.</S>
			<S sid="65" ssid="43">The function ? selects the Arabic word at the position being linked or in the case of segmentation features,one of the segmentations of that position.</S>
			<S sid="66" ssid="44">We re strict the history context to select from the current English word and words to the left as well as thecurrent word?s WordNet (Miller, 1990) synset as re quired by the features defined below.</S>
			<S sid="67" ssid="45">As in (Cherryand Lin, 2003), the above functions simplify the con ditioning portion, h by utilizing only the words andcontext involved in the link li.</S>
			<S sid="68" ssid="46">Training is done us ing the IIS technique (Della Pietra et al, 1995) and convergence often occurs in 3-10 iterations.</S>
			<S sid="69" ssid="47">The five types of features which are utilized in the system are described below.</S>
			<S sid="70" ssid="48">Phrase to phrase (for example, idiomatic phrases)alignments are intepreted as each English word com ing from each of the Arabic words.</S>
			<S sid="71" ssid="49">3.2.1 Lexical Features The lexical features are similar to the translationmatrix of the IBM Model 1.</S>
			<S sid="72" ssid="50">However, there is a sign ficant out of vocabulary (OOV) issue in the model since training data is limited.</S>
			<S sid="73" ssid="51">All words that have a corpus frequency of 1 are left out of the model and classed into an unknown word class in order to explicitly model connecting unknown words.</S>
			<S sid="74" ssid="52">From the training data we obtain 50K lexical features, and applying the Arabic segmenter obtain another 17K lexical features of the form ?(English content word, Arabic stem).</S>
			<S sid="75" ssid="53">3.2.2 Arabic Segmentation Features An Arabic segmenter similar to (Lee et al, 2003)provides the segmentation features.</S>
			<S sid="76" ssid="54">A small dictionary is used (with 71 rules) to restrict the set of Ara bic segments that can align to English stopwords, for example that ?the?</S>
			<S sid="77" ssid="55">aligns to ?Al#?</S>
			<S sid="78" ssid="56">and that ?for?, ?in?and ?to?</S>
			<S sid="79" ssid="57">align to ?b#?</S>
			<S sid="80" ssid="58">and ?her?</S>
			<S sid="81" ssid="59">aligns with the suffix ?+hA?.</S>
			<S sid="82" ssid="60">Segmentation features also help align un known words, as stems might be seen in the training corpus with other prefixes or suffixes.</S>
			<S sid="83" ssid="61">Additionally, the ability to align the prefix and suffix accurately,tends to ?drag?</S>
			<S sid="84" ssid="62">the unknown stem to its English tar get.</S>
			<S sid="85" ssid="63">91 3.2.3 WordNet Features WordNet features provide normalization on the English words.</S>
			<S sid="86" ssid="64">The feature is instantiated for nouns,adjectives, adverbs and verbs following their definitions in WordNet.</S>
			<S sid="87" ssid="65">If the Arabic word has a seg mentation then the feature is ?(WordNet synset id, Arabic stem), otherwise it is ?(WordNet synset id,Arabic word).</S>
			<S sid="88" ssid="66">The feature ties together English syn onyms and helps improve recall of the aligner.</S>
			<S sid="89" ssid="67">3.2.4 Spelling Feature The spelling feature is applied only on unknownwords and is used to measure the string kernel dis tance(Lodhi et al, 2000) between romanized Arabicand English words.</S>
			<S sid="90" ssid="68">The feature is designed primar ily to link unknown names.</S>
			<S sid="91" ssid="69">For example, ?Clinton?is written as ?klyntwn?</S>
			<S sid="92" ssid="70">in one of its romanized Arabic versions.</S>
			<S sid="93" ssid="71">In a sentence, measuring the string ker nel distance shows a correlation between these names even though there is not much overlap between thecharacters.</S>
			<S sid="94" ssid="72">The feature has four possible values: no match, somematch, goodmatch, and exact.</S>
			<S sid="95" ssid="73">3.2.5 Dynamic Features Dynamic features are defined on the lattice of thesearch algorithm.</S>
			<S sid="96" ssid="74">These features fire when the pre vious source and target word pair are linked.</S>
			<S sid="97" ssid="75">For example, one such feature is ?b# in?</S>
			<S sid="98" ssid="76">and if on the hypothesis we have just linked this pair and the nextEnglish word is being aligned to the stem of the Ara bic word where this prefix occurs, this feature fires and boosts the probability that the next words are aligned.</S>
			<S sid="99" ssid="77">The basic intuition behind this feature is that words inside prepositional phrases tend to align, which is similar to the dependency structure feature of (Cherry and Lin, 2003).At training time, the lattice reduces to the single path provided by the annotation.</S>
			<S sid="100" ssid="78">Since this fea ture tends to suffer from the drag of function words, we insist that the next words that are being linked have at least one feature that applies.</S>
			<S sid="101" ssid="79">All word pairslinked in the training data have lexical features as de scribed above, and if both source and target words are unknown they have a single feature for their link.</S>
			<S sid="102" ssid="80">Applying dynamic features on words that have atleast one other feature prevents words which are completely unrelated from being linked because of a fea ture about the context of the words.</S>
			<S sid="103" ssid="81">Two types of dynamic features are distinguished: (a) English word with Arabic prefix/suffix and (b) English word with Arabic stem.</S>
	</SECTION>
	<SECTION title="Smoothing the Observation Model. " number="4">
			<S sid="104" ssid="1">Since the annotated training data for word alignmentis limited and a much larger parallel corpus is avail able for other aligners, we smooth the observation Anno.</S>
			<S sid="105" ssid="2">1 Anno.</S>
			<S sid="106" ssid="3">1?</S>
			<S sid="107" ssid="4">Anno.</S>
			<S sid="108" ssid="5">2 Correction Anno.</S>
			<S sid="109" ssid="6">1 96.5 92.4 91.7 Anno.</S>
			<S sid="110" ssid="7">1?</S>
			<S sid="111" ssid="8">95.2 ? 93.2 Table 1: F-measure for human performance on word alignment for Arabic-English.</S>
			<S sid="112" ssid="9">probability with an IBM Model 1 estimate, p(li|tM1 , sK1 ) = 1 Z pME(li|t M 1 , sK1 )?pM1(s|ti)1??</S>
			<S sid="113" ssid="10">where ? is set to 0.9 in the experiments below.</S>
			<S sid="114" ssid="11">In the equation above, the s represents the Arabic word that is being linked from the English word ti.When ? is set to 1.0 there is no smoothing per formed and performance degrades to F=84.0 from the best system performance (F=87.8).</S>
			<S sid="115" ssid="12">When ? isset to 0, the model uses only the IBM Model 1 distri bution and the resulting aligner is similar to an HMM aligner with the transition shape discussed above and yields performance of F=73.2.</S>
	</SECTION>
	<SECTION title="Search Algorithm. " number="5">
			<S sid="116" ssid="1">A beam search algorithm is utilized with the English words consumed in sequence and the Arabic word positions serving as states in the search process.</S>
			<S sid="117" ssid="2">Inorder to take advantage of the transition model de scribed above, a large beam must be maintained.</S>
			<S sid="118" ssid="3">To see this, note that English words often repeat in a sentence and the models will tend to link the wordto all Arabic positions which have the same Ara bic content.</S>
			<S sid="119" ssid="4">In traditional algorithms, the Markov assumption is made and hypothesis are merged if they have the same history in the previous time step.</S>
			<S sid="120" ssid="5">However, here we maintain all hypotheses and merge only if the paths are same for 30 words which is the average sentence length.</S>
	</SECTION>
	<SECTION title="Experimental Data. " number="6">
			<S sid="121" ssid="1">We have word aligned a portion of the Arabic Tree bank (4300 sentences) and material from the LDC news sources (LDC, 2005) to obtain a total of 10.3K sentence pairs for training.</S>
			<S sid="122" ssid="2">As a test of alignment, we use the first 50 sentences of the MT03 Evaluationtest set which has 1313 Arabic words and 1528 En glish words 2.</S>
			<S sid="123" ssid="3">In terms of annotation guidelines, we use the following instructions: (a) Align determiners to their head nouns, (b) Alignments are done word by word unless the phrase is idiomatic in which case the entire phrase to phrase alignment was marked, (c) spontaneous words are marked as being part of a 2The test data is available by contacting the authors.</S>
			<S sid="124" ssid="4">92 1K 3K 5K 7K 9K 10.3K # of features 15510 32111 47962 63140 73650 80321 English % OOV 15.9 8.2 5.5 4.4 4.05 3.6 Arabic % OOV 31 19.6 15.6 13.2 10.8 10.3 F-measure 83.2 85.4 86.5 87.4 87.5 87.8 Table 2: Varying Training data size.</S>
			<S sid="125" ssid="5">phrase wherever possible but left unaligned if there is no evidence to link the word.</S>
			<S sid="126" ssid="6">In order to measure alignment performance, we use the standard AER measure (Och and Ney, 2000) but consider all links as sure.</S>
			<S sid="127" ssid="7">This measure is then related to the F-measure which can be defined in terms of precision and recall as Precision The number of correct word links over the total number of proposed links.</S>
			<S sid="128" ssid="8">Recall The number of correct word links over the total number of links in the reference.</S>
			<S sid="129" ssid="9">and the usual definition of the F-measure, F = 2PR(R+ P ) and define the alignment error as AER = 1 ? F .In this paper, we report our results in terms of F measure over aligned links.</S>
			<S sid="130" ssid="10">Note that links to theNULL state (unaligned English words) are not included in the F-measure.</S>
			<S sid="131" ssid="11">Systems are compared rel ative to the reduction in AER.</S>
			<S sid="132" ssid="12">6.1 Annotator Agreement.</S>
			<S sid="133" ssid="13">We measure intra/inter-annotator agreement on thetest set in order to determine the feasibility of hu man annotation of word links.</S>
			<S sid="134" ssid="14">These are shown in Table 1.</S>
			<S sid="135" ssid="15">In the table, the column for ?Annotator 1 Correction?</S>
			<S sid="136" ssid="16">is the first annotator correcting his own word alignments after a span of a year.</S>
			<S sid="137" ssid="17">After two weeks, the annotator (Annotator 1?)</S>
			<S sid="138" ssid="18">was given the same material with all the links removed and asked to realign and we see that there is more discrepancy in resulting alignments.</S>
			<S sid="139" ssid="19">The differences are largely on the head concept where determiners are attachedand the alignment of spontaneous words.</S>
			<S sid="140" ssid="20">The perfor mance with a second annotator is in the same range as the reannotation by a single annotator.</S>
	</SECTION>
	<SECTION title="Experiments. " number="7">
			<S sid="141" ssid="1">In order to evaluate the performance of the algo rithm, we investigate the effect due to: (a) increasing the training data size, (b) additional feature types, and (c) comparable algorithms.</S>
			<S sid="142" ssid="2">7.1 Training Data Size.</S>
			<S sid="143" ssid="3">We varied the training data size from 1K sentences to the complete set in Table 2.</S>
			<S sid="144" ssid="4">Each batch re-estimates the unknown word class by creating a vocabulary on the training set.</S>
			<S sid="145" ssid="5">The trend indicates a reasonable progression of performance and more data is required to determine the saturation point.</S>
			<S sid="146" ssid="6">7.2 Feature Types.</S>
			<S sid="147" ssid="7">The results obtained by different feature sets areshown in Table 3.</S>
			<S sid="148" ssid="8">Each feature type was added incre mentally (Add Feature column) to the line above to determine the effect of the individual feature typesand then removed incrementally from the full sys tem (Subtract Feature column) in order to see the final effect.</S>
			<S sid="149" ssid="9">The results indicate that lexical featuresare the most important type of feature; segmenta tion features further reduce the AER by 15.8%.</S>
			<S sid="150" ssid="10">The other features add small gains in performance which,although are not statistically significant for the align ment F-measure, are important in terms of feature extraction.</S>
			<S sid="151" ssid="11">Segmentation features discussed above result in both suffix and prefix features as well asstem features.</S>
			<S sid="152" ssid="12">In the Subtract column, for the seg mentation feature, only the suffix and prefix features were removed.</S>
			<S sid="153" ssid="13">This result indicates that most of thealignment improvement from the segmentation fea ture comes in the form of new lexical features to link Arabic stems and English words.</S>
			<S sid="154" ssid="14">7.3 Comparison to other alignment.</S>
			<S sid="155" ssid="15">algorithms In order to gauge the performance of the algorithmwith respect to other alignment strategies, we provide results using GIZA++ and an HMM Max Poste rior Algorithm (Ge, 2004).</S>
			<S sid="156" ssid="16">These algorithms, as well as the Model 1 smoothing for the MaxEnt aligner, are all trained on a corpus of 500K sentence pairsfrom the UN parallel corpus and the LDC news cor pora released for 2005 (LDC, 2005).</S>
			<S sid="157" ssid="17">Note that these algorithms are unsupervised by design but we utilizethem to have a baseline for comparing the perfor mance of this supervised approach.</S>
			<S sid="158" ssid="18">7.3.1 HMM Max Posterior AlignerThe maximum-posterior word alignments are obtained by finding the link configuration that maxi 93 System # of Add Subtract feats Feature Feature Word pairs 50070 85.03 76.3 Spelling 4 85.11 87.7 Segmentation 70 87.39 87.5(*) WordNet 13789 87.54 87.5 Dynamic-Words 1952 87.80 87.1 Dynamic-Segmentation 42 87.84 87.8 Table 3: Alignment performance in terms of the feature types utilized.</S>
			<S sid="159" ssid="19">F-Measure GIZA++ 79.5 HMM 76.3 MaxEnt 87.8 Table 4: Alignment performance mizes the posterior state probability.</S>
			<S sid="160" ssid="20">In contrast, in performing a Viterbi alignment, we compute the best state sequence given the observation.</S>
			<S sid="161" ssid="21">The maximum posterior computes the best state one at a time and iterates over all possible combinations.</S>
			<S sid="162" ssid="22">Once we find the maximum in the posterior probability matrix,we also know the corresponding state and observa tion which is nothing but the word pair (sj , ti).</S>
			<S sid="163" ssid="23">We will then align the pair and continue to find the next posterior maximum and align the resulting pair.</S>
			<S sid="164" ssid="24">At each iteration of the process, a word pair is aligned.</S>
			<S sid="165" ssid="25">The process is repeated until either every word in one (or both) language is aligned or no more maximum can be found, whichever happens first.</S>
			<S sid="166" ssid="26">7.3.2 GIZA Alignment In order to contrast our algorithm, we ranGIZA++ in the standard configuration which im plies 5 iterations of IBM Model 1, HMM, Model 3 and Model 4.</S>
			<S sid="167" ssid="27">All parameters are left to their default values.</S>
			<S sid="168" ssid="28">The results using the three different aligners is shown in Table 4.</S>
			<S sid="169" ssid="29">The reduction in AER over theGIZA++ system is 40.5% and over the HMM sys tem is 48.5%.</S>
			<S sid="170" ssid="30">The Wilcoxon signed-rank test yieldsa probability of 0.39 for rejecting the GIZA++ align ment over the HMM alignment, whereas the MaxEnt algorithm should be rejected with a probability of1.7e-6 over the HMM algorithm and similarly MaxEnt should be rejected with a probability of 0.9e 6 over the GIZA++ algorithm.</S>
			<S sid="171" ssid="31">These significance tests indicate that the MaxEnt algorithm presented above is significantly better than either GIZA++ or HMM.</S>
			<S sid="172" ssid="32">Figure 2: An alignment showing a split link from an Arabic word.</S>
	</SECTION>
	<SECTION title="Phrase Extraction. " number="8">
			<S sid="173" ssid="1">Once an alignment is obtained, phrases which sat isfy the inverse projection constraint are extracted(although earlier this constraint was called consis tent alignments (Och et al, 1999)).</S>
			<S sid="174" ssid="2">This constraint enforces that a sequence of source words align to a sequence of target words as defined by the lowest and highest target index, and when the target words are projected back to the source language through the alignment, the original source sequence is retrieved.</S>
			<S sid="175" ssid="3">Examination of the hand alignment training datashowed that this criteria is often violated for Arabic and English.</S>
			<S sid="176" ssid="4">Prepositional phrases with adjectives often require a split?</S>
			<S sid="177" ssid="5">for example, the align ment shown in Figure 2 has ?of its relations?</S>
			<S sid="178" ssid="6">aligned to a word in Arabic and ?tense?</S>
			<S sid="179" ssid="7">aligned to the next word.</S>
			<S sid="180" ssid="8">The inverse projection constraint fails in thiscase, and in the experiments below, we relax this con straint and generate features for single source words as long as the target phrase has a gap less than 2English words.</S>
			<S sid="181" ssid="9">This relaxation allows a pair of ad jectives to modify the head noun.</S>
			<S sid="182" ssid="10">In future work we explore the use of features with variables to be filled at decode time.</S>
	</SECTION>
	<SECTION title="Translation Experiments. " number="9">
			<S sid="183" ssid="1">The experiments in machine translation are carriedout on a phrase based decoder similar to the one de 94 MT03 MT04 MT05 GIZA++ 0.454 ? ?</S>
			<S sid="184" ssid="2">HMM 0.459 0.419 0.456 MaxEnt 0.468 0.433 0.451 Combined 0.479 0.437 0.465 Significance 0.017 0.020 ? Table 5: Machine Translation Performance using the NIST 2005 Bleu scorerscribed in (Tillmann and Ney, 2003).</S>
			<S sid="185" ssid="3">In order to con trast the performance of the extracted features, we compare the translation performance to (a) a system built from alignments proposed by an HMM Max Posterior Aligner, and (b) a system built from GIZAalignments.</S>
			<S sid="186" ssid="4">All other parameters of the decoder re main constant and only the feature set is changed for these experiments.</S>
			<S sid="187" ssid="5">As training data, we use the UN parallel corpus and the LDC news corpora released in 2005.</S>
			<S sid="188" ssid="6">Comparison should therefore be only madeacross systems reported here and not to earlier eval uations or other systems.</S>
			<S sid="189" ssid="7">The results are shown in Table 5.</S>
			<S sid="190" ssid="8">Combination of the phrasal features from theHMM and MaxEnt alignments results in the ?Combined?</S>
			<S sid="191" ssid="9">system.</S>
			<S sid="192" ssid="10">The Combined system performs bet ter in all cases; in MT03 and MT04 the MaxEntderived features perform better than the HMM sys tem.</S>
			<S sid="193" ssid="11">In MT05, there is a slight degradation which isnot significant and the combination system still re sults in an improvement over either system.</S>
			<S sid="194" ssid="12">Since the MaxEnt aligner has access to a unique resource,every attempt was made to make that resource avail able to the other systems.</S>
			<S sid="195" ssid="13">Although GIZA++ and HMM can not directly utilize word aligned data, thetraining data for MaxEnt was converted to paral lel sentences where each sentence has only the pair of linked words.</S>
			<S sid="196" ssid="14">The resulting numbers make both HMM and GIZA much closer in performance to theMaxEnt aligner but the results are better for com paring alignment methods.</S>
	</SECTION>
	<SECTION title="Error Analysis and Discussion. " number="10">
			<S sid="197" ssid="1">The alignment errors made by the system can be attributed to ? English words that require multi-word Arabic states, for example (a) dates which are writtenin Arabic in more than one form ?kAnwn Al vAny / ynAyr?</S>
			<S sid="198" ssid="2">for ?january?, and (b) compound words like ?rAm Allh?</S>
			<S sid="199" ssid="3">in English is ?Ramallah?.</S>
			<S sid="200" ssid="4">Rare translation of a common Arabic word as well as a common English word used as the translation for a rare Arabic word.</S>
			<S sid="201" ssid="5">Parallel corpora mismatch: training material for translation is processed at a document level and yet systems often operate at a sentence level.Human translators often use pronouns for earlier mentioned names although in the source lan guage the name is repeated.</S>
			<S sid="202" ssid="6">Information whichis sometimes repeated in the source in an ear lier sentence is dropped in future sentences ofthe document.</S>
			<S sid="203" ssid="7">Document level features are re quired to allow the system to have information to leave these words unaligned.</S>
			<S sid="204" ssid="8">Figure 3 shows a human alignment on the left and a machine output on the right.</S>
			<S sid="205" ssid="9">The columns next to the words indicate whether the alignments are ?good?</S>
			<S sid="206" ssid="10">or ?extra?</S>
			<S sid="207" ssid="11">which indicates that these words are aligned to the special NULL state.</S>
			<S sid="208" ssid="12">There are two examples of multi-word Arabic states shown: (a) for ?january?, and (b) the English word ?agenda?.</S>
			<S sid="209" ssid="13">The system aligns ?the?</S>
			<S sid="210" ssid="14">before committee and it seemsin this case its an annotation error.</S>
			<S sid="211" ssid="15">In this exam ple the Arabic words lnAHyp, AltnZym, wAlAEdAdand Allwjsty are all unknown words in the vocabu lary yet the system managed to link 3 out 4 words correctly.While significant gains have been made in align ment performance, these gains have not directly translated to machine translation improvements.</S>
			<S sid="212" ssid="16">In fact, although the GIZA system is better than the HMM system at alignment, the machine translationresult on MT03 indicates a slight degradation (al though it is not statistically significant).</S>
			<S sid="213" ssid="17">The prime reason for this is that features extracted from the alignments are aggregated over the training corpusand this process helps good alignments to have significantly better counts than errors in alignment.</S>
			<S sid="214" ssid="18">Align ing rare words correctly should help performance but since their count is low it is not reflected in bleu scores.</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number="11">
			<S sid="215" ssid="1">This paper presented a word aligner trained on anno tated data.</S>
			<S sid="216" ssid="2">While the performance of the aligner isshown to be significantly better than other unsuper vised algorithms, the utility of these alignments in machine translation is still an open subject although gains are shown in two of the test sets.</S>
			<S sid="217" ssid="3">Since featuresare extracted from a parallel corpus, most of the in formation relating to the specific sentence alignment is lost in the aggregation of features across sentences.</S>
			<S sid="218" ssid="4">Improvements in capturing sentence context could allow the machine translation system to use a rare but correct link appropriately.</S>
			<S sid="219" ssid="5">Another significant result is that a small amount (5K sentences) of word-aligned data is sufficient for this algorithm since a provision is made to handle 95 Figure 3: An example sentence with human output on the left and system output on the right.</S>
			<S sid="220" ssid="6">unknown words appropriately.</S>
	</SECTION>
	<SECTION title="Acknowledgements. " number="12">
			<S sid="221" ssid="1">This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No.</S>
			<S sid="222" ssid="2">N66001-99-2-8916.</S>
			<S sid="223" ssid="3">The views and findings contained in this material are those of the authors and do not necessarily reflect the position or policy of the U.S. government and no official endorsement should be inferred.</S>
			<S sid="224" ssid="4">This paper owes much to the collaboration of the Statistical MT group at IBM.</S>
	</SECTION>
</PAPER>
