<PAPER>
  <S sid="0">The Necessity Of Parsing For Predicate Argument Recognition</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.</S>
    <S sid="2" ssid="2">Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text.</S>
    <S sid="3" ssid="3">In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter &amp;quot;chunked&amp;quot; representation of the input can be as effective for the purposes of semantic role identification.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">Over the past decade, most work in the field of information extraction has shifted from complex rule-based, systems designed to handle a wide variety of semantic phenomena including quantification, anaphora, aspect and modality (e.g.</S>
    <S sid="5" ssid="2">Alshawi (1992)), to simpler finite-state or statistical systems such as Hobbs et al. (1997) and Miller et al.</S>
    <S sid="6" ssid="3">(1998).</S>
    <S sid="7" ssid="4">Much of the evaluation of these systems has been conducted on extracting relations for specific semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences.</S>
    <S sid="8" ssid="5">Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates.</S>
    <S sid="9" ssid="6">The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic realization of arguments of the predicates of the general English lexicon by annotating a corpus with semantic roles.</S>
    <S sid="10" ssid="7">Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization.</S>
    <S sid="11" ssid="8">In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem.</S>
    <S sid="12" ssid="9">We measure the effect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction.</S>
    <S sid="13" ssid="10">Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles.</S>
    <S sid="14" ssid="11">The system first passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features.</S>
    <S sid="15" ssid="12">Both training and test sentences were automatically parsed, as no hand-annotated parse trees were available for the corpus.</S>
    <S sid="16" ssid="13">While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this effect.</S>
    <S sid="17" ssid="14">Of the systems evaluated for the Message Understanding Conference task, Miller et al. (1998) made use of an integrated syntactic and semantic model producing a full parse tree, and achieved results comparable to other systems that did not make use of a complete parse.</S>
    <S sid="18" ssid="15">As in the FrameNet case, the parser was not trained on the corpus for which semantic annotations were available, and the effect of better, or even perfect, parses could not be measured.</S>
    <S sid="19" ssid="16">One of the differences between the two semantic annotation projects is that the sentences chosen for annotation for Propbank are from the same Wall Street Journal corpus chosen for annotation for the original Penn Treebank project, and thus hand-checked syntactic parse trees are available for the entire dataset.</S>
    <S sid="20" ssid="17">In this paper, we compare the performance of a system based on goldstandard parses with one using automatically generated parser output.</S>
    <S sid="21" ssid="18">We also examine whether it is possible that the additional information contained in a full parse tree is negated by the errors present in automatic parser output, by testing a role-labeling system based on a flat or &amp;quot;chunked&amp;quot; representation of the input.</S>
  </SECTION>
  <SECTION title="2 The Data" number="2">
    <S sid="22" ssid="1">The results in this paper are primarily derived from the Propbank corpus, and will be compared to earlier results from the FrameNet corpus.</S>
    <S sid="23" ssid="2">Before proceeding to the experiments, this section will briefly describe the similarities and differences between the two sets of data.</S>
    <S sid="24" ssid="3">While the goals of the two projects are similar in many respects, their methodologies are quite different.</S>
    <S sid="25" ssid="4">FrameNet is focused on semantic frames, which are defined as schematic representation of situations involving various participants, props, and other conceptual roles (Fillmore, 1976).</S>
    <S sid="26" ssid="5">The project methodology has proceeded on a frameby-frame basis, that is by first choosing a semantic frame, defining the frame and its participants or frame elements, and listing the various lexical predicates which invoke the frame, and then finding example sentences of each predicate in the corpus (the British National Corpus was used) and annotating each frame element.</S>
    <S sid="27" ssid="6">The example sentences were chosen primarily for coverage of all the syntactic realizations of the frame elements, and simple examples of these realizations were preferred over those involving complex syntactic structure not immediate relevant to the lexical predicate itself.</S>
    <S sid="28" ssid="7">From the perspective of an automatic classification system, the overrepresentation of rare syntactic realizations may cause the system to perform more poorly than it might on more statistically representative data.</S>
    <S sid="29" ssid="8">On the other hand, the exclusion of complex examples may make the task artificially easy.</S>
    <S sid="30" ssid="9">Only sentences where the lexical predicate was used &amp;quot;in frame&amp;quot; were annotated.</S>
    <S sid="31" ssid="10">A word with multiple distinct senses would generally be analyzed as belonging to different frames in each sense, but may only be found in the FrameNet corpus in the sense for which a frame has been defined.</S>
    <S sid="32" ssid="11">It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure.</S>
    <S sid="33" ssid="12">A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the ramifications for automatic classification are discussed more thoroughly in (Gildea and Jurafsky, 2002).</S>
    <S sid="34" ssid="13">The philosophy of the Propbank project can be likened to FrameNet without frames.</S>
    <S sid="35" ssid="14">While the semantic roles of FrameNet are defined at the level of the frame, in Propbank, roles are defined on a per-predicate basis.</S>
    <S sid="36" ssid="15">The core arguments of each predicate are simply numbered, while remaining arguments are given labels such as &amp;quot;temporal&amp;quot; or &amp;quot;locative&amp;quot;.</S>
    <S sid="37" ssid="16">While the two types of label names are reminiscent of the traditional argument/adjunct distinction, this is primarily as a convenience in defining roles, and no claims are intended as to optionality or other traditional argument/adjunct tests.</S>
    <S sid="38" ssid="17">To date, Propbank has addressed only verbs, where FrameNet includes nouns and adjectives.</S>
    <S sid="39" ssid="18">Propbank's annotation process has proceeded from the most to least common verbs, and all examples of each verb from the corpus are annotated.</S>
    <S sid="40" ssid="19">Thus, the data for each predicate are statistically representative of the corpus, as are the frequencies of the predicates themselves.</S>
    <S sid="41" ssid="20">Annotation takes place with reference to the Penn Treebank trees not only are annotators shown the trees when analyzing a sentence, they are constrained to assign the semantic labels to portions of the sentence corresponding to nodes in the tree.</S>
    <S sid="42" ssid="21">Propbank annotators tag all examples of a given verb, regardless of word sense.</S>
    <S sid="43" ssid="22">The tagging guidelines for a verb may contain many &amp;quot;rolesets&amp;quot;, corresponding to word sense at a relatively coarsegrained level.</S>
    <S sid="44" ssid="23">The need for multiple rolesets is determined by the roles themselves, that is, uses of the verb with different arguments are given separate rolesets.</S>
    <S sid="45" ssid="24">However, the preliminary version of the data used in the experiments below are not tagged for word sense, or for the roleset used.</S>
    <S sid="46" ssid="25">Sense tagging is planned for a second pass through the data.</S>
    <S sid="47" ssid="26">In many cases the roleset can be determined from the argument annotations themselves.</S>
    <S sid="48" ssid="27">However, we did not make any attempt to distinguish sense in our experiments, and simply attempted to predict argument labels based on the identity of the lexical predicate.</S>
  </SECTION>
  <SECTION title="3 The Experiments" number="3">
    <S sid="49" ssid="1">In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997).</S>
    <S sid="50" ssid="2">We will briefly review their probability model before adapting the system to handle unparsed data.</S>
    <S sid="51" ssid="3">Probabilities of a parse constituent belonging to a given semantic role were calculated from the following features: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S).</S>
    <S sid="52" ssid="4">Phrase types were derived automatically from parse trees generated by the parser, as shown in Figure 1.</S>
    <S sid="53" ssid="5">The parse constituent spanning each set of words annotated as an argument was found, and the constituent's nonterminal label was taken as the phrase type.</S>
    <S sid="54" ssid="6">As an example of how this feature is useful, in communication frames, the SpEAKER is likely to appear as a noun phrase, Topic as a prepositional phrase or noun phrase, and MEDiUM as a prepositional phrase, as in: &amp;quot;We talked about the proposal over the phone.&amp;quot; When no parse constituent was found with boundaries matching those of an argument during testing, the largest constituent beginning at the argument's left boundary and lying entirely within the element was used to calculate the features.</S>
    <S sid="55" ssid="7">Parse Tree Path: This feature is designed to capture the syntactic relation of a constituent to the predicate.</S>
    <S sid="56" ssid="8">It is defined as the path from the predicate through the parse tree to the constituent in question, represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree, as shown in Figure 2.</S>
    <S sid="57" ssid="9">Although the path is composed as a string of symbols, our systems will treat the string as an atomic value.</S>
    <S sid="58" ssid="10">The path includes, as the first element of the string, the part of speech of the predicate, and, as the last element, the phrase type or syntactic category of the sentence constituent marked as an argument.</S>
    <S sid="59" ssid="11">Position: This feature simply indicates whether the constituent to be labeled occurs before or after the predicate defining the semantic frame.</S>
    <S sid="60" ssid="12">This feature is highly correlated with grammatical function, since subjects will generally appear before a verb, and objects after.</S>
    <S sid="61" ssid="13">This feature may overcome the shortcomings of reading grammatical function from the parse tree, as well as errors in the parser output.</S>
    <S sid="62" ssid="14">He ate some pancakes Figure 2: In this example, the path from the predicate ate to the argument He can be represented as VBfVPfS&#65533;NP, with f indicating upward movement in the parse tree and &#65533; downward movement.</S>
    <S sid="63" ssid="15">Voice: The distinction between active and passive verbs plays an important role in the connection between semantic role and grammatical function, since direct objects of active verbs correspond to subjects of passive verbs.</S>
    <S sid="64" ssid="16">From the parser output, verbs were classified as active or passive by building a set of 10 passive-identifying patterns.</S>
    <S sid="65" ssid="17">Each of the patterns requires both a passive auxiliary (some form of &amp;quot;to be&amp;quot; or &amp;quot;to get&amp;quot;) and a past participle.</S>
    <S sid="66" ssid="18">Head Word: Lexical dependencies provide important information in labeling semantic roles, as one might expect from their use in statistical models for parsing.</S>
    <S sid="67" ssid="19">Since the parser used assigns each constituent a head word as an integral part of the parsing model, the head words of the constituents can be read from the parser output.</S>
    <S sid="68" ssid="20">For example, in a communication frame, noun phrases headed by &amp;quot;Bill&amp;quot;, &amp;quot;brother&amp;quot;, or &amp;quot;he&amp;quot; are more likely to be the SpEAKER, while those headed by &amp;quot;proposal&amp;quot;, &amp;quot;story&amp;quot;, or &amp;quot;question&amp;quot; are more likely to be the Topic.</S>
    <S sid="69" ssid="21">To predict argument roles in new data, we wish to estimate the probability of each role given these five features and the predicate p: P(rlpt, path, position, voice, hw, p).</S>
    <S sid="70" ssid="22">Due to the sparsity of the data, it is not possible to estimate this probability from the counts in the training.</S>
    <S sid="71" ssid="23">Instead, we estimate probabilities from various subsets of the features, and interpolate a linear combination of the resulting distributions.</S>
    <S sid="72" ssid="24">The interpolation is performed over the most specific distributions for which data are available, which can be thought of as choosing the topmost distributions available from a backoff lattice, shown in We applied the same system, using the same features to a preliminary release of the Propbank data.</S>
    <S sid="73" ssid="25">The dataset used contained annotations for 26,138 predicate-argument structures containing 65,364 individual arguments and containing examples from 1,527 lexical predicates (types).</S>
    <S sid="74" ssid="26">In order to provide results comparable with the statistical parsing literature, annotations from Section 23 of the Treebank were used as the test set; all other sections were included in the training set.</S>
    <S sid="75" ssid="27">The system was tested under two conditions, one in which it is given the constituents which are arguments to the predicate and merely has to predict the correct role, and one in which it has to both find the arguments in the sentence and label them correctly.</S>
    <S sid="76" ssid="28">Results are shown in Tables 1 and 2.</S>
    <S sid="77" ssid="29">Although results for Propbank are lower than for FrameNet, this appears to be primarily due to the smaller number of training examples for each predicate, rather than the difference in annotation style between the two corpora.</S>
    <S sid="78" ssid="30">The FrameNet data contained at least ten examples from each predicate, while 17% of the Propbank data had fewer than ten training examples.</S>
    <S sid="79" ssid="31">Removing these examples from the test set gives 84.1% accuracy with gold-standard parses and 80.5% accuracy with automatic parses.</S>
    <S sid="80" ssid="32">As our path feature is a somewhat unusual way of looking at parse trees, its behavior in the system warrants a closer look.</S>
    <S sid="81" ssid="33">The path feature is most useful as a way of finding arguments in the unknown boundary condition.</S>
    <S sid="82" ssid="34">Removing the path feature from the known-boundary system results in only a small degradation in performance, from 82.3% to 81.7%.</S>
    <S sid="83" ssid="35">One reason for the relatively small impact may be sparseness of the feature &#65533; 7% of paths in the test set are unseen in training data.</S>
    <S sid="84" ssid="36">The most common values of the feature are shown in Table 3, where the first two rows correspond to standard subject and object positions.</S>
    <S sid="85" ssid="37">One reason for sparsity is seen in the third row: in the Treebank, the adjunction of an adverbial phrase or modal verb can cause an additional VP node to appear in our path feature.</S>
    <S sid="86" ssid="38">We tried two variations of the path feature to address this problem.</S>
    <S sid="87" ssid="39">The first collapses sequences of nodes with the same label, for example combining rows 2 and 3 of Table 3.</S>
    <S sid="88" ssid="40">The second variation uses only two values for the feature: NP under S (subject position), and NP under VP (object position).</S>
    <S sid="89" ssid="41">Neither variation improved performance in the known boundary condition.</S>
    <S sid="90" ssid="42">As a gauge of how closely the Propbank argument labels correspond to the path feature overall, we note that by always assigning the most common role for each path, for example always assigning ARG0 to the subject position, and using no other features, we obtain the correct role 69.4% of the time, vs. 82.3% for the complete system.</S>
  </SECTION>
  <SECTION title="4 Is Parsing Necessary?" number="4">
    <S sid="91" ssid="1">Many recent information extraction systems for limited domains have relied on finite-state systems that do not build a full parse tree for the sentence being analyzed.</S>
    <S sid="92" ssid="2">Among such systems, (Hobbs et al., 1997) built finite-state recognizers for various entities, which were then cascaded to form recognizers for higher-level relations, while (Ray and Craven, 2001) used low-level &amp;quot;chunks&amp;quot; from a general-purpose syntactic analyzer as observations in a trained Hidden Markov Model.</S>
    <S sid="93" ssid="3">Such an approach has a large advantage in speed, as the extensive search of modern statistical parsers is avoided.</S>
    <S sid="94" ssid="4">It is also possible that this approach may be more robust to error than parsers.</S>
    <S sid="95" ssid="5">Although we expect the attachment decisions made by a parser to be relevant to determining whether a constituent of a sentence is an argument of a particular predicate, and what its relation to the predicate is, those decisions may be so frequently incorrect that a much simpler system can do just as well.</S>
    <S sid="96" ssid="6">In this section we test this hypothesis by comparing a system which is given only a flat, &amp;quot;chunked&amp;quot; representation of the input sentence to the parse-tree-based systems described above.</S>
    <S sid="97" ssid="7">In this representation, base-level constituent boundaries and labels are present, but there are no dependencies between constituents, as shown by the following sample sentence: (3) [NP Big investment banks] [V P refused to step] [ADVP up] [PP to] [NP the plate] [V P to support] [NP the beleaguered floor traders] [PP by] [V P buying] [NP big blocks] [PP of] [NP stock] , [NP traders] [V P say] .</S>
    <S sid="98" ssid="8">Our chunks were derived from the Treebank trees using the conversion described by Tjong Kim Sang and Buchholz (2000).</S>
    <S sid="99" ssid="9">Thus, the experiments were carried out using &amp;quot;goldstandard&amp;quot; rather than automatically derived chunk boundaries, which we believe will provide an upper bound on the performance of a chunkbased system.</S>
    <S sid="100" ssid="10">The information provided by the parse tree can be decomposed into three pieces: the constituent boundaries, the grammatical relationship between predicate and argument, expressed by our path feature, and the head word of each candidate constituent.</S>
    <S sid="101" ssid="11">We will examine the contribution of each of these information sources, beginning with the problem of assigning the correct role in the case where the boundaries of the arguments in the sentence are known, and then turning to the problem of finding arguments in the sentence.</S>
    <S sid="102" ssid="12">When the argument boundaries are known, the grammatical relationship of the the constituent to the predicate turns out to be of little value.</S>
    <S sid="103" ssid="13">Removing the path feature from the system described above results in only a small degradation in performance, from 82.3% to 81.7%.</S>
    <S sid="104" ssid="14">While the path feature serves to distinguish subjects from objects, the combination of the constituent position before or after the predicate and the active/passive voice feature serves the same purpose.</S>
    <S sid="105" ssid="15">However, this result still makes use of the parser output for finding the constituent's head word.</S>
    <S sid="106" ssid="16">We implemented a simple algorithm to guess the argument's head word from the chunked output: if the argument begins at a chunk boundary, taking the last word of the chunk, and in all other cases, taking the first word of the argument.</S>
    <S sid="107" ssid="17">This heuristic matches the head word read from the parse tree 77% of the the time, as it correctly identifies the final word of simple noun phrases as the head, the preposition as the head of prepositional phrases, and the complementizer as the head of sentential complements.</S>
    <S sid="108" ssid="18">Using this process for determining head words, the system drops to 77.0% accuracy, indicating that identifying the relevant head word from semantic role prediction is in itself an important function of the parser.</S>
    <S sid="109" ssid="19">This chunker-based result is only slightly lower than the 79.2% obtained using automatic parses in the known boundary condition.</S>
    <S sid="110" ssid="20">These results for the known boundary condition are summarized in Table 4. gold parse auto parse gold parse chunks Table 4: Summary of results for known boundary condition We might expect the information provided by the parser to be more important in identifying the arguments in the sentence than in assigning them the correct role.</S>
    <S sid="111" ssid="21">While it is easy to guess whether a noun phrase is a subject or object given only its position relative to the predicate, identifying complex noun phrases and determining whether they are arguments of a verb may be more difficult without the attachment information provided by the parser.</S>
    <S sid="112" ssid="22">To test this, we implemented a system in which the argument labels were assigned to chunks, with the path feature used by the parse-tree-based system replaced by a number expressing the distance in chunks to the left or right of the predicate.</S>
    <S sid="113" ssid="23">Of the 3990 arguments in our test set, only 39.8% correspond to a single chunk in the flattened sentence representation, giving an upper bound to the performance of this system.</S>
    <S sid="114" ssid="24">In particular, sentential complements (which comprise 11% of the data) and prepositional phrases (which comprise 10%) always correspond to more than one chunk, and therefore cannot be correctly labeled by our system which assigns roles to single chunks.</S>
    <S sid="115" ssid="25">In fact, this system achieves 27.6% precision and 22.0% recall.</S>
    <S sid="116" ssid="26">In order to see how much of the performance degradation is caused by the difficulty of finding exact argument boundaries in the chunked representation, we can relax the scoring criteria to count as correct all cases where the system correctly identifies the first chunk belonging to an argument.</S>
    <S sid="117" ssid="27">For example, if the system assigns the correct label to the preposition beginning a prepositional phrase, the argument will be counted as correct, even though the system does not find the argument's righthand boundary.</S>
    <S sid="118" ssid="28">With this scoring regime, the chunk-based system performs at 49.5% precision and 35.1% recall, still significantly lower than the 57.7% precision/50.0% recall for exact matches using automatically generated parses.</S>
    <S sid="119" ssid="29">Results for the unknown boundary condition are summarized in Table 5. gold parse auto parse chunk chunk, relaxed scoring Table 5: Summary of results for unknown boundary condition As an example for comparing the behavior of the tree-based and chunk-based systems, consider the following sentence, with human annotations showing the arguments of the predicate support: (4) [ARG0 Big investment banks] refused to step up to the plate to support [ARG1 the beleaguered floor traders] [MNR by buying big blocks of stock] , traders say .</S>
    <S sid="120" ssid="30">Our tree-based system assigned the following analysis: floor traders] [MNR by buying big blocks of stock] , traders say .</S>
    <S sid="121" ssid="31">In this case, the system failed to find the predicate's ARG0 relation, because it is syntactically distant from the verb support.</S>
    <S sid="122" ssid="32">The original Treebank syntactic tree contains a trace which would allow one to recover this relation, co-indexing the empty subject position of support with the noun phrase &amp;quot;Big investment banks&amp;quot;.</S>
    <S sid="123" ssid="33">However, our automatic parser output does not include such traces, nor does our system make use of them.</S>
    <S sid="124" ssid="34">The chunk-based system assigns the following argument labels: (6) Big investment banks refused to step up to [ARG0 the plate] to support [ARG1 the beleaguered floor traders] by buying big blocks of stock , traders say .</S>
    <S sid="125" ssid="35">Here, as before, the true ARG0 relation is not found, and it would be difficult to imagine identifying it without building a complete syntactic parse of the sentence.</S>
    <S sid="126" ssid="36">But now, unlike in the tree-based output, the ARG0 label is mistakenly attached to a noun phrase immediately before the predicate.</S>
    <S sid="127" ssid="37">The ARG1 relation in direct object position is fairly easily identifiable in the chunked representation as a noun phrase directly following the verb.</S>
    <S sid="128" ssid="38">The prepositional phrase expressing the Manner relation, however, is not identified by the chunk-based system.</S>
    <S sid="129" ssid="39">The tree-based system's path feature for this constituent is VBTVP&#65533;PP, which identifies the prepositional phrase as attaching to the verb, and increases its probability of being assigned an argument label.</S>
    <S sid="130" ssid="40">The chunkbased system sees this as a prepositional phrase appearing as the second chunk after the predicate.</S>
    <S sid="131" ssid="41">Although this may be a typical position for the Manner relation, the fact that the preposition attaches to the predicate rather than to its direct object is not represented.</S>
    <S sid="132" ssid="42">In interpreting these results, it is important to keep in mind the differences between this task and other information extraction datasets.</S>
    <S sid="133" ssid="43">In comparison to the domain-specific relations evaluated by the Message Understanding Conference (MUC) tasks, we have a wider variety of relations but fewer training instances for each.</S>
    <S sid="134" ssid="44">The relations may themselves be less susceptible to finite state methods.</S>
    <S sid="135" ssid="45">For example, a named-entity system which indentifies corporation names can go a long way towards finding the &amp;quot;employment&amp;quot; relation of MUC, and similarly systems for tagging genes and proteins help a great deal for relations in the biomedical domain.</S>
    <S sid="136" ssid="46">Both Propbank and FrameNet tend to include longer arguments with internal syntactic structure, making parsing decisions more important in finding argument boundaries.</S>
    <S sid="137" ssid="47">They also involve abstract relations, with a wide variety of possible fillers for each role.</S>
  </SECTION>
  <SECTION title="5 Conclusion" number="5">
    <S sid="138" ssid="1">Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk's other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al. (1997), McCallum et al.</S>
    <S sid="139" ssid="2">(2000) and Lafferty et al. (2001).</S>
    <S sid="140" ssid="3">While a more elaborate finite-state system might do better, it is possible that additional features would not be helpful given the small amount of data for each predicate.</S>
    <S sid="141" ssid="4">By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a flat representation of the data.</S>
    <S sid="142" ssid="5">We feel that our results show that statistical parsers, although computationally expensive, do a good job of providing relevant information for semantic interpretation.</S>
    <S sid="143" ssid="6">Not only the constituent structure but also head word information, produced as a side product, are important features.</S>
    <S sid="144" ssid="7">Parsers, however, still have a long way to go.</S>
    <S sid="145" ssid="8">Our results using hand-annotated parse trees show that improvements in parsing should translate directly into better semantic interpretations.</S>
    <S sid="146" ssid="9">Acknowledgments This work was undertaken with funding from the Institute for Research in Cognitive Science at the University of Pennsylvania and from the Propbank project, DoD Grant MDA904-00C-2136.</S>
  </SECTION>
</PAPER>
