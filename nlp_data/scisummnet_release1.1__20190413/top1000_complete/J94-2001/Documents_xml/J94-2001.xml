<PAPER>
  <S sid="0">Tagging English Text With A Probabilistic Model</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence.</S>
    <S sid="2" ssid="2">The main novelty of these experiments is the use of untagged text in the training of the model.</S>
    <S sid="3" ssid="3">We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided.</S>
    <S sid="4" ssid="4">Two approaches in particular are compared and combined: &#8226; using text that has been tagged by hand and computing relative frequency counts, &#8226; using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.</S>
    <S sid="5" ssid="5">Experiments show that the best training is obtained by using as much tagged text as possible.</S>
    <S sid="6" ssid="6">They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.</S>
    <S sid="7" ssid="7">In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="8" ssid="1">Institut EURECOM In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence.</S>
    <S sid="9" ssid="2">The main novelty of these experiments is the use of untagged text in the training of the model.</S>
    <S sid="10" ssid="3">We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided.</S>
    <S sid="11" ssid="4">Two approaches in particular are compared and combined: Experiments show that the best training is obtained by using as much tagged text as possible.</S>
    <S sid="12" ssid="5">They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.</S>
    <S sid="13" ssid="6">In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="14" ssid="1">A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence.</S>
    <S sid="15" ssid="2">Two main approaches have generally been considered: Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992).</S>
    <S sid="16" ssid="3">More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989).</S>
    <S sid="17" ssid="4">Through these different approaches, some common points have emerged: These kinds of considerations fit nicely inside a probabilistic formulation of the problem (Beale 1985; Garside and Leech 1985), which offers the following advantages: In this paper we present a particular probabilistic model, the triclass model, and results from experiments involving different ways to estimate its parameters, with the intention of maximizing the ability of the model to tag text accurately.</S>
    <S sid="18" ssid="5">In particular, we are interested in a way to make the best use of untagged text in the training of the model.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="19" ssid="1">We suppose that the user has defined a set of tags (attached to words).</S>
    <S sid="20" ssid="2">Consider a sentence W = w1w2 wn, and a sequence of tags T =-- tit2 tn, of the same length.</S>
    <S sid="21" ssid="3">We call the pair (W, T) an alignment.</S>
    <S sid="22" ssid="4">We say that word w, has been assigned the tag t, in this alignment.</S>
    <S sid="23" ssid="5">We assume that the tags have some linguistic meaning for the user, so that among all possible alignments for a sentence there is a single one that is correct from a grammatical point of view.</S>
    <S sid="24" ssid="6">A tagging procedure is a procedure 0 that selects a sequence of tags (and so defines an alignment) for each sentence.</S>
    <S sid="25" ssid="7">0:W--+T=0(W) There are (at least) two measures for the quality of a tagging procedure: In practice, performance at sentence level is generally lower than performance at word level, since all the words have to be tagged correctly for the sentence to be tagged correctly.</S>
    <S sid="26" ssid="8">The standard measure used in the literature is performance at word level, and this is the one considered here.</S>
  </SECTION>
  <SECTION title="3." number="4">
    <S sid="27" ssid="1">In the probabilistic formulation of the tagging problem we assume that the alignments are generated by a probabilistic model according to a probability distribution: p(W, T) In this case, depending on the criterion that we choose for evaluation, the optimal tagging procedure is as follows: We call this procedure Viterbi tagging.</S>
    <S sid="28" ssid="2">It is achieved using a dynamic programming scheme. where 0(W), is the tag assigned to word w, by the tagging procedure in the context of the sentence W. We call this procedure Maximum It is interesting to note that the most commonly used method is Viterbi tagging (see DeRose 1988; Church 1989) although it is not the optimal method for evaluation at word level.</S>
    <S sid="29" ssid="3">The reasons for this preference are presumably that: However, in our experiments, we will show that Viterbi and ML tagging result in very similar performance.</S>
    <S sid="30" ssid="4">Of course, the real tags have not been generated by a probabilistic model and, even if they had been, we would not be able to determine this model exactly because of practical limitations.</S>
    <S sid="31" ssid="5">Therefore the models that we construct will only be approximations of an ideal model that does not exist.</S>
    <S sid="32" ssid="6">It so happens that despite these assumptions and approximations, these models are still able to perform reasonably well.</S>
  </SECTION>
  <SECTION title="4." number="5">
    <S sid="33" ssid="1">We have the mathematical expression: The triclass (or tri-POS Perouault 19861, or tri-Ggram Kodogno et al. 19871, or HK) model is based on the following approximations: (the name HK model comes from the notation chosen for these probabilities).</S>
    <S sid="34" ssid="2">In order to define the model completely we have to specify the values of all h and k probabilities.</S>
    <S sid="35" ssid="3">If Nw is the size of the vocabulary and NT the number of different tags, then there are: The total number of free parameters is then: Note that this number grows only linearly with respect to the size of the vocabulary, which makes this model attractive for vocabularies of a very large size.</S>
    <S sid="36" ssid="4">The triclass model by itself allows any word to have any tag.</S>
    <S sid="37" ssid="5">However, if we have a dictionary that specifies the list of possible tags for each word, we can use this information to constrain the model: if t is not a valid tag for the word w, then we are sure that There are thus at most as many nonzero values for the k probabilities as there are possible pairs (word, tag) allowed in the dictionary.</S>
    <S sid="38" ssid="6">If we have some tagged text available we can compute the number of times N(w,t) a given word w appears with the tag t, and the number of times N(ti, t2, t3) the sequence (t1, t2, t3) appears in this text.</S>
    <S sid="39" ssid="7">We can then estimate the probabilities h and k by computing the relative frequencies of the corresponding events on this data: These estimates assign a probability of zero to any sequence of tags that did not occur in the training data.</S>
    <S sid="40" ssid="8">But such sequences may occur if we consider other texts.</S>
    <S sid="41" ssid="9">A probability of zero for a sequence creates problems because any alignment that contains this sequence will get a probability of zero.</S>
    <S sid="42" ssid="10">Therefore, it may happen that, for some sequences of words, all alignments get a probability of zero and the model becomes useless for such sentences.</S>
    <S sid="43" ssid="11">To avoid this, we interpolate these distributions with uniform distributions, i.e.</S>
    <S sid="44" ssid="12">:onsider the interpolated model defined by: where number of words that have the tag t The interpolation coefficient A is computed using the deleted interpolation algorithm (Jelinek and Mercer 1980) (it would also be possible to use two coefficients, one for the interpolation on h, one for the interpolation on k).</S>
    <S sid="45" ssid="13">The value of this coefficient is expected to increase if we increase the size of the training text, since the relative frequencies should be more reliable.</S>
    <S sid="46" ssid="14">This interpolation procedure is also called &amp;quot;smoothing.&amp;quot; Smoothing is performed as follows: It can be noted that more complicated interpolation schemes are possible.</S>
    <S sid="47" ssid="15">For example, different coefficients can be used depending on the count of (t1, t2), with the intuition that relative frequencies can be trusted more when this count is high.</S>
    <S sid="48" ssid="16">Another possibilitity is to interpolate also with models of different orders, such as hrf (t3/t2) or hrf (t3).</S>
    <S sid="49" ssid="17">Smoothing can also be achieved with procedures other than interpolation.</S>
    <S sid="50" ssid="18">One example is the &amp;quot;backing-off&amp;quot; strategy proposed by Katz (1987).</S>
    <S sid="51" ssid="19">Using a triclass model M it is possible to compute the probability of any sequence of words W according to this model: where the sum is taken over all possible alignments.</S>
    <S sid="52" ssid="20">The Maximum Likelihood (ML) training finds the model M that maximizes the probability of the training text: where the product is taken over all the sentences W in the training text.</S>
    <S sid="53" ssid="21">This is the problem of training a hidden Markov model (it is hidden because the sequence of tags is hidden).</S>
    <S sid="54" ssid="22">A well-known solution to this problem is the Forward-Backward (FB) or Baum&#8212;Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer 1983), which iteratively constructs a sequence of models that improve the probability of the training data.</S>
    <S sid="55" ssid="23">The advantage of this approach is that it does not require any tagging of the text, but makes the assumption that the correct model is the one in which tags are used to best predict the word sequence.</S>
  </SECTION>
  <SECTION title="6." number="6">
    <S sid="56" ssid="1">The Viterbi algorithm is easily implemented using a dynamic programming scheme (Bellman 1957).</S>
    <S sid="57" ssid="2">The Maximum Likelihood algorithm appears more complex at first glance, because it involves computing the sum of the probabilities of a large number of alignments.</S>
    <S sid="58" ssid="3">However, in the case of a hidden Markov model, these computations can be arranged in a way similar to the one used during the FB algorithm, so that the overall amount of computation needed becomes linear in the length of the sentence (Baum and Eagon 1967).</S>
  </SECTION>
  <SECTION title="7." number="7">
    <S sid="59" ssid="1">The main objective of this paper is to compare RF and ML training.</S>
    <S sid="60" ssid="2">This is done in Section 7.2.</S>
    <S sid="61" ssid="3">We also take advantage of the environment that we have set up to perform other experiments, described in Section 7.3, that have some theoretical interest, but did not bring any improvement in practice.</S>
    <S sid="62" ssid="4">One concerns the difference between Viterbi and ML tagging, and the other concerns the use of constraints during training.</S>
    <S sid="63" ssid="5">We shall begin by describing the textual data that we are using, before presenting the different tagging experiments using these various training and tagging methods.</S>
    <S sid="64" ssid="6">We use the &amp;quot;treebank&amp;quot; data described in Beale (1988).</S>
    <S sid="65" ssid="7">It contains 42,186 sentences (about one million words) from the Associated Press.</S>
    <S sid="66" ssid="8">These sentences have been tagged manually at the Unit for Computer Research on the English Language (University of Lancaster, U.K.), in collaboration with IBM U.K. (Winchester) and the IBM Speech Recognition group in Yorktown Heights (USA).</S>
    <S sid="67" ssid="9">In fact, these sentences are not only tagged but also parsed.</S>
    <S sid="68" ssid="10">However, we do not use the information contained in the parse.</S>
    <S sid="69" ssid="11">In the treebank 159 different tags are used.</S>
    <S sid="70" ssid="12">These tags were projected on a smaller system of 76 tags designed by Evelyne Tzoukermann and Peter Brown (see Appendix).</S>
    <S sid="71" ssid="13">The results quoted in this paper all refer to this smaller system.</S>
    <S sid="72" ssid="14">We built a dictionary that indicates the list of possible tags for each word, by taking all the words that occur in this text and, for each word, all the tags that are assigned to it somewhere in the text.</S>
    <S sid="73" ssid="15">In some sense, this is an optimal dictionary for this data, since a word will not have all its possible tags (in the language), but only the tags that it actually had within the text.</S>
    <S sid="74" ssid="16">We separated this data into two parts: In this experiment, we extracted N tagged sentences from the training data.</S>
    <S sid="75" ssid="17">We then computed the relative frequencies on these sentences and built a &amp;quot;smoothed&amp;quot; model using the procedure previously described.</S>
    <S sid="76" ssid="18">This model was then used to tag the 2,000 test sentences.</S>
    <S sid="77" ssid="19">We experimented with different values of N, for each of which we indicate the value of the interpolation coefficient and the number and percentage of correctly tagged words.</S>
    <S sid="78" ssid="20">Results are indicated in Table 1.</S>
    <S sid="79" ssid="21">As expected, as the size of the training increases, the interpolation coefficient increases and the quality of the tagging improves.</S>
    <S sid="80" ssid="22">When N = 0, the model is made up of uniform distributions.</S>
    <S sid="81" ssid="23">In this case, all alignments for a sentence are equally probable, so that the choice of the correct tag is just a choice at random.</S>
    <S sid="82" ssid="24">However, the percentage of correct tags is relatively high (more than three out of four) because: Note that this behavior is obviously very dependent on the system of tags that is used.</S>
    <S sid="83" ssid="25">It can be noted that reasonable results are obtained quite rapidly.</S>
    <S sid="84" ssid="26">Using 2,000 tagged sentences (less than 50,000 words), the tagging error rate is already less than 5%.</S>
    <S sid="85" ssid="27">Using 10 times as much data (20,000 tagged sentences) provides an improvement of only 1.5%.</S>
    <S sid="86" ssid="28">ML training, Viterbi tagging In ML training we take all the training data available (40,186 sentences) but we only use the word sequences, not the associated tags (except to compute the initial model, as will be described later).</S>
    <S sid="87" ssid="29">This is possible since the FB algorithm is able to train the model using the word sequence only.</S>
    <S sid="88" ssid="30">In the first experiment we took the model made up of uniform distributions as the initial one.</S>
    <S sid="89" ssid="31">The only constraints in this model came from the values k(w It) that were set to zero when the tag t was not possible for the word w (as found in the dictionary).</S>
    <S sid="90" ssid="32">We then ran the FB algorithm and evaluated the quality of the tagging.</S>
    <S sid="91" ssid="33">The results are shown in Figure 1.</S>
    <S sid="92" ssid="34">(Perplexity is a measure of the average branching factor for probabilistic models.)</S>
    <S sid="93" ssid="35">This figure shows that ML training both improves the perplexity of the model and reduces the tagging error rate.</S>
    <S sid="94" ssid="36">However, this error rate remains at a relatively high level&#8212;higher than that obtained with a RF training on 100 tagged sentences.</S>
    <S sid="95" ssid="37">Having shown that ML training is able to improve the uniform model, we then wanted to know if it was also able to improve more accurate models.</S>
    <S sid="96" ssid="38">We therefore took as the initial model each of the models obtained previously by RF training and, for each one, performed ML training using all of the training word sequences.</S>
    <S sid="97" ssid="39">The results are shown graphically in Figure 2 and numerically in Table 2.</S>
    <S sid="98" ssid="40">These results show that, when we use few tagged data, the model obtained by relative frequency is not very good and Maximum Likelihood training is able to improve it.</S>
    <S sid="99" ssid="41">However, as the amount of tagged data increases, the models obtained by Relative Frequency are more accurate and Maximum Likelihood training improves on the initial iterations only, but after deteriorates.</S>
    <S sid="100" ssid="42">If we use more than 5,000 tagged sentences, even the first iteration of ML training degrades the tagging.</S>
    <S sid="101" ssid="43">(This number is of course dependent on both the particular system of tags and the kind of text used in this experiment).</S>
    <S sid="102" ssid="44">These results call for some comments.</S>
    <S sid="103" ssid="45">ML training is a theoretically sound procedure, and one that is routinely and successfully used in speech recognition to estimate the parameters of hidden Markov models that describe the relations between sequences of phonemes and the speech signal.</S>
    <S sid="104" ssid="46">Although ML training is guaranteed to improve perplexity, perplexity is not necessarily related to tagging accuracy, and it is possible to improve one while degrading the other.</S>
    <S sid="105" ssid="47">Also, in the case of tagging, ML training from various initial points (top line corresponds to N=100, bottom line to N=a11). the relations between words and tags are much more precise than the relations between phonemes and speech signals (where the correct correspondence is harder to define precisely).</S>
    <S sid="106" ssid="48">Some characteristics of ML training, such as the effect of smoothing probabilities, are probably more suited to speech than to tagging.</S>
    <S sid="107" ssid="49">For this experiment we considered the initial model built by RF training over the whole training data and all the successive models created by the iterations of ML training.</S>
    <S sid="108" ssid="50">For each of these models we performed Viterbi tagging and ML tagging on the same test data, then evaluated and compared the number of tagging errors produced by these two methods.</S>
    <S sid="109" ssid="51">The results are shown in Table 3.</S>
    <S sid="110" ssid="52">The models obtained at different iterations are related, so one should not draw strong conclusions about the definite superiority of one tagging procedure.</S>
    <S sid="111" ssid="53">However, the difference in error rate is very small, and shows that the choice of the tagging procedure is not as critical as the kind of training material.</S>
    <S sid="112" ssid="54">Following a suggestion made by F. Jelinek, we investigated the effect of constraining the ML training by imposing constraints on the probabilities.</S>
    <S sid="113" ssid="55">This idea comes from the observation that the amount of training data needed to properly estimate the model increases with the number of free parameters of the model.</S>
    <S sid="114" ssid="56">In the case of little training data, adding reasonable constraints on the shape of the models that are looked for reduces the number of free parameters and should improve the quality of the estimates.</S>
    <S sid="115" ssid="57">We tried two different constraints: The tw-constrained ML training is similar to the standard ML training, except that the probabilities p(t/w) are not changed at the end of an iteration.</S>
    <S sid="116" ssid="58">The results in Table 4 show the number of tagging errors when the model is trained with the standard or tw-constrained ML training.</S>
    <S sid="117" ssid="59">They show that the tw-constrained ML training still degrades the RF training, but not as quickly as the standard ML.</S>
    <S sid="118" ssid="60">We have not tested what happens when smaller training data is used to build the initial model. t-constraint This constraint is more difficult to implement than the previous one because the probabilities p(t) are not the parameters of the model, but a combination of these parameters.</S>
    <S sid="119" ssid="61">With the help of R. Polyak we have designed an iterative procedure that allows the likelihood to be improved while preserving the values of p(t).</S>
    <S sid="120" ssid="62">We do not have sufficient space to describe this procedure here.</S>
    <S sid="121" ssid="63">Because of its greater computational complexity, we have only applied it to a biclass model, i.e. a model where The initial model is estimated by relative frequency on the whole training data and Viterbi tagging is used.</S>
    <S sid="122" ssid="64">As in the previous experiment, the results in Table 5 show the number of tagging errors when the model is trained with the standard or t-constrained ML training.</S>
    <S sid="123" ssid="65">They show that the t-constrained ML training still degrades the RF training, but not as quickly as the standard ML.</S>
    <S sid="124" ssid="66">Again, we have not tested what happens when smaller training data is used to build the initial model.</S>
    <S sid="125" ssid="67">8.</S>
    <S sid="126" ssid="68">Conclusion The results presented in this paper show that estimating the parameters of the model by counting relative frequencies over a very large amount of hand-tagged text lead to the best tagging accuracy.</S>
    <S sid="127" ssid="69">Maximum Likelihood training is guaranteed to improve perplexity, but will not necessarily improve tagging accuracy.</S>
    <S sid="128" ssid="70">In our experiments, ML training degrades the performance unless the initial model is already very bad.</S>
    <S sid="129" ssid="71">The preceding results suggest that the optimal strategy to build the best possible model for tagging is the following: whichever occurs first.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="130" ssid="1">I would like to thank Peter Brown, Fred Jelinek, John Lafferty, Robert Mercer, Salim Roukos, and other members of the Continuous Speech Recognition group for the fruitful discussions I had with them throughout this work.</S>
    <S sid="131" ssid="2">I also want to thank one of the referees for his judicious comments.</S>
  </SECTION>
</PAPER>
