<PAPER>
	<S sid="0">Anaphora For Everyone: Pronominal Anaphora Resolution Without A Parser</S><ABSTRACT>
		<S sid="1" ssid="1">We present an algorithm for anaphora res- olutkm which is a modified and extended version of that developed by (Lappin and Leass,/994).</S>
		<S sid="2" ssid="2">In contrast to that work, our al- gorithm does not require in-depth, full, syn..</S>
		<S sid="3" ssid="3">tactic parsing of text.</S>
		<S sid="4" ssid="4">Instead, with minimal compromise in output quality, the modifica- tions enable the resolution process to work from tile output of a part of speech tag- ge~; enriched only with annotations of gram- matica\] functkm of lexical items in the in- put text stream.</S>
		<S sid="5" ssid="5">Evaluation of the results of our in-tplementation demonstrates that ac- curate anaphora resolution can be realized within natural anguage processing fl'ame- works which do not--~,)r cannot- employ ro- bust and rcqiable parsing components.</S>
	</ABSTRACT>
	<SECTION title="Overview" number="1">
			<S sid="6" ssid="6">(l,appin and Leass, 1994) describe an algorithm for pronominal anaphora resolution with high rate of cor- rect analyses.</S>
			<S sid="7" ssid="7">While one of the strong points of this algorithm is that it operates primarily on syntactic in- formation ahme, this also turns out to be a limiting factor for its wide use: current state-of-the-art of prac- tically applicable parsing technology still falls short of robust and reliable delivery of syntactic analysis of real texts to the level of detail and precision that the filters a nd constraints described by I ,appin and l ,eass assume.</S>
			<S sid="8" ssid="8">We are particularly interested in a class of text pro- cessing applications, capable of delivery of content analysis to a depth inw~lving non-trivial amount of discourse processing, including anaphora resolution.</S>
			<S sid="9" ssid="9">The operational context prohibits us from making any assumptions concerning domain, style, and genre of input; as a result, we have developed a text processing framework which builds its capabilities entirely on the basis of a considerably shallower linguistic analysis of the input stream, thus trading off depth of base level analysis for breadth of cown:age.</S>
			<S sid="10" ssid="10">In this paper, we present work on modifying the lmp- pin/Leass algorithm in a way which enables it to work off a flat morpho-syntactic analysis of the sentences of a text, while retaining a degree of quality and accuracy in pronorainal anaphora resolution comparable to that reported in (Lappin and l,eass, 1994).</S>
			<S sid="11" ssid="11">The modifica- tions discussed below make the algorithm available to a wide range of text processing frameworks, which, due to the lack of full syntactic parsing capability, nor- really would have been unable to use this high preci- sion anap hora resolution tool.</S>
			<S sid="12" ssid="12">The work is additionally important, we feel, as it shows that informatkm about the content and logical structure of a text, in princi-.</S>
			<S sid="13" ssid="13">pie a core requirement for higher level semantic and discourse processes, can be effectively approximated by the right mix of constituent analysis and inferences about functional relations.</S>
	</SECTION>
	<SECTION title="General outline of the algorithm. " number="2">
			<S sid="14" ssid="1">The base level linguistic analysis for actaphora resolu- tion is the output of a part of speech tagger, augmented with syntactic function annotatkms for each input to.</S>
			<S sid="15" ssid="2">ken; this kind of analysis is generated by the mor- pbosyntactic tagging system described in (Voutilainen et al, 1992), (Karlsson et al, 1995) (hencehvth 1,1NC:- ~;olq').</S>
			<S sid="16" ssid="3">In addition to extremely high levels of accuracy in recall and precision of tag assignment ((VoutiJainen et al, 1992) report 99.77?/,, overall recall and 95.54% overall preciskm, over a variety of text genres, and in comparison with other state-of-the-art tagging sys- tems), the primary motivation for adopting this system is the requirement todevelop a robust ext processor- with anaphora resolution being just one of its discourse analysis functkms capable of reliably handling arbi- trary kinds of input.</S>
			<S sid="17" ssid="4">The tagger provides a very simple analysis of the structure of the text: for each lexical item in each sen- tence, it provides a set of values which indicate the morphological, lexical, grammatical nd syntactic fea- tures of the item in tile context in which it appears.</S>
			<S sid="18" ssid="5">In addition, the modified algorithm we present requh:es annota tion of the input text stream by a simple position-- identification function which associates an integer with each token in a text sequentially (we will refer to a to- ken's integer value as its oJ~et).</S>
			<S sid="19" ssid="6">As an example, given the text "For 1995 the company set up its headquar- ters in Hall \] l, the newest and most presti-.</S>
			<S sid="20" ssid="7">gious of CeBIT's 23 hal Is." tile anaphora resolutkm algorithm would be presented with the h}llowing analysis tream.</S>
			<S sid="21" ssid="8">Note, in particu-.</S>
			<S sid="22" ssid="9">lar, the grammatical function information (e.g., @SUl~J, O)q.FMAINV) and the integer values (e.g., "offt 39") asso- cia ted with each token.</S>
			<S sid="23" ssid="10">"For /o f f139" "for" PREP @ADVL "1995/o f f140 ....</S>
			<S sid="24" ssid="11">1995" NUM CARD @&lt;P " the/o f f l41" "the" DET CENTRAL ART SG/PL @DN&gt; "company/o f f142" "company" N NOM SG/PL @SUBJ "set/off143" "set" V PAST VF IN @+FMAINV "up/of f144" "up" ADV ADVL @ADVL " i t s /o f f145 .... it" PRON GEN SG3 @GN&gt; "headquar ters /o f f146 .... headquar ters" N NOM SG/PL @OBJ " in /o f f147 .... in" PREP @&lt;NOM @ADVL "Ha l l /o f f148 .... hal l" N NOM SG @NN&gt; " l l /o f f149" "Ii" NUM CARD @&lt;P "$ , /o f f l50 ....</S>
			<S sid="25" ssid="12">," PUNCT " the/o f f l51" "the" DET CENTRAL ART SG/PL @DN&gt; "newest /o f f152 .... new" A SUP @PCOMPL-O "and/of f153 .... and" CC @CC "most /o f f154" "much" ADV SUP @AD-A&gt; "pres t ig ious /o f f155 .... p res t ig ious" A ABS @&lt;P "of /o f f156 .... of" PREP @&lt;NOM-OF "CeBIT ' s /o f f157" "cebit" N GEN SG @GN&gt; "23/0f f158 ....</S>
			<S sid="26" ssid="13">23" NUM CARD @QN&gt; "ha l l s /o f f159 .... hal l" N NOM PL @&lt;P "$ . /o f f160 ....</S>
			<S sid="27" ssid="14">PUNCT 2.1 Data collection.</S>
			<S sid="28" ssid="15">Although LINGSOFT does not provide specific infor- mation about constituent structure, partial constituen- cy-specifically, identification of sequences of tokens as phrasal units--can be inferred from the analysis by running the tagged text through a set of filters, which are stated as regular expressions over metatokens such as the ones illustrated above.</S>
			<S sid="29" ssid="16">For the purposes of anaphora resolution, the pri- mary data set consists of a complete listing of all noun phrases, reduced to modifier-head sequences.</S>
			<S sid="30" ssid="17">This data set is obtained by means of a phrasal grammar whose patterns characterize the composition of a noun phrase (NP) in terms of possible token sequences.</S>
			<S sid="31" ssid="18">The output of NP identification is a set of token/feature matrix/offset sequences, where offset value is deter- mined by the offset of the first token in the sequence.</S>
			<S sid="32" ssid="19">The offset indicates the position of the NP in the text, and so provides crucial information about precedence relations.</S>
			<S sid="33" ssid="20">A secondary data set consists of observations about the syntactic ontexts in which the NPs identified by the phrasal grammar appear.</S>
			<S sid="34" ssid="21">These observations are derived using a set of patterns designed to detect nom- inal sequences in two subordinate syntactic environ- ments: containment in an adverbial adjunct and con- tainment in an NP (i.e., containment in a prepositional or clausal complement of a noun, or containment in a relative clause).</S>
			<S sid="35" ssid="22">This is accomplished by running a set of patterns which identify NPs that occur locally to ad- verbs, relative pronouns, and noun-preposition r noun- complementizer sequences over the tagged text in con- junction with the basic NP patterns described above.</S>
			<S sid="36" ssid="23">Because the syntactic,patterns are stated as regular ex- pressions, misanalyses are inevitable.</S>
			<S sid="37" ssid="24">In practice, how- ever, the extent o which incorrect analyses of syntactic context affect the overall accuracy of the algorithm is not large; we will return to a discussion of this point in section 4.</S>
			<S sid="38" ssid="25">A third set of patterns identifies and tags occurrences of "expletive" it.</S>
			<S sid="39" ssid="26">These patterns target occurrences of the pronoun it in certain contexts, e.g., as the subject of members of a specific set of verbs (seem, appear, etc.), or as the subject of adjectives with clausal complements.</S>
			<S sid="40" ssid="27">Once the extraction procedures are complete and the results unified, a set of discourse referents--abstract ob- jects which represent the participants inthe discourse-- is generated from the set of NP observations.</S>
			<S sid="41" ssid="28">A particu- larly convenient implementation f discourse referents is to represent them as objects in the Common Lisp Object System, with slots which encode the following information parameters (where ADJUNCT and EMBED indicate whether a discourse referent was observed in either of the two syntactic ontexts discussed above): TEXT: text form TYPE: referential type (e.g., REF, PRO, RFLX) AGR: person, number, gender GFUN: grammatical function ADJUNCT: T o r NIL EMBED: T o r NIL POS: text position Note that each discourse referent contains information about itself and the context in which it appears, but the only information about its relation to other dis- course referents is in the form of precedence r lations (as determined by text position).</S>
			<S sid="42" ssid="29">The absence of explicit information about configurational relations marks the crucial difference between our algorithm and the Lap- pin/Leass algorithm.</S>
			<S sid="43" ssid="30">(Lappin and Leass, 1994) use configurational information in two ways: as a factor in the determination of the salience of a discourse refer- ent (discussed below), and as input to a set of disjoint reference filters.</S>
			<S sid="44" ssid="31">Our implementation seeks to perform exactly the same tasks by inferring hierarchical rela- tions from a less rich base.</S>
			<S sid="45" ssid="32">The modifications and assumptions required to accomplish this goal will be highlighted in the following discussion.</S>
			<S sid="46" ssid="33">2.2 Anaphora resolution.</S>
			<S sid="47" ssid="34">Once the representation f the text has been recast as a set of discourse referents (ordered by offset value), it is sent to the anaphora resolution algorithm proper.</S>
			<S sid="48" ssid="35">The basic logic of the algorithm parallels that of the Lap- pin/Leass algorithm.</S>
			<S sid="49" ssid="36">The interpretation procedure in- volves moving through the text sentence by sentence and interpreting the discourse referents in each sen- tence from left to right.</S>
			<S sid="50" ssid="37">There are two possible in- terpretations of a discourse referent: either it is taken to introduce a new participant in the discourse, or it is taken to refer to a previously interpreted iscourse referent.</S>
			<S sid="51" ssid="38">Coreference is determined by first eliminating from consideration those discourse referents to which an anaphoric expression cannot possibly refer, then se- lecting the optimal antecedent from the candidates that remain, where optimality is determined by a salience measure.</S>
			<S sid="52" ssid="39">In order to present the details of anaphora resolution, we define below our notions--and implementations-- of coreference and salience.</S>
			<S sid="53" ssid="40">2.2.1 Coreference As in the Lappin and Leass algorithm, the anaphor- antecedent relation is established between two dis- course referents (cf.</S>
			<S sid="54" ssid="41">(Helm, 1982), (Kamp, 1981)), @hile the more general notion of coreference is represented in terms of equivalence classes of anaphorically re- lated discourse referents, which we will refer to as "COREF classes".</S>
			<S sid="55" ssid="42">Thus, the problem of interpreting an anaphoric expression boils down to the problem of es- tablishing an anaphoric link between the anaphor and some previously interpreted iscourse referent (pos- sibly another anaphor); a consequence of establishing 114 this link is that the anaphor becomes a member of the COREF class already associated with its antecedent.</S>
			<S sid="56" ssid="43">In our implementation, COREF classes are repre- sented as objects in the Common Lisp Object System which contain information about the COREF class as a whole, including canonical form (typically deter- mined by the discourse referent which introduces the class), membership, and, most importantly, salience (discussed below).</S>
			<S sid="57" ssid="44">1 The connection between a dis- course referent and its COREF class is mediated through the COREF object as follows: every discourse referent includes an information parameter which is a pointer to a COREF object; discourse referents which have been determined to be coreferential share the same COREF value (and so literally point to the same object).</S>
			<S sid="58" ssid="45">Imple- menting coreference in this way provides a means of getting from any discourse referent in a COREF class to information about the class as a whole.</S>
			<S sid="59" ssid="46">2.2.2 Salience The information parameter of a COREF object most cru- cial to anaphora resolution is its salience, which is de- termined by the status of the members of the COREF class it re.presents with respect to 10 contextual, gram- matical, and syntactic onstraints.</S>
			<S sid="60" ssid="47">Following (Lappin and Leass, 1994), we will refer to these constraints as "salience factors".</S>
			<S sid="61" ssid="48">Individual salience factors are asso- ciated with numerical values; the overall salience, or "salience weight" of a COREF is the sum of the values of the salience factors that are satisfied by some member of the COREF class (note that values may be satisfied at most once by each member of the class).</S>
			<S sid="62" ssid="49">The salience factors used by our algorithm are defined below with their values.</S>
			<S sid="63" ssid="50">Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as deter- mined by a text-segmentation algorithm which follows (Hearst, 1994).</S>
			<S sid="64" ssid="51">SENT-S: 100 iff in the current sentence CNTX-S: 50 iff in the current context SUBJ-S: 80 iff GFUN = subject EXST-S: 70 iff in an existential construction POSS-S: 65 iff GFUN = possessive ACC-S: 50 iff GFUN = direct object DAT-S: 40 iff GFUN = indirect object OBLQ-S: 30 iff the complement of a preposition HEAD-S: 80 iff EMBED = NIL ARG-S: 50 iff ADJUNCT = NIL Note that the values of salience factors are arbitrary; what is crucial, as pointed out by (Lappin and Leass, 1994), is the relational structure imposed on the factors by these values.</S>
			<S sid="65" ssid="52">The relative ranking of the factors is justified both linguistically, as a reflection of the role of the functional hierarchy in determining anaphoric relations (cf.</S>
			<S sid="66" ssid="53">(Keenan and Comrie, 1977)), as well as by experimental results--both Lappin and Leass' and our own.</S>
			<S sid="67" ssid="54">For all factors except CNTX-S and POSS-S, we adopt the values derived from a series of experiments described in (Lappin and Leass, 1994) which used dif- ferent settings to determine the relative importance of 1The implementation of aCOREF object needs to be aware of po- tenlial circularities, thus a COREF does not actually contain its member discourse r ferents, but rather alisting of their offsets, each factor as a function of the overall success of the algorithm.</S>
			<S sid="68" ssid="55">Our values for CNTX-S and POSS-S were de- termined using similar tests.</S>
			<S sid="69" ssid="56">An important feature of our implementation of salience, following that of Lappin and Leass, is that it is variable: the salience of a COREF class decreases and increases according to the frequency of reference to the class.</S>
			<S sid="70" ssid="57">When an anaphoric link is established between a pronoun and a previously introduced iscourse refer- ent, the pronoun is added to the COREF class associated with the discourse referent, its COREF value is set to the COREF value of the antecedent (i.e., to the COREF ob- ject which represents he class), and the salience of the COREF object is recalculated according to how the new member satisfies the set of salience factors.</S>
			<S sid="71" ssid="58">This final step raises the overall salience of the COREF, since the new member will minimally satisfy SENT-S and CNTX-S.</S>
			<S sid="72" ssid="59">Salience is not stable, however: in order to realisti- cally represent the local prominence of discourse ref- erents in a text, a decay function is built into the algo- rithm, so that salience weight decreases over time.</S>
			<S sid="73" ssid="60">If new members are not added, the salience weight of a COREF eventually reduces to zero.</S>
			<S sid="74" ssid="61">The consequence of this variability in salience is that a very general heuris- tic for anaphora resolution is established: resolve a pronoun to the most salient candidate antecedent.</S>
			<S sid="75" ssid="62">2.2.3 Interpretation As noted above, in terms of overall strategy, the resolu- tion procedure follows that of Lappin and Leass.</S>
			<S sid="76" ssid="63">The first step in interpreting the discourse referents in a new sentence isto decrease the salience weights of the COREF classes that have already been established by a factor of two.</S>
			<S sid="77" ssid="64">Next, the algorithm locates all non-anaphoric dis- course referents in the sentence under consideration, generates a new COREF class for each one, and calcu- lates its salience weight according to how the discourse referent satisfies the set of salience factors.</S>
			<S sid="78" ssid="65">The second step involves the interpretation f lexical anaphors (reflexives and reciprocals).</S>
			<S sid="79" ssid="66">A list of candi- date antecedent-anaphor pairs is generated for every lexical anaphor, based on the hypothesis that a lexical anaphor must refer to a coargument.</S>
			<S sid="80" ssid="67">In the absence of configurational information, coarguments are iden- tified using grammatical function information (as de- termined by LINGSOFT) and precedence relations.</S>
			<S sid="81" ssid="68">A reflexive can have one of three possible grammatical function values: direct object, indirect object, or oblique.</S>
			<S sid="82" ssid="69">In the first case, the closest preceding discourse referent with grammatical function value subject is identified as a possible antecedent.</S>
			<S sid="83" ssid="70">In the latter cases, both the clos- est preceding subject and the closest preceding direct object hat is not separated from the anaphor by a sub- ject are identified as possible antecedents.</S>
			<S sid="84" ssid="71">If more than one possible antecedent is located for a lexical anaphor, the one with the highest salience weight is determined to be the actual antecedent.</S>
			<S sid="85" ssid="72">Once an antecedent has been located, the anaphor is added to the COREF class associated with the antecedent, and the salience of the COREF class is recalculatec~ accordingly.</S>
			<S sid="86" ssid="73">The final step is the interpretation f pronouns.</S>
			<S sid="87" ssid="74">The basic resolution heuristic, as noted above, is quite sim- ple: generate a set of candidate antecedents, then es- tablish coreference with the candidate which has the greatest salience weight (in the event of a tie, the clos- est candidateis chosen).</S>
			<S sid="88" ssid="75">In order to generate the candi- date set, however, those discourse referents with which 115 a pronoun cannot refer must be eliminated from consid- eration.</S>
			<S sid="89" ssid="76">This is accomplished by running the overall candidate pool (the set of interpreted iscourse ref- erents whose salience values exceed an arbitrarily set threshold) through two sets of filters: a set of morpho- logical agreement filters, which eliminate from consid- eration any discourse referent which disagrees in per- son, numbeb or gender with the pronoun, and a set of disjoint reference filters.</S>
			<S sid="90" ssid="77">The determination f disjoint reference represents a significant point of divergence between our algorithm and the Lappin/Leass algorithm, because, as is well known, configurational relations play a prominent role in determining which constituents in a sentence a pro- noun may refer to.</S>
			<S sid="91" ssid="78">Three conditions are of particular relevance to the anaphora resolution algorithm: Condition \]: A pronoun cannot corefer with a coargument.</S>
			<S sid="92" ssid="79">Condition 2: A pronoun cannot corefer with a nonpronominal constituent which it both commands and precedes.</S>
			<S sid="93" ssid="80">Condition 3: A pronoun cannot corefer with a constituent which contains it.</S>
			<S sid="94" ssid="81">In the absence of configurafional information, our al- gorithm relies on inferences from grammatical func- tion and precedence todetermine disjoint reference.</S>
			<S sid="95" ssid="82">In practice, even without accurate information about con- stituent structure, the syntactic filters described below are extremely accurate (see the discussion of this point in section 4).</S>
			<S sid="96" ssid="83">Condition i is implemented bylocating all discourse referents with GFUN value direct object, indirect object, or oblique which follow a pronoun with GFUN value subject or direct object, as long as no subject intervenes (the hypothesis being that a subject indicates the beginning of the next clause).</S>
			<S sid="97" ssid="84">Discourse referents which satisfy these conditions are identified as disjoint.</S>
			<S sid="98" ssid="85">Condition 2 is implemented by locating for ev- ery non-adjunct and non-embedded pronoun the set of non-pronominal discourse referents in its sentence which follow it, and eliminating these as potential an- tecedents.</S>
			<S sid="99" ssid="86">In effect, the command relation is inferred from precedence and the information provided by the syntactic patterns: an argument which is neither con- tained in an adjunct nor embedded in another nominal commands those expressions which it precedes.</S>
			<S sid="100" ssid="87">Condition 3 makes use of the observation that a dis- course referent contains every object o its right with a non-nil EMBED value.</S>
			<S sid="101" ssid="88">The algorithm identifies as dis- joint a discourse referent and every pronoun which fol- lows it and has a non-nil EMBED value, until a discourse referent with EMBED value NIL is located (marking the end of the containment domain).</S>
			<S sid="102" ssid="89">Condiditon 3 also rules out coreference between a genitive pronoun and the NP it modifies.</S>
			<S sid="103" ssid="90">After the morphological nd syntactic filters have been applied, the set of discourse referents that remain constitute the set of candidate antecedents for the pro- noun.</S>
			<S sid="104" ssid="91">The candidate set is subjected to a final evalu- ation procedure which performs two functions: it de- creases the salience of candidates which the pronoun precedes (cataphora is penalized), and it increases the sa li ence of candida tes which satisfy either a locality or a parallelism condition (described below), both of which apply to intrasentential c ndidates.</S>
			<S sid="105" ssid="92">The h)cality heuristic isdesigned to negate the effects of subordinationwhen both candidate and anaphor ap- pear in the same subordinate context, the assumption being that the prominence of a candidate should be de- termined with respect o the position of the anaphor.</S>
			<S sid="106" ssid="93">This is a point of difference between our algorithm and the one described in (Lappin and Leass, 1994).</S>
			<S sid="107" ssid="94">The salience of a candidate which is determined tobe in the same subordinate context as a pronoun (determined as a function of precedence r lations and EMBED and ADJUNCT values) is temporarily increased to the level it would have were the candidate not in the subordi- nate context; the level is returned to normal after the anaphor is resolved.</S>
			<S sid="108" ssid="95">The parallelism heuristic rewards candidates which are such that the pair consisting of the GFUN values of candidate and anaphor are identical to GFUN values of a previously identified anaphor-antecedent pair.</S>
			<S sid="109" ssid="96">This parallelism heuristic differs from a similar one used by the Lappin/Leass algorithm, which rewards candi- dates whose grammatical function is identical to that of an anaphor.</S>
			<S sid="110" ssid="97">Once the generation and evaluation of the candidate set is complete, the candidates are ranked according to salience weight, and the candidate with the high- est salience weight is determined tobe the antecedent of the pronoun under consideration.</S>
			<S sid="111" ssid="98">In the event of a tie, the candidate which most immediately precedes the anaphor is selected as the antededent (where prece- dence is determined by comparing offset values).</S>
			<S sid="112" ssid="99">The COREF value of the pronoun is set to that of the an- tecedent, adding it to the the antecedent's COREF class, and the salience of the class is recalculated accordingly.</S>
	</SECTION>
	<SECTION title="Example output. " number="3">
			<S sid="113" ssid="1">The larger context from which the sample analysis in the beginning of Section 2 was taken is as follows: "...while Apple and its PowerPC partners claimed some prime real estate on the show floor, Apple's most interesting offerings de- buted behind the scenes.</S>
			<S sid="114" ssid="2">Gone was the nar- row corner booth that Apple shoehorned its products into last year.</S>
			<S sid="115" ssid="3">For 1995 the com- pany set up its headquarters in Hall 11, the newest and most prestigious of CeNT's 23 halls."</S>
			<S sid="116" ssid="4">The anaphora resolution algorithm generates the fol- lowing analysis for the first italicized pronoun.</S>
			<S sid="117" ssid="5">For each candidate, ~ the annotation i square brackets in- dicates its offset value, and the number to the right indicates its salience weight at the point of interpreta- tkm of the pronoun.</S>
			<S sid="118" ssid="6">ANA: its \[@off/\]33\] CND: Apple \[@of 1/131\] 432 Apple \[/aol f/10\] \] 352 its \[@off/\].03\] 352 App\]e's \[@offf/\] I 5\] 1352 prilne real estat(!</S>
			<S sid="119" ssid="7">\[@off/\]08\] 165 show f loor \ [ (aof f /1 \ ]2 l \]55 year \[@o~f/137 I 310/3 The candidate set illustrates several important points.</S>
			<S sid="120" ssid="8">First, the equality in salience weights of the candi- dates at offsets 101, 103, and 115 is a consequence of 2Note that our syntactic filters are quite capable of discarding a number of configurationally inappropriate antecedents, which appear to satisfy the precedence r lation.</S>
			<S sid="121" ssid="9">116 the fact that these discourse referents are members of the same COP, Et ~' class.</S>
			<S sid="122" ssid="10">Their unification into a single class indicates both successful anaphora resolution (of the pronoun at offset 103), as well as the operation of higherqevel discourse processing designed to identify all references to a particular COREF class, not just the anaphoric ones (cf.</S>
			<S sid="123" ssid="11">(Kennedy and Boguraev, :1996)).</S>
			<S sid="124" ssid="12">The higher salience of the optimal candidate--which ix also a member of this COREF class--shows the effect of the locality heuristic described in section 2.2.3.</S>
			<S sid="125" ssid="13">Both the pronoun and the candidate appear in the same sub- ordinate context (within a relative clause); as a result the salience of the candidate (but not of the class to which it bekmgs) is temporarily boosted to negate the effect of subordinatkm.</S>
			<S sid="126" ssid="14">An abbreviated candidate set for the second itali- cized pronoun is given below: ANA: i t s {61of f /145\ ] CND: company \[(,)ot I / 142 \] :H,0 App l e ((,!of 17/ 13 / \] 192 it:~:; {(aof I / I 3 ~ \] 192 This set is interesting because it illustrates the promi- nent role of SENT-S in controlling salience: company ix correctly identified as the antecedent of the pronotm, despite the frequency of mention of members of the COREF class containing Apple and its, because it occurs in the same sentence as the anaphor.</S>
			<S sid="127" ssid="15">Of course, this ex- ample also indicates the need fl~r additional heuristics designed to connect company with Apple, since these discourse referents clearly make reference to the same object.</S>
			<S sid="128" ssid="16">We are currentlyworking towards this goal; see (Kennedy and Boguraev, \]996) for discussion.</S>
			<S sid="129" ssid="17">'l'he following text segment illust rates the resolution of in tersen ten tia l a napho ra.</S>
			<S sid="130" ssid="18">"Sun's prototype lntemet access device uses a 1-10-Mhz MicroSPARCprocesso~; and is diskless.</S>
			<S sid="131" ssid="19">Its dimensions are 5.5 inches x 9 inches x 2inches."</S>
			<S sid="132" ssid="20">ANA: \]its \[\[aol f /347\ ] CNI): IAlte~:ileL access devic() \[(,!o~\[/33\[i\] 180 M i c KOf;PARCI)rOC e!s sot \[(4oEI /34\] \] 16!i ~;un 's \[&lt;4o1 f /3 : t3 I \ [40 The first sentence in this fl'agment introduces three dis- course referents bearing different grammatical func- tions, none of which appear in subordinate contexts.</S>
			<S sid="133" ssid="21">Since the sentence in which the anaphor occurs does not contain any candidates (the discourse referent in- troduced by dimensions ix eliminated from considera- tion by both the morphok)gical nct disjoint reference filters), only those from the previous entence are con- sidered (each is compatible with the morphological requirements of the anaphor).</S>
			<S sid="134" ssid="22">These are ranked ac- cording to salience weight, where the crucial factor is grammatical function value.</S>
			<S sid="135" ssid="23">The result of the ranking is that Internet access device--the candidate which satis- fies the highest-weighted salience facto1, SUBl-S--is the optimal candidate, and so correctly identified as the an tecedent</S>
	</SECTION>
	<SECTION title="Evaluation. " number="4">
			<S sid="136" ssid="1">Quantitative evaluation shows the anaphora resolution algorithm described here to run at a rate of 75'70 accu- racy.</S>
			<S sid="137" ssid="2">The data set on which the evaluatkm was based consisted of 27 texts, taken from a random selection of genres, including press releases, product annotmce- meats, news stories, magazine articles, and other doc- uments existing as World Wide Web pages.</S>
			<S sid="138" ssid="3">Within these texts, we counted 3(16 third person anaphoric pro- nouns; of these, 231l were correctly resolved to the dis- course referent identified as the antecedent by the first author.</S>
			<S sid="139" ssid="4">3 This rate of accuracy is clearly comparable to that of the Lappin/Leass algorithm, which (Lappin and Leass, \] 994) report as 85?/,,.</S>
			<S sid="140" ssid="5">Several observations about he results and the com- parison with (lmppin and I,eass, 1994) are in order.</S>
			<S sid="141" ssid="6">First, and most obviously, some deterioratkm in qual- ity is to be expected, given the relatively impoverished linguistic base we start with.</S>
			<S sid="142" ssid="7">Second, it is important to note that this is not just a matter of simple comparison.</S>
			<S sid="143" ssid="8">The results in (l.appin and Leass, 1994) describe the output of the procedttre applied to a singh,' text genre: computer manuals.</S>
			<S sid="144" ssid="9">Ar- guably, this is an example of a particularly well be- haved text; in any case, it is not clear how the figure would be normalized over a wide range of text types, some of them not completely 'clean', as is the case with our data.</S>
			<S sid="145" ssid="10">Third, close analysis of the most common types of error our algorithm currently makes reveals two spe- cific configurations in the input which confuse the pro- cedure and contribute to the error rate: gender mis- match (35% of errors) and certain long range contextttal (stylistic) phenomena, best exemplified by text contain- ing quoted passages in-line (14% of errors).</S>
			<S sid="146" ssid="11">Implementing a gender (dis-)agreement fil er is not technically complex; as noted above, the current algo- rithrn contains one.</S>
			<S sid="147" ssid="12">The persistence of gender mis- matches in the output simply reflects the lack of a con- sistent gender slot in the I,\[NGSOFT tagger output.</S>
			<S sid="148" ssid="13">Aug- menting the algorithm with a lexical database which includes more detailed gender information will result in improved accuracy.</S>
			<S sid="149" ssid="14">Ensuring proper interpretatkm of anaphors both within and outside of quoted text requires, in effect, a method of evaluating quoted speech separately from its surrotmdingcnntext.</S>
			<S sid="150" ssid="15">Al hough acomplex problem, we feel that this is possible, given that our input data stream embodies a richer notkm of position and con- text, as a resu\[t of an independent text segmentation procedure adapted from (\[ learst, 1994) (and discussed above in section 2.2.2).</S>
			<S sid="151" ssid="16">What is worth noting is the small number of errors which can be directly attributed to the absence of con- figurational inh~rmation.</S>
			<S sid="152" ssid="17">Of the 75 misinterpreted pro- nouns, only 2 inw~lved a failure to establish configu- ratkmally determined disjoint reference (both of these inw~lved Condition 3), and only an additional several errors could be tmambiguously traced to a failure to correctly identify the syntactic ontext in which a dis~ course referent appeared (as determined by a misfireof the salience factors ensitive to syntactic context, I lEAD- S and ARC:S).</S>
			<S sid="153" ssid="18">Overall, these considerations lead to two conchl-.</S>
			<S sid="154" ssid="19">sions.</S>
			<S sid="155" ssid="20">First, with the incorporation of more explicit morphological nd contextual information, it should 3The set of 306 "anaphoric" pronouns excluded 30 occurrences of "expletive" it not identified by the expletive patterns (prhnari ly occurrences in object position), as well as 6 occurrences of it which referred to a VP or propositional constituent.</S>
			<S sid="156" ssid="21">We are currently mfinin g the existing expletive patterns for improved accuracy.</S>
			<S sid="157" ssid="22">117 be possible to increase the overall quality of our out- put, bringing it much closer in line with Lappin and Leass' results.</S>
			<S sid="158" ssid="23">Again, straight comparison would not be trivial, as e.g. quoted text passages are not a natural part of computer manuals, and are, on the other hand, an extremely common occurrence in the types of text we are dealing with.</S>
			<S sid="159" ssid="24">Second, and most importantly, the absence of ex- plicit configurational information does not result in a substantial degradation i the accuracy of an anaphora resolution algorithm that is otherwise similar to that described in (Lappin and Leass, 1994).</S>
	</SECTION>
	<SECTION title="Conc lus ion. " number="5">
			<S sid="160" ssid="1">Lappin and Leass' algorithm for pronominal anaphora resolution is capable of high accuracy, but requires in- depth, full, syntactic parsing of text.</S>
			<S sid="161" ssid="2">The modifications of that algorithm that we have developed make it avail- able to a larger set of text processing frameworks, as we assume a considerably 'poorer' analysis ubstrate.</S>
			<S sid="162" ssid="3">While adaptations to the input format and interpreta- tion procedures have necessarily addressed the issues of coping with a less rich level of linguistic analysis, there is only a small compromise in the quality of the results.</S>
			<S sid="163" ssid="4">Our evaluation indicates that the problems with the current implementation donot stem from the absence of a parse, but rather from factors which can be addressed within the constraints imposed by the shallow base analysis.</S>
			<S sid="164" ssid="5">The overall success of the algo- rithm is important, then, not only for the immediate utility of the particular modifications, but also because the strategy we have developed for circumventing the need for full syntactic analysis is applicable to other in- terpretation tasks which, like the problem of anaphora resolution, lie in the space of higher level semantic and discourse analysis.</S>
	</SECTION>
</PAPER>
