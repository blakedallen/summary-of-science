Using Lexical Chains For Text Summarization
We investigate one technique to produce a summary of an original text without requiring its full semantic interpretation, but instead relying on a model of the topic progression in the text derived from lexical chains.
We present a new algorithm to compute lexical chains in a text, merging several robust knowledge sources: the WordNet thesaurus, a part-of-speech tagger and shallow parser for the identification of nominal groups, and a segmentation algorithm derived from (Hearst, 1994).
Summarization proceeds in three steps: the original text is first segmented, lexical chains are constructed, strong chains are identified and significant sentences are extracted from the text.
We present in this paper empirical results on the identification of strong chains and of significant sentences.
We find that the use of a part-of-speech tagger could eliminate wrong inclusions of words such as read, which has both noun and verb entries in WordNet.
In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary.
Cohesion is achieved through the use in the text of semantically related terms, reference, ellipse and conjunctions.
