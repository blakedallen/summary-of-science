<PAPER>
  <S sid="0">Automatic Rule Induction For Unknown-Word Guessing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Words unknown to the lexicon present a substantial problem to NLP modules that rely on morphosyntactic information, such as part-of-speech taggers or syntactic parsers.</S>
    <S sid="2" ssid="2">In this paper we present a technique for fully automatic acquisition of rules that guess possible part-of-speech tags for unknown words using their starting and ending segments.</S>
    <S sid="3" ssid="3">The learning is performed from a general-purpose lexicon and word frequencies collected from a raw corpus.</S>
    <S sid="4" ssid="4">Three complimentary sets of word-guessing rules are statistically induced: prefix morphological rules, suffix morphological rules and ending-guessing rules.</S>
    <S sid="5" ssid="5">Using the proposed technique, unknown-word-guessing rule sets were induced and integrated into a stochastic tagger and a rule-based tagger, which were then applied to texts with unknown words.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="6" ssid="1">Words unknown to the lexicon present a substantial problem to NLP modules that rely on morphosyntactic information, such as part-of-speech taggers or syntactic parsers.</S>
    <S sid="7" ssid="2">In this paper we present a technique for fully automatic acquisition of rules that guess possible part-of-speech tags for unknown words using their starting and ending segments.</S>
    <S sid="8" ssid="3">The learning is performed from a general-purpose lexicon and word frequencies collected from a raw corpus.</S>
    <S sid="9" ssid="4">Three complimentary sets of word-guessing rules are statistically induced: prefix morphological rules, suffix morphological rules and ending-guessing rules.</S>
    <S sid="10" ssid="5">Using the proposed technique, unknown-word-guessing rule sets were induced and integrated into a stochastic tagger and a rule-based tagger, which were then applied to texts with unknown words.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="11" ssid="1">Words unknown to the lexicon present a substantial problem to NLP modules (as, for instance, part-of-speech (os-) taggers) that rely on information about words, such as their part of speech, number, gender, or case.</S>
    <S sid="12" ssid="2">Taggers assign a single POS-tag to a word-token, provided that it is known what POS-tags this word can take on in general and the context in which this word was used.</S>
    <S sid="13" ssid="3">A POS-tag stands for a unique set of morpho-syntactic features, as exemplified in Table 1, and a word can take several Pos-tags, which constitute an ambiguity class or POS-class for this word.</S>
    <S sid="14" ssid="4">Words with their Pos-classes are usually kept in a lexicon.</S>
    <S sid="15" ssid="5">For every input word-token, the tagger accesses the lexicon, determines possible POS-tags this word can take on, and then chooses the most appropriate one.</S>
    <S sid="16" ssid="6">However, some domain-specific words or infrequently used morphological variants of general-purpose words can be missing from the lexicon and thus, their Pos-classes should be guessed by the system and only then sent to the disambiguation module.</S>
    <S sid="17" ssid="7">The simplest approach to Pos-class guessing is either to assign all possible tags to an unknown word or to assign the most probable one, which is proper singular noun for capitalized words and common singular noun otherwise.</S>
    <S sid="18" ssid="8">The appealing feature of these approaches is their extreme simplicity.</S>
    <S sid="19" ssid="9">Not surprisingly, their performance is quite poor: if a word is assigned all possible tags, the search space for the disambiguation of a single POS-tag increases and makes it fragile; if every unknown word is classified as a noun, there will be no difficulties for disambiguation but accuracy will suffer&#8212;such a guess is not reliable enough.</S>
    <S sid="20" ssid="10">To assign capitalized unknown words the category proper noun seems a good heuristic, but may not always work.</S>
    <S sid="21" ssid="11">As argued in Church (1988), who proposes a more elaborated heuristic, Dermatas and Kokkinakis (1995) proposed a simple probabilistic approach to unknown-word guessing: verb present, 3d person verb, present, non-3d Example take took taking taken takes take Meaning Example Tag the probability that an unknown word has a particular POS-tag is estimated from the probability distribution of hapax words (words that occur only once) in the previously seen texts.'</S>
    <S sid="22" ssid="12">Whereas such a guesser is more accurate than the naive assignments and easily trainable, the tagging performance on unknown words is reported to be only about 66% correct for English.2 More advanced word-guessing methods use word features such as leading and trailing word segments to determine possible tags for unknown words.</S>
    <S sid="23" ssid="13">Such methods can achieve better performance, reaching tagging accuracy of up to 85% on unknown words for English (Brill 1994; Weischedel et al. 1993).</S>
    <S sid="24" ssid="14">The Xerox tagger (Cutting et al. 1992) comes with a set of rules that assign an unknown word a set of possible POS-tags (i.e., Pos-class) on the basis of its ending segment.</S>
    <S sid="25" ssid="15">We call such rules endingguessing rules because they rely only on ending segments in their predictions.</S>
    <S sid="26" ssid="16">For example, an ending-guessing rule can predict that a word is a gerund or an adjective if it ends with ing.</S>
    <S sid="27" ssid="17">The ending-guessing approach was elaborated in Weischedel et al. (1993), where an unknown word was guessed by using the probability for an unknown word to be of a particular POS-tag, given its capitalization feature and its ending.</S>
    <S sid="28" ssid="18">Brill (1994, 1995) describes a system of rules that uses both ending-guessing and more morphologically motivated rules.</S>
    <S sid="29" ssid="19">A morphological rule, unlike an ending-guessing rule, uses information about morphologically related words already known to the lexicon in its prediction.</S>
    <S sid="30" ssid="20">For instance, a morphologically motivated guessing rule can say that a word is an adjective if adding the suffix /y to it will result in a word.</S>
    <S sid="31" ssid="21">Clearly, ending-guessing rules have wider coverage than morphologically oriented ones, but their predictions can be less accurate.</S>
    <S sid="32" ssid="22">The major topic in the development of word-Pos guessers is the strategy used for the acquisition of the guessing rules.</S>
    <S sid="33" ssid="23">A rule-based tagger described in Voutilainen (1995) was equipped with a set of guessing rules that had been hand-crafted using knowledge of English morphology and intuitions.</S>
    <S sid="34" ssid="24">A more appealing approach is automatic acquisition of such rules from available lexical resources, since it is usually less labor-intensive and less error-prone.</S>
    <S sid="35" ssid="25">Zhang and Kim (1990) developed a system for automated learning of morphological word formation rules.</S>
    <S sid="36" ssid="26">This system divides a string into three regions and infers from training examples their correspondence to underlying morphological features.</S>
    <S sid="37" ssid="27">Kupiec (1992) describes a guessing component that uses a prespecified list of suffixes (or rather endings) and then statistically learns the predictive properties of those endings from an untagged corpus.</S>
    <S sid="38" ssid="28">In Brill (1994, 1995) a transformation-based learner that learns guessing rules from a pretagged training corpus is outlined: First the unknown words are labeled as common nouns and a list of generic transformations is defined.</S>
    <S sid="39" ssid="29">Then the learner tries to instantiate the generic transformations with word features observed in the text.</S>
    <S sid="40" ssid="30">A statistical-based suffix learner is presented in Schmid (1994).</S>
    <S sid="41" ssid="31">From a training corpus, it constructs a suffix tree where every suffix is associated with its information measure to emit a particular Pos-tag.</S>
    <S sid="42" ssid="32">Although the learning process in these systems is fully automated and the accuracy of obtained guessing rules reaches current state-of-the-art levels, for estimation of their parameters they require significant amounts of specially prepared training data&#8212;a large training corpus (usually pretagged), training examples, and so on.</S>
    <S sid="43" ssid="33">In this paper, we describe a novel, fully automatic technique for the induction of POS-class-guessing rules for unknown words.</S>
    <S sid="44" ssid="34">This technique has been partially outlined in (Mikheev 1996a, 1996b) and, along with a level of accuracy for the induced rules that is higher than any previously quoted, it has an advantage in terms of quantity and simplicity of annotation of data for training.</S>
    <S sid="45" ssid="35">Unlike many other approaches, which implicitly or explicitly assume that the surface manifestations of morpho-syntactic features of unknown words are different from those of general language, we argue that within the same language unknown words obey general morphological regularities.</S>
    <S sid="46" ssid="36">In our approach, we do not require large amounts of annotated text but employ fully automatic statistical learning using a pre-existing general-purpose lexicon mapped to a particular tag set and word-frequency distribution collected from a raw corpus.</S>
    <S sid="47" ssid="37">The proposed technique is targeted to the acquisition of both morphological and ending-guessing rules, which then can be applied cascadingly using the most accurate guessing rules first.</S>
    <S sid="48" ssid="38">The rule induction process is guided by a thorough guessing-rule evaluation methodology that employs precision, recall, and coverage as evaluation metrics.</S>
    <S sid="49" ssid="39">In the rest of the paper we first introduce the kinds of guessing rules to be induced and then present a semi-unsupervised' statistical rule induction technique using data derived from the CELEX lexical database (Burnage 1990).</S>
    <S sid="50" ssid="40">Finally we evaluate the induced guessing rules by removing all the hapax words from the lexicon and tagging the Brown Corpus (Francis and Kucera 1982) by a stochastic tagger and a rule-based tagger.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="51" ssid="1">There are two kinds of word-guessing rules employed by our cascading guesser: morphological rules and nonmorphological ending-guessing rules.</S>
    <S sid="52" ssid="2">Morphological wordguessing rules describe how one word can be guessed given that another word is known.</S>
    <S sid="53" ssid="3">Unlike morphological guessing rules, nonmorphological rules do not require the base form of an unknown word to be listed in the lexicon.</S>
    <S sid="54" ssid="4">Such rules guess the Pos-class for a word on the basis of its ending or leading segments alone.</S>
    <S sid="55" ssid="5">This is especially important when dealing with uninflected words and domain-specific sublanguages where many highly specialized words can be encountered.</S>
    <S sid="56" ssid="6">In English, as in many other languages, morphological word formation is realized by affixation: prefixation and suffixation.</S>
    <S sid="57" ssid="7">Thus, in general, each kind of guessing rule can be further subcategorized depending on whether it is applied to the beginning or tail of an unknown word.</S>
    <S sid="58" ssid="8">To mirror this classification, we will introduce a general schemata for guessing rules and a guessing rule will be seen as a particular instantiation of this schemata.</S>
    <S sid="59" ssid="9">A guessing-rule schemata is a structure G =x:{b,e} [&#8212;S +M ?I-class &#8212;4Z-class] where For example, the rule e[&#8212;ied +y ?</S>
    <S sid="60" ssid="10">(VB VBP) &#8212;*(JJ VBD VBN)] says that if there is an unknown word which ends with ied, we should strip this ending from it and append the string y to the remaining part.</S>
    <S sid="61" ssid="11">If we then find this word in the lexicon as (VB VBP) (base verb or verb of present tense non-3d form), we conclude that the unknown word is of the category (II VBD VBN) (adjective, past verb, or participle).</S>
    <S sid="62" ssid="12">Thus, for instance, if the word specified was unknown to the lexicon, this rule first would try to segment the required ending ied (specified &#8212; ied = specif), then add to the result the mutative segment y (specif + y = specify), and, if the word specify was found in the lexicon as (VB VBP), the unknown word specified would be classified as (II VBD VBN).</S>
    <S sid="63" ssid="13">Since the mutative segment can be an empty string, regular morphological formations can be captured as well.</S>
    <S sid="64" ssid="14">For instance, the rule says that if segmenting the prefix un from an unknown word results in a word that is found in the lexicon as a past verb and participle (VBD VBN), we conclude that the unknown word is an adjective (n).</S>
    <S sid="65" ssid="15">This rule will, for instance, correctly classify the word unscrewed if the word screwed is listed in the lexicon as (VBD VBN).</S>
    <S sid="66" ssid="16">When setting the S segment to an empty string and the M segment to a non-empty string, the schemata allows for cases when a secondary form is listed in the lexicon and the base form is not.</S>
    <S sid="67" ssid="17">For instance, the rule e[&#8212;&amp;quot;&amp;quot; +ed ?</S>
    <S sid="68" ssid="18">(VBD VBN) &#8212;(VB VBP)] says that if adding the segment ed to the end of an unknown word results in a word that is found in the lexicon as a past verb and participle (VBD VBN), then the unknown word is a base or non-3d present verb (VB VBP).</S>
    <S sid="69" ssid="19">The general schemata can also capture ending-guessing rules if the /-class is set to be &amp;quot;void.&amp;quot; This indicates that no stem lookup is required.</S>
    <S sid="70" ssid="20">Naturally, the mutative segment of such rules is always set to an empty string.</S>
    <S sid="71" ssid="21">For example, an endingguessing rule e[&#8211;ing +&amp;quot; ?&#8212; &#8211;(JJ NN VBG)} says that if a word ends with ing it can be an adjective, a noun, or a gerund.</S>
    <S sid="72" ssid="22">Unlike a morphological rule, this rule does not check whether the substring preceding the ing-ending is listed in the lexicon with a particular Pos-class.</S>
    <S sid="73" ssid="23">The proposed guessing-rule schemata is in fact quite similar to the set of generic transformations for unknown-word guessing developed by Brill (1995).</S>
    <S sid="74" ssid="24">There are, however, three major differences: Brill's system has two transformations that our schemata do not capture: when a particular character appears in a word and when a word appears in a particular context.</S>
    <S sid="75" ssid="25">The latter transformation is, in fact, due to the peculiarities of Brill's tagging algorithm and, in other approaches, is captured at the disambiguation phase of the tagger itself.</S>
    <S sid="76" ssid="26">The former feature is indirectly captured in our approach.</S>
    <S sid="77" ssid="27">It has been noticed (as in [Weischedel et al., 1993], for example) that capitalized and hyphenated words have a different distribution from other words.</S>
    <S sid="78" ssid="28">Our morphological rules account for this difference by checking the stem of the word.</S>
    <S sid="79" ssid="29">The ending-guessing rules, on the other hand, do not use information about stems.</S>
    <S sid="80" ssid="30">Thus if the ending s predicts that a word can be a plural noun or a 3d form of a verb, the information that this word was capitalized can narrow the considered set of POS-tags to plural proper noun.</S>
    <S sid="81" ssid="31">We therefore decided to collect ending-guessing rules separately for capitalized words, hyphenated words, and all other words.</S>
    <S sid="82" ssid="32">In our experiments, we restricted ourselves to the production of six different guessing-rule sets, which seemed most appropriate for English: As already mentioned, we see features that our guessing-rule schemata is intended to capture as general language regularities rather than properties of rare or corpusspecific words only.</S>
    <S sid="83" ssid="33">This significantly simplifies training data requirements: we can induce guessing rules from a general-purpose lexicon.'</S>
    <S sid="84" ssid="34">First, we no longer depend on the size or even existence of an annotated training corpus.</S>
    <S sid="85" ssid="35">Second, we do not require any annotation to be done for the training; instead, we reuse the information stated in the lexicon, which we can automatically map to a particular tag set that a tagger is trained to.</S>
    <S sid="86" ssid="36">We also use the actual frequencies of word usage, collected from a raw corpus.</S>
    <S sid="87" ssid="37">This allows for the discrimination between rules that are no longer productive (but have left their imprint on the basic lexicon) and rules that are productive in real-life texts.</S>
    <S sid="88" ssid="38">For guessing rules to capture general language regularities, the lexicon should be as general as possible (i.e., should list all possible POS-tags for a word) and large.</S>
    <S sid="89" ssid="39">The corresponding corpus should also be large enough to obtain reliable estimates of word-frequency distribution for at least 10,000-15,000 words.</S>
    <S sid="90" ssid="40">Since a word can take on several different POS-tags, in the lexicon it can be represented as a [string/Pos-class] record, where the Pos-class is a set of one or more POS-tags.</S>
    <S sid="91" ssid="41">For instance, the entry for the word book, which can be a noun (NN) or a verb (VB) would look like [book (NN VB)].</S>
    <S sid="92" ssid="42">Thus the nth entry of the lexicon (wn) can be represented as [W C], where W is the surface lexical form and C is its Pos-class.</S>
    <S sid="93" ssid="43">Different lexicon entries can share the same Pos-class but they cannot share the same surface lexical form.</S>
    <S sid="94" ssid="44">In our experiments, we used a lexicon derived from CELEX (Burnage 1990), a large multilingual database that includes extensive lexicons of English, Dutch, and German.</S>
    <S sid="95" ssid="45">We constructed an English lexicon of 72,136 word forms with morphological features, which we then mapped into the Penn Treebank tag set (Marcus, Marcinkiewicz, and Santorini 1993).</S>
    <S sid="96" ssid="46">The most frequent open-class tags of this tag set are shown in Table 1.</S>
    <S sid="97" ssid="47">Word-frequency distribution was estimated from the Brown Corpus, which reflects multidomain language use.</S>
    <S sid="98" ssid="48">As usual, we separated the test sample from the training sample.</S>
    <S sid="99" ssid="49">Here we followed the suggestion that the unknown words actually are quite similar to words that occur only once (hapax words) in the corpus (Dermatas and Kokkinakis 1995; Baayen and Sproat 1995).</S>
    <S sid="100" ssid="50">We put all the hapax words from the Brown Corpus that were found in the cELEx-derived lexicon into the test collection (test lexicon) and all other words from the cELEx-derived lexicon into the training lexicon.</S>
    <S sid="101" ssid="51">In the test lexicon, we also included the hapax words not found in the cELEx-derived lexicon, assigning them the POS-tags they had in the Brown Corpus.</S>
    <S sid="102" ssid="52">Then we filtered out words shorter than four characters, nonwords such as numbers or alpha-numerals, which usually are handled at the tokenization phase, and all closed-class words,' which we assume will always be present in the lexicon.</S>
    <S sid="103" ssid="53">Thus after all these transformations we obtained a lexicon of 59,268 entries for training and the test lexicon of 17,868 entries.</S>
    <S sid="104" ssid="54">Our guessing-rule induction technique uses the training and test data prepared as described above and can be seen as a sampling for the best performing rule set from a collection of automatically produced rule sets.</S>
    <S sid="105" ssid="55">Here is a brief outline of its major phases: For the extraction of the initial sets of prefix and suffix morphological guessing rules (Prefix, Suffix&#176;, and Suffixl), we define the operator vn where the index n specifies the length of the mutative ending of the main word.</S>
    <S sid="106" ssid="56">Thus when the index n is set to 0 the result of the application of the vo operator will be a morphological rule with no mutative segment.</S>
    <S sid="107" ssid="57">The vi operator will extract the rules with the alterations in the last letter of the main word.</S>
    <S sid="108" ssid="58">When the v operator is applied to a pair of entries from the lexicon ([W C], and [W C]1), first, it segments the last (or first) n characters of the shorter word (W)) and stores this in the M element of the rule.</S>
    <S sid="109" ssid="59">Then it tries to segment an affix by subtracting the shorter word (WI) without the mutative ending from the longer word (W1).</S>
    <S sid="110" ssid="60">If the subtraction results in an non-empty string and the mutative segment is not duplicated in the affix, the system creates a morphological rule with the Pos-class of the shorter word (CI) as the I-class, the Pos-class of the longer word (C1) as the R-class and the segmented affix itself in the S field.</S>
    <S sid="111" ssid="61">For example: [booked (ll VBD VBN)] Vo [book (NN VB)] e[&#8212;ed +&amp;quot;&amp;quot; ?</S>
    <S sid="112" ssid="62">(NN VB) &#8212;(11 VBD VBN)] [advisable (JJ)] Vi [advise (NN VB)] el&#8212;able +&amp;quot;e&amp;quot; ?</S>
    <S sid="113" ssid="63">(NN VB) The v operator is applied to all possible pairs of lexical entries sequentially, and, if a rule produced by such an application has already been extracted from another pair, its frequency count (f) is incremented.</S>
    <S sid="114" ssid="64">Thus, prefix and suffix morphological rules together with their frequencies are produced.</S>
    <S sid="115" ssid="65">Next, we cut out the most infrequent rules, which might bias further learning.</S>
    <S sid="116" ssid="66">To do that we eliminate all the rules with frequency f less than a certain threshold 0, which usually is set quite low: 2-4.</S>
    <S sid="117" ssid="67">Such filtering reduces the rule sets more than tenfold.</S>
    <S sid="118" ssid="68">To collect the ending-guessing rules, we set the upper limit on the ending length equal to five characters and thus collect from the lexicon all possible word-endings of length 1, 2, 3, 4, and 5, together with the pos-classes of the words in which these endings appeared.</S>
    <S sid="119" ssid="69">We also set the minimum length of the remaining substring to three characters.</S>
    <S sid="120" ssid="70">We define the unary operator A, which produces a set of ending-guessing rules from a word in the lexicon ([W Cb).</S>
    <S sid="121" ssid="71">For instance, from a lexicon entry [different (JN the operator A will produce five ending-guessing rules: The A operator is applied to each entry in the lexicon, and if a rule it produces has already been extracted from another entry in the lexicon, its frequency count (f) is incremented.</S>
    <S sid="122" ssid="72">Then the infrequent rules with f &lt; 19 are eliminated from the endingguessing rule set.</S>
    <S sid="123" ssid="73">After applying the A and v operations to the training lexicon, we obtained rule collections of 40,000-50,000 entries.</S>
    <S sid="124" ssid="74">Filtering out the rules with frequency counts of 1 reduced the collections to 5,000-7,000 entries.</S>
    <S sid="125" ssid="75">Of course, not all acquired rules are equally good at predicting word classes: some rules are more accurate in their guesses and some rules are more frequent in their application.</S>
    <S sid="126" ssid="76">For every rule acquired, we need to estimate whether it is an effective rule worth retaining in the working rule set.</S>
    <S sid="127" ssid="77">To do so, we perform a statistical experiment as follows: we take each rule from the extracted rule sets, one by one, take each wordtype from the training lexicon and guess its Pos-class using the rule, if the rule is applicable to the word.</S>
    <S sid="128" ssid="78">For example, if a guessing rule strips off a particular suffix and a current word from the lexicon does not have this suffix, we classify that word and the rule as incompatible and the rule as not applicable to that word.</S>
    <S sid="129" ssid="79">If a rule is applicable to a word, we compare the result of the guess with the information listed in the lexicon.</S>
    <S sid="130" ssid="80">If the guessed class is the same as the class stated in the lexicon, we count it as a hit or success, otherwise it is a failure.</S>
    <S sid="131" ssid="81">Then, since we are interested in the application of the rules to word-tokens in the corpus, we multiply the result of the guess by the corpus frequency of the word.</S>
    <S sid="132" ssid="82">If we keep the sample space for each rule separate from the others, we have a binomial experiment.</S>
    <S sid="133" ssid="83">The value of a guessing rule closely correlates with its estimated proportion of success (p), which is the proportion of all positive outcomes (x) of the rule application to the total number of the trials (n), which are, in fact, the number of all the word tokens that are compatible to the rule in the corpus: x: number of successful guesses The p estimate is a good indicator of the rule accuracy but it frequently suffers from large estimation error due to insufficient training data.</S>
    <S sid="134" ssid="84">For example, if a rule was found to apply just once and the total number of observations was also one, its estimate p has the maximal value (1) but clearly this is not a very reliable estimate.</S>
    <S sid="135" ssid="85">We tackle this problem by calculating the lower confidence limit 711 for the rule estimate, which can be seen as the minimal expected value of p for the rule if we were to draw a large number of samples.</S>
    <S sid="136" ssid="86">Thus with a certain confidence a we can assume that if we used more training data, the rule estimate 19 would be not worse than the irk.</S>
    <S sid="137" ssid="87">The rule estimate then will be taken at its lowest possible value which is the lrL limit itself.</S>
    <S sid="138" ssid="88">First we adjust the rule estimate so that we have no zeros in positive (f9) or negative (1 &#8212; p) outcome probabilities, by adding some floor values to the numerator and denominator: where t(1-a)/2 is a coefficient of the t-distribution.</S>
    <S sid="139" ssid="89">It has two parameters: a, the level of confidence and df, the number of degrees of freedom, which is one less than the sample size (df = n -1). td(c_a)/2 can be looked up in the tables for the t-distribution listed in every textbook on statistics.</S>
    <S sid="140" ssid="90">We adopted 90% confidence for which tdc-c_0.90)/2=todf.05 takes values depending on the sample size as in Figure 1.</S>
    <S sid="141" ssid="91">Using IrL instead of /5 for rule scoring favors higher estimates (15) obtained over larger samples (n).</S>
    <S sid="142" ssid="92">Even if one rule has a high estimate value but that estimate was obtained over a small sample, another rule with a lower estimate value but obtained over a large sample might be valued higher by rL.</S>
    <S sid="143" ssid="93">This rule-scoring function resembles the one used by Tzoukermann, Radev, and Gale (1995) for scoring Pos-disambiguation rules for the French tagger.</S>
    <S sid="144" ssid="94">The main difference between the two functions is that there the t value was implicitly assumed to be 1, which corresponds to a confidence level of 68% on a very large sample.</S>
    <S sid="145" ssid="95">Another important consideration for rating a word-guessing rule is that the longer the affix or ending (S) of this rule, the more confident we are that it is not a coincidental one, even on small samples.</S>
    <S sid="146" ssid="96">For example, if the estimate for the word-ending o was obtained over a sample of five words and the estimate for the word-ending fulness was also obtained over a sample of five words, the latter is more representative, even though the sample size is the same.</S>
    <S sid="147" ssid="97">Thus we need to adjust the estimation error in accordance with the length of the affix or ending.</S>
    <S sid="148" ssid="98">A good way to do this is to decrease it proportionally to a value that increases along with the increase of the length.</S>
    <S sid="149" ssid="99">A suitable solution is to use the logarithm of the affix length: When the length of S (the affix or ending) is 1, the estimation error is not changed since log(1) is 0.</S>
    <S sid="150" ssid="100">For the rules with an affix or ending length of 2 the estimation error is reduced by 1 + log(2) = 1.3, for the length 3 this will be 1 + log(3) = 1.48, etc.</S>
    <S sid="151" ssid="101">The longer the length, the smaller the sample that will be considered representative enough for a confident rule estimation.</S>
    <S sid="152" ssid="102">Setting the threshold (8s) at a certain level we include in the working rule sets only those rules whose scores are higher than the threshold.</S>
    <S sid="153" ssid="103">The method for finding the optimal threshold is based on empirical evaluations of the rule sets and is described in Section 3.4.</S>
    <S sid="154" ssid="104">Usually, the threshold is set in the range of 65-80 points and the rule sets are reduced down to a few hundred entries.</S>
    <S sid="155" ssid="105">For example, when we set the threshold (Os) to 75 points, the obtained ending-guessing rule collection (Ending*) comprised 1,876 rules, the suffix rule collection without mutation (Suffix&#176;) comprised 591 rules, the suffix rule collection with mutation (Suffixi) comprised 912 entries and the prefix rule collection (Prefix) comprised 235 rules.</S>
    <S sid="156" ssid="106">Table 2 shows the highest-rated rules from the induced Prefix and Suffix&#176; rule sets.</S>
    <S sid="157" ssid="107">In general, it looks as though the induced morphological guessing rules largely consist of the standard rules of English morphology and also include a small proportion of rules that do not belong to the known morphology of English.</S>
    <S sid="158" ssid="108">For instance, the suffix rule a -et +&amp;quot; ?</S>
    <S sid="159" ssid="109">(NN) &#8212;*(NN)] does not stand for any well-known morphological rule, but its prediction is as good as those of the standard morphological rules.</S>
    <S sid="160" ssid="110">The same situation can be seen with the prefix rule b[ -St +&amp;quot;&amp;quot; ?</S>
    <S sid="161" ssid="111">(NNS) &#8212;*(NNS)], which is quite predictive but at the same time is not a standard English morphological rule.</S>
    <S sid="162" ssid="112">The ending-guessing rules, naturally, include some proper English suffixes but mostly they are simply highly predictive ending segments of words.</S>
    <S sid="163" ssid="113">Rules which have scored lower than the threshold are merged together into more general rules.</S>
    <S sid="164" ssid="114">These new rules, if they score above the threshold, can also be included in the working rule sets.</S>
    <S sid="165" ssid="115">We merge together two rules if they scored below the threshold and have the same affix (S), mutative segment (M), and initial class (46 We define the rule-merging operator e: A, e A, Ar: [S1,M,I,RUR1I if S, = Si &amp; M, = MI &amp; I, = This operator merges two rules with the same affix (S), mutative segment (M) and the initial class (/) into one rule, with the resulting class being the union of the two merged resulting classes.</S>
    <S sid="166" ssid="116">For example, Lexicon entry and guesser's categorization for [developed (1.1 VBD VBN)].</S>
    <S sid="167" ssid="117">The score of the resulting rule will be higher than the scores of the individual rules since the number of positive observations increases and the number of the trials remains the same.</S>
    <S sid="168" ssid="118">After a successful application of the ED operator, the resulting general rule is substituted for the two merged ones.</S>
    <S sid="169" ssid="119">To perform such rule merging over a rule set the rules that have not been included into the working rule set are first sorted by their score and the rules with the best scores are merged first.</S>
    <S sid="170" ssid="120">After each successful merging, the resulting rule is rescored.</S>
    <S sid="171" ssid="121">This is done recursively until the score of the resulting rule does not exceed the threshold, at which point it is added to the working rule sets.</S>
    <S sid="172" ssid="122">This process is applied until no merges can be done to the rules that scored poorly.</S>
    <S sid="173" ssid="123">In our experiment we noticed that the merging added 30-40% new rules to the working rule sets, and therefore the final number of rules for the induced sets were: Prefix &#8212; 348, Suffix&#176; &#8212; 975, Suffixl&#8212; 1,263 and Ending* &#8212; 2,196.</S>
    <S sid="174" ssid="124">There are two important questions that arise at the rule acquisition stage: how to choose the scoring threshold 0, and what the performance of the rule sets produced with different thresholds is.</S>
    <S sid="175" ssid="125">The task of assigning a set of PUS-tags to a word is actually quite similar to the task of document categorization where a document is assigned a set of descriptors that represent its contents.</S>
    <S sid="176" ssid="126">There are a number of standard parameters (Lewis 1991) used for measuring performance on this kind of task.</S>
    <S sid="177" ssid="127">For example, suppose that a word can take on one or more PUS-tags from the set of open-class POS-tags: (JJ NN NNS RB VB VBD VBG VBN VBZ).</S>
    <S sid="178" ssid="128">To see how well the guesser performs, we can compare the results of the guessing with the PUS-tags known to be true for the word (i.e., listed in the lexicon).</S>
    <S sid="179" ssid="129">Let us take, for instance, a lexicon entry [developed (JJ VBD VBN)].</S>
    <S sid="180" ssid="130">Suppose that the guesser categorized it as [developed (JJ NN RB VBD VBZ)].</S>
    <S sid="181" ssid="131">We can represent this situation as in Figure 2.</S>
    <S sid="182" ssid="132">The performance of the guesser can be measured in: The interpretation of these percentages is by no means straightforward, as there is no straightforward way of combining these different measures into a single one.</S>
    <S sid="183" ssid="133">For example, these measures assume that all combinations of Pips-tags will be equally hard to disambiguate for the tagger, which is not necessarily the case.</S>
    <S sid="184" ssid="134">Obviously, the most important measure is recall since we want all possible categories for a word to be guessed.</S>
    <S sid="185" ssid="135">Precision seems to be slightly less important since the disambiguator should be able to handle additional noise but obviously not in large amounts.</S>
    <S sid="186" ssid="136">Coverage is a very important measure for a rule set, since a rule set that can guess very accurately but only for a tiny proportion of words is of questionable value.</S>
    <S sid="187" ssid="137">Thus, we will try to maximize recall first, then coverage, and, finally, precision.</S>
    <S sid="188" ssid="138">We will measure the aggregate by averaging over measures per word (micro-average), i.e., for every single word from the test collection the precision and recall of the guesses are calculated, and then we average over these values.</S>
    <S sid="189" ssid="139">To find the optimal threshold (0,) for the production of a guessing rule set, we generated a number of similar rule sets using different thresholds and evaluated them against the training lexicon and the test lexicon of unseen 17,868 hapax words.</S>
    <S sid="190" ssid="140">Every word from the two lexicons was guessed by a rule set and the results were compared with the information the word had in the lexicon.</S>
    <S sid="191" ssid="141">For every application of a rule set to a word, we computed the precision and recall, and then using the total number of guessed words we computed the coverage.</S>
    <S sid="192" ssid="142">We noticed certain regularities in the behavior of the metrics in response to the change of the threshold: recall improves as the threshold increases while coverage drops proportionally.</S>
    <S sid="193" ssid="143">This is not surprising: the higher the threshold, the fewer the inaccurate rules included in the rule set, but at the same time the fewer the words that can be handled.</S>
    <S sid="194" ssid="144">An interesting behavior is shown by precision: first, it grows proportionally along with the increase of the threshold, but then, at high thresholds, it decreases.</S>
    <S sid="195" ssid="145">This means that among very confident rules with very high scores, there are many quite general ones.</S>
    <S sid="196" ssid="146">The best thresholds were obtained in the range of 70-80 points.</S>
    <S sid="197" ssid="147">Table 3 displays the metrics for the best-scored (by aggregate of the three metrics on the training and the test samples) rule sets.</S>
    <S sid="198" ssid="148">As the baseline standard, we took the ending-guessing rule set supplied with the Xerox tagger (Cutting et al. 1992).</S>
    <S sid="199" ssid="149">When we compared the Xerox ending guesser with the induced ending-guessing rule set (Ending*), we saw that its precision was about 6% poorer and, most importantly, it could handle 6% fewer unknown words.</S>
    <S sid="200" ssid="150">Finally, we measured the performance of the cascading application of the induced rule sets when the morphological guessing rules were applied before the ending-guessing rules (Prefix+Suffix4Suffixl+Ending-c*).</S>
    <S sid="201" ssid="151">We detected that the cascading application of the morphological rule sets together with the ending-guessing rules increases the overall precision of the guessing by about 8%.</S>
    <S sid="202" ssid="152">This made the improvement over the baseline Xerox guesser 13% in precision and 7% in coverage on the test sample.</S>
  </SECTION>
  <SECTION title="4." number="4">
    <S sid="203" ssid="1">The direct evaluation phase gave us a basis for setting the threshold to produce the best-performing rule sets.</S>
    <S sid="204" ssid="2">The task of unknown-word guessing is, however, a subtask of the overall part-of-speech tagging process.</S>
    <S sid="205" ssid="3">Our main interest is in how the advantage of one rule set over another will affect the tagging performance.</S>
    <S sid="206" ssid="4">Therefore, we performed an evaluation of the impact of the word guessers on tagging accuracy.</S>
    <S sid="207" ssid="5">In this evaluation we used the cascading guesser with two different taggers: a c++ implemented bigram HMM tagger akin to one described in Kupiec (1992) and the rule-based tagger of Brill (1995).</S>
    <S sid="208" ssid="6">Because of the similarities in the algorithms with the LISP implemented Xerox tagger, we could directly use the Xerox guessing rule set with the HMM tagger.</S>
    <S sid="209" ssid="7">Brill's tagger came pretrained on the Brown Corpus and had a corresponding guessing component.</S>
    <S sid="210" ssid="8">This gave us a search-space of four basic combinations: the HMM tagger equipped with the Xerox guesser, the Brill tagger with its original guesser, the HMM tagger with our cascading (Prefix+Suffix&#176;+Suffixl+Ending-c*) guesser and the Brill tagger with the cascading guesser.</S>
    <S sid="211" ssid="9">We also tried hybrid tagging using the output of the HMM tagger as the input to Brill's final state tagger, but it gave poorer results than either of the taggers and we decided not to consider this tagging option.</S>
    <S sid="212" ssid="10">We evaluated the taggers with the guessing components on all fifteen subcorpora of the Brown Corpus, one after another.</S>
    <S sid="213" ssid="11">The HMM tagger was trained on the Brown Corpus in such a way that the subcorpus used for the evaluation was not seen at the training phase.</S>
    <S sid="214" ssid="12">All the hapax words and capitalized words with frequency less than 20 were not seen at the training of the cascading guesser.</S>
    <S sid="215" ssid="13">These words were not used in the training of the tagger either.</S>
    <S sid="216" ssid="14">This means that neither the HMM tagger nor the cascading guesser had been trained on the texts and words used for evaluation.</S>
    <S sid="217" ssid="15">We do not know whether the same holds for the Brill tagger and the Brill and Xerox guessers since we took them pretrained.</S>
    <S sid="218" ssid="16">For words that the guessing components failed to guess, we applied the standard method of classifying them as common nouns (NN) if they were not capitalized inside a sentence and proper nouns (NNP) otherwise.</S>
    <S sid="219" ssid="17">When we used the cascading guesser with the Brill tagger we interfaced them on the level of the lexicon: we guessed the unknown words before the tagging and added them to the lexicon listing the most likely tags first as required.'</S>
    <S sid="220" ssid="18">Here we want to clarify that we evaluated the overall results of the Brill tagger rather than just its unknown-word tagging component.</S>
    <S sid="221" ssid="19">Another point to mention is that, since we included the guessed words in the lexicon, the Brill tagger could use for the transformations all relevant POStags for unknown words.</S>
    <S sid="222" ssid="20">This is quite different from the output of the original Brill's guesser, which provides only one POS-tag for an unknown word.</S>
    <S sid="223" ssid="21">In our tagging experiments, we measured the error rate of tagging on unknown words using different guessers.</S>
    <S sid="224" ssid="22">Since, arguably, the guessing of proper nouns is easier than is the guessing of other categories, we also measured the error rate for the subcategory of capitalized unknown words separately.</S>
    <S sid="225" ssid="23">The error rate for a category of words was calculated as follows: Total _Words _in _Set _X Wrongly _Tagged _Words _from _Set _X Thus, for instance, the error rate of tagging the unknown words is the proportion of the mistagged unknown words to all unknown words.</S>
    <S sid="226" ssid="24">To see the distribution of the workload between different guessing rule sets we also measured the coverage of a guessing rule set: We collected the error and coverage measures for each of the fifteen subcorpora8 of the Brown Corpus separately, and, using the bootstrap replicate technique (Efron and Tibshirani 1993), we calculated the mean and the standard error for each combination of the taggers with the guessing components.</S>
    <S sid="227" ssid="25">For the fifteen accuracy means {cii, J12, , (65} obtained upon tagging the fifteen subcorpora of the Brown Corpus, we generated a large number of bootstrap replicates of the form {b1, b2,.</S>
    <S sid="228" ssid="26">, b15} where each mean was randomly chosen with replacements such as, for instance, Using these replicates, we calculated the mean and standard error of the whole bootstrap distribution as follows: where This way of calculating the estimated standard error for the mean does not assume the normal distribution and hence provides more accurate results.</S>
    <S sid="229" ssid="27">We noticed a certain inconsistency in the markup of proper nouns (NNP) in the Brown Corpus supplied with the Penn Treebank.</S>
    <S sid="230" ssid="28">Quite often obvious proper nouns as, for instance, Summerdale, Russia, or Rochester were marked as common nouns (NN) and sometimes lower-cased common nouns such as business or church were marked as proper nouns.</S>
    <S sid="231" ssid="29">Thus we decided not to count as an error the mismatch of the NN/NNP tags.</S>
    <S sid="232" ssid="30">Using the HMM tagger with the lexicon containing all the words from the Brown Corpus, we obtained the error rate (mean) 0* (.</S>
    <S sid="233" ssid="31">)=4.003093 with the standard error s^eB=0.155599.</S>
    <S sid="234" ssid="32">This agrees with the results on the closed dictionary (i.e., without unknown words) obtained by other researchers for this class of the model on the same corpus (Kupiec 1992; DeRose 1988).</S>
    <S sid="235" ssid="33">The Brill tagger showed some better results: error rate (mean) O*(.</S>
    <S sid="236" ssid="34">)=3.327366 with the standard error &#167;63=0.123903.</S>
    <S sid="237" ssid="35">Although our primary goal was not to compare the taggers themselves but rather their performance with the guessing components, we attribute the difference in their performance to the fact that Brill's tagger uses the information about the most likely tag for a word whereas the HMM tagger did not have this information and instead used the priors for a set of POS-tags (ambiguity class).</S>
    <S sid="238" ssid="36">When we removed from the lexicon all the hapax words and, following the recommendation of Church (1988), all the capitalized words with frequency less than 20, we obtained some 51,522 unknown word-tokens (25,359 wordtypes) out of more than a million word-tokens in the Brown Corpus.</S>
    <S sid="239" ssid="37">We tagged the fifteen subcorpora of the Brown Corpus by the four combinations of the taggers and the guessers using the lexicon of 22,260 word-types.</S>
    <S sid="240" ssid="38">Table 4 displays the tagging results on the unknown words obtained by the four different combinations of taggers and guessers.</S>
    <S sid="241" ssid="39">It shows the overall error rate on unknown words and also displays the distribution of the error rate and the coverage between unknown proper nouns and the other unknown words.</S>
    <S sid="242" ssid="40">Indeed the error rate on the proper nouns was much smaller than on the rest of the unknown words, which means that they are much easier to guess.</S>
    <S sid="243" ssid="41">We can also see a difference in the distribution (coverage) of the unknown words using different taggers.</S>
    <S sid="244" ssid="42">This can be accounted for by the fact that the unguessed capitalized words were taken by default to be proper nouns and that the Brill tagger and the HMM tagger had slightly different strategies to apply to the first word of a sentence.</S>
    <S sid="245" ssid="43">The cascading guesser outperformed the other two guessers in general and most importantly in the non-proper noun category, where it had an advantage of 6.5% over Brill's guesser and about 8.7% over Xerox's guesser.</S>
    <S sid="246" ssid="44">In our experiments the category of unknown proper nouns had a larger share (6364%) than we expect in real life because all the capitalized words with frequency less than 20 were taken out of the lexicon.</S>
    <S sid="247" ssid="45">The cascading guesser also helped to improve the accuracy on unknown proper nouns by about 1% in comparison to Brill's guesser and about 3&#176;/0 in comparison to Xerox's guesser.</S>
    <S sid="248" ssid="46">The cascading guesser outperformed the other two guessers on every subcorpus of the Brown Corpus.</S>
    <S sid="249" ssid="47">Table 5 shows the distribution of the workload and the tagging accuracy among the different rule sets of the cascading guesser.</S>
    <S sid="250" ssid="48">The default assignment of the NN tag to unguessed words performed very poorly, having the error rate of 44%.</S>
    <S sid="251" ssid="49">When we compared this distribution to that of the Xerox guesser we saw that the accuracy of the Xerox guesser itself was only about 6.5% lower than that of the cascading guesser' and the fact that it could handle 6% fewer unknown words than the cascading guesser resulted in the increase of incorrect assignments by the default strategy.</S>
    <S sid="252" ssid="50">There were three types of mistaggings on unknown words detected in our experiments.</S>
    <S sid="253" ssid="51">Mistagging of the first type occurred when a guesser provided a broader POS-class for an unknown word than a lexicon would, and the tagger had difficulties with its disambiguation.</S>
    <S sid="254" ssid="52">This was especially the case with the words that were guessed as noun/adjective (NN 11) but, in fact, act only as one of them (as do, for example, many hyphenated words).</S>
    <S sid="255" ssid="53">Another highly ambiguous group is the ing words, which, in general, can act as nouns, adjectives, and gerunds and only direct lexicalization can restrict the search-space, as in the case of the word seeing, which cannot act as an adjective.</S>
    <S sid="256" ssid="54">The second type of mistagging was caused by incorrect assignments by the guesser.</S>
    <S sid="257" ssid="55">Usually this was the case with irregular words such as cattle or data, which were wrongly guessed to be singular nouns (NN) but in fact were plural nouns (NNs).</S>
    <S sid="258" ssid="56">We also did not include the &amp;quot;foreign word&amp;quot; category (my) in the set of tags to guess, but this did not do too much harm because these words were very infrequent in the texts.</S>
    <S sid="259" ssid="57">And the third type of mistagging occurred when the word-POS guesser assigned the correct Pos-class to a word but the tagger still disambiguated this class incorrectly.</S>
    <S sid="260" ssid="58">This was the most frequent type of error, which accounted for more than 60% of the mistaggings on unknown words.</S>
  </SECTION>
  <SECTION title="5." number="5">
    <S sid="261" ssid="1">We have presented a technique for fully automated statistical acquisition of rules that guess possible POS-tags for words unknown to the lexicon.</S>
    <S sid="262" ssid="2">This technique does not require specially prepared training data and uses for training a pre-existing generalpurpose lexicon and word frequencies collected from a raw corpus.</S>
    <S sid="263" ssid="3">Using such training data, three types of guessing rules are induced: prefix morphological rules, suffix morphological rules, and ending-guessing rules.</S>
    <S sid="264" ssid="4">Evaluation of tagging accuracy on unknown words using texts and words unseen at the training phase showed that tagging with the automatically induced cascading guesser was consistently more accurate than previously quoted results known to the author (85%).</S>
    <S sid="265" ssid="5">Tagging accuracy on unknown words using the cascading guesser was 87.7-88.7%.</S>
    <S sid="266" ssid="6">The cascading guesser outperformed the guesser supplied with the Xerox tagger and the guesser supplied with Brill's tagger both on unknown proper nouns (which is a relatively easy-to-guess category of words) and on the rest of the unknown words, where it had an advantage of 6.5-8.5.%.</S>
    <S sid="267" ssid="7">When the unknown words were made known to the lexicon, the accuracy of tagging was 93.6-94.3% which makes the accuracy drop caused by the cascading guesser to be less than 6% in general.</S>
    <S sid="268" ssid="8">Another important conclusion from the evaluation experiments is that the morphological guessing rules do improve guessing performance.</S>
    <S sid="269" ssid="9">Since they are more accurate than ending-guessing rules they were applied first and improved the precision of the guesses by about 8%.</S>
    <S sid="270" ssid="10">This resulted in about 2% higher accuracy in the tagging of unknown words.</S>
    <S sid="271" ssid="11">The ending-guessing rules constitute the backbone of the guesser and cope with unknown words without clear morphological structure.</S>
    <S sid="272" ssid="12">For instance, discussing the problem of unknown words for the robust parsing Bod (1995, 84) writes: &amp;quot;Notice that richer, morphological annotation would not be of any help here; the words &amp;quot;return&amp;quot;, &amp;quot;stop&amp;quot; and &amp;quot;cost&amp;quot; do not have a morphological structure on the basis of which their possible lexical categories can be predicted.&amp;quot; When we applied the ending-guessing rules to these words, the words return and stop were correctly classified as noun/verbs (NN vs vim)) and only the word cost failed to be guessed by the rules.</S>
    <S sid="273" ssid="13">The acquired guessing rules employed in our cascading guesser are, in fact, of a standard nature, which, in some form or other, is present in other word-Pos guessers.</S>
    <S sid="274" ssid="14">For instance, our ending-guessing rules are akin to those of Xerox and the morphological rules resemble some rules of Brill's, but ours use more constraints and provide a set of all possible tags for a word rather than a single best tag.</S>
    <S sid="275" ssid="15">The two additional types of features used by Brill's guesser are implicitly represented in our approach as well: One of the Brill schemata checks the context of an unknown word.</S>
    <S sid="276" ssid="16">In our approach we guess the words using their features only and provide several possibilities for a word; then at the disambiguation phase the context is used to choose the right tag.</S>
    <S sid="277" ssid="17">As for Brill's schemata that checks the presence of a particular character in an unknown word, we capture a similar feature by collecting the endingguessing rules for proper nouns and hyphenated words separately.</S>
    <S sid="278" ssid="18">We believe that the technique for the induction of the ending-guessing rules is quite similar to that of Xeroxl&#176; or Schmid (1994) but differs in the scoring and pruning methods.</S>
    <S sid="279" ssid="19">The major advantage of the proposed technique can be seen in the cascading application of the different sets of guessing rules and in far superior training data.</S>
    <S sid="280" ssid="20">We use for training a pre-existing general-purpose (as opposed to corpus-tuned) lexicon.</S>
    <S sid="281" ssid="21">This has three advantages: &#8226; the size of the training lexicon is large and does not depend on the size or even the existence of the annotated corpus.</S>
    <S sid="282" ssid="22">This allows for the induction of more rules than from a lexicon derived from an annotated corpus.</S>
    <S sid="283" ssid="23">For instance, the ending guesser of Xerox includes 536 rules whereas our Ending* guesser includes 2,196 guessing rules; &#8226; the information listed in a general-purpose lexicon can be considered to be of better quality than that derived from an annotated corpus, since it lists all possible readings for a word rather than only those that happen to occur in the corpus.</S>
    <S sid="284" ssid="24">We also believe that general-purpose lexicons contain less erroneous information than those derived from annotated corpora; &#8226; the amount of work required to prepare the training lexicon is minimal and does not require any additional manual annotation.</S>
    <S sid="285" ssid="25">Our experiments with the lexicon derived from the CELEX lexical database and word frequencies derived from the Brown Corpus resulted in guessing rule sets that proved to be domain- and corpus-independent (but tag-set-dependent), producing similar results on texts of different origins.</S>
    <S sid="286" ssid="26">An interesting by-product of the proposed rule-induction technique is the automatic discovery of the template morphological rules advocated in Mikheev and Liubushkina (1995).</S>
    <S sid="287" ssid="27">The induced morphological guessing rules turned out to consist mostly of the expected prefixes and suffixes of English and closely resemble the rules employed by the ispell UNIX spell-checker.</S>
    <S sid="288" ssid="28">The rule acquisition and evaluation methods described here are implemented as a modular set of c++ and AWK tools, and the guesser is easily extendible to sublanguage-specific regularities and retrainable to new tag sets and other languages, provided that these languages have affixational morphology.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="6">
    <S sid="289" ssid="1">I would like to thank the anonymous referees for helpful comments on an earlier draft of this paper.</S>
  </SECTION>
</PAPER>
