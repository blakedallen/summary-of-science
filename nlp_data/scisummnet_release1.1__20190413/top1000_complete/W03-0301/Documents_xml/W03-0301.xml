<PAPER>
  <S sid="0">An Evaluation Exercise For Word Alignment</S>
  <ABSTRACT>
    <S sid="1" ssid="1">BiBr.EF.1 BiBr.EF.2 BiBr.EF.3 BiBr.EF.4 BiBr.EF.5 BiBr.EF.6 BiBr.EF.7 BiBr.EF.8 Limited Unlimited Unlimited Limited Unlimited Unlimited Limited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP reverse direction of BiBr.EF.1 reverse direction of BiBr.EF.2 reverse direction of BiBr.EF.3 intersection of BiBr.EF.1 &amp; BiBr.EF.3 intersection of BiBr.EF.3 &amp; BiBr.EF.6 ProAlign.EF.1 Unlimited cohesion between source and target language + English parser + distributional similarity for English words Ralign.EF.1 Limited Giza (IBM Model 2) + recursive parallel segmentation UMD.EF.1 Limited IBM Model 2, trained with 1/20 of the corpus, distortion 2, iterations 4 XRCE.Base.EF.1 XRCE.Nolem.EF.2 XRCE.Nolem.EF.3 Limited GIZA++ (IBM Model 4) with English and French lemmatizer GIZA++ only (IBM Model 4), trained with 1/4 of the corpus GIZA++ only (IBM Model 4), trained with 1/2 of the corpus Table 2: Short description for English-French systems System Resources Description BiBr.RE.1 BiBr.RE.2 BiBr.RE.3 Limited Unlimited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP</S>
  </ABSTRACT>
  <SECTION title="1 Defining a Word Alignment Shared Task" number="1">
    <S sid="2" ssid="1">The task of word alignment consists of finding correspondences between words and phrases in parallel texts.</S>
    <S sid="3" ssid="2">Assuming a sentence aligned bilingual corpus in languages L1 and L2, the task of a word alignment system is to indicate which word token in the corpus of language L1 corresponds to which word token in the corpus of language L2.</S>
    <S sid="4" ssid="3">As part of the HLT/NAACL 2003 workshop on &#8221;Building and Using Parallel Texts: Data Driven Machine Translation and Beyond&#8221;, we organized a shared task on word alignment, where participating teams were provided with training and test data, consisting of sentence aligned parallel texts, and were asked to provide automatically derived word alignments for all the words in the test set.</S>
    <S sid="5" ssid="4">Data for two language pairs were provided: (1) EnglishFrench, representing languages with rich resources (20 million word parallel texts), and (2) Romanian-English, representing languages with scarce resources (1 million word parallel texts).</S>
    <S sid="6" ssid="5">Similar with the Machine Translation evaluation exercise organized by NIST1, two subtasks were defined, with teams being encouraged to participate in both subtasks. use any resources in addition to those provided.</S>
    <S sid="7" ssid="6">Such resources had to be explicitly mentioned in the system description.</S>
    <S sid="8" ssid="7">Test data were released one week prior to the deadline for result submissions.</S>
    <S sid="9" ssid="8">Participating teams were asked to produce word alignments, following a common format as specified below, and submit their output by a certain deadline.</S>
    <S sid="10" ssid="9">Results were returned to each team within three days of submission.</S>
    <S sid="11" ssid="10">The word alignment result files had to include one line for each word-to-word alignment.</S>
    <S sid="12" ssid="11">Additionally, lines in the result files had to follow the format specified in Fig.1.</S>
    <S sid="13" ssid="12">While the SIP and confidence fields overlap in their meaning, the intent of having both fields available is to enable participating teams to draw their own line on what they consider to be a Sure or Probable alignment.</S>
    <S sid="14" ssid="13">Both these fields were optional, with some standard values assigned by default.</S>
    <S sid="15" ssid="14">Consider the following two aligned sentences: where: o confidence is a real number, in the range (0-1] (1 meaning highly confident, 0 meaning not confident); this field is optional, and by default confidence number of 1 was assumed. aligned (English token 4 aligned with French token 4), and counts towards the final evaluation figures.</S>
    <S sid="16" ssid="15">Alternatively, systems could also provide an SIP marker and/or a confidence score, as shown in the following example: with missing SIP fields considered by default to be S, and missing confidence scores considered by default 1.</S>
    <S sid="17" ssid="16">The annotation guide and illustrative word alignment examples were mostly drawn from the Blinker Annotation Project.</S>
    <S sid="18" ssid="17">Please refer to (Melamed, 1999, pp.</S>
    <S sid="19" ssid="18">169&#8211;182) for additional details.</S>
    <S sid="20" ssid="19">[French]: &lt;s snum=18&gt; fixe moi ton salaire , et je te le donnerai .</S>
    <S sid="21" ssid="20">&lt;/s&gt; and he said from the English sentence has no corresponding translation in French, and therefore all these words are aligned with the token id 0.</S>
    <S sid="22" ssid="21">... 18 1 0 18 2 0 18 3 0 18 4 0 ... since the words do not correspond one to one, and yet the two phrases mean the same thing in the given context, the phrases should be linked as wholes, by linking each word in one to each word in another.</S>
    <S sid="23" ssid="22">For the example above, this translates into 12 wordto-word alignments:</S>
  </SECTION>
  <SECTION title="2 Resources" number="2">
    <S sid="24" ssid="1">The shared task included two different language pairs: the alignment of words in English-French parallel texts, and in Romanian-English parallel texts.</S>
    <S sid="25" ssid="2">For each language pair, training data were provided to participants.</S>
    <S sid="26" ssid="3">Systems relying only on these resources were considered part of the Limited Resources subtask.</S>
    <S sid="27" ssid="4">Systems making use of any additional resources (e.g. bilingual dictionaries, additional parallel corpora, and others) were classified under the Unlimited Resources category.</S>
    <S sid="28" ssid="5">Two sets of training data were made available. for pages containing potential parallel translations were manually identified (mainly from the archives of Romanian newspapers).</S>
    <S sid="29" ssid="6">Next, texts were automatically downloaded and sentence aligned.</S>
    <S sid="30" ssid="7">A manual verification of the alignment was also performed.</S>
    <S sid="31" ssid="8">These data collection process resulted in a corpus of about 850,000 Romanian words, and about 900,000 English words.</S>
    <S sid="32" ssid="9">All data were pre-tokenized.</S>
    <S sid="33" ssid="10">For English and French, we used a version of the tokenizers provided within the EGYPT Toolkit2.</S>
    <S sid="34" ssid="11">For Romanian, we used our own tokenizer.</S>
    <S sid="35" ssid="12">Identical tokenization procedures were used for training, trial, and test data.</S>
    <S sid="36" ssid="13">Two sets of trial data were made available at the same time training data became available.</S>
    <S sid="37" ssid="14">Trial sets consisted of sentence aligned texts, provided together with manually determined word alignments.</S>
    <S sid="38" ssid="15">The main purpose of these data was to enable participants to better understand the format required for the word alignment result files.</S>
    <S sid="39" ssid="16">Trial sets consisted of 37 English-French, and 17 Romanian-English aligned sentences.</S>
    <S sid="40" ssid="17">A total of 447 English-French aligned sentences (Och and Ney, 2000), and 248 Romanian-English aligned sentences were released one week prior to the deadline.</S>
    <S sid="41" ssid="18">Participants were required to run their word alignment systems on these two sets, and submit word alignments.</S>
    <S sid="42" ssid="19">Teams were allowed to submit an unlimited number of results sets for each language pair.</S>
    <S sid="43" ssid="20">The gold standard for the two language pair alignments were produced using slightly different alignment procedures, which allowed us to study different schemes for producing gold standards for word aligned data.</S>
    <S sid="44" ssid="21">For English-French, annotators where instructed to assign a Sure or Probable tag to each word alignment they produced.</S>
    <S sid="45" ssid="22">The intersection of the Sure alignments produced by the two annotators led to the final Sure aligned set, while the reunion of the Probable alignments led to the final Probable aligned set.</S>
    <S sid="46" ssid="23">The Sure alignment set is guaranteed to be a subset of the Probable alignment set.</S>
    <S sid="47" ssid="24">The annotators did not produce any NULL alignments.</S>
    <S sid="48" ssid="25">Instead, we assigned NULL alignments as a default backup mechanism, which forced each word to belong to at least one alignment.</S>
    <S sid="49" ssid="26">The English-French aligned data were produced by Franz Och and Hermann Ney (Och and Ney, 2000).</S>
    <S sid="50" ssid="27">For Romanian-English, annotators were instructed to assign an alignment to all words, with specific instructions as to when to assign a NULL alignment.</S>
    <S sid="51" ssid="28">Annotators were not asked to assign a Sure or Probable label.</S>
    <S sid="52" ssid="29">Instead, we had an arbitration phase, where a third annotator judged the cases where the first two annotators disagreed.</S>
    <S sid="53" ssid="30">Since an inter-annotator agreement was reached for all word alignments, the final resulting alignments were considered to be Sure alignments.</S>
  </SECTION>
  <SECTION title="3 Evaluation Measures" number="3">
    <S sid="54" ssid="1">Evaluations were performed with respect to four different measures.</S>
    <S sid="55" ssid="2">Three of them &#8211; precision, recall, and Fmeasure &#8211; represent traditional measures in Information Retrieval, and were also frequently used in previous word alignment literature.</S>
    <S sid="56" ssid="3">The fourth measure was originally introduced by (Och and Ney, 2000), and proposes the notion of quality of word alignment.</S>
    <S sid="57" ssid="4">Given an alignment A, and a gold standard alignment ~, each such alignment set eventually consisting of two sets As, .Ap, and 9s, 9p corresponding to Sure and Probable alignments, the following measures are defined (where T is the alignment type, and can be set to either S or P).</S>
    <S sid="58" ssid="5">Each word alignment submission was evaluated in terms of the above measures.</S>
    <S sid="59" ssid="6">Moreover, we conducted two sets of evaluations for each submission: &#8226; NULL-Align, where each word was enforced to belong to at least one alignment; if a word did not belong to any alignment, a NULL Probable alignment was assigned by default.</S>
    <S sid="60" ssid="7">This set of evaluations pertains to full coverage word alignments.</S>
    <S sid="61" ssid="8">We conducted therefore 14 evaluations for each submission file: AER, Sure/Probable Precision, Sure/Probable Recall, and Sure/Probable F-measure, with a different figure determined for NULL-Align and NO-NULL-Align alignments.</S>
  </SECTION>
  <SECTION title="4 Participating Systems" number="4">
    <S sid="62" ssid="1">Seven teams from around the world participated in the word alignment shared task.</S>
    <S sid="63" ssid="2">Table 1 lists the names of the participating systems, the corresponding institutions, and references to papers in this volume that provide detailed descriptions of the systems and additional analysis of their results.</S>
    <S sid="64" ssid="3">All seven teams participated in the Romanian-English subtask, and five teams participated in the English-French subtask.3 There were no restrictions placed on the number of submissions each team could make.</S>
    <S sid="65" ssid="4">This resulted in a total of 27 submissions from the seven teams, where 14 sets of results were submitted for the English-French subtask, and 13 for the Romanian-English subtask.</S>
    <S sid="66" ssid="5">Of the 27 total submissions, there were 17 in the Limited resources subtask, and 10 in the Unlimited resources subtask.</S>
    <S sid="67" ssid="6">Tables 2 and 3 show all of the submissions for each team in the two subtasks, and provide a brief description of their approaches.</S>
    <S sid="68" ssid="7">While each participating system was unique, there were a few unifying themes.</S>
    <S sid="69" ssid="8">Four teams had approaches that relied (to varying degrees) on an IBM model of statistical machine translation (Brown et al., 1993).</S>
    <S sid="70" ssid="9">UMD was a straightforward implementation of IBM Model 2, BiBr employed a boosting procedure in deriving an IBM Model 1 lexicon, Ralign used IBM Model 2 as a foundation for their recursive splitting procedure, and XRCE used IBM Model 4 as a base for alignment with lemmatized text and bilingual lexicons.</S>
    <S sid="71" ssid="10">Two teams made use of syntactic structure in the text to be aligned.</S>
    <S sid="72" ssid="11">ProAlign satisfies constraints derived from a dependency tree parse of the English sentence being aligned.</S>
    <S sid="73" ssid="12">BiBr also employs syntactic constraints that must be satisfied.</S>
    <S sid="74" ssid="13">However, these come from parallel text that has been shallowly parsed via a method known as bilingual bracketing.</S>
    <S sid="75" ssid="14">Three teams approached the shared task with baseline or prototype systems.</S>
    <S sid="76" ssid="15">Fourday combines several intuitive baselines via a nearest neighbor classifier, RACAI carries out a greedy alignment based on an automatically extracted dictionary of translations, and UMD&#8217;s implementation of IBM Model 2 provides an experimental platform for their future work incorporating prior knowledge about cognates.</S>
    <S sid="77" ssid="16">All three of these systems were developed within a short period of time before and during the shared task.</S>
  </SECTION>
  <SECTION title="5 Results and Discussion" number="5">
    <S sid="78" ssid="1">Tables 4 and 5 list the results obtained by participating systems in the Romanian-English task.</S>
    <S sid="79" ssid="2">Similarly, results obtained during the English-French task are listed in Tables 6 and 7.</S>
    <S sid="80" ssid="3">For Romanian-English, limited resources, XRCE systems (XRCE.Nolem-56k.RE.2 and XRCE.Trilex.RE.3) seem to lead to the best results.</S>
    <S sid="81" ssid="4">These are systems that are based on GIZA++, with or without additional resources (lemmatizers and lexicons).</S>
    <S sid="82" ssid="5">For unlimited resources, ProAlign.RE.1 has the best performance.</S>
    <S sid="83" ssid="6">For English-French, Ralign.EF.1 has the best performance for limited resources, while ProAlign.EF.1 has again the largest number of top ranked figures for unlimited resources.</S>
    <S sid="84" ssid="7">To make a cross-language comparison, we paid particular attention to the evaluation of the Sure alignments, since these were collected in a similar fashion (an agreement had to be achieved between two different annotators).</S>
    <S sid="85" ssid="8">The results obtained for the English-French Sure alignments are significantly higher (80.54% best Fmeasure) than those for Romanian-English Sure alignments (71.14% best F-measure).</S>
    <S sid="86" ssid="9">Similarly, AER for English-French (5.71% highest error reduction) is clearly better than the AER for Romanian-English (28.86% highest error reduction).</S>
    <S sid="87" ssid="10">This difference in performance between the two data sets is not a surprise.</S>
    <S sid="88" ssid="11">As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data.</S>
    <S sid="89" ssid="12">Increased performance is therefore expected when larger training data sets are available.</S>
    <S sid="90" ssid="13">The only evaluation set where Romanian-English data leads to better performance is the Probable alignments set.</S>
    <S sid="91" ssid="14">We believe however that these figures are not directly comparable, since the English-French Probable alignments were obtained as a reunion of the alignments assigned by two different annotators, while for the Romanian-English Probable set two annotators had to reach an agreement (that is, an intersection of their individual alignment assignments).</S>
    <S sid="92" ssid="15">Interestingly, in an overall evaluation, the limited resources systems seem to lead to better results than those with unlimited resources.</S>
    <S sid="93" ssid="16">Out of 28 different evaluation figures, 20 top ranked figures are provided by systems with limited resources.</S>
    <S sid="94" ssid="17">This suggests that perhaps using a large number of additional resources does not seem to improve a lot over the case when only parallel texts are employed.</S>
    <S sid="95" ssid="18">Ranked results for all systems are plotted in Figures 2 and 3.</S>
    <S sid="96" ssid="19">In the graphs, systems are ordered based on their AER scores.</S>
    <S sid="97" ssid="20">System names are preceded by a marker to indicate the system type: L stands for Limited Resources, and U stands for Unlimited Resources.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="6">
    <S sid="98" ssid="1">A shared task on word alignment was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts.</S>
    <S sid="99" ssid="2">In this paper, we presented the task definition, and resources involved, and shortly described the participating systems.</S>
    <S sid="100" ssid="3">The shared task included Romanian-English and English-French sub-tasks, and drew the participation of seven teams from around the world.</S>
    <S sid="101" ssid="4">Comparative evaluations of results led to interesting insights regarding the impact on performance of (1) various alignment algorithms, (2) large or small amounts of training data, and (3) type of resources available.</S>
    <S sid="102" ssid="5">Data and evaluation software used in this exercise are available online at http://www.cs.unt.edu/&#732;rada/wpt.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="103" ssid="1">There are many people who contributed greatly to making this word alignment evaluation task possible.</S>
    <S sid="104" ssid="2">We are grateful to all the participants in the shared task, for their hard work and involvement in this evaluation exercise.</S>
    <S sid="105" ssid="3">Without them, all these comparative analyses of word alignment techniques would not be possible.</S>
    <S sid="106" ssid="4">We are very thankful to Franz Och from ISI and Hermann Ney from RWTH Aachen for kindly making their English-French word aligned data available to the workshop participants; the Hansards made available by Ulrich Germann from ISI constituted invaluable data for the English-French shared task.</S>
    <S sid="107" ssid="5">We would also like to thank the student volunteers from the Department of English, Babes-Bolyai University, Cluj-Napoca, Romania who helped creating the Romanian-English word aligned data.</S>
    <S sid="108" ssid="6">We are also grateful to all the Program Committee members of the current workshop, for their comments and suggestions, which helped us improve the definition of this shared task.</S>
    <S sid="109" ssid="7">In particular, we would like to thank Dan Melamed for suggesting the two different subtasks (limited and unlimited resources), and Michael Carl and Phil Resnik for initiating interesting discussions regarding phrase-based evaluations.</S>
  </SECTION>
</PAPER>
