<PAPER>
	<S sid="0">Online Large-Margin Training for Statistical Machine Translation</S><ABSTRACT>
		<S sid="1" ssid="1">We achieved a state of the art performance in statistical machine translation by using a large number of features with an onlinelarge-margin training algorithm.</S>
		<S sid="2" ssid="2">The mil lions of parameters were tuned only on a small development set consisting of less than1K sentences.</S>
		<S sid="3" ssid="3">Experiments on Arabic-to English translation indicated that a modeltrained with sparse binary features outper formed a conventional SMT system with a small number of features.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="4" ssid="4">The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based ei ther on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al, 2003; Chiang, 2005) orsyntax-based translation (Galley et al, 2006).</S>
			<S sid="5" ssid="5">How ever, it does not scale well with a large number of features of the order of millions.</S>
			<S sid="6" ssid="6">Tillmann and Zhang (2006), Liang et al (2006) and Bangalore et al (2006) introduced sparse binary features for statistical machine translation trained ona large training corpus.</S>
			<S sid="7" ssid="7">In this framework, the prob lem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing.</S>
			<S sid="8" ssid="8">However, the use of alarge number of features did not provide any signifi cant improvements over a conventional small feature set.</S>
			<S sid="9" ssid="9">Bangalore et al (2006) trained the lexical choice model by using Conditional Random Fields (CRF) realized on a WFST.</S>
			<S sid="10" ssid="10">Their modeling was reduced toMaximum Entropy Markov Model (MEMM) to han dle a large number of features which, in turn, faced the labeling bias problem (Lafferty et al, 2001).</S>
			<S sid="11" ssid="11">Tillmann and Zhang (2006) trained their feature set using an online discriminative algorithm.</S>
			<S sid="12" ssid="12">Since thedecoding is still expensive, their online training approach is approximated by enlarging a merged k best list one-by-one with a 1-best output.</S>
			<S sid="13" ssid="13">Lianget al (2006) introduced an averaged perceptron al gorithm, but employed only 1-best translation.</S>
			<S sid="14" ssid="14">In Watanabe et al (2006a), binary features were trained only on a small development set using a variant of voted perceptron for reranking k-best translations.</S>
			<S sid="15" ssid="15">Thus, the improvement is merely relative to the baseline translation system, namely whether or not there is a good translation in their k-best.We present a method to estimate a large num ber of parameters ? of the order of millions ? using an online training algorithm.</S>
			<S sid="16" ssid="16">Although itwas intuitively considered to be prone to overfit ting, training on a small development set ? lessthan 1K sentences ? was sufficient to achieve improved performance.</S>
			<S sid="17" ssid="17">In this method, each train ing sentence is decoded and weights are updated atevery iteration (Liang et al, 2006).</S>
			<S sid="18" ssid="18">When updating model parameters, we employ a memorization variant of a local updating strategy (Liang et al, 2006) in which parameters are optimized toward a set of good translations found in the k-best listacross iterations.</S>
			<S sid="19" ssid="19">The objective function is an ap proximated BLEU (Watanabe et al, 2006a) thatscales the loss of a sentence BLEU to a document wise loss.</S>
			<S sid="20" ssid="20">The parameters are trained using the 764Margin Infused Relaxed Algorithm (MIRA) (Cram mer et al, 2006).</S>
			<S sid="21" ssid="21">MIRA is successfully employed in dependency parsing (McDonald et al, 2005) or the joint-labeling/chunking task (Shimizu and Haas,2006).</S>
			<S sid="22" ssid="22">Experiments were carried out on an Arabicto-English translation task, and we achieved significant improvements over conventional minimum er ror training with a small number of features.This paper is organized as follows: First, Section 2 introduces the framework of statistical ma chine translation.</S>
			<S sid="23" ssid="23">As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al, 2006b) originally proposed by Chiang (2005).</S>
			<S sid="24" ssid="24">In Section 3, a set of binary sparse features are defined including numeric features for our baseline system.</S>
			<S sid="25" ssid="25">Section 4 introduces an online large-margin training algorithm using MIRA with our key components.</S>
			<S sid="26" ssid="26">The experiments are presented in Section 5 followed by discussion in Section 6.</S>
	</SECTION>
	<SECTION title="Statistical Machine Translation. " number="2">
			<S sid="27" ssid="1">We use a log-linear approach (Och, 2003) in whicha foreign language sentence f is translated into an other language, for example English, e, by seeking a maximum solution: e?</S>
			<S sid="28" ssid="2">= argmax e wT ? h( f , e) (1) where h( f , e) is a large-dimension feature vector.</S>
			<S sid="29" ssid="3">w is a weight vector that scales the contribution from each feature.</S>
			<S sid="30" ssid="4">Each feature can take any real value, such as the log of the n-gram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence.</S>
			<S sid="31" ssid="5">2.1 Hierarchical Phrase-based SMT.</S>
			<S sid="32" ssid="6">Chiang (2005) introduced the hierarchical phrase based translation approach, in which non-terminalsare embedded in each phrase.</S>
			<S sid="33" ssid="7">A translation is gener ated by hierarchically combining phrases using the non-terminals.</S>
			<S sid="34" ssid="8">Such a quasi-syntactic structure can naturally capture the reordering of phrases that is notdirectly modeled by a conventional phrase-based approach (Koehn et al, 2003).</S>
			<S sid="35" ssid="9">The non-terminal em bedded phrases are learned from a bilingual corpuswithout a linguistically motivated syntactic struc ture.</S>
			<S sid="36" ssid="10">Based on hierarchical phrase-based modeling, we adopted the left-to-right target generation method (Watanabe et al, 2006b).</S>
			<S sid="37" ssid="11">This method is able to generate translations efficiently, first, by simplifyingthe grammar so that the target side takes a phrase prefixed form, namely a target normalized form.</S>
			<S sid="38" ssid="12">Second, a translation is generated in a left-to-right manner, similar to the phrase-based approach using Earley-style top-down parsing on the source side.</S>
			<S sid="39" ssid="13">Coupled with the target normalized form, n-gram language models are efficiently integrated during the search even with a higher order of n. 2.2 Target Normalized Form.</S>
			<S sid="40" ssid="14">In Chiang (2005), each production rule is restrictedto a rank-2 or binarized form in which each rule contains at most two non-terminals.</S>
			<S sid="41" ssid="15">The target normal ized form (Watanabe et al, 2006b) further imposes a constraint whereby the target side of the aligned right-hand side is restricted to a Greibach Normal Form like structure: X ? ?</S>
			<S sid="42" ssid="16">?, ?b?,?</S>
			<S sid="43" ssid="17">(2) where X is a non-terminal, ? is a source side string ofarbitrary terminals and/or non-terminals.</S>
			<S sid="44" ssid="18">?b? is a corresponding target side where ?b is a string of termi nals, or a phrase, and ? is a (possibly empty) stringof non-terminals.</S>
			<S sid="45" ssid="19">defines one-to-one mapping be tween non-terminals in ? and ?.</S>
			<S sid="46" ssid="20">The use of phrase?b as a prefix maintains the strength of the phrase base framework.</S>
			<S sid="47" ssid="21">A contiguous English side with a(possibly) discontiguous foreign language side pre serves phrase-bounded local word reordering.</S>
			<S sid="48" ssid="22">At the same time, the target normalized framework stillcombines phrases hierarchically in a restricted man ner.</S>
			<S sid="49" ssid="23">2.3 Left-to-Right Target Generation.</S>
			<S sid="50" ssid="24">Decoding is performed by parsing on the source side and by combining the projected target side.</S>
			<S sid="51" ssid="25">We applied an Earley-style top-down parsing approach(Wu and Wong, 1998; Watanabe et al, 2006b; Zoll mann and Venugopal, 2006).</S>
			<S sid="52" ssid="26">The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner.</S>
			<S sid="53" ssid="27">The search is guided with a push-down automaton, which keeps track of the span of uncovered source 765word positions.</S>
			<S sid="54" ssid="28">Combined with the rest-cost esti mation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation.The use of a target normalized form further sim plifies the decoding procedure.</S>
			<S sid="55" ssid="29">Since the rule formdoes not allow any holes for the target side, the integration with an n-gram language model is straightforward: the prefixed phrases are simply concate nated and intersected with n-gram.</S>
	</SECTION>
	<SECTION title="Features. " number="3">
			<S sid="56" ssid="1">3.1 Baseline Features.</S>
			<S sid="57" ssid="2">The hierarchical phrase-based translation system employs standard numeric value features: ? n-gram language model to capture the fluency of the target side.</S>
			<S sid="58" ssid="3">Hierarchical phrase translation probabilities in both directions, h(?|?b?) and h(?b?|?), estimated by relative counts, count(?, ?b?).</S>
			<S sid="59" ssid="4">Word-based lexically weighted models ofhlex(?|?b?) and hlex(?b?|?)</S>
			<S sid="60" ssid="5">using lexical transla tion models.</S>
			<S sid="61" ssid="6">Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models (Bender et al, 2004).</S>
			<S sid="62" ssid="7">Word/hierarchical-phrase length penalties.?</S>
			<S sid="63" ssid="8">Backtrack-based penalties inspired by the dis tortion penalties in phrase-based modeling (Watanabe et al, 2006b).</S>
			<S sid="64" ssid="9">3.2 Sparse Features.</S>
			<S sid="65" ssid="10">In addition to the baseline features, a large number of binary features are integrated in our MT system.</S>
			<S sid="66" ssid="11">We may use any binary features, such as h( f , e) = ? ?</S>
			<S sid="67" ssid="12">1 English word ?violate?</S>
			<S sid="68" ssid="13">and Arabic.</S>
			<S sid="69" ssid="14">word ?tnthk?</S>
			<S sid="70" ssid="15">appeared in e and f . 0 otherwise.The features are designed by considering the decod ing efficiency and are based on the word alignmentstructure preserved in hierarchical phrase translation pairs (Zens and Ney, 2006).</S>
			<S sid="71" ssid="16">When hierarchical phrases are extracted, the word alignment is pre served.</S>
			<S sid="72" ssid="17">If multiple word alignments are observed ei?1 ei ei+1 ei+2 ei+3 ei+4 f j?1 f j f j+1 f j+2 f j+3 Figure 1: An example of sparse features for a phrase translation.with the same source and target sides, only the fre quently observed word alignment is kept to reduce the grammar size.</S>
			<S sid="73" ssid="18">3.2.1 Word Pair FeaturesWord pair features reflect the word correspon dence in a hierarchical phrase.</S>
			<S sid="74" ssid="19">Figure 1 illustratesan example of sparse features for a phrase trans lation pair f j, ..., f j+2 and ei, ..., ei+3 1.</S>
			<S sid="75" ssid="20">From theword alignment encoded in this phrase, we can ex tract word pair features of (ei, f j+1), (ei+2, f j+2) and (ei+3, f j).</S>
			<S sid="76" ssid="21">The bigrams of word pairs are also used to capture the contextual dependency.</S>
			<S sid="77" ssid="22">We assumethat the word pairs follow the target side order ing.</S>
			<S sid="78" ssid="23">For instance, we define ((ei?1, f j?1), (ei, f j+1)),((ei, f j+1), (ei+2, f j+2)) and ((ei+2, f j+2), (ei+3, f j)) in dicated by the arrows in Figure 1.</S>
			<S sid="79" ssid="24">Extracting bigram word pair features followingthe target side ordering implies that the corresponding source side is reordered according to the tar get side.</S>
			<S sid="80" ssid="25">The reordering of hierarchical phrases is represented by using contextually dependent word pairs across their boundaries, as with the feature ((ei?1, f j?1), (ei, f j+1)) in Figure 1.</S>
			<S sid="81" ssid="26">3.2.2 Insertion Features The above features are insufficient to capture the translation because spurious words are sometimesinserted in the target side.</S>
			<S sid="82" ssid="27">Therefore, insertion fea tures are integrated in which no word alignment isassociated in the target.</S>
			<S sid="83" ssid="28">The inserted words are asso ciated with all the words in the source sentence, such as (ei+1, f1), ..., (ei+1, fJ) for the non-aligned word ei+1 with the source sentence f J1 in Figure 1.</S>
			<S sid="84" ssid="29">In the 1For simplicity, we show an example of phrase translation pairs, but it is trivial to define the features over hierarchical phrases.</S>
			<S sid="85" ssid="30">766 f j?1 f j f j+1 f j+2 f j+3 X 1 X 2 X 3 Figure 2: Example hierarchical features.same way, we will be able to include deletion fea tures where a non-aligned source word is associated with the target sentence.</S>
			<S sid="86" ssid="31">However, this would lead to complex decoding in which all the translated wordsare memorized for each hypothesis, and thus not in tegrated in our feature set.</S>
			<S sid="87" ssid="32">3.2.3 Target Bigram Features Target side bigram features are also included todirectly capture the fluency as in the n-gram language model (Roark et al, 2004).</S>
			<S sid="88" ssid="33">For instance, bi gram features of (ei?1, ei), (ei, ei+1), (ei+1, ei+2)... are observed in Figure 1.</S>
			<S sid="89" ssid="34">3.2.4 Hierarchical Features In addition to the phrase motivated features, weincluded features inspired by the hierarchical struc ture.</S>
			<S sid="90" ssid="35">Figure 2 shows an example of hierarchical phrases in the source side, consisting of X 1 ? ?</S>
			<S sid="91" ssid="36">f j?1X 2 f j+3 ? , X 2 ? ?</S>
			<S sid="92" ssid="37">f j f j+1X 3 ? and X 3 ? ?</S>
			<S sid="93" ssid="38">f j+2 ? .</S>
			<S sid="94" ssid="39">Hierarchical features capture the dependency of the source words in a parent phrase to the source words in child phrases, such as ( f j?1, f j), ( f j?1, f j+1),( f j+3, f j), ( f j+3, f j+1), ( f j, f j+2) and ( f j+1, f j+2) as in dicated by the arrows in Figure 2.</S>
			<S sid="95" ssid="40">The hierarchical features are extracted only for those source wordsthat are aligned with the target side to limit the fea ture size.</S>
			<S sid="96" ssid="41">3.3 Normalization.</S>
			<S sid="97" ssid="42">In order to achieve the generalization capability, the following normalized tokens are introduced for each surface form: ? Word class or POS.</S>
			<S sid="98" ssid="43">4-letter prefix and suffix.</S>
			<S sid="99" ssid="44">For instance, the word Algorithm 1 Online Training Algorithm Training data: T = {( f t, et)}Tt=1 m-best oracles: O = {}Tt=1 i = 0 1: for n = 1, ..., N do 2: for t = 1, ..., T do 3: Ct ? bestk( f t; wi) 4: Ot ? oraclem(Ot ? Ct; et) 5: wi+1 = update wi using Ct w.r.t. Ot 6: i = i + 1 7: end for 8: end for 9: return ?NT i=1 w i NT ?violate?</S>
			<S sid="100" ssid="45">is normalized to ?viol+?</S>
			<S sid="101" ssid="46">and ?+late?</S>
			<S sid="102" ssid="47">by taking the prefix and suffix, respectively.?</S>
			<S sid="103" ssid="48">Digits replaced by a sequence of ?@?.</S>
			<S sid="104" ssid="49">For ex ample, the word ?2007/6/27?</S>
			<S sid="105" ssid="50">is represented as ?@@@@/@/@@?.We consider all possible combination of those token types.</S>
			<S sid="106" ssid="51">For example, the word pair feature (vi olate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type.</S>
	</SECTION>
	<SECTION title="Online Large-Margin Training. " number="4">
			<S sid="107" ssid="1">Algorithm 1 is our generic online training algo rithm.</S>
			<S sid="108" ssid="2">The algorithm is slightly different from other online training algorithms (Tillmann and Zhang,2006; Liang et al, 2006) in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a met ric, i.e. BLEU (Papineni et al, 2002).</S>
			<S sid="109" ssid="3">In line 3,a k-best list is generated by bestk(?)</S>
			<S sid="110" ssid="4">using the cur rent weight vector wi for the training instance of( f t, et).</S>
			<S sid="111" ssid="5">Each training instance has multiple (or, pos sibly one) reference translations et for the source sentence f t. Using the k-best list, m-best oracletranslations Ot is updated by oraclem(?)</S>
			<S sid="112" ssid="6">for every it eration (line 4).</S>
			<S sid="113" ssid="7">Usually, a decoder cannot generatetranslations that exactly match the reference transla tions due to its beam search pruning and OOV.</S>
			<S sid="114" ssid="8">Thus, we cannot always assign scores for each reference translation.</S>
			<S sid="115" ssid="9">Therefore, possible oracle translations are maintained according to an objective function, 767 i.e. BLEU.</S>
			<S sid="116" ssid="10">Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations inadvance.</S>
			<S sid="117" ssid="11">Liang et al (2006) presented a similar up dating strategy in which parameters were updated toward an oracle translation found in Ct, but ignored potentially better translations discovered in the past iterations.</S>
			<S sid="118" ssid="12">New wi+1 is computed using the k-best list Ct with respect to the oracle translations Ot (line 5).</S>
			<S sid="119" ssid="13">After N iterations, the algorithm returns an averaged weight vector to avoid overfitting (line 9).</S>
			<S sid="120" ssid="14">The key to thisonline training algorithm is the selection of the up dating scheme in line 5.</S>
			<S sid="121" ssid="15">4.1 Margin Infused Relaxed Algorithm.</S>
			<S sid="122" ssid="16">The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al, 2006) is an online version of thelarge-margin training algorithm for structured classification (Taskar et al, 2004) that has been suc cessfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006).</S>
			<S sid="123" ssid="17">The basic idea is to keep the norm of the updates to the weight vector as small as possible, considering a margin at least as large as the loss of the incorrect classification.</S>
			<S sid="124" ssid="18">Line 5 of the weight vector update procedure in Algorithm 1 is replaced by the solution of: w?i+1 = argmin wi+1 ||wi+1 ? wi|| + C ? e?,e? ?(e?, e?)</S>
			<S sid="125" ssid="19">subject to si+1( f t, e?)</S>
			<S sid="126" ssid="20">si+1( f t, e?)</S>
			<S sid="127" ssid="21">+ ?(e?, e?)</S>
			<S sid="128" ssid="22">L(e?, e?; et) ?(e?, e?)</S>
			<S sid="129" ssid="23">0 ?e? ? Ot,?e? ? Ct (3) where si( f t, e) = { wi}T ? h( f t, e).</S>
			<S sid="130" ssid="24">is a non negative slack variable and C ? 0 is a constant to control the influence to the objective function.</S>
			<S sid="131" ssid="25">Alarger C implies larger updates to the weight vec tor.</S>
			<S sid="132" ssid="26">L(?)</S>
			<S sid="133" ssid="27">is a loss function, for instance difference of BLEU, that measures the difference between e?</S>
			<S sid="134" ssid="28">and e?</S>
			<S sid="135" ssid="29">according to the reference translations et. In thisupdate, a margin is created for each correct and in correct translation at least as large as the loss of the incorrect translation.</S>
			<S sid="136" ssid="30">A larger error means a largerdistance between the scores of the correct and incor rect translations.</S>
			<S sid="137" ssid="31">Following McDonald et al (2005), only k-best translations are used to form the margins in order to reduce the number of constraints in Eq.</S>
			<S sid="138" ssid="32">3.In the translation task, multiple translations are ac ceptable.</S>
			<S sid="139" ssid="33">Thus, margins for m-oracle translation arecreated, which amount to m ? k large-margin con straints.</S>
			<S sid="140" ssid="34">In this online training, only active features constrained by Eq.</S>
			<S sid="141" ssid="35">3 are kept and updated, unlike offline training in which all possible features have to be extracted and selected in advance.</S>
			<S sid="142" ssid="36">The Lagrange dual form of Eq.</S>
			<S sid="143" ssid="37">3 is: max?(?)?0 ? 1 2 || ? e?,e? ?(e?, e?)</S>
			<S sid="144" ssid="38">( h( f t, e?)</S>
			<S sid="145" ssid="39">h( f t, e?)</S>
			<S sid="146" ssid="40">) ||2 + ? e?,e? ?(e?, e?)L(e?, e?; et) ? ?</S>
			<S sid="147" ssid="41">e?,e? ?(e?, e?)</S>
			<S sid="148" ssid="42">( si( f t, e?)</S>
			<S sid="149" ssid="43">si( f t, e?)</S>
			<S sid="150" ssid="44">) subject to ? e?,e? ?(e?, e?)</S>
			<S sid="151" ssid="45">C (4) with the weight vector update: wi+1 = wi + ? e?,e? ?(e?, e?)</S>
			<S sid="152" ssid="46">( h( f t, e?)</S>
			<S sid="153" ssid="47">h( f t, e?)</S>
			<S sid="154" ssid="48">) (5)Equation 4 is solved using a QP-solver, such as a co ordinate ascent algorithm, by heuristically selecting (e?, e?)</S>
			<S sid="155" ssid="49">and by updating ?(?)</S>
			<S sid="156" ssid="50">iteratively: ?(e?, e?)</S>
			<S sid="157" ssid="51">= max (0, ?(e?, e?)</S>
			<S sid="158" ssid="52">+ ?(e?, e?)) (6) ?(e?, e?)</S>
			<S sid="159" ssid="53">= L(e?, e?; et) ?</S>
			<S sid="160" ssid="54">( si( f t, e?)</S>
			<S sid="161" ssid="55">si( f t, e?)</S>
			<S sid="162" ssid="56">) ||h( f t, e?)</S>
			<S sid="163" ssid="57">h( f t, e?)||2 C is used to clip the amount of updates.A single oracle with 1-best translation is analyti cally solved without a QP-solver and is represented as the following perceptron-like update (Shimizu and Haas, 2006): ? = max ? ?</S>
			<S sid="164" ssid="58">0, min ? ?</S>
			<S sid="165" ssid="59">C, L(e?, e?; et) ?</S>
			<S sid="166" ssid="60">( si( f t, e?)</S>
			<S sid="167" ssid="61">si( f t, e?)</S>
			<S sid="168" ssid="62">) ||h( f t, e?)</S>
			<S sid="169" ssid="63">h( f t, e?)||2 ? ?</S>
			<S sid="170" ssid="64">Intuitively, the update amount is controlled by themargin and the loss between the correct and incorrect translations and by the closeness of two transla tions in terms of feature vectors.</S>
			<S sid="171" ssid="65">Indeed, Liang et al (2006) employed an averaged perceptron algorithm in which ? value was always set to one.</S>
			<S sid="172" ssid="66">Tillmann and Zhang (2006) used a different update style based on a convex loss function: ? = ?L(e?, e?; et) ?max ( 0, 1 ?</S>
			<S sid="173" ssid="67">( si( f t, e?)</S>
			<S sid="174" ssid="68">si( f t, e?)</S>
			<S sid="175" ssid="69">)) 768 Table 1: Experimental results obtained by varying normalized tokens used with surface form.</S>
			<S sid="176" ssid="70"># features 2003 (dev) 2004 2005 NIST BLEU [%] NIST BLEU [%] NIST BLEU [%] surface form 492K 11.32 54.11 10.57 49.01 10.77 48.05 w/ prefix/suffix 4,204K 12.38 63.87 10.42 48.74 10.58 47.18 w/ word class 2,689K 10.87 49.59 10.63 49.55 10.89 48.79 w/ digits 576K 11.01 50.72 10.66 49.67 10.84 48.39 all token types 13,759K 11.24 52.85 10.66 49.81 10.85 48.41 where ? &gt; 0 is a learning rate for controlling the convergence.</S>
			<S sid="177" ssid="71">4.2 Approximated BLEU.</S>
			<S sid="178" ssid="72">We used the BLEU score (Papineni et al, 2002) as the loss function computed by: BLEU(E; E) = exp ? ?</S>
			<S sid="179" ssid="73">1 N N ? n=1 log pn(E, E) ? ?</S>
			<S sid="180" ssid="74">BP(E, E) (7) where pn(?)</S>
			<S sid="181" ssid="75">is the n-gram precision of hypothesized translations E = {et}Tt=1 given reference translations E = {et}Tt=1 and BP(?)</S>
			<S sid="182" ssid="76">1 is a brevity penalty.</S>
			<S sid="183" ssid="77">BLEUis computed for a set of sentences, not for a single sentence.</S>
			<S sid="184" ssid="78">Our algorithm requires frequent up dates on the weight vector, which implies higher cost in computing the document-wise BLEU.</S>
			<S sid="185" ssid="79">Tillmann and Zhang (2006) and Liang et al (2006) solved the problem by introducing a sentence-wise BLEU.</S>
			<S sid="186" ssid="80">However, the use of the sentence-wise scoring does not translate directly into the document-wise score because of the n-gram precision statistics and the brevity penalty statistics aggregated for a sentence set.</S>
			<S sid="187" ssid="81">Thus, we use an approximated BLEU score that basically computes BLEU for a sentence set, but accumulates the difference for a particular sentence (Watanabe et al, 2006a).</S>
			<S sid="188" ssid="82">The approximated BLEU is computed as follows: Given oracle translations O for T , we maintain the best oracle translations OT1 = { e?1, ..., e?T } . The ap-.</S>
			<S sid="189" ssid="83">proximated BLEU for a hypothesized translation e?</S>
			<S sid="190" ssid="84">for the training instance ( f t, et) is computed over OT1 except for e?t, which is replaced by e?: BLEU({e?1, ..., e?t?1, e?, e?t+1, ..., e?T }; E)The loss computed by the approximated BLEU measures the document-wise loss of substituting the cor rect translation e?t into an incorrect translation e?.</S>
			<S sid="191" ssid="85">The score can be regarded as a normalization which scales a sentence-wise score into a document-wise score.</S>
	</SECTION>
	<SECTION title="Experiments. " number="5">
			<S sid="192" ssid="1">We employed our online large-margin training pro cedure for an Arabic-to-English translation task.The training data were extracted from the Ara bic/English news/UN bilingual corpora supplied by LDC.</S>
			<S sid="193" ssid="2">The data amount to nearly 3.8M sentences.</S>
			<S sid="194" ssid="3">The Arabic part of the bilingual data is tokenized by isolating Arabic scripts and punctuation marks.</S>
			<S sid="195" ssid="4">Thedevelopment set comes from the MT2003 Arabic English NIST evaluation test set consisting of 663 sentences in the news domain with four reference translations.</S>
			<S sid="196" ssid="5">The performance is evaluated by the news domain MT2004/MT2005 test set consisting of 707 and 1,056 sentences, respectively.The hierarchical phrase translation pairs are ex tracted in a standard way (Chiang, 2005): First, the bilingual data are word alignment annotated byrunning GIZA++ (Och and Ney, 2003) in two di rections.</S>
			<S sid="197" ssid="6">Second, the word alignment is refined by a grow-diag-final heuristic (Koehn et al, 2003).</S>
			<S sid="198" ssid="7">Third, phrase translation pairs are extracted together with hierarchical phrases by considering holes.</S>
			<S sid="199" ssid="8">In the last step, the hierarchical phrases are constrainedso that they follow the target normalized form con straint.</S>
			<S sid="200" ssid="9">A 5-gram language model is trained on the English side of the bilingual data combined with the English Gigaword from LDC.First, the use of normalized token types in Sec tion 3.3 is evaluated in Table 1.</S>
			<S sid="201" ssid="10">In this setting, all the structural features in Section 3.2 are used, but differentiated by the normalized tokens combinedwith surface forms.</S>
			<S sid="202" ssid="11">Our online large-margin train ing algorithm performed 50 iterations constrained 769 Table 2: Experimental results obtained by incrementally adding structural features.</S>
			<S sid="203" ssid="12"># features 2003 (dev) 2004 2005 NIST BLEU [%] NIST BLEU [%] NIST BLEU [%] word pairs 11,042K 11.05 51.63 10.43 48.69 10.73 47.72 + target bigram 11,230K 11.19 53.49 10.40 48.60 10.66 47.47 + insertion 13,489K 11.21 52.20 10.77 50.33 10.93 48.08 + hierarchical 13,759K 11.24 52.85 10.66 49.81 10.85 48.41 Table 3: Experimental results for varying k-best and m-oracle translations.</S>
			<S sid="204" ssid="13"># features 2003 (dev) 2004 2005 NIST BLEU [%] NIST BLEU [%] NIST BLEU [%] baseline 10.64 46.47 10.83 49.33 10.90 47.03 1-oracle 1-best 8,735K 11.25 52.63 10.82 50.77 10.93 48.11 1-oracle 10-best 10,480K 11.24 53.45 10.55 49.10 10.82 48.49 10-oracle 1-best 8,416K 10.70 47.63 10.83 48.88 10.76 46.00 10-oracle 10-best 13,759K 11.24 52.85 10.66 49.81 10.85 48.41 sentence-BLEU 14,587K 11.10 51.17 10.82 49.97 10.86 47.04 by 10-oracle and 10-best list.</S>
			<S sid="205" ssid="14">When decoding, a 1000-best list is generated to achieve better oracle translations.</S>
			<S sid="206" ssid="15">The training took nearly 1 day using 8cores of Opteron.</S>
			<S sid="207" ssid="16">The translation quality is eval uated by case-sensitive NIST (Doddington, 2002) and BLEU (Papineni et al, 2002)2.</S>
			<S sid="208" ssid="17">The table alsoshows the number of active features in which non zero values were assigned as weights.</S>
			<S sid="209" ssid="18">The addition of prefix/suffix tokens greatly increased the number of active features.</S>
			<S sid="210" ssid="19">The setting severely overfit to the development data, and therefore resulted in worse results in open tests.</S>
			<S sid="211" ssid="20">The word class3 with surfaceform avoided the overfitting problem.</S>
			<S sid="212" ssid="21">The digit sequence normalization provides a similar generaliza tion capability despite of the moderate increase in the active feature size.</S>
			<S sid="213" ssid="22">By including all token types, we achieved better NIST/BLEU scores for the 2004and 2005 test sets.</S>
			<S sid="214" ssid="23">This set of experiments indi cates that a token normalization is useful especially trained on a small data.</S>
			<S sid="215" ssid="24">Second, we used all the normalized token types,but incrementally added structural features in Ta ble 2.</S>
			<S sid="216" ssid="25">Target bigram features account for only the fluency of the target side without considering thesource/target correspondence.</S>
			<S sid="217" ssid="26">Therefore, the in 2We used the tool available at http://www.nist.gov/ speech/tests/mt/ 3We induced 50 classes each for English and Arabic.</S>
			<S sid="218" ssid="27">clusion of target bigram features clearly overfit to the development data.</S>
			<S sid="219" ssid="28">The problem is resolved byadding insertion features which can take into ac count an agreement with the source side that is notdirectly captured by word pair features.</S>
			<S sid="220" ssid="29">Hierarchi cal features are somewhat effective in the 2005 test set by considering the dependency structure of the source side.Finally, we compared our online training algo rithm with sparse features with a baseline system in Table 3.</S>
			<S sid="221" ssid="30">The baseline hierarchical phrase-based system is trained using standard max-BLEU training (MERT) without sparse features (Och, 2003).</S>
			<S sid="222" ssid="31">Table 3 shows the results obtained by varying the m-oracle and k-best size (k, m = 1, 10) using all structural features and all token types.</S>
			<S sid="223" ssid="32">We also experimentedsentence-wise BLEU as an objective function constrained by 10-oracle and 10-best list.</S>
			<S sid="224" ssid="33">Even the 1oracle 1-best configuration achieved significant im provements over the baseline system.</S>
			<S sid="225" ssid="34">The use ofa larger k-best list further optimizes to the devel opment set, but at the cost of degraded translation quality in the 2004 test set.</S>
			<S sid="226" ssid="35">The larger m-oracle size seems to be harmful if coupled with the 1-best list.As indicated by the reduced active feature size, 1 best translation seems to be updated toward worse translations in 10-oracles that are ?close?</S>
			<S sid="227" ssid="36">in terms of features.</S>
			<S sid="228" ssid="37">We achieved significant improvements 770 Table 4: Two-fold cross validation experiments.</S>
			<S sid="229" ssid="38">closed test open test NIST BLEU NIST BLEU [%] [%] baseline 10.71 44.79 10.68 44.44 online 11.58 53.42 10.90 47.64 when the k-best list size was also increased.</S>
			<S sid="230" ssid="39">The use of sentence-wise BLEU as an objective provides almost no improvement in the 2005 test set, but is comparable for the 2004 test set.</S>
			<S sid="231" ssid="40">As observed in three experiments, the 2004/2005 test sets behaved differently, probably because ofthe domain mismatch.</S>
			<S sid="232" ssid="41">Thus, we conducted a two fold cross validation using the 2003/2004/2005 test sets to observe the effect of optimization as shown in Table 44.</S>
			<S sid="233" ssid="42">The MERT baseline system performedsimilarly both in closed and open tests.</S>
			<S sid="234" ssid="43">Our online large-margin training with 10-oracle and 10 best constraints and the approximated BLEU lossfunction significantly outperformed the baseline sys tem in the open test.</S>
			<S sid="235" ssid="44">The development data is almost doubled in this setting.</S>
			<S sid="236" ssid="45">The MERT approach seems to be confused with the slightly larger data and with the mixed domains from different epochs.</S>
	</SECTION>
	<SECTION title="Discussion. " number="6">
			<S sid="237" ssid="1">In this work, the translation model consisting of millions of features are successfully integrated.</S>
			<S sid="238" ssid="2">In or der to avoid poor overfitting, features are limited to word-based features, but are designed to reflect the structures inside hierarchical phrases.</S>
			<S sid="239" ssid="3">One of the benefit of MIRA is its flexibility.</S>
			<S sid="240" ssid="4">We may includeas many constraints as possible, like m-oracle con straints in our experiments.</S>
			<S sid="241" ssid="5">Although we describedexperiments on the hierarchical phrase-based trans lation, the online training algorithm is applicable toany translation systems, such as phrase-based trans lations and syntax-based translations.</S>
			<S sid="242" ssid="6">Online discriminative training has already been studied by Tillmann and Zhang (2006) and Lianget al (2006).</S>
			<S sid="243" ssid="7">In their approach, training was per formed on a large corpus using the sparse features ofphrase translation pairs, target n-grams and/or bag of-word pairs inside phrases.</S>
			<S sid="244" ssid="8">In Tillmann and Zhang 4We split data by document, not by sentence.</S>
			<S sid="245" ssid="9">(2006), k-best list generation is approximated by a step-by-step one-best merging method that separates the decoding and training steps.</S>
			<S sid="246" ssid="10">The weight vector update scheme is very similar to MIRA but basedon a convex loss function.</S>
			<S sid="247" ssid="11">Our method directly em ploys the k-best list generated by the fast decoding method (Watanabe et al, 2006b) at every iteration.One of the benefits is that we avoid the rather expen sive cost of merging the k-best list especially when handling millions of features.Liang et al (2006) employed an averaged percep tron algorithm.</S>
			<S sid="248" ssid="12">They decoded each training instance and performed a perceptron update to the weight vector.</S>
			<S sid="249" ssid="13">An incorrect translation was updated towardan oracle translation found in a k-best list, but discarded potentially better translations in the past iter ations.</S>
			<S sid="250" ssid="14">An experiment has been undertaken using a small development set together with sparse features for the reranking of a k-best translation (Watanabe et al,2006a).</S>
			<S sid="251" ssid="15">They relied on a variant of a voted perceptron, and achieved significant improvements.</S>
			<S sid="252" ssid="16">How ever, their work was limited to reranking, thus the improvement was relative to the performance of the baseline system, whether or not there was a good translation in a list.</S>
			<S sid="253" ssid="17">In our work, the sparse features are directly integrated into the DP-based search.</S>
			<S sid="254" ssid="18">The design of the sparse features was inspired by Zens and Ney (2006).</S>
			<S sid="255" ssid="19">They exploited theword alignment structure inside the phrase translation pairs for discriminatively training a reordering model in their phrase-based translation.</S>
			<S sid="256" ssid="20">The re ordering model simply classifies whether to perform monotone decoding or not.</S>
			<S sid="257" ssid="21">The trained model is treated as a single feature function integrated in Eq.</S>
			<S sid="258" ssid="22">1.</S>
			<S sid="259" ssid="23">Our approach differs in that each sparse feature is. individually integrated in Eq.</S>
			<S sid="260" ssid="24">1.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="7">
			<S sid="261" ssid="1">We exploited a large number of binary features for statistical machine translation.</S>
			<S sid="262" ssid="2">The model wastrained on a small development set.</S>
			<S sid="263" ssid="3">The optimiza tion was carried out by MIRA, which is an onlineversion of the large-margin training algorithm.</S>
			<S sid="264" ssid="4">Mil lions of sparse features are intuitively considered prone to overfitting, especially when trained on a small development set.</S>
			<S sid="265" ssid="5">However, our algorithm with 771millions of features achieved very significant im provements over a conventional method with a small number of features.</S>
			<S sid="266" ssid="6">This result indicates that we can easily experiment many alternative features evenwith a small data set, but we believe that our ap proach can scale well to a larger data set for furtherimproved performance.</S>
			<S sid="267" ssid="7">Future work involves scal ing up to larger data and more features.</S>
			<S sid="268" ssid="8">Acknowledgements We would like to thank reviewers and our colleagues for useful comment and discussion.</S>
	</SECTION>
</PAPER>
