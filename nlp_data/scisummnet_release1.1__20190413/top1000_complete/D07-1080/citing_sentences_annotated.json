[
  {
    "citance_No": 1, 
    "citing_paper_id": "D08-1024", 
    "citing_paper_authority": 74, 
    "citing_paper_authors": "David, Chiang | Yuval, Marton | Philip, Resnik", 
    "raw_text": "computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007)", 
    "clean_text": "Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D10-1059", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Samidh, Chatterjee | Nicola, Cancedda", 
    "raw_text": "Work has been done to investigate aperceptron-like on line margin training for statisit ical machine translation (Watanabe et al, 2007)", 
    "clean_text": "Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D12-1037", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Lemao, Liu | Hailong, Cao | Taro, Watanabe | Tiejun, Zhao | Mo, Yu | Conghui, Zhu", 
    "raw_text": "MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an on line one which updates with each example (Watanabe et al2007) or part of examples (Chiang et al2008)", 
    "clean_text": "MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D08-1076", 
    "citing_paper_authority": 38, 
    "citing_paper_authors": "Wolfgang, Macherey | Franz Josef, Och | Ignacio, Thayer | Jakob, Uszkoreit", 
    "raw_text": "The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007)", 
    "clean_text": "The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W11-2135", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Alexandre, Allauzen | H&eacute;l&egrave;ne, Bonneau-Maynard | Hai-Son, Le | Aur&eacute;lien, Max | Guillaume, Wisniewski | Fran&ccedil;ois, Yvon | Gilles, Adda | Josep Maria, Crego | Adrien, Lardilleux | Thomas, Lavergne | Artem, Sokolov", 
    "raw_text": "Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization", 
    "clean_text": "Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W10-1757", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Kevin, Duh | Katsuhito, Sudoh | Hajime, Tsukada | Hideki, Isozaki | Masaaki, Nagata", 
    "raw_text": "Sparse features used in re ranking are extracted according to (Watanabe et al, 2007)", 
    "clean_text": "Sparse features used in reranking are extracted according to (Watanabe et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W10-1757", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Kevin, Duh | Katsuhito, Sudoh | Hajime, Tsukada | Hideki, Isozaki | Masaaki, Nagata", 
    "raw_text": "Thus we see the need for a feature extraction procedure. (Watanabe et al, 2007) also reports the possibility of over fitting in their dataset (Arabic-English newswire translation), especially when domain differences are present", 
    "clean_text": "(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W10-1757", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Kevin, Duh | Katsuhito, Sudoh | Hajime, Tsukada | Hideki, Isozaki | Masaaki, Nagata", 
    "raw_text": "In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process", 
    "clean_text": "In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "D12-1077", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Graham, Neubig | Taro, Watanabe | Shinsuke, Mori", 
    "raw_text": "The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al2006), and extended to use large-margin training in an on line frame work (Watanabe et al2007)", 
    "clean_text": "The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007).", 
    "keep_for_gold": 0
  }
]
