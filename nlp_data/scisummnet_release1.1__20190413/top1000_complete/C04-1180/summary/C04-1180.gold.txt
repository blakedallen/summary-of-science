Wide-Coverage Semantic Representations From A CCG Parser
This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser.
Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation.
We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.
We believe this is a major step towards wide-coverage semantic interpretation, one of the key objectives of the field of NLP.
We present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data.
We consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon.
