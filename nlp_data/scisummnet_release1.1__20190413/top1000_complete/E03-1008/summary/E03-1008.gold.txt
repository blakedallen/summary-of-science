Bootstrapping Statistical Parsers From Small Datasets
We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences.
Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers.
In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material.
We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used.
We examine self-training for PCFG parsing in the small seed case (< 1k labeled data).
We report either minor improvements or significant damage from using self-training for parsing.
We find degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set.
