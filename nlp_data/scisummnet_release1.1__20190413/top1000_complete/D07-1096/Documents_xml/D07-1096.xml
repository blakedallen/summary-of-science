<PAPER>
	<S sid="0">The CoNLL 2007 Shared Task on Dependency Parsing</S><ABSTRACT>
		<S sid="1" ssid="1">The Conference on Computational Natural Language Learning features a shared task, inwhich participants train and test their learn ing systems on the same data sets.</S>
		<S sid="2" ssid="2">In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track.</S>
		<S sid="3" ssid="3">In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages.</S>
		<S sid="4" ssid="4">In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">Previous shared tasks of the Conference on Compu tational Natural Language Learning (CoNLL) havebeen devoted to chunking (1999, 2000), clause iden tification (2001), named entity recognition (2002, 2003), and semantic role labeling (2004, 2005).</S>
			<S sid="6" ssid="6">In 2006 the shared task was multilingual dependency parsing, where participants had to train a single parser on data from thirteen different languages, which enabled a comparison not only of parsing and learning methods, but also of the performance that can be achieved for different languages (Buchholz and Marsi, 2006).</S>
			<S sid="7" ssid="7">In dependency-based syntactic parsing, the task is to derive a syntactic structure for an input sentence by identifying the syntactic head of each word in the sentence.</S>
			<S sid="8" ssid="8">This defines a dependency graph, where the nodes are the words of the input sentence and the arcs are the binary relations from head to dependent.</S>
			<S sid="9" ssid="9">Often, but not always, it is assumed that all words except one have a syntactic head, which means that the graph will be a tree with the single independent word as the root.</S>
			<S sid="10" ssid="10">In labeled dependency parsing, we additionally require the parser to assign a specific type (or label) to each dependency relation holding between a head word and a dependent word.</S>
			<S sid="11" ssid="11">In this year?s shared task, we continue to explore data-driven methods for multilingual dependencyparsing, but we add a new dimension by also intro ducing the problem of domain adaptation.</S>
			<S sid="12" ssid="12">The way this was done was by having two separate tracks: a multilingual track using essentially the same setup as last year, but with partly different languages, and a domain adaptation track, where the task was to usemachine learning to adapt a parser for a single lan guage to a new domain.</S>
			<S sid="13" ssid="13">In total, test results weresubmitted for twenty-three systems in the multilin gual track, and ten systems in the domain adaptationtrack (six of which also participated in the multilingual track).</S>
			<S sid="14" ssid="14">Not everyone submitted papers describ ing their system, and some papers describe more than one system (or the same system in both tracks), which explains why there are only (!)</S>
			<S sid="15" ssid="15">twenty-one papers in the proceedings.</S>
			<S sid="16" ssid="16">In this paper, we provide task definitions for the two tracks (section 2), describe data sets extracted from available treebanks (section 3), report results for all systems in both tracks (section 4), give an overview of approaches used (section 5), provide a first analysis of the results (section 6), and conclude with some future directions (section 7).</S>
			<S sid="17" ssid="17">915</S>
	</SECTION>
	<SECTION title="Task Definition. " number="2">
			<S sid="18" ssid="1">In this section, we provide the task definitions that were used in the two tracks of the CoNLL 2007 Shard Task, the multilingual track and the domain adaptation track, together with some background and motivation for the design choices made.</S>
			<S sid="19" ssid="2">First of all, we give a brief description of the data format and evaluation metrics, which were common to the two tracks.</S>
			<S sid="20" ssid="3">2.1 Data Format and Evaluation Metrics.</S>
			<S sid="21" ssid="4">The data sets derived from the original treebanks (section 3) were in the same column-based format as for the 2006 shared task (Buchholz and Marsi, 2006).</S>
			<S sid="22" ssid="5">In this format, sentences are separated by ablank line; a sentence consists of one or more to kens, each one starting on a new line; and a token consists of the following ten fields, separated by a single tab character: 1.</S>
			<S sid="23" ssid="6">ID: Token counter, starting at 1 for each new.</S>
			<S sid="24" ssid="7">sentence.</S>
			<S sid="25" ssid="8">2.</S>
			<S sid="26" ssid="9">FORM: Word form or punctuation symbol..</S>
			<S sid="27" ssid="10">underscore if not available.</S>
			<S sid="28" ssid="11">4.</S>
			<S sid="29" ssid="12">CPOSTAG: Coarse-grained part-of-speech tag,.</S>
			<S sid="30" ssid="13">where the tagset depends on the language.</S>
			<S sid="31" ssid="14">5.</S>
			<S sid="32" ssid="15">POSTAG: Fine-grained part-of-speech tag,.</S>
			<S sid="33" ssid="16">where the tagset depends on the language, or identical to the coarse-grained part-of-speech tag if not available.</S>
			<S sid="34" ssid="17">6.</S>
			<S sid="35" ssid="18">FEATS: Unordered set of syntactic and/or mor-.</S>
			<S sid="36" ssid="19">phological features (depending on the particu lar language), separated by a vertical bar (|), or an underscore if not available.</S>
			<S sid="37" ssid="20">7.</S>
			<S sid="38" ssid="21">HEAD: Head of the current token, which is. either a value of ID or zero (0).</S>
			<S sid="39" ssid="22">Note that, depending on the original treebank annotation, there may be multiple tokens with HEAD=0.</S>
			<S sid="40" ssid="23">8. DEPREL: Dependency relation to the HEAD..</S>
			<S sid="41" ssid="24">The set of dependency relations depends on the particular language.</S>
			<S sid="42" ssid="25">Note that, dependingon the original treebank annotation, the dependency relation when HEAD=0 may be mean ingful or simply ROOT.</S>
			<S sid="43" ssid="26">9.</S>
			<S sid="44" ssid="27">PHEAD: Projective head of current token,.</S>
			<S sid="45" ssid="28">which is either a value of ID or zero (0), or an underscore if not available.</S>
			<S sid="46" ssid="29">10.</S>
			<S sid="47" ssid="30">PDEPREL: Dependency relation to the.</S>
			<S sid="48" ssid="31">PHEAD, or an underscore if not available.</S>
			<S sid="49" ssid="32">The PHEAD and PDEPREL were not used at all in this year?s data sets (i.e., they always contained underscores) but were maintained for compatibilitywith last year?s data sets.</S>
			<S sid="50" ssid="33">This means that, in prac tice, the first six columns can be considered as input to the parser, while the HEAD and DEPREL fields are the output to be produced by the parser.</S>
			<S sid="51" ssid="34">Labeled training sets contained all ten columns; blind test sets only contained the first six columns; and gold standard test sets (released only after the end of the test period) again contained all ten columns.</S>
			<S sid="52" ssid="35">All data files were encoded in UTF-8.</S>
			<S sid="53" ssid="36">The official evaluation metric in both tracks wasthe labeled attachment score (LAS), i.e., the per centage of tokens for which a system has predicted the correct HEAD and DEPREL, but results were also reported for unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD, and the label accuracy (LA), i.e., the percentage oftokens with correct DEPREL.</S>
			<S sid="54" ssid="37">One important difference compared to the 2006 shared task is that all to kens were counted as ?scoring tokens?, including inparticular all punctuation tokens.</S>
			<S sid="55" ssid="38">The official eval uation script, eval07.pl, is available from the shared task website.1 2.2 Multilingual Track.</S>
			<S sid="56" ssid="39">The multilingual track of the shared task was organized in the same way as the 2006 task, with an notated training and test data from a wide range of languages to be processed with one and the same parsing system.</S>
			<S sid="57" ssid="40">This system must therefore be able to learn from training data, to generalize to unseentest data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters.</S>
			<S sid="58" ssid="41">Par ticipants in the multilingual track were expected to submit parsing results for all languages involved.</S>
			<S sid="59" ssid="42">1http://depparse.uvt.nl/depparse-wiki/SoftwarePage 916 One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order.</S>
			<S sid="60" ssid="43">This explains the interest in recent years for multilingual evaluation of dependency parsers.</S>
			<S sid="61" ssid="44">Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and theparsing methodology proposed by Kudo and Mat sumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English.</S>
			<S sid="62" ssid="45">The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and theparser of Nivre et al (2007) to ten different languages.</S>
			<S sid="63" ssid="46">But by far the largest evaluation of mul tilingual dependency parsing systems so far was the2006 shared task, where nineteen systems were eval uated on data from thirteen languages (Buchholz and Marsi, 2006).</S>
			<S sid="64" ssid="47">One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an important problem for future research.</S>
			<S sid="65" ssid="48">In order to provide an extended empirical foundation for such research, we tried to select the languages and data sets for this year?s task based on the following desiderata:?</S>
			<S sid="66" ssid="49">The selection of languages should be typolog ically varied and include both new languages and old languages (compared to 2006).</S>
			<S sid="67" ssid="50">The creation of the data sets should involve as little conversion as possible from the original treebank annotation, meaning that preference should be given to treebanks with dependency annotation.</S>
			<S sid="68" ssid="51">The training data sets should include at least 50,000 tokens and at most 500,000 tokens.2 The final selection included data from Arabic, Basque, Catalan, Chinese, Czech, English, Greek, Hungarian, Italian, and Turkish.</S>
			<S sid="69" ssid="52">The treebanks from 2The reason for having an upper bound on the training set size was the fact that, in 2006, some participants could not train on all the data for some languages because of time limitations.</S>
			<S sid="70" ssid="53">Similar considerations also led to the decision to have a smaller number of languages this year (ten, as opposed to thirteen).</S>
			<S sid="71" ssid="54">which the data sets were extracted are described in section 3.</S>
			<S sid="72" ssid="55">2.3 Domain Adaptation Track.</S>
			<S sid="73" ssid="56">One well known characteristic of data-driven pars ing systems is that they typically perform muchworse on data that does not come from the training domain (Gildea, 2001).</S>
			<S sid="74" ssid="57">Due to the large over head in annotating text with deep syntactic parse trees, the need to adapt parsers from domains withplentiful resources (e.g., news) to domains with little resources is an important problem.</S>
			<S sid="75" ssid="58">This prob lem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest.Almost all prior work on domain adaptation as sumes one of two scenarios.</S>
			<S sid="76" ssid="59">In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown thatthis may lead to substantial improvements.</S>
			<S sid="77" ssid="60">This includes the work of Roark and Bacchiani (2003), Flo rian et al (2004), Chelba and Acero (2004), Daume?</S>
			<S sid="78" ssid="61">and Marcu (2006), and Titov and Henderson (2006).</S>
			<S sid="79" ssid="62">Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing.</S>
			<S sid="80" ssid="63">The second scenario assumes that there are no annotated resources in the target domain.</S>
			<S sid="81" ssid="64">This is a more realistic situation and is considerably more difficult.</S>
			<S sid="82" ssid="65">Recent work by McClosky et al (2006)and Blitzer et al (2006) have shown that the exis tence of a large unlabeled corpus in the new domain can be leveraged in adaptation.</S>
			<S sid="83" ssid="66">For this shared-task,we are assuming the latter setting ? no annotated re sources in the target domain.</S>
			<S sid="84" ssid="67">Obtaining adequate annotated syntactic resourcesfor multiple languages is already a challenging prob lem, which is only exacerbated when these resources must be drawn from multiple and diverse domains.</S>
			<S sid="85" ssid="68">As a result, the only language that could be feasibly tested in the domain adaptation track was English.</S>
			<S sid="86" ssid="69">The setup for the domain adaptation track was asfollows.</S>
			<S sid="87" ssid="70">Participants were provided with a large an notated corpus from the source domain, in this case sentences from the Wall Street Journal.</S>
			<S sid="88" ssid="71">Participants were also provided with data from three different target domains: biomedical abstracts (developmentdata), chemical abstracts (test data 1), and parent child dialogues (test data 2).</S>
			<S sid="89" ssid="72">Additionally, a large 917unlabeled corpus for each data set (training, devel opment, test) was provided.</S>
			<S sid="90" ssid="73">The goal of the task was to use the annotated source data, plus any unlabeled data, to produce a parser that is accurate for each of the test sets from the target domains.3 Participants could submit systems in either the ?open?</S>
			<S sid="91" ssid="74">or ?closed?</S>
			<S sid="92" ssid="75">class (or both).</S>
			<S sid="93" ssid="76">The closed classrequires a system to use only those resources provided as part of the shared task.</S>
			<S sid="94" ssid="77">The open class al lows a system to use additional resources provided those resources are not drawn from the same domain as the development or test sets.</S>
			<S sid="95" ssid="78">An example might be a part-of-speech tagger trained on the entire PennTreebank and not just the subset provided as train ing data, or a parser that has been hand-crafted or trained on a different training set.</S>
	</SECTION>
	<SECTION title="Treebanks. " number="3">
			<S sid="96" ssid="1">In this section, we describe the treebanks used in the shared task and give relevant information about the data sets created from them.</S>
			<S sid="97" ssid="2">3.1 Multilingual Track.</S>
			<S sid="98" ssid="3">Arabic The analytical syntactic annotation of the Prague Arabic Dependency Treebank (PADT) (Hajic?</S>
			<S sid="99" ssid="4">et al, 2004) can be considered a pure dependency annotation.</S>
			<S sid="100" ssid="5">The conversion, done by Otakar Smrz, from the original format to the column-based format described in section 2.1 was therefore relatively straightforward, although not all the information in the original annotation could be transfered to the new format.</S>
			<S sid="101" ssid="6">PADT was one of the treebanks used in the 2006 shared task but then only contained about 54,000 tokens.</S>
			<S sid="102" ssid="7">Since then, the size of the treebank has more than doubled, with around 112,000 tokens.</S>
			<S sid="103" ssid="8">In addition, the morphological annotation has been made more informative.</S>
			<S sid="104" ssid="9">It is also worth noting that the parsing units in this treebank are in many cases larger than conventional sentences, which partly explains the high average number of tokens per ?sentence?</S>
			<S sid="105" ssid="10">(Buchholz and Marsi, 2006).</S>
			<S sid="106" ssid="11">3Note that annotated development data for the target domainwas only provided for the development domain, biomedical abstracts.</S>
			<S sid="107" ssid="12">For the two test domains, chemical abstracts and parentchild dialogues, the only annotated data sets were the gold stan dard test sets, released only after test runs had been submitted.</S>
			<S sid="108" ssid="13">Basque For Basque, we used the 3LB Basquetreebank (Aduriz et al, 2003).</S>
			<S sid="109" ssid="14">At present, the tree bank consists of approximately 3,700 sentences, 334of which were used as test data.</S>
			<S sid="110" ssid="15">The treebank com prises literary and newspaper texts.</S>
			<S sid="111" ssid="16">It is annotated in a dependency format and was converted to the CoNLL format by a team led by Koldo Gojenola.</S>
			<S sid="112" ssid="17">Catalan The Catalan section of the CESS-ECESyntactically and Semantically Annotated Cor pora (Mart??</S>
			<S sid="113" ssid="18">et al, 2007) is annotated with, among other things, constituent structure and grammatical functions.</S>
			<S sid="114" ssid="19">A head percolation table was used for automatically converting the constituent trees into dependency trees.</S>
			<S sid="115" ssid="20">The original data only contains functions related to the verb, and a function tablewas used for deriving the remaining syntactic func tions.</S>
			<S sid="116" ssid="21">The conversion was performed by a team led by Llu??s Ma`rquez and Anto`nia Mart??.</S>
			<S sid="117" ssid="22">Chinese The Chinese data are taken from theSinica treebank (Chen et al, 2003), which contains both syntactic functions and semantic func tions.</S>
			<S sid="118" ssid="23">The syntactic head was used in the conversion to the CoNLL format, carried out by Yu-Ming Hsieh and the organizers of the 2006 shared task, and thesyntactic functions were used wherever it was pos sible.</S>
			<S sid="119" ssid="24">The training data used is basically the sameas for the 2006 shared task, except for a few correc tions, but the test data is new for this year?s shared task.</S>
			<S sid="120" ssid="25">It is worth noting that the parsing units in this treebank are sometimes smaller than conventionalsentence units, which partly explains the low aver age number of tokens per ?sentence?</S>
			<S sid="121" ssid="26">(Buchholz and Marsi, 2006).</S>
			<S sid="122" ssid="27">Czech The analytical syntactic annotation of the Prague Dependency Treebank (PDT) (Bo?hmova?</S>
			<S sid="123" ssid="28">et al., 2003) is a pure dependency annotation, just as for PADT.</S>
			<S sid="124" ssid="29">It was also used in the shared task 2006, but there are two important changes compared tolast year.</S>
			<S sid="125" ssid="30">First, version 2.0 of PDT was used in stead of version 1.0, and a conversion script wascreated by Zdenek Zabokrtsky, using the new XML based format of PDT 2.0.</S>
			<S sid="126" ssid="31">Secondly, due to the upper bound on training set size, only sections 1?3 of PDT constitute the training data, which amounts to some 450,000 tokens.</S>
			<S sid="127" ssid="32">The test data is a small subset of the development test set of PDT.</S>
			<S sid="128" ssid="33">918English For English we used the Wall Street Jour nal section of the Penn Treebank (Marcus et al,1993).</S>
			<S sid="129" ssid="34">In particular, we used sections 2-11 for training and a subset of section 23 for testing.</S>
			<S sid="130" ssid="35">As a pre processing stage we removed many functions tagsfrom the non-terminals in the phrase structure repre sentation to make the representations more uniformwith out-of-domain test sets for the domain adapta tion track (see section 3.2).</S>
			<S sid="131" ssid="36">The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a).</S>
			<S sid="132" ssid="37">This work was done by Ryan McDonald.</S>
			<S sid="133" ssid="38">Greek The Greek Dependency Treebank(GDT) (Prokopidis et al, 2005) adopts a de pendency structure annotation very similar to those of PDT and PADT, which means that the conversionby Prokopis Prokopidis was relatively straightfor ward.</S>
			<S sid="134" ssid="39">GDT is one of the smallest treebanks in this year?s shared task (about 65,000 tokens) and contains sentences of Modern Greek.</S>
			<S sid="135" ssid="40">Just like PDT and PADT, the treebank contains more than one level of annotation, but we only used the analytical level of GDT.</S>
			<S sid="136" ssid="41">Hungarian For the Hungarian data, the Szegedtreebank (Csendes et al, 2005) was used.</S>
			<S sid="137" ssid="42">The tree bank is based on texts from six different genres, ranging from legal newspaper texts to fiction.</S>
			<S sid="138" ssid="43">Theoriginal annotation scheme is constituent-based, fol lowing generative principles.</S>
			<S sid="139" ssid="44">It was converted into dependencies by Zo?ltan Alexin based on heuristics.</S>
			<S sid="140" ssid="45">Italian The data set used for Italian is a subsetof the balanced section of the Italian Syntactic Semantic Treebank (ISST) (Montemagni et al,2003) and consists of texts from the newspaper Cor riere della Sera and from periodicals.</S>
			<S sid="141" ssid="46">A team led by Giuseppe Attardi, Simonetta Montemagni, and Maria Simi converted the annotation to the CoNLLformat, using information from two different anno tation levels, the constituent structure level and the dependency structure level.</S>
			<S sid="142" ssid="47">Turkish For Turkish we used the METU-Sabanc?</S>
			<S sid="143" ssid="48">Turkish Treebank (Oflazer et al, 2003), which was also used in the 2006 shared task.</S>
			<S sid="144" ssid="49">A new test set of about 9,000 tokens was provided by Gu?ls?en Eryig?it (Eryig?it, 2007), who also handled the conversion to the CoNLL format, which means that we could use all the approximately 65,000 tokens of the originaltreebank for training.</S>
			<S sid="145" ssid="50">The rich morphology of Turkish requires the basic tokens in parsing to be inflec tional groups (IGs) rather than words.</S>
			<S sid="146" ssid="51">IGs of a single word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files).</S>
			<S sid="147" ssid="52">Sentences do not necessarily have a unique root; most internal punctuation and a few foreign words also have HEAD=0.</S>
			<S sid="148" ssid="53">3.2 Domain Adaptation Track.</S>
			<S sid="149" ssid="54">As mentioned previously, the source data is drawn from a corpus of news, specifically the Wall Street Journal section of the Penn Treebank (Marcus et al,1993).</S>
			<S sid="150" ssid="55">This data set is identical to the English train ing set from the multilingual track (see section 3.1).</S>
			<S sid="151" ssid="56">For the target domains we used three different labeled data sets.</S>
			<S sid="152" ssid="57">The first two were annotated as part of the PennBioIE project (Kulick et al, 2004) and consist of sentences drawn from either biomedical or chemical research abstracts.</S>
			<S sid="153" ssid="58">Like the source WSJ corpus, this data is annotated using thePenn Treebank phrase structure scheme.</S>
			<S sid="154" ssid="59">To con vert these sets to dependency structures we used the same procedure as before (Johansson and Nugues,2007a).</S>
			<S sid="155" ssid="60">Additional care was taken to remove sen tences that contained non-WSJ part-of-speech tagsor non-terminals (e.g., HYPH part-of-speech tag in dicating a hyphen).</S>
			<S sid="156" ssid="61">Furthermore, the annotation scheme for gaps and traces was made consistent with the Penn Treebank wherever possible.</S>
			<S sid="157" ssid="62">As already mentioned, the biomedical data set was distributed as a development set for the training phase, while the chemical data set was only used for final testing.</S>
			<S sid="158" ssid="63">The third target data set was taken from theCHILDES database (MacWhinney, 2000), in partic ular the EVE corpus (Brown, 1973), which has beenannotated with dependency structures.</S>
			<S sid="159" ssid="64">Unfortu nately the dependency labels of the CHILDES datawere inconsistent with those of the WSJ, biomedi cal and chemical data sets, and we therefore opted to only evaluate unlabeled accuracy for this data set.</S>
			<S sid="160" ssid="65">Furthermore, there was an inconsistency in how main and auxiliary verbs were annotated for this data set relative to others.</S>
			<S sid="161" ssid="66">As a result of this, submitting 919 Multilingual Domain adaptation Ar Ba Ca Ch Cz En Gr Hu It Tu PCHEM CHILDES Language family Sem.</S>
			<S sid="162" ssid="67">Isol.</S>
			<S sid="163" ssid="68">Rom.</S>
			<S sid="164" ssid="69">Sin.</S>
			<S sid="165" ssid="70">Sla.</S>
			<S sid="166" ssid="71">Ger.</S>
			<S sid="167" ssid="72">Hel.</S>
			<S sid="168" ssid="73">F.-U.</S>
			<S sid="169" ssid="74">Rom.</S>
			<S sid="170" ssid="75">Tur.</S>
			<S sid="171" ssid="76">Ger.</S>
			<S sid="172" ssid="77">Annotation d d c+f c+f d c+f d c+f c+f d c+f d Training data Development data Tokens (k) 112 51 431 337 432 447 65 132 71 65 5 Sentences (k) 2.9 3.2 15.0 57.0 25.4 18.6 2.7 6.0 3.1 5.6 0.2 Tokens/sentence 38.3 15.8 28.8 5.9 17.0 24.0 24.2 21.8 22.9 11.6 25.1 LEMMA Yes Yes Yes No Yes No Yes Yes Yes Yes No No.</S>
			<S sid="173" ssid="78">CPOSTAG 15 25 17 13 12 31 18 16 14 14 25 No.</S>
			<S sid="174" ssid="79">POSTAG 21 64 54 294 59 45 38 43 28 31 37 No.</S>
			<S sid="175" ssid="80">FEATS 21 359 33 0 71 0 31 50 21 78 0 No.</S>
			<S sid="176" ssid="81">DEPREL 29 35 42 69 46 20 46 49 22 25 18 No.</S>
			<S sid="177" ssid="82">DEPREL H=0 18 17 1 1 8 1 22 1 1 1 1 % HEAD=0 8.7 9.7 3.5 16.9 11.6 4.2 8.3 4.6 5.4 12.8 4.0 % HEAD left 79.2 44.5 60.0 24.7 46.9 49.0 44.8 27.4 65.0 3.8 50.0 % HEAD right 12.1 45.8 36.5 58.4 41.5 46.9 46.9 68.0 29.6 83.4 46.0 HEAD=0/sentence 3.3 1.5 1.0 1.0 2.0 1.0 2.0 1.0 1.2 1.5 1.0 % Non-proj.</S>
			<S sid="178" ssid="83">arcs 0.4 2.9 0.1 0.0 1.9 0.3 1.1 2.9 0.5 5.5 0.4 % Non-proj.</S>
			<S sid="179" ssid="84">sent.</S>
			<S sid="180" ssid="85">10.1 26.2 2.9 0.0 23.2 6.7 20.3 26.4 7.4 33.3 8.0 Punc.</S>
			<S sid="181" ssid="86">attached S S A S S A S A A S A DEPRELS for punc.</S>
			<S sid="182" ssid="87">10 13 6 29 16 13 15 1 10 12 8 Test data PCHEM CHILDES Tokens 5124 5390 5016 5161 4724 5003 4804 7344 5096 4513 5001 4999 Sentences 131 334 167 690 286 214 197 390 249 300 195 666 Tokens/sentence 39.1 16.1 30.0 7.5 16.5 23.4 24.4 18.8 20.5 15.0 25.6 12.9 % New words 12.44 24.98 4.35 9.70 12.58 3.13 12.43 26.10 15.07 36.29 31.33 6.10 % New lemmas 2.82 11.13 3.36 n/a 5.28 n/a 5.82 14.80 8.24 9.95 n/a n/a Table 1: Characteristics of the data sets for the 10 languages of the multilingual track and the development set and the two test sets of the domain adaptation track.</S>
			<S sid="183" ssid="88">920results for the CHILDES data was considered op tional.</S>
			<S sid="184" ssid="89">Like the chemical data set, this data set was only used for final testing.</S>
			<S sid="185" ssid="90">Finally, a large corpus of unlabeled in-domaindata was provided for each data set and made avail able for training.</S>
			<S sid="186" ssid="91">This data was drawn from theWSJ, PubMed.com (specific to biomedical and chemical research literature), and the CHILDES data base.The data was tokenized to be as consistent as pos sible with the WSJ training set.</S>
			<S sid="187" ssid="92">3.3 Overview.</S>
			<S sid="188" ssid="93">Table 1 describes the characteristics of the data sets.</S>
			<S sid="189" ssid="94">For the multilingual track, we provide statistics over the training and test sets; for the domain adaptationtrack, the statistics were extracted from the develop ment set.</S>
			<S sid="190" ssid="95">Following last year?s shared task practice (Buchholz and Marsi, 2006), we use the following definition of projectivity: An arc (i, j) is projective iff all nodes occurring between i and j are dominated by i (where dominates is the transitive closure of the arc relation).</S>
			<S sid="191" ssid="96">In the table, the languages are abbreviated to their first two letters.</S>
			<S sid="192" ssid="97">Language families are: Semitic, Isolate, Romance, Sino-Tibetan, Slavic, Germanic, Hellenic, Finno-Ugric, and Turkic.</S>
			<S sid="193" ssid="98">The type of the original annotation is either constituents plus (some)functions (c+f) or dependencies (d).</S>
			<S sid="194" ssid="99">For the train ing data, the number of words and sentences are given in multiples of thousands, and the averagelength of a sentence in words (including punctuation tokens).</S>
			<S sid="195" ssid="100">The following rows contain information about whether lemmas are available, the num ber of coarse- and fine-grained part-of-speech tags, the number of feature components, and the number of dependency labels.</S>
			<S sid="196" ssid="101">Then information is given on how many different dependency labels can co-occurwith HEAD=0, the percentage of HEAD=0 depen dencies, and the percentage of heads preceding (left) or succeeding (right) a token (giving an indication of whether a language is predominantly head-initial or head-final).</S>
			<S sid="197" ssid="102">This is followed by the average numberof HEAD=0 dependencies per sentence and the per centage of non-projective arcs and sentences.</S>
			<S sid="198" ssid="103">The last two rows show whether punctuation tokens are attached as dependents of other tokens (A=Always,S=Sometimes) and specify the number of depen dency labels that exist for punctuation tokens.</S>
			<S sid="199" ssid="104">Note that punctuation is defined as any token belonging to the UTF-8 category of punctuation.</S>
			<S sid="200" ssid="105">This means, for example, that any token having an underscore in the FORM field (which happens for word-internal IGs in Turkish) is also counted as punctuation here.For the test sets, the number of words and sen tences as well as the ratio of words per sentence are listed, followed by the percentage of new words and lemmas (if applicable).</S>
			<S sid="201" ssid="106">For the domain adaptation sets, the percentage of new words is computed with regard to the training set (Penn Treebank).</S>
	</SECTION>
	<SECTION title="Submissions and Results. " number="4">
			<S sid="202" ssid="1">As already stated in the introduction, test runs weresubmitted for twenty-three systems in the multilin gual track, and ten systems in the domain adaptationtrack (six of which also participated in the multilin gual track).</S>
			<S sid="203" ssid="2">In the result tables below, systems are identified by the last name of the teammember listed first when test runs were uploaded for evaluation.</S>
			<S sid="204" ssid="3">In general, this name is also the first author of a paper describing the system in the proceedings, but there are a few exceptions and complications.</S>
			<S sid="205" ssid="4">First of all, for four out of twenty-seven systems, no paper was submitted to the proceedings.</S>
			<S sid="206" ssid="5">This is the case for the systems of Jia, Maes et al, Nash, and Zeman, which is indicated by the fact that these names appear initalics in all result tables.</S>
			<S sid="207" ssid="6">Secondly, two teams sub mitted two systems each, which are described in a single paper by each team.</S>
			<S sid="208" ssid="7">Thus, the systems called ?Nilsson?</S>
			<S sid="209" ssid="8">and ?Hall, J.?</S>
			<S sid="210" ssid="9">are both described in Hall et al.</S>
			<S sid="211" ssid="10">(2007a), while the systems called ?Duan (1)?</S>
			<S sid="212" ssid="11">and ?Duan (2)?</S>
			<S sid="213" ssid="12">are both described in Duan et al (2007).</S>
			<S sid="214" ssid="13">Finally, please pay attention to the fact that there are two teams, where the first author?s last name is Hall.</S>
			<S sid="215" ssid="14">Therefore, we use ?Hall, J.?</S>
			<S sid="216" ssid="15">and ?Hall, K.?, to disambiguate between the teams involving Johan Hall (Hall et al, 2007a) and Keith Hall (Hall et al, 2007b), respectively.</S>
			<S sid="217" ssid="16">Tables 2 and 3 give the scores for the multilingual track in the CoNLL 2007 shared task.</S>
			<S sid="218" ssid="17">The Averagecolumn contains the average score for all ten lan guages, which determines the ranking in this track.Table 4 presents the results for the domain adapta tion track, where the ranking is determined based on the PCHEM results only, since the CHILDES data set was optional.</S>
			<S sid="219" ssid="18">Note also that there are no labeled 921 Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish Nilsson 80.32(1) 76.52(1) 76.94(1) 88.70(1) 75.82(15) 77.98(3) 88.11(5) 74.65(2) 80.27(1) 84.40(1) 79.79(2) Nakagawa 80.29(2) 75.08(2) 72.56(7) 87.90(3) 83.84(2) 80.19(1) 88.41(3) 76.31(1) 76.74(8) 83.61(3) 78.22(5) Titov 79.90(3) 74.12(6) 75.49(3) 87.40(6) 82.14(7) 77.94(4) 88.39(4) 73.52(10) 77.94(4) 82.26(6) 79.81(1) Sagae 79.90(4) 74.71(4) 74.64(6) 88.16(2) 84.69(1) 74.83(8) 89.01(2) 73.58(8) 79.53(2) 83.91(2) 75.91(10) Hall, J. 79.80(5)* 74.75(3) 74.99(5) 87.74(4) 83.51(3) 77.22(6) 85.81(12) 74.21(6) 78.09(3) 82.48(5) 79.24(3) Carreras 79.09(6)* 70.20(11) 75.75(2) 87.60(5) 80.86(10) 78.60(2) 89.61(1) 73.56(9) 75.42(9) 83.46(4) 75.85(11) Attardi 78.27(7) 72.66(8) 69.48(12) 86.86(7) 81.50(8) 77.37(5) 85.85(10) 73.92(7) 76.81(7) 81.34(8) 76.87(7) Chen 78.06(8) 74.65(5) 72.39(8) 86.66(8) 81.24(9) 73.69(10) 83.81(13) 74.42(3) 75.34(10) 82.04(7) 76.31(9) Duan (1) 77.70(9)* 69.91(13) 71.26(9) 84.95(10) 82.58(6) 75.34(7) 85.83(11) 74.29(4) 77.06(5) 80.75(9) 75.03(12) Hall, K. 76.91(10)* 73.40(7) 69.81(11) 82.38(14) 82.77(4) 72.27(12) 81.93(15) 74.21(5) 74.20(11) 80.69(10) 77.42(6) Schiehlen 76.18(11) 70.08(12) 66.77(14) 85.75(9) 80.04(11) 73.86(9) 86.21(9) 72.29(12) 73.90(12) 80.46(11) 72.48(15) Johansson 75.78(12)* 71.76(9) 75.08(4) 83.33(12) 76.30(14) 70.98(13) 80.29(17) 72.77(11) 71.31(13) 77.55(14) 78.46(4) Mannem 74.54(13)* 71.55(10) 65.64(15) 84.47(11) 73.76(17) 70.68(14) 81.55(16) 71.69(13) 70.94(14) 78.67(13) 76.42(8) Wu 73.02(14)* 66.16(14) 70.71(10) 81.44(15) 74.69(16) 66.72(16) 79.49(18) 70.63(14) 69.08(15) 78.79(12) 72.52(14) Nguyen 72.53(15)* 63.58(16) 58.18(17) 83.23(13) 79.77(12) 72.54(11) 86.73(6) 70.42(15) 68.12(17) 75.06(16) 67.63(17) Maes 70.66(16)* 65.12(15) 69.05(13) 79.21(16) 70.97(18) 67.38(15) 69.68(21) 68.59(16) 68.93(16) 73.63(18) 74.03(13) Canisius 66.99(17)* 59.13(18) 63.17(16) 75.44(17) 70.45(19) 56.14(17) 77.27(19) 60.35(18) 64.31(19) 75.57(15) 68.09(16) Jia 63.00(18)* 63.37(17) 57.61(18) 23.35(20) 76.36(13) 54.95(18) 82.93(14) 65.45(17) 66.61(18) 74.65(17) 64.68(18) Zeman 54.87(19) 46.06(20) 50.61(20) 62.94(19) 54.49(20) 50.21(20) 53.59(22) 55.29(19) 55.24(20) 62.13(19) 58.10(19) Marinov 54.55(20)* 54.00(19) 51.24(19) 69.42(18) 49.87(21) 53.47(19) 52.11(23) 54.33(20) 44.47(21) 59.75(20) 56.88(20) Duan (2) 24.62(21)* 82.64(5) 86.69(7) 76.89(6) Nash 8.65(22)* 86.49(8) Shimizu 7.20(23) 72.02(20) Table 2: Labeled attachment score (LAS) for the multilingual track in the CoNLL 2007 shared task.</S>
			<S sid="220" ssid="19">Teams are denoted by the last name of their first member, with italics indicating that there is no corresponding paper in the proceedings.</S>
			<S sid="221" ssid="20">The number in parentheses next to each score gives the rank.</S>
			<S sid="222" ssid="21">A star next to a score in the Average column indicates a statistically significant difference with the next lower rank.</S>
			<S sid="223" ssid="22">Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish Nakagawa 86.55(1)* 86.09(1) 81.04(5) 92.86(4) 88.88(2) 86.28(1) 90.13(2) 84.08(1) 82.49(3) 87.91(1) 85.77(3) Nilsson 85.71(2) 85.81(2) 82.84(1) 93.12(3) 84.52(12) 83.59(4) 88.93(5) 81.22(4) 83.55(1) 87.77(2) 85.77(2) Titov 85.62(3) 83.18(7) 81.93(2) 93.40(1) 87.91(4) 84.19(3) 89.73(4) 81.20(5) 82.18(4) 86.26(6) 86.22(1) Sagae 85.29(4)* 84.04(4) 81.19(3) 93.34(2) 88.94(1) 81.27(8) 89.87(3) 80.37(11) 83.51(2) 87.68(3) 82.72(9) Carreras 84.79(5) 81.48(10) 81.11(4) 92.46(5) 86.20(9) 85.16(2) 90.63(1) 81.37(3) 79.92(9) 87.19(4) 82.41(10) Hall, J. 84.74(6)* 84.21(3) 80.61(6) 92.20(6) 87.60(5) 82.35(6) 86.77(12) 80.66(9) 81.71(6) 86.26(5) 85.04(5) Attardi 83.96(7)* 82.53(8) 76.88(11) 91.41(7) 86.73(8) 83.40(5) 86.99(10) 80.75(8) 81.81(5) 85.54(8) 83.56(7) Chen 83.22(8) 83.49(5) 78.65(8) 90.87(8) 85.91(10) 80.14(11) 84.91(13) 81.16(6) 79.25(11) 85.91(7) 81.92(12) Hall, K. 83.08(9) 83.45(6) 78.55(9) 87.80(15) 87.91(3) 78.47(12) 83.21(15) 82.04(2) 79.34(10) 84.81(9) 85.18(4) Duan (1) 82.77(10) 79.04(13) 77.59(10) 89.71(12) 86.88(7) 80.82(10) 86.97(11) 80.77(7) 80.66(7) 84.20(11) 81.03(13) Schiehlen 82.42(11)* 81.07(11) 73.30(14) 90.79(10) 85.45(11) 81.73(7) 88.91(6) 80.47(10) 78.61(12) 84.54(10) 79.33(15) Johansson 81.13(12)* 80.91(12) 80.43(7) 88.34(13) 81.30(15) 77.39(13) 81.43(18) 79.58(12) 75.53(15) 81.55(15) 84.80(6) Mannem 80.30(13) 81.56(9) 72.88(15) 89.81(11) 78.84(17) 77.20(14) 82.81(16) 78.89(13) 75.39(16) 82.91(12) 82.74(8) Nguyen 80.00(14)* 73.46(18) 69.15(18) 88.12(14) 84.05(13) 80.91(9) 88.01(7) 77.56(15) 78.13(13) 80.40(16) 80.19(14) Jia 78.46(15) 74.20(17) 70.24(16) 90.83(9) 83.39(14) 70.41(18) 84.37(14) 75.65(16) 77.19(14) 82.36(14) 75.96(17) Wu 78.44(16)* 77.05(14) 75.77(12) 85.85(16) 79.71(16) 73.07(16) 81.69(17) 78.12(14) 72.39(18) 82.57(13) 78.15(16) Maes 76.60(17)* 75.47(16) 75.27(13) 84.35(17) 76.57(18) 74.03(15) 71.62(21) 75.19(17) 72.93(17) 78.32(18) 82.21(11) Canisius 74.83(18)* 76.89(15) 70.17(17) 81.64(18) 74.81(19) 72.12(17) 78.23(19) 72.46(18) 67.80(19) 79.08(17) 75.14(18) Zeman 62.02(19)* 58.55(20) 57.42(20) 68.50(20) 62.93(20) 59.19(20) 58.33(22) 62.89(19) 59.78(20) 68.27(19) 64.30(19) Marinov 60.83(20)* 64.27(19) 58.55(19) 74.22(19) 56.09(21) 59.57(19) 54.33(23) 61.18(20) 50.39(21) 65.52(20) 64.13(20) Duan (2) 25.53(21)* 86.94(6) 87.87(8) 80.53(8) Nash 8.77(22)* 87.71(9) Shimizu 7.79(23) 77.91(20) Table 3: Unlabeled attachment scores (UAS) for the multilingual track in the CoNLL 2007 shared task.Teams are denoted by the last name of their first member, with italics indicating that there is no correspond ing paper in the proceedings.</S>
			<S sid="224" ssid="23">The number in parentheses next to each score gives the rank.</S>
			<S sid="225" ssid="24">A star next to a score in the Average column indicates a statistically significant difference with the next lower rank.</S>
			<S sid="226" ssid="25">922 LAS UAS Team PCHEM-c PCHEM-o PCHEM-c PCHEM-o CHILDES-c CHILDES-o Sagae 81.06(1) 83.42(1) Attardi 80.40(2) 83.08(3) 58.67(3) Dredze 80.22(3) 83.38(2) 61.37(1) Nguyen 79.50(4)* 82.04(4)* Jia 76.48(5)* 78.92(5)* 57.43(5) Bick 71.81(6)* 78.48(1)* 74.71(6)* 81.62(1)* 58.07(4) 62.49(1) Shimizu 64.15(7)* 63.49(2) 71.25(7)* 70.01(2)* Zeman 50.61(8) 54.57(8) 58.89(2) Schneider 63.01(3)* 66.53(3)* 60.27(2) Watson 55.47(4) 62.79(4) 45.61(3) Wu 52.89(6) Table 4: Labeled (LAS) and unlabeled (UAS) attachment scores for the closed (-c) and open (-o) classes of the domain adaptation track in the CoNLL 2007 shared task.</S>
			<S sid="227" ssid="26">Teams are denoted by the last name of their first member, with italics indicating that there is no corresponding paper in the proceedings.</S>
			<S sid="228" ssid="27">The number in parentheses next to each score gives the rank.</S>
			<S sid="229" ssid="28">A star next to a score in the PCHEM columns indicates a statistically significant difference with the next lower rank.attachment scores for the CHILDES data set, for reasons explained in section 3.2.</S>
			<S sid="230" ssid="29">The number in paren theses next to each score gives the rank.</S>
			<S sid="231" ssid="30">A star next to a score indicates that the difference with the nextlower rank is significant at the 5% level using a z test for proportions.</S>
			<S sid="232" ssid="31">A more complete presentation of the results, including the significance results for all the tasks and their p-values, can be found on the shared task website.4 Looking first at the results in the multilingual track, we note that there are a number of systems performing at almost the same level at the top of the ranking.</S>
			<S sid="233" ssid="32">For the average labeled attachment score, the difference between the top score (Nilsson) andthe fifth score (Hall, J.)</S>
			<S sid="234" ssid="33">is no more than half a percentage point, and there are generally very few significant differences among the five or six best sys tems, regardless of whether we consider labeled or unlabeled attachment score.</S>
			<S sid="235" ssid="34">For the closed class of the domain adaptation track, we see a very similar pattern, with the top system (Sagae) being followed very closely by two other systems.</S>
			<S sid="236" ssid="35">For the open class, the results are more spread out, but then thereare very few results in this class.</S>
			<S sid="237" ssid="36">It is also worth not ing that the top scores in the closed class, somewhat unexpectedly, are higher than the top scores in the 4http://nextens.uvt.nl/depparse-wiki/AllScores open class.</S>
			<S sid="238" ssid="37">But before we proceed to a more detailed analysis of the results (section 6), we will make an attempt to characterize the approaches represented by the different systems.</S>
	</SECTION>
	<SECTION title="Approaches. " number="5">
			<S sid="239" ssid="1">In this section we give an overview of the models, inference methods, and learning methods used in theparticipating systems.</S>
			<S sid="240" ssid="2">For obvious reasons the dis cussion is limited to systems that are described bya paper in the proceedings.</S>
			<S sid="241" ssid="3">But instead of describ ing the systems one by one, we focus on the basic methodological building blocks that are often foundin several systems although in different combina tions.</S>
			<S sid="242" ssid="4">For descriptions of the individual systems, we refer to the respective papers in the proceedings.</S>
			<S sid="243" ssid="5">Section 5.1 is devoted to system architectures.</S>
			<S sid="244" ssid="6">We then describe the two main paradigms for learning and inference, in this year?s shared task as well as in last year?s, which we call transition-based parsers (section 5.2) and graph-based parsers (section 5.3), adopting the terminology of McDonald and Nivre (2007).5 Finally, we give an overview of the domain adaptation methods that were used (section 5.4).</S>
			<S sid="245" ssid="7">5This distinction roughly corresponds to the distinction made by Buchholz and Marsi (2006) between ?stepwise?</S>
			<S sid="246" ssid="8">and ?all-pairs?</S>
			<S sid="247" ssid="9">approaches.</S>
			<S sid="248" ssid="10">923 5.1 Architectures.</S>
			<S sid="249" ssid="11">Most systems perform some amount of pre- andpost-processing, making the actual parsing compo nent part of a sequential workflow of varying lengthand complexity.</S>
			<S sid="250" ssid="12">For example, most transition based parsers can only build projective dependencygraphs.</S>
			<S sid="251" ssid="13">For languages with non-projective depen dencies, graphs therefore need to be projectivized for training and deprojectivized for testing (Hall et al., 2007a; Johansson and Nugues, 2007b; Titov and Henderson, 2007).</S>
			<S sid="252" ssid="14">Instead of assigning HEAD and DEPREL in a single step, some systems use a two-stage approach for attaching and labeling dependencies (Chen et al, 2007; Dredze et al, 2007).</S>
			<S sid="253" ssid="15">In the first step unlabeled dependencies are generated, in the second step these are labeled.</S>
			<S sid="254" ssid="16">This is particularly helpful for factored parsing models, in which label decisions cannot be easily conditioned on larger parts of the structure due to the increased complexity of inference.</S>
			<S sid="255" ssid="17">Onesystem (Hall et al, 2007b) extends this two-stage ap proach to a three-stage architecture where the parser and labeler generate an n-best list of parses which in turn is reranked.6 In ensemble-based systems several base parsers provide parsing decisions, which are added together for a combined score for each potential dependencyarc.</S>
			<S sid="256" ssid="18">The tree that maximizes the sum of these com bined scores is taken as the final output parse.</S>
			<S sid="257" ssid="19">This technique is used by Sagae and Tsujii (2007) and in the Nilsson system (Hall et al, 2007a).</S>
			<S sid="258" ssid="20">It is worthnoting that both these systems combine transition based base parsers with a graph-based method for parser combination, as first described by Sagae and Lavie (2006).</S>
			<S sid="259" ssid="21">Data-driven grammar-based parsers, such as Bick (2007), Schneider et al (2007), and Watson andBriscoe (2007), need pre- and post-processing in order to map the dependency graphs provided as train ing data to a format compatible with the grammar used, and vice versa.</S>
			<S sid="260" ssid="22">5.2 Transition-Based Parsers.</S>
			<S sid="261" ssid="23">Transition-based parsers build dependency graphs by performing sequences of actions, or transitions.</S>
			<S sid="262" ssid="24">Both learning and inference is conceptualized in 6They also flip the order of the labeler and the reranker.</S>
			<S sid="263" ssid="25">terms of predicting the correct transition based onthe current parser state and/or history.</S>
			<S sid="264" ssid="26">We can fur ther subclassify parsers with respect to the model (or transition system) they adopt, the inference method they use, and the learning method they employ.</S>
			<S sid="265" ssid="27">5.2.1 Models The most common model for transition-based parsers is one inspired by shift-reduce parsing, where a parser state contains a stack of partially processed tokens and a queue of remaining input tokens, and where transitions add dependency arcs and perform stack and queue operations.</S>
			<S sid="266" ssid="28">This type of model is used by the majority of transition-based parsers (Attardi et al, 2007; Duan et al, 2007; Hallet al, 2007a; Johansson and Nugues, 2007b; Man nem, 2007; Titov and Henderson, 2007; Wu et al, 2007).</S>
			<S sid="267" ssid="29">Sometimes it is combined with an explicit probability model for transition sequences, which may be conditional (Duan et al, 2007) or generative (Titov and Henderson, 2007).</S>
			<S sid="268" ssid="30">An alternative model is based on the list-based parsing algorithm described by Covington (2001),which iterates over the input tokens in a sequen tial manner and evaluates for each preceding token whether it can be linked to the current token or not.This model is used by Marinov (2007) and in com ponent parsers of the Nilsson ensemble system (Hall et al, 2007a).</S>
			<S sid="269" ssid="31">Finally, two systems use models based on LR parsing (Sagae and Tsujii, 2007; Watson and Briscoe, 2007).</S>
			<S sid="270" ssid="32">5.2.2 InferenceThe most common inference technique in transition based dependency parsing is greedy deterministic search, guided by a classifier for predicting the next transition given the current parser state and history,processing the tokens of the sentence in sequen tial left-to-right order7 (Hall et al, 2007a; Mannem, 2007; Marinov, 2007; Wu et al, 2007).</S>
			<S sid="271" ssid="33">Optionally multiple passes over the input are conducted until no tokens are left unattached (Attardi et al, 2007).</S>
			<S sid="272" ssid="34">As an alternative to deterministic parsing, several parsers use probabilistic models and maintain a heap or beam of partial transition sequences in order to pick the most probable one at the end of the sentence 7For diversity in parser ensembles, right-to-left parsers are also used.</S>
			<S sid="273" ssid="35">924 (Duan et al, 2007; Johansson and Nugues, 2007b; Sagae and Tsujii, 2007; Titov and Henderson, 2007).</S>
			<S sid="274" ssid="36">One system uses as part of their parsing pipeline a ?neighbor-parser?</S>
			<S sid="275" ssid="37">that attaches adjacent words and a ?root-parser?</S>
			<S sid="276" ssid="38">that identifies the root word(s) of asentence (Wu et al, 2007).</S>
			<S sid="277" ssid="39">In the case of grammar based parsers, a classifier is used to disambiguate in cases where the grammar leaves some ambiguity (Schneider et al, 2007; Watson and Briscoe, 2007) 5.2.3 Learning Transition-based parsers either maintain a classifierthat predicts the next transition or a global proba bilistic model that scores a complete parse.</S>
			<S sid="278" ssid="40">To train these classifiers and probabilitistic models several approaches were used: SVMs (Duan et al, 2007; Hall et al, 2007a; Sagae and Tsujii, 2007), modified finite Newton SVMs (Wu et al, 2007), maximum entropy models (Sagae and Tsujii, 2007), multiclassaveraged perceptron (Attardi et al, 2007) and max imum likelihood estimation (Watson and Briscoe, 2007).In order to calculate a global score or probabil ity for a transition sequence, two systems used a Markov chain approach (Duan et al, 2007; Sagae and Tsujii, 2007).</S>
			<S sid="279" ssid="41">Here probabilities from the output of a classifier are multiplied over the whole sequence of actions.</S>
			<S sid="280" ssid="42">This results in a locally normalized model.</S>
			<S sid="281" ssid="43">Two other entries used MIRA (Mannem,2007) or online passive-aggressive learning (Johansson and Nugues, 2007b) to train a globally normalized model.</S>
			<S sid="282" ssid="44">Titov and Henderson (2007) used an in cremental sigmoid Bayesian network to model the probability of a transition sequence and estimated model parameters using neural network learning.</S>
			<S sid="283" ssid="45">5.3 Graph-Based Parsers.</S>
			<S sid="284" ssid="46">While transition-based parsers use training data to learn a process for deriving dependency graphs, graph-based parsers learn a model of what it meansto be a good dependency graph given an input sen tence.</S>
			<S sid="285" ssid="47">They define a scoring or probability function over the set of possible parses.</S>
			<S sid="286" ssid="48">At learning timethey estimate parameters of this function; at pars ing time they search for the graph that maximizes this function.</S>
			<S sid="287" ssid="49">These parsers mainly differ in the type and structure of the scoring function (model),the search algorithm that finds the best parse (inference), and the method to estimate the function?s pa rameters (learning).</S>
			<S sid="288" ssid="50">5.3.1 Models The simplest type of model is based on a sum oflocal attachment scores, which themselves are cal culated based on the dot product of a weight vector and a feature representation of the attachment.</S>
			<S sid="289" ssid="51">Thistype of scoring function is often referred to as a first order model.8 Several systems participating in this year?s shared task used first-order models (Schiehlen and Spranger, 2007; Nguyen et al, 2007; Shimizu and Nakagawa, 2007; Hall et al, 2007b).</S>
			<S sid="290" ssid="52">Canisius and Tjong Kim Sang (2007) cast the same type ofarc-based factorization as a weighted constraint sat isfaction problem.</S>
			<S sid="291" ssid="53">Carreras (2007) extends the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model.</S>
			<S sid="292" ssid="54">In contrast to previous work where this was constrained to sibling relations of the dependent (McDonald and Pereira, 2006), here head-grandchild relations can be taken into account.</S>
			<S sid="293" ssid="55">In all of the above cases the scoring function isdecomposed into functions that score local proper ties (arcs, pairs of adjacent arcs) of the graph.</S>
			<S sid="294" ssid="56">By contrast, the model of Nakagawa (2007) considersglobal properties of the graph that can take multi ple arcs into account, such as multiple siblings and children of a node.</S>
			<S sid="295" ssid="57">5.3.2 Inference Searching for the highest scoring graph (usually atree) in a model depends on the factorization cho sen and whether we are looking for projective ornon-projective trees.</S>
			<S sid="296" ssid="58">Maximum spanning tree algorithms can be used for finding the highest scor ing non-projective tree in a first-order model (Hall et al, 2007b; Nguyen et al, 2007; Canisius and Tjong Kim Sang, 2007; Shimizu and Nakagawa,2007), while Eisner?s dynamic programming algorithm solves the problem for a first-order factoriza tion in the projective case (Schiehlen and Spranger,2007).</S>
			<S sid="297" ssid="59">Carreras (2007) employs his own exten sion of Eisner?s algorithm for the case of projectivetrees and second-order models that include head grandparent relations.</S>
			<S sid="298" ssid="60">8It is also known as an edge-factored model.</S>
			<S sid="299" ssid="61">925 The methods presented above are mostly efficient and always exact.</S>
			<S sid="300" ssid="62">However, for models that takeglobal properties of the tree into account, they can not be applied.</S>
			<S sid="301" ssid="63">Instead Nakagawa (2007) uses Gibbssampling to obtain marginal probabilities of arcs be ing included in the tree using his global model and then applies a maximum spanning tree algorithm to maximize the sum of the logs of these marginals and return a valid cycle-free parse.</S>
			<S sid="302" ssid="64">5.3.3 Learning Most of the graph-based parsers were trained usingan online inference-based method such as passive aggressive learning (Nguyen et al, 2007; Schiehlen and Spranger, 2007), averaged perceptron (Carreras, 2007), or MIRA (Shimizu and Nakagawa, 2007), while some systems instead used methods based on maximum conditional likelihood (Nakagawa, 2007; Hall et al, 2007b).</S>
			<S sid="303" ssid="65">5.4 Domain Adaptation.</S>
			<S sid="304" ssid="66">5.4.1 Feature-Based ApproachesOne way of adapting a learner to a new domain without using any unlabeled data is to only include fea tures that are expected to transfer well (Dredze et al., 2007).</S>
			<S sid="305" ssid="67">In structural correspondence learning a transformation from features in the source domain to features of the target domain is learnt (Shimizu and Nakagawa, 2007).</S>
			<S sid="306" ssid="68">The original source features along with their transformed versions are then used to train a discriminative parser.</S>
			<S sid="307" ssid="69">5.4.2 Ensemble-Based Approaches Dredze et al (2007) trained a diverse set of parsers in order to improve cross-domain performance byincorporating their predictions as features for an other classifier.</S>
			<S sid="308" ssid="70">Similarly, two parsers trained with different learners and search directions were used in the co-learning approach of Sagae and Tsujii (2007).</S>
			<S sid="309" ssid="71">Unlabeled target data was processed with both parsers.</S>
			<S sid="310" ssid="72">Sentences that both parsers agreed on were then added to the original training data.</S>
			<S sid="311" ssid="73">This combined data set served as training data for one of the original parsers to produce the final system.</S>
			<S sid="312" ssid="74">In a similar fashion, Watson and Briscoe (2007) used a variant of self-training to make use of the unlabeled target data.</S>
			<S sid="313" ssid="75">5.4.3 Other Approaches Attardi et al (2007) learnt tree revision rules for the target domain by first parsing unlabeled target data using a strong parser; this data was then combined with labeled source data; a weak parser was applied to this new dataset; finally tree correction rules are collected based on the mistakes of the weak parser with respect to the gold data and the output of the strong parser.</S>
			<S sid="314" ssid="76">Another technique used was to filter sentences of the out-of-domain corpus based on their similarity to the target domain, as predicted by a classifier (Dredze et al, 2007).</S>
			<S sid="315" ssid="77">Only if a sentence was judged similar to target domain sentences was it included in the training set.Bick (2007) used a hybrid approach, where a data driven parser trained on the labeled training data was given access to the output of a Constraint Grammar parser for English run on the same data.</S>
			<S sid="316" ssid="78">Finally,Schneider et al (2007) learnt collocations and rela tional nouns from the unlabeled target data and used these in their parsing algorithm.</S>
	</SECTION>
	<SECTION title="Analysis. " number="6">
			<S sid="317" ssid="1">Having discussed the major approaches taken in the two tracks of the shared task, we will now return tothe test results.</S>
			<S sid="318" ssid="2">For the multilingual track, we com pare results across data sets and across systems, and report results from a parser combination experiment involving all the participating systems (section 6.1).</S>
			<S sid="319" ssid="3">For the domain adaptation track, we sum up the most important findings from the test results (section 6.2).</S>
			<S sid="320" ssid="4">6.1 Multilingual Track.</S>
			<S sid="321" ssid="5">6.1.1 Across Data Sets The average LAS over all systems varies from 68.07 for Basque to 80.95 for English.</S>
			<S sid="322" ssid="6">Top scores varyfrom 76.31 for Greek to 89.61 for English.</S>
			<S sid="323" ssid="7">In gen eral, there is a good correlation between the top scores and the average scores.</S>
			<S sid="324" ssid="8">For Greek, Italian, and Turkish, the top score is closer to the average score than the average distance, while for Czech, the distance is higher.</S>
			<S sid="325" ssid="9">The languages that produced themost stable results in terms of system ranks with re spect to LAS are Hungarian and Italian.</S>
			<S sid="326" ssid="10">For UAS, Catalan also falls into this group.</S>
			<S sid="327" ssid="11">The language that 926 Setup Arabic Chinese Czech Turkish 2006 without punctuation 66.9 90.0 80.2 65.7 2007 without punctuation 75.5 84.9 80.0 71.6 2006 with punctuation 67.0 90.0 80.2 73.8 2007 with punctuation 76.5 84.7 80.2 79.8 Table 5: A comparison of the LAS top scores from 2006 and 2007.</S>
			<S sid="328" ssid="12">Official scoring conditions in boldface.</S>
			<S sid="329" ssid="13">For Turkish, scores with punctuation also include word-internal dependencies.</S>
			<S sid="330" ssid="14">produced the most unstable results with respect to LAS is Turkish.In comparison to last year?s languages, the lan guages involved in the multilingual track this year can be more easily separated into three classes with respect to top scores: ? Low (76.31?76.94): Arabic, Basque, Greek ? Medium (79.19?80.21): Czech, Hungarian, Turkish ? High (84.40?89.61): Catalan, Chinese, English, Italian It is interesting to see that the classes are more easilydefinable via language characteristics than via char acteristics of the data sets.</S>
			<S sid="331" ssid="15">The split goes across training set size, original data format (constituentvs.</S>
			<S sid="332" ssid="16">dependency), sentence length, percentage of unknown words, number of dependency labels, and ra tio of (C)POSTAGS and dependency labels.</S>
			<S sid="333" ssid="17">The class with the highest top scores contains languages with a rather impoverished morphology.</S>
			<S sid="334" ssid="18">Mediumscores are reached by the two agglutinative lan guages, Hungarian and Turkish, as well as by Czech.</S>
			<S sid="335" ssid="19">The most difficult languages are those that combinea relatively free word order with a high degree of in flection.</S>
			<S sid="336" ssid="20">Based on these characteristics, one would expect to find Czech in the last class.</S>
			<S sid="337" ssid="21">However, theCzech training set is four times the size of the train ing set for Arabic, which is the language with the largest training set of the difficult languages.However, it would be wrong to assume that train ing set size alone is the deciding factor.</S>
			<S sid="338" ssid="22">A closer look at table 1 shows that while Basque and Greekin fact have small training data sets, so do Turkish and Italian.</S>
			<S sid="339" ssid="23">Another factor that may be associated with the above classification is the percent age of new words (PNW) in the test set.</S>
			<S sid="340" ssid="24">Thus, theexpectation would be that the highly inflecting lan guages have a high PNW while the languages with little morphology have a low PNW.</S>
			<S sid="341" ssid="25">But again, thereis no direct correspondence.</S>
			<S sid="342" ssid="26">Arabic, Basque, Cata lan, English, and Greek agree with this assumption: Catalan and English have the smallest PNW, and Arabic, Basque, and Greek have a high PNW.</S>
			<S sid="343" ssid="27">But the PNW for Italian is higher than for Arabic and Greek, and this is also true for the percentage of new lemmas.</S>
			<S sid="344" ssid="28">Additionally, the highest PNW can be found in Hungarian and Turkish, which reach higherscores than Arabic, Basque, and Greek.</S>
			<S sid="345" ssid="29">These con siderations suggest that highly inflected languages with (relatively) free word order need more training data, a hypothesis that will have to be investigated further.</S>
			<S sid="346" ssid="30">There are four languages which were included inthe shared tasks on multilingual dependency parsing both at CoNLL 2006 and at CoNLL 2007: Arabic, Chinese, Czech, and Turkish.</S>
			<S sid="347" ssid="31">For all four lan guages, the same treebanks were used, which allows a comparison of the results.</S>
			<S sid="348" ssid="32">However, in some cases the size of the training set changed, and at least one treebank, Turkish, underwent a thorough correction phase.</S>
			<S sid="349" ssid="33">Table 5 shows the top scores for LAS.</S>
			<S sid="350" ssid="34">Since the official scores excluded punctuation in 2006 but includes it in 2007, we give results both with and without punctuation for both years.For Arabic and Turkish, we see a great improve ment of approximately 9 and 6 percentage points.</S>
			<S sid="351" ssid="35">For Arabic, the number of tokens in the training set doubled, and the morphological annotation was made more informative.</S>
			<S sid="352" ssid="36">The combined effect ofthese changes can probably account for the substan tial improvement in parsing accuracy.</S>
			<S sid="353" ssid="37">For Turkish, the training set grew in size as well, although only by600 sentences, but part of the improvement for Turkish may also be due to continuing efforts in error cor 927 rection and consistency checking.</S>
			<S sid="354" ssid="38">We see that the choice to include punctuation or not makes a large difference for the Turkish scores, since non-final IGs of a word are counted as punctuation (because they have the underscore character as their FORM value), which means that word-internal dependency links are included if punctuation is included.9 However, regardless of whether we compare scores with or without punctuation, we see a genuine improvement of approximately 6 percentage points.</S>
			<S sid="355" ssid="39">For Chinese, the same training set was used.</S>
			<S sid="356" ssid="40">Therefore, the drop from last year?s top score to thisyear?s is surprising.</S>
			<S sid="357" ssid="41">However, last year?s top scor ing system for Chinese (Riedel et al, 2006), which did not participate this year, had a score that wasmore than 3 percentage points higher than the sec ond best system for Chinese.</S>
			<S sid="358" ssid="42">Thus, if we comparethis year?s results to the second best system, the dif ference is approximately 2 percentage points.</S>
			<S sid="359" ssid="43">This final difference may be attributed to the properties of the test sets.</S>
			<S sid="360" ssid="44">While last year?s test set was taken from the treebank, this year?s test set contains texts from other sources.</S>
			<S sid="361" ssid="45">The selection of the textual basis also significantly changed average sentence length: The Chinese training set has an average sentence lengthof 5.9.</S>
			<S sid="362" ssid="46">Last year?s test set alo had an average sen tence length of 5.9.</S>
			<S sid="363" ssid="47">However, this year, the average sentence length is 7.5 tokens, which is a significant increase.</S>
			<S sid="364" ssid="48">Longer sentences are typically harder to parse due to the increased likelihood of ambiguous constructions.</S>
			<S sid="365" ssid="49">Finally, we note that the performance for Czech is almost exactly the same as last year, despite the fact that the size of the training set has been reduced to approximately one third of last year?s training set.It is likely that this in fact represents a relative im provement compared to last year?s results.</S>
			<S sid="366" ssid="50">6.1.2 Across Systems The LAS over all languages ranges from 80.32 to54.55.</S>
			<S sid="367" ssid="51">The comparison of the system ranks averaged over all languages with the ranks for single lan 9The decision to include word-internal dependencies in thisway can be debated on the grounds that they can be parsed de terministically.</S>
			<S sid="368" ssid="52">On the other hand, they typically correspond toregular dependencies captured by function words in other lan guages, which are often easy to parse as well.</S>
			<S sid="369" ssid="53">It is thereforeunclear whether scores are more inflated by including word internal dependencies or deflated by excluding them.</S>
			<S sid="370" ssid="54">guages show considerably more variation than last year?s systems.</S>
			<S sid="371" ssid="55">Buchholz and Marsi (2006) report that ?[f]or most parsers, their ranking differs at most a few places from their overall ranking?.</S>
			<S sid="372" ssid="56">This year,for all of the ten best performing systems with re spect to LAS, there is at least one language for which their rank is at least 5 places different from theiroverall rank.</S>
			<S sid="373" ssid="57">The most extreme case is the top per forming Nilsson system (Hall et al, 2007a), which reached rank 1 for five languages and rank 2 fortwo more languages.</S>
			<S sid="374" ssid="58">Their only outlier is for Chi nese, where the system occupies rank 14, with a LAS approximately 9 percentage points below the top scoring system for Chinese (Sagae and Tsujii, 2007).</S>
			<S sid="375" ssid="59">However, Hall et al (2007a) point out that the official results for Chinese contained a bug, and the true performance of their system was actuallymuch higher.</S>
			<S sid="376" ssid="60">The greatest improvement of a system with respect to its average rank occurs for En glish, for which the system by Nguyen et al (2007) improved from the average rank 15 to rank 6.</S>
			<S sid="377" ssid="61">Twomore outliers can be observed in the system of Jo hansson and Nugues (2007b), which improves from its average rank 12 to rank 4 for Basque and Turkish.</S>
			<S sid="378" ssid="62">The authors attribute this high performance to their parser?s good performance on small training sets.However, this hypothesis is contradicted by their re sults for Greek and Italian, the other two languages with small training sets.</S>
			<S sid="379" ssid="63">For these two languages, the system?s rank is very close to its average rank.</S>
			<S sid="380" ssid="64">6.1.3 An Experiment in System Combination Having the outputs of many diverse dependencyparsers for standard data sets opens up the interest ing possibility of parser combination.</S>
			<S sid="381" ssid="65">To combine the outputs of each parser we used the method of Sagae and Lavie (2006).</S>
			<S sid="382" ssid="66">This technique assigns to each possible labeled dependency a weight that isequal to the number of systems that included the de pendency in their output.</S>
			<S sid="383" ssid="67">This can be viewed as an arc-based voting scheme.</S>
			<S sid="384" ssid="68">Using these weightsit is possible to search the space of possible depen dency trees using directed maximum spanning tree algorithms (McDonald et al, 2005).</S>
			<S sid="385" ssid="69">The maximum spanning tree in this case is equal to the tree that on average contains the labeled dependencies that most systems voted for.</S>
			<S sid="386" ssid="70">It is worth noting that variants of this scheme were used in two of the participating 928 5 10 15 20Number of Systems 80 82 84 86 88 Accu racy Unlabeled AccuracyLabeled Accuracy Figure 1: System Combination systems, the Nilsson system (Hall et al, 2007a) and the system of Sagae and Tsujii (2007).Figure 1 plots the labeled and unlabeled accuracies when combining an increasing number of sys tems.</S>
			<S sid="387" ssid="71">The data used in the plot was the output of allcompeting systems for every language in the multilingual track.</S>
			<S sid="388" ssid="72">The plot was constructed by sorting the systems based on their average labeled accuracy scores over all languages, and then incremen tally adding each system in descending order.10 We can see that both labeled and unlabeled accuracy are significantly increased, even when just the top three systems are included.</S>
			<S sid="389" ssid="73">Accuracy begins to degrade gracefully after about ten different parsers have been added.</S>
			<S sid="390" ssid="74">Furthermore, the accuracy never falls below the performance of the top three systems.</S>
			<S sid="391" ssid="75">6.2 Domain Adaptation Track.</S>
			<S sid="392" ssid="76">For this task, the results are rather surprising.</S>
			<S sid="393" ssid="77">A lookat the LAS and UAS for the chemical research ab stracts shows that there are four closed systems that outperform the best scoring open system.</S>
			<S sid="394" ssid="78">The best system (Sagae and Tsujii, 2007) reaches an LAS of 81.06 (in comparison to their LAS of 89.01 for theEnglish data set in the multilingual track).</S>
			<S sid="395" ssid="79">Consider ing that approximately one third of the words of the chemical test set are new, the results are noteworthy.</S>
			<S sid="396" ssid="80">The next surprise is to be found in the relatively low UAS for the CHILDES data.</S>
			<S sid="397" ssid="81">At a first glance, this data set has all the characteristics of an easy 10The reason that there is no data point for two parsers is that the simple voting scheme adopted only makes sense with at least three parsers voting.</S>
			<S sid="398" ssid="82">set; the average sentence is short (12.9 words), and the percentage of new words is also small (6.10%).</S>
			<S sid="399" ssid="83">Despite these characteristics, the top UAS reaches 62.49 and is thus more than 10 percentage points below the top UAS for the chemical data set.</S>
			<S sid="400" ssid="84">One major reason for this is that auxiliary and main verb dependencies are annotated differently in the CHILDES data than in the WSJ training set.</S>
			<S sid="401" ssid="85">As aresult of this discrepancy, participants were not re quired to submit results for the CHILDES data.</S>
			<S sid="402" ssid="86">The best performing system on the CHILDES corpus is an open system (Bick, 2007), but the distance tothe top closed system is approximately 1 percent age point.</S>
			<S sid="403" ssid="87">In this domain, it seems more feasible touse general language resources than for the chemi cal domain.</S>
			<S sid="404" ssid="88">However, the results prove that the extra effort may be unnecessary.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="7">
			<S sid="405" ssid="1">Two years of dependency parsing in the CoNLL shared task has brought an enormous boost to thedevelopment of dependency parsers for multiple lan guages (and to some extent for multiple domains).But even though nineteen languages have been covered by almost as many different parsing and learn ing approaches, we still have only vague ideas about the strengths and weaknesses of different methodsfor languages with different typological characteris tics.</S>
			<S sid="406" ssid="2">Increasing our knowledge of the multi-causal relationship between language structure, annotation scheme, and parsing and learning methods probablyremains the most important direction for future re search in this area.</S>
			<S sid="407" ssid="3">The outputs of all systems for alldata sets from the two shared tasks are freely avail able for research and constitute a potential gold mine for comparative error analysis across languages and systems.</S>
			<S sid="408" ssid="4">For domain adaptation we have barely scratched the surface so far.</S>
			<S sid="409" ssid="5">But overcoming the bottleneckof limited annotated resources for specialized do mains will be as important for the deployment of human language technology as being able to handle multiple languages in the future.</S>
			<S sid="410" ssid="6">One result fromthe domain adaptation track that may seem surprising at first is the fact that closed class systems outperformed open class systems on the chemical ab stracts.</S>
			<S sid="411" ssid="7">However, it seems that the major problem in 929 adapting pre-existing parsers to the new domain was not the domain as such but the mapping from the native output of the parser to the kind of annotationprovided in the shared task data sets.</S>
			<S sid="412" ssid="8">Thus, find ing ways of reusing already invested development efforts by adapting the outputs of existing systemsto new requirements, without substantial loss in ac curacy, seems to be another line of research that may be worth pursuing.</S>
			<S sid="413" ssid="9">AcknowledgmentsFirst and foremost, we want to thank all the peo ple and organizations that generously provided us with treebank data and helped us prepare the data sets and without whom the shared task would have been literally impossible: Otakar Smrz, CharlesUniversity, and the LDC (Arabic); Maxux Aranzabe, Kepa Bengoetxea, Larraitz Uria, Koldo Gojenola, and the University of the Basque Coun try (Basque); Ma.</S>
			<S sid="414" ssid="10">Anto`nia Mart??</S>
			<S sid="415" ssid="11">Anton??n, Llu??s Ma`rquez, Manuel Bertran, Mariona Taule?, DifdaMonterde, Eli Comelles, and CLiC-UB (Cata lan); Shih-Min Li, Keh-Jiann Chen, Yu-Ming Hsieh, and Academia Sinica (Chinese); Jan Hajic?, Zdenek Zabokrtsky, Charles University, and the LDC (Czech); Brian MacWhinney, Eric Davis, the CHILDES project, the Penn BioIE project, and the LDC (English); Prokopis Prokopidis and ILSP(Greek); Csirik Ja?nos and Zolta?n Alexin (Hun garian); Giuseppe Attardi, Simonetta Montemagni, Maria Simi, Isidoro Barraco, Patrizia Topi, Kiril Ribarov, Alessandro Lenci, Nicoletta Calzolari, ILC, and ELRA (Italian); Gu?ls?en Eryig?it, Kemal Oflazer, and Ruket C?ak?c?</S>
			<S sid="416" ssid="12">(Turkish).</S>
			<S sid="417" ssid="13">Secondly, we want to thank the organizers of last year?s shared task, Sabine Buchholz, Amit Dubey, Erwin Marsi, and Yuval Krymolowski, who solved all the really hard problems for us and answered all our questions, as well as our colleagues who helped review papers: Jason Baldridge, Sabine Buchholz,James Clarke, Gu?ls?en Eryig?it, Kilian Evang, Ju lia Hockenmaier, Yuval Krymolowski, Erwin Marsi, Bea?ta Megyesi, Yannick Versley, and Alexander Yeh.</S>
			<S sid="418" ssid="14">Special thanks to Bertjan Busser and Erwin Marsi for help with the CoNLL shared task website and many other things, and to Richard Johansson for letting us use his conversion tool for English.</S>
			<S sid="419" ssid="15">Thirdly, we want to thank the program chairs for EMNLP-CoNLL 2007, Jason Eisner and Taku Kudo, the publications chair, Eric Ringger, the SIGNLL officers, Antal van den Bosch, Hwee Tou Ng, and Erik Tjong Kim Sang, and members of the LDC staff, Tony Castelletto and Ilya Ahtaridis, for great cooperation and support.</S>
			<S sid="420" ssid="16">Finally, we want to thank the following people,who in different ways assisted us in the organi zation of the CoNLL 2007 shared task: Giuseppe Attardi, Eckhard Bick, Matthias Buch-Kromann,Xavier Carreras, Tomaz Erjavec, Svetoslav Mari nov, Wolfgang Menzel, Xue Nianwen, Gertjan van Noord, Petya Osenova, Florian Schiel, Kiril Simov, Zdenka Uresova, and Heike Zinsmeister.</S>
	</SECTION>
</PAPER>
