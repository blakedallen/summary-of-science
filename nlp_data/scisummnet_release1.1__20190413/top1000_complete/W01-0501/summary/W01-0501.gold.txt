Limitations Of Co-Training For Natural Language Learning From Large Datasets
Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data.
This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.
This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.
Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data.
However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement.
To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling.
Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks.
We show that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned.
