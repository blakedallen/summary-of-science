<PAPER>
  <S sid="0">Limitations Of Co-Training For Natural Language Learning From Large Datasets</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data.</S>
    <S sid="2" ssid="2">This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.</S>
    <S sid="3" ssid="3">This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.</S>
    <S sid="4" ssid="4">Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data.</S>
    <S sid="5" ssid="5">However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement.</S>
    <S sid="6" ssid="6">To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling.</S>
    <S sid="7" ssid="7">Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="8" ssid="1">Co-Training (Blum and Mitchell, 1998) is a weakly supervised paradigm for learning a classification task from a small set of labeled data and a large set of unlabeled data, using separate, but redundant, views of the data.</S>
    <S sid="9" ssid="2">While previous research (summarized in Section 2) has investigated the theoretical basis of co-training, this study is motivated by practical concerns.</S>
    <S sid="10" ssid="3">We seek to apply the co-training paradigm to problems in natural language learning, with the goal of reducing the amount of humanannotated data required for developing natural language processing components.</S>
    <S sid="11" ssid="4">In particular, many natural language learning tasks contrast sharply with the classification tasks previously studied in conjunction with co-training in that they require hundreds of thousands, rather than hundreds, of training examples.</S>
    <S sid="12" ssid="5">Consequently, our focus on natural language learning raises the question of how co-training scales when a large number of training examples are required to achieve usable performance levels.</S>
    <S sid="13" ssid="6">This case study of co-training for natural language learning addresses the scalability question using the task of base noun phrase identification.</S>
    <S sid="14" ssid="7">For this task, co-training reduces by 36% the difference in error between classifiers trained on 500 labeled examples and classifiers trained on 211,000 labeled examples.</S>
    <S sid="15" ssid="8">While this result is satisfying, further investigation reveals that deterioration in the quality of the labeled data accumulated by co-training hinders further improvement.</S>
    <S sid="16" ssid="9">We address this problem with a moderately supervised variant, corrected co-training, that employs a human annotator to correct the errors made during bootstrapping.</S>
    <S sid="17" ssid="10">Corrected co-training proves to be quite successful, bridging the remaining gap in accuracy.</S>
    <S sid="18" ssid="11">Analysis of corrected co-training illuminates an interesting tension within weakly supervised learning, between the need to bootstrap accurate labeled data, and the need to cover the desired task.</S>
    <S sid="19" ssid="12">We evaluate one approach, using corrected co-training, to resolving this tension; and as another approach, we suggest combining weakly supervised learning with active learning (Cohn et al., 1994).</S>
    <S sid="20" ssid="13">The next section of this paper introduces issues and concerns surrounding co-training.</S>
    <S sid="21" ssid="14">Sections 3 and 4 describe the base noun phrase bracketing task, and the application of cotraining to the task, respectively.</S>
    <S sid="22" ssid="15">Section 5 contains an evaluation of co-training for base noun identification.</S>
  </SECTION>
  <SECTION title="2 Theoretical and Practical Considerations for Co-Training" number="2">
    <S sid="23" ssid="1">The co-training paradigm applies when accurate classification hypotheses for a task can be learned from either of two sets of features of the data, each called a view.</S>
    <S sid="24" ssid="2">For example, Blum and Mitchell (1998) describe a web page classification task, in which the goal is to determine whether or not a given web page is a university faculty member's home page.</S>
    <S sid="25" ssid="3">For this task, they suggest the following two views: (1) the words contained in the text of the page; for example, research interests or publications; (2) the words contained in links pointing to the page; for example, my advisor.</S>
    <S sid="26" ssid="4">The intuition behind Blum and Mitchell's cotraining algorithm CT' (Figure 1) is that two views of the data can be used to train two classifiers that can help each other.</S>
    <S sid="27" ssid="5">Each classifier is trained using one view of the labeled data.</S>
    <S sid="28" ssid="6">Then it predicts labels for instances of the unlabeled data.</S>
    <S sid="29" ssid="7">By selecting its most confident predictions and adding the corresponding instances with their predicted labels to the labeled data, each classifier can add to the other's available training data.</S>
    <S sid="30" ssid="8">Continuing the above example, web pages pointed to by my advisor links can be used to train the page classifier, while web pages about research interests and publications can be used to train the link classifier.</S>
    <S sid="31" ssid="9">Initial studies of co-training focused on the applicability of the co-training paradigm, and in particular, on clarifying the assumptions needed to ensure the effectiveness of the CT algorithm.</S>
    <S sid="32" ssid="10">Blum and Mitchell (1998) presented a PAC-style analysis of co-training, introducing the concept of compatibility between the target function and the unlabeled data: that is, the target function should assign the same label to an instance regardless of which view it sees.</S>
    <S sid="33" ssid="11">They made two additional important points: first, that each view of the data should itself be sufficient for learning the classification task; and repeat until done train classifier h1 on view V1 of L train classifier h2 on view V2 of L allow h1 to posit labels for examples in U allow h2 to posit labels for examples in U add hi's most confidently labeled examples to L add h2's most confidently labeled examples to L second, that the views should be conditionally independent of each other in order to be useful.</S>
    <S sid="34" ssid="12">They proved that under these assumptions, a task that is learnable with random classification noise is learnable with co-training.</S>
    <S sid="35" ssid="13">In experiments with the CT algorithm, they noticed that it is important to preserve the distribution of class labels in the growing body of labeled data.</S>
    <S sid="36" ssid="14">Finally, they demonstrated the effectiveness of co-training on a web page classification task similar to that described above.</S>
    <S sid="37" ssid="15">Collins and Singer (1999) were concerned that the CT algorithm does not strongly enforce the requirement that hypothesis functions should be compatible with the unlabeled data.</S>
    <S sid="38" ssid="16">They introduced an algorithm, CoBoost, that directly minimizes mismatch between views of the unlabeled data, using a combination of ideas from co-training and AdaBoost (Freund and Shapire, 1997).</S>
    <S sid="39" ssid="17">Nigam and Ghani (2000) performed the most thorough empirical investigation of the desideratum of conditional independence of views underlying co-training.</S>
    <S sid="40" ssid="18">Their experiments suggested that view independence does indeed affect the performance of co-training; but that CT, when compared to other algorithms that use labeled and unlabeled data, such as EM (Dempster et al., 1977; Nigam et al., 2000), may still prove effective even when an explicit feature split is unknown, provided that there is enough implicit redundancy in the data.</S>
    <S sid="41" ssid="19">In contrast to previous investigations of the theoretical basis of co-training, this study is motivated by practical concerns about the application of weakly supervised learning to problems in natural language learning (NLL).</S>
    <S sid="42" ssid="20">Many NLL tasks contrast in two ways with the web page classification task studied in previous work on co-training.</S>
    <S sid="43" ssid="21">First, the web page task factors naturally into page and link views, while other NLL tasks may not have such natural views.</S>
    <S sid="44" ssid="22">Second, many NLL problems require hundreds of thousands of training examples, while the web page task can be learned using hundreds of examples.</S>
    <S sid="45" ssid="23">Consequently, our focus on natural language learning introduces new questions about the scalability of the co-training paradigm.</S>
    <S sid="46" ssid="24">First, can co-training be applied to learning problems without natural factorizations into views?</S>
    <S sid="47" ssid="25">Nigam and Ghani's study suggests a qualified affirmative answer to this question, for a text classification task designed to contain redundant information; however, it is desirable to continue investigation of the issue for large-scale NLL tasks.</S>
    <S sid="48" ssid="26">Second, how does co-training scale when a large number of training examples are required to achieve usable performance levels?</S>
    <S sid="49" ssid="27">It is plausible to expect that the CT algorithm will not scale well, due to mistakes made by the view classifiers.</S>
    <S sid="50" ssid="28">To elaborate, the view classifiers may occasionally add incorrectly labeled instances to the labeled data.</S>
    <S sid="51" ssid="29">If many iterations of CT are required for learning the task, degradation in the quality of the labeled data may become a problem, in turn affecting the quality of subsequent view classifiers.</S>
    <S sid="52" ssid="30">For large-scale learning tasks, the effectiveness of co-training may be dulled over time.</S>
    <S sid="53" ssid="31">Finally, we note that the accuracy of automatically accumulated training data is an important issue for many bootstrapping learning methods (e.g.</S>
    <S sid="54" ssid="32">Yarowsky (1995), Riloff and Jones (1999)), suggesting that the rewards of understanding and dealing with this issue may be significant.</S>
  </SECTION>
  <SECTION title="3 Base Noun Phrase Identification" number="3">
    <S sid="55" ssid="1">Base noun phrases (base NPs) are traditionally defined as nonrecursive noun phrases, i.e.</S>
    <S sid="56" ssid="2">NPs that do not contain NPs.</S>
    <S sid="57" ssid="3">(Figure 2a illustrates base NPs with a short example.)</S>
    <S sid="58" ssid="4">Base noun phrase identification is the task of locating the base NPs in a sentence from the words of the sentence and their part-of-speech tags.</S>
    <S sid="59" ssid="5">Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g.</S>
    <S sid="60" ssid="6">Mitra et al. (1997)) and question answering (e.g.</S>
    <S sid="61" ssid="7">Cardie et al. (2000)) systems.</S>
    <S sid="62" ssid="8">Many corpus-based methods have been applied to the task, including statistical methods (e.g.</S>
    <S sid="63" ssid="9">Church (1988)), transformation-based learning (e.g.</S>
    <S sid="64" ssid="10">Ramshaw and Marcus (1998)), rote sequence learning (e.g.</S>
    <S sid="65" ssid="11">Cardie and Pierce (1998)), memory-based sequence learning (e.g.</S>
    <S sid="66" ssid="12">Argamon et al. (1999)), and memory-based learning (e.g.</S>
    <S sid="67" ssid="13">Sang and Veenstra (1999)), among others.</S>
    <S sid="68" ssid="14">Our case study employs a well-known bracket representation, introduced by Ramshaw and Marcus, wherein each word of a sentence is tagged with one of the following tags: I, meaning the word is within a bracket (inside); 0, meaning the word is not within a bracket (outside); or B, meaning the word is within a bracket, but not the same bracket as the preceding word, i.e. the word begins a new bracket.</S>
    <S sid="69" ssid="15">Thus, the bracketing task is transformed into a word tagging task.</S>
    <S sid="70" ssid="16">Figure 2b repeats the example sentence, showing the JOB tag representation.</S>
    <S sid="71" ssid="17">Training examples for JOB tagging have the form where wo is the focus word (i.e. the word whose tag is to be learned) and to is its syntactic category (i.e. part-of-speech) tag.</S>
    <S sid="72" ssid="18">Words to the left and right of the focus word are included for context.</S>
    <S sid="73" ssid="19">Finally, / is the JOB tag of wo.</S>
    <S sid="74" ssid="20">Figure 2c illustrates a few instances taken from the example sentence.</S>
    <S sid="75" ssid="21">We chose naive bayes classifiers for the study, first, because they are convenient to use and, indeed, have been used in previous co-training studies; and second, because they are particularly well-suited to co-training by virtue of calculating probabilities for each prediction.</S>
    <S sid="76" ssid="22">For an instance x, the classifier determines the maximum a posteriori label as follows.</S>
    <S sid="77" ssid="23">In experiments with these naive bayes JOB classifiers, we found that very little accuracy was sacrificed when the word information (i.e. wi) was ignored by the classifier.2 We therefore substitute the simpler term P(ti 1/) for P(wiltill) above.</S>
    <S sid="78" ssid="24">The probabilities P(ti 1/) are estimated from the training data by determining the fraction of the instances labeled 1 that have syntactic Here N(x) denotes the frequency of event x in the training data.</S>
    <S sid="79" ssid="25">This estimate smoothes the training probability by including virtual (unseen) samples for each part-of-speech tag (of which there are 45).</S>
  </SECTION>
  <SECTION title="4 Co-Training for JOB Classifiers" number="4">
    <S sid="80" ssid="1">To apply co-training, the base NP classification task must first be factored into views.</S>
    <S sid="81" ssid="2">For the JOB instances (vectors of part-of-speech tags indexed from &#8212;k to k) a view corresponds to a subset of the set of indices {&#8212;k, , k} .</S>
    <S sid="82" ssid="3">The most natural views are perhaps {&#8212;k, , Of and {0, , k}, indicating that one classifier looks at the focus tag and the tags to its left, while the other looks at the focus tag and the tags to its right.</S>
    <S sid="83" ssid="4">Note that these views certainly violate the desideratum of conditional independence between view features since both include the focus tag.</S>
    <S sid="84" ssid="5">Other views, such as left/right views omitting the focus tag, for example, may be more theoretically attractive, but we found that the left/right views including focus proved most effectual in practice.</S>
    <S sid="85" ssid="6">The JOB tagging task requires some minor modifications to the CT algorithm.</S>
    <S sid="86" ssid="7">First, it is impractical for the co-training classifiers to predict labels for each instance from the enormous set of unlabeled data.</S>
    <S sid="87" ssid="8">Instead, a smaller data pool is maintained, fed with randomly selected instances from the larger set.3 Second, the JOB tagging task is a ternary, rather than a binary, classification.</S>
    <S sid="88" ssid="9">Furthermore, the distribution of labels in the training data is more unbalanced than the distribution of positive and negative examples in the web page task: namely, 53.9% of examples are labeled I, 44.0% 0, and 2.1% B.</S>
    <S sid="89" ssid="10">Since it is impractical to add, say, 27 I, 22 0, and 1 B, to the labeled data at each step of co-training, instead, instances are selected by first choosing a label 1 at random according to the label distribution, then adding the instance 3This standard modification was introduced by Blum and Mitchell (1998) in an effort to cover the underlying distribution of unlabeled instances; however, Nigam and Ghani (2000) found it to be unnecessary in their experiments. train classifier h1 on view V1 of L train classifier h2 on view V2 of L transfer randomly selected examples from U to U' until = u for he h2} allow h to posit labels for all examples in U' repeat g times select label 1 at random according to DL most confidently labeled 1 to the labeled data.</S>
    <S sid="90" ssid="11">This procedure preserves the distribution of labels in the labeled data as instances are labeled and added.</S>
    <S sid="91" ssid="12">The modified CT algorithm is presented in Figure 3.</S>
  </SECTION>
  <SECTION title="5 Evaluation" number="5">
    <S sid="92" ssid="1">We evaluate co-training for JOB classification using a standard data set assembled by Ramshaw and Marcus from sections 15 18 (training data, 211727 instances) and 20 (test data, 47377 instances) of the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>
    <S sid="93" ssid="2">Training instances consist of part-ofspeech tag and JOB label for a focus word, along with contexts of two part-of-speech tags to the left and right of the focus.</S>
    <S sid="94" ssid="3">Our goal accuracy of 95.17% is the performance of a supervised JOB classifier trained on the correctly labeled version of the full training data.</S>
    <S sid="95" ssid="4">(In our experiments the goal classifier uses the left view of the data, which actually outperforms the combined left/right view.)</S>
    <S sid="96" ssid="5">For initial labeled data, the first L instances of the training data are given their correct labels.</S>
    <S sid="97" ssid="6">We determined the best setting for the parameters of the CT algorithm by testing multiple values: L (initial amount of labeled data) varied from 10 to 5000, then u (pool size) from 200 to 5000, then g (growth size) from 1 to 50.</S>
    <S sid="98" ssid="7">The best setting, in terms of effectiveness of co-training in improving the accuracy of the classifier, was L = 500,u = 1000,g = 5.</S>
    <S sid="99" ssid="8">These values are used throughout the evaluation unless noted otherwise.</S>
    <S sid="100" ssid="9">Co-Training.</S>
    <S sid="101" ssid="10">We observe the progress of the co-training process by determining, at each iteration, the accuracy of the co-training classifiers over the test data.</S>
    <S sid="102" ssid="11">We also record the accuracy of the growing body of labeled data.</S>
    <S sid="103" ssid="12">These measurements can be plotted to depict a learning curve, indicating the progress of cotraining as the classifier accuracy changes.</S>
    <S sid="104" ssid="13">Figure 4 presents two representative curves, one for the left context classifier and one for the labeled data.</S>
    <S sid="105" ssid="14">(The right context classifier behaves similarly to the left, but its performance is slightly worse.)</S>
    <S sid="106" ssid="15">As shown, co-training results in improvement in test accuracy over the initial classifier after about 160 iterations, reducing by 36% the difference in error between the co-training classifier and the goal classifier.</S>
    <S sid="107" ssid="16">Unfortunately, the improvement in test accuracy does not continue as co-training progresses; rather, performance peaks, then declines somewhat before stabilizing at around 92.5%.</S>
    <S sid="108" ssid="17">We hypothesize that this decline is due to degradation in the quality of the labeled data.</S>
    <S sid="109" ssid="18">This hypothesis is supported by Figure 4b, indicating that labeled data accuracy decreases steadily before stabilizing at around 94%.</S>
    <S sid="110" ssid="19">Note that the accuracy of the classifier stabilizes at a point a bit lower than the stable accuracy of the labeled data, as would be expected if labeled data quality hinders further improvement from cotraining.</S>
    <S sid="111" ssid="20">Furthermore, co-training for base NP identification seems to be quite sensitive to the CT parameter settings.</S>
    <S sid="112" ssid="21">For example, with L = 200 the co-training classifiers appear not to be accurate enough to sustain co-training, while with L = 1000, they are too accurate, in the sense that co-training contributes very little accuracy before the labeled data deteriorates (Figure 5).</S>
    <S sid="113" ssid="22">In the next sections, we address the problems of data degradation and parameter sensitivity for co-training.</S>
    <S sid="114" ssid="23">Corrected Co-Training.</S>
    <S sid="115" ssid="24">As shown above, the degradation of the labeled data introduces a scalability problem for co-training because successive view classifiers use successively poorer quality data for training.</S>
    <S sid="116" ssid="25">A straightforward solution to this problem is to have a human anized, as co-training achieves 95.03% accuracy, just 0.14% away from the goal, after 600 iterations (and reaches 95.12% after 800 iterations).</S>
    <S sid="117" ssid="26">Additionally, the human annotator reviews 6000 examples and corrects only 358.</S>
    <S sid="118" ssid="27">Thus, by limiting the number of unlabeled examples under consideration with the hope of forcing broader task coverage we achieve essentially the goal accuracy in fewer iterations and with fewer corrections!</S>
    <S sid="119" ssid="28">Surprisingly, the error rate of the view classifiers per iteration remains essentially unchanged despite the reduction of the pool of unlabeled examples to choose from.</S>
    <S sid="120" ssid="29">We believe the preceding experiment illuminates a fundamental tension in weakly supervised learning, between automatically obtaining reliable training data (usually requiring familiar examples), and adequately covering the learning task (usually requiring unfamiliar examples).</S>
    <S sid="121" ssid="30">This tension suggests that combining weakly supervised learning methods with active learning methods might be a fruitful endeavor.</S>
    <S sid="122" ssid="31">On one hand, the goal of weakly supervised learning is to bootstrap a classifier from small amounts of labeled data and large amounts of unlabeled data, often by automatically labeling some of the unlabeled data.</S>
    <S sid="123" ssid="32">On the other hand, the goal of active learning is to process (unlabeled) training examples in the order in which they are most useful or informative to the classifier (Cohn et al., 1994).</S>
    <S sid="124" ssid="33">Usefulness is commonly quantified as the learner's uncertainty about the class of an example (Lewis and Catlett, 1994).</S>
    <S sid="125" ssid="34">This neatly dovetails with the criterion for selecting instances to label in CT. We envision a learner that would alternate between selecting its most certain unlabeled examples to label and present to the human for acknowledgment, and selecting its most uncertain examples to present to the human for annotation.</S>
    <S sid="126" ssid="35">Ideally, efficient automatic bootstrapping would be complemented by good coverage of the task.</S>
    <S sid="127" ssid="36">We leave evaluation of this possibility to future work.</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="6">
    <S sid="128" ssid="1">This case study explored issues involved with applying co-training to the natural language processing task of identifying base noun phrases, particularly, the scalability of cotraining for large-scale problems.</S>
    <S sid="129" ssid="2">Our experiments indicate that co-training is an effective method for learning bracketers from small amounts of labeled data.</S>
    <S sid="130" ssid="3">Naturally, the resulting classifier does not perform as well as a fully supervised classifier trained on hundreds of times as much labeled data, but if the difference in accuracy is less important than the effort required to produce the labeled training data, co-training is especially attractive.</S>
    <S sid="131" ssid="4">Furthermore, our experiments support the hypothesis that labeled data quality is a crucial issue for co-training.</S>
    <S sid="132" ssid="5">Our moderately supervised variant, corrected co-training, maintains labeled data quality without unduly increasing the burden on the human annotator.</S>
    <S sid="133" ssid="6">Corrected co-training bridges the gap in accuracy between weak initial classifiers and fully supervised classifiers.</S>
    <S sid="134" ssid="7">Finally, as an approach to resolving the tension in weakly supervised learning between accumulating accurate training data and covering the desired task, we suggest combining weakly supervised methods such as co-training or selftraining with active learning.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="135" ssid="1">Thanks to three anonymous reviewers for their comments and suggestions.</S>
    <S sid="136" ssid="2">This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 9454149, 0081334, and 0074896.</S>
  </SECTION>
</PAPER>
