<PAPER>
  <S sid="0">Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper presents the first round of the on Textual Entailment for organized within SemEval-2012.</S>
    <S sid="2" ssid="2">The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario.</S>
    <S sid="3" ssid="3">Participants were presented with datasets for different language pairs, where multi-directional entailment relations (&#8220;forward&#8221;, &#8220;backward&#8221;, &#8220;bidirectional&#8221;, &#8220;no entailment&#8221;) had to be identified.</S>
    <S sid="4" ssid="4">We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization.</S>
    <S sid="6" ssid="2">Cross-linguality represents a dimension of the TE recognition problem that has been so far only partially investigated.</S>
    <S sid="7" ssid="3">The great potential for integrating monolingual TE recognition components into NLP architectures has been reported in several areas, including question answering, information retrieval, information extraction, and document summarization.</S>
    <S sid="8" ssid="4">However, mainly due to the absence of cross-lingual textual entailment (CLTE) recognition</S>
  </SECTION>
  <SECTION title="Danilo Giampiccolo CELCT" number="2">
    <S sid="9" ssid="1">components, similar improvements have not been achieved yet in any cross-lingual application.</S>
    <S sid="10" ssid="2">The CLTE task aims at prompting research to fill this gap.</S>
    <S sid="11" ssid="3">Along such direction, research can now benefit from recent advances in other fields, especially machine translation (MT), and the availability of: i) large amounts of parallel and comparable corpora in many languages, ii) open source software to compute word-alignments from parallel corpora, and iii) open source software to set up MT systems.</S>
    <S sid="12" ssid="4">We believe that all these resources can positively contribute to develop inference mechanisms for multilingual data.</S>
    <S sid="13" ssid="5">Content synchronization represents a challenging application scenario to test the capabilities of advanced NLP systems.</S>
    <S sid="14" ssid="6">Given two documents about the same topic written in different languages (e.g.</S>
    <S sid="15" ssid="7">Wiki pages), the task consists of automatically detecting and resolving differences in the information they provide, in order to produce aligned, mutually enriched versions of the two documents.</S>
    <S sid="16" ssid="8">Towards this objective, a crucial requirement is to identify the information in one page that is either equivalent or novel (more informative) with respect to the content of the other.</S>
    <S sid="17" ssid="9">The task can be naturally cast as an entailment recognition problem, where bidirectional and unidirectional entailment judgments for two text fragments are respectively mapped into judgments about semantic equivalence and novelty.</S>
    <S sid="18" ssid="10">Alternatively, the task can be seen as a machine translation evaluation problem, where judgments about semantic equivalence and novelty depend on the possibility to fully or partially translate a text fragment into the other.</S>
    <S sid="19" ssid="11">The recent advances on monolingual TE on the one hand, and the methodologies used in Statistical Machine Translation (SMT) on the other, offer promising solutions to approach the CLTE task.</S>
    <S sid="20" ssid="12">In line with a number of systems that model the RTE task as a similarity problem (i.e. handling similarity scores between T and H as useful evidence to draw entailment decisions), the standard sentence and word alignment programs used in SMT offer a strong baseline for CLTE.</S>
    <S sid="21" ssid="13">However, although representing a solid starting point to approach the problem, similarity-based techniques are just approximations, open to significant improvements coming from semantic inference at the multilingual level (e.g. cross-lingual entailment rules such as &#8220;perro&#8221;&#8594;&#8220;animal&#8221;).</S>
    <S sid="22" ssid="14">Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional).</S>
    <S sid="23" ssid="15">Thanks to the contiguity between CLTE, TE and SMT, the proposed task provides an interesting scenario to approach the issues outlined above from different perspectives, and large room for mutual improvement.</S>
  </SECTION>
  <SECTION title="2 The task" number="3">
    <S sid="24" ssid="1">Given a pair of topically related text fragments (T1 and T2) in different languages, the CLTE task consists of automatically annotating it with one of the following entailment judgments (see Figure 1 for Spanish/English examples of each judgment): In this task, both T1 and T2 are assumed to be true statements.</S>
    <S sid="25" ssid="2">Although contradiction is relevant from an application-oriented perspective, contradictory pairs are not present in the dataset created for the first round of the task.</S>
  </SECTION>
  <SECTION title="3 Dataset description" number="4">
    <S sid="26" ssid="1">Four CLTE corpora have been created for the following language combinations: Spanish/English (SP-EN), Italian/English (IT-EN), French/English (FR-EN), German/English (DE-EN).</S>
    <S sid="27" ssid="2">The datasets are released in the XML format shown in Figure 1.</S>
    <S sid="28" ssid="3">The dataset was created following the crowdsourcing methodology proposed in (Negri et al., 2011), which consists of the following steps: only the pairs where the difference between the number of words in T1 and T2 (length diff) was below a fixed threshold (10 words) were retained.1 The final result is a monolingual English dataset annotated with multi-directional entailment judgments, which are well distributed over length diff values ranging from 0 to 9; To ensure the good quality of the datasets, all the collected pairs were manually checked and corrected when necessary.</S>
    <S sid="29" ssid="4">Only pairs with agreement between two expert annotators were retained.</S>
    <S sid="30" ssid="5">The final result is a multilingual parallel entailment corpus, where T1s are in 5 different languages (i.e.</S>
    <S sid="31" ssid="6">English, Spanish, German, Italian, and French), and T2s are in English.</S>
    <S sid="32" ssid="7">It&#8217;s worth mentioning that the monolingual English corpus, a by-product of our data collection methodology, will be publicly released as a further contribution to the research community.2 Each dataset consists of 1,000 pairs (500 for training and 500 for test), balanced across the four entailment judgments (bidirectional, forward, backward, and no entailment).</S>
    <S sid="33" ssid="8">For each language combination, the distribution of the four entailment judgments according to length diff is shown in Figure 2.</S>
    <S sid="34" ssid="9">Vertical bars represent, for each length diff value, the proportion of pairs belonging to the four entailment classes.</S>
    <S sid="35" ssid="10">As can be seen, the length diff constraint applied to the length difference in the monolingual English pairs (step 3 of the creation process) is substantially reflected in the cross-lingual datasets for all language combinations.</S>
    <S sid="36" ssid="11">In fact, as shown in Table 1, the majority of the pairs is always included in the same length diff range (approximately [-5,+5]) and, within this range, the distribution of the four classes is substantially uniform.</S>
    <S sid="37" ssid="12">Our assumption is that such data distribution makes entailment judgments based on mere surface features such as sentence length ineffective, thus encouraging the development of alternative, deeper processing strategies.</S>
  </SECTION>
  <SECTION title="4 Evaluation metrics and baselines" number="5">
    <S sid="38" ssid="1">Evaluation results have been automatically computed by comparing the entailment judgments returned by each system with those manually assigned by human annotators.</S>
    <S sid="39" ssid="2">The metric used for systems&#8217; ranking is accuracy over the whole test set, i.e. the number of correct judgments out of the total number of judgments in the test set.</S>
    <S sid="40" ssid="3">Additionally, we calculated precision, recall, and F1 measures for each of the four entailment judgment categories taken separately.</S>
    <S sid="41" ssid="4">These scores aim at giving participants the possibility to gain clearer insights into their system&#8217;s behavior on the entailment phenomena relevant to the task.</S>
    <S sid="42" ssid="5">For each language combination, two baselines considering the length difference between T1 and T2 have been calculated (besides the trivial 0.25 accuracy score obtained by assigning each test pair in the balanced dataset to one of the four classes): judgments returned by the two classifiers are composed into a single multi-directional judgment (&#8220;YES-YES&#8221;=&#8220;bidirectional&#8221;, &#8220;YESNO&#8221;=&#8220;forward&#8221;, &#8220;NO-YES&#8221;=&#8220;backward&#8221;, &#8220;NO-NO&#8221;=&#8220;no entailment&#8221;); Both the baselines have been calculated with the LIBSVM package (Chang and Lin, 2011), using a linear kernel with default parameters.</S>
    <S sid="43" ssid="6">Baseline results are reported in Table 2.</S>
    <S sid="44" ssid="7">Although the four CLTE datasets are derived from the same monolingual EN-EN corpus, baseline results present slight differences due to the effect of translation into different languages.</S>
  </SECTION>
  <SECTION title="5 Submitted runs and results" number="6">
    <S sid="45" ssid="1">Participants were allowed to submit up to five runs for each language combination.</S>
    <S sid="46" ssid="2">A total of 17 teams registered to participate in the task and downloaded the training set.</S>
    <S sid="47" ssid="3">Out of them, 12 downloaded the test set and 10 (including one of the task organizers) submitted valid runs.</S>
    <S sid="48" ssid="4">Eight teams produced submissions for all the language combinations, while two teams participated only in the SP-EN task.</S>
    <S sid="49" ssid="5">In total, 92 runs have been submitted and evaluated (29 for SP-EN, and 21 for each of the other language pairs).</S>
    <S sid="50" ssid="6">Despite the novelty and the difficulty of the problem, these numbers demonstrate the interest raised by the task, and the overall success of the initiative.</S>
    <S sid="51" ssid="7">Accuracy results are reported in Table 3.</S>
    <S sid="52" ssid="8">As can be seen from the table, overall accuracy scores are quite different across language pairs, with the highest result on SP-EN (0.632), which is considerably higher than the highest score on DE-EN (0.558).</S>
    <S sid="53" ssid="9">This might be due to the fact that most of the participating systems rely on a &#8220;pivoting&#8221; approach that addresses CLTE by automatically translating T1 in the same language of T2 (see Section 6).</S>
    <S sid="54" ssid="10">Regarding the DE-EN dataset, pivoting methods might be penalized by the lower quality of MT output when German T1s are translated into English.</S>
    <S sid="55" ssid="11">The comparison with baselines results leads to interesting observations.</S>
    <S sid="56" ssid="12">First of all, while all systems significantly outperform the lowest 1-class baseline (0.25), both other baselines are surprisingly hard to beat.</S>
    <S sid="57" ssid="13">This shows that, despite the effort in keeping the distribution of the entailment classes uniform across different length diff values, eliminating the correlation between sentences&#8217; length and correct entailment decisions is difficult.</S>
    <S sid="58" ssid="14">As a consequence, although disregarding semantic aspects of the problem, features considering such information are quite effective.</S>
    <S sid="59" ssid="15">In general, systems performed better on the SPEN dataset, with most results above the binary baseline (8 out of 10), and half of the systems above the multi-class baseline.</S>
    <S sid="60" ssid="16">For the other language pairs the results are lower, with only 3 out of 8 participants above the two baselines in all datasets.</S>
    <S sid="61" ssid="17">Average results reflect this situation: the average scores are always above the binary baseline, whereas only the SP-EN average result is higher than the multiclass baseline(0.44 vs. 0.43).</S>
    <S sid="62" ssid="18">To better understand the behaviour of each system (also in relation to the different language combinations), Table 4 provides separate precision, recall, and F1 scores for each entailment judgment, calculated over the best runs of each participating team.</S>
    <S sid="63" ssid="19">Overall, the results suggest that the &#8220;bidirectional&#8221; and &#8220;no entailment&#8221; categories are more problematic than &#8220;forward&#8221; and &#8220;backward&#8221; judgments.</S>
    <S sid="64" ssid="20">For most datasets, in fact, systems&#8217; performance on &#8220;bidirectional&#8221; and &#8220;no entailment&#8221; is significantly lower, typically on recall.</S>
    <S sid="65" ssid="21">Except for the DE-EN dataset (more problematic on &#8220;forward&#8221;), also average F1 results on these judgments are lower.</S>
    <S sid="66" ssid="22">This might be due to the fact that, for all datasets, the vast majority of &#8220;bidirectional&#8221; and &#8220;no entailment&#8221; judgments falls in a length diff range where the distribution of the four classes is more uniform (see Figure 2).</S>
    <S sid="67" ssid="23">Similar reasons can justify the fact that &#8220;backward&#8221; entailment results are consistently higher on all datasets.</S>
    <S sid="68" ssid="24">Compared with &#8220;forward&#8221; entailment, these judgments are in fact less scattered across the entire length diff range (i.e. less intermingled with the other classes).</S>
  </SECTION>
  <SECTION title="6 Approaches" number="7">
    <S sid="69" ssid="1">A rough classification of the approaches adopted by participants can be made along two orthogonal dimensions, namely: Concerning the former dimension, most of the systems (6 out of 10) adopted a pivoting approach, relying on Google Translate (4 systems), Microsoft Bing Translator (1), or a combination of Google, Bing, and other MT systems (1) to produce English T2s.</S>
    <S sid="70" ssid="2">Regarding the latter dimension, the compositional approach was preferred to multi-class classification (6 out of 10).</S>
    <S sid="71" ssid="3">The best performing system relies on a &#8220;hybrid&#8221; approach (combining monolingual and cross-lingual alignments) and a compositional strategy.</S>
    <S sid="72" ssid="4">Besides the frequent recourse to MT tools, other resources used by participants include: on-line dictionaries for the translation of single words, word alignment tools, part-of-speech taggers, NP chunkers, named entity recognizers, stemmers, stopwords lists, and Wikipedia as an external multilingual corpus.</S>
    <S sid="73" ssid="5">More in detail: BUAP [pivoting, compositional] (Vilari&#732;no et al., 2012) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (Google Translate3 and the OpenOffice Thesaurus4).</S>
    <S sid="74" ssid="6">Similarity measures (e.g.</S>
    <S sid="75" ssid="7">Jaccard index) and rules are respectively used to annotate the two resulting sentence pairs with entailment judgments and combine them in a single decision.</S>
    <S sid="76" ssid="8">CELI [cross lingual, compositional &amp; multiclass] (Kouylekov, 2012) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting.</S>
    <S sid="77" ssid="9">Word overlap and similarity measures are then used in different approaches to the task.</S>
    <S sid="78" ssid="10">In one run (Run 1), they are used to train a classifier that assigns separate entailment judgments for each direction.</S>
    <S sid="79" ssid="11">Such judgments are finally composed into a single one for each pair.</S>
    <S sid="80" ssid="12">In the other runs, the same features are used for multi-class classification.</S>
    <S sid="81" ssid="13">DirRelCond3 [cross lingual, compositional] (Perini, 2012) uses bilingual dictionaries (Freedict5 and WordReference6) to translate content words into English.</S>
    <S sid="82" ssid="14">Then, entailment decisions are taken combining directional relatedness scores between words in both directions (Perini, 2011).</S>
    <S sid="83" ssid="15">FBK [cross lingual, compositional &amp; multiclass] (Mehdad et al., 2012a) uses cross-lingual matching features extracted from lexical phrase tables, semantic phrase tables, and dependency relations (Mehdad et al., 2011; Mehdad et al., 2012b; Mehdad et al., 2012c).</S>
    <S sid="84" ssid="16">The features are used for multi-class and binary classification using SVMs.</S>
    <S sid="85" ssid="17">HDU [hybrid, compositional] (W&#168;aschle and Fendrich, 2012) uses a combination of binary classifiers for each entailment direction.</S>
    <S sid="86" ssid="18">The classifiers use both monolingual alignment features based on METEOR (Banerjee and Lavie, 2005) alignments (translations obtained from Google Translate), and cross-lingual alignment features based on GIZA++ (Och and Ney, 2000) (word alignments learned on Europarl).</S>
    <S sid="87" ssid="19">ICT [pivoting, compositional] (Meng et al., 2012) adopts a pivoting method (using Google Translate and an in-house hierarchical MT system), and the open source EDITS system (Kouylekov and Negri, 2010) to calculate similarity scores between monolingual English pairs.</S>
    <S sid="88" ssid="20">Separate unidirectional entailment judgments obtained from binary classifier are combined to return one of the four valid CLTE judgments.</S>
    <S sid="89" ssid="21">JU-CSE-NLP [pivoting, compositional] (Neogi et al., 2012) uses Microsoft Bing translator7 to produce monolingual English pairs.</S>
    <S sid="90" ssid="22">Separate lexical mapping scores are calculated (from T1 to T2 and vice-versa) considering different types of information and similarity metrics.</S>
    <S sid="91" ssid="23">Binary entailment decisions are then heuristically combined into single decisions.</S>
    <S sid="92" ssid="24">Sagan [pivoting, multi-class] (Castillo and Cardenas, 2012) adopts a pivoting method using Google Translate, and trains a monolingual system based on a SVM multi-class classifier.</S>
    <S sid="93" ssid="25">A CLTE corpus derived from the RTE-3 dataset is also used as a source of additional training material.</S>
    <S sid="94" ssid="26">SoftCard [pivoting, multi-class] (Jimenez et al., 2012) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union.</S>
    <S sid="95" ssid="27">Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity (computed with edit-distance) UAlacant [pivoting, multi-class] (Espl`a-Gomis et al., 2012) exploits translations obtained from Google Translate, Microsoft Bing translator, and the Apertium open-source MT platform (Forcada et al., 2011).8 Then, a multi-class SVM classifier is used to take entailment decisions using information about overlapping sub-segments as features.</S>
  </SECTION>
  <SECTION title="7 Conclusion" number="8">
    <S sid="96" ssid="1">Despite the novelty of the problem and the difficulty to capture multi-directional entailment relations across languages, the first round of the Crosslingual Textual Entailment for Content Synchronization task organized within SemEval-2012 was a successful experience.</S>
    <S sid="97" ssid="2">This year a new interesting challenge has been proposed, a benchmark for four language combinations has been released, baseline results have been proposed for comparison, and a monolingual English dataset has been produced as a by-product which can be useful for monolingual TE research.</S>
    <S sid="98" ssid="3">The interest shown by participants was encouraging: 10 teams submitted a total of 92 runs for all the language pairs proposed.</S>
    <S sid="99" ssid="4">Overall, the results achieved on all datasets are encouraging, with best systems significantly outperforming the proposed baselines.</S>
    <S sid="100" ssid="5">It is worth observing that the nature of the task, which lies between semantics and machine translation, led to the participation of teams coming from both these communities, showing interesting opportunities for integration and mutual improvement.</S>
    <S sid="101" ssid="6">The proposed approaches reflect this situation, with teams traditionally working on MT now dealing with entailment, and teams traditionally participating in the RTE challenges now dealing with cross-lingual alignment techniques.</S>
    <S sid="102" ssid="7">Our ambition, for the future editions of the CLTE task, is to further consolidate the bridge between the semantics and MT communities.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="9">
    <S sid="103" ssid="1">This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).</S>
    <S sid="104" ssid="2">The authors would also like to acknowledge Giovanni Moretti from CELCT for evaluation scripts and technical assistance, and the volunteer translators that contributed to the creation of the dataset:</S>
  </SECTION>
</PAPER>
