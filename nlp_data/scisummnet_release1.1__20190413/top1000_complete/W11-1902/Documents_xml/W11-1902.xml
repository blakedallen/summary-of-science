<PAPER>
  <S sid="0">Stanford&#226;&#8364;&#8482;s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper details the coreference resolution system submitted by Stanford at the CoNLL- 2011 shared task.</S>
    <S sid="2" ssid="2">Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information.</S>
    <S sid="3" ssid="3">All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster.</S>
    <S sid="4" ssid="4">We participated in both the open and closed tracks and submitted results using both predicted and gold mentions.</S>
    <S sid="5" ssid="5">Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).</S>
    <S sid="7" ssid="2">Our system extends the multi-pass sieve system of Raghunathan et al. (2010), which applies tiers of deterministic coreference models one at a time from highest to lowest precision.</S>
    <S sid="8" ssid="3">Each tier builds on the entity clusters constructed by previous models in the sieve, guaranteeing that stronger features are given precedence over weaker ones.</S>
    <S sid="9" ssid="4">Furthermore, this model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster.</S>
    <S sid="10" ssid="5">We made three considerable extensions to the Raghunathan et al. (2010) model.</S>
    <S sid="11" ssid="6">First, we added five additional sieves, the majority of which address the semantic similarity between mentions, e.g., using WordNet distance, and shallow discourse understanding, e.g., linking speakers to compatible pronouns.</S>
    <S sid="12" ssid="7">Second, we incorporated a mention detection sieve at the beginning of the processing flow.</S>
    <S sid="13" ssid="8">This sieve filters our syntactic constituents unlikely to be mentions using a simple set of rules on top of the syntactic analysis of text.</S>
    <S sid="14" ssid="9">And lastly, we added a post-processing step, which guarantees that the output of our system is compatible with the shared task and OntoNotes specifications (Hovy et al., 2006; Pradhan et al., 2007).</S>
    <S sid="15" ssid="10">Using this system, we participated in both the closed1 and open2 tracks, using both predicted and gold mentions.</S>
    <S sid="16" ssid="11">Using predicted mentions, our system had an overall score of 57.8 in the closed track and 58.3 in the open track.</S>
    <S sid="17" ssid="12">These were the top scores in both tracks.</S>
    <S sid="18" ssid="13">Using gold mentions, our system scored 60.7 in the closed track in 61.4 in the open track.</S>
    <S sid="19" ssid="14">We describe the architecture of our entire system in Section 2.</S>
    <S sid="20" ssid="15">In Section 3 we show the results of several experiments, which compare the impact of the various features in our system, and analyze the performance drop as we switch from gold mentions and annotations (named entity mentions and parse trees) to predicted information.</S>
    <S sid="21" ssid="16">We also report in this section our official results in the testing partition.</S>
  </SECTION>
  <SECTION title="2 System Architecture" number="2">
    <S sid="22" ssid="1">Our system consists of three main stages: mention detection, followed by coreference resolution, and finally, post-processing.</S>
    <S sid="23" ssid="2">In the first stage, mentions are extracted and relevant information about mentions, e.g., gender and number, is prepared for the next step.</S>
    <S sid="24" ssid="3">The second stage implements the actual coreference resolution of the identified mentions.</S>
    <S sid="25" ssid="4">Sieves in this stage are sorted from highest to lowest precision.</S>
    <S sid="26" ssid="5">For example, the first sieve (i.e., highest precision) requires an exact string match between a mention and its antecedent, whereas the last one (i.e., lowest precision) implements pronominal coreference resolution.</S>
    <S sid="27" ssid="6">Post-processing is performed to adjust our output to the task specific constraints, e.g., removing singletons.</S>
    <S sid="28" ssid="7">It is important to note that the first system stage, i.e., the mention detection sieve, favors recall heavily, whereas the second stage, which includes the actual coreference resolution sieves, is precision oriented.</S>
    <S sid="29" ssid="8">Our results show that this design lead to state-of-the-art performance despite the simplicity of the individual components.</S>
    <S sid="30" ssid="9">This strategy has been successfully used before for information extraction, e.g., in the BioNLP 2009 event extraction shared task (Kim et al., 2009), several of the top systems had a first high-recall component to identify event anchors, followed by high-precision classifiers, which identified event arguments and removed unlikely event candidates (Bj&#168;orne et al., 2009).</S>
    <S sid="31" ssid="10">In the coreference resolution space, several works have shown that applying a list of rules from highest to lowest precision is beneficial for coreference resolution (Baldwin, 1997; Raghunathan el al., 2010).</S>
    <S sid="32" ssid="11">However, we believe we are the first to show that this high-recall/high-precision strategy yields competitive results for the complete task of coreference resolution, i.e., including mention detection and both nominal and pronominal coreference.</S>
    <S sid="33" ssid="12">In our particular setup, the recall of the mention detection component is more important than its precision, because any missed mentions are guaranteed to affect the final score, but spurious mentions may not impact the overall score if they are left as singletons, which are discarded by our post-processing step.</S>
    <S sid="34" ssid="13">Therefore, our mention detection algorithm focuses on attaining high recall rather than high precision.</S>
    <S sid="35" ssid="14">We achieve our goal based on the list of sieves sorted by recall (from highest to lowest).</S>
    <S sid="36" ssid="15">Each sieve uses syntactic parse trees, identified named entity mentions, and a few manually written patterns based on heuristics and OntoNotes specifications (Hovy et al., 2006; Pradhan et al., 2007).</S>
    <S sid="37" ssid="16">In the first and highest recall sieve, we mark all noun phrase (NP), possessive pronoun, and named entity mentions in each sentence as candidate mentions.</S>
    <S sid="38" ssid="17">In the following sieves, we remove from this set all mentions that match any of the exclusion rules below: of 8 words, e.g., there, ltd., hmm.</S>
    <S sid="39" ssid="18">Note that the above rules extract both mentions in appositive and copulative relations, e.g., [[Yongkang Zhou], the general manager] or [Mr. Savoca] had been [a consultant... ].</S>
    <S sid="40" ssid="19">These relations are not annotated in the OntoNotes corpus, e.g., in the text [[Yongkang Zhou], the general manager], only the larger mention is annotated.</S>
    <S sid="41" ssid="20">However, appositive and copulative relations provide useful (and highly precise) information to our coreference sieves.</S>
    <S sid="42" ssid="21">For this reason, we keep these mentions as candidates, and remove them later during post-processing.</S>
    <S sid="43" ssid="22">Once mentions are extracted, we sort them by sentence number, and left-to-right breadth-first traversal order in syntactic trees in the same sentence (Hobbs, 1977).</S>
    <S sid="44" ssid="23">We select for resolution only the first mentions in each cluster,3 for two reasons: (a) the first mention tends to be better defined (Fox, 1993), which provides a richer environment for feature extraction; and (b) it has fewer antecedent candidates, which means fewer opportunities to make a mistake.</S>
    <S sid="45" ssid="24">For example, given the following ordered list of mentions, {mi, m2, m3, m4, m1&#65533;, m6}, where the subscript indicates textual order and the superscript indicates cluster id, our model will attempt to resolve only m2 and m4.</S>
    <S sid="46" ssid="25">Furthermore, we discard first mentions that start with indefinite pronouns (e.g., some, other) or indefinite articles (e.g., a, an) if they have no antecedents that have the exact same string extents.</S>
    <S sid="47" ssid="26">For each selected mention mi, all previous mentions mi&#8722;1, ... , m1 become antecedent candidates.</S>
    <S sid="48" ssid="27">All sieves traverse the candidate list until they find a coreferent antecedent according to their criteria or reach the end of the list.</S>
    <S sid="49" ssid="28">Crucially, when comparing two mentions, our approach uses information from the entire clusters that contain these mentions instead of using just information local to the corresponding mentions.</S>
    <S sid="50" ssid="29">Specifically, mentions in a cluster share their attributes (e.g., number, gender, animacy) between them so coreference decision are better informed.</S>
    <S sid="51" ssid="30">For example, if a cluster contains two mentions: a group of students, which is singular, and five students, which is plural, the number attribute of the entire cluster becomes singular or plural, which allows it to match other mentions that are both singular and plural.</S>
    <S sid="52" ssid="31">Please see (Raghunathan et al., 2010) for more details.</S>
    <S sid="53" ssid="32">The core of our coreference resolution system is an incremental extension of the system described in Raghunathan et al. (2010).</S>
    <S sid="54" ssid="33">Our core model includes two new sieves that address nominal mentions and are inserted based on their precision in a held-out corpus (see Table 1 for the complete list of sieves deployed in our system).</S>
    <S sid="55" ssid="34">Since these two sieves use 3We initialize the clusters as singletons and grow them progressively in each sieve.</S>
    <S sid="56" ssid="35">Ordered sieves simple lexical constraints without semantic information, we consider them part of the baseline model.</S>
    <S sid="57" ssid="36">Relaxed String Match: This sieve considers two nominal mentions as coreferent if the strings obtained by dropping the text following their head words are identical, e.g., [Clinton] and [Clinton, whose term ends in January].</S>
    <S sid="58" ssid="37">Proper Head Word Match: This sieve marks two mentions headed by proper nouns as coreferent if they have the same head word and satisfy the following constraints: Not i-within-i - same as Raghunathan et al. (2010).</S>
    <S sid="59" ssid="38">No location mismatches - the modifiers of two mentions cannot contain different location named entities, other proper nouns, or spatial modifiers.</S>
    <S sid="60" ssid="39">For example, [Lebanon] and [southern Lebanon] are not coreferent.</S>
    <S sid="61" ssid="40">No numeric mismatches - the second mention cannot have a number that does not appear in the antecedent, e.g., [people] and [around 200 people] are not coreferent.</S>
    <S sid="62" ssid="41">In addition to the above, a few more rules are added to get better performance for predicted mentions.</S>
    <S sid="63" ssid="42">Pronoun distance - sentence distance between a pronoun and its antecedent cannot be larger than 3.</S>
    <S sid="64" ssid="43">Bare plurals - bare plurals are generic and cannot have a coreferent antecedent.</S>
    <S sid="65" ssid="44">We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng &amp; Cardie, 2002; Daum&#180;e &amp; Marcu, 2005; Ponzetto &amp; Strube, 2006; Ng, 2007; Yang &amp; Su, 2007; Bengston &amp; Roth, 2008; Huang et al., 2009; inter alia).</S>
    <S sid="66" ssid="45">Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases.</S>
    <S sid="67" ssid="46">The following steps generate a query for these resources from a mention cluster.</S>
    <S sid="68" ssid="47">First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones.</S>
    <S sid="69" ssid="48">In case of ties, we select the longer string.</S>
    <S sid="70" ssid="49">For example, the mention selected from the cluster {President George W. Bush, president, he} is President George W. Bush.</S>
    <S sid="71" ssid="50">Second, if this mention returns nothing from the knowledge bases, we implement the following query relaxation algorithm: (a) remove the text following the mention head word; (b) select the lowest noun phrase (NP) in the parse tree that includes the mention head word; (c) use the longest proper noun (NNP*) sequence that ends with the head word; (d) select the head word.</S>
    <S sid="72" ssid="51">For example, the query president Bill Clinton, whose term ends in January is successively changed to president Bill Clinton, then Bill Clinton, and finally Clinton.</S>
    <S sid="73" ssid="52">If multiple records are returned, we keep the top two for Wikipedia and Freebase, and all synsets for WordNet.</S>
    <S sid="74" ssid="53">This sieve addresses name aliases, which are detected as follows.</S>
    <S sid="75" ssid="54">Two mentions headed by proper nouns are marked as aliases (and stored in the same entity cluster) if they appear in the same Wikipedia infobox or Freebase record in either the &#8216;name&#8217; or &#8216;alias&#8217; field, or they appear in the same synset in WordNet.</S>
    <S sid="76" ssid="55">As an example, this sieve correctly detects America Online and AOL as aliases.</S>
    <S sid="77" ssid="56">We also tested the utility of Wikipedia categories, but found little gain over morpho-syntactic features.</S>
    <S sid="78" ssid="57">This sieve marks two nominal mentions as coreferent if they are linked by a WordNet lexical chain that traverses hypernymy or synonymy relations.</S>
    <S sid="79" ssid="58">We use all synsets for each mention, but restrict it to mentions that are at most three sentences apart, and lexical chains of length at most four.</S>
    <S sid="80" ssid="59">This sieve correctly links Britain with country, and plane with aircraft.</S>
    <S sid="81" ssid="60">To increase the precision of the above two sieves, we use additional constraints before two mentions can match: attribute agreement (number, gender, animacy, named entity labels), no i-within-i, no location or numeric mismatches (as in Section 2.3.1), and we do not use the abstract entity synset in WordNet, except in chains that include &#8216;organization&#8217;.</S>
    <S sid="82" ssid="61">This sieve matches speakers to compatible pronouns, using shallow discourse understanding to handle quotations and conversation transcripts.</S>
    <S sid="83" ssid="62">Although more complex discourse constraints have been proposed, it has been difficult to show improvements (Tetreault &amp; Allen, 2003; 2004).</S>
    <S sid="84" ssid="63">We begin by identifying speakers within text.</S>
    <S sid="85" ssid="64">In non-conversational text, we use a simple heuristic that searches for the subjects of reporting verbs (e.g., say) in the same sentence or neighboring sentences to a quotation.</S>
    <S sid="86" ssid="65">In conversational text, speaker information is provided in the dataset.</S>
    <S sid="87" ssid="66">The extracted speakers then allow us to implement the following sieve heuristics: For example, I, my, and she in the following sentence are coreferent: &#8220;[I] voted for [Nader] because [he] was most aligned with [my] values,&#8221; [she] said.</S>
    <S sid="88" ssid="67">In addition to the above sieve, we impose speaker constraints on decisions made by subsequent sieves: For example, [my] and [he] are not coreferent in the above example (third constraint).</S>
    <S sid="89" ssid="68">To guarantee that the output of our system matches the shared task requirements and the OntoNotes annotation specification, we implement two postprocessing steps:</S>
  </SECTION>
  <SECTION title="3 Results and Discussion" number="3">
    <S sid="90" ssid="1">Table 2 shows the performance of our mention detection algorithm.</S>
    <S sid="91" ssid="2">We show results before and after coreference resolution and post-processing (when singleton mentions are removed).</S>
    <S sid="92" ssid="3">We also list results with gold and predicted linguistic annotations (i.e., syntactic parses and named entity recognition).</S>
    <S sid="93" ssid="4">The table shows that the recall of our approach is 92.8% (if gold annotations are used) or 87.9% (with predicted annotations).</S>
    <S sid="94" ssid="5">In both cases, precision is low because our algorithm generates many spurious mentions due to its local nature.</S>
    <S sid="95" ssid="6">However, as the table indicates, many of these mentions are removed during post-processing, because they are assigned to singleton clusters during coreference resolution.</S>
    <S sid="96" ssid="7">The two main causes for our recall errors are lack of recognition of event mentions (e.g., verbal mentions such as growing) and parsing errors.</S>
    <S sid="97" ssid="8">Parsing errors often introduce incorrect mention boundaries, which yield both recall and precision errors.</S>
    <S sid="98" ssid="9">For example, our system generates the predicted mention, the working meeting of the &#8221;863 Program&#8221; today, for the gold mention the working meeting of the &#8221;863 Program&#8221;.</S>
    <S sid="99" ssid="10">Due to this boundary mismatch, all mentions found to be coreferent with this predicted mention are counted as precision errors, and all mentions in the same coreference cluster with the gold mention are counted as recall errors.</S>
    <S sid="100" ssid="11">Table 3 lists the results of our end-to-end system on the development partition.</S>
    <S sid="101" ssid="12">&#8220;External Resources&#8221;, which were used only in the open track, includes: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources, (b) an animacy list (Ji and Lin, 2009), (c) a country and state gazetteer, and (d) a demonym list.</S>
    <S sid="102" ssid="13">&#8220;Discourse&#8221; stands for the sieve introduced in Section 2.3.3.</S>
    <S sid="103" ssid="14">&#8220;Semantics&#8221; stands for the sieves presented in Section 2.3.2.</S>
    <S sid="104" ssid="15">The table shows that the discourse sieve yields an improvement of almost 2 points to the overall score (row 1 versus 3), and external resources contribute 0.5 points.</S>
    <S sid="105" ssid="16">On the other hand, the semantic sieves do not help (row 3 versus 4).</S>
    <S sid="106" ssid="17">The latter result contradicts our initial experiments, where we measured a minor improvement when these sieves were enabled and gold mentions were used.</S>
    <S sid="107" ssid="18">Our hypothesis is that, when predicted mentions are used, the semantic sieves are more likely to link spurious mentions to existing clusters, thus introducing precision errors.</S>
    <S sid="108" ssid="19">This suggests that a different tuning of the sieve parameters is required for the predicted mention scenario.</S>
    <S sid="109" ssid="20">For this reason, we did not use the semantic sieves for our submission.</S>
    <S sid="110" ssid="21">Hence, rows 2 and 3 in the table show the performance of our official submission in the development set, in the closed and open tracks respectively.</S>
    <S sid="111" ssid="22">The last three rows in Table 3 give insight on the impact of gold information.</S>
    <S sid="112" ssid="23">This analysis indicates that using gold linguistic annotation yields an improvement of only 2 points.</S>
    <S sid="113" ssid="24">This implies that the quality of current linguistic processors is sufficient for the task of coreference resolution.</S>
    <S sid="114" ssid="25">On the other hand, using gold mentions raises the overall score by 15 points.</S>
    <S sid="115" ssid="26">This clearly indicates that pipeline architectures where mentions are identified first are inadequate for this task, and that coreference resolution might benefit from the joint modeling of mentions and coreference chains.</S>
    <S sid="116" ssid="27">Finally, Table 4 lists our results on the held-out testing partition.</S>
    <S sid="117" ssid="28">Note that in this dataset, the gold mentions included singletons and generic mentions as well, whereas in development (lines 6 and 7 in Table 3), gold mentions included only mentions part of an actual coreference chain.</S>
    <S sid="118" ssid="29">This explains the large difference between, say, line 6 in Table 3 and line 4 in Table 4.</S>
    <S sid="119" ssid="30">Our scores are comparable to previously reported state-of-the-art results for coreference resolution with predicted mentions.</S>
    <S sid="120" ssid="31">For example, Haghighi and Klein (2010) compare four state-of-the-art systems on three different corpora and report B3 scores between 63 and 77 points.</S>
    <S sid="121" ssid="32">While the corpora used in (Haghighi and Klein, 2010) are different from the one in this shared task, our result of 68 B3 suggests that our system&#8217;s performance is competitive.</S>
    <S sid="122" ssid="33">In this task, our submissions in both the open and the closed track obtained the highest scores.</S>
  </SECTION>
  <SECTION title="4 Conclusion" number="4">
    <S sid="123" ssid="1">In this work we showed how a competitive end-toend coreference resolution system can be built using only deterministic models (or sieves).</S>
    <S sid="124" ssid="2">Our approach starts with a high-recall mention detection component, which identifies mentions using only syntactic information and named entity boundaries, followed by a battery of high-precision deterministic coreference sieves, applied one at a time from highest to lowest precision.</S>
    <S sid="125" ssid="3">These models incorporate lexical, syntactic, semantic, and discourse information, and have access to document-level information (i.e., we share mention attributes across clusters as they are built).</S>
    <S sid="126" ssid="4">For this shared task, we extended our existing system with new sieves that model shallow discourse (i.e., speaker identification) and semantics (lexical chains and alias detection).</S>
    <S sid="127" ssid="5">Our results demonstrate that, despite their simplicity, deterministic models for coreference resolution obtain competitive results, e.g., we obtained the highest scores in both the closed and open tracks (57.8 and 58.3 respectively).</S>
    <S sid="128" ssid="6">The code used for this shared task is publicly released.5</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="5">
    <S sid="129" ssid="1">We thank the shared task organizers for their effort.</S>
    <S sid="130" ssid="2">This material is based upon work supported by the Air Force Research Laboratory (AFRL) under prime contract no.</S>
    <S sid="131" ssid="3">FA8750-09-C-0181.</S>
    <S sid="132" ssid="4">Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).</S>
  </SECTION>
</PAPER>
