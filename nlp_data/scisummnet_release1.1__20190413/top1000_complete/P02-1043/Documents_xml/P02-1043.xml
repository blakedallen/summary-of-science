<PAPER>
	<S sid="0">Generative Models For Statistical Parsing With Combinatory Categorial Grammar</S><ABSTRACT>
		<S sid="1" ssid="1">This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.</S>
		<S sid="2" ssid="2">These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.</S>
		<S sid="3" ssid="3">According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar.</S>
		<S sid="4" ssid="4">Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="5" ssid="5">The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank.</S>
			<S sid="6" ssid="6">However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parsevalmeasures without committing itself on certain se mantically significant decisions, such as predicting null elements arising from deletion or movement.</S>
			<S sid="7" ssid="7">The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.</S>
			<S sid="8" ssid="8">CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames.</S>
			<S sid="9" ssid="9">As a re sult, the categorial lexicon extracted for this purposefrom the training corpus has 1207 categories, com pared with the 48 POS-tags of the Penn Treebank.On the other hand, grammar rules in CCG are lim ited to a small number of simple unary and binary combinatory schemata such as function application and composition.</S>
			<S sid="10" ssid="10">This results in a smaller and less overgenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate gories in sections 02-21, instead of &gt;12,400 in the original Treebank representation (Collins, 1999)).</S>
	</SECTION>
	<SECTION title="Evaluating a CCG parser. " number="2">
			<S sid="11" ssid="1">Since CCG produces unary and binary branching trees with a very fine-grained category set, CCG Parseval scores cannot be compared with scores of standard Treebank parsers.</S>
			<S sid="12" ssid="2">Therefore, we alsoevaluate performance using a dependency evaluation reported by Collins (1999), which counts word word dependencies as determined by local trees and their labels.</S>
			<S sid="13" ssid="3">According to this metric, a local tree with parent node P, head daughter H and non-head daughter S (and position of S relative to P, ie.</S>
			<S sid="14" ssid="4">leftor right, which is implicit in CCG categories) de fines a hP;H;Si dependency between the head word of S, wS, and the head word of H , wH.</S>
			<S sid="15" ssid="5">This measureis neutral with respect to the branching factor.</S>
			<S sid="16" ssid="6">Fur thermore, as noted by Hockenmaier (2001), it doesnot penalize equivalent analyses of multiple modi Computational Linguistics (ACL), Philadelphia, July 2002, pp.</S>
			<S sid="17" ssid="7">335-342.</S>
			<S sid="18" ssid="8">Proceedings of the 40th Annual Meeting of the Association for Pierre Vinken ; 61 years old ; will join the board as a nonexecutive director Nov 29 N=N N ; N=N N (S[adj]nNP)nNP ; (S[dcl]nNP)=(S[b]nNP) ((S[b]nNP)=PP)=NP NP=N N PP=NP NP=N N=N N ((SnNP)n(SnNP))=N N &gt; &gt; &gt; &gt; &gt; N N N N (SnNP)n(SnNP) &gt; NP NP NP NP &lt; &gt; &gt; NP S[adj]nNP (S[b]nNP)=PP PP &gt; NPnNP S[b]nNP &lt; &lt; NP S[b]nNP &gt; NP S[dcl]nNP &lt; S[dcl] Figure 1: A CCG derivation in our corpus fiers.</S>
			<S sid="19" ssid="9">In the unlabeled case hi (where it only matters whether word a is a dependent of word b, not whatthe label of the local tree is which defines this depen dency), scores can be compared across grammars with different sets of labels and different kinds of trees.</S>
			<S sid="20" ssid="10">In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser.</S>
			<S sid="21" ssid="11">For further discussion we refer the reader to Clark and Hockenmaier (2002) .</S>
	</SECTION>
	<SECTION title="CCGbank?a CCG treebank. " number="3">
			<S sid="22" ssid="1">CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002).</S>
			<S sid="23" ssid="2">Almost alltypes of construction?with the exception of gap ping and UCP (?Unlike Coordinate Phrases?)</S>
			<S sid="24" ssid="3">arecovered by the translation procedure, which pro cesses 98.3% of the sentences in the training corpus (WSJ sections 02-21) and 98.5% of the sentences in the test corpus (WSJ section 23).</S>
			<S sid="25" ssid="4">The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992).</S>
			<S sid="26" ssid="5">Figure1 shows a derivation taken from CCGbank.</S>
			<S sid="27" ssid="6">Categories, such as ((S[b]nNP)=PP)=NP, encode unsaturated subcat frames.</S>
			<S sid="28" ssid="7">The complement-adjunct distinction is made explicit; for instance as a nonexecutive director is marked up as PP-CLR in the Tree bank, and hence treated as a PP-complement of join, whereas Nov. 29 is marked up as an NP-TMP and therefore analyzed as VP modifier.</S>
			<S sid="29" ssid="8">The -CLR tag is not in fact a very reliable indicator of whether a constituent should be treated as a complement, but the translation to CCG is automatic and must do the best it can with the information in the Treebank.The verbal categories in CCGbank carry features distinguishing declarative verbs (and auxiliaries) from past participles in past tense, past par ticiples for passive, bare infinitives and ing-forms.</S>
			<S sid="30" ssid="9">There is a separate level for nouns and noun phrases, but, like the nonterminal NP in the Penn Treebank, noun phrases do not carry any number agreement.</S>
			<S sid="31" ssid="10">The derivations in CCGbank are ?normal-form?</S>
			<S sid="32" ssid="11">in the sense that analyses involving the combinatory rules of type-raising and composition are only used when syntactically necessary.</S>
	</SECTION>
	<SECTION title="Generative models of CCG derivations. " number="4">
			<S sid="33" ssid="1">Expansion HeadCat NonHeadCat P(exp j : : : ) P(H j : : : ) P(S j : : : ) Baseline P P;exp P;exp;H + Conj P;con jP P;exp;con jP P;exp;H ;con jP + Grandparent P;GP P;GP;exp P;GP;exp;H + ? P#?L;RP P;exp#?L;RP P;exp;H#?L;RP Table 1: The unlexicalized models The models described here are all extensions of a very simple model which models derivations by a top-down tree-generating process.</S>
			<S sid="34" ssid="2">This model was originally described in Hockenmaier (2001), where it was applied to a preliminary version of CCGbank, and its definition is repeated here in the top row of Table 1.</S>
			<S sid="35" ssid="3">Given a (parent) node with category P, choose the expansion exp of P, where exp can beleaf (for lexical categories), unary (for unary ex pansions such as type-raising), left (for binary trees where the head daughter is left) or right (binary trees, head right).</S>
			<S sid="36" ssid="4">If P is a leaf node, generate its head word w. Otherwise, generate the category ofits head daughter H . If P is binary branching, gen erate the category of its non-head daughter S (a complement or modifier of H).The model itself includes no prior knowledge spe cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability).</S>
			<S sid="37" ssid="5">All the experiments reported in this section were conducted using sections 02-21 of CCGbank as training corpus, and section 23 as test corpus.</S>
			<S sid="38" ssid="6">We replace all rare words in the training data with their POS-tag.</S>
			<S sid="39" ssid="7">For all experiments reported here and in section 5, the frequency threshold was set to 5.</S>
			<S sid="40" ssid="8">LikeCollins (1999), we assume that the test data is POS tagged, and can therefore replace unknown words inthe test data with their POS-tag, which is more ap propriate for a formalism like CCG with a large set of lexical categories than one generic token for all unknown words.</S>
			<S sid="41" ssid="9">The performance of the baseline model is shown in the top row of table 3.</S>
			<S sid="42" ssid="10">For six out of the 2379 sentences in our test corpus we do not get a parse.1The reason is that a lexicon consisting of the word category pairs observed in the training corpus does not contain all the entries required to parse the test corpus.</S>
			<S sid="43" ssid="11">We discuss a simple, but imperfect, solution to this problem in section 7.</S>
	</SECTION>
	<SECTION title="Extending the baseline model. " number="5">
			<S sid="44" ssid="1">State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes.</S>
			<S sid="45" ssid="2">We too can extend the baseline model described in the previous section by including more features.</S>
			<S sid="46" ssid="3">Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly.</S>
			<S sid="47" ssid="4">In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i</S></SECTION></PAPER>
