Generative Models For Statistical Parsing With Combinatory Categorial Grammar
This paper compares a number of generative probability models for a wide-coverage Combinatory Categorial Grammar (CCG) parser.
These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar.
In contrast to Gildea (2001), we find a significant improvement from modeling word-word dependencies.
The CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing.
The dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features.
