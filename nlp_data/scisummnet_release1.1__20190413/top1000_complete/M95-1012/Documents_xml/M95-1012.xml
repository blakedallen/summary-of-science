<PAPER>
  <S sid="0" ssid="0">MITRE: DESCRIPTION OF THE ALEMBIC SYSTEM USED FOR MUC-6 John Aberdeen, John Burger, David Day, Lynette Hirschman, Patricia Robinson, and Marc Vilain The MITRE Corporation 202 Burlington Rd .</S>
  <S sid="1" ssid="1">Bedford, MA 01730 {aberdeen, john, clay, lynette, parann, mbv}@mitre .org As with several other veteran Muc participants, MITRES Alembic system has undergone a major trans - formation in the past two years.</S>
  <S sid="2" ssid="2">The genesis of this transformation occurred during a dinner conversation at the last Muc conference, MUC-5 .</S>
  <S sid="3" ssid="3">At that time, several of us reluctantly admitted that our major impediment towards improved performance was reliance on then-standard linguistic models of syntax.</S>
  <S sid="4" ssid="4">We knew we would need an alternative to traditional linguistic grammars, even to the somewhat non-traditional categoria l pseudo-parser we had in place at the time.</S>
  <S sid="5" ssid="5">The problem was, which alternative ?</S>
  <S sid="6" ssid="6">The answer came in the form of rule sequences, an approach Eric Brill originally laid out in his work o n part-of-speech tagging [5, 7] .</S>
  <S sid="7" ssid="7">Rule sequences now underlie all the major processing steps in Alembic.</S>
  <S sid="8" ssid="8">part-of- speech tagging, syntactic analysis, inference, and even some of the set-fill processing in the Template Elemen t task (TE) .</S>
  <S sid="9" ssid="9">We have found this approach to provide almost an embarrassment of advantages, speed an d accuracy being the most externally visible benefits .</S>
  <S sid="10" ssid="10">In addition, most of our rule sequence processors ar e trainable, typically from small samples .</S>
  <S sid="11" ssid="11">The rules acquired in this way also have the characteristic that the y allow one to readily mix hand-crafted and machine-learned elements .</S>
  <S sid="12" ssid="12">We have exploited this opportunity t o apply both machine-learned and hand-crafted rules extensively, choosing in some instances to run sequence s that were primarily machine-learned, and in other cases to run sequences that were entirely crafted by hand .</S>
  <S sid="13" ssid="13">ALEMBICS OVERALL ARCHITECTUR E For all the changes that the system has undergone, the coarse architecture of the Muc-6 version ofAlembic is remarkably close to that of its predecessors .</S>
  <S sid="14" ssid="14">As illustrated in Fig.</S>
  <S sid="15" ssid="15">r, below, processing is still divided int o three main steps : a UNIX- and c-based preprocess, a Lisp-based syntactic analysis, and a Lisp-based inferenc e phase.</S>
  <S sid="16" ssid="16">Beyond these coarse-grain similarities, the system diverges significantly from earlier incarnations .</S>
  <S sid="17" ssid="17">We replaced our categorial grammar pseudo-parser, as suggested above .</S>
  <S sid="18" ssid="18">We also redesigned the preprocess fro m the ground up .</S>
  <S sid="19" ssid="19">Only the inferential back end of the system is largely unchanged .</S>
  <S sid="20" ssid="20">The internal module-by-module architecture of the current Alembic is illustrated in Fig .</S>
  <S sid="21" ssid="21">The central innovation in the system is its approach to syntactic analysis, which is now performed through a sequence of phrase-finding rules that are processed by a simple interpreter .</S>
  <S sid="22" ssid="22">The interpreter has somewhat less recognition power than a finite-state machine, and operates by successively relabeling the input according t o the rule actions?more on this below.</S>
  <S sid="23" ssid="23">In support of the syntactic phrase finder, or phraser as we call it, the input text must be tagged for part-of-speech .</S>
  <S sid="24" ssid="24">This part-of-speech tagging is the principal role of the UNIX preprocess, and it is itself supported by a number of pretaggers (e .g., for labeling dates and title words) and zoners (e.g., for word tokenization, sentence boundary determination and headline segmentation) .</S>
  <S sid="25" ssid="25">The phrases that are parsed by the phraser are subsequently mapped to facts in the inferential database, a mapping mediated by a simple semantic interpreter .</S>
  <S sid="26" ssid="26">We then exploit inference to instantiate domai n Syntactic analysis (Lisp)  Tractable inference (Lisp) IL  u  u NE markup  TE templates  5T templates Figure 1 : Coarse-grained system architecture .</S>
  <S sid="27" ssid="27">UNIX preprocess 141 Template Printing gazetteer Figure 2 : Processing modules in Alembic.</S>
  <S sid="28" ssid="28">constraints and resolve restricted classes of coreference .</S>
  <S sid="29" ssid="29">The inference system also supports equality reasonin g by congruence closure, and this equality machinery is in turn exploited to perform TE-specific processing, i n particular, acronym and alias merging.</S>
  <S sid="30" ssid="30">Finally, the template generation module forms the final TE and S T output by a roughly one-to-one mapping from facts in the inferential database to templates .</S>
  <S sid="31" ssid="31">THE PREPROCESSORS As noted above, the UNIX-based portion of the system is primarily responsible for part-of-speech tagging .</S>
  <S sid="32" ssid="32">Prior to the part-of-speech tagger, however, a text to be processed by Alembic passes through severa l preprocess stages ; each preprocessor "enriches" the text by means of SGML tags .</S>
  <S sid="33" ssid="33">All of these preprocess components are implemented with LEX (the lexical analyzer generator) and are very fast .</S>
  <S sid="34" ssid="34">An initial preprocessor, the punctoker, makes decisions about word boundaries that are not coincident with whitespace.</S>
  <S sid="35" ssid="35">It tokenizes abbreviations (e.g., "Dr .</S>
  <S sid="36" ssid="36">"), and decides when sequences of punctuation and alphabeti c characters are to be broken up into several lexemes (e .g., "Singapore-based") .</S>
  <S sid="37" ssid="37">The punctoker wraps &lt;LEX&gt; tags around text where necessary to indicate its decisions, as in the following: Singapore&lt; L EX pos=JJ&gt;-based&lt;/LEX &gt; As this example suggests, in some cases, the punctoker guides subsequent part-of-speech tagging by addin g a part-of-speech attribute to the &lt;LEX&gt; tags that it emits .</S>
  <S sid="38" ssid="38">The parasenter zones text for paragraph and sentence boundaries, the former being unnecessary for Muc-6.</S>
  <S sid="39" ssid="39">The sentence tagging component is both simple and conservative .</S>
  <S sid="40" ssid="40">If any end-of-sentence punctuation has no t been "explained" by the punctoker as part of a lexeme, as in abbreviations, it is taken to indicate a sentenc e boundary.</S>
  <S sid="41" ssid="41">The parasenter is also intended to filter lines in the text body that begin with "?"</S>
  <S sid="42" ssid="42">(but see our error analysis below) .</S>
  <S sid="43" ssid="43">A separate hl-taggeris invoked to zone sentence-like constructs in the headline field .</S>
  <S sid="44" ssid="44">The preprocess includes specialized phrase taggers.</S>
  <S sid="45" ssid="45">The title-tagger marks personal titles, making distinc- tions along the lines drawn by the NE and ST tasks .</S>
  <S sid="46" ssid="46">Included are personal honorifics (Dr., Ms .)</S>
  <S sid="47" ssid="47">; military an d religious titles (Vicar, Sgt .)</S>
  <S sid="48" ssid="48">; corporate posts (CEO, chairman) ; and "profession" words (analyst, spokesperson) .</S>
  <S sid="49" ssid="49">The date-tagger identifies TIMEX phrases.</S>
  <S sid="50" ssid="50">It uses a lex-based scanner as a front-end for tokenizing and typing its input ; then a pattern-matching engine finds the actual date phrases .</S>
  <S sid="51" ssid="51">The date-tagger is fast, sinc e the pattern matcher itself is highly optimized, and since the lex-based front-end does not actually tokenize th e input or fire the pattern-matcher unless it suspects that a date phrase may be occurring in the text .</S>
  <S sid="52" ssid="52">TE Processin g Acronyms Aliases H UNIX pre-process Zoning , Pre-tagging , fart-of-speech tagging t Phraser NE Rules TE Rules CorpNP Rules 5,T Rule_ 142 Both the date- and title-tagger can tag a phrase as either (I) a single SGML element, or (2) individual lexemes , with special attributes that indicate the beginning and end of the matrix phrase, as i n &lt;LEX post=start&gt;chief&lt;/LEX &gt; &lt;LEX post=mid&gt;executive&lt;/LEX &gt; &lt;LEX post=end&gt;officer&lt;/LEX &gt; We adopted this LEX-based phrase encoding so as to simplify (and speed up) the input scanner of the part- of-speech tagger.</S>
  <S sid="53" ssid="53">In addition, a phrases LEX tags can encode parts-of-speech to help guide the p-o-s tagger.</S>
  <S sid="54" ssid="54">THE PART-OF-SPEECH TAGGER Our part-of-speech tagger is closest among the components of our Muc-6 system to Brills original work o n rule sequences [S, 6, 7] .</S>
  <S sid="55" ssid="55">The tagger is in fact a re-implementation of Brills widely-disseminated system, wit h various speed and maintainability improvements.</S>
  <S sid="56" ssid="56">Most of the rule sequences that drive the tagger were automatically learned from hand-tagged corpora, rather than hand-crafted by human engineers .</S>
  <S sid="57" ssid="57">However, the rules are in a human-understandable form, and thus hand-crafted rules can easily be combined with automatically learned rules, a property which we exploited in the Muc-6 version of Alembic.</S>
  <S sid="58" ssid="58">The tagger operates on text that has been lexicalized through pre-processing .</S>
  <S sid="59" ssid="59">The following, for example , is how a sample walkthrough sentence is passed to the part-of-speech tagger.</S>
  <S sid="60" ssid="60">Note how punctuation has bee n tokenized, and "Mr." has been identified as a title and assigned the part-of-speech NNP (proper noun) .</S>
  <S sid="61" ssid="61">&lt;5&gt;Even so&lt;lex&gt;,&lt;/lex&gt; &lt;LEX pos=NNP ttl=WHOLE&gt;Mr .&lt;/LEX&gt; Dooner is on the prowl for more creative talen t and is interested in acquiring a hot agency&lt;lex&gt; .&lt;/lex&gt;&lt;/5 &gt; The part-of-speech tagger first assigns initial parts-of-speech by consulting a large lexicon .</S>
  <S sid="62" ssid="62">The lexicon maps words to their most frequently occurring tag in the training corpus .</S>
  <S sid="63" ssid="63">Words that do not appear in th e lexicon are assigned a default tag of NN (common noun) or NNP (proper noun), depending on capitalization.</S>
  <S sid="64" ssid="64">For unknown words, after a default tag is assigned, lexical rules apply to improve the initial guess .</S>
  <S sid="65" ssid="65">These rules operate principally by inspecting the morphology of words .</S>
  <S sid="66" ssid="66">For example, an early rule in the lexical rul e sequence retags unknown words ending in "ly" with the 10 tag (adverb) .</S>
  <S sid="67" ssid="67">In the sentence above, the only unknown word ("Dooner") is not subject to retagging by lexical rules; in fact, the default NNP tag assignment i s correct.</S>
  <S sid="68" ssid="68">Lexical rules play a larger role when the default tagging lexicon is less complete than our own, which we generated from the whole Brown Corpus plus 3 million words of Wall Street Journal text .</S>
  <S sid="69" ssid="69">For example, in our experiments tagging Spanish texts (for which we had much smaller lexica), we have found that lexica l rules play a larger role (this can also be partially attributed to the more inflected nature of Spanish) .</S>
  <S sid="70" ssid="70">After the initial tagging, contextual rules apply in an attempt to further fix errors in the tagging .</S>
  <S sid="71" ssid="71">These rules reassign a words tag on the basis of neighboring words and their tags .</S>
  <S sid="72" ssid="72">In this sentence, "more" changes from its initial JJR (comparative adjective) to RBR (comparative adverb) .</S>
  <S sid="73" ssid="73">Note that this change is arguably erroneous, depending on how one reads the scope of "more".</S>
  <S sid="74" ssid="74">This tagging is changed by the following rule , which roughly reads: change word W from JJR to ROR if the the word to Ws immediate right is tagged JJ JJR RBR nexttag JJ Table 1, below, illustrates the tagging process .</S>
  <S sid="75" ssid="75">The sample sentence is on the first line; its initial lexicon- based tagging is on the second line; the third line shows the final tagging produced by the contextual rules .</S>
  <S sid="76" ssid="76">In controlled experiments, we measured the taggers accuracy on Wall Street Journal text at 95.1% based on a training set of140,000 words.</S>
  <S sid="77" ssid="77">The production version of the tagger, which we used for Muc-6, relies on the Even so .</S>
  <S sid="78" ssid="78">Mr. Dooner Is on the prowl for more creative talent and Is Interested In acquiring a hot agency rb rb  nnp NNP vbz In dt  nn  in JJR  jj  nn  cc vbz  jj  In  vim  dt jj  n n rb rb  nnp NNP vbz In dt  nn  in RBR  jj  nn  cc vbz  jj  In  vim  dt jj  n n Table 1 : Tagging a text with the lexicon (line 2) and contextual rules (line 3) .</S>
  <S sid="79" ssid="79">Note the defaul t lexicon assignment of nnp to "Dooner" and the rule-based correction of "more" .</S>
  <S sid="80" ssid="80">143 learned rules from Brills release 1 .1 (148 lexical rules, 283 contextual rules), for which Brill has measure d accuracies that are 2-3 percentage points higher than in our own smaller-scale experiments .</S>
  <S sid="81" ssid="81">For MUC-6, we combined these rules with 19 hand-crafted contextual rules that correct residual tagging errors that were especially detrimental to our NE performance.</S>
  <S sid="82" ssid="82">Tagger throughput is around 3000 words/sec.</S>
  <S sid="83" ssid="83">THE PHRASER The Alembic phrase finder, or phraser for short, performs the bulk of the systems syntactic analysis .</S>
  <S sid="84" ssid="84">As noted above, it has somewhat less recognition power than a finite-state machine, and as such shares many characteristics of pattern-matching systems, such as CIRCUS [10] or FASTUS [2] .</S>
  <S sid="85" ssid="85">Where it differs from these systems is in being driven by rule sequences.</S>
  <S sid="86" ssid="86">We have experimented with both automatically-learned rul e sequences and hand-crafted ones .</S>
  <S sid="87" ssid="87">In the system we fielded for Muc-6, we ended up running entirely with hand-crafted sequences, as they outperformed the automatically-learned rules .</S>
  <S sid="88" ssid="88">How the phraser works The phraser process operates in several steps .</S>
  <S sid="89" ssid="89">First, a set of initial phrasing functions is applied to all of the sentences to be analyzed .</S>
  <S sid="90" ssid="90">These functions are responsible for seeding the sentences with likely candidate phrases of various kinds.</S>
  <S sid="91" ssid="91">This seeding process is driven by word lists, part-of-speech information, and pre- taggings provided by the preprocessors.</S>
  <S sid="92" ssid="92">Initial phrasing produces a number of phrase structures, many o f which have the initial null labeling (none), while some have been assigned an initial label (e .g., num) .</S>
  <S sid="93" ssid="93">The following example shows a sample sentence from the walkthrough message after initial phrasing .</S>
  <S sid="94" ssid="94">Yesterday, &lt;none&gt;McCann&lt;/none&gt; made official what had been widely anticipated : &lt;ttl&gt;Mr.&lt;/ttl &gt; &lt;none&gt;James&lt;/none&gt;, &lt;num&gt;57&lt;/num&gt; years old, is stepping down as &lt;post&gt;chief executive officer&lt;/post&gt; o n &lt;date&gt;July 1&lt;/date&gt; and will retire as &lt;post&gt;chairman&lt;/post&gt; at the end of the year .</S>
  <S sid="95" ssid="95">The post, ttl, and date phrases were identified by the title and date taggers .</S>
  <S sid="96" ssid="96">Mr. James num-tagged age is identified on the basis of part of speech information, as is the organization name "McCann" .</S>
  <S sid="97" ssid="97">Once the initial phrasing has taken place, the phraser proceeds with phrase identification proper .</S>
  <S sid="98" ssid="98">This is driven by a sequence of phrase-finding rules .</S>
  <S sid="99" ssid="99">Each rule in the sequence is applied in turn against all of the phrases in all the sentences under analysis.</S>
  <S sid="100" ssid="100">If the antecedents of the rule are satisfied by a phrase, then th e action indicated by the rule is executed immediately.</S>
  <S sid="101" ssid="101">The action can either change the label of the satisfyin g phrase, grow its boundaries, or create new phrases .</S>
  <S sid="102" ssid="102">After the nth rule is applied in this way against every phrase in all the sentences, the n+lth rule is applied in the same way, until all rules have been applied .</S>
  <S sid="103" ssid="103">After all of the rules have been applied, the phraser is done.</S>
  <S sid="104" ssid="104">It is important to note that the search strategy in the phraser differs significantly from that in standar d parsers.</S>
  <S sid="105" ssid="105">In standard parsing, one searches for any and all rules whose antecedents might apply given the stat e of the parsers chart : all these rules become candidates for application, and indeed they all are applie d (modulo higher-order search control) .</S>
  <S sid="106" ssid="106">In our phraser, only the current rule in a rule sequence is tested: the rule is applied wherever this test succeeds, and the rule is never revisited at any subsequent stage of processing .</S>
  <S sid="107" ssid="107">After the final rule of a sequence is run, no further processing occurs .</S>
  <S sid="108" ssid="108">The language of phraser rule s The language of the phraser rules is as simple as their control strategy .</S>
  <S sid="109" ssid="109">Rules can test lexemes to the left and right of the phrase, or they can look at the lexemes in the phrase .</S>
  <S sid="110" ssid="110">Tests in turn can be part-of-speech queries, literal lexeme matches, tests for the presence of neighboring phrases, or the application of predicates that are evaluated by invoking a Lisp procedure.</S>
  <S sid="111" ssid="111">There are several reasons for keeping this rule languag e simple.</S>
  <S sid="112" ssid="112">In the case of hand-crafted rules, it facilitates the process of designing a rule sequence.</S>
  <S sid="113" ssid="113">In the case of machine-learned rules, it restricts the size of the search space on each epoch of the learning regimen, thus making it tractable.</S>
  <S sid="114" ssid="114">In either case, the overall processing power derives as much from the fact that the rule s are sequenced, and feed each other in turn, as it does from the expressiveness of the rule language.</S>
  <S sid="115" ssid="115">144 To make this clearer, consider a simple named entity rule that Is applied to identifying persons.</S>
  <S sid="116" ssid="116">(clef-phraser label  none left-1  phrase ttl label-action person) This rule changes the label of a phrase from none to person if the phrase is bordered on its left by a ttl phrase .</S>
  <S sid="117" ssid="117">On the sample sentence, this rule causes the following relabeling of the phrase around "James" .</S>
  <S sid="118" ssid="118">Yesterday, &lt;none&gt;McCann&lt;/none&gt; made official what had been widely anticipated : &lt;ttl&gt;Mr.&lt;/ttl&gt; &lt;person&gt;James&lt;/person&gt;, &lt;num&gt;57&lt;/num&gt; years old, is stepping down as &lt;post&gt;chief executiv e officer&lt;/post&gt; on &lt;date&gt;July 1&lt;/date&gt; and will retire as &lt;post&gt;chairman&lt;/post&gt; at the end of the year .</S>
  <S sid="119" ssid="119">Once this rule has run, the labelings it instantiates become available as input to subsequent rules in th e sequence, e.g., rules that attach the title to the person in "Mr. James", that attach the age apposition, and s o forth .</S>
  <S sid="120" ssid="120">Phraser rules do make mistakes, but as with other sequence-based processors, the phraser applies later rules in a sequence to patch errors made by earlier rules .</S>
  <S sid="121" ssid="121">In the walkthrough message, for example, "Amarati &amp; Purls" is identified as an organization, which ultimately leads to an incorrect org tag for "Martin Purls", since this persons name shares a common substring with the organization name .</S>
  <S sid="122" ssid="122">However, rules that find personal names occur later in our named entities sequence than those which find organizations, thus allowing th e phraser to correctly relabel "Martin Purls" as a person on the basis of a test for common first names.</S>
  <S sid="123" ssid="123">Rule sequences for MUG6 For MUC-6, Alembic relies on three sequences of phraser rules, divided roughly into rules for generatin g NE-specific phrases, those for finding TE-related phrases, and those for ST phrases.</S>
  <S sid="124" ssid="124">The division is only rough , as the NE sequence yields some number of TE-related phrases as a side-effect of searching for named entities .</S>
  <S sid="125" ssid="125">To illustrate this process, consider the following walkthrough sentence, as tagged by the NE rule sequence .</S>
  <S sid="126" ssid="126">But the bragging rights to &lt;org&gt;Coke&lt;/org&gt; s ubiquitous advertising belongs to &lt;org&gt;Creative Artists Agency &lt;/org&gt;, the big &lt;location&gt;Hollywood&lt;/location&gt; talent agency .</S>
  <S sid="127" ssid="127">The org label on "Creative Artists Agency" was set by a predicate that tests for org keywords (like "Agency") .</S>
  <S sid="128" ssid="128">"Coke" was found to be an org elsewhere in the document, and the label was then percolated .</S>
  <S sid="129" ssid="129">Finally, the location label on "Hollywood" was set by a predicate that inspects the tried-and-not-so-true TIPSTER gazetteer .</S>
  <S sid="130" ssid="130">What is important to note about these NE phraser rules is that they do not rely on a large database o f known company names .</S>
  <S sid="131" ssid="131">Instead, the rules are designed to recognize organization names in almost complet e absence of any information about particular organization names (with the sole exception of a few acronyms such as IBM, GM, etc.)</S>
  <S sid="132" ssid="132">This seems to auger well for the ability to apply Alembic to different application tasks .</S>
  <S sid="133" ssid="133">Proceeding beyond named entities, the phraser next applies its TE-specific rule sequence .</S>
  <S sid="134" ssid="134">This sequence performs manipulations that resemble NP parsing, e.g., attaching locational modifiers.</S>
  <S sid="135" ssid="135">In addition, a subsequence of TE rules concentrates on recognizing potential organization descriptors .</S>
  <S sid="136" ssid="136">These rules generate so-called corpnp phrases, that is noun phrases that are headed by an organizational common noun (such a s "agency", "maker", and of course "company") .</S>
  <S sid="137" ssid="137">The rules expand these heads leftwards to incorporate lexemes that satisfy a set of part-of-speech constraints .</S>
  <S sid="138" ssid="138">One such phrase, for example, is in the sample sentence above .</S>
  <S sid="139" ssid="139">But the bragging rights to &lt;org&gt;Coke&lt;/org&gt;  s ubiquitous advertising belongs to &lt;org&gt;&lt;org&gt;Creative Artist s Agency &lt;/org&gt;, &lt;corpnp&gt; the big &lt;location&gt;Hollywood&lt;/location&gt; talent agency&lt;/corpnp&gt;&lt;/org &gt; After corpnp phrases have been marked, another collection of TE rules associates these phrases with neigh- boring org phrases .</S>
  <S sid="140" ssid="140">In this case such a phrase is found two places to the left (on the other side of a comma) , so a new org phrase is created which spans both the original org phrase and its corpnp neighbor .</S>
  <S sid="141" ssid="141">Note that these rule sequences encode a semantic grammar .</S>
  <S sid="142" ssid="142">Organizationally-headed noun phrases are labeled as org, regardless of whether they are simple proper names or more complex constituents such as th e 145 * * * TOTAL SLOT SCORES * * * +	 +	 + SLOT  POS ACTT COR PAR INCI SPU MIS NONI REC PRE UND OVG ERR SU B +	 +	 + &lt;enamex&gt;938 9911 881 0 01 110 57 01 94 89 6 11 16 0 type 938 9911 775 0 1061 110 57 01 83 78 6 11 26 1 2 text 938 9911 840 0 411 110 57 01 90 85 6 11 20 5 subto 1876 19821 1615 0 1471 220 114 01 86 81 6 11 23 8 +	 +	 + ALL OB 2286 24061 1993 0 1631 250 130 01 87 83 6 10 21  8 MATCHD 2156 21561 1993 0 1631 0 0 01 92 92 0 0 8  8 +	 +	 + P&amp;R  2P&amp;R  P&amp;2R F-MEASURES  84 .95  83 .67  86 .2 8 * * * TASK SUBCATEGORIZATION SCORES * * * +	 +	 + SLOT  POS ACTT COR PAR INC SPU MIS NONI REC PRE UND OVG ERR SUB +	 +	 + Enamex : organi 454 4931 392 0 281 73 34 0I 86 80 7 15 26 7 person 373 3641 292 0 601 12 21 0I 78 80 6 3 24 17 locati 111 1341 91 0 181 25 2 0I 82 68 2 19 33 1 6 Figure4: Performance of rules learned for theENAMEXportion of theNEtask(unofficialscore) org-corpnp apposition above.</S>
  <S sid="143" ssid="143">This semantic characteristic of the phraser grammar is clearer still with ST rules .</S>
  <S sid="144" ssid="144">These rules are responsible for finding phrases denoting events relevant to the MUC-6 scenario templates . )</S>
  <S sid="145" ssid="145">For the succession scenario, this consists of a few key phrase types, most salient among them : job (a post at an org), job-in and job-out (fully specified successions) and post-in and post-out (partially specified successions) .</S>
  <S sid="146" ssid="146">The following example shows the ST phrases parsed out of a key sentence from the walkthrough message.</S>
  <S sid="147" ssid="147">Yesterday, &lt;person&gt;McCann&lt;/person&gt; made official what had been widely anticipated : &lt;post-out &gt; &lt;person &gt; &lt; person &gt;&lt;ttl &gt;Mr.</S>
  <S sid="148" ssid="148">&lt;/ttl&gt; &lt;person&gt;James&lt;/person&gt; &lt;/person&gt; , &lt;age&gt;&lt;num&gt;57&lt;/num&gt; years old&lt;/age &gt; &lt;/person&gt; , is stepping down a s &lt;post&gt;chief executive officer&lt;/post&gt; &lt;/post-out&gt; [ .</S>
  <S sid="149" ssid="149">The post-out phrase encodes the resignation of a person in a post.</S>
  <S sid="150" ssid="150">Note that in the formal evaluation we failed to find a more-correct job-out phrase, which should have included "McCann" .</S>
  <S sid="151" ssid="151">This happened because we did not successfully identify "McCann" as an organization, thus precluding the formation of the job-out phrase .</S>
  <S sid="152" ssid="152">Learning Phrase Rules We have applied the same general error-reduction learning approach that Brill designed for generating part-of-speech rules to the problem of learning phraser rules in support of the NE task.</S>
  <S sid="153" ssid="153">The official version of Alembic for MUC-6 did not use any of the rule sequences generated by this phrase rule learner, but we hav e since generated unofficial scores .</S>
  <S sid="154" ssid="154">In these runs we used phrase rules that had been learned for the ENAMEX expressions only?we still used the hand-coded pre-processors and phraser rules for recognizing TIMEX and NUMEX phrases .</S>
  <S sid="155" ssid="155">Our performance on this task is shown in Fig .</S>
  <S sid="156" ssid="156">These rules yield six fewer points o f P&amp;R than the hand-coded ENAMEX rules?still an impressive result for machine-learned rules .</S>
  <S sid="157" ssid="157">Interestingly, the bulk of the additional error in the machine-learned rules is not with the "hard" organization names, but with person names OR=-If, LP=-14) and locations (AR-I2, AP=-18) .</S>
  <S sid="158" ssid="158">1 We put about one staff week of work into the sT task, during which we experienced steep hill-climbing on the training set .</S>
  <S sid="159" ssid="159">Never- theless, we felt that the maturity of our sT processing was sufficiently questionable to preclude participating in the official evaluation .</S>
  <S sid="160" ssid="160">The present discussion should be taken in this light, i .e., with the understanding that it was not officially evaluated atMuc-6 .</S>
  <S sid="161" ssid="161">146 PHRASE INTERPRETATION AND INFERENC E The inference component is central to all processing beyond phrase identification .</S>
  <S sid="162" ssid="162">It has three roles .</S>
  <S sid="163" ssid="163">As a representational substrate, it records propositions encoding the semantics of parsed phrases ; ?</S>
  <S sid="164" ssid="164">As an equational system, it allows initially distinct semantic individuals to be equated to each other, an d allows propositions about these individuals to be merged through congruence closure .</S>
  <S sid="165" ssid="165">As a limited inference system, it allows domain-specific and general constraints to be instantiate d through carefully controlled forward-chaining .</S>
  <S sid="166" ssid="166">Phrase Interpretation Facts enter the propositional database as the result of phrase interpretation .</S>
  <S sid="167" ssid="167">The phrase interpreter is controlled by a small set of Lisp interpretation functions, roughly one for each phrase type .</S>
  <S sid="168" ssid="168">Base-level phrases, i .e.</S>
  <S sid="169" ssid="169">phrases with no embedded phrases, are mapped to unary interpretations.</S>
  <S sid="170" ssid="170">The phras e &lt;person&gt;IZobert L. James&lt;/person&gt;, for example is mapped to the following propositional fact .</S>
  <S sid="171" ssid="171">Note the pers-01 term in this proposition: it designates the semantic individual denoted by the phrase, and is generated in the process of interpretation .</S>
  <S sid="172" ssid="172">person(pers-01 ) Complex phrases, those with embedded phrases, are typically interpreted as conjuncts of simple r interpretations (the exception being NP coordination, as in "chairman and chief executive") .</S>
  <S sid="173" ssid="173">Consider the phrase "Mr. James, 57 years old" which is parsed by the phraser as follows .</S>
  <S sid="174" ssid="174">Note in particular that the overal l person-age apposition is itself parsed as a person phrase .</S>
  <S sid="175" ssid="175">&lt;person&gt;&lt;person&gt;Mr.</S>
  <S sid="176" ssid="176">James&lt;/person&gt;, &lt;age&gt;&lt;num&gt;57&lt;/num&gt; years old&lt;/age&gt;&lt;/person &gt; The treatment of age appositions is compositional, as is the case for the interpretation of all but a few complex phrases .</S>
  <S sid="177" ssid="177">Once again, the embedded base-level phrase ends up interpreted as a unary person fact.</S>
  <S sid="178" ssid="178">The semantic account of the overall apposition ends up as a has-age relation modifying pers-02, the semanti c individual for the embedded person phrase .</S>
  <S sid="179" ssid="179">This proposition designates the semantic relationship between a person and that persons age .</S>
  <S sid="180" ssid="180">More precisely, the following facts are added to the inferential database .</S>
  <S sid="181" ssid="181">person(pers-02 ) has-age((pers-02, age-03) ha-04 ) age(age-03 ) What appears to be a spare argument to the has-age predicate above is the event individual for the predicate.</S>
  <S sid="182" ssid="182">Such arguments denote events themselves (in this case the event of being a particular number o f years old), as opposed to the individuals participating in the events (the individual and his or her age) .</S>
  <S sid="183" ssid="183">This treatment is similar to the partial Davidsonian analysis of events due to Hobbs [8] .</S>
  <S sid="184" ssid="184">Note that event indi- viduals are by definition only associated with relations, not unary predicates .</S>
  <S sid="185" ssid="185">As a point of clarification, note that the inference system does not encode facts at the predicate calculu s level so much as at the interpretation level made popular in such systems as the SRI core language engine [1, 3] .</S>
  <S sid="186" ssid="186">In other words, the representation is actually a structured attribute-value graph such as the following, whic h encodes the age apposition above .</S>
  <S sid="187" ssid="187">[[head :person] [proxy pers-02] [modifiers [[head has-age] [proxy ha-04] [arguments (pers-02 [[head age ] [proxy age-03]])]]] ] The first two fields correspond to the embedded phrase : the head field is a semantic sort, and the proxy field holds the designator for the semantic individual denoted by the phrase .</S>
  <S sid="188" ssid="188">The interpretation encoding the 147 overall apposition ends up in the modifiers slot, an approach adopted from the standard linguistic account o f phrase modification.</S>
  <S sid="189" ssid="189">Inference in Alembic is actually performed directly on interpretation structures, an d there is no need for a separate translation from interpretations to more traditional-looking propositions .</S>
  <S sid="190" ssid="190">The propositional notation is more perspicuous to the reader, and we have adopted it here .</S>
  <S sid="191" ssid="191">Finally, note that the phrase interpretation machinery maintains pointers between semantic individual s and the surface strings from which they originated .</S>
  <S sid="192" ssid="192">One of the fortunate?if unexpected?consequences o f the phrasers semantic grammar is that maintaining these cross-references is considerably simpler than was th e case in our more linguistically-inspired categorial parser of old .</S>
  <S sid="193" ssid="193">Except for the ORG_DESCRIPTOR slot, the fil l rules line up more readily with semantic notions than with syntactic considerations, e .g., maximal projections .</S>
  <S sid="194" ssid="194">Equality reasoning Much of the strength of this inferential framework derives from its equality mechanism .</S>
  <S sid="195" ssid="195">This subcomponent allows one to make two semantic individuals co-designating, i.e., to "equate" them .</S>
  <S sid="196" ssid="196">Facts that formerly held of only one individual are then copied to its co-designating siblings .</S>
  <S sid="197" ssid="197">This in turn enables inference that may have been previously inhibited because the necessary antecedents were distributed ove r (what were then) distinct individuals .</S>
  <S sid="198" ssid="198">This equality machinery is exploited at many levels in processing semantic and domain constraints .</S>
  <S sid="199" ssid="199">One of the clearest such uses is in enforcing the semantics of coreference, either definite reference or appositional coreference.</S>
  <S sid="200" ssid="200">Take for example the following phrase from the walkthrough message, which we show here a s parsed by the phraser.</S>
  <S sid="201" ssid="201">&lt;org&gt; &lt;org&gt;Creative Artists Agency&lt;/org&gt; , &lt;orgnp&gt;the big &lt;location&gt;Hollywood&lt;/Iocation&gt; talent agency&lt;/orgnp &gt; &lt;/org &gt; In propositional terms, the embedded organization is interpreted a s organization(org-05)  "Creative Artists Agency" The appositive noun phrase is interpreted a s organization(org-06)  "the .</S>
  <S sid="202" ssid="202">.agency" geo-region(geo-07)  "Hollywood " has-location((org-06, geo-07) hasloc-08)  locational pre-modifier Pressing on, the phraser parses the overall org-orgnp apposition as an overarching org .</S>
  <S sid="203" ssid="203">To interpret th e apposition, the interpreter also adds the following proposition to the database .</S>
  <S sid="204" ssid="204">entity-np-app((org-05, org-06) e-n-a-09) This ultimately causes org-05 and org-06 to become co-designating through the equality system, and th e following fact appears in the inferential database .</S>
  <S sid="205" ssid="205">has-location((org-05, geo-07) hasloc-10)  i.e., Creative Artists Agency is located in Hollywoo d This propagation of facts from one individual CO its co-designating siblings is the heart of our coreferenc e mechanism.</S>
  <S sid="206" ssid="206">Its repercussions are particularly critical to the subsequent stage of template generation.</S>
  <S sid="207" ssid="207">By propagating facts in this way, we can dramatically simplify the process of collating information into templates , since all the information relevant to, say, an individual company will have been attached to that company b y equality reasoning .</S>
  <S sid="208" ssid="208">We will touch on this point again below .</S>
  <S sid="209" ssid="209">Inference The final role of the Alembic inference component is to derive new facts through the application o f carefully-controlled forward inference.</S>
  <S sid="210" ssid="210">As was the case with our MUC-5 system, the present Alembic allows only limited forward inference.</S>
  <S sid="211" ssid="211">Though the full details of this inference process are of legitimate interest i n 148 their own right, we will only note some highlights here .</S>
  <S sid="212" ssid="212">To begin with, the tractability of forward inference in this framework is guaranteed just in case the inference axioms meet a certain syntactic requirement .</S>
  <S sid="213" ssid="213">To date, all the rules we have written for even complex domains, such as the joint-venture task in MUC-5, have met this criterion.</S>
  <S sid="214" ssid="214">Aside from this theoretical bound on computation, we have found in practice that th e inference system is remarkably fast, with semantic interpretation, equality reasoning, rule application, and al l other aspects of inference together accounting for 6-7% of all processing time in Alembic.</S>
  <S sid="215" ssid="215">Details are in [II] .</S>
  <S sid="216" ssid="216">We exploited inference rules in several primary ways for the TE and ST tasks.</S>
  <S sid="217" ssid="217">The first class of inferenc e rules enforce so-called terminological reasoning, local inference that composes the meaning of words .</S>
  <S sid="218" ssid="218">One such rule distributes the meaning of certain adjectives such as "retired" across coordinated titles, as in "retire d chairman and CEO" .</S>
  <S sid="219" ssid="219">The phrase parses as follows; note the embedded post semantic phrase types .</S>
  <S sid="220" ssid="220">&lt;post &gt; &lt; post-qua I &gt; retired &lt;/post- aua I &gt; &lt;post &gt; &lt;post&gt;chai rman &lt;/post&gt; and &lt;post&gt;CEO&lt;/post &gt; &lt;/post&gt; &lt;/post&gt; This particular example propositionalizes as follows, where the group construct denotes a plural individual in Landmans sense (roughly a set [9]) .</S>
  <S sid="221" ssid="221">title(ttl-11)  "chairman" title(ttl-12)  "CEO" group((ttl-11, ttl-12) grp-13)  "chairman and CEO" retired-ttl(grp-13)  "retired " To shift the scope of "retired" from the overall coordination to individual titles, the following rule applies .</S>
  <S sid="222" ssid="222">retired-ttl(ttl) &lt;?</S>
  <S sid="223" ssid="223">group((ttl, x) grp) + retired(grp ) This rule yields the fact retired-ttl(ttl-11), and a similar rule yields retired-ttl(ttl-12) .</S>
  <S sid="224" ssid="224">Other like rules distribute coordinated titles across the title-holder, and so forth .</S>
  <S sid="225" ssid="225">The fact that multiple rules are needed t o distribute adjectives over coordinated noun phrases is one of the drawbacks of semantic grammars .</S>
  <S sid="226" ssid="226">On the other hand, these rules simplify semantic characteristics of distributivity by deferring questions of scope and non-compositionality to a later stage, i .e., inference.</S>
  <S sid="227" ssid="227">Interpretation procedures can thus remain composi- tional, which makes them substantially simpler to write .</S>
  <S sid="228" ssid="228">Additionally, these kinds of distribution rules further contribute to collating facts relevant to template generation onto the individuals for which these facts hold .</S>
  <S sid="229" ssid="229">Of greater importance, however, is the fact that inference rules are the mechanism by which we instantiat e domain-specific constraints and set up the particulars required for scenario-level templates .</S>
  <S sid="230" ssid="230">Some of this information is again gained by fairly straightforward compositional means .</S>
  <S sid="231" ssid="231">For example, the phrase "Walter IZawleigh Jr ., retired chairman of Mobil Corp ."</S>
  <S sid="232" ssid="232">yields a succession template through the mediation of on e inference rule .</S>
  <S sid="233" ssid="233">The phrase is compositionally interpreted as organization(org-14) title(ttl-15) retired-ttl(ttl-15) job((ttl-15, org-14), job-16 ) person(pers-17) holds job((pers-17, job-16) h -j-18 ) The rule that maps these propositions to a succession event i s job-out(pers, ttl, org) &lt;?</S>
  <S sid="234" ssid="234">holds job((pers,job) x) +job((ttl, org), job) + retired-ttl(ttl ) When applied to the above propositions this rule yields job-out((pers-17, ttl-15, org-07) j-o-19) .</S>
  <S sid="235" ssid="235">This fact is all that is required for the template generator to subsequently issue the appropriate succession event templates .</S>
  <S sid="236" ssid="236">149 The most interesting form of domain inference is not compositional of course, but based on discours e considerations.</S>
  <S sid="237" ssid="237">In the present ST task, for example, succession events are not always fully fleshed out, bu t depend for their complete interpretation on information provided earlier in the discourse .</S>
  <S sid="238" ssid="238">In the walkthrough message, this kind of contextualized interpretation is required early on : Yesterday, McCann made official what had been widely anticipated : Mr. James, 57 years old, is stepping down a s chief executive officer on July 1 [ .</S>
  <S sid="239" ssid="239">He will be succeeded by Mr. Donner, 45.</S>
  <S sid="240" ssid="240">ST-level phrasing and interpretation of this passage produces two relevant facts, a job-out for the firs t clause, and a successor for the second .</S>
  <S sid="241" ssid="241">Note that although successor is normally a two-place relation, its valence here is one by virtue of the phraser not finding a named person as a subject to the clause .</S>
  <S sid="242" ssid="242">person(pers-20)  "Mr. James" title(ttl-21)  ; "chairman " organization(org-22)  "McCann " job-out((pers-20, ttl-21, org-22) j-o-23) person(pers-24)  "Mr. Dooner" successor((pers-24) succ-25 ) One approach to contextualizing the succession clause in this text would require first resolving th e pronominal subject "He" to "Mr. James" and then exploiting any job change facts that held about thi s antecedent.</S>
  <S sid="243" ssid="243">An equally effective approach, and simpler, is to ignore the pronoun and reason directly from th e successor fact to any contextualizing job-out fact .</S>
  <S sid="244" ssid="244">The rule that accomplishes this i s job-in(pers-a, ttl, org) &lt;?</S>
  <S sid="245" ssid="245">successor((pers-a) succ) + job-out-in-context?</S>
  <S sid="246" ssid="246">((succ, job-out) x-1) + job-out((pers-b, ttl, org) x-2 ) The mysterious-looking job-out-in-context?</S>
  <S sid="247" ssid="247">predicate implements a simple discourse model : it is true just in case its second argument is the most immediate job-out fact in the context of its first argument .</S>
  <S sid="248" ssid="248">Context- encoding facts are not explicitly present in the database, as their numbers would be legion, but are instantiate d "on demand" when a rule attempts to match such a fact .</S>
  <S sid="249" ssid="249">Note that what counts as a most immediate contex- tualized fact is itself determined by a separate search procedure .</S>
  <S sid="250" ssid="250">The simple-minded strategy we adopted here is to proceed backwards from the current sentence, searching for the most recent sentence containing a n occurrence of ajob-out phrase, and returning the semantic individual it denotes .</S>
  <S sid="251" ssid="251">In this example, the job-out- in-contexa predicate succeeds by binding the succ variable to j-o-23, with the rule overall yielding a job-in fact .</S>
  <S sid="252" ssid="252">job-in((pers-24, ttl-21, org-22) j-i-26 ) As with job-out, this fact is mapped directly by the template generator to an incoming succession template.</S>
  <S sid="253" ssid="253">Note that this process of combining a job-out and successor fact effectively achieves what is ofte n accomplished in data extraction systems by template merging .</S>
  <S sid="254" ssid="254">However, since the merging takes place in th e inferential database, with propagation of relevant facts as a side-effect, the process is greatly simplified an d obviates the need for explicit template comparisons .</S>
  <S sid="255" ssid="255">One final wrinkle must be noted .</S>
  <S sid="256" ssid="256">Inference is generally a non-deterministic search problem, with no firm guarantee as to whether facts will be derived in the same chronological order as the sentences which underli e the facts .</S>
  <S sid="257" ssid="257">Rules that require contextualized facts, however, crucially rely on the chronological order of th e sentences underlying these facts.</S>
  <S sid="258" ssid="258">We have thus pulled these rules out of the main fray of inference, and apply them only after all other forward chaining is complete .</S>
  <S sid="259" ssid="259">In fact, these rules are organized as a Brill-style rule sequence, where each rule is allowed to run to quiescence at only one point in the sequence before the nex t rule becomes active.</S>
  <S sid="260" ssid="260">It is our hypothesis, though, that alldomain inference rules can be so organized, not jus t contextualized ones, and that by this organizational scheme, rules can be automatically learned from example .</S>
  <S sid="261" ssid="261">TASK SPECIFIC PROCESSING AND TEMPLATE GENERATIO N Aside from phrasing and inference, a relatively small?but critical?amount of processing is required t o perform the Muc-6 named entities and template generation tasks .</S>
  <S sid="262" ssid="262">150 For NE, little is actually required beyond careful document management and printing routines .</S>
  <S sid="263" ssid="263">TIMEX forms, introduced by the preprocessing date-tagger, must be preserved through the rest of the processing pipe .</S>
  <S sid="264" ssid="264">Named entity phrases that encode an intermediate stage of NE processing must be suppressed at printout .</S>
  <S sid="265" ssid="265">Examples such as these abound, but by and large, Alembics NE output is simply a direct readout of the resul t of running the named entity phraser rules .</S>
  <S sid="266" ssid="266">Name coreference in TE Of all three tasks, TE is actually the one that explicitly requires most idiosyncratic processing beyon d phrasing and inference .</S>
  <S sid="267" ssid="267">Specifically, this task is the crucible for name coreference, i .e., the process by which short name forms are reconciled with their originating long forms .</S>
  <S sid="268" ssid="268">This merging process takes place by iterating over the semantic individuals in the inferential database tha t are of a namable sort (e.g., person or organization) .</S>
  <S sid="269" ssid="269">Every such pair of same-sort individuals is compared t o determine whether one is a derivative form of the other .</S>
  <S sid="270" ssid="270">Several tests are possible .</S>
  <S sid="271" ssid="271">If the forms are identical strings, as in the frequently repeated "Dooner", or "McCann" in th e walkthrough article, then they are merged.</S>
  <S sid="272" ssid="272">If one form is a shortening of the other, as in "Mr. James" for "Robert L. James", then the short form is merged as an alias of the longer .</S>
  <S sid="273" ssid="273">If one form appears to be an acronym for the other, as in "CAA" and "Creative Artist s Agency", then the forms should be merged, with the acronym designated as an alias .</S>
  <S sid="274" ssid="274">Merging two forms takes place in several steps .</S>
  <S sid="275" ssid="275">First, their respective semantic individuals are equated in the inferential database .</S>
  <S sid="276" ssid="276">This allows facts associated with one form to become propagated to the other.</S>
  <S sid="277" ssid="277">In this way, the nationality information in "Japanese giant NEC" becomes associated with the canonical nam e "Nippon Electric Corp ."</S>
  <S sid="278" ssid="278">As a second step, names that are designated as aliases are recorded as such .</S>
  <S sid="279" ssid="279">Template generation We mentioned above that the inferential architecture that we have adopted here is in good part motivate d by a desire to simplify template generation .</S>
  <S sid="280" ssid="280">Indeed, template generation consists of nothing more than reading out the relevant propositions from the database .</S>
  <S sid="281" ssid="281">For the TE task, this means identifying person and organization individuals by matching on person(x) o r organization(y) .</S>
  <S sid="282" ssid="282">For each so-matching semantic individual, we create a skeletal template .</S>
  <S sid="283" ssid="283">The skeleton is initialized with name and alias strings that were attached to the semantic individuals during name merging .</S>
  <S sid="284" ssid="284">I t is further fleshed out by looking up related facts that hold of the matched individual, e.g., has-location(y, z ) for organizations or has-title(x, w) for persons .</S>
  <S sid="285" ssid="285">These facts are literally just read out of the database .</S>
  <S sid="286" ssid="286">Finalization routines are then invoked on the near-final template to fill the ORG TYPE slot and to normalize the geographical fills of the ORG_LOCALE and ORG COUNTRY slots .</S>
  <S sid="287" ssid="287">PERFORMANCE ANALYSIS We participated in two officially-scored tasks at MUC-6, named entities and template elements .</S>
  <S sid="288" ssid="288">As noted above, we put roughly a staff week into customizing the system to handle the scenario templates task, bu t chose not to participate in the evaluation because another staff week or so would have been required to achieve performance on a par with other parts of the system.</S>
  <S sid="289" ssid="289">Overall performance On the named entity task, we obtained an official P&amp;R score of 91 .2, where the separate precision and recal l scores were both officially reported to be 91 .</S>
  <S sid="290" ssid="290">The overall score is remarkably close to our performance on th e 151 Formal training  Official test Recall Precision Recall Precision Recall A Precis .</S>
  <S sid="291" ssid="291">A organization 86 92 84 92 -2 ?</S>
  <S sid="292" ssid="292">name 76 78 77 80 +1 +2 alias 60 79 56 78 -4* - 1 descriptor 27 62 16 49 -11 * -13* type 83 90 81 89 -2 - 1 locale 46 87 43 87 -3 ?</S>
  <S sid="293" ssid="293">country 47 88 45 93 -2 +5 person 94 92 95 87 +1 -5 * name 93 91 93 84 ?</S>
  <S sid="294" ssid="294">_7* alias 94 95 86 96 -8* +1 title 95 96 94 93 -1 -3 All Objects 75 86 73 85 -2 - 1 F-Measure, unrevised 80.21 78.52 -1.69 Table 2: Slot-by-slot performance differences, TE task (unrevised scores) .</S>
  <S sid="295" ssid="295">dry-run test set which served as our principal source of data for NE training and self-evaluation .</S>
  <S sid="296" ssid="296">To be precise, our final dry-run P&amp;R score prior to the MUC-6 evaluation run was 91.8, a scant o.6 higher than the officially measured evaluation score.</S>
  <S sid="297" ssid="297">The fact that the score dropped so little is encouraging to us .</S>
  <S sid="298" ssid="298">On the template elements task, our initial TE score was P&amp;R=78 .5, and our revised official score was 77.</S>
  <S sid="299" ssid="299">Once again, this performance is encouragingly close to Alembics performance on our final self-evaluation using the formal training data set .</S>
  <S sid="300" ssid="300">By the non-revised metric, we achieved a performance of P&amp;R=80.2 on the training data, with an overall drop of 1 .7 points ofP&amp;R between training and official test.</S>
  <S sid="301" ssid="301">Table 2 summarizes slot-by-slot differences between our training and test performance on the TE task.</S>
  <S sid="302" ssid="302">The major differences we noted between training and testing performance lie in the organization alias and descriptor slots, and in th e person name and alias fields ; we have marked these discrepancies with asterisks (*) and will address their caus e later on in this document .</S>
  <S sid="303" ssid="303">Walkthrough errors In order to quantify Alembics performance on the walkthrough message, we compiled an exhaustive analysis of our errors on this text.</S>
  <S sid="304" ssid="304">This was a difficult message for us, and we scored substantially less well on this message than in average, especially on the NE task .</S>
  <S sid="305" ssid="305">To our surprise, the majority of our errors was du e not to knowledge gaps in the named entity tagger, so much as to odd bugs and incompletely thought-ou t design decisions.</S>
  <S sid="306" ssid="306">Table 3 summarizes these errors .</S>
  <S sid="307" ssid="307">Entries marked with daggers (t) correspond to knowledg e gaps, e.g., missing or incorrect rules ; the other entries are coding or design problems.</S>
  <S sid="308" ssid="308">Fully half the problem instances were due to string matching issues for short name forms .</S>
  <S sid="309" ssid="309">For example, by not treating embedde d mid-word hyphens as white space, we failed to process "McCann" as a shortened form of "McCann-Erickson".</S>
  <S sid="310" ssid="310">Turning now to the template element task, we note that the largest fraction of TE errors are repercussions of errors committed while performing the NE task.</S>
  <S sid="311" ssid="311">In particular, the people-name companies that wer e treated as persons during NE processing in turn led to spurious person templates .</S>
  <S sid="312" ssid="312">The magnitude of the NE error is mitigated by the fact that identical mentions of incorrectly-tagged named entities are merged for th e sake of TE template generation, and thus do not all individually spawn spurious templates .</S>
  <S sid="313" ssid="313">Among the TE errors not arising from NE processing errors, note in particular those that occurred on the most difficult slots , ORG DESCRIPTOR, ORG_LOCALE, and ORG_COUNTRY.</S>
  <S sid="314" ssid="314">These are all due in this case to missing locational an d 15 2 Nature of the problem Problem cases Resulting errors Naive string matching "McCann " vs. "McCann-Erickson " 9 inc type "John Dooner" vs. "John J .</S>
  <S sid="315" ssid="315">Dooner Jr." 1 inc text, 1 spu type/text Missing phraser patterns t "Fallon McElligott" ?</S>
  <S sid="316" ssid="316">treated as person 1 inc type "Tasters Choice" ?</S>
  <S sid="317" ssid="317">naive ` s prorpssing 1 spu type/text Poor phraser patterns t "Coca-Cola Classic" ?</S>
  <S sid="318" ssid="318">zealous org rule 1 spu type/text Missing date patterns t " the 21st century" 1 mis type/text Ambiguous name "New York Times" ?</S>
  <S sid="319" ssid="319">not an org 1 spu type/text Misc.</S>
  <S sid="320" ssid="320">embarrassing bugs "James" in &lt;HL&gt; ?</S>
  <S sid="321" ssid="321">treated as location 1 inc. type "J .</S>
  <S sid="322" ssid="322">Walter Thompson" ?</S>
  <S sid="323" ssid="323">punctoker lost "J ."</S>
  <S sid="324" ssid="324">1 inc type, 1 inc text Table 3 : NE errors on walkthrough message Nature of the problem Problem cases Resulting errors Repercussions of NE errors "Walter Thompson " , "Fallon McElligott", 3 spu pers .</S>
  <S sid="325" ssid="325">alias "McCann" all treated as person "John Dooner" treated as two persons 2 spu pets, 1 mis pers.</S>
  <S sid="326" ssid="326">alias "Coca-Cola Classic" treated as organization 1 inc org .</S>
  <S sid="327" ssid="327">namett , 1 inc. org alias Missing org.</S>
  <S sid="328" ssid="328">NP patternst " the agency with billings of $400 million " 2 mis org .</S>
  <S sid="329" ssid="329">descriptor "one of the largest world-wide agencies " Missing location patterns t "Cokes headquarters in Atlanta" 1 mis org .</S>
  <S sid="330" ssid="330">locale/country Org.</S>
  <S sid="331" ssid="331">type determination t "Creative Artists Agency" ?</S>
  <S sid="332" ssid="332">treated as gov.</S>
  <S sid="333" ssid="333">org type Acronym resolution snafu "CAA" vs. "Creative Artists Agency" 1 inc org .</S>
  <S sid="334" ssid="334">namett , 1 mis org .</S>
  <S sid="335" ssid="335">alias Enthusiastic scorer mapping "New York Times " (spurious entity) mapped to 1 inc .</S>
  <S sid="336" ssid="336">namett "Fallon McElligott" (inc .</S>
  <S sid="337" ssid="337">entity type ) Table 4 : TE errors on walkthrough message organizational NP phraser rules, which is consistent with trends we noted during training.</S>
  <S sid="338" ssid="338">These observation s are summarized in Table 4.</S>
  <S sid="339" ssid="339">Once again, single daggers (t) mark errors attributable to knowledge gaps .</S>
  <S sid="340" ssid="340">Note also that because of lenient template mappings on the part of the scorer, a number of errors that might intuitively have been whole organization template errors turned out only to manifest themselves a s organization name errors .</S>
  <S sid="341" ssid="341">These cases are marked with double daggers (tt) .</S>
  <S sid="342" ssid="342">Other trend s In addition to this analysis of the single walkthrough message, we opened up some to% of the test data t o inspection, and performed a rough trend estimation .</S>
  <S sid="343" ssid="343">In particular, we wanted to explain the slot-by-slot discrepancies we had noted between our training and test performance (cf.</S>
  <S sid="344" ssid="344">We found a com- bination of knowledge gaps, known design problems that had been left unaddressed by the time of the evaluation run, and some truly embarrassing bugs .</S>
  <S sid="345" ssid="345">To dispense quickly with the latter, we admit to failing to filter lines beginning with "0" in the body of the message .</S>
  <S sid="346" ssid="346">This was due to the fact that earlier training data had these lines marked with &lt;5&gt; tags, whereas the official test data did not.</S>
  <S sid="347" ssid="347">These 0-lines were so rare in the formal training data that we had simply no t noticed our omission.</S>
  <S sid="348" ssid="348">This primarily affected our NUMEX and TIMEX precision in the named entity task .</S>
  <S sid="349" ssid="349">153 In the template element task, our largest performance drop was on the ORG_DESRIPTOR SIOt, where we los t ii points of recall and 13 points of precision.</S>
  <S sid="350" ssid="350">This can be largely attributed to knowledge gaps in our phrase r rules for organizational noun phrases .</S>
  <S sid="351" ssid="351">In particular, we were missing a large number of head nouns that would have been required to identify relevant descriptor NPS .</S>
  <S sid="352" ssid="352">On the PERSON_NAME and PERSON_ALIAS Slots, we respectively found a 7 point drop in precision and an 8 point drop in recall .</S>
  <S sid="353" ssid="353">These were due to the same problem, a known flaw that had been left unaddressed i n the days leading to the evaluation .</S>
  <S sid="354" ssid="354">In particular, we had failed to merge short name forms that appeared i n headlines with the longer forms that appeared in the body of the message .</S>
  <S sid="355" ssid="355">For example, "James" in the walkthrough headline field should have been merged with "Robert L. James" in the body of the message .</S>
  <S sid="356" ssid="356">Because these short forms went unmerged, they in turn spawned incorrect person templates, hence the dro p in PERSON and PERSON_NAME precision.</S>
  <S sid="357" ssid="357">For the same reason, the templates that were generated for the long forms of these names ended up without their alias slot filled, accounting for the drop in PERSON_ALIAS recall .</S>
  <S sid="358" ssid="358">A similar problem held true for the ORG ALIAS slot.</S>
  <S sid="359" ssid="359">In this case, we failed both to extract organizatio n templates from the headline fields, or merge short name forms from headlines with longer forms in the tex t bodies.</S>
  <S sid="360" ssid="360">We were aware of these "mis-features" in our handling of person and organization name templates , but had left these problems unaddressed since they seemed to have only minimal impact on the forma l training data.</S>
  <S sid="361" ssid="361">Errare humanum est.</S>
  <S sid="362" ssid="362">POST-HOC EXPERIMENTS With this error analysis behind us, we pursued a number of post-hoc experiments .</S>
  <S sid="363" ssid="363">Most interesting among them was a simple attempt at improving recall on organization names.</S>
  <S sid="364" ssid="364">Indeed, Alembic has only a short list of known organizations?less than a half-dozen in total .</S>
  <S sid="365" ssid="365">Virtually all of the organizations found by Alembic are recognized from first principles .</S>
  <S sid="366" ssid="366">We decided to compare this strategy with one that uses a large lexicon of organization names .</S>
  <S sid="367" ssid="367">All of the Muc-6 NE training set was used to generate a list of i,8o8 distinct organization name strings .</S>
  <S sid="368" ssid="368">This could certainly be larger, but seemed a reasonable size .</S>
  <S sid="369" ssid="369">Nonetheless, this lexicon by itself got less than half of the organizations in the official named-entity test corpus : organization recall was 45 and precision 91 .</S>
  <S sid="370" ssid="370">Another interesting question is how much an organization lexicon might have helped had it been added to our rule-based phrasing algorithm, not simply used by itself.</S>
  <S sid="371" ssid="371">This configuration actually decreased ou r performance slightly (F-score down by 0 .5 points of P&amp;R), trading a slight increase in organization recall for a larger decrease in precision .</S>
  <S sid="372" ssid="372">The biggest problem here is due to overgeneration (up from 4% to 6%), and partial matches such as the following, Kraft &lt;ENAMEX&gt;General Foods&lt;/ENAMEX &gt; First &lt; E NA M EX&gt; Fidel ity&lt; /ENAMEX &gt; where "General Foods" and "Fidelity" were in the training corpus for the organization lexicon, but the longer names above were not .</S>
  <S sid="373" ssid="373">Admittedly, the way we integrated the organization lexicon into Alembic was relatively naYve, thereb y leading to some of these silly precision errors .</S>
  <S sid="374" ssid="374">We believe that if we more intelligently took advantage of thi s knowledge source, we could reduce the additional precision errors almost entirely .</S>
  <S sid="375" ssid="375">In addition, we were disappointed by the fact that our exhaustive compilation only produced somewhat less than 2,000 organi- zation names, and only led to a piffling improvement in recall .</S>
  <S sid="376" ssid="376">Perhaps had we made use of larger name lists , we might have obtained better recall improvements?a case in point is the gargantuan Dunn &amp; Bradstree t listing exploited by Knight-Ridder for their named entities tagger [ 41 .</S>
  <S sid="377" ssid="377">Note, however, that all but a few of the organizations that were found in both the training name list and the test data were found by Alembic from first principles anyway .</S>
  <S sid="378" ssid="378">We may thus tentatively conclude that short of being as encyclopedic as the D&amp;B listing, a larger, better-integrated organization lexicon may have provided no more than a limite d improvement in F-score .</S>
  <S sid="379" ssid="379">To further improve our organization tagging, it appears that we will simply have t o expend more energy writing named entity phraser rules .</S>
  <S sid="380" ssid="380">154 CONCLUDING THOUGHT S All in all, we are quite pleased with the performance of Alembic in Muc-6.</S>
  <S sid="381" ssid="381">While we regret no t participating in the ST task, we do believe that the framework was up to it, especially in light of our TE scores .</S>
  <S sid="382" ssid="382">There were many lessons learned, and there will continue to be, as we further analyze our results and mak e improvements to the system .</S>
  <S sid="383" ssid="383">Several points stand out .</S>
  <S sid="384" ssid="384">We had hoped to avoid full NP parsing, but the definition of the ORG DESCRIPTOR slot clearly requires this, and we will need to return to larger-scale parsing strategies in the future.</S>
  <S sid="385" ssid="385">We had hoped to include more machine-learned phraser rules, and as the rule learner matures, we almost certainly will .</S>
  <S sid="386" ssid="386">One thing is clear to us, however, and that is that rule sequences are an extremely powerful tool .</S>
  <S sid="387" ssid="387">They were easy to hand-craft and adapt to the MUC-6 requirements.</S>
  <S sid="388" ssid="388">They run fast .</S>
  <S sid="389" ssid="389">And they work well .</S>
  <S sid="390" ssid="390">References [1] Alshawi, H .</S>
  <S sid="391" ssid="391">&amp; Van Eijck, J .</S>
  <S sid="392" ssid="392">"Logical forms in the core language engine" .</S>
  <S sid="393" ssid="393">In Proceedings of the 27th Meeting ofthe Assoc.</S>
  <S sid="394" ssid="394">iationfor Computational Linguistics (ACL-89) .</S>
  <S sid="395" ssid="395">Vancouver, E.c.,1985.</S>
  <S sid="396" ssid="396">[2] Appelt, D ., Hobbs, J ., Bear, J ., Israel, D., &amp; Tyson, M .</S>
  <S sid="397" ssid="397">"FASTUS: A finite-state processor for infor - mation extraction from real-world text".</S>
  <S sid="398" ssid="398">In Proceedings of the 13th Intl.</S>
  <S sid="399" ssid="399">Joint Conference on Artificial Intelligence (IJCAR93) .</S>
  <S sid="400" ssid="400">Chambery,1993 .</S>
  <S sid="401" ssid="401">&amp; Vilain, M. "The relation-based knowledge representation of King Kong" .</S>
  <S sid="402" ssid="402">SIGART Bulletin, 2(3),15-21.</S>
  <S sid="403" ssid="403">[4] Borkovsky, A.</S>
  <S sid="404" ssid="404">"Knight-Ridders value adding name finder : a variation on the theme of FASTUS" .</S>
  <S sid="405" ssid="405">In Proceedings ofthe 6th Message Understanding Conference (Muc-6) .</S>
  <S sid="406" ssid="406">"Some advances in rule-based part of speech tagging" .</S>
  <S sid="407" ssid="407">In Proceedings of the 12th Nationa l Conference on Artificial Intelligence (AAAI-94) .</S>
  <S sid="408" ssid="408">Seattle, 1 994 .</S>
  <S sid="409" ssid="409">[6] Brill, E. A corpus-based approach to language learning.</S>
  <S sid="410" ssid="410">Doctoral Dissertation, Univ.</S>
  <S sid="411" ssid="411">[7] Brill, E. "A simple rule-based part of speech tagger" .</S>
  <S sid="412" ssid="412">In Proceedings of the 3rd Conference on Applied Natural Language Processing (Applied ACL 92) .</S>
  <S sid="413" ssid="413">"Ontological promiscuity" .</S>
  <S sid="414" ssid="414">In Proceedings of the 23rd Meeting of the Association fo r Computational Linguistics (ACL-85) .</S>
  <S sid="415" ssid="415">Chicago, Ill., 1985.</S>
  <S sid="416" ssid="416">[9] Landman, F .</S>
  <S sid="417" ssid="417">Linguistics and Philosophy, 12(3), 359-605 and 12(4), 723-744.</S>
  <S sid="418" ssid="418">[to] Lehnert, W., McCarthy, J ., Soderland, S ., Riloff, E., Cardie, C., Peterson, J ., Feng, F ., Dolan, C., &amp; Goldman, S .</S>
  <S sid="419" ssid="419">"University of Massachusetts/Hughes: Description of the CIRCUS system as used for Muc - 5" .</S>
  <S sid="420" ssid="420">In Proceedings of the 5th Message Understanding Conference (MUC-5) .</S>
  <S sid="421" ssid="421">Baltimore, Md ., 1993 .</S>
  <S sid="422" ssid="422">[11] Vilain, M .</S>
  <S sid="423" ssid="423">Semantic inference in natural language: validating a tractable approach .</S>
  <S sid="424" ssid="424">In Proceedings of the 24th Intl.</S>
  <S sid="425" ssid="425">Joint Conference on Artificial Intelligence (IJCA1 9 5) .</S>
  <S sid="426" ssid="426">Montreal, 1995 .</S>
</PAPER>
