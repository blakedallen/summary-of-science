<PAPER>
  <S sid="0">Chinese Whispers - An Efficient Graph Clustering Algorithm And Its Application To Natural Language Processing Problems</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges.</S>
    <S sid="2" ssid="2">After a detailed definition of the algorithm and a discussion of its strengths and weaknesses, the performance of Chinese Whispers is measured on Natural Language Processing (NLP) problems as diverse as language separation, acquisition of syntactic word classes and word sense disambiguation.</S>
    <S sid="3" ssid="3">At this, the fact is employed that the small-world property holds for many graphs in NLP.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">Clustering is the process of grouping together objects based on their similarity to each other.</S>
    <S sid="5" ssid="2">In the field of Natural Language Processing (NLP), there are a variety of applications for clustering.</S>
    <S sid="6" ssid="3">The most popular ones are document clustering in applications related to retrieval and word clustering for finding sets of similar words or concept hierarchies.</S>
    <S sid="7" ssid="4">Traditionally, language objects are characterized by a feature vector.</S>
    <S sid="8" ssid="5">These feature vectors can be interpreted as points in a multidimensional space.</S>
    <S sid="9" ssid="6">The clustering uses a distance metric, e.g. the cosine of the angle between two such vectors.</S>
    <S sid="10" ssid="7">As in NLP there are often several thousand features, of which only a few correlate with each other at a time &#8211; think about the number of different words as opposed to the number of words occurring in a sentence &#8211; dimensionality reduction techniques can greatly reduce complexity without considerably losing accuracy.</S>
    <S sid="11" ssid="8">An alternative representation that does not deal with dimensions in space is the graph representation.</S>
    <S sid="12" ssid="9">A graph represents objects (as nodes) and their relations (as edges).</S>
    <S sid="13" ssid="10">In NLP, there are a variety of structures that can be naturally represented as graphs, e.g. lexical-semantic word nets, dependency trees, co-occurrence graphs and hyperlinked documents, just to name a few.</S>
    <S sid="14" ssid="11">Clustering graphs is a somewhat different task than clustering objects in a multidimensional space: There is no distance metric; the similarity between objects is encoded in the edges.</S>
    <S sid="15" ssid="12">Objects that do not share an edge cannot be compared, which gives rise to optimization techniques.</S>
    <S sid="16" ssid="13">There is no centroid or &#8216;average cluster member&#8217; in a graph, permitting centroid-based techniques.</S>
    <S sid="17" ssid="14">As data sets in NLP are usually large, there is a strong need for efficient methods, i.e. of low computational complexities.</S>
    <S sid="18" ssid="15">In this paper, a very efficient graph-clustering algorithm is introduced that is capable of partitioning very large graphs in comparatively short time.</S>
    <S sid="19" ssid="16">Especially for smallworld graphs (Watts, 1999), high performance is reached in quality and speed.</S>
    <S sid="20" ssid="17">After explaining the algorithm in the next section, experiments with synthetic graphs are reported in section 3.</S>
    <S sid="21" ssid="18">These give an insight about the algorithm&#8217;s performance.</S>
    <S sid="22" ssid="19">In section 4, experiments on three NLP tasks are reported, section 5 concludes by discussing extensions and further application areas.</S>
  </SECTION>
  <SECTION title="2 Chinese Whispers Algorithm" number="2">
    <S sid="23" ssid="1">In this section, the Chinese Whispers (CW) algorithm is outlined.</S>
    <S sid="24" ssid="2">After recalling important concepts from Graph Theory (cf.</S>
    <S sid="25" ssid="3">Bollob&#225;s 1998), we describe two views on the algorithm.</S>
    <S sid="26" ssid="4">The Workshop on TextGraphs, at HLT-NAACL 2006, pages 73&#8211;80, New York City, June 2006. c&#65533;2006 Association for Computational Linguistics second view is used to relate CW to another graph clustering algorithm, namely MCL (van Dongen, 2000).</S>
    <S sid="27" ssid="5">We use the following notation throughout this paper: Let G=(V,E) be a weighted graph with nodes (vi)EV and weighted edges (vi, vj, wij) EE with weight wij.</S>
    <S sid="28" ssid="6">If (vi, vj, wij)EE implies (vj, vi, wij)EE, then the graph is undirected.</S>
    <S sid="29" ssid="7">If all weights are 1, G is called unweighted.</S>
    <S sid="30" ssid="8">The degree of a node is the number of edges a node takes part in.</S>
    <S sid="31" ssid="9">The neighborhood of a node v is defined by the set of all nodes v&#8217; such that (v,v&#8217;,w)EE or (v&#8217;,v,w)EE; it consists of all nodes that are connected to v. The adjacency matrix AG of a graph G with n nodes is an nxn matrix where the entry aij denotes the weight of the edge between vi and vj , 0 otherwise.</S>
    <S sid="32" ssid="10">The class matrix DG of a Graph G with n nodes is an nxn matrix where rows represent nodes and columns represent classes (ci)EC.</S>
    <S sid="33" ssid="11">The value dij at row i and column j represents the amount of vi as belonging to a class cj.</S>
    <S sid="34" ssid="12">For convention, class matrices are row-normalized; the i-th row denotes a distribution of vi over C. If all rows have exactly one non-zero entry with value 1, DG denotes a hard partitioning of V, soft partitioning otherwise.</S>
    <S sid="35" ssid="13">CW is a very basic &#8211; yet effective &#8211; algorithm to partition the nodes of weighted, undirected graphs.</S>
    <S sid="36" ssid="14">It is motivated by the eponymous children&#8217;s game, where children whisper words to each other.</S>
    <S sid="37" ssid="15">While the game&#8217;s goal is to arrive at some funny derivative of the original message by passing it through several noisy channels, the CW algorithm aims at finding groups of nodes that broadcast the same message to their neighbors.</S>
    <S sid="38" ssid="16">It can be viewed as a simulation of an agent-based social network; for an overview of this field, see (Amblard 2002).</S>
    <S sid="39" ssid="17">The algorithm is outlined in figure 1: initialize: forall vi in V: class(vi)=i; while changes: forall v in V, randomized order: class(v)=highest ranked class in neighborhood of v; Intuitively, the algorithm works as follows in a bottom-up fashion: First, all nodes get different classes.</S>
    <S sid="40" ssid="18">Then the nodes are processed for a small number of iterations and inherit the strongest class in the local neighborhood.</S>
    <S sid="41" ssid="19">This is the class whose sum of edge weights to the current node is maximal.</S>
    <S sid="42" ssid="20">In case of multiple strongest classes, one is chosen randomly.</S>
    <S sid="43" ssid="21">Regions of the same class stabilize during the iteration and grow until they reach the border of a stable region of another class.</S>
    <S sid="44" ssid="22">Note that classes are updated immediately: a node can obtain classes from the neighborhood that were introduced there in the same iteration.</S>
    <S sid="45" ssid="23">Figure 2 illustrates how a small unweighted graph is clustered into two regions in three iterations.</S>
    <S sid="46" ssid="24">Different classes are symbolized by different shades of grey.</S>
    <S sid="47" ssid="25">It is possible to introduce a random mutation rate that assigns new classes with a probability decreasing in the number of iterations as described in (Biemann &amp; Teresniak 2005).</S>
    <S sid="48" ssid="26">This showed having positive effects for small graphs because of slower convergence in early iterations.</S>
    <S sid="49" ssid="27">The CW algorithm cannot cross component boundaries, because there are no edges between nodes belonging to different components.</S>
    <S sid="50" ssid="28">Further, nodes that are not connected by any edge are discarded from the clustering process, which possibly leaves a portion of nodes unclustered.</S>
    <S sid="51" ssid="29">Formally, CW does not converge, as figure 3 exemplifies: here, the middle node&#8217;s neighborhood consists of a tie which can be decided in assigning the class of the left or the class of the right nodes in any iteration all over again.</S>
    <S sid="52" ssid="30">Ties, however, do not play a major role in weighted graphs.</S>
    <S sid="53" ssid="31">Apart from ties, the classes usually do not change any more after a handful of iterations.</S>
    <S sid="54" ssid="32">The number of iterations depends on the diameter of the graph: the larger the distance between two nodes is, the more iterations it takes to percolate information from one to another.</S>
    <S sid="55" ssid="33">The result of CW is a hard partitioning of the given graph into a number of partitions that emerges in the process &#8211; CW is parameter-free.</S>
    <S sid="56" ssid="34">It is possible to obtain a soft partitioning by assigning a class distribution to each node, based on the weighted distribution of (hard) classes in its neighborhood in a final step.</S>
    <S sid="57" ssid="35">The outcomes of CW resemble those of MinCut (Wu &amp; Leahy 1993): Dense regions in the graph are grouped into one cluster while sparsely connected regions are separated.</S>
    <S sid="58" ssid="36">In contrast to Min-Cut, CW does not find an optimal hierarchical clustering but yields a non-hierarchical (flat) partition.</S>
    <S sid="59" ssid="37">Furthermore, it does not require any threshold as input parameter and is more efficient.</S>
    <S sid="60" ssid="38">Another algorithm that uses only local contexts for time-linear clustering is DBSCAN as, described in (Ester et al. 1996), needing two input parameters (although the authors propose an interactive approach to determine them).</S>
    <S sid="61" ssid="39">DBSCAN is especially suited for graphs with a geometrical interpretation, i.e. the objects have coordinates in a multidimensional space.</S>
    <S sid="62" ssid="40">A quite similar algorithm to CW is MAJORCLUST (Stein &amp; Niggemann 1996), which is based on a comparable idea but converges slower.</S>
    <S sid="63" ssid="41">As CW is a special case of Markov-ChainClustering (MCL) (van Dongen, 2000), we spend a few words on explaining it.</S>
    <S sid="64" ssid="42">MCL is the parallel simulation of all possible random walks up to a finite length on a graph G. The idea is that random walkers are more likely to end up in the same cluster where they started than walking across clusters.</S>
    <S sid="65" ssid="43">MCL simulates flow on a graph by repeatedly updating transition probabilities between all nodes, eventually converging to a transition matrix after k steps that can be interpreted as a clustering of G. This is achieved by alternating an expansion step and an inflation step.</S>
    <S sid="66" ssid="44">The expansion step is a matrix multiplication of MG with the current transition matrix.</S>
    <S sid="67" ssid="45">The inflation step is a column-wise non-linear operator that increases the contrast between small and large transition probabilities and normalizes the columnwise sums to 1.</S>
    <S sid="68" ssid="46">The k matrix multiplications of the expansion step of MCL lead to its time-complexity of O(k&#8226;n2).</S>
    <S sid="69" ssid="47">It has been observed in (van Dongen, 2000), that only the first couple of iterations operate on dense matrices &#8211; when using a strong inflation operator, matrices in the later steps tend to be sparse.</S>
    <S sid="70" ssid="48">The author further discusses pruning schemes that keep only some of the largest entries per column, leading to drastic optimization possibilities.</S>
    <S sid="71" ssid="49">But the most aggressive sort of pruning is not considered: only keeping one single largest entry.</S>
    <S sid="72" ssid="50">Exactly this is conducted in the basic CW process.</S>
    <S sid="73" ssid="51">Let maxrow(.) be an operator that operates row-wise on a matrix and sets all entries of a row to zero except the largest entry, which is set to 1.</S>
    <S sid="74" ssid="52">Then the algorithm is denoted as simple as this: By applying maxrow(.</S>
    <S sid="75" ssid="53">), Dt-1 has exactly n non-zero entries.</S>
    <S sid="76" ssid="54">This causes the time-complexity to be dependent on the number of edges, namely O(k&#8226;|E|).</S>
    <S sid="77" ssid="55">In the worst case of a fully connected graph, this equals the time-complexity of MCL.</S>
    <S sid="78" ssid="56">A problem with the matrix CW process is that it does not necessarily converge to an iterationinvariant class matrix D, but rather to a pair of oscillating class matrices.</S>
    <S sid="79" ssid="57">Figure 5 shows an example.</S>
    <S sid="80" ssid="58">This is caused by the stepwise update of the class matrix.</S>
    <S sid="81" ssid="59">As opposed to this, the CW algorithm as outlined in figure 1 continuously updates D after the processing of each node.</S>
    <S sid="82" ssid="60">To avoid these oscillations, one of the following measures can be taken: While converging to the same limits, the continuous update strategy converges the fastest because prominent classes are spread much faster in early iterations.</S>
  </SECTION>
  <SECTION title="3 Experiments with synthetic graphs" number="3">
    <S sid="83" ssid="1">The analysis of the CW process is difficult due to its nonlinear nature.</S>
    <S sid="84" ssid="2">Its run-time complexity indicates that it cannot directly optimize most global graph cluster measures because of their NPcompleteness (&#352;&#237;ma and Schaeffer, 2005).</S>
    <S sid="85" ssid="3">Therefore we perform experiments on synthetic graphs to empirically arrive at an impression of our algorithm's abilities.</S>
    <S sid="86" ssid="4">All experiments were conducted with an implementation following figure 1.</S>
    <S sid="87" ssid="5">For experiments with synthetic graphs, we restrict ourselves to unweighted graphs, if not stated explicitly.</S>
    <S sid="88" ssid="6">A cluster algorithm should keep dense regions together while cutting apart regions that are sparsely connected.</S>
    <S sid="89" ssid="7">The highest density is reached in fully connected sub-graphs of n nodes, a.k.a. ncliques.</S>
    <S sid="90" ssid="8">We define an n-bipartite-clique as a graph of two n-cliques, which are connected such that each node has exactly one edge going to the clique it, does not belong to.</S>
    <S sid="91" ssid="9">Figures 5 and 6 are n-partite cliques for n=4,10.</S>
    <S sid="92" ssid="10">We clearly expect a clustering algorithm to cut the two cliques apart.</S>
    <S sid="93" ssid="11">As we operate on unweighted graphs, however, CW is left with two choices: producing two clusters or grouping all nodes into one cluster.</S>
    <S sid="94" ssid="12">This is largely dependent on the random choices in very early iterations - if the same class is assigned to several nodes in both cliques, it will finally cover the whole graph.</S>
    <S sid="95" ssid="13">It is clearly a drawback that the outcome of CW is non-deterministic.</S>
    <S sid="96" ssid="14">Only half of the experiments with 4-bipartite cliques resulted in separation.</S>
    <S sid="97" ssid="15">However, the problem is most dramatic on small graphs and ceases to exist for larger graphs as demonstrated in figure 7.</S>
    <S sid="98" ssid="16">A structure that has been reported to occur in an enormous number of natural systems is the small world (SW) graph.</S>
    <S sid="99" ssid="17">Space prohibits an in-depth discussion, which can be found in (Watts 1999).</S>
    <S sid="100" ssid="18">Here, we restrict ourselves to SW-graphs in language data.</S>
    <S sid="101" ssid="19">In (Ferrer-i-Cancho and Sole, 2001), co-occurrence graphs as used in the experiment section are reported to possess the small world property, i.e. a high clustering coefficient and short average path length between arbitrary nodes.</S>
    <S sid="102" ssid="20">Steyvers and Tenenbaum (2005) show that association networks as well as semantic resources are scale-free SW-graphs: their degree distribution follows a power law.</S>
    <S sid="103" ssid="21">A generative model is provided that generates undirected, scalefree SW-graphs in the following way: We start with a small number of fully connected nodes.</S>
    <S sid="104" ssid="22">When adding a new node, an existing node v is chosen with a probability according to its degree.</S>
    <S sid="105" ssid="23">The new node is connected to M nodes in the neighborhood of v. The generative model is parameterized by the number of nodes n and the network's mean connectivity, which approaches 2M for large n. Let us assume that we deal with natural systems that can be characterized by small world graphs.</S>
    <S sid="106" ssid="24">If two or more of those systems interfere, their graphs are joined by merging some nodes, retaining their edges.</S>
    <S sid="107" ssid="25">A graph-clustering algorithm should split up the resulting graph in its previous parts, at least if not too many nodes were merged.</S>
    <S sid="108" ssid="26">We conducted experiments to measure CW's performance on SW-graph mixtures: We generated graphs of various sizes, merged them by twos to a various extent and measured the amount of cases where clustering with CW leads to the reconstruction of the original parts.</S>
    <S sid="109" ssid="27">When generating SW-graphs with the SteyversTenenbaum model, we fixed M to 10 and varied n and the merge rate r, which is the fraction of nodes of the smaller graph that is merged with nodes of the larger graph.</S>
    <S sid="110" ssid="28">Figure 8 summarizes the results for equisized mixtures of 300, 3,000 and 30,000 nodes and mixtures of 300 with 30,000 nodes.</S>
    <S sid="111" ssid="29">It is not surprising that separating the two parts is more difficult for higher r. Results are not very sensitive to size and size ratio, indicating that CW is able to identify clusters even if they differ considerably in size &#8211; it even performs best at the skewed mixtures.</S>
    <S sid="112" ssid="30">At merge rates between 20% and 30%, still more then half of the mixtures are separated correctly and can be found when averaging CW&#8217;s outcome over several runs.</S>
    <S sid="113" ssid="31">As formally, the algorithm does not converge, it is important to define a stop criterion or to set the number of iterations.</S>
    <S sid="114" ssid="32">To show that only a few iterations are needed until almost-convergence, we measured the normalized Mutual Information (MI)1 between the clustering in the 50th iteration and the clusterings of earlier iterations.</S>
    <S sid="115" ssid="33">This was conducted for two unweighted SW-graphs with 1,000 (1K) and 10,000 (10K) nodes, M=5 and a weighted 7-lingual co-occurrence graph (cf. section 4.1) with 22,805 nodes and 232,875 edges.</S>
    <S sid="116" ssid="34">Table 1 indicates that for unweighted graphs, changes are only small after 20-30 iterations.</S>
    <S sid="117" ssid="35">In iterations 40-50, the normalized MI-values do not improve any more.</S>
    <S sid="118" ssid="36">The weighted graph converges much faster due to fewer ties and reaches a stable plateau after only 6 iterations.</S>
  </SECTION>
  <SECTION title="4 NLP Experiments" number="4">
    <S sid="119" ssid="1">In this section, some experiments with graphs originating from natural language data are presented.</S>
    <S sid="120" ssid="2">First, we define the notion of cooccurrence graphs, which are used in sections 4.1 and 4.3: Two words co-occur if they can both be found in a certain unit of text, here a sentence.</S>
    <S sid="121" ssid="3">Employing a significance measure, we determine whether their co-occurrences are significant or random.</S>
    <S sid="122" ssid="4">In this case, we use the log-likelihood measure as described in (Dunning 1993).</S>
    <S sid="123" ssid="5">We use the words as nodes in the graph.</S>
    <S sid="124" ssid="6">The weight of an 1 defined for two random variables X and Y as (H(X)+H(Y)H(X,Y))/max(H(X),H(Y)) with H(X) entropy.</S>
    <S sid="125" ssid="7">A value of 0 denotes indepenence, 1 is perfect congruence. edge between two words is set to the significance value of their co-occurrence, if it exceeds a certain threshold.</S>
    <S sid="126" ssid="8">In the experiments, we used significances from 15 on.</S>
    <S sid="127" ssid="9">The entirety of words that are involved in at least one edge together with these edges is called co-occurrence graph (cf.</S>
    <S sid="128" ssid="10">Biemann et al. 2004).</S>
    <S sid="129" ssid="11">In general, CW produces a large number of clusters on real-world graphs, of which the majority is very small.</S>
    <S sid="130" ssid="12">For most applications, it might be advisable to define a minimum cluster size or something alike.</S>
    <S sid="131" ssid="13">This section shortly reviews the results of (Biemann and Teresniak, 2005), where CW was first described.</S>
    <S sid="132" ssid="14">The task was to separate a multilingual corpus by languages, assuming its tokenization in sentences.</S>
    <S sid="133" ssid="15">The co-occurrence graph of a multilingual corpus resembles the synthetic SW-graphs: Every language forms a separate co-occurrence graph, some words that are used in more than one language are members of several graphs, connecting them.</S>
    <S sid="134" ssid="16">By CW-partitioning, the graph is split into its monolingual parts.</S>
    <S sid="135" ssid="17">These parts are used as word lists for word-based language identification.</S>
    <S sid="136" ssid="18">(Biemann and Teresniak, 2005) report almost perfect performance on getting 7lingual corpora with equisized parts sorted apart as well as highly skewed mixtures of two languages.</S>
    <S sid="137" ssid="19">In the process, language-ambiguous words are assigned to only one language, which did not hurt performance due to the high redundancy of the task.</S>
    <S sid="138" ssid="20">However, it would have been possible to use the soft partitioning to acquire a distribution over languages for each word.</S>
    <S sid="139" ssid="21">For the acquisition of word classes, we use a different graph: the second-order graph on neighboring co-occurrences.</S>
    <S sid="140" ssid="22">To set up the graph, a co-occurrence calculation is performed which yields significant word pairs based on their occurrence as immediate neighbors.</S>
    <S sid="141" ssid="23">This can be perceived as a bipartite graph, figure 9a gives a toy example.</S>
    <S sid="142" ssid="24">Note that if similar words occur in both parts, they form two distinct nodes.</S>
    <S sid="143" ssid="25">This graph is transformed into a second-order graph by comparing the number of common right and left neighbors for two words.</S>
    <S sid="144" ssid="26">The similarity (edge weight) between two words is the sum of common neighbors.</S>
    <S sid="145" ssid="27">Figure 9b depicts the secondorder graph derived from figure 9a and its partitioning by CW.</S>
    <S sid="146" ssid="28">The word-class-ambiguous word &#8220;drink&#8221; (to drink the drink) is responsible for all intra-cluster edges.</S>
    <S sid="147" ssid="29">The hypothesis here is that words sharing many neighbors should usually be observed with the same part-of-speech and get high weights in the second order graph.</S>
    <S sid="148" ssid="30">In figure 9, three clusters are obtained that correspond to different parts-of-speech (POS).</S>
    <S sid="149" ssid="31">To test this on a large scale, we computed the second-order similarity graph for the British National Corpus (BNC), excluding the most frequent 2000 words and drawing edges between words if they shared at least four left and right neighbors.</S>
    <S sid="150" ssid="32">The clusters are checked against a lexicon that contains the most frequent tag for each word in the BNC.</S>
    <S sid="151" ssid="33">The largest clusters are presented in table 2 .</S>
    <S sid="152" ssid="34">In total, CW produced 282 clusters, of which 26 exceed a size of 100.</S>
    <S sid="153" ssid="35">The weighted average of cluster purity (i.e. the number of predominant tags divided by cluster size) was measured at 88.8%, which exceeds significantly the precision of 53% on word type as reported by Sch&#252;tze (1995) on a related task.</S>
    <S sid="154" ssid="36">How to use this kind of word clusters to improve the accuracy of POS-taggers is outlined in (Ushioda, 1996).</S>
    <S sid="155" ssid="37">The task of word sense induction (WSI) is to find the different senses of a word.</S>
    <S sid="156" ssid="38">The number of senses is not known in advance, therefore has to be determined by the method.</S>
    <S sid="157" ssid="39">Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph.</S>
    <S sid="158" ssid="40">While there, edges between words are drawn iff words co-occur in enumerations, we use the cooccurrence graph.</S>
    <S sid="159" ssid="41">Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.</S>
    <S sid="160" ssid="42">We replace MCL by CW.</S>
    <S sid="161" ssid="43">The clusters are interpreted as representations of word senses.</S>
    <S sid="162" ssid="44">To judge results, the methodology of (Bordag, 2006) is adopted: To evaluate word sense induction, two sub-graphs induced by the neighborhood of different words are merged.</S>
    <S sid="163" ssid="45">The algorithm's ability to separate the merged graph into its previous parts can be measured in an unsupervised way.</S>
    <S sid="164" ssid="46">Bordag defines four measures: We used the same program to compute cooccurrences on the same corpus (the BNC).</S>
    <S sid="165" ssid="47">Therefore it is possible to directly compare our results to Bordag&#8217;s, who uses a triplet-based hierarchical graph clustering approach.</S>
    <S sid="166" ssid="48">The method was chosen because of its appropriateness for unlabelled data: without linguistic preprocessing like tagging or parsing, only the disambiguation mechanism is measured and not the quality of the preprocessing steps.</S>
    <S sid="167" ssid="49">We provide scores for his test 1 (word classes separately) and test 3 (words of different frequency bands).</S>
    <S sid="168" ssid="50">Data was obtained from BNC's raw text; evaluation was performed for 45 test words.</S>
    <S sid="169" ssid="51">Results (tables 3 and 4) suggest that both algorithms arrive at about equal overall performance (P and R).</S>
    <S sid="170" ssid="52">Chinese Whispers clustering is able to capture the same information as a specialized graph-clustering algorithm for WSI, given the same input.</S>
    <S sid="171" ssid="53">The slightly superior performance on rR and rP indicates that CW leaves fewer words unclustered, which can be advantageous when using the clusters as clues in word sense disambiguation.</S>
  </SECTION>
  <SECTION title="5 Conclusion" number="5">
    <S sid="172" ssid="1">Chinese Whispers, an efficient graph-clustering algorithm was presented and described in theory and practice.</S>
    <S sid="173" ssid="2">Experiments with synthetic graphs showed that for small graphs, results can be inconclusive due to its non-deterministic nature.</S>
    <S sid="174" ssid="3">But while there exist plethora of clustering approaches that can deal well with small graphs, the power of CW lies in its capability of handling very large graphs in reasonable time.</S>
    <S sid="175" ssid="4">The application field of CW rather lies in size regions, where other approaches&#8217; solutions are intractable.</S>
    <S sid="176" ssid="5">On the NLP data discussed, CW performs equally or better than other clustering algorithms.</S>
    <S sid="177" ssid="6">As CW &#8211; like other graph clustering algorithms &#8211; chooses the number of classes on its own and can handle clusters of different sizes, it is especially suited for NLP problems, where class distributions are often highly skewed and the number of classes (e.g. in WSI) is not known beforehand.</S>
    <S sid="178" ssid="7">To relate the partitions, it is possible to set up a hierarchical version of CW in the following way: The nodes of equal class are joined to hyper-nodes.</S>
    <S sid="179" ssid="8">Edge weights between hyper-nodes are set according to the number of inter-class edges between the corresponding nodes.</S>
    <S sid="180" ssid="9">This results in flat hierarchies.</S>
    <S sid="181" ssid="10">In further works it is planned to apply CW to other graphs, such as the co-citation graph of Citeseer, the co-citation graph of web pages and the link structure of Wikipedia.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="6">
    <S sid="182" ssid="1">Thanks go to Stefan Bordag for kindly providing his WSI evaluation framework.</S>
    <S sid="183" ssid="2">Further, the author would like to thank Sebastian Gottwald and Rocco Gwizdziel for a platform-independent GUI implementation of CW, which is available for download from the author&#8217;s homepage.</S>
  </SECTION>
</PAPER>
