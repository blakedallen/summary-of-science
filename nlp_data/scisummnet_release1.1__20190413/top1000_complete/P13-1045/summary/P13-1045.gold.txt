Parsing with Compositional Vector Grammars
Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness.
Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations.
The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%.
It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser.
The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.
Recursive neural networks, which have the ability to generate a tree structured output, have already been applied to natural language parsing, we extended them to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013).
