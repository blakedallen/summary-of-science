[
  {
    "citance_No": 1, 
    "citing_paper_id": "P13-1055", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Olivier, Ferret", 
    "raw_text": "and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category", 
    "clean_text": "The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pado and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P13-1055", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Olivier, Ferret", 
    "raw_text": "We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since werely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers", 
    "clean_text": "We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since werely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1133", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Jonathan, Berant | Percy, Liang", 
    "raw_text": "We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context", 
    "clean_text": "We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P14-1024", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Yulia, Tsvetkov | Leonid, Boytsov | Anatole, Gershman | Eric H., Nyberg | Chris, Dyer", 
    "raw_text": "In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing ,including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012)", 
    "clean_text": "In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P13-2087", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Igor, Labutov | Hod, Lipson", 
    "raw_text": "Source embeddings: We employ three external embeddings (obtained from (Turian et al, 2010)) induced using the following models: 1 )hierarchi cal log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models? 2) Collobert and Weston? s (C& amp; W) deep-learning architecture, and 3) Huanget.al? s polysemous neural language model (HUANG) (Huang et al, 2012)", 
    "clean_text": "Source embeddings: We employ three external embeddings (obtained from (Turian et al, 2010)) induced using the following models: 1) hierarchical log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models, 2) Collobert and Weston's (C&W) deep-learning architecture, and 3) Huang et al's polysemous neural language model (HUANG) (Huang et al, 2012).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P14-2131", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Mohit, Bansal | Kevin, Gimpel | Karen, Livescu", 
    "raw_text": "Several researchers have made their trained representations publicly avail 809 Representation Source Corpus Types, Tokens V D Time BROWN Koo et al (2008) BLLIP 317K, 43M 316,710? 2.5 days? SENNA Collobert et al (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months? TURIAN Turian et al (2010) RCV1 269K, 37M 268,810 50 few weeks? HUANG Huang et al (2012) Wikipedia 8.3M, 1.8B 100,232 50? CBOW, SKIP, SKIP DEP Mikolov et al (2013a) BLLIP 317K, 43M 316,697 100 24mins", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P14-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marco, Baroni | Georgiana, Dinu | Germ\u00c3\u00a1n, Kruszewski", 
    "raw_text": "Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper)", 
    "clean_text": "Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P14-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marco, Baroni | Georgiana, Dinu | Germ\u00c3\u00a1n, Kruszewski", 
    "raw_text": "The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1)", 
    "clean_text": "The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1).", 
    "keep_for_gold": 0
  }
]
