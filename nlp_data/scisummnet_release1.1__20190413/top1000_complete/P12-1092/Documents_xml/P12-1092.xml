<PAPER>
	<S sid="0">Improving Word Representations via Global Context and Multiple Word Prototypes</S><ABSTRACT>
		<S sid="1" ssid="1">Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems.</S>
		<S sid="2" ssid="2">However, most of these models arebuilt with only local context and one represen tation per word.</S>
		<S sid="3" ssid="3">This is problematic becausewords are often polysemous and global con text can also provide useful information for learning word meanings.</S>
		<S sid="4" ssid="4">We present a new neural network architecture which 1) learnsword embeddings that better capture the se mantics of words by incorporating both local and global document context, and 2) accountsfor homonymy and polysemy by learning mul tiple embeddings per word.</S>
		<S sid="5" ssid="5">We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate ourmodel on it, showing that our model outper forms competitive baselines and other neural language models.</S>
		<S sid="6" ssid="6">1</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="7" ssid="7">Vector-space models (VSM) represent word meanings with vectors that capture semantic and syntac tic information of words.</S>
			<S sid="8" ssid="8">These representations can be used to induce similarity measures by computingdistances between the vectors, leading to many useful applications, such as information retrieval (Manning et al, 2008), document classification (Sebas tiani, 2002) and question answering (Tellex et al, 2003).</S>
			<S sid="9" ssid="9">1The dataset and word vectors can be downloaded at http://ai.stanford.edu/?ehhuang/.</S>
			<S sid="10" ssid="10">Despite their usefulness, most VSMs share acommon problem that each word is only repre sented with one vector, which clearly fails to capture homonymy and polysemy.</S>
			<S sid="11" ssid="11">Reisinger and Mooney (2010b) introduced a multi-prototype VSM whereword sense discrimination is first applied by clus tering contexts, and then prototypes are built using the contexts of the sense-labeled words.</S>
			<S sid="12" ssid="12">However, in order to cluster accurately, it is important to capture both the syntax and semantics of words.</S>
			<S sid="13" ssid="13">While many approaches use local contexts to disambiguate word meaning, global contexts can also provide useful topical information (Ng and Zelle, 1997).</S>
			<S sid="14" ssid="14">Several studies in psychology have also shown that global context can help language comprehension (Hess et al., 1995) and acquisition (Li et al, 2000).We introduce a new neural-network-based lan guage model that distinguishes and uses both local and global context via a joint training objective.</S>
			<S sid="15" ssid="15">Themodel learns word representations that better capture the semantics of words, while still keeping syn tactic information.</S>
			<S sid="16" ssid="16">These improved representations can be used to represent contexts for clustering wordinstances, which is used in the multi-prototype version of our model that accounts for words with mul tiple senses.</S>
			<S sid="17" ssid="17">We evaluate our new model on the standard WordSim-353 (Finkelstein et al, 2001) dataset that includes human similarity judgments on pairs of words, showing that combining both local and global context outperforms using only local orglobal context alone, and is competitive with state of-the-art methods.</S>
			<S sid="18" ssid="18">However, one limitation of this evaluation is that the human judgments are on pairs 873 Global ContextLocal Context scorel scoreg Document he walks to the bank...</S>
			<S sid="19" ssid="19">... sum score river water shore global semantic vector ? play weighted average Figure 1: An overview of our neural language model.</S>
			<S sid="20" ssid="20">The model makes use of both local and global context to compute a score that should be large for the actual next word (bank in the example), compared to the score for other words.</S>
			<S sid="21" ssid="21">When word meaning is still ambiguous given local context, information in global context can help disambiguation.</S>
			<S sid="22" ssid="22">of words presented in isolation, ignoring meaning variations in context.</S>
			<S sid="23" ssid="23">Since word interpretation in context is important especially for homonymous and polysemous words, we introduce a new dataset with human judgments on similarity between pairs of words in sentential context.</S>
			<S sid="24" ssid="24">To capture interestingword pairs, we sample different senses of words us ing WordNet (Miller, 1995).</S>
			<S sid="25" ssid="25">The dataset includes verbs and adjectives, in addition to nouns.</S>
			<S sid="26" ssid="26">We show that our multi-prototype model improves upon thesingle-prototype version and outperforms other neu ral language models and baselines on this dataset.</S>
	</SECTION>
	<SECTION title="Global Context-Aware Neural Language. " number="2">
			<S sid="27" ssid="1">Model In this section, we describe the training objective of our model, followed by a description of the neural network architecture, ending with a brief description of our model?s training method.</S>
			<S sid="28" ssid="2">2.1 Training Objective.</S>
			<S sid="29" ssid="3">Our model jointly learns word representations while learning to discriminate the next word given a short word sequence (local context) and the document (global context) in which the word sequence occurs.Because our goal is to learn useful word representa tions and not the probability of the next word given previous words (which prohibits looking ahead), our model can utilize the entire document to provide global context.</S>
			<S sid="30" ssid="4">Given a word sequence s and document d inwhich the sequence occurs, our goal is to discrim inate the correct last word in s from other random words.</S>
			<S sid="31" ssid="5">We compute scores g(s, d) and g(sw, d) where sw is swith the last word replaced by wordw, and g(?, ?) is the scoring function that represents the neural networks used.</S>
			<S sid="32" ssid="6">We want g(s, d) to be larger than g(sw, d) by a margin of 1, for any other wordw in the vocabulary, which corresponds to the train ing objective of minimizing the ranking loss for each (s, d) found in the corpus: Cs,d = ? w?V max(0, 1?</S>
			<S sid="33" ssid="7">g(s, d) + g(sw, d)) (1)Collobert and Weston (2008) showed that this rank ing approach can produce good word embeddings that are useful in several NLP tasks, and allowsmuch faster training of the model compared to op timizing log-likelihood of the next word.</S>
			<S sid="34" ssid="8">2.2 Neural Network Architecture.</S>
			<S sid="35" ssid="9">We define two scoring components that contribute to the final score of a (word sequence, document) pair.</S>
			<S sid="36" ssid="10">The scoring components are computed by two neural networks, one capturing local context and the other global context, as shown in Figure 1.</S>
			<S sid="37" ssid="11">We now describe how each scoring component is computed.The score of local context uses the local word se quence s. We first represent the word sequence s as 874 an ordered list of vectors x = (x1, x2, ..., xm) where xi is the embedding of word i in the sequence, which is a column in the embedding matrix L ? Rn?|V | where |V | denotes the size of the vocabulary.</S>
			<S sid="38" ssid="12">The columns of this embedding matrix L are the wordvectors and will be learned and updated during train ing.</S>
			<S sid="39" ssid="13">To compute the score of local context, scorel, we use a neural network with one hidden layer: a1 = f(W1[x1;x2; ...;xm] + b1) (2) scorel = W2a1 + b2 (3) where [x1;x2; ...;xm] is the concatenation of the m word embeddings representing sequence s, f is an element-wise activation function such as tanh, a1 ? Rh?1 is the activation of the hidden layer with h hidden nodes, W1 ? Rh?(mn) and W2 ? R1?h are respectively the first and second layer weights of the neural network, and b1, b2 are the biases of each layer.</S>
			<S sid="40" ssid="14">For the score of the global context, we representthe document also as an ordered list of word em beddings, d = (d1, d2, ..., dk).</S>
			<S sid="41" ssid="15">We first compute theweighted average of all word vectors in the docu ment: c = ?k i=1w(ti)di ?k i=1w(ti) (4)where w(?)</S>
			<S sid="42" ssid="16">can be any weighting function that cap tures the importance of word ti in the document.</S>
			<S sid="43" ssid="17">We use idf-weighting as the weighting function.</S>
			<S sid="44" ssid="18">We use a two-layer neural network to compute the global context score, scoreg, similar to the above: a1 (g) = f(W (g)1 [c;xm] + b (g) 1 ) (5) scoreg = W (g) 2 a (g) 1 + b (g) 2 (6) where [c;xm] is the concatenation of the weighted average document vector and the vector of the last word in s, a1(g) ? Rh (g)?1 is the activation of the hidden layer with h(g) hidden nodes, W (g)1 ? Rh (g)?(2n) and W (g)2 ? R 1?h(g) are respectively the first and second layer weights of the neural network, and b(g)1 , b (g) 2 are the biases of each layer.</S>
			<S sid="45" ssid="19">Note that instead of using the document where the sequenceoccurs, we can also specify a fixed k &gt; m that cap tures larger context.</S>
			<S sid="46" ssid="20">The final score is the sum of the two scores: score = scorel + scoreg (7) The local score preserves word order and syntactic information, while the global score uses a weighted average which is similar to bag-of-words features, capturing more of the semantics and topics of the document.</S>
			<S sid="47" ssid="21">Note that Collobert and Weston (2008)?s language model corresponds to the network using only local context.</S>
			<S sid="48" ssid="22">2.3 Learning.</S>
			<S sid="49" ssid="23">Following Collobert and Weston (2008), we sample the gradient of the objective by randomly choosing a word from the dictionary as a corrupt example for each sequence-document pair, (s, d), and take thederivative of the ranking loss with respect to the parameters: weights of the neural network and the em bedding matrix L. These weights are updated via backpropagation.</S>
			<S sid="50" ssid="24">The embedding matrix L is theword representations.</S>
			<S sid="51" ssid="25">We found that word embed dings move to good positions in the vector spacefaster when using mini-batch L-BFGS (Liu and Nocedal, 1989) with 1000 pairs of good and corrupt examples per batch for training, compared to stochas tic gradient descent.</S>
	</SECTION>
	<SECTION title="Multi-Prototype Neural Language. " number="3">
			<S sid="52" ssid="1">Model Despite distributional similarity models?</S>
			<S sid="53" ssid="2">successfulapplications in various NLP tasks, one major limi tation common to most of these models is that they assume only one representation for each word.</S>
			<S sid="54" ssid="3">Thissingle-prototype representation is problematic be cause many words have multiple meanings, whichcan be wildly different.</S>
			<S sid="55" ssid="4">Using one representa tion simply cannot capture the different meanings.</S>
			<S sid="56" ssid="5">Moreover, using all contexts of a homonymous or polysemous word to build a single prototype could hurt the representation, which cannot represent any one of the meanings well as it is influenced by all meanings of the word.</S>
			<S sid="57" ssid="6">Instead of using only one representation per word,Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word.</S>
			<S sid="58" ssid="7">We show how our 875model can readily adopt the multi-prototype ap proach.</S>
			<S sid="59" ssid="8">We present a way to use our learnedsingle-prototype embeddings to represent each con text window, which can then be used by clustering to perform word sense discrimination (Schu?tze, 1998).</S>
			<S sid="60" ssid="9">In order to learn multiple prototypes, we firstgather the fixed-sized context windows of all occur rences of a word (we use 5 words before and after the word occurrence).</S>
			<S sid="61" ssid="10">Each context is represented by a weighted average of the context words?</S>
			<S sid="62" ssid="11">vectors, where again, we use idf-weighting as the weightingfunction, similar to the document context representation described in Section 2.2.</S>
			<S sid="63" ssid="12">We then use spheri cal k-means to cluster these context representations, which has been shown to model semantic relations well (Dhillon and Modha, 2001).</S>
			<S sid="64" ssid="13">Finally, each wordoccurrence in the corpus is re-labeled to its associated cluster and is used to train the word representa tion for that cluster.Similarity between a pair of words (w,w?) us ing the multi-prototype approach can be computed with or without context, as defined by Reisinger and Mooney (2010b): AvgSimC(w,w?) = 1 K2 k?</S>
			<S sid="65" ssid="14">i=1 k?</S>
			<S sid="66" ssid="15">j=1 p(c, w, i)p(c?, w?, j)d(?i(w), ?j(w ?)) (8) where p(c, w, i) is the likelihood that word w is inits cluster i given context c, ?i(w) is the vector rep resenting the i-th cluster centroid of w, and d(v, v?)is a function computing similarity between two vectors, which can be any of the distance functions pre sented by Curran (2004).</S>
			<S sid="67" ssid="16">The similarity measure canbe computed in absence of context by assuming uni form p(c, w, i) over i.</S>
	</SECTION>
	<SECTION title="Experiments. " number="4">
			<S sid="68" ssid="1">In this section, we first present a qualitative analysiscomparing the nearest neighbors of our model?s embeddings with those of others, showing our embed dings better capture the semantics of words, with the use of global context.</S>
			<S sid="69" ssid="2">Our model also improves thecorrelation with human judgments on a word simi larity task.</S>
			<S sid="70" ssid="3">Because word interpretation in context is important, we introduce a new dataset with humanjudgments on similarity of pairs of words in sentential context.</S>
			<S sid="71" ssid="4">Finally, we show that our model outper forms other methods on this dataset and also that themulti-prototype approach improves over the single prototype approach.</S>
			<S sid="72" ssid="5">We chose Wikipedia as the corpus to train all models because of its wide range of topics andword usages, and its clean organization of docu ment by topic.</S>
			<S sid="73" ssid="6">We used the April 2010 snapshot of the Wikipedia corpus (Shaoul and Westbury, 2010),with a total of about 2 million articles and 990 mil lion tokens.</S>
			<S sid="74" ssid="7">We use a dictionary of the 30,000 most frequent words in Wikipedia, converted to lowercase.</S>
			<S sid="75" ssid="8">In preprocessing, we keep the frequent num bers intact and replace each digit of the uncommon numbers to ?DG?</S>
			<S sid="76" ssid="9">so as to preserve information suchas it being a year (e.g. ?DGDGDGDG?).</S>
			<S sid="77" ssid="10">The converted numbers that are rare are mapped to a NUM BER token.</S>
			<S sid="78" ssid="11">Other rare words not in the dictionary are mapped to an UNKNOWN token.For all experiments, our models use 50 dimensional embeddings.</S>
			<S sid="79" ssid="12">We use 10-word windows of text as the local context, 100 hidden units, and no weight regularization for both neural networks.</S>
			<S sid="80" ssid="13">Formulti-prototype variants, we fix the number of pro totypes to be 10.</S>
			<S sid="81" ssid="14">4.1 Qualitative Evaluations.</S>
			<S sid="82" ssid="15">In order to show that our model learns more seman tic word representations with global context, we give the nearest neighbors of our single-prototype model versus C&amp;W?s, which only uses local context.</S>
			<S sid="83" ssid="16">Thenearest neighbors of a word are computed by com paring the cosine similarity between the center word and all other words in the dictionary.</S>
			<S sid="84" ssid="17">Table 1 shows the nearest neighbors of some words.</S>
			<S sid="85" ssid="18">The nearest neighbors of ?market?</S>
			<S sid="86" ssid="19">that C&amp;W?s embeddings give are more constrained by the syntactic constraint that words in plural form are only close to other words in plural form, whereas our model captures that the singular and plural forms of a word are similar inmeaning.</S>
			<S sid="87" ssid="20">Other examples show that our model induces nearest neighbors that better capture seman tics.</S>
			<S sid="88" ssid="21">Table 2 shows the nearest neighbors of our model using the multi-prototype approach.</S>
			<S sid="89" ssid="22">We see that the clustering is able to group contexts of different 876 Center Word C&amp;W Our Model markets firms, industries, stores market, firms, businesses American Australian, Indian, Italian U.S., Canadian, African illegal alleged, overseas, bannedharmful, prohib ited, convictedTable 1: Nearest neighbors of words based on cosine sim ilarity.</S>
			<S sid="90" ssid="23">Our model is less constrained by syntax and is more semantic.</S>
			<S sid="91" ssid="24">Center Word Nearest Neighbors bank 1 corporation, insurance, company bank 2 shore, coast, direction star 1 movie, film, radio star 2 galaxy, planet, moon cell 1 telephone, smart, phone cell 2 pathology, molecular, physiology left 1 close, leave, live left 2 top, round, right Table 2: Nearest neighbors of word embeddings learned by our model using the multi-prototype approach basedon cosine similarity.</S>
			<S sid="92" ssid="25">The clustering is able to find the dif ferent meanings, usages, and parts of speech of the words.</S>
			<S sid="93" ssid="26">meanings of a word into separate groups, allowingour model to learn multiple meaningful representa tions of a word.</S>
			<S sid="94" ssid="27">4.2 WordSim-353.</S>
			<S sid="95" ssid="28">A standard dataset for evaluating vector-space mod els is the WordSim-353 dataset (Finkelstein et al, 2001), which consists of 353 pairs of nouns.</S>
			<S sid="96" ssid="29">Each pair is presented without context and associated with13 to 16 human judgments on similarity and re latedness on a scale from 0 to 10.</S>
			<S sid="97" ssid="30">For example, (cup,drink) received an average score of 7.25, while (cup,substance) received an average score of 1.92.</S>
			<S sid="98" ssid="31">Table 3 shows our results compared to previous methods, including C&amp;W?s language model and the hierarchical log-bilinear (HLBL) model (Mnih andHinton, 2008), which is a probabilistic, linear neu ral model.</S>
			<S sid="99" ssid="32">We downloaded these embeddings from Turian et al (2010).</S>
			<S sid="100" ssid="33">These embeddings were trained on the smaller corpus RCV1 that contains one yearof Reuters English newswire, and show similar cor relations on the dataset.</S>
			<S sid="101" ssid="34">We report the result of Model Corpus ??</S>
			<S sid="102" ssid="35">100 Our Model-g Wiki.</S>
			<S sid="103" ssid="36">22.8 C&amp;W RCV1 29.5 HLBL RCV1 33.2 C&amp;W* Wiki.</S>
			<S sid="104" ssid="37">49.8 C&amp;W Wiki.</S>
			<S sid="105" ssid="38">55.3 Our Model Wiki.</S>
			<S sid="106" ssid="39">64.2 Our Model* Wiki.</S>
			<S sid="107" ssid="40">71.3 Pruned tf-idf Wiki.</S>
			<S sid="108" ssid="41">73.4 ESA Wiki.</S>
			<S sid="109" ssid="42">75 Tiered Pruned tf-idf Wiki.</S>
			<S sid="110" ssid="43">76.9 Table 3: Spearman?s ? correlation on WordSim-353, showing our model?s improvement over previous neural models for learning word embeddings.</S>
			<S sid="111" ssid="44">C&amp;W* is the word embeddings trained and provided by C&amp;W. OurModel* is trained without stop words, while Our Model g uses only global context.</S>
			<S sid="112" ssid="45">Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included.</S>
			<S sid="113" ssid="46">our re-implementation of C&amp;W?s model trained onWikipedia, showing the large effect of using a dif ferent corpus.</S>
			<S sid="114" ssid="47">Our model is able to learn more semantic word embeddings and noticeably improves upon C&amp;W?smodel.</S>
			<S sid="115" ssid="48">Note that our model achieves higher corre lation (64.2) than either using local context alone (C&amp;W: 55.3) or using global context alone (Our Model-g: 22.8).</S>
			<S sid="116" ssid="49">We also found that correlation can be further improved by removing stop words (71.3).Thus, each window of text (training example) contains more information but still preserves some syn tactic information as the words are still ordered in the local context.</S>
			<S sid="117" ssid="50">4.3 New Dataset: Word Similarity in Context.</S>
			<S sid="118" ssid="51">The many previous datasets that associate human judgments on similarity between pairs of words, such as WordSim-353, MC (Miller and Charles, 1991) and RG (Rubenstein and Goodenough, 1965),have helped to advance the development of vector space models.</S>
			<S sid="119" ssid="52">However, common to all datasets is that similarity scores are given to pairs of words inisolation.</S>
			<S sid="120" ssid="53">This is problematic because the mean ings of homonymous and polysemous words depend highly on the words?</S>
			<S sid="121" ssid="54">contexts.</S>
			<S sid="122" ssid="55">For example, in the two phrases, ?he swings the baseball bat?</S>
			<S sid="123" ssid="56">and ?the 877 Word 1 Word 2 Located downtown along the east bank of the Des Moines River ...</S>
			<S sid="124" ssid="57">This is the basis of all money laundering , a track record of depositing clean money before slipping through dirty money ...</S>
			<S sid="125" ssid="58">Inside the ruins , there are bats and a bowl with Pokeys that fills with sand over the course of the race , and the music changes somewhat while inside ...</S>
			<S sid="126" ssid="59">An aggressive lower order batsman who usually bats at No. 11 , Muralitharan is known for his tendency to back away to leg and slog ...</S>
			<S sid="127" ssid="60">An example of legacy left in the Mideast from these nobles is the Krak des Chevaliers ? enlargement by the Counts of Tripoli and Toulouse ...</S>
			<S sid="128" ssid="61">... one should not adhere to a particular explanation , only in such measure as to be ready to abandon it if it be proved with certainty to be false ...</S>
			<S sid="129" ssid="62">... and Andy ?s getting ready to pack his bags and head up to Los Angeles tomorrow to get ready to fly back home on Thursday ... she encounters Ben ( Duane Jones ) , who arrives in a pickup truck and defends the house against another pack of zombies ...</S>
			<S sid="130" ssid="63">In practice , there is an unknown phase delay between the transmitter and receiver that must be compensated by ? synchronization ? of the receivers local oscillator ... but Gilbert did not believe that she was dedicated enough , and when she missed a rehearsal , she was dismissed ...</S>
			<S sid="131" ssid="64">Table 4: Example pairs from our new dataset.</S>
			<S sid="132" ssid="65">Note that words in a pair can be the same word and have different parts of speech.</S>
			<S sid="133" ssid="66">bat flies?, bat has completely different meanings.</S>
			<S sid="134" ssid="67">It is unclear how this variation in meaning is accounted for in human judgments of words presented without context.</S>
			<S sid="135" ssid="68">One of the main contributions of this paper is the creation of a new dataset that addresses this issue.</S>
			<S sid="136" ssid="69">The dataset has three interesting characteristics: 1) human judgments are on pairs of words presented in sentential context, 2) word pairs and their contextsare chosen to reflect interesting variations in mean ings of homonymous and polysemous words, and 3) verbs and adjectives are present in addition to nouns.</S>
			<S sid="137" ssid="70">We now describe our methodology in constructing the dataset.</S>
			<S sid="138" ssid="71">4.3.1 Dataset Construction Our procedure of constructing the dataset consists of three steps: 1) select a list a words, 2) for each word, select another word to form a pair, 3) for each word in a pair, find a sentential context.</S>
			<S sid="139" ssid="72">We now describe each step in detail.</S>
			<S sid="140" ssid="73">In step 1, in order to make sure we select a diverse list of words, we consider three attributes of a word: frequency in a corpus, number of parts of speech, and number of synsets according to WordNet.</S>
			<S sid="141" ssid="74">For frequency, we divide words into three groups, top 2,000 most frequent, between 2,000 and 5,000, and between 5,000 to 10,000 based on occurrences in Wikipedia.</S>
			<S sid="142" ssid="75">For number of parts of speech, we group words based on their number of possible parts of speech (noun, verb or adjective), from 1 to 3.</S>
			<S sid="143" ssid="76">We also group words by their number of synsets: [0,5],[6,10], [11, 20], and [20, max].</S>
			<S sid="144" ssid="77">Finally, we sam ple at most 15 words from each combination in the Cartesian product of the above groupings.</S>
			<S sid="145" ssid="78">In step 2, for each of the words selected in step 1, we want to choose the other word so that the paircaptures an interesting relationship.</S>
			<S sid="146" ssid="79">Similar to Manandhar et al (2010), we use WordNet to first ran domly select one synset of the first word, we then construct a set of words in various relations to thefirst word?s chosen synset, including hypernyms, hy ponyms, holonyms, meronyms and attributes.</S>
			<S sid="147" ssid="80">We randomly select a word from this set of words as the second word in the pair.</S>
			<S sid="148" ssid="81">We try to repeat the abovetwice to generate two pairs for each word.</S>
			<S sid="149" ssid="82">In addi tion, for words with more than five synsets, we allow the second word to be the same as the first, but with different synsets.</S>
			<S sid="150" ssid="83">We end up with pairs of words as well as the one chosen synset for each word in the pairs.</S>
			<S sid="151" ssid="84">In step 3, we aim to extract a sentence from Wikipedia for each word, which contains the word and corresponds to a usage of the chosen synset.We first find all sentences in which the word oc curs.</S>
			<S sid="152" ssid="85">We then POS tag2 these sentences and filter out those that do not match the chosen POS.</S>
			<S sid="153" ssid="86">To find the 2We used the MaxEnt Treebank POS tagger in the python nltk library.</S>
			<S sid="154" ssid="87">878 Model ??</S>
			<S sid="155" ssid="88">100 C&amp;W-S 57.0 Our Model-S 58.6 Our Model-M AvgSim 62.8 Our Model-M AvgSimC 65.7 tf-idf-S 26.3 Pruned tf-idf-S 62.5 Pruned tf-idf-M AvgSim 60.4 Pruned tf-idf-M AvgSimC 60.5 Table 5: Spearman?s ? correlation on our new dataset.</S>
			<S sid="156" ssid="89">Our Model-S uses the single-prototype approach, while Our Model-M uses the multi-prototype approach.AvgSim calculates similarity with each prototype con tributing equally, while AvgSimC weighs the prototypes according to probability of the word belonging to that prototype?s cluster.</S>
			<S sid="157" ssid="90">word usages that correspond to the chosen synset, we first construct a set of related words of the chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes.</S>
			<S sid="158" ssid="91">Using this set of related words, we filter out a sentence if the document in which the sentence appears does not include one of the related words.</S>
			<S sid="159" ssid="92">Finally, we randomly select one sentence from those that are left.</S>
			<S sid="160" ssid="93">Table 4 shows some examples from the dataset.</S>
			<S sid="161" ssid="94">Note that the dataset alo includes pairs of the same word.</S>
			<S sid="162" ssid="95">Single-prototype models would give the maxsimilarity score for those pairs, which can be prob lematic depending on the words?</S>
			<S sid="163" ssid="96">contexts.</S>
			<S sid="164" ssid="97">Thisdataset requires models to examine context when de termining word meaning.</S>
			<S sid="165" ssid="98">Using Amazon Mechanical Turk, we collected 10 human similarity ratings for each pair, as Snow et al.</S>
			<S sid="166" ssid="99">(2008) found that 10 non-expert annotators can achieve very close inter-annotator agreement with expert raters.</S>
			<S sid="167" ssid="100">To ensure worker quality, we only allowed workers with over 95% approval rate to work on our task.</S>
			<S sid="168" ssid="101">Furthermore, we discarded all ratings by a worker if he/she entered scores out ofthe accepted range or missed a rating, signaling low quality work.</S>
			<S sid="169" ssid="102">We obtained a total of 2,003 word pairs and their sentential contexts.</S>
			<S sid="170" ssid="103">The word pairs consist of 1,712 unique words.</S>
			<S sid="171" ssid="104">Of the 2,003 word pairs, 1328 are noun-noun pairs, 399 verb-verb, 140 verb-noun, 97adjective-adjective, 30 noun-adjective, and 9 verb adjective.</S>
			<S sid="172" ssid="105">241 pairs are same-word pairs.</S>
			<S sid="173" ssid="106">4.3.2 Evaluations on Word Similarity in ContextFor evaluation, we also compute Spearman corre lation between a model?s computed similarity scores and human judgments.</S>
			<S sid="174" ssid="107">Table 5 compares different models?</S>
			<S sid="175" ssid="108">results on this dataset.</S>
			<S sid="176" ssid="109">We compare against the following baselines: tf-idf represents words in a word-word matrix capturing co-occurrence counts in all 10-word context windows.</S>
			<S sid="177" ssid="110">Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance.</S>
			<S sid="178" ssid="111">We report the resultof this pruning technique after tuning the thresh old value on this dataset, removing all but the top 200 features in each word vector.</S>
			<S sid="179" ssid="112">We tried the same multi-prototype approach and used sphericalk-means3 to cluster the contexts using tf-idf representations, but obtained lower numbers than single prototype (55.4 with AvgSimC).</S>
			<S sid="180" ssid="113">We then tried using pruned tf-idf representations on contexts with our clustering assignments (included in Table 5), but still got results worse than the single-prototype version of the pruned tf-idf model (60.5 with AvgSimC).</S>
			<S sid="181" ssid="114">This suggests that the pruned tf-idf representations might be more susceptible to noise or mistakes in context clustering.By utilizing global context, our model outper forms C&amp;W?s vectors and the above baselines on this dataset.</S>
			<S sid="182" ssid="115">With multiple representations per word, we show that the multi-prototype approachcan improve over the single-prototype version with out using context (62.8 vs. 58.6).</S>
			<S sid="183" ssid="116">Moreover, using AvgSimC4 which takes contexts into account, the multi-prototype model obtains the best performance (65.7).</S>
	</SECTION>
	<SECTION title="Related Work. " number="5">
			<S sid="184" ssid="1">Neural language models (Bengio et al, 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Schwenk and Gauvain, 2002; Emami et al, 2003) have been shown to be very powerful at languagemodeling, a task where models are asked to ac curately predict the next word given previously seen words.</S>
			<S sid="185" ssid="2">By using distributed representations of 3We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5).</S>
			<S sid="186" ssid="3">4probability of being in a cluster is calculated as the inverse of the distance to the cluster centroid.</S>
			<S sid="187" ssid="4">879 words which model words?</S>
			<S sid="188" ssid="5">similarity, this type of models addresses the data sparseness problem that n-gram models encounter when large contexts areused.</S>
			<S sid="189" ssid="6">Most of these models used relative local contexts of between 2 to 10 words.</S>
			<S sid="190" ssid="7">Schwenk and Gau vain (2002) tried to incorporate larger context by combining partial parses of past word sequences anda neural language model.</S>
			<S sid="191" ssid="8">They used up to 3 previ ous head words and showed increased performance on language modeling.</S>
			<S sid="192" ssid="9">Our model uses a similar neural network architecture as these models and usesthe ranking-loss training objective proposed by Col lobert and Weston (2008), but introduces a new way to combine local and global context to train word embeddings.Besides language modeling, word embeddings induced by neural language models have been use ful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al,2011a).</S>
			<S sid="193" ssid="10">However, they have not been directly eval uated on word similarity tasks, which are importantfor tasks such as information retrieval and summarization.</S>
			<S sid="194" ssid="11">Our experiments show that our word em beddings are competitive in word similarity tasks.</S>
			<S sid="195" ssid="12">Most of the previous vector-space models use a single vector to represent a word even though many words have multiple meanings.</S>
			<S sid="196" ssid="13">The multi-prototypeapproach has been widely studied in models of cat egorization in psychology (Rosseel, 2002; Griffiths et al, 2009), while Schu?tze (1998) used clustering of contexts to perform word sense discrimination.</S>
			<S sid="197" ssid="14">Reisinger and Mooney (2010b) combined the twoapproaches and applied them to vector-space mod els, which was further improved in Reisinger and Mooney (2010a).</S>
			<S sid="198" ssid="15">Two other recent papers (Dhillon et al, 2011; Reddy et al, 2011) present models for constructing word representations that deal with context.</S>
			<S sid="199" ssid="16">It would be interesting to evaluate those models on our new dataset.</S>
			<S sid="200" ssid="17">Many datasets with human similarity ratings on pairs of words, such as WordSim-353 (Finkelstein et al, 2001), MC (Miller and Charles, 1991) and RG (Rubenstein and Goodenough, 1965), have beenwidely used to evaluate vector-space models.</S>
			<S sid="201" ssid="18">Moti vated to evaluate composition models, Mitchell andLapata (2008) introduced a dataset where an intransitive verb, presented with a subject noun, is com pared to another verb chosen to be either similar or dissimilar to the intransitive verb in context.</S>
			<S sid="202" ssid="19">The context is short, with only one word, and only verbs are compared.</S>
			<S sid="203" ssid="20">Erk and Pado?</S>
			<S sid="204" ssid="21">(2008), Thater et al (2011) and Dinu and Lapata (2010) evaluated wordsimilarity in context with a modified task where systems are to rerank gold-standard paraphrase candi dates given the SemEval 2007 Lexical SubstitutionTask dataset.</S>
			<S sid="205" ssid="22">This task only indirectly evaluates sim ilarity as only reranking of already similar words are evaluated.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="6">
			<S sid="206" ssid="1">We presented a new neural network architecture thatlearns more semantic word representations by us ing both local and global context in learning.</S>
			<S sid="207" ssid="2">These learned word embeddings can be used to represent word contexts as low-dimensional weighted average vectors, which are then clustered to form different meaning groups and used to learn multi-prototype vectors.</S>
			<S sid="208" ssid="3">We introduced a new dataset with human judgments on similarity between pairs of words in context, so as to evaluate model?s abilities to capture homonymy and polysemy of words in context.</S>
			<S sid="209" ssid="4">Ournew multi-prototype neural language model outperforms previous neural models and competitive base lines on this new dataset.</S>
			<S sid="210" ssid="5">Acknowledgments The authors gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.</S>
			<S sid="211" ssid="6">FA8750-09-C-0181, and the DARPA DeepLearning program under contract number FA8650 10-C-7020.</S>
			<S sid="212" ssid="7">Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.</S>
	</SECTION>
</PAPER>
