<PAPER>
  <S sid="0">Improvements In Phrase-Based Statistical Machine Translation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups.</S>
    <S sid="2" ssid="2">We describe the baseline phrase-based translation system and various refinements.</S>
    <S sid="3" ssid="3">We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length.</S>
    <S sid="4" ssid="4">We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards.</S>
    <S sid="5" ssid="5">For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words.</S>
    <S sid="6" ssid="6">The translation results for the Xerox and Canadian Hansards task are very promising.</S>
    <S sid="7" ssid="7">The system even outperforms the alignment template system.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="8" ssid="1">In statistical machine translation, we are given a source language (&#8216;French&#8217;) sentence fJ1 = f1 ... fj ... fJ, which is to be translated into a target language (&#8216;English&#8217;) sentence eI1 = e1 ... ei ... eI.</S>
    <S sid="9" ssid="2">Among all possible target language sentences, we will choose the sentence with the highest probability: The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation (Brown et al., 1990).</S>
    <S sid="10" ssid="3">It allows an independent modeling of target language model Pr(eI1) and translation model Pr(fJ1 |eI1)1.</S>
    <S sid="11" ssid="4">The target language model describes the well-formedness of the target language sentence.</S>
    <S sid="12" ssid="5">The translation model links the source language sentence to the target language sentence.</S>
    <S sid="13" ssid="6">It can be further decomposed into alignment and lexicon model.</S>
    <S sid="14" ssid="7">The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S>
    <S sid="15" ssid="8">We have to maximize over all possible target language sentences.</S>
    <S sid="16" ssid="9">An alternative to the classical source-channel approach is the direct modeling of the posterior probability Pr(eI1|fJ1 ).</S>
    <S sid="17" ssid="10">Using a log-linear model (Och and Ney, 2002), we obtain: Here, Z(fJ1 ) denotes the appropriate normalization constant.</S>
    <S sid="18" ssid="11">As a decision rule, we obtain: This approach is a generalization of the source-channel approach.</S>
    <S sid="19" ssid="12">It has the advantage that additional models or feature functions can be easily integrated into the overall system.</S>
    <S sid="20" ssid="13">The model scaling factors &#955;M1 are trained according to the maximum entropy principle, e.g. using the GIS algorithm.</S>
    <S sid="21" ssid="14">Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003).</S>
    <S sid="22" ssid="15">The remaining part of this work is structured as follows: in the next section, we will describe the baseline phrase-based translation model and the extraction of bilingual phrases.</S>
    <S sid="23" ssid="16">Then, we will describe refinements of the baseline model.</S>
    <S sid="24" ssid="17">In Section 4, we will describe a monotone search algorithm.</S>
    <S sid="25" ssid="18">Its complexity is linear in the sentence length.</S>
    <S sid="26" ssid="19">The next section contains the statistics of the corpora that were used.</S>
    <S sid="27" ssid="20">Then, we will investigate the degree of monotonicity and present the translation results for three tasks: Verbmobil, Xerox and Canadian Hansards.</S>
  </SECTION>
  <SECTION title="2 Phrase-Based Translation" number="2">
    <S sid="28" ssid="1">One major disadvantage of single-word based approaches is that contextual information is not taken into account.</S>
    <S sid="29" ssid="2">The lexicon probabilities are based only on single words.</S>
    <S sid="30" ssid="3">For many words, the translation depends heavily on the surrounding words.</S>
    <S sid="31" ssid="4">In the single-word based translation approach, this disambiguation is addressed by the language model only, which is often not capable of doing this.</S>
    <S sid="32" ssid="5">One way to incorporate the context into the translation model is to learn translations for whole phrases instead of single words.</S>
    <S sid="33" ssid="6">Here, a phrase is simply a sequence of words.</S>
    <S sid="34" ssid="7">So, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations.</S>
    <S sid="35" ssid="8">The system somehow has to learn which phrases are translations of each other.</S>
    <S sid="36" ssid="9">Therefore, we use the following approach: first, we train statistical alignment models using GIZA++ and compute the Viterbi word alignment of the training corpus.</S>
    <S sid="37" ssid="10">This is done for both translation directions.</S>
    <S sid="38" ssid="11">We take the union ofboth alignments to obtain a symmetrized word alignment matrix.</S>
    <S sid="39" ssid="12">This alignment matrix is the starting point for the phrase extraction.</S>
    <S sid="40" ssid="13">The following criterion defines the set of bilingual phrases BP of the sentence pair (fJ1 ; eI1) and the alignment matrix A &#8838; J &#215; I that is used in the translation system.</S>
    <S sid="41" ssid="14">This criterion is identical to the alignment template criterion described in (Och et al., 1999).</S>
    <S sid="42" ssid="15">It means that two phrases are considered to be translations of each other, if the words are aligned only within the phrase pair and not to words outside.</S>
    <S sid="43" ssid="16">The phrases have to be contiguous.</S>
    <S sid="44" ssid="17">To use phrases in the translation model, we introduce the hidden variable S. This is a segmentation of the sentence pair (fJ 1 ; eI 1) into K phrases ( &#732;fK 1 ; &#732;eK1 ).</S>
    <S sid="45" ssid="18">We use a one-toone phrase alignment, i.e. one source phrase is translated by exactly one target phrase.</S>
    <S sid="46" ssid="19">Thus, we obtain: In the preceding step, we used the maximum approximation for the sum over all segmentations.</S>
    <S sid="47" ssid="20">Next, we allow only translations that are monotone at the phrase level.</S>
    <S sid="48" ssid="21">So, the phrase &#732;f1 is produced by &#732;e1, the phrase &#732;f2 is produced by &#732;e2, and so on.</S>
    <S sid="49" ssid="22">Within the phrases, the reordering is learned during training.</S>
    <S sid="50" ssid="23">Therefore, there is no constraint on the reordering within the phrases.</S>
    <S sid="51" ssid="24">Here, we have assumed a zero-order model at the phrase level.</S>
    <S sid="52" ssid="25">Finally, we have to estimate the phrase translation probabilities p(&#732;f|&#732;e).</S>
    <S sid="53" ssid="26">This is done via relative frequencies: Here, N( &#732;f, &#732;e) denotes the count of the event that f&#732; has been seen as a translation of &#732;e.</S>
    <S sid="54" ssid="27">If one occurrence of e&#732; has N &gt; 1 possible translations, each of them contributes to N(&#732;f, &#732;e) with 1/N.</S>
    <S sid="55" ssid="28">These counts are calculated from the training corpus.</S>
    <S sid="56" ssid="29">Using a bigram language model and assuming Bayes decision rule, Equation (2), we obtain the following search criterion: For the preceding equation, we assumed the segmentation probability p(S|eI1) to be constant.</S>
    <S sid="57" ssid="30">The result is a simple translation model.</S>
    <S sid="58" ssid="31">If we interpret this model as a feature function in the direct approach, we obtain: We use the maximum approximation for the hidden variable S. Therefore, the feature functions are dependent on S. Although the number of phrases K is implicitly given by the segmentation S, we used both S and K to make this dependency more obvious.</S>
    <S sid="59" ssid="32">In this section, we will describe refinements of the phrase-based translation model.</S>
    <S sid="60" ssid="33">First, we will describe two heuristics: word penalty and phrase penalty.</S>
    <S sid="61" ssid="34">Second, we will describe a single-word based lexicon model.</S>
    <S sid="62" ssid="35">This will be used to smooth the phrase translation probabilities.</S>
    <S sid="63" ssid="36">In addition to the baseline model, we use two simple heuristics, namely word penalty and phrase penalty: The word penalty feature is simply the target sentence length.</S>
    <S sid="64" ssid="37">In combination with the scaling factor this results in a constant cost per produced target language word.</S>
    <S sid="65" ssid="38">With this feature, we are able to adjust the sentence length.</S>
    <S sid="66" ssid="39">If we set a negative scaling factor, longer sentences are more penalized than shorter ones, and the system will favor shorter translations.</S>
    <S sid="67" ssid="40">Alternatively, by using a positive scaling factors, the system will favor longer translations.</S>
    <S sid="68" ssid="41">Similar to the word penalty, the phrase penalty feature results in a constant cost per produced phrase.</S>
    <S sid="69" ssid="42">The phrase penalty is used to adjust the average length of the phrases.</S>
    <S sid="70" ssid="43">A negative weight, meaning real costs per phrase, results in a preference for longer phrases.</S>
    <S sid="71" ssid="44">A positive weight, meaning a bonus per phrase, results in a preference for shorter phrases.</S>
    <S sid="72" ssid="45">We are using relative frequencies to estimate the phrase translation probabilities.</S>
    <S sid="73" ssid="46">Most of the longer phrases are seen only once in the training corpus.</S>
    <S sid="74" ssid="47">Therefore, pure relative frequencies overestimate the probability of those phrases.</S>
    <S sid="75" ssid="48">To overcome this problem, we use a word-based lexicon model to smooth the phrase translation probabilities.</S>
    <S sid="76" ssid="49">For a source word f and a target phrase e&#732; = ei2 This models a disjunctive interaction, also called noisyOR gate (Pearl, 1988).</S>
    <S sid="77" ssid="50">The idea is that there are multiple independent causes ei2 i1 that can generate an event f. It can be easily integrated into the search algorithm.</S>
    <S sid="78" ssid="51">The corresponding feature function is: hlex(fJ1 , eI1, S, K) = log K H7 k p(f7|&#732;ek) H 7=7k&#8722;1+1 k=1 Here, jk and ik denote the final position of phrase number k in the source and the target sentence, respectively, and we define j0 := 0 and i0 := 0.</S>
    <S sid="79" ssid="52">To estimate the single-word based translation probabilities p(f|e), we use smoothed relative frequencies.</S>
    <S sid="80" ssid="53">The smoothing method we apply is absolute discounting with interpolation: This method is well known from language modeling (Ney et al., 1997).</S>
    <S sid="81" ssid="54">Here, d is the nonnegative discounting parameter, &#945;(e) is a normalization constant and &#946; is the normalized backing-off distribution.</S>
    <S sid="82" ssid="55">To compute the counts, we use the same word alignment matrix as for the extraction of the bilingual phrases.</S>
    <S sid="83" ssid="56">The symbol N(e) denotes the unigram count of a word e and N(f, e) denotes the count of the event that the target language word e is aligned to the source language word f. If one occurrence of e has N &gt; 1 aligned source words, each of them contributes with a count of 1/N.</S>
    <S sid="84" ssid="57">The formula for &#945;(e) is: This formula is a generalization of the one typically used in publications on language modeling.</S>
    <S sid="85" ssid="58">This generalization is necessary, because the lexicon counts may be fractional whereas in language modeling typically integer counts are used.</S>
    <S sid="86" ssid="59">Additionally, we want to allow discounting values d greater than one.</S>
    <S sid="87" ssid="60">One effect of the discounting parameter d is that all lexicon entries with a count less than d are discarded and the freed probability mass is redistributed among the other entries.</S>
    <S sid="88" ssid="61">As backing-off distribution &#946;(f), we consider two alternatives.</S>
    <S sid="89" ssid="62">The first one is a uniform distribution and the second one is the unigram distribution: Here, Vf denotes the vocabulary size of the source language and N(f) denotes the unigram count of a source word f.</S>
  </SECTION>
  <SECTION title="4 Monotone Search" number="3">
    <S sid="90" ssid="1">The monotone search can be efficiently computed with dynamic programming.</S>
    <S sid="91" ssid="2">The resulting complexity is linear in the sentence length.</S>
    <S sid="92" ssid="3">We present the formulae for a bigram language model.</S>
    <S sid="93" ssid="4">This is only for notational convenience.</S>
    <S sid="94" ssid="5">The generalization to a higher order language model is straightforward.</S>
    <S sid="95" ssid="6">For the maximization problem in (11), we define the quantity Q(j,e) as the maximum probability of a phrase sequence that ends with the language word e and covers positions 1 to j of the source sentence.</S>
    <S sid="96" ssid="7">Q(J + 1, $) is the probability of the optimum translation.</S>
    <S sid="97" ssid="8">The $ symbol is the sentence boundary marker.</S>
    <S sid="98" ssid="9">We obtain the following dynamic programming recursion.</S>
    <S sid="99" ssid="10">Here, M denotes the maximum phrase length in the source language.</S>
    <S sid="100" ssid="11">During the search, we store backpointers to the maximizing arguments.</S>
    <S sid="101" ssid="12">After performing the search, we can generate the optimum translation.</S>
    <S sid="102" ssid="13">The resulting algorithm has a worst-case complexity of O(J &#183; M &#183; Ve &#183; E).</S>
    <S sid="103" ssid="14">Here, Ve denotes the vocabulary size of the target language and E denotes the maximum number of phrase translation candidates for a source language phrase.</S>
    <S sid="104" ssid="15">Using efficient data structures and taking into account that not all possible target language phrases can occur in translating a specific source language sentence, we can perform a very efficient search.</S>
    <S sid="105" ssid="16">This monotone algorithm is especially useful for language pairs that have a similar word order, e.g.</S>
    <S sid="106" ssid="17">SpanishEnglish or French-English.</S>
  </SECTION>
  <SECTION title="5 Corpus Statistics" number="4">
    <S sid="107" ssid="1">In the following sections, we will present results on three tasks: Verbmobil, Xerox and Canadian Hansards.</S>
    <S sid="108" ssid="2">Therefore, we will show the corpus statistics for each of these tasks in this section.</S>
    <S sid="109" ssid="3">The training corpus (Train) of each task is used to train a word alignment and then extract the bilingual phrases and the word-based lexicon.</S>
    <S sid="110" ssid="4">The remaining free parameters, e.g. the model scaling factors, are optimized on the development corpus (Dev).</S>
    <S sid="111" ssid="5">The resulting system is then evaluated on the test corpus (Test).</S>
    <S sid="112" ssid="6">Verbmobil Task.</S>
    <S sid="113" ssid="7">The first task we will present results on is the German&#8211;English Verbmobil task (Wahlster, 2000).</S>
    <S sid="114" ssid="8">The domain of this corpus is appointment scheduling, travel planning, and hotel reservation.</S>
    <S sid="115" ssid="9">It consists of transcriptions of spontaneous speech.</S>
    <S sid="116" ssid="10">Table 1 shows the corpus statistics of this task.</S>
    <S sid="117" ssid="11">Xerox task.</S>
    <S sid="118" ssid="12">Additionally, we carried out experiments on the Spanish&#8211;English Xerox task.</S>
    <S sid="119" ssid="13">The corpus consists of technical manuals.</S>
    <S sid="120" ssid="14">This is a rather limited domain task.</S>
    <S sid="121" ssid="15">Table 2 shows the training, development and test corpus statistics.</S>
    <S sid="122" ssid="16">Canadian Hansards task.</S>
    <S sid="123" ssid="17">Further experiments were carried out on the French&#8211;English Canadian Hansards task.</S>
    <S sid="124" ssid="18">This task contains the proceedings of the Canadian parliament.</S>
    <S sid="125" ssid="19">About 3 million parallel sentences of this bilingual data have been made available by the Linguistic Data Consortium (LDC).</S>
    <S sid="126" ssid="20">Here, we use a subset of the data containing only sentences with a maximum length of 30 words.</S>
    <S sid="127" ssid="21">This task covers a large variety of topics, so this is an open-domain corpus.</S>
    <S sid="128" ssid="22">This is also reflected by the large vocabulary size.</S>
    <S sid="129" ssid="23">Table 3 shows the training and test corpus statistics.</S>
  </SECTION>
  <SECTION title="6 Degree of Monotonicity" number="5">
    <S sid="130" ssid="1">In this section, we will investigate the effect of the monotonicity constraint.</S>
    <S sid="131" ssid="2">Therefore, we compute how many of the training corpus sentence pairs can be produced with the monotone phrase-based search.</S>
    <S sid="132" ssid="3">We compare this to the number of sentence pairs that can be produced with a nonmonotone phrase-based search.</S>
    <S sid="133" ssid="4">To make these numbers more realistic, we use leaving-one-out.</S>
    <S sid="134" ssid="5">Thus phrases that are extracted from a specific sentence pair are not used to check its monotonicity.</S>
    <S sid="135" ssid="6">With leaving-one-out it is possible that even the nonmonotone search cannot generate a sentence pair.</S>
    <S sid="136" ssid="7">This happens if a sentence pair contains a word that occurs only once in the training corpus.</S>
    <S sid="137" ssid="8">All phrases that might produce this singleton are excluded because of the leaving-one-out principle.</S>
    <S sid="138" ssid="9">Note that all these monotonicity consideration are done at the phrase level.</S>
    <S sid="139" ssid="10">Within the phrases arbitrary reorderings are allowed.</S>
    <S sid="140" ssid="11">The only restriction is that they occur in the training corpus.</S>
    <S sid="141" ssid="12">Table 4 shows the percentage of the training corpus that can be generated with monotone and nonmonotone phrase-based search.</S>
    <S sid="142" ssid="13">The number of sentence pairs that can be produced with the nonmonotone search gives an estimate of the upper bound for the sentence error rate of the phrase-based system that is trained on the given data.</S>
    <S sid="143" ssid="14">The same considerations hold for the monotone search.</S>
    <S sid="144" ssid="15">The maximum source phrase length for the Verbmobil task and the Xerox task is 12, whereas for the Canadian Hansards task we use a maximum of 4, because of the large corpus size.</S>
    <S sid="145" ssid="16">This explains the rather low coverage on the Canadian Hansards task for both the nonmonotone and the monotone search.</S>
    <S sid="146" ssid="17">For the Xerox task, the nonmonotone search can produce 75.1% of the sentence pairs whereas the monotone can produce 65.3%.</S>
    <S sid="147" ssid="18">The ratio of the two numbers measures how much the system deteriorates by using the monotone search and will be called the degree of monotonicity.</S>
    <S sid="148" ssid="19">For the Xerox task, the degree of monotonicity is 87.0%.</S>
    <S sid="149" ssid="20">This means the monotone search can produce 87.0% of the sentence pairs that can be produced with the nonmonotone search.</S>
    <S sid="150" ssid="21">We see that for the SpanishEnglish Xerox task and for the French-English Canadian Hansards task, the degree of monotonicity is rather high.</S>
    <S sid="151" ssid="22">For the German-English Verbmobil task it is significantly lower.</S>
    <S sid="152" ssid="23">This may be caused by the rather free word order in German and the long range reorderings that are necessary to translate the verb group.</S>
    <S sid="153" ssid="24">It should be pointed out that in practice the monotone search will perform better than what the preceding estimates indicate.</S>
    <S sid="154" ssid="25">The reason is that we assumed a perfect nonmonotone search, which is difficult to achieve in practice.</S>
    <S sid="155" ssid="26">This is not only a hard search problem, but also a complicated modeling problem.</S>
    <S sid="156" ssid="27">We will see in the next section that the monotone search will perform very well on both the Xerox task and the Canadian Hansards task.</S>
  </SECTION>
  <SECTION title="7 Translation Results" number="6">
    <S sid="157" ssid="1">So far, in machine translation research a single generally accepted criterion for the evaluation of the experimental results does not exist.</S>
    <S sid="158" ssid="2">Therefore, we use a variety of different criteria.</S>
    <S sid="159" ssid="3">The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated sentence into the reference sentence.</S>
    <S sid="160" ssid="4">A shortcoming of the WER is that it requires a perfect word order.</S>
    <S sid="161" ssid="5">The word order of an acceptable sentence can be different from that of the target sentence, so that the WER measure alone could be misleading.</S>
    <S sid="162" ssid="6">The PER compares the words in the two sentences ignoring the word order.</S>
    <S sid="163" ssid="7">This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a reference translation with a penalty for too short sentences (Papineni et al., 2001).</S>
    <S sid="164" ssid="8">BLEU measures accuracy, i.e. large BLEU scores are better.</S>
    <S sid="165" ssid="9">This score is similar to BLEU.</S>
    <S sid="166" ssid="10">It is a weighted ngram precision in combination with a penalty for too short sentences (Doddington, 2002).</S>
    <S sid="167" ssid="11">NIST measures accuracy, i.e. large NIST scores are better.</S>
    <S sid="168" ssid="12">For the Verbmobil task, we have multiple references available.</S>
    <S sid="169" ssid="13">Therefore on this task, we compute all the preceding criteria with respect to multiple references.</S>
    <S sid="170" ssid="14">To indicate this, we will precede the acronyms with an m (multiple) if multiple references are used.</S>
    <S sid="171" ssid="15">For the other two tasks, only single references are used.</S>
    <S sid="172" ssid="16">In this section, we will describe the systems that were used.</S>
    <S sid="173" ssid="17">On the one hand, we have three different variants of the single-word based model IBM4.</S>
    <S sid="174" ssid="18">On the other hand, we have two phrase-based systems, namely the alignment templates and the one described in this work.</S>
    <S sid="175" ssid="19">Single-Word Based Systems (SWB).</S>
    <S sid="176" ssid="20">First, there is a monotone search variant (Mon) that translates each word of the source sentence from left to right.</S>
    <S sid="177" ssid="21">The second variant allows reordering according to the so-called IBM constraints (Berger et al., 1996).</S>
    <S sid="178" ssid="22">Thus up to three words may be skipped and translated later.</S>
    <S sid="179" ssid="23">This system will be denoted by IBM.</S>
    <S sid="180" ssid="24">The third variant implements special German-English reordering constraints.</S>
    <S sid="181" ssid="25">These constraints are represented by a finite state automaton and optimized to handle the reorderings of the German verb group.</S>
    <S sid="182" ssid="26">The abbreviation for this variant is GE.</S>
    <S sid="183" ssid="27">It is only used for the German-English Verbmobil task.</S>
    <S sid="184" ssid="28">This is just an extremely brief description of these systems.</S>
    <S sid="185" ssid="29">For details, see (Tillmann and Ney, 2003).</S>
    <S sid="186" ssid="30">Phrase-Based System (PB).</S>
    <S sid="187" ssid="31">For the phrase-based system, we use the following feature functions: a trigram language model, the phrase translation model and the word-based lexicon model.</S>
    <S sid="188" ssid="32">The latter two feature functions are used for both directions: p(f|e) and p(e|f).</S>
    <S sid="189" ssid="33">Additionally, we use the word and phrase penalty feature functions.</S>
    <S sid="190" ssid="34">The model scaling factors are optimized on the development corpus with respect to mWER similar to (Och, 2003).</S>
    <S sid="191" ssid="35">We use the Downhill Simplex algorithm from (Press et al., 2002).</S>
    <S sid="192" ssid="36">We do not perform the optimization on N-best lists but we retranslate the whole development corpus for each iteration of the optimization algorithm.</S>
    <S sid="193" ssid="37">This is feasible because this system is extremely fast.</S>
    <S sid="194" ssid="38">It takes only a few seconds to translate the whole development corpus for the Verbmobil task and the Xerox task; for details see Section 8.</S>
    <S sid="195" ssid="39">In the experiments, the Downhill Simplex algorithm converged after about 200 iterations.</S>
    <S sid="196" ssid="40">This method has the advantage that it is not limited to the model scaling factors as the method described in (Och, 2003).</S>
    <S sid="197" ssid="41">It is also possible to optimize any other parameter, e.g. the discounting parameter for the lexicon smoothing.</S>
    <S sid="198" ssid="42">Alignment Template System (AT).</S>
    <S sid="199" ssid="43">The alignment template system (Och et al., 1999) is similar to the system described in this work.</S>
    <S sid="200" ssid="44">One difference is that the alignment templates are not defined at the word level but at a word class level.</S>
    <S sid="201" ssid="45">In addition to the word-based trigram model, the alignment template system uses a classbased fivegram language model.</S>
    <S sid="202" ssid="46">The search algorithm of the alignment templates allows arbitrary reorderings of the templates.</S>
    <S sid="203" ssid="47">It penalizes reorderings with costs that are linear in the jump width.</S>
    <S sid="204" ssid="48">To make the results as comparable as possible, the alignment template system and the phrase-based system start from the same word alignment.</S>
    <S sid="205" ssid="49">The alignment template system uses discriminative training of the model scaling factors as described in (Och and Ney, 2002).</S>
    <S sid="206" ssid="50">We start with the Verbmobil results.</S>
    <S sid="207" ssid="51">We studied smoothing the lexicon probabilities as described in Section 3.2.</S>
    <S sid="208" ssid="52">The results are summarized in Table 5.</S>
    <S sid="209" ssid="53">We see that the uniform smoothing method improves translation quality.</S>
    <S sid="210" ssid="54">There is only a minor improvement, but it is consistent among all evaluation criteria.</S>
    <S sid="211" ssid="55">It is statistically significant at the 94% level.</S>
    <S sid="212" ssid="56">The unigram method hurts performance.</S>
    <S sid="213" ssid="57">There is a degradation of the mWER of 0.9%.</S>
    <S sid="214" ssid="58">In the following, all phrase-based systems use the uniform smoothing method.</S>
    <S sid="215" ssid="59">The translation results of the different systems are shown in Table 6.</S>
    <S sid="216" ssid="60">Obviously, the monotone phrase-based system outperforms the monotone single-word based system.</S>
    <S sid="217" ssid="61">The result of the phrase-based system is comparable to the nonmonotone single-word based search with the IBM constraints.</S>
    <S sid="218" ssid="62">With respect to the mPER, the PB system clearly outperforms all single-word based systems.</S>
    <S sid="219" ssid="63">If we compare the monotone phrase-based system with the nonmonotone alignment template system, we see that the mPERs are similar.</S>
    <S sid="220" ssid="64">Thus the lexical choice of words is of the same quality.</S>
    <S sid="221" ssid="65">Regarding the other evaluation criteria, which take the word order into account, the nonmonotone search of the alignment templates has a clear advantage.</S>
    <S sid="222" ssid="66">This was already indicated by the low degree of monotonicity on this task.</S>
    <S sid="223" ssid="67">The rather free word order in German and the long range dependencies of the verb group make reorderings necessary.</S>
    <S sid="224" ssid="68">The translation results for the Xerox task are shown in Table 7.</S>
    <S sid="225" ssid="69">Here, we see that both phrase-based systems clearly outperform the single-word based systems.</S>
    <S sid="226" ssid="70">The PB system performs best on this task.</S>
    <S sid="227" ssid="71">Compared to the AT system, the BLEU score improves by 4.1% absolute.</S>
    <S sid="228" ssid="72">The improvement of the PB system with respect to the AT system is statistically significant at the 99% level.</S>
    <S sid="229" ssid="73">The translation results for the Canadian Hansards task are shown in Table 8.</S>
    <S sid="230" ssid="74">As on the Xerox task, the phrase-based systems perform better than the single-word based systems.</S>
    <S sid="231" ssid="75">The monotone phrase-based system yields even better results than the alignment template system.</S>
    <S sid="232" ssid="76">This improvement is consistent among all evaluation criteria and it is statistically significant at the 99% level.</S>
  </SECTION>
  <SECTION title="8 Efficiency" number="7">
    <S sid="233" ssid="1">In this section, we analyze the translation speed of the phrase-based translation system.</S>
    <S sid="234" ssid="2">All experiments were carried out on an AMD Athlon with 2.2GHz.</S>
    <S sid="235" ssid="3">Note that the systems were not optimized for speed.</S>
    <S sid="236" ssid="4">We used the best performing systems to measure the translation times.</S>
    <S sid="237" ssid="5">The translation speed of the monotone phrase-based system for all three tasks is shown in Table 9.</S>
    <S sid="238" ssid="6">For the Xerox task, the translation process takes less than 7 seconds for the whole 10K words test set.</S>
    <S sid="239" ssid="7">For the Verbmobil task, the system is even slightly faster.</S>
    <S sid="240" ssid="8">It takes about 1.6 seconds to translate the whole test set.</S>
    <S sid="241" ssid="9">For the Canadian Hansards task, the translation process is much slower, but the average time per sentence is still less than 1 second.</S>
    <S sid="242" ssid="10">We think that this slowdown can be attributed to the large training corpus.</S>
    <S sid="243" ssid="11">The system loads only phrase pairs into memory if the source phrase occurs in the test corpus.</S>
    <S sid="244" ssid="12">Therefore, the large test corpus size for this task also affects the translation speed.</S>
    <S sid="245" ssid="13">In Fig.</S>
    <S sid="246" ssid="14">1, we see the average translation time per sentence as a function of the sentence length.</S>
    <S sid="247" ssid="15">The translation times were measured for the translation of the 5432 test sentences of the Canadian Hansards task.</S>
    <S sid="248" ssid="16">We see a clear linear dependency.</S>
    <S sid="249" ssid="17">Even for sentences of thirty words, the translation takes only about 1.5 seconds.</S>
  </SECTION>
  <SECTION title="9 Related Work" number="8">
    <S sid="250" ssid="1">Recently, phrase-based translation approaches became more and more popular.</S>
    <S sid="251" ssid="2">Some examples are the alignment template system in (Och et al., 1999; Och and Ney, 2002) that we used for comparison.</S>
    <S sid="252" ssid="3">In (Zens et al., 2002), a simple phrase-based approach is described that served as starting point for the system in this work.</S>
    <S sid="253" ssid="4">(Marcu and Wong, 2002) presents a joint probability model for phrase-based translation.</S>
    <S sid="254" ssid="5">It does not use the word alignment for extracting the phrases, but directly generates a phrase alignment.</S>
    <S sid="255" ssid="6">In (Koehn et al., 2003), various aspects of phrase-based systems are compared, e.g. the phrase extraction method, the underlying word alignment model, or the maximum phrase length.</S>
    <S sid="256" ssid="7">(Tomas and Casacuberta, 2003) describes a linear interpolation of a phrase-based and an alignment template-based approach.</S>
  </SECTION>
  <SECTION title="10 Conclusions" number="9">
    <S sid="257" ssid="1">We described a phrase-based translation approach.</S>
    <S sid="258" ssid="2">The basic idea of this approach is to remember all bilingual phrases that have been seen in the word-aligned training corpus.</S>
    <S sid="259" ssid="3">As refinements of the baseline model, we described two simple heuristics: the word penalty feature and the phrase penalty feature.</S>
    <S sid="260" ssid="4">Additionally, we presented a single-word based lexicon with two smoothing methods.</S>
    <S sid="261" ssid="5">The model scaling factors were optimized with respect to the mWER on the development corpus.</S>
    <S sid="262" ssid="6">We described a highly efficient monotone search algorithm.</S>
    <S sid="263" ssid="7">The worst-case complexity of this algorithm is linear in the sentence length.</S>
    <S sid="264" ssid="8">This leads to an impressive translation speed of more than 1000 words per second for the Verbmobil task and for the Xerox task.</S>
    <S sid="265" ssid="9">Even for the Canadian Hansards task the translation of sentences of length 30 takes only about 1.5 seconds.</S>
    <S sid="266" ssid="10">The described search is monotone at the phrase level.</S>
    <S sid="267" ssid="11">Within the phrases, there are no constraints on the reorderings.</S>
    <S sid="268" ssid="12">Therefore, this method is best suited for language pairs that have a similar order at the level of the phrases learned by the system.</S>
    <S sid="269" ssid="13">Thus, the translation process should require only local reorderings.</S>
    <S sid="270" ssid="14">As the experiments have shown, Spanish-English and French-English are examples of such language pairs.</S>
    <S sid="271" ssid="15">For these pairs, the monotone search was found to be sufficient.</S>
    <S sid="272" ssid="16">The phrase-based approach clearly outperformed the singleword based systems.</S>
    <S sid="273" ssid="17">It showed even better performance than the alignment template system.</S>
    <S sid="274" ssid="18">The experiments on the German-English Verbmobil task outlined the limitations of the monotone search.</S>
    <S sid="275" ssid="19">As the low degree of monotonicity indicated, reordering plays an important role on this task.</S>
    <S sid="276" ssid="20">The rather free word order in German as well as the verb group seems to be difficult to translate.</S>
    <S sid="277" ssid="21">Nevertheless, when ignoring the word order and looking at the mPER only, the monotone search is competitive with the best performing system.</S>
    <S sid="278" ssid="22">For further improvements, we will investigate the usefulness of additional models, e.g. modeling the segmentation probability.</S>
    <S sid="279" ssid="23">Also, slightly relaxing the monotonicity constraint in a way that still allows an efficient search is of high interest.</S>
    <S sid="280" ssid="24">In spirit of the IBM reordering constraints of the single-word based models, we could allow a phrase to be skipped and to be translated later.</S>
  </SECTION>
  <SECTION title="Acknowledgment" number="10">
    <S sid="281" ssid="1">This work has been partially funded by the EU project TransType 2, IST-2001-32091.</S>
  </SECTION>
</PAPER>
