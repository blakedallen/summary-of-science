<PAPER>
	<S sid="0">New Ranking Algorithms For Parsing And Tagging: Kernels Over Discrete Structures And The Voted Perceptron</S><ABSTRACT>
		<S sid="1" ssid="1">This paper introduces new learning al gorithms for natural language processing based on the perceptron algorithm.</S>
		<S sid="2" ssid="2">We show how the algorithms can be efficientlyapplied to exponential sized representations of parse trees, such as the ?all sub trees?</S>
		<S sid="3" ssid="3">(DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence.We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named entity extraction from web data.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="4" ssid="4">The perceptron algorithm is one of the oldest algorithms in machine learning, going back to (Rosen blatt 1958).</S>
			<S sid="5" ssid="5">It is an incredibly simple algorithm toimplement, and yet it has been shown to be com petitive with more recent learning methods such as support vector machines ? see (Freund &amp; Schapire 1999) for its application to image classification, for example.</S>
			<S sid="6" ssid="6">This paper describes how the perceptron andvoted perceptron algorithms can be used for pars ing and tagging problems.</S>
			<S sid="7" ssid="7">Crucially, the algorithmscan be efficiently applied to exponential sized repre sentations of parse trees, such as the ?all subtrees?</S>
			<S sid="8" ssid="8">(DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a taggedsentence.</S>
			<S sid="9" ssid="9">It might seem paradoxical to be able to ef ficiently learn and apply a model with an exponential number of features.1 The key to our algorithms is the 1Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper.</S>
			<S sid="10" ssid="10">?kernel?</S>
			<S sid="11" ssid="11">trick ((Cristianini and Shawe-Taylor 2000) discuss kernel methods at length).</S>
			<S sid="12" ssid="12">We describe how the inner product between feature vectors in these representations can be calculated efficiently using dynamic programming algorithms.</S>
			<S sid="13" ssid="13">This leads topolynomial time2 algorithms for training and applying the perceptron.</S>
			<S sid="14" ssid="14">The kernels we describe are related to the kernels over discrete structures in (Haus sler 1999; Lodhi et al 2001).</S>
			<S sid="15" ssid="15">A previous paper (Collins and Duffy 2001) showed improvements over a PCFG in parsing the ATIS task.</S>
			<S sid="16" ssid="16">In this paper we show that the method scales to far more complex domains.</S>
			<S sid="17" ssid="17">In parsing Wall Street Journal text, the method gives a 5.1% relative reduction in error rate over the model of (Collins1999).</S>
			<S sid="18" ssid="18">In the second domain, detecting namedentity boundaries in web data, we show a 15.6% rel ative error reduction (an improvement in F-measure from 85.3% to 87.6%) over a state-of-the-art model, a maximum-entropy tagger.</S>
			<S sid="19" ssid="19">This result is derived using a new kernel, for tagged sequences, described in this paper.</S>
			<S sid="20" ssid="20">Both results rely on a new approach that incorporates the log-probability from a baseline model, in addition to the ?all-fragments?</S>
			<S sid="21" ssid="21">features.</S>
	</SECTION>
	<SECTION title="Feature?Vector Representations of Parse. " number="2">
			<S sid="22" ssid="1">Trees and Tagged SequencesThis paper focuses on the task of choosing the cor rect parse or tag sequence for a sentence from agroup of ?candidates?</S>
			<S sid="23" ssid="2">for that sentence.</S>
			<S sid="24" ssid="3">The candi dates might be enumerated by a number of methods.The experiments in this paper use the top </S></SECTION></PAPER>
