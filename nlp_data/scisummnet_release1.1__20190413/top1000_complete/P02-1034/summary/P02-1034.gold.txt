New Ranking Algorithms For Parsing And Tagging: Kernels Over Discrete Structures And The Voted Perceptron
This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm.
We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the “all subtrees” (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence.
We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.
Convolution kernels are used to implicitly define a tree substructure space.
The tree kernel is proposed for syntactic parsing reranking.
Tree kernels evaluate the similarity between two trees in terms of their overlap, generally measured as the number of common substructures.
