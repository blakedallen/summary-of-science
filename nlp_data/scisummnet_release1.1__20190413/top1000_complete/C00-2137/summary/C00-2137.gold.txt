More Accurate Tests For The Statistical Significance Of Result Differences
Statistical significance testing of differences in values of metrics like recall, precision and balanced F-score is a necessary part of empirical natural language processing.
Unfortunately, we find in a set of experiments that many commonly used tests often underestimate the significance and so are less likely to detect differences that exist between different techniques.
This underestimation comes from an independence assumption that is often violated.
We point out some useful tests that do not make this assumption, including computationally-intensive randomization tests.
Standard deviations for F scores are estimated with bootstrap resampling.
