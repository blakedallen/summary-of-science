[
  {
    "citance_No": 1, 
    "citing_paper_id": "W06-2929", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Markus, Dreyer | David A., Smith | Noah A., Smith", 
    "raw_text": "The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative mini mum risk re ranker (D. Smith and Eisner, 2006)", 
    "clean_text": "The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D08-1076", 
    "citing_paper_authority": 38, 
    "citing_paper_authors": "Wolfgang, Macherey | Franz Josef, Och | Ignacio, Thayer | Jakob, Uszkoreit", 
    "raw_text": "A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface", 
    "clean_text": "A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D10-1059", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Samidh, Chatterjee | Nicola, Cancedda", 
    "raw_text": "Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss orrisk", 
    "clean_text": "Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D08-1065", 
    "citing_paper_authority": 38, 
    "citing_paper_authors": "Roy W., Tromble | Shankar, Kumar | Franz Josef, Och | Wolfgang, Macherey", 
    "raw_text": "Thislinearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score", 
    "clean_text": "This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P09-1067", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Zhifei, Li | Jason M., Eisner | Sanjeev P., Khudanpur", 
    "raw_text": "In the geometric interpolation above, the weight? n controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006) .Lastly, note that Viterbi and variational approximation are different ways to approximate the ex act probability p (y| x), and each of them has pros and cons", 
    "clean_text": "In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P09-1067", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Zhifei, Li | Jason M., Eisner | Sanjeev P., Khudanpur", 
    "raw_text": "We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) 17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W09-0424", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Zhifei, Li | Chris, Callison-Burch | Chris, Dyer | Sanjeev P., Khudanpur | Lane, Schwartz | Wren N. G., Thornton | Jonathan, Weese | Omar F., Zaidan", 
    "raw_text": "We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique)", 
    "clean_text": "Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W11-2130", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Barry, Haddow | Abhishek, Arun | Philipp, Koehn", 
    "raw_text": "Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006)", 
    "clean_text": "Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W11-2119", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Antti-Veikko I., Rosti | Bing, Zhang | Spyros, Matsoukas | Richard M., Schwartz", 
    "raw_text": "The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices", 
    "clean_text": "The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W11-2119", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Antti-Veikko I., Rosti | Bing, Zhang | Spyros, Matsoukas | Richard M., Schwartz", 
    "raw_text": "The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner,2006), and brevity penalty with a differentiable approximation:? (x)= ex? 1 1 +e1000x+ 1 (5) An N-best list represents a subset of the search space and multiple decoding iterations with N-best list merging is required to improve convergence", 
    "clean_text": "The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W08-0304", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Daniel, Cer | Daniel, Jurafsky | Christopher D., Manning", 
    "raw_text": "Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temper8However, we speculate that similar results could be obtained using a uniform distribution over (? 1, 1 )ature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface", 
    "clean_text": "Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W08-0304", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Daniel, Cer | Daniel, Jurafsky | Christopher D., Manning", 
    "raw_text": "The techniques of Zens et al (2007)& amp; Smithand Eisner (2006) regularize by implicitly smoothing over nearby plateaus in the error surface", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N12-1026", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Taro, Watanabe", 
    "raw_text": "In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution", 
    "clean_text": "In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N12-1013", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Veselin, Stoyanov | Jason M., Eisner", 
    "raw_text": "However, it is popular and sometimes faster to do 1-best decoding, so we also include experiments where the test-time system returns a 1 best value of y (or an approximation to this if the CRF is loopy), based on max-product BP inference. Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006)", 
    "clean_text": "Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P11-1071", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Simon, Zwarts | Mark, Johnson", 
    "raw_text": "This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems", 
    "clean_text": "This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P11-1071", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Simon, Zwarts | Mark, Johnson", 
    "raw_text": "Inspired by our evaluation metric, we devised an approximate expected f-score loss function FLoss. FLossT (w)= 1? 2Ew [c] g+ Ew [e] This approximation assumes that the expectations approximately distribute over the division: see Jansche (2005) and Smith and Eisner (2006) for other approximations to expected f-score and methods foroptimising them", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D07-1055", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Richard, Zens | Sa&scaron;a, Hasan | Hermann, Ney", 
    "raw_text": "An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training", 
    "clean_text": "An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D07-1055", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Richard, Zens | Sa&scaron;a, Hasan | Hermann, Ney", 
    "raw_text": "We can there fore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities", 
    "clean_text": "We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "N10-1112", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Kevin, Gimpel | Noah A., Smith", 
    "raw_text": "In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures", 
    "clean_text": "In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W10-1721", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aaron, Phillips", 
    "raw_text": "Cunei? s built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list", 
    "clean_text": "Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list.", 
    "keep_for_gold": 0
  }
]
