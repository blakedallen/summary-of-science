<PAPER>
  <S sid="0">Minimum Risk Annealing For Training Log-Linear Models</S>
  <ABSTRACT>
    <S sid="1" ssid="1">10 restarts 1 restart 793 Optimization Procedure labeled dependency acc.</S>
    <S sid="2" ssid="2">[%] Slovenian Bulgarian Dutch Max. like.</S>
    <S sid="3" ssid="3">27.78 47.23 36.78 Min. error 22.52 54.72 36.78 Ann. min. risk 31.16 54.66 36.71 2: dependency accuracy on parsing 200sentence test corpora, after training 10 experts on 1000 senand fitting their weights 200 more.</S>
    <S sid="4" ssid="4">For Slovenian, minimum risk annealing is significantly better than the other training methods, while minimum error is significantly worse.</S>
    <S sid="5" ssid="5">For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other.</S>
    <S sid="6" ssid="6">For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped.</S>
    <S sid="7" ssid="7">Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do.</S>
    <S sid="8" ssid="8">The orthogonal of Bayes risk decoding achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004).</S>
    <S sid="9" ssid="9">In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000).</S>
    <S sid="10" ssid="10">We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder.</S>
    <S sid="11" ssid="11">Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).</S>
    <S sid="12" ssid="12">Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.</S>
    <S sid="13" ssid="13">The distinction is in using a loss function to calculate the required margins.</S>
    <S sid="14" ssid="14">8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood&#8212;it produces lower-error systems.</S>
    <S sid="15" ssid="15">Different methods can be used to attempt this global, non-convex optimization.</S>
    <S sid="16" ssid="16">We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.</S>
    <S sid="17" ssid="17">It never does significantly worse.</S>
    <S sid="18" ssid="18">With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.</S>
    <S sid="19" ssid="19">References L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer.</S>
    <S sid="20" ssid="20">1988.</S>
    <S sid="21" ssid="21">A new algorithm for the estimation of hidden model parameters.</S>
    <S sid="22" ssid="22">In pages 493&#8211;496.</S>
    <S sid="23" ssid="23">E. Charniak and M. Johnson.</S>
    <S sid="24" ssid="24">2005.</S>
    <S sid="25" ssid="25">Coarse-to-fine n-best and maxent discriminative reranking.</S>
    <S sid="26" ssid="26">In pages 173&#8211;180.</S>
    <S sid="27" ssid="27">S. F. Chen and R. Rosenfeld.</S>
    <S sid="28" ssid="28">1999.</S>
    <S sid="29" ssid="29">A gaussian prior for smoothing maximum entropy models.</S>
    <S sid="30" ssid="30">Technical report, CS Dept., Carnegie Mellon University.</S>
    <S sid="31" ssid="31">K. Crammer, R. McDonald, and F. Pereira.</S>
    <S sid="32" ssid="32">2004.</S>
    <S sid="33" ssid="33">New large algorithms for structured prediction.</S>
    <S sid="34" ssid="34">In Structured Outputs M. Dreyer, D. A. Smith, and N. A. Smith.</S>
    <S sid="35" ssid="35">2006.</S>
    <S sid="36" ssid="36">Vine parsing and minimum risk reranking for speed and precision.</S>
    <S sid="37" ssid="37">In G. Elidan and N. Friedman.</S>
    <S sid="38" ssid="38">2005.</S>
    <S sid="39" ssid="39">Learning hidden variable The information bottleneck approach.</S>
    <S sid="40" ssid="40">6:81&#8211;127.</S>
    <S sid="41" ssid="41">V. Goel and W. J. Byrne.</S>
    <S sid="42" ssid="42">2000.</S>
    <S sid="43" ssid="43">Minimum Bayes-Risk auspeech recognition.</S>
    <S sid="44" ssid="44">Speech and Lan- 14(2):115&#8211;135.</S>
    <S sid="45" ssid="45">J. T. Goodman.</S>
    <S sid="46" ssid="46">1996.</S>
    <S sid="47" ssid="47">Parsing algorithms and metrics.</S>
    <S sid="48" ssid="48">In pages 177&#8211;183.</S>
    <S sid="49" ssid="49">Hinton.</S>
    <S sid="50" ssid="50">1999.</S>
    <S sid="51" ssid="51">Products of experts.</S>
    <S sid="52" ssid="52">In of volume 1, pages 1&#8211;6.</S>
    <S sid="53" ssid="53">K.-U.</S>
    <S sid="54" ssid="54">Hoffgen, H.-U.</S>
    <S sid="55" ssid="55">Simon, and K. S. Van Horn.</S>
    <S sid="56" ssid="56">1995. trainability of single neurons. of Computer and 50(1):114&#8211;125.</S>
    <S sid="57" ssid="57">D. S. Johnson and F. P. Preparata.</S>
    <S sid="58" ssid="58">1978.</S>
    <S sid="59" ssid="59">The densest hemiproblem.</S>
    <S sid="60" ssid="60">Comp.</S>
    <S sid="61" ssid="61">6(93&#8211;107).</S>
    <S sid="62" ssid="62">S. Katagiri, B.-H. Juang, and C.-H. Lee.</S>
    <S sid="63" ssid="63">1998.</S>
    <S sid="64" ssid="64">Pattern recognition using a family of design algorithms based upon the probabilistic descent method.</S>
    <S sid="65" ssid="65">86(11):2345&#8211;2373, November.</S>
    <S sid="66" ssid="66">P. Koehn, F. J. Och, and D. Marcu.</S>
    <S sid="67" ssid="67">2003.</S>
    <S sid="68" ssid="68">Statistical phrasetranslation.</S>
    <S sid="69" ssid="69">In pages 48&#8211;54.</S>
    <S sid="70" ssid="70">S. Kumar and W. Byrne.</S>
    <S sid="71" ssid="71">2004.</S>
    <S sid="72" ssid="72">Minimum bayes-risk decodfor statistical machine translation.</S>
    <S sid="73" ssid="73">In J. Lafferty, A. McCallum, and F. C. N. Pereira.</S>
    <S sid="74" ssid="74">2001.</S>
    <S sid="75" ssid="75">Conditional random fields: Probabilistic models for segmenting labeling sequence data.</S>
    <S sid="76" ssid="76">In F. J. Och.</S>
    <S sid="77" ssid="77">2003.</S>
    <S sid="78" ssid="78">Minimum error rate training in statistical translation.</S>
    <S sid="79" ssid="79">In pages 160&#8211;167.</S>
    <S sid="80" ssid="80">K. Papineni, S. Roukos, T. Ward, and W.-J.</S>
    <S sid="81" ssid="81">Zhu.</S>
    <S sid="82" ssid="82">2002.</S>
    <S sid="83" ssid="83">A method for automatic evaluation of machine In pages 311&#8211;318.</S>
    <S sid="84" ssid="84">K. A. Papineni.</S>
    <S sid="85" ssid="85">1999.</S>
    <S sid="86" ssid="86">Discriminative training via linear In A. Rao and K. Rose.</S>
    <S sid="87" ssid="87">2001.</S>
    <S sid="88" ssid="88">Deterministically annealed deof Hidden Markov Model speech recognizers. on Speech and Audio 9(2):111&#8211;126.</S>
    <S sid="89" ssid="89">K. Rose.</S>
    <S sid="90" ssid="90">1998.</S>
    <S sid="91" ssid="91">Deterministic annealing for clustering, compression, classification, regression, and related optimizaproblems.</S>
    <S sid="92" ssid="92">86(11):2210&#8211;2239.</S>
    <S sid="93" ssid="93">N. A. Smith and J. Eisner.</S>
    <S sid="94" ssid="94">2004.</S>
    <S sid="95" ssid="95">Annealing techniques for statistical language learning.</S>
    <S sid="96" ssid="96">In pages 486&#8211;493.</S>
  </ABSTRACT>
  <SECTION title="1 Direct Minimization of Error" number="1">
    <S sid="97" ssid="1">Researchers in empirical natural language processing have expended substantial ink and effort in developing metrics to evaluate systems automatically against gold-standard corpora.</S>
    <S sid="98" ssid="2">The ongoing evaluation literature is perhaps most obvious in the machine translation community&#8217;s efforts to better BLEU (Papineni et al., 2002).</S>
    <S sid="99" ssid="3">Despite this research, parsing or machine translation systems are often trained using the much simpler and harsher metric of maximum likelihood.</S>
    <S sid="100" ssid="4">One reason is that in supervised training, the log-likelihood objective function is generally convex, meaning that it has a single global maximum that can be easily found (indeed, for supervised generative models, the parameters at this maximum may even have a closed-form solution).</S>
    <S sid="101" ssid="5">In contrast to the likelihood surface, the error surface for discrete structured prediction is not only riddled with local minima, but piecewise constant This work was supported by an NSF graduate research fellowship for the first author and by NSF ITR grant IIS0313193 and ONR grant N00014-01-1-0685.</S>
    <S sid="102" ssid="6">The views expressed are not necessarily endorsed by the sponsors.</S>
    <S sid="103" ssid="7">We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. and not everywhere differentiable with respect to the model parameters (Figure 1).</S>
    <S sid="104" ssid="8">Despite these difficulties, some work has shown it worthwhile to minimize error directly (Och, 2003; Bahl et al., 1988).</S>
    <S sid="105" ssid="9">We show improvements over previous work on error minimization by minimizing the risk or expected error&#8212;a continuous function that can be derived by combining the likelihood with any evaluation metric (&#167;2).</S>
    <S sid="106" ssid="10">Seeking to avoid local minima, deterministic annealing (Rose, 1998) gradually changes the objective function from a convex entropy surface to the more complex risk surface (&#167;3).</S>
    <S sid="107" ssid="11">We also discuss regularizing the objective function to prevent overfitting (&#167;4).</S>
    <S sid="108" ssid="12">We explain how to compute expected loss under some evaluation metrics common in natural language tasks (&#167;5).</S>
    <S sid="109" ssid="13">We then apply this machinery to training log-linear combinations of models for dependency parsing and for machine translation (&#167;6).</S>
    <S sid="110" ssid="14">Finally, we note the connections of minimum risk training to max-margin training and minimum Bayes risk decoding (&#167;7), and recapitulate our results (&#167;8).</S>
  </SECTION>
  <SECTION title="2 Training Log-Linear Models" number="2">
    <S sid="111" ssid="1">In this work, we focus on rescoring with loglinear models.</S>
    <S sid="112" ssid="2">In particular, our experiments consider log-linear combinations of a relatively small number of features over entire complex structures, such as trees or translations, known in some previous work as products of experts (Hinton, 1999) or logarithmic opinion pools (Smith et al., 2005).</S>
    <S sid="113" ssid="3">A feature in the combined model might thus be a log probability from an entire submodel.</S>
    <S sid="114" ssid="4">Giving this feature a small or negative weight can discount a submodel that is foolishly structured, badly trained, or redundant with the other features.</S>
    <S sid="115" ssid="5">For each sentence xi in our training corpus S, we are given Ki possible analyses yi,i, ... yi,K,.</S>
    <S sid="116" ssid="6">(These may be all of the possible translations or parse trees; or only the Ki most probable under some other model; or only a random sample of size Ki.)</S>
    <S sid="117" ssid="7">Each analysis has a vector of real-valued features (i.e., factors, or experts) denoted fi,k.</S>
    <S sid="118" ssid="8">The score of the analysis yi,k is &#952; &#183; fi,k, the dot product of its features with a parameter vector &#952;.</S>
    <S sid="119" ssid="9">For each sentence, we obtain a normalized probability distribution over the Ki analyses as We wish to adjust this model&#8217;s parameters &#952; to minimize the severity of the errors we make when using it to choose among analyses.</S>
    <S sid="120" ssid="10">A loss function Ly*(y) assesses a penalty for choosing y when y&#8727; is correct.</S>
    <S sid="121" ssid="11">We will usually write this simply as L(y) since y&#8727; is fixed and clear from context.</S>
    <S sid="122" ssid="12">For clearer exposition, we assume below that the total loss over some test corpus is the sum of the losses on individual sentences, although we will revisit that assumption in &#167;5.</S>
    <S sid="123" ssid="13">One training criterion directly mimics test conditions.</S>
    <S sid="124" ssid="14">It looks at the loss incurred if we choose the best analysis of each xi according to the model: Since small changes in &#952; either do not change the best analysis or else push a different analysis to the top, this objective function is piecewise constant, hence not amenable to gradient descent.</S>
    <S sid="125" ssid="15">Och (2003) observed, however, that the piecewiseconstant property could be exploited to characterize the function exhaustively along any line in parameter space, and hence to minimize it globally along that line.</S>
    <S sid="126" ssid="16">By calling this global line minimization as a subroutine of multidimensional optimization, he was able to minimize (2) well enough to improve over likelihood maximization for training factored machine translation systems.</S>
    <S sid="127" ssid="17">Instead of considering only the best hypothesis for any &#952;, we can minimize risk, i.e., the expected loss under p&#952; across all analyses yi: This &#8220;smoothed&#8221; objective is now continuous and differentiable.</S>
    <S sid="128" ssid="18">However, it no longer exactly mimics test conditions, and it typically remains nonconvex, so that gradient descent is still not guaranteed to find a global minimum.</S>
    <S sid="129" ssid="19">Och (2003) found that such smoothing during training &#8220;gives almost identical results&#8221; on translation metrics.</S>
    <S sid="130" ssid="20">The simplest possible loss function is 0/1 loss, where L(y) is 0 if y is the true analysis y&#8727;i and 1 otherwise.</S>
    <S sid="131" ssid="21">This loss function does not attempt to give partial credit.</S>
    <S sid="132" ssid="22">Even in this simple case, assuming P =6 NP, there exists no general polynomial-time algorithm for even approximating (2) to within any constant factor, even for Ki = 2 (Hoffgen et al., 1995, from Theorem 4.10.4).1 The same is true for for (3), since for Ki = 2 it can be easily shown that the min 0/1 risk is between 50% and 100% of the min 0/1 loss.</S>
    <S sid="133" ssid="23">Rather than minimizing a loss function suited to the task, many systems (especially for language modeling) choose simply to maximize the probability of the gold standard.</S>
    <S sid="134" ssid="24">The log of this likelihood is a convex function of the parameters &#952;: where y&#8727;i is the true analysis of sentence xi.</S>
    <S sid="135" ssid="25">The only wrinkle is that p&#952;(y&#8727;i  |xi) may be left undefined by equation (1) if y&#8727;i is not in our set of Ki hypotheses.</S>
    <S sid="136" ssid="26">When maximizing likelihood, therefore, we will replace y&#8727;i with the min-loss analysis in the hypothesis set; if multiple analyses tie 1Known algorithms are exponential but only in the dimensionality of the feature space (Johnson and Preparata, 1978). for this honor, we follow Charniak and Johnson (2005) in summing their probabilities.2 Maximizing (4) is equivalent to minimizing an upper bound on the expected 0/1 loss Ei(1 &#8722; p&#952;(yi  |xi)).</S>
    <S sid="137" ssid="27">Though the log makes it tractable, this remains a 0/1 objective that does not give partial credit to wrong answers, such as imperfect but useful translations.</S>
    <S sid="138" ssid="28">Most systems should be evaluated and preferably trained on less harsh metrics.</S>
  </SECTION>
  <SECTION title="3 Deterministic Annealing" number="3">
    <S sid="139" ssid="1">To balance the advantages of direct loss minimization, continuous risk minimization, and convex optimization, deterministic annealing attempts the solution of increasingly difficult optimization problems (Rose, 1998).</S>
    <S sid="140" ssid="2">Adding a scale hyperparameter &#947; to equation (1), we have the following family of distributions: When &#947; = 0, all yi,k are equally likely, giving the uniform distribution; when &#947; = 1, we recover the model in equation (1); and as &#947; &#8212;* oc, we approach the winner-take-all Viterbi function that assigns probability 1 to the top-scoring analysis.</S>
    <S sid="141" ssid="3">For a fixed &#947;, deterministic annealing solves 2An alternative would be to artificially add yz (e.g., the reference translation(s)) to the hypothesis set during training.</S>
    <S sid="142" ssid="4">We then increase &#947; according to some schedule and optimize &#952; again.</S>
    <S sid="143" ssid="5">When &#947; is low, the smooth objective might allow us to pass over local minima that could open up at higher &#947;.</S>
    <S sid="144" ssid="6">Figure 3 shows how the smoothing is gradually weakened to reach the risk objective (3) as &#947; &#8212;* 1 and approach the true error objective (2) as &#947; &#8212;* oc.</S>
    <S sid="145" ssid="7">Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate.</S>
    <S sid="146" ssid="8">Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004).</S>
    <S sid="147" ssid="9">Other work on &#8220;generalized probabilistic descent&#8221; minimizes a similar objective function but with &#947; held constant (Katagiri et al., 1998).</S>
    <S sid="148" ssid="10">Although the entropy is generally higher at lower values of &#947;, it varies as the optimization changes &#952;.</S>
    <S sid="149" ssid="11">In particular, a pure unregularized loglinear model such as (5) is really a function of &#947;&#183;&#952;, so the optimizer could exactly compensate for increased &#947; by decreasing the &#952; vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose &#947; and &#952; accordingly: min Ep-Y,e[L(yi,k)] &#8722; T &#183; H(p&#947;,&#952;) (7) &#947;,&#952; In place of a schedule for raising &#947;, we now use a cooling schedule to lower T from oc to &#8722;oc, thereby weakening the preference for high entropy.</S>
    <S sid="150" ssid="12">The Lagrange multiplier T on entropy is called &#8220;temperature&#8221; due to a satisfying connection to statistical mechanics.</S>
    <S sid="151" ssid="13">Once T is quite cool, it is common in practice to switch to raising &#947; directly and rapidly (quenching) until some convergence criterion is met (Rao and Rose, 2001).</S>
  </SECTION>
  <SECTION title="4 Regularization" number="4">
    <S sid="152" ssid="1">Informally, high temperature or &#947; &lt; 1 smooths our model during training toward higher-entropy conditional distributions that are not so peaked at the desired analyses y* .</S>
    <S sid="153" ssid="2">Another reason for such smoothing is simply to prevent overfitting to these training examples.</S>
    <S sid="154" ssid="3">A typical way to control overfitting is to use a quadratic regularizing term, ||&#952;||2 or more generally Ed &#952;2d/2&#963;2d.</S>
    <S sid="155" ssid="4">Keeping this small keeps weights low and entropy high.</S>
    <S sid="156" ssid="5">We may add this regularizer to equation (6) or (7).</S>
    <S sid="157" ssid="6">In the maximum likelihood framework, we may subtract it from equation (4), which is equivalent to maximum a posteriori estimation with a diagonal Gaussian prior (Chen and Rosenfeld, 1999).</S>
    <S sid="158" ssid="7">The variance a2d may reflect a prior belief about the potential usefulness of feature d, or may be tuned on heldout data.</S>
    <S sid="159" ssid="8">Another simple regularization method is to stop cooling before T reaches 0 (cf.</S>
    <S sid="160" ssid="9">Elidan and Friedman (2005)).</S>
    <S sid="161" ssid="10">If loss on heldout data begins to increase, we may be starting to overfit.</S>
    <S sid="162" ssid="11">This technique can be used along with annealing or quadratic regularization and can achieve additional accuracy gains, which we report elsewhere (Dreyer et al., 2006).</S>
  </SECTION>
  <SECTION title="5 Computing Expected Loss" number="5">
    <S sid="163" ssid="1">At each temperature setting of deterministic annealing, we need to minimize the expected loss on the training corpus.</S>
    <S sid="164" ssid="2">We now discuss how this expectation is computed.</S>
    <S sid="165" ssid="3">When rescoring, we assume that we simply wish to combine, in some way, statistics of whole sentences4 to arrive at the overall loss for the corpus.</S>
    <S sid="166" ssid="4">We consider evaluation metrics for natural language tasks from two broadly applicable classes: linear and nonlinear.</S>
    <S sid="167" ssid="5">A linear metric is a sum (or other linear combination) of the loss or gain on individual sentences.</S>
    <S sid="168" ssid="6">Accuracy&#8212;in dependency parsing, part-of-speech tagging, and other labeling tasks&#8212;falls into this class, as do recall, word error rate in ASR, and the crossing-brackets metric in parsing.</S>
    <S sid="169" ssid="7">Thanks to the linearity of expectation, we can easily compute our expected loss in equation (6) by adding up the expected loss on each sentence.</S>
    <S sid="170" ssid="8">Some other metrics involve nonlinear combinations over the sentences of the corpus.</S>
    <S sid="171" ssid="9">One common example is precision, P def = Pi ci/Pi ai, where ci is the number of correctly posited elements, and ai is the total number of posited elements, in the decoding of sentence i.</S>
    <S sid="172" ssid="10">(Depending on the task, the elements may be words, bigrams, labeled constituents, etc.)</S>
    <S sid="173" ssid="11">Our goal is to maximize P, so during a step of deterministic annealing, we need to maximize the expectation of P when the sentences are decoded randomly according to equation (5).</S>
    <S sid="174" ssid="12">Although this expectation is continuous and differentiable as a function of 0, unfortunately it seems hard to compute for any given 0.</S>
    <S sid="175" ssid="13">We observe however that an equivalent goal is to minimize &#8722; log P. Taking that as our loss function instead, equation (6) now needs to minimize the expectation of &#8722; log P,5 which decomposes somewhat more nicely: = E[log A] &#8722; E[log C] (8) where the integer random variables A = Pi ai and C = Pi ci count the number of posited and correctly posited elements over the whole corpus.</S>
    <S sid="176" ssid="14">To approximate E[g(A)], where g is any twicedifferentiable function (here g = log), we can approximate g locally by a quadratic, given by the Taylor expansion of g about A&#8217;s mean &#181;A = E[A]: Here &#181;A = Pi &#181;ai and Q2A = Pi Q2ai, since A is a sum of independent random variables ai (i.e., given the current model parameters 0, our randomized decoder decodes each sentence independently).</S>
    <S sid="177" ssid="15">In other words, given our quadratic approximation to g, E[g(A)] depends on the (true) distribution of A only through the single-sentence means &#181;ai and variances a2ai, which can be found by enumerating the Ki decodings of sentence i.</S>
    <S sid="178" ssid="16">The approximation becomes arbitrarily good as we anneal -y &#8212;* oc, since then Q2A &#8212;* 0 and E[g(A)] focuses on g near &#181;A.</S>
    <S sid="179" ssid="17">For equation (8), and E[log C] is found similarly.</S>
    <S sid="180" ssid="18">Similar techniques can be used to compute the expected logarithms of some other non-linear metrics, such as F-measure (the harmonic mean of precision and recall)6 and Papineni et al. (2002)&#8217;s BLEU translation metric (the geometric mean of several precisions).</S>
    <S sid="181" ssid="19">In particular, the expectation of log BLEU distributes over its N + 1 summands: where Pn is the precision of the n-gram elements in the decoding.7 As is standard in MT research, we take wn = 1/N and N = 4.</S>
    <S sid="182" ssid="20">The first term in the BLEU score is the log brevity penalty, a continuous function of A1 (the total number of unigram tokens in the decoded corpus) that fires only if A1 &lt; r (the average word count of the reference corpus).</S>
    <S sid="183" ssid="21">We again use a Taylor series to approximate the expected log brevity penalty.</S>
    <S sid="184" ssid="22">We mention an alternative way to compute (say) the expected precision C/A: integrate numerically over the joint density of C and A.</S>
    <S sid="185" ssid="23">How can we obtain this density?</S>
    <S sid="186" ssid="24">As (C, A) = Ei(ci, ai) is a sum of independent random length-2 vectors, its mean vector and 2 x 2 covariance matrix can be respectively found by summing the means and covariance matrices of the (ci, ai), each exactly computed from the distribution (5) over Ki hypotheses.</S>
    <S sid="187" ssid="25">We can easily approximate (C, A) by the (continuous) bivariate normal with that mean and covariance matrix8&#8212;or else accumulate an exact representation of its (discrete) probability mass function by a sequence of numerical convolutions.</S>
  </SECTION>
  <SECTION title="6 Experiments" number="6">
    <S sid="188" ssid="1">We tested the above training methods on two different tasks: dependency parsing and phrasebased machine translation.</S>
    <S sid="189" ssid="2">Since the basic setup was the same for both, we outline it here before describing the tasks in detail.</S>
    <S sid="190" ssid="3">In both cases, we start with 8 to 10 models (the &#8220;experts&#8221;) already trained on separate training data.</S>
    <S sid="191" ssid="4">To find the optimal coefficients 0 for a loglinear combination of these experts, we use separate development data, using the following procedure due to Och (2003): Our experiments simply compare three procedures at step 4.</S>
    <S sid="192" ssid="5">We may either Since these different optimization procedures will usually find different 0 at step 4, their K-best lists will diverge after the first iteration.</S>
    <S sid="193" ssid="6">For final testing, we selected among several variants of each procedure using a separate small heldout set.</S>
    <S sid="194" ssid="7">Final results are reported for a larger, disjoint test set.</S>
    <S sid="195" ssid="8">For our machine translation experiments, we trained phrase-based alignment template models of Finnish-English, French-English, and GermanEnglish, as follows.</S>
    <S sid="196" ssid="9">For each language pair, we aligned 100,000 sentence pairs from European Parliament transcripts using GIZA++.</S>
    <S sid="197" ssid="10">We then used Philip Koehn&#8217;s phrase extraction software to merge the GIZA++ alignments and to extract and score the alignment template model&#8217;s phrases (Koehn et al., 2003).</S>
    <S sid="198" ssid="11">The Pharaoh phrase-based decoder uses precisely the setup of this paper.</S>
    <S sid="199" ssid="12">It scores a candidate translation (including its phrasal alignment to the original text) as 0 &#8226; f, where f is a vector of the following 8 features: Our goal was to train the weights 0 of these 8 features.</S>
    <S sid="200" ssid="13">We used the method described above, employing the Pharaoh decoder at step 2 to generate the 200-best translations according to the current 0.</S>
    <S sid="201" ssid="14">As explained above, we compared three procedures at step 4: maximum log-likelihood by gradient ascent; minimum error using Och&#8217;s linesearch method; and annealed minimum risk.</S>
    <S sid="202" ssid="15">As our development data for training 0, we used 200 sentence pairs for each language pair.</S>
    <S sid="203" ssid="16">Since our methods can be tuned with hyperparameters, we used performance on a separate 200sentence held-out set to choose the best hyperparameter values.</S>
    <S sid="204" ssid="17">The hyperparameter levels for each method were distribution on [&#8722;1, 1] x [&#8722;1, 1] x &#8226; &#8226; &#8226; , when optimizing 0 at an iteration of step 4.10 by half at each step; then we quenched by doubling -y at each step.</S>
    <S sid="205" ssid="18">(We also ran experiments with quadratic regularization with all Qd at 0.5, 1, or 2 (&#167;4) in addition to the entropy constraint.</S>
    <S sid="206" ssid="19">Also, instead of the entropy constraint, we simply annealed on -y while adding a quadratic regularization term.</S>
    <S sid="207" ssid="20">None of these regularized models beat the best setting of standard deterministic annealing on heldout or test data.)</S>
    <S sid="208" ssid="21">Final results on a separate 2000-sentence test set are shown in table 1.</S>
    <S sid="209" ssid="22">We evaluated translation using BLEU with one reference translation and ngrams up to 4.</S>
    <S sid="210" ssid="23">The minimum risk annealing procedure significantly outperformed maximum likelihood and minimum error training in all three language pairs (p &lt; 0.001, paired-sample permutation test with 1000 replications).</S>
    <S sid="211" ssid="24">Minimum risk annealing generally outperformed minimum error training on the held-out set, regardless of the starting temperature T. However, higher starting temperatures do give better performance and a more monotonic learning curve (Figure 3), a pattern that held up on test data.</S>
    <S sid="212" ssid="25">(In the same way, for minimum error training, 10That is, we run step 4 from several starting points, finishing at several different points; we pick the finishing point with lowest development error (2).</S>
    <S sid="213" ssid="26">This reduces the sensitivity of this method to the starting value of 0.</S>
    <S sid="214" ssid="27">Maximum likelihood is not sensitive to the starting value of 0 because it has only a global optimum; annealed minimum risk is not sensitive to it either, because initially -y Pz&#65533; 0, making equation (6) flat. more random restarts give better performance and a more monotonic learning curve&#8212;see Figure 4.)</S>
    <S sid="215" ssid="28">Minimum risk annealing did not always win on the training set, suggesting that its advantage is not superior minimization but rather superior generalization: under the risk criterion, multiple lowloss hypotheses per sentence can help guide the learner to the right part of parameter space.</S>
    <S sid="216" ssid="29">Although the components of the translation and language models interact in complex ways, the improvement on Finnish-English may be due in part to the higher weight that minimum risk annealing found for the word penalty.</S>
    <S sid="217" ssid="30">That system is therefore more likely to produce shorter output like i have taken note of your remarks and i also agree with that . than like this longer output from the minimum-error-trained system: i have taken note ofyour remarks and i shall also agree with all that the union.</S>
    <S sid="218" ssid="31">We annealed using our novel expected-BLEU approximation from &#167;5.</S>
    <S sid="219" ssid="32">We found this to perform significantly better on BLEU evaluation than if we trained with a &#8220;linearized&#8221; BLEU that summed per-sentence BLEU scores (as used in minimum Bayes risk decoding by Kumar and Byrne (2004)).</S>
    <S sid="220" ssid="33">We trained dependency parsers for three different languages: Bulgarian, Dutch, and Slovenian.11 Input sentences to the parser were already tagged for parts of speech.</S>
    <S sid="221" ssid="34">Each parser employed 10 experts, each parameterized as a globally normalized loglinear model (Lafferty et al., 2001).</S>
    <S sid="222" ssid="35">For example, the 9th component of the feature vector fz&#65533;k (which described the kth parse of the ith sentence) was the log of that parse&#8217;s normalized probability according to the 9th expert.</S>
    <S sid="223" ssid="36">Each expert was trained separately to maximize the conditional probability of the correct parse given the sentence.</S>
    <S sid="224" ssid="37">We used 10 iterations of gradient ascent.</S>
    <S sid="225" ssid="38">To speed training, for each of the first 9 iterations, the gradient was estimated on a (different) sample of only 1000 training sentences.</S>
    <S sid="226" ssid="39">We then trained the vector 0, used to combine the experts, to minimize the number of labeled dependency attachment errors on a 200-sentence development set.</S>
    <S sid="227" ssid="40">Optimization proceeded over lists of the 200-best parses of each sentence produced by a joint decoder using the 10 experts.</S>
    <S sid="228" ssid="41">Evaluating on labeled dependency accuracy on 200 test sentences for each language, we see that minimum error and annealed minimum risk training are much closer than for MT.</S>
    <S sid="229" ssid="42">For Bulgarian and Dutch, they are statistically indistinguishable using a paired-sample permutations test with 1000 replications.</S>
    <S sid="230" ssid="43">Indeed, on Dutch, all three optimization procedures produce indistinguishable results.</S>
    <S sid="231" ssid="44">On Slovenian, annealed minimum risk training does show a significant improvement over the other two methods.</S>
    <S sid="232" ssid="45">Overall, however, the results for this task are mediocre.</S>
    <S sid="233" ssid="46">We are still working on improving the underlying experts.</S>
  </SECTION>
  <SECTION title="7 Related Work" number="7">
    <S sid="234" ssid="1">We have seen that annealed minimum risk training provides a useful alternative to maximum likelihood and minimum error training.</S>
    <S sid="235" ssid="2">In our experiments, it never performed significantly worse 11For information on these corpora, see the CoNLL-X shared task on multilingual dependency parsing: http: //nextens.uvt.nl/~conll/. than either and in some cases significantly helped.</S>
    <S sid="236" ssid="3">Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do.</S>
    <S sid="237" ssid="4">The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004).</S>
    <S sid="238" ssid="5">In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000).</S>
    <S sid="239" ssid="6">We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder.</S>
    <S sid="240" ssid="7">Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).</S>
    <S sid="241" ssid="8">Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.</S>
    <S sid="242" ssid="9">The distinction is in using a loss function to calculate the required margins.</S>
  </SECTION>
  <SECTION title="8 Conclusions" number="8">
    <S sid="243" ssid="1">Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood&#8212;it produces lower-error systems.</S>
    <S sid="244" ssid="2">Different methods can be used to attempt this global, non-convex optimization.</S>
    <S sid="245" ssid="3">We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.</S>
    <S sid="246" ssid="4">It never does significantly worse.</S>
    <S sid="247" ssid="5">With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.</S>
  </SECTION>
</PAPER>
