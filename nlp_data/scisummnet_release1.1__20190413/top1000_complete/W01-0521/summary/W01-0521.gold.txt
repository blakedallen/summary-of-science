Corpus Variation And Parser Performance
Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.
While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora.
We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.
This leads us to a technique for pruning parameters to reduce the size of the parsing model.
We show that the accuracy of parsers trained on the Penn Treebank degrades when applied to different genres and domains.
We report results on sentences of 40 or less words on all the Brown corpus sections combined, for which we obtain 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus.
