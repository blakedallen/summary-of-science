<PAPER>
  <S sid="0">A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins.</S>
    <S sid="2" ssid="2">We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees.</S>
    <S sid="3" ssid="3">Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">Dependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning.</S>
    <S sid="5" ssid="2">Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010).</S>
    <S sid="6" ssid="3">Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011).</S>
    <S sid="7" ssid="4">Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011).</S>
    <S sid="8" ssid="5">It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger.</S>
    <S sid="9" ssid="6">This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which not only can perform their own part-of-speech tagging but normally give better parsing accuracy when they are allowed to do so.</S>
    <S sid="10" ssid="7">This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing.</S>
    <S sid="11" ssid="8">It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other.</S>
    <S sid="12" ssid="9">Thus, Lee et al. (2011) show that a discriminative model for joint morphological disambiguation and dependency parsing outperforms a pipeline model in experiments on Latin, Ancient Greek, Czech and Hungarian.</S>
    <S sid="13" ssid="10">However, Li et al. (2011) and Hatori et al.</S>
    <S sid="14" ssid="11">(2011) report improvements with a joint model also for Chinese, which is not a richly inflected language but is nevertheless rich in part-of-speech ambiguities.</S>
    <S sid="15" ssid="12">In this paper, we present a transition-based model for joint part-of-speech tagging and labeled dependency parsing with non-projective trees.</S>
    <S sid="16" ssid="13">Experiments show that joint modeling improves both tagging and parsing accuracy, leading to state-of-the-art accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English.</S>
    <S sid="17" ssid="14">To our knowledge, this is the first joint system that performs labeled dependency parsing.</S>
    <S sid="18" ssid="15">It is also the first joint system that achieves state-of-the-art accuracy for non-projective dependency parsing.</S>
  </SECTION>
  <SECTION title="2 Transition-Based Tagging and Parsing" number="2">
    <S sid="19" ssid="1">Transition-based dependency parsing was pioneered by Yamada and Matsumoto (2003) and Nivre et al. (2004), who used classifiers trained to predict individual actions of a deterministic shift-reduce parser.</S>
    <S sid="20" ssid="2">Recent research has shown that better accuracy can be achieved by using beam search and optimizing models on the entire sequence of decisions needed to parse a sentence instead of single actions (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Clark, 2011; Zhang and Nivre, 2011; Bohnet, 2011).</S>
    <S sid="21" ssid="3">In addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which were beyond the scope of early systems (Attardi, 2006; Nivre, 2007; Nivre, 2009; Titov et al., 2009).</S>
    <S sid="22" ssid="4">In this section, we start by defining a transition system for joint tagging and parsing based on the non-projective transition system proposed in Nivre (2009).</S>
    <S sid="23" ssid="5">We then show how to perform beam search and structured online learning with this model, and conclude by discussing feature representations.</S>
    <S sid="24" ssid="6">Given a set P of part-of-speech tags and a set D of dependency labels, a tagged dependency tree for a sentence x = w1, ... , wn is a directed tree T = (Vx, A) with labeling functions 7r and 6 such that: The set Vx of nodes is the set of positive integers up to and including n, each corresponding to the linear position of a word in the sentence, plus an extra artificial root node 0.</S>
    <S sid="25" ssid="7">The set A of arcs is a set of pairs (i, j), where i is the head node and j is the dependent node.</S>
    <S sid="26" ssid="8">The functions 7r and 6 assign a unique part-of-speech label to each node/word and a unique dependency label to each arc, respectively.</S>
    <S sid="27" ssid="9">This notion of dependency tree differs from the standard definition only by including part-of-speech labels as well as dependency labels (K&#168;ubler et al., 2009).</S>
    <S sid="28" ssid="10">Following Nivre (2008), we define a transition system for dependency parsing as a quadruple 5 = (C, T, cs, Ct), where A transition sequence for a sentence x in 5 is a sequence of configuration-transition pairs C0,m = In this paper, we take the set C of configurations to be the set of all 5-tuples c = (E, B, A, 7r, 6) such that E (the stack) and B (the buffer) are disjoint sublists of the nodes Vx of some sentence x, A is a set of dependency arcs over Vx, and 7r and 6 are labeling functions as defined above.</S>
    <S sid="29" ssid="11">We take the initial configuration for a sentence x = w1, ... , wn to be cs(x) = ([0], [1, ... , n], 11, 1, 1), where L is the function that is undefined for all arguments, and we take the set Ct of terminal configurations to be the set of all configurations of the form c = ([0], [ ], A, 7r, 6) (for any A, 7r and 6).</S>
    <S sid="30" ssid="12">The tagged dependency tree defined for x by c = (E, B, A, 7r, 6) is the tree (Vx, A) with labeling functions 7r and 6, which we write TREE(x, c).</S>
    <S sid="31" ssid="13">The set T of transitions is shown in Figure 1.</S>
    <S sid="32" ssid="14">The LEFT-ARCd and RIGHT-ARCd transitions both add an arc (with dependency label d) between the two nodes on top of the stack and replaces these nodes by the head node of the new arc (which is the rightmost node for LEFT-ARCd and the leftmost node for RIGHT-ARCd).</S>
    <S sid="33" ssid="15">The SHIFTp transition extracts the first node in the buffer, pushes it onto the stack and labels it with the part-of-speech tag p. The SWAP transition extracts the second topmost node from the stack and moves it back to the buffer, subject to the condition that the two top nodes on the stack are still in the order given by the sentence.</S>
    <S sid="34" ssid="16">Except for the addition of a tag parameter p to the SHIFT transition, this is equivalent to the system described in Nivre (2009), which thanks to the SWAP transition can handle arbitrary non-projective trees.</S>
    <S sid="35" ssid="17">The soundness and completeness results given in that paper trivially carry over to the new system.</S>
    <S sid="36" ssid="18">The only thing to note is that, before a terminal configuration can be reached, every word has to be pushed onto the stack in a SHIFTp transition, which ensures that every node/word in the output tree will be tagged.</S>
    <S sid="37" ssid="19">While early transition-based parsers generally used greedy best-first inference and locally trained classifiers, recent work has shown that higher accuracy can be obtained using beam search and global structure learning to mitigate error propagation.</S>
    <S sid="38" ssid="20">In particular, it seems that the globally learned models can exploit a much richer feature space than locally trained classifiers, as shown by Zhang and Nivre (2011).</S>
    <S sid="39" ssid="21">Since joint tagging and parsing increases the size of the search space and is likely to require novel features, we use beam search in combination with structured perceptron learning.</S>
    <S sid="40" ssid="22">The beam search algorithm used to derive the best parse y for a sentence x is outlined in Figure 2.</S>
    <S sid="41" ssid="23">In addition to the sentence x, it takes as input a weight vector w corresponding to a linear model for scoring transitions out of configurations and two prunw and beam parameters b1 and b2.</S>
    <S sid="42" ssid="24">The symbols h.c, h.s and h.f denote, respectively, the configuration, score and feature representation of a hypothesis h; h.c.A denotes the arc set of h.c. ing parameters b1 and b2.</S>
    <S sid="43" ssid="25">A parse hypothesis h is represented by a configuration h.c, a score h.s and a feature vector h.f for the transition sequence up to h.c.</S>
    <S sid="44" ssid="26">Hypotheses are stored in the list BEAM, which is sorted by descending scores and initialized to hold the hypothesis h0 corresponding to the initial configuration cs(x) with score 0.0 and all features set to 0.0 (lines 1&#8211;4).</S>
    <S sid="45" ssid="27">In the main loop (lines 5&#8211;13), a set of new hypotheses is derived and stored in the list TMP, which is finally pruned and assigned as the new value of BEAM.</S>
    <S sid="46" ssid="28">The main loop terminates when all hypotheses in BEAM contain terminal configurations, and the dependency tree extracted from the top scoring hypothesis is returned (lines 14&#8211;16).</S>
    <S sid="47" ssid="29">The set of new hypotheses is created in two nested loops (lines 7&#8211;12), where every hypothesis h in BEAM is updated using every permissible transition t for the configuration h.c.</S>
    <S sid="48" ssid="30">The feature representation of the new hypothesis is obtained by adding the feature vector f(t, h.c) for the current configurationtransition pair to the feature vector of the old hypothesis (line 9).</S>
    <S sid="49" ssid="31">Similarly, the score of the new hypothesis is the sum of the score f(t, h.c) &#183; w of the current configuration-transition pair and the score of the old hypothesis (line 10).</S>
    <S sid="50" ssid="32">The feature representation/score of a complete parse y for x with transition sequence C0,m is thus the sum of the feature representations/scores of the configurationtransition pairs in C0,m: Finally, the configuration of the new hypothesis is obtained by evaluating t(h.c) (line 11).</S>
    <S sid="51" ssid="33">The new hypothesis is then inserted into TMP in score-sorted order (line 12).</S>
    <S sid="52" ssid="34">The pruning parameters b1 and b2 determine the number of hypotheses allowed in the beam and at the same time control the tradeoff between syntactic and morphological ambiguity.</S>
    <S sid="53" ssid="35">First, we extract the b1 highest scoring hypotheses with distinct dependency trees.</S>
    <S sid="54" ssid="36">Then we extract the b2 highest scoring remaining hypotheses, which will typically be tagging variants of dependency trees that are already in the beam.</S>
    <S sid="55" ssid="37">In this way, we prevent the beam from getting filled up with too many tagging variants of the same dependency tree, which was found to be harmful in preliminary experiments.</S>
    <S sid="56" ssid="38">One final thing to note about the inference algorithm is that the notion of permissibility for a transition t out of a configuration c can be used to capture not only formal constraints on transitions &#8211; such as the fact that it is impossible to perform a SHIFTp transition with an empty buffer or illegal to perform a LEFT-ARCd transition with the special root node on top of the stack &#8211; but also to filter out unlikely dependency labels or tags.</S>
    <S sid="57" ssid="39">Thus, in the experiments later on, we will typically constrain the parser so that SHIFTp is permissible only if p is one of the k best part-of-speech tags with a score no more than &#945; below the score of the 1-best tag, as determined by a preprocessing tagger.</S>
    <S sid="58" ssid="40">We also filter out instances of LEFT-ARCd and RIGHT-ARCd, where d does not occur in the training data for the predicted part-ofspeech tag combination of the head and dependent.</S>
    <S sid="59" ssid="41">This procedure leads to a significant speed up.</S>
    <S sid="60" ssid="42">In order to learn a weight vector w from a training set {(xj, yj)1 j=1 of sentences with their tagged dependency trees, we use a variant of the structured perceptron, introduced by Collins (2002), which makes N iterations over the training data and updates the weight vector for every sentence xj where the highest scoring parse y* is different from yj.</S>
    <S sid="61" ssid="43">More precisely, we use the passive-aggressive update of Crammer et al. (2006): where We also use the early update strategy found beneficial for parsing in several previous studies (Collins and Roark, 2004; Zhang and Clark, 2008; Huang and Sagae, 2010), which means that, during learning, we terminate the beam search as soon as the hypothesis corresponding to the gold parse yj falls out of the beam and update with respect to the partial transition sequence constructed up to that point.</S>
    <S sid="62" ssid="44">Finally, we use the standard technique of averaging over all weight vectors, as originally proposed by Collins (2002).</S>
    <S sid="63" ssid="45">As already noted, the feature representation f(x, y) of an input sentence x with parse y decomposes into feature representations f(c, t) for the transitions t(c) needed to derive y from cs(x).</S>
    <S sid="64" ssid="46">Features may refer to any aspect of a configuration, as encoded in the stack E, the buffer B, the arc set A and the labelings 7r and S. In addition, we assume that each word w in the input is assigned up to k candidate part-of-speech tags 7ri(w) with corresponding scores s(7ri(w)). use Ei and Bi to denote the ith token in the stack E and buffer B, respectively, with indexing starting at 0, and we use the following functors to extract properties of a token: &#960;i() = ith best tag; s(&#960;i()) = score of ith best tag; &#960;() = finally predicted tag; w() = word form; pi() = word prefix of i characters; si() = word suffix of i characters.</S>
    <S sid="65" ssid="47">Score differences are binned in discrete steps of 0.05.</S>
    <S sid="66" ssid="48">The bulk of features used in our system are taken from Zhang and Nivre (2011), although with two important differences.</S>
    <S sid="67" ssid="49">First of all, like Hatori et al. (2011), we have omitted all features that presuppose an arc-eager parsing order, since our transition system defines an arc-standard order.</S>
    <S sid="68" ssid="50">Secondly, any feature that refers to the part-of-speech tag of a word w in the buffer B will in our system refer to the topscoring tag &#960;1(w), rather than the finally predicted tag.</S>
    <S sid="69" ssid="51">By contrast, for a word in the stack E, part-ofspeech features refer to the tag &#960;(w) chosen when shifting w onto the stack (which may or may not be the same as &#960;1(w)).</S>
    <S sid="70" ssid="52">In addition to the standard features for transitionbased dependency parsing, we have added features specifically to improve the tagging step in the joint model.</S>
    <S sid="71" ssid="53">The templates for these features, which are specified in Figure 3, all involve the ith best tag assigned to the first word of the buffer B (the next word to be shifted in a SHIFTP transition) in combination with neighboring words, word prefixes, word suffixes, score differences and tag rank.</S>
    <S sid="72" ssid="54">Finally, in some experiments, we make use of two additional feature sets, which we call graph features (G) and cluster features (C), respectively.</S>
    <S sid="73" ssid="55">Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008).</S>
    <S sid="74" ssid="56">However, while their features were limited to certain first- and second-order factors, we use features over second- and third-order factors as found in the parsers of Bohnet and Kuhn (2012).</S>
    <S sid="75" ssid="57">These features are scored as soon as the factors are completed, using a technique that is similar to what Hatori et al. (2011) call delayed features, although they use it for part-of-speech tags in the lookahead while we use it for subgraphs of the dependency tree.</S>
    <S sid="76" ssid="58">Cluster features, finally, are features over word clusters, as first used by Koo et al. (2008), which replace part-of-speech tag features.2 We use a hash kernel to map features to weights.</S>
    <S sid="77" ssid="59">It has been observed that most of the computing time in feature-rich parsers is spent retrieving the index of each feature in the weight vector (Bohnet, 2010).</S>
    <S sid="78" ssid="60">This is usually done via a hash table, but significant speedups can be achieved by using a hash kernel, which simply replaces table lookup by a hash function (Bloom, 1970; Shi et al., 2009; Bohnet, 2010).</S>
    <S sid="79" ssid="61">The price to pay for these speedups is that there may be collisions, so that different features are mapped to the same index, but this is often compensated by the fact that the lower time and memory requirements of the hash kernel enables the use of negative features, that is, features that are never seen in the training set but occur in erroneous hypotheses at training time and can therefore be helpful also at inference time.</S>
    <S sid="80" ssid="62">As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010).</S>
  </SECTION>
  <SECTION title="3 Experiments" number="3">
    <S sid="81" ssid="1">We have evaluated the model for joint tagging and dependency parsing on four typologically diverse languages: Chinese, Czech, English, and German.</S>
    <S sid="82" ssid="2">Most of the experiments use the CoNLL 2009 data sets with the training, development and test split used in the Shared Task (Haji&#711;c et al., 2009), but for better comparison with previous work we also report results for the standard benchmark data sets for Chinese and English.</S>
    <S sid="83" ssid="3">For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al. (2011).3 For English, this is the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 In order to assign k-best part-of-speech tags and scores to words in the training set, we used a perceptron tagger with 10-fold jack-knifing.</S>
    <S sid="84" ssid="4">The same type of tagger was trained on the entire training set in order to supply tags for the development and test sets.</S>
    <S sid="85" ssid="5">The feature set of the tagger was optimized for English and German and provides state-of-theart accuracy for these two languages.</S>
    <S sid="86" ssid="6">The 1-best tagging accuracy for section 23 of the Penn Treebank is 97.28, which is on a par with Toutanova et al. (2003).</S>
    <S sid="87" ssid="7">For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RF-Tagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RF-Tagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Treebank.</S>
    <S sid="88" ssid="8">We could not use the larger training set as it contains the test set of the CoNLL 2009 data that we use to evaluate the joint model.</S>
    <S sid="89" ssid="9">For Czech, the 1best tagging accuracy is 99.11 and for Chinese 92.65 on the CoNLL 2009 test set.</S>
    <S sid="90" ssid="10">We trained parsers with 25 iterations and report results for the model obtained after the last iteration.</S>
    <S sid="91" ssid="11">For cluster features, available only for English and German, we used standard Brown clusters based on the English and German Gigaword Corpus.</S>
    <S sid="92" ssid="12">We restricted the vocabulary to words that occur at least 10 times, used 800 clusters, and took cluster prefixes of length 6 to define features.</S>
    <S sid="93" ssid="13">We report the following evaluation metrics: partof-speech accuracy (POS), unlabeled attachment score (UAS), labeled attachment score (LAS), and tagged labeled attachment score (TLAS).</S>
    <S sid="94" ssid="14">TLAS is a new metric defined as the percentage of words that are assigned the correct part-of-speech tag, the correct head and the correct dependency label.</S>
    <S sid="95" ssid="15">In line with previous work, punctuation is included in the evaluation for the CoNLL data sets but excluded for the two benchmark data sets.</S>
    <S sid="96" ssid="16">Table 1 presents results on the development sets of the CoNLL 2009 shared task with varying values of the two tag parameters k (number of candidates) and &#945; (maximum score difference to 1-best tag) and beam parameters fixed at b1 = 40 and b2 = 4.</S>
    <S sid="97" ssid="17">We use the combined TLAS score on the development set to select the optimal settings for each language.</S>
    <S sid="98" ssid="18">For Chinese, we obtain the best result with 3 tags and a threshold of 0.1.6 Compared to the baseline, we observe a POS improvement of 0.60 and a LAS improvement of 0.51.</S>
    <S sid="99" ssid="19">For Czech, we get the best TLAS with k = 3 and &#945; = 0.2, where POS improves by 0.06 and LAS by 0.46.</S>
    <S sid="100" ssid="20">For English, the best setting is k = 2 and &#945; = 0.1 with a POS improvement of 0.17 and a LAS improvement of 0.62.</S>
    <S sid="101" ssid="21">For German, finally, we see the greatest improvement with k = 3 the updated scores later reported due to some improvements of the parser.</S>
    <S sid="102" ssid="22">Rows 3&#8211;4: Baseline (k = 1) and best settings for k and &#945; on development set.</S>
    <S sid="103" ssid="23">Rows 5&#8211;6: Wider beam (b1 = 80) and added graph features (G) and cluster features (C).</S>
    <S sid="104" ssid="24">Second beam parameter b2 fixed at 4 in all cases. and &#945; = 0.3, where POS improves by 0.66 and LAS by 0.86.</S>
    <S sid="105" ssid="25">Table 2 shows the results on the CoNLL 2009 test sets.</S>
    <S sid="106" ssid="26">For all languages except English, we obtain state-of-the-art results already with bi = 40 (row 4), and for all languages both tagging and parsing accuracy improve compared to the baseline (row 3).</S>
    <S sid="107" ssid="27">The improvement in TLAS is statistically significant with p &lt; 0.01 for all languages (paired t-test).</S>
    <S sid="108" ssid="28">Row 5 shows the scores with a beam of 80 and the additional graph features.</S>
    <S sid="109" ssid="29">Here the LAS scores for Chinese, Czech and German are higher than the best results on the CoNLL 2009 data sets, and the score for English is highly competitive.</S>
    <S sid="110" ssid="30">For Chinese, we achieve 78.51 LAS, which is 1.5 percentage points higher than the reference score, while the POS score is 0.54 higher than our baseline.</S>
    <S sid="111" ssid="31">For Czech, we get 83.73 LAS, which is by far the highest score reported for this data set, together with state-of-the-art POS accuracy.</S>
    <S sid="112" ssid="32">For German, we obtain 89.05 LAS and 97.78 POS, which in both cases is substantially better than in the CoNLL shared task.</S>
    <S sid="113" ssid="33">We believe it is also the highest POS accuracy ever reported for a tagger/parser trained only on the Tiger Treebank.</S>
    <S sid="114" ssid="34">Row 6, finally, presents results with added cluster features for English and German, which results in additional improvements in all metrics.</S>
    <S sid="115" ssid="35">Table 3 gives the results for the Penn Treebank converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).</S>
    <S sid="116" ssid="36">We use k = 3 and &#945; = 0.4, which gave the best results on the development set.</S>
    <S sid="117" ssid="37">The UAS improves by 0.24 when we do joint tagging and parsing.</S>
    <S sid="118" ssid="38">The POS accuracy improves slightly by 0.12 but to a lower degree than for the English CoNLL data where we observed an improvement of 0.20.</S>
    <S sid="119" ssid="39">Nonetheless, the improvement in the joint TLAS score is statistically significant at p &lt; 0.01 (paired t-test).</S>
    <S sid="120" ssid="40">Our joint tagger and dependency parser with graph features gives very competitive unlabeled dependency scores for English with 93.38 UAS.</S>
    <S sid="121" ssid="41">To the best of our knowledge, this is the highest score reported for a (transition-based) dependency parser that does not use additional information sources.</S>
    <S sid="122" ssid="42">By adding cluster features and widening the beam to bi = 80, we achieve 93.67 UAS.</S>
    <S sid="123" ssid="43">We also obtain a POS accuracy of 97.42, which is on a par with the best results obtained using semi-supervised taggers (S&#248;gaard, 2011).</S>
    <S sid="124" ssid="44">Table 4 shows the results for the Chinese Penn Treebank CTB 5.1 together with related work.</S>
    <S sid="125" ssid="45">In experiments with the development set, we could confirm the results from the Chinese CoNLL data set and obtained the best results with the same settings (k = 3, &#945; = 0.1).</S>
    <S sid="126" ssid="46">With bi = 40, UAS improves by 0.25 and POS by 0.30, and the TLAS improvement is again highly significant (p &lt; 0.01, paired t-test).</S>
    <S sid="127" ssid="47">We get the highest UAS, 81.42, with a beam of 80 and added graph features, in which case POS accuracy increases from 92.81 to 93.24.</S>
    <S sid="128" ssid="48">Since our tagger was not optimized for Chinese, we have lower baseline results for the tagger than both Li et al. (2011) and Hatori et al.</S>
    <S sid="129" ssid="49">(2011) but still manage to achieve the highest reported UAS.</S>
    <S sid="130" ssid="50">The speed of the joint tagger and dependency parser is quite reasonable with about 0.4 seconds per sentence on the WSJ-PTB test set, given that we perform tagging and labeled parsing with a beam of 80 while incorporating the features of a third-order graph-based model.</S>
    <S sid="131" ssid="51">Experiments were performed on a computer with an Intel i7-3960X CPU (3.3 GHz and 6 cores).</S>
    <S sid="132" ssid="52">These performance values are preliminary since we are still working on the speed-up of the parser.</S>
    <S sid="133" ssid="53">In order to better understand the benefits of the joint model, we performed an error analysis for German parts of speech in German with F-scores for the left-handside category.</S>
    <S sid="134" ssid="54">ADJ* (ADJD or ADJA) = adjective; ADV = adverb; ART = determiner; APPR = preposition; NE = proper noun; NN = common noun; PRELS = relative pronoun; VVFIN = finite verb; VVINF = non-finite verb; VAFIN = finite auxiliary verb; VAINF = non-finite auxiliary verb; VVPP = participle; XY = not a word.</S>
    <S sid="135" ssid="55">We use &#945;* to denote the set of categories with &#945; as a prefix. and English, where we compared the baseline and the joint model with respect to F-scores for individual part-of-speech categories and dependency labels.</S>
    <S sid="136" ssid="56">For the part-of-speech categories, we found an improvement across the board for both languages, with no category having a significant decrease in F-score, but we also found some interesting patterns for categories that improved more than the average.</S>
    <S sid="137" ssid="57">Table 5 shows selected entries from the confusion matrix for German, where we see substantial improvements for finite and non-finite verbs, which are often morphologically ambiguous but which can be disambiguated using syntactic context.</S>
    <S sid="138" ssid="58">We also see improved accuracies for common and proper nouns, which are both capitalized in standard German orthography and therefore often mistagged, and for relative pronouns, which are less often confused for determiners in the joint model.</S>
    <S sid="139" ssid="59">Table 6 gives a similar snapshot for English, and we again see improvements for verb categories that are often morphologically ambiguous, such as past participles, which can be confused for past tense verbs, and present tense verbs in third person singular, which can be confused for nouns.</S>
    <S sid="140" ssid="60">We also see some improvement for the singular noun categoparts of speech in English with F-scores for the left-handside category.</S>
    <S sid="141" ssid="61">DT = determiner; IN = preposition or subordinating conjunction; JJ = adjective; JJR = comparative adjective; NN = singular or mass noun; NNS = plural noun; POS = possessive clitic; RB = adverb; RBR = comparative adverb; RP = particle; UH = interjection; VB = base form verb; VBD = past tense verb; VBG = gerund or present participle; VBN = past participle; VBP = present tense verb, not 3rd person singular; VBZ = present tense verb, 3rd person singular.</S>
    <S sid="142" ssid="62">We use &#945;* to denote the set of categories with &#945; as a prefix. ry and for adverbs, which are less often confused for prepositions or subordinating conjunctions thanks to the syntactic information in the joint model.</S>
    <S sid="143" ssid="63">For dependency labels, it is hard to extract any striking patterns and it seems that we mainly see an improvement in overall parsing accuracy thanks to less severe tagging errors.</S>
    <S sid="144" ssid="64">However, it is worth observing that, for both English and German, we see significant F-score improvements for the core grammatical functions subject (91.3 &#8212;* 92.1 for German, 95.6 &#8212;* 96.1 for English) and object (86.9 &#8212;* 87.9 for German, 90.2 &#8212;* 91.9 for English).</S>
  </SECTION>
  <SECTION title="4 Related Work" number="4">
    <S sid="145" ssid="1">Our work is most closely related to Lee et al. (2011), Li et al.</S>
    <S sid="146" ssid="2">(2011) and Hatori et al. (2011), who all present discriminative models for joint tagging and dependency parsing.</S>
    <S sid="147" ssid="3">However, all three models only perform unlabeled parsing, while our model incorporates dependency labels into the parsing process.</S>
    <S sid="148" ssid="4">Whereas Lee et al. (2011) and Li et al.</S>
    <S sid="149" ssid="5">(2011) take a graph-based approach to dependency parsing, Hatori et al. (2011) use a transition-based model similar to ours but limited to projective dependency trees.</S>
    <S sid="150" ssid="6">Both Li et al. (2011) and Hatori et al.</S>
    <S sid="151" ssid="7">(2011) only evaluate their model on Chinese, and of these only Hatori et al. (2011) report consistent improvements in both tagging and parsing accuracy.</S>
    <S sid="152" ssid="8">Like our system, the parser of Lee et al. (2011) can handle nonprojective trees and experimental results are presented for four languages, but their graph-based model is relatively simple and the baselines therefore well below the state of the art.</S>
    <S sid="153" ssid="9">We are thus the first to show consistent improvements in both tagging and (labeled) parsing accuracy across typologically diverse languages at the state-of-the-art level.</S>
    <S sid="154" ssid="10">Moreover, the capacity to handle non-projective dependencies, which is crucial to attain good performance on Czech and German, does not seem to hurt performance on English and Chinese, where the benchmark sets contain only projective trees.</S>
    <S sid="155" ssid="11">The use of beam search in transition-based dependency parsing in order to mitigate the problem of error propagation was first proposed by Johansson and Nugues (2006), although they still used a locally trained model.</S>
    <S sid="156" ssid="12">Globally normalized models were first explored by Titov and Henderson (2007), who were also the first to use a parameterized SHIFT transition like the one found in both Hatori et al. (2011) and our own work, although Titov and Henderson (2007) used it to define a generative model by parameterizing the SHIFT transition by an input word.</S>
    <S sid="157" ssid="13">Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graphbased features to a transition-based parser.</S>
    <S sid="158" ssid="14">This approach was further pursued in Zhang and Clark (2011) and was used by Zhang and Nivre (2011) to achieve state-of-the-art results in dependency parsing for both Chinese and English through the addition of rich non-local features.</S>
    <S sid="159" ssid="15">Huang and Sagae (2010) combined structured perceptron learning and beam search with the use of a graph-structured stack to allow ambiguity packing in the beam, a technique that was reused by Hatori et al. (2011).</S>
    <S sid="160" ssid="16">Finally, as noted in the introduction, although joint tagging and parsing is rare in dependency parsing, most state-of-the-art parsers based on PCFG models naturally incorporate part-of-speech tagging and usually achieve better parsing accuracy (albeit not always tagging accuracy) with a joint model than with a pipeline approach (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Petrov et al., 2006).</S>
    <S sid="161" ssid="17">Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing.</S>
  </SECTION>
  <SECTION title="5 Conclusion" number="5">
    <S sid="162" ssid="1">We have presented the first system for joint partof-speech tagging and labeled dependency parsing with non-projective dependency trees.</S>
    <S sid="163" ssid="2">Evaluation on four languages shows consistent improvements in both tagging and parsing accuracy over a pipeline system with state-of-the-art results across the board.</S>
    <S sid="164" ssid="3">The error analysis reveals improvements in tagging accuracy for syntactically central categories, mainly verbs, with improvement in syntactic accuracy for core grammatical functions as a result.</S>
    <S sid="165" ssid="4">In future work we intend to explore joint models that incorporate not only basic part-of-speech tags but also more fine-grained morphological features.</S>
  </SECTION>
</PAPER>
