<PAPER>
  <S sid="0">Graph-Based Generation Of Referring Expressions</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This article describes a new approach to the generation of referring expressions.</S>
    <S sid="2" ssid="2">We propose to formalize a scene (consisting of a set of objects with various properties and relations) as a labeled directed graph and describe content selection (which properties to include in a referring expression) as a subgraph construction problem.</S>
    <S sid="3" ssid="3">Cost functions are used to guide the search process and to give preference to some solutions over others.</S>
    <S sid="4" ssid="4">The current approach has four main advantages: (1) Graph structures have been studied extensively, and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs; (2) many existing generation algorithms can be reformulated in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="5" ssid="1">This article describes a new approach to the generation of referring expressions.</S>
    <S sid="6" ssid="2">We propose to formalize a scene (consisting of a set of objects with various properties and relations) as a labeled directed graph and describe content selection (which properties to include in a referring expression) as a subgraph construction problem.</S>
    <S sid="7" ssid="3">Cost functions are used to guide the search process and to give preference to some solutions over others.</S>
    <S sid="8" ssid="4">The current approach has four main advantages: (1) Graph structures have been studied extensively, and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs; (2) many existing generation algorithms can be reformulated in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="9" ssid="1">The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002).</S>
    <S sid="10" ssid="2">In this article, we present a general, graph-theoretic approach to the generation of referring expressions.</S>
    <S sid="11" ssid="3">We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem.</S>
    <S sid="12" ssid="4">The graph perspective has four main advantages.</S>
    <S sid="13" ssid="5">(1) There are many attractive and well-understood algorithms for dealing with graph structures (see, e.g., Gibbons [1985], Cormen, Leiserson, and Rivest [1990], or Chartrand and Oellermann [1993]).</S>
    <S sid="14" ssid="6">In this article, we describe a straightforward branch and bound algorithm for finding the relevant subgraphs in which cost functions are used to guide the search process.</S>
    <S sid="15" ssid="7">(2) By defining different cost functions for the graph perspective, we can simulate (and improve) some of the well-known algorithms for the generation of referring expressions mentioned above.</S>
    <S sid="16" ssid="8">This facilitates the formal comparison of these algorithms and makes it easier to transfer results from one algorithm to another.</S>
    <S sid="17" ssid="9">(3) The graph perspective provides a clean solution for some problems that have plagued earlier algorithms.</S>
    <S sid="18" ssid="10">For instance, the generation of relational expressions (i.e., referring expressions that include references to other objects) is enhanced by the fact that both properties and relations are formalized in the same way, namely, as edges in a graph.</S>
    <S sid="19" ssid="11">(4) The combined use of graphs and cost functions paves the way for a natural integration of traditional rule-based approaches to generating referring expressions and more recent statistical approaches, such as Langkilde and Knight (1998) and Malouf (2000), in a single algorithm.</S>
    <S sid="20" ssid="12">The outline of this article is as follows.</S>
    <S sid="21" ssid="13">In Section 2 the content selection problem for generating referring expressions is explained, and some well-known solutions to the problem are discussed.</S>
    <S sid="22" ssid="14">In Section 3, we describe how scenes can be modeled as labeled directed graphs and show how content selection can be formalized as a subgraph construction problem.</S>
    <S sid="23" ssid="15">Section 4 contains a sketch of the basic generation algorithm, which is illustrated with a worked example.</S>
    <S sid="24" ssid="16">In Section 5 various ways to formalize cost functions are discussed and compared.</S>
    <S sid="25" ssid="17">We end with some concluding remarks and a discussion of future research directions in Section 6.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="26" ssid="1">There are many different algorithms for the generation of referring expressions, each with its own objectives: Some aim at producing the shortest possible description (e.g., Dale&#8217;s [1992] full brevity algorithm), others focus on psychological realism (e.g., Dale and Reiter&#8217;s [1995] incremental algorithm) or realistic output (e.g., Horacek 1997).</S>
    <S sid="27" ssid="2">The degree of detail in which the various algorithms are described differs considerably.</S>
    <S sid="28" ssid="3">Some algorithms are fully formalized and come with explicit characterizations of their complexity (e.g., Dale and Reiter 1995; van Deemter 2000); others are more conceptual and concentrate on exploring new directions (e.g., Stone and Webber 1998).</S>
    <S sid="29" ssid="4">Despite such differences, most algorithms deal with the same problem definition.</S>
    <S sid="30" ssid="5">They take as input a single object v (the target object) for which a referring expression is to be generated and a set of objects (the distractors) from which the target object needs to be distinguished (we use the terminology from Dale and Reiter [1995]).</S>
    <S sid="31" ssid="6">The task of the algorithm is to determine which set of properties is needed to single out the target object v from the distractors.</S>
    <S sid="32" ssid="7">This is known as the content determination problem for referring expressions.</S>
    <S sid="33" ssid="8">On the basis of this set of properties a distinguishing description for v can be generated.</S>
    <S sid="34" ssid="9">Most algorithms do not address the surface realization problem (how the selected properties should be realized in natural language) in much detail; it is usually assumed that once the content for a referring expression has been determined, a standard realizer such as KPML (Bateman 1997) or SURGE (Elhaded and Robin 1997) can convert the meaning representation to natural language.</S>
    <S sid="35" ssid="10">Consider the example scene in Figure 1.</S>
    <S sid="36" ssid="11">In this scene, as in any other scene, we see a finite domain of entities D with properties P. In this particular scene, D = {d1,d2,d3,d4} is the set of entities and P = { dog, cat, brown, black+white, large, small } is the set of properties.</S>
    <S sid="37" ssid="12">A scene is usually represented as a database (or knowledge base) listing the properties of each element in D. Thus: d1: dog (d1) small (d1) brown (d1) d2: dog (d2) large (d2) brown (d2) d3: dog (d3) large (d3) black+white (d3) d4: cat (d4) small (d4) brown (d4) A simple example scene consisting of some domestic animals.</S>
    <S sid="38" ssid="13">In what is probably the key reference on the topic, Dale and Reiter (1995) describe and discuss a number of algorithms for the generation of referring expressions.</S>
    <S sid="39" ssid="14">One of these is the full brevity algorithm (originally due to Dale 1992).</S>
    <S sid="40" ssid="15">This algorithm first tries to generate a distinguishing description for the target object v using one single property.</S>
    <S sid="41" ssid="16">If this fails, it considers all possible combinations of two properties to see if any of these suffices for the generation of a distinguishing description, and so on.</S>
    <S sid="42" ssid="17">It is readily seen that this algorithm will output the shortest possible description, if one exists.</S>
    <S sid="43" ssid="18">Suppose the full brevity algorithm is used to generate a description for d1 in Figure 1.</S>
    <S sid="44" ssid="19">There is no single property that distinguishes the target object d1 from the distractors {d2, d3, d4}.</S>
    <S sid="45" ssid="20">But when considering all pairs of properties the algorithm will find that one such pair rules out all distractors, namely, small and dog; &#8220;the small dog&#8221; is a successful and minimal distinguishing description for d1.</S>
    <S sid="46" ssid="21">Dale and Reiter point out that the full brevity algorithm is both computationally infeasible (NP hard) and psychologically unrealistic.</S>
    <S sid="47" ssid="22">They offer the incremental algorithm as an alternative.</S>
    <S sid="48" ssid="23">The incremental algorithm considers properties for selection in a predetermined order, based on the idea that human speakers and listeners prefer certain kinds of properties (or attributes) when describing objects from a given domain.</S>
    <S sid="49" ssid="24">For instance, when discussing domestic animals, it seems likely that a human speaker would first describe an animal by its type (is it a dog? is it a cat?).</S>
    <S sid="50" ssid="25">If that does not suffice, first absolute attributes like color are tried, followed by relative ones such as size.</S>
    <S sid="51" ssid="26">In sum: The list of preferred attributes for our example domain would be ( type, color, size ).</S>
    <S sid="52" ssid="27">Essentially, the incremental algorithm iterates through this list, and for each property it encounters, it determines whether adding this property to the properties selected so far would rule out any of the remaining distractors.</S>
    <S sid="53" ssid="28">If so, it is included in the list of selected properties.</S>
    <S sid="54" ssid="29">There is one exception to this general strategy: Type information is always included, even if it rules out no distractors.</S>
    <S sid="55" ssid="30">The algorithm stops when all distractors are ruled out (success) or when the end of the list of preferred attributes is reached (failure).</S>
    <S sid="56" ssid="31">Suppose we apply the incremental algorithm to d1 from Figure 1 with ( type, color, size ) as preferred attributes.</S>
    <S sid="57" ssid="32">The type of d1 listed in the database is dog.</S>
    <S sid="58" ssid="33">This property is selected (since type information is always selected).</S>
    <S sid="59" ssid="34">It rules out d4 (which is a cat).</S>
    <S sid="60" ssid="35">Next we consider the color of d1; the animal is brown.</S>
    <S sid="61" ssid="36">This property rules out d3 (which is a black and white dog) and is selected.</S>
    <S sid="62" ssid="37">Finally, we consider the size of our target object, which is small.</S>
    <S sid="63" ssid="38">This properly rules out the remaining distractor d2 (which is a large brown dog) and hence is included as well.</S>
    <S sid="64" ssid="39">At this point, all distractors are ruled out (success!</S>
    <S sid="65" ssid="40">), and the set of selected properties is {dog, brown, small}, which a linguistic realizer might express as &#8220;the small brown dog.&#8221; This is a successful distinguishing description but not a minimal one: The property brown is, strictly speaking, made redundant by the later inclusion of the property small.</S>
    <S sid="66" ssid="41">Since there is no backtracking in the incremental algorithm however, every selected property is realized (hence &#8220;incremental&#8221;).</S>
    <S sid="67" ssid="42">This aspect is largely responsible for the computational efficiency of the algorithm (it has a polynomial complexity), but Dale and Reiter (1995, page 248) also claim that it is &#8220;psychologically realistic.&#8221; They point out that sometimes people may describe an object as &#8220;the white bird&#8221; even though the simpler &#8220;the bird&#8221; would have been sufficient (cf.</S>
    <S sid="68" ssid="43">Pechmann [1989]; see, however, Krahmer and Theune [2002] for discussion).</S>
    <S sid="69" ssid="44">Even though there are various useful and interesting algorithms for the generation of referring expressions, a number of open questions remain.</S>
    <S sid="70" ssid="45">Recently there has been an increased interest in statistical approaches to natural language generation.</S>
    <S sid="71" ssid="46">For example, Malouf (2000) has shown that large corpora can be used to determine the order of realization of sequences of prenominal adjectives.</S>
    <S sid="72" ssid="47">It is unclear how such statistical work on generation can be combined with older, rule-based work such as the algorithms just discussed.</S>
    <S sid="73" ssid="48">In addition, many algorithms still have difficulties with the generation of relational descriptions (descriptions that include references to other objects to single out the target object from its distractors).</S>
    <S sid="74" ssid="49">To illustrate the problem, consider the scene depicted in Figure 2.</S>
    <S sid="75" ssid="50">In this scene we again see a finite domain of entities D with certain properties P. Here, D = {d1, d2, d3, d4} is the set of entities, and P = { dog, doghouse, small, large, brown, white } is the set of properties.</S>
    <S sid="76" ssid="51">Clearly no algorithm can generate a distinguishing description referring to d1 on this basis.</S>
    <S sid="77" ssid="52">Intuitively, d1 can be distinguished from d2 only using its relation to the doghouse d3.</S>
    <S sid="78" ssid="53">To facilitate this we extend the scene description with a set of relations R = { left of, right of, contain, in }.</S>
    <S sid="79" ssid="54">A few algorithms have been developed that address the issue of relational descriptions.</S>
    <S sid="80" ssid="55">The earliest is from Dale and Haddock (1992), who offer an extension of the full brevity algorithm.</S>
    <S sid="81" ssid="56">The Dale and Haddock algorithm has a problem with infinite recursions; it may produce descriptions like &#8220;the dog in the doghouse that contains a dog that is inside a doghouse....&#8221; Dale and Haddock, somewhat ad hoc, solve this problem by stipulating that a property or relation may be used only once.</S>
    <S sid="82" ssid="57">Krahmer and Theune (2002) (see also Theune [2000]) describe an extension of the incremental algorithm that allows for relational descriptions.</S>
    <S sid="83" ssid="58">Their extension suffers from what may be called the problem of forced incrementality: When a first relation fails to rule out all remaining distractors, additional relations will be tried incrementally.</S>
    <S sid="84" ssid="59">Although it could be argued that incremental selection of properties is psychologically plausible, it seems less plausible for relations.</S>
    <S sid="85" ssid="60">It is unlikely that someone would describe an A graph representation of the scene in Figure 2. object as &#8220;the dog next to the tree in front of the garage&#8221; in a situation in which &#8220;the dog in front of the garage&#8221; would suffice.</S>
    <S sid="86" ssid="61">As we shall argue, the graph perspective provides a clean solution for these problems.</S>
  </SECTION>
  <SECTION title="3." number="4">
    <S sid="87" ssid="1">In the previous section we saw that a scene can be described in terms of a domain of entities D with properties P and relations R. Such a scene can be represented as a labeled directed graph (see, e.g., Wilson [1996] for a gentle introduction or Berge [1985] for a more specialized one).</S>
    <S sid="88" ssid="2">Let L = P U R be the set of labels with P and R disjoint (i.e., P fl R = 0).</S>
    <S sid="89" ssid="3">Then G = (VG, EG) is a labeled directed graph, where VG C D is the set of vertices (or nodes) and EG C VG x L x VG is the set of labeled directed edges (or arcs).</S>
    <S sid="90" ssid="4">Where this can be done without creating confusion, the graph subscript is omitted.</S>
    <S sid="91" ssid="5">Throughout this article we use the following notations.</S>
    <S sid="92" ssid="6">If G = (V, E) is a graph and e = (v, l, w) an edge (with l E L), then the extension of G with e, denoted as G + e, is the graph (V U {v, w}, E U {e}).</S>
    <S sid="93" ssid="7">Moreover, with EG(v, w) we refer to the set of edges in EG from v to w; that is, EG(v, w) = {e E EG  |e = (v, l, w), for l E L}.</S>
    <S sid="94" ssid="8">The scene given in Figure 2, for example, can now be represented by the graph in Figure 3.</S>
    <S sid="95" ssid="9">This graph models the respective spatial relations between the two chihuahuas, between the two doghouses, and between each dog and the nearest doghouse.</S>
    <S sid="96" ssid="10">For the sake of transparency we have not modeled the relations between the dogs and the distant doghouses (i.e., between d1 and d4 and between d2 and d3).</S>
    <S sid="97" ssid="11">(It is worth stressing that adding these edges would not result in different outcomes in the discussion below).</S>
    <S sid="98" ssid="12">Note that properties (such as dog) are always modeled as loops, Some graphs for referring expressions, with circles around the intended referent. that is, as edges that start and end in the same vertex.</S>
    <S sid="99" ssid="13">Relations may have different start and end vertices, but they do not have to (consider potentially reflexive relations such as shave).</S>
    <S sid="100" ssid="14">Finally, note that the graph sometimes contains properties of various levels of specificity (e.g., chihuahua and dog).</S>
    <S sid="101" ssid="15">This aspect of scene graphs will be further discussed in Section 5.</S>
    <S sid="102" ssid="16">Now the content determination problem for referring expressions can be formulated as a graph construction problem.</S>
    <S sid="103" ssid="17">To decide which information to include in a referring expression for an object v E V, we construct a connected directed labeled graph over the set of labels L and an arbitrary set of vertices, but including v. A graph is connected iff there is a path (a list of vertices in which each vertex has an edge from itself to the next vertex) between each pair of vertices.</S>
    <S sid="104" ssid="18">Informally, we say that a vertex (&#8220;the intended referent&#8221;) from a graph H refers to a given entity in the scene graph G iff the graph H can be &#8220;placed over&#8221; the scene graph G in such a way that the vertex being referred to is &#8220;placed over&#8221; the vertex of the given entity in G and each edge from H with label l can be &#8220;placed over&#8221; an edge from G with the same label.</S>
    <S sid="105" ssid="19">Furthermore, a vertex-graph pair is distinguishing iff it refers to exactly one vertex in the scene graph.</S>
    <S sid="106" ssid="20">Consider the three vertex-graph pairs in Figure 4, in which circled vertices stand for the intended referent.</S>
    <S sid="107" ssid="21">Graph (i) refers to all vertices of the graph in Figure 3 (every object in the scene is next to some other object), graph (ii) can refer to both d1 and d2, and graph (iii) is distinguishing in that it can refer only to d1.</S>
    <S sid="108" ssid="22">Note that the graphs might be realized as something next to something else, a chihuahua, and the dog in the doghouse, respectively.</S>
    <S sid="109" ssid="23">Here we concentrate on the generation of distinguishing vertex-graph pairs.</S>
    <S sid="110" ssid="24">Formally, the notion that a graph H = (VH, EH) can be &#8220;placed over&#8221; another graph G = ft, EG) corresponds to the notion of a subgraph isomorphism (see, e.g., Three distinguishing vertex-graph pairs referring to d1 in Figure 3.</S>
    <S sid="111" ssid="25">Read and Corneil [1977] for an overview).</S>
    <S sid="112" ssid="26">H can be &#8220;placed over&#8221; G iff there exists a subgraph G' = (VG,, EG,) of G such that H is isomorphic to G'.</S>
    <S sid="113" ssid="27">H is isomorphic to G' iff there exists a bijection 7r: VH &#8594; VG, such that for all vertices v,w E VH and all l E L: (v, l, w) E EH &#65533;* (7r.v,l, 7r.w) E EG In words: The bijective function 7r maps all the vertices in H to corresponding vertices in G' in such a way that any edge with label l between vertices v and w in H is matched with an edge with the same label between the G' counterparts of v and w (i.e., 7r.v and 7r.w, respectively).</S>
    <S sid="114" ssid="28">When H is isomorphic to some subgraph of G by an isomorphism 7r, we write H C,r G. Given a graph H and a vertex v in H, and a graph G and a vertex w in G, we define that the pair (v, H) refers to the pair (w, G) iff H is connected and H C,r G and 7r.v = w. Furthermore, (v, H) uniquely refers to (w, G) (i.e., (v, H) is distinguishing) iff (v, H) refers to (w, G) and there is no vertex w' in G different from w such that (v, H) refers to (w', G).</S>
    <S sid="115" ssid="29">The problem considered in this article can now be formalized as follows: Given a graph G and a vertex w in G, find a pair (v, H) such that (v, H) uniquely refers to (w, G).</S>
    <S sid="116" ssid="30">Consider, for instance, the task of finding a pair (v,H) that uniquely refers to the vertex labeled d1 in Figure 3.</S>
    <S sid="117" ssid="31">It is easily seen that there are a number of such pairs, three of which are depicted in Figure 5.</S>
    <S sid="118" ssid="32">We would like to have a mechanism that allows us to give certain solutions to this kind of task preference over other solutions.</S>
    <S sid="119" ssid="33">For this purpose we shall use cost functions.</S>
    <S sid="120" ssid="34">In general, a cost function is a function that assigns to each subgraph of a scene graph a non-negative number.</S>
    <S sid="121" ssid="35">As we shall see, by defining cost functions in different ways, we can mimic various algorithms for the generation of referring expressions known from the literature.</S>
    <S sid="122" ssid="36">The basic decision problem for subgraph isomorphism (i.e., testing whether a graph H is isomorphic to a subgraph of G) is known to be NP-complete (see, e.g., Garey and Johnson [1979]).</S>
    <S sid="123" ssid="37">Here we are interested in connected H, but unfortunately that restriction does not reduce the theoretical complexity.</S>
    <S sid="124" ssid="38">Note that this characterization of the worst-case complexity holds for graphs in which all edges have the same label; in that case each edge from H can potentially be matched to any edge from G. The bestcase complexity is given when each edge is uniquely labeled.</S>
    <S sid="125" ssid="39">In practice, the situation will most often be somewhere between these extremes.</S>
    <S sid="126" ssid="40">In general, we can say that the more diverse the labeling of edges in the graph of a particular scene is, the sooner a distinguishing vertex-graph pair will be found.</S>
    <S sid="127" ssid="41">It is worth pointing out that there are various alternatives to full subgraph isomorphism that have a lower complexity.</S>
    <S sid="128" ssid="42">For instance, as soon as an upper bound K is defined on the number of edges in a distinguishing graph, the problem loses its intractability (for relatively small K) and becomes solvable, in the worst case, in polynomial O(nK) time, where n is number of edges in the graph G. Restricting the problem in such a way is rather harmless for our current purposes, as it prohibits the generation only of distinguishing descriptions with more than K properties, and for all practical purposes K can be small (referring expressions usually express a limited number of properties).</S>
    <S sid="129" ssid="43">Defining an upper bound K, however, does have a disadvantage: We lose completeness (see van Deemter [2002]).</S>
    <S sid="130" ssid="44">In particular, the algorithm will fail for objects that can be uniquely described only with K + 1 (or more) edges.</S>
    <S sid="131" ssid="45">Of course, one could argue that in such cases objects should be distinguished using other means (e.g., by pointing).</S>
    <S sid="132" ssid="46">Nevertheless, it is worthwhile to look for classes of graphs for which the subgraph isomorphism problem can be solved more efficiently, without postulating upper bounds.</S>
    <S sid="133" ssid="47">For instance, if G and H are planar (simple) graphs the problem can be solved in time linear in the number of vertices of G (Eppstein 1999).</S>
    <S sid="134" ssid="48">Basically, a planar graph is one that can be drawn on a plane in such a way that there are no crossing edges (thus, for instance, the graph in Figure 3 is planar, as is any graph with only four vertices).</S>
    <S sid="135" ssid="49">In general, there is no a priori reason to assume that our scene representations will be planar.</S>
    <S sid="136" ssid="50">Yet every nonplanar graph can be modified into a closely related planar one.</S>
    <S sid="137" ssid="51">We briefly address planarization of scene graphs in the Appendix.</S>
    <S sid="138" ssid="52">A final alternative is worth mentioning.</S>
    <S sid="139" ssid="53">The general approach to the problem of subgraph isomorphism detection assumes that both graphs are given on-line.</S>
    <S sid="140" ssid="54">For our current purposes, however, it may happen that the scene graph is fixed and known beforehand, and only the referring graph is unknown and given on-line.</S>
    <S sid="141" ssid="55">Messmer and Bunke (1995, 1998) describe a method that converts the known graph (or model graph, as they call it) into a decision tree.</S>
    <S sid="142" ssid="56">At run time, the input graph is classified by the decision tree, which detects subgraph isomorphisms.</S>
    <S sid="143" ssid="57">The disadvantage of this approach is that the decision tree may contain, in the worst case, an exponential number of nodes.</S>
    <S sid="144" ssid="58">But the main advantage is that the complexity of the new subgraph isomorphism algorithm is only quadratic in the number of vertices of the input referring graph.</S>
    <S sid="145" ssid="59">Note that with this approach we do not lose information from the scene graph, nor do we lose completeness.</S>
    <S sid="146" ssid="60">In sum, the basic approach to subgraph isomorphisms is NP-complete, but there exist various reformulations of the problem that can be solved more efficiently.</S>
    <S sid="147" ssid="61">Deciding which (combination) of these is the most suitable in practice, however, is beyond the scope of this article.</S>
    <S sid="148" ssid="62">Finally, it is worth stressing that the NP-completeness is due to the presence of edges representing relations between different vertices.</S>
    <S sid="149" ssid="63">If we restrict the approach to properties (looping edges), testing for subgraph isomorphisms becomes trivial.</S>
  </SECTION>
  <SECTION title="4." number="5">
    <S sid="150" ssid="1">In this section we give a high-level sketch of the graph-based generation algorithm.</S>
    <S sid="151" ssid="2">The algorithm (called makeReferringExpression) consists of two main components, a subgraph construction algorithm (called findGraph) and a subgraph isomorphism testing algorithm (called matchGraphs).</S>
    <S sid="152" ssid="3">For expository reasons we do not address optimization strategies (but see Section 6).</S>
    <S sid="153" ssid="4">We assume that a scene graph G = (VG, EG) is given.</S>
    <S sid="154" ssid="5">The algorithm systematically tries all relevant subgraphs H of the scene graph G by starting with the subgraph containing only the vertex v (the target object) and expanding it recursively by trying to add edges from G that are adjacent to the subgraph H constructed up to that point.</S>
    <S sid="155" ssid="6">In this way we know that the results will be a connected subgraph.</S>
    <S sid="156" ssid="7">We refer to this set of adjacent edges as the H neighbors in G (denoted as G.neighbors(H)).</S>
    <S sid="157" ssid="8">Formally: The algorithm returns the cheapest (least expensive) distinguishing subgraph H that refers to v, if such a distinguishing graph exists; otherwise it returns the undefined null graph -L. We use cost functions to guide the search process and to give preference to some solutions over others.</S>
    <S sid="158" ssid="9">If H = (VH, EH) is a subgraph of G, then the costs of H, denoted as cost(H), can be given by summing over the costs associated with the vertices and edges of H. Formally: In fact, this is only one possible way to define a cost function.</S>
    <S sid="159" ssid="10">The only hard requirement cost functions have to fulfill is monotonicity.</S>
    <S sid="160" ssid="11">That is, adding an edge e to a graph G should never result in a graph cheaper than G. Formally: dG' C G de E EG: cost(G') &lt; cost(G' + e) The monotonicity assumption helps reduce the search space, since extensions of subgraphs with a cost greater than the best subgraph found up to that point can safely be ignored.</S>
    <S sid="161" ssid="12">Naturally, the costs of the undefined graph (cost(-L)) are not defined.</S>
    <S sid="162" ssid="13">It is worth stressing that the cost function is global: It determines the costs of entire graphs.</S>
    <S sid="163" ssid="14">This implies that the cheapest distinguishing graph is not necessarily the smallest distinguishing graph; a graph consisting of two or more edges may be cheaper than a graph containing one expensive edge.</S>
    <S sid="164" ssid="15">We now illustrate the algorithm with an example.</S>
    <S sid="165" ssid="16">Suppose the scene graph G is as given in Figure 3 and that we want to generate a referring expression for object d1 Sketch of the main function (makeReferringExpression) and the subgraph construction function (findGraph). in this graph.</S>
    <S sid="166" ssid="17">Let us assume for the sake of illustration that the cost function is defined in such a way that adding a vertex or an edge always costs one point.</S>
    <S sid="167" ssid="18">Thus, for each v E VH and for each e E EH: cost(v) = cost(e) = 1.</S>
    <S sid="168" ssid="19">(In the next section we describe a number of more interesting cost functions and discuss the impact these have on the output of the algorithm.)</S>
    <S sid="169" ssid="20">We call the function makeReferringExpression (given in Figure 6) with d1 as parameter.</S>
    <S sid="170" ssid="21">In this function the variable bestGraph (for the best solution found up to that point) is initialized as the null graph (there is no best solution yet), and the variable H (for the distinguishing subgraph under construction) is initialized as the graph containing only vertex d1 (i.e., (i) in Figure 7).</S>
    <S sid="171" ssid="22">Then the function findGraph (see also Figure 6) is called, with parameters d1, bestGraph, and H. To begin with, whether a first non-null bestGraph has been found is checked and, if one has, whether the costs of H (the graph under construction) are higher than the costs of the bestGraph found up to that point.</S>
    <S sid="172" ssid="23">If the costs of H are higher, it is not worth extending H since, due to the monotonicity constraint, it will never end up being cheaper than the current bestGraph.</S>
    <S sid="173" ssid="24">During the first iteration we have no non-null bestGraph, so we continue.</S>
    <S sid="174" ssid="25">Next the set of distractors is calculated.</S>
    <S sid="175" ssid="26">In terms of the graph perspective, this is the set of vertices in the scene graph G (other than the target vertex v) to which the graph H refers.</S>
    <S sid="176" ssid="27">It is easily seen that the initial value of H refers to every vertex in G. Hence, as one would expect, the initial set of distractors is VG\{d1} = {d2,d3,d4}.</S>
    <S sid="177" ssid="28">Then the current set of distractors is checked to determine whether it is empty.</S>
    <S sid="178" ssid="29">If it is, we have managed to find a distinguishing graph, which is subsequently stored in the variable bestGraph.</S>
    <S sid="179" ssid="30">In the first iteration, this is obviously not the case, and we continue, recursively trying to extend H by adding Three values for H in the generation process for d1. adjacent (neighboring) edges until either a distinguishing graph has been constructed (all distractors are ruled out) or the costs of H exceed the costs of the bestGraph found so far.</S>
    <S sid="180" ssid="31">While bestGraph is still the null graph, the algorithm continues until H is a distinguishing graph.</S>
    <S sid="181" ssid="32">Which is the first distinguishing graph to be found (if more than one exists) depends on the order in which the adjacent edges are tried (see also Section 5.1).</S>
    <S sid="182" ssid="33">Suppose for the sake of argument that the first distinguishing graph to be found is (ii) in Figure 7.</S>
    <S sid="183" ssid="34">This graph is returned and stored in bestGraph.</S>
    <S sid="184" ssid="35">The costs associated with this graph are five points (two vertices and three edges).</S>
    <S sid="185" ssid="36">At this stage in the generation process only graphs with costs lower than five points are worth investigating.</S>
    <S sid="186" ssid="37">In fact, there are only a few distinguishing graphs that cost less than this.</S>
    <S sid="187" ssid="38">After a number of iterations the algorithm will find the cheapest solution (given this particular, simple definition of the cost function), which is (iii) in Figure 7.</S>
    <S sid="188" ssid="39">That this distinguishing graph does not include type information does not necessarily mean that such information should not be realized.</S>
    <S sid="189" ssid="40">It means only that type information is, strictly speaking, not necessary to distinguish the intended referent from the distractors.</S>
    <S sid="190" ssid="41">We return to this issue in Section 5.2.</S>
    <S sid="191" ssid="42">Figure 8 sketches the part of the algorithm that tests for subgraph isomorphism, matchGraphs.</S>
    <S sid="192" ssid="43">This function is called each time the distractor set is calculated.</S>
    <S sid="193" ssid="44">It tests whether the pair (v,H) can refer to (w, G), or put differently, it checks whether there exists an isomorphism 7r such that H C&#960; G with 7r.v = w. The function matchGraphs first determines whether the looping edges starting from vertex v match those of w. If EH(v, v) is not a subset of EG(w,w) (e.g., v is a dog and w is a doghouse), we can immediately discard the matching.</S>
    <S sid="194" ssid="45">Otherwise we start with the matching 7r.v = w and try to expand it recursively.</S>
    <S sid="195" ssid="46">At each recursion step a fresh and as yet unmatched vertex y from VH is selected that is adjacent to one of the vertices in the current domain of 7r (notated dom(7r)).</S>
    <S sid="196" ssid="47">For each y we calculate the set Z of possible vertices in G to which y can be Sketch of the function testing for subgraph isomorphism (matchGraphs). matched.</S>
    <S sid="197" ssid="48">This set consists of all the vertices in G that have the same looping edges as y and the same edges to and from other vertices in the domain of the current matching function 7r.</S>
    <S sid="198" ssid="49">Formally: (The H.neighbors(y) are the vertices in H that are adjacent to y, that is, those vertices that are connected to y by an edge.)</S>
    <S sid="199" ssid="50">The matching can now possibly be extended with 7r.y = z, for each z &#8712; Z.</S>
    <S sid="200" ssid="51">The algorithm then branches over all these possibilities.</S>
    <S sid="201" ssid="52">Once a mapping 7r has been found that has exactly as many elements as H has vertices, we have found a subgraph isomorphism.</S>
    <S sid="202" ssid="53">If there are still unmatched vertices in H, or if all possible extensions with vertex y have been checked and no matching can be found, the test for subgraph isomorphism has failed.</S>
    <S sid="203" ssid="54">The algorithm outlined in Figures 6 and 8 has been implemented in Java 2 (J2SE, version 1.4).</S>
    <S sid="204" ssid="55">The implemented version of the algorithm is actually more efficient than the sketch suggests, because various calculations need not be repeated in each iteration (the set of distractors and the set G.neighbors(H), for example).</S>
    <S sid="205" ssid="56">In addition, the user has the possibility of specifying the cost function in whatever way he or she sees fit.</S>
    <S sid="206" ssid="57">A full-fledged performance analysis of the current implementation is beyond the scope of this article.</S>
    <S sid="207" ssid="58">Such an analysis would be complicated by the fact that there are many kinds of graphs (dense, sparse, (un)connected, etc.</S>
    <S sid="208" ssid="59">), and the performance results will vary with the properties of the scene graph.</S>
    <S sid="209" ssid="60">If the scene graph is fully connected, finding distinguishing graphs is much harder then when it is fully unconnected.</S>
    <S sid="210" ssid="61">Nevertheless, to give the reader some insight into the performance of the implementation, we applied it to seven test scene graphs of a specific form.</S>
    <S sid="211" ssid="62">The first is our running example: the graph in Figure 3.</S>
    <S sid="212" ssid="63">The other six are obtained by scaling up this graph and permutating (plus, if necessary, adding) properties to make sure that each object can be uniquely described.</S>
    <S sid="213" ssid="64">This implies that only the first test graph is fully connected.</S>
    <S sid="214" ssid="65">The other graphs consist of n4 connected subgraphs, where n is the number of vertices in the graph.</S>
    <S sid="215" ssid="66">Thus, the bigger graphs are relatively less complex than the smaller ones.</S>
    <S sid="216" ssid="67">Each graph consists of 4n looping edges (representing properties) and 3n nonlooping edges (representing spatial relations), again with n the number of vertices.</S>
    <S sid="217" ssid="68">The test was performed on a Windows 2000 PC with a 900 mHz AMD Athlon Processor and 128 Mb RAM.</S>
    <S sid="218" ssid="69">The results are given in Table 1.</S>
    <S sid="219" ssid="70">We measured the system time right before and right after the call of the main function makeReferringExpression.</S>
    <S sid="220" ssid="71">We computed the differences between these two times for a number of target objects from the scene graphs (4 and 8 objects for the first two graphs, 16 for the remaining ones).</S>
    <S sid="221" ssid="72">Note that this measurement does not include the time Java requires for initialization or background activities such as garbage collection.</S>
    <S sid="222" ssid="73">Table 1 shows that even for the larger graphs, the program is able to find minimal distinguishing graphs relatively quickly.</S>
    <S sid="223" ssid="74">The current implementation is a straightforward one (see also Section 6), so optimization, possibly in combination with heuristics, is likely to show further improvements in performance.</S>
  </SECTION>
  <SECTION title="5." number="6">
    <S sid="224" ssid="1">The algorithm described in the previous section (with the uniform cost function assigning one point to each edge and vertex) can be seen as a generalization of Dale&#8217;s (1992) full brevity algorithm, in the sense that there is a guarantee that the algorithm will output the shortest possible description, if one exists.</S>
    <S sid="225" ssid="2">It is also an extension of the full brevity algorithm, since it allows for relational descriptions.</S>
    <S sid="226" ssid="3">In this respect it is comparable to the Dale and Haddock (1991) algorithm, granted that here the problems with infinite recursions do not arise, since a particular edge is either present in a graph or not.</S>
    <S sid="227" ssid="4">Moreover, the approach is fully general and applies to n-ary relations (and relation/property combinations) as well.</S>
    <S sid="228" ssid="5">It is worth noting that Dale&#8217;s (1992) greedy heuristic algorithm (also discussed in Dale and Reiter [1995]) can be cast in the graph framework as well.</S>
    <S sid="229" ssid="6">In fact, this would give us a handle on the order in which different adjacent edges could be tried.</S>
    <S sid="230" ssid="7">The edges associated with the intended referent should be sorted on their descriptive power, which is inversely proportional to the number of occurrences of that particular edge in the scene graph.</S>
    <S sid="231" ssid="8">The algorithm then adds the most discriminating edge (i.e., the one removing most distractors) first.</S>
    <S sid="232" ssid="9">If there are various equally distinguishing edges, the cheapest one is added.</S>
    <S sid="233" ssid="10">This process is then repeated until a distinguishing graph is found.</S>
    <S sid="234" ssid="11">In fact, such a greedy strategy could be used to produce a first nonminimal distinguishing graph.</S>
    <S sid="235" ssid="12">Subsequently we could call findGraph with this graph as initial value of bestGraph, instead of the null graph &#8869;.</S>
    <S sid="236" ssid="13">In this way, we would be able to find minimal graphs more efficiently.</S>
    <S sid="237" ssid="14">The characteristic properties of Dale and Reiter&#8217;s (1995) incremental algorithm can be incorporated into the graph framework as follows.</S>
    <S sid="238" ssid="15">First, the list of preferred attributes can be modeled in terms of the cost function: All type edges should be cheaper than all other edges.</S>
    <S sid="239" ssid="16">In fact, (basic level, see below) type edges could be free.</S>
    <S sid="240" ssid="17">Moreover, the edges corresponding to absolute properties (color) should cost less than those corresponding to relative ones (size).</S>
    <S sid="241" ssid="18">This gives us exactly the effect of having a list of preferred attributes ( type, color, size ).</S>
    <S sid="242" ssid="19">It also implies that the type of an object is always included if it is in any way distinguishing.</S>
    <S sid="243" ssid="20">That by itself does not guarantee that type is always included.</S>
    <S sid="244" ssid="21">The incremental nature of the incremental algorithm can be obtained by ordering edges with respect to their costs.</S>
    <S sid="245" ssid="22">Now the cheapest edges (i.e, those expressing type information) should be tried first, and more expensive edges should be tried later.</S>
    <S sid="246" ssid="23">In addition, the algorithm should terminate as soon as it has found a distinguishing graph.</S>
    <S sid="247" ssid="24">This would guarantee that bargain type loops are always included, and the algorithm would output (iii) from Figure 4 instead of (iii) from Another characteristic property of the original incremental algorithm (not discussed in section 2) is the use of a subsumption hierarchy.</S>
    <S sid="248" ssid="25">A chihuahua, for instance, can be referred to as either a chihuahua or a dog.</S>
    <S sid="249" ssid="26">The latter has a special status and is called the basic level value (see, e.g., Rosch [1978]).</S>
    <S sid="250" ssid="27">According to Dale and Reiter (1995) and Reiter (1990), human speakers have a general preference for basic level values and move to more specific (subsumed) values only if these are more informative.</S>
    <S sid="251" ssid="28">This notion of a subsumption hierarchy can be modeled using the cost function.</S>
    <S sid="252" ssid="29">For a given attribute, the basic level edges should be assigned the lowest costs, and those farthest away from the basic level edge should have the highest costs.</S>
    <S sid="253" ssid="30">This implies that adding an edge labeled dog is cheaper than adding an edge labeled chihuahua.</S>
    <S sid="254" ssid="31">Hence a chihuahua edge will be selected only when there are fewer (or less expensive) additional edges required to construct a distinguishing graph than would be the case for a graph including a dog edge.</S>
    <S sid="255" ssid="32">Note that (assuming that the scene representation is well defined) a distinguishing graph can never contain both a dog and a chihuahua edge, since there will always be a cheaper distinguishing graph omitting one of the two edges.</S>
    <S sid="256" ssid="33">In sum, we can recast the incremental algorithm quite easily in terms of graphs.</S>
    <S sid="257" ssid="34">The original incremental algorithm operates only on properties (looped edges in graph terminology).</S>
    <S sid="258" ssid="35">Recall that when all edges in a scene graph are of the looping variety, testing for subgraph isomorphism becomes trivial.</S>
    <S sid="259" ssid="36">The graph-theoretical reformulation of the incremental algorithm does not fully exploit the possibilities offered by the graph framework and the use of cost functions.</S>
    <S sid="260" ssid="37">First, from the graph-theoretical perspective, the generation of relational descriptions poses no problems.</S>
    <S sid="261" ssid="38">Note that the use of a cost function to simulate subsumption hierarchies for properties carries over directly to relations; for instance, the costs of adding an edge labeled next to should be less than those of adding one labeled left of or right of.</S>
    <S sid="262" ssid="39">Hence, next to will be preferred, unless using left of or right of requires fewer (or less expensive) additional edges for the construction of a distinguishing graph.</S>
    <S sid="263" ssid="40">Another advantage of the way the graphbased algorithm models the list of preferred attributes is that finer-grained distinctions can be made than can with the incremental algorithm.</S>
    <S sid="264" ssid="41">In particular, we are not forced to say that values of the attribute type are always cheaper than values of the attribute color.</S>
    <S sid="265" ssid="42">Instead of assigning costs to attributes, we can assign costs to values of attributes.</S>
    <S sid="266" ssid="43">This gives us the freedom to assign edges labeled with a common color value (e.g., brown) a lower cost than edges labeled with obscure type values, such as Polish owczarek nizinny sheepdog.</S>
    <S sid="267" ssid="44">This implies that it will be cheaper to construct a distinguishing graph referring to an object using two cheap edges (the brown dog) than with one particularly expensive edge (the Polish owczarek nizinny sheepdog).</S>
    <S sid="268" ssid="45">Various aspects of other algorithms can be captured in the graph-based algorithm as well.</S>
    <S sid="269" ssid="46">To further illustrate the flexibility of the graph perspective, we briefly discuss two such aspects.</S>
    <S sid="270" ssid="47">5.3.1 Plurals.</S>
    <S sid="271" ssid="48">Van Deemter&#8217;s (2000) proposal to generate distinguishing plural descriptions (such as the dogs) can be modeled using graphs in the following way.</S>
    <S sid="272" ssid="49">Van Deemter&#8217;s algorithm takes as input a set of target objects, which, in our case, translates into a set of vertices W from the scene graph (W C_ VG).</S>
    <S sid="273" ssid="50">Now the algorithm tries to generate a vertex-graph pair (v, H) that uniquely refers to (W, G).</S>
    <S sid="274" ssid="51">The definition of &#8220;uniquely referring graphs&#8221; has to be generalized slightly to accommodate plurals.</S>
    <S sid="275" ssid="52">The constructed subgraph should refer to each of the vertices in the set W, but not to any of the vertices in the scene graph outside this set.</S>
    <S sid="276" ssid="53">Formally, (v, H) uniquely refers to (W, G) iff H is connected, and for each w E W there is a bijection &#960; such that H C&#960; G, with &#960;.v = w and there is no w' E G\W such that (v, H) refers to (w', G).</S>
    <S sid="277" ssid="54">Observe that the singular case defined in Section 4 is obtained by restricting W to singleton sets.</S>
    <S sid="278" ssid="55">In this way, the basic algorithm can generate both singular and plural distinguishing descriptions.</S>
    <S sid="279" ssid="56">5.3.2 Context and Salience.</S>
    <S sid="280" ssid="57">An object that has been mentioned in recent context is linguistically salient and hence can often be referred to using fewer properties; an animal that is first described as &#8220;the large black dog with the hanging ears&#8221; may subsequently be referred to using an anaphoric description such as &#8220;the dog.&#8221; Krahmer and Theune (2002) model this phenomenon by assigning salience weights (sws) to objects.</S>
    <S sid="281" ssid="58">For this purpose they use a version of centering theory (Grosz, Joshi, and Weinstein 1995) augmented with a recency effect essentially due to Haji&#728;cov&#180;a (1993).</S>
    <S sid="282" ssid="59">Krahmer and Theune then define the set of distractors as the set of objects with a salience weight higher than or equal to that of the target object.</S>
    <S sid="283" ssid="60">In terms of the graphtheoretical framework, this would go as follows.</S>
    <S sid="284" ssid="61">First, we assign salience weights to the vertices in the scene graph using the salience weights definition proposed by Krahmer and Theune.</S>
    <S sid="285" ssid="62">Subsequently, in the sketch of the basic algorithm (Figure 6), the set of distractors should be redefined as follows: In I n E VG n matchGraphs(v, H, n, G) n n =&#65533; v n sw(n) &gt; sw(v)} That is, the distractor set is restricted to those vertices n in the scene graph G that currently are at least as salient as the target object v. For target objects that are linguistically salient, this will typically lead to a reduction of the distractor set.</S>
    <S sid="286" ssid="63">Consequently, distinguishing graphs for these target objects will generally be smaller than those for nonsalient objects.</S>
    <S sid="287" ssid="64">Moreover, we will be able to find distinguishing graphs for a salient object v relatively fast, since we already have a distinguishing graph (constructed for the first definite reference to v) and we can use this graph as our initial value of bestGraph.</S>
    <S sid="288" ssid="65">One of the important open questions in natural language generation is how the common rule-based approaches to generation can be combined with recent insights from statistical natural language processing (see, e.g., Langkilde and Knight [1998] and Malouf [2000] for partial answers).</S>
    <S sid="289" ssid="66">The approach proposed in this article makes it possible to combine graph reformulations of well-known rule-based generation algorithms with stochastic cost functions (the result resembles a Markov model).</S>
    <S sid="290" ssid="67">Such a cost function could be derived from a sufficiently large corpus.</S>
    <S sid="291" ssid="68">For instance, as a first approximation we could define the costs of adding an edge e in terms of the probability P(e) that e occurs in a distinguishing description (estimated by counting occurrences): Thus, properties that occur frequently are cheap; properties that are relatively rare are expensive.</S>
    <S sid="292" ssid="69">In this way, we would probably derive that polish owczarek nizinny sheepdog indeed costs more (and is thus less likely to be selected) than brown.</S>
    <S sid="293" ssid="70">Even though this first approximation already has some interesting consequences, it is probably not enough to obtain a plausible and useful cost function.</S>
    <S sid="294" ssid="71">For instance, it is unlikely that the co-occurrence of edges is fully independent; a husky is likely to be white, and a chihuahua is not.</S>
    <S sid="295" ssid="72">Such dependencies are not modeled by the definition given above.</S>
    <S sid="296" ssid="73">In addition, properties referring to size such as small and large probably occur more often in a corpus than properties referring to colors such as brown or yellow, which at first sight appears to run counter to the earlier observation that speakers generally prefer absolute properties over relative ones.</S>
    <S sid="297" ssid="74">The reason for this, however, is probably that there are simply fewer ways to describe the size than there are to describe the color of objects.</S>
    <S sid="298" ssid="75">Searching for a more sophisticated method of defining stochastic cost functions is therefore an interesting line of future research.</S>
    <S sid="299" ssid="76">In this article, we have presented a new approach to the content determination problem for referring expressions.</S>
    <S sid="300" ssid="77">We proposed to model scenes as labeled directed graphs, in which objects are represented as vertices and the properties and relations of these objects are represented as edges.</S>
    <S sid="301" ssid="78">The problem of finding a referring expression for an object is treated as finding a subgraph of the scene graph that is isomorphic to the intended referent but not to any other object.</S>
    <S sid="302" ssid="79">The theoretical complexity of this reformulation of the content determination problem is NP-complete, but there exist various restrictions (planar graphs, decision trees for fixed scene graphs, upper bound to the number of edges in a distinguishing graph) that have a polynomial complexity.</S>
    <S sid="303" ssid="80">We have described a general and fully implemented algorithm, based on the subgraph isomorphism idea, consisting of two main functions: one that constructs referring graphs and one that tests for subgraph isomorphisms.</S>
    <S sid="304" ssid="81">Cost functions are used to guide the search process and to give preference to some solutions over others.</S>
    <S sid="305" ssid="82">Optimization has not been the focus of this article, but we came across various heuristic strategies that would speed up the algorithm.</S>
    <S sid="306" ssid="83">For instance, we can try edges in the order determined by the cost function (from cheap to more expensive), and we can use a greedy algorithm to find a first distinguishing graph quickly.</S>
    <S sid="307" ssid="84">In general, one of the advantages of the graph perspective is that many efficient algorithms for dealing with graph structures are known.</S>
    <S sid="308" ssid="85">We can use those algorithms to formulate more efficient versions of the subgraph construction component (perhaps using the method of Tarjan [1972]; see also Sedgewick [1988]) and of the subgraph isomorphism testing component (e.g., using the aforementioned approach of Messmer and Bunke [1995, 1998]).</S>
    <S sid="309" ssid="86">The graph perspective has a number of attractive properties.</S>
    <S sid="310" ssid="87">(1) By reformulating the content determination problem as a graph construction problem, we can directly apply the many techniques and algorithms for dealing with graph structures.</S>
    <S sid="311" ssid="88">(2) The use of cost functions allows us to model different search methods, each restricting the search space in its own way.</S>
    <S sid="312" ssid="89">By defining cost functions in different ways, we can mimic and extend various well-known algorithms from the literature (see also Krahmer, van Erk, and Verleg [2001]).</S>
    <S sid="313" ssid="90">(3) The generation of relational descriptions is straightforward; the problems that plague some other algorithms for the generation of relational descriptions do not arise.</S>
    <S sid="314" ssid="91">Moreover, the approach to relations proposed here is fully general: It applies to all n-ary relations, not just binary ones.</S>
    <S sid="315" ssid="92">(4) The use of cost functions paves the way for integrating statistical information directly into the generation process.</S>
    <S sid="316" ssid="93">In fact, performing experiments with various ways to estimate stochastic cost functions from corpora is one path for future research that we have identified.</S>
    <S sid="317" ssid="94">Besides looking for graph-based optimizations and performing experiments with stochastic cost functions, there are three other lines for future research we would like to mention.</S>
    <S sid="318" ssid="95">The first concerns the construction of scene graphs.</S>
    <S sid="319" ssid="96">How should the decision be made as to which aspects of a scene to represent in the graph?</S>
    <S sid="320" ssid="97">Naturally, the algorithm can only refer to entities that are modeled in the scene graph, but representing every possible object in a single graph will lead to an explosion of edges and vertices.</S>
    <S sid="321" ssid="98">Perhaps some notion of focus of attention can be used to restrict the scene graph.</S>
    <S sid="322" ssid="99">It would also be interesting to look for automatic methods for the construction of scene graphs.</S>
    <S sid="323" ssid="100">We might use computer vision algorithms (see, e.g., Faugeras [1993]), which are often graph-based themselves, for this purpose.</S>
    <S sid="324" ssid="101">For example, Bauckhage et al. (1999) describe an assembly system in which computer vision is used to convert a workspace with various building blocks into a labeled directed scene graph.</S>
    <S sid="325" ssid="102">Note that this approach is also able to deal with dynamic scenes; it can track changes in the workspace (which is required for handling the assembly process).</S>
    <S sid="326" ssid="103">Another issue that we have not discussed in much detail is linguistic realization.</S>
    <S sid="327" ssid="104">How should the information contained in a referring graph be expressed in natural language?</S>
    <S sid="328" ssid="105">So far, we have assumed that a distinguishing graph can simply be constructed first and subsequently fed into a realization engine.</S>
    <S sid="329" ssid="106">There may, however, be certain dependencies between content selection and realization (see, e.g., Horacek [1997] and Krahmer and Theune [2002]).</S>
    <S sid="330" ssid="107">One way to take these dependencies into account would be to reformulate the cost function in such a way that it promotes graphs that can easily be realized and punishes graphs that are more difficult to realize.</S>
    <S sid="331" ssid="108">A final aspect of the graph model that deserves further investigation is based on the fact that we can look at a graph such as that in Figure 3 as a Kripke model.</S>
    <S sid="332" ssid="109">Kripke models are used in model-theoretic semantics for modal logics.</S>
    <S sid="333" ssid="110">The advantage of looking at graphs such as that in Figure 3 as Kripke models is that we can use tools from modal logic to reason about these structures.</S>
    <S sid="334" ssid="111">For example, we can reformulate the problem of determining the content of a distinguishing description in terms of hybrid logic (see, e.g., Blackburn [2000]) as follows: @i&#981; &#8743; Aj(i =&#65533; j &#8212; _@j&#981;) In words: When we want to refer to vertex i, we are looking for that distinguishing formula &#981; that is true of (&#8220;at&#8221;) i but not of any j different from i.</S>
    <S sid="335" ssid="112">One advantage of this logical perspective is that logical properties that are not covered by most generation algorithms (such as not having a certain property; see van Deemter [2002]) fit in very well with this perspective.</S>
  </SECTION>
  <SECTION title="Appendix: Planarizing Scene Graphs" number="7">
    <S sid="336" ssid="1">Planar graphs may be relevant for our current purposes, since subgraph isomorphism can be tested more efficiently on planar graphs than on arbitrary graphs.</S>
    <S sid="337" ssid="2">There are two ways in which a nonplanar graph G can be turned into a planar one G' (see Liebers [2001] for a recent overview of planarization algorithms): Either the graph G can be pruned (using vertex or edge deletion) or it can be extended (for instance, using vertex splitting or by inserting vertices at crossings).</S>
    <S sid="338" ssid="3">A disadvantage of the extension approach is that we lose the intuitive one-to-one correspondence between potential target objects and vertices in the scene graph, since the additional vertices only serve the purpose of planarizing the graph and do not represent objects in a scene.</S>
    <S sid="339" ssid="4">A disadvantage of the pruning approach is that we lose information.</S>
    <S sid="340" ssid="5">The presence of a cost function, however, is potentially very useful, since it allows us to avoid eliminating comparatively cheap (and thus more frequently selected) edges.</S>
    <S sid="341" ssid="6">Here, for the sake of illustration, we briefly describe a weighted greedy pruning algorithm that turns an arbitrary scene graph G = (VG, EG) with n vertices and m edges into a planar graph G' = (VG,, EGI) with n vertices and at most m edges.</S>
    <S sid="342" ssid="7">We start from the graph G' = (VG, OG), where OG is the set of looping edges from the scene graph, that is, OG = UvEVG EG(v,v).</S>
    <S sid="343" ssid="8">Next we order the remaining edges from the scene graph RG = EG\OG with respect to their costs; the cheapest one comes first, the more expensive ones come later, in order of increasing expense.</S>
    <S sid="344" ssid="9">For each e E RG, we check whether G' + e is planar (e.g., using the algorithm from Hopcroft and Tarjan [1974]).</S>
    <S sid="345" ssid="10">If it is, e is added to EGI.</S>
    <S sid="346" ssid="11">The algorithm terminates when RG = 0.</S>
    <S sid="347" ssid="12">The result is a maximal planar subgraph G' of the scene graph G that differs from G only possibly in the deletion of certain relatively expensive nonlooping (relational) edges.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="348" ssid="1">We would like to thank Dennis van Oort and Denis Gerritsen for their help in the implementation and Alexander Koller and Kees van Deemter for some very useful discussions.</S>
    <S sid="349" ssid="2">Thanks are also due to Paul Piwek, Mari&#168;et Theune, Ielka van der Sluis, and the anonymous reviewers for helpful comments on an earlier version of this article.</S>
  </SECTION>
</PAPER>
