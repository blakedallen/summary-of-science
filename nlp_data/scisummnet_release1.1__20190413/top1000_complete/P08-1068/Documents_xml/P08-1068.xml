<PAPER>
  <S sid="0">Simple Semi-supervised Dependency Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We present a simple and effective semisupervised method for training dependency parsers.</S>
    <S sid="2" ssid="2">We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus.</S>
    <S sid="3" ssid="3">We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions.</S>
    <S sid="4" ssid="4">For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">In natural language parsing, lexical information is seen as crucial to resolving ambiguous relationships, yet lexicalized statistics are sparse and difficult to estimate directly.</S>
    <S sid="6" ssid="2">It is therefore attractive to consider intermediate entities which exist at a coarser level than the words themselves, yet capture the information necessary to resolve the relevant ambiguities.</S>
    <S sid="7" ssid="3">In this paper, we introduce lexical intermediaries via a simple two-stage semi-supervised approach.</S>
    <S sid="8" ssid="4">First, we use a large unannotated corpus to define word clusters, and then we use that clustering to construct a new cluster-based feature mapping for a discriminative learner.</S>
    <S sid="9" ssid="5">We are thus relying on the ability of discriminative learning methods to identify and exploit informative features while remaining agnostic as to the origin of such features.</S>
    <S sid="10" ssid="6">To demonstrate the effectiveness of our approach, we conduct experiments in dependency parsing, which has been the focus of much recent research&#8212;e.g., see work in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007).</S>
    <S sid="11" ssid="7">The idea of combining word clusters with discriminative learning has been previously explored by Miller et al. (2004), in the context of namedentity recognition, and their work directly inspired our research.</S>
    <S sid="12" ssid="8">However, our target task of dependency parsing involves more complex structured relationships than named-entity tagging; moreover, it is not at all clear that word clusters should have any relevance to syntactic structure.</S>
    <S sid="13" ssid="9">Nevertheless, our experiments demonstrate that word clusters can be quite effective in dependency parsing applications.</S>
    <S sid="14" ssid="10">In general, semi-supervised learning can be motivated by two concerns: first, given a fixed amount of supervised data, we might wish to leverage additional unlabeled data to facilitate the utilization of the supervised corpus, increasing the performance of the model in absolute terms.</S>
    <S sid="15" ssid="11">Second, given a fixed target performance level, we might wish to use unlabeled data to reduce the amount of annotated data necessary to reach this target.</S>
    <S sid="16" ssid="12">We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Haji&#711;c, 1998; Haji&#711;c et al., 2001) (see Sections 4.1 and 4.3).</S>
    <S sid="17" ssid="13">By conducting experiments on datasets of varying sizes, we demonstrate that for fixed levels of performance, the cluster-based approach can reduce the need for supervised data by roughly half, which is a substantial savings in data-annotation costs (see Sections 4.2 and 4.4).</S>
    <S sid="18" ssid="14">The remainder of this paper is divided as follows: Section 2 gives background on dependency parsing and clustering, Section 3 describes the cluster-based features, Section 4 presents our experimental results, Section 5 discusses related work, and Section 6 concludes with ideas for future research.</S>
  </SECTION>
  <SECTION title="2 Background" number="2">
    <S sid="19" ssid="1">Recent work (Buchholz and Marsi, 2006; Nivre et al., 2007) has focused on dependency parsing.</S>
    <S sid="20" ssid="2">Dependency syntax represents syntactic information as a network of head-modifier dependency arcs, typically restricted to be a directed tree (see Figure 1 for an example).</S>
    <S sid="21" ssid="3">Dependency parsing depends critically on predicting head-modifier relationships, which can be difficult due to the statistical sparsity of these word-to-word interactions.</S>
    <S sid="22" ssid="4">Bilexical dependencies are thus ideal candidates for the application of coarse word proxies such as word clusters.</S>
    <S sid="23" ssid="5">In this paper, we take a part-factored structured classification approach to dependency parsing.</S>
    <S sid="24" ssid="6">For a given sentence x, let Y(x) denote the set of possible dependency structures spanning x, where each y E Y(x) decomposes into a set of &#8220;parts&#8221; r E y.</S>
    <S sid="25" ssid="7">In the simplest case, these parts are the dependency arcs themselves, yielding a first-order or &#8220;edge-factored&#8221; dependency parsing model.</S>
    <S sid="26" ssid="8">In higher-order parsing models, the parts can consist of interactions between more than two words.</S>
    <S sid="27" ssid="9">For example, the parser of McDonald and Pereira (2006) defines parts for sibling interactions, such as the trio &#8220;plays&#8221;, &#8220;Elianti&#8221;, and &#8220;.&#8221; in Figure 1.</S>
    <S sid="28" ssid="10">The Carreras (2007) parser has parts for both sibling interactions and grandparent interactions, such as the trio &#8220;*&#8221;, &#8220;plays&#8221;, and &#8220;Haag&#8221; in Figure 1.</S>
    <S sid="29" ssid="11">These kinds of higher-order factorizations allow dependency parsers to obtain a limited form of context-sensitivity.</S>
    <S sid="30" ssid="12">Given a factorization of dependency structures into parts, we restate dependency parsing as the folAbove, we have assumed that each part is scored by a linear model with parameters w and featuremapping f(&#183;).</S>
    <S sid="31" ssid="13">For many different part factorizations and structure domains Y(&#183;), it is possible to solve the above maximization efficiently, and several recent efforts have concentrated on designing new maximization algorithms with increased contextsensitivity (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007).</S>
    <S sid="32" ssid="14">In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992).</S>
    <S sid="33" ssid="15">We chose to work with the Brown algorithm due to its simplicity and prior success in other NLP applications (Miller et al., 2004; Liang, 2005).</S>
    <S sid="34" ssid="16">However, we expect that our approach can function with other clustering algorithms (as in, e.g., Li and McCallum (2005)).</S>
    <S sid="35" ssid="17">We briefly describe the Brown algorithm below.</S>
    <S sid="36" ssid="18">The input to the algorithm is a vocabulary of words to be clustered and a corpus of text containing these words.</S>
    <S sid="37" ssid="19">Initially, each word in the vocabulary is considered to be in its own distinct cluster.</S>
    <S sid="38" ssid="20">The algorithm then repeatedly merges the pair of clusters which causes the smallest decrease in the likelihood of the text corpus, according to a class-based bigram language model defined on the word clusters.</S>
    <S sid="39" ssid="21">By tracing the pairwise merge operations, one obtains a hierarchical clustering of the words, which can be represented as a binary tree as in Figure 2.</S>
    <S sid="40" ssid="22">Within this tree, each word is uniquely identified by its path from the root, and this path can be compactly represented with a bit string, as in Figure 2.</S>
    <S sid="41" ssid="23">In order to obtain a clustering of the words, we select all nodes at a certain depth from the root of the hierarchy.</S>
    <S sid="42" ssid="24">For example, in Figure 2 we might select the four nodes at depth 2 from the root, yielding the clusters {apple,pear}, {Apple,IBM}, {bought,run}, and {of,in}.</S>
    <S sid="43" ssid="25">Note that the same clustering can be obtained by truncating each word&#8217;s bit-string to a 2-bit prefix.</S>
    <S sid="44" ssid="26">By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al., 2004).</S>
    <S sid="45" ssid="27">For all of the experiments in this paper, we used the Liang (2005) implementation of the Brown algorithm to obtain the necessary word clusters.</S>
  </SECTION>
  <SECTION title="3 Feature design" number="3">
    <S sid="46" ssid="1">Key to the success of our approach is the use of features which allow word-cluster-based information to assist the parser.</S>
    <S sid="47" ssid="2">The feature sets we used are similar to other feature sets in the literature (McDonald et al., 2005a; Carreras, 2007), so we will not attempt to give a exhaustive description of the features in this section.</S>
    <S sid="48" ssid="3">Rather, we describe our features at a high level and concentrate on our methodology and motivations.</S>
    <S sid="49" ssid="4">In our experiments, we employed two different feature sets: a baseline feature set which draws upon &#8220;normal&#8221; information sources such as word forms and parts of speech, and a cluster-based feature set that also uses information derived from the Brown cluster hierarchy.</S>
    <S sid="50" ssid="5">Our first-order baseline feature set is similar to the feature set of McDonald et al. (2005a), and consists of indicator functions for combinations of words and parts of speech for the head and modifier of each dependency, as well as certain contextual tokens.1 Our second-order baseline features are the same as those of Carreras (2007) and include indicators for triples of part of speech tags for sibling interactions and grandparent interactions, as well as additional bigram features based on pairs of words involved these higher-order interactions.</S>
    <S sid="51" ssid="6">Examples of baseline features are provided in Table 1. tag.</S>
    <S sid="52" ssid="7">Abbreviations: ht = head POS, hw = head word, hc4 = 4-bit prefix of head, hc6 = 6-bit prefix of head, hc* = full bit string of head; mt,mw,mc4,mc6,mc* = likewise for modifier; st,gt,sc4,gc4,... = likewise for sibling and grandchild.</S>
    <S sid="53" ssid="8">The first- and second-order cluster-based feature sets are supersets of the baseline feature sets: they include all of the baseline feature templates, and add an additional layer of features that incorporate word clusters.</S>
    <S sid="54" ssid="9">Following Miller et al. (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity.</S>
    <S sid="55" ssid="10">We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al. (2004) work (between 12 and 20 bits) performed poorly in dependency parsing.2 After experimenting with many different feature configurations, we eventually settled on a simple but effective methodology.</S>
    <S sid="56" ssid="11">First, we found that it was helpful to employ two different types of word clusters: 2.</S>
    <S sid="57" ssid="12">Full bit strings,3 which we used as substitutes for word forms.</S>
    <S sid="58" ssid="13">Using these two types of clusters, we generated new features by mimicking the template structure of the original baseline features.</S>
    <S sid="59" ssid="14">For example, the baseline feature set includes indicators for word-to-word and tag-to-tag interactions between the head and modifier of a dependency.</S>
    <S sid="60" ssid="15">In the cluster-based feature set, we correspondingly introduce new indicators for interactions between pairs of short bit-string prefixes and pairs of full bit strings.</S>
    <S sid="61" ssid="16">Some examples of cluster-based features are given in Table 1.</S>
    <S sid="62" ssid="17">Second, we found it useful to concentrate on &#8220;hybrid&#8221; features involving, e.g., one bit-string and one part of speech.</S>
    <S sid="63" ssid="18">In our initial attempts, we focused on features that used cluster information exclusively.</S>
    <S sid="64" ssid="19">While these cluster-only features provided some benefit, we found that adding hybrid features resulted in even greater improvements.</S>
    <S sid="65" ssid="20">One possible explanation is that the clusterings generated by the Brown algorithm can be noisy or only weakly relevant to syntax; thus, the clusters are best exploited when &#8220;anchored&#8221; to words or parts of speech.</S>
    <S sid="66" ssid="21">Finally, we found it useful to impose a form of vocabulary restriction on the cluster-based features.</S>
    <S sid="67" ssid="22">Specifically, for any feature that is predicated on a word form, we eliminate this feature if the word in question is not one of the top-N most frequent words in the corpus.</S>
    <S sid="68" ssid="23">When N is between roughly 100 and 1,000, there is little effect on the performance of the cluster-based feature sets.4 In addition, the vocabulary restriction reduces the size of the feature sets to managable proportions.</S>
  </SECTION>
  <SECTION title="4 Experiments" number="4">
    <S sid="69" ssid="1">In order to evaluate the effectiveness of the clusterbased feature sets, we conducted dependency parsing experiments in English and Czech.</S>
    <S sid="70" ssid="2">We test the features in a wide range of parsing configurations, including first-order and second-order parsers, and labeled and unlabeled parsers.5 3As in Brown et al. (1992), we limit the clustering algorithm so that it recovers at most 1,000 distinct bit-strings; thus full bit strings are not equivalent to word forms.</S>
    <S sid="71" ssid="3">The English experiments were performed on the Penn Treebank (Marcus et al., 1993), using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to a dependency tree representation.6 We split the Treebank into a training set (Sections 2&#8211;21), a development set (Section 22), and several test sets (Sections 0,7 1, 23, and 24).</S>
    <S sid="72" ssid="4">The data partition and head rules were chosen to match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006).</S>
    <S sid="73" ssid="5">The part of speech tags for the development and test data were automatically assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus; to generate part of speech tags for the training data, we used 10-way jackknifing.8 English word clusters were derived from the BLLIP corpus (Charniak et al., 2000), which contains roughly 43 million words of Wall Street Journal text.9 The Czech experiments were performed on the Prague Dependency Treebank 1.0 (Haji&#711;c, 1998; Haji&#711;c et al., 2001), which is directly annotated with dependency structures.</S>
    <S sid="74" ssid="6">To facilitate comparisons with previous work (McDonald et al., 2005b; McDonald and Pereira, 2006), we used the training/development/test partition defined in the corpus and we also used the automatically-assigned part of speech tags provided in the corpus.10 Czech word clusters were derived from the raw text section of the PDT 1.0, which contains about 39 million words of newswire text.11 We trained the parsers using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which represents a balance between strong performance and fast training times.</S>
    <S sid="75" ssid="7">To select the number of iterations of perceptron training, we performed up to 30 iterations and chose the iteration which optimized accuracy on the development set.</S>
    <S sid="76" ssid="8">Our feature mappings are quite high-dimensional, so we eliminated all features which occur only once in the training data.</S>
    <S sid="77" ssid="9">The resulting models still had very high dimensionality, ranging from tens of millions to as many as a billion features.12 All results presented in this section are given in terms of parent-prediction accuracy, which measures the percentage of tokens that are attached to the correct head token.</S>
    <S sid="78" ssid="10">For labeled dependency structures, both the head token and dependency label must be correctly predicted.</S>
    <S sid="79" ssid="11">In addition, in English parsing we ignore the parent-predictions of punctuation tokens,13 and in Czech parsing we retain the punctuation tokens; this matches previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006).</S>
    <S sid="80" ssid="12">In our English experiments, we tested eight different parsing configurations, representing all possible choices between baseline or cluster-based feature sets, first-order (Eisner, 2000) or second-order (Carreras, 2007) factorizations, and labeled or unlabeled parsing.</S>
    <S sid="81" ssid="13">Table 2 compiles our final test results and also includes two results from previous work by McDonald et al. (2005a) and McDonald and Pereira (2006), for the purposes of comparison.</S>
    <S sid="82" ssid="14">We note a few small differences between our parsers and the 12Due to the sparsity of the perceptron updates, however, only a small fraction of the possible features were active in our trained models.</S>
    <S sid="83" ssid="15">13A punctuation token is any token whose gold-standard part of speech tag is one of {&#8216;&#8216; &#8217;&#8217; .1. parsers evaluated in this previous work.</S>
    <S sid="84" ssid="16">First, the MD1 and MD2 parsers were trained via the MIRA algorithm (Crammer and Singer, 2003; Crammer et al., 2004), while we use the averaged perceptron.</S>
    <S sid="85" ssid="17">In addition, the MD2 model uses only sibling interactions, whereas the dep2/dep2c parsers include both sibling and grandparent interactions.</S>
    <S sid="86" ssid="18">There are some clear trends in the results of Table 2.</S>
    <S sid="87" ssid="19">First, performance increases with the order of the parser: edge-factored models (dep1 and MD1) have the lowest performance, adding sibling relationships (MD2) increases performance, and adding grandparent relationships (dep2) yields even better accuracies.</S>
    <S sid="88" ssid="20">Similar observations regarding the effect of model order have also been made by Carreras (2007).</S>
    <S sid="89" ssid="21">Second, note that the parsers using cluster-based feature sets consistently outperform the models using the baseline features, regardless of model order or label usage.</S>
    <S sid="90" ssid="22">Some of these improvements can be quite large; for example, a first-order model using cluster-based features generally performs as well as a second-order model using baseline features.</S>
    <S sid="91" ssid="23">Moreover, the benefits of cluster-based feature sets combine additively with the gains of increasing model order.</S>
    <S sid="92" ssid="24">For example, consider the unlabeled parsers in Table 2: on Section 23, increasing the model order from dep1 to dep2 results in a relative reduction in error of roughly 13%, while introducing clusterbased features from dep2 to dep2c yields an additional relative error reduction of roughly 14%.</S>
    <S sid="93" ssid="25">As a final note, all 16 comparisons between cluster-based features and baseline features shown in Table 2 are statistically significant.14 We performed additional experiments to evaluate the effect of the cluster-based features as the amount of training data is varied.</S>
    <S sid="94" ssid="26">Note that the dependency parsers we use require the input to be tagged with parts of speech; thus the quality of the part-ofspeech tagger can have a strong effect on the performance of the parser.</S>
    <S sid="95" ssid="27">In these experiments, we consider two possible scenarios: for training both tagger and parser.</S>
    <S sid="96" ssid="28">Table 3 displays the accuracy of first- and secondorder models when trained on smaller portions of the Treebank, in both scenarios described above.</S>
    <S sid="97" ssid="29">Note that the cluster-based features obtain consistent gains regardless of the size of the training set.</S>
    <S sid="98" ssid="30">When the tagger is trained on the reduced-size datasets, the gains of cluster-based features are more pronounced, but substantial improvements are obtained even when the tagger is accurate.</S>
    <S sid="99" ssid="31">It is interesting to consider the amount by which cluster-based features reduce the need for supervised data, given a desired level of accuracy.</S>
    <S sid="100" ssid="32">Based on Table 3, we can extrapolate that cluster-based features reduce the need for supervised data by roughly a factor of 2.</S>
    <S sid="101" ssid="33">For example, the performance of the dep1c and dep2c models trained on 1k sentences is roughly the same as the performance of the dep1 and dep2 models, respectively, trained on 2k sentences.</S>
    <S sid="102" ssid="34">This approximate data-halving effect can be observed throughout the results in Table 3.</S>
    <S sid="103" ssid="35">When combining the effects of model order and cluster-based features, the reductions in the amount of supervised data required are even larger.</S>
    <S sid="104" ssid="36">For example, in scenario 1 the dep2c model trained on 1k sentences is close in performance to the dep1 model trained on 4k sentences, and the dep2c model trained on 4k sentences is close to the dep1 model trained on the entire training set (roughly 40k sentences).</S>
    <S sid="105" ssid="37">In our Czech experiments, we considered only unlabeled parsing,15 leaving four different parsing configurations: baseline or cluster-based features and first-order or second-order parsing.</S>
    <S sid="106" ssid="38">Note that our feature sets were originally tuned for English parsing, and except for the use of Czech clusters, we made no attempt to retune our features for Czech.</S>
    <S sid="107" ssid="39">Czech dependency structures may contain nonprojective edges, so we employ a maximum directed spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b) as our firstorder parser for Czech.</S>
    <S sid="108" ssid="40">For the second-order parsing experiments, we used the Carreras (2007) parser.</S>
    <S sid="109" ssid="41">Since this parser only considers projective dependency structures, we &#8220;projectivized&#8221; the PDT 1.0 training set by finding, for each sentence, the projective tree which retains the most correct dependencies; our second-order parsers were then trained with respect to these projective trees.</S>
    <S sid="110" ssid="42">The development and test sets were not projectivized, so our secondorder parser is guaranteed to make errors in test sentences containing non-projective dependencies.</S>
    <S sid="111" ssid="43">To overcome this, McDonald and Pereira (2006) use a two-stage approximate decoding process in which the output of their second-order parser is &#8220;deprojectivized&#8221; via greedy search.</S>
    <S sid="112" ssid="44">For simplicity, we did not implement a deprojectivization stage on top of our second-order parser, but we conjecture that such techniques may yield some additional performance gains; we leave this to future work.</S>
    <S sid="113" ssid="45">Table 4 gives accuracy results on the PDT 1.0 test set for our unlabeled parsers.</S>
    <S sid="114" ssid="46">As in the English experiments, there are clear trends in the results: parsers using cluster-based features outperform parsers using baseline features, and secondorder parsers outperform first-order parsers.</S>
    <S sid="115" ssid="47">Both of the comparisons between cluster-based and baseline features in Table 4 are statistically significant.16 Table 5 compares accuracy results on the PDT 1.0 test set for our parsers and several other recent papers.</S>
    <S sid="116" ssid="48">As in our English experiments, we performed additional experiments on reduced sections of the PDT; the results are shown in Table 6.</S>
    <S sid="117" ssid="49">For simplicity, we did not retrain a tagger for each reduced dataset, so we always use the (automatically-assigned) part of speech tags provided in the corpus.</S>
    <S sid="118" ssid="50">Note that the cluster-based features obtain improvements at all training set sizes, with data-reduction factors similar to those observed in English.</S>
    <S sid="119" ssid="51">For example, the dep1c model trained on 4k sentences is roughly as good as the dep1 model trained on 8k sentences.</S>
    <S sid="120" ssid="52">Here, we present two additional results which further explore the behavior of the cluster-based feature sets.</S>
    <S sid="121" ssid="53">In Table 7, we show the development-set performance of second-order parsers as the threshold for lexical feature elimination (see Section 3.2) is varied.</S>
    <S sid="122" ssid="54">Note that the performance of cluster-based features is fairly insensitive to the threshold value, whereas the performance of baseline features clearly degrades as the vocabulary size is reduced.</S>
    <S sid="123" ssid="55">In Table 8, we show the development-set performance of the first- and second-order parsers when features containing part-of-speech-based information are eliminated.</S>
    <S sid="124" ssid="56">Note that the performance obtained by using clusters without parts of speech is close to the performance of the baseline features.</S>
  </SECTION>
  <SECTION title="5 Related Work" number="5">
    <S sid="125" ssid="1">As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach.</S>
    <S sid="126" ssid="2">Our research, however, applies this technique to dependency parsing rather than named-entity recognition.</S>
    <S sid="127" ssid="3">In this paper, we have focused on developing new representations for lexical information.</S>
    <S sid="128" ssid="4">Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007).</S>
    <S sid="129" ssid="5">These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand.</S>
    <S sid="130" ssid="6">Crucially, however, these methods do not exploit unlabeled data when learning their representations.</S>
    <S sid="131" ssid="7">Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task.</S>
    <S sid="132" ssid="8">Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased features can be viewed as being a kind of &#8220;backed-off&#8221; version of the baseline features.</S>
    <S sid="133" ssid="9">However, our work is focused on discriminative learning as opposed to generative models.</S>
    <S sid="134" ssid="10">Semi-supervised phrase structure parsing has been previously explored by McClosky et al. (2006), who applied a reranked parser to a large unsupervised corpus in order to obtain additional training data for the parser; this self-training appraoch was shown to be quite effective in practice.</S>
    <S sid="135" ssid="11">However, their approach depends on the usage of a high-quality parse reranker, whereas the method described here simply augments the features of an existing parser.</S>
    <S sid="136" ssid="12">Note that our two approaches are compatible in that we could also design a reranker and apply self-training techniques on top of the clusterbased features.</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="6">
    <S sid="137" ssid="1">In this paper, we have presented a simple but effective semi-supervised learning approach and demonstrated that it achieves substantial improvement over a competitive baseline in two broad-coverage dependency parsing tasks.</S>
    <S sid="138" ssid="2">Despite this success, there are several ways in which our approach might be improved.</S>
    <S sid="139" ssid="3">To begin, recall that the Brown clustering algorithm is based on a bigram language model.</S>
    <S sid="140" ssid="4">Intuitively, there is a &#8220;mismatch&#8221; between the kind of lexical information that is captured by the Brown clusters and the kind of lexical information that is modeled in dependency parsing.</S>
    <S sid="141" ssid="5">A natural avenue for further research would be the development of clustering algorithms that reflect the syntactic behavior of words; e.g., an algorithm that attempts to maximize the likelihood of a treebank, according to a probabilistic dependency model.</S>
    <S sid="142" ssid="6">Alternately, one could design clustering algorithms that cluster entire head-modifier arcs rather than individual words.</S>
    <S sid="143" ssid="7">Another idea would be to integrate the clustering algorithm into the training algorithm in a limited fashion.</S>
    <S sid="144" ssid="8">For example, after training an initial parser, one could parse a large amount of unlabeled text and use those parses to improve the quality of the clusters.</S>
    <S sid="145" ssid="9">These improved clusters can then be used to retrain an improved parser, resulting in an overall algorithm similar to that of McClosky et al. (2006).</S>
    <S sid="146" ssid="10">Setting aside the development of new clustering algorithms, a final area for future work is the extension of our method to new domains, such as conversational text or other languages, and new NLP problems, such as machine translation.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="7">
    <S sid="147" ssid="1">The authors thank the anonymous reviewers for their insightful comments.</S>
    <S sid="148" ssid="2">Many thanks also to Percy Liang for providing his implementation of the Brown algorithm, and Ryan McDonald for his assistance with the experimental setup.</S>
    <S sid="149" ssid="3">The authors gratefully acknowledge the following sources of support.</S>
    <S sid="150" ssid="4">Terry Koo was funded by NSF grant DMS-0434222 and a grant from NTT, Agmt.</S>
    <S sid="151" ssid="5">Dtd.</S>
    <S sid="152" ssid="6">6/21/1998.</S>
    <S sid="153" ssid="7">Xavier Carreras was supported by the Catalan Ministry of Innovation, Universities and Enterprise, and a grant from NTT, Agmt.</S>
    <S sid="154" ssid="8">Dtd.</S>
    <S sid="155" ssid="9">6/21/1998.</S>
    <S sid="156" ssid="10">Michael Collins was funded by NSF grants 0347631 and DMS-0434222.</S>
  </SECTION>
</PAPER>
