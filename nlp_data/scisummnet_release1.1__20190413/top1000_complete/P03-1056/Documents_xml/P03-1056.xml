<PAPER>
  <S sid="0">Is It Harder To Parse Chinese Or The Chinese Treebank?</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Table 3: Frequency of parse error types. hancements that can be used to address it.</S>
    <S sid="2" ssid="2">4.1 Analysis by error type and PCFG-enrichment fixes Multilevel VP adjunction errors (Figure 5) are common in models without parent annotation, although even with parent annotation the presence of VP coordination would give multilevel VP adjunction nonzero probability.</S>
    <S sid="3" ssid="3">We address this error by taking advantage of the CTB's principled VP annotation practices, marking adjunction, complementation, and coordination VP levels, which builds the flat adjunction constraint back into the structure of the head daughter.</S>
    <S sid="4" ssid="4">NP-NP modification, depicted in NNM&#177; in Figure 4, was the most common error seen; the greater prevalence of false positives is likely a result of the overall PCFG parsing preference for flatter structures.</S>
    <S sid="5" ssid="5">This type of parse ambiguity is grounded in the semantic ambiguity of compound noun interpretation.</S>
    <S sid="6" ssid="6">This semantic ambiguity exists in English as well, as in the NP-NP modification false positive/negative PNM{+/&#8212;} (non-NP) prenominal mod. false positive/negative CRD{H,L}{V,N} incorrect {high/low} coordination attachment of righthand {verbal/nominal} material X/Y incorrect adjunction into X; rect site was Y mistag X/Y category Y mistagged as X that only mistaggings leading to constituentlevel parse errors were tallied.</S>
    <S sid="7" ssid="7">VP (-ADJ) ADVP ADVP VP (-COMP) NP AD AD VV NP 1 1 1 NP i&#8226; Itg 4k 4g* A A Fi' positively investigate profession NR NN NR I I I NN VP i4 A * 01 ADVP VP Shanghai customs Chorigm rig office AD ADVP VP I NR NN NR NN i&#8226; AD VV NP I I I I i4 A * 01 044' Itg 4k 4g* A A Fi' Shanghai customs Chorigming office positively investigate high-risk profession NP NNM&#177;* Figure 5: Flat (corpus) versus multilevel (incorrect-parse) adjunction.</S>
    <S sid="8" ssid="8">Parenthesized material is category-modification.</S>
    <S sid="9" ssid="9">CP NP I exports NP VP -** NP VP -** VV NP realized I gross exports CRDHN* NP PU NP NP problems CRDLN CP NP NP PU NP unmet &#8226; fig_ conditions &#235; problems VP NP ADVP VP nationwide most long NP VP ADVP VP nationwide -------most long Figure 4: Major parse ambiguities.</S>
    <S sid="10" ssid="10">Starred examples are correct in corpus; alternates are parse errors. string speculator Richard Denthese structures are typically bracketed flat in the ETB, underspecifying the semantic relations relative to the CTB.</S>
    <S sid="11" ssid="11">In CTB parsing, this type of ambiguity is difficult to resolve; different compound NP parses differ in dependency structure, so the dependency model resolves errors when word frequencies are large enough to be reliable, but this is often not possible.</S>
    <S sid="12" ssid="12">We found that the internal distributions of (i) NP modifiers of NPs and (ii) left-modified NPs both differ from the internal distribution of NPs in general; we take advantage of this in the PCFG model by marking both types (i) and (ii), which reduces the bias against NP-NP modification in compound NPs.</S>
    <S sid="13" ssid="13">Prenominal modification errors, illustrated in PNM of Figure 4, are rather infrequent, despite the natural parallel with PP attachment ambiguity in English.</S>
    <S sid="14" ssid="14">Due to the highly articulated structure of prenominal modifiers, it seems difficult to address this problem directly; one measure we found somewhat successful is to mark IP daughters of prenominal modification.</S>
    <S sid="15" ssid="15">Coordination scope errors occured in two major varieties: those where the misattached right conjunct is verbal (a VP or IP), and those where it is nominal the latter case is illustrated in and CRDLN in Figure The equivverbal coordination is generally marked with commas, whereas nominal coordination is marked with conjoiners or the mostly noun-conjoining punctuation mark &amp;quot;, &amp;quot; IP NP heretofore unmet conditions NP IP IP NP VP IP PU IP PU IP NP VP he I I president may VV him say president may he I meet him 120, say Figure 6: Ambiguity between communication verb subcategorization frame (left; corpus) and high coordination attachment (right; incorrect parse). ocal majority of low over high verbal attachment errors contrasts qualitatively with ETB parsing, where low attachment is more common and parsers tend to err toward high attachment.</S>
    <S sid="16" ssid="16">There are two major sources of ambiguous attachment sites: (i) any VP can be parsed as an IP plus a unary IP&#8212;NP, so due to pro-drop any VP coordination is ambiguous with a higher IP coordination; (ii) VPs are multilevel, giving rise to ambiguities of scope over adjuncts.</S>
    <S sid="17" ssid="17">It seems that (i) is a difficult problem; in some cases, certain &amp;quot;discourse-level&amp;quot; adverbs such as IP modification and are thus strong indicators of high attachment.</S>
    <S sid="18" ssid="18">To capture this we mark those adverbs possessing an IP grandparent.</S>
    <S sid="19" ssid="19">We address (ii) to some extent by marking VPs as adjunction or complementation structures, as shown before in Figure 5; in training data, only like-type VPs are coordinated.</S>
    <S sid="20" ssid="20">With nominal coordination scope errors, the situation is different: we found no false low attachments.</S>
    <S sid="21" ssid="21">False high scopings can be reduced by marking NP conjuncts.</S>
    <S sid="22" ssid="22">(Charniak, 2000) claims that a similar strategy proved effective for WSJ parsing.</S>
  </ABSTRACT>
  <SECTION title="1 Background" number="1">
    <S sid="23" ssid="1">Even narrow-coverage context-free natural language grammars produce explosive ambiguity (Church and Patil, 1982).</S>
    <S sid="24" ssid="2">Today's treebankderived broad-coverage CFGs generate even more, some of it genuine linguistic ambiguity and some of it artificial (see (Krotov et al., 1998) and (Johnson, 1998) for discussion).</S>
    <S sid="25" ssid="3">Corpusbased statistical parsing is a leading technique to deal with this extreme ambiguity; the vast bulk of work in this field has been done in English, using the Wall Street Journal section of the English Penn Treebank (ETB).</S>
    <S sid="26" ssid="4">State-ofthe-art statistical parsing techniques now handle most ambiguity adequately; the best statistical parsers for the ETB are now at roughly 90% labeled bracketing accuracy (Charniak, 2000; Collins, 2000).</S>
    <S sid="27" ssid="5">The remaining difficult-toresolve ambiguities are fairly well-understood for English perhaps the best-known are flat versus embedded adjunction structures (see (Johnson, 1998) for discussion) and NP-conjunct versus flat NP coordinations but are hardly analyzed at all for any other language.</S>
    <S sid="28" ssid="6">More recently, however, a wider variety of parsed corpora has become available in other languages.</S>
    <S sid="29" ssid="7">We take advantage of the recently released Penn Chinese Treebank (version 2.0, abbreviated here as CTB; (Xue et al., 2002)) to address these questions for Chinese, a language with less morphology and more mixed headedness than English.</S>
    <S sid="30" ssid="8">Chinese, as we will show, has a rather different set of salient ambiguities from the perspective of statistical parsing.</S>
    <S sid="31" ssid="9">This section provides background on relevant linguistic differences between Chinese and English, and on relevant tree-structural differences between the two treebanks.</S>
    <S sid="32" ssid="10">1.1 Linguistic differences between English and Chinese Chinese and English are both isolating languages: they rely primarily on relatively rigid phrase structure rather than rich morphological information to encode functional relations between elements.</S>
    <S sid="33" ssid="11">For purposes of statistical parsing, three salient differences distinguish the two languages.</S>
    <S sid="34" ssid="12">First, Chinese makes less use of function words and morphology than English: determinerless nouns are more widespread, plural marking is restricted and rare, and verbs appear in a unique form with few supporting function words.</S>
    <S sid="35" ssid="13">Second, whereas English is largely left-headed and right-branching, Chinese is more mixed: most categories are right-headed, but verbal and prepositional complements follow their heads (Figure 2).</S>
    <S sid="36" ssid="14">Significantly, this means that attachment ambiguity among a verb's complements, a major source of parsing ambiguity in English, is rare in Chinese.</S>
    <S sid="37" ssid="15">The third major difference is subject pro-drop the null realization of uncontrolled pronominal subjects which is widespread in Chinese, but rare in English.</S>
    <S sid="38" ssid="16">This creates ambiguities between parses of subjectless structures as IP (equivalent to ETB's S) or as VP, and between interpretations of preverbal NPs as NP adjuncts or as subjects.</S>
    <S sid="39" ssid="17">The CTB consists of 325 newswire articles; 291 are on economic topics, 34 on politics and culture.</S>
    <S sid="40" ssid="18">Past work on CTB parsing (Bikel and Chiang, 2000; Chiang and Bikel, 2002) has used articles 1-270 for training, 301-325 for development, and 271-300 for testing.</S>
    <S sid="41" ssid="19">We found, however, that this development set was uncharacteristic of the corpus as a whole and not ideal for development.</S>
    <S sid="42" ssid="20">As an extreme example, the word *`- appears in it 28 times as a measure word, meaning 'point', twice as an adverb, and once as a verb; in the rest of the corpus it appears eight times as a verb, once as an adverb, and never as a measure word.</S>
    <S sid="43" ssid="21">There turns out to be a high concentration of articles on non-economic topics in 301-325 (the problem with *`- arising from sports articles).</S>
    <S sid="44" ssid="22">Therefore, for this paper we set aside articles 1-25 for development and used 26-270 as training during development.</S>
    <S sid="45" ssid="23">During development, we found the difference in parse accuracy for 1-25 and 301-325 to range around a remarkable 10%.</S>
    <S sid="46" ssid="24">Whereas ETB annotation strongly reflects late 1970s mainstream transformational grammar, CTB annotation draws primarily on Government-Binding (GB) theory from the 1980s.</S>
    <S sid="47" ssid="25">GB differs from the former in two major respects: first, it rigidly requires phrasal projection of all lexical categories; second, it sharply distinguishes between levels of adjunction and complementation.</S>
    <S sid="48" ssid="26">Both these differences are noticeable when comparing treebanks.</S>
    <S sid="49" ssid="27">The first difference, projection of phrasal categories, is particularly prominent within NPs: CTB adjective-noun modification, for example, is always at the level of ADJP and NP, whereas in English it can be a direct rewrite of NP to JJ and NN tags (Figure 1).</S>
    <S sid="50" ssid="28">The second difference, distinction of adjunction and complementation levels, has been made only for VP (Figure 2), consistent with the headedness issues described in Section 1.1.</S>
    <S sid="51" ssid="29">The rigid requirement of phrasal category projection is also manifest elsewhere: all Chinese prenominal relative-clause equivalents have a level of CP annotation, equivalent to ETB's SBAR, containing a null WH-NP, even though Chinese has no relative pronouns (the overt prenominal modification marker, Aiv , introduces another level of CP annotation when present, as seen in Figure 3, but in this case the unary CP is compressed under standard pre-parsing tree transformations) .1 In the corresponding case for the ETB, reduced relative clauses are annotated as VPs.</S>
    <S sid="52" ssid="30">These annotation practices have a strong effect on the gross statistics of the CTB after standard tree normalizations for parsing.</S>
    <S sid="53" ssid="31">The CTB has far fewer rule types than an ETB of equivalent size, and has a considerably lower branching factor.</S>
    <S sid="54" ssid="32">In particular a far higher proportion of 'Standard tree normalizations are: the removal of empty nodes and nodes dominating no non-empty terminals, and the subsequent removal of A over A unaries.</S>
    <S sid="55" ssid="33">ETB and CTB.</S>
    <S sid="56" ssid="34">Rules and UnRu are the number of rule types and unary rule types respectively; BF is average Branching Factor and UnTok is percentage unary of local tree tokens.</S>
    <S sid="57" ssid="35">CTB rewrite rules are unary (Table 1).2 This is consonant with the behavior of simple PCFGs on training data, as shown in Table 2.</S>
    <S sid="58" ssid="36">Parent and grandparent annotation (Johnson, 1998) has a much stronger effect on training-data parsing for ETB than for CTB.</S>
    <S sid="59" ssid="37">We believe that the greater precision/recall split seen here for CTB is also due to its lower branching factor.</S>
  </SECTION>
  <SECTION title="2 Parsing model" number="2">
    <S sid="60" ssid="1">We use the factored parsing model of (Klein and Manning, 2002).</S>
    <S sid="61" ssid="2">Parsing in this model involves combining two independent parses: one of a non-lexicalized, maximum likelihoodestimated (MLE) PCFG model and another of a constituent-free dependency parse.</S>
    <S sid="62" ssid="3">In addition to simplifying the parameterization of the parsing model and maintaining exactness, this 2WSJ-small is a randomly selected tenth of the full English Wall Street Journal corpus. model offers the prospect of increased flexibility in tuning the individual parse models.</S>
    <S sid="63" ssid="4">In particular, linguistic generalizations corresponding to category refinements are easily implemented via category-splitting in the PCFG model, without concern for affecting the dependency model.</S>
    <S sid="64" ssid="5">In adapting this parsing model to Chinese, we have retained unchanged the dependency model developed for English; the model backs off to tags, and backoff parameters remain the same.3 In all cases, test input to the parser was segmented but untagged.4 Our focus in parser development has been to refine the PCFG model via stepwise refinements informed by major observed ambiguity classes.</S>
    <S sid="65" ssid="6">We illustrate that each of these refinements can effectively be viewed as an amendment to the independence assumptions made by a simple PCFG.</S>
  </SECTION>
  <SECTION title="3 PCFG development" number="3">
    <S sid="66" ssid="1">The simplest systematic augmentations to basic PCFG models are the inclusion of various types of contextual information in the structure of individual node labels.</S>
    <S sid="67" ssid="2">In principle, any contextual information could be used, but in practice two types are most heavily relied on: (i) information highly local to the enhanced node; and (ii) a unique preterminal/terminal pair identified as the head of the node.</S>
    <S sid="68" ssid="3">These practices have correlates in contemporary linguistic theory as principles of locality and lex3An algorithm to determine the head daughter of every non-terminal node is necessary for the dependency model and for grammar markovization (Collins, 1999), and since the CTB and ETB have different grammars, we did write a simple headfinder for the CTB grammar.</S>
    <S sid="69" ssid="4">4For unknown words we estimated P(wordltag) based on the first character of the word. foreign investment icalism (Sag and Wasow, 1999).</S>
    <S sid="70" ssid="5">For headship, the choices of node enhancement strategy are fairly limited; for enrichment by local context, there are far more choices.</S>
    <S sid="71" ssid="6">Of the simplest local-context enrichment strategies, the one that has proven effective on a systematic basis involves parent annotation; (Johnson, 1998) showed that when uniformly applied, it considerably improved WSJ Treebank parsing.</S>
    <S sid="72" ssid="7">Uniform enhancement by other local context, such as sisters, daughters, or cousins, quickly leads to unacceptable sparseness under MLE.</S>
    <S sid="73" ssid="8">To begin development, we tested the interaction of complete parent and/or grandparent annotation with PCFG markovization (see (Collins, 1999; Charniak, 2000) for discussion).</S>
    <S sid="74" ssid="9">The indications for the utility of parent annotation in CTB parsing are mixed.</S>
    <S sid="75" ssid="10">The CTB is smaller and thus more susceptible to grammar fragmentation, but it is also less flat (see Table 1).</S>
    <S sid="76" ssid="11">We found that first-order markovization was superior to zero-order, second-order, and unmarkovized PCFGs for all levels of ancestor annotation, and that within first-order markovization parent annotation was slightly superior to no annotation, with grandparent annotation decidedly worse.</S>
  </SECTION>
  <SECTION title="4 Error analysis for parser development" number="4">
    <S sid="77" ssid="1">Keeping in mind that less fragmented grammars are more robust to further category-splitting, we systematically investigated the major sources of error for the factored model with an unannotated first-order markov PCFG grammar whose only enrichment of CTB annotation was a refinement of punctuation tags along ETB lines, which achieved an Fl of 80.7%.</S>
    <S sid="78" ssid="2">To assess the major sources of parsing difficulty for Chinese, we tabulated frequencies of major types of parsing errors in a 100-sentence subset of our development set.</S>
    <S sid="79" ssid="3">Table 3 gives a breakdown of the major error types found; Figure 4 gives examples of unfamiliar major error types.5 In the next section we describe each major error type, analyze its causes, and suggest simple PCFG enhancements that can be used to address it.</S>
    <S sid="80" ssid="4">4.1 Analysis by error type and PCFG-enrichment fixes Multilevel VP adjunction errors (Figure 5) are common in models without parent annotation, although even with parent annotation the presence of VP coordination would give multilevel VP adjunction nonzero probability.</S>
    <S sid="81" ssid="5">We address this error by taking advantage of the CTB's principled VP annotation practices, marking adjunction, complementation, and coordination VP levels, which builds the flat adjunction constraint back into the structure of the head daughter.</S>
    <S sid="82" ssid="6">NP-NP modification, depicted in NNM&#177; in Figure 4, was the most common error seen; the greater prevalence of false positives is likely a result of the overall PCFG parsing preference for flatter structures.</S>
    <S sid="83" ssid="7">This type of parse ambiguity is grounded in the semantic ambiguity of compound noun interpretation.</S>
    <S sid="84" ssid="8">This semantic ambiguity exists in English as well, as in the NNM{&#177;/&#8212;} NP-NP modification false positive/negative PNM{+/&#8212;} (non-NP) prenominal mod. false positive/negative CRD{H,L}{V,N} incorrect {high/low} coordination attachment of righthand {verbal/nominal} material Adj X/Y incorrect adjunction into X; correct site was Y mistag X/Y category Y mistagged as X &#176;Note that only mistaggings leading to constituentlevel parse errors were tallied.</S>
    <S sid="85" ssid="9">ETB string commodity speculator Richard Dennis, but these structures are typically bracketed flat in the ETB, underspecifying the semantic relations relative to the CTB.</S>
    <S sid="86" ssid="10">In CTB parsing, this type of ambiguity is difficult to resolve; different compound NP parses differ in dependency structure, so the dependency model resolves errors when word frequencies are large enough to be reliable, but this is often not possible.</S>
    <S sid="87" ssid="11">We found that the internal distributions of (i) NP modifiers of NPs and (ii) left-modified NPs both differ from the internal distribution of NPs in general; we take advantage of this in the PCFG model by marking both types (i) and (ii), which reduces the bias against NP-NP modification in compound NPs.</S>
    <S sid="88" ssid="12">Prenominal modification errors, illustrated in PNM of Figure 4, are rather infrequent, despite the natural parallel with PP attachment ambiguity in English.</S>
    <S sid="89" ssid="13">Due to the highly articulated structure of prenominal modifiers, it seems difficult to address this problem directly; one measure we found somewhat successful is to mark IP daughters of prenominal modification.</S>
    <S sid="90" ssid="14">Coordination scope errors occured in two major varieties: those where the misattached right conjunct is verbal (a VP or IP), and those where it is nominal the latter case is illustrated in CRDHN and CRDLN in Figure 4.7 The equivocal majority of low over high verbal attachment errors contrasts qualitatively with ETB parsing, where low attachment is more common and parsers tend to err toward high attachment.</S>
    <S sid="91" ssid="15">There are two major sources of ambiguous attachment sites: (i) any VP can be parsed as an IP plus a unary IP&#8212;NP, so due to pro-drop any VP coordination is ambiguous with a higher IP coordination; (ii) VPs are multilevel, giving rise to ambiguities of scope over adjuncts.</S>
    <S sid="92" ssid="16">It seems that (i) is a difficult problem; in some cases, certain &amp;quot;discourse-level&amp;quot; adverbs such as however and )E,42t_ A./especially prefer IP modification and are thus strong indicators of high attachment.</S>
    <S sid="93" ssid="17">To capture this we mark those adverbs possessing an IP grandparent.</S>
    <S sid="94" ssid="18">We address (ii) to some extent by marking VPs as adjunction or complementation structures, as shown before in Figure 5; in training data, only like-type VPs are coordinated.</S>
    <S sid="95" ssid="19">With nominal coordination scope errors, the situation is different: we found no false low attachments.</S>
    <S sid="96" ssid="20">False high scopings can be reduced by marking NP conjuncts.</S>
    <S sid="97" ssid="21">(Charniak, 2000) claims that a similar strategy proved effective for WSJ parsing.</S>
    <S sid="98" ssid="22">A related error arises from the introduction of IPs by communication verbs and commas, as in Figure 6.</S>
    <S sid="99" ssid="23">Only a few verbs in our training set take IPs this way, so we address this ambiguity with subcategorization annotation (Collins, 1999), marking VVs possessing IP sisters.</S>
    <S sid="100" ssid="24">Most adjunction errors, such as into IP rather than VP, are in principle semantically impotent, since in both cases they are associated with the same verbal head.</S>
    <S sid="101" ssid="25">In practice, however, many adjuncts are NPs, and ambiguous adjunctions into IP are superficially indistinguishable from subjects due to pro-drop.</S>
    <S sid="102" ssid="26">NP adjuncts into VP are not ambiguous in this way, and from inspection the annotation practice of the Chinese Treebank appears to be to put NP adjuncts into VP unless they are followed by an overt subject, or otherwise distinguished as IP-level (e.g., have scope over a clear IP coordination).</S>
    <S sid="103" ssid="27">This could be dealt with in PCFG annotation in two ways.</S>
    <S sid="104" ssid="28">One is to retain subject (and/or non-subject) functional marking from the CTB.</S>
    <S sid="105" ssid="29">The other is to mark VP adjuncts.</S>
    <S sid="106" ssid="30">In practice, we found that the former hurt performance whereas the latter helped somewhat.</S>
    <S sid="107" ssid="31">The strongest mistagging tendency was to tag verbs (VV) as common nouns (NN).</S>
    <S sid="108" ssid="32">Upon manual examination, the asymmetry of N-as-V and V-as-N mistagging frequency seems in line with the global prior over POS tags; the overall N:V ratio in the corpus is 2.5:1.</S>
    <S sid="109" ssid="33">We briefly explain here why N/V ambiguity is a hard problem in Chinese.</S>
    <S sid="110" ssid="34">All natural languages possess derivational means by which roots can switch between nominal and verbal categories.</S>
    <S sid="111" ssid="35">When there is overt morphological marking for these processes, as is always the case in Russian or German (and also as with Chinese and English suffixes such as '/-ify) there is no ambiguity.</S>
    <S sid="112" ssid="36">But the sparse morphology of English and Chinese means that frequently there is noun/verb ambiguity at the word level.</S>
    <S sid="113" ssid="37">For English, most cases of this ambiguity can be resolved by the linguist on the basis of paradigmatic substitution in static context: whether an instance of the word raise, for example, is a noun or a verb can be quickly determined by checking whether raised can be substituted in the same context.</S>
    <S sid="114" ssid="38">Chinese, on the other hand, has no morphological paradigms, so any test to determine the part of speech must be made with syntagmatic substitution: whether the word can take an adverbial modifier or a prenominal modifier, for example.</S>
    <S sid="115" ssid="39">In both languages, there are borderline cases, but they are handled differently by the respective treebanks.</S>
    <S sid="116" ssid="40">In English, gerunds (VBG) have both nominal and verbal properties.</S>
    <S sid="117" ssid="41">In the English Treebank, they have a single POS tag, but their distribution overlaps with both nouns and verbs, so that for example they can head both NPs and VPs.</S>
    <S sid="118" ssid="42">In Chinese, on the other hand, the tag assigned to N/V-ambiguous words is determined by the external context, specifically their maximal phrase: with the exception of domination by FRAG, all nouns are immediately dominated by NP, and all verbs by VP.</S>
    <S sid="119" ssid="43">To test the impact of Chinese Treebank N/V tagging practices, we tried training the parser with NN and VV training tags merged.</S>
    <S sid="120" ssid="44">This yielded a 5.4% drop in Fl for the vanilla PCFG, and a much smaller drop of 1.7% for the refined model, suggesting at first glance that context plus correct independence assumptions can compensate for most of the distributional information gained from N/V tag priors.</S>
    <S sid="121" ssid="45">But we also tried the same experiment with the ETB using the small training set and a vanilla PCFG, and found, remarkably, practically no effect: precision increased by 0.06%, and recall decreased by 0.21%.</S>
    <S sid="122" ssid="46">Although this result calls for further investigation, we tentatively conclude that in English, for most N/V-ambiguous tokens, morphology and POS prior contribute essentially nothing that cannot be adduced from the token's surrounding function-word context; whereas in Chinese, it seems that the lack of function words puts a much greater burden on prior knowledge of an N/V-ambiguous word's distributional behavior.</S>
    <S sid="123" ssid="47">When we included all the PCFG enhancements listed in this section, we achieved an increase of 1.4% in Fl; interestingly, precision actually decreased by 0.4% whereas recall increased by 3.2%.</S>
    <S sid="124" ssid="48">Our PCFG enhancements were most effective at reducing NP-NP modification error, incorrect recursive bracketings, and IP/VP attachment errors.</S>
    <S sid="125" ssid="49">They were less effective at improving coordination attachment resolution and prenominal modification.</S>
    <S sid="126" ssid="50">Although they were not directly identified as solutions to common types of errors, we identifled several more PCFG refinements that reflect linguistically motivated generalizations and improve parsing performance.</S>
    <S sid="127" ssid="51">Generalizing from the specific error classes analyzed in the previous section, a key problem in Chinese parsing seems to be separating nonequivalent classes of IP.</S>
    <S sid="128" ssid="52">Two major classes of ambiguity are involved in IP membership: (i) the presence or absence in IP of subjects and adjuncts; and (ii) coordinate attachment of verbal material.</S>
    <S sid="129" ssid="53">We found that these ambiguities were most effectively dealt with by marking root IPs as well as those in certain sister contexts.</S>
    <S sid="130" ssid="54">Again in this case, CTB annotation practices introduce category conflations that, when reified in a PCFG, lead to false independence assumptions.</S>
    <S sid="131" ssid="55">For example, the BA marker is descriptively used in Chinese to preverbalize objects.</S>
    <S sid="132" ssid="56">Its syntax, however, is controversial (Bender, 2001).</S>
    <S sid="133" ssid="57">In the CTB, BA heads a VP and always has a unique sister IP; but that IP essentially always rewrites as NP VP.</S>
    <S sid="134" ssid="58">With these refinements of IP we achieved another 0.4% in error reduction, for a final development set figure of 82.1% LP, 83.1% LR, and 82.6% Fl.</S>
    <S sid="135" ssid="59">We then ran the same model on the test set used in previous work (Bikel and Chiang, 2000; Chiang and Bikel, 2002).</S>
    <S sid="136" ssid="60">Results are shown in Table 4.</S>
  </SECTION>
  <SECTION title="5 Results and Discussion" number="5">
    <S sid="137" ssid="1">The trends we obtained are different enough from previous work to merit discussion.</S>
    <S sid="138" ssid="2">As shown in Table 4, previous work on CTB parsing consistently achieved higher results on precision than on recall.</S>
    <S sid="139" ssid="3">This is consonant with our initial experiments in CTB PCFG parsing: on the development set, a vanilla PCFG showed a 7% precision/recall split in favor of precision (the split for our small WSJ training set is 4.2% in the same direction).</S>
    <S sid="140" ssid="4">We suspect that this is due to the low branching factor in the CTB, which increases the potential reward from the parser's perspective for picking flatter structures.</S>
    <S sid="141" ssid="5">Simply splitting punctuation along the lines of English, combined with PCFG markovization and the introduction of a dependency model factor, reduced the LP/LR split to 1.1%.</S>
    <S sid="142" ssid="6">From there to our final model, nearly all improvement was in recall: precision improved by 0.3% to the final figure, whereas recall jumped by 3.4%.</S>
    <S sid="143" ssid="7">We interpret these results as indicating that we have unlocked a heretofore undiscovered space of independence-assumption refinements for CTB parsing, suggesting that there is still considerable room for improvement in CTB parsing even with a small (90,000-word) training set; a parser-combining model such as that proposed in (Henderson and Brill, 1999), for example, might be effective here.</S>
    <S sid="144" ssid="8">This is an encouraging result for the use of detailed error analysis followed by focused treestructure enhancements to improved parser performance.</S>
    <S sid="145" ssid="9">However, we found two limitations to our methodology.</S>
    <S sid="146" ssid="10">First, some important and addressable error types are relatively rare in Treebank data.</S>
    <S sid="147" ssid="11">The 100-sentence chunk of development data we chose to analyze simply did not contain any instances of BA, discussed above in Section 4.3, but errors involving BA occurred three times elsewhere in the development set.</S>
    <S sid="148" ssid="12">Second, some common error types are not the result of simple and easily fixable shortcomings in independence assumptions.</S>
    <S sid="149" ssid="13">In particular, we found that coordination scoping ambiguity and N/V tag ambiguity are major sources of relatively catastrophic error for our parser.</S>
    <S sid="150" ssid="14">Interestingly, coordination scope ambiguity is recognized as perhaps the most recalcitrant problem in ETB parsing, while many cases of N/V ambiguity are particularly difficult points of linguistic analysis for Chinese, as discussed in Section 4.2.</S>
    <S sid="151" ssid="15">For the future, we believe that there is still room for considerable improvement in CTB parsing under our model.</S>
    <S sid="152" ssid="16">In addition to further PCFG refinements, tuning the dependency model may lead to improved performance.</S>
    <S sid="153" ssid="17">We found that head-dependent distances in the CTB are larger than in the ETB, consistent with the greater degree of center-embedding resulting from the mixed headedness of Chinese, and suggesting that a dependency model developed for English may not be optimal for Chinese.</S>
    <S sid="154" ssid="18">Since NP is right-headed while VP and IP are leftheaded, an improved dependency model may be the best place to address at least one of the key problems we have identified for CTB parsing.</S>
  </SECTION>
  <SECTION title="6 Acknowledgements" number="6">
    <S sid="155" ssid="1">We are grateful to Dan Klein for valuable input, and for the parser implementation used here.</S>
    <S sid="156" ssid="2">This paper is based on research supported by the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program.</S>
  </SECTION>
</PAPER>
