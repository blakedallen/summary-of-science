<PAPER>
  <S sid="0">Using WordNet-Based Context Vectors To Estimate The Semantic Relatedness Of Concepts</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this paper, we introduce a WordNetbased measure of semantic relatedness by combining the structure and content of WordNet with co&#8211;occurrence information derived from raw text.</S>
    <S sid="2" ssid="2">We use the co&#8211;occurrence information along with the definitions to build vectors corresponding to each concept in Word- Net.</S>
    <S sid="3" ssid="3">Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors.</S>
    <S sid="4" ssid="4">We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness.</S>
    <S sid="5" ssid="5">This measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech.</S>
    <S sid="6" ssid="6">In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the co&#8211;occurrence information.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">Humans are able to quickly judge the relative semantic relatedness of pairs of concepts.</S>
    <S sid="8" ssid="2">For example, most would agree that feather is more related to bird than it is to tree.</S>
    <S sid="9" ssid="3">This ability to assess the semantic relatedness among concepts is important for Natural Language Understanding.</S>
    <S sid="10" ssid="4">Consider the following sentence: He swung the bat, hitting the ball into the stands.</S>
    <S sid="11" ssid="5">A reader likely uses domain knowledge of sports along with the realization that the baseball senses of hitting, bat, ball and stands are all semantically related, in order to determine that the event being described is a baseball game.</S>
    <S sid="12" ssid="6">Consequently, a number of techniques have been proposed over the years, that attempt to automatically compute the semantic relatedness of concepts to correspond closely with human judgments (Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Leacock and Chodorow, 1998).</S>
    <S sid="13" ssid="7">It has also been shown that these techniques prove useful for tasks such as word sense disambiguation (Patwardhan et al., 2003), real-word spelling correction (Budanitsky and Hirst, 2001) and information extraction (Stevenson and Greenwood, 2005), among others.</S>
    <S sid="14" ssid="8">In this paper we introduce a WordNet-based measure of semantic relatedness inspired by Harris&#8217; Distributional Hypothesis (Harris, 1985).</S>
    <S sid="15" ssid="9">The distributional hypothesis suggests that words that are similar in meaning tend to occur in similar linguistic contexts.</S>
    <S sid="16" ssid="10">Additionally, numerous studies (Carnine et al., 1984; Miller and Charles, 1991; McDonald and Ramscar, 2001) have shown that context plays a vital role in defining the meanings of words.</S>
    <S sid="17" ssid="11">(Landauer and Dumais, 1997) describe a context vector-based method that simulates learning of word meanings from raw text.</S>
    <S sid="18" ssid="12">(Sch&#168;utze, 1998) has also shown that vectors built from the contexts of words are useful representations of word meanings.</S>
    <S sid="19" ssid="13">Our Gloss Vector measure of semantic relatedness is based on second order co&#8211;occurrence vectors (Sch&#168;utze, 1998) in combination with the structure and content of WordNet (Fellbaum, 1998), a semantic network of concepts.</S>
    <S sid="20" ssid="14">This measure captures semantic information for concepts from contextual information drawn from corpora of text.</S>
    <S sid="21" ssid="15">We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness.</S>
    <S sid="22" ssid="16">This measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech.</S>
    <S sid="23" ssid="17">In addition, it is adaptable since any corpora can be used to derive the word vectors.</S>
    <S sid="24" ssid="18">This paper is organized as follows.</S>
    <S sid="25" ssid="19">We start with a description of second order context vectors in general, and then define the Gloss Vector measure in particular.</S>
    <S sid="26" ssid="20">We present an extensive evaluation of the measure, both with respect to human relatedness judgments and also relative to its performance when used in a word sense disambiguation algorithm based on semantic relatedness.</S>
    <S sid="27" ssid="21">The paper concludes with an analysis of our results, and some discussion of related and future work.</S>
  </SECTION>
  <SECTION title="2 Second Order Context Vectors" number="2">
    <S sid="28" ssid="1">Context vectors are widely used in Information Retrieval and Natural Language Processing.</S>
    <S sid="29" ssid="2">Most often they represent first order co&#8211;occurrences, which are simply words that occur near each other in a corpus of text.</S>
    <S sid="30" ssid="3">For example, police and car are likely first order co&#8211;occurrences since they commonly occur together.</S>
    <S sid="31" ssid="4">A first order context vector for a given word would simply indicate all the first order co&#8211;occurrences of that word as found in a corpus.</S>
    <S sid="32" ssid="5">However, our Gloss Vector measure is based on second order co&#8211;occurrences (Sch&#168;utze, 1998).</S>
    <S sid="33" ssid="6">For example, if car and mechanic are first order co&#8211; occurrences, then mechanic and police would be second order co&#8211;occurrences since they are both first order co&#8211;occurrences of car.</S>
    <S sid="34" ssid="7">Sch&#168;utze&#8217;s method starts by creating a Word Space, which is a co&#8211;occurrence matrix where each row can be viewed as a first order context vector.</S>
    <S sid="35" ssid="8">Each cell in this matrix represents the frequency with which two words occur near one another in a corpus of text.</S>
    <S sid="36" ssid="9">The Word Space is usually quite large and sparse, since there are many words in the corpus and most of them don&#8217;t occur near each other.</S>
    <S sid="37" ssid="10">In order to reduce the dimensionality and the amount of noise, non&#8211;content stop words such as the, for, a, etc. are excluded from being rows or columns in the Word Space.</S>
    <S sid="38" ssid="11">Given a Word Space, a context can then be represented by second order co&#8211;occurrences (context vector).</S>
    <S sid="39" ssid="12">This is done by finding the resultant of the first order context vectors corresponding to each of the words in that context.</S>
    <S sid="40" ssid="13">If a word in a context does not have a first order context vector created for it, or if it is a stop word, then it is excluded from the resultant.</S>
    <S sid="41" ssid="14">For example, suppose we have the following context: The paintings were displayed in the art gallery.</S>
    <S sid="42" ssid="15">The second order context vector would be the resultant of the first order context vectors for painting, display, art, and gallery.</S>
    <S sid="43" ssid="16">The words were, in, and the are excluded from the resultant since we consider them as stop words in this example.</S>
    <S sid="44" ssid="17">Figure 1 shows how the second order context vector might be visualized in a 2-dimensional space.</S>
    <S sid="45" ssid="18">Intuitively, the orientation of each second order context vector is an indicator of the domains or topics (such as biology or baseball) that the context is associated with.</S>
    <S sid="46" ssid="19">Two context vectors that lie close together indicate a considerable contextual overlap, which suggests that they are pertaining to the same meaning of the target word.</S>
  </SECTION>
  <SECTION title="3 Gloss Vectors in Semantic Relatedness" number="3">
    <S sid="47" ssid="1">In this research, we create a Gloss Vector for each concept (or word sense) represented in a dictionary.</S>
    <S sid="48" ssid="2">While we use WordNet as our dictionary, the method can apply to other lexical resources.</S>
    <S sid="49" ssid="3">A Gloss Vector is a second order context vector formed by treating the dictionary definition of a concept as a context, and finding the resultant of the first order context vectors of the words in the definition.</S>
    <S sid="50" ssid="4">In particular, we define a Word Space by creating first order context vectors for every word w that is not a stop word and that occurs above a minimum frequency in our corpus.</S>
    <S sid="51" ssid="5">The specific steps are as follows: &#8594; The first order context vector w, therefore, encodes the co&#8211;occurrence information of word w. For example, consider the gloss of lamp &#8211; an artificial source of visible illumination.</S>
    <S sid="52" ssid="6">The Gloss Vector for lamp would be formed by adding the first order context vectors of artificial, source, visible and illumination.</S>
    <S sid="53" ssid="7">In these experiments, we use WordNet as the corpus of text for deriving first order context vectors.</S>
    <S sid="54" ssid="8">We take the glosses for all of the concepts in WordNet and view that as a large corpus of text.</S>
    <S sid="55" ssid="9">This corpus consists of approximately 1.4 million words, and results in a Word Space of approximately 20,000 dimensions, once low frequency and stop words are removed.</S>
    <S sid="56" ssid="10">We chose the WordNet glosses as a corpus because we felt the glosses were likely to contain content rich terms that would distinguish between the various concepts more distinctly than would text drawn from a more generic corpus.</S>
    <S sid="57" ssid="11">However, in our future work we will experiment with other corpora as the source of first order context vectors, and other dictionaries as the source of glosses.</S>
    <S sid="58" ssid="12">The first order context vectors as well as the Gloss Vectors usually have a very large number of dimensions (usually tens of thousands) and it is not easy to visualize this space.</S>
    <S sid="59" ssid="13">Figure 2 attempts to illustrate these vectors in two dimensions.</S>
    <S sid="60" ssid="14">The words tennis and food are the dimensions of this 2dimensional space.</S>
    <S sid="61" ssid="15">We see that the first order context vector for serve is approximately halfway between tennis and food, since the word serve could mean to &#8220;serve the ball&#8221; in the context of tennis or could mean &#8220;to serve food&#8221; in another context.</S>
    <S sid="62" ssid="16">The first order context vectors for eat and cutlery are very close to food, since they do not have a sense that is related to tennis.</S>
    <S sid="63" ssid="17">The gloss for the word fork, &#8220;cutlery used to serve and eat food&#8221;, contains the words cutlery, serve, eat and food.</S>
    <S sid="64" ssid="18">The Gloss Vector for fork is formed by adding the first order context vectors of cutlery, serve, eat and food.</S>
    <S sid="65" ssid="19">Thus, fork has a Gloss Vector which is heavily weighted towards food.</S>
    <S sid="66" ssid="20">The concept of food, therefore, is in the same semantic space as and is related to the concept of fork.</S>
    <S sid="67" ssid="21">Similarly, we expect that in a high dimensional space, the Gloss Vector of fork would be heavily weighted towards all concepts that are semantically related to the concept of fork.</S>
    <S sid="68" ssid="22">Additionally, the previous demonstration involved a small gloss for representing fork.</S>
    <S sid="69" ssid="23">Using augmented glosses, described in section 3.2, we achieve better representations of concepts to build Gloss Vectors upon.</S>
    <S sid="70" ssid="24">The formulation of the Gloss Vector measure described above is independent of the dictionary used and is independent of the corpus used.</S>
    <S sid="71" ssid="25">However, dictionary glosses tend to be rather short, and it is possible that even closely related concepts will be defined using different sets of words.</S>
    <S sid="72" ssid="26">Our belief is that two synonyms that are used in different glosses will tend to have similar Word Vectors (because their co&#8211;occurrence behavior should be similar).</S>
    <S sid="73" ssid="27">However, the brevity of dictionary glosses may still make it difficult to create Gloss Vectors that are truly representative of the concept.</S>
    <S sid="74" ssid="28">= Word Vector = Gloss Vector (Banerjee and Pedersen, 2003) encounter a similar issue when measuring semantic relatedness by counting the number of matching words between the glosses of two different concepts.</S>
    <S sid="75" ssid="29">They expand the glosses of concepts in WordNet with the glosses of concepts that are directly linked by a WordNet relation.</S>
    <S sid="76" ssid="30">We adopt the same technique here, and use the relations in WordNet to augment glosses for the Gloss Vector measure.</S>
    <S sid="77" ssid="31">We take the gloss of a given concept, and concatenate to it the glosses of all the concepts to which it is directly related according to WordNet.</S>
    <S sid="78" ssid="32">The Gloss Vector for that concept is then created from this big concatenated gloss.</S>
  </SECTION>
  <SECTION title="4 Other Measures of Relatedness" number="4">
    <S sid="79" ssid="1">Below we briefly describe five alternative measures of semantic relatedness, and then go on to include them as points of comparison in our experimental evaluation of the Gloss Vector measure.</S>
    <S sid="80" ssid="2">All of these measures depend in some way upon WordNet.</S>
    <S sid="81" ssid="3">Four of them limit their measurements to nouns located in the WordNet is-a hierarchy.</S>
    <S sid="82" ssid="4">Each of these measures takes two WordNet concepts (i.e., word senses or synsets) c1 and c2 as input and return a numeric score that quantifies their degree of relatedness.</S>
    <S sid="83" ssid="5">(Leacock and Chodorow, 1998) finds the path length between c1 and c2 in the is-a hierarchy of WordNet.</S>
    <S sid="84" ssid="6">The path length is then scaled by the depth of the hierarchy (D) in which they reside to obtain the relatedness of the two concepts.</S>
    <S sid="85" ssid="7">(Resnik, 1995) introduced a measure that is based on information content, which are numeric quantities that indicate the specificity of concepts.</S>
    <S sid="86" ssid="8">These values are derived from corpora, and are used to augment the concepts in WordNet&#8217;s is-a hierarchy.</S>
    <S sid="87" ssid="9">The measure of relatedness between two concepts is the information content of the most specific concept that both concepts have in common (i.e., their lowest common subsumer in the is-a hierarchy).</S>
    <S sid="88" ssid="10">(Jiang and Conrath, 1997) extends Resnik&#8217;s measure to combine the information contents of c1, c2 and their lowest common subsumer.</S>
    <S sid="89" ssid="11">(Lin, 1998) also extends Resnik&#8217;s measure, by taking the ratio of the shared information content to that of the individual concepts.</S>
    <S sid="90" ssid="12">(Banerjee and Pedersen, 2003) introduce Extended Gloss Overlaps, which is a measure that determines the relatedness of concepts proportional to the extent of overlap of their WordNet glosses.</S>
    <S sid="91" ssid="13">This simple definition is extended to take advantage of the complex network of relations in WordNet, and allows the glosses of concepts to include the glosses of synsets to which they are directly related in WordNet.</S>
  </SECTION>
  <SECTION title="5 Evaluation" number="5">
    <S sid="92" ssid="1">As was done by (Budanitsky and Hirst, 2001), we evaluated the measures of relatedness in two ways.</S>
    <S sid="93" ssid="2">First, they were compared against human judgments of relatedness.</S>
    <S sid="94" ssid="3">Second, they were used in an application that would benefit from the measures.</S>
    <S sid="95" ssid="4">The effectiveness of the particular application was an indirect indicator of the accuracy of the relatedness measure used.</S>
    <S sid="96" ssid="5">One obvious metric for evaluating a measure of semantic relatedness is its correspondence with the human perception of relatedness.</S>
    <S sid="97" ssid="6">Since semantic relatedness is subjective, and depends on the human view of the world, comparison with human judgments is a self-evident metric for evaluation.</S>
    <S sid="98" ssid="7">This was done by (Budanitsky and Hirst, 2001) in their comparison of five measures of semantic relatedness.</S>
    <S sid="99" ssid="8">We follow a similar approach in evaluating the Gloss Vector measure.</S>
    <S sid="100" ssid="9">We use a set of 30 word pairs from a study carried out by (Miller and Charles, 1991).</S>
    <S sid="101" ssid="10">These word pairs are a subset of 65 word pairs used by (Rubenstein and Goodenough, 1965), in a similar study almost 25 years earlier.</S>
    <S sid="102" ssid="11">In this study, human subjects assigned relatedness scores to the selected word pairs.</S>
    <S sid="103" ssid="12">The word pairs selected for this study ranged from highly related pairs to unrelated pairs.</S>
    <S sid="104" ssid="13">We use these human judgments for our evaluation.</S>
    <S sid="105" ssid="14">Each of the word pairs have been scored by humans on a scale of 0 to 5, where 5 is the most related.</S>
    <S sid="106" ssid="15">The mean of the scores of each pair from all subjects is considered as the &#8220;human relatedness score&#8221; for that pair.</S>
    <S sid="107" ssid="16">The pairs are then ranked with respect to their scores.</S>
    <S sid="108" ssid="17">The most related pair is the first on the list and the least related pair is at the end of the list.</S>
    <S sid="109" ssid="18">We then have each of the measures of relatedness score the word pairs and a another ranking of the word pairs is created corresponding to each of the measures.</S>
    <S sid="110" ssid="19">Spearman&#8217;s Correlation Coefficient (Spearman, 1904) is used to assess the equivalence of two rankings.</S>
    <S sid="111" ssid="20">If the two rankings are exactly the same, the Spearman&#8217;s correlation coefficient between these two rankings is 1.</S>
    <S sid="112" ssid="21">A completely reversed ranking gets a value of &#8722;1.</S>
    <S sid="113" ssid="22">The value is 0 when there is no relation between the rankings.</S>
    <S sid="114" ssid="23">We determine the correlation coefficient of the ranking of each measure with that of the human relatedness.</S>
    <S sid="115" ssid="24">We use the relatedness scores from both the human studies &#8211; the Miller and Charles study as well as the Rubenstein and Goodenough research.</S>
    <S sid="116" ssid="25">Table 1 summarizes the results of our experiment.</S>
    <S sid="117" ssid="26">We observe that the Gloss Vector has the highest correlation with humans in both cases.</S>
    <S sid="118" ssid="27">Note that in our experiments with the Gloss Vector measure, we have used not only the gloss of the concept but augmented that with the gloss of all the concepts directly related to it according to WordNet.</S>
    <S sid="119" ssid="28">We observed a significant drop in performance when we used just the glosses of the concept alone, showing that the expansion is necessary.</S>
    <S sid="120" ssid="29">In addition, the frequency cutoffs used to construct the Word Space played a critical role.</S>
    <S sid="121" ssid="30">The best setting of the frequency cutoffs removed both low and high frequency words, which eliminates two different sources of noise.</S>
    <S sid="122" ssid="31">Very low frequency words do not occur enough to draw distinctions among different glosses, whereas high frequency words occur in many glosses, and again do not provide useful information to distinguish among glosses.</S>
    <S sid="123" ssid="32">An application-oriented comparison of five measures of semantic relatedness was presented in (Budanitsky and Hirst, 2001).</S>
    <S sid="124" ssid="33">In that study they evaluate five WordNet-based measures of semantic relatedness with respect to their performance in context sensitive spelling correction.</S>
    <S sid="125" ssid="34">We present the results of an application-oriented evaluation of the measures of semantic relatedness.</S>
    <S sid="126" ssid="35">Each of the seven measures of semantic relatedness was used in a word sense disambiguation algorithm described by (Banerjee and Pedersen, 2003).</S>
    <S sid="127" ssid="36">Word sense disambiguation is the task of determining the meaning (from multiple possibilities) of a word in its given context.</S>
    <S sid="128" ssid="37">For example, in the sentence The ex-cons broke into the bank on Elm street, the word bank has the &#8220;financial institution&#8221; sense as opposed to the &#8220;edge of a river&#8221; sense.</S>
    <S sid="129" ssid="38">Banerjee and Pedersen attempt to perform this task by measuring the relatedness of the senses of the target word to those of the words in its context.</S>
    <S sid="130" ssid="39">The sense of the target word that is most related to its context is selected as the intended sense of the target word.</S>
    <S sid="131" ssid="40">The experimental data used for this evaluation is the SENSEVAL-2 test data.</S>
    <S sid="132" ssid="41">It consists of 4,328 instances (or contexts) that each includes a single ambiguous target word.</S>
    <S sid="133" ssid="42">Each instance consists of approximately 2-3 sentences and one occurrence of a target word.</S>
    <S sid="134" ssid="43">1,754 of the instances include nouns as target words, while 1,806 are verbs and 768 are adjectives.</S>
    <S sid="135" ssid="44">We use the noun data to compare all six of the measures, since four of the measures are limited to nouns as input.</S>
    <S sid="136" ssid="45">The accuracy of disambiguation when performed using each of the measures for nouns is shown in Table 2.</S>
  </SECTION>
  <SECTION title="6 Gloss Vector Tuning" number="6">
    <S sid="137" ssid="1">As discussed in earlier sections, the Gloss Vector measure builds a word space consisting of first order context vectors corresponding to every word in a corpus.</S>
    <S sid="138" ssid="2">Gloss vectors are the resultant of a number of first order context vectors.</S>
    <S sid="139" ssid="3">All of these vectors encode semantic information about the concepts or the glosses that the vectors represent.</S>
    <S sid="140" ssid="4">We note that the quality of the words used as the dimensions of these vectors plays a pivotal role in getting accurate relatedness scores.</S>
    <S sid="141" ssid="5">We find that words corresponding to very specific concepts and are highly indicative of a few topics, make good dimensions.</S>
    <S sid="142" ssid="6">Words that are very general in nature and that appear all over the place add noise to the vectors.</S>
    <S sid="143" ssid="7">In an earlier section we discussed using stop words and frequency cutoffs to keep only the high &#8220;information content&#8221; words.</S>
    <S sid="144" ssid="8">In addition to those, we also experimented with a term frequency &#183; inverse document frequency cutoff.</S>
    <S sid="145" ssid="9">Term frequency and inverse document frequency are commonly used metrics in information retrieval.</S>
    <S sid="146" ssid="10">For a given word, term frequency (tf) is the number of times a word appears in the corpus.</S>
    <S sid="147" ssid="11">The document frequency is number of documents in which the word occurs.</S>
    <S sid="148" ssid="12">Inverse document frequency (idf) is then computed as The tf &#183; idf value is an indicator of the specificity of a word.</S>
    <S sid="149" ssid="13">The higher the tf &#183; idf value, the lower the specificity.</S>
    <S sid="150" ssid="14">Figure 3 shows a plot of tf &#183; idf cutoff on the x-axis against the correlation of the Gloss Vector measure with human judgments on the y-axis.</S>
    <S sid="151" ssid="15">The tf &#183;idf values ranged from 0 to about 4200.</S>
    <S sid="152" ssid="16">Note that we get lower correlation as the cutoff is raised.</S>
  </SECTION>
  <SECTION title="7 Analysis" number="7">
    <S sid="153" ssid="1">We observe from the experimental results that the Gloss Vector measure corresponds the most with human judgment of relatedness (with a correlation of almost 0.9).</S>
    <S sid="154" ssid="2">We believe this is probably because the Gloss Vector measure most closely imitates the representation of concepts in the human mind.</S>
    <S sid="155" ssid="3">(Miller and Charles, 1991) suggest that the cognitive representation of a word is an abstraction derived from its contexts (encountered by the person).</S>
    <S sid="156" ssid="4">Their study also suggested the semantic similarity of two words depends on the overlap between their contextual representations.</S>
    <S sid="157" ssid="5">The Gloss Vector measure uses the contexts of the words and creates a vector representation of these.</S>
    <S sid="158" ssid="6">The overlap between these vector representations is used to compute the semantic similarity of concepts.</S>
    <S sid="159" ssid="7">(Landauer and Dumais, 1997) additionally perform singular value decomposition (SVD) on their context vector representation of words and they show that reducing the number of dimensions of the vectors using SVD more accurately simulates learning in humans.</S>
    <S sid="160" ssid="8">We plan to try SVD on the Gloss Vector measure in future work.</S>
    <S sid="161" ssid="9">In the application-oriented evaluation, the Gloss Vector measure performed relatively well (about 41% accuracy).</S>
    <S sid="162" ssid="10">However, unlike the human study, it did not outperform all the other measures.</S>
    <S sid="163" ssid="11">We think there are two possible explanations for this.</S>
    <S sid="164" ssid="12">First, the word pairs used in the human relatedness study are all nouns, and it is possible that the Gloss Vector measure performs better on nouns than on other parts of speech.</S>
    <S sid="165" ssid="13">In the application-oriented evaluation the measure had to make judgments for all parts of speech.</S>
    <S sid="166" ssid="14">Second, the application itself affects the performance of the measure.</S>
    <S sid="167" ssid="15">The Word Sense Disambiguation algorithm starts by selecting a context of 5 words from around the target word.</S>
    <S sid="168" ssid="16">These context words contain words from all parts of speech.</S>
    <S sid="169" ssid="17">Since the Jiang-Conrath measure assigns relatedness scores only to noun concepts, its behavior would differ from that of the Vector measure which would accept all words and would be affected by the noise introduced from unrelated concepts.</S>
    <S sid="170" ssid="18">Thus the context selection factors into the accuracy obtained.</S>
    <S sid="171" ssid="19">However, for evaluating the measure as being suitable for use in real applications, the Gloss Vector measure proves relatively accurate.</S>
    <S sid="172" ssid="20">The Gloss Vector measure can draw conclusions about any two concepts, irrespective of partof-speech.</S>
    <S sid="173" ssid="21">The only other measure that can make this same claim is the Extended Gloss Overlaps measure.</S>
    <S sid="174" ssid="22">We would argue that Gloss Vectors present certain advantages over it.</S>
    <S sid="175" ssid="23">The Extended Gloss Overlap measure looks for exact string overlaps to measure relatedness.</S>
    <S sid="176" ssid="24">This &#8220;exactness&#8221; works against the measure, in that it misses potential matches that intuitively would contribute to the score (For example, silverware with spoon).</S>
    <S sid="177" ssid="25">The Gloss Vector measure is more robust than the Extended Gloss Overlap measure, in that exact matches are not required to identify relatedness.</S>
    <S sid="178" ssid="26">The Gloss Vector measure attempts to overcome this &#8220;exactness&#8221; by using vectors that capture the contextual representation of all words.</S>
    <S sid="179" ssid="27">So even though silverware and spoon do not overlap, their contextual representations would overlap to some extent.</S>
  </SECTION>
  <SECTION title="8 Related Work" number="8">
    <S sid="180" ssid="1">(Wilks et al., 1990) describe a word sense disambiguation algorithm that also uses vectors to determine the intended sense of an ambiguous word.</S>
    <S sid="181" ssid="2">In their approach, they use dictionary definitions from LDOCE (Procter, 1978).</S>
    <S sid="182" ssid="3">The words in these definitions are used to build a co&#8211;occurrence matrix, which is very similar to our technique of using the WordNet glosses for our Word Space.</S>
    <S sid="183" ssid="4">They augment their dictionary definitions with similar words, which are determined using the co&#8211; occurrence matrix.</S>
    <S sid="184" ssid="5">Each concept in LDOCE is then represented by an aggregate vector created by adding the co&#8211;occurrence counts for each of the words in the augmented definition of the concept.</S>
    <S sid="185" ssid="6">The next step in their algorithm is to form a context vector.</S>
    <S sid="186" ssid="7">The context of the ambiguous word is first augmented using the co&#8211;occurrence matrix, just like the definitions.</S>
    <S sid="187" ssid="8">The context vector is formed by taking the aggregate of the word vectors of the words in the augmented context.</S>
    <S sid="188" ssid="9">To disambiguate the target word, the context vector is compared to the vectors corresponding to each meaning of the target word in LDOCE, and that meaning is selected whose vector is mathematically closest to that of the context.</S>
    <S sid="189" ssid="10">Our approach differs from theirs in two primary respects.</S>
    <S sid="190" ssid="11">First, rather than creating an aggregate vector for the context we compare the vector of each meaning of the ambiguous word with the vectors of each of the meanings of the words in the context.</S>
    <S sid="191" ssid="12">This adds another level of indirection in the comparison and attempts to use only the relevant meanings of the context words.</S>
    <S sid="192" ssid="13">Secondly, we use the structure of WordNet to augment the short glosses with other related glosses.</S>
    <S sid="193" ssid="14">(Niwa and Nitta, 1994) compare dictionary based vectors with co&#8211;occurrence based vectors, where the vector of a word is the probability that an origin word occurs in the context of the word.</S>
    <S sid="194" ssid="15">These two representations are evaluated by applying them to real world applications and quantifying the results.</S>
    <S sid="195" ssid="16">Both measures are first applied to word sense disambiguation and then to the learning of positives or negatives, where it is required to determine whether a word has a positive or negative connotation.</S>
    <S sid="196" ssid="17">It was observed that the co&#8211; occurrence based idea works better for the word sense disambiguation and the dictionary based approach gives better results for the learning ofpositives or negatives.</S>
    <S sid="197" ssid="18">From this, the conclusion is that the dictionary based vectors contain some different semantic information about the words and warrants further investigation.</S>
    <S sid="198" ssid="19">It is also observed that for the dictionary based vectors, the network of words is almost independent of the dictionary that is used, i.e. any dictionary should give us almost the same network.</S>
    <S sid="199" ssid="20">(Inkpen and Hirst, 2003) also use gloss&#8211;based context vectors in their work on the disambiguation of near&#8211;synonyms &#8211; words whose senses are almost indistinguishable.</S>
    <S sid="200" ssid="21">They disambiguate near&#8211;synonyms in text using various indicators, one of which is context-vector-based.</S>
    <S sid="201" ssid="22">Context Vectors are created for the context of the target word and also for the glosses of each sense of the target word.</S>
    <S sid="202" ssid="23">Each gloss is considered as a bag of words, where each word has a corresponding Word Vector.</S>
    <S sid="203" ssid="24">These vectors for the words in a gloss are averaged to get a Context Vector corresponding to the gloss.</S>
    <S sid="204" ssid="25">The distance between the vector corresponding to the text and that corresponding to the gloss is measured (as the cosine of the angle between the vectors).</S>
    <S sid="205" ssid="26">The nearness of the vectors is used as an indicator to pick the correct sense of the target word.</S>
  </SECTION>
  <SECTION title="9 Conclusion" number="9">
    <S sid="206" ssid="1">We introduced a new measure of semantic relatedness based on the idea of creating a Gloss Vector that combines dictionary content with corpus based data.</S>
    <S sid="207" ssid="2">We find that this measure correlates extremely well with the results of these human studies, and this is indeed encouraging.</S>
    <S sid="208" ssid="3">We believe that this is due to the fact that the context vector may be closer to the semantic representation of concepts in humans.</S>
    <S sid="209" ssid="4">This measure can be tailored to particular domains depending on the corpus used to derive the co&#8211;occurrence matrices, and makes no restrictions on the parts of speech of the concept pairs to be compared.</S>
    <S sid="210" ssid="5">We also demonstrated that the Vector measure performs relatively well in an application-oriented setup and can be conveniently deployed in a real world application.</S>
    <S sid="211" ssid="6">It can be easily tweaked and modified to work in a restricted domain, such as bio-informatics or medicine, by selecting a specialized corpus to build the vectors.</S>
  </SECTION>
  <SECTION title="10 Acknowledgments" number="10">
    <S sid="212" ssid="1">This research was partially supported by a National Science Foundation Faculty Early CAREER Development Award (#0092784).</S>
    <S sid="213" ssid="2">All of the experiments in this paper were carried out with the WordNet::Similarity package, which is freely available for download from http://search.cpan.org/dist/WordNet-Similarity.</S>
  </SECTION>
</PAPER>
