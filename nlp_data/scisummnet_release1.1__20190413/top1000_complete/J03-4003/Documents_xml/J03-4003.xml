<PAPER>
  <S sid="0">Head-Driven Statistical Models For Natural Language Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This article describes three statistical models for natural language parsing.</S>
    <S sid="2" ssid="2">The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree.</S>
    <S sid="3" ssid="3">Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram dependencies, and preferences for close attachment.</S>
    <S sid="4" ssid="4">All of these preferences are expressed by probabilities conditioned on lexical heads.</S>
    <S sid="5" ssid="5">The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature.</S>
    <S sid="6" ssid="6">To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies.</S>
    <S sid="7" ssid="7">We analyze various characteristics of the models through experiments on parsing accuracy, by collectingfrequencies ofvarious structures in the treebank, and through linguistically motivated examples.</S>
    <S sid="8" ssid="8">Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="9" ssid="1">This article describes three statistical models for natural language parsing.</S>
    <S sid="10" ssid="2">The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree.</S>
    <S sid="11" ssid="3">Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment.</S>
    <S sid="12" ssid="4">All of these preferences are expressed by probabilities conditioned on lexical heads.</S>
    <S sid="13" ssid="5">The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature.</S>
    <S sid="14" ssid="6">To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies.</S>
    <S sid="15" ssid="7">We analyze various characteristics of the models through experiments on parsing accuracy, by collectingfrequencies ofvarious structures in the treebank, and through linguistically motivated examples.</S>
    <S sid="16" ssid="8">Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="17" ssid="1">Ambiguity is a central problem in natural language parsing.</S>
    <S sid="18" ssid="2">Combinatorial effects mean that even relatively short sentences can receive a considerable number of parses under a wide-coverage grammar.</S>
    <S sid="19" ssid="3">Statistical parsing approaches tackle the ambiguity problem by assigning a probability to each parse tree, thereby ranking competing trees in order of plausibility.</S>
    <S sid="20" ssid="4">In many statistical models the probability for each candidate tree is calculated as a product of terms, each term corresponding to some substructure within the tree.</S>
    <S sid="21" ssid="5">The choice of parameterization is essentially the choice of how to represent parse trees.</S>
    <S sid="22" ssid="6">There are two critical questions regarding the parameterization of a parsing approach: In this article we explore these issues within the framework of generative models, more precisely, the history-based models originally introduced to parsing by Black et al. (1992).</S>
    <S sid="23" ssid="7">In a history-based model, a parse tree is represented as a sequence of decisions, the decisions being made in some derivation of the tree.</S>
    <S sid="24" ssid="8">Each decision has an associated probability, and the product of these probabilities defines a probability distribution over possible derivations.</S>
    <S sid="25" ssid="9">We first describe three parsing models based on this approach.</S>
    <S sid="26" ssid="10">The models were originally introduced in Collins (1997); the current article1 gives considerably more detail about the models and discusses them in greater depth.</S>
    <S sid="27" ssid="11">In Model 1 we show one approach that extends methods from probabilistic context-free grammars (PCFGs) to lexicalized grammars.</S>
    <S sid="28" ssid="12">Most importantly, the model has parameters corresponding to dependencies between pairs of headwords.</S>
    <S sid="29" ssid="13">We also show how to incorporate a &#8220;distance&#8221; measure into these models, by generalizing the model to a history-based approach.</S>
    <S sid="30" ssid="14">The distance measure allows the model to learn a preference for close attachment, or right-branching structures.</S>
    <S sid="31" ssid="15">In Model 2, we extend the parser to make the complement/adjunct distinction, which will be important for most applications using the output from the parser.</S>
    <S sid="32" ssid="16">Model 2 is also extended to have parameters corresponding directly to probability distributions over subcategorization frames for headwords.</S>
    <S sid="33" ssid="17">The new parameters lead to an improvement in accuracy.</S>
    <S sid="34" ssid="18">In Model 3 we give a probabilistic treatment of wh-movement that is loosely based on the analysis of wh-movement in generalized phrase structure grammar (GPSG) (Gazdar et al. 1985).</S>
    <S sid="35" ssid="19">The output of the parser is now enhanced to show trace coindexations in wh-movement cases.</S>
    <S sid="36" ssid="20">The parameters in this model are interesting in that they correspond directly to the probability of propagating GPSG-style slash features through parse trees, potentially allowing the model to learn island constraints.</S>
    <S sid="37" ssid="21">In the three models a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree.</S>
    <S sid="38" ssid="22">Independence assumptions then follow naturally, leading to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, lexical dependencies, wh-movement, and preferences for close attachment.</S>
    <S sid="39" ssid="23">All of these preferences are expressed by probabilities conditioned on lexical heads.</S>
    <S sid="40" ssid="24">For this reason we refer to the models as head-driven statistical models.</S>
    <S sid="41" ssid="25">We describe evaluation of the three models on the Penn Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz 1993).</S>
    <S sid="42" ssid="26">Model 1 achieves 87.7% constituent precision and 87.5% consituent recall on sentences of up to 100 words in length in section 23 of the treebank, and Models 2 and 3 give further improvements to 88.3% constituent precision and 88.0% constituent recall.</S>
    <S sid="43" ssid="27">These results are competitive with those of other models that have been applied to parsing the Penn Treebank.</S>
    <S sid="44" ssid="28">Models 2 and 3 produce trees with information about wh-movement or subcategorization.</S>
    <S sid="45" ssid="29">Many NLP applications will need this information to extract predicate-argument structure from parse trees.</S>
    <S sid="46" ssid="30">The rest of the article is structured as follows.</S>
    <S sid="47" ssid="31">Section 2 gives background material on probabilistic context-free grammars and describes how rules can be &#8220;lexicalized&#8221; through the addition of headwords to parse trees.</S>
    <S sid="48" ssid="32">Section 3 introduces the three probabilistic models.</S>
    <S sid="49" ssid="33">Section 4 describes various refinments to these models.</S>
    <S sid="50" ssid="34">Section 5 discusses issues of parameter estimation, the treatment of unknown words, and also the parsing algorithm.</S>
    <S sid="51" ssid="35">Section 6 gives results evaluating the performance of the models on the Penn Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz 1993).</S>
    <S sid="52" ssid="36">Section 7 investigates various aspects of the models in more detail.</S>
    <S sid="53" ssid="37">We give a detailed analysis of the parser&#8217;s performance on treebank data, including results on different constituent types.</S>
    <S sid="54" ssid="38">We also give a breakdown of precision and recall results in recovering various types of dependencies.</S>
    <S sid="55" ssid="39">The intention is to give a better idea of the strengths and weaknesses of the parsing models.</S>
    <S sid="56" ssid="40">Section 7 goes on to discuss the distance features in the models, the implicit assumptions that the models make about the treebank annotation style, and the way that context-free rules in the original treebank are broken down, allowing the models to generalize by producing new rules on test data examples.</S>
    <S sid="57" ssid="41">We analyze these phenomena through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples.</S>
    <S sid="58" ssid="42">Finally, section 8 gives more discussion, by comparing the models to others that have been applied to parsing the treebank.</S>
    <S sid="59" ssid="43">We aim to give some explanation of the differences in performance among the various models.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="60" ssid="1">Probabilistic context-free grammars are the starting point for the models in this article.</S>
    <S sid="61" ssid="2">For this reason we briefly recap the theory behind nonlexicalized PCFGs, before moving to the lexicalized case.</S>
    <S sid="62" ssid="3">Following Hopcroft and Ullman (1979), we define a context-free grammar G as a 4-tuple (N, E, A, R), where N is a set of nonterminal symbols, E is an alphabet, A is a distinguished start symbol in N, and R is a finite set of rules, in which each rule is of the form X &#8594; &#946; for some X E N, &#946; E (N U E)&#8727;.</S>
    <S sid="63" ssid="4">The grammar defines a set of possible strings in the language and also defines a set of possible leftmost derivations under the grammar.</S>
    <S sid="64" ssid="5">Each derivation corresponds to a tree-sentence pair that is well formed under the grammar.</S>
    <S sid="65" ssid="6">A probabilistic context-free grammar is a simple modification of a context-free grammar in which each rule in the grammar has an associated probability P(&#946;  |X).</S>
    <S sid="66" ssid="7">This can be interpreted as the conditional probability of X&#8217;s being expanded using the rule X &#8594; &#946;, as opposed to one of the other possibilities for expanding X listed in the grammar.</S>
    <S sid="67" ssid="8">The probability of a derivation is then a product of terms, each term corresponding to a rule application in the derivation.</S>
    <S sid="68" ssid="9">The probability of a given tree-sentence pair (T, S) derived by n applications of context-free rules LHSi &#8594; RHSi (where LHS stands for &#8220;left-hand side,&#8221; RHS for &#8220;right-hand side&#8221;), 1 &lt; i &lt; n, under the PCFG is Booth and Thompson (1973) specify the conditions under which the PCFG does in fact define a distribution over the possible derivations (trees) generated by the underlying grammar.</S>
    <S sid="69" ssid="10">The first condition is that the rule probabilities define conditional distributions over how each nonterminal in the grammar can expand.</S>
    <S sid="70" ssid="11">The second is a technical condition that guarantees that the stochastic process generating trees terminates in a finite number of steps with probability one.</S>
    <S sid="71" ssid="12">A central problem in PCFGs is to define the conditional probability P(&#946;  |X) for each rule X &#8594; &#946; in the grammar.</S>
    <S sid="72" ssid="13">A simple way to do this is to take counts from a treebank and then to use the maximum-likelihood estimates: If the treebank has actually been generated from a probabilistic context-free grammar with the same rules and nonterminals as the model, then in the limit, as the training sample size approaches infinity, the probability distribution implied by these estimates will converge to the distribution of the underlying grammar.2 Once the model has been trained, we have a model that defines P(T, S) for any sentence-tree pair in the grammar.</S>
    <S sid="73" ssid="14">The output on a new test sentence S is the most likely tree under this model, The parser itself is an algorithm that searches for the tree, Tbest, that maximizes P(T, S).</S>
    <S sid="74" ssid="15">In the case of PCFGs, this can be accomplished using a variant of the CKY algorithm applied to weighted grammars (providing that the PCFG can be converted to an equivalent PCFG in Chomsky normal form); see, for example, Manning and Sch&#168;utze (1999).</S>
    <S sid="75" ssid="16">If the model probabilities P(T, S) are the same as the true distribution generating training and test examples, returning the most likely tree under P(T, S) will be optimal in terms of minimizing the expected error rate (number of incorrect trees) on newly drawn test examples.</S>
    <S sid="76" ssid="17">Hence if the data are generated by a PCFG, and there are enough training examples for the maximum-likelihood estimates to converge to the true values, then this parsing method will be optimal.</S>
    <S sid="77" ssid="18">In practice, these assumptions cannot be verified and are arguably quite strong, but these limitations have not prevented generative models from being successfully applied to many NLP and speech tasks.</S>
    <S sid="78" ssid="19">(See Collins [2002] for a discussion of other ways of conceptualizing the parsing problem.)</S>
    <S sid="79" ssid="20">In the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), which is the source of data for our experiments, the rules are either internal to the tree, where LHS is a nonterminal and RHS is a string of one or more nonterminals, or lexical, where LHS is a part-of-speech tag and RHS is a word.</S>
    <S sid="80" ssid="21">(See Figure 1 for an example.)</S>
    <S sid="81" ssid="22">A PCFG can be lexicalized3 by associating a word w and a part-of-speech (POS) tag t with each nonterminal X in the tree.</S>
    <S sid="82" ssid="23">(See Figure 2 for an example tree.)</S>
    <S sid="83" ssid="24">The PCFG model can be applied to these lexicalized rules and trees in exactly the same way as before.</S>
    <S sid="84" ssid="25">Whereas before the nonterminals were simple (for example, S or NP), they are now extended to include a word and part-of-speech tag (for example, S(bought,VBD) or NP(IBM,NNP)).</S>
    <S sid="85" ssid="26">Thus we write a nonterminal as X(x), where x = (w, t) and X is a constituent label.</S>
    <S sid="86" ssid="27">Formally, nothing has changed, we have just vastly increased the number of nonterminals in the grammar (by up to a factor of |V |x |T |, 2 This point is actually more subtle than it first appears (we thank one of the anonymous reviewers for pointing this out), and we were unable to find proofs of this property in the literature for PCFGs.</S>
    <S sid="87" ssid="28">The rule probabilities for any nonterminal that appears with probability greater than zero in parse derivations will converge to their underlying values, by the usual properties of maximum-likelihood estimation for multinomial distributions.</S>
    <S sid="88" ssid="29">Assuming that the underlying PCFG generating training examples meet both criteria in Booth and Thompson (1973), it can be shown that convergence of rule probabilities implies that the distribution over trees will converge to that of the underlying PCFG, at least when Kullback-Liebler divergence or the infinity norm is taken to be the measure of distance between the two distributions.</S>
    <S sid="89" ssid="30">Thanks to Tommi Jaakkola and Nathan Srebro for discussions on this topic.</S>
    <S sid="90" ssid="31">A lexicalized parse tree and a list of the rules it contains. where |V |is the number of words in the vocabulary and |T  |is the number of part-ofspeech tags).</S>
    <S sid="91" ssid="32">Although nothing has changed from a formal point of view, the practical consequences of expanding the number of nonterminals quickly become apparent when one is attempting to define a method for parameter estimation.</S>
    <S sid="92" ssid="33">The simplest solution would be to use the maximum-likelihood estimate as in equation (1), for example, But the addition of lexical items makes the statistics for this estimate very sparse: The count for the denominator is likely to be relatively low, and the number of outcomes (possible lexicalized RHSs) is huge, meaning that the numerator is very likely to be zero.</S>
    <S sid="93" ssid="34">Predicting the whole lexicalized rule in one go is too big a step.</S>
    <S sid="94" ssid="35">One way to overcome these sparse-data problems is to break down the generation of the RHS of each rule into a sequence of smaller steps, and then to make independence assumptions to reduce the number of parameters in the model.</S>
    <S sid="95" ssid="36">The decomposition of rules should aim to meet two criteria.</S>
    <S sid="96" ssid="37">First, the steps should be small enough for the parameter estimation problem to be feasible (i.e., in terms of having sufficient training data to train the model, providing that smoothing techniques are used to mitigate remaining sparse-data problems).</S>
    <S sid="97" ssid="38">Second, the independence assumptions made should be linguistically plausible.</S>
    <S sid="98" ssid="39">In the next sections we describe three statistical parsing models that have an increasing degree of linguistic sophistication.</S>
    <S sid="99" ssid="40">Model 1 uses a decomposition of which parameters corresponding to lexical dependencies are a natural result.</S>
    <S sid="100" ssid="41">The model also incorporates a preference for right-branching structures through conditioning on &#8220;distance&#8221; features.</S>
    <S sid="101" ssid="42">Model 2 extends the decomposition to include a step in which subcategorization frames are chosen probabilistically.</S>
    <S sid="102" ssid="43">Model 3 handles wh-movement by adding parameters corresponding to slash categories being passed from the parent of the rule to one of its children or being discharged as a trace.</S>
  </SECTION>
  <SECTION title="3." number="4">
    <S sid="103" ssid="1">This section describes how the generation of the RHS of a rule is broken down into a sequence of smaller steps in model 1.</S>
    <S sid="104" ssid="2">The first thing to note is that each internal rule in a lexicalized PCFG has the form4 H is the head-child of the rule, which inherits the headword/tag pair h from its parent P. L1(l1) ... Ln(ln) and R1(r1) ... Rm(rm) are left and right modifiers of H. Either n or m may be zero, and n = m = 0 for unary rules.</S>
    <S sid="105" ssid="3">Figure 2 shows a tree that will be used as an example throughout this article.</S>
    <S sid="106" ssid="4">We will extend the left and right sequences to include a terminating STOP symbol, allowing a Markov process to model the left and right sequences.</S>
    <S sid="107" ssid="5">Thus Ln+1 = Rm+1 = STOP.</S>
    <S sid="108" ssid="6">Note that lexical rules, in contrast to the internal rules, are completely deterministic.</S>
    <S sid="109" ssid="7">They always take the form where P is a part-of-speech tag, h is a word-tag pair (w, t), and the rule rewrites to just the word w. (See Figure 2 for examples of lexical rules.)</S>
    <S sid="110" ssid="8">Formally, we will always take a lexicalized nonterminal P(h) to expand deterministically (with probability one) in this way if P is a part-of-speech symbol.</S>
    <S sid="111" ssid="9">Thus for the parsing models we require the nonterminal labels to be partitioned into two sets: part-of-speech symbols and other nonterminals.</S>
    <S sid="112" ssid="10">Internal rules always have an LHS in which P is not a part-of-speech symbol.</S>
    <S sid="113" ssid="11">Because lexicalized rules are deterministic, they will not be discussed in the remainder of this article: All of the modeling choices concern internal rules.</S>
    <S sid="114" ssid="12">The probability of an internal rule can be rewritten (exactly) using the chain rule of probabilities: (The subscripts h, l and r are used to denote the head, left-modifier, and right-modifier parameter types, respectively.)</S>
    <S sid="115" ssid="13">Next, we make the assumption that the modifiers are generated independently of each other: In summary, the generation of the RHS of a rule such as (2), given the LHS, has been decomposed into three steps:5 For example, the probability of the rule S(bought) &#8594; NP(week) NP(IBM) VP(bought) would be estimated as In this example, and in the examples in the rest of the article, for brevity we omit the part-of-speech tags associated with words, writing, for example S(bought) rather than S(bought,VBD).</S>
    <S sid="116" ssid="14">We emphasize that throughout the models in this article, each word is always paired with its part of speech, either when the word is generated or when the word is being conditioned upon.</S>
    <S sid="117" ssid="15">3.1.1 Adding Distance to the Model.</S>
    <S sid="118" ssid="16">In this section we first describe how the model can be extended to be &#8220;history-based.&#8221; We then show how this extension can be utilized in incorporating &#8220;distance&#8221; features into the model.</S>
    <S sid="119" ssid="17">Black et al. (1992) originally introduced history-based models for parsing.</S>
    <S sid="120" ssid="18">Equations (3) and (4) of the current article made the independence assumption that each modifier is generated independently of the others (i.e., that the modifiers are generated independently of everything except P, H, and h).</S>
    <S sid="121" ssid="19">In general, however, the probability of generating each modifier could depend on any function of the previous modifiers, head/parent category, and headword.</S>
    <S sid="122" ssid="20">Moreover, if the top-down derivation order is fully specified, then the probability of generating a modifier can be conditioned on any structure that has been previously generated.</S>
    <S sid="123" ssid="21">The remainder of this article assumes that the derivation order is depth-first: that is, each modifier recursively generates the subtree below it before the next modifier is generated.</S>
    <S sid="124" ssid="22">(Figure 3 gives an example that illustrates this.)</S>
    <S sid="125" ssid="23">The models in Collins (1996) showed that the distance between words standing in head-modifier relationships was important, in particular, that it is important to capture a preference for right-branching structures (which almost translates into a preference for dependencies between adjacent words) and a preference for dependencies not to cross a verb.</S>
    <S sid="126" ssid="24">In this section we describe how this information can be incorporated into model 1.</S>
    <S sid="127" ssid="25">In section 7.2, we describe experiments that evaluate the effect of these features on parsing accuracy.</S>
    <S sid="128" ssid="26">Figure 3 A partially completed tree derived depth-first.</S>
    <S sid="129" ssid="27">&#8220;???</S>
    <S sid="130" ssid="28">?&#8221; marks the position of the next modifier to be generated&#8212;it could be a nonterminal/headword/head-tag triple, or the STOP symbol.</S>
    <S sid="131" ssid="29">The distribution over possible symbols in this position could be conditioned on any previously generated structure, that is, any structure appearing in the figure.</S>
    <S sid="132" ssid="30">The next child, R3(r3), is generated with probability P(R3(r3) I P, H, h, distancer(2)).</S>
    <S sid="133" ssid="31">The distance is a function of the surface string below previous modifiers R1 and R2.</S>
    <S sid="134" ssid="32">In principle the model could condition on any structure dominated by H, R1, or R2 (or, for that matter, on any structure previously generated elsewhere in the tree).</S>
    <S sid="135" ssid="33">Distance can be incorporated into the model by modifying the independence assumptions so that each modifier has a limited dependence on the previous modifiers: Here distancel and distancer are functions of the surface string below the previous modifiers.</S>
    <S sid="136" ssid="34">(See Figure 4 for illustration.)</S>
    <S sid="137" ssid="35">The distance measure is similar to that in Collins (1996), a vector with the following two elements: (1) Is the string of zero length?</S>
    <S sid="138" ssid="36">(2) Does the string contain a verb?</S>
    <S sid="139" ssid="37">The first feature allows the model to learn a preference for right-branching structures.</S>
    <S sid="140" ssid="38">The second feature6 allows the model to learn a preference for modification of the most recent verb.7 The tree depicted in Figure 2 illustrates the importance of the complement/adjunct distinction.</S>
    <S sid="141" ssid="39">It would be useful to identify IBM as a subject and Last week as an adjunct (temporal modifier), but this distinction is not made in the tree, as both NPs are in the same position8 (sisters to a VP under an S node).</S>
    <S sid="142" ssid="40">From here on we will identify complements9 by attaching a -C suffix to nonterminals.</S>
    <S sid="143" ssid="41">Figure 5 shows the tree in Figure 2 with added complement markings.</S>
    <S sid="144" ssid="42">A postprocessing stage could add this detail to the parser output, but there are a couple of reasons for making the distinction while parsing.</S>
    <S sid="145" ssid="43">First, identifying complements is complex enough to warrant a probabilistic treatment.</S>
    <S sid="146" ssid="44">Lexical information is needed (for example, knowledge that week is likely to be a temporal modifier).</S>
    <S sid="147" ssid="45">Knowledge about subcategorization preferences (for example, that a verb takes exactly one subject) is also required.</S>
    <S sid="148" ssid="46">For example, week can sometimes be a subject, as in Last week was a good one, so the model must balance the preference for having a subject against the relative improbability of week&#8217;s being the headword of a subject.</S>
    <S sid="149" ssid="47">These problems are not restricted to NPs; compare The spokeswoman said (SBAR that the asbestos was dangerous) with Bonds beat short-term investments (SBAR because the market is down), in which an SBAR headed by that is a complement, but an SBAR headed by because is an adjunct.</S>
    <S sid="150" ssid="48">A second reason for incorporating the complement/adjunct distinction into the parsing model is that this may help parsing accuracy.</S>
    <S sid="151" ssid="49">The assumption that complements are generated independently of one another often leads to incorrect parses.</S>
    <S sid="152" ssid="50">(See Figure 6 for examples.)</S>
    <S sid="153" ssid="51">In addition, the first child following the head of a prepositional phrase is marked as a complement.</S>
    <S sid="154" ssid="52">3.2.2 Probabilities over Subcategorization Frames.</S>
    <S sid="155" ssid="53">Model 1 could be retrained on training data with the enhanced set of nonterminals, and it might learn the lexical properties that distinguish complements and adjuncts (IBM vs. week, or that vs. because).</S>
    <S sid="156" ssid="54">It would still suffer, however, from the bad independence assumptions illustrated in Figure 6.</S>
    <S sid="157" ssid="55">To solve these kinds of problems, the generative process is extended to include a probabilistic choice of left and right subcategorization frames: Thus the subcategorization requirements are added to the conditioning context.</S>
    <S sid="158" ssid="56">As complements are generated they are removed from the appropriate subcategorization multiset.</S>
    <S sid="159" ssid="57">Most importantly, the probability of generating the STOP symbol will be zero when the subcategorization frame is non-empty, and the probability of generating a particular complement will be zero when that complement is not in the subcategorization frame; thus all and only the required complements will be generated.</S>
    <S sid="160" ssid="58">The probability of the phrase S(bought) &#8594; NP(week) NP-C(IBM) VP(bought) is now Here the head initially decides to take a single NP-C (subject) to its left and no complements to its right.</S>
    <S sid="161" ssid="59">NP-C(IBM) is immediately generated as the required subject, and NP-C is removed from LC, leaving it empty when the next modifier, NP(week), is generated.</S>
    <S sid="162" ssid="60">The incorrect structures in Figure 6 should now have low probability, because Plc({NP-C,NP-C}  |S,VP,was) and Prc({NP-C,VP-C}  |VP,VB,was) should be small.</S>
    <S sid="163" ssid="61">Another obstacle to extracting predicate-argument structure from parse trees is whmovement.</S>
    <S sid="164" ssid="62">This section describes a probabilistic treatment of extraction from relative clauses.</S>
    <S sid="165" ssid="63">Noun phrases are most often extracted from subject position, object position, or from within PPs: It might be possible to write rule-based patterns that identify traces in a parse tree.</S>
    <S sid="166" ssid="64">We argue again, however, that this task is best integrated into the parser: The task is complex enough to warrant a probabilistic treatment, and integration may help parsing accuracy.</S>
    <S sid="167" ssid="65">A couple of complexities are that modification by an SBAR does not always involve extraction (e.g., the fact (SBAR that besoboru is played with a ball and a bat)), and it is not uncommon for extraction to occur through several constituents (e.g., The changes (SBAR that he said the government was prepared to make TRACE)).</S>
    <S sid="168" ssid="66">One hope is that an integrated treatment of traces will improve the parameterization of the model.</S>
    <S sid="169" ssid="67">In particular, the subcategorization probabilities are smeared by extraction.</S>
    <S sid="170" ssid="68">In examples (1), (2), and (3), bought is a transitive verb; but without knowledge of traces, example (2) in training data will contribute to the probability of bought&#8217;s being an intransitive verb.</S>
    <S sid="171" ssid="69">Formalisms similar to GPSG (Gazdar et al. 1985) handle wh-movement by adding a gap feature to each nonterminal in the tree and propagating gaps through the tree until they are finally discharged as a trace complement (see Figure 7).</S>
    <S sid="172" ssid="70">In extraction cases the Penn Treebank annotation coindexes a TRACE with the WHNP head of the SBAR, so it is straightforward to add this information to trees in training data.</S>
    <S sid="173" ssid="71">A +gap feature can be added to nonterminals to describe wh-movement.</S>
    <S sid="174" ssid="72">The top-level NP initially generates an SBAR modifier but specifies that it must contain an NP trace by adding the +gap feature.</S>
    <S sid="175" ssid="73">The gap is then passed down through the tree, until it is discharged as a TRACE complement to the right of bought.</S>
    <S sid="176" ssid="74">Given that the LHS of the rule has a gap, there are three ways that the gap can be passed down to the RHS: Head: The gap is passed to the head of the phrase, as in rule (3) in Figure 7.</S>
    <S sid="177" ssid="75">Left, Right: The gap is passed on recursively to one of the left or right modifiers of the head or is discharged as a TRACE argument to the left or right of the head.</S>
    <S sid="178" ssid="76">In rule (2) in Figure 7, it is passed on to a right modifier, the S complement.</S>
    <S sid="179" ssid="77">In rule (4), a TRACE is generated to the right of the head VB.</S>
    <S sid="180" ssid="78">We specify a parameter type Pg(G |P, h, H) where G is either Head, Left, or Right.</S>
    <S sid="181" ssid="79">The generative process is extended to choose among these cases after generating the head of the phrase.</S>
    <S sid="182" ssid="80">The rest of the phrase is then generated in different ways depending on how the gap is propagated.</S>
    <S sid="183" ssid="81">In the Head case the left and right modifiers are generated as normal.</S>
    <S sid="184" ssid="82">In the Left and Right cases a +gap requirement is added to either the left or right SUBCAT variable.</S>
    <S sid="185" ssid="83">This requirement is fulfilled (and removed from the subcategorization list) when either a trace or a modifier nonterminal that has the +gap feature, is generated.</S>
    <S sid="186" ssid="84">For example, rule (2) in Figure 7, SBAR(that)(+gap) &#8594; WHNP(that) S-C(bought)(+gap), has probability In rule (2), Right is chosen, so the +gap requirement is added to RC.</S>
    <S sid="187" ssid="85">Generation of S-C(bought)(+gap) fulfills both the S-C and +gap requirements in RC.</S>
    <S sid="188" ssid="86">In rule (4), Right is chosen again.</S>
    <S sid="189" ssid="87">Note that generation of TRACE satisfies both the NP-C and +gap subcategorization requirements.</S>
  </SECTION>
  <SECTION title="4." number="5">
    <S sid="190" ssid="1">Sections 3.1 to 3.3 described the basic framework for the parsing models in this article.</S>
    <S sid="191" ssid="2">In this section we describe how some linguistic phenomena (nonrecursive NPs and coordination, for example) clearly violate the independence assumptions of the general models.</S>
    <S sid="192" ssid="3">We describe a number of these special cases, in each instance arguing that the phenomenon violates the independence assumptions, then describing how the model can be refined to deal with the problem.</S>
    <S sid="193" ssid="4">We define nonrecursive NPs (from here on referred to as base-NPs and labeled NPB rather than NP) as NPs that do not directly dominate an NP themselves, unless the dominated NP is a possessive NP (i.e., it directly dominates a POS-tag POS).</S>
    <S sid="194" ssid="5">Figure 8 gives some examples.</S>
    <S sid="195" ssid="6">Base-NPs deserve special treatment for three reasons: &#8226; The boundaries of base-NPs are often strongly marked.</S>
    <S sid="196" ssid="7">In particular, the start points of base-NPs are often marked with a determiner or another distinctive item, such as an adjective.</S>
    <S sid="197" ssid="8">Because of this, the probability of generating the STOP symbol should be greatly increased when the previous modifier is, for example, a determiner.</S>
    <S sid="198" ssid="9">As they stand, the independence assumptions in the three models lose this information.</S>
    <S sid="199" ssid="10">The probability of NPB(dog) &#8594; DT(the) NN(dog) would be estimated as11 In making the independence assumption the model will fail to learn that the STOP symbol is very likely to follow a determiner.</S>
    <S sid="200" ssid="11">As a result, the model will assign unreasonably high probabilities to NPs such as [NP yesterday the dog] in sentences such as Yesterday the dog barked. nonterminal is an NPB.</S>
    <S sid="201" ssid="12">Specifically, equations (5) and (6) are modified to be The modifier and previous-modifier nonterminals are always adjacent, so the distance variable is constant and is omitted.</S>
    <S sid="202" ssid="13">For the purposes of this model, L0(l0) and R0(r0) are defined to be H(h).</S>
    <S sid="203" ssid="14">The probability of the previous example is now Coordination constructions are another example in which the independence assumptions in the basic models fail badly (at least given the current annotation method in the treebank).</S>
    <S sid="204" ssid="15">Figure 9 shows how coordination is annotated in the treebank.12 To use an example to illustrate the problems, take the rule NP(man) &#8594; NP(man) CC(and) NP(dog), which has probability The independence assumptions mean that the model fails to learn that there is always exactly one phrase following the coordinator (CC).</S>
    <S sid="205" ssid="16">The basic probability models will give much too high probabilities to unlikely phrases such as NP &#8594; NP CC or NP &#8594; NP CC NP NP.</S>
    <S sid="206" ssid="17">For this reason we alter the generative process to allow generation of both the coordinator and the following phrase in one step; instead of just generating a nonterminal at each step, a nonterminal and a binary-valued coord flag are generated. coord = 1 if there is a coordination relationship.</S>
    <S sid="207" ssid="18">In the generative process, generation of a coord = 1 flag along with a modifier triggers an additional step in the generative (a) The generic way of annotating coordination in the treebank.</S>
    <S sid="208" ssid="19">(b) and (c) show specific examples (with base-NPs added as described in section 4.1).</S>
    <S sid="209" ssid="20">Note that the first item of the conjunct is taken as the head of the phrase. process, namely, the generation of the coordinator tag/word pair, parameterized by the P,, parameter.</S>
    <S sid="210" ssid="21">For the preceding example this would give probability Note the new type of parameter, P,,, for the generation of the coordinator word and POS tag.</S>
    <S sid="211" ssid="22">The generation of coord=1 along with NP(dog) in the example implicitly requires generation of a coordinator tag/word pair through the P,, parameter.</S>
    <S sid="212" ssid="23">The generation of this tag/word pair is conditioned on the two words in the coordination dependency (man and dog in the example) and the label on their relationship (NP,NP,NP in the example, representing NP coordination).</S>
    <S sid="213" ssid="24">The coord flag is implicitly zero when normal nonterminals are generated; for example, the phrase S(bought) &#8594; NP(week) NP(IBM) VP(bought) now has probability This section describes our treatment of &#8220;punctuation&#8221; in the model, where &#8220;punctuation&#8221; is used to refer to words tagged as a comma or colon.</S>
    <S sid="214" ssid="25">Previous work&#8212;the generative models described in Collins (1996) and the earlier version of these models described in Collins (1997)&#8212;conditioned on punctuation as surface features of the string, treating it quite differently from lexical items.</S>
    <S sid="215" ssid="26">In particular, the model in Collins (1997) failed to generate punctuation, a deficiency of the model.</S>
    <S sid="216" ssid="27">This section describes how punctuation is integrated into the generative models.</S>
    <S sid="217" ssid="28">Our first step is to raise punctuation as high in the parse trees as possible.</S>
    <S sid="218" ssid="29">Punctuation at the beginning or end of sentences is removed from the training/test data altogether.13 All punctuation items apart from those tagged as comma or colon (items such as quotation marks and periods, tagged &#8220; &#8221; or . ) are removed altogether.</S>
    <S sid="219" ssid="30">These transformations mean that punctuation always appears between two nonterminals, as opposed to appearing at the end of a phrase.</S>
    <S sid="220" ssid="31">(See Figure 10 for an example.)</S>
    <S sid="221" ssid="32">A parse tree before and after punctuation transformations.</S>
    <S sid="222" ssid="33">13 As one of the anonymous reviewers of this article pointed out, this choice of discarding the sentence-final punctuation may not be optimal, as the final punctuation mark may well carry useful information about the sentence structure.</S>
    <S sid="223" ssid="34">Punctuation is then treated in a very similar way to coordination: Our intuition is that there is a strong dependency between the punctuation mark and the modifier generated after it.</S>
    <S sid="224" ssid="35">Punctuation is therefore generated with the following phrase through a punc flag that is similar to the coord flag (a binary-valued feature equal to one if a punctuation mark is generated with the following phrase).</S>
    <S sid="225" ssid="36">Under this model, NP(Vinken) &#8594; NPB(Vinken) ,(,) ADJP(old) would have probability Pp is a new parameter type for generation of punctuation tag/word pairs.</S>
    <S sid="226" ssid="37">The generation of punc=1 along with ADJP(old) in the example implicitly requires generation of a punctuation tag/word pair through the Pp parameter.</S>
    <S sid="227" ssid="38">The generation of this tag/word pair is conditioned on the two words in the punctuation dependency (Vinken and old in the example) and the label on their relationship (NP,NPB,ADJP in the example.)</S>
    <S sid="228" ssid="39">Sentences in the treebank occur frequently with PRO subjects that may or may not be controlled: As the treebank annotation currently stands, the nonterminal is S whether or not a sentence has an overt subject.</S>
    <S sid="229" ssid="40">This is a problem for the subcategorization probabilities in models 2 and 3: The probability of having zero subjects, Pl,({}  |S, VP, verb), will be fairly high because of this.</S>
    <S sid="230" ssid="41">In addition, sentences with and without subjects appear in quite different syntactic environments.</S>
    <S sid="231" ssid="42">For these reasons we modify the nonterminal for sentences without subjects to be SG (see figure 11).</S>
    <S sid="232" ssid="43">The resulting model has a cleaner division of subcategorization: Pl,({NP-C}  |S, VP, verb) ,: 1 and Pl,({NP-C}  |SG, VP, verb) = 0.</S>
    <S sid="233" ssid="44">The model will learn probabilistically the environments in which S and SG are likely to appear.</S>
    <S sid="234" ssid="45">As a final step, we use the rule concerning punctuation introduced in Collins (1996) to impose a constraint as follows.</S>
    <S sid="235" ssid="46">If for any constituent Z in the chart Z &#8594; &lt;..X Y..&gt; two of its children X and Y are separated by a comma, then the last word in Y must be directly followed by a comma, or must be the last word in the sentence.</S>
    <S sid="236" ssid="47">In training data 96% of commas follow this rule.</S>
    <S sid="237" ssid="48">The rule has the benefit of improving efficiency by reducing the number of constituents in the chart.</S>
    <S sid="238" ssid="49">It would be preferable to develop a probabilistic analog of this rule, but we leave this to future research.</S>
    <S sid="239" ssid="50">(a) The treebank annotates sentences with empty subjects with an empty -NONE- element under subject position; (b) in training (and for evaluation), this null element is removed; (c) in models 2 and 3, sentences without subjects are changed to have a nonterminal SG.</S>
  </SECTION>
  <SECTION title="5." number="6">
    <S sid="240" ssid="1">Table 1 shows the various levels of back-off for each type of parameter in the model.</S>
    <S sid="241" ssid="2">Note that we decompose PL(Li(lwi, lti), c, p  |P, H, w, t, &#8710;, LC) (where lwi and lti are the word and POS tag generated with nonterminal Li, c and p are the coord and punc flags associated with the nonterminal, and &#8710; is the distance measure) into the product where e1, e2, and e3 are maximum-likelihood estimates with the context at levels 1, 2, and 3 in the table, and A1, A2 and A3 are smoothing parameters, where 0 &lt; Ai &lt; 1.</S>
    <S sid="242" ssid="3">We use the smoothing method described in Bikel et al. (1997), which is derived from a method described in Witten and Bell (1991).</S>
    <S sid="243" ssid="4">First, say that the most specific estimate e1 = n1 f1 ; that is, f1 is the value of the denominator count in the relative frequency estimate.</S>
    <S sid="244" ssid="5">Second, define u1 to be the number of distinct outcomes seen in the f1 events in training data.</S>
    <S sid="245" ssid="6">The variable u1 can take any value from one to f1 inclusive.</S>
    <S sid="246" ssid="7">Then we set Analogous definitions for f2 and u2 lead to A2 = f2 f2+5u2 .</S>
    <S sid="247" ssid="8">The coefficient five was chosen to maximize accuracy on the development set, section 0 of the treebank (in practice it was found that any value in the range 2&#8211;5 gave a very similar level of performance).</S>
    <S sid="248" ssid="9">All words occurring less than six times14 in training data, and words in test data that have never been seen in training, are replaced with the UNKNOWN token.</S>
    <S sid="249" ssid="10">This allows the model to handle robustly the statistics for rare or new words.</S>
    <S sid="250" ssid="11">Words in test data that have not been seen in training are deterministically assigned the POS tag that is assigned by the tagger described in Ratnaparkhi (1996).</S>
    <S sid="251" ssid="12">As a preprocessing step, the tagger is used to decode each test data sentence.</S>
    <S sid="252" ssid="13">All other words are tagged during parsing, the output from Ratnaparkhi&#8217;s tagger being ignored.</S>
    <S sid="253" ssid="14">The POS tags allowed for each word are limited to those that have been seen in training data for that word (any tag/word pairs not seen in training would give an estimate of zero in the PL2 and PR2 distributions).</S>
    <S sid="254" ssid="15">The model is fully integrated, in that part-of-speech tags are statistically generated along with words in the models, so that the parser will make a statistical decision as to the most likely tag for each known word in the sentence.</S>
    <S sid="255" ssid="16">The parsing algorithm for the models is a dynamic programming algorithm, which is very similar to standard chart parsing algorithms for probabilistic or weighted grammars.</S>
    <S sid="256" ssid="17">The algorithm has complexity O(n5), where n is the number of words in the string.</S>
    <S sid="257" ssid="18">In practice, pruning strategies (methods that discard lower-probability constituents in the chart) can improve efficiency a great deal.</S>
    <S sid="258" ssid="19">The appendices of Collins (1999) give a precise description of the parsing algorithms, an analysis of their computational complexity, and also a description of the pruning methods that are employed.</S>
    <S sid="259" ssid="20">See Eisner and Satta (1999) for an O(n4) algorithm for lexicalized grammars that could be applied to the models in this paper.</S>
    <S sid="260" ssid="21">Eisner and Satta (1999) also describe an O(n3) algorithm for a restricted class of lexicalized grammars; it is an open question whether this restricted class includes the models in this article.</S>
  </SECTION>
  <SECTION title="6." number="7">
    <S sid="261" ssid="1">The parser was trained on sections 2&#8211;21 of the Wall Street Journal portion of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) (approximately 40,000 sentences) and tested on section 23 (2,416 sentences).</S>
    <S sid="262" ssid="2">We use the PARSEVAL measures (Black et al. 1991) to compare performance: number of correct constituents in proposed parse number of constituents in proposed parse number of correct constituents in proposed parse number of constituents in treebank parse Crossing brackets = number of constituents that violate constituent boundaries with a constituent in the treebank parse For a constituent to be &#8220;correct,&#8221; it must span the same set of words (ignoring punctuation, i.e., all tokens tagged as commas, colons, or quotation marks) and have the same label15 as a constituent in the treebank parse.</S>
    <S sid="263" ssid="3">Table 2 shows the results for models 1, 2 and 3 and a variety of other models in the literature.</S>
    <S sid="264" ssid="4">Two models (Collins 2000; Charniak 2000) outperform models 2 and 3 on section 23 of the treebank.</S>
    <S sid="265" ssid="5">Collins (2000) uses a technique based on boosting algorithms for machine learning that reranks n-best output from model 2 in this article.</S>
    <S sid="266" ssid="6">Charniak (2000) describes a series of enhancements to the earlier model of Charniak (1997).</S>
    <S sid="267" ssid="7">The precision and recall of the traces found by Model 3 were 93.8% and 90.1%, respectively (out of 437 cases in section 23 of the treebank), where three criteria must be met for a trace to be &#8220;correct&#8221;: (1) It must be an argument to the correct headword; (2) It must be in the correct position in relation to that headword (preceding or following); 15 Magerman (1995) collapses ADVP and PRT into the same label; for comparison, we also removed this distinction when calculating scores.</S>
    <S sid="268" ssid="8">Results on Section 23 of the WSJ Treebank.</S>
    <S sid="269" ssid="9">LR/LP = labeled recall/precision.</S>
    <S sid="270" ssid="10">CBs is the average number of crossing brackets per sentence.</S>
    <S sid="271" ssid="11">0 CBs, &lt; 2 CBs are the percentage of sentences with 0 or &lt; 2 crossing brackets respectively.</S>
    <S sid="272" ssid="12">All the results in this table are for models trained and tested on the same data, using the same evaluation metric.</S>
    <S sid="273" ssid="13">(Note that these results show a slight improvement over those in (Collins 97); the main model changes were the improved treatment of punctuation (section 4.3) together with the addition of the Pp and Pcc parameters.) and (3) It must be dominated by the correct nonterminal label.</S>
    <S sid="274" ssid="14">For example, in Figure 7, the trace is an argument to bought, which it follows, and it is dominated by a VP.</S>
    <S sid="275" ssid="15">Of the 437 cases, 341 were string-vacuous extraction from subject position, recovered with 96.3% precision and 98.8% recall; and 96 were longer distance cases, recovered with 81.4% precision and 59.4% recall.16</S>
  </SECTION>
  <SECTION title="7." number="8">
    <S sid="276" ssid="1">This section discusses some aspects of the models in more detail.</S>
    <S sid="277" ssid="2">Section 7.1 gives a much more detailed analysis of the parsers&#8217; performance.</S>
    <S sid="278" ssid="3">In section 7.2 we examine 16 We exclude infinitival relative clauses from these figures (for example, I called a plumber TRACE to fix the sink, where plumber is coindexed with the trace subject of the infinitival).</S>
    <S sid="279" ssid="4">The algorithm scored 41% precision and 18% recall on the 60 cases in section 23&#8212;but infinitival relatives are extremely difficult even for human annotators to distinguish from purpose clauses (in this case, the infinitival could be a purpose clause modifying called) (Ann Taylor, personal communication, 1997). the distance features in the model.</S>
    <S sid="280" ssid="5">In section 7.3 we examine how the model interacts with the Penn Treebank style of annotation.</S>
    <S sid="281" ssid="6">Finally, in section 7.4 we discuss the need to break down context-free rules in the treebank in such a way that the model will generalize to give nonzero probability to rules not seen in training.</S>
    <S sid="282" ssid="7">In each case we use three methods of analysis.</S>
    <S sid="283" ssid="8">First, we consider how various aspects of the model affect parsing performance, through accuracy measurements on the treebank.</S>
    <S sid="284" ssid="9">Second, we look at the frequency of different constructions in the treebank.</S>
    <S sid="285" ssid="10">Third, we consider linguistically motivated examples as a way of justifying various modeling choices.</S>
    <S sid="286" ssid="11">In this section we look more closely at the parser, by evaluating its performance on specific constituents or constructions.</S>
    <S sid="287" ssid="12">The intention is to get a better idea of the parser&#8217;s strengths and weaknesses.</S>
    <S sid="288" ssid="13">First, Table 3 has a breakdown of precision and recall by constituent type.</S>
    <S sid="289" ssid="14">Although somewhat useful in understanding parser performance, a breakdown of accuracy by constituent type fails to capture the idea of attachment accuracy.</S>
    <S sid="290" ssid="15">For this reason we also evaluate the parser&#8217;s precision and recall in recovering dependencies between words.</S>
    <S sid="291" ssid="16">This gives a better indication of the accuracy on different kinds of attachments.</S>
    <S sid="292" ssid="17">A dependency is defined as a triple with the following elements (see Figure 12 for an example tree and its associated dependencies): Recall and precision for different constituent types, for section 0 of the treebank with model 2.</S>
    <S sid="293" ssid="18">Label is the nonterminal label; Proportion is the percentage of constituents in the treebank section 0 that have this label; Count is the number of constituents that have this label.</S>
    <S sid="294" ssid="19">A tree and its associated dependencies.</S>
    <S sid="295" ssid="20">Note that in &#8220;normalizing&#8221; dependencies, all POS tags are replaced with TAG, and the NP-C parent in the fifth relation is replaced with NP.</S>
    <S sid="296" ssid="21">In addition, the relation is &#8220;normalized&#8221; to some extent.</S>
    <S sid="297" ssid="22">First, all POS tags are replaced with the token TAG, so that POS-tagging errors do not lead to errors in dependencies.17 Second, any complement markings on the parent or head nonterminal are removed.</S>
    <S sid="298" ssid="23">For example, (NP-C, NPB, PP, R) is replaced by (NP, NPB, PP, R).</S>
    <S sid="299" ssid="24">This prevents parsing errors where a complement has been mistaken to be an adjunct (or vice versa), leading to more than one dependency error.</S>
    <S sid="300" ssid="25">As an example, in Figure 12, if the NP the man with the telescope was mistakenly identified as an adjunct, then without normalization, this would lead to two dependency errors: Both the PP dependency and the verb-object relation would be incorrect.</S>
    <S sid="301" ssid="26">With normalization, only the verb-object relation is incorrect.</S>
    <S sid="302" ssid="27">Under this definition, gold-standard and parser-output trees can be converted to sets of dependencies, and precision and recall can be calculated on these dependencies.</S>
    <S sid="303" ssid="28">Dependency accuracies are given for section 0 of the treebank in table 4.</S>
    <S sid="304" ssid="29">Table 5 gives a breakdown of the accuracies by dependency type.</S>
    <S sid="305" ssid="30">Table 6 shows the dependency accuracies for eight subtypes of dependency that together account for 94% of all dependencies: complement, or ( VP TAG ** ), where ** is any complement except VP-C (i.e., auxiliary-verb&#8212;verb dependencies are excluded).</S>
    <S sid="306" ssid="31">The most frequent verb complements, subject-verb and object-verb, are recovered with over 95% precision and 92% recall.</S>
    <S sid="307" ssid="32">A conclusion to draw from these accuracies is that the parser is doing very well at recovering the core structure of sentences: complements, sentential heads, and base-NP relationships (NP chunks) are all recovered with over 90% accuracy.</S>
    <S sid="308" ssid="33">The main sources of errors are adjuncts.</S>
    <S sid="309" ssid="34">Coordination is especially difficult for the parser, most likely because it often involves a dependency between two content words, leading to very sparse statistics.</S>
    <S sid="310" ssid="35">The distance measure, whose implementation was described in section 3.1.1, deserves more discussion and motivation.</S>
    <S sid="311" ssid="36">In this section we consider it from three perspectives: its influence on parsing accuracy; an analysis of distributions in training data that are sensitive to the distance variables; and some examples of sentences in which the distance measure is useful in discriminating among competing analyses.</S>
    <S sid="312" ssid="37">7.2.1 Impact of the Distance Measure on Accuracy.</S>
    <S sid="313" ssid="38">Table 7 shows the results for models 1 and 2 with and without the adjacency and verb distance measures.</S>
    <S sid="314" ssid="39">It is clear that the distance measure improves the models&#8217; accuracy.</S>
    <S sid="315" ssid="40">What is most striking is just how badly model 1 performs without the distance measure.</S>
    <S sid="316" ssid="41">Looking at the parser&#8217;s output, the reason for this poor performance is that the adjacency condition in the distance measure is approximating subcategorization information.</S>
    <S sid="317" ssid="42">In particular, in phrases such as PPs and SBARs (and, to a lesser extent, in VPs) that almost always take exactly one complement to the right of their head, the adjacency feature encodes this monovalency through parameters P(STOPIPP/SBAR, adjacent) = 0 and P(STOPIPP/SBAR, not adjacent) = 1.</S>
    <S sid="318" ssid="43">Figure 13 shows some particularly bad structures returned by model 1 with no distance variables.</S>
    <S sid="319" ssid="44">Another surprise is that subcategorization can be very useful, but that the distance measure has masked this utility.</S>
    <S sid="320" ssid="45">One interpretation in moving from the least parameterized model (Model 1 [No, No]) to the fully parameterized model (Model 2 [Yes, Yes]) is that the adjacency condition adds around 11% in accuracy; the verb condition adds another 1.5%; and subcategorization finally adds a mere 0.8%.</S>
    <S sid="321" ssid="46">Under this interpretation subcategorization information isn&#8217;t all that useful (and this was my original assumption, as this was the order in which features were originally added to the model).</S>
    <S sid="322" ssid="47">But under another interpretation subcategorization is very useful: In moving from Model 1 (No, No) to Model 2 (No, No), we see a 10% improvement as a result of subcategorization parameters; adjacency then adds a 1.5% improvement; and the verb condition adds a final 1% improvement.</S>
    <S sid="323" ssid="48">From an engineering point of view, given a choice of whether to add just distance or subcategorization to the model, distance is preferable.</S>
    <S sid="324" ssid="49">But linguistically it is clear that adjacency can only approximate subcategorization and that subcategorization is Distribution of nonterminals generated as postmodifiers to an NP (see tree to the left), at various distances from the head.</S>
    <S sid="325" ssid="50">A = True means the modifier is adjacent to the head, V = True means there is a verb between the head and the modifier.</S>
    <S sid="326" ssid="51">Distributions were calculated from the first 10000 events for each of the three cases in sections 2-21 of the treebank. more &#8220;correct&#8221; in some sense.</S>
    <S sid="327" ssid="52">In free-word-order languages, distance may not approximate subcategorization at all well: A complement may appear to either the right or left of the head, confusing the adjacency condition.</S>
    <S sid="328" ssid="53">7.2.2 Frequencies in Training Data.</S>
    <S sid="329" ssid="54">Tables 8 and 9 show the effect of distance on the distribution of modifiers in two of the most frequent syntactic environments: NP and verb modification.</S>
    <S sid="330" ssid="55">The distribution varies a great deal with distance.</S>
    <S sid="331" ssid="56">Most striking is the way that the probability of STOP increases with increasing distance: from 71% to 89% to 98% in the NP case, from 8% to 60% to 96% in the verb case.</S>
    <S sid="332" ssid="57">Each modifier probability generally decreases with distance.</S>
    <S sid="333" ssid="58">For example, the probability of seeing a PP modifier to an NP decreases from 17.7% to 5.57% to 0.93%.</S>
    <S sid="334" ssid="59">Distribution of nonterminals generated as postmodifiers to a verb within a VP (see tree to the left), at various distances from the head.</S>
    <S sid="335" ssid="60">A = True means the modifier is adjacent to the head; V = True means there is a verb between the head and the modifier.</S>
    <S sid="336" ssid="61">The distributions were calculated from the first 10000 events for each of the distributions in sections 2&#8211;21.</S>
    <S sid="337" ssid="62">Auxiliary verbs (verbs taking a VP complement to their right) were excluded from these statistics. components of the distance measure allow the model to learn a preference for rightbranching structures.</S>
    <S sid="338" ssid="63">First, consider the adjacency condition.</S>
    <S sid="339" ssid="64">Figure 14 shows some examples in which right-branching structures are more frequent.</S>
    <S sid="340" ssid="65">Using the statistics from Tables 8 and 9, the probability of the alternative structures can be calculated.</S>
    <S sid="341" ssid="66">The results are given below.</S>
    <S sid="342" ssid="67">The right-branching structures get higher probability (although this is before the lexical-dependency probabilities are multiplied in, so this &#8220;prior&#8221; preference for right-branching structures can be overruled by lexical preferences).</S>
    <S sid="343" ssid="68">If the distance variables were not conditioned on, the product of terms for the two alternatives would be identical, and the model would have no preference for one structure over another.</S>
    <S sid="344" ssid="69">Probabilities for the two alternative PP structures in Figure 14 (excluding probability terms that are constant across the two structures; A=1 means distance is adjacent, A=0 means not adjacent) are as follows: Some alternative structures for the same surface sequence of chunks (NPB PP PP in the first case, NPB PP SBAR in the second case) in which the adjacency condition distinguishes between the two structures.</S>
    <S sid="345" ssid="70">The percentages are taken from sections 2&#8211;21 of the treebank.</S>
    <S sid="346" ssid="71">In both cases right-branching structures are more frequent.</S>
    <S sid="347" ssid="72">= 0.177 x 0.0557 x 0.8853 x 0.7078 = 0.006178 Probabilities for the SBAR case in Figure 14, assuming the SBAR contains a verb (V=0 means modification does not cross a verb, V=1 means it does), are as follows: Some alternative structures for the same surface sequence of chunks in which the verb condition in the distance measure distinguishes between the two structures.</S>
    <S sid="348" ssid="73">In both cases the low-attachment analyses will get higher probability under the model, because of the low probability of generating a PP modifier involving a dependency that crosses a verb.</S>
    <S sid="349" ssid="74">(X stands for any nonterminal.) ples in which the verb condition is important in differentiating the probability of two structures.</S>
    <S sid="350" ssid="75">In both cases an adjunct can attach either high or low, but high attachment results in a dependency&#8217;s crossing a verb and has lower probability.</S>
    <S sid="351" ssid="76">An alternative to the surface string feature would be a predicate such as were any of the previous modifiers in X, where X is a set of nonterminals that are likely to contain a verb, such as VP, SBAR, S, or SG.</S>
    <S sid="352" ssid="77">This would allow the model to handle cases like the first example in Figure 15 correctly.</S>
    <S sid="353" ssid="78">The second example shows why it is preferable to condition on the surface string.</S>
    <S sid="354" ssid="79">In this case the verb is &#8220;invisible&#8221; to the top level, as it is generated recursively below the NP object.</S>
    <S sid="355" ssid="80">7.2.5 Structural versus Semantic Preferences.</S>
    <S sid="356" ssid="81">One hypothesis would be that lexical statistics are really what is important in parsing: that arriving at a correct interpretation for a sentence is simply a matter of finding the most semantically plausible analysis, and that the statistics related to lexical dependencies approximate this notion of plausibility.</S>
    <S sid="357" ssid="82">Implicitly, we would be just as well off (maybe even better off) if statistics were calculated between items at the predicate-argument level, with no reference to structure.</S>
    <S sid="358" ssid="83">The distance preferences under this interpretation are just a way of mitigating sparse-data problems: When the lexical statistics are too sparse, then falling back on some structural preference is not ideal, but is at least better than chance.</S>
    <S sid="359" ssid="84">This hypothesis is suggested by previous work on specific cases of attachment ambiguity such as PP attachment (see, e.g., Collins and Brooks 1995), which has showed that models will perform better given lexical statistics, and that a straight structural preference is merely a fallback.</S>
    <S sid="360" ssid="85">But some examples suggest this is not the case: that, in fact, many sentences have several equally semantically plausible analyses, but that structural preferences distinguish strongly among them.</S>
    <S sid="361" ssid="86">Take the following example (from Pereira and Warren 1980): Surprisingly, this sentence has two analyses: Bill can be the deep subject of either believed or shot.</S>
    <S sid="362" ssid="87">Yet people have a very strong preference for Bill to be doing the shooting, so much so that they may even miss the second analysis.</S>
    <S sid="363" ssid="88">(To see that the dispreferred analysis is semantically quite plausible, consider Bill believed John to have been shot.)</S>
    <S sid="364" ssid="89">As evidence that structural preferences can even override semantic plausibility, take the following example (from Pinker 1994): This sentence is a garden path: The structural preference for yesterday to modify the most recent verb is so strong that it is easy to miss the (only) semantically plausible interpretation, paraphrased as Flip said yesterday that Squeaky will do the work.</S>
    <S sid="365" ssid="90">The model makes the correct predictions in these cases.</S>
    <S sid="366" ssid="91">In example (4), the statistics in Table 9 show that a PP is nine times as likely to attach low as to attach high when two verbs are candidate attachment points (the chances of seeing a PP modifier are 15.8% and 1.73% in columns 1 and 5 of the table, respectively).</S>
    <S sid="367" ssid="92">In example (5), the probability of seeing an NP (adjunct) modifier to do in a nonadjacent but non-verbcrossing environment is 2.11% in sections 2&#8211;21 of the treebank (8 out of 379 cases); in contrast, the chance of seeing an NP adjunct modifying said across a verb is 0.026% (1 out of 3,778 cases).</S>
    <S sid="368" ssid="93">The two probabilities differ by a factor of almost 80.</S>
    <S sid="369" ssid="94">Figures 16 and 17 show some alternative styles of syntactic annotation.</S>
    <S sid="370" ssid="95">The Penn Treebank annotation style tends to leave trees quite flat, typically with one level of structure for each X-bar level; at the other extreme are completely binary-branching representations.</S>
    <S sid="371" ssid="96">The two annotation styles are in some sense equivalent, in that it is easy to define a one-to-one mapping between them.</S>
    <S sid="372" ssid="97">But crucially, two different annotation styles may lead to quite different parsing accuracies for a given model, even if the two representations are equivalent under some one-to-one mapping.</S>
    <S sid="373" ssid="98">A parsing model does not need to be tied to the annotation style of the treebank on which it is trained.</S>
    <S sid="374" ssid="99">The following procedure can be used to transform trees in both training and test data into a new representation: Alternative annotation styles for a sentence S with a verb head V, left modifiers X1, X2, and right modifiers Y1, Y2: (a) the Penn Treebank style of analysis (one level of structure for each bar level); (b) an alternative but equivalent binary branching representation.</S>
    <S sid="375" ssid="100">Alternative annotation styles for a noun phrase with a noun head N, left modifiers X1, X2, and right modifiers Y1, Y2: (a) the Penn Treebank style of analysis (one level of structure for each bar level, although note that both the nonrecursive and the recursive noun phrases are labeled NP; (b) an alternative but equivalent binary branching representation; (a) our modification of the Penn Treebank style to differentiate recursive and nonrecursive NPs (in some sense NPB is a bar 1 structure and NP is a bar 2 structure).</S>
    <S sid="376" ssid="101">As long as there is a one-to-one mapping between the treebank and the new representation, nothing is lost in making such a transformation.</S>
    <S sid="377" ssid="102">Goodman (1997) and Johnson (1997) both suggest this strategy.</S>
    <S sid="378" ssid="103">Goodman (1997) converts the treebank into binary-branching trees.</S>
    <S sid="379" ssid="104">Johnson (1997) considers conversion to a number of different representations and discusses how this influences accuracy for nonlexicalized PCFGs.</S>
    <S sid="380" ssid="105">The models developed in this article have tacitly assumed the Penn Treebank style of annotation and will perform badly given other representations (for example, binary-branching trees).</S>
    <S sid="381" ssid="106">This section makes this point more explicit, describing exactly what annotation style is suitable for the models and showing how other annotation styles will cause problems.</S>
    <S sid="382" ssid="107">This dependence on Penn Treebank&#8211;style annotations does not imply that the models are inappropriate for a treebank annotated in a different style: In this case we simply recommend transforming the trees into flat, one-levelper-X-bar-level trees before training the model, as in the three-step procedure outlined above.</S>
    <S sid="383" ssid="108">Other models in the literature are also very likely to be sensitive to annotation style.</S>
    <S sid="384" ssid="109">Charniak&#8217;s (1997) models will most likely perform quite differently with binarybranching trees (for example, his current models will learn that rules such as VP &#8594; V SG PP are very rare, but with binary-branching structures, this context sensitivity will be lost).</S>
    <S sid="385" ssid="110">The models of Magerman (1995) and Ratnaparkhi (1997) use contextual predicates that would most likely need to be modified given a different annotation style.</S>
    <S sid="386" ssid="111">Goodman&#8217;s (1997) models are the exception, as he already specifies that the treebank should be transformed into his chosen representation, binary-branching trees. resentations in Figures 16 and 17 have the same lexical dependencies (providing that the binary-branching structures are centered about the head of the phrase, as in the examples).</S>
    <S sid="387" ssid="112">The difference between the representations involves structural preferences such as the right-branching preferences encoded by the distance measure.</S>
    <S sid="388" ssid="113">Applying the models in this article to treebank analyses that use this type of &#8220;head-centered&#8221; BB = binary-branching structures; FLAT = Penn treebank style annotations.</S>
    <S sid="389" ssid="114">In each case the binary-branching annotation style prevents the model from learning that these structures should receive low probability because of the long distance dependency associated with the final PP (in boldface). binary-branching tree will result in a distance measure that incorrectly encodes a preference for right-branching structures.</S>
    <S sid="390" ssid="115">To see this, consider the examples in Figure 18.</S>
    <S sid="391" ssid="116">In each binary-branching example, the generation of the final modifying PP is &#8220;blind&#8221; to the distance between it and the head that it modifies.</S>
    <S sid="392" ssid="117">At the top level of the tree, it is apparently adjacent to the head; crucially, the closer modifier (SG in (a), the other PP in (b)) is hidden lower in the tree structure.</S>
    <S sid="393" ssid="118">So the model will be unable to differentiate generation of the PP in adjacent versus nonadjacent or non-verb-crossing versus verb-crossing environments, and the structures in Figure 18 will be assigned unreasonably high probabilities.</S>
    <S sid="394" ssid="119">This does not mean that distance preferences cannot be encoded in a binarybranching PCFG.</S>
    <S sid="395" ssid="120">Goodman (1997) achieves this by adding distance features to the nonterminals.</S>
    <S sid="396" ssid="121">The spirit of this implementation is that the top-level rules VP &#8594; VP PP and NP &#8594; NP PP would be modified to VP &#8594; VP(+rverb) PP and NP &#8594; NP(+rmod) PP, respectively, where (+rverb) means a phrase in which the head has a verb in its right modifiers, and (+rmod) means a phrase that has at least one right modifier to the head.</S>
    <S sid="397" ssid="122">The model will learn from training data that P(VP &#8594; VP(+rverb) PP|VP) &#65533; P(VP &#8594; VP(-rverb) PP|VP), that is, that a prepositional-phrase modification is much more likely when it does not cross a verb. shows the modification to the Penn Treebank annotation to relabel base-NPs as NPB.</S>
    <S sid="398" ssid="123">It also illustrates a problem that arises if a distinction between the two is not made: Structures such as that in Figure 19(b) are assigned high probabilities even if they Examples of other phrases in the Penn Treebank in which nonrecursive and recursive phrases are not differentiated. are never seen in training data.</S>
    <S sid="399" ssid="124">(Johnson [1997] notes that this structure has a higher probability than the correct, flat structure, given counts taken from the treebank for a standard PCFG.)</S>
    <S sid="400" ssid="125">The model is fooled by the binary-branching style into modeling both PPs as being adjacent to the head of the noun phrase, so 19(b) will be assigned a very high probability.</S>
    <S sid="401" ssid="126">This problem does not apply only to NPs: Other types of phrases such as adjectival phrases (ADJPs) or adverbial phrases (ADVPs) also have nonrecursive (bar 1) and recursive (bar 2) levels, which are not differentiated in the Penn Treebank.</S>
    <S sid="402" ssid="127">(See Figure 20 for examples.)</S>
    <S sid="403" ssid="128">Ideally these cases should be differentiated too: We did not implement this change because it is unlikely to make much difference in accuracy, given the relative infrequency of these cases (excluding coordination cases, and looking at the 80,254 instances in sections 2&#8211;21 of the Penn Treebank in which a parent and head nonterminal are the same: 94.5% are the NP case; 2.6% are cases of coordination in which a punctuation mark is the coordinator;18 only 2.9% are similar to those in Figure 20).</S>
    <S sid="404" ssid="129">The parsing approaches we have described concentrate on breaking down context-free rules in the treebank into smaller components.</S>
    <S sid="405" ssid="130">Lexicalized rules were initially broken down to bare-bones Markov processes, then increased dependency on previously generated modifiers was built back up through the distance measure and subcategorization.</S>
    <S sid="406" ssid="131">Even with this additional context, the models are still able to recover rules in test data that have never been seen in training data.</S>
    <S sid="407" ssid="132">An alternative, proposed in Charniak (1997), is to limit parsing to those contextfree rules seen in training data.</S>
    <S sid="408" ssid="133">A lexicalized rule is predicted in two steps.</S>
    <S sid="409" ssid="134">First, the whole context-free rule is generated.</S>
    <S sid="410" ssid="135">Second, the lexical items are filled in.</S>
    <S sid="411" ssid="136">The probability of a rule is estimated as19 The estimation technique used in Charniak (1997) for the CF rule probabilities interpolates several estimates, the lowest being P(Ln ... L1HR1 ... Rm)  |P).</S>
    <S sid="412" ssid="137">Any rules not seen in training data will be assigned zero probability with this model.</S>
    <S sid="413" ssid="138">Parse trees in test data will be limited to include rules seen in training.</S>
    <S sid="414" ssid="139">A problem with this approach is coverage.</S>
    <S sid="415" ssid="140">As shown in this section, many test data sentences will require rules that have not been seen in training.</S>
    <S sid="416" ssid="141">This gives motivation for breaking down rules into smaller components.</S>
    <S sid="417" ssid="142">This section motivates the need to break down rules from four perspectives.</S>
    <S sid="418" ssid="143">First, we discuss how the Penn Treebank annotation style leads to a very large number of grammar rules.</S>
    <S sid="419" ssid="144">Second, we assess the extent of the coverage problem by looking at rule frequencies in training data.</S>
    <S sid="420" ssid="145">Third, we conduct experiments to assess the impact of the coverage problem on accuracy.</S>
    <S sid="421" ssid="146">Fourth, we discuss how breaking rules down may improve estimation as well as coverage. the Penn Treebank annotation style has already been discussed, in section 7.3.</S>
    <S sid="422" ssid="147">The flatness of the trees leads to a very large (and constantly growing) number of rules, primarily because the number of adjuncts to a head is potentially unlimited: For example, there can be any number of PP adjuncts to a head verb.</S>
    <S sid="423" ssid="148">A binary-branching (Chomsky adjunction) grammar can generate an unlimited number of adjuncts with very few rules.</S>
    <S sid="424" ssid="149">For example, the following grammar generates any sequence VP &#8594; V In contrast, the Penn Treebank style would create a new rule for each number of PPs seen in training data.</S>
    <S sid="425" ssid="150">The grammar would be and so on Other adverbial adjuncts, such as adverbial phrases or adverbial SBARs, can also modify a verb several times, and all of these different types of adjuncts can be seen together in the same rule.</S>
    <S sid="426" ssid="151">The result is a combinatorial explosion in the number of rules.</S>
    <S sid="427" ssid="152">To give a flavor of this, here is a random sample of rules of the format VP &#8594; VB modifier* that occurred only once in sections 2&#8211;21 of the Penn Treebank: It is not only verb phrases that cause this kind of combinatorial explosion: Other phrases, in particular nonrecursive noun phrases, also contribute a huge number of rules.</S>
    <S sid="428" ssid="153">The next section considers the distributional properties of the rules in more detail.</S>
    <S sid="429" ssid="154">Note that there is good motivation for the Penn Treebank&#8217;s decision to represent rules in this way, rather than with rules expressing Chomsky adjunction (i.e., a schema in which complements and adjuncts are separated, through rule types (VP &#8594; VB {complement}*) and (VP &#8594; VP {adjunct})).</S>
    <S sid="430" ssid="155">First, it allows the argument/adjunct distinction for PP modifiers to verbs to be left undefined: This distinction was found to be very difficult for annotators.</S>
    <S sid="431" ssid="156">Second, in the surface ordering (as opposed to deep structure), adjuncts are often found closer to the head than complements, thereby yielding structures that fall outside the Chomsky adjunction schema.</S>
    <S sid="432" ssid="157">For example, a rule such as (VP &#8594; VB NP-C PP SBAR-C) is found very frequently in the Penn Treebank; SBAR complements nearly always extrapose over adjuncts.</S>
    <S sid="433" ssid="158">7.4.2 Quantifying the Coverage Problem.</S>
    <S sid="434" ssid="159">To quantify the coverage problem, rules were collected from sections 2&#8211;21 of the Penn Treebank.</S>
    <S sid="435" ssid="160">Punctuation was raised as high as possible in the tree, and the rules did not have complement markings or the distinction between base-NPs and recursive NPs.</S>
    <S sid="436" ssid="161">Under these conditions, 939,382 rule tokens were collected; there were 12,409 distinct rule types.</S>
    <S sid="437" ssid="162">We also collected the count for each rule.</S>
    <S sid="438" ssid="163">Table 10 shows some statistics for these rules.</S>
    <S sid="439" ssid="164">A majority of rules in the grammar (6,765, or 54.5%) occur only once.</S>
    <S sid="440" ssid="165">These rules account for 0.72% of rules by token.</S>
    <S sid="441" ssid="166">That is, if one of the 939,382 rule tokens in sections 2&#8211;21 of the treebank were drawn at random, there would be a 0.72% chance of its being the only instance of that rule in the 939,382 tokens.</S>
    <S sid="442" ssid="167">On the other hand, if a rule were drawn at random from the 12,409 rules in the grammar induced from those sections, there would be a 54.5% chance of that rule&#8217;s having occurred only once.</S>
    <S sid="443" ssid="168">The percentage by token of the one-count rules is an indication of the coverage problem.</S>
    <S sid="444" ssid="169">From this estimate, 0.72% of all rules (or 1 in 139 rules) required in test data would never have been seen in training.</S>
    <S sid="445" ssid="170">It was also found that 15.0% (1 in 6.67) of all sentences have at least one rule that occurred just once.</S>
    <S sid="446" ssid="171">This gives an estimate that roughly 1 in 6.67 sentences in test data will not be covered by a grammar induced from 40,000 sentences in the treebank.</S>
    <S sid="447" ssid="172">If the complement markings are added to the nonterminals, and the base-NP/nonrecursive NP distinction is made, then the coverage problem is made worse.</S>
    <S sid="448" ssid="173">Table 11 gives the statistics in this case.</S>
    <S sid="449" ssid="174">By our counts, 17.1% of all sentences (1 in 5.8 sentences) contain at least 1 one-count rule. the impact of the coverage problem on parsing accuracy.</S>
    <S sid="450" ssid="175">Section 0 of the treebank was parsed with models 1 and 2 as before, but the parse trees were restricted to include rules already seen in training data.</S>
    <S sid="451" ssid="176">Table 12 shows the results.</S>
    <S sid="452" ssid="177">Restricting the rules leads to a 0.5% decrease in recall and a 1.6% decrease in precision for model 1, and a 0.9% decrease in recall and a 2.0% decrease in precision for model 2. only motivation for breaking down rules.</S>
    <S sid="453" ssid="178">The method may also improve estimation.</S>
    <S sid="454" ssid="179">To see this, consider the rules headed by told, whose counts are shown in Table 13.</S>
    <S sid="455" ssid="180">Estimating the probability P(Rule  |VP, told) using Charniak&#8217;s (1997) method would interpolate two maximum-likelihood estimates: &#955;Pml(Rule  |VP, told) + (1 &#8722; &#955;)Pml(Rule  |VP) Estimation interpolates between the specific, lexically sensitive distribution in Table 13 and the nonlexical estimate based on just the parent nonterminal, VP.</S>
    <S sid="456" ssid="181">There are many different rules in the more specific distribution (26 different rule types, out of 147 tokens in which told was a VP head), and there are several one-count rules (11 cases).</S>
    <S sid="457" ssid="182">From these statistics &#955; would have to be relatively low.</S>
    <S sid="458" ssid="183">There is a high chance that a new rule for told will be required in test data; therefore a reasonable amount of probability mass must be left to the backed-off estimate Pml(Rule  |VP).</S>
    <S sid="459" ssid="184">This estimation method is missing a crucial generalization: In spite of there being many different rules, the distribution over subcategorization frames is much sharper.</S>
    <S sid="460" ssid="185">Told is seen with only five subcategorization frames in training data: The large number of rules is almost entirely due to adjuncts or punctuation appearing after or between complements.</S>
    <S sid="461" ssid="186">The estimation method in model 2 effectively estimates the probability of a rule as The left and right subcategorization frames, LC and RC, are chosen first.</S>
    <S sid="462" ssid="187">The entire rule is then generated by Markov processes.</S>
    <S sid="463" ssid="188">Once armed with the Pl, and Pr, parameters, the model has the ability to learn the generalization that told appears with a quite limited, sharp distribution over subcategorization frames.</S>
    <S sid="464" ssid="189">Say that these parameters are again estimated through interpolation, for example In this case &#955; can be quite high.</S>
    <S sid="465" ssid="190">Only five subcategorization frames (as opposed to 26 rule types) have been seen in the 147 cases.</S>
    <S sid="466" ssid="191">The lexically specific distribution Pml(LC I VP, told) can therefore be quite highly trusted.</S>
    <S sid="467" ssid="192">Relatively little probability mass is left to the backed-off estimate.</S>
    <S sid="468" ssid="193">In summary, from the distributions in Table 13, the model should be quite uncertain about what rules told can appear with.</S>
    <S sid="469" ssid="194">It should be relatively certain, however, about the subcategorization frame.</S>
    <S sid="470" ssid="195">Introducing subcategorization parameters allows the model to generalize in an important way about rules.</S>
    <S sid="471" ssid="196">We have carefully isolated the &#8220;core&#8221; of rules&#8212;the subcategorization frame&#8212;that the model should be certain about.</S>
    <S sid="472" ssid="197">We should note that Charniak&#8217;s method will certainly have some advantages in estimation: It will capture some statistical properties of rules that our independence assumptions will lose (e.g., the distribution over the number of PP adjuncts seen for a particular head).</S>
  </SECTION>
  <SECTION title="8." number="9">
    <S sid="473" ssid="1">Unfortunately, because of space limitations, it is not possible to give a complete review of previous work in this article.</S>
    <S sid="474" ssid="2">In the next two sections we give a detailed comparison of the models in this article to the lexicalized PCFG model of Charniak (1997) and the history-based models of Jelinek et al. (1994), Magerman (1995), and Ratnaparkhi (1997).</S>
    <S sid="475" ssid="3">For discussion of additional related work, chapter 4 of Collins (1999) attempts to give a comprehensive review of work on statistical parsing up to around 1998.</S>
    <S sid="476" ssid="4">Of particular relevance is other work on parsing the Penn WSJ Treebank (Jelinek et al. 1994; Magerman 1995; Eisner 1996a, 1996b; Collins 1996; Charniak 1997; Goodman 1997; Ratnaparkhi 1997; Chelba and Jelinek 1998; Roark 2001).</S>
    <S sid="477" ssid="5">Eisner (1996a, 1996b) describes several dependency-based models that are also closely related to the models in this article.</S>
    <S sid="478" ssid="6">Collins (1996) also describes a dependency-based model applied to treebank parsing.</S>
    <S sid="479" ssid="7">Goodman (1997) describes probabilistic feature grammars and their application to parsing the treebank.</S>
    <S sid="480" ssid="8">Chelba and Jelinek (1998) describe an incremental, history-based parsing approach that is applied to language modeling for speech recognition.</S>
    <S sid="481" ssid="9">History-based approaches were introduced to parsing in Black et al. (1992).</S>
    <S sid="482" ssid="10">Roark (2001) describes a generative probabilistic model of an incremental parser, with good results in terms of both parse accuracy on the treebank and also perplexity scores for language modeling.</S>
    <S sid="483" ssid="11">Earlier work that is of particular relevance considered the importance of relations between lexical heads for disambiguation in parsing.</S>
    <S sid="484" ssid="12">See Hindle and Rooth (1991) for one of the earliest pieces of research on this topic in the context of prepositional-phrase attachment ambiguity.</S>
    <S sid="485" ssid="13">For work that uses lexical relations for parse disambiguation&#8212; all with very promising results&#8212;see Sekine et al. (1992), Jones and Eisner (1992a, 1992b), and Alshawi and Carter (1994).</S>
    <S sid="486" ssid="14">Statistical models of lexicalized grammatical formalisms also lead to models with parameters corresponding to lexical dependencies.</S>
    <S sid="487" ssid="15">See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on stochastic tree-adjoining grammars.</S>
    <S sid="488" ssid="16">Joshi and Srinivas (1994) describe an alternative &#8220;supertagging&#8221; model for tree-adjoining grammars.</S>
    <S sid="489" ssid="17">See Alshawi (1996) for work on stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic version of link grammar.</S>
    <S sid="490" ssid="18">De Marcken (1995) considers stochastic lexicalized PCFGs, with specific reference to EM methods for unsupervised training.</S>
    <S sid="491" ssid="19">Seneff (1992) describes the use of Markov models for rule generation, which is closely related to the Markov-style rules in the models in the current article.</S>
    <S sid="492" ssid="20">Finally, note that not all machine-learning methods for parsing are probabilistic.</S>
    <S sid="493" ssid="21">See Brill (1993) and Hermjakob and Mooney (1997) for rule-based learning systems.</S>
    <S sid="494" ssid="22">In recent work, Chiang (2000) has shown that the models in the current article can be implemented almost unchanged in a stochastic tree-adjoining grammar.</S>
    <S sid="495" ssid="23">Bikel (2000) has developed generative statistical models that integrate word sense information into the parsing process.</S>
    <S sid="496" ssid="24">Eisner (2002) develops a sophisticated generative model for lexicalized context-free rules, making use of a probabilistic model of lexicalized transformations between rules.</S>
    <S sid="497" ssid="25">Blaheta and Charniak (2000) describe methods for the recovery of the semantic tags in the Penn Treebank annotations, a significant step forward from the complement/adjunct distinction recovered in model 2 of the current article.</S>
    <S sid="498" ssid="26">Charniak (2001) gives measurements of perplexity for a lexicalized PCFG.</S>
    <S sid="499" ssid="27">Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing.</S>
    <S sid="500" ssid="28">Miller et al. (2000) develop generative, lexicalized models for information extraction of relations.</S>
    <S sid="501" ssid="29">The approach enhances nonterminals in the parse trees to carry semantic labels and develops a probabilistic model that takes these labels into account.</S>
    <S sid="502" ssid="30">Collins et al. (1999) describe how the models in the current article were applied to parsing Czech.</S>
    <S sid="503" ssid="31">Charniak (2000) describes a parsing model that also uses Markov processes to generate rules.</S>
    <S sid="504" ssid="32">The model takes into account much additional context (such as previously generated modifiers, or nonterminals higher in the parse trees) through a maximum-entropy-inspired model.</S>
    <S sid="505" ssid="33">The use of additional features gives clear improvements in performance.</S>
    <S sid="506" ssid="34">Collins (2000) shows similar improvements through a quite different model based on boosting approaches to reranking (Freund et al. 1998).</S>
    <S sid="507" ssid="35">An initial model&#8212;in fact Model 2 described in the current article&#8212;is used to generate N-best output.</S>
    <S sid="508" ssid="36">The reranking approach attempts to rerank the N-best lists using additional features that are not used in the initial model.</S>
    <S sid="509" ssid="37">The intention of this approach is to allow greater flexibility in the features that can be included in the model.</S>
    <S sid="510" ssid="38">Finally, Bod (2001) describes a very different approach (a DOP approach to parsing) that gives excellent results on treebank parsing, comparable to the results of Charniak (2000) and Collins (2000).</S>
    <S sid="511" ssid="39">We now give a more detailed comparison of the models in this article to the parser of Charniak (1997).</S>
    <S sid="512" ssid="40">The model described in Charniak (1997) has two types of parameters: For example, the dependency parameter for an NP headed by profits, which is the subject of the verb rose, would be P(profits I NP, S, rose).</S>
    <S sid="513" ssid="41">This nonterminal could expand with any of the rules S &#8594; 0 in the grammar.</S>
    <S sid="514" ssid="42">The rule probability is defined as P(S &#8594; 0|rose, S, VP).</S>
    <S sid="515" ssid="43">So the rule probability depends on the nonterminal being expanded, its headword, and also its parent.</S>
    <S sid="516" ssid="44">The next few sections give further explanation of the differences between Charniak&#8217;s models and the models in this article. features of Charniak&#8217;s model.</S>
    <S sid="517" ssid="45">First, the rule probabilities are conditioned on the parent of the nonterminal being expanded.</S>
    <S sid="518" ssid="46">Our models do not include this information, although distinguishing recursive from nonrecursive NPs can be considered a reduced form of this information.</S>
    <S sid="519" ssid="47">(See section 7.3.2 for a discussion of this distinction; the arguments in that section are also motivation for Charniak&#8217;s choice of conditioning on the parent.)</S>
    <S sid="520" ssid="48">Second, Charniak uses word-class information to smooth probabilities and reports a 0.35% improvement from this feature.</S>
    <S sid="521" ssid="49">Finally, Charniak uses 30 million words of text for unsupervised training.</S>
    <S sid="522" ssid="50">A parser is trained from the treebank and used to parse this text; statistics are then collected from this machine-parsed text and merged with the treebank statistics to train a second model.</S>
    <S sid="523" ssid="51">This gives a 0.5% improvement in performance.</S>
    <S sid="524" ssid="52">Charniak&#8217;s dependency parameters are conditioned on less information.</S>
    <S sid="525" ssid="53">As noted previously, whereas our parameters are PL2(lwi  |Li, lti, c, p, P, H, w, t, &#8710;, LC), Charniak&#8217;s parameters in our notation would be PL2(lwi  |Li, P, w).</S>
    <S sid="526" ssid="54">The additional information included in our models is as follows: H The head nonterminal label (VP in the previous profits/rose example).</S>
    <S sid="527" ssid="55">At first glance this might seem redundant: For example, an S will usually take a VP as its head.</S>
    <S sid="528" ssid="56">In some cases, however, the head label can vary: For example, an S can take another S as its head in coordination cases. lti, t The POS tags for the head and modifier words.</S>
    <S sid="529" ssid="57">Inclusion of these tags allows our models to use POS tags as word class information.</S>
    <S sid="530" ssid="58">Charniak&#8217;s model may be missing an important generalization in this respect.</S>
    <S sid="531" ssid="59">Charniak (2000) shows that using the POS tags as word class information in the model is important for parsing accuracy. c The coordination flag.</S>
    <S sid="532" ssid="60">This distinguishes, for example, coordination cases from appositives: Charniak&#8217;s model will have the same parameter&#8212;P(modifier| head, NP, NP)&#8212;in both of these cases. p, &#8710;,LC/RC The punctuation, distance, and subcategorization variables.</S>
    <S sid="533" ssid="61">It is difficult to tell without empirical tests whether these features are important. model are effectively decomposed into our L1 parameters (section 5.1), the head parameters, and&#8212;in models 2 and 3&#8212;the subcategorization and gap parameters.</S>
    <S sid="534" ssid="62">This decomposition allows our model to assign probability to rules not seen in training data: See section 7.4 for an extensive discussion. tures to encode preferences for right-branching structures.</S>
    <S sid="535" ssid="63">Charniak&#8217;s model does not represent this information explicitly but instead learns it implicitly through rule probabilities.</S>
    <S sid="536" ssid="64">For example, for an NP PP PP sequence, the preference for a right-branching structure is encoded through a much higher probability for the rule NP &#8594; NP PP than for the rule NP &#8594; NP PP PP.</S>
    <S sid="537" ssid="65">(Note that conditioning on the rule&#8217;s parent is needed to disallow the structure [NP [NP PP] PP]; see Johnson [1997] for further discussion.)</S>
    <S sid="538" ssid="66">This strategy does not encode all of the information in the distance measure.</S>
    <S sid="539" ssid="67">The distance measure effectively penalizes rules NP &#8594; NPB NP PP where the middle NP contains a verb: In this case the PP modification results in a dependency that crosses a verb.</S>
    <S sid="540" ssid="68">Charniak&#8217;s model is unable to distinguish cases in which the middle NP contains a verb (i.e., the PP modification crosses a verb) from those in which it does not.</S>
    <S sid="541" ssid="69">We now make a detailed comparison of our models to the history-based models of Ratnaparkhi (1997), Jelinek et al. (1994), and Magerman (1995).</S>
    <S sid="542" ssid="70">A strength of these models is undoubtedly the powerful estimation techniques that they use: maximum-entropy modeling (in Ratnaparkhi 1997) or decision trees (in Jelinek et al. 1994 and Magerman 1995).</S>
    <S sid="543" ssid="71">A weakness, we will argue in this section, is the method of associating parameters with transitions taken by bottom-up, shift-reduce-style parsers.</S>
    <S sid="544" ssid="72">We give examples in which this method leads to the parameters&#8217; unnecessarily fragmenting the training data in some cases or ignoring important context in other cases.</S>
    <S sid="545" ssid="73">Similar observations have been made in the context of tagging problems using maximum-entropy models (Lafferty, McCallum, and Pereira 2001; Klein and Manning 2002).</S>
    <S sid="546" ssid="74">We first analyze the model of Magerman (1995) through three common examples of ambiguity: PP attachment, coordination, and appositives.</S>
    <S sid="547" ssid="75">In each case a word sequence S has two competing structures, T1 and T2, with associated decision sequences (d1,.</S>
    <S sid="548" ssid="76">.</S>
    <S sid="549" ssid="77">.</S>
    <S sid="550" ssid="78">, dn) and (e1,.</S>
    <S sid="551" ssid="79">.</S>
    <S sid="552" ssid="80">.</S>
    <S sid="553" ssid="81">, em), respectively.</S>
    <S sid="554" ssid="82">Thus the probability of the two structures can be written as It will be useful to isolate the decision between the two structures to a single probability term.</S>
    <S sid="555" ssid="83">Let the value j be the minimum value of i such that di =&#65533; ei.</S>
    <S sid="556" ssid="84">Then we can rewrite the two probabilities as follows: The first thing to note is that 11i=1...j&#8722;1 P(di|d1 ... di&#8722;1,S) = 11i=1...j&#8722;1 P(ei|e1 ... ei&#8722;1, S), so that these probability terms are irrelevant to the decision between the two structures.</S>
    <S sid="557" ssid="85">We make one additional assumption, that This is justified for the examples in this section, because once the jth decision is made, the following decisions are practically deterministic.</S>
    <S sid="558" ssid="86">Equivalently, we are assuming that P(T1|S) +P(T2|S) &#8776; 1, that is, that very little probability mass is lost to trees other than T1 or T2.</S>
    <S sid="559" ssid="87">Given these two equalities, we have isolated the decision between the two structures to the parameters P(dj|d1 ... dj&#8722;1, S) and P(ej|e1 ... ej&#8722;1, S).</S>
    <S sid="560" ssid="88">Figure 21 shows a case of PP attachment.</S>
    <S sid="561" ssid="89">The first thing to note is that the PP attachment decision is made before the PP is even built.</S>
    <S sid="562" ssid="90">The decision is linked to the NP preceding the preposition: whether the arc above the NP should go left or right.</S>
    <S sid="563" ssid="91">The next thing to note is that at least one important feature, the verb, falls outside of the conditioning context.</S>
    <S sid="564" ssid="92">(The model considers only information up to two constituents preceding or following the location of the decision.)</S>
    <S sid="565" ssid="93">This could be repaired by considering additional context, but there is no fixed bound on how far the verb can be from the decision point.</S>
    <S sid="566" ssid="94">Note also that in other cases the method fragments the data in unnecessary ways.</S>
    <S sid="567" ssid="95">Cases in which the verb directly precedes the NP, or is one place farther to the left, are treated separately.</S>
    <S sid="568" ssid="96">Figure 22 shows a similar example, NP coordination ambiguity.</S>
    <S sid="569" ssid="97">Again, the pivotal decision is made in a somewhat counterintuitive location: at the NP preceding the coordinator.</S>
    <S sid="570" ssid="98">At this point the NP following the coordinator has not been built, and its head noun is not in the contextual window.</S>
    <S sid="571" ssid="99">Figure 23 shows an appositive example in which the head noun of the appositive NP is not in the contextual window when the decision is made.</S>
    <S sid="572" ssid="100">These last two examples can be extended to illustrate another problem.</S>
    <S sid="573" ssid="101">The NP after the conjunct or comma could be the subject of a following clause.</S>
    <S sid="574" ssid="102">For example, (a) and (b) are two candidate structures for the same sequence of words.</S>
    <S sid="575" ssid="103">(c) shows the first decision (labeled &#8220;?&#8221;) in which the two structures differ.</S>
    <S sid="576" ssid="104">The arc above the NP can go either left (for high attachment (a) of the appositive phrase) or right (for noun attachment (b) of the appositive phrase). in John likes Mary and Bill loves Jill, the decision not to coordinate Mary and Bill is made just after the NP Mary is built.</S>
    <S sid="577" ssid="105">At this point, the verb loves is outside the contextual window, and the model has no way of telling that Bill is the subject of the following clause.</S>
    <S sid="578" ssid="106">The model is assigning probability mass to globally implausible structures as a result of points of local ambiguity in the parsing process.</S>
    <S sid="579" ssid="107">Some of these problems can be repaired by changing the derivation order or the conditioning context.</S>
    <S sid="580" ssid="108">Ratnaparkhi (1997) has an additional chunking stage, which means that the head noun does fall within the contextual window for the coordination and appositive cases.</S>
  </SECTION>
  <SECTION title="9." number="10">
    <S sid="581" ssid="1">The models in this article incorporate parameters that track a number of linguistic phenomena: bigram lexical dependencies, subcategorization frames, the propagation of slash categories, and so on.</S>
    <S sid="582" ssid="2">The models are generative models in which parse trees are decomposed into a number of steps in a top-down derivation of the tree and the decisions in the derivation are modeled as conditional probabilities.</S>
    <S sid="583" ssid="3">With a careful choice of derivation and independence assumptions, the resulting model has parameters corresponding to the desired linguistic phenomena.</S>
    <S sid="584" ssid="4">In addition to introducing the three parsing models and evaluating their performance on the Penn Wall Street Journal Treebank, we have aimed in our discussion (in sections 7 and 8) to give more insight into the models: their strengths and weaknesses, the effect of various features on parsing accuracy, and the relationship of the models to other work on statistical parsing.</S>
    <S sid="585" ssid="5">In conclusion, we would like to highlight the following points: subcategorization parameters performs very poorly (76.5% precision, 75% recall), suggesting that the adjacency feature is capturing some subcategorization information in the model 1 parser.</S>
    <S sid="586" ssid="6">The results in Table 7 show that the subcategorization, adjacency, and &#8220;verb-crossing&#8221; features all contribute significantly to model 2&#8217;s (and by implication model 3&#8217;s) performance.</S>
    <S sid="587" ssid="7">&#8226; Section 7.3 described how the three models are well-suited to the Penn Treebank style of annotation, and how certain phenomena (particularly the distance features) may fail to be modeled correctly given treebanks with different annotation styles.</S>
    <S sid="588" ssid="8">This may be an important point to bear in mind when applying the models to other treebanks or other languages.</S>
    <S sid="589" ssid="9">In particular, it may be important to perform transformations on some structures in treebanks with different annotation styles.</S>
    <S sid="590" ssid="10">&#8226; Section 7.4 gave evidence showing the importance of the models&#8217; ability to break down the context-free rules in the treebank, thereby generalizing to produce new rules on test examples.</S>
    <S sid="591" ssid="11">Table 12 shows that precision on section 0 of the treebank decreases from 89.0% to 87.0% and recall decreases from 88.8% to 87.9% when the model is restricted to produce only those context-free rules seen in training data.</S>
    <S sid="592" ssid="12">Jelinek et al. (1994), and Magerman (1995).</S>
    <S sid="593" ssid="13">Although certainly similar to Charniak&#8217;s model, the three models in this article have some significant differences, which are identified in section 8.1.</S>
    <S sid="594" ssid="14">(Another important difference&#8212;the ability of models 1, 2, and 3 to generalize to produce context-free rules not seen in training data&#8212;was described in section 7.4.)</S>
    <S sid="595" ssid="15">Section 8.2 showed that the parsing models of Ratnaparkhi (1997), Jelinek et al. (1994), and Magerman (1995) can suffer from very similar problems to the &#8220;label bias&#8221; or &#8220;observation bias&#8221; problem observed in tagging models, as described in Lafferty, McCallum, and Pereira (2001) and Klein and Manning (2002).</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="11">
    <S sid="596" ssid="1">My Ph.D. thesis is the basis of the work in this article; I would like to thank Mitch Marcus for being an excellent Ph.D. thesis adviser, and for contributing in many ways to this research.</S>
    <S sid="597" ssid="2">I would like to thank the members of my thesis committee&#8212;Aravind Joshi, Mark Liberman, Fernando Pereira, and Mark Steedman&#8212;for the remarkable breadth and depth of their feedback.</S>
    <S sid="598" ssid="3">The work benefited greatly from discussions with Jason Eisner, Dan Melamed, Adwait Ratnaparkhi, and Paola Merlo.</S>
    <S sid="599" ssid="4">Thanks to Dimitrios Samaras for giving feedback on many portions of the work.</S>
    <S sid="600" ssid="5">I had discussions with many other people at IRCS, University of Pennsylvnia, which contributed quite directly to this research: supervision was the beginning of this research.</S>
    <S sid="601" ssid="6">Finally, thanks to the anonymous reviewers for their comments.</S>
  </SECTION>
</PAPER>
