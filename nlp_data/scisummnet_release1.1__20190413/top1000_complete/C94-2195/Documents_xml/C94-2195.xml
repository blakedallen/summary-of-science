<PAPER>
	<S sid="0">A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation</S><ABSTRACT>
		<S sid="1" ssid="1">I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment disam- biguation, and present results colnparing peffo&gt; mange of this algorithm with other corpus-based approaches to this problem.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="2" ssid="2">Prel)ositioual phrase attachment disambiguation is a difficult problem.</S>
			<S sid="3" ssid="3">Take, for example, the sen- rouge: ( l ) Buy a ear \[p,o with a steering wheel\].</S>
			<S sid="4" ssid="4">We would guess that the correct interpretation is that one should buy cars that come with steer- ing wheels, and not that one should use a steering wheel as barter for purchasing a car.</S>
			<S sid="5" ssid="5">\]n this case, we are helped by our world knowledge about auto- mobiles and automobile parts, and about typical methods of barter, which we can draw upon to cor- rectly disambignate he sentence.</S>
			<S sid="6" ssid="6">Beyond possibly needing such rich semantic or conceptual int'ornla- tion, A l tmann and Steedman (AS88) show that there a,re certain cases where a discourse model is needed to correctly disambiguate prepositional phrase atta.chment.</S>
			<S sid="7" ssid="7">However, while there are certainly cases of an&gt; biguity that seem to need some deep knowledge, either linguistic or conceptual, one might ask whag sort of performance could 1oe achieved by a sys- tem thai uses somewhat superficial knowledge au- *Parts of this work done a.t the Computer and hP lbrmation Science Department, University of Penn- sylvania were supported by by DARPA and AFOSR jointly under grant No.</S>
			<S sid="8" ssid="8">AFOSR-90-0066, and by ARO grant No.</S>
			<S sid="9" ssid="9">DAAL 03-89-C0031 PR\[ (first author) and by an IBM gradmtte fellowship (second author).</S>
			<S sid="10" ssid="10">This work was also supported at MIT by ARPA under Con- tract N000t4-89-J-la32= monitored through the Office of Naval resear&lt;:h (lirst a.uthor).</S>
			<S sid="11" ssid="11">tomatically ~xtracted from a large corpus.</S>
			<S sid="12" ssid="12">Recent work has shown thai; this approach olds promise (H\]~,91, HR93).</S>
			<S sid="13" ssid="13">hi this paper we describe a new rule-based ap- proach to prepositional phrase attachment, disam- biguation.</S>
			<S sid="14" ssid="14">A set of silnple rules is learned au- tomatically to try to prediet proper attachment based on any of a number of possible contextual giles.</S>
			<S sid="15" ssid="15">Baseline l l indle and Rooth (IIR91, 1\[17{93) describe corpus-based approach to disambiguating between prepositional phrase attachlnent to the main verb and to the object nonn phrase (such as in the ex- ample sentence above).</S>
			<S sid="16" ssid="16">They first point out that simple attachment s rategies snch as right associa- tion (Kim73) and miuimal a.tbtchment (Fra78) do not work well i,l practice' (see (WFB90)).</S>
			<S sid="17" ssid="17">They then suggest using lexical preference, estimated from a large corpus of text, as a method of re- solving attachment ambiguity, a technique the}' call "lexical association."</S>
			<S sid="18" ssid="18">From a large corpus of pursed text, they first find all nonn phrase heads, and then record the verb (if' any) that precedes the head, and the preposition (if any) that follows it, as well as some other syntactic inforlnation about the sentence.</S>
			<S sid="19" ssid="19">An algorithm is then specified 1,o try to extract attachment information h'om this table of co-occurrences.</S>
			<S sid="20" ssid="20">I!'or instance, a table entry is cousidered a definite instance of the prepositional phrase attaching to the noun if: '\['he noun phrase occm:s in a context where no verb could license the prepositional phrase, specifically if the noun phrase is in a subjeet or other pre-verbal position.</S>
			<S sid="21" ssid="21">They specify seven different procedures for decid- ing whether a table entry is au instance of no attachment, sure noun attach, sm:e verb attach, or all ambiguous attach.</S>
			<S sid="22" ssid="22">Using these procedures, they are able to extract frequency information, 1198 counting t, he numl)e,r of times a ptu:ticular verb or ncmn a.ppe~u:s with a pal:tieuh~r l~reposition.</S>
			<S sid="23" ssid="23">These frequen(;ies erve a.s training d~t;a for the statistical model they use to predict correct i~ttachmenL To dismnbigu;~te s ntence (l), they would compute the likelihood of the preposition with giwm the verb buy, {rod eolltrast that with the likelihood of that preposition given I:he liOttll whed.</S>
			<S sid="24" ssid="24">()he, problem wit;h this ,~pproa~ch is tll~tt it is limited in what rel~tionships are examined to make mi ~d;tachment decision.</S>
			<S sid="25" ssid="25">Simply extending t\[indle and l{,ooth's model to allow R)r relalion- ships such as tlml~ I)e.tweell the verb and the' ob- ject o\[' the preposition would i:esult ill too large a. parameter spa.ce, given ~my realistic quantity of traiuing data.</S>
			<S sid="26" ssid="26">Another prol)lem of the method, shared by ma.ny statistical approaches, is that the.</S>
			<S sid="27" ssid="27">model ~(:quired (Inring training is rel)reser~ted in a huge, t~d)le of probabilities, pl:ecludiug any stra.ightf'orward analysis of its workings.</S>
			<S sid="28" ssid="28">' l~-ansformat ion-Based Er ror -Dr iven Learn ing Tra, nS\]bl'm~d;ion-lmsed errol:-dHven learlting is ~ sin@e learning a.lgorithm tlmt has t)eeu applied to a. number of natural la.ngm,ge prol)ie.ms, includ- Jllg l)a.t't O\[' speech tagging and syuta.cl, ic l)m:sing (1h:i92, \]h:i93a, Bri!)gb, Bri9d).</S>
			<S sid="29" ssid="29">Figure :1 illus- trates the learning l)l:OCC'SS, l:irsL, tlll;21nlola, ted text; is l)assed through the initial-st;ate mmota- tot.</S>
			<S sid="30" ssid="30">'l'lw~ initial-stat, e area)tater can range in com- plexity from quite trivial (e.g. assigning rmtdom strll(:ttll:C) to quit, e sophistica.ted (e.g. assigning the output of a. I{nowledge-based ;/l/llot;~l, tol' that was created by hand).</S>
			<S sid="31" ssid="31">Ouce text has beeu passed through the iuitia.l-state almOl, at.or, it.</S>
			<S sid="32" ssid="32">is then (;ore- pared to the h'ugh,, as indicated ill a luamlally an- nota,teA eorl)llS , and transformations are le~u'ned that can be applied to the oul, put of the iuitial state remora, tot t;o make it, better resemble the :ruffs.</S>
			<S sid="33" ssid="33">So far, ouly ~ greedy search al)proach as been used: at eaeh itera.tion o\[' learning, t.he tra nsfo&gt; nl~tion is found whose application results in the greatest iml)rovenmnt; ha.t transfk)rmation is then added to the ordered trmlsforlmLtiou list and the corpus is upd~d.ed by a.pplying the.</S>
			<S sid="34" ssid="34">learned trans formation.</S>
			<S sid="35" ssid="35">(See, (I{,Mg,\[) for a detailed discussiou of this algorithm in the context of machiue, le, aru-- iug issues.)</S>
			<S sid="36" ssid="36">Ottce 3,11 ordered list; of transform~tions i learned, new text, can be mmotated hy first aI&gt; plying the initial state ~mnotator to it and then applying each o\[' the traaM'ormations, iu order.</S>
			<S sid="37" ssid="37">UNANNOTATI{D \] "I'I~X'I' 1NH'\[AI, l STATE ANNOTATliD TEXT TI~.I j'\['l l , ~ e , N El( ~-~ RUI ,I-S Figure \[: Transfonm~tion-I~ased Error.-Driven l,earlfiUg.</S>
			<S sid="38" ssid="38">r lh:ansformation-B ased Prepos i t iona l Phrase At tachment We will now show how transformation-based e.rrol&gt; driwm IGmfing can be used to resolve prep(~si- tiered phrase at, tachment ambiguity.</S>
			<S sid="39" ssid="39">The l)reposi- tioiml phrase a.tt~Munent |ea.riter learns tra.nsfor-- Ill~ttiollS \[?onl a C,)l:l&gt;tls O\[ 4-tuples of the \['orm (v I11 I\] 1|9), where v is ~1 w;rl), nl is the head of its objecl, llolni \]phrase, i ) is the \])l'epositioll, and 11:2 is the head of the noun phrase, governed by the prel)c, sition (for e,-:anq~le, sce/v :1~' bo:q/,l o,/p the h711/~2).</S>
			<S sid="40" ssid="40">1,'or all sentences that conlbrm to this pattern in the Penn Treeb~mk W{dl St, l:eet 3ourlml corpns (MSM93), such a 4-tuplc was formed, attd each :l-tuple was paired with the at~aehnteut de- cision used in the Treebauk parse) '\['here were 12,766 4q;ul)les in all, which were randomly split into 12,206 trnining s**mples and 500 test samples.</S>
			<S sid="41" ssid="41">\[n this e?periment (as in (\[II~,9\], I\]l{93)), tim at- tachment choice For l)repositional i)hrases was I)e- I,ween the oh.iecl~ mmn and l,he matrix verb.</S>
			<S sid="42" ssid="42">\[n the initial sl,~te mmotator, all prepositional phrases I \])at.terns were extra.clxxl usJ.ng tgrep, a. tree-based grep program written by Rich Pito.</S>
			<S sid="43" ssid="43">'\]'\]te 4-tuples were cxtract;ed autom~tk:ally, a.ud mista.kes were not.</S>
			<S sid="44" ssid="44">m~vn tta.lly pruned out.</S>
			<S sid="45" ssid="45">1199 are attached to the object, noun.</S>
			<S sid="46" ssid="46">2 This is tile at- tachment predicted by right association (Kim73).</S>
			<S sid="47" ssid="47">The allowable transforlnations are described by the following templates: ? Change the attachment location from X to Y if: - n l i sW - n2 is W - v isW -- p is W - n l is W1 and n2 is W2 - n l i sWl andv isW2 Here "from X to Y" can be either "from nl to v" or "from v to nl ," W (W1, W2, etc.) can be any word, and the ellipsis indicates that the complete set of transformations permits matching on any combination of values for v, n l , p, and n2, with the exception of patterns that specify vahms for all four.</S>
			<S sid="48" ssid="48">For example, one allowable transformation would be Change the attachment location from nl to v if p is "until".</S>
			<S sid="49" ssid="49">Learning proceeds as follows.</S>
			<S sid="50" ssid="50">First, the train- ing set is processed according to the start state annotator, in this case attaching all prepositional phrases low (attached to nl) . Then, in essence, each possible transtbrmation is scored by apply- ing it to the corpus and cornputing the reduction (or increase) in error rate.</S>
			<S sid="51" ssid="51">in reality, the search is data driven, and so the vast majority of al- lowable transformations are not examined.</S>
			<S sid="52" ssid="52">The best-scoring transformation then becomes the first transformation i the learned list.</S>
			<S sid="53" ssid="53">It is applied to the training corpus, and learning continues on the modified corpus.</S>
			<S sid="54" ssid="54">This process is iterated until no rule can he found that reduces the error rate.</S>
			<S sid="55" ssid="55">In the experiment, a tol, al of 471 transfor- mations were learned - - Figure 3 shows the first twenty.</S>
			<S sid="56" ssid="56">3 Initial accuracy on the test set is 64.0% when prepositional phrases are always attached to the object noun.</S>
			<S sid="57" ssid="57">After applying the transforma- tions, accuracy increases to 80.8%.</S>
			<S sid="58" ssid="58">Figure 2 shows a plot of test-set accuracy as a function of the nulnber of training instances.</S>
			<S sid="59" ssid="59">It is interesting to note that the accuracy curve has not yet, reached a 2If it is the case that attaching to the verb would be a better start state in some corpora, this decision could be parameterized.</S>
			<S sid="60" ssid="60">ZIn transformation #8, word token amount appears because it was used as the head noun for noun phrases representing percentage amounts, e.g. "5%."</S>
			<S sid="61" ssid="61">The rule captures the very regular appearance in the Penn Tree- bank Wall Street Journal corpus of parses like Sales for the yea," \[v'P rose \[Np5Yo\]\[pP in fiscal 1988\]\].</S>
			<S sid="62" ssid="62">Accuracy 81.00 rl 80.00 !!</S>
			<S sid="63" ssid="63">79,00 t 77.00 !--R . . .</S>
			<S sid="64" ssid="64">/ - -F . . .</S>
			<S sid="65" ssid="65">%oo!1 / I 74:001 . . .</S>
			<S sid="66" ssid="66">_ _ t ....</S>
			<S sid="67" ssid="67">_ _ 73.00 j - 72.00 l l i _ __ / __ . ,?!&gt;2 - 70.00 69.00 68.00 67.00 64.00 0.00 5.00 I q ! i t T!aining Size x 103 10.00 Figure 2: Accuracy as a function of l;raining corpus size (no word class information).</S>
			<S sid="68" ssid="68">plateau, suggesting that more training data wonld lead to further improvements.</S>
			<S sid="69" ssid="69">Adding Word Class In format ion In the above experiment, all trans\[brmations are.</S>
			<S sid="70" ssid="70">triggered hy words or groups of words, and it is surprising that good performance is achieved even in spite of the inevitable sparse data problems.</S>
			<S sid="71" ssid="71">There are a number of ways to address the sparse data problem.</S>
			<S sid="72" ssid="72">One of the obvious ways, mapping words to part of speech, seerns unlikely to help.</S>
			<S sid="73" ssid="73">h&gt; stead, semanl, ic class information is an attracLive alternative.</S>
			<S sid="74" ssid="74">We incorporated the idea of using semantic ino tbrmation in the lbllowing way.</S>
			<S sid="75" ssid="75">Using the Word~ Net noun hierarchy (Milg0), each noun in the ffa{ning and test corpus was associated with a set containing the noun itself ph.ts the name of every semantic lass that noun appears in (if any).</S>
			<S sid="76" ssid="76">4 The transformation template is modified so that in ad- dition to asking if a nmm matches ome word W, 4Class names corresponded to unique "synonynl set" identifiers within the WordNet noun database.</S>
			<S sid="77" ssid="77">A noun "appears in" a class if it falls within the hy- ponym (IS-A) tree below that class.</S>
			<S sid="78" ssid="78">In the experiments reported here we used WordNet version :l.2.</S>
			<S sid="79" ssid="79">1200 1 2 4 5 (3 7 8 9 10 II 12 :13 \]4 15 \[6 17 \[8 119 2()_ Change Att{:~ehment Location l"r~m~ To ( ;oMit ion N1 V P is at N\ ] \ / P is as N1 V I ) is iulo N:I \/ P is ,l}'om N:I V P is with N\] V N2 is year N 1 V P is by I?</S>
			<S sid="80" ssid="80">is i~ and N I V NI ix amounl N \[ \/ \]' is lhrough NI V \]) is d'urb~g NI V V ix p,ul N1 V N2 is mou.lk N\[ V 1' is ulldcr NJ V 1 ) is after V is have and N1 V I' is b~ N:\[ V P is wilk.oul V NI P is of V is buy and N1 \/ P is for N:I V P is beJbl"( V is have and NI V P is o~ x/ L( V v ~ V v / V V ,/ Figure 3: The \[irst 20 transforntat;ions learned tbr preposil;ional phrase ~ttachme, n|;.</S>
			<S sid="81" ssid="81">it: (~an a/so ask if" it is a~ member of some class C. s This al)proaeh I;o data.</S>
			<S sid="82" ssid="82">sparseness i similar to tllat of (l{,es93b, li, l\[93), where {~ method ix proposed for using WordNet in conjunction with a corpus to ohtain class-based statisl, ie,q.</S>
			<S sid="83" ssid="83">()lit' method here is ltlllC\]l simpler, however, in I;hat we a.re only us- ing Boolean values to indieal;e whel;her ~ word can be a member of' a class, rather than esl, imat ing ~ filll se{, of joint probabil it ies involving (:lasses.</S>
			<S sid="84" ssid="84">Since the tr;ulsformation-based al)l/roach with classes cCm gener~dize ill a way that the approach without classes is ml~l)le to, we woldd expect f'cwer l;ransf'ormal;ions to be necessary, l!;xperimeaH, ally, this is indeed the case.</S>
			<S sid="85" ssid="85">In a second experiment;, l;raining a.ml testing were era:tied out on the same samples as i , the previous experiment, bul; I;his t ime using the ext, ende, d tra ns lbrmat ion t(;ml)la.tes for word classes.</S>
			<S sid="86" ssid="86">A total of 266 transformations were learned.</S>
			<S sid="87" ssid="87">Applying l.hese transt'ormai.ions to the test set l'eslllted in a.n accuracy of' 81.8%.</S>
			<S sid="88" ssid="88">\[n figure 4 we show tile lirst 20 tra.nsform{~l, ions lem'ned using ilOllll classes.</S>
			<S sid="89" ssid="89">Class descriptions arc surrounded by square bracl{ets.</S>
			<S sid="90" ssid="90">(; 'Phe first; grans- Ibrmation st~l.cs thai.</S>
			<S sid="91" ssid="91">if" N2 is a. nomt I, hal; describes time (i.e. ix a. member of WordNet class that in- cludes tim nouns "y(;ar," "month," "week," and others), thell the preltositiomd phrase should be al;tache(\[ t,() the w;rb, since, tim(; is \]nlMl more l ikely Io modify a yet'It (e.g. le,vc lh(: re(cling iu an hour) thaJl a, lloun.</S>
			<S sid="92" ssid="92">This exlw, r iment also demonstrates how rely \[~?~l;ul:e-based lexicon or word classiflcat, ion scheme cau triviaJly be incorlJorated into the learner, by exLencling l;ransfot'nlal,iolls to allow thent to make l'efel'eAlc(?</S>
			<S sid="93" ssid="93">|;o it WOl:(\[ g i l t \ [ { l i ly O\[' its features.</S>
			<S sid="94" ssid="94">\],valuation against Other Algorithms In (lIl~91, HR93), tra.inittg is done on a superset el' sentence types ttsed ill train- ing the transforlJ~atiolFbased learner.</S>
			<S sid="95" ssid="95">The transformation-based learner is I, rained on sen- tences containing v, n\[ and p, whereas the algo- r i thm describe.d by l l indle and I~,ooth ca.n zdso use sentences (;ontailfing only v and p, (n' only nl and i1.</S>
			<S sid="96" ssid="96">\[11 their lmper, they tra.in on ow~r 200,000 sen- Lettces with prel)ositions f'rotn the Associated Press (APt newswire, trod I;hey quote a.n accuracy of 78- 80% on AP test &amp;~ta..</S>
			<S sid="97" ssid="97">~' For reasons of ~: u n- t ime c\[lk:icn(:y, transfonmLl, ions tmddng re\['crence 1:o tile classes of both n l a,nd n2 were IlOI; p(~l?lXiitl, tR(I. GI;or expository purposes, the u.iqm'.</S>
			<S sid="98" ssid="98">WordNet id('.ntilicrs luwe been replaced by words Lh~LL describe the cont, cnt of the class.</S>
			<S sid="99" ssid="99">1207 (~lml~.ge \] At tachment , / Location / # li'rom t 'Fo \[ Condition 1 N1 V N2 is \[time\]</S>
	</SECTION>
	<SECTION title="N1 V P is al " number="2">
			<S sid="100" ssid="1">4 N1 V P is into 5 N 1 V P is from 6 N1 V 1 ) is wilh 7 N1 V P is of P is in and NI is 8 N 1 V \[measure, quanlily, amou~l\] P is by all.el 9 N1 V N2 is \[abslraclion\] I 0 NI V P is lhro'ugh 1) is in and N I is 11 NI V \[group,group.in.g\].</S>
			<S sid="101" ssid="2">12 V N 1 V is be 13 NI V V is pul.</S>
			<S sid="102" ssid="3">14 NI V P is under.</S>
			<S sid="103" ssid="4">P is i~ and N\] is 15 N1 V \[written co.mmlu~ication\] 16 N1 V l ) is wilhoul 17 N1 V P is during 18 N 1 V 19 NI V. 20 N1.</S>
			<S sid="104" ssid="5">V l ) is on and Nt is \[U~.ing\] P is after V is buy and P is for Figure 4: The first 20 transformations learned for prepositional phrase attachment, using noun classes.</S>
			<S sid="105" ssid="6">of Method Accuracy Transforms t-Scores 70.4- 75.8 '\]?anstbrma~ions 80.8 471 Trans\['ormati ons (no N2) 79.2 418 Transformations (classes) 8:1.8 26(5 Figure 5: Comparing l{esults in PP Attachment.</S>
			<S sid="106" ssid="7">In order to compare the two approaches, we reimplemen:ed the ~flgorithm fi'om (IIR.91) and tested it using the same training and test set used for the above experiments.</S>
			<S sid="107" ssid="8">Doing so re- sull;ed in an attachment accuracy of 70.4%.</S>
			<S sid="108" ssid="9">Next, the training set was expanded to include not only the cases o\[' ambiguous attachment \]Fonnd in the parsed Wall Street Journal corpus, as before, but also all the unambiguous prepositional phrase at- tachments tbnnd in the corpus, as well (contiml- ing to exclnde the tesl, set, of course).</S>
			<S sid="109" ssid="10">Accuracy improved to 75.8% r using the larger training set, still significantly lower than accuracy obtained us-- lag tam tl:ansformal;ion-based approach.</S>
			<S sid="110" ssid="11">The t.ech- nique described in (Res93b, 1{1t93), which com- bined Hindle and Rooth's lexical association tech- nique with a WordNet-based conceptual associa- tion measure, resulted in an accuracy of 76.0%, also lower than the results obtained using trans- formations.</S>
			<S sid="111" ssid="12">Since llindle and Rooth's approach does not make reference to n2, we re-ran the transformation-learner disalk)wing all transforma- tions that make reference ~o n2.</S>
			<S sid="112" ssid="13">Doing so resulted in an accuracy of 79.2%.</S>
			<S sid="113" ssid="14">See figure 5 h)r a sun&gt; mary of results.</S>
			<S sid="114" ssid="15">It is possihle Lo compare; the results described here with a somewhat similar approach devel-.</S>
			<S sid="115" ssid="16">oped independently by Ratnaparkhi and I/,oukos (l{R94), since they also used training and test datt~ drawn from the Penn Treebank's Wall Street Jour- nal corpus.</S>
			<S sid="116" ssid="17">Instead of' using mammlly coustructed lexical classes, they nse word classes arrived at via mutmd information clustering in a training corpus (BDd+92), resulting in a representation i which each word is represented by a sequence of bits.</S>
			<S sid="117" ssid="18">As in the experiments here, their statistical model also makes use of a 4-tuple context (v, c&lt;l, p, n2), and can use the identit.ies of the words, class inl'or- marion (tbr them, wdues of any of the class bits), rThe difference between these results ~nd tile result they quoted is likely due to a much bLrger training set used in their origimd experiments.</S>
			<S sid="118" ssid="19">1202 or both Mnds of i ld'ormation as eotll;extual fea- tlll?eS riley {lescril)e a search process use(\[ to {letePn6\]m what, sul)set of the available ill\['or,~Ht- l ion will Im used in the model.</S>
			<S sid="119" ssid="20">(\]iv{;\]\] a eh{}ice of features, they train ;t prol}abi/islie model For I)r(Sitclcoutext), and in {.esl.ing choose Site :-: v oP Site = n l a~ccordi\]lg I;o which has {he higher eomlitional probal)i\]ity.</S>
			<S sid="120" ssid="21">t~,atnal)~Pkhi and Roukos rel}ort an aecuraey oi' 81.6% using bot, h word and class iui'orma, tion on Wall SI;re.et 3ourna\] text,, using a t:raining COl:- pus twice as la, rgc as that used in ouP experiments.</S>
			<S sid="121" ssid="22">They also report that a (leeision tree mode/ eon- st\];u(:t~d using the same features m,d I,i;aining data ac\[lieve{I I)erformanee of 77.71~, (}n t\[:e same I.est set, A llUll ll)el' o\[' other reseaPehers have exl)lored eorlms-I)ased approaches I;o l)repositional phrase attaehmet, t disaml)iguation tM~t n\]~d{c use of word classes, l"or example, Weisehed{q cl al.</S>
			<S sid="122" ssid="23">(WAIH91) and Basili el al.</S>
			<S sid="123" ssid="24">(BI}V91) bol,\]l deseril)e the use of lnanual ly coustrueted, donmhv Sl){~eitic word classes together with cori}us-tmsed si,t~tisties in o f d{2r to resolve i)rel)ositional 1)hrase a.t, taehlllellt &amp;Ill-.</S>
			<S sid="124" ssid="25">I}iguity.</S>
			<S sid="125" ssid="26">I{e(;a.llSe these papers deseril)e results ol)- tained on different corpora, however, it is (lifIicull; to II~,:'tl,:.{; a. 1)(;r\['(}rllla, iic{!</S>
			<S sid="126" ssid="27">COl\[lD~/l:iSOll, Conc lus ions The.</S>
			<S sid="127" ssid="28">tPansl 'ormation-hased approach to resolving preposit ional phl:ase disanlbiguat ion has a mlmt)er of advaiH;ages over (}l,\]ler ;i.l)l)roatehes.</S>
			<S sid="128" ssid="29">\[11 a (\]irect eoml);u:ison with lexical association, higher ble(;ll- vaey is achieved using words alolm (wen though at tachment inf\}rnlation is captured i*l a relatively small numl)er of simple, rea(lable rules, as opl)osed to a. large lllllll\])eF Of lexical co-oeetlrreltee l)l'o\])a -- I)ilities.</S>
			<S sid="129" ssid="30">\]u addit ion, we have shown how the l;raus\['orln~Lion-based l arner can casity be e?.- tended to incorporate word-class i/fformatiou.</S>
			<S sid="130" ssid="31">This resulted in a slight; increase in 1)erformanee, but, more notal)\]y it resulted in a reduct;ion hy roughly half in the l;ota\[ mnnl)er of transfor- mat ion rules needed.</S>
			<S sid="131" ssid="32">And in (:outrast to ap- pro~ches using class--based prol)abil istic models (BPV91, Res93e, WAI~ F91) or classes derived vi;~ statist ical clusl.ering methods (1~.R94), t:his {ech- l l ique pro(hlees a, I:HIO set that (:al}l;ltr{es eolteepl:~lal geueralizal;ions couciseIy a.ml ill \]mman-rea{Ial}\]e for I n. F/\]rthel:lllOl:e, iuso\['ar as (:oHq)a, risolls e&amp;ll I)o ina(h- all lOl lg separa, Le exl'}el:llllel/l;s l lsi lt~ Wai l Street Jour\]ml training aml test data (( l lRgl) , reiml)l('meute(l as reI)oPted above; (l{es93e, 1t1193); (IH1.94)), the rule-based approach de..</S>
			<S sid="132" ssid="33">scribed here achieves better perl'orlttaucc, using ml algol:ithm tlmt is eoncel}tually quite Mml)le am/ iu l)l'~l.(;tiea\] teFlttS extretuely easy to i lnplenlel~t, s A more genera\] point ix tha.t the transl'orm~d,ion-based ;~l)l}roateh is eas- ily a(lapl,ed t;o s i tuat ions in which some learning 1"rein a (:orpus is desiral)le, 1}ui, hand-construetc{I l}l:ior knowledge is also available.</S>
			<S sid="133" ssid="34">Exist ing knowl- e{lge, such as structural strategies or even a priori h;xieal l}references, (;all 1)e incorl)orated into I;he start state annotator , so theft the learning ~dgo.</S>
			<S sid="134" ssid="35">I:ithm begins with n,ore refiued input.</S>
			<S sid="135" ssid="36">And knowu exceptious {:au 1)e handh'(l t ransparent ly simply hy adding add\]: \[onal rules to tim set thai; is learned, IlSillg tile sall le representat io\] l . A disadwmtage of the al)l)roach is that it re- quires supervised t ra in ing that is, a representa- tive set of "true" c~ses t'FOlll which Co learn.</S>
			<S sid="136" ssid="37">Ilow- ever, this l)eeomes less of a probh'.m as atmotated eorl}ora beeolne increasingly available, and sug- gests the comhinat ion o1:' supexvised and uusuper vised methods as a.u ilfl;eresth G ave\]me \['or \['urther rese;ire\] \[.</S>
	</SECTION>
</PAPER>
