Large Language Models in Machine Translation
This paper reports on the benefits of large-scale statistical language modeling in machine translation.
A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams.
It is capable of providing smoothed probabilities for fast, single-pass decoding.
We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.
5-gram word language models in English are trained on a variety of monolingual corpora.
In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of k grams tends to be so large that we sometimes need cutoffs even in a distributed environment.
To scale LMs to larger corpora with higher-order dependencies we consider distributed language models that scale more readily.
Stupid back off smoothing is significantly more efficient to train and deploy in a distributed framework than a context dependent smoothing scheme such as Kneser-Ney.
We show that each doubling of the training data from the news domain (used to build the language model) leads to improvements of approximately 0.5 BLEU points.
We used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data.
