<PAPER>
  <S sid="0">Language Independent NER Using A Maximum Entropy Tagger</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Entity Recognition systems need to integrate a wide variety of information for optimal performance.</S>
    <S sid="2" ssid="2">This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy.</S>
    <S sid="3" ssid="3">The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type.</S>
    <S sid="5" ssid="2">Thus methods used for part of speech (POS) tagging and chunking can also be used for NER.</S>
    <S sid="6" ssid="3">The papers from the CoNLL-2002 shared task which used such methods (e.g.</S>
    <S sid="7" ssid="4">Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002).</S>
    <S sid="8" ssid="5">However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger.</S>
    <S sid="9" ssid="6">Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method.</S>
    <S sid="10" ssid="7">We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger.</S>
    <S sid="11" ssid="8">We report reasonable precision and recall (84.9 F-score) for the CoNLL-2003 English test data, and an F-score of 68.4 for the CoNLL-2003 German test data.</S>
    <S sid="12" ssid="9">Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers.</S>
    <S sid="13" ssid="10">In contrast, a ME tagger can easily deal with diverse, overlapping features.</S>
    <S sid="14" ssid="11">We also use a Gaussian prior on the parameters for effective smoothing over the large feature space.</S>
  </SECTION>
  <SECTION title="2 The ME Tagger" number="2">
    <S sid="15" ssid="1">The ME tagger is based on Ratnaparkhi (1996)&#8217;s POS tagger and is described in Curran and Clark (2003) .</S>
    <S sid="16" ssid="2">The tagger uses models of the form: where y is the tag, x is the context and the fi(x, y) are the features with associated weights &#955;i.</S>
    <S sid="17" ssid="3">The probability of a tag sequence y1 ... yn given a sentence w1 ... wn is approximated as follows: where xi is the context for word wi.</S>
    <S sid="18" ssid="4">The tagger uses beam search to find the most probable sequence given the sentence.</S>
    <S sid="19" ssid="5">The features are binary valued functions which pair a tag with various elements of the context; for example: &#65533; Generalised Iterative Scaling (GIS) is used to estimate the values of the weights.</S>
    <S sid="20" ssid="6">The tagger uses a Gaussian prior over the weights (Chen et al., 1999) which allows a large number of rare, but informative, features to be used without overfitting.</S>
  </SECTION>
  <SECTION title="3 The Data" number="3">
    <S sid="21" ssid="1">We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for the CoNLL2002 shared task (Tjong Kim Sang, 2002).</S>
    <S sid="22" ssid="2">Each word in the data sets is annotated with a named entity tag plus POS tag, and the words in the German and English data also have a chunk tag.</S>
    <S sid="23" ssid="3">Our system does not currently exploit the chunk tags.</S>
    <S sid="24" ssid="4">There are 4 types of entities to be recognised: persons, locations, organisations, and miscellaneous entities not belonging to the other three classes.</S>
    <S sid="25" ssid="5">The 2002 data uses the IOB-2 format in which a B-XXX tag indicates the first word of an entity of type XXX and I-XXX is used for subsequent words in an entity of type XXX.</S>
    <S sid="26" ssid="6">The tag O indicates words outside of a named entity.</S>
    <S sid="27" ssid="7">The 2003 data uses a variant of IOB-2, IOB-1, in which I-XXX is used for all words in an entity, including the first word, unless the first word separates contiguous entities of the same type, in which case B-XXX is used.</S>
  </SECTION>
  <SECTION title="4 The Feature Set" number="4">
    <S sid="28" ssid="1">Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger.</S>
    <S sid="29" ssid="2">The first set of features apply to rare words, i.e. those which appear less than 5 times in the training data.</S>
    <S sid="30" ssid="3">The first two kinds of features encode prefixes and suffixes less than length 5, and the remaining rare word features encode other morphological characteristics.</S>
    <S sid="31" ssid="4">These features are important for tagging unknown and rare words.</S>
    <S sid="32" ssid="5">The remaining features are the word, POS tag, and NE tag history features, using a window size of 2.</S>
    <S sid="33" ssid="6">Note that the NEi&#8722;2NEi&#8722;1 feature is a composite feature of both the previous and previous-previous NE tags.</S>
    <S sid="34" ssid="7">Table 2 lists the extra features used in our final system.</S>
    <S sid="35" ssid="8">These features have been shown to be useful in other NER systems.</S>
    <S sid="36" ssid="9">The additional orthographic features have proved useful in other systems, for example Carreras et al. (2002), Borthwick (1999) and Zhou and Su (2002).</S>
    <S sid="37" ssid="10">Some of the rows in Table 2 describe sets of contextual predicates.</S>
    <S sid="38" ssid="11">The wi is only digits predicates apply to words consisting of all digits.</S>
    <S sid="39" ssid="12">They encode the length of the digit string with separate predicates for lengths 1&#8211;4 and a single predicate for lengths greater than 4.</S>
    <S sid="40" ssid="13">Titlecase applies to words with an initial uppercase letter followed by all lowercase (e.g.</S>
    <S sid="41" ssid="14">Mr).</S>
    <S sid="42" ssid="15">Mixedcase applies to words with mixed lower- and uppercase (e.g.</S>
    <S sid="43" ssid="16">CityBank).</S>
    <S sid="44" ssid="17">The length predicates encode the number of characters in the word from 1 to 15, with a single predicate for lengths greater than 15.</S>
    <S sid="45" ssid="18">The next set of contextual predicates encode extra information about NE tags in the current context.</S>
    <S sid="46" ssid="19">The memory NE tag predicate (see e.g.</S>
    <S sid="47" ssid="20">Malouf (2002)) records the NE tag that was most recently assigned to the current word.</S>
    <S sid="48" ssid="21">The use of beam-search tagging means that tags can only be recorded from previous sentences.</S>
    <S sid="49" ssid="22">This memory is cleared at the beginning of each document.</S>
    <S sid="50" ssid="23">The unigram predicates (see e.g.</S>
    <S sid="51" ssid="24">Tsukamoto et al. (2002)) encode the most probable tag for the next words in the window.</S>
    <S sid="52" ssid="25">The unigram probabilities are relative frequencies obtained from the training data.</S>
    <S sid="53" ssid="26">This feature enables us to know something about the likely NE tag of the next word before reaching it.</S>
    <S sid="54" ssid="27">Most systems use gazetteers to encode information about personal and organisation names, locations and trigger words.</S>
    <S sid="55" ssid="28">There is considerable variation in the size of the gazetteers used.</S>
    <S sid="56" ssid="29">Some studies found that gazetteers did not improve performance (e.g.</S>
    <S sid="57" ssid="30">Malouf (2002)) whilst others gained significant improvement using gazetteers and triggers (e.g.</S>
    <S sid="58" ssid="31">Carreras et al. (2002)).</S>
    <S sid="59" ssid="32">Our system incorporates only English and Dutch first name and last name gazetteers as shown in Table 6.</S>
    <S sid="60" ssid="33">These gazetteers are used for predicates applied to the current, previous and next word in the window.</S>
    <S sid="61" ssid="34">Collins (2002) includes a number of interesting contextual predicates for NER.</S>
    <S sid="62" ssid="35">One feature we have adapted encodes whether the current word is more frequently seen lowercase than uppercase in a large external corpus.</S>
    <S sid="63" ssid="36">This feature is useful for disambiguating beginning of sentence capitalisation and tagging sentences which are all capitalised.</S>
    <S sid="64" ssid="37">The frequency counts have been obtained from 1 billion words of English newspaper text collected by Curran and Osborne (2002).</S>
    <S sid="65" ssid="38">Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes.</S>
    <S sid="66" ssid="39">This involves mapping characters to classes and merging adjacent characters of the same type.</S>
    <S sid="67" ssid="40">For example, Moody becomes Aa, A.B.C. becomes A.A.A. and 1,345.05 becomes 0,0.0.</S>
    <S sid="68" ssid="41">The classes are used to define unigram, bigram and trigram contextual predicates over the window.</S>
    <S sid="69" ssid="42">We have also defined additional composite features which are a combination of atomic features; for example, a feature which is active for mid-sentence titlecase words seen more frequently as lowercase than uppercase in a large external corpus.</S>
  </SECTION>
  <SECTION title="5 Results" number="5">
    <S sid="70" ssid="1">The baseline development results for English using the supertagger features only are given in Table 3.</S>
    <S sid="71" ssid="2">The full system results for the English development data are given in Table 7.</S>
    <S sid="72" ssid="3">Clearly the additional features have a significant impact on both precision and recall scores across all entities.</S>
    <S sid="73" ssid="4">We have found that the word type features are particularly useful, as is the memory feature.</S>
    <S sid="74" ssid="5">The performance of the final system drops by 1.97% if these features are removed.</S>
    <S sid="75" ssid="6">The performance of the system if the gazetteer features are removed is given in Table 4.</S>
    <S sid="76" ssid="7">The sizes of our gazetteers are given in Table 6.</S>
    <S sid="77" ssid="8">We have experimented with removing the other contextual predicates but each time performance was reduced, except for the next-next unigram tag feature which was switched off for all final experiments.</S>
    <S sid="78" ssid="9">The results for the Dutch test data are given in Table 5.</S>
    <S sid="79" ssid="10">These improve upon the scores of the best performing system at CoNLL-2002 (Carreras et al., 2002).</S>
    <S sid="80" ssid="11">The final results for the English test data are given in Table 7.</S>
    <S sid="81" ssid="12">These are significantly lower than the results for the development data.</S>
    <S sid="82" ssid="13">The results for the German development and test sets are given in Table 7.</S>
    <S sid="83" ssid="14">For the German NER we removed the lowercase more frequent than uppercase feature.</S>
    <S sid="84" ssid="15">Apart from this change, the system was identical.</S>
    <S sid="85" ssid="16">We did not add any extra gazetteer information for German.</S>
  </SECTION>
  <SECTION title="6 Conclusion" number="6">
    <S sid="86" ssid="1">Our NER system demonstrates that using a large variety of features produces good performance.</S>
    <S sid="87" ssid="2">These features can be defined and extracted in a language independent manner, as our results for German, Dutch and English show.</S>
    <S sid="88" ssid="3">Maximum entropy models are an effective way of incorporating diverse and overlapping features.</S>
    <S sid="89" ssid="4">Our maximum entropy tagger employs Gaussian smoothing which allows a large number of sparse, but informative, features to be used without overfitting.</S>
    <S sid="90" ssid="5">Using a wider context window than 2 words may improve performance; a reranking phase using global features may also improve performance (Collins, 2002).</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="7">
    <S sid="91" ssid="1">We would like to thank Jochen Leidner for help collecting the Gazetteers.</S>
    <S sid="92" ssid="2">This research was supported by a Commonwealth scholarship and a Sydney University Travelling scholarship to the first author, and EPSRC grant GR/M96889.</S>
  </SECTION>
</PAPER>
