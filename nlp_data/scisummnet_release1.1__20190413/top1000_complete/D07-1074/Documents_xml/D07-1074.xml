<PAPER>
  <S sid="0" ssid="0">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp.</S>
  <S sid="1" ssid="1">708?716, Prague, June 2007. c?2007 Association for Computational Linguistics Large-Scale Named Entity Disambiguation Based on Wikipedia Data  Silviu Cucerzan Microsoft Research One Microsoft Way, Redmond, WA 98052, USA silviu@microsoft.com  Abstract This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results.</S>
  <S sid="2" ssid="2">It describes in detail the disam- biguation paradigm employed and the information extraction process from Wikipedia.</S>
  <S sid="3" ssid="3">Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented sys- tem shows high disambiguation accuracy on both news stories and Wikipedia articles.</S>
  <S sid="4" ssid="4">1 Introduction and Related Work The ability to identify the named entities (such as people and locations) has been established as an important task in several areas, including topic de- tection and tracking, machine translation, and in- formation retrieval.</S>
  <S sid="5" ssid="5">Its goal is the identification of mentions of entities in text (also referred to as sur- face forms henceforth), and their labeling with one of several entity type labels.</S>
  <S sid="6" ssid="6">Note that an entity (such as George W. Bush, the current president of the U.S.) can be referred to by multiple surface forms (e.g., ?George Bush?</S>
  <S sid="7" ssid="7">and a sur- face form (e.g., ?Bush?)</S>
  <S sid="8" ssid="8">can refer to multiple enti- ties (e.g., two U.S. presidents, the football player Reggie Bush, and the rock band called Bush).</S>
  <S sid="9" ssid="9">When it was introduced, in the 6th Message Un- derstanding Conference (Grishman and Sundheim, 1996), the named entity recognition task comprised three entity identification and labeling subtasks: ENAMEX (proper names and acronyms designat- ing persons, locations, and organizations), TIMEX (absolute temporal terms) and NUMEX (numeric expressions, monetary expressions, and percent- ages).</S>
  <S sid="10" ssid="10">Since 1995, other similar named entity rec- ognition tasks have been defined, among which CoNLL (e.g., Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004).</S>
  <S sid="11" ssid="11">In addi- tion to structural disambiguation (e.g., does ?the Alliance for Democracy in Mali?</S>
  <S sid="12" ssid="12">mention one, two, or three entities?)</S>
  <S sid="13" ssid="13">and entity labeling (e.g., does ?Washington went ahead?</S>
  <S sid="14" ssid="14">mention a person, a place, or an organization?</S>
  <S sid="15" ssid="15">), MUC and ACE also included a within document coreference task, of grouping all the mentions of an entity in a docu- ment together (Hirschman and Chinchor, 1997).</S>
  <S sid="16" ssid="16">When breaking the document boundary and scal- ing entity tracking to a large document collection or the Web, resolving semantic ambiguity becomes of central importance, as many surface forms turn out to be ambiguous.</S>
  <S sid="17" ssid="17">For example, the surface form ?Texas?</S>
  <S sid="18" ssid="18">is used to refer to more than twenty different named entities in Wikipedia.</S>
  <S sid="19" ssid="19">In the con- text ?former Texas quarterback James Street?, Texas refers to the University of Texas at Austin; in the context ?in 2000, Texas released a greatest hits album?, Texas refers to the British pop band; in the context ?Texas borders Oklahoma on the north?, it refers to the U.S. state; while in the con- text ?the characters in Texas include both real and fictional explorers?, the same surface form refers to the novel written by James A. Michener.</S>
  <S sid="20" ssid="20">Bagga and Baldwin (1998) tackled the problem of cross-document coreference by comparing, for any pair of entities in two documents, the word vectors built from all the sentences containing mentions of the targeted entities.</S>
  <S sid="21" ssid="21">Ravin and Kazi (1999) further refined the method of solving co- reference through measuring context similarity and integrated it into Nominator (Wacholder et al., 1997), which was one of the first successful sys- tems for named entity recognition and co-reference resolution.</S>
  <S sid="22" ssid="22">However, both studies targeted the clus- tering of all mentions of an entity across a given document collection rather than the mapping of these mentions to a given reference list of entities.</S>
  <S sid="23" ssid="23">A body of work that did employ reference entity lists targeted the resolution of geographic names in 708 text.</S>
  <S sid="24" ssid="24">Woodruff and Plaunt (1994) used a list of 80k geographic entities and achieved a disambiguation precision of 75%.</S>
  <S sid="25" ssid="25">Kanada (1999) employed a list of 96k entities and reported 96% precision for geo- graphic name disambiguation in Japanese text.</S>
  <S sid="26" ssid="26">Smith and Crane (2002) used the Cruchley?s and the Getty thesauri, in conjunction with heuristics inspired from the Nominator work, and obtained between 74% and 93% precision at recall levels of 89-99% on five different history text corpora.</S>
  <S sid="27" ssid="27">Overell and R?ger (2006) also employed the Getty thesaurus as reference and used Wikipedia to develop a co-occurrence model and to test their system.</S>
  <S sid="28" ssid="28">In many respects, the problem of resolving am- biguous surface forms based on a reference list of entities is similar to the lexical sample task in word sense disambiguation (WSD).</S>
  <S sid="29" ssid="29">This task, which has supported large-scale evaluations ?</S>
  <S sid="30" ssid="30">SENSEVAL 1-3 (Kilgarriff and Rosenzweig, 2000; Edmonds and Cotton, 2001; Mihalcea et al., 2004) ?</S>
  <S sid="31" ssid="31">aims to as- sign dictionary meanings to all the instances of a predetermined set of polysemous words in a corpus (for example, choose whether the word ?church?</S>
  <S sid="32" ssid="32">refers to a building or an institution in a given con- text).</S>
  <S sid="33" ssid="33">However, these evaluations did not include proper noun disambiguation and omitted named entity meanings from the targeted semantic labels and the development and test contexts (e.g., ?Church and Gale showed that the frequency [..]?).</S>
  <S sid="34" ssid="34">The problem of resolving ambiguous names also arises naturally in Web search.</S>
  <S sid="35" ssid="35">For queries such as ?Jim Clark?</S>
  <S sid="36" ssid="36">or ?Michael Jordan?, search engines return blended sets of results referring to many different people.</S>
  <S sid="37" ssid="37">Mann and Yarowsky (2003) ad- dressed the task of clustering the Web search re- sults for a set of ambiguous personal names by employing a rich feature space of biographic facts obtained via bootstrapped extraction patterns.</S>
  <S sid="38" ssid="38">They reported 88% precision and 73% recall in a three-way classification (most common, secondary, and other uses).</S>
  <S sid="39" ssid="39">Raghavan et al.</S>
  <S sid="40" ssid="40">(2004) explored the use of entity language models for tasks such as clustering enti- ties by profession and classifying politicians as liberal or conservative.</S>
  <S sid="41" ssid="41">To build the models, they recognized the named entities in the TREC-8 cor- pus and computed the probability distributions over words occurring within a certain distance of any instance labeled as Person of the canonical surface form of 162 famous people.</S>
  <S sid="42" ssid="42">Our aim has been to build a named entity recog- nition and disambiguation system that employs a comprehensive list of entities and a vast amount of world knowledge.</S>
  <S sid="43" ssid="43">Thus, we turned our attention to the Wikipedia collection, the largest organized knowledge repository on the Web (Remy, 2002).</S>
  <S sid="44" ssid="44">Wikipedia was successfully employed previously by Strube and Ponzetto (2006) and Gabrilovich and Markovitch (2007) to devise methods for computing semantic relatedness of documents, WikiRelate!</S>
  <S sid="45" ssid="45">and Explicit Semantic Analysis (ESA), respec- tively.</S>
  <S sid="46" ssid="46">For any pair of words, WikiRelate!</S>
  <S sid="47" ssid="47">attempts to find a pair of articles with titles that contain those words and then computes their relatedness from the word-based similarity of the articles and the distance between the articles?</S>
  <S sid="48" ssid="48">categories in the Wikipedia category tree.</S>
  <S sid="49" ssid="49">ESA works by first build- ing an inverted index from words to all Wikipedia articles that contain them.</S>
  <S sid="50" ssid="50">Then, it estimates a re- latedness score for any two documents by using the inverted index to build a vector over Wikipedia articles for each document and by computing the cosine similarity between the two vectors.</S>
  <S sid="51" ssid="51"></S>
  <S sid="52" ssid="52">They employed several of the disambiguation resources discussed in this paper (Wikipedia entity pages, redirection pages, categories, and hyperlinks) and built a context- article cosine similarity model and an SVM based on a taxonomy kernel.</S>
  <S sid="53" ssid="53">They evaluated their models for person name disambiguation over 110, 540, and 2,847 categories, reporting accuracies between 55.4% and 84.8% on (55-word context, entity) pairs extracted from Wikipedia, depending on the model and the development/test data employed.</S>
  <S sid="54" ssid="54">The system discussed in this paper performs both named entity identification and disambiguation.</S>
  <S sid="55" ssid="55">The entity identification and in-document corefer- ence components resemble the Nominator system (Wacholder et al., 1997).</S>
  <S sid="56" ssid="56">However, while Nomina- tor made heavy use of heuristics and lexical clues to solve the structural ambiguity of entity men- tions, we employ statistics extracted from Wikipe- dia and Web search results.</S>
  <S sid="57" ssid="57">The disambiguation component, which constitutes the main focus of the paper, employs a vast amount of contextual and category information automatically extracted from Wikipedia over a space of 1.4 million distinct enti- ties/concepts, making extensive use of the highly interlinked structure of this collection.</S>
  <S sid="58" ssid="58">We aug- ment the Wikipedia category information with in- formation automatically extracted from Wikipedia list pages and use it in conjunction with the context information in a vectorial model that employs a novel disambiguation method.</S>
  <S sid="59" ssid="59">709 2 The Disambiguation Paradigm We present in this section an overview of the pro- posed disambiguation model and the world knowl- edge data employed in the instantiation of the model discussed in this paper.</S>
  <S sid="60" ssid="60">The formal model is discussed in detailed in Section 5.</S>
  <S sid="61" ssid="61">The world knowledge used includes the known entities (most articles in Wikipedia are associated to an entity/concept), their entity class when avail- able (Person, Location, Organization, and Miscel- laneous), their known surface forms (terms that are used to mention the entities in text), contextual evidence (words or other entities that describe or co-occur with an entity), and category tags (which describe topics to which an entity belongs to).</S>
  <S sid="62" ssid="62">For example, Figure 1 shows nine of the over 70 different entities that are referred to as ?Columbia?</S>
  <S sid="63" ssid="63">in Wikipedia and some of the category and contex- tual information associated with one of these enti- ties, the Space Shuttle Columbia.</S>
  <S sid="64" ssid="64">The disambiguation process uses the data associ- ated with the known surface forms identified in a document and all their possible entity disambigua- tions to maximize the agreement between the con- text data stored for the candidate entities and the contextual information in the document, and also, the agreement among the category tags of the can- didate entities.</S>
  <S sid="65" ssid="65">For example, a document that con- tains the surface forms ?Columbia?</S>
  <S sid="66" ssid="66">and ?Discovery?</S>
  <S sid="67" ssid="67">is likely to refer to the Space Shuttle Columbia and the Space Shuttle Discovery because these candidate entities share the category tags LIST_astronomical_topics, CAT_Manned_space- craft, CAT_Space_Shuttles (the extraction of such tags is presented in Section 3.2), while other entity disambiguations, such as Columbia Pictures and Space Shuttle Discovery, do not share any com- mon category tags.</S>
  <S sid="68" ssid="68">The agreement maximization process is discussed in depth in Section 5.</S>
  <S sid="69" ssid="69">This process is based on the assumption that typically, all instances of a surface form in a document have the same meaning.</S>
  <S sid="70" ssid="70">Nonetheless, there are a non-negligible number of cases in which the one sense per discourse assumption (Gale et al., 1992) does not hold.</S>
  <S sid="71" ssid="71">To address this problem, we employ an iterative approach, of shrinking the context size used to disambiguate surface forms for which there is no dominating entity disambiguation at document level, perform- ing the disambiguation at the paragraph level and then at the sentence level if necessary.</S>
  <S sid="72" ssid="72">The model of storing the information ex- tracted from Wikipedia into two databases.</S>
  <S sid="73" ssid="73">3 Information Extraction from Wikipedia We discuss now the extraction of entities and the three main types of disambiguation clues (entity surface forms, category tags, and contexts) used by the implemented system.</S>
  <S sid="74" ssid="74">While this information extraction was performed on the English version of the Wikipedia collection, versions in other lan- guages or other collections, such as Encarta or WebMD, could be targeted in a similar manner.</S>
  <S sid="75" ssid="75">When processing the Wikipedia collection, we distinguish among four types of articles: entity pages, redirecting pages, disambiguation pages, and list pages.</S>
  <S sid="76" ssid="76">The characteristics of these articles and the processing applied to each type to extract the three sets of clues employed by the disam- biguation model are discussed in the next three subsections.</S>
  <S sid="77" ssid="77">3.1 Surface Form to Entity Mappings There are four sources that we use to extract entity surface forms: the titles of entity pages, the titles of redirecting pages, the disambiguation pages, and the references to entity pages in other Wikipedia articles.</S>
  <S sid="78" ssid="78">An entity page is an article that contains information focused on one single entity, such as a person, a place, or a work of art.</S>
  <S sid="79" ssid="79">For example, Wikipedia contains a page titled ?Texas (TV se- ries)?, which offers information about the soap opera that aired on NBC from 1980 until 1982.</S>
  <S sid="80" ssid="80">A redirecting page typically contains only a refer- ence to an entity page.</S>
  <S sid="81" ssid="81">For example, the article titled ?Another World in Texas?</S>
  <S sid="82" ssid="82">contains a redirec- Columbia  Colombia Columbia University Columbia River Columbia Pictures Columbia Bicycles Space Shuttle Columbia USS Columbia Columbia, Maryland Columbia, California ... Space Shuttle Columbia  Tags: Manned spacecraft Space program fatalities Space Shuttles  Contexts: NASA Kennedy Space Center Eileen Collins Entities Surface Forms 710 tion to the article titled ?Texas (TV series)?.</S>
  <S sid="83" ssid="83">From these two articles, we extract the entity Texas (TV series) and its surface forms Texas (TV series), Texas and Another World in Texas.</S>
  <S sid="84" ssid="84">As shown in this example, we store not only the exact article titles but also the corresponding forms from which we eliminate appositives (either within parentheses or following a comma).</S>
  <S sid="85" ssid="85">We also extract surface form to entity mappings from Wikipedia disambiguation pages, which are specially marked articles having as title a surface form, typically followed by the word ?disambigua- tion?</S>
  <S sid="86" ssid="86">(e.g., ?Texas (disambiguation)?</S>
  <S sid="87" ssid="87">), and con- taining a list of references to pages for entities that are typically mentioned using that surface form.</S>
  <S sid="88" ssid="88">Additionally, we extract all the surface forms used at least in two articles to refer to a Wikipedia entity page.</S>
  <S sid="89" ssid="89">Illustratively, the article for Pam Long contains the following Wikitext, which uses the surface form ?Texas?</S>
  <S sid="90" ssid="90">to refer to Texas (TV series): After graduation, she went to [[New York City]] and played Ashley Linden on [[Texas (TV series)|Texas]] from [[1981]] to [[1982]].</S>
  <S sid="91" ssid="91">In Wikitext, the references to other Wikipedia ar- ticles are within pairs of double square brackets.</S>
  <S sid="92" ssid="92">If a reference contains a vertical bar then the text at the left of the bar is the name of the referred article (e.g.</S>
  <S sid="93" ssid="93">?Texas (TV Series)?</S>
  <S sid="94" ssid="94">), while the text at the right of the bar (e.g., ?Texas?)</S>
  <S sid="95" ssid="95">is the surface form that is displayed (also referred to as the anchor text of the link).</S>
  <S sid="96" ssid="96">Otherwise, the surface form shown in the text is identical to the title of the Wikipedia article referred (e.g., ?New York City?).</S>
  <S sid="97" ssid="97">Using these four sources, we extracted more than 1.4 million entities, with an average of 2.4 surface forms per entity.</S>
  <S sid="98" ssid="98">We obtained 377k entities with one surface form, 166k entities with two surface forms, and 79k entities with three surface forms.</S>
  <S sid="99" ssid="99">At the other extreme, we extracted one entity with no less than 99 surface forms.</S>
  <S sid="100" ssid="100">3.2 Category Information All articles that are titled ?List of [?]?</S>
  <S sid="101" ssid="101">or ?Table of [?]?</S>
  <S sid="102" ssid="102">are treated separately as list pages.</S>
  <S sid="103" ssid="103">They were built by Wikipedia contributors to group enti- ties of the same type together (e.g., ?List of an- thropologists?, ?List of animated television series?, etc.)</S>
  <S sid="104" ssid="104">and are used by our system to extract category tags for the entities listed in these articles.</S>
  <S sid="105" ssid="105">The tags are named after the title of the Wikipedia list page.</S>
  <S sid="106" ssid="106">For example, from the article ?List of band name etymologies?, the system extracts the category tag LIST_band_name_etymologies and labels all the entities referenced in the list, including Texas (band), with this tag.</S>
  <S sid="107" ssid="107">This process resulted in the extraction of more than 1 million (entity, tag) pairs.</S>
  <S sid="108" ssid="108">After a post-processing phase that discards tempo- ral tags, as well as several types of non-useful tags such as ?people by name?</S>
  <S sid="109" ssid="109">and ?places by name?, we obtained a filtered list of 540 thousand pairs.</S>
  <S sid="110" ssid="110">We also exploit the fact that Wikipedia enables contributors to assign categories to each article, which are defined as ?major topics that are likely to be useful to someone reading the article?.</S>
  <S sid="111" ssid="111">Be- cause any Wikipedia contributor can add a cate- gory to any article and the work of filtering out bogus assignments is tedious, these categories seem to be noisier than the lists, but they can still provide a tremendous amount of information.</S>
  <S sid="112" ssid="112">We extracted the categories of each entity page and assigned them as tags to the corresponding entity.</S>
  <S sid="113" ssid="113">Again, we employed some basic filtering to discard meta-categories (e.g., ?Articles with unsourced statements?)</S>
  <S sid="114" ssid="114">and categories not useful for the proc- ess of disambiguation through tag agreement (e.g., ?Living people?, ?1929 births?).</S>
  <S sid="115" ssid="115">This extraction process resulted in 2.65 million (entity, tag) pairs over a space of 139,029 category tags.</S>
  <S sid="116" ssid="116">We also attempted to extract category tags based on lexicosyntactic patterns, more specifically from enumerations of entities.</S>
  <S sid="117" ssid="117">For example, the para- graph titled ?Music of Scotland?</S>
  <S sid="118" ssid="118">(shown below in Wikitext) in the Wikipedia article on Scotland con- tains an enumeration of entities, which can be la- beled ENUM_Scotland_PAR_Music_of_Scotland: Modern Scottish [[pop music]] has produced many international bands including the [[Bay City Rollers]], [[Primal Scream]], [[Simple Minds]], [[The Proclaim- ers]], [[Deacon Blue]], [[Texas (band)|Texas]], [[Franz Ferdinand]], [[Belle and Sebastian]], and [[Travis (band)|Travis]], as well as individual artists such as [[Gerry Rafferty]], [[Lulu]], [[Annie Lennox]] and [[Lloyd Cole]], and world-famous Gaelic groups such as [[Runrig]] and [[Capercaillie (band)|Capercaillie]].</S>
  <S sid="119" ssid="119">Lexicosyntactic patterns have been employed successfully in the past (e.g., Hearst, 1992; Roark and Charniak, 1998; Cederberg and Widdows, 2003), and this type of tag extraction is still a promising direction for the future.</S>
  <S sid="120" ssid="120">However, the brute force approach we tried ?</S>
  <S sid="121" ssid="121">of indiscriminately tagging the entities of enumerations of four or more entities ?</S>
  <S sid="122" ssid="122">was found to introduce a large amount of noise into the system in our develop- ment experiments.</S>
  <S sid="123" ssid="123">711 3.3 Contexts To extract contextual clues for an entity, we use the information present in that entity?s page and in the other articles that explicitly refer to that entity.</S>
  <S sid="124" ssid="124">First, the appositives in the titles of entity pages, which are eliminated to derive entity surface forms (as discussed in Section 3.1) are saved as contex- tual clues.</S>
  <S sid="125" ssid="125">For example, ?TV series?</S>
  <S sid="126" ssid="126">becomes a context for the entity Texas (TV series).</S>
  <S sid="127" ssid="127">We then extract all the entity references in the entity page.</S>
  <S sid="128" ssid="128">For example, from the article on Texas (band), for which a snippet in Wikitext is shown below, we extract as contexts the references pop music, Glasgow, Scotland, and so on: Texas is a [[pop music]] band from [[Glasgow]], [[Scotland]], [[United Kingdom]].</S>
  <S sid="129" ssid="129">They were founded by [[Johnny McElhone]] in [[1986 in music|1986]] and had their performing debut in [[March]] [[1988]] at [?]</S>
  <S sid="130" ssid="130">Reciprocally, we also extract from the same ar- ticle that the entity Texas (band) is a good context for pop music, Glasgow, Scotland, etc.</S>
  <S sid="131" ssid="131">The number of contexts extracted in this manner is overwhelming and had to be reduced to a man- ageable size.</S>
  <S sid="132" ssid="132">In our development experiments, we explored various ways of reducing the context in- formation, for example, by extracting only entities with a certain number of mentions in an article, or by discarding mentions with low TF*IDF scores (Salton, 1989).</S>
  <S sid="133" ssid="133">In the end, we chose a strategy in which we employ as contexts for an entity two category of references: those mentioned in the first paragraph of the targeted entity page, and those for which the corresponding pages refer back to the targeted entity.</S>
  <S sid="134" ssid="134">For example, Pam Long and Texas (TV series) are extracted as relevant contexts for each other because their corresponding Wikipedia articles reference one another ?</S>
  <S sid="135" ssid="135">a relevant snippet from the Pam Long article is cited in Section 3.1 and a snippet from the article for Texas (TV se- ries) that references Pam Long is shown below: In 1982 [[Gail Kobe]] became executive producer and [[Pam Long]] became headwriter.</S>
  <S sid="136" ssid="136">In this manner, we extracted approximately 38 million (entity, context) pairs.</S>
  <S sid="137" ssid="137">4 Document Analysis In this section, we describe concisely the main text processing and entity identification components of the implemented system.</S>
  <S sid="138" ssid="138">We will then focus on the novel entity disambiguation component, which we propose and evaluate in this paper, in Section 5.</S>
  <S sid="139" ssid="139">An overview of the processes employed by the proposed system.</S>
  <S sid="140" ssid="140">Figure 2 outlines the processes and the resources that are employed by the implemented system in the analysis of text documents.</S>
  <S sid="141" ssid="141">First, the system splits a document into sentences and truecases the beginning of each sentence, hypothesizing whether the first word is part of an entity or it is capitalized because of orthographic conventions.</S>
  <S sid="142" ssid="142">It also identi- fies titles and hypothesizes the correct case for all words in the titles.</S>
  <S sid="143" ssid="143">This is done based on statistics extracted from a one-billion-word corpus, with back-off to Web statistics.</S>
  <S sid="144" ssid="144">In a second stage, a hybrid named-entity recog- nizer based on capitalization rules, Web statistics, and statistics extracted from the CoNLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003) identifies the  boundaries of  the entity  mentions in the text and assigns each set of mentions sharing the same surface form a probabil- ity distribution over four labels: Person, Location, Organization, and Miscellaneous.1 The named en- tity recognition component resolves the structural ambiguity with regard to conjunctions (e.g., ?Bar- nes and Noble?, ?Lewis and Clark?</S>
  <S sid="145" ssid="145">), possessives (e.g., ?Alices Adventures in Wonderland?, ?Brit- ains Tony Blair?</S>
  <S sid="146" ssid="146">), and prepositional attachment (e.g., ?Whitney Museum of American Art?, ?Whitney Museum in New York?)</S>
  <S sid="147" ssid="147">by using the surface form information extracted from Wikipe- dia, when available, with back-off to co-occurrence counts on the Web, in a similar way to Lapata and Keller (2004).</S>
  <S sid="148" ssid="148">Recursively, for each ambiguous term T0 of the form T1 Particle T2, where Particle is one of a possessive pronoun, a coordinative con- junction, and a preposition, optionally followed by a determiner, and the terms T1 and T2 are se-  1 While the named entity labels are used only to solve in- document coreferences by the current system, as described further in this section, preliminary experiments of probabilisti- cally labeling the Wikipedia pages show that the these labels could also be used successfully in the disambiguation process.</S>
  <S sid="149" ssid="149"></S>
  <S sid="150" ssid="150"></S>
  <S sid="151" ssid="151">?, which forces the engine to return only documents in which the whole terms T1 and T2 appear.</S>
  <S sid="152" ssid="152">We then count the number of times the snippets of the top N = 200 search results returned contain the term T0 and compare it with an empirically obtained threshold to hypothesize whether T0 is the mention of one entity or encompasses the mentions of two entities, T1 and T2.</S>
  <S sid="153" ssid="153">As Wacholder et al.</S>
  <S sid="154" ssid="154">(1997) noted, it is fairly common for one of the mentions of an entity in a document to be a long, typical surface form of that entity (e.g., ?George W.</S>
  <S sid="155" ssid="155">), while the other mentions are shorter surface forms (e.g., ?Bush?).</S>
  <S sid="156" ssid="156">Therefore, before attempting to solve the semantic ambiguity, the system hypothesizes in-document coreferences and maps short surface forms to longer surface forms with the same dominant label (for example, ?Brown?/PERSON can be mapped to ?Michael Brown?/PERSON).</S>
  <S sid="157" ssid="157">Acronyms are also re- solved in a similar manner when possible.</S>
  <S sid="158" ssid="158">In the third stage, the contextual and category in- formation extracted from Wikipedia is used to dis- ambiguate the entities in the text.</S>
  <S sid="159" ssid="159">This stage is discussed formally in Section 5 and evaluated in Section 6.</S>
  <S sid="160" ssid="160">Note that the performance of the disam- biguation component is meaningful only when most named entity mentions are accurately identi- fied in text.</S>
  <S sid="161" ssid="161">Thus, we first measured the perform- ance of the named entity recognition component on the CoNLL 2003 test set and obtained a competi- tive F-measure of 0.835 (82.2% precision  and 84.8% recall).</S>
  <S sid="162" ssid="162">Finally, the implemented system creates hyper- links to the appropriate pages in Wikipedia.</S>
  <S sid="163" ssid="163">Figure 3 shows the output of the implemented system on a sample news story, in which the identified and dis- ambiguated surface forms are hyperlinked to Wikipedia articles.</S>
  <S sid="164" ssid="164">5 The Disambiguation Component The disambiguation process employs a vector space model, in which a vectorial representation of the processed document is compared with the vec- torial representations of the Wikipedia entities.</S>
  <S sid="165" ssid="165">Once the named entity surface forms were identi- fied and the in-document coreferences hypothe- sized, the system retrieves all possible entity disambiguations of each surface form.</S>
  <S sid="166" ssid="166">Their Wikipedia contexts that occur in the document and their category tags are aggregated into a document vector, which is subsequently compared with the Wikipedia entity vector (of categories and con- texts) of each possible entity disambiguation.</S>
  <S sid="167" ssid="167">We then choose the assignment of entities to surface forms that maximizes the similarity between the document vector and the entity vectors, as we ex- plain further.</S>
  <S sid="168" ssid="168"></S>
  <S sid="169" ssid="169">An entity e can then be represented as a vector ?e?</S>
  <S sid="170" ssid="170"></S>
  <S sid="171" ssid="171"></S>
  <S sid="172" ssid="172">{0,1}N, corre- sponding to the context and category information, respectively: 1, if ci is a context for entity e ?ei =    { 0, otherwise 1, if tj is a category tag for e ?eM+j ={ 0, otherwise.</S>
  <S sid="173" ssid="173">Screenshot of the implemented system showing an example of analyzed text.</S>
  <S sid="174" ssid="174">The superimposed tooltips show how several of the surface forms were disambiguated based on the context and category agreement method.</S>
  <S sid="175" ssid="175">(s) denote the set of entities that are known to have a surface form s. For example, recalling Figure 1, Colombia (the country) and Columbia University are entities that are known to have the surface form ?Columbia?.</S>
  <S sid="176" ssid="176"></S>
  <S sid="177" ssid="177">= )( )( || DSs se TeTd ?</S>
  <S sid="178" ssid="178"></S>
  <S sid="179" ssid="179"></S>
  <S sid="180" ssid="180">&gt;&lt;+&gt;&lt; n i n j TeTe n i Ce ee ij jii nss n d 1 11),..,( ,,maxarg )(..)1( 1 ???</S>
  <S sid="181" ssid="181">?&lt; ,  denotes the scalar product of vectors.</S>
  <S sid="182" ssid="182">Note that the quality of an assignment of an entity to a surface form depends on all the other assign- ments made, which makes this a difficult optimiza- tion problem.</S>
  <S sid="183" ssid="183"></S>
  <S sid="184" ssid="184">&gt;?&lt; n i Tee ssee ii nn d 1)(..)(),..,( ,maxarg 11 ??</S>
  <S sid="185" ssid="185">(2) Indeed, using the definition of d and partitioning the context and category components, we can re- write the sum in equation (2) as ( ) (q.e.d.)</S>
  <S sid="186" ssid="186"></S>
  <S sid="187" ssid="187">&gt;&lt;+&gt;&lt; =&gt;?&lt;+&gt;&lt; =&gt;?&lt;+&gt;&lt; n i n j T se eTe n i Ce n i Te n j se TeTe n i Ce n i TeTTe n i Ce ij j ii i j ii iii d d dd ?</S>
  <S sid="188" ssid="188">2 We use the notation d to emphasize that this vector contains information that was not present in the original document D. Note now that the maximization of the sum in (2) is equivalent to the maximization of each of its terms, which means that the computation reduces to nid Tee se ii ii ..1,,maxarg )( ?&gt;?&lt; ?</S>
  <S sid="189" ssid="189">., or  equivalently, nid Tee se ii ii ..1,||||,maxarg 2 )( ?</S>
  <S sid="190" ssid="190">(3) Our disambiguation process therefore employs two steps: first, it builds the extended document vector and second, it maximizes the scalar products in equation (3).</S>
  <S sid="191" ssid="191"></S>
  <S sid="192" ssid="192">Also note that we are not normalizing the scalar products by the norms of the vectors (which would lead to the computation of cosine similarity).</S>
  <S sid="193" ssid="193">In this manner, we implicitly account for the fre- quency with which a surface form is used to men- tion various entities and for the importance of these entities (important entities have longer Wikipedia articles, are mentioned more frequently in other articles, and also tend to have more category tags).</S>
  <S sid="194" ssid="194">While rarely, one surface form can be used to mention two or more different entities in a docu- ment (e.g., ?Supreme Court?</S>
  <S sid="195" ssid="195">may refer to the fed- eral institution in one paragraph and to a state?s judicial institution in another paragraph).</S>
  <S sid="196" ssid="196">To ac- count for such cases, the described disambiguation process is performed iteratively for the instances of the surface forms with multiple disambiguations with similarity scores higher than an empirically determined threshold, by shrinking the context used for the disambiguation of each instance from document level to paragraph level, and if neces- sary, to sentence level.</S>
  <S sid="197" ssid="197">6 Evaluation We used as development data for building the de- scribed system the Wikipedia collection as of April 2, 2006 and a set of 100 news stories on a diverse range of topics.</S>
  <S sid="198" ssid="198">For the final evaluation, we per- formed data extraction from the September 11, 2006 version of the Wikipedia collection.</S>
  <S sid="199" ssid="199">We evaluated the system in two ways: on a set of Wikipedia articles, by comparing the system out- put with the references created by human contribu- tors, and on a set of news stories, by doing a post- hoc evaluation of the system output.</S>
  <S sid="200" ssid="200">The evalua- tion data can be downloaded from http://research.</S>
  <S sid="201" ssid="201">microsoft.com/users/silviu/WebAssistant/TestData.</S>
  <S sid="202" ssid="202">714 In both settings, we computed a disambiguation baseline in the following manner: for each surface form, if there was an entity page or redirect page whose title matches exactly the surface form then we chose the corresponding entity as the baseline disambiguation; otherwise, we chose the entity most frequently mentioned in Wikipedia using that surface form.</S>
  <S sid="203" ssid="203">6.1 Wikipedia Articles We selected at random 350 Wikipedia entity pages and we discarded their content during the informa- tion extraction phase.</S>
  <S sid="204" ssid="204">We then performed an auto- matic evaluation, in which we compared the hyperlinks created by our system with the links created by the Wikipedia contributors.</S>
  <S sid="205" ssid="205">In an at- tempt to discard most of the non-named entities, we only kept for evaluation the surface forms that started with an uppercase letter.</S>
  <S sid="206" ssid="206">The test articles contained 5,812 such surface forms.</S>
  <S sid="207" ssid="207">551 of them referenced non-existing articles (for example, the filmography section of a director contained linked mentions of all his movies although many of them did not have an associated Wikipedia page).</S>
  <S sid="208" ssid="208">Also, 130 of the surface forms were not used in other Wikipedia articles and therefore both the baseline and the proposed system could not hypothesize a disambiguation for them.</S>
  <S sid="209" ssid="209">The accuracy on the re- maining 5,131 surface forms was 86.2% for the baseline system and 88.3% for the proposed sys- tem.</S>
  <S sid="210" ssid="210">A McNemar test showed that the difference is not significant, the main cause being that the ma- jority of the test surface forms were unambiguous.</S>
  <S sid="211" ssid="211">When restricting the test set only to the 1,668 am- biguous surface forms, the difference in accuracy between the two systems is significant at p = 0.01.</S>
  <S sid="212" ssid="212">An error analysis showed that the Wikipedia set used as gold standard contained relatively many surface forms with erroneous or out-of-date links, many of them being correctly disambiguated by the proposed system (thus, counted as errors).</S>
  <S sid="213" ssid="213">For example, the test page ?The Gods (band)?</S>
  <S sid="214" ssid="214">links to Paul Newton, the painter, and Uriah Heep, which is a disambiguation page, probably because the origi- nal pages changed over time, while the proposed system correctly hypothesizes links to Paul New- ton (musician) and Uriah Heep (band).</S>
  <S sid="215" ssid="215">6.2 News Stories We downloaded the top two stories in the ten MSNBC news categories (Business, U.S.</S>
  <S sid="216" ssid="216">Politics, Entertainment, Health, Sports, Tech &amp; Science, Travel, TV News, U.S. News, and World News) as of January 2, 2007 and we used them as input to our system.</S>
  <S sid="217" ssid="217">We then performed a post-hoc evalua- tion of the disambiguations hypothesized for the surface forms correctly identified by the system (i.e.</S>
  <S sid="218" ssid="218">if the boundaries of a surface form were not identified correctly then we disregarded it).</S>
  <S sid="219" ssid="219">We defined a disambiguation to be correct if it represented the best possible Wikipedia article that would satisfy a user?s need for information and incorrect otherwise.</S>
  <S sid="220" ssid="220">For example, the article Viking program is judged as correct for ?Viking Landers?, for which there is no separate article in the Wi- kipedia collection.</S>
  <S sid="221" ssid="221">Linking a surface form to a wrong Wikipedia article was counted as an error regardless whether or not an appropriate Wikipedia article existed.</S>
  <S sid="222" ssid="222">When the system could not disam- biguate a surface form (e.g.</S>
  <S sid="223" ssid="223">?Bama?, and ?Harris County Jail?</S>
  <S sid="224" ssid="224">), we performed a search in Wikipedia for the appropriate entity.</S>
  <S sid="225" ssid="225">If an article for that entity existed (e.g., ?N Sync and Alabama) then we counted that instance as an error.</S>
  <S sid="226" ssid="226">Other- wise, we counted it separately as non-recallable (e.g.</S>
  <S sid="227" ssid="227">there is no Wikipedia article for the Harris County Jail entity and the article for Harris County, Texas does not discuss the jail system).</S>
  <S sid="228" ssid="228">The test set contained 756 surface forms, of which 127 were non-recallable.</S>
  <S sid="229" ssid="229">The proposed sys- tem obtained an accuracy of 91.4%, versus a 51.7% baseline (significant at p = 0.01).</S>
  <S sid="230" ssid="230">An analy- sis of these data showed not only that the most common surface forms used in news are highly ambiguous but also that a large number of Wikipe- dia pages with titles that are popular surface forms in news discuss subjects different from those with common news usage (e.g., the page titled ?China?</S>
  <S sid="231" ssid="231">discusses the Chinese civilization and is not the correct assignment for the Peoples Republic of China entity; similarly, the default page for ?Blackberry?</S>
  <S sid="232" ssid="232">talks about the fruit rather than the wireless company with the same name).</S>
  <S sid="233" ssid="233">7 Conclusions and Potential Impact We presented a large scale named entity disam- biguation system that employs a huge amount of information automatically extracted from Wikipe- dia over a space of more than 1.4 million entities.</S>
  <S sid="234" ssid="234">In tests on both real news data and Wikipedia text, the system obtained accuracies exceeding 91% and 88%.</S>
  <S sid="235" ssid="235">Because the entity recognition and disam- 715 biguation processes employed use very little lan- guage-dependent resources additional to Wikipe- dia, the system can be easily adapted to languages other than English.</S>
  <S sid="236" ssid="236">The system described in this paper has been fully implemented as a Web browser (Figure 3), which can analyze any Web page or client text document.</S>
  <S sid="237" ssid="237">The application on a large scale of such an entity extraction and disambiguation system could result in a move from the current space of words to a space of concepts, which enables several paradigm shifts and opens new research directions, which we are currently investigating, from entity-based in- dexing and searching of document collections to personalized views of the Web through entity- based user bookmarks.</S>
  <S sid="238" ssid="238">Acknowledgments The author would like to gratefully thank Mike Schultz and Robert Ragno for their help in building the system infrastructure, Microsoft Live Search for providing access to their search engine, and the anonymous reviewers for their useful comments.</S>
  <S sid="239" ssid="239">References Bagga, A. and B. Baldwin.</S>
  <S sid="240" ssid="240">Entity-based cross- document coreferencing using the vector space model.</S>
  <S sid="241" ssid="241">In Proceedings of COLING-ACL, 79-85.</S>
  <S sid="242" ssid="242"></S>
  <S sid="243" ssid="243">Using Encyclopedic Knowledge for Named Entity Disambiguation.</S>
  <S sid="244" ssid="244">In Proceedings of EACL, 9-16.</S>
  <S sid="245" ssid="245">Cederberg, S. and D. Widdows.</S>
  <S sid="246" ssid="246">Using LSA and noun coordination information to improve the preci- sion and recall of hyponymy extraction.</S>
  <S sid="247" ssid="247">In Proceed- ings of CoNLL, 111-118.</S>
  <S sid="248" ssid="248">Doddington, G., A. Mitchell, M. Przybocki, L. Ram- shaw, S. Strassel, and R. Weischedel.</S>
  <S sid="249" ssid="249">task definitions and performance measures.</S>
  <S sid="250" ssid="250">In Proceedings of LREC, 837-840.</S>
  <S sid="251" ssid="251">Edmonds, P. and S. Cotton.</S>
  <S sid="252" ssid="252">Senseval-2 overview.</S>
  <S sid="253" ssid="253">In Proceedings of SENSEVAL-2, 1-6.</S>
  <S sid="254" ssid="254">Gabrilovich, E. and S. Markovitch.</S>
  <S sid="255" ssid="255">Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</S>
  <S sid="256" ssid="256">Proceedings of IJCAI, 1606-1611.</S>
  <S sid="257" ssid="257">Gale, W., K. Church, and D. Yarowsky.</S>
  <S sid="258" ssid="258">One sense per discourse.</S>
  <S sid="259" ssid="259">In Proceedings of the 4th DARPA SNL Workshop, 233-237.</S>
  <S sid="260" ssid="260">Grishman, R. and B. Sundheim.</S>
  <S sid="261" ssid="261">Message Under- standing Conference - 6: A brief history.</S>
  <S sid="262" ssid="262">In Proceed- ings of COLING, 466-471.</S>
  <S sid="263" ssid="263">Hearst, M. 1992.</S>
  <S sid="264" ssid="264">Automatic Acquisition of Hyponyms from Large Text Corpora.</S>
  <S sid="265" ssid="265">COLING, 539-545.</S>
  <S sid="266" ssid="266">Hirschman, L. and N. Chinchor.</S>
  <S sid="267" ssid="267">MUC-7 Corefer- ence Task Definition.</S>
  <S sid="268" ssid="268">In Proceedings of MUC-7.</S>
  <S sid="269" ssid="269">A method of geographical name ex- traction from Japanese text.</S>
  <S sid="270" ssid="270">In Proceedings of CIKM, 46-54.</S>
  <S sid="271" ssid="271">Kilgarriff, A. and J. Rosenzweig.</S>
  <S sid="272" ssid="272">Framework and results for English Senseval.</S>
  <S sid="273" ssid="273">Computers and Humani- ties, Special Issue on SENSEVAL, 15-48.</S>
  <S sid="274" ssid="274">Lapata, M. and F. Keller.</S>
  <S sid="275" ssid="275">The Web as a Baseline: Evaluating the Performance of Unsupervised Web- based Models for a Range of NLP Tasks.</S>
  <S sid="276" ssid="276">In Proceed- ings of HLT, 121-128.</S>
  <S sid="277" ssid="277">Mann, G. S. and D. Yarowsky.</S>
  <S sid="278" ssid="278">Unsupervised Personal Name Disambiguation.</S>
  <S sid="279" ssid="279">In Proceedings of CoNLL, 33-40.</S>
  <S sid="280" ssid="280">Mihalcea, R., T. Chklovski, and A. Kilgarriff.</S>
  <S sid="281" ssid="281">The Sen- seval-3 English lexical sample task.</S>
  <S sid="282" ssid="282">In Proceedings of SENSEVAL-3, 25-28.</S>
  <S sid="283" ssid="283">Overell, S., and S. R?ger.</S>
  <S sid="284" ssid="284">2006 Identifying and ground- ing descriptions of places.</S>
  <S sid="285" ssid="285">In SIGIR Workshop on Geographic Information Retrieval.</S>
  <S sid="286" ssid="286">Raghavan, H., J. Allan, and A. McCallum.</S>
  <S sid="287" ssid="287">An exploration of entity models, collective classification and relation description.</S>
  <S sid="288" ssid="288">In KDD Workshop on Link Analysis and Group Detection.</S>
  <S sid="289" ssid="289">Ravin, Y. and Z. Kazi.</S>
  <S sid="290" ssid="290">Is Hillary Rodham Clinton the President?</S>
  <S sid="291" ssid="291">In ACL Workshop on Coreference and its Applications.</S>
  <S sid="292" ssid="292">Wikipedia: The free encyclopedia.</S>
  <S sid="293" ssid="293">In Online Information Review, 26(6): 434.</S>
  <S sid="294" ssid="294">Roark, B. and E. Charniak.</S>
  <S sid="295" ssid="295">Noun-phrase co- occurrence statistics for semi-automatic semantic lexicon construction.</S>
  <S sid="296" ssid="296">In Proceedings of COLING- ACL, 1110-1116.</S>
  <S sid="297" ssid="297">Salton, G. 1989.</S>
  <S sid="298" ssid="298">Automatic Text Processing.</S>
  <S sid="299" ssid="299">Addison- Wesley.</S>
  <S sid="300" ssid="300">Smith, D. A. and G. Crane.</S>
  <S sid="301" ssid="301">Disambiguating geo- graphic names in a historic digital library.</S>
  <S sid="302" ssid="302">In Pro- ceedings of ECDL, 127-136.</S>
  <S sid="303" ssid="303">Strube, M. and S. P. Ponzeto.</S>
  <S sid="304" ssid="304">Com- puting semantic relatedness using Wikipedia.</S>
  <S sid="305" ssid="305">In Pro- ceedings of AAAI, 1419-1424.</S>
  <S sid="306" ssid="306">Tjong Kim Sang, E. F. and F. De Meulder.</S>
  <S sid="307" ssid="307">Intro- duction to the CoNLL-2003 Shared Task: Language- Independent Named Entity Recognition.</S>
  <S sid="308" ssid="308">In Proceed- ings of CoNLL, 142-147.</S>
  <S sid="309" ssid="309">Wacholder, N., Y. Ravin, and M. Choi.</S>
  <S sid="310" ssid="310">Disam- biguation of proper names in text.</S>
  <S sid="311" ssid="311">In Proceedings of ANLP, 202-208.</S>
  <S sid="312" ssid="312">Woodruff, A. G. and C. Paunt.</S>
  <S sid="313" ssid="313">GIPSY:Automatic geo- graphic indexing of documents.</S>
  <S sid="314" ssid="314">Journal of the Ameri- can Society for Information Science and Technology, 45(9):645-655.</S>
</PAPER>
