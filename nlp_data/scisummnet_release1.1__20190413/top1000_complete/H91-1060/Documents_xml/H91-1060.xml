<PAPER>
  <S sid="0" ssid="0">A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. I(lavans, M. Liberman, M. Jl4arcus, S. Roukos, B. Santorini, T. Strzalkowski IBM Research Division, Thomas J. Watson Research Center Yorktown Heights, NY 10598 The problem of quantitatively comparing tile perfor- mance of different broad-coverage rammars of En- glish has to date resisted solution.</S>
  <S sid="1" ssid="1">Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even tile simplest sentences.</S>
  <S sid="2" ssid="2">For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (IIewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boe- ing), Don tfindle (AT&amp;T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in com- mon only the following constituents, when each gram- marian provides the single parse which he/she would ideally want his/her grammar to specify for three sam- ple Brown Corpus sentences: The famed Yankee Clipper, now retired, has been as- sisting (as (a batting coach)).</S>
  <S sid="3" ssid="3">One of those cai)ital-gains ventures, ill fact, has sad- dled him (with Gore Court).</S>
  <S sid="4" ssid="4">lie said this constituted a (very serious) misuse (of the (Criminal court) processes).</S>
  <S sid="5" ssid="5">Specific differences among grammars which con- tribute to this apparent disparateness of analysis in- clude the treatmeat of punctuation as independent to- kens or, on the other hand, as parasites on the words to which they attach in writing; the recursive attach- ment of auxiliary elements to the right of Verb Phrase nodes, versus their incorporation there en bloc; the grouping of pre-infinitiva,1 "to" either with the main verb alone or with the entire Verb Phrase that it in- tro(luces; and the employment or non-employment of "null nodes" as a device in the grammar; as well as 306 other differences.</S>
  <S sid="6" ssid="6">Despite the seeming intractability of this problem, it appears to us that a solution to it is now at hand.</S>
  <S sid="7" ssid="7">We propose an evaluation pro- cedure with these characteristics: it judges a parse based only on the constituent boundaries it stipulates (and not the names it assigns to these constituents); it compares the parse to a "hand-parse" of the same sentence from the University of Pennsylvania Tree- bank; and it yields two principal measures for each parse submitted.</S>
  <S sid="8" ssid="8">The procedure has three steps.</S>
  <S sid="9" ssid="9">For each parse to be evaluated: (1) erase from the fully-parsed sentence all instances of: auxiliaries, "not", pre-infinitival "to", null categories, possessive ndings (% and ), and all word-external punctuation (e.g. "</S>
  <S sid="10" ssid="10">, ; - ) ;  (2) recur- sively erase all parenthesis pairs enclosing either a sin- gle constituent or word, or nothing at all; (3) compute goodness cores (Crossing Parentheses, and Recall) for the input parse, by comparing it to a similarly- reduced version of the Penn Treebank parse of the same sentence.</S>
  <S sid="11" ssid="11">For example, for the Brown Corpus sentence: Miss Xydis was best when she did not need to be too probing, consider the candidate parse: (S(NP-s(PNP(PNP Miss) (PNP Xydis))) (VP(VPAST was) (ADJP(ADJ best))) (S(COMP(WIIADVP(WI[ADV when))) ( ie -s  (PRO she)) (VP ((VPAST did) (NEG ,tot) (V need)) (VP((X to) (V be)) (ADJP(ADV too) (ADJ probing))))))(?</S>
  <S sid="12" ssid="12">After step-one rasures, this becomes: (S(NP-s(PNP(PNP Miss) (PNP Xydis))) (VP(VPAST was) (ADJP(ADJ best))) (S(COMP(WIIADVP(WIIADV wheu))) (NP-s (PRO she)) (VP((VPAST) (NEG) (V need)) (VP((X)  (V be)) (ADJP(ADV too) (ADJ probing)))))) (?</S>
  <S sid="13" ssid="13">(FIN)) And after step-two erasures: (S(NP-s Miss Xydis) (VP was best) (S when she (VP need (V be (ADJP too probing))))) The Uuiversity of Pennsylvania Treebank output for this sentence, after steps one and two have been ap- plied to it, is: (S(NP Miss Xydis) (VP was best (SBAR when (S she (VP need (VP be (ADJP too probing))))))) Step three consists of comparing the candidate parse to the treebank parse and deriving two scores: (1) The Crossing Parentheses score is the number of times the treebank has a parenthesization such as, say, (A (B C)) and the parse being evaluated has a paren- thesization for the same input of ((A B) C)), i.e.</S>
  <S sid="14" ssid="14">there are parentheses which "cross".</S>
  <S sid="15" ssid="15">(2) The Recall score is the number of parenthesis pairs in the intersection of tlle candidate and treebank parses (T intersection C) divided by the number of parenthesis pairs in the treebank parse T, viz.</S>
  <S sid="16" ssid="16">(T intersection C) / T. This score provides an additional measure of the degree of fit between the standard and tile candidate parses; in theory a RecMl of 1 certifies a candidate parse as in- cluding all constituent boundaries that are essential to the analysis of the input sentence.</S>
  <S sid="17" ssid="17">We applie d this metric to 14 sentences selected from the Brown Cor- pus and analyzed by each of the grammarians named above in the manner that each wished his/her gram- mar to do.</S>
  <S sid="18" ssid="18">Instead of using the UPenn Treebank as a standard, we used the automaticMly computed "ma- jority parse" of each sentence obtained from the set of candidate parses themselves.</S>
  <S sid="19" ssid="19">The average Crossing Parentheses rate over all our grammars was .4%, with a corresponding Recall score of 94%.</S>
  <S sid="20" ssid="20">We have agreed on three additionM categories of systematic alteration to our input parses which we believe will significantly improve the correlation between our "ideal parses", i.e.</S>
  <S sid="21" ssid="21">our individuM goals, and our standard.</S>
  <S sid="22" ssid="22">Even at the current level of fit, we feel comfortable Mlow- ing one of our number, the UPenn parse, to serve as the standard parse, since, crucially, it.</S>
  <S sid="23" ssid="23">is produced by hand.</S>
  <S sid="24" ssid="24">Our intention is to apply the current metric to more Brown Corpus data "ideally parsed" by us, and then to employ it to measure the performance of our grammars, run automatically, on a 1)enchma.rk set of sentences.</S>
  <S sid="25" ssid="25">APPENDIX: EVALUATION PROCEDURE FOR COMPUTER ENGLISH GRAMMARS O.</S>
  <S sid="26" ssid="26">Input format A parse for evaluation should consist initially of: (a) the input word string, tokenized as follows: (I) Any tokens containing punctuation marks are enclosed by vertical bars, e.g.</S>
  <S sid="27" ssid="27">~DAlbert~ I~,oooI (2) Contracted forms in which the abbreviated verb is used in the sentence under analysis as a main verb, as opposed to an auxiliary, are to be split: you ve  -&gt; you lvel (In "Youve a good reason for that."</S>
  <S sid="28" ssid="28">but not in "Youve been here often.")</S>
  <S sid="29" ssid="29">Johns -&gt; John lsl (In "Johns (i.e.</S>
  <S sid="30" ssid="30">is) a good friend" or "Johns (i.e.</S>
  <S sid="31" ssid="31">has) a good friend" but not "Johns (i.e.</S>
  <S sid="32" ssid="32">is) leaving" and not "Johns (i.e.</S>
  <S sid="33" ssid="33">has) been here" (3) Hyphenated words, numbers and miscellaneous digital expressions are left as is (i.e.</S>
  <S sid="34" ssid="34">not split), i.e.</S>
  <S sid="35" ssid="35">~co-signersl (and not "co I-I signers")| 12,0001 (and not "2 I , I  0 o 0")~ lall-womanl~ Ififty-threel: Ifree-for-alll| 56th~ 13/.1~ 1212-~88-9o271~ (b) the parse of the input word string with respect to the grammar under evaluation (I) Each grammatical constituent of the input is grouped using a pair of parentheses, e.g.</S>
  <S sid="36" ssid="36">307 "(((I)) ((see) ((Ed))))" (2) Constituent labels may, optionally, immediately fol low left parentheses and~or immediately precede right parentheses, e.g.</S>
  <S sid="37" ssid="37">(S (N (N Sue)) (V (V sees) (N (N Tom))))  = : ( ( (Sue)  ) ( ( sees) ( (Tom)  ) ) )e tc .</S>
  <S sid="38" ssid="38">I. Erasures of Input Elements The f irst of the two steps necessary to prepare init ial parsed input for evaluat ion consists  of erasing the fo l lowing types of word (token) str ings from the parse: (a) Auxi l iar ies Examples are : "would go there" -?</S>
  <S sid="39" ssid="39">"go there", "has been laughing" - ?</S>
  <S sid="40" ssid="40">"laughing", "does sing it correct ly" - ?</S>
  <S sid="41" ssid="41">"sing it correctly", but not: "is a cup", "is blue", "has a dollar", "does the laundry" (b) "Not" E.g.</S>
  <S sid="42" ssid="42">"is not in here" -&gt; "is in here", "Not precisely asleep, John sort of dozed" - ?</S>
  <S sid="43" ssid="43">"precisely asleep, John sort of dozed" (c ) Pre- inf init ival "to" E.g.</S>
  <S sid="44" ssid="44">"she opted to retire" - ?</S>
  <S sid="45" ssid="45">"she opted retire", "how to construe it" - ?</S>
  <S sid="46" ssid="46">"how construe it" (d) Null categories Example 1 : ("getting more pro letters than con"): (NXc (Qr more ) (NX (A pro ) (Npl letters)) (Than than ) (NX (A con) (Npl ) ) ) NOTA BENE - ?</S>
  <S sid="47" ssid="47">(NXc (Qr more ) (NX (A pro ) (Npl letters)) (Than than ) (NX (A con) ( ))); NOTA BENE Example 2 : ("The lawyer with whom I studied law"): (NP (DET The ) (N lawyer) (S-REL (PP (P with) (NP whom ) ) (NP I) (VP (V studied) (NP (N law)) (PP 0)))) NOTA BENE - ?</S>
  <S sid="48" ssid="48">(NP (DET The) (N lawyer ) (S- REL (PP (P with) (NP whom) ) (NP I) (VP (V studied) (NP (N law)) (PP ) ) )) NOTA BENE (e) Possess ive endings ( s,  ) E.g. "</S>
  <S sid="49" ssid="49">ILorisl mother" (i.e.</S>
  <S sid="50" ssid="50">the mother  of Lori) - ?</S>
  <S sid="51" ssid="51">"Lori mother" (f) Word-external  punctuat ion (quotes, commas, periods, dashes, etc. )</S>
  <S sid="52" ssid="52">The "blue book" was there - ?</S>
  <S sid="53" ssid="53">The blue book was there Your f i rst  , second and third ideas -?</S>
  <S sid="54" ssid="54">Your f i rst second and third ideas This is it.</S>
  <S sid="55" ssid="55">-&gt; This is it A l l - -or  almost al l - -of  them -&gt; All or almost all of  them But leave as is: 13,~56 18.2gl 13/17/g01 111:301 Ip.m.I I1)1 Ieh.D.I IU.N.</S>
  <S sid="56" ssid="56">I Ineer-do-welll 308 2.</S>
  <S sid="57" ssid="57">Erasures of Constituent Delimiters, i.e.</S>
  <S sid="58" ssid="58">Parentheses The second of the two steps necessary to prepare initial parsed input for evaluation consists of erasing parenthesis pairs, proceeding recursively, from the most to the least deeply embedded portion of the parenthesization, whenever they enclose either a single constituent or word, or nothing at all.</S>
  <S sid="59" ssid="59">Example: "Miss Xydis was best when she did not need to be too probing."</S>
  <S sid="60" ssid="60">Original parse (S (NP-s (PNP (PNP Miss ) (PNP Xydis ))) (VP (VPAST was ) (ADJP (ADJ best ))) (S (COMP (WHADVP (WHADV when ))) (NP-s (PRO she )) (VP ((VPAST did ) (MEG not ) (V need )) (vP ((x to ) (V be )) (ADJP (ADV too ) (ADJ probing ) ))))) (?</S>
  <S sid="61" ssid="61">Parse with all erasures performed except those of const i tuentdel imiters (parentheses): (S (NP-s (PNP (PNP Miss ) (PNP Xydis ))) (VP (VPAST was ) (ADJP (ADJ best ))) (S (COMP (WHADVP (WHADV when ))) (NP-s (PRO she )) (VP ((VPAST ) (MEG ) (V need )) (vP ((x ) (V be )) (ADJP (ADV too ) (ADJ probing ) ))))) (?</S>
  <S sid="62" ssid="62">Parse with all constituent delimiters erased which are superfluous by the above definition: (S (NP-s Miss Xydis ) (VP was best ) (S when she (vP need (vP be (ADJP too probing))))) NOTE: Any single-word adverbs which are left behind, as it were, by the erasure of auxil iary elements, are attached to the highest node of the immediately fol lowing verb constituent.</S>
  <S sid="63" ssid="63">Example: (will probably have) (seen Milton) -&gt; ( probably ) (seen Milton) -&gt; (probably seen Milton) 3.</S>
  <S sid="64" ssid="64">Redefinit ion of Selected Constituents The third step in the process of preparing initial parsed input for evaluation is necessary only if the parse submitted treats any of three particular constructions in a manner different from the canonical analysis currently accepted by the group.</S>
  <S sid="65" ssid="65">This step consists of redrawing constituent boundaries in conformity with the adopted standard.</S>
  <S sid="66" ssid="66">The three constructions involved are extraposition, modification of noun phrases, and sequences of prepositions which occur constituent-init ial ly and~or 309 particles which occur constituent-finally.</S>
  <S sid="67" ssid="67">(a) Extraposition The treatment accepted at present attaches the extraposed clause to the topmost node of the host (sentential) clause.</S>
  <S sid="68" ssid="68">Example: If initial analysis is: (It (is (necessary (for us to leave)))) Then change to standard as follows: (It (is necessary) (for us to leave)) NOTE: The fol lowing is not an example of extraposition, and therefore not to be modified, although it seems to differ only minimally from a genuine extraposition sentence such as: "It seemed like a good idea to begin early": (It (seemed (like ((a good meeting) (to begin early))))) (b) Modification of Noun Phrases The treatment accepted at present attaches the modified "core" noun phrase and all of its modifiers from a single (noun phrase) node: Example: If initial analysis is: ((((the tree (that (we saw))) (with (orange leaves))) (that (was (very old)))) Then change to standard as follows: ((the tree) (that (we saw)) (with (orange leaves)) (that (was (very old)))) (c) Sequences of Constituent-Initial Prepositions and~or Constituent-Final Particles For sequences of prepositions occurring at the start of a prepositional phrase, the currently accepted practice is to attach each individually to the preposit ional-phrase node.</S>
  <S sid="69" ssid="69">For sequences of particles which come at the end of a verb phrase or other constituent with a verbal head, the adopted standard is, likewise, to attach each individually to the top node of the constituent: Example: If initial analysis is: (We (were (out (of (oatmeal cookies))))) Then change to standard as follows: (We (were (out of (oatmeal cookies)))) ~.</S>
  <S sid="70" ssid="70">Computation of Evaluation Statistics (a) Number of Constituents Incompatible With Standard Parse For the sentence under analysis, compare the constituents as del imited by the standard parse with those del imited by the parse for evaluation.</S>
  <S sid="71" ssid="71">The first statistic computed for each sentence is the number of constituents in the parse being evaluated which "cross", i.e.</S>
  <S sid="72" ssid="72">are neither subsstrings nor superstrings of, the constituents of the standard parse.</S>
  <S sid="73" ssid="73">Example: Standard parse: ((The prospect) (of (cutting back spending))) Parse for evaluation: (The (prospect (of ((cutting back) spending)))) The (non-unary) constituents of the parse for evaluation are: 310 1.</S>
  <S sid="74" ssid="74">The prospect of cutting backspend ing 2. prospect of cutting back spending 3. of cutting back spending 4. cutting back spending 5. cutting back While both constituents 2 and 5 differ from the standard, only 2 qualif ies as a "crossing" violation, as 5 is merely a substring of a constituent of the standard parse.</S>
  <S sid="75" ssid="75">So the "Constituents Incompatible With Standard" score for this sentence is I.</S>
  <S sid="76" ssid="76">(b) "Recall" and "Precision" of Parse Being Evaluated As a preliminary to computing Recall: Number of Standard-Parse Constituents in Candidate Total Number of Standard-Parse Constituents and Precision: Number of Candidate-Parse Constituents in Standard Total Number of Candidate-Parse Constituents the total number of constituents in the standard parse, and in the candidate parse, are simply counted.</S>
  <S sid="77" ssid="77">Notice that "Number of Standard-Parse Constituents in Candidate" and "Number of Candidate-Parse Constituents in Standard" are merely different names for the same object--the intersection of the set of standard-parse constituents with the set of candidate-parse constituents.</S>
  <S sid="78" ssid="78">So the final count prel iminary to the computation of Recall and Precision is the number of elements in that intersection.</S>
  <S sid="79" ssid="79">To return to the first example of the last subsection: Standard parse: ((The prospect) (of (cutting back spending))) Parse for evaluation: (The (prospect (of ((cutting back) spending)))) there are  4 standard-parse constituents, if the convention is adopted of excluding unary constituents~ and 5 candidate-parse constituents, under the same convention.</S>
  <S sid="80" ssid="80">Three of these are common to both sets, i.e.</S>
  <S sid="81" ssid="81">the intersection here is 3.</S>
  <S sid="82" ssid="82">Computing Recall and Precision is accomplished for this parse as follows: Recall = 3 / Precision = 3 / 5 .</S>
  <S sid="83" ssid="83">(C) Combining Statistics Gathered In order to evaluate a set of parses, first simply compute a distribution over "Incompatible Constituents" scores for the parses in the set, e.g.</S>
  <S sid="84" ssid="84">Incompatible Constituents: 0 I 2 Frequency: 3 I I (Total = 5) Next, average the Recall and Precision scores for the various parses in the set, e.g.</S>
  <S sid="85" ssid="85">Average Recall = (3 /4  + 7 /8 + 2/4  + 518 + 314)  / 5 = .700 Average Precision = (3 /5 + 7 /10  + 2 /5  + 5 /10 + 315)  / 5 = .560 311</S>
</PAPER>
