<PAPER>
  <S sid="0">Collective Information Extraction With Relational Markov Networks</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Most information extraction (IE) systems treat separate potential extractions as independent.</S>
    <S sid="2" ssid="2">However, in many cases, considering influences potential extractions could improve overall accuracy.</S>
    <S sid="3" ssid="3">Statistical methods on models, such as random fields have been shown to be an effective approach to learning accurate IE systems.</S>
    <S sid="4" ssid="4">We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions.</S>
    <S sid="5" ssid="5">This allows for &amp;quot;collective information extraction&amp;quot; that exploits the mutual influence between possible extractions.</S>
    <S sid="6" ssid="6">Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">Information extraction (IE), locating references to specific types of items in natural-language documents, is an important task with many practical applications.</S>
    <S sid="8" ssid="2">Since IE systems are difficult and time-consuming to construct, most recent research has focused on empirical techniques that automatically construct information extractors by training on supervised corpora (Cardie, 1997; Califf, 1999).</S>
    <S sid="9" ssid="3">One of the current best empirical approaches to IE is conditional random fields (CRF's) (Lafferty et al., 2001).</S>
    <S sid="10" ssid="4">CRF's are a restricted class of undirected graphical models (Jordan, 1999) designed for sequence segmentation tasks such as IE, part-of-speech (POS) tagging (Lafferty et al., 2001), and shallow parsing (Sha and Pereira, 2003).</S>
    <S sid="11" ssid="5">In a recent follow-up to previously published experiments comparing a large variety of IE-learning methods (including HMM, SVM, MaxEnt, and rule-based methods) on the task of tagging references to human proteins in Medline abstracts (Bunescu et al., 2004), CRF's were found to significantly out-perform competing techniques.</S>
    <S sid="12" ssid="6">As typically applied, CRF's, like almost all IE methods, assume separate extractions are independent and treat each potential extraction in isolation.</S>
    <S sid="13" ssid="7">However, in many cases, considering influences between extractions can be very useful.</S>
    <S sid="14" ssid="8">For example, in our protein-tagging task, repeated references to the same protein are common.</S>
    <S sid="15" ssid="9">If the context surrounding one occurrence of a phrase is very indicative of it being a protein, then this should also influence the tagging of another occurrence of the same phrase in a different context which is not indicative of protein references.</S>
    <S sid="16" ssid="10">Relational Markov Networks (RMN's) (Taskar et al., 2002) are a generalization of CRF's that allow for collective classification of a set of related entities by integrating information from features of individual entities as well as the relations between them.</S>
    <S sid="17" ssid="11">Results on classifying connected sets of web pages have verified the advantage of this approach (Taskar et al., 2002).</S>
    <S sid="18" ssid="12">In this paper, we present an approach to collective information extraction using RMN's that simultaneously extracts all of the information from a document by exploiting the textual content and context of each relevant substring as well as the document relationships between them.</S>
    <S sid="19" ssid="13">Experiments on human protein tagging demonstrate the advantages of collective extraction on several annotated corpora of Medline abstracts.</S>
  </SECTION>
  <SECTION title="2 The RMN Framework for Entity Recognition" number="2">
    <S sid="20" ssid="1">Given a collection of documents D, we associate with each document dE Da set of candidate entities d.E, in our case a restricted set of token sequences from the document.</S>
    <S sid="21" ssid="2">Each entity e E d.E is characterized by a predefined set of boolean features e.F.</S>
    <S sid="22" ssid="3">This set of features is the same for all candidate entities, and it can be assimilated with the relational database definition of a table.</S>
    <S sid="23" ssid="4">One particular feature is e.label which is set to 1 if e is considered a valid extraction, and 0 otherwise.</S>
    <S sid="24" ssid="5">In this document model, labels are the only hidden features, and the inference procedure will try to find a most probable assignment of values to labels, given the current model parameters.</S>
    <S sid="25" ssid="6">Each document is associated with an undirected graphical model, with nodes corresponding directly to entity features, one node for each feature of each candidate entity in the document.</S>
    <S sid="26" ssid="7">The set of edges is created by matching clique templates against the entire set of entities d.E.</S>
    <S sid="27" ssid="8">A clique template is a procedure that finds all subsets of entities satisfying a given constraint, after which, for each entity subset, it connects a selected set of feature nodes so that they form a clique.</S>
    <S sid="28" ssid="9">Formally, there is a set of clique templates C, with each template c E C specified by: Given a set, E, of nodes, Mc(E) C 2E consists of subsets of entities whose feature nodes S, are to be connected in a clique.</S>
    <S sid="29" ssid="10">In previous applications of RMNs, the selected subsets of entities for a given template have the same size; however, our clique templates may match a variable number of entities.</S>
    <S sid="30" ssid="11">The set 5, may contain the same feature from different entities.</S>
    <S sid="31" ssid="12">Usually, for each entity in the matching set, its label is included in Sc.</S>
    <S sid="32" ssid="13">All these will be illustrated with examples in Sections 4 and 5 where the clique templates used in our model are described in detail.</S>
    <S sid="33" ssid="14">Depending on the number of hidden labels in we define two categories of clique templates: After the graph model for a document d has been completed with cliques from all templates, the probability distribution over the random field of hidden entity labels d.Y given the observed features d.X is computed as: The above distribution presents the RMN as a Markov random field (MRF) with the clique templates as a method for tying potential values across different cliques in the graphical model.</S>
  </SECTION>
  <SECTION title="3 Candidate Entities and Entity Features" number="3">
    <S sid="34" ssid="1">Like most entity names, almost all proteins in our data are base noun phrases or parts of them.</S>
    <S sid="35" ssid="2">Therefore, such substrings are used to determine candidate entities.</S>
    <S sid="36" ssid="3">To avoid missing options, we adopt a very broad definition of base noun phrase.</S>
    <S sid="37" ssid="4">Definition 1: A base noun phrase is a maximal contiguous sequence of tokens whose POS tags are from { &amp;quot;JJ&amp;quot;, &amp;quot;VBN&amp;quot;, &amp;quot;VBG&amp;quot;, &amp;quot;POS&amp;quot;, &amp;quot;NN&amp;quot;, &amp;quot;NNS&amp;quot;, &amp;quot;NNP&amp;quot;, &amp;quot;NNPS&amp;quot;, &amp;quot;CD&amp;quot;, &amp;quot;-&amp;quot;}, and whose last word (the head) is tagged either as a noun, or a number.</S>
    <S sid="38" ssid="5">Candidate extractions consist of base NPs, augmented with all their contiguous subsequences headed by a noun or number.</S>
    <S sid="39" ssid="6">The set of features associated with each candidate is based on the feature templates introduced in (Collins, 2002), used there for training a ranking algorithm on the extractions returned by a maximum-entropy tagger.</S>
    <S sid="40" ssid="7">Many of these features use the concept of word type, which allows a different form of token generalization than POS tags.</S>
    <S sid="41" ssid="8">The short type of a word is created by replacing any maximal contiguous sequences of capital letters with 'A', of lowercase letters with 'a', and of digits with '0'.</S>
    <S sid="42" ssid="9">For example, the word TGF-1 would be mapped to type A-0.</S>
    <S sid="43" ssid="10">Consequently, each token position i in a candidate extraction provides three types of information: the word itself wi, its POS tag t, and its short type si.</S>
    <S sid="44" ssid="11">The full set of features types is listed in Table 1, where we consider a generic elabel &#966;HD=enzyme elabel &#966;PF=A0_a ... &#966;SF=A0_a ... &#966;SF=a Note that the factor graph above has an equivalent RMN graph consisting of a one-node clique only, on which it is hard to visualize the various potentials involved.</S>
    <S sid="45" ssid="12">There are cases where different factor graphs may yield the same underlying RMN graph, which makes the factor graph representation preferable.</S>
  </SECTION>
  <SECTION title="5 Global Clique Templates" number="4">
    <S sid="46" ssid="1">Global clique templates enable us to model hypothesized influences between entities from the same document.</S>
    <S sid="47" ssid="2">They connect the label nodes of two or more entities, which, in the factor graph, translates into potential nodes connected to at least two label nodes.</S>
    <S sid="48" ssid="3">In our experiments we use three global templates: Overlap Template (OT): No two entity names overlap in the text i.e if the span of one entity is [Si, el] and the span of another entity is [82, e2], and Si &lt; 82, then el &lt; 82.</S>
    <S sid="49" ssid="4">Repeat Template (RT): If multiple entities in the same document are repetitions of the same name, their labels tend to have the same value (i.e. most of them are protein names, or most of them are not protein names).</S>
    <S sid="50" ssid="5">Later we discuss situations in which repetitions of the same protein name are not tagged as proteins, and design an approach to handle this.</S>
    <S sid="51" ssid="6">Acronym Template (AT): It is common convention that a protein is first introduced by its long name, immediately followed by its short-form (acronym) in parentheses.</S>
    <S sid="52" ssid="7">The definition of a candidate extraction from Section 3 leads to many overlapping entities.</S>
    <S sid="53" ssid="8">For example, 'glutathione S - transferase' is a base NP, and it generates five candidate extractions: 'glutathione', 'glutathione S', 'glutathione S - transferase', 'S - transferase', and 'transferase'.</S>
    <S sid="54" ssid="9">If 'gintathione S - transferase' has label-value 1, because the other four entities overlap with it, they should all have label-value 0.</S>
    <S sid="55" ssid="10">This type of constraint is enforced by the overlap template whose M operator matches any two overlapping candidate entities, and which connects their label nodes (specified in S) through a potential node with a potential function cb that allows at most one of them to have label-value 1, as illustrated in Table 2.</S>
    <S sid="56" ssid="11">Continuing with the previous example, because 'gintathione S' and 'S - transferase' are two overlapping entities, the factor graph model will contain an overlap potential node connected to the label nodes of these two entities.</S>
    <S sid="57" ssid="12">An alternative solution for the overlap template is to create a potential node for each token position that is covered by at least two candidate entities in the document, and connect it to their label nodes.</S>
    <S sid="58" ssid="13">The difference in this case is that the potential node will be connected to a variable number of entity label nodes.</S>
    <S sid="59" ssid="14">However this second approach has the advantage of creating fewer potential nodes in the document factor graph, which results in faster inference.</S>
    <S sid="60" ssid="15">We could specify the potential for the repeat template in a similar 2-by-2 table, this time leaving the table entries to be learned, given that it is not a hard constraint.</S>
    <S sid="61" ssid="16">However we can do better by noting that the vast majority of cases where a repeated protein name is not also tagged as a protein happens when it is part of a larger phrase that is tagged.</S>
    <S sid="62" ssid="17">For example, 'HDAC1 enzyme' is a protein name, therefore 'HDAC1' is not tagged in this phrase, even though it may have been tagged previously in the abstract where it was not followed by 'enzyme'.</S>
    <S sid="63" ssid="18">We need a potential that allows two entities with the same text to have different labels if the entity with label-value 0 is inside another entity with label-value 1.</S>
    <S sid="64" ssid="19">But a candidate entity may be inside more than one &amp;quot;including&amp;quot; entity, and the number of including entities may vary from one candidate extraction to another.</S>
    <S sid="65" ssid="20">Using the example from Section 5.1, the candidate entity 'glutathione' is included in two other entities: 'glutathione S' and 'glutathione S - transferase'.</S>
    <S sid="66" ssid="21">In order to instantiate potentials over variable number of label nodes, we introduce a logical OR clique template that matches a variable number of entities.</S>
    <S sid="67" ssid="22">When this template matches a subset of entities el, e2, ... , en, it will create an auxiliary OR entity e,, with a single feature e&#8222; .1 abel .</S>
    <S sid="68" ssid="23">The potential function is set so that it assigns a non-zero potential only when e, .1 abel = el .1 abel V e2.1abel V ...V en .1 abel .</S>
    <S sid="69" ssid="24">The cliques are only created as needed, e.g. when the auxiliary OR variable is required by repeat and acronym clique templates.</S>
    <S sid="70" ssid="25">Figure 3 shows the factor graph for a samverges, it gives a good approximation to the correct marginals.</S>
    <S sid="71" ssid="26">The algorithm works by altering the belief at each label node by repeatedly passing messages between the node and all potential nodes connected to it (Kschischang et al., 2001).</S>
    <S sid="72" ssid="27">As many of the label nodes are indirectly connected through potential nodes instantiated by global templates, their belief values will propagate in the graph and mutually influence each other, leading in the end to a collective labeling decision.</S>
    <S sid="73" ssid="28">The time complexity of computing messages from a potential node to a label node is exponential in the number of label nodes attached to the potential.</S>
    <S sid="74" ssid="29">Since this &amp;quot;fan-in&amp;quot; can be large for OR potential nodes, this step required optimization.</S>
    <S sid="75" ssid="30">Fortunately, due to the special form of the OR potential, and the normalization before each message-passing step, we were able to develop a linear-time algorithm for this special case.</S>
    <S sid="76" ssid="31">Details are omitted due to limited space.</S>
  </SECTION>
  <SECTION title="7 Learning Potentials in Factor Graphs" number="5">
    <S sid="77" ssid="1">Following a maximum likelihood estimation, we shall use the log-linear representation of potentials: where A is a vector of binary features, one for each configuration of values for X, and K. Let w be the concatenated vector of all potential parameters wc.</S>
    <S sid="78" ssid="2">One approach to finding the maximum-likelihood solution for w is to use a gradient-based method, which requires computing the gradient of the log-likelihood with respect to potential parameters wc.</S>
    <S sid="79" ssid="3">It can be shown that this gradient is equal with the difference between the empirical counts of fc and their expectation under the current set of parameters w. This expectation is expensive to compute, since it requires summing over all possible configurations of candidate entity labels from a given document.</S>
    <S sid="80" ssid="4">To circumvent this complexity, we use Collins' voted perceptron approach (Collins, 2002), which approximates the full expectation of fc with the fc counts for the most likely labeling under the current parameters, w. In all our experiments, the perceptron was run for 50 epochs, with a learning rate set at 0.01.</S>
  </SECTION>
  <SECTION title="8 Experimental Results" number="6">
    <S sid="81" ssid="1">We have tested the RMN approach on two datasets that have been hand-tagged for human protein names.</S>
    <S sid="82" ssid="2">The first dataset is Yapexl which consists of 200 Medline abstracts.</S>
    <S sid="83" ssid="3">Of these, 147 have been randomly selected by posing a query containing the (Mesh) terms protein binding, interaction, and molecular to Medline, while the rest of 53 have been extracted randomly from the GENIA corpus (Collier et al., 1999).</S>
    <S sid="84" ssid="4">It contains a total of 3713 protein references.</S>
    <S sid="85" ssid="5">The second dataset is Aimed2 which has been previously used for training the protein interaction extraction systems in (Bunescu et al., 2004).</S>
    <S sid="86" ssid="6">It consists of 225 Medline abstracts, of which 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction.</S>
    <S sid="87" ssid="7">There are 4084 protein references in this dataset.</S>
    <S sid="88" ssid="8">We compared the performance of three systems: LT-RMN is the RMN approach using local templates and the overlap template, GLT-RMN is the full RMN approach, using both local and global templates, and CRF, which uses a CRF for labeling token sequences.</S>
    <S sid="89" ssid="9">We used the CRF implementation from (McCallum, 2002) with the set of tags and features used by the MaximumEntropy tagger described in (Bunescu et al., 2004).</S>
    <S sid="90" ssid="10">All Medline abstracts were tokenized and then POS tagged using Brill's tagger (Brill, 1995).</S>
    <S sid="91" ssid="11">Each extracted protein name in the test data was compared to the human-tagged data, with the positions taken into account.</S>
    <S sid="92" ssid="12">Two extractions are considered a match if they consist of the same character sequence in the same position in the text.</S>
    <S sid="93" ssid="13">Results are shown in Tables 3 and 4 which give average precision, recall, and F-measure using 10-fold cross validation.</S>
    <S sid="94" ssid="14">These tables show that, in terms of Fmeasure, the use of global templates for modto improve a Maximum-Entropy tagger; however, these features do not fully capture the mutual influence between the labels of acronyms and their long forms, or between entity repetitions.</S>
    <S sid="95" ssid="15">In particular, they only allow earlier extractions in a document to influence later ones and not vice-versa.</S>
    <S sid="96" ssid="16">The RMN approach handles these and potentially other mutual influences between entities in a more complete, probabilistically sound manner.</S>
  </SECTION>
  <SECTION title="10 Conclusions and Future Work" number="7">
    <S sid="97" ssid="1">We have presented an approach to collective information extraction that uses Relational Markov Networks to reason about the mutual influences between multiple extractions.</S>
    <S sid="98" ssid="2">A new type of clique template &#8212; the logical OR template &#8212; was introduced, allowing a variable number of relevant entities to be used by other clique templates.</S>
    <S sid="99" ssid="3">Soft correlations between repetitions and acronyms and their long form in the same document have been captured by global clique templates, allowing for local extraction decisions to propagate and mutually influence each other.</S>
    <S sid="100" ssid="4">Regarding future work, a richer set of features for the local templates would likely improve performance.</S>
    <S sid="101" ssid="5">Currently, LT-RMN's accuracy is still significantly less than CRF's, which limits the performance of the full system.</S>
    <S sid="102" ssid="6">Another limitation is the approximate inference used by both RMN methods.</S>
    <S sid="103" ssid="7">The number of factor graphs for which the sum-product algorithm did not converge was non-negligible, and our approach stopped after a fix number of iterations.</S>
    <S sid="104" ssid="8">Besides exploring improvements to loopy belief propagation that increase computational cost (Yedidia et al., 2000), we intend to examine alternative approximate-inference methods.</S>
  </SECTION>
  <SECTION title="11 Acknowledgements" number="8">
    <S sid="105" ssid="1">This work was partially supported by grants IIS-0117308 and IIS-0325116 from the NSF.</S>
  </SECTION>
</PAPER>
