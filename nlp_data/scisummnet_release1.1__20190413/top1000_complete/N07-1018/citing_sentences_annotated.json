[
  {
    "citance_No": 1, 
    "citing_paper_id": "P07-1094", 
    "citing_paper_authority": 79, 
    "citing_paper_authors": "Sharon, Goldwater | Thomas L., Griffiths", 
    "raw_text": "However, as noted by Johnson et al (2007), this choice of? leads to difficulties with MAPestimation", 
    "clean_text": "However, as noted by Johnson et al (2007), this choice of beta leads to difficulties with MAP estimation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1027", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mark, Johnson | Anne, Christophe | Emmanuel, Dupoux | Katherine, Demuth", 
    "raw_text": "Adaptor grammars are a framework for Bayesianinference of a certain class of hierarchical non parametric models (Johnson et al, 2007b)", 
    "clean_text": "Adaptor grammars are a framework for Bayesian inference of a certain class of hierarchical nonparametric models (Johnson et al, 2007b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1027", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mark, Johnson | Anne, Christophe | Emmanuel, Dupoux | Katherine, Demuth", 
    "raw_text": "Adaptor Grammars are formally defined in Johnson et al (2007b), which should be consulted for technical details", 
    "clean_text": "Adaptor Grammars are formally defined in Johnson et al (2007b), which should be consulted for technical details.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P14-1027", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mark, Johnson | Anne, Christophe | Emmanuel, Dupoux | Katherine, Demuth", 
    "raw_text": "trees. There are several different procedures for infer ring the parse trees and the rule probabilities given a corpus of strings: Johnson et al (2007b) describe aMCMC sampler and Cohen et al (2010) describe a Variational Bayes procedure", 
    "clean_text": "There are several different procedures for inferring the parse trees and the rule probabilities given a corpus of strings: Johnson et al (2007b) describe a MCMC sampler and Cohen et al (2010) describe a Variational Bayes procedure.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P10-2057", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Trung H., Bui | Stanley, Peters", 
    "raw_text": "Hidden Markov Models (HMMs), a special case of DBNs, are a classical method for important NLP applications such as unsupervised part-of-speech tagging (Gael et al., 2009) and grammar induction (Johnson et al, 2007) as well as for ASR", 
    "clean_text": "Hidden Markov Models (HMMs), a special case of DBNs, are a classical method for important NLP applications such as unsupervised part-of-speech tagging (Gael et al., 2009) and grammar induction (Johnson et al, 2007) as well as for ASR.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "N09-1062", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Trevor, Cohn | Sharon, Goldwater | Philip, Blunsom", 
    "raw_text": "Our model is similar to the Adaptor Grammar model of Johnson et al (2007b), which is also a kind of Bayesian nonparametric tree-substitution grammar", 
    "clean_text": "Our model is similar to the Adaptor Grammar model of Johnson et al (2007b), which is also a kind of Bayesian nonparametric tree-substitution grammar.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "N09-1062", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Trevor, Cohn | Sharon, Goldwater | Philip, Blunsom", 
    "raw_text": "Our model is similar in this way to the Adap tor Grammar model of Johnson et al (2007a)", 
    "clean_text": "Our model is similar in this way to the Adaptor Grammar model of Johnson et al (2007a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N09-1062", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Trevor, Cohn | Sharon, Goldwater | Philip, Blunsom", 
    "raw_text": "Given a sample of (e,?,?) 5 we can reason over the space of possible trees using a Metropolis-Hastings sampler (Johnson et al, 2007a) coupled with a Monte Carlo integral (Bod, 2003)", 
    "clean_text": "Given a sample, we can reason over the space of possible trees using a Metropolis-Hastings sampler (Johnson et al, 2007a) coupled with a Monte Carlo integral (Bod, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N09-1062", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Trevor, Cohn | Sharon, Goldwater | Philip, Blunsom", 
    "raw_text": "A natural proposal distribution, p? (d|w), is the maximum a posterior (MAP) grammar given the elementary tree analysis of our training set (analogous to the PCFG approximation used in Johnson et al (2007a))", 
    "clean_text": "A natural proposal distribution, p(d|w), is the maximum a posterior (MAP) grammar given the elementary tree analysis of our training set (analogous to the PCFG approximation used in Johnson et al (2007a)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W09-1114", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Abhishek, Arun | Chris, Dyer | Barry, Haddow | Philip, Blunsom | Adam, Lopez | Philipp, Koehn", 
    "raw_text": "To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al (2007a)", 
    "clean_text": "To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al (2007a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "N09-1036", 
    "citing_paper_authority": 34, 
    "citing_paper_authors": "Mark, Johnson | Sharon, Goldwater", 
    "raw_text": "This section informally introduces adaptor gram mars using unsupervised word segmentation as a motivating application; see Johnson et al (2007b) for a formal definition of adaptor grammars", 
    "clean_text": "This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al (2007b) for a formal definition of adaptor grammars.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N09-1036", 
    "citing_paper_authority": 34, 
    "citing_paper_authors": "Mark, Johnson | Sharon, Goldwater", 
    "raw_text": "is a PCFG with nonterminals N, terminals W, rules R, start symbol S? N and rule probabilities?, where? r is the probability of rule r? R, A? N is the set of adapted nonterminals and C is a vector of adaptors indexed by elements of A, so CX is the adaptor for adapted nonterminal X? A. Informally, an adaptor CX nondeterministically maps a stream of trees from a base distribution HX whose support is TX (the set of subtrees whose root node is X? N generated by the grammar? s rules) into another stream of trees whose support is also TX. Inadaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al (2007b)", 
    "clean_text": "In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al (2007b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N09-1036", 
    "citing_paper_authority": 34, 
    "citing_paper_authors": "Mark, Johnson | Sharon, Goldwater", 
    "raw_text": "Johnson et al (2007a) describe Gibbs samplers for Bayesian inference of PCFG rule probabilities?, and these techniques can be used directly with adap tor grammars as well", 
    "clean_text": "Johnson et al (2007a) describe Gibbs samplers for Bayesian inference of PCFG rule probabilities, and these techniques can be used directly with adaptor grammars as well.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N09-1036", 
    "citing_paper_authority": 34, 
    "citing_paper_authors": "Mark, Johnson | Sharon, Goldwater", 
    "raw_text": "described in Johnson et al (2007a)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N09-1036", 
    "citing_paper_authority": 34, 
    "citing_paper_authors": "Mark, Johnson | Sharon, Goldwater", 
    "raw_text": "Theadap tor grammar algorithm described in Johnson et al (2007b) repeatedly re samples parses for the sentences of the training data", 
    "clean_text": "The adaptor grammar algorithm described in Johnson et al (2007b) repeatedly resamples parses for the sentences of the training data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P13-1077", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Trevor, Cohn | Gholamreza, Haffari", 
    "raw_text": "This is closely related to adaptor grammars (Johnson et al., 2007a), which also generate full tree rewrite sin a monolingual setting", 
    "clean_text": "This is closely related to adaptor grammars (Johnson et al., 2007a), which also generate full tree rewrites in a monolingual setting.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P13-1077", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Trevor, Cohn | Gholamreza, Haffari", 
    "raw_text": "This work was inspired by adaptor grammars (Johnson et al, 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree", 
    "clean_text": "This work was inspired by adaptor grammars (Johnson et al, 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P13-1077", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Trevor, Cohn | Gholamreza, Haffari", 
    "raw_text": "The full grammar transform for inside inference is shown in Table 1. The sampling algorithm closely follows the process for sampling derivations from BayesianPCFGs (Johnson et al, 2007b)", 
    "clean_text": "The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs (Johnson et al, 2007b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P09-1009", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Benjamin, Snyder | Tahira, Naseem | Regina, Barzilay", 
    "raw_text": "The samples form a Markov chain which is guaranteed to converge to the true joint distribution over all sentences. In the monolingual setting, there is a well known tree sampling algorithm (Johnson et al,2007)", 
    "clean_text": "In the monolingual setting, there is a well known tree sampling algorithm (Johnson et al,2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P11-1145", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Ivan, Titov | Alexandre, Klementiev", 
    "raw_text": "However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large", 
    "clean_text": "However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large.", 
    "keep_for_gold": 0
  }
]
