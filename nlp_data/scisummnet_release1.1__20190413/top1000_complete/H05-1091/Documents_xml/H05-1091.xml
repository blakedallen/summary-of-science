<PAPER>
	<S sid="0">A Shortest Path Dependency Kernel For Relation Extraction</S><ABSTRACT>
		<S sid="1" ssid="1">We present a novel approach to relation extraction, based on the observation thatthe information required to assert a rela tionship between two named entities in the same sentence is typically capturedby the shortest path between the two entities in the dependency graph.</S>
		<S sid="2" ssid="2">Exper iments on extracting top-level relationsfrom the ACE (Automated Content Ex traction) newspaper corpus show that thenew shortest path dependency kernel outperforms a recent approach based on de pendency tree kernels.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="3" ssid="3">One of the key tasks in natural language process ing is that of Information Extraction (IE), which istraditionally divided into three subproblems: coref erence resolution, named entity recognition, and relation extraction.</S>
			<S sid="4" ssid="4">Consequently, IE corpora are typically annotated with information corresponding to these subtasks (MUC (Grishman, 1995), ACE(NIST, 2000)), facilitating the development of sys tems that target only one or a subset of the threeproblems.</S>
			<S sid="5" ssid="5">In this paper we focus exclusively on extracting relations between predefined types of entities in the ACE corpus.</S>
			<S sid="6" ssid="6">Reliably extracting relations between entities in natural-language docu ments is still a difficult, unsolved problem, whose inherent difficulty is compounded by the emergenceof new application domains, with new types of nar rative that challenge systems developed for previouswell-studied domains.</S>
			<S sid="7" ssid="7">The accuracy level of current syntactic and semantic parsers on natural lan guage text from different domains limit the extent to which syntactic and semantic information can be used in real IE systems.</S>
			<S sid="8" ssid="8">Nevertheless, various linesof work on relation extraction have shown experimentally that the use of automatically derived syntactic information can lead to significant improvements in extraction accuracy.</S>
			<S sid="9" ssid="9">The amount of syntactic knowledge used in IE systems varies from part of-speech only (Ray and Craven, 2001) to chunking(Ray and Craven, 2001) to shallow parse trees (Ze lenko et al, 2003) to dependency trees derived from full parse trees (Culotta and Sorensen, 2004).</S>
			<S sid="10" ssid="10">Eventhough exhaustive experiments comparing the per formance of a relation extraction system based on these four levels of syntactic information are yet tobe conducted, a reasonable assumption is that the extraction accuracy increases with the amount of syn tactic information used.</S>
			<S sid="11" ssid="11">The performance howeverdepends not only on the amount of syntactic infor mation, but also on the details of the exact modelsusing this information.</S>
			<S sid="12" ssid="12">Training a machine learn ing system in a setting where the information usedfor representing the examples is only partially rele vant to the actual task often leads to overfitting.</S>
			<S sid="13" ssid="13">It is therefore important to design the IE system so that the input data is stripped of unnecessary features as much as possible.</S>
			<S sid="14" ssid="14">In the case of the tree kernels from (Zelenko et al, 2003; Culotta and Sorensen, 2004), the authors reduce each relation example to the smallest subtree in the parse or dependency tree that includes both entities.</S>
			<S sid="15" ssid="15">We will show in this paper that increased extraction performance can be 724 obtained by designing a kernel method that uses an even smaller part of the dependency structure ? theshortest path between the two entities in the undi rected version of the dependency graph.</S>
	</SECTION>
	<SECTION title="Dependency Graphs. " number="2">
			<S sid="16" ssid="1">Let e1 and e2 be two entities mentioned in the samesentence such that they are observed to be in a re lationship R, i.e R(e1, e2) = 1.</S>
			<S sid="17" ssid="2">For example, R can specify that entity e1 is LOCATED (AT) entity e2.</S>
			<S sid="18" ssid="3">Figure 1 shows two sample sentences from ACE, with entity mentions in bold.</S>
			<S sid="19" ssid="4">Correspondingly, the first column in Table 1 lists the four relations of typeLOCATED that need to be extracted by the IE sys tem.</S>
			<S sid="20" ssid="5">We assume that a relation is to be extractedonly between entities mentioned in the same sen tence, and that the presence or absence of a relation is independent of the text preceding or following the sentence.</S>
			<S sid="21" ssid="6">This means that only information derived from the sentence including the two entities will be relevant for relation extraction.</S>
			<S sid="22" ssid="7">Furthermore, with each sentence we associate its dependency graph,with words figured as nodes and word-word dependencies figured as directed edges, as shown in Fig ure 1.</S>
			<S sid="23" ssid="8">A subset of these word-word dependencies capture the predicate-argument relations present inthe sentence.</S>
			<S sid="24" ssid="9">Arguments are connected to their target predicates either directly through an arc point ing to the predicate (?troops ? raided?), or indirectly through a preposition or infinitive particle (?warning?</S>
			<S sid="25" ssid="10">to ? stop?).</S>
			<S sid="26" ssid="11">Other types of word-word dependen cies account for modifier-head relationships present in adjective-noun compounds (?several ? stations?), noun-noun compounds (?pumping ? stations?), or adverb-verb constructions (?recently ? raided?).</S>
			<S sid="27" ssid="12">In Figure 1 we show the full dependency graphs for two sentences from the ACE newspaper corpus.Word-word dependencies are typically catego rized in two classes as follows:?</S>
			<S sid="28" ssid="13">[Local Dependencies] These correspond to local predicate-argument (or head-modifier) constructions such as ?troops ? raided?, or ?pump ing ? stations?</S>
			<S sid="29" ssid="14">in Figure 1.?</S>
			<S sid="30" ssid="15">[Non-local Dependencies] Long-distance dependencies arise due to various linguistic con structions such as coordination, extraction,raising and control.</S>
			<S sid="31" ssid="16">In Figure 1, among non local dependencies are ?troops ? warning?, or ?ministers ? preaching?.</S>
			<S sid="32" ssid="17">A Context Free Grammar (CFG) parser can be used to extract local dependencies, which for each sentence form a dependency tree.</S>
			<S sid="33" ssid="18">Mildly contextsensitive formalisms such as Combinatory Categorial Grammar (CCG) (Steedman, 2000) model word word dependencies more directly and can be used to extract both local and long-distance dependencies, giving rise to a directed acyclic graph, as illustrated in Figure 1.</S>
	</SECTION>
	<SECTION title="The Shortest Path Hypothesis. " number="3">
			<S sid="34" ssid="1">If e1 and e2 are two entities mentioned in the samesentence such that they are observed to be in a relationship R, our hypothesis stipulates that the contribution of the sentence dependency graph to establishing the relationship R(e1, e2) is almost exclu sively concentrated in the shortest path between e1 and e2 in the undirected version of the dependency graph.</S>
			<S sid="35" ssid="2">If entities e1 and e2 are arguments of the same predicate, then the shortest path between them willpass through the predicate, which may be con nected directly to the two entities, or indirectly through prepositions.</S>
			<S sid="36" ssid="3">If e1 and e2 belong to different predicate-argument structures that share a common argument, then the shortest path will pass through this argument.</S>
			<S sid="37" ssid="4">This is the case with the shortest pathbetween ?stations?</S>
			<S sid="38" ssid="5">and ?workers?</S>
			<S sid="39" ssid="6">in Figure 1, passing through ?protesters?, which is an argument com mon to both predicates ?holding?</S>
			<S sid="40" ssid="7">and ?seized?.</S>
			<S sid="41" ssid="8">In Table 1 we show the paths corresponding to the four relation instances encoded in the ACE corpus for thetwo sentences from Figure 1.</S>
			<S sid="42" ssid="9">All these paths sup port the LOCATED relationship.</S>
			<S sid="43" ssid="10">For the first path, it is reasonable to infer that if a PERSON entity (e.g. ?protesters?)</S>
			<S sid="44" ssid="11">is doing some action (e.g. ?seized?)</S>
			<S sid="45" ssid="12">to a FACILITY entity (e.g. ?station?), then the PERSONentity is LOCATED at that FACILITY entity.</S>
			<S sid="46" ssid="13">The sec ond path captures the fact that the same PERSON entity (e.g. ?protesters?)</S>
			<S sid="47" ssid="14">is doing two actions (e.g.?holding?</S>
			<S sid="48" ssid="15">and ?seized?)</S>
			<S sid="49" ssid="16">, one action to a PERSON entity (e.g. ?workers?), and the other action to a FACIL ITY entity (e.g. ?station?).</S>
			<S sid="50" ssid="17">A reasonable inference in this case is that the ?workers?</S>
			<S sid="51" ssid="18">are LOCATED at the 725 S1 = =S2 Protesters stations workers Troops churches ministers seized several pumping , holding 127 Shell hostage . recently have raided , warning to stop preaching . Figure 1: Sentences as dependency graphs.</S>
			<S sid="52" ssid="19">Relation Instance Shortest Path in Undirected Dependency Graph S1: protesters AT stations protesters ??</S>
			<S sid="53" ssid="20">seized ??</S>
			<S sid="54" ssid="21">stations S1: workers AT stations workers ??</S>
			<S sid="55" ssid="22">holding ??</S>
			<S sid="56" ssid="23">protesters ??</S>
			<S sid="57" ssid="24">seized ??</S>
			<S sid="58" ssid="25">stations S2: troops AT churches troops ??</S>
			<S sid="59" ssid="26">raided ??</S>
			<S sid="60" ssid="27">churches S2: ministers AT churches ministers ??</S>
			<S sid="61" ssid="28">warning ??</S>
			<S sid="62" ssid="29">troops ??</S>
			<S sid="63" ssid="30">raided ??</S>
			<S sid="64" ssid="31">churches Table 1: Shortest Path representation of relations.</S>
			<S sid="65" ssid="32">?station?.</S>
			<S sid="66" ssid="33">In Figure 2 we show three more examples of the LOCATED (AT) relationship as dependency pathscreated from one or two predicate-argument struc tures.</S>
			<S sid="67" ssid="34">The second example is an interesting case,as it illustrates how annotation decisions are accom modated in our approach.</S>
			<S sid="68" ssid="35">Using a reasoning similarwith that from the previous paragraph, it is reason able to infer that ?troops?</S>
			<S sid="69" ssid="36">are LOCATED in ?vans?, and that ?vans?</S>
			<S sid="70" ssid="37">are LOCATED in ?city?.</S>
			<S sid="71" ssid="38">However, because ?vans?</S>
			<S sid="72" ssid="39">is not an ACE markable, it cannot participate in an annotated relationship.</S>
			<S sid="73" ssid="40">Therefore, ?troops?</S>
			<S sid="74" ssid="41">is annotated as being LOCATED in ?city?,which makes sense due to the transitivity of the relation LOCATED.</S>
			<S sid="75" ssid="42">In our approach, this leads to shortest paths that pass through two or more predicate argument structures.The last relation example is a case where there ex ist multiple shortest paths in the dependency graph between the same two entities ? there are actually two different paths, with each path replicated into three similar paths due to coordination.</S>
			<S sid="76" ssid="43">Our current approach considers only one of the shortest paths, nevertheless it seems reasonable to investigate usingall of them as multiple sources of evidence for rela tion extraction.</S>
			<S sid="77" ssid="44">There may be cases where e1 and e2 belongto predicate-argument structures that have no argument in common.</S>
			<S sid="78" ssid="45">However, because the dependency graph is always connected, we are guaranteed to find a shortest path between the two enti ties.</S>
			<S sid="79" ssid="46">In general, we shall find a shortest sequence of predicate-argument structures with target predicates P1, P2, ..., Pn such that e1 is an argument of P1, e2 isan argument of Pn, and any two consecutive predi cates Pi and Pi+1 share a common argument (whereby ?argument?</S>
			<S sid="80" ssid="47">we mean both arguments and com plements).</S>
	</SECTION>
	<SECTION title="Learning with Dependency Paths. " number="4">
			<S sid="81" ssid="1">The shortest path between two entities in a depen dency graph offers a very condensed representationof the information needed to assess their relationship.</S>
			<S sid="82" ssid="2">A dependency path is represented as a sequence of words interspersed with arrows that in 726 (1) He had no regrets for his actions in Brcko.</S>
			<S sid="83" ssid="3">his?</S>
			<S sid="84" ssid="4">actions?</S>
			<S sid="85" ssid="5">in?</S>
			<S sid="86" ssid="6">Brcko (2) U.S. troops today acted for the first time to capture an alleged Bosnian war criminal, rushing from unmarked vans parked in the northern Serb-dominated city of Bijeljina.</S>
			<S sid="87" ssid="7">troops?</S>
			<S sid="88" ssid="8">rushing?</S>
			<S sid="89" ssid="9">from?</S>
			<S sid="90" ssid="10">vans?</S>
			<S sid="91" ssid="11">parked?</S>
			<S sid="92" ssid="12">in?</S>
			<S sid="93" ssid="13">city (3) Jelisic created an atmosphere of terror at the camp by killing, abusing and threatening the detainees.</S>
			<S sid="94" ssid="14">detainees?</S>
			<S sid="95" ssid="15">killing?</S>
			<S sid="96" ssid="16">Jelisic?</S>
			<S sid="97" ssid="17">created?</S>
			<S sid="98" ssid="18">at?</S>
			<S sid="99" ssid="19">camp detainees?</S>
			<S sid="100" ssid="20">abusing?</S>
			<S sid="101" ssid="21">Jelisic?</S>
			<S sid="102" ssid="22">created?</S>
			<S sid="103" ssid="23">at?</S>
			<S sid="104" ssid="24">camp detainees?</S>
			<S sid="105" ssid="25">threatning?</S>
			<S sid="106" ssid="26">Jelisic?</S>
			<S sid="107" ssid="27">created?</S>
			<S sid="108" ssid="28">at?</S>
			<S sid="109" ssid="29">camp detainees?</S>
			<S sid="110" ssid="30">killing?</S>
			<S sid="111" ssid="31">by?</S>
			<S sid="112" ssid="32">created?</S>
			<S sid="113" ssid="33">at?</S>
			<S sid="114" ssid="34">camp detainees?</S>
			<S sid="115" ssid="35">abusing?</S>
			<S sid="116" ssid="36">by?</S>
			<S sid="117" ssid="37">created?</S>
			<S sid="118" ssid="38">at?</S>
			<S sid="119" ssid="39">camp detainees?</S>
			<S sid="120" ssid="40">threatening?</S>
			<S sid="121" ssid="41">by?</S>
			<S sid="122" ssid="42">created?</S>
			<S sid="123" ssid="43">at?</S>
			<S sid="124" ssid="44">camp Figure 2: Relation examples.dicate the orientation of each dependency, as illustrated in Table 1.</S>
			<S sid="125" ssid="45">These paths however are completely lexicalized and consequently their performance will be limited by data sparsity.</S>
			<S sid="126" ssid="46">We can al leviate this by categorizing words into classes with varying degrees of generality, and then allowing paths to use both words and their classes.</S>
			<S sid="127" ssid="47">Examples of word classes are part-of-speech (POS) tags and generalizations over POS tags such as Noun, Active Verb or Passive Verb.</S>
			<S sid="128" ssid="48">The entity type is also used forthe two ends of the dependency path.</S>
			<S sid="129" ssid="49">Other poten tially useful classes might be created by associatingwith each noun or verb a set of hypernyms corre sponding to their synsets in WordNet.</S>
			<S sid="130" ssid="50">The set of features can then be defined as aCartesian product over these word classes, as illus trated in Figure 3 for the dependency path between?protesters?</S>
			<S sid="131" ssid="51">and ?station?</S>
			<S sid="132" ssid="52">in sentence S1.</S>
			<S sid="133" ssid="53">In this rep resentation, sparse or contiguous subsequences of nodes along the lexicalized dependency path (i.e. path fragments) are included as features simply byreplacing the rest of the nodes with their correspond ing generalizations.The total number of features generated by this de pendency path is 4?1?3?1?4, and some of them are listed in Table 2.</S>
			<S sid="134" ssid="54">protesters NNS Noun PERSON ? ?</S>
			<S sid="135" ssid="55">[ seized VBD Verb ] ? [?]?</S>
			<S sid="136" ssid="56">stations NNS Noun FACILITY ? ?</S>
			<S sid="137" ssid="57">Figure 3: Feature generation from dependency path.</S>
			<S sid="138" ssid="58">protesters ? seized ? stations Noun ? Verb ? Noun PERSON ? seized ? FACILITY PERSON ? Verb ? FACILITY ...</S>
			<S sid="139" ssid="59">(48 features) Table 2: Sample Features.</S>
			<S sid="140" ssid="60">For verbs and nouns (and their respective word classes) occurring along a dependency path we also use an additional suffix ?(-)?</S>
			<S sid="141" ssid="61">to indicate a negative polarity item.</S>
			<S sid="142" ssid="62">In the case of verbs, this suffix is usedwhen the verb (or an attached auxiliary) is modi fied by a negative polarity adverb such as ?not?</S>
			<S sid="143" ssid="63">or ?never?.</S>
			<S sid="144" ssid="64">Nouns get the negative suffix whenever they are modified by negative determiners such as ?no?, ?neither?</S>
			<S sid="145" ssid="65">or ?nor?.</S>
			<S sid="146" ssid="66">For example, the phrase ?Henever went to Paris?</S>
			<S sid="147" ssid="67">is associated with the depen dency path ?He ? went(-) ? to ? Paris?.</S>
			<S sid="148" ssid="68">Explicitly creating for each relation example avector with a position for each dependency path fea ture is infeasible, due to the high dimensionality ofthe feature space.</S>
			<S sid="149" ssid="69">Here we can exploit dual learning algorithms that process examples only via computing their dot-products, such as the Support Vec tor Machines (SVMs) (Vapnik, 1998; Cristianiniand Shawe-Taylor, 2000).</S>
			<S sid="150" ssid="70">These dot-products be tween feature vectors can be efficiently computed through a kernel function, without iterating over allthe corresponding features.</S>
			<S sid="151" ssid="71">Given the kernel func tion, the SVM learner tries to find a hyperplane that separates positive from negative examples and at thesame time maximizes the separation (margin) be tween them.</S>
			<S sid="152" ssid="72">This type of max-margin separator hasbeen shown both theoretically and empirically to re sist overfitting and to provide good generalization performance on unseen examples.</S>
			<S sid="153" ssid="73">Computing the dot-product (i.e. kernel) between two relation examples amounts to calculating the 727 number of common features of the type illustrated in Table 2.</S>
			<S sid="154" ssid="74">If x = x1x2...xm and y = y1y2...yn are two relation examples, where xi denotes the set ofword classes corresponding to position i (as in Fig ure 3), then the number of common features between x and y is computed as in Equation 1.</S>
			<S sid="155" ssid="75">K(x, y) = { 0, m 6= n ?n i=1 c(xi, yi), m = n (1) where c(xi, yi) = |xi?yi| is the number of common word classes between xi and yi.</S>
			<S sid="156" ssid="76">This is a simple kernel, whose computation takes O(n) time.</S>
			<S sid="157" ssid="77">If the two paths have different lengths,they correspond to different ways of expressing a re lationship ? for instance, they may pass through a different number of predicate argument structures.</S>
			<S sid="158" ssid="78">Consequently, the kernel is defined to be 0 in this case.</S>
			<S sid="159" ssid="79">Otherwise, it is the product of the number of common word classes at each position in the two paths.</S>
			<S sid="160" ssid="80">As an example, let us consider two instances of the LOCATED relationship: 1.</S>
			<S sid="161" ssid="81">?his actions in Brcko?, and 2.</S>
			<S sid="162" ssid="82">?his arrival in Beijing?.</S>
			<S sid="163" ssid="83">Their corresponding dependency paths are: 1.</S>
			<S sid="164" ssid="84">?his ? actions ? in ? Brcko?, and 2.</S>
			<S sid="165" ssid="85">?his ? arrival ? in ? Beijing?.</S>
			<S sid="166" ssid="86">Their representation as a sequence of sets of word classes is given by: 1.</S>
			<S sid="167" ssid="87">x = [x1 x2 x3 x4 x5 x6 x7], where x1 = {his, PRP, PERSON}, x2 = {?}, x3 = {actions, NNS, Noun}, x4 = {?}, x5 = {in, IN}, x6 = {?}, x7 = {Brcko, NNP, Noun, LOCATION} 2.</S>
			<S sid="168" ssid="88">y = [y1 y2 y3 y4 y5 y6 y7], where y1 = {his, PRP, PERSON}, y2 = {?}, y3 = {arrival, NN, Noun}, y4 = {?}, y5 = {in, IN}, y6 = {?}, y7 = {Beijing, NNP, Noun, LOCATION} Based on the formula from Equation 1, the kernel is computed as K(x, y) = 3?1?1?1?2?1?3 = 18.</S>
			<S sid="169" ssid="89">We use this relation kernel in conjunction with SVMs in order to find decision hyperplanes that best separate positive examples from negative examples.We modified the LibSVM1 package for SVM learn ing by plugging in the kernel described above, and used its default one-against-one implementation for multiclass classification.</S>
	</SECTION>
	<SECTION title="Experimental Evaluation. " number="5">
			<S sid="170" ssid="1">We applied the shortest path dependency kernel to the problem of extracting top-level relations from the ACE corpus (NIST, 2000), the version used for the September 2002 evaluation.</S>
			<S sid="171" ssid="2">The training part of this dataset consists of 422 documents, witha separate set of 97 documents allocated for test ing.</S>
			<S sid="172" ssid="3">This version of the ACE corpus contains three types of annotations: coreference, named entities and relations.</S>
			<S sid="173" ssid="4">Entities can be of the type PERSON,ORGANIZATION, FACILITY, LOCATION, and GEO POLITICAL ENTITY.</S>
			<S sid="174" ssid="5">There are 5 general, top-levelrelations: ROLE, PART, LOCATED, NEAR, and SOCIAL.</S>
			<S sid="175" ssid="6">The ROLE relation links people to an organization to which they belong, own, founded, or provide some service.</S>
			<S sid="176" ssid="7">The PART relation indicates subset relationships, such as a state to a nation, or a subsidiary to its parent company.</S>
			<S sid="177" ssid="8">The AT relation indi cates the location of a person or organization at somelocation.</S>
			<S sid="178" ssid="9">The NEAR relation indicates the proximity of one location to another.</S>
			<S sid="179" ssid="10">The SOCIAL relation links two people in personal, familial or profes sional relationships.</S>
			<S sid="180" ssid="11">Each top-level relation type is further subdivided into more fine-grained subtypes,resulting in a total of 24 relation types.</S>
			<S sid="181" ssid="12">For exam ple, the LOCATED relation includes subtypes such as LOCATED-AT, BASED-IN, and RESIDENCE.</S>
			<S sid="182" ssid="13">In total, there are 7,646 intra-sentential relations, of which 6,156 are in the training data and 1,490 in the test data.</S>
			<S sid="183" ssid="14">We assume that the entities and their labels areknown.</S>
			<S sid="184" ssid="15">All preprocessing steps ? sentence segmentation, tokenization, and POS tagging ? were per formed using the OpenNLP2 package.</S>
			<S sid="185" ssid="16">5.1 Extracting dependencies using a CCG.</S>
			<S sid="186" ssid="17">parser CCG (Steedman, 2000) is a type-driven theory of grammar where most language-specific aspects ofthe grammar are specified into lexicon.</S>
			<S sid="187" ssid="18">To each lex 1URL:http://www.csie.ntu.edu.tw/?cjlin/libsvm/ 2URL: http://opennlp.sourceforge.net 728 ical item corresponds a set of syntactic categories specifying its valency and the directionality of itsarguments.</S>
			<S sid="188" ssid="19">For example, the words from the sen tence ?protesters seized several stations?</S>
			<S sid="189" ssid="20">are mapped in the lexicon to the following categories: protesters : NP seized : (S\NP )/NP several : NP/NP stations : NP The transitive verb ?seized?</S>
			<S sid="190" ssid="21">expects two arguments: a noun phrase to the right (the object) and another noun phrase to the left (the subject).</S>
			<S sid="191" ssid="22">Similarly, the adjective ?several?</S>
			<S sid="192" ssid="23">expects a noun phrase to its right.</S>
			<S sid="193" ssid="24">Depending on whether its valency is greater than zero or not, a syntactic category is called a functor or an argument.</S>
			<S sid="194" ssid="25">In the example above, ?seized?</S>
			<S sid="195" ssid="26">and?several?</S>
			<S sid="196" ssid="27">are functors, while ?protesters?</S>
			<S sid="197" ssid="28">and ?sta tions?</S>
			<S sid="198" ssid="29">are arguments.</S>
			<S sid="199" ssid="30">Syntactic categories are combined using a smallset of typed combinatory rules such as functional ap plication, composition and type raising.</S>
			<S sid="200" ssid="31">In Table 3we show a sample derivation based on three func tional applications.</S>
			<S sid="201" ssid="32">protesters seized several stations NP (S\NP )/NP NP/NP NP NP (S\NP )/NP NP NP S\NP S Table 3: Sample derivation.In order to obtain CCG derivations for all sen tences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002)3.</S>
			<S sid="202" ssid="33">This parser also outputs a list of dependen cies, with each dependency represented as a 4-tuple ?f, a, wf , wa?, where f is the syntactic category of the functor, a is the argument number, wf is the head word of the functor, and wa is the head word of theargument.</S>
			<S sid="203" ssid="34">For example, the three functional appli cations from Table 3 result in the functor-argument dependencies enumerated below in Table 4.</S>
			<S sid="204" ssid="35">3URL:http://www.ircs.upenn.edu/?juliahr/Parser/ f a wf wa NP/NP 1 ?several?</S>
			<S sid="205" ssid="36">?stations?</S>
			<S sid="206" ssid="37">(S\NP )/NP 2 ?seized?</S>
			<S sid="207" ssid="38">?stations?</S>
			<S sid="208" ssid="39">(S\NP )/NP 1 ?seized?</S>
			<S sid="209" ssid="40">?protesters?</S>
			<S sid="210" ssid="41">Table 4: Sample dependencies.</S>
			<S sid="211" ssid="42">Because predicates (e.g. ?seized?)</S>
			<S sid="212" ssid="43">and adjuncts(e.g. ?several?)</S>
			<S sid="213" ssid="44">are always represented as functors, while complements (e.g. ?protesters?</S>
			<S sid="214" ssid="45">and ?sta tions?)</S>
			<S sid="215" ssid="46">are always represented as arguments, it isstraightforward to transform a functor-argument de pendency into a head-modifier dependency.</S>
			<S sid="216" ssid="47">The head-modifier dependencies corresponding to the three functor-argument dependencies in Table 4 are:?protesters ? seized?, ?stations ? seized?, and ?sev eral ? stations?.</S>
			<S sid="217" ssid="48">Special syntactic categories are assigned in CCGto lexical items that project unbounded dependen cies, such as the relative pronouns ?who?, ?which?</S>
			<S sid="218" ssid="49">and ?that?.</S>
			<S sid="219" ssid="50">Coupled with a head-passing mechanism, these categories allow the extraction of long-range dependencies.</S>
			<S sid="220" ssid="51">Together with the local word-worddependencies, they create a directed acyclic depen dency graph for each parsed sentence, as shown in Figure 1.</S>
			<S sid="221" ssid="52">5.2 Extracting dependencies using a CFG.</S>
			<S sid="222" ssid="53">parser Local dependencies can be extracted from a CFG parse tree using simple heuristic rules for findingthe head child for each type of constituent.</S>
			<S sid="223" ssid="54">Alter natively, head-modifier dependencies can be directly output by a parser whose model is based on lexical dependencies.</S>
			<S sid="224" ssid="55">In our experiments, we used the full parse output from Collins?</S>
			<S sid="225" ssid="56">parser (Collins, 1997), in which every non-terminal node is already annotated with head information.</S>
			<S sid="226" ssid="57">Because local dependencies assemble into a tree for each sentence, there is onlyone (shortest) path between any two entities in a de pendency tree.</S>
			<S sid="227" ssid="58">5.3 Experimental Results.</S>
			<S sid="228" ssid="59">A recent approach to extracting relations is described in (Culotta and Sorensen, 2004).</S>
			<S sid="229" ssid="60">The au thors use a generalized version of the tree kernel from (Zelenko et al, 2003) to compute a kernel over 729 relation examples, where a relation example consists of the smallest dependency tree containing the two entities of the relation.</S>
			<S sid="230" ssid="61">Precision and recall values are reported for the task of extracting the 5 top-levelrelations in the ACE corpus under two different sce narios: ? [S1] This is the classic setting: one multi-classSVM is learned to discriminate among the 5 top level classes, plus one more class for the no-relation cases.?</S>
			<S sid="231" ssid="62">[S2] Because of the highly skewed data distribution, the recall of the SVM approach in the first sce nario is very low.</S>
			<S sid="232" ssid="63">In (Culotta and Sorensen, 2004) the authors propose doing relation extraction in twosteps: first, one binary SVM is trained for relation detection, which means that all positive rela tion instances are combined into one class.</S>
			<S sid="233" ssid="64">Then the thresholded output of this binary classifier is used as training data for a second multi-class SVM, which is trained for relation classification.</S>
			<S sid="234" ssid="65">The same kernel is used in both stages.</S>
			<S sid="235" ssid="66">We present in Table 5 the performance of our shortest path (SP) dependency kernel on the task ofrelation extraction from ACE, where the dependencies are extracted using either a CCG parser (SP CCG), or a CFG parser (SP-CFG).</S>
			<S sid="236" ssid="67">We also show the results presented in (Culotta and Sorensen, 2004) for their best performing kernel K4 (a sum between a bag-of-words kernel and their dependency kernel) under both scenarios.</S>
			<S sid="237" ssid="68">Method Precision Recall F-measure (S1) SP-CCG 67.5 37.2 48.0 (S1) SP-CFG 71.1 39.2 50.5 (S1) K4 70.3 26.3 38.0 (S2) SP-CCG 63.7 41.4 50.2 (S2) SP-CFG 65.5 43.8 52.5 (S2) K4 67.1 35.0 45.8 Table 5: Extraction Performance on ACE.</S>
			<S sid="238" ssid="69">The shortest-path dependency kernels outperform the dependency kernel from (Culotta and Sorensen,2004) in both scenarios, with a more significant dif ference for SP-CFG.</S>
			<S sid="239" ssid="70">An error analysis revealed thatCollins?</S>
			<S sid="240" ssid="71">parser was better at capturing local depen dencies, hence the increased accuracy of SP-CFG.</S>
			<S sid="241" ssid="72">Another advantage of our shortest-path dependency kernels is that their training and testing are very fast ? this is due to representing the sentence as a chainof dependencies on which a fast kernel can be com puted.</S>
			<S sid="242" ssid="73">All the four SP kernels from Table 5 take between 2 and 3 hours to train and test on a 2.6GHz Pentium IV machine.</S>
			<S sid="243" ssid="74">To avoid numerical problems, we constrained the dependency paths to pass through at most 10 words(as observed in the training data) by setting the kernel to 0 for longer paths.</S>
			<S sid="244" ssid="75">We also tried the alterna tive solution of normalizing the kernel, however this led to a slight decrease in accuracy.</S>
			<S sid="245" ssid="76">Having longer paths give larger kernel scores in the unnormalizedversion does not pose a problem because, by definition, paths of different lengths correspond to disjoint sets of features.</S>
			<S sid="246" ssid="77">Consequently, the SVM algorithm will induce lower weights for features occur ring in longer paths, resulting in a linear separator that works irrespective of the size of the dependency paths.</S>
	</SECTION>
	<SECTION title="Related Work. " number="6">
			<S sid="247" ssid="1">In (Zelenko et al, 2003), the authors do relation extraction using a tree kernel defined over shallow parse tree representations of sentences.</S>
			<S sid="248" ssid="2">The same tree kernel is slightly generalized in (Culotta andSorensen, 2004) and used in conjunction with dependency trees.</S>
			<S sid="249" ssid="3">In both approaches, a relation in stance is defined to be the smallest subtree in the parse or dependency tree that includes both entities.</S>
			<S sid="250" ssid="4">In this paper we argued that the information relevant to relation extraction is almost entirely concentrated in the shortest path in the dependency tree, leading to an even smaller representation.</S>
			<S sid="251" ssid="5">Another difference between the tree kernels above and our new kernel is that the tree kernels used for relation extraction are opaque i.e. the semantics of the dimensions in the corresponding Hilbert space is not obvious.</S>
			<S sid="252" ssid="6">For the shortest-path kernels, the semantics is known bydefinition: each path feature corresponds to a dimen sion in the Hilbert space.</S>
			<S sid="253" ssid="7">This transparency allows us to easily restrict the types of patterns counted by the kernel to types that we deem relevant for relationextraction.</S>
			<S sid="254" ssid="8">The tree kernels are also more time con suming, especially in the sparse setting, where they count sparse subsequences of children common to nodes in the two trees.</S>
			<S sid="255" ssid="9">In (Zelenko et al, 2003), the 730 tree kernel is computed in O(mn) time, where m and n are the number of nodes in the two trees.</S>
			<S sid="256" ssid="10">This changes to O(mn3) in the sparse setting.Our shortest-path intuition bears some similar ity with the underlying assumption of the relational pathfinding algorithm from (Richards and Mooney,1992) : ?in most relational domains, important con cepts will be represented by a small number of fixedpaths among the constants defining a positive instance ? for example, the grandparent relation is de fined by a single fixed path consisting of two parent relations.?</S>
			<S sid="257" ssid="11">We can see this happening also in the task of relation extraction from ACE, where ?importantconcepts?</S>
			<S sid="258" ssid="12">are the 5 types of relations, and the ?con stants?</S>
			<S sid="259" ssid="13">defining a positive instance are the 5 types of entities.</S>
	</SECTION>
	<SECTION title="Future Work. " number="7">
			<S sid="260" ssid="1">Local and non-local (deep) dependencies are equally important for finding relations.</S>
			<S sid="261" ssid="2">In this paper we tried extracting both types of dependencies using a CCG parser, however another approach is to recover deepdependencies from syntactic parses, as in (Camp bell, 2004; Levy and Manning, 2004).</S>
			<S sid="262" ssid="3">This mayhave the advantage of preserving the quality of local dependencies while completing the representa tion with non-local dependencies.Currently, the method assumes that the named entities are known.</S>
			<S sid="263" ssid="4">A natural extension is to automati cally extract both the entities and their relationships.</S>
			<S sid="264" ssid="5">Recent research (Roth and Yih, 2004) indicates thatintegrating entity recognition with relation extraction in a global model that captures the mutual influ ences between the two tasks can lead to significant improvements in accuracy.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="8">
			<S sid="265" ssid="1">We have presented a new kernel for relation extraction based on the shortest-path between the two rela tion entities in the dependency graph.</S>
			<S sid="266" ssid="2">Comparative experiments on extracting top-level relations from the ACE corpus show significant improvements over a recent dependency tree kernel.</S>
	</SECTION>
	<SECTION title="Acknowledgements. " number="9">
			<S sid="267" ssid="1">This work was supported by grants IIS-0117308 and IIS-0325116 from the NSF.</S>
	</SECTION>
</PAPER>
