<PAPER>
  <S sid="0">Preemptive Information Extraction Using Unrestricted Relation Discovery</S>
  <ABSTRACT>
    <S sid="1" ssid="1">surface text patterns for a question answering system. of the 40th Annual Meeting of the As</S>
  </ABSTRACT>
  <SECTION title="1 Background" number="1">
    <S sid="2" ssid="1">Every day, a large number of news articles are created and reported, many of which are unique.</S>
    <S sid="3" ssid="2">But certain types of events, such as hurricanes or murders, are reported again and again throughout a year.</S>
    <S sid="4" ssid="3">The goal of Information Extraction, or IE, is to retrieve a certain type of news event from past articles and present the events as a table whose columns are filled with a name of a person or company, according to its role in the event.</S>
    <S sid="5" ssid="4">However, existing IE techniques require a lot of human labor.</S>
    <S sid="6" ssid="5">First, you have to specify the type of information you want and collect articles that include this information.</S>
    <S sid="7" ssid="6">Then, you have to analyze the articles and manually craft a set of patterns to capture these events.</S>
    <S sid="8" ssid="7">Most existing IE research focuses on reducing this burden by helping people create such patterns.</S>
    <S sid="9" ssid="8">But each time you want to extract a different kind of information, you need to repeat the whole process: specify articles and adjust its patterns, either manually or semiautomatically.</S>
    <S sid="10" ssid="9">There is a bit of a dangerous pitfall here.</S>
    <S sid="11" ssid="10">First, it is hard to estimate how good the system can be after months of work.</S>
    <S sid="12" ssid="11">Furthermore, you might not know if the task is even doable in the first place.</S>
    <S sid="13" ssid="12">Knowing what kind of information is easily obtained in advance would help reduce this risk.</S>
    <S sid="14" ssid="13">An IE task can be defined as finding a relation among several entities involved in a certain type of event.</S>
    <S sid="15" ssid="14">For example, in the MUC-6 management succession scenario, one seeks a relation between COMPANY, PERSON and POST involved with hiring/firing events.</S>
    <S sid="16" ssid="15">For each row of an extracted table, you can always read it as &#8220;COMPANY hired (or fired) PERSON for POST.&#8221; The relation between these entities is retained throughout the table.</S>
    <S sid="17" ssid="16">There are many existing works on obtaining extraction patterns for pre-defined relations (Riloff, 1996; Yangarber et al., 2000; Agichtein and Gravano, 2000; Sudo et al., 2003).</S>
    <S sid="18" ssid="17">Unrestricted Relation Discovery is a technique to automatically discover such relations that repeatedly appear in a corpus and present them as a table, with absolutely no human intervention.</S>
    <S sid="19" ssid="18">Unlike most existing IE research, a user does not specify the type of articles or information wanted.</S>
    <S sid="20" ssid="19">Instead, a system tries to find all the kinds of relations that are reported multiple times and can be reported in tabular form.</S>
    <S sid="21" ssid="20">This technique will open up the possibility of trying new IE scenarios.</S>
    <S sid="22" ssid="21">Furthermore, the system itself can be used as an IE system, since an obtained relation is already presented as a table.</S>
    <S sid="23" ssid="22">If this system works to a certain extent, tuning an IE system becomes a search problem: all the tables are already built &#8220;preemptively.&#8221; A user only needs to search for a relevant table.</S>
    <S sid="24" ssid="23">We implemented a preliminary system for this technique and obtained reasonably good performance.</S>
    <S sid="25" ssid="24">Table 1 is a sample relation that was extracted as a table by our system.</S>
    <S sid="26" ssid="25">The columns of the table show article dates, names of hurricanes and the places they affected respectively.</S>
    <S sid="27" ssid="26">The headers of the table and its keywords were also extracted automatically.</S>
  </SECTION>
  <SECTION title="2 Basic Idea" number="2">
    <S sid="28" ssid="1">In Unrestricted Relation Discovery, the discovery process (i.e. creating new tables) can be formulated as a clustering task.</S>
    <S sid="29" ssid="2">The key idea is to cluster a set of articles that contain entities bearing a similar relation to each other in such a way that we can construct a table where the entities that play the same role are placed in the same column.</S>
    <S sid="30" ssid="3">Suppose that there are two articles A and B, and both report hurricane-related news.</S>
    <S sid="31" ssid="4">Article A contains two entities &#8220;Katrina&#8221; and &#8220;New Orleans&#8221;, and article B contains &#8220;Longwang&#8221; and &#8220;Taiwan&#8221;.</S>
    <S sid="32" ssid="5">These entities are recognized by a Named Entity (NE) tagger.</S>
    <S sid="33" ssid="6">We want to discover a relation among them.</S>
    <S sid="34" ssid="7">First, we introduce a notion called &#8220;basic pattern&#8221; to form a relation.</S>
    <S sid="35" ssid="8">A basic pattern is a part of the text that is syntactically connected to an entity.</S>
    <S sid="36" ssid="9">Some examples are &#8220;X is hit&#8221; or &#8220;Y&#8217;s residents&#8221;.</S>
    <S sid="37" ssid="10">Figure 1 shows several basic patterns connected to the entities &#8220;Katrina&#8221; and &#8220;New Orleans&#8221; in article A.</S>
    <S sid="38" ssid="11">Similarly, we obtain the basic patterns for article B.</S>
    <S sid="39" ssid="12">Now, in Figure 2, both entities &#8220;Katrina&#8221; and &#8220;Longwang&#8221; have the basic pattern &#8220;headed&#8221; in common.</S>
    <S sid="40" ssid="13">In this case, we connect these two entities to each other.</S>
    <S sid="41" ssid="14">Furthermore, there is also a common basic pattern &#8220;was-hit&#8221; shared by &#8220;New Orleans&#8221; and &#8220;Taiwan&#8221;.</S>
    <S sid="42" ssid="15">Now, we found two sets of entities that can be placed in correspondence at the same time.</S>
    <S sid="43" ssid="16">What does this mean?</S>
    <S sid="44" ssid="17">We can infer that both entity sets (&#8220;Katrina&#8221;-&#8220;New Orleans&#8221; and &#8220;Longwang&#8221;-&#8220;Taiwan&#8221;) represent a certain relation that has something in common: a hurricane name and the place it affected.</S>
    <S sid="45" ssid="18">By finding multiple parallel correspondences between two articles, we can estimate the similarity of their relations.</S>
    <S sid="46" ssid="19">Generally, in a clustering task, one groups items by finding similar pairs.</S>
    <S sid="47" ssid="20">After finding a pair of articles that have a similar relation, we can bring them into the same cluster.</S>
    <S sid="48" ssid="21">In this case, we cluster articles by using their basic patterns as features.</S>
    <S sid="49" ssid="22">However, each basic pattern is still connected to its entity so that we can extract the name from it.</S>
    <S sid="50" ssid="23">We can consider a basic pattern to represent something like the &#8220;role&#8221; of its entity.</S>
    <S sid="51" ssid="24">In this example, the entities that had &#8220;headed&#8221; as a basic pattern are hurricanes, and the entities that had &#8220;was-hit&#8221; as a basic pattern are the places it affected.</S>
    <S sid="52" ssid="25">By using basic patterns, we can align the entities into the corresponding column that represents a certain role in the relation.</S>
    <S sid="53" ssid="26">From this example, we create a two-by-two table, where each column represents the roles of the entities, and each row represents a different article, as shown in the bottom of Figure 2.</S>
    <S sid="54" ssid="27">We can extend this table by finding another article in the same manner.</S>
    <S sid="55" ssid="28">In this way, we gradually extend a table while retaining a relation among its columns.</S>
    <S sid="56" ssid="29">In this example, the obtained table is just what an IE system (whose task is to find a hurricane name and the affected place) would create.</S>
    <S sid="57" ssid="30">However, these articles might also include other things, which could represent different relations.</S>
    <S sid="58" ssid="31">For example, the governments might call for help or some casualties might have been reported.</S>
    <S sid="59" ssid="32">To obtain such relations, we need to choose different entities from the articles.</S>
    <S sid="60" ssid="33">Several existing works have tried to extract a certain type of relation by manually choosing different pairs of entities (Brin, 1998; Ravichandran and Hovy, 2002).</S>
    <S sid="61" ssid="34">Hasegawa et al. (2004) tried to extract multiple relations by choosing entity types.</S>
    <S sid="62" ssid="35">We assume that we can find such relations by trying all possible combinations from a set of entities we have chosen in advance; some combinations might represent a hurricane and government relation, and others might represent a place and its casualties.</S>
    <S sid="63" ssid="36">To ensure that an article can have several different relations, we let each article belong to several different clusters.</S>
    <S sid="64" ssid="37">In a real-world situation, only using basic patterns sometimes gives undesired results.</S>
    <S sid="65" ssid="38">For example, &#8220;(President) Bush flew to Texas&#8221; and &#8220;(Hurricane) Katrina flew to New Orleans&#8221; both have a basic pattern &#8220;flew to&#8221; in common, so &#8220;Bush&#8221; and &#8220;Katrina&#8221; would be put into the same column.</S>
    <S sid="66" ssid="39">But we want to separate them in different tables.</S>
    <S sid="67" ssid="40">To alleviate this problem, we put an additional restriction on clustering.</S>
    <S sid="68" ssid="41">We use a bag-of-words approach to discriminate two articles: if the word-based similarity between two articles is too small, we do not bring them together into the same cluster (i.e. table).</S>
    <S sid="69" ssid="42">We exclude names from the similarity calculation at this stage because we want to link articles about the same type of event, not the same instance.</S>
    <S sid="70" ssid="43">In addition, we use the frequency of each basic pattern to compute the similarity of relations, since basic patterns like &#8220;say&#8221; or &#8220;have&#8221; appear in almost every article and it is dangerous to rely on such expressions.</S>
    <S sid="71" ssid="44">In the above explanation, we have assumed that we can obtain enough basic patterns from an article.</S>
    <S sid="72" ssid="45">However, the actual number of basic patterns that one can find from a single article is usually not enough, because the number of sentences is rather small in comparison to the variation of expressions.</S>
    <S sid="73" ssid="46">So having two articles that have multiple basic patterns in common is very unlikely.</S>
    <S sid="74" ssid="47">We extend the number of articles for obtaining basic patterns by using a cluster of comparable articles that report the same event instead of a single article.</S>
    <S sid="75" ssid="48">We call this cluster of articles a &#8220;basic cluster.&#8221; Using basic clusters instead of single articles also helps to increase the redundancy of data.</S>
    <S sid="76" ssid="49">We can give more confidence to repeated basic patterns.</S>
    <S sid="77" ssid="50">Note that the notion of &#8220;basic cluster&#8221; is different from the clusters used for creating tables explained above.</S>
    <S sid="78" ssid="51">In the following sections, a cluster for creating a table is called a &#8220;metacluster,&#8221; because this is a cluster of basic clusters.</S>
    <S sid="79" ssid="52">A basic cluster consists of a set of articles that report the same event which happens at a certain time, and a metacluster consists of a set of events that contain the same relation over a certain period.</S>
    <S sid="80" ssid="53">We try to increase the number of articles in a basic cluster by looking at multiple news sources simultaneously.</S>
    <S sid="81" ssid="54">We use a clustering algorithm that uses a vector-space-model to obtain basic clusters.</S>
    <S sid="82" ssid="55">Then we apply cross-document coreference resolution to connect entities of different articles within a basic cluster.</S>
    <S sid="83" ssid="56">This way, we can increase the number of basic patterns connected to each entity.</S>
    <S sid="84" ssid="57">Also, it allows us to give a weight to entities.</S>
    <S sid="85" ssid="58">We calculate their weights using the number of occurrences within a cluster and their position within an article.</S>
    <S sid="86" ssid="59">These entities are used to obtain basic patterns later.</S>
    <S sid="87" ssid="60">We also use a parser and tree normalizer to generate basic patterns.</S>
    <S sid="88" ssid="61">The format of basic patterns is crucial to performance.</S>
    <S sid="89" ssid="62">We think a basic pattern should be somewhat specific, since each pattern should capture an entity with some relevant context.</S>
    <S sid="90" ssid="63">But at the same time a basic pattern should be general enough to reduce data sparseness.</S>
    <S sid="91" ssid="64">We choose a predicate-argument structure as a natural solution for this problem.</S>
    <S sid="92" ssid="65">Compared to traditional constituent trees, a predicate-argument structure is a higher-level representation of sentences that has gained wide acceptance from the natural language community recently.</S>
    <S sid="93" ssid="66">In this paper we used a logical feature structure called GLARF proposed by Meyers et al. (2001a).</S>
    <S sid="94" ssid="67">A GLARF converter takes a syntactic tree as an input and augments it with several features.</S>
    <S sid="95" ssid="68">Figure 3 shows a sample GLARF structure obtained from the sentence &#8220;Katrina hit Louisiana&#8217;s coast.&#8221; We used GLARF for two reasons: first, unlike traditional constituent parsers, GLARF has an ability to regularize several linguistic phenomena such as participial constructions and coordination.</S>
    <S sid="96" ssid="69">This allows us to handle this syntactic variety in a uniform way.</S>
    <S sid="97" ssid="70">Second, an output structure can be easily converted into a directed graph that represents the relationship between each word, without losing significant information from the original sentence.</S>
    <S sid="98" ssid="71">Compared to an ordinary constituent tree, it is easier to extract syntactic relationships.</S>
    <S sid="99" ssid="72">In the next section, we discuss how we used this structure to generate basic patterns.</S>
  </SECTION>
  <SECTION title="3 Implementation" number="3">
    <S sid="100" ssid="1">The overall process to generate basic patterns and discover relations from unannotated news articles is shown in Figure 4.</S>
    <S sid="101" ssid="2">Theoretically this could be a straight pipeline, but due to the nature of the implementation we process some stages separately and combine them in the later stage.</S>
    <S sid="102" ssid="3">In the following subsection, we explain each component.</S>
    <S sid="103" ssid="4">First of all, we need a lot of news articles from multiple news sources.</S>
    <S sid="104" ssid="5">We created a simple web crawler that extract the main texts from web pages.</S>
    <S sid="105" ssid="6">We observed that the crawler can correctly take the main texts from about 90% of the pages from each news site.</S>
    <S sid="106" ssid="7">We ran the crawler every day on several news sites.</S>
    <S sid="107" ssid="8">Then we applied a simple clustering algorithm to the obtained articles in order to find a set of articles that talk about exactly the same news and form a basic cluster.</S>
    <S sid="108" ssid="9">We eliminate stop words and stem all the other words, then compute the similarity between two articles by using a bag-of-words approach.</S>
    <S sid="109" ssid="10">In news articles, a sentence that appears in the beginning of an article is usually more important than the others.</S>
    <S sid="110" ssid="11">So we preserved the word order to take into account the location of each sentence.</S>
    <S sid="111" ssid="12">First we computed a word vector from each article: where Vw(A) is a vector element of word w in article A, IDF(w) is the inverse document frequency of word w, and POS(w, A) is a list of w&#8217;s positions in the article. avgwords is the average number of words for all articles.</S>
    <S sid="112" ssid="13">Then we calculated the cosine value of each pair of vectors: We computed the similarity of all possible pairs of articles from the same day, and selected the pairs whose similarity exceeded a certain threshold (0.65 in this experiment) to form a basic cluster.</S>
    <S sid="113" ssid="14">After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article.</S>
    <S sid="114" ssid="15">The current implementation of a GLARF converter gives about 75% F-score using parser output.</S>
    <S sid="115" ssid="16">For the details of GLARF representation and its conversion, see Meyers et al. (2001b).</S>
    <S sid="116" ssid="17">In parallel with parsing and GLARFing, we also apply NE tagging and coreference resolution for each article in a basic cluster.</S>
    <S sid="117" ssid="18">We used an HMM-based NE tagger whose performance is about 85% in Fscore.</S>
    <S sid="118" ssid="19">This NE tagger produces ACE-type Named Entities 1: PERSON, ORGANIZATION, GPE, LOCATION and FACILITY 2.</S>
    <S sid="119" ssid="20">After applying singledocument coreference resolution for each article, we connect the entities among different articles in the same basic cluster to obtain cross-document coreference entities with simple string matching.</S>
    <S sid="120" ssid="21">After getting a GLARF structure for each sentence and a set of documents whose entities are tagged and connected to each other, we merge the two outputs and create a big network of GLARF structures whose nodes are interconnected across different sentences/articles.</S>
    <S sid="121" ssid="22">Now we can generate basic patterns for each entity.</S>
    <S sid="122" ssid="23">First, we compute the weight for each cross-document entity E in a certain basic cluster as follows: where e &#8712; E is an entity within one article and mentions(e) and firstsent(e) are the number of mentions of entity e in a document and the position of the sentence where entity e first appeared, respectively.</S>
    <S sid="123" ssid="24">C is a constant value which was 0.5 in this experiment.</S>
    <S sid="124" ssid="25">To reduce combinatorial complexity, we took only the five most highly weighted entities from each basic cluster to generate basic patterns.</S>
    <S sid="125" ssid="26">We observed these five entities can cover major relations that are reported in a basic cluster.</S>
    <S sid="126" ssid="27">Next, we obtain basic patterns from the GLARF structures.</S>
    <S sid="127" ssid="28">We used only the first ten sentences in each article for getting basic patterns, as most important facts are usually written in the first few sentences of a news article.</S>
    <S sid="128" ssid="29">Figure 5 shows all the basic patterns obtained from the sentence &#8220;Katrina hit Louisiana&#8217;s coast.&#8221; The shaded nodes &#8220;Katrina&#8221; and &#8220;Louisiana&#8221; are entities from which each basic pattern originates.</S>
    <S sid="129" ssid="30">We take a path of GLARF nodes from each entity node until it reaches any predicative node: noun, verb, or adjective in this case.</S>
    <S sid="130" ssid="31">Since the nodes &#8220;hit&#8221; and &#8220;coast&#8221; can be predicates in this example, we obtain three unique paths &#8220;Louisiana+T-POS:coast (Louisiana&#8217;s coast)&#8221;, &#8220;Katrina+SBJ:hit (Katrina hit something)&#8221;, and &#8220;Katrina+SBJ:hit-OBJ:coast (Katrina hit some coast)&#8221;.</S>
    <S sid="131" ssid="32">To increase the specificity of patterns, we generate extra basic patterns by adding a node that is immediately connected to a predicative node.</S>
    <S sid="132" ssid="33">(From this example, we generate two basic patterns: &#8220;hit&#8221; and &#8220;hit-coast&#8221; from the &#8220;Katrina&#8221; node.)</S>
    <S sid="133" ssid="34">Notice that in a GLARF structure, the type of each argument such as subject or object is preserved in an edge even if we extract a single path of a graph.</S>
    <S sid="134" ssid="35">Now, we replace both entities &#8220;Katrina&#8221; and &#8220;Louisiana&#8221; with variables based on their NE tags and obtain parameterized patterns: &#8220;GPE+T-POS:coast (Louisiana&#8217;s coast)&#8221;, &#8220;PER+SBJ:hit (Katrina hit something)&#8221;, and &#8220;PER+SBJ:hit-OBJ:coast (Katrina hit some coast)&#8221;.</S>
    <S sid="135" ssid="36">After taking all the basic patterns from every basic cluster, we compute the Inverse Cluster Frequency (ICF) of each unique basic pattern.</S>
    <S sid="136" ssid="37">ICF is similar to the Inverse Document Frequency (IDF) of words, which is used to calculate the weight of each basic pattern for metaclustering.</S>
    <S sid="137" ssid="38">Finally, we can perform metaclustering to obtain tables.</S>
    <S sid="138" ssid="39">We compute the similarity between each basic cluster pair, as seen in Figure 6.</S>
    <S sid="139" ssid="40">XA and XB are the set of cross-document entities from basic clusters cA and cB, respectively.</S>
    <S sid="140" ssid="41">We examine all possible mappings of relations (parallel mappings of multiple entities) from both basic clusters, and find all the mappings M whose similarity score exceeds a certain threshold. wordsim(cA, cB) is the bag-of-words similarity of two clusters.</S>
    <S sid="141" ssid="42">As a weighting function we used ICF: We then sort the similarities of all possible pairs of basic clusters, and try to build a metacluster by taking the most strongly connected pair first.</S>
    <S sid="142" ssid="43">Note that in this process we may assign one basic cluster to several different metaclusters.</S>
    <S sid="143" ssid="44">When a link is found between two basic clusters that were already assigned to a metacluster, we try to put them into all the existing metaclusters it belongs to.</S>
    <S sid="144" ssid="45">However, we allow a basic cluster to be added only if it can fill all the columns in that table.</S>
    <S sid="145" ssid="46">In other words, the first two basic clusters (i.e. an initial two-row table) determines its columns and therefore define the relation of that table.</S>
  </SECTION>
  <SECTION title="4 Experiment and Evaluation" number="4">
    <S sid="146" ssid="1">We used twelve newspapers published mainly in the U.S. We collected their articles over two months (from Sep. 21, 2005 - Nov. 27, 2005).</S>
    <S sid="147" ssid="2">We obtained 643,767 basic patterns and 7,990 unique types.</S>
    <S sid="148" ssid="3">Then we applied metaclustering to these basic clusters and obtained 302 metaclusters (tables).</S>
    <S sid="149" ssid="4">We then removed duplicated rows and took only the tables that had 3 or more rows.</S>
    <S sid="150" ssid="5">Finally we had 101 tables.</S>
    <S sid="151" ssid="6">The total number the of articles and clusters we used are shown in Table 2.</S>
    <S sid="152" ssid="7">We evaluated the obtained tables as follows.</S>
    <S sid="153" ssid="8">For each row in a table, we added a summary of the source articles that were used to extract the relation.</S>
    <S sid="154" ssid="9">Then for each table, an evaluator looks into every row and its source article, and tries to come up with a sentence that explains the relation among its columns.</S>
    <S sid="155" ssid="10">The description should be as specific as possible.</S>
    <S sid="156" ssid="11">If at least half of the rows can fit the explanation, the table is considered &#8220;consistent.&#8221; For each consistent table, the evaluator wrote down the sentence using variable names ($1, $2, ...) to refer to its columns.</S>
    <S sid="157" ssid="12">Finally, we counted the number of consistent tables.</S>
    <S sid="158" ssid="13">We also counted how many rows in each table can fit the explanation.</S>
    <S sid="159" ssid="14">We evaluated 48 randomly chosen tables.</S>
    <S sid="160" ssid="15">Among these tables, we found that 36 tables were consistent.</S>
    <S sid="161" ssid="16">We also counted the total number of rows that fit each description, shown in Table 3.</S>
    <S sid="162" ssid="17">Table 4 shows the descriptions of the selected tables.</S>
    <S sid="163" ssid="18">The largest consistent table was about hurricanes (Table 5).</S>
    <S sid="164" ssid="19">Although we cannot exactly measure the recall of each table, we tried to estimate the recall by comparing this hurricane table to a manually created one (Table 6).</S>
    <S sid="165" ssid="20">We found 6 out of 9 hurricanes 3.</S>
    <S sid="166" ssid="21">It is worth noting that most of these hurricane names were automatically disambiguated although our NE tagger didn&#8217;t distinguish a hurricane name from a person ber of fitted/total rows. name.</S>
    <S sid="167" ssid="22">The second largest table (about nominations of officials) is shown in Table 7.</S>
    <S sid="168" ssid="23">We reviewed 10 incorrect rows from various tables and found 4 of them were due to coreference errors and one error was due to a parse error.</S>
    <S sid="169" ssid="24">The other 4 errors were due to multiple basic patterns distant from each other that happened to refer to a different event reported in the same cluster.</S>
    <S sid="170" ssid="25">The causes of the one remaining error was obscure.</S>
    <S sid="171" ssid="26">Most inconsistent tables were a mixture of multiple relations and some of their rows still looked consistent.</S>
    <S sid="172" ssid="27">We have a couple of open questions.</S>
    <S sid="173" ssid="28">First, the overall recall of our system might be lower than existing IE systems, as we are relying on a cluster of comparable articles rather than a single document to discover an event.</S>
    <S sid="174" ssid="29">We might be able to improve this in the future by adjusting the basic clustering algorithm or weighting schema of basic patterns.</S>
    <S sid="175" ssid="30">Secondly, some combinations of basic patterns looked inherently vague.</S>
    <S sid="176" ssid="31">For example, we used the two basic patterns &#8220;pitched&#8221; and &#8220;&#8217;s-series&#8221; in the following sentence (the patterns are underlined): Ervin Santana pitched 5 1-3 gutsy innings in his postseason debut for the Angels, Adam Kennedy hit a goahead triple that sent Yankees outfielders crashing to the ground, and Los Angeles beat New York 5-3 Monday night in the decisive Game 5 of their AL playoff series.</S>
    <S sid="177" ssid="32">It is not clear whether this set of patterns can yield any meaningful relation.</S>
    <S sid="178" ssid="33">We are not sure how much this sort of table can affect overall IE performance.</S>
  </SECTION>
  <SECTION title="5 Conclusion" number="5">
    <S sid="179" ssid="1">In this paper we proposed Preemptive Information Extraction as a new direction of IE research.</S>
    <S sid="180" ssid="2">As its key technique, we presented Unrestricted Relation Discovery that tries to find parallel correspondences between multiple entities in a document, and perform clustering using basic patterns as features.</S>
    <S sid="181" ssid="3">To increase the number of basic patterns, we used a cluster of comparable articles instead of a single document.</S>
    <S sid="182" ssid="4">We presented the implementation of our preliminary system and its outputs.</S>
    <S sid="183" ssid="5">We obtained dozens of usable tables.</S>
    <S sid="184" ssid="6">Sep. and Nov. (from Wikipedia).</S>
    <S sid="185" ssid="7">Rows with a star (*) were actually extracted.</S>
    <S sid="186" ssid="8">The number of the source articles that contained a mention of the hurricane is shown in the right column.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="6">
    <S sid="187" ssid="1">This research was supported by the National Science Foundation under Grant IIS-00325657.</S>
    <S sid="188" ssid="2">This paper does not necessarily reflect the position of the U.S. Government.</S>
    <S sid="189" ssid="3">We would like to thank Prof. Ralph Grishman who provided useful suggestions and discussions.</S>
  </SECTION>
</PAPER>
