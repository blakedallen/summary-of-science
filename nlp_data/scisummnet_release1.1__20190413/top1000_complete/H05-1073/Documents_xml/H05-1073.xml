<PAPER>
	<S sid="0">Emotions From Text: Machine Learning For Text-Based Emotion Prediction</S><ABSTRACT>
		<S sid="1" ssid="1">In addition to information, text con tains attitudinal, and more specifically, emotional content.</S>
		<S sid="2" ssid="2">This paper exploresthe text-based emotion prediction prob lem empirically, using supervised machinelearning with the SNoW learning architecture.</S>
		<S sid="3" ssid="3">The goal is to classify the emotional affinity of sentences in the narra tive domain of children?s fairy tales, forsubsequent usage in appropriate expressive rendering of text-to-speech synthe sis.</S>
		<S sid="4" ssid="4">Initial experiments on a preliminarydata set of 22 fairy tales show encourag ing results over a na??ve baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning.</S>
		<S sid="5" ssid="5">We also discuss results for a tripartite model which covers emotional valence, as well as feature set alernations.</S>
		<S sid="6" ssid="6">In addition, we present plans for a more cognitively soundsequential model, taking into considera tion a larger set of basic emotions.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="7" ssid="7">Text does not only communicate informative con tents, but also attitudinal information, includingemotional states.</S>
			<S sid="8" ssid="8">The following reports on an em pirical study of text-based emotion prediction.</S>
			<S sid="9" ssid="9">Section 2 gives a brief overview of the intendedapplication area, whereas section 3 summarizes re lated work.</S>
			<S sid="10" ssid="10">Next, section 4 explains the empirical study, including the machine learning model, thecorpus, the feature set, parameter tuning, etc. Section 5 presents experimental results from two classi fication tasks and feature set modifications.</S>
			<S sid="11" ssid="11">Section 6 describes the agenda for refining the model, before presenting concluding remarks in 7.</S>
	</SECTION>
	<SECTION title="Application area: Text-to-speech. " number="2">
			<S sid="12" ssid="1">Narrative text is often especially prone to having emotional contents.</S>
			<S sid="13" ssid="2">In the literary genre of fairy tales, emotions such as HAPPINESS and ANGER and related cognitive states, e.g. LOVE or HATE, becomeintegral parts of the story plot, and thus are of particular importance.</S>
			<S sid="14" ssid="3">Moreover, the story teller read ing the story interprets emotions in order to orally convey the story in a fashion which makes the story come alive and catches the listeners?</S>
			<S sid="15" ssid="4">attention.</S>
			<S sid="16" ssid="5">In speech, speakers effectively express emotions by modifying prosody, including pitch, intensity, and durational cues in the speech signal.</S>
			<S sid="17" ssid="6">Thus, inorder to make text-to-speech synthesis sound as natural and engaging as possible, it is important to con vey the emotional stance in the text.</S>
			<S sid="18" ssid="7">However, thisimplies first having identified the appropriate emo tional meaning of the corresponding text passage.</S>
			<S sid="19" ssid="8">Thus, an application for emotional text-to-speech synthesis has to solve two basic problems.</S>
			<S sid="20" ssid="9">First,what emotion or emotions most appropriately de scribe a certain text passage, and second, given a text passage and a specified emotional mark-up, how to render the prosodic contour in order to convey the emotional content, (Cahn, 1990).</S>
			<S sid="21" ssid="10">The text-based emotion prediction task (TEP) addresses the first of these two problems.</S>
			<S sid="22" ssid="11">579</S>
	</SECTION>
	<SECTION title="Previous work. " number="3">
			<S sid="23" ssid="1">For a complete general overview of the field of affective computing, see (Picard, 1997).</S>
			<S sid="24" ssid="2">(Liu, Lieberman and Selker, 2003) is a rare study in textbased inference of sentence-level emotional affin ity.</S>
			<S sid="25" ssid="3">The authors adopt the notion of basic emotions, cf.</S>
			<S sid="26" ssid="4">(Ekman, 1993), and use six emotion categories: ANGER, DISGUST, FEAR, HAPPINESS, SADNESS, SURPRISE.</S>
			<S sid="27" ssid="5">They critique statistical NLP for being unsuccessful at the small sentence level, and insteaduse a database of common-sense knowledge and create affect models which are combined to form a rep resentation of the emotional affinity of a sentence.</S>
			<S sid="28" ssid="6">At its core, the approach remains dependent on anemotion lexicon and hand-crafted rules for conceptual polarity.</S>
			<S sid="29" ssid="7">In order to be effective, emotion recog nition must go beyond such resources; the authors note themselves that lexical affinity is fragile.</S>
			<S sid="30" ssid="8">The method was tested on 20 users?</S>
			<S sid="31" ssid="9">preferences for an email-client, based on user-composed text emails describing short but colorful events.</S>
			<S sid="32" ssid="10">While the users preferred the emotional client, this evaluation does not reveal emotion classification accuracy, nor how well the model generalizes on a large data set.</S>
			<S sid="33" ssid="11">Whereas work on emotion classification fromthe point of view of natural speech and human computer dialogues is fairly extensive, e.g.</S>
			<S sid="34" ssid="12">(Scherer,2003), (Litman and Forbes-Riley, 2004), this appears not to be the case for text-to-speech synthe sis (TTS).</S>
			<S sid="35" ssid="13">A short study by (Sugimoto et al, 2004) addresses sentence-level emotion recognition forJapanese TTS.</S>
			<S sid="36" ssid="14">Their model uses a composition as sumption: the emotion of a sentence is a function of the emotional affinity of the words in the sentence.</S>
			<S sid="37" ssid="15">They obtain emotional judgements of 73 adjectives and a set of sentences from 15 human subjects andcompute words?</S>
			<S sid="38" ssid="16">emotional strength based on the ra tio of times a word or a sentence was judged to fall into a particular emotion bucket, given the number of human subjects.</S>
			<S sid="39" ssid="17">Additionally, they conducted aninteractive experiment concerning the acoustic ren dering of emotion, using manual tuning of prosodicparameters for Japanese sentences.</S>
			<S sid="40" ssid="18">While the au thors actually address the two fundamental problems of emotional TTS, their approach is impractical and most likely cannot scale up for a real corpus.</S>
			<S sid="41" ssid="19">Again, while lexical items with clear emotional meaning,such as happy or sad, matter, emotion classifica tion probably needs to consider additional inferencemechanisms.</S>
			<S sid="42" ssid="20">Moreover, a na??ve compositional ap proach to emotion recognition is risky due to simplelinguistic facts, such as context-dependent seman tics, domination of words with multiple meanings, and emotional negation.Many NLP problems address attitudinal mean ing distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g.</S>
			<S sid="43" ssid="21">(Wiebe et al, 2004), measuring strength of subjective clauses (Wilson, Wiebe and Hwa, 2004), determining word polarity (Hatzivassiloglou and McKeown, 1997) or texts?</S>
			<S sid="44" ssid="22">attitudinal valence, e.g.</S>
			<S sid="45" ssid="23">(Turney, 2002), (Bai, Padman and Airoldi, 2004), (Beineke, Hastie and Vaithyanathan, 2003), (Mullen and Collier, 2003), (Pang and Lee, 2003).</S>
			<S sid="46" ssid="24">Here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passagesin children?s stories, and eventually use this information for rendering expressive child-directed sto rytelling in a text-to-speech application.</S>
			<S sid="47" ssid="25">This can be useful, e.g. in therapeutic education of children with communication disorders (van Santen et al, 2003).</S>
	</SECTION>
	<SECTION title="Empirical study. " number="4">
			<S sid="48" ssid="1">This part covers the experimental study with a formal problem definition, computational implementa tion, data, features, and a note on parameter tuning.</S>
			<S sid="49" ssid="2">4.1 Machine learning model.</S>
			<S sid="50" ssid="3">Determining emotion of a linguistic unit can be cast as a multi-class classification problem.</S>
			<S sid="51" ssid="4">Forthe flat case, let T denote the text, and s an em bedded linguistic unit, such as a sentence, where s ? T . Let k be the number of emotion classes E = {em1, em2, .., emk}, where em1 denotes the special case of neutrality, or absence of emotion.</S>
			<S sid="52" ssid="5">The goal is to determine a mapping function f : s ? emi, such that we obtain an ordered labeled pair (s, emi).</S>
			<S sid="53" ssid="6">The mapping is based on F = {f1, f2, .., fn}, where F contains the features derived from the text.</S>
			<S sid="54" ssid="7">Furthermore, if multiple emotion classes can characterize s, then given E?</S>
			<S sid="55" ssid="8">E, the target of the mapping function becomes the ordered pair (s,E?).Finally, as further discussed in section 6, the hierarchical case of label assignment requires a sequen 580tial model that further defines levels of coarse ver sus fine-grained classifiers, as done by (Li and Roth, 2002) for the question classification problem.</S>
			<S sid="56" ssid="9">4.2 Implementation.</S>
			<S sid="57" ssid="10">Whereas our goal is to predict finer emotional mean ing distinctions according to emotional categories in speech; in this study, we focus on the basic task of recognizing emotional passages and on determining their valence (i.e. positive versus negative) becausewe currently do not have enough training data to ex plore finer-grained distinctions.</S>
			<S sid="58" ssid="11">The goal here is to get a good understanding of the nature of the TEP problem and explore features which may be useful.We explore two cases of flat classification, using a variation of the Winnow update rule implemented in the SNoW learning architecture (Carl son et al, 1999),1 which learns a linear classifierin feature space, and has been successful in sev eral NLP applications, e.g. semantic role labeling (Koomen, Punyakanok, Roth and Yih, 2005).</S>
			<S sid="59" ssid="12">In the first case, the set of emotion classes E consists of EMOTIONAL versus non-emotional or NEUTRAL, i.e. E = {N,E}.</S>
			<S sid="60" ssid="13">In the second case, E has been incremented with emotional distinctions accordingto the valence, i.e. E = {N,PE,NE}.</S>
			<S sid="61" ssid="14">Experi ments used 10-fold cross-validation, with 90% train and 10% test data.2 4.3 Data.</S>
			<S sid="62" ssid="15">The goal of our current data annotation project is to annotate a corpus of approximately 185 children stories, including Grimms?, H.C. Andersen?s and B.Potter?s stories.</S>
			<S sid="63" ssid="16">So far, the annotation process pro ceeds as follows: annotators work in pairs on the same stories.</S>
			<S sid="64" ssid="17">They have been trained separately andwork independently in order to avoid any annota tion bias and get a true understanding of the task difficulty.</S>
			<S sid="65" ssid="18">Each annotator marks the sentence levelwith one of eight primary emotions, see table 1, re flecting an extended set of basic emotions (Ekman, 1993).</S>
			<S sid="66" ssid="19">In order to make the annotation process more focused, emotion is annotated from the point of view of the text, i.e. the feeler in the sentence.</S>
			<S sid="67" ssid="20">While the primary emotions are targets, the sentences are also 1Available from http://l2r.cs.uiuc.edu/?cogcomp/2Experiments were also run for Perceptron, however the re sults are not included.</S>
			<S sid="68" ssid="21">Overall, Perceptron performed worse.</S>
			<S sid="69" ssid="22">marked for other affective contents, i.e. background mood, secondary emotions via intensity, feeler, andtextual cues.</S>
			<S sid="70" ssid="23">Disagreements in annotations are re solved by a second pass of tie-breaking by the first author, who chooses one of the competing labels.</S>
			<S sid="71" ssid="24">Eventually, the completed annotations will be made available.</S>
			<S sid="72" ssid="25">Table 1: Basic emotions used in annotation Abbreviation Emotion class A ANGRY D DISGUSTED F FEARFUL H HAPPY Sa SAD Su+ POSITIVELY SURPRISED Su- NEGATIVELY SURPRISEDEmotion annotation is hard; interannotator agreement currently range at ? = .24 ? .51, with the ra tio of observed annotation overlap ranging between45-64%, depending on annotator pair and stories as signed.</S>
			<S sid="73" ssid="26">This is expected, given the subjective natureof the annotation task.</S>
			<S sid="74" ssid="27">The lack of a clear defini tion for emotion vs. non-emotion is acknowledgedacross the emotion literature, and contributes to dy namic and shifting annotation targets.</S>
			<S sid="75" ssid="28">Indeed, acommon source of confusion is NEUTRAL, i.e. de ciding whether or not a sentence is emotional or non-emotional.</S>
			<S sid="76" ssid="29">Emotion perception also depends on which character?s point-of-view the annotator takes,and on extratextual factors such as annotator?s per sonality or mood.</S>
			<S sid="77" ssid="30">It is possible that by focusing more on the training of annotator pairs, particularlyon joint training, agreement might improve.</S>
			<S sid="78" ssid="31">However, that would also result in a bias, which is prob ably not preferable to actual perception.</S>
			<S sid="79" ssid="32">Moreover,what agreement levels are needed for successful ex pressive TTS remains an empirical question.The current data set consisted of a preliminary an notated and tie-broken data set of 1580 sentence, or 22 Grimms?</S>
			<S sid="80" ssid="33">tales.</S>
			<S sid="81" ssid="34">The label distribution is in table.</S>
			<S sid="82" ssid="35">2.</S>
			<S sid="83" ssid="36">NEUTRAL was most frequent with 59.94%..</S>
			<S sid="84" ssid="37">Table 2: Percent of annotated labels A D F H 12.34% 0.89% 7.03% 6.77%N SA SU+ SU.</S>
			<S sid="85" ssid="38">59.94% 7.34% 2.59% 3.10% 581 Table 3: % EMOTIONAL vs. NEUTRAL examples E N 40.06% 59.94% Table 4: % POSITIVE vs. NEGATIVE vs. NEUTRAL PE NE N 9.87% 30.19% 59.94% Next, for the purpose of this study, all emotionalclasses, i.e. A, D, F, H, SA, SU+, SU-, were com bined into one emotional superclass E for the firstexperiment, as shown in table 3.</S>
			<S sid="86" ssid="39">For the second experiment, we used two emotional classes, i.e. pos itive versus negative emotions; PE={H, SU+} and NE={A, D, F, SA, SU-}, as seen in table 4.</S>
			<S sid="87" ssid="40">4.4 Feature set.</S>
			<S sid="88" ssid="41">The feature extraction was written in python.</S>
			<S sid="89" ssid="42">SNoW only requires active features as input, which resulted in a typical feature vector size of around 30 features.The features are listed below.</S>
			<S sid="90" ssid="43">They were imple mented as boolean values, with continuous valuesrepresented by ranges.</S>
			<S sid="91" ssid="44">The ranges generally over lapped, in order to get more generalization coverage.</S>
			<S sid="92" ssid="45">1.</S>
			<S sid="93" ssid="46">First sentence in story.</S>
			<S sid="94" ssid="47">2.</S>
			<S sid="95" ssid="48">Conjunctions of selected features (see below).</S>
			<S sid="96" ssid="49">3.</S>
			<S sid="97" ssid="50">Direct speech (i.e. whole quote) in sentence.</S>
			<S sid="98" ssid="51">4.</S>
			<S sid="99" ssid="52">Thematic story type (3 top and 15 sub-types).</S>
	</SECTION>
	<SECTION title="Special punctuation (! and ?). " number="5">
			<S sid="100" ssid="1">7.</S>
			<S sid="101" ssid="2">Sentence length in words (0-1, 2-3, 4-8, 9-15,.</S>
			<S sid="102" ssid="3">16-25, 26-35, &gt;35) 8.</S>
			<S sid="103" ssid="4">Ranges of story progress (5-100%, 15-100%,.</S>
			<S sid="104" ssid="5">80-100%, 90-100%) 9.</S>
			<S sid="105" ssid="6">Percent of JJ, N, V, RB (0%, 1-100%, 50-.</S>
			<S sid="106" ssid="7">100%, 80-100%) 10.</S>
			<S sid="107" ssid="8">V count in sentence, excluding participles (0-1, 0-3, 0-5, 0-7, 0-9, &gt; 9) 11.</S>
			<S sid="108" ssid="9">Positive and negative word counts ( ? 1, ? 2,.</S>
			<S sid="109" ssid="10">3, ? 4, ? 5, ? 6) 12.</S>
			<S sid="110" ssid="11">WordNet emotion words.</S>
			<S sid="111" ssid="12">13.</S>
			<S sid="112" ssid="13">Interjections and affective words.</S>
			<S sid="113" ssid="14">14.</S>
			<S sid="114" ssid="15">Content BOW: N, V, JJ, RB words by POS.</S>
			<S sid="115" ssid="16">Feature conjunctions covered pairings of counts of positive and negative words with range of story progress or interjections, respectively.Feature groups 1, 3, 5, 6, 7, 8, 9, 10 and 14 are extracted automatically from the sentences in the sto ries; with the SNoW POS-tagger used for features 9, 10, and 14.</S>
			<S sid="116" ssid="17">Group 10 reflects how many verbs are active in a sentence.</S>
			<S sid="117" ssid="18">Together with the quotation and punctuation, verb domination intends to capture the assumption that emotion is often accompanied by increased action and interaction.</S>
			<S sid="118" ssid="19">Feature group 4 is based on Finish scholar Antti Aarne?s classesof folk-tale types according to their informative the matic contents (Aarne, 1964).</S>
			<S sid="119" ssid="20">The current tales have 3 top story types (ANIMAL TALES, ORDINARY FOLK-TALES, and JOKES AND ANECDOTES), and 15 subtypes (e.g. supernatural helpers is a subtype of the ORDINARY FOLK-TALE).</S>
			<S sid="120" ssid="21">This feature intends to provide an idea about the story?s general affectivepersonality (Picard, 1997), whereas the feature re flecting the story progress is hoped to capture that some emotions may be more prevalent in certain sections of the story (e.g. the happy end).For semantic tasks, words are obviously impor tant.</S>
			<S sid="121" ssid="22">In addition to considering ?content words?, we also explored specific word lists.</S>
			<S sid="122" ssid="23">Group 11 uses 2 lists of 1636 positive and 2008 negative words, obtained from (Di Cicco et al, online).</S>
			<S sid="123" ssid="24">Group 12 uses lexical lists extracted from WordNet (Fellbaum, 1998), on the basis of the primary emotion wordsin their adjectival and nominal forms.</S>
			<S sid="124" ssid="25">For the adjectives, Py-WordNet?s (Steele et al, 2004) SIMI LAR feature was used to retrieve similar items ofthe primary emotion adjectives, exploring one addi tional level in the hierarchy (i.e. similar items of all senses of all words in the synset).</S>
			<S sid="125" ssid="26">For the nouns andany identical verbal homonyms, synonyms and hy ponyms were extracted manually.3 Feature group 13used a short list of 22 interjections collected manu ally by browsing educational ESL sites, whereas theaffective word list of 771 words consisted of a combination of the non-neutral words from (Johnson Laird and Oatley, 1989) and (Siegle, online).</S>
			<S sid="126" ssid="27">Only a subset of these lexical lists actually occurred.4 3Multi-words were transformed to hyphenated form.4At this point, neither stems and bigrams nor a list of ono matopoeic words contribute to accuracy.</S>
			<S sid="127" ssid="28">Intermediate resource processing inserted some feature noise.</S>
			<S sid="128" ssid="29">582 The above feature set is henceforth referred to as all features, whereas content BOW is just group 14.</S>
			<S sid="129" ssid="30">The content BOW is a more interesting baseline than the na??ve one, P(Neutral), i.e. always assigning the most likely NEUTRAL category.</S>
			<S sid="130" ssid="31">Lastly, emotions blend and transform (Liu, Lieberman and Selker,2003).</S>
			<S sid="131" ssid="32">Thus, emotion and background mood of im mediately adjacent sentences, i.e. the sequencing, seems important.</S>
			<S sid="132" ssid="33">At this point, it is not implemented automatically.</S>
			<S sid="133" ssid="34">Instead, it was extracted from themanual emotion and mood annotations.</S>
			<S sid="134" ssid="35">If sequenc ing seemed important, an automatic method using sequential target activation could be added next.</S>
			<S sid="135" ssid="36">4.5 Parameter tuning.</S>
			<S sid="136" ssid="37">The Winnow parameters that were tuned included promotional ?, demotional ?, activation threshold?, initial weights ?, and the regularization parame ter, S, which implements a margin between positive and negative examples.</S>
			<S sid="137" ssid="38">Given the currently fairlylimited data, results from 2 alternative tuning meth ods, applied to all features, are reported.</S>
			<S sid="138" ssid="39">For the condition called sep-tune-eval, 50% of the sentences were randomly selected and set aside to be used for the parameter tuningprocess only.</S>
			<S sid="139" ssid="40">Of this subset, 10% were subsequently randomly chosen as test set with the remaining 90% used for training during the automatic tuning process, which covered 4356 different parameter combinations.</S>
			<S sid="140" ssid="41">Resulting pa rameters were: ? = 1.1, ? = 0.5, ? = 5, ? = 1.0, S = 0.5.</S>
			<S sid="141" ssid="42">The remaining half of the data was used for training and testing in the 10-fold cross-validation evaluation.</S>
			<S sid="142" ssid="43">(Also, note the slight change for P(Neutral) in table 5, due to randomly splitting the data.)</S>
			<S sid="143" ssid="44">Given that the data set is currently small, for the condition named same-tune-eval, tuning was performed automatically on all data using a slightly smaller set of combinations, and thenmanually adjusted against the 10-fold cross validation process.</S>
			<S sid="144" ssid="45">Resulting parameters were: ? = 1.2, ? = 0.9, ? = 4, ? = 1, S = 0.5.</S>
			<S sid="145" ssid="46">All data was used for evaluation.</S>
			<S sid="146" ssid="47">Emotion classification was sensitive to the selected tuning data.</S>
			<S sid="147" ssid="48">Generally, a smaller tuning set resultedin pejorative parameter settings.</S>
			<S sid="148" ssid="49">The random selec tion could make a difference, but was not explored.</S>
			<S sid="149" ssid="50">5 Results and discussion.</S>
			<S sid="150" ssid="51">This section first presents the results from experiments with the two different confusion sets de scribed above, as well as feature experimentation.</S>
			<S sid="151" ssid="52">5.1 Classification results.</S>
			<S sid="152" ssid="53">Average accuracy from 10-fold cross validation forthe first experiment, i.e. classifying sentences as either NEUTRAL or EMOTIONAL, are included in ta ble 5 and figure 1 for the two tuning conditions on the main feature sets and baselines.</S>
			<S sid="153" ssid="54">As expected, Table 5: Mean classification accuracy: N vs. E, 2 conditions same-tune-eval sep-tune-eval P(Neutral) 59.94 60.05 Content BOW 61.01 58.30 All features except BOW 64.68 63.45 All features 68.99 63.31 All features + sequencing 69.37 62.94 degree of success reflects parameter settings, bothfor content BOW and all features.</S>
			<S sid="154" ssid="55">Nevertheless, un der these circumstances, performance above a na??vebaseline and a BOW approach is obtained.</S>
			<S sid="155" ssid="56">More over, sequencing shows potential for contributing in one case.</S>
			<S sid="156" ssid="57">However, observations also point to three issues: first, the current data set appears tobe too small.</S>
			<S sid="157" ssid="58">Second, the data is not easily separa ble.</S>
			<S sid="158" ssid="59">This comes as no surprise, given the subjectivenature of the task, and the rather low interannota tor agreement, reported above.</S>
			<S sid="159" ssid="60">Moreover, despite the schematic narrative plots of children?s stories, tales still differ in their overall affective orientation, which increases data complexity.</S>
			<S sid="160" ssid="61">Third and finally, the EMOTION class is combined by basic emotion labels, rather than an original annotated label.</S>
			<S sid="161" ssid="62">More detailed averaged results from 10-fold cross-validation are included in table 6 using all features and the separated tuning and evaluationdata condition sep-tune-eval.</S>
			<S sid="162" ssid="63">With these parame ters, approximately 3% improvement in accuracy over the na??ve baseline P(Neutral) was recorded, and 5% over the content BOW, which obviously did poorly with these parameters.</S>
			<S sid="163" ssid="64">Moreover, precision is 583 0 10 20 30 40 50 60 70 same-tune-eval sep-tune-eval Tuning sets % Accuracy P(Neutral) Content BOWAll features except BOW All featuresAll features + sequencing Figure 1: Accuracy under different conditions (in %) Table 6: Classifying N vs. E (all features, sep-tune-eval) Measure N E Averaged accuracy 0.63 0.63 Averaged error 0.37 0.37 Averaged precision 0.66 0.56 Averaged recall 0.75 0.42 Averaged F-score 0.70 0.47 higher than recall for the combined EMOTION class.</S>
			<S sid="164" ssid="65">In comparison, with the same-tune-eval procedure, the accuracy improved by approximately 9% over P(Neutral) and by 8% over content BOW.</S>
			<S sid="165" ssid="66">In the second experiment, the emotion category was split into two classes: emotions with positiveversus negative valence.</S>
			<S sid="166" ssid="67">The results in terms of precision, recall, and F-score are included in table 7, us ing all features and the sep-tune-eval condition.</S>
			<S sid="167" ssid="68">Thedecrease in performance for the emotion classes mir rors the smaller amounts of data available for each class.</S>
			<S sid="168" ssid="69">As noted in section 4.3, only 9.87% of the sentences were annotated with a positive emotion,and the results for this class are worse.</S>
			<S sid="169" ssid="70">Thus, perfor mance seems likely to improve as more annotated story data becomes available; at this point, we are experimenting with merely around 12% of the total texts targeted by the data annotation project.</S>
			<S sid="170" ssid="71">5.2 Feature experiments.</S>
			<S sid="171" ssid="72">Emotions are poorly understood, and it is espe cially unclear which features may be important for their recognition from text.</S>
			<S sid="172" ssid="73">Thus, we experimented Table 7: N, PE, and NE (all features, sep-tune-eval) N NE PE Averaged precision 0.64 0.45 0.13 Averaged recall 0.75 0.27 0.19 Averaged F-score 0.69 0.32 0.13 Table 8: Feature group members Word lists interj., WordNet, affective lists, pos/neg Syntactic length ranges, % POS, V-count ranges Story-related % story-progress, 1st sent., story type Orthographic punctuation, upper-case words, quote Conjunctions Conjunctions with pos/neg Content BOW Words (N,V,Adj, Adv) with different feature configurations.</S>
			<S sid="173" ssid="74">Starting with all features, again using 10-fold cross-validation forthe separated tuning-evaluation condition sep-tuneeval, one additional feature group was removed un til none remained.</S>
			<S sid="174" ssid="75">The feature groups are listed intable 8.</S>
			<S sid="175" ssid="76">Figure 2 on the next page shows the accuracy at each step of the cumulative subtraction process.</S>
			<S sid="176" ssid="77">While some feature groups, e.g. syntactic, ap peared less important, the removal order mattered;e.g. if syntactic features were removed first, accuracy decreased.</S>
			<S sid="177" ssid="78">This fact also illustrated that fea tures work together; removing any group degraded performance because features interact and there isno true independence.</S>
			<S sid="178" ssid="79">It was observed that features?</S>
			<S sid="179" ssid="80">contributions were sensitive to parameter tun ing.</S>
			<S sid="180" ssid="81">Clearly, further work on developing features which fit the TEP problem is needed.</S>
	</SECTION>
	<SECTION title="Refining the model. " number="6">
			<S sid="181" ssid="1">This was a ?first pass?</S>
			<S sid="182" ssid="2">of addressing TEP for TTS.</S>
			<S sid="183" ssid="3">At this point, the annotation project is still on-going, and we only had a fairly small data set to draw on.Nevertheless, results indicate that our learning ap proach benefits emotion recognition.</S>
			<S sid="184" ssid="4">For example, the following instances, also labeled with the same valence by both annotators, were correctly classifiedboth in the binary (N vs. E) and the tripartite polar ity task (N, NE, PE), given the separated tuning and evaluation data condition, and using all features: (1a) E/NE: Then he offered the dwarfs money, and prayed and besought them to let him take her away; but they said, ?We will not part with her for all the gold in the world.?</S>
			<S sid="185" ssid="5">584 Cumulative removal of feature groups 61.81 63.31 62.57 57.95 58.30 58.93 59.56 55 60 65 All features - Word lists - Syntactic - Story-related - Orthographic - Conjunctions - Content words % A ccur acy All features P(Neutral) BOW Figure 2: Averaged effect of feature group removal, using sep-tune-eval (1b) N: And so the little girl really did grow up; her skin was as white as snow, her cheeks as rosy as the blood, and her hair as black as ebony; and she was called Snowdrop.</S>
			<S sid="186" ssid="6">(2a) E/NE: ?Ah,?</S>
			<S sid="187" ssid="7">she answered, ?have I not reason to weep?</S>
			<S sid="188" ssid="8">(2b) N: Nevertheless, he wished to try him first, and took a stone in his hand and squeezed it together so that water dropped out of it.</S>
			<S sid="189" ssid="9">Cases (1a) and (1b) are from the well-known FOLK TALE Snowdrop, also called Snow White.</S>
			<S sid="190" ssid="10">(1a)and (1b) are also correctly classified by the sim ple content BOW approach, although our approach has higher prediction confidence for E/NE (1a); it also considers, e.g. direct speech, a fairly high verb count, advanced story progress, connotative wordsand conjunctions thereof with story progress fea tures, all of which the BOW misses.</S>
			<S sid="191" ssid="11">In addition, thesimple content BOW approach makes incorrect pre dictions at both the bipartite and tripartite levels forexamples (2a) and (2b) from the JOKES AND ANEC DOTES stories Clever Hans and The Valiant LittleTailor, while our classifier captures the affective dif ferences by considering, e.g. distinctions in verbcount, interjection, POS, sentence length, connota tions, story subtype, and conjunctions.</S>
			<S sid="192" ssid="12">Next, we intend to use a larger data set to conduct a more complete study to establish mature findings.We also plan to explore finer emotional meaning dis tinctions, by using a hierarchical sequential modelwhich better corresponds to different levels of cognitive difficulty in emotional categorization by humans, and to classify the full set of basic level emo tional categories discussed in section 4.3.</S>
			<S sid="193" ssid="13">Sequential modeling of simple classifiers has been successfully employed to question classification, for example by (Li and Roth, 2002).</S>
			<S sid="194" ssid="14">In addition, we are working on refining and improving the feature set, and given more data, tuning can be improved on a sufficiently large development set.</S>
			<S sid="195" ssid="15">The three subcorpora in the annotation project can reveal how authorship affects emotion perception and classification.</S>
			<S sid="196" ssid="16">Moreover, arousal appears to be an important dimension for emotional prosody (Scherer, 2003), especially in storytelling (Alm and Sproat, 2005).Thus, we are planning on exploring degrees of emotional intensity in a learning scenario, i.e. a prob lem similar to measuring strength of opinion clauses (Wilson, Wiebe and Hwa, 2004).</S>
			<S sid="197" ssid="17">Finally, emotions are not discrete objects; rather they have transitional nature, and blend and overlap along the temporal dimension.</S>
			<S sid="198" ssid="18">For example, (Liu,Lieberman and Selker, 2003) include parallel estimations of emotional activity, and include smooth 585 ing techniques such as interpolation and decay to capture sequential and interactive emotional activity.</S>
			<S sid="199" ssid="19">Observations from tales indicate that some emotions are more likely to be prolonged than others.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="7">
			<S sid="200" ssid="1">This paper has discussed an empirical study of thetext-based emotion prediction problem in the domain of children?s fairy tales, with child-directed ex pressive text-to-speech synthesis as goal.</S>
			<S sid="201" ssid="2">Besidesreporting on encouraging results in a first set of com putational experiments using supervised machine learning, we have set forth a research agenda for tackling the TEP problem more comprehensively.</S>
	</SECTION>
	<SECTION title="Acknowledgments. " number="8">
			<S sid="202" ssid="1">We are grateful to the annotators, in particular A. Rasmussen and S. Siddiqui.</S>
			<S sid="203" ssid="2">We also thank two anonymous reviewers for comments.</S>
			<S sid="204" ssid="3">This work was funded by NSF under award ITR-#0205731, and NS ITR IIS-0428472.</S>
			<S sid="205" ssid="4">The annotation is supported byUIUC?s Research Board.</S>
			<S sid="206" ssid="5">The authors take sole re sponsibility for the work.</S>
	</SECTION>
</PAPER>
