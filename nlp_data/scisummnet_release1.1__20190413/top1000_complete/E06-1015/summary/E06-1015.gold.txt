Making Tree Kernels Practical For Natural Language Learning
In recent years tree kernels have been proposed for the automatic learning of natural language applications.
Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.
In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.
Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.
We introduce a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules.
