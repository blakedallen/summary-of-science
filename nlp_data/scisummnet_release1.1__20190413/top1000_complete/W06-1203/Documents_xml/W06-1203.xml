<PAPER>
  <S sid="0">Automatic Identification Of Non-Compositional Multi-Word Expressions Using Latent Semantic Analysis</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have noncompositional meanings.</S>
    <S sid="2" ssid="2">We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional.</S>
    <S sid="3" ssid="3">We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="4" ssid="1">Identifying non-compositional (or idiomatic) multi-word expressions (MWEs) is an important subtask for any computational system (Sag et al., 2002), and significant attention has been paid to practical methods for solving this problem in recent years (Lin, 1999; Baldwin et al., 2003; Villada Moir&#180;on and Tiedemann, 2006).</S>
    <S sid="5" ssid="2">While corpus-based techniques for identifying collocational multi-word expressions by exploiting statistical properties of the co-occurrence of the component words have become increasingly sophisticated (Evert and Krenn, 2001; Evert, 2004), it is well known that mere co-occurrence does not well distinguish compositional from non-compositional expressions (Manning and Sch&#168;utze, 1999, Ch.</S>
    <S sid="6" ssid="3">5).</S>
    <S sid="7" ssid="4">While expressions which may potentially have idiomatic meanings can be identified using various lexical association measures (Evert and Krenn, 2001; Evert and Kermes, 2003), other techniques must be used to determining whether or not a particular MWE does, in fact, have an idiomatic use.</S>
    <S sid="8" ssid="5">In this paper we explore the hypothesis that the local linguistic context can provide adequate cues for making this determination and propose one method for doing this.</S>
    <S sid="9" ssid="6">We characterize our task on analogy with wordsense disambiguation (Sch&#168;utze, 1998; Ide and V&#180;eronis, 1998).</S>
    <S sid="10" ssid="7">As noted by Sch&#168;utze, WSD involves two related tasks: the general task of sense discrimination&#8212;determining what senses a given word has&#8212;and the more specific task of sense selection&#8212;determining for a particular use of the word in context which sense was intended.</S>
    <S sid="11" ssid="8">For us the discrimination task involves determining for a given expression whether it has a non-compositional interpretation in addition to its compositional interpretation, and the selection task involves determining in a given context, whether a given expression is being used compositionally or non-compostionally.</S>
    <S sid="12" ssid="9">The German expression ins Wasser fallen, for example, has a noncompositional interpretation on which it means &#8216;to fail to happen&#8217; (as in (1)) and a compositional interpretation on which it means &#8216;to fall into water (as in (2)).1 The discrimination task, then, is to identify ins Wasser fallen as an MWE that has an idiomatic meaning and the selection task is to determine that in (1) it is the compositional meaning that is intended, while in (2) it is the non-compositional meaning.</S>
    <S sid="13" ssid="10">Following Sch&#168;utze (1998) and Landauer &amp; Dumais (1997) our general assumption is that the meaning of an expression can be modelled in terms of the words that it co-occurs with: its co-occurrence signature.</S>
    <S sid="14" ssid="11">To determine whether a phrase has a non-compositional meaning we compute whether the co-occurrence signature of the phrase is systematically related to the cooccurrence signatures of its parts.</S>
    <S sid="15" ssid="12">Our hypothesis is that a systematic relationship is indicative of compositional interpretation and lack of a systematic relationship is symptomatic of noncompositionality.</S>
    <S sid="16" ssid="13">In other words, we expect compositional MWEs to appear in contexts more similar to those in which their component words appear than do non-compositional MWEs.</S>
    <S sid="17" ssid="14">In this paper we describe two experiments that test this hypothesis.</S>
    <S sid="18" ssid="15">In the first experiment we seek to confirm that the local context of a known idiom can reliably distinguish idiomatic uses from non-idiomatic uses.</S>
    <S sid="19" ssid="16">In the second experiment we attempt to determine whether the difference between the contexts in which an MWE appears and the contexts in which its component words appear can indeed serve to tell us whether the MWE has an idiomatic use.</S>
    <S sid="20" ssid="17">In our experiments we make use of lexical semantic analysis (LSA) as a model of contextsimilarity (Deerwester et al., 1990).</S>
    <S sid="21" ssid="18">Since this technique is often used to model meaning, we will speak in terms of &#8220;meaning&#8221; similiarity.</S>
    <S sid="22" ssid="19">It should be clear, however, that we are only using the LSA vectors&#8212;derived from context of occurrence in a corpus&#8212;to model meaning and meaning composition in a very rough way.</S>
    <S sid="23" ssid="20">Our hope is simply that this rough model is sufficient to the task of identifying non-compositional MWEs.</S>
  </SECTION>
  <SECTION title="2 Previous work" number="2">
    <S sid="24" ssid="1">Recent work which attempts to discriminate between compositional and non-compositional MWEs include Lin (1999), who used mutualinformation measures identify such phrases, Baldwin et al. (2003), who compare the distribution of the head of the MWE with the distribution of the entire MWE, and Vallada Moir&#180;on &amp; Tiedemann (2006), who use a word-alignment strategy to identify non-compositional MWEs making use of parallel texts.</S>
    <S sid="25" ssid="2">Schone &amp; Jurafsky (2001) applied LSA to MWE identification, althought they did not focus on distinguishing compositional from non-compositional MWEs.</S>
    <S sid="26" ssid="3">Lin&#8217;s goal, like ours, was to discriminate noncompositional MWEs from compositional MWEs.</S>
    <S sid="27" ssid="4">His method was to compare the mutual information measure of the constituents parts of an MWE with the mutual information of similar expressions obtained by substituting one of the constituents with a related word obtained by thesaurus lookup.</S>
    <S sid="28" ssid="5">The hope was that a significant difference between these measures, as in the case of red tape (mutual information: 5.87) compared to yellow tape (3.75) or orange tape (2.64), would be characteristic of non-compositional MWEs.</S>
    <S sid="29" ssid="6">Although intuitively appealing, Lin&#8217;s algorithm only achieves precision and recall of 15.7% and 13.7%, respectively (as compared to a gold standard generate from an idiom dictionary&#8212;but see below for discussion).</S>
    <S sid="30" ssid="7">Schone &amp; Jurafsky (2001) evaluated a number of co-occurrence-based metrics for identifying MWEs, showing that, as suggested by Lin&#8217;s results, there was need for improvement in this area.</S>
    <S sid="31" ssid="8">Since LSA has been used in a number of meaning-related language tasks to good effect (Landauer and Dumais, 1997; Landauer and Psotka, 2000; Cederberg and Widdows, 2003), they had hoped to improve their results by identify non-compositional expressions using a method similar to that which we are exploring here.</S>
    <S sid="32" ssid="9">Although they do not demonstrate that this method actually identifies non-compositional expressions, they do show that the LSA similarity technique only improves MWE identification minimally.</S>
    <S sid="33" ssid="10">Baldwin et al., (2003) focus more narrowly on distinguishing English noun-noun compounds and verb-particle constructions which are compositional from those which are not compositional.</S>
    <S sid="34" ssid="11">Their approach is methodologically similar to ours, in that they compute similarity on the basis of contexts of occurrance, making use of LSA.</S>
    <S sid="35" ssid="12">Their hypothesis is that high LSA-based similarity between the MWE and each of its constituent parts is indicative of compositionality.</S>
    <S sid="36" ssid="13">They evaluate their technique by assessing the correlation between high semantic similarity of the constituents of an MWE to the MWE as a whole with the likelihood that the MWE appears in WordNet as a hyponym of one of the constituents.</S>
    <S sid="37" ssid="14">While the expected correlation was not attested, we suspect this to be more an indication of the inappropriateness of the evaluation used than of the faultiness of the general approach.</S>
    <S sid="38" ssid="15">Lin, Baldwin et al., and Schone &amp; Jurafsky, all use as their gold standard either idiom dictionaries or WordNet (Fellbaum, 1998).</S>
    <S sid="39" ssid="16">While Schone &amp; Jurafsky show that WordNet is as good a standard as any of a number of machine readable dictionaries, none of these authors shows that the MWEs that appear in WordNet (or in the MRDs) are generally non-compositional, in the relevant sense.</S>
    <S sid="40" ssid="17">As noted by Sag et al. (2002) many MWEs are simply &#8220;institutionalized phrases&#8221; whose meanings are perfectly compositional, but whose frequency of use (or other non-linguistic factors) make them highly salient.</S>
    <S sid="41" ssid="18">It is certainly clear that many MWEs that appear in WordNet&#8212;examples being law student, medical student, college man&#8212;are perfectly compositional semantically.</S>
    <S sid="42" ssid="19">Zhai (1997), in an early attempt to apply statistical methods to the extraction of noncompositional MWEs, made use of what we take to be a more appropriate evaluation metric.</S>
    <S sid="43" ssid="20">In his comparison among a number of different heuristics for identifying non-compositional noun-noun compounds, Zhai did his evaluation by applying each heuristic to a corpus of items hand-classified as to their compositionality.</S>
    <S sid="44" ssid="21">Although Zhai&#8217;s classification appears to be problematic, we take this to be the appropirate paradigm for evaluation in this domain, and we adopt it here.</S>
  </SECTION>
  <SECTION title="3 Proceedure" number="3">
    <S sid="45" ssid="1">In our work we made use of the Word Space model of (semantic) similiarty (Sch&#168;utze, 1998) and extended it slightly to MWEs.</S>
    <S sid="46" ssid="2">In this framework, &#8220;meaning&#8221; is modeled as an n-dimensional vector, derived via singular value decomposition (Deerwester et al., 1990) from word co-occurrence counts for the expression in question, a technique frequently referred to as Latent Semantic Analysis (LSA).</S>
    <S sid="47" ssid="3">This kind of dimensionality reduction has been shown to improve performance in a number of text-based domains (Berry et al., 1999).</S>
    <S sid="48" ssid="4">For our experiments we used a local German newspaper corpus.2 We built our LSA model with the Infomap Software package.3, using the 1000 most frequent words not on the 102-word hand-generated stop list as the content-bearing dimension words (the columns of the matrix).</S>
    <S sid="49" ssid="5">The 20,000 most frequent content words were assigned row values by counting occurrences within a 30word window.</S>
    <S sid="50" ssid="6">SVD was used to reduce the dimensionality from 1000 to 100, resulting in 100 dimensional &#8220;meaning&#8221;-vectors for each word.</S>
    <S sid="51" ssid="7">In our experiments, MWEs were assigned meaningvectors as a whole, using the same proceedure.</S>
    <S sid="52" ssid="8">For meaning similarity we adopt the standard measure of cosine of the angle between two vectors (the normalized correlation coefficient) as a metric (Sch&#168;utze, 1998; Baeza-Yates and Ribeiro-Neto, 1999).</S>
    <S sid="53" ssid="9">On this metric, two expressions are taken to be unrelated if their meaning vectors are orthogonal (the cosine is 0) and synonymous if their vectors are parallel (the cosine is 1).</S>
    <S sid="54" ssid="10">Figure 1 illustrates such a vector space in two dimensions.</S>
    <S sid="55" ssid="11">Note that the meaning vector for L&#168;offel &#8216;spoon&#8217; is quite similar to that for essen &#8216;to eat&#8217; but distant from sterben &#8216;to die&#8217;, while the meaning vector for the MWE den L&#168;offel abgeben is close to that for sterben.</S>
    <S sid="56" ssid="12">Indeed den L&#168;offel abgeben, like to kick the bucket, is a noncompositional idiom meaning &#8216;to die&#8217;.</S>
    <S sid="57" ssid="13">While den L&#168;offel abgeben is used almost exclusively in its idiomatic sense (all four occurrences in our corpus), many MWEs are used regularly in both their idiomatic and in their literal senses.</S>
    <S sid="58" ssid="14">About two thirds of the uses of the MWE ins Wasser fallen in our corpus are idiomatic uses, and the remaing one third are literal uses.</S>
    <S sid="59" ssid="15">In our first experiment we tested the hypothesis that these uses could reliably be distinguished using distribution-based models of their meaning.</S>
    <S sid="60" ssid="16">For this experiment we manually annotated the 67 occurrences of ins Wasser fallen in our corpus as to whether the expression was used compositionally (literally) or non-compositionally (idiomatically).4 Marking this distinction we generate an LSA meaning vectors for the compositional uses and an LSA meaning vector for the non-compositional uses of ins Wasser fallen.</S>
    <S sid="61" ssid="17">The vectors turned out, as expected, to be almost orthogonal, with a cosine of the angle between them of 0.02.</S>
    <S sid="62" ssid="18">This result confirms that the linguistic contexts in which the literal and the idiomatic use of ins Wasser fallen appear are very different, indicating&#8212;not surprisingly&#8212;that the semantic difference between the literal meaning and the idiomatic meaning is reflected in the way these these phrases are used.</S>
    <S sid="63" ssid="19">Our next task was to investigate whether this difference could be used in particular cases to determine what the intended use of an MWE in a particular context was.</S>
    <S sid="64" ssid="20">To evaluate this, we did a 10-fold cross-validation study, calculating the literal and idiomatic vectors for ins Wasser fallen on the basis of the training data and doing a simple nearest neighbor classification of each memember of the test set on the basis of the meaning vectors computed from its local context (the 30 word window).</S>
    <S sid="65" ssid="21">Our result of an average accurace of 72% for our LSA-based classifier far exceeds the simple maximum-likelihood baseline of 58%.</S>
    <S sid="66" ssid="22">In the final part of this experiment we compared the meaning vector that was computed by summing over all uses of ins Wasser fallen with the literal and idiomatic vectors from above.</S>
    <S sid="67" ssid="23">Since idiomatic uses of ins Wasserfallen prevail in the corpus (2/3 vs. 1/3), it is not surprisingly that the similarity to the literal vector (0.0946) is much than similarity to the idiomatic vector (0.3712).</S>
    <S sid="68" ssid="24">To summarize Experiment I, which is a variant of a supervised phrase sense disambiguation task, demonstrates that we can use LSA to distinguish between literal and the idiomatic usage of an MWE by using local linguistic context.</S>
    <S sid="69" ssid="25">4This was a straightforward task; two annotators annotated independently, with very high agreement&#8212;kappa score of over 0.95 (Carletta, 1996).</S>
    <S sid="70" ssid="26">Occurrences on which the annotators disagreed were thrown out.</S>
    <S sid="71" ssid="27">Of the 64 occurrences we used, 37 were idiomatic and 27 were literal.</S>
    <S sid="72" ssid="28">In our second experiment we sought to make use of the fact that there are typically clear distributional difference between compositional and non-compositional uses of MWEs to determine whether a given MWE indeed has noncompositional uses at all.</S>
    <S sid="73" ssid="29">In this experiment we made use of a test set of German Preposition-Noun-Verb &#8220;collocation candidate&#8221; database whose extraction is described by Krenn (2000) and which has been made available electronically.5 From this database only word combinations with frequency of occurrence more than 30 in our test corpus were considered.</S>
    <S sid="74" ssid="30">Our task was to classify these 81 potential MWEs according whether or not thay have an idiomatic meaning.</S>
    <S sid="75" ssid="31">To accomplish this task we took the following approach.</S>
    <S sid="76" ssid="32">We computed on the basis of the distribution of the components of the MWE an estimate for the compositional meaning vector for the MWE.</S>
    <S sid="77" ssid="33">We then compared this to the actual vector for the MWE as a whole, with the expectation MWEs which indeed have non-compositinoal uses will be distinguished by a relatively low vector similarity between the estimated compositional meaning vector and the actual meaning vector.</S>
    <S sid="78" ssid="34">In other words small similarity values should be diagnostic for the presense of non-compositinoal uses of the MWE.</S>
    <S sid="79" ssid="35">We calculated the estimated compositional meaning vector by taking it to be the sum of the meaning vector of the parts, i.e., the compositional meaning of an expression w1w2 consisting of two words is taken to be sum of the meaning vectors for the constituent words.6 In order to maximize the independent contribution of the constituent words, the meaning vectors for these words were always computed from contexts in which they appear alone (that is, not in the local context of the other constituent).</S>
    <S sid="80" ssid="36">We call the estimated compositional meaning vector the &#8220;composed&#8221; vector.7 The comparisons we made are illustrated in Figure 2, where vectors for the MWE auf die Strecke bleiben &#8216;to fall by the wayside&#8217; and the words Strecke &#8216;route&#8217; and bleiben &#8216;to stay&#8217; are mapped into two dimensions8.</S>
    <S sid="81" ssid="37">(the words Autobahn &#8216;highway&#8217; and eigenst&#168;andig &#8216;independent&#8217; are given for comparison).</S>
    <S sid="82" ssid="38">Here we see that the linear combination of the component words of the MWE is clearly distinct from that of the MWE as a whole.</S>
    <S sid="83" ssid="39">As a further illustration of the difference between the composed vector and the MWE vector, in Table 2 we list the words whose meaning vector is most similar to that of the MWE auf dis Strecke bleiben along with their similarity values, and in Table 3 we list those words whose meaning vector is most similar to the composed vector.</S>
    <S sid="84" ssid="40">The semantic differences among these two classes are readily apparent.</S>
    <S sid="85" ssid="41">0.769663 0.732372 0.731411 0.717294 0.704939 strecken &#8216;to lengthen&#8217; 0.743309 fahren &#8216;to drive&#8217; 0.741059 laufen &#8216;to run&#8217; 0.726631 fahrt &#8216;drives&#8217; 0.712352 schlie&#223;en &#8216;to close&#8217; 0.704364 We recognize that the composed vector is clearly nowhere near a perfect model of compositional meaning in the general case.</S>
    <S sid="86" ssid="42">This can be illustrated by considering, for example, the MWE fire breathing.</S>
    <S sid="87" ssid="43">This expression is clearly compositional, as it denotes the process of producing combusting exhalation, exactly what the semantic combination rules of the English would predict.</S>
    <S sid="88" ssid="44">Nevertheless the distribution of fire breathing is quite unrelated to that of its constituents fire and breathing ( the former appears frequently with dragon and circus while the later appear frequently with blaze and lungs, respectively).</S>
    <S sid="89" ssid="45">Despite these principled objections, the composed vector provides a useful baseline for our investigation.</S>
    <S sid="90" ssid="46">We should note that a number of researchers in the LSA tradition have attempted to provide more compelling combinatory functions to capture the non-linearity of linguistic compositional interpretation (Kintsch, 2001; Widdows and Peters, 2003).</S>
    <S sid="91" ssid="47">As a check we chose, at random, a number of simple clearly-compositional word combinations (not from the candidate MWE list).</S>
    <S sid="92" ssid="48">We expected that on the whole these would evidence a very high similarity measure when compared with their associated composed vector, and this is indeed the case, as shown in Table 1.</S>
    <S sid="93" ssid="49">We also compared the literal and non-literal vectors for ins Wasser fallen from the first experiment with the composed vector, computed out of the meaning vectors for Wasser and for fallen.9 The difference isn&#8217;t large, but nevertheless the composed vector is more similar to the literal vector (cosine of 0.2937) than to the non-literal vector (cosine of 0.1733).</S>
    <S sid="94" ssid="50">Extending to the general case, our task was to compare the composed vector to the actual vector for all the MWEs in our test set.</S>
    <S sid="95" ssid="51">The resulting cosine similarity values range from 0.01 to 0.80.</S>
    <S sid="96" ssid="52">Our hope was that there would be a similarity threshold for distinguishing MWEs that have non-compositional interpretations from those that do not.</S>
    <S sid="97" ssid="53">Indeed of the MWEs with a similarity values of under 0.1, just over half are MWEs which were hand-annotated to have non-literal uses.10 It used in their idiomatic sense (apparently for humorous effect) particularly frequently in contexts in which elements of the literal meaning were also present.11 is clear then that the technique described is, prima facie, capable of detecting idiomatic MWEs.</S>
    <S sid="98" ssid="54">To evaluate the method, we used the careful manual annotation of the PNV database described by Krenn (2000) as our gold standard.</S>
    <S sid="99" ssid="55">By adopting different threshholds for the classification decision, we obtained a range of results (trading off precision and recall).</S>
    <S sid="100" ssid="56">Table 4 illustrates this range.</S>
    <S sid="101" ssid="57">The F-score measure is maximized in our experiments by adopting a similarity threshold of 0.2.</S>
    <S sid="102" ssid="58">This means that MWEs which have a meaning vector whose cosine is under this value when compared with with the combined vector should be classified as having a non-literal meaning.</S>
    <S sid="103" ssid="59">To compare our method with that proposed by Baldwin et al. (2003), we applied their method to our materials, generating LSA vectors for the component content words in our candidate MWEs and comparing their semantic similarity to the MWEs LSA vector as a whole, with the expectation being that low similarity between the MWE as a whole and its component words is indication of the non-compositionality of the MWE.</S>
    <S sid="104" ssid="60">The results are given in Table 5.</S>
    <S sid="105" ssid="61">It is clear that while Baldwin et al.&#8217;s expectation is borne out in the case of the constituent noun (the non-head), it is not in the case of the constituent verb (the head).</S>
    <S sid="106" ssid="62">Even in the case of the nouns, however, the results are, for the most part, markedly inferior to the results we achieved using the composed vectors.</S>
    <S sid="107" ssid="63">There are a number of issues that complicate the workability of the unsupervised technique described here.</S>
    <S sid="108" ssid="64">We rely on there being enough non-compositional uses of an idiomatic MWE in the corpus that the overall meaning vector for the MWE reflects this usage.</S>
    <S sid="109" ssid="65">If the literal meaning is overwhelmingly frequent, this will reduce the effectivity of the method significantly.</S>
    <S sid="110" ssid="66">A second problem concerns the relationship between the literal and the non-literal meaning.</S>
    <S sid="111" ssid="67">Our technique relies on these meaning being highly distinct.</S>
    <S sid="112" ssid="68">If the meanings are similar, it is likely that local context will be inadequate to distinguish a compositional from a non-compositional use of the expression.</S>
    <S sid="113" ssid="69">In our investigation it became apparent, in fact, that in the newspaper genre, highly idiomatic expressions such as ins Wasser fallen were often</S>
  </SECTION>
  <SECTION title="Appendix I." number="4">
    <S sid="114" ssid="1">To summarize, in order to classify an MWE as non-compositional, we compute an approximation of its compositional meaning and compare this with the meaning of the expression as it is used on the whole.</S>
    <S sid="115" ssid="2">One of the obvious improvements to the algorithm could come from better models for simulating compositional meaning.</S>
    <S sid="116" ssid="3">A further issue that can be explored is whether linguistic preprocessing would influence the results.</S>
    <S sid="117" ssid="4">We worked only on raw text data.</S>
    <S sid="118" ssid="5">There is some evidence (Baldwin et al., 2003) that part of speech tagging might improve results in this kind of task.</S>
    <S sid="119" ssid="6">We also only considered local word sequences.</S>
    <S sid="120" ssid="7">Certainly some recognition of the syntactic structure would improve results.</S>
    <S sid="121" ssid="8">These are, however, more general issues associated with MWE processing.</S>
    <S sid="122" ssid="9">Rather promising results were attained using only local context, however.</S>
    <S sid="123" ssid="10">Our study shows that the F-score measure is maximized by taking as threshold for distinguishing non-compositional phrases from compositional ones a cosine similarity value somewhere between 0.1-0.2.</S>
    <S sid="124" ssid="11">An important point to be explored is that compositionality appears to come in degrees.</S>
    <S sid="125" ssid="12">As Bannard and Lascarides (2003) have noted, MWEs &#8220;do not fall cleanly into the binary classes of compositional and non-compositional expressions, but populate a continuum between the two extremes.&#8221; While our experiment was designed to classify MWEs, the technique described here, of course, provides a means, if rather a blunt one, for quantifying the degreee of compositonality of an expression.</S>
  </SECTION>
</PAPER>
