Learning Dependency Translation Models As Collections Of Finite-State Head Transducers
The paper defines weighted head transducers, finite-state machines that perform middle-outstring transduction.
These transducers are strictly more expressive than the special case of standard left-to-right finite-state transducers.
Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically.
A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model.
A method for automatically training a dependency transduction model from a set of input-output example strings is presented.
The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments.
Experimental results are given for applying the training method to translation from English to Spanish and Japanese.
We treat translation as a process of simultaneous induction of source and target dependency trees using head transduction.
We present a two-level arranged word ordering and chunk ordering by a hierarchically organized collection of finite state transducers.
We induce parallel tree structures from unbracketed parallel text, modeling the generation of each node's children with a finite-state transducer.
