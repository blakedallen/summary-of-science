Driving Semantic Parsing from the World&rsquo;s Response
Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms.
Providing this supervision is a major bottleneck in scaling semantic parsers.
This paper presents a new learning paradigm aimed at alleviating the supervision burden.
We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world.
In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision.
Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers.
We train systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers.
