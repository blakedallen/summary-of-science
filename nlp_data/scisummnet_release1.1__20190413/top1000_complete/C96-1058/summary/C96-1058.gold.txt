Three New Probabilistic Models For Dependency Parsing: An Exploration
After presenting a novel O(n3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it.
We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (e) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer.
We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank).
In these results, the generative model performs significantly better than the others, and does about equally well at assigning part-of-speech tags.
The proposed parsing algorithm is sufficient for searching over all projective trees in O (n3) time.
