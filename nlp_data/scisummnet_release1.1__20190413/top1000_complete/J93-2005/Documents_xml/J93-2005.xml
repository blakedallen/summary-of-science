<PAPER>
  <S sid="0">Lexical Semantic Techniques For Corpus Analysis</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this paper we outline a research program for computational linguistics, making extensive use of text corpora.</S>
    <S sid="2" ssid="2">We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence.</S>
    <S sid="3" ssid="3">The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items.</S>
    <S sid="4" ssid="4">Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic among words appearing in systems. illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary.</S>
    <S sid="5" ssid="5">In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools.</S>
    <S sid="6" ssid="6">Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="7" ssid="1">In this paper we outline a research program for computational linguistics, making extensive use of text corpora.</S>
    <S sid="8" ssid="2">We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence.</S>
    <S sid="9" ssid="3">The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items.</S>
    <S sid="10" ssid="4">Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems.</S>
    <S sid="11" ssid="5">We illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary.</S>
    <S sid="12" ssid="6">In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools.</S>
    <S sid="13" ssid="7">Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="14" ssid="1">The proliferation of on-line textual information poses an interesting challenge to linguistic researchers for several reasons.</S>
    <S sid="15" ssid="2">First, it provides the linguist with sentence and word usage information that has been difficult to collect and consequently largely ignored by linguists.</S>
    <S sid="16" ssid="3">Second, it has intensified the search for efficient automated indexing and retrieval techniques.</S>
    <S sid="17" ssid="4">Full-text indexing, in which all the content words in a document are used as keywords, is one of the most promising of recent automated approaches, yet its mediocre precision and recall characteristics indicate that there is much room for improvement (Croft 1989).</S>
    <S sid="18" ssid="5">The use of domain knowledge can enhance the effectiveness of a full-text system by providing related terms that can be used to broaden, narrow, or refocus a query at retrieval time (Debili, Fluhr, and Radasua 1988; Anick et al. 1989.</S>
    <S sid="19" ssid="6">Likewise, domain knowledge may be applied at indexing time to do word sense disambiguation (Krovetz and Croft 1989) or content analysis (Jacobs 1991).</S>
    <S sid="20" ssid="7">Unfortunately, for many domains, such knowledge, even in the form of a thesaurus, is either not available or is incomplete with respect to the vocabulary of the texts indexed.</S>
    <S sid="21" ssid="8">In this paper we examine how linguistic phenomena such as metonymy and polysemy might be exploited for the semantic tagging of lexical items.</S>
    <S sid="22" ssid="9">Unlike purely statistical collocational analyses, employing a semantic theory allows for the automatic construction of deeper semantic relationships among words appearing in collocational systems.</S>
    <S sid="23" ssid="10">We illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary.</S>
    <S sid="24" ssid="11">In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations (Wilks 1978) can also be acquired from corpora, when analyzed with the appropriate semantic tools.</S>
    <S sid="25" ssid="12">Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses.</S>
    <S sid="26" ssid="13">The aim of our research is to discover what kinds of knowledge can be reliably acquired through the use of these methods, exploiting, as they do, general linguistic knowledge rather than domain knowledge.</S>
    <S sid="27" ssid="14">In this respect, our program is similar to Zernik's (1989) work on extracting verb semantics from corpora using lexical categories.</S>
    <S sid="28" ssid="15">Our research, however, differs in two respects: first, we employ a more expressive lexical semantics; second, our focus is on all major categories in the language, and not just verbs.</S>
    <S sid="29" ssid="16">This is important since for full-text information retrieval, information about nominals is paramount, as most queries tend to be expressed as conjunctions of nouns.</S>
    <S sid="30" ssid="17">From a theoretical perspective, we believe that the contribution of the lexical semantics of nominals to the overall structure of the lexicon has been somewhat neglected, relative to that of verbs.</S>
    <S sid="31" ssid="18">While Zernik (1989) presents ambiguity and metonymy as a potential obstacle to effective corpus analysis, we believe that the existence of motivated metonymic structures actually provides valuable clues for semantic analysis of nouns in a corpus.</S>
    <S sid="32" ssid="19">We will assume, for this paper, the general framework of a generative lexicon as outlined in Pustejovsky (1991).</S>
    <S sid="33" ssid="20">In particular, we make use of the principles of type coercion and qualia structure.</S>
    <S sid="34" ssid="21">This model of semantic knowledge associated with words is based on a system of generative devices that is able to recursively define new word senses for lexical items in the language.</S>
    <S sid="35" ssid="22">These devices and the associated dictionary make up a generative lexicon, where semantic information is distributed throughout the lexicon to all categories.</S>
    <S sid="36" ssid="23">The general framework assumes four basic levels of semantic description: argument structure, qualia structure, lexical inheritance structure, and event structure.</S>
    <S sid="37" ssid="24">Connecting these different levels is a set of generative devices that provide for the compositional interpretation of words in context.</S>
    <S sid="38" ssid="25">The most important of these devices is a semantic transformation called type coercion&#8212;analogous to coercion in programming languages&#8212;which captures the semantic relatedness between syntactically distinct expressions.</S>
    <S sid="39" ssid="26">As an operation on types within a A-calculus, type coercion can be seen as transforming a monomorphic language into one with polymorphic types (cf.</S>
    <S sid="40" ssid="27">Cardelli and Wegner 1985).</S>
    <S sid="41" ssid="28">Argument, event, and qualia types must conform to the well-formedness conditions defined by the type system defined by the lexical inheritance structure when undergoing operations of semantic composition.'</S>
    <S sid="42" ssid="29">One component of this approach, the qualia structure, specifies the different aspects of a word's meaning through the use of subtyping.</S>
    <S sid="43" ssid="30">These include the subtypes CONSTITUTIVE, FORMAL, TELIC, and AGENTIVE.</S>
    <S sid="44" ssid="31">To illustrate how these are used, the qualia structure for book is given below.2 This structured representation allows one to use the same lexical entry in different contexts, where the word refers to different qualia of the noun's denotation.</S>
    <S sid="45" ssid="32">For example, the sentences in (1)&#8212;(3) below refer to different aspects (or qualia) of the general meaning of book.3 Example 1 This book weighs four ounces.</S>
    <S sid="46" ssid="33">Example 2 John finished a book.</S>
    <S sid="47" ssid="34">This is an interesting book.</S>
    <S sid="48" ssid="35">Example 1 makes reference to the formal role, while 3 refers to the constitutive role.</S>
    <S sid="49" ssid="36">Example 2, however, can refer to either the telic or the agentive aspects given above.</S>
    <S sid="50" ssid="37">The utility of such knowledge for information retrieval is readily apparent.</S>
    <S sid="51" ssid="38">This theory claims that noun meanings should make reference to related concepts and the relations into which they enter.</S>
    <S sid="52" ssid="39">The qualia structure, thus, can be viewed as a kind of generic template for structuring this knowledge.</S>
    <S sid="53" ssid="40">Such information about how nouns relate to other lexical items and their concepts might prove to be much more useful in full-text information retrieval than what has come from standard statistical techniques.</S>
    <S sid="54" ssid="41">To illustrate how such semantic structuring might be useful, consider the general class of artifact nouns.</S>
    <S sid="55" ssid="42">A generative view of the lexicon predicts that by classifying an element into a particular category, we can generate many aspects of its semantic structure, and hence, its syntactic behavior.</S>
    <S sid="56" ssid="43">For example, the representation above for book refers to several word senses, all of which are logically related by the semantic template for an artifactual object.</S>
    <S sid="57" ssid="44">That is, it contains information, it has a material extension, it serves some function, and it is created by some particular act or event.</S>
    <S sid="58" ssid="45">In the qualia structures given below, we adopt the convention that [a, 0] denotes conjunction of formulas within the feature structure, while [a; 0] will denote disjunction.</S>
    <S sid="59" ssid="46">Such an analysis allows us to minimally structure objects according to these four qualia.</S>
    <S sid="60" ssid="47">As an example of how objects cluster according to these dimensions, we will briefly consider three object types: (1) containers (of information), e.g., book, tape, record; (2) instruments, e.g., gun, hammer, paintbrush; and (3) figure-ground objects, e.g., door, room, fireplace.</S>
    <S sid="61" ssid="48">Because of how their qualia structures differ, these classes appear in vastly different grammatical contexts.</S>
    <S sid="62" ssid="49">As with containers in general, information containers permit metonymic extensions between the container and the material contained within it.</S>
    <S sid="63" ssid="50">Collocations such as those in Examples 4 through 7 indicate that this metonymy is grammaticalized through specific and systematic head-PP constructions. read the information on the tape Instruments, on the other hand, display classic agent&#8212;instrument causative alternations, such as those in Examples 8 through 11 (cf.</S>
    <S sid="64" ssid="51">Fillmore 1968; Lakoff 1968, 1970).</S>
    <S sid="65" ssid="52">... smash the vase with the hammer The hammer smashed the vase.</S>
    <S sid="66" ssid="53">... kill him with a gun The gun killed him.</S>
    <S sid="67" ssid="54">Finally, figure-ground nominals (Pustejovsky and Anick 1988) permit perspective shifts such as those in Examples 12 through 15.</S>
    <S sid="68" ssid="55">These are nouns that refer to physical objects as well as the specific enclosure or aperture associated with it.</S>
    <S sid="69" ssid="56">John painted the door.</S>
    <S sid="70" ssid="57">John walked through the door.</S>
    <S sid="71" ssid="58">John is scrubbing the fireplace.</S>
    <S sid="72" ssid="59">The smoke filled the fireplace.</S>
    <S sid="73" ssid="60">That is, paint and scrub are actions on physical objects while walk through and fi// are processes in spaces.</S>
    <S sid="74" ssid="61">These collocational patterns, we argue, are systematically predictable from the lexical semantics of the noun, and we term such sets of collocated structures lexical conceptual paradigms (LCPs).4 To make this point clearer, let us consider a specific example of an LCP from the computer science domain, namely for the noun tape.</S>
    <S sid="75" ssid="62">Because of the particular metonymy observed for a noun like tape, we will classify it as belonging to the container/containee LCP.</S>
    <S sid="76" ssid="63">This general class is represented as follows, where P and Q are predicate variables:5 The LCP is a generic qualia structure that captures not only the semantic relationship between arguments types of a relation, but also, through corpus-tuning, the collocation relations that realize these roles.</S>
    <S sid="77" ssid="64">The telic function of a container, for example, is the relation hold, but this underspecifies which spatial prepositions would adequately satisfy this semantic relation (e.g. in, on, inside, etc.).</S>
    <S sid="78" ssid="65">In this view, a noun such as tape would have the following qualia structure: This states that a tape is an &amp;quot;information container&amp;quot; that is also a two-dimensional physical object, where the information is written onto the object.'</S>
    <S sid="79" ssid="66">With such nouns, a logical metonymy exists (as the result of type coercion), when the logical argument of a semantic type, which is selected by a function of some sort, denotes the semantic type itself.</S>
    <S sid="80" ssid="67">Thus, in this example, the type selected for by a verb such as read refers to the &amp;quot;information&amp;quot; argument for tape, while a verb such as carry would select for the &amp;quot;physical object&amp;quot; argument.</S>
    <S sid="81" ssid="68">They are, however, logically related, since the noun itself denotes a relation.</S>
    <S sid="82" ssid="69">The representation above simply states that any semantics for tape must logically make reference to the object itself (formal), what it can contain (const), what purpose it serves (telic), and how it arises (agentive).</S>
    <S sid="83" ssid="70">This provides us with a semantic representation that can capture the multiple perspectives a single lexical item may assume in different contexts.</S>
    <S sid="84" ssid="71">Yet, the qualia for a lexical item such as tape are not isolated values for that one word, but are integrated into a global knowledge base indicating how these senses relate to other lexical items and their senses.</S>
    <S sid="85" ssid="72">This is the contribution of inheritance and the hierarchical structuring of knowledge (cf.</S>
    <S sid="86" ssid="73">Evans and Gazdar 1990; Copestake and Briscoe 1992; Russell et al. 1992).</S>
    <S sid="87" ssid="74">In Pustejovsky (1991) it is suggested that there are two types of relational structures for lexical knowledge; a fixed inheritance similar to that of an is-a hierarchy (cf.</S>
    <S sid="88" ssid="75">Touretzky 1986); and a dynamic structure that operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories.'</S>
    <S sid="89" ssid="76">Reviewing briefly, the basic idea is that semantics allows for the dynamic creation of arbitrary concepts through the application of certain transformations to lexical meanings.</S>
    <S sid="90" ssid="77">Thus for every predicate, Q, we can generate its opposition, -Q.</S>
    <S sid="91" ssid="78">Similarly, these two predicates can be related temporally to generate the transition events defining this opposition.</S>
    <S sid="92" ssid="79">These operations include but may not be limited to: negation; &lt;, temporal precedence; &gt;, temporal succession; =, temporal equivalence; and act, an operator adding agency to an argument.</S>
    <S sid="93" ssid="80">We will call the concept space generated by these operations the Projective Conclusion Space of a specific quale for a lexical item.</S>
    <S sid="94" ssid="81">To return to the example of tape above, the predicates read and copy are related to the telic value by just such an operation, while predicates such as mount and dismount&#8212;i.e. unmount&#8212;are related to the formal role.</S>
    <S sid="95" ssid="82">Following the previous discussion, with mounted as the predicate Q, successive applications of the negation and temporal precedence operators derives the transition verbs mount and dismount.'</S>
    <S sid="96" ssid="83">We return to a discussion of this in Section 3, and to how this space relates to statistically significant collocations in text.</S>
    <S sid="97" ssid="84">It is our view that the approach outlined above for representing lexical knowledge can be put to use in the service of information retrieval tasks.</S>
    <S sid="98" ssid="85">In this respect, our proposal can be compared to attempts at object classification in information science.</S>
    <S sid="99" ssid="86">One approach, known as faceted classification (Vickery 1975) proceeds roughly as follows: collect all terms lying within a field; then group the terms into facets by assigning them to categories.</S>
    <S sid="100" ssid="87">Typical examples of this are state, property, reaction, and device.</S>
    <S sid="101" ssid="88">However, each subject area is likely to have its own sets of categories, which makes it difficult to re-use a set of facet classifications.9 Even if the relational information provided by the qualia structure and inheritance would improve performance in information retrieval tasks, one problem still remains, namely that it would be very time-consuming to hand-code such structures for all nouns in a domain.</S>
    <S sid="102" ssid="89">Since it is our belief that such representations are generic structures across all domains, it is our long-term goal to develop methods for automatically extracting these relations and values from on-line corpora.</S>
    <S sid="103" ssid="90">In the sections that follow, we describe several experiments indicating that the qualia structures do, in fact, correlate with well-behaved collocational patterns, thereby allowing us to perform structure-matching operations over corpora to find these relations.</S>
    <S sid="104" ssid="91">In this section we discuss briefly how a lexical semantic theory can help in extracting information from machine-readable dictionaries (MRDs).</S>
    <S sid="105" ssid="92">We describe research on conversion of a machine-tractable dictionary (Wilks et al. 1993) into a usable lexical knowledge base (Boguraev 1991).</S>
    <S sid="106" ssid="93">Although the results here are preliminary, it is important to mention the process of converting an MRD into a lexical knowledge base, so that the process of corpus-tuning is put into the proper perspective.</S>
    <S sid="107" ssid="94">The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978).</S>
    <S sid="108" ssid="95">These are then automatically adapted to the format of generative lexical structures.</S>
    <S sid="109" ssid="96">It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992).</S>
    <S sid="110" ssid="97">Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques.</S>
    <S sid="111" ssid="98">Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et al. (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section.</S>
    <S sid="112" ssid="99">Our goal is to automate, to the extent possible, the initial construction of these structures.</S>
    <S sid="113" ssid="100">Extensive research has been done on the kind of information needed by natural language programs and on the representation of that information (Wang, Vandendorpe, and Evens 1985; Ahlswede and Evens 1988).</S>
    <S sid="114" ssid="101">Following Boguraev et al. (1989) and Wilks et al. of 1989), we believe that much of what is needed for NLP lexicons can be found either explicitly or implicitly in a dictionary, and empirical evidence suggests that this information gives rise to a sufficiently rich lexical representation for use in extracting information from texts.</S>
    <S sid="115" ssid="102">Techniques for identifying explicit information in machine-readable dictionaries have been developed by many researchers (Boguraev et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al.</S>
    <S sid="116" ssid="103">1990) and are well understood.</S>
    <S sid="117" ssid="104">Many properties of a word sense or the semantic relationships between word senses are available in MRDs, but this information can only be identified computationally through some analysis of the definition text of an entry (Atkins 1991).</S>
    <S sid="118" ssid="105">Some research has already been done in this area.</S>
    <S sid="119" ssid="106">Alshawi (1987), Boguraev et al. (1989), Vossen, Meijs, and den Broeder (1989), and the work described in Wilks et al.</S>
    <S sid="120" ssid="107">(1992) have made explicit some kinds of implicit information found in MRDs.</S>
    <S sid="121" ssid="108">Here we propose to refine and merge some of the previous techniques to make explicit the implicit information specified by a theory of generative lexicons.</S>
    <S sid="122" ssid="109">Given what we described above for the lexical structures for nominals, we can identify these semantic relations in the OALD and LDOCE by pattern matching on the parse trees of definitions.</S>
    <S sid="123" ssid="110">To illustrate what specific information can be derived by automatic seeding from machine-readable dictionaries, consider the following examples.1&#176; For example, the LDOCE definition for book is: &amp;quot;a collection of sheets of paper fastened together as a thing to be read, or to be written in&amp;quot; while the OALD provides a somewhat different definition: &amp;quot;number of sheet of papers, either printed or blank, fastened together in a cover.&amp;quot; Note that both definitions are close to, but not identical to the information structure suggested in the previous section, using a qualia structure for nominals.</S>
    <S sid="124" ssid="111">LDOCE suggests write in rather than write as the value for the telic role, while the OALD suggests nothing for this role.</S>
    <S sid="125" ssid="112">Furthermore, although the physical contents of a book as &amp;quot;a collection of sheets of paper&amp;quot; is mentioned, nowhere is information made reference to in the definition.</S>
    <S sid="126" ssid="113">When the dictionary fails to provide the value for a semantic role, the information must be either hand-entered or the lexical structure must be tuned against a large corpus, in the hope of extracting such features automatically.</S>
    <S sid="127" ssid="114">We turn to this issue in the next two sections.</S>
    <S sid="128" ssid="115">Although the two dictionaries differ in substantial respects, it is remarkable how systematic the definition structures are for extracting semantic information, if there is a clear idea how this information should be structured.</S>
    <S sid="129" ssid="116">For example, from the following OALD definition for cigarette, cigarette n roll of shredded tobacco enclosed in thin paper for smoking. the initial lexical structure below is generated.</S>
    <S sid="130" ssid="117">Parsing the LDOCE entry for the same noun results in a different lexical structure: cigarette n finely cut shredded tobacco rolled in a narrow tube of thin paper for smoking. gls(cigarette, syn( [type (n) , One obvious problem with the above representation is that there is no information indicating how the word being defined binds to the relations in the qualia.</S>
    <S sid="131" ssid="118">Currently, subsequent routines providing for argument binding analyze the relational structure for particular aspects of noun meaning, giving us a lexical structure fairly close to what we need for representation and retrieval purposes, although the result is in no way ideal or uniform over all nominal forms.</S>
    <S sid="132" ssid="119">(cf.</S>
    <S sid="133" ssid="120">Cowie, Guthrie, and Pustejovsky [1992] for details of this operation on LDOCE.</S>
    <S sid="134" ssid="121">):&amp;quot; In a related set of experiments performed while constructing a large lexical database for data extraction purposes, we seeded a lexicon with 6000 verbs from LDOCE.</S>
    <S sid="135" ssid="122">This process and the corpus tuning for both argument typing and subcategorization acquisition are described in Cowie, Guthrie, and Pustejovsky (1992) and Pustejovsky et al. (1992).</S>
    <S sid="136" ssid="123">In summary, based on a theory of lexical semantics, we have discussed how an MRD can be useful as a corpus for automatically seeding lexical structures.</S>
    <S sid="137" ssid="124">Rather than addressing the specific problems inherent in converting MRDs into useful lexicons, we have emphasized how it provides us, in a sense, with a generic vocabulary from which to begin lexical acquisition over corpora.</S>
    <S sid="138" ssid="125">In the next section, we will address the problem of taking these initial, and often very incomplete lexical structures, and enriching them with information acquired from corpus analysis.</S>
    <S sid="139" ssid="126">As mentioned in the previous section, the power of a generative lexicon is that it takes much of the burden of semantic interpretation off of the verbal system by supplying a much richer semantics for nouns and adjectives.</S>
    <S sid="140" ssid="127">This makes the lexical structures ideal as an initial representation for knowledge acquisition and subsequent information retrieval tasks.</S>
  </SECTION>
  <SECTION title="3." number="3">
    <S sid="141" ssid="1">A machine-readable dictionary provides the raw material from which to construct computationally useful representations of the generic vocabulary contained within it.</S>
    <S sid="142" ssid="2">The lexical structures discussed in the previous section are one example of how such information can be exploited.</S>
    <S sid="143" ssid="3">Many sublanguages, however, are poorly represented in on-line dictionaries, if represented at all.</S>
    <S sid="144" ssid="4">Vocabularies geared to specialized domains will be necessary for many applications, such as text categorization and information retrieval.</S>
    <S sid="145" ssid="5">The second area of our research program that we discuss is aimed at developing techniques for building sublanguage lexicons via syntactic and statistical corpus analysis coupled with analytic techniques based on the tenets of generative lexicon theory.</S>
    <S sid="146" ssid="6">To understand fully the experiments described in the next two sections, we will refer to several semantic notions introduced in previous sections.</S>
    <S sid="147" ssid="7">These include type coercion, where a lexical item requires a specific type specification for its argument, and 11 As one reviewer correctly pointed out, more than simple argument binding is involved here.</S>
    <S sid="148" ssid="8">For example, the model must know that paper can enclose shredded tobacco, but not the reverse.</S>
    <S sid="149" ssid="9">Such information, typically part of commonsense knowledge, is well outside the domain of lexical semantics, as envisioned here.</S>
    <S sid="150" ssid="10">One approach to this problem, consistent with our methodology, is to examine the corpus and the collocations that result from training on specific qualia relations.</S>
    <S sid="151" ssid="11">Further work will hopefully clarify the nature of this problem, and whether it is best treated lexically or not. the argument is able to change type accordingly&#8212;this explains the behavior of logical metonymy and the syntactic variation seen in complements to verbs and nominals; and cospecification, a semantic tagging of what collocational patterns the lexical item may enter into.</S>
    <S sid="152" ssid="12">Metonymy, in this view, can be seen as a case of the &amp;quot;licensed violation&amp;quot; of selectional restrictions.</S>
    <S sid="153" ssid="13">For example, while the verb announce selects for a human subject, sentences like The Dow Corporation announced third quarter losses are not only an acceptable paraphrase of the selectionally correct form Mr. Dow Jr. announced third quarter losses for Dow Corp, but they are the preferred form in the corpora being examined.</S>
    <S sid="154" ssid="14">This is an example of subject type coercion, where the semantics for Dow Corp as a company must specify that there is a human typically associated with such official pronouncements (see Section 5).12 For one set of experiments, we used a corpus of approximately 3,000 articles written by Digital Equipment Corporation's Customer Support Specialists for an on-line computer troubleshooting library.</S>
    <S sid="155" ssid="15">The articles, each one- to two-page long descriptions of a problem and its solution, comprise about 1 million words.</S>
    <S sid="156" ssid="16">Our analysis proceeds in two phases.</S>
    <S sid="157" ssid="17">In the first phase, we pre-process the corpus to build a database of phrasal relationships.</S>
    <S sid="158" ssid="18">This consists briefly of the following steps: indicators.</S>
    <S sid="159" ssid="19">Any words that are ambiguous with respect to category are disambiguated according to a set of several dozen ordered disambiguation heuristics, which choose a category based on the categories of the words immediately preceding and following the ambiguous term. transitions, to indicate likely phrase boundaries.</S>
    <S sid="160" ssid="20">No attempt is made to construct a full parse tree or resolve prepositional phrase attachment, conjunction scoping, etc.</S>
    <S sid="161" ssid="21">A concordance is constructed, identifying, for each word appearing in the corpus, the set of sentences, phrases, and phrase locations in which the word appears.</S>
    <S sid="162" ssid="22">12 Within the current framework, a distinction is made between logical metonymy, where the metonymic extension or relation is transparent from the lexical semantics of the coerced phrase, and conventional metonymy, where the relation may not be directly calculated from information provided grammatically.</S>
    <S sid="163" ssid="23">For example, in the sentence &amp;quot;The Boston office called today,&amp;quot; it is not clear from logical metonymy what relation Boston bears to office other than location; i.e., it is not obvious that it is a branch office.</S>
    <S sid="164" ssid="24">This is well beyond lexical semantics (cf.</S>
    <S sid="165" ssid="25">Lakoff 1987 and Martin 1990).</S>
    <S sid="166" ssid="26">The database of partially parsed sentences provides the raw material for a number of sublanguage analyses.</S>
    <S sid="167" ssid="27">This begins the second phase of analysis: querying and thesaurus browsing.</S>
    <S sid="168" ssid="28">We construct bracketed noun compounds from our database of partial parses in a two-step process.</S>
    <S sid="169" ssid="29">The first simply searches the corpus for (recurring) contiguous sequences of nouns.</S>
    <S sid="170" ssid="30">Then, to bracket each compound that includes more than two nouns, we test whether possible subcomponents of the phrase exist on their own (as complete noun compounds) elsewhere in the corpus.</S>
    <S sid="171" ssid="31">Sample bracketed compounds derived from the computer troubleshooting database include [ [syst em management] utility], [TK50 [tape drive] ], [ [database management] system].</S>
    <S sid="172" ssid="32">2.</S>
    <S sid="173" ssid="33">Generation of taxonomic relationships on the basis of collocational information.</S>
    <S sid="174" ssid="34">Technical sublanguages often express subclass relationships in noun compounds of the form &lt;instance-name&gt; &lt;class-name&gt;, as in &amp;quot;Unix operating system&amp;quot; and &amp;quot;C language.&amp;quot; Unfortunately, noun compounds are also employed to express numerous other relationships, as in &amp;quot;Unix kernel&amp;quot; and &amp;quot;C debugger.&amp;quot; We have found, however, that collocational evidence can be employed to suggest which noun compounds reflect taxonomic relationships, using a strategy similar to that employed by Hindle (1990) for detecting synonyms.</S>
    <S sid="175" ssid="35">Given a term T, we extract from the phrase database those nouns N, that appear as the head of any phrase in which T is the immediately preceding term.</S>
    <S sid="176" ssid="36">These nouns represent candidate classes of which T may be a member.</S>
    <S sid="177" ssid="37">We then generate the set of verbs that take T as direct object and calculate the mutual information value for each verb/T collocation (cf.</S>
    <S sid="178" ssid="38">Hindle 1990).</S>
    <S sid="179" ssid="39">We do the same for each noun N. Under the assumption that instance and class nouns are likely to co-occur with the same verbs, we compute a similarity score between T and each noun N&#8222; by summing the product of the mutual information values for those verbs occurring with both nouns.</S>
    <S sid="180" ssid="40">(Verbs with negative mutual information values are left out of the calculation.)</S>
    <S sid="181" ssid="41">The noun with the highest similarity score is often the class of which T is an instance, as illustrated by the sample results in Figure 1.</S>
    <S sid="182" ssid="42">For each word displayed in Figure 1, its &amp;quot;class&amp;quot; is the head noun with the highest similarity score.</S>
    <S sid="183" ssid="43">Other head nouns occurring with the word as modifier are listed as well.</S>
    <S sid="184" ssid="44">As with all the automated procedures described here, this algorithm yields useful, but imperfect results.</S>
    <S sid="185" ssid="45">The class chosen for &amp;quot;VMS,&amp;quot; for example, is incorrect, and may reflect the fact that in a DEC troubleshooting database, authors see no need to further specify VMS as &amp;quot;VMS operating system.&amp;quot; A more interesting observation is that, among the collocations associated with the terms, there are often several that might qualify as classes of which the term is an instance, e.g., DECWindows could also be classified as &amp;quot;software&amp;quot;; TK50 might also qualify as &amp;quot;tape.&amp;quot; From a generative lexicon perspective, these alternative classifications reflect multiple inheritance through the noun's qualia.</S>
    <S sid="186" ssid="46">That is, &amp;quot;cartridge&amp;quot; is further specifying the formal role of tape for TK50.</S>
    <S sid="187" ssid="47">DECWindows is functionally an &amp;quot;environment,&amp;quot; its telic role, while &amp;quot;software&amp;quot; characterizes its formal quale.</S>
    <S sid="188" ssid="48">3.</S>
    <S sid="189" ssid="49">Extraction of information relating to noun's qualia.</S>
    <S sid="190" ssid="50">Under certain circumstances, it may be possible to elicit information about a noun's qualia from automated procedures on a corpus.</S>
    <S sid="191" ssid="51">In this line of research, we hayed employed the notion of &amp;quot;lexical conceptual paradigm&amp;quot; described above.</S>
    <S sid="192" ssid="52">An LCP relates a set of syntactic behaviors to the lexical semantic structures of the participating lexical items.</S>
    <S sid="193" ssid="53">For example, the set of expressions involving the word &amp;quot;tape&amp;quot; in the context of its use as a secondary storage device suggests that it fits the container artifact schema of the qualia structure, with &amp;quot;information&amp;quot; and &amp;quot;file&amp;quot; as its containees: As mentioned in Section 1, containers tend to appear as objects of the prepositions to, from, in, and on as well as in direct object position, in which case they are typically serving metonymically for the containee.</S>
    <S sid="194" ssid="54">Thus, the container LCP relates the set of generalized syntactic patterns V, Ni {to, from, on} Nk vi N This LCP includes a nominal alternation between the container and containee in the object position of verbs.</S>
    <S sid="195" ssid="55">For tape, this alternation is manifested for verbs that predicate the telic role of data storage but not the formal role of physical object, which refers to the object as a whole regardless of its contents: We have explored the use of heuristics to distinguish those predicates that relate to the Telic quale of the noun.</S>
    <S sid="196" ssid="56">Consider the word tape, which occurs as the direct object in 107 sentences in our corpus.</S>
    <S sid="197" ssid="57">It appears with a total of 34 different verbs.</S>
    <S sid="198" ssid="58">By applying the mutual information metric (MI) to the verb&#8212;object pairs, we can sort the verbs accordingly, giving us the table of verbs most highly associated with tape, shown in Figure 2.</S>
    <S sid="199" ssid="59">While the mutual information statistic does a good job of identifying verbs that semantically relate to the word tape, it provides no information about how the verbs relate to the noun's qualia structure.</S>
    <S sid="200" ssid="60">That is, verbs such as unload, position, and mount are selecting for the formal quale of tape, a physical object that can be physically manipulated with respect to a tape drive.</S>
    <S sid="201" ssid="61">Read, write, and copy, on the other hand, relate to the telic role, the function of a tape as a medium for storing information.</S>
    <S sid="202" ssid="62">Our hypothesis was that the nominal alternation can help to distinguish the two sets of verbs.</S>
    <S sid="203" ssid="63">We reasoned that, if the alternation is based on the container/containee metonymy, then it will be those verbs that apply to the telic role of the direct object that participate in the alternation.</S>
    <S sid="204" ssid="64">We tested this hypothesis as follows.</S>
    <S sid="205" ssid="65">We generated a candidate set of containees for tape by identifying all the nouns that appeared in the corpus to the left of the adjunct on tape.</S>
    <S sid="206" ssid="66">Intersection and set difference for three container nouns.</S>
    <S sid="207" ssid="67">Then we took the set of verbs that had one of these containee nouns as a direct object and compared this set to the set of verbs that had the container noun tape as a direct object in the corpus.</S>
    <S sid="208" ssid="68">According to our hypothesis, verbs applying to the telic role should appear in the intersection of these two sets (as a result of the alternation), while those applying to the formal role will appear in the set difference {verbs with containers as direct object}&#8212;{verbs with containees as direct object}.</S>
    <S sid="209" ssid="69">The difference operation should serve to remove any verbs that co-occur with containee objects.</S>
    <S sid="210" ssid="70">Figure 3 shows the results of intersection and set difference for three container nouns tape, disk, and directory.</S>
    <S sid="211" ssid="71">The results indicate that the container LCP is able to differentiate nouns with respect to their telic and formal qualia, for the nouns tape and disk but not for directory.</S>
    <S sid="212" ssid="72">The poor discrimination in the latter case can be attributed to the fact that a directory is a recursive container.</S>
    <S sid="213" ssid="73">A directory contains files, and a directory is itself a file.</S>
    <S sid="214" ssid="74">Therefore, verbs that apply to the formal role of directory are likely to apply to the formal role of objects contained in directories (such as other directories).</S>
    <S sid="215" ssid="75">This can be seen as a shortcoming of the container LCP for the task at hand, but may be a useful way of diagnosing when containers contain objects functionally similar to themselves.</S>
    <S sid="216" ssid="76">The result of this corpus acquisition procedure is a kind of minimal faceted analysis for the noun tape, as illustrated below, showing only the qualia that are relevant to the discussion.'</S>
    <S sid="217" ssid="77">13 Because the technique was sensitive to grammatical position of the object NP, the argument can be bound to the appropriate variable in the relation expressed in the qualia.</S>
    <S sid="218" ssid="78">It should be pointed out that these qualia values do not carry event place variables, since such discrimination was beyond the scope of this experiment.</S>
    <S sid="219" ssid="79">What is interesting about the qualia values is how close they are to the concepts in the projective conclusion space of tape, as mentioned in Section 1.</S>
    <S sid="220" ssid="80">To illustrate this procedure on another semantic category, consider the term mouse in its computer artifact sense.</S>
    <S sid="221" ssid="81">In our corpus, it appears in the object position of the verb use in a &amp;quot;use NP to&amp;quot; construction, as well as the object of the preposition with following a transitive verb and its object: These constructions are symptomatic of its role as an instrument; and the VP complement of to as well as the VP dominating the with-PP identify the telic predicates for the noun.</S>
    <S sid="222" ssid="82">Other verbs, for which mouse appears as a direct object are currently defaulted into the formal role, resulting in an entry for mouse as follows: The above experiments have met with limited success, enough to warrant continuing our application of lexical semantic theory to knowledge acquisition from corpora, but not enough to remove the human from the loop.</S>
    <S sid="223" ssid="83">As they currently exist, the algorithms described here can be used as tools to help the knowledge engineer extract useful information from on-line textual sources, and in some applications (e.g., a &amp;quot;related terms&amp;quot; thesaurus for full text information retrieval) may provide a useful way to heuristically organize sublanguage terminology when human resources are unavailable.</S>
  </SECTION>
  <SECTION title="4." number="4">
    <S sid="224" ssid="1">The purpose of the research described in this section is to experiment with the automatic acquisition of semantic tags for words in a sublanguage, tags well beyond that available from the seeding of MRDs.</S>
    <S sid="225" ssid="2">The identification of semantic tags is the result of type coercion on known syntactic forms, to induce a semantic feature, such as [+event] or [+object].</S>
    <S sid="226" ssid="3">A pervasive example of type coercion is seen in the complements of aspectual verbs such as begin and finish, and verbs such as enjoy.</S>
    <S sid="227" ssid="4">That is, in sentences such as &amp;quot;John began the book,&amp;quot; the normal complement expected is an action or event of some sort, most often expressed by a gerundive or infinitival phrase: &amp;quot;John began reading the book,&amp;quot; &amp;quot;John began to read the book.&amp;quot; In Pustejovsky (1991) it was argued that in such cases, the verb need not have multiple subcategorizations, but only one deep semantic type, in this case, an event.</S>
    <S sid="228" ssid="5">Thus, the verb coerces its complement (e.g.</S>
    <S sid="229" ssid="6">&amp;quot;the book&amp;quot;) into an event related to that object.</S>
    <S sid="230" ssid="7">Such information can be represented by means of a representational schema called qualia structure, which, among other things, specifies the relations associated with objects.</S>
    <S sid="231" ssid="8">Counts for objects of begin/V.</S>
    <S sid="232" ssid="9">In related work being carried out with Mats Rooth of the University of Stuttgart, we are exploring what the range of coercion types is, and what environments they may appear in, as discovered in corpora.</S>
    <S sid="233" ssid="10">Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be.</S>
    <S sid="234" ssid="11">Using techniques described in Church and Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), Figure 4 shows some examples of the most frequent V-0 pairs from the AP corpus.</S>
    <S sid="235" ssid="12">Corpus studies confirm similar results for &amp;quot;weakly intensional contexts&amp;quot; such as the complement of coercive verbs such as veto.</S>
    <S sid="236" ssid="13">These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of &amp;quot;the proposal to,&amp;quot; thereby clothing the complement within an intensional context.</S>
    <S sid="237" ssid="14">The examples in Figure 5 with the verb veto indicate two things: first, that such coercions are regular and pervasive in corpora; second, that almost anything can be vetoed, but that the most frequently occurring objects are closest to the type selected by the verb.</S>
    <S sid="238" ssid="15">What these data show is that the highest count complement types match the type required by the verb; namely, that one vetoes a bill or proposal to do something, not the thing itself.</S>
    <S sid="239" ssid="16">These nouns can therefore be used with some predictive certainty for inducing the semantic type in coercive environments such as &amp;quot;veto the expedition.&amp;quot; This work is still preliminary, however, and requires further examination (Pustejovsky and Rooth [unpublished]).</S>
    <S sid="240" ssid="17">In this section, we present another experiment indicating the feasibility of inducing semantic tags for lexical items from corpora.'</S>
    <S sid="241" ssid="18">Imagine being able to take the V-0 pairs Counts for objects of veto/V. such as those given in Section 4.1, and then applying semantic tags to the verbs that are appropriate to the role they play for that object (i.e., induction of the qualia roles for that noun).</S>
    <S sid="242" ssid="19">This is similar to the experiment reported on in Section 3.</S>
    <S sid="243" ssid="20">Here we apply a similar technique to a much larger corpus, in order to induce the agentive role for nouns; that is, the semantic predicate associated with bringing about the object.</S>
    <S sid="244" ssid="21">In this example we look at the behavior of noun phrases and the prepositional phrases that follow them.</S>
    <S sid="245" ssid="22">In particular, we look at the co-occurrence of nominals with between, with, and to.</S>
    <S sid="246" ssid="23">Table 1 shows results of the conflating noun plus preposition patterns.</S>
    <S sid="247" ssid="24">The percentage shown indicates the ratio of the particular collocation to the key word.</S>
    <S sid="248" ssid="25">Mutual information (MI) statistics for the two words in collocation are also shown.</S>
    <S sid="249" ssid="26">What these results indicate is that induction of semantic type from conflating syntactic patterns is possible.</S>
    <S sid="250" ssid="27">Based on the semantic types for these prepositions, the syntactic evidence suggests that there is an equivalence class where each preposition makes reference to a symmetric relation between the arguments in the following two patterns: We then take these results and, for those nouns where the association ratios for N with and N between are similar, we pair them with the set of verbs governing these &amp;quot;NP PP&amp;quot; combinations in corpus, effectively partitioning the original V-0 set into [+agentive] predicates and [&#8212;agentive] predicates.</S>
    <S sid="251" ssid="28">These are semantic n-grams rather than direct interpretations of the prepositions.</S>
    <S sid="252" ssid="29">What these expressions in effect indicate is the range of semantic environments they will appear in.</S>
    <S sid="253" ssid="30">That is, in sentences like those in Example 16, the force of the relational nouns agreement and talks is that they are unsaturated for the predicate bringing about this relation.</S>
    <S sid="254" ssid="31">In 17, on the other hand, the NPs headed by agreement and talks are saturated in this respect.</S>
    <S sid="255" ssid="32">If our hypothesis is correct, we expect that verbs governing nominals collocated with a with-phrase will be mostly those predicates referring to the agentive quale of the nominal.</S>
    <S sid="256" ssid="33">This is because the with-phrase is unsaturated as a predicate, and acts to identify the agent of the verb as its argument (cf.</S>
    <S sid="257" ssid="34">Nilsen (1973)).</S>
    <S sid="258" ssid="35">This is confirmed by our data, shown in Figure 6.</S>
    <S sid="259" ssid="36">Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already.</S>
    <S sid="260" ssid="37">Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/O.</S>
    <S sid="261" ssid="38">Thus, weak semantic types can be induced on the basis of syntactic behavior.</S>
    <S sid="262" ssid="39">There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992).</S>
    <S sid="263" ssid="40">We share with these researchers a general dependence on well-behaved collocational patterns and distributional structures.</S>
    <S sid="264" ssid="41">Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself, whether it involves selectional restrictions or semantic types.</S>
  </SECTION>
  <SECTION title="5." number="5">
    <S sid="265" ssid="1">In the previous section we presented algorithms for extracting collocational information from corpora, in order to supplement and fine-tune the lexical structures seeded by a machine-readable dictionary.</S>
    <S sid="266" ssid="2">In this section we demonstrate that, in addition to conventional lexical semantic relations, it is also possible to acquire information concerning lexical presuppositions and preferences from corpora, when analyzed with the appropriate semantic tools.</S>
    <S sid="267" ssid="3">In particular, we will discuss a phenomenon we call discourse polarity, and how corpus-based experiments provide clues toward the representation of this phenomenon, as well as information on preference relations.</S>
    <S sid="268" ssid="4">As we have seen, providing a representational system for lexical semantic relations is a nontrivial task.</S>
    <S sid="269" ssid="5">Representing presuppositional information, however, is even more daunting.</S>
    <S sid="270" ssid="6">Nevertheless, there are some systematic semantic generalizations associated with such subtle lexical inferences.</S>
    <S sid="271" ssid="7">To illustrate this, consider the following examples taken from the Wall Street Journal Corpus, involving the verb insist.</S>
    <S sid="272" ssid="8">But the BNL sources yesterday insisted that the head office was aware of only a small portion of the credits to Iraq made by Atlanta.</S>
    <S sid="273" ssid="9">Mr. Smale, who ordinarily insists on a test market before a national roll-out, told the team to go ahead&#8212;although he said he was skeptical that Pringle's could survive, Mr. Tucker says.</S>
    <S sid="274" ssid="10">The Cantonese insist that their fish be &amp;quot;fresh,&amp;quot; though one whiff of Hong Kong harbor and the visitor may yearn for something shipped from distant seas.</S>
    <S sid="275" ssid="11">Example 25 Money isn't the issue, Mr. Bush insists.</S>
    <S sid="276" ssid="12">From analyzing these and similar data, a pattern emerges concerning the use of verbs like insist in discourse; namely, the co-occurrence with discourse markers denoting negative affect, such as although and but, as well as literal negatives, e.g., no and not.</S>
    <S sid="277" ssid="13">This is reminiscent of the behavior of negative polarity items such as any more and at all.</S>
    <S sid="278" ssid="14">Such lexical items occur only in the context of negatives within a certain structural configuration.'</S>
    <S sid="279" ssid="15">In a similar way, verbs such as insist seem to require an overt or implicit negation within the immediate discourse context, rather than within the clause.</S>
    <S sid="280" ssid="16">For this reason, we will call such verbs discourse polarity items.</S>
    <S sid="281" ssid="17">For our purposes, the significance of such data is twofold: first, experiments on corpora can test and confirm linguistic intuitions concerning a subtle semantic judgment; second, if such knowledge is in fact so systematic, then it must be at least partially represented in the lexical semantics of the verb.</S>
    <S sid="282" ssid="18">To test whether the intuitions supported by the above data could be confirmed in corpora, Bergler (1991) derived the statistical co-occurrence of insist with discourse polarity markers in the 7 million-word corpus of Wall Street Journal articles.</S>
    <S sid="283" ssid="19">She derived the statistics reported in Figure 7.</S>
    <S sid="284" ssid="20">Let us assume, on the basis of this preliminary date presented in Bergler (1992) that these verbs in fact do behave as discourse polarity items.</S>
    <S sid="285" ssid="21">The question then</S>
  </SECTION>
  <SECTION title="Keywords Count Comments" number="6">
    <S sid="286" ssid="1">insist 586 occurrences throughout the corpus insist on 109 these have been cleaned by hand and are actually occurrences of the idiom insist on rather than accidental co-occurrences. insist &amp; but 117 occurrences of both insist and but in the same sentence insist &amp; negation 186 includes not and n't insist Sr subjunctive 159 includes would, could, should, and be Negative markers with insist in WSJC immediately arises as to how we represent this type of knowledge.</S>
    <S sid="287" ssid="2">Using the language of the qualia structure discussed above, we can make explicit reference to the polarity behavior, in the following informal but intuitive representation for the verb insist!'</S>
    <S sid="288" ssid="3">This entry states that in the REPORTING&#8212;VERB sense of the word, insist is a relation between an individual and a statement that is the negation of a proposition, /p, presupposed in the context of the utterance.</S>
    <S sid="289" ssid="4">As argued in Pustejovsky (1991) and Miller and Fellbaum (1991), such simple oppositional predicates form a central part of our lexicalization of concepts.</S>
    <S sid="290" ssid="5">Semantically motivated collocations such as these extracted from large corpora can provide presuppositional information for words that would otherwise be missing from the lexical semantics of an entry.</S>
    <S sid="291" ssid="6">While full automatic extraction of semantic collocations is not yet feasible, some recent research in related areas is promising.</S>
    <S sid="292" ssid="7">Hindle (1990) reports interesting results of this kind based on literal collocations, where he parses the corpus (Hindle 1983) into predicate&#8212;argument structures and applies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weigh the association between the predicate and each of its arguments.</S>
    <S sid="293" ssid="8">For example, as a list of the most frequent objects for the verb drink in his corpus, Hindle found beer, tea, Pepsi, and champagne.</S>
    <S sid="294" ssid="9">Based on the distributional hypothesis that the degree of shared contexts is a similarity measure for words, he develops a similarity metric for nouns based on their substitutability in certain verb contexts.</S>
    <S sid="295" ssid="10">Hindle thus finds sets of semantically similar nouns based on syntactic co-occurrence data.</S>
    <S sid="296" ssid="11">The sets he extracts are promising; for example, the ten most similar nouns to treaty in his corpus are agreement, plan, constitution, contract, proposal, accord, amendment, rule, law, and legislation.</S>
    <S sid="297" ssid="12">This work is very close in spirit to our own investigation here; the emphasis on syntactic co-occurrence enables Hindle to extract his similarity lists automatically; they are therefore easy to compile for different corpora, different sublanguages, etc.</S>
    <S sid="298" ssid="13">Here we are attempting to use these techniques together with a model of lexical meaning, to capture deeper lexical semantic collocations; e.g., the generalization that the list of objects occurring for the word drink contains only liquids.</S>
    <S sid="299" ssid="14">In the final part of this section, we turn to how the analysis of corpora can provide lexical semantic preferences for verb selection.</S>
    <S sid="300" ssid="15">As discussed above, there is a growing body of research on deriving collocations from corpora (cf.</S>
    <S sid="301" ssid="16">Church and Hanks 1990; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1993; Smadja 1991a, 1991b; Calzolari and Bindi 1990).</S>
    <S sid="302" ssid="17">Here we employ the tools of semantic analysis from Section 1 to examine the behavior of metonymy with reporting verbs.</S>
    <S sid="303" ssid="18">We will show, on the basis of corpus analysis, how verbs display marked differences in the ability to license metonymic operations over their arguments.</S>
    <S sid="304" ssid="19">Such information, we argue, is part of the preference semantics for a sublanguage, as automatically derived from corpus.</S>
    <S sid="305" ssid="20">Metonymy can be seen as a case of &amp;quot;licensed violation&amp;quot; of selectional restrictions.</S>
    <S sid="306" ssid="21">For example, while the verb announce selects for a human subject, sentences like The Phantasie Corporation announced third quarter losses are not only an acceptable paraphrase of the selectionally correct form Mr. Phantasie Jr. announced third quarter losses for Phantasie Corp, but they are the preferred form in the Wall Street Journal).</S>
    <S sid="307" ssid="22">This is an example of subject type coercion, as discussed in Section 1.</S>
    <S sid="308" ssid="23">For example, the qualia structure for a noun such as corporation might be represented as below: The metonymic extension in this example is straightforward: a spokesman, executive, or otherwise legitimate representative &amp;quot;speaking for&amp;quot; a company or institution can be metonymically replaced by that company or institution.'</S>
    <S sid="309" ssid="24">We find that this type of metonymic extension for the subject is natural and indeed very frequent with reporting verbs Bergler (1991), such as announce, report, release, and claim, while it is in general not possible with other verbs selecting human subjects, e.g., the verbs of contemplation (such as contemplate, consider, and think).</S>
    <S sid="310" ssid="25">However, there are subtle differences in the occurrence of such metonymies for the different members of the same semantic verb class that arise from corpus analysis.</S>
    <S sid="311" ssid="26">A reporting verb is an utterance verb that is used to relate the words of a source.</S>
    <S sid="312" ssid="27">In a careful study of seven reporting verbs on a 250,000-word corpus of Time magazine articles from 1963, we found that the preference for different metonymic extensions varies considerably within this field (Bergler 1991).</S>
    <S sid="313" ssid="28">Figure 8 shows the findings for the words insist, deny, admit, claim, announce, said, and told for two metonymic extensions, namely where a group stands for an individual (Analysts said .</S>
    <S sid="314" ssid="29">.</S>
    <S sid="315" ssid="30">.) and where a company or other institution stands for the individual (IBM announced ... ).19 The difference in patterns of metonymic behavior is quite striking: semantically similar verbs seem to pattern similarly over all three categories; admit, insist, and deny show a closer resemblance to each other than to any of the others, while said and Preference for metonymies for said in a 160,000-word fragment of the Wall Street Journal corpus. told form a category by themselves.</S>
    <S sid="316" ssid="31">There may be a purely semantic explanation why said and told seem not to prefer the metonymic use in subject position; e.g., perhaps these verbs relate more closely to the act of uttering, or perhaps they are too informal, stylistically.</S>
    <S sid="317" ssid="32">Evidence from other corpora, however, suggests that such information is accurately characterized as lexical preference.</S>
    <S sid="318" ssid="33">An initial experiment on a subset of the Wall Street Journal Corpus, for example, shows that said has a quite different metonymic distribution there, reported in Figure 9.</S>
    <S sid="319" ssid="34">In this corpus we discovered that subject selection for an individual person appeared in only 50% of the sentences, while a company/institution appeared in 34% of the cases.</S>
    <S sid="320" ssid="35">This difference could either be attributed to a difference in style between Time magazine and the Wall Street Journal or perhaps to a difference in general usage between 1963 and 1989.</S>
    <S sid="321" ssid="36">The statistics presented here can of course not determine the reason for the difference, but rather help establish the lexical semantic preferences that exist in a certain corpus and sublanguage.</S>
    <S sid="322" ssid="37">An important question related to the extraction of preference information is what the corpus should be.</S>
    <S sid="323" ssid="38">Recent effort has been spent constructing balanced corpora, containing text from different styles and sources, such as novels, newspaper texts, scientific journal articles, etc.</S>
    <S sid="324" ssid="39">The assumption is of course that given a representative mix of samples of language use, we can extract the general properties and usage of words.</S>
    <S sid="325" ssid="40">But if we gain access to sophisticated automatic corpus analysis tools such as those discussed above, and indeed if we have specialized algorithms for sublanguage extraction, then homogeneous corpora might provide better data.</S>
    <S sid="326" ssid="41">The few examples of lexical preference mentioned in this section might not tell us anything conclusive for the definitive usage of a word such as said, if there even exists such a notion.</S>
    <S sid="327" ssid="42">Nevertheless the statistics provide an important tool for text analysis within the corpus from which they are derived.</S>
    <S sid="328" ssid="43">Because we can systematically capture the violation of selectional restrictions (as semantically predicted), there is no need for a text analysis system to perform extensive commonsense inferencing.</S>
    <S sid="329" ssid="44">Thus, such presupposition and preference statistics are vital to efficient processing of real text.</S>
  </SECTION>
  <SECTION title="6." number="7">
    <S sid="330" ssid="1">In this paper we have presented a particularly directed program of research for how text corpora can contribute to linguistics and computational linguistics.</S>
    <S sid="331" ssid="2">We first presented a representation language for lexical knowledge, the generative lexicon, and demonstrated how it facilitates the structuring of lexical relations among words, looking in particular at the problems of metonymy and polysemy.</S>
    <S sid="332" ssid="3">Such a framework for lexical knowledge suggests that there are richer relationships among words in text beyond that of simple co-occurrence that can be extracted automatically.</S>
    <S sid="333" ssid="4">The work suggests how linguistic phenomena such as metonymy and polysemy might be exploited for knowledge acquisition for lexical items.</S>
    <S sid="334" ssid="5">Unlike purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems.</S>
    <S sid="335" ssid="6">We illustrated the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary.</S>
    <S sid="336" ssid="7">In addition to conventional lexical semantic relations, we then showed how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools.</S>
    <S sid="337" ssid="8">In conclusion, we feel that the application of computational resources to the analysis of text corpora has and will continue to have a profound effect on the direction of linguistic and computational linguistic research.</S>
    <S sid="338" ssid="9">Unlike previous attempts at corpus research, the current focus is supported and guided by theoretical tools, and not merely statistical techniques.</S>
    <S sid="339" ssid="10">We should furthermore welcome the ability to expand the data set used for the confirmation of linguistic hypotheses.</S>
    <S sid="340" ssid="11">At the same time, we must remember that statistical results themselves reveal nothing, and require careful and systematic interpretation by the investigator to become linguistic data.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="341" ssid="1">This research was supported by DARPA contract MDA904-91-C-9328.</S>
    <S sid="342" ssid="2">We would like to thank Scott Waterman for his assistance in preparing the statistics.</S>
    <S sid="343" ssid="3">We would also like to thank Mats Rooth, Scott Waterman, and four anonymous reviewers for useful comments and discussion.</S>
  </SECTION>
</PAPER>
