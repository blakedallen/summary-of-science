<PAPER>
  <S sid="0">Polylingual Topic Models</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.</S>
    <S sid="2" ssid="2">Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages.</S>
    <S sid="3" ssid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
    <S sid="4" ssid="4">We explore the model&#8217;s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S>
    <S sid="6" ssid="2">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>
    <S sid="7" ssid="3">Much of this work, however, has occurred in monolingual contexts.</S>
    <S sid="8" ssid="4">In an increasingly connected world, the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.</S>
    <S sid="9" ssid="5">In this paper, we present the polylingual topic model (PLTM).</S>
    <S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
    <S sid="11" ssid="7">There are many potential applications for polylingual topic models.</S>
    <S sid="12" ssid="8">Although research literature is typically written in English, bibliographic databases often contain substantial quantities of work in other languages.</S>
    <S sid="13" ssid="9">To perform topic-based bibliometric analysis on these collections, it is necessary to have topic models that are aligned across languages.</S>
    <S sid="14" ssid="10">Such analysis could be significant in tracking international research trends, where language barriers slow the transfer of ideas.</S>
    <S sid="15" ssid="11">Previous work on bilingual topic modeling has focused on machine translation applications, which rely on sentence-aligned parallel translations.</S>
    <S sid="16" ssid="12">However, the growth of the internet, and in particular Wikipedia, has made vast corpora of topically comparable texts&#8212;documents that are topically similar but are not direct translations of one another&#8212;considerably more abundant than ever before.</S>
    <S sid="17" ssid="13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>
    <S sid="18" ssid="14">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
    <S sid="19" ssid="15">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
    <S sid="20" ssid="16">We also explore how the characteristics of different languages affect topic model performance.</S>
    <S sid="21" ssid="17">The second corpus, Wikipedia articles in twelve languages, contains sets of documents that are not translations of one another, but are very likely to be about similar concepts.</S>
    <S sid="22" ssid="18">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages, and to detect differences in topic emphasis between languages.</S>
    <S sid="23" ssid="19">The internet makes it possible for people all over the world to access documents from different cultures, but readers will not be fluent in this wide variety of languages.</S>
    <S sid="24" ssid="20">By linking topics across languages, polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="25" ssid="1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007).</S>
    <S sid="26" ssid="2">Tam, Lane and Schultz (Tam et al., 2007) also show improvements in machine translation using bilingual topic models.</S>
    <S sid="27" ssid="3">Both of these translation-focused topic models infer word-to-word alignments as part of their inference procedures, which would become exponentially more complex if additional languages were added.</S>
    <S sid="28" ssid="4">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S>
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
    <S sid="30" ssid="6">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>
    <S sid="31" ssid="7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
    <S sid="32" ssid="8">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>
    <S sid="33" ssid="9">They find, for example, that English blog posts about the Nintendo Wii often relate to a hack, which cannot be mentioned in Japanese posts due to Japanese intellectual property law.</S>
    <S sid="34" ssid="10">Similarly, posts about whaling often use (positive) nationalist language in Japanese and (negative) environmentalist language in English.</S>
  </SECTION>
  <SECTION title="3 Polylingual Topic Model" number="3">
    <S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
    <S sid="36" ssid="2">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S>
    <S sid="37" ssid="3">PLTM assumes that the documents in a tuple share the same tuple-specific distribution over topics.</S>
    <S sid="38" ssid="4">This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.</S>
    <S sid="39" ssid="5">Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.</S>
    <S sid="40" ssid="6">Anew document tuple w = (w1, ... , wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter &#945; and base measure m: Then, for each language l, a latent topic assignment is drawn for each token in that language: Finally, the observed tokens are themselves drawn using the language-specific topic parameters: wl &#8764; P(wl  |zl,&#934;l) = 11n &#966;lwl |zl .</S>
    <S sid="41" ssid="7">(3) The graphical model is shown in figure 1.</S>
    <S sid="42" ssid="8">Given a corpus of training and test document tuples&#8212;W and W', respectively&#8212;two possible inference tasks of interest are: computing the probability of the test tuples given the training tuples and inferring latent topic assignments for test documents.</S>
    <S sid="43" ssid="9">These tasks can either be accomplished by averaging over samples of &#934;1, .</S>
    <S sid="44" ssid="10">.</S>
    <S sid="45" ssid="11">.</S>
    <S sid="46" ssid="12">, &#934;L and &#945;m from P(&#934;1, ... , &#934;L, &#945;m  |W', &#946;) or by evaluating a point estimate.</S>
    <S sid="47" ssid="13">We take the latter approach, and use the MAP estimate for &#945;m and the predictive distributions over words for &#934;1, .</S>
    <S sid="48" ssid="14">.</S>
    <S sid="49" ssid="15">.</S>
    <S sid="50" ssid="16">, &#934;L.The probability of held-out document tuples W' given training tuples W is then approximated by Topic assignments for a test document tuple sampling.</S>
    <S sid="51" ssid="17">Gibbs sampling involves sequentially resampling each zln from its conditional posterior: where z\l,n is the current set of topic assignments for all other tokens in the tuple, while (Nt)\l,n is the number of occurrences of topic t in the tuple, excluding zln, the variable being resampled.</S>
  </SECTION>
  <SECTION title="4 Results on Parallel Text" number="4">
    <S sid="52" ssid="1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S>
    <S sid="53" ssid="2">In this case, we can be confident that the topic distribution is genuinely shared across all languages.</S>
    <S sid="54" ssid="3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents), we use direct translations to explore the characteristics of the model.</S>
    <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
    <S sid="56" ssid="5">These texts consist of roughly a decade of proceedings of the European parliament.</S>
    <S sid="57" ssid="6">For our purposes we use alignments at the speech level rather than the sentence level, as in many translation tasks using this corpus.</S>
    <S sid="58" ssid="7">We also remove the twenty-five most frequent word types for efficiency reasons.</S>
    <S sid="59" ssid="8">The remaining collection consists of over 121 million words.</S>
    <S sid="60" ssid="9">Details by language are shown in Table 1.</S>
    <S sid="61" ssid="10">ES otros otras otro otra parte dem&#8225;s FI muiden toisaalta muita muut muihin muun FR autres autre part cTMt6 ailleurs m6me IT altri altre altro altra dall parte NL andere anderzijds anderen ander als kant PT outros outras outro lado outra noutros SV andra sidan &#338; annat ena annan The concentration parameter &#945; for the prior over document-specific topic distributions is initialized to 0.01 T, while the base measure m is initialized to the uniform distribution.</S>
    <S sid="62" ssid="11">Hyperparameters &#945;m are re-estimated every 10 Gibbs iterations.</S>
    <S sid="63" ssid="12">Figure 2 shows the most probable words in all languages for four example topics, from PLTM with 400 topics.</S>
    <S sid="64" ssid="13">The first topic contains words relating to the European Central Bank.</S>
    <S sid="65" ssid="14">This topic provides an illustration of the variation in technical terminology captured by PLTM, including the wide array of acronyms used by different languages.</S>
    <S sid="66" ssid="15">The second topic, concerning children, demonstrates the variability of everyday terminology: although the four Romance languages are closely related, they use etymologically unrelated words for children.</S>
    <S sid="67" ssid="16">(Interestingly, all languages except Greek and Finnish use closely related words for &#8220;youth&#8221; or &#8220;young&#8221; in a separate topic.)</S>
    <S sid="68" ssid="17">The third topic demonstrates differences in inflectional variation.</S>
    <S sid="69" ssid="18">English and the Romance languages use only singular and plural versions of &#8220;objective.&#8221; The other Germanic languages include compound words, while Greek and Finnish are dominated by inflected variants of the same lexical item.</S>
    <S sid="70" ssid="19">The final topic demonstrates that PLTM effectively clusters &#8220;syntactic&#8221; words, as well as more semantically specific nouns, adjectives and verbs.</S>
    <S sid="71" ssid="20">Although the topics in figure 2 seem highly focused, it is interesting to ask whether the model is genuinely learning mixtures of topics or simply assigning entire document tuples to single topics.</S>
    <S sid="72" ssid="21">To answer this question, we compute the posterior probability of each topic in each tuple under the trained model.</S>
    <S sid="73" ssid="22">If the model assigns all tokens in a tuple to a single topic, the maximum posterior topic probability for that tuple will be near to 1.0.</S>
    <S sid="74" ssid="23">If the model assigns topics uniformly, the maximum topic probability will be near 1/T.</S>
    <S sid="75" ssid="24">We compute histograms of these maximum topic probabilities for T &#8712; {50,100, 200, 400, 800}.</S>
    <S sid="76" ssid="25">For clarity, rather than overlaying five histograms, figure 3 shows the histograms converted into smooth curves using a kernel density estimator.1 Although there is a small bump around 1.0 (for extremely short documents, e.g., &#8220;Applause&#8221;), values are generally closer to, but greater than, 1/T.</S>
    <S sid="77" ssid="26">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S>
    <S sid="78" ssid="27">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple), we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S>
    <S sid="79" ssid="28">For each tuple we can then calculate the JensenShannon divergence (the average of the KL divergences between each distribution and a mean distribution) between these distributions.</S>
    <S sid="80" ssid="29">Figure 4 shows the density of these divergences for different numbers of topics.</S>
    <S sid="81" ssid="30">As with the previous figure, there are a small number of documents that contain only one topic in all languages, and thus have zero divergence.</S>
    <S sid="82" ssid="31">These tend to be very short, formulaic parliamentary responses, however.</S>
    <S sid="83" ssid="32">The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that, for each tuple, the model is not simply assigning all tokens in a particular language to a single topic.</S>
    <S sid="84" ssid="33">As the number of topics increases, greater variability in topic distributions causes divergence to increase.</S>
    <S sid="85" ssid="34">Smoothed histograms of inter&#8722;language JS divergence A topic model specifies a probability distribution over documents, or in the case of PLTM, document tuples.</S>
    <S sid="86" ssid="35">Given a set of training document tuples, PLTM can be used to obtain posterior estimates of &#934;', ... , &#934;L and &#945;m.</S>
    <S sid="87" ssid="36">The probability of previously unseen held-out document tuples given these estimates can then be computed.</S>
    <S sid="88" ssid="37">The higher the probability of the held-out document tuples, the better the generalization ability of the model.</S>
    <S sid="89" ssid="38">Analytically calculating the probability of a set of held-out document tuples given &#934;1, ... , &#934;L and &#945;m is intractable, due to the summation over an exponential number of topic assignments for these held-out documents.</S>
    <S sid="90" ssid="39">However, recently developed methods provide efficient, accurate estimates of this probability.</S>
    <S sid="91" ssid="40">We use the &#8220;left-to-right&#8221; method of (Wallach et al., 2009).</S>
    <S sid="92" ssid="41">We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.</S>
    <S sid="93" ssid="42">Table 2 shows the log probability of held-out data in nats per word for PLTM and LDA, both trained with 200 topics.</S>
    <S sid="94" ssid="43">There is substantial variation between languages.</S>
    <S sid="95" ssid="44">Additionally, the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.</S>
    <S sid="96" ssid="45">It is important to note, however, that these results do not imply that LDA should be preferred over PLTM&#8212;that choice depends upon the needs of the modeler.</S>
    <S sid="97" ssid="46">Rather, these results are intended as a quantitative analysis of the difference between the two models.</S>
    <S sid="98" ssid="47">As the number of topics is increased, the word counts per topic become very sparse in monolingual LDA models, proportional to the size of the vocabulary.</S>
    <S sid="99" ssid="48">Figure 5 shows the proportion of all tokens in English and Finnish assigned to each topic under LDA and PLTM with 800 topics.</S>
    <S sid="100" ssid="49">More than 350 topics in the Finnish LDA model have zero tokens assigned to them, and almost all tokens are assigned to the largest 200 topics.</S>
    <S sid="101" ssid="50">English has a larger tail, with non-zero counts in all but 16 topics.</S>
    <S sid="102" ssid="51">In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.</S>
    <S sid="103" ssid="52">PLTM topics therefore have a higher granularity &#8211; i.e., they are more specific.</S>
    <S sid="104" ssid="53">This result is important: informally, we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S>
    <S sid="105" ssid="54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>
    <S sid="106" ssid="55">For example, a journal might publish papers in English, French, German and Italian.</S>
    <S sid="107" ssid="56">No paper is exactly comparable to any other paper, but they are all roughly topically similar.</S>
    <S sid="108" ssid="57">If we wish to perform topic-based bibliometric analysis, it is vital to be able to track the same topics across all languages.</S>
    <S sid="109" ssid="58">One simple way to achieve this topic alignment is to add a small set of comparable document tuples that provide sufficient &#8220;glue&#8221; to bind the topics together.</S>
    <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
    <S sid="111" ssid="60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.</S>
    <S sid="112" ssid="61">To do this, we divide the corpus W into two sets of document tuples: a &#8220;glue&#8221; set G and a &#8220;separate&#8221; set S such that |G |/ |W |= p. In other words, the proportion of tuples in the corpus that are treated as &#8220;glue&#8221; (i.e., placed in G) is p. For every tuple in S, we assign each document in that tuple to a new singledocument tuple.</S>
    <S sid="113" ssid="62">By doing this, every document in S has its own distribution over topics, independent of any other documents.</S>
    <S sid="114" ssid="63">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>
    <S sid="115" ssid="64">FR russie tch&#180;etch&#180;enie union avec russe r&#180;egion IT ho presidente mi perch&#180;e relazione votato lang Topics at P = 0.25 DE ru&#223;land russland russischen tschetschenien ukraine EN russia russian chechnya cooperation region belarus FR russie tch&#180;etch&#180;enie avec russe russes situation IT russia unione cooperazione cecenia regione russa We train PLTM with 100 topics on corpora with p E 10.01, 0.05, 0.1, 0.25, 0.5}.</S>
    <S sid="116" ssid="65">We use 1000 iterations of Gibbs sampling with Q = 0.01.</S>
    <S sid="117" ssid="66">Hyperparameters &#945;m are re-estimated every 10 iterations.</S>
    <S sid="118" ssid="67">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>
    <S sid="119" ssid="68">The lower the divergence, the more similar the distributions are to each other.</S>
    <S sid="120" ssid="69">From the results in figure 4, we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.</S>
    <S sid="121" ssid="70">Table 3 shows mean JS divergences for each value of p. As expected, JS divergence is greater than that obtained when all tuples are left intact.</S>
    <S sid="122" ssid="71">Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.</S>
    <S sid="123" ssid="72">Example topics for p = 0.01 and p = 0.25 are shown in table 4.</S>
    <S sid="124" ssid="73">At p = 0.01 (1% &#8220;glue&#8221; documents), German and French both include words relating to Russia, while the English and Italian word distributions appear locally consistent but unrelated to Russia.</S>
    <S sid="125" ssid="74">At p = 0.25, the top words for all four languages are related to Russia.</S>
    <S sid="126" ssid="75">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>
    <S sid="127" ssid="76">One area for future work is to explore whether initialization techniques or better representations of topic co-occurrence might result in alignment of topics with a smaller proportion of comparable texts.</S>
    <S sid="128" ssid="77">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>
    <S sid="129" ssid="78">We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al., 2008).</S>
    <S sid="130" ssid="79">In the early statistical translation model work at IBM, these representations were called &#8220;cepts,&#8221; short for concepts (Brown et al., 1993).</S>
    <S sid="131" ssid="80">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>
    <S sid="132" ssid="81">Unlike previous work (Koehn and Knight, 2002), we evaluate all words, not just nouns.</S>
    <S sid="133" ssid="82">We collected bilingual lexica mapping English words to German, Greek, Spanish, French, Italian, Dutch and Swedish.</S>
    <S sid="134" ssid="83">Each lexicon is a set of pairs consisting of an English word and a translated word, 1we, wt}.</S>
    <S sid="135" ssid="84">We do not consider multi-word terms.</S>
    <S sid="136" ssid="85">We expect that simple analysis of topic assignments for sequential words would yield such collocations, but we leave this for future work.</S>
    <S sid="137" ssid="86">For every topic t we select a small number K of the most probable words in English (e) and in each &#8220;translation&#8221; language (E): Wte and Wtt, respectively.</S>
    <S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
    <S sid="139" ssid="88">Results for K = 1, that is, considering only the single most probable word for each language, are shown in figure 6.</S>
    <S sid="140" ssid="89">Precision at this level is relatively high, above 50% for Spanish, French and Italian with T = 400 and 800.</S>
    <S sid="141" ssid="90">Many of the candidate pairs that were not in the bilingual lexica were valid translations (e.g.</S>
    <S sid="142" ssid="91">EN &#8220;comitology&#8221; and IT lang Topics at P = 0.01 &#8220;comitalogia&#8221;) that simply were not in the lexica.</S>
    <S sid="143" ssid="92">We also do not count morphological variants: the model finds EN &#8220;rules&#8221; and DE &#8220;vorschriften,&#8221; but the lexicon contains only &#8220;rule&#8221; and &#8220;vorschrift.&#8221; Results remain strong as we increase K. With K = 3, T = 800, 1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S>
    <S sid="144" ssid="93">The number of such pairs that appear in bilingual lexica is shown on the y-axis.</S>
    <S sid="145" ssid="94">For T = 800, the top English and Spanish words in 448 topics were exact translations of one another.</S>
    <S sid="146" ssid="95">In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S>
    <S sid="147" ssid="96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S>
    <S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
    <S sid="149" ssid="98">It is not necessarily clear that PLTM will be effective at identifying translations.</S>
    <S sid="150" ssid="99">In finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.</S>
    <S sid="151" ssid="100">We are therefore interested in determining whether the information in the document-specific topic distributions is sufficient to identify semantically identical documents.</S>
    <S sid="152" ssid="101">We begin by dividing the data into a training set of 69,550 document tuples and a test set of 17,435 document tuples.</S>
    <S sid="153" ssid="102">In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.</S>
    <S sid="154" ssid="103">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S>
    <S sid="155" ssid="104">Finally, for each pair of languages (&#8220;query&#8221; and &#8220;target&#8221;) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S>
    <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
    <S sid="157" ssid="106">For each document in the query language we rank all documents in the target language and record the rank of the actual translation.</S>
    <S sid="158" ssid="107">Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.</S>
    <S sid="159" ssid="108">Cosine-based rankings are significantly worse.</S>
    <S sid="160" ssid="109">It is important to note that the length of documents matters.</S>
    <S sid="161" ssid="110">As noted before, many of the documents in the EuroParl collection consist of short, formulaic sentences.</S>
    <S sid="162" ssid="111">Restricting the query/target pairs to only those with query and target documents that are both longer than 50 words results in significant improvement and reduced variance: the average proportion of query documents for which the true translation is ranked highest goes from 53.9% to 72.7%.</S>
    <S sid="163" ssid="112">Performance continues to improve with longer documents, most likely due to better topic inference.</S>
    <S sid="164" ssid="113">Results vary by language.</S>
    <S sid="165" ssid="114">Table 5 shows results for all target languages with English as a query language.</S>
    <S sid="166" ssid="115">Again, English generally performs better with Romance languages than Germanic languages.</S>
  </SECTION>
  <SECTION title="5 Results on Comparable Texts" number="5">
    <S sid="167" ssid="1">Directly parallel translations are rare in many languages and can be extremely expensive to produce.</S>
    <S sid="168" ssid="2">However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.</S>
    <S sid="169" ssid="3">In this section, we explore two questions relating to comparable text corpora and polylingual topic modeling.</S>
    <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
    <S sid="171" ssid="5">This property is useful for building machine translation systems as well as for human readers who are either learning new languages or analyzing texts in languages they do not know.</S>
    <S sid="172" ssid="6">Second, because comparable texts may not use exactly the same topics, it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S>
    <S sid="173" ssid="7">We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh, German, Greek, English, Farsi, Finnish, French, Hebrew, Italian, Polish, Russian and Turkish.</S>
    <S sid="174" ssid="8">These versions of Wikipedia were selected to provide a diverse range of language families, geographic areas, and quantities of text.</S>
    <S sid="175" ssid="9">We preprocessed the data by removing tables, references, images and info-boxes.</S>
    <S sid="176" ssid="10">We dropped all articles in non-English languages that did not link to an English article.</S>
    <S sid="177" ssid="11">In the English version of Wikipedia we dropped all articles that were not linked to by any other language in our set.</S>
    <S sid="178" ssid="12">For efficiency, we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language.</S>
    <S sid="179" ssid="13">Even with these restrictions, the size of the corpus is 148.5 million words.</S>
    <S sid="180" ssid="14">We present results for a PLTM with 400 topics.</S>
    <S sid="181" ssid="15">1000 Gibbs sampling iterations took roughly four days on one CPU with current hardware.</S>
    <S sid="182" ssid="16">As with EuroParl, we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S>
    <S sid="183" ssid="17">We can then average over all such document-document divergences for each pair of languages to get an overall &#8220;disagreement&#8221; score between languages.</S>
    <S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
    <S sid="185" ssid="19">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission &#948;&#953;&#945;&#963;&#964;&#951;&#956;&#953;&#954;&#972; sts nasa &#945;&#947;&#947;&#955; small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm&#352;inen space lento spatiale mission orbite mars satellite spatial &#1514;&#1497;&#1504;&#1499;&#1493;&#1514; &#1488; &#1512;&#1493;&#1491;&#1499; &#1500;&#1500; &#1495; &#1509;&#1512; &#1488;&#1492; &#1500;&#1500;&#1495;&#1492; spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa &#1082;&#1086;&#1089;&#1084;&#1080;&#1095;&#1077;&#1089;&#1082;&#1080;&#1081; &#1089;&#1086;&#1102;&#1079; &#1082;&#1086;&#1089;&#1084;&#1080;&#1095;&#1077;&#1089;&#1082;&#1086;&#1075;&#1086; &#1089;&#1087;&#1091;&#1090;&#1085;&#1080;&#1082; &#1089;&#1090;&#1072;&#1085;&#1094;&#1080;&#1080; uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la &#953;&#963;&#960;&#945;&#957;&#943;&#945;&#962; &#953;&#963;&#960;&#945;&#957;&#943;&#945; de &#953;&#963;&#960;&#945;&#957;&#972;&#962; &#957;&#964;&#949; &#956;&#945;&#948;&#961;&#943;&#964;&#951; de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa&#324;ski hiszpanii la juan y &#1076;&#1077; &#1084;&#1072;&#1076;&#1088;&#1080;&#1076; &#1080;&#1089;&#1087;&#1072;&#1085;&#1080;&#1080; &#1080;&#1089;&#1087;&#1072;&#1085;&#1080;&#1103; &#1080;&#1089;&#1087;&#1072;&#1085;&#1089;&#1082;&#1080;&#1081; de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk &#960;&#959;&#953;&#951;&#964;&#942;&#962; &#960;&#959;&#943;&#951;&#963;&#951; &#960;&#959;&#953;&#951;&#964;&#942; &#941;&#961;&#947;&#959; &#960;&#959;&#953;&#951;&#964;&#941;&#962; &#960;&#959;&#953;&#942;&#956;&#945;&#964;&#945; poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
    <S sid="186" ssid="20">Although we find that if Wikipedia contains an article on a particular subject in some language, the article will tend to be topically similar to the articles about that subject in other languages, we also find that across the whole collection different languages emphasize topics to different extents.</S>
    <S sid="187" ssid="21">To demonstrate the wide variation in topics, we calculated the proportion of tokens in each language assigned to each topic.</S>
    <S sid="188" ssid="22">Figure 8 represents the estimated probabilities of topics given a specific language.</S>
    <S sid="189" ssid="23">Competitive cross-country skiing (left) accounts for a significant proportion of the text in Finnish, but barely exists in Welsh and the languages in the Southeastern region.</S>
    <S sid="190" ssid="24">Meanwhile, interest in actors and actresses (center) is consistent across all languages.</S>
    <S sid="191" ssid="25">Finally, historical topics, such as the Byzantine and Ottoman empires (right) are strong in all languages, but show geographical variation: interest centers around the empires.</S>
  </SECTION>
  <SECTION title="6 Conclusions" number="6">
    <S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
    <S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
    <S sid="194" ssid="3">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S>
    <S sid="195" ssid="4">Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>
    <S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
  </SECTION>
  <SECTION title="7 Acknowledgments" number="7">
    <S sid="197" ssid="1">The authors thank Limin Yao, who was involved in early stages of this project.</S>
    <S sid="198" ssid="2">This work was supported in part by the Center for Intelligent Information Retrieval, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant number IIS-0326249, and in part by Army prime contract number W911NF-07-1-0216 and University of Pennsylvania subaward number 103548106, and in part by National Science Foundation under NSF grant #CNS-0619337.</S>
    <S sid="199" ssid="3">Any opinions, findings and conclusions or recommendations expressed in this material are the authors&#8217; and do not necessarily reflect those of the sponsor.</S>
  </SECTION>
</PAPER>
