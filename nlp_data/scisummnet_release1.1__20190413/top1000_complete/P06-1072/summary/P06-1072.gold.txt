Annealing Structural Bias In Multilingual Weighted Grammar Induction
We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).
Next, by annealing the free parameter that controls this bias, we achieve further improvements.
We then describe an alternative kind of structural bias, toward "broken" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement.
We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17% (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date.
Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems.
We penalize the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words.
We propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed.
Our annealing approach tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step.
