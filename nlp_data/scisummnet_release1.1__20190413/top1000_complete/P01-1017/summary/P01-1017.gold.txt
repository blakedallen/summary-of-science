Immediate-Head Parsing For Language Models
We present two language models based upon an “immediate-head” parser — our name for a parser that conditions all events below a constituent c upon the head of c.
While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology.
The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model.
For the better of our two models these improvements are 24% and 14% respectively.
We also suggest that improvement of the underlying parser should significantly improve the model’s perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models.
The model presented identifies both syntactic structural and lexical dependencies that aid in language modeling.
These contexts include syntactic structure such as parent and grandparent category labels as well as lexical items such as the head of the parent or the head of a sibling constituent.
