Revisiting Readability: A Unified Framework for Predicting Text Quality
We combine lexical, syntactic, and discourse features to produce a highly predictive model
of human readers' judgments of text readability.
This is the first study to take into account such a variety of linguistic factors and
the first to empirically demonstrate that discourse relations are strongly associated with
the perceived quality of text.
We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus.
We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability.
Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks.
We propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality.
When readability is targeted towards adult competent language users a more prominent role is played by discourse features.
Five annotators have assessed the overall text quality of each article on a scale from 1 to 5.
We find non-significant correlation for the mean number of words per sentence and the mean number of characters per word.
