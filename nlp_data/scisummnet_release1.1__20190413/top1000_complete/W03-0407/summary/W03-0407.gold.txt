Bootstrapping POS-Taggers Using Unlabelled Data
This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output.
Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set.
We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature.
Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets.
Further results show that this form of co-training considerably out-performs self-training.
However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.
We report positive results with little labeled training data but negative results when the amount of labeled training data increases.
We define self-training as a procedure in which a tagger is retrained on its own labeled cache at each round.
