[
  {
    "citance_No": 1, 
    "citing_paper_id": "P10-2001", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Takashi, Onishi | Masao, Utiyama | Eiichiro, Sumita", 
    "raw_text": "Dyer (2009) also employeda segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation", 
    "clean_text": "Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "E12-3008", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jan A., Botha", 
    "raw_text": "All language models were trained on the target side of the preprocessed bi text containing 38 million tokens, and tested on all the Germandevelopment data (i.e .news-test2008,9,10) .Compound segmentation To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bi text", 
    "clean_text": "To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "C10-1045", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Spence, Green | Christopher D., Manning", 
    "raw_text": "Despite their simplicity ,unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009) .13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabicsegmenter, configured to replicate ATBsegmentation (Habash and Rambow, 2005)", 
    "clean_text": "Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P11-2031", 
    "citing_paper_authority": 61, 
    "citing_paper_authors": "Jonathan H., Clark | Chris, Dyer | Alon, Lavie | Noah A., Smith", 
    "raw_text": "The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words) .4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009)", 
    "clean_text": "The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W10-1707", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Vladimir, Eidelman | Chris, Dyer | Philip, Resnik", 
    "raw_text": "These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009) .In order to construct diverse and accurate segmentation lattices, we built a maximum entropy model of compound word splitting which makes use of a small number of dense features, such as frequency of hypothesized morphemes as separate units in a monolingual corpus, number of predicted morphemes, and number of letters in a predicted morpheme", 
    "clean_text": "These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P11-1130", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Preslav, Nakov | Hwee Tou, Ng", 
    "raw_text": "6We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner", 
    "clean_text": "We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "C10-2096", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Haitao, Mi | Liang, Huang | Qun, Liu", 
    "raw_text": "Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system", 
    "clean_text": "Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W11-2139", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Chris, Dyer | Kevin, Gimpel | Jonathan H., Clark | Noah A., Smith", 
    "raw_text": "compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009)", 
    "clean_text": "Since German is a language that makes productive use of \"closed\" compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W11-2140", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Vladimir, Eidelman | Kristy, Hollingshead | Philip, Resnik", 
    "raw_text": "These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009)", 
    "clean_text": "These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P12-1002", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Patrick, Simianer | Stefan, Riezler | Chris, Dyer", 
    "raw_text": "All data was tokenized and lowercased; German compounds were split (Dyer, 2009)", 
    "clean_text": "All data was tokenized and lowercased; German compounds were split (Dyer, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P12-1016", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Spence, Green | John, DeNero", 
    "raw_text": "The model assigns four labels:? I: Continuation of a morpheme? O: Outside morpheme (whitespace)? B: Beginning of a morpheme? F: Non-native character (s) 2Segmentation also improves translation of compounding languages such as German (Dyer, 2009) and Finnish (Macherey et al, 2011)", 
    "clean_text": "Segmentation also improves translation of compounding languages such as German (Dyer, 2009) and Finnish (Macherey et al, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W10-1760", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Michael, Paul | Andrew, Finch | Eiichiro, Sumita", 
    "raw_text": "An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where amaximum entropy model is used to assign prob abilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model", 
    "clean_text": "An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W10-1734", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Fabienne, Fritzinger | Alexander, Fraser", 
    "raw_text": "Dyer (2009) applied this to German using a lattice encoding different segmentations of German words", 
    "clean_text": "Dyer (2009) applied this to German using a lattice encoding different segmentations of German words.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P11-1140", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Klaus, Macherey | Andrew, Dai | David, Talbot | Ashok C., Popat | Franz Josef, Och", 
    "raw_text": "Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system. To train the model, reference segmentations are required", 
    "clean_text": "Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W12-3157", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Joern, Wuebker | Hermann, Ney", 
    "raw_text": "Dyer (2009) introduces a maximum entropy model for compound word splitting, which heuses to create word lattices for translation in put", 
    "clean_text": "Dyer (2009) introduces a maximum entropy model for compound word splitting, which he uses to create word lattices for translation input.", 
    "keep_for_gold": 0
  }
]
