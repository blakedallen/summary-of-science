<PAPER>
  <S sid="0">Automatic Tagging Of Arabic Text: From Raw Text To Base Phrase Chunks</S>
  <ABSTRACT/>
  <SECTION title="Abstract 1 Introduction" number="1">
    <S sid="1" ssid="1">Arabic is garnering attention in the NLP community due to its socio-political importance and its linguistic differences from Indo-European languages.</S>
    <S sid="2" ssid="2">These linguistic characteristics, especially dialect differences and complex morphology present interesting challenges for NLP researchers.</S>
    <S sid="3" ssid="3">But like most non-European languages, Arabic is lacking in annotated resources and tools.</S>
    <S sid="4" ssid="4">Fully automated fundamental NLP tools such as Tokenizers, Part Of Speech (POS) Taggers and Base Phrase (BP) Chunkers are still not available for Arabic.</S>
    <S sid="5" ssid="5">Meanwhile, these tools are readily available and have achieved remarkable accuracy and sophistication for the processing of many European languages.</S>
    <S sid="6" ssid="6">With the release of the Arabic Penn TreeBank 1 (v2.0),1 the story is about to change.</S>
    <S sid="7" ssid="7">In this paper, we propose solutions to the problems of Tokenization, POS Tagging and BP Chunking of Arabic text.</S>
    <S sid="8" ssid="8">By Tokenization we mean the process of segmenting clitics from stems, since in Arabic, prepositions, conjunctions, and some pronouns are cliticized (orthographically and phonological fused) onto stems.</S>
    <S sid="9" ssid="9">Separating conjunctions from the following noun, for example, is a key first step in parsing.</S>
    <S sid="10" ssid="10">By POS Tagging, we mean the standard problem of annotating these segmented words with parts of speech drawn from the &#8216;collapsed&#8217; Arabic Penn TreeBank POS tagset.</S>
    <S sid="11" ssid="11">Base Phrase (BP) Chunking is the process of creating non-recursive base phrases such as noun phrases, adjectival phrases, verb phrases, preposition phrases, etc.</S>
    <S sid="12" ssid="12">For each of these tasks, we adopt a supervised machine learning perspective using Support Vector Machines (SVMs) trained on the Arabic TreeBank, leveraging off of already existing algorithms for English.</S>
    <S sid="13" ssid="13">The results are comparable to state-of-the-art results on English text when trained on similar sized data.</S>
  </SECTION>
  <SECTION title="2 Arabic Language and Data" number="2">
    <S sid="14" ssid="1">Arabic is a Semitic language with rich templatic morphology.</S>
    <S sid="15" ssid="2">An Arabic word may be composed of a stem (consisting of a consonantal root and a template), plus affixes and clitics.</S>
    <S sid="16" ssid="3">The affixes include inflectional markers for tense, gender, and/or number.</S>
    <S sid="17" ssid="4">The clitics include some (but not all) prepositions, conjunctions, determiners, possessive pronouns and pronouns.</S>
    <S sid="18" ssid="5">Some are proclitic ( attaching to the beginning of a stem) and some enclitics (attaching to the end of a stem).</S>
    <S sid="19" ssid="6">The following is an example of the different morphological segments in the word which means and by their virtues.</S>
    <S sid="20" ssid="7">Arabic is read from right to left hence the directional switch in the English gloss.</S>
    <S sid="21" ssid="8">Arabic: Translit: hm At Hsn b w Gloss: their s virtue by and The set of possible proclitics comprises the prepositions b,l,k , meaning by/with, to, as, respectively, the conjunctions w, f&#10026; , meaning and, then, respectively, and the definite article or determiner Al , meaning the.</S>
    <S sid="22" ssid="9">Arabic words may have a conjunction and a prepostition and a determiner cliticizing to the beginning of a word.</S>
    <S sid="23" ssid="10">The set ofpossible enclitics comprises the pronouns and (possessive pronouns) &#9734;y, nA, k, kmA, km, knA, kn, h, hA, hmA, hnA, hm, hn , respectively, my (mine), our (ours), To date, there are no fully automated systems addressing the community&#8217;s need for fundamental language processing tools for Arabic text.</S>
    <S sid="24" ssid="11">In this paper, we present a Support Vector Machine (SVM) based approach to automatically tokenize (segmenting off clitics), part-ofspeech (POS) tag and annotate base phrases (BPs) in Arabic text.</S>
    <S sid="25" ssid="12">We adapt highly accurate tools that have been developed for English text and apply them to Arabic text.</S>
    <S sid="26" ssid="13">Using standard evaluation metrics, we report that the SVM-TOR tokenizer achieves an score of 99.12, the SVM-POS tagger achieves an accuracy of 95.49%, and the SVM-BP chunker yields an score of 92.08. your (yours), your (yours) [masc. dual], your (yours) [masc. pl.</S>
    <S sid="27" ssid="14">], your (yours) [fem. dual], your (yours) [fem. pl.</S>
    <S sid="28" ssid="15">], him (his), her (hers), their (theirs) [masc. dual], their (theirs) [fem. dual], their (theirs) [masc. pl], their (theirs) [fem. pl.].</S>
    <S sid="29" ssid="16">An Arabic word may only have a single enclitic at the end.</S>
    <S sid="30" ssid="17">In this paper, stems+affixes, proclitics, enclitics and punctuation are referred to as tokens.</S>
    <S sid="31" ssid="18">We define a token as a space delimited unit in clitic tokenized text.</S>
    <S sid="32" ssid="19">We adopt a supervised learning approach, hence the need for annotated training data.</S>
    <S sid="33" ssid="20">Such data are available from the Arabic TreeBank,2 a modern standard Arabic corpus containing Agence France Presse (AFP) newswire articles ranging over a period of 5 months from July through November of 2000.</S>
    <S sid="34" ssid="21">The corpus comprises 734 news articles (140k words corresponding to 168k tokens after semi-automatic segmentation) covering various topics such as sports, politics, news, etc.</S>
  </SECTION>
  <SECTION title="3 Related Work" number="3">
    <S sid="35" ssid="1">To our knowledge, there are no systems that automatically tokenize and POS Arabic text as such.</S>
    <S sid="36" ssid="2">The current standard approach to Arabic tokenization and POS tagging &#8212;adopted in the Arabic TreeBank &#8212;relies on manually choosing the appropriate analysis from among the multiple analyses rendered by AraMorph, a sophisticated rule based morphological analyzer by Buckwalter.3 Morphological analysis may be characterized as the process of segmenting a surface word form into its component derivational and inflectional morphemes.</S>
    <S sid="37" ssid="3">In a language such as Arabic, which exhibits both inflectional and derivational morphology, the morphological tags tend to be fine grained amounting to a large number of tags &#8212; AraMorph has 135 distinct morphological labels &#8212; in contrast to POS tags which are typically coarser grained.</S>
    <S sid="38" ssid="4">Using AraMorph, the choice of an appropriate morphological analysis entails clitic tokenization as well assignment of a POS tag.</S>
    <S sid="39" ssid="5">Such morphological labels are potentially useful for NLP applications, yet the necessary manual choice renders it an expensive process.</S>
    <S sid="40" ssid="6">On the other hand, Khoja (Khoja, 2001) reports preliminary results on a hybrid, statistical and rule based, POS tagger, APT.</S>
    <S sid="41" ssid="7">APT yields 90% accuracy on a tag set of 131 tags including both POS and inflection morphology information.</S>
    <S sid="42" ssid="8">APT is a two-step hybrid system with rules and a Viterbi algorithm for statistically determining the appropriate POS tag.</S>
    <S sid="43" ssid="9">Given the tag set, APT is more of a morphological analyzer than a POS tagger.</S>
  </SECTION>
  <SECTION title="4 SVM Based Approach" number="4">
    <S sid="44" ssid="1">In the literature, various machine learning approaches are applied to the problem of POS tagging and BP Chunking.</S>
    <S sid="45" ssid="2">Such problems are cast as a classification problem where, given a number of features extracted from a predefined linguistic context, the task is to predict the class of a token.</S>
    <S sid="46" ssid="3">Support Vector Machines (SVMs) (Vapnik, 1995) are one class of such model.</S>
    <S sid="47" ssid="4">SVMs are a supervised learning algorithm that has the advantage of being robust where it can handle a large number of (overlapping) features with good generalization performance.</S>
    <S sid="48" ssid="5">Consequently, SVMs have been applied in many NLP tasks with great success (Joachims, 1998; Kudo and Matsumato, 2000; Hacioglu and Ward, 2003).</S>
    <S sid="49" ssid="6">We adopt a tagging perspective for the three tasks.</S>
    <S sid="50" ssid="7">Thereby, we address them using the same SVM experimental setup which comprises a standard SVM as a multiclass classifier (Allwein et al., 2000).</S>
    <S sid="51" ssid="8">The difference for the three tasks lies in the input, context and features.</S>
    <S sid="52" ssid="9">None of the features utilized in our approach is explicitly language dependent.</S>
    <S sid="53" ssid="10">The following subsections illustrate the different tasks and their corresponding features and tag sets.</S>
    <S sid="54" ssid="11">We approach word tokenization (segmenting off clitics) as a one-of-six classification task, in which each letter in a word is tagged with a label indicating its morphological identity.4 Therefore, a word may have proclitics and enclitic from the lists described in Section 2.</S>
    <S sid="55" ssid="12">A word may have no clitics at all, hence the .</S>
    <S sid="56" ssid="13">Input: A sequence of transliterated Arabic characters processed from left-to-right with &#8221;break&#8221; markers for word boundaries.</S>
    <S sid="57" ssid="14">Context: A fixed-size window of -5/+5 characters centered at the character in focus.</S>
    <S sid="58" ssid="15">Features: All characters and previous tag decisions within the context.</S>
    <S sid="59" ssid="16">Tag Set: The tag set is B-PRE1, B-PRE2, B-WRD, IWRD, B-SUFF, I-SUFF where I denotes inside a segment, B denotes beginning of a segment, PRE1 and PRE2 are proclitic tags, SUFF is an enclitic, and WRD is the stem plus any affixes and/or the determiner Al.</S>
    <S sid="60" ssid="17">Table 1 illustrates the correct tagging of the example above, w-b-hsnAt-hm, &#8217;and by their virtues&#8217;.</S>
    <S sid="61" ssid="18">We model this task as a 1-of-24 classification task, where the class labels are POS tags from the collapsed tag set in the Arabic TreeBank distribution.</S>
    <S sid="62" ssid="19">The training data is derived from the collapsed POS-tagged Treebank.</S>
    <S sid="63" ssid="20">Input: A sequence of tokens processed from left-to-right.</S>
    <S sid="64" ssid="21">Context: A window of -2/+2 tokens centered at the focus token.</S>
    <S sid="65" ssid="22">Features: Every character -gram, that occurs in the focus token, the 5 tokens themselves, their &#8216;type&#8217; from the set alpha, numeric , and POS tag decisions for previous tokens within context.</S>
    <S sid="66" ssid="23">Tag Set: The utilized tag set comprises the 24 collapsed tags available in the Arabic TreeBank distribution.</S>
    <S sid="67" ssid="24">This collapsed tag set is a manually reduced form of the 135 morpho-syntactic tags created by AraMorph.</S>
    <S sid="68" ssid="25">The tag set is as follows: CC, CD, CONJ+NEG PART, DT, FW, IN, JJ, NN, NNP, NNPS, NNS, NO FUNC, NUMERIC COMMA, PRP, PRP$, PUNC, RB, UH, VBD, VBN, VBP, WP, WRB .</S>
    <S sid="69" ssid="26">In this task, we use a setup similar to that of (Kudo and Matsumato, 2000), where 9 types of chunked phrases are recognized using a phrase IOB tagging scheme; Inside I a phrase, Outside O a phrase, and Beginning B of a phrase.</S>
    <S sid="70" ssid="27">Thus the task is a one of 19 classification task (since there are I and B tags for each chunk phrase type, and a single O tag).</S>
    <S sid="71" ssid="28">The training data is derived from the Arabic TreeBank using the ChunkLink software.5.</S>
    <S sid="72" ssid="29">ChunkLink flattens the tree to a sequence of base (non-recursive) phrase chunks with their IOB labels.</S>
    <S sid="73" ssid="30">The following example illustrates the tagging scheme: Input: A sequence of (word, POS tag) pairs.</S>
    <S sid="74" ssid="31">Context: A window of -2/+2 tokens centered at the focus token.</S>
    <S sid="75" ssid="32">Features: Word and POS tags that fall in the context along with previous IOB tags within the context.</S>
  </SECTION>
  <SECTION title="5 Evaluation" number="5">
    <S sid="76" ssid="1">The Arabic TreeBank consists of 4519 sentences.</S>
    <S sid="77" ssid="2">The development set, training set and test set are the same for all the experiments.</S>
    <S sid="78" ssid="3">The sentences are randomly distributed with 119 sentences in the development set, 400 sentences in the test set and 4000 sentences in the training set.</S>
    <S sid="79" ssid="4">The data is transliterated in the Arabic TreeBank into Latin based ASCII characters using the Buckwalter transliteration scheme.6 We used the non vocalized version of the treebank for all the experiments.</S>
    <S sid="80" ssid="5">All the data is derived from the parsed trees in the treebank.</S>
    <S sid="81" ssid="6">We use a standard SVM with a polynomial kernel, of degree 2 and C=1.7 Standard metrics of Accuracy (Acc), Precision (Prec), Recall (Rec), and the F-measure, , on the test set are utilized.8 Results: Table 2 presents the results obtained using the current SVM based approach, SVM-TOK, compared against two rule-based baseline approaches, RULE and RULE+DICT.</S>
    <S sid="82" ssid="7">RULE marks a prefix if a word starts with one of five proclitic letters described in Section 4.1.</S>
    <S sid="83" ssid="8">A suffix is marked if a word ends with any of the possessive pronouns, enclitics, mentioned above in Section 4.1.</S>
    <S sid="84" ssid="9">A small set of 17 function words that start with the proclitic letters is explicitly excluded.</S>
    <S sid="85" ssid="10">RULE+DICT only applies the tokenization rules in RULE if the token does not occur in a dictionary.</S>
    <S sid="86" ssid="11">The dictionary used comprises the 47,261 unique non vocalized word entries in the first column of Buckwalter&#8217;s dictStem, freely available with the AraMorph distribution.</S>
    <S sid="87" ssid="12">In some cases, dictionary entries retain inflectional morphology and clitics.</S>
    <S sid="88" ssid="13">Tags: O B-VP B-NP I-NP Translit: w qAlt rwv $wArtz Arabic: Gloss: and said Ruth Schwartz and SVM-TOK is only about 5% better (absolute) than the baseline RULE+DICT.</S>
    <S sid="89" ssid="14">While RULE+DICT could certainly be improved with larger dictionaries, however, the largest dictionary will still have coverage problems, therefore, there is a role for a data-driven approach such as SVM-TOK.</S>
    <S sid="90" ssid="15">An analysis of the confusion matrix for SVM-TOK shows that the most confusion occurs with the PREF2 class.</S>
    <S sid="91" ssid="16">This is hardly surprising since PREF2 is an infix category, and thus has two ambiguous boundaries.</S>
    <S sid="92" ssid="17">Results: Table 3 shows the results obtained with the SVM based POS tagger, SVM-POS, and the results obtained with a simple baseline, BASELINE, where the most frequent POS tag associated with a token from the training set is assigned to it in the test set.</S>
    <S sid="93" ssid="18">If the token does not occur in the training data, the token is assigned the NN tag as a default tag.</S>
    <S sid="94" ssid="19">Discussion: The performance of SVM-POS is better than the baseline BASELINE.</S>
    <S sid="95" ssid="20">50% of the errors encountered result from confusing nouns, NN, with adjectives, JJ, or vice versa.</S>
    <S sid="96" ssid="21">This is to be expected since these two categories are confusable in Arabic leading to inconsistencies in the training data.</S>
    <S sid="97" ssid="22">For example, the word for United in United States ofAmerica or United Nations is randomly tagged as a noun, or an adjective in the training data.</S>
    <S sid="98" ssid="23">We applied a similar SVM based POS tagging system to English text using the English TreeBank.</S>
    <S sid="99" ssid="24">The size of the training and test data corresponded to those evaluated in the Arabic experiments.</S>
    <S sid="100" ssid="25">The English experiment resulted in an accuracy of 94.97%, which is comparable to the Arabic SVM-POS results of 95.49%.</S>
    <S sid="101" ssid="26">Results: Table 4 illustrates the results obtained by SVM-BP Discussion: The overall performance of SVM-BP is score of 92.08.</S>
    <S sid="102" ssid="27">These results are interesting in light of state-of-the-art for English BP chunking performance which is at an score of 93.48, against a baseline of 77.7 in CoNLL 2000 shared task (Tjong et al., 2000).</S>
    <S sid="103" ssid="28">It is worth noting that SVM-BP trained on the English TreeBank, with a comparable training and test size data to those of the Arabic experiment, yields an score of 93.05.</S>
    <S sid="104" ssid="29">The best results obtained are for VP and PP, yielding scores of 97.6 and 98.4, respectively.</S>
  </SECTION>
  <SECTION title="6 Conclusions &amp; Future Directions" number="6">
    <S sid="105" ssid="1">We have presented a machine-learning approach using SVMs to solve the problem of automatically annotating Arabic text with tags at different levels; namely, tokenization at morphological level, POS tagging at lexical level, and BP chunking at syntactic level.</S>
    <S sid="106" ssid="2">The technique is language independent and highly accurate with an score of 99.12 on the tokenization task, 95.49% accuracy on the POS tagging task and score of 92.08 on the BP Chunking task.</S>
    <S sid="107" ssid="3">To the best of our knowledge, these are the first results reported for these tasks in Arabic natural language processing.</S>
    <S sid="108" ssid="4">We are currently trying to improve the performance of the systems by using additional features, a wider context and more data created semi-automatically using an unannotated large Arabic corpus.</S>
    <S sid="109" ssid="5">In addition, we are trying to extend the approach to semantic chunking by handlabeling a part of Arabic TreeBank with arguments or semantic roles for training.</S>
  </SECTION>
</PAPER>
