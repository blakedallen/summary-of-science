<PAPER>
  <S sid="0">Text Chunking Using Transformation-Based Learning</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Eric Brill introduced transformation-based learning and showed that it can do part-ofspeech tagging with fairly high accuracy.</S>
    <S sid="2" ssid="2">The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive &amp;quot;baseNP&amp;quot; chunks.</S>
    <S sid="3" ssid="3">For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word.</S>
    <S sid="4" ssid="4">In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence.</S>
    <S sid="5" ssid="5">Some interesting adaptations to the transformation-based learning approach are also suggested by this application.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Text chunking involves dividing sentences into nonoverlapping segments on the basis of fairly superficial analysis.</S>
    <S sid="7" ssid="2">Abney (1991) has proposed this as a useful and relatively tractable precursor to full parsing, since it provides a foundation for further levels of analysis including verb-argument identification, while still allowing more complex attachment decisions to be postponed to a later phase.</S>
    <S sid="8" ssid="3">Since chunking includes identifying the non-recursive portions of noun phrases, it can also be useful for other purposes including index term generation.</S>
    <S sid="9" ssid="4">Most efforts at superficially extracting segments from sentences have focused on identifying low-level noun groups, either using hand-built grammars and finite state techniques or using statistical models like HMMs trained from corpora.</S>
    <S sid="10" ssid="5">In this paper, we target a somewhat higher level of chunk structure using Brill's (1993b) transformation-based learning mechanism, in which a sequence of transformational rules is learned from a corpus; this sequence iteratively improves upon a baseline model for some interpretive feature of the text.</S>
    <S sid="11" ssid="6">This technique has previously been used not only for part-of-speech tagging (Brill, 1994), but also for prepositional phrase attachment disambiguation (Brill and Resnik, 1994), and assigning unlabeled binary-branching tree structure to sentences (Brill, 1993a).</S>
    <S sid="12" ssid="7">Because transformation-based learning uses patternaction rules based on selected features of the local context, it is helpful for the values being predicted to also be encoded locally.</S>
    <S sid="13" ssid="8">In the text-chunking application, encoding the predicted chunk structure in tags attached to the words, rather than as brackets between words, avoids many of the difficulties with unbalanced bracketings that would result if such local rules were allowed to insert or alter inter-word brackets directly.</S>
    <S sid="14" ssid="9">In this study, training and test sets marked with two different types of chunk structure were derived algorithmically from the parsed data in the Penn Treebank corpus of Wall Street Journal text (Marcus et al., 1994).</S>
    <S sid="15" ssid="10">The source texts were then run through Brill's part-of-speech tagger (Brill, 1993c), and, as a baseline heuristic, chunk structure tags were assigned to each word based on its part-of-speech tag.</S>
    <S sid="16" ssid="11">Rules were then automatically learned that updated these chunk structure tags based on neighboring words and their part-of-speech and chunk tags.</S>
    <S sid="17" ssid="12">Applying transformation-based learning to text chunking turns out to be different in interesting ways from its use for part-of-speech tagging.</S>
    <S sid="18" ssid="13">The much smaller tagset calls for a different organization of the computation, and the fact that part-of-speech assignments as well as word identities are fixed suggests different optimizations.</S>
  </SECTION>
  <SECTION title="2 Text Chunking" number="2">
    <S sid="19" ssid="1">Abney (1991) has proposed text chunking as a useful preliminary step to parsing.</S>
    <S sid="20" ssid="2">His chunks are inspired in part by psychological studies of Gee and Grosjean (1983) that link pause durations in reading and naive sentence diagraming to text groupings that they called 0-phrases, which very roughly correspond to breaking the string after each syntactic head that is a content word.</S>
    <S sid="21" ssid="3">Abney's other motivation for chunking is procedural, based on the hypothesis that the identification of chunks can be done fairly dependably by finite state methods, postponing the decisions that require higher-level analysis to a parsing phase that chooses how to combine the chunks.</S>
    <S sid="22" ssid="4">Existing efforts at identifying chunks in text have been focused primarily on low-level noun group identification, frequently as a step in deriving index terms, motivated in part by the limited coverage of present broad-scale parsers when dealing with unrestricted text.</S>
    <S sid="23" ssid="5">Some researchers have applied grammar-based methods, combining lexical data with finite state or other grammar constraints, while others have worked on inducing statistical models either directly from the words or from automatically assigned part-of-speech classes.</S>
    <S sid="24" ssid="6">On the grammar-based side, Bourigault (1992) describes a system for extracting &amp;quot;terminological noun phrases&amp;quot; from French text.</S>
    <S sid="25" ssid="7">This system first uses heuristics to find &amp;quot;maximal length noun phrases&amp;quot;, and then uses a grammar to extract &amp;quot;terminological units.&amp;quot; For example, from the maximal NP le disque dur de la station de travail it extracts the two terminological phrases disque dur, and station de travail.</S>
    <S sid="26" ssid="8">Bourigault claims that the grammar can parse &amp;quot;around 95% of the maximal length noun phrases&amp;quot; in a test corpus into possible terminological phrases, which then require manual validation.</S>
    <S sid="27" ssid="9">However, because its goal is terminological phrases, it appears that this system ignores NP chunk-initial determiners and other initial prenominal modifiers, somewhat simplifying the parsing task.</S>
    <S sid="28" ssid="10">Voutilainen (1993), in his impressive NPtool system, uses an approach that is in some ways similar to the one used here, in that he adds to his part-of-speech tags a new kind of tag that shows chunk structure; the chunk tag &amp;quot;&#169;&gt;N&amp;quot;, for example, is used for determiners and premodifiers, both of which group with the following noun head.</S>
    <S sid="29" ssid="11">He uses a lexicon that lists all the possible chunk tags for each word combined with hand-built constraint grammar patterns.</S>
    <S sid="30" ssid="12">These patterns eliminate impossible readings to identify a somewhat idiosyncratic kind of target noun group that does not include initial determiners but does include postmodifying prepositional phrases (including determiners).</S>
    <S sid="31" ssid="13">Voutilainen claims recall rates of 98.5% or better with precision of 95% or better.</S>
    <S sid="32" ssid="14">However, the sample NPtool analysis given in the appendix of (Voutilainen, 1993), appears to be less accurate than claimed in general, with 5 apparent mistakes (and one unresolved ambiguity) out of the 32 NP chunks in that sample, as listed in Table 1.</S>
    <S sid="33" ssid="15">These putative errors, combined with the claimed high performance, suggest that NPtool's definition of NP chunk is also tuned for extracting terminological phrases, and thus excludes many kinds of NP premodifiers, again simplifying the chunking task.</S>
    <S sid="34" ssid="16">NPtool parse Apparent correct parse less [time] [less time] the other hand &#8226; the [other hand] many [advantages] [many advantages] [binary addressing] [binary addressing and and [instruction formats] instruction formats] a purely [binary computer] a [purely binary computer] Kupiec (1993) also briefly mentions the use of finite state NP recognizers for both English and French to prepare the input for a program that identified the correspondences between NPs in bilingual corpora, but he does not directly discuss their performance.</S>
    <S sid="35" ssid="17">Using statistical methods, Church's Parts program (1988), in addition to identifying parts of speech, also inserted brackets identifying core NPs.</S>
    <S sid="36" ssid="18">These brackets were placed using a statistical model trained on Brown corpus material in which NP brackets had been inserted semi-automatically.</S>
    <S sid="37" ssid="19">In the small test sample shown, this system achieved 98% recall for correct brackets.</S>
    <S sid="38" ssid="20">At about the same time, Ejerhed (1988), working with Church, performed comparisons between finite state methods and Church's stochastic models for identifying both non-recursive clauses and non-recursive NPs in English text.</S>
    <S sid="39" ssid="21">In those comparisons, the stochastic methods outperformed the hand built finite-state models, with claimed accuracies of 93.5% (clauses) and 98.6% (NPs) for the statistical models compared to to 87% (clauses) and 97.8% (NPs) for the finite-state methods.</S>
    <S sid="40" ssid="22">Running Church's program on test material, however, reveals that the definition of NP embodied in Church's program is quite simplified in that it does not include, for example, structures or words conjoined within NP by either explicit conjunctions like &amp;quot;and&amp;quot; and &amp;quot;or&amp;quot;, or implicitly by commas.</S>
    <S sid="41" ssid="23">Church's chunker thus assigns the following NP chunk structures: [a Skokie] , [Ill.] , [subsidiary] [newer] , [big-selling prescriptions drugs] [the inefficiency] , [waste] and [lack] of [coordination] [Kidder] , [Peabody] Sz [Co] It is difficult to compare performance figures between studies; the definitions of the target chunks and the evaluation methodologies differ widely and are frequently incompletely specified.</S>
    <S sid="42" ssid="24">All of the cited performance figures above also appear to derive from manual checks by the investigators of the system's predicted output, and it is hard to estimate the impact of the system's suggested chunking on the judge's determination.</S>
    <S sid="43" ssid="25">We believe that the work reported here is the first study which has attempted to find NP chunks subject only to the limitation that the structures recognized do not include recursively embedded NPs, and which has measured performance by automatic comparison with a preparsed corpus.</S>
    <S sid="44" ssid="26">We performed experiments using two different chunk structure targets, one that tried to bracket non-recursive &amp;quot;baseNPs&amp;quot; and one that partitioned sentences into non-overlapping N-type and V-type chunks, loosely following Abney's model.</S>
    <S sid="45" ssid="27">Training and test materials with chunk tags encoding each of these kinds of structure were derived automatically from the parsed Wall Street Journal text in the Penn Treebank (Marcus et al., 1994).</S>
    <S sid="46" ssid="28">While this automatic derivation process introduced a small percentage of errors of its own, it was the only practical way both to provide the amount of training data required and to allow for fully-automatic testing.</S>
    <S sid="47" ssid="29">The goal of the &amp;quot;baseNP&amp;quot; chunks was to identify essentially the initial portions of nonrecursive noun phrases up to the head, including determiners but not including postmodifying prepositional phrases or clauses.</S>
    <S sid="48" ssid="30">These chunks were extracted from the Treebank parses, basically by selecting NPs that contained no nested NPs1.</S>
    <S sid="49" ssid="31">The handling of conjunction followed that of the Treebank annotators as to whether to show separate baseNPs or a single baseNP spanning the conjunction2.</S>
    <S sid="50" ssid="32">Possessives were treated as a special case, viewing the possessive marker as the first word of a new baseNP, thus flattening the recursive structure in a useful way.</S>
    <S sid="51" ssid="33">The following sentences give examples of this baseNP chunk structure: During [N the third quarter N] , [N Compaq N] purchased [N a former Wang Laboratories manufacturing facility N] in [N Sterling N], [N Scotland N] , which will be used for [N international service and repair operations N] .</S>
    <S sid="52" ssid="34">[N The government NJ has [N other agencies and instruments N] for pursuing [N these other objectives N] .</S>
    <S sid="53" ssid="35">Even [N Mao Tse-tung iv] [N 's China Ad began in [N 1949 N] with [N a partnership N] between [N the communists N] and [N a number N] of [N smaller , non-communist parties N] &#8226; The chunks in the partitioning chunk experiments were somewhat closer to Abney's model, where the prepositions in prepositional phrases are included with the object NP up to the head in a single N-type chunk.</S>
    <S sid="54" ssid="36">This created substantial additional ambiguity for the system, which had to distinguish prepositions from particles.</S>
    <S sid="55" ssid="37">The handling of conjunction again follows the Treebank parse with nominal conjuncts parsed in the Treebank as a single NP forming a single N chunk, while those parsed as conjoined NPs become separate chunks, with any coordinating conjunctions attached like prepositions to the following N chunk.</S>
    <S sid="56" ssid="38">The portions of the text not involved in N-type chunks were grouped as chunks termed Vtype, though these &amp;quot;V&amp;quot; chunks included many elements that were not verbal, including adjective phrases.</S>
    <S sid="57" ssid="39">The internal structure of these V-type chunks loosely followed the Treebank parse, though V chunks often group together elements that were sisters in the underlying parse tree.</S>
    <S sid="58" ssid="40">Again, the possessive marker was viewed as initiating a new N-type chunk.</S>
    <S sid="59" ssid="41">The following sentences are annotated with these partitioning N and V chunks: [N Some bankers NI [v are reporting v] [N more inquiries than usual N] [N about CDs N] [N since Friday NJ .</S>
    <S sid="60" ssid="42">'This heuristic fails in some cases.</S>
    <S sid="61" ssid="43">For example, Treebank uses the label NAC for some NPs functioning as premodifiers, like &amp;quot;Bank of England&amp;quot; in &amp;quot;Robin Leigh-Pemberton, Bank of England governor, conceded..&amp;quot;; in such cases, &amp;quot;governor&amp;quot; is not included in any baseNP chunk.</S>
    <S sid="62" ssid="44">'Non-constituent NP conjunction, which Treebank labels NX, is another example that still causes problems.</S>
    <S sid="63" ssid="45">[N Eastern Airlines N] [N 'creditors N] [v have begun exploring v] [N alternative approaches N] [N to a Chapter 11 reorganization N] [v because v] [N they N][v are unhappy v] [N with the carrier N] [N 's latest proposal N] .</S>
    <S sid="64" ssid="46">[N Indexing NI [N for the most part N] [v has involved simply buying v] [v and then holding v] [N stocks NI [N in the correct mix N] [v to mirror Id [N a stock market barometer N] .</S>
    <S sid="65" ssid="47">These two kinds of chunk structure derived from the Treebank data were encoded as chunk tags attached to each word and provided the targets for the transformation-based learning.</S>
  </SECTION>
  <SECTION title="3 The Transformation-based Learning Paradigm" number="3">
    <S sid="66" ssid="1">As shown in Fig.</S>
    <S sid="67" ssid="2">1, transformation-based learning starts with a supervised training corpus that specifies the correct values for some linguistic feature of interest, a baseline heuristic for predicting initial values for that feature, and a set of rule templates that determine a space of possible transformational rules.</S>
    <S sid="68" ssid="3">The patterns of the learned rules match to particular combinations of features in the neighborhood surrounding a word, and their action is to change the system's current guess as to the feature for that word.</S>
    <S sid="69" ssid="4">To learn a model, one first applies the baseline heuristic to produce initial hypotheses for each site in the training corpus.</S>
    <S sid="70" ssid="5">At each site where this baseline prediction is not correct, the templates are then used to form instantiated candidate rules with patterns that test selected features in the neighborhood of the word and actions that correct the currently incorrect tag assignment.</S>
    <S sid="71" ssid="6">This process eventually identifies all the rule candidates generated by that template set that would have a positive effect on the current tag assignments anywhere in the corpus.</S>
    <S sid="72" ssid="7">Those candidate rules are then tested against the rest of corpus, to identify at how many locations they would cause negative changes.</S>
    <S sid="73" ssid="8">One of those rules whose net score (positive changes minus negative changes) is maximal is then selected, applied to the corpus, and also written out as the first rule in the learned sequence.</S>
    <S sid="74" ssid="9">This entire learning process is then repeated on the transformed corpus: deriving candidate rules, scoring them, and selecting one with the maximal positive effect.</S>
    <S sid="75" ssid="10">This process is iterated, leading to an ordered sequence of rules, with rules discovered first ordered before those discovered later.</S>
    <S sid="76" ssid="11">The predictions of the model on new text are determined by beginning with the baseline heuristic prediction and then applying each rule in the learned rule sequence in turn.</S>
  </SECTION>
  <SECTION title="4 Transformational Text Chunking" number="4">
    <S sid="77" ssid="1">This section discusses how text chunking can be encoded as a tagging problem that can be conveniently addressed using transformational learning.</S>
    <S sid="78" ssid="2">We also note some related adaptations in the procedure for learning rules that improve its performance, taking advantage of ways in which this task differs from the learning of part-of-speech tags.</S>
    <S sid="79" ssid="3">Applying transformational learning to text chunking requires that the system's current hypotheses about chunk structure be represented in a way that can be matched against the pattern parts of rules.</S>
    <S sid="80" ssid="4">One way to do this would be to have patterns match tree fragments and actions modify tree geometries, as in Brill's transformational parser (1993a).</S>
    <S sid="81" ssid="5">In this work, we have found it convenient to do so by encoding the chunking using an additional set of tags, so that each word carries both a part-of-speech tag and also a &amp;quot;chunk tag&amp;quot; from which the chunk structure can be derived.</S>
    <S sid="82" ssid="6">In the baseNP experiments aimed at non-recursive NP structures, we use the chunk tag set {I, 0, B}, where words marked I are inside some baseNP, those marked 0 are outside, and the B tag is used to mark the left most item of a baseNP which immediately follows another baseNP.</S>
    <S sid="83" ssid="7">In these tests, punctuation marks were tagged in the same way as words.</S>
    <S sid="84" ssid="8">In the experiments that partitioned text into N and V chunks, we use the chunk tag set {BN, N, By, V, P}, where BN marks the first word and N the succeeding words in an N-type group while BV and V play the same role for V-type groups.</S>
    <S sid="85" ssid="9">Punctuation marks, which are ignored in Abney's chunk grammar, but which the Treebank data treats as normal- lexical items with their own part-of-speech tags, are unambiguously assigned the chunk tag P. Items tagged P are allowed to appear within N or V chunks; they are irrelevant as far as chunk boundaries are concerned, but they are still available to be matched against as elements of the left hand sides of rules.</S>
    <S sid="86" ssid="10">Encoding chunk structure with tags attached to words rather than non-recursive bracket markers inserted between words has the advantage that it limits the dependence between different elements of the encoded representation.</S>
    <S sid="87" ssid="11">While brackets must be correctly paired in order to derive a chunk structure, it is easy to define a mapping that can produce a valid chunk structure from any sequence of chunk tags; the few hard cases that arise can be handled completely locally.</S>
    <S sid="88" ssid="12">For example, in the baseNP tag set, whenever a B tag immediately follows an 0, it must be treated as an I, and, in the partitioning chunk tag set, wherever a V tag immediately follows an N tag without any intervening By, it must be treated as a By.</S>
    <S sid="89" ssid="13">Transformational learning begins with some initial &amp;quot;baseline&amp;quot; prediction, which here means a baseline assignment of chunk tags to words.</S>
    <S sid="90" ssid="14">Reasonable suggestions for baseline heuristics after a text has been tagged for part-of-speech might include assigning to each word the chunk tag that it carried most frequently in the training set, or assigning each part-of-speech tag the chunk tag that was most frequently associated with that part-of-speech tag in the training.</S>
    <S sid="91" ssid="15">We tested both approaches, and the baseline heuristic using part-of-speech tags turned out to do better, so it was the one used in our experiments.</S>
    <S sid="92" ssid="16">The part-of-speech tags used by this baseline heuristic, and then later also matched against by transformational rule patterns, were derived by running the raw texts in a prepass through Brill's transformational part-of-speech tagger (Brill, 1993c).</S>
    <S sid="93" ssid="17">In transformational learning, the space of candidate rules to be searched is defined by a set of rule templates that each specify a small number of particular feature sets as the relevant factors that a rule's left-hand-side pattern should examine, for example, the part-of-speech tag of the word two to the left combined with the actual word one to the left.</S>
    <S sid="94" ssid="18">In the preliminary scan of the corpus for each learning pass, it is these templates that are applied to each location whose current tag is not correct, generating a candidate rule that would apply at least at that one location, matching those factors and correcting the chunk tag assignment.</S>
    <S sid="95" ssid="19">When this approach is applied to part-of-speech tagging, the possible sources of evidence for templates involve the identities of words within a neighborhood of some appropriate size and their current part-of-speech tag assignments.</S>
    <S sid="96" ssid="20">In the text chunking application, the tags being assigned are chunk structure tags, while the part-of-speech tags are a fixed part of the environment, like the lexical identities of the words themselves.</S>
    <S sid="97" ssid="21">This additional class of available information causes a significant increase in the number of reasonable templates if templates for a wide range of the possible combinations of evidence are desired.</S>
    <S sid="98" ssid="22">The distributed version of Brill's tagger (Brill, 1993c) makes use of 26 templates, involving various mixes of word and part-of-speech tests on neighboring words.</S>
    <S sid="99" ssid="23">Our tests were performed using 100 templates; these included almost all of Brill's combinations, and extended them to include references to chunk tags as well as to words and part-of-speech tags.</S>
    <S sid="100" ssid="24">The set of 100 rule templates used here was built from repetitions of 10 basic patterns, shown on the left side of Table 2 as they apply to words.</S>
    <S sid="101" ssid="25">The same 10 patterns can also be used to match against part-of-speech tags, encoded as Po, P_1, etc.</S>
    <S sid="102" ssid="26">(In other tests, we have explored mixed templates, that match against both word and part-of-speech values, but no mixed templates were used in these experiments.)</S>
    <S sid="103" ssid="27">These 20 word and part-of-speech patterns were then combined with each of the 5 different chunk tag patterns shown on the right side of the table.</S>
    <S sid="104" ssid="28">The cross product of the 20 word and part-of-speech patterns with the 5 chunk tag patterns determined the full set of 100 templates used.</S>
  </SECTION>
  <SECTION title="5 Algorithm Design Issues" number="5">
    <S sid="105" ssid="1">The large increase in the number of rule templates in the text chunking application when compared to part-of-speech tagging pushed the training process against the available limits in terms of both space and time, particularly when combined with the desire to work with the largest possible training sets.</S>
    <S sid="106" ssid="2">Various optimizations proved to be crucial to make the tests described feasible.</S>
    <S sid="107" ssid="3">One change in the algorithm is related to the smaller size of the tag set.</S>
    <S sid="108" ssid="4">In Brill's tagger (Brill, 1993c), an initial calculation in each pass computes the confusion matrix for the current tag assignments and sorts the entries of that [old-tag x new-tag] matrix, so that candidate rules can then be processed in decreasing order of the maximum possible benefit for any rule changing, say, old tag I to new tag J.</S>
    <S sid="109" ssid="5">The search for the best-scoring rule can then be halted when a cell of the confusion matrix is reached whose maximum possible benefit is less than the net benefit of some rule already encountered.</S>
    <S sid="110" ssid="6">The power of that approach is dependent on the fact that the confusion matrix for part-ofspeech tagging partitions the space of candidate rules into a relatively large number of classes, so that one is likely to be able to exclude a reasonably large portion of the search space.</S>
    <S sid="111" ssid="7">In a chunk tagging application, with only 3 or 4 tags in the effective tagset, this approach based on the confusion matrix offers much less benefit.</S>
    <S sid="112" ssid="8">However, even though the confusion matrix does not usefully subdivide the space of possible rules when the tag set is this small, it is still possible to apply a similar optimization by sorting the entire list of candidate rules on the basis of their positive scores, and then processing the candidate rules (which means determining their negative scores and thus their net scores) in order of decreasing positive scores.</S>
    <S sid="113" ssid="9">By keeping track of the rule with maximum benefit seen so far, one can be certain of having found one of the globally best rules when one reaches candidate rules in the sorted list whose positive score is not greater than the net score of the best rule so far.</S>
    <S sid="114" ssid="10">In earlier work on transformational part-of-speech tagging (Ramshaw and Marcus, 1994), we noted that it is possible to greatly speed up the learning process by constructing a full, bidirectional index linking each candidate rule to those locations in the corpus at which it applies and each location in the corpus to those candidate rules that apply there.</S>
    <S sid="115" ssid="11">Such an index allows the process of applying rules to be performed without having to search through the corpus.</S>
    <S sid="116" ssid="12">Unfortunately, such complete indexing proved to be too costly in terms of physical memory to be feasible in this application.</S>
    <S sid="117" ssid="13">However, it is possible to construct a limited index that lists for each candidate rule those locations in the corpus at which the static portions of its left-hand-side pattern match.</S>
    <S sid="118" ssid="14">Because this index involves only the stable word identity and part-of-speech tag values, it does not require updating; thus it can be stored more compactly, and it is also not necessary to maintain back pointers from corpus locations to the applicable rules.</S>
    <S sid="119" ssid="15">This kind of partial static index proved to be a significant advantage in the portion of the program where candidate rules with relatively high positive scores are being tested to determine their negative scores, since it avoids the necessity of testing such rules against every location in the corpus.</S>
    <S sid="120" ssid="16">We also investigated a new heuristic to speed up the computation: After each pass, we disable all rules whose positive score is significantly lower than the net score of the best rule for the current pass.</S>
    <S sid="121" ssid="17">A disabled rule is then reenabled whenever enough other changes have been made to the corpus that it seems possible that the score of that rule might have changed enough to bring it back into contention for the top place.</S>
    <S sid="122" ssid="18">This is done by adding some fraction of the changes made in each pass to the positive scores of the disabled rules, and reenabling rules whose adjusted positive scores came within a threshold of the net score of the successful rule on some pass.</S>
    <S sid="123" ssid="19">Note that this heuristic technique introduces some risk of missing the actual best rule in a pass, due to its being incorrectly disabled at the time.</S>
    <S sid="124" ssid="20">However, empirical comparisons between runs with and without rule disabling suggest that conservative use of this technique can produce an order of magnitude speedup while imposing only a very slight cost in terms of suboptimality of the resulting learned rule sequence.</S>
  </SECTION>
  <SECTION title="6 Results" number="6">
    <S sid="125" ssid="1">The automatic derivation of training and testing data from the Treebank analyses allowed for fully automatic scoring, though the scores are naturally subject to any remaining systematic errors in the data derivation process as well as to bona fide parsing errors in the Treebank source.</S>
    <S sid="126" ssid="2">Table 3 shows the results for the baseNP tests, and Table 4 shows the results for the partitioning chunks task.</S>
    <S sid="127" ssid="3">Since training set size has a significant effect on the results, values are shown for three different training set sizes.</S>
    <S sid="128" ssid="4">(The test set in all cases was 50K words.</S>
    <S sid="129" ssid="5">Training runs were halted after the first 500 rules; rules learned after that point affect relatively few locations in the training set and have only a very slight effect for good or ill on test set performance.)</S>
    <S sid="130" ssid="6">The first line in each table gives the performance of the baseline system, which assigned a baseNP or chunk tag to each word on the basis of the POS tag assigned in the prepass.</S>
    <S sid="131" ssid="7">Performance is stated in terms of recall (percentage of correct chunks found) and precision (percentage of chunks found that are correct), where both ends of a chunk had to match exactly for it to be counted.</S>
    <S sid="132" ssid="8">The raw percentage of correct chunk tags is also given for each run, and for each performance measure, the relative error reduction compared to the baseline is listed.</S>
    <S sid="133" ssid="9">The partitioning chunks do appear to be somewhat harder to predict than baseNP chunks.</S>
    <S sid="134" ssid="10">The higher error reduction for the former is partly due to the fact that the part-of-speech baseline for that task is much lower.</S>
    <S sid="135" ssid="11">To give a sense of the kinds of rules being learned-, the first 10 rules from the 200K baseNP run are shown in Table 5.</S>
    <S sid="136" ssid="12">It is worth glossing the rules, since one of the advantages of transformationbased learning is exactly that the resulting model is easily interpretable.</S>
    <S sid="137" ssid="13">In the first of the baseNP rules, adjectives (with part-of-speech tag JJ) that are currently tagged I but that are followed by words tagged 0 have their tags changed to 0.</S>
    <S sid="138" ssid="14">In Rule 2, determiners that are preceded by two words both tagged I have their own tag changed to B, marking the beginning of a baseNP that happens to directly follow another.</S>
    <S sid="139" ssid="15">(Since the tag B is only used when baseNPs abut, the baseline system tags determiners as I.)</S>
    <S sid="140" ssid="16">Rule 3 takes words which immediately follow determiners tagged I that in turn follow something tagged 0 and changes their tag to also be I.</S>
    <S sid="141" ssid="17">Rules 4-6 are similar to Rule 2, marking the initial words of baseNPs that directly follow another baseNP.</S>
    <S sid="142" ssid="18">Rule 7 marks conjunctions (with part-of-speech tag CC) as I if they follow an I and precede a noun, since such conjunctions are more likely to be embedded in a single baseNP than to separate two baseNPs, and Rules 8 and 9 do the same.</S>
    <S sid="143" ssid="19">(The word &amp;quot;&amp;&amp;quot; in rule 8 comes mostly from company names in the Wall St. Journal source data.)</S>
    <S sid="144" ssid="20">Finally, Rule 10 picks up cases like &amp;quot;including about four million shares&amp;quot; where &amp;quot;about&amp;quot; is used as a quantifier rather than preposition.</S>
    <S sid="145" ssid="21">A similar list of the first ten rules for the chunk task can be seen in Table 6.</S>
    <S sid="146" ssid="22">To gloss a few of these, in the first rule here, determiners (with part-of-speech tag DT), which usually begin N chunks and thus are assigned the baseline tag BN, have their chunk tags changed to N if they follow a word whose tag is also BN.</S>
    <S sid="147" ssid="23">In Rule 2, sites currently tagged N but which fall at the beginning of a sentence have their tags switched to BN.</S>
    <S sid="148" ssid="24">(The dummy tag Z and word ZZZ indicate that the locations one to the left are beyond the sentence boundaries.)</S>
    <S sid="149" ssid="25">Rule 3 changes N to BN after a comma (which is tagged P), and in Rule 4, locations tagged BN are switched to BV if the following location is tagged V and has the part-of-speech tag VB.</S>
    <S sid="150" ssid="26">The fact that this system includes lexical rule templates that refer to actual words sets it apart from approaches that rely only on part-of-speech tags to predict chunk structure.</S>
    <S sid="151" ssid="27">To explore how much difference in performance those lexical rule templates make, we repeated the above test runs omitting templates that refer to specific words.</S>
    <S sid="152" ssid="28">The results for these runs, in Tables 7 and 8, suggest that the lexical rules improve performance on the baseNP chunk task by about 1% (roughly 5% of the overall error reduction) and on the partitioning chunk task by about 5% (roughly 10% of the error reduction).</S>
    <S sid="153" ssid="29">Thus lexical rules appear to be making a limited contribution in determining baseNP chunks, but a more significant one for the partitioning chunks.</S>
    <S sid="154" ssid="30">A rough hand categorization of a sample of the errors from a baseNP run indicates that many fall into classes that are understandably difficult for any process using only local word and partof-speech patterns to resolve.</S>
    <S sid="155" ssid="31">The most frequent single confusion involved words tagged VBG and VBN, whose baseline prediction given their part-of-speech tag was 0, but which also occur frequently inside baseNPs.</S>
    <S sid="156" ssid="32">The system did discover some rules that allowed it to fix certain classes of VBG and VBN mistaggings, for example, rules that retagged VBNs as I when they preceded an NN or NNS tagged I.</S>
    <S sid="157" ssid="33">However, many also remained unresolved, and many of those appear to be cases that would require more than local word and part-of-speech patterns to resolve.</S>
    <S sid="158" ssid="34">The second most common class of errors involved conjunctions, which, combined with the former class, make up half of all the errors in the sample.</S>
    <S sid="159" ssid="35">The Treebank tags the words &amp;quot;and&amp;quot; and frequently &amp;quot;,&amp;quot; with the part-of-speech tag CC, which the baseline system again predicted would fall most often outside of a baseNP3.</S>
    <S sid="160" ssid="36">However, the Treebank parses do also frequently classify conjunctions of Ns or NPs as a single baseNP, and again there appear to be insufficient clues in the word and tag contexts for the current system to make the distinction.</S>
    <S sid="161" ssid="37">Frequently, in fact, the actual choice of structure assigned by the Treebank annotators seemed largely dependent on semantic indications unavailable to the transformational learner.</S>
  </SECTION>
  <SECTION title="7 Future Directions" number="7">
    <S sid="162" ssid="1">We are planning to explore several different paths that might increase the system's power to distinguish the linguistic contexts in which particular changes would be useful.</S>
    <S sid="163" ssid="2">One such direction is to expand the template set by adding templates that are sensitive to the chunk structure.</S>
    <S sid="164" ssid="3">For example, instead of referring to the word two to the left, a rule pattern could refer to the first word in the current chunk, or the last word of the previous chunk.</S>
    <S sid="165" ssid="4">Another direction would be to enrich the vocabulary of chunk tags, so that they could be used during the learning process to encode contextual features for use by later rules in the sequence.</S>
    <S sid="166" ssid="5">We would also like to explore applying these same kinds of techniques to building larger scale structures, in which larger units are assembled or predicate/argument structures derived by combining chunks.</S>
    <S sid="167" ssid="6">One interesting direction here would be to explore the use of chunk structure tags that encode a form of dependency grammar, where the tag &amp;quot;N+2&amp;quot; might mean that the current word is to be taken as part.of the unit headed by the N two words to the right.</S>
  </SECTION>
  <SECTION title="8 Conclusions" number="8">
    <S sid="168" ssid="1">By representing text chunking as a kind of tagging problem, it becomes possible to easily apply transformation-based learning.</S>
    <S sid="169" ssid="2">We have shown that this approach is able to automatically induce a chunking model from supervised training that achieves recall and precision of 92% for baseNP chunks and 88% for partitioning N and V chunks.</S>
    <S sid="170" ssid="3">Such chunking models provide a useful and feasible next step in textual interpretation that goes beyond part-of-speech tagging, and that serve as a foundation both for larger-scale grouping and for direct extraction of subunits like index terms.</S>
    <S sid="171" ssid="4">In addition, some variations in the transformation-based learning algorithm are suggested by this application that may also be useful in other settings.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="9">
    <S sid="172" ssid="1">We would like to thank Eric Brill for making his system widely available, and Ted Briscoe and David Yarowsky for helpful comments, including the suggestion to test the system's performance without lexical rule templates.</S>
    <S sid="173" ssid="2">'Note that this is one of the cases where Church's chunker allows separate NP fragments to count as chunks.</S>
  </SECTION>
</PAPER>
