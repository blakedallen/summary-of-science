<PAPER>
  <S sid="0">Bayesian Unsupervised Topic Segmentation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper describes a novel Bayesian approach to unsupervised topic segmentation.</S>
    <S sid="2" ssid="2">Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution.</S>
    <S sid="3" ssid="3">We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation.</S>
    <S sid="4" ssid="4">This contrasts with previous approaches, which relied on hand-crafted cohesion metrics.</S>
    <S sid="5" ssid="5">The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems.</S>
    <S sid="6" ssid="6">Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets.</S>
    <S sid="7" ssid="7">We also show that both an entropy-based analysis and a well-known previous technique can be de</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="8" ssid="1">Topic segmentation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments.</S>
    <S sid="9" ssid="2">Hearst&#8217;s TEXTTILING (1994) introduced the idea that unsupervised segmentation can be driven by lexical cohesion, as high-quality segmentations feature homogeneous lexical distributions within each topic segment.</S>
    <S sid="10" ssid="3">Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation.</S>
    <S sid="11" ssid="4">But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems.</S>
    <S sid="12" ssid="5">For example, consider cue phrases, which are explicit discourse markers such as &#8220;now&#8221; or &#8220;however&#8221; (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996).</S>
    <S sid="13" ssid="6">Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models.</S>
    <S sid="14" ssid="7">One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006).</S>
    <S sid="15" ssid="8">Without supervision, it is not possible to combine such metrics with additional sources of information.</S>
    <S sid="16" ssid="9">Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (Malioutov and Barzilay, 2006; Galley et al., 2003).</S>
    <S sid="17" ssid="10">In this paper, we situate lexical cohesion in a Bayesian framework, allowing other sources of information to be incorporated without the need for labeled data.</S>
    <S sid="18" ssid="11">We formalize lexical cohesion in a generative model in which the text for each segment is produced by a distinct lexical distribution.</S>
    <S sid="19" ssid="12">Lexically-consistent segments are favored by this model because probability mass is conserved for a narrow subset of words.</S>
    <S sid="20" ssid="13">Thus, lexical cohesion arises naturally through the generative process, and other sources of information &#8211; such as cue words &#8211; can easily be incorporated as emissions from the segment boundaries.</S>
    <S sid="21" ssid="14">More formally, we treat the words in each sentence as draws from a language model associated with the topic segment.</S>
    <S sid="22" ssid="15">This is related to topicmodeling methods such as latent Dirichlet allocation (LDA; Blei et al. 2003), but here the induced topics are tied to a linear discourse structure.</S>
    <S sid="23" ssid="16">This property enables a dynamic programming solution to find the exact maximum-likelihood segmentation.</S>
    <S sid="24" ssid="17">We consider two approaches to handling the language models: estimating them explicitly, and integrating them out, using the Dirichlet Compound Multinomial distribution (also known as the multivariate Polya distribution).</S>
    <S sid="25" ssid="18">We model cue phrases as generated from a separate multinomial that is shared across all topics and documents in the dataset; a high-likelihood model will obtain a compact set of cue phrases.</S>
    <S sid="26" ssid="19">The addition of cue phrases renders our dynamic programming-based inference inapplicable, so we design a sampling-based inference technique.</S>
    <S sid="27" ssid="20">This algorithm can learn in a completely unsupervised fashion, but it also provides a principled mechanism to improve search through the addition of declarative linguistic knowledge.</S>
    <S sid="28" ssid="21">This is achieved by biasing the selection of samples towards boundaries with known cue phrases; this does not change the underlying probabilistic model, but guides search in the direction of linguistically-plausible segmentations.</S>
    <S sid="29" ssid="22">We evaluate our algorithm on corpora of spoken and written language, including the benchmark ICSI meeting dataset (Janin et al., 2003) and a new textual corpus constructed from the contents of a medical textbook.</S>
    <S sid="30" ssid="23">In both cases our model achieves performance surpassing multiple state-of-the-art baselines.</S>
    <S sid="31" ssid="24">Moreover, we demonstrate that the addition of cue phrases can further improve segmentation performance over cohesion-based methods.</S>
    <S sid="32" ssid="25">In addition to the practical advantages demonstrated by these experimental results, our model reveals interesting theoretical properties.</S>
    <S sid="33" ssid="26">Other researchers have observed relationships between discourse structure and entropy (e.g., Genzel and Charniak, 2002).</S>
    <S sid="34" ssid="27">We show that in a special case of our model, the segmentation objective is equal to a weighted sum of the negative entropies for each topic segment.</S>
    <S sid="35" ssid="28">This finding demonstrates that a relationship between discourse segmentation and entropy is a natural consequence of modeling topic structure in a generative Bayesian framework.</S>
    <S sid="36" ssid="29">In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model.</S>
  </SECTION>
  <SECTION title="2 Related Work" number="2">
    <S sid="37" ssid="1">Existing unsupervised cohesion-based approaches can be characterized in terms of the metric used to quantify cohesion and the search technique.</S>
    <S sid="38" ssid="2">Galley et al. (2003) characterize cohesion in terms of lexical chains &#8211; repetitions of a given lexical item over some fixed-length window of sentences.</S>
    <S sid="39" ssid="3">In their unsupervised model, inference is performed by selecting segmentation points at the local maxima of the cohesion function.</S>
    <S sid="40" ssid="4">Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences.</S>
    <S sid="41" ssid="5">Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model.</S>
    <S sid="42" ssid="6">Both of these last two systems use dynamic programming to search the space of segmentations.</S>
    <S sid="43" ssid="7">An alternative Bayesian approach to segmentation was proposed by Purver et al. (2006).</S>
    <S sid="44" ssid="8">They assume a set of documents that is characterized by some number of hidden topics that are shared across multiple documents.</S>
    <S sid="45" ssid="9">They then build a linear segmentation by adding a switching variable to indicate whether the topic distribution for each sentence is identical to that of its predecessor.</S>
    <S sid="46" ssid="10">Unlike Purver et al., we do not assume a dataset in which topics are shared across multiple documents; indeed, our model can be applied to single documents individually.</S>
    <S sid="47" ssid="11">Additionally, the inference procedure of Purver et al. requires sampling multiple layers of hidden variables.</S>
    <S sid="48" ssid="12">In contrast, our inference procedure leverages the nature of linear segmentation to search only in the space of segmentation points.</S>
    <S sid="49" ssid="13">The relationship between discourse structure and cue phrases has been studied extensively; for an early example of computational work on this topic, see (Grosz, 1977).</S>
    <S sid="50" ssid="14">Passonneau and Litman (1993) were the first to investigate the relationship between cue phrases and linear segmentation.</S>
    <S sid="51" ssid="15">More recently, cue phrases have been applied to topic segmentation in the supervised setting.</S>
    <S sid="52" ssid="16">In a supervised system that is distinct from the unsupervised model described above, Galley et al. (2003) automatically identify candidate cue phrases by mining labeled data for words that are especially likely to appear at segment boundaries; the presence of cue phrases is then used as a feature in a rule-based classifier for linear topic segmentation.</S>
    <S sid="53" ssid="17">Elsner and Charniak (2008) specify a list of cue phrases by hand; the cue phrases are used as a feature in a maximum-entropy classifier for conversation disentanglement.</S>
    <S sid="54" ssid="18">Unlike these approaches, we identify candidate cue phrases automatically from unlabeled data and incorporate them in the topic segmentation task without supervision.</S>
  </SECTION>
  <SECTION title="3 Lexical Cohesion in a Bayesian Framework" number="3">
    <S sid="55" ssid="1">The core idea of lexical cohesion is that topicallycoherent segments demonstrate compact and consistent lexical distributions (Halliday and Hasan, 1976).</S>
    <S sid="56" ssid="2">Lexical cohesion can be placed in a probabilistic context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment.</S>
    <S sid="57" ssid="3">Formally, if sentence t is in segment j, then the bag of words xt is drawn from the multinomial language model &#952;j.</S>
    <S sid="58" ssid="4">This is similar in spirit to hidden topic models such as latent Dirichlet allocation (Blei et al., 2003), but rather than assigning a hidden topic to each word, we constrain the topics to yield a linear segmentation of the document.</S>
    <S sid="59" ssid="5">We will assume that topic breaks occur at sentence boundaries, and write zt to indicate the topic assignment for sentence t. The observation likelihood is, where X is the set of all T sentences, z is the vector of segment assignments for each sentence, and &#920; is the set of all K language models.2 A linear segmentation is ensured by the additional constraint that zt must be equal to either zt&#8722;1 (the previous sentence&#8217;s segment) or zt&#8722;1 + 1 (the next segment).</S>
    <S sid="60" ssid="6">To obtain a high likelihood, the language models associated with each segment should concentrate their probability mass on a compact subset of words.</S>
    <S sid="61" ssid="7">Language models that spread their probability mass over a broad set of words will induce a lower likelihood.</S>
    <S sid="62" ssid="8">This is consistent with the principle of lexical cohesion.</S>
    <S sid="63" ssid="9">Thus far, we have described a segmentation in terms of two parameters: the segment indices z, and the set of language models &#920;.</S>
    <S sid="64" ssid="10">For the task of segmenting documents, we are interested only in the segment indices, and would prefer not to have to search in the space of language models as well.</S>
    <S sid="65" ssid="11">We consider two alternatives: taking point estimates of the language models (Section 3.1), and analytically marginalizing them out (Section 3.2).</S>
    <S sid="66" ssid="12">One way to handle the language models is to choose a single point estimate for each set of segmentation points z.</S>
    <S sid="67" ssid="13">Suppose that each language model is drawn from a symmetric Dirichlet prior: &#952;j &#8212; Dir(&#952;0).</S>
    <S sid="68" ssid="14">Let nj be a vector in which each element is the sum of the lexical counts over all the sentences in segment j: nj,i = E{t:zt=j} mt,i, where mt,i is the count of word i in sentence t. Assuming that each xt &#8212; &#952;j, then the posterior distribution for &#952;j is Dirichlet with vector parameter nj +&#952;0 (Bernardo and Smith, 2000).</S>
    <S sid="69" ssid="15">The expected value of this distribution is the multinomial distribution &#710;&#952;j, where, In this equation, W indicates the number of words in the vocabulary.</S>
    <S sid="70" ssid="16">Having obtained an estimate for the language model &#710;&#952;j, the observed data likelihood for segment j is a product over each sentence in the segment, 2Our experiments will assume that the number of topics K is known.</S>
    <S sid="71" ssid="17">This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006).</S>
    <S sid="72" ssid="18">By viewing the likelihood as a product over all terms in the vocabulary, we observe interesting connections with prior work on segmentation and information theory.</S>
    <S sid="73" ssid="19">In this section, we explain how our model generalizes the well-known method of Utiyama and Isahara (2001; hereafter U&amp;I).</S>
    <S sid="74" ssid="20">As in our work, Utiyama and Isahara propose a probabilistic framework based on maximizing the compactness of the language models induced for each segment.</S>
    <S sid="75" ssid="21">Their likelihood equation is identical to our equations 3-5.</S>
    <S sid="76" ssid="22">They then define the language models for each segment as &#65533;Bj,i = nj,iW1 , without rigorous justifiW+Ei nj,i cation.</S>
    <S sid="77" ssid="23">This form is equivalent to Laplacian smoothing (Manning and Sch&#168;utze, 1999), and is a special case of our equation 2, with B0 = 1.</S>
    <S sid="78" ssid="24">Thus, the language models in U&amp;I can be viewed as the expectation of the posterior distribution p(Bj|{xt : zt = j}, B0), in the special case that B0 = 1.</S>
    <S sid="79" ssid="25">Our approach generalizes U&amp;I and provides a Bayesian justification for the language models that they apply.</S>
    <S sid="80" ssid="26">The remainder of the paper further extends this work by marginalizing out the language model, and by adding cue phrases.</S>
    <S sid="81" ssid="27">We empirically demonstrate that these extensions substantially improve performance.</S>
    <S sid="82" ssid="28">Our model also has a connection to entropy, and situates entropy-based segmentation within a Bayesian framework.</S>
    <S sid="83" ssid="29">Equation 1 defines the objective function as a product across sentences; using equations 3-5 we can decompose this across segments instead.</S>
    <S sid="84" ssid="30">Working in logarithms, The last line substitutes in the logarithm of equation 5.</S>
    <S sid="85" ssid="31">Setting B0 = 0 and rearranging equation 2, we obtain nj,i = Nj&#65533;Bj,i, with Nj = PW i nj,i, the total number of words in segment j.</S>
    <S sid="86" ssid="32">Substituting this into equation 6, we obtain where H(Bj) is the negative entropy of the multinomial Bj.</S>
    <S sid="87" ssid="33">Thus, with B0 = 0, the log conditional probability in equation 6 is optimized by a segmentation that minimizes the weighted sum of entropies per segment, where the weights are equal to the segment lengths.</S>
    <S sid="88" ssid="34">This result suggests intriguing connections with prior work on the relationship between entropy and discourse structure (e.g., Genzel and Charniak, 2002; Sporleder and Lapata, 2006).</S>
    <S sid="89" ssid="35">The previous subsection uses point estimates of the language models to reveal connections to entropy and prior work on segmentation.</S>
    <S sid="90" ssid="36">However, point estimates are theoretically unsatisfying from a Bayesian perspective, and better performance may be obtained by marginalizing over all possible laneach segment, so the overall likelihood for the pointestimate version also decomposes across segments.</S>
    <S sid="91" ssid="37">Any objective function that can be decomposed into a product across segments can be maximized using dynamic programming.</S>
    <S sid="92" ssid="38">We define B(t) as the value of the objective function for the optimal segmentation up to sentence t. The contribution to the objective function from a single segment between sentences t' and t is written, b(t', t) = p({xt, ... xt}|zt-...t = j) where pdcm refers to the Dirichlet compound multinomial distribution (DCM), also known as the multivariate Polya distribution (Johnson et al., 1997).</S>
    <S sid="93" ssid="39">The DCM distribution expresses the expectation over all multinomial language models, when conditioning on the Dirichlet prior &#952;0.</S>
    <S sid="94" ssid="40">When &#952;0 is a symmetric Dirichlet prior, where nj,i is the count of word i in segment j, and Nj = PWi nj,i, the total number of words in the segment.</S>
    <S sid="95" ssid="41">The symbol F refers to the Gamma function, an extension of the factorial function to real numbers.</S>
    <S sid="96" ssid="42">Using the DCM distribution, we can compute the data likelihood for each segment from the lexical counts over the entire segment.</S>
    <S sid="97" ssid="43">The overall observation likelihood is a product across the likelihoods for each segment.</S>
    <S sid="98" ssid="44">The optimal segmentation maximizes the joint probability, p(X, z|&#952;0) = p(X|z, &#952;0)p(z).</S>
    <S sid="99" ssid="45">We assume that p(z) is a uniform distribution over valid segmentations, and assigns no probability mass to invalid segmentations.</S>
    <S sid="100" ssid="46">The data likelihood is defined for point estimate language models in equation 5 and for marginalized language models in equation 7.</S>
    <S sid="101" ssid="47">Note that equation 7 is written as a product over segments.</S>
    <S sid="102" ssid="48">The point estimates for the language models depend only on the counts within The maximum value of the objective function is then given by the recurrence relation, B(t) = maxt,&lt;t B(t')b(t'+1, t), with the base case B(0) = 1.</S>
    <S sid="103" ssid="49">These values can be stored in a table of size T (equal to the number of sentences); this admits a dynamic program that performs inference in polynomial time.3 If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size TK.</S>
    <S sid="104" ssid="50">The Dirichlet compound multinomial integrates over language models, but we must still set the prior &#952;0.</S>
    <S sid="105" ssid="51">We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization framework (Gauvain and Lee, 1994).</S>
    <S sid="106" ssid="52">In the E-step, we estimate a segmentation z&#65533; of the dataset, as described in Section 3.3.</S>
    <S sid="107" ssid="53">In the M-step, we maximize p(&#952;0|X, z) &#8733; p(X|&#952;0, z)p(&#952;0).</S>
    <S sid="108" ssid="54">Assuming a non-informative hyperprior p(&#952;0), we maximize the likelihood in Equation 7 across all documents.</S>
    <S sid="109" ssid="55">The maximization is performed using a gradient-based search; the gradients are dervied by Minka (2003).</S>
    <S sid="110" ssid="56">This procedure is iterated until convergence or a maximum of twenty iterations.</S>
  </SECTION>
  <SECTION title="4 Cue Phrases" number="4">
    <S sid="111" ssid="1">One of the key advantages of a Bayesian framework for topic segmentation is that it permits the principled combination of multiple data sources, even without labeled data.</S>
    <S sid="112" ssid="2">We are especially interested in cue phrases, which are explicit markers for discourse structure, such as &#8220;now&#8221; or &#8220;first&#8221; (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996).</S>
    <S sid="113" ssid="3">Cue phrases have previously been used in supervised topic segmentation (e.g., Galley et al. 2003); we show how they can be used in an unsupervised setting.</S>
    <S sid="114" ssid="4">The previous section modeled lexical cohesion by treating the bag of words in each sentence as a series of draws from a multinomial language model indexed by the topic segment.</S>
    <S sid="115" ssid="5">To incorporate cue phrases, this generative model is modified to reflect the idea that some of the text will be topic-specific, but other terms will be topic-neutral cue phrases that express discourse structure.</S>
    <S sid="116" ssid="6">This idea is implemented by drawing the text at each topic boundary from a special language model &#966;, which is shared across all topics and all documents in the dataset.</S>
    <S sid="117" ssid="7">For sentences that are not at segment boundaries, the likelihood is as before: p(xt|z, o, &#966;) = Q i&#8712;xt &#952;zt,i.</S>
    <S sid="118" ssid="8">For sentences that immediately follow segment boundaries, we draw the first ` words from &#966; instead.</S>
    <S sid="119" ssid="9">Writing x&#65533;`) t for the ` cue words in xt, and Rt for the remaining words, the likelihood for a segment-initial sentence is, We draw &#966; from a symmetric Dirichlet prior &#966;0.</S>
    <S sid="120" ssid="10">Following prior work (Galley et al., 2003; Litman and Passonneau, 1995), we consider only the first word of each sentence as a potential cue phrase; thus, we set ` = 1 in all experiments.</S>
    <S sid="121" ssid="11">To estimate or marginalize the language models o and &#966;, it is necessary to maintain lexical counts for each segment and for the segment boundaries.</S>
    <S sid="122" ssid="12">The counts for &#966; are summed across every segment in the entire dataset, so shifting a boundary will affect the probability of every segment, not only the adjacent segments as before.</S>
    <S sid="123" ssid="13">Thus, the factorization that enabled dynamic programming inference in Section 3.3 is no longer applicable.</S>
    <S sid="124" ssid="14">Instead, we must resort to approximate inference.</S>
    <S sid="125" ssid="15">Sampling-based inference is frequently used in related Bayesian models.</S>
    <S sid="126" ssid="16">Such approaches build a stationary Markov chain by repeatedly sampling among the hidden variables in the model.</S>
    <S sid="127" ssid="17">The most commonly-used sampling-based technique is Gibbs sampling, which iteratively samples from the conditional distribution of each hidden variable (Bishop, 2006).</S>
    <S sid="128" ssid="18">However, Gibbs sampling is slow to converge to a stationary distribution when the hidden variables are tightly coupled.</S>
    <S sid="129" ssid="19">This is the case in linear topic segmentation, due to the constraint that zt E {zt&#8722;1, zt&#8722;1 + 11 (see Section 3).</S>
    <S sid="130" ssid="20">For this reason, we apply the more general Metropolis-Hastings algorithm, which permits sampling arbitrary transformations of the latent variables.</S>
    <S sid="131" ssid="21">In our framework, such transformations correspond to moves through the space of possible segmentations.</S>
    <S sid="132" ssid="22">A new segmentation z0 is drawn from the previous hypothesized segmentation z based on a proposal distribution q(z0|z).4 The probability of accepting a proposed transformation depends on the ratio of the joint probabilities and a correction term for asymmetries in the proposal distribution: The Metropolis-Hastings algorithm guarantees that by accepting samples at this ratio, our sampling procedure will converge to the stationary distribution for the hidden variables z.</S>
    <S sid="133" ssid="23">When cue phrases are included, the observation likelihood is written: As in Section 3.2, we can marginalize over the language models.</S>
    <S sid="134" ssid="24">We obtain a product of DCM distributions: one for each segment, and one for all cue phrases in the dataset.</S>
    <S sid="135" ssid="25">Metropolis-Hastings requires a proposal distribution to sample new configurations.</S>
    <S sid="136" ssid="26">The proposal distri4Because the cue phrase language model 0 is used across the entire dataset, transformations affect the likelihood of all documents in the corpus.</S>
    <S sid="137" ssid="27">For clarity, our exposition will focus on the single-document case. bution does not affect the underlying probabilistic model &#8211; Metropolis-Hastings will converge to the same underlying distribution for any non-degenerate proposal.</S>
    <S sid="138" ssid="28">However, a well-chosen proposal distribution can substantially speed convergence.</S>
    <S sid="139" ssid="29">Our basic proposal distribution selects an existing segmentation point with uniform probability, and considers a set of local moves.</S>
    <S sid="140" ssid="30">The proposal is constructed so that no probability mass is allocated to moves that change the order of segment boundaries, or merge two segments; one consequence of this restriction is that moves cannot add or remove segments.5 We set the proposal distribution to decrease exponentially with the move distance, thus favoring incremental transformations to the segmentation.</S>
    <S sid="141" ssid="31">More formally, let d(z &#8212;* z') &gt; 0 equal the distance that the selected segmentation point is moved when we transform the segmentation from z to z'.</S>
    <S sid="142" ssid="32">We can write the proposal distribution q(z'  |z) a c(z &#8212;* z')d(z &#8212;* z')A, where A &lt; 0 sets the rate of exponential decay and c is an indicator function enforcing the constraint that the moves do not reach or cross existing segmentation points.6 We can also incorporate declarative linguistic knowledge by biasing the proposal distribution in favor of moves that place boundaries near known cue phrase markers.</S>
    <S sid="143" ssid="33">We multiply the unnormalized chance of proposing a move to location z &#8212;* z' by a term equal to one plus the number of candidate cue phrases in the segment-initial sentences in the new configuration z', written num-cue(z').</S>
    <S sid="144" ssid="34">Formally, qling(z'  |z') a (1 + num-cue(z'))q(z'  |z).</S>
    <S sid="145" ssid="35">We use a list of cue phrases identified by Hirschberg and Litman (1993).</S>
    <S sid="146" ssid="36">We evaluate our model with both the basic and linguistically-enhanced proposal distributions.</S>
    <S sid="147" ssid="37">As in section 3.4, we set the priors 00 and 00 using gradient-based search.</S>
    <S sid="148" ssid="38">In this case, we perform gradient-based optimization after epochs of 1000 max-move, where max-move is the maximum move-length, set to 5 in our experiments.</S>
    <S sid="149" ssid="39">These parameters affect the rate of convergence but are unrelated to the underlying probability model.</S>
    <S sid="150" ssid="40">In the limit of enough samples, all nonpathological settings will yield the same segmentation results.</S>
    <S sid="151" ssid="41">Metropolis-Hasting steps.</S>
    <S sid="152" ssid="42">Interleaving samplingbased inference with direct optimization of parameters can be considered a form of Monte Carlo Expectation-Maximization (MCEM; Wei and Tanner, 1990).</S>
  </SECTION>
  <SECTION title="5 Experimental Setup" number="5">
    <S sid="153" ssid="1">Corpora We evaluate our approach on corpora from two different domains: transcribed meetings and written text.</S>
    <S sid="154" ssid="2">For multi-speaker meetings, we use the ICSI corpus of meeting transcripts (Janin et al., 2003), which is becoming a standard for speech segmentation (e.g., Galley et al. 2003; Purver et al.</S>
    <S sid="155" ssid="3">2006).</S>
    <S sid="156" ssid="4">This dataset includes transcripts of 75 multi-party meetings, of which 25 are annotated for segment boundaries.</S>
    <S sid="157" ssid="5">For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chapter into the sections indicated by the author.</S>
    <S sid="158" ssid="6">This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter).</S>
    <S sid="159" ssid="7">Each chapter contains an average of 140 sentences, giving an average of 28 sentences per segment.</S>
    <S sid="160" ssid="8">Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores.</S>
    <S sid="161" ssid="9">Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other.</S>
    <S sid="162" ssid="10">WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not.</S>
    <S sid="163" ssid="11">Pk and WindowDiff are penalties, so lower values indicate better segmentations.</S>
    <S sid="164" ssid="12">We use the evaluation source code provided by Malioutov and Barzilay (2006).</S>
    <S sid="165" ssid="13">System configuration We evaluate our Bayesian approach both with and without cue phrases.</S>
    <S sid="166" ssid="14">Without cue phrases, we use the dynamic programming inference described in section 3.3.</S>
    <S sid="167" ssid="15">This system is referred to as BAYESSEG in Table 1.</S>
    <S sid="168" ssid="16">When adding cue phrases, we use the Metropolis-Hastings model described in 4.1.</S>
    <S sid="169" ssid="17">Both basic and linguisticallymotivated proposal distributions are evaluated (see Section 4.2); these are referred to as BAYESSEGCUE and BAYESSEG-CUE-PROP in the table.</S>
    <S sid="170" ssid="18">For the sampling-based systems, results are averaged over five runs.</S>
    <S sid="171" ssid="19">The initial configuration is obtained from the dynamic programming inference, and then 100,000 sampling iterations are performed.</S>
    <S sid="172" ssid="20">The final segmentation is obtained by annealing the last 25,000 iterations to a temperature of zero.</S>
    <S sid="173" ssid="21">The use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based inference is common (e.g., Finkel 2005; Goldwater 2007).</S>
    <S sid="174" ssid="22">The total running time of our system is on the order of three minutes per document.</S>
    <S sid="175" ssid="23">Due to memory constraints, we divide the textbook dataset into ten parts, and perform inference in each part separately.</S>
    <S sid="176" ssid="24">We may achieve better results by performing inference over the entire dataset simultaneously, due to pooling counts for cue phrases across all documents.</S>
    <S sid="177" ssid="25">Baselines We compare against three competitive alternative systems from the literature: U&amp;I (Utiyama and Isahara, 2001); LCSEG (Galley et al., 2003); MCS (Malioutov and Barzilay, 2006).</S>
    <S sid="178" ssid="26">All three systems are described in the related work (Section 2).</S>
    <S sid="179" ssid="27">In all cases, we use the publicly available executables provided by the authors.</S>
    <S sid="180" ssid="28">Parameter settings For LCSEG, we use the parameter values specified in the paper (Galley et al., 2003).</S>
    <S sid="181" ssid="29">MCS requires parameter settings to be tuned on a development set.</S>
    <S sid="182" ssid="30">Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006).</S>
    <S sid="183" ssid="31">Our system does not require parameter tuning; priors are re-estimated as described in Sections 3.4 and 4.3.</S>
    <S sid="184" ssid="32">U&amp;I requires no parameter tuning, and is used &#8220;out of the box.&#8221; In all experiments, we assume that the number of desired segments is provided.</S>
    <S sid="185" ssid="33">Preprocessing Standard preprocessing techniques are applied to the text for all comparisons.</S>
    <S sid="186" ssid="34">The Porter (1980) stemming algorithm is applied to group equivalent lexical items.</S>
    <S sid="187" ssid="35">A set of stop-words is also removed, using the same list originally employed by several competitive systems (Choi, 2000; ter performance.</S>
    <S sid="188" ssid="36">BAYESSEG is the cohesion-only Bayesian system with marginalized language models.</S>
    <S sid="189" ssid="37">BAYESSEG-CUE is the Bayesian system with cue phrases.</S>
    <S sid="190" ssid="38">BAYESSEG-CUE-PROP adds the linguisticallymotivated proposal distribution.</S>
    <S sid="191" ssid="39">Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006).</S>
  </SECTION>
  <SECTION title="6 Results" number="6">
    <S sid="192" ssid="1">Table 1 presents the performance results for three instantiations of our Bayesian framework and three competitive alternative systems.</S>
    <S sid="193" ssid="2">As shown in the table, the Bayesian models achieve the best results on both metrics for both corpora.</S>
    <S sid="194" ssid="3">On the medical textbook corpus, the Bayesian systems achieve a raw performance gain of 2-3% with respect to all baselines on both metrics.</S>
    <S sid="195" ssid="4">On the ICSI meeting corpus, the Bayesian systems perform 4-5% better than the best baseline on the Pk metric, and achieve smaller improvement on the WindowDiff metric.</S>
    <S sid="196" ssid="5">The results on the meeting corpus also compare favorably with the topic-modeling method of Purver et al. (2006), who report a Pk of .289 and a WindowDiff of .329.</S>
    <S sid="197" ssid="6">Another observation from Table 1 is that the contribution of cue phrases depends on the dataset.</S>
    <S sid="198" ssid="7">Cue phrases improve performance on the meeting corpus, but not on the textbook corpus.</S>
    <S sid="199" ssid="8">The effectiveness of cue phrases as a feature depends on whether the writer or speaker uses them consistently.</S>
    <S sid="200" ssid="9">At the same time, the addition of cue phrases prevents the use of exact inference techniques, which may explain the decline in results for the meetings dataset.</S>
    <S sid="201" ssid="10">To investigate the quality of the cue phrases that our model extracts, we list its top ten cue phrases for each dataset in Table 2.</S>
    <S sid="202" ssid="11">Cue phrases are ranked by their chi-squared value, which is computed based on the number of occurrences for each word at the beginning of a hypothesized segment, as compared to the expectation.</S>
    <S sid="203" ssid="12">For cue phrases listed in bold, the chi-squared value is statistically significant at the level of p &lt; .01, indicating that the frequency with which the cue phrase appears at the beginning of segments is unlikely to be a chance phenomenon.</S>
    <S sid="204" ssid="13">As shown in the left column of the table, our model has identified several strong cue phrases from the meeting dataset which appear to be linguistically plausible.</S>
    <S sid="205" ssid="14">Galley et al. (2003) performed a similar chi-squared analysis, but used the true segment boundaries in the labeled data; this can be thought of as a sort of ground truth.</S>
    <S sid="206" ssid="15">Four of the ten cue phrases identified by our system overlap with their analysis; these are indicated with asterisks.</S>
    <S sid="207" ssid="16">In contrast to our model&#8217;s success at extracting cue phrases from the meeting dataset, only very common words are selected for the textbook dataset.</S>
    <S sid="208" ssid="17">This may help to explain why cue phrases improve performance for meeting transcripts, but not for the textbook.</S>
  </SECTION>
  <SECTION title="7 Conclusions" number="7">
    <S sid="209" ssid="1">This paper presents a novel Bayesian approach to unsupervised topic segmentation.</S>
    <S sid="210" ssid="2">Our algorithm is capable of incorporating both lexical cohesion and cue phrase features in a principled manner, and outperforms state-of-the-art baselines on text and transcribed speech corpora.</S>
    <S sid="211" ssid="3">We have developed exact and sampling-based inference techniques, both of which search only over the space of segmentations and marginalize out the associated language models.</S>
    <S sid="212" ssid="4">Finally, we have shown that our model provides a theoretical framework with connections to information theory, while also generalizing and justifying prior work.</S>
    <S sid="213" ssid="5">In the future, we hope to explore the use of similar Bayesian techniques for hierarchical segmentation, and to incorporate additional features such as prosody and speaker change information.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="214" ssid="1">The authors acknowledge the support of the National Science Foundation (CAREER grant IIS0448168) and the Microsoft Research Faculty Fellowship.</S>
    <S sid="215" ssid="2">Thanks to Aaron Adler, S. R. K. Branavan, Harr Chen, Michael Collins, Randall Davis, Dan Roy, David Sontag and the anonymous reviewers for helpful comments and suggestions.</S>
    <S sid="216" ssid="3">We also thank Michel Galley, Igor Malioutov, and Masao Utiyama for making their topic segmentation code publically available.</S>
    <S sid="217" ssid="4">Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF.</S>
  </SECTION>
</PAPER>
