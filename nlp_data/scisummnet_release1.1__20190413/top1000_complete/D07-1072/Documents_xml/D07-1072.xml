<PAPER>
	<S sid="0">The Infinite PCFG Using Hierarchical Dirichlet Processes</S><ABSTRACT>
		<S sid="1" ssid="1">We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process (HDP).</S>
		<S sid="2" ssid="2">Our HDP-PCFG model allows the complexity of the grammar to grow as more training data is available.</S>
		<S sid="3" ssid="3">In addition to presenting a fully Bayesianmodel for the PCFG, we also develop an ef ficient variational inference procedure.</S>
		<S sid="4" ssid="4">Onsynthetic data, we recover the correct grammar without having to specify its complexity in advance.</S>
		<S sid="5" ssid="5">We also show that our tech niques can be applied to full-scale parsingapplications by demonstrating its effective ness in learning state-split grammars.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="6" ssid="6">Probabilistic context-free grammars (PCFGs) havebeen a core modeling technique for many aspects of linguistic structure, particularly syntac tic phrase structure in treebank parsing (Charniak, 1996; Collins, 1999).</S>
			<S sid="7" ssid="7">An important question when learning PCFGs is how many grammar symbols to allocate to the learning algorithm based on the amount of available data.</S>
			<S sid="8" ssid="8">The question of ?how many clusters (symbols)??</S>
			<S sid="9" ssid="9">has been tackled in the Bayesian nonparametricsliterature via Dirichlet process (DP) mixture mod els (Antoniak, 1974).</S>
			<S sid="10" ssid="10">DP mixture models have since been extended to hierarchical Dirichlet processes (HDPs) and HDP-HMMs (Teh et al, 2006; Beal et al., 2002) and applied to many different types of clustering/induction problems in NLP (Johnson et al., 2006; Goldwater et al, 2006).In this paper, we present the hierarchical Dirich let process PCFG (HDP-PCFG).</S>
			<S sid="11" ssid="11">a nonparametric Bayesian model of syntactic tree structures based on Dirichlet processes.</S>
			<S sid="12" ssid="12">Specifically, an HDP-PCFG is defined to have an infinite number of symbols; the Dirichlet process (DP) prior penalizes the use of more symbols than are supported by the training data.</S>
			<S sid="13" ssid="13">Note that ?nonparametric?</S>
			<S sid="14" ssid="14">does not mean ?noparameters?; rather, it means that the effective num ber of parameters can grow adaptively as the amount of data increases, which is a desirable property of a learning algorithm.As models increase in complexity, so does the un certainty over parameter estimates.</S>
			<S sid="15" ssid="15">In this regime, point estimates are unreliable since they do not take into account the fact that there are different amountsof uncertainty in the various components of the pa rameters.</S>
			<S sid="16" ssid="16">The HDP-PCFG is a Bayesian model which naturally handles this uncertainty.</S>
			<S sid="17" ssid="17">We present an efficient variational inference algorithm for theHDP-PCFG based on a structured mean-field ap proximation of the true posterior over parameters.The algorithm is similar in form to EM and thus inherits its simplicity, modularity, and efficiency.</S>
			<S sid="18" ssid="18">Un like EM, however, the algorithm is able to take theuncertainty of parameters into account and thus in corporate the DP prior.Finally, we develop an extension of the HDP PCFG for grammar refinement (HDP-PCFG-GR).Since treebanks generally consist of coarselylabeled context-free tree structures, the maximum likelihood treebank grammar is typically a poormodel as it makes overly strong independence as sumptions.</S>
			<S sid="19" ssid="19">As a result, many generative approaches to parsing construct refinements of the treebankgrammar which are more suitable for the model ing task.</S>
			<S sid="20" ssid="20">Lexical methods split each pre-terminal symbol into many subsymbols, one for each word,and then focus on smoothing sparse lexical statis 688 tics (Collins, 1999; Charniak, 2000).</S>
			<S sid="21" ssid="21">Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al, 2005; Petrov et al, 2006).</S>
			<S sid="22" ssid="22">We apply our HDP-PCFG-GRmodel to automatically learn the number of subsym bols for each symbol.</S>
	</SECTION>
	<SECTION title="Models based on Dirichlet processes. " number="2">
			<S sid="23" ssid="1">At the heart of the HDP-PCFG is the Dirichlet pro cess (DP) mixture model (Antoniak, 1974), which isthe nonparametric Bayesian counterpart to the clas sical finite mixture model.</S>
			<S sid="24" ssid="2">In order to build up an understanding of the HDP-PCFG, we first review the Bayesian treatment of the finite mixture model (Section 2.1).</S>
			<S sid="25" ssid="3">We then consider the DP mixture model (Section 2.2) and use it as a building block for developing nonparametric structured versions of the HMM (Section 2.3) and PCFG (Section 2.4).</S>
			<S sid="26" ssid="4">Our presentation highlights the similarities between these models so that each step along this progression reflects only the key differences.</S>
			<S sid="27" ssid="5">2.1 Bayesian finite mixture model.</S>
			<S sid="28" ssid="6">We begin by describing the Bayesian finite mixture model to establish basic notation that will carry over the more complex models we consider later.</S>
			<S sid="29" ssid="7">Bayesian finite mixture model ? ?</S>
			<S sid="30" ssid="8">Dirichlet(?, . . .</S>
			<S sid="31" ssid="9">, ?) [draw component probabilities] For each component z ? {1, . . .</S>
			<S sid="32" ssid="10">,K}: ??z ? G0 [draw component parameters] For each data point i ? {1, . . .</S>
			<S sid="33" ssid="11">, n}: ?zi ? Multinomial(?)</S>
			<S sid="34" ssid="12">[choose component] ?xi ? F (?;?zi) [generate data point]The model has K components whose prior dis tribution is specified by ? = (?1, . . .</S>
			<S sid="35" ssid="13">, ?K).</S>
			<S sid="36" ssid="14">The Dirichlet hyperparameter ? controls how uniformthis distribution is: as ? increases, it becomes in creasingly likely that the components have equal probability.</S>
			<S sid="37" ssid="15">For each mixture component z ? {1, . . .</S>
			<S sid="38" ssid="16">,K}, the parameters of the component ?z aredrawn from some prior G0.</S>
			<S sid="39" ssid="17">Given the model param eters (?,?), the data points are generated i.i.d. by first choosing a component and then generating from a data model F parameterized by that component.</S>
			<S sid="40" ssid="18">In document clustering, for example, each datapoint xi is a document represented by its term frequency vector.</S>
			<S sid="41" ssid="19">Each component (cluster) z has multinomial parameters ?z which specifies adistribution F (?;?z) over words.</S>
			<S sid="42" ssid="20">It is custom ary to use a conjugate Dirichlet prior G0 =Dirichlet(??, . . .</S>
			<S sid="43" ssid="21">over the multinomial parameters, which can be interpreted as adding ???1 pseu docounts for each word.</S>
			<S sid="44" ssid="22">2.2 DP mixture model.</S>
			<S sid="45" ssid="23">We now consider the extension of the Bayesian finite mixture model to a nonparametric Bayesian mixture model based on the Dirichlet process.</S>
			<S sid="46" ssid="24">We focus on the stick-breaking representation (Sethuraman,1994) of the Dirichlet process instead of the stochastic process definition (Ferguson, 1973) or the Chinese restaurant process (Pitman, 2002).</S>
			<S sid="47" ssid="25">The stick breaking representation captures the DP prior most explicitly and allows us to extend the finite mixture model with minimal changes.</S>
			<S sid="48" ssid="26">Later, it will enable us to readily define structured models in a form similar to their classical versions.</S>
			<S sid="49" ssid="27">Furthermore, an efficient variational inference algorithm can be developed in this representation (Section 2.6).</S>
			<S sid="50" ssid="28">The key difference between the Bayesian finite mixture model and the DP mixture model is that the latter has a countably infinite number of mixture components while the former has a predefined K. Note that if we have an infinite number of mixture components, it no longer makes sense to consider a symmetric prior over the component probabilities; the prior over component probabilities must decay in some way.</S>
			<S sid="51" ssid="29">The stick-breaking distribution achieves this as follows.</S>
			<S sid="52" ssid="30">We write ? ?</S>
			<S sid="53" ssid="31">GEM(?)</S>
			<S sid="54" ssid="32">to mean that ? = (?1, ?2, . . .</S>
			<S sid="55" ssid="33">) is distributed according to the stick-breaking distribution.</S>
			<S sid="56" ssid="34">Here, the concentrationparameter ? controls the number of effective com ponents.</S>
			<S sid="57" ssid="35">To draw ? ?</S>
			<S sid="58" ssid="36">GEM(?), we first generatea countably infinite collection of stick-breaking pro portions u1, u2, . . .</S>
			<S sid="59" ssid="37">, where each uz ? Beta(1, ?).</S>
			<S sid="60" ssid="38">The stick-breaking weights ? are then defined in terms of the stick proportions: ?z = uz ? z?&lt;z (1 ? uz?).</S>
			<S sid="61" ssid="39">(1) The procedure for generating ? can be viewed asiteratively breaking off remaining portions of a unit 689 0 1?1 ?2 ?3 ...</S>
			<S sid="62" ssid="40">Figure 1: A sample ? ?</S>
			<S sid="63" ssid="41">GEM(1).</S>
			<S sid="64" ssid="42">length stick (Figure 1).</S>
			<S sid="65" ssid="43">The component probabilities {?z} will decay exponentially in expectation, but there is always some probability of getting a smallercomponent before a larger one.</S>
			<S sid="66" ssid="44">The parameter ? de termines the decay of these probabilities: a larger ? implies a slower decay and thus more components.</S>
			<S sid="67" ssid="45">Given the component probabilities, the rest of the DP mixture model is identical to the finite mixture model: DP mixture model ? ?</S>
			<S sid="68" ssid="46">GEM(?)</S>
			<S sid="69" ssid="47">[draw component probabilities] For each component z ? {1, 2, . . .</S>
			<S sid="70" ssid="48">}: ??z ? G0 [draw component parameters] For each data point i ? {1, . . .</S>
			<S sid="71" ssid="49">, n}: ?zi ? Multinomial(?)</S>
			<S sid="72" ssid="50">[choose component] ?xi ? F (?;?zi) [generate data point xn] 2.3 HDP-HMM.</S>
			<S sid="73" ssid="51">The next stop on the way to the HDP-PCFG is the HDP hidden Markov model (HDP-HMM) (Beal et al., 2002; Teh et al, 2006).</S>
			<S sid="74" ssid="52">An HMM consists of a set of hidden states, where each state can be thought of as a mixture component.</S>
			<S sid="75" ssid="53">The parameters of the mixture component are the emission and transition parameters.</S>
			<S sid="76" ssid="54">The main aspect that distinguishes itfrom a flat finite mixture model is that the transition parameters themselves must specify a distribu tion over next states.</S>
			<S sid="77" ssid="55">Hence, we have not just onetop-level mixture model over states, but also a col lection of mixture models, one for each state.</S>
			<S sid="78" ssid="56">In developing a nonparametric version of the HMM in which the number of states is infinite, we need to ensure that the transition mixture models of each state share a common inventory of possiblenext states.</S>
			<S sid="79" ssid="57">We can achieve this by tying these mix ture models together using the hierarchical Dirichlet process (HDP) (Teh et al, 2006).</S>
			<S sid="80" ssid="58">The stick-breaking representation of an HDP is defined as follows: first,the top-level stick-breaking weights ? are drawn ac cording to the stick-breaking prior as before.</S>
			<S sid="81" ssid="59">Then, a new set of stick-breaking weights ??</S>
			<S sid="82" ssid="60">are generated according based on ?: ??</S>
			<S sid="83" ssid="61">DP(??,?), (2) where the distribution of DP can be characterized in terms of the following finite partition property: for all partitions of the positive integers into sets A1, . . .</S>
			<S sid="84" ssid="62">, Am, (??(A1), . . .</S>
			<S sid="85" ssid="63">,??(Am)) (3) ? Dirichlet ( ???(A1), . . .</S>
			<S sid="86" ssid="64">, ???(Am) ) , where ?(A) = ? k?A ?k. 1 The resulting ??</S>
			<S sid="87" ssid="65">is an-.</S>
			<S sid="88" ssid="66">other distribution over the positive integers whosesimilarity to ? is controlled by a concentration pa rameter ??.</S>
			<S sid="89" ssid="67">HDP-HMM ? ?</S>
			<S sid="90" ssid="68">GEM(?)</S>
			<S sid="91" ssid="69">[draw top-level state weights] For each state z ? {1, 2, . . .</S>
			<S sid="92" ssid="70">}: ??Ez ? Dirichlet(?)</S>
			<S sid="93" ssid="71">[draw emission parameters] ??Tz ? DP(?</S>
			<S sid="94" ssid="72">?, ?) [draw transition parameters] For each time step i ? {1, . . .</S>
			<S sid="95" ssid="73">, n}: ?xi ? F (?;?Ezi) [emit current observation] ?zi+1 ? Multinomial(?Tzi) [choose next state]Each state z is associated with emission param eters ?Ez . In addition, each z is also associatedwith transition parameters ?Tz , which specify a distribution over next states.</S>
			<S sid="96" ssid="74">These transition parame ters are drawn from a DP centered on the top-level stick-breaking weights ? according to Equations (2) and (3).</S>
			<S sid="97" ssid="75">Assume that z1 is always fixed to a special START state, so we do not need to generate it.</S>
			<S sid="98" ssid="76">2.4 HDP-PCFG.</S>
			<S sid="99" ssid="77">We now present the HDP-PCFG, which is the focus of this paper.</S>
			<S sid="100" ssid="78">For simplicity, we consider Chomsky normal form (CNF) grammars, which has two typesof rules: emissions and binary productions.</S>
			<S sid="101" ssid="79">We con sider each grammar symbol as a mixture component whose parameters are the rule probabilities for that symbol.</S>
			<S sid="102" ssid="80">In general, we do not know the appropriate number of grammar symbols, so our strategy is to let the number of grammar symbols be infinite and place a DP prior over grammar symbols.</S>
			<S sid="103" ssid="81">1Note that this property is a specific instance of the general stochastic process definition of Dirichlet processes.</S>
			<S sid="104" ssid="82">690 HDP-PCFG ? ?</S>
			<S sid="105" ssid="83">GEM(?)</S>
			<S sid="106" ssid="84">[draw top-level symbol weights] For each grammar symbol z ? {1, 2, . . .</S>
			<S sid="107" ssid="85">}: ??Tz ? Dirichlet(?</S>
			<S sid="108" ssid="86">T ) [draw rule type parameters] ??Ez ? Dirichlet(?</S>
			<S sid="109" ssid="87">E) [draw emission parameters] ??Bz ? DP(?</S>
			<S sid="110" ssid="88">B ,??T ) [draw binary production parameters] For each node i in the parse tree: ?ti ? Multinomial(?Tzi) [choose rule type] ?If ti = EMISSION: ??xi ? Multinomial(?Ezi) [emit terminal symbol] ?If ti = BINARY-PRODUCTION: ??(zL(i), zR(i)) ? Multinomial(?</S>
			<S sid="111" ssid="89">B zi) [generate children symbols] ? ?Bz ?Tz ?Ez z ? z1 z2 x2 z3 x3 T Parameters Trees Figure 2: The definition and graphical model of the HDP-PCFG.</S>
			<S sid="112" ssid="90">Since parse trees have unknown structure, there is no convenient way of representing them in the visual language of traditional graphical models.</S>
			<S sid="113" ssid="91">Instead, we show a simple fixed example tree.</S>
			<S sid="114" ssid="92">Node 1 has two children, 2 and 3, each of which has one observed terminal child.</S>
			<S sid="115" ssid="93">We use L(i) and R(i) to denote the left and right children of node i. In the HMM, the transition parameters of a statespecify a distribution over single next states; similarly, the binary production parameters of a gram mar symbol must specify a distribution over pairs of grammar symbols for its children.</S>
			<S sid="116" ssid="94">We adapt theHDP machinery to tie these binary production distri butions together.</S>
			<S sid="117" ssid="95">The key difference is that now wemust tie distributions over pairs of grammar sym bols together via distributions over single grammar symbols.</S>
			<S sid="118" ssid="96">Another difference is that in the HMM, at each time step, both a transition and a emission are made, whereas in the PCFG either a binary production or an emission is chosen.</S>
			<S sid="119" ssid="97">Therefore, each grammar symbol must also have a distribution over the type of rule to apply.</S>
			<S sid="120" ssid="98">In a CNF PCFG, there are only two types of rules, but this can be easily generalized to include unary productions, which we use for our parsing experiments.</S>
			<S sid="121" ssid="99">To summarize, the parameters of each grammar symbol z consists of (1) a distribution over a finitenumber of rule types ?Tz , (2) an emission distribution ?Ez over terminal symbols, and (3) a binary production distribution ?Bz over pairs of children gram mar symbols.</S>
			<S sid="122" ssid="100">Figure 2 describes the model in detail.Figure 3 shows the generation of the binary pro duction distributions ?Bz . We draw ? B z from a DP centered on ??T , which is the product distribution over pairs of symbols.</S>
			<S sid="123" ssid="101">The result is a doubly-infinitematrix where most of the probability mass is con state right child state left child state right child state left child state ? ?</S>
			<S sid="124" ssid="102">GEM(?)</S>
			<S sid="125" ssid="103">??T ?Bz ? DP(??</S>
			<S sid="126" ssid="104">T )Figure 3: The generation of binary production prob abilities given the top-level symbol probabilities ?.</S>
			<S sid="127" ssid="105">First, ? is drawn from the stick-breaking prior, as in any DP-based model (a).</S>
			<S sid="128" ssid="106">Next, the outer-product ??T is formed, resulting in a doubly-infinite matrix matrix (b).</S>
			<S sid="129" ssid="107">We use this as the base distribution for generating the binary production distribution from a DP centered on ??T (c).centrated in the upper left, just like the top-level dis tribution ??T . Note that we have replaced the general 691 G0 and F (?Ezi) pair with Dirichlet(?</S>
			<S sid="130" ssid="108">E) and Multinomial(?Ezi) to specialize to natural language, but there is no difficulty in working with parse trees with arbitrary non-multinomial observations or more sophisticated word models.</S>
			<S sid="131" ssid="109">In many natural language applications, there is a hard distinction between pre-terminal symbols(those that only emit a word) and non-terminal sym bols (those that only rewrite as two non-terminal or pre-terminal symbols).</S>
			<S sid="132" ssid="110">This can be accomplished by letting ?T = (0, 0), which forces a draw ?Tz to assign probability 1 to one rule type.</S>
			<S sid="133" ssid="111">An alternative definition of an HDP-PCFG would be as follows: for each symbol z, draw a distributionover left child symbols lz ? DP(?)</S>
			<S sid="134" ssid="112">and an inde pendent distribution over right child symbols rz ?DP(?).</S>
			<S sid="135" ssid="113">Then define the binary production distribu tion as their cross-product ?Bz = lzr T z . This alsoyields a distribution over symbol pairs and hence de fines a different type of nonparametric PCFG.</S>
			<S sid="136" ssid="114">This model is simpler and does not require any additional machinery beyond the HDP-HMM.</S>
			<S sid="137" ssid="115">However, the modeling assumptions imposed by this alternative are unappealing as they assume the left child and right child are independent given the parent, which is certainly not the case in natural language.</S>
			<S sid="138" ssid="116">2.5 HDP-PCFG for grammar refinement.</S>
			<S sid="139" ssid="117">An important motivation for the HDP-PCFG is thatof refining an existing treebank grammar to alle viate unrealistic independence assumptions and to improve parsing accuracy.</S>
			<S sid="140" ssid="118">In this scenario, the set of symbols is known, but we do not know howmany subsymbols to allocate per symbol.</S>
			<S sid="141" ssid="119">We in troduce the HDP-PCFG for grammar refinement (HDP-PCFG-GR), an extension of the HDP-PCFG, for this task.</S>
			<S sid="142" ssid="120">The essential difference is that now we have a collection of HDP-PCFG models for each symbol s ? S, each one operating at the subsymbol level.</S>
			<S sid="143" ssid="121">While these HDP-PCFGs are independent in the prior, they are coupled through their interactions inthe parse trees.</S>
			<S sid="144" ssid="122">For completeness, we have also in cluded unary productions, which are essentially the PCFG counterpart of transitions in HMMs.</S>
			<S sid="145" ssid="123">Finally,since each node i in the parse tree involves a symbol subsymbol pair (si, zi), each subsymbol needs to specify a distribution over both child symbols and subsymbols.</S>
			<S sid="146" ssid="124">The former can be handled through a finite Dirichlet distribution since all symbols are known and observed, but the latter must be handledwith the Dirichlet process machinery, since the num ber of subsymbols is unknown.</S>
			<S sid="147" ssid="125">HDP-PCFG for grammar refinement (HDP-PCFG-GR) For each symbol s ? S: ??s ? GEM(?)</S>
			<S sid="148" ssid="126">[draw subsymbol weights] ?For each subsymbol z ? {1, 2, . . .</S>
			<S sid="149" ssid="127">}: ???Tsz ? Dirichlet(?</S>
			<S sid="150" ssid="128">T ) [draw rule type parameters] ???Esz ? Dirichlet(?</S>
			<S sid="151" ssid="129">E(s)) [draw emission parameters] ???usz ? Dirichlet(?</S>
			<S sid="152" ssid="130">u) [unary symbol productions] ???bsz ? Dirichlet(?</S>
			<S sid="153" ssid="131">b) [binary symbol productions] ??For each child symbol s?</S>
			<S sid="154" ssid="132">S: ????Uszs?</S>
			<S sid="155" ssid="133">DP(?</S>
			<S sid="156" ssid="134">U ,?s?) [unary subsymbol prod.]</S>
			<S sid="157" ssid="135">??For each pair of children symbols (s?, s??)</S>
			<S sid="158" ssid="136">S ? S: ????Bszs?s??</S>
			<S sid="159" ssid="137">DP(?</S>
			<S sid="160" ssid="138">B ,?s??</S>
			<S sid="161" ssid="139">T s??)</S>
			<S sid="162" ssid="140">[binary subsymbol] For each node i in the parse tree: ?ti ? Multinomial(?Tsizi) [choose rule type] ?If ti = EMISSION: ??xi ? Multinomial(?Esizi) [emit terminal symbol] ?If ti = UNARY-PRODUCTION: ??sL(i) ? Multinomial(?</S>
			<S sid="163" ssid="141">u sizi) [generate child symbol] ??zL(i) ? Multinomial(?</S>
			<S sid="164" ssid="142">U sizisL(i)) [child subsymbol] ?If ti = BINARY-PRODUCTION: ??(sL(i), sR(i)) ? Mult(?sizi) [children symbols] ??(zL(i), zR(i)) ? Mult(?</S>
			<S sid="165" ssid="143">B sizisL(i)sR(i)) [subsymbols] 2.6 Variational inference.</S>
			<S sid="166" ssid="144">We present an inference algorithm for the HDP PCFG model described in Section 2.4, which can also be adapted to the HDP-PCFG-GR model with a bit more bookkeeping.</S>
			<S sid="167" ssid="145">Most previous inference algorithms for DP-based models involve sampling(Escobar and West, 1995; Teh et al, 2006).</S>
			<S sid="168" ssid="146">How ever, we chose to use variational inference (Bleiand Jordan, 2005), which provides a fast determin istic alternative to sampling, hence avoiding issues of diagnosing convergence and aggregating samples.Furthermore, our variational inference algorithm establishes a strong link with past work on PCFG refinement and induction, which has traditionally em ployed the EM algorithm.</S>
			<S sid="169" ssid="147">In EM, the E-step involves a dynamic program that exploits the Markov structure of the parse tree, and the M-step involves computing ratios based onexpected counts extracted from the E-step.</S>
			<S sid="170" ssid="148">Our vari ational algorithm resembles the EM algorithm in form, but the ratios in the M-step are replaced withweights that reflect the uncertainty in parameter es 692 ??Bz ?Tz ?Ez z ? z1 z2 z3 T Parameters Trees Figure 4: We approximate the true posterior p overparameters ? and latent parse trees z using a structured mean-field distribution q, in which the distri bution over parameters are completely factorized but the distribution over parse trees is unconstrained.</S>
			<S sid="171" ssid="149">timates.</S>
			<S sid="172" ssid="150">Because of this procedural similarity, our method is able to exploit the desirable properties of EM such as simplicity, modularity, and efficiency.</S>
			<S sid="173" ssid="151">2.7 Structured mean-field approximation.</S>
			<S sid="174" ssid="152">We denote parameters of the HDP-PCFG as ? =(?,?), where ? denotes the top-level symbol prob abilities and ? denotes the rule probabilities.</S>
			<S sid="175" ssid="153">The hidden variables of the model are the training parse trees z. We denote the observed sentences as x. The goal of Bayesian inference is to compute the posterior distribution p(?, z | x).</S>
			<S sid="176" ssid="154">The central idea behind variational inference is to approximate this intractable posterior with a tractable approximation.</S>
			<S sid="177" ssid="155">In particular, we want to find the best distribution q?</S>
			<S sid="178" ssid="156">as defined by q?</S>
			<S sid="179" ssid="157">def = argmin q?Q KL(q(?, z)||p(?, z | x)), (4) where Q is a tractable subset of distributions.</S>
			<S sid="180" ssid="158">We use a structured mean-field approximation, meaning that we only consider distributions that factorize as follows (Figure 4): Q def = { q(z)q(?)</S>
			<S sid="181" ssid="159">K? z=1 q(?Tz )q(?</S>
			<S sid="182" ssid="160">E z )q(?</S>
			<S sid="183" ssid="161">B z ) } .</S>
			<S sid="184" ssid="162">(5) We further restrict q(?Tz ), q(?</S>
			<S sid="185" ssid="163">E z ), q(?</S>
			<S sid="186" ssid="164">B z ) to be Dirichlet distributions, but allow q(z) to be any multinomial distribution.</S>
			<S sid="187" ssid="165">We constrain q(?)</S>
			<S sid="188" ssid="166">to be a degenerate distribution truncated at K; i.e., ?z = 0 for z &gt; K. While the posterior grammar does havean infinite number of symbols, the exponential decay of the DP prior ensures that most of the probability mass is contained in the first few symbols (Ishwaran and James, 2001).2 While our variational ap proximation q is truncated, the actual PCFG model is not.</S>
			<S sid="189" ssid="167">AsK increases, our approximation improves.</S>
			<S sid="190" ssid="168">2.8 Coordinate-wise ascent.</S>
			<S sid="191" ssid="169">The optimization problem defined by Equation (4)is intractable and nonconvex, but we can use a simple coordinate-ascent algorithm that iteratively op timizes each factor of q in turn while holding the others fixed.</S>
			<S sid="192" ssid="170">The algorithm turns out to be similar in form to EM for an ordinary PCFG: optimizing q(z) is the analogue of the E-step, and optimizing q(?)</S>
			<S sid="193" ssid="171">is the analogue of the M-step; however, optimizing q(?)</S>
			<S sid="194" ssid="172">has no analogue in EM.</S>
			<S sid="195" ssid="173">We summarize each of these updates below (see (Liang et al, 2007) for complete derivations).</S>
			<S sid="196" ssid="174">Parse trees q(z): The distribution over parse treesq(z) can be summarized by the expected suffi cient statistics (rule counts), which we denote as C(z ? zl zr) for binary productions and C(z ? x) for emissions.</S>
			<S sid="197" ssid="175">We can compute these expected counts using dynamic programming as in the E-step of EM.</S>
			<S sid="198" ssid="176">While the classical E-step uses the current ruleprobabilities ?, our mean-field approximation in volves an entire distribution q(?).</S>
			<S sid="199" ssid="177">Fortunately, wecan still handle this case by replacing each rule probability with a weight that summarizes the uncer tainty over the rule probability as represented by q. We define this weight in the sequel.</S>
			<S sid="200" ssid="178">It is a common perception that Bayesian inference is slow because one needs to compute integrals.</S>
			<S sid="201" ssid="179">Our mean-field inference algorithm is a counterexample:because we can represent uncertainty over rule prob abilities with single numbers, much of the existing PCFG machinery based on EM can be modularly imported into the Bayesian framework.</S>
			<S sid="202" ssid="180">Rule probabilities q(?): For an ordinary PCFG, the M-step simply involves taking ratios of expected2In particular, the variational distance between the stickbreaking distribution and the truncated version decreases expo nentially as the truncation level K increases.</S>
			<S sid="203" ssid="181">693 counts: ?Bz (zl, zr) = C(z ? zl zr) C(z ? ??)</S>
			<S sid="204" ssid="182">(6) For the variational HDP-PCFG, the optimal q(?)</S>
			<S sid="205" ssid="183">is given by the standard posterior update for Dirichlet distributions:3 q(?Bz ) = Dirichlet(?</S>
			<S sid="206" ssid="184">B z ;?</S>
			<S sid="207" ssid="185">B??T + ~C(z)), (7)where ~C(z) is the matrix of counts of rules with lefthand side z. These distributions can then be summa rized with multinomial weights which are the onlynecessary quantities for updating q(z) in the next it eration: WBz (zl, zr) def = expEq[log?Bz (zl, zr)] (8) = e?(C(z?zl zr)+?</S>
			<S sid="208" ssid="186">B?zl?zr ) e?(C(z???)+?B) , (9) where ?(?)</S>
			<S sid="209" ssid="187">is the digamma function.</S>
			<S sid="210" ssid="188">The emission parameters can be defined similarly.</S>
			<S sid="211" ssid="189">Inspection of Equations (6) and (9) reveals that the only difference between the maximum likelihood and the mean-fieldupdate is that the latter applies the exp(?(?)) func tion to the counts (Figure 5).</S>
			<S sid="212" ssid="190">When the truncation K is large, ?B?zl?zr is near 0 for most right-hand sides (zl, zr), so exp(?(?)) hasthe effect of downweighting counts.</S>
			<S sid="213" ssid="191">Since this sub traction affects large counts more than small counts,there is a rich-get-richer effect: rules that have al ready have large counts will be preferred.</S>
			<S sid="214" ssid="192">Specifically, consider a set of rules with the same left-hand side.</S>
			<S sid="215" ssid="193">The weights for all these rules only differ in the numerator (Equation (9)), so applying exp(?(?)) creates a local preference for right-hand sides with larger counts.</S>
			<S sid="216" ssid="194">Also note that the rule weights are not normalized; they always sum to at most one and are equal to one exactly when q(?)</S>
			<S sid="217" ssid="195">isdegenerate.</S>
			<S sid="218" ssid="196">This lack of normalization gives an extra degree of freedom not present in maximum like lihood estimation: it creates a global preference for left-hand sides that have larger total counts.</S>
			<S sid="219" ssid="197">Top-level symbol probabilities q(?): Recall that we restrict q(?)</S>
			<S sid="220" ssid="198">= ???(?), so optimizing ? is equivalent to finding a single best ??.</S>
			<S sid="221" ssid="199">Unlike q(?)</S>
			<S sid="222" ssid="200">3Because we have truncated the top-level symbol weights, the DP prior on ?Bz reduces to a finite Dirichlet distribution.</S>
			<S sid="223" ssid="201">0 0.5 1 1.5 2 0 0.5 1 1.5 2 x exp(?(x )) x Figure 5: The exp(?(?)) function, which is used in computing the multinomial weights for mean-fieldinference.</S>
			<S sid="224" ssid="202">It has the effect of reducing a larger frac tion of small counts than large counts.</S>
			<S sid="225" ssid="203">and q(z), there is no closed form expression forthe optimal ??, and the objective function (Equa tion (4)) is not convex in ??.</S>
			<S sid="226" ssid="204">Nonetheless, we canapply a standard gradient projection method (Bert sekas, 1999) to improve ??</S>
			<S sid="227" ssid="205">to a local maxima.</S>
			<S sid="228" ssid="206">The part of the objective function in Equation (4) that depends on ??</S>
			<S sid="229" ssid="207">is as follows: L(??)</S>
			<S sid="230" ssid="208">= logGEM(??;?)+ (10) K?</S>
			<S sid="231" ssid="209">z=1 Eq[logDirichlet(?Bz ;?</S>
			<S sid="232" ssid="210">B????T )]See Liang et al (2007) for the derivation of the gradient.</S>
			<S sid="233" ssid="211">In practice, this optimization has very little ef fect on performance.</S>
			<S sid="234" ssid="212">We suspect that this is because the objective function is dominated by p(x | z) andp(z | ?), while the contribution of p(?</S>
			<S sid="235" ssid="213">| ?) is mi nor.</S>
	</SECTION>
	<SECTION title="Experiments. " number="3">
			<S sid="236" ssid="1">We now present an empirical evaluation of the HDPPCFG(-GR) model and variational inference tech niques.</S>
			<S sid="237" ssid="2">We first give an illustrative example of theability of the HDP-PCFG to recover a known gram mar and then present the results of experiments on large-scale treebank parsing.</S>
			<S sid="238" ssid="3">3.1 Recovering a synthetic grammar.</S>
			<S sid="239" ssid="4">In this section, we show that the HDP-PCFG-GR can recover a simple grammar while a standard 694 S ? X1X1 | X2X2 | X3X3 | X4X4 X1 ? a1 | b1 | c1 | d1 X2 ? a2 | b2 | c2 | d2 X3 ? a3 | b3 | c3 | d3 X4 ? a4 | b4 | c4 | d4 S Xi Xi {ai, bi, ci, di} {ai, bi, ci, di} (a) (b) Figure 6: (a) A synthetic grammar with a uniform distribution over rules.</S>
			<S sid="240" ssid="5">(b) The grammar generates trees of the form shown on the right.PCFG fails to do so because it has no built-in con trol over grammar complexity.</S>
			<S sid="241" ssid="6">From the grammar in Figure 6, we generated 2000 trees.</S>
			<S sid="242" ssid="7">The two terminalsymbols always have the same subscript, but we col lapsed Xi to X in the training data.</S>
			<S sid="243" ssid="8">We trained the HDP-PCFG-GR, with truncation K = 20, for bothS and X for 100 iterations.</S>
			<S sid="244" ssid="9">We set al hyperparame ters to 1.</S>
			<S sid="245" ssid="10">Figure 7 shows that the HDP-PCFG-GR recoversthe original grammar, which contains only 4 sub symbols, leaving the other 16 subsymbols unused.</S>
			<S sid="246" ssid="11">The standard PCFG allocates all the subsymbols to fit the exact co-occurrence statistics of left and right terminals.Recall that a rule weight, as defined in Equation (9), is analogous to a rule probability for stan dard PCFGs.</S>
			<S sid="247" ssid="12">We say a rule is effective if its weight is at least 10?6 and its left hand-side has posterior is also at least 10?6.</S>
			<S sid="248" ssid="13">In general, rules with weightsmaller than 10?6 can be safely pruned without af fect parsing accuracy.</S>
			<S sid="249" ssid="14">The standard PCFG uses all 20 subsymbols of both S and X to explain the data, resulting in 8320 effective rules; in contrast, the HDP-PCFG uses only 4 subsymbols for X and 1 forS, resulting in only 68 effective rules.</S>
			<S sid="250" ssid="15">If the thresh old is relaxed from 10?6 to 10?3, then only 20 rules are effective, which corresponds exactly to the true grammar.</S>
			<S sid="251" ssid="16">3.2 Parsing the Penn Treebank.</S>
			<S sid="252" ssid="17">In this section, we show that our variational HDP PCFG can scale up to real-world data sets.</S>
			<S sid="253" ssid="18">We ranexperiments on the Wall Street Journal (WSJ) por tion of the Penn Treebank.</S>
			<S sid="254" ssid="19">We trained on sections 2?21, used section 24 for tuning hyperparameters, and tested on section 22.</S>
			<S sid="255" ssid="20">We binarize the trees in the treebank as follows:for each non-terminal node with symbol X , we in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 0.25 subsymbol pos ter ior 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 0.25 subsymbol pos ter ior standard PCFG HDP-PCFG Figure 7: The posteriors over the subsymbols of thestandard PCFG is roughly uniform, whereas the pos teriors of the HDP-PCFG is concentrated on four subsymbols, which is the true number of symbols in the grammar.</S>
			<S sid="256" ssid="21">troduce a right-branching cascade of new nodes with symbol X . The end result is that each node has at most two children.</S>
			<S sid="257" ssid="22">To cope with unknown words, we replace any word appearing fewer than 5 timesin the training set with one of 50 unknown word to kens derived from 10 word-form features.</S>
			<S sid="258" ssid="23">Our goal is to learn a refined grammar, where eachsymbol in the training set is split into K subsym bols.</S>
			<S sid="259" ssid="24">We compare an ordinary PCFG estimated with maximum likelihood (Matsuzaki et al, 2005) andthe HDP-PCFG estimated using the variational in ference algorithm described in Section 2.6.To parse new sentences with a grammar, we com pute the posterior distribution over rules at each spanand extract the tree with the maximum expected cor rect number of rules (Petrov and Klein, 2007).</S>
			<S sid="260" ssid="25">3.2.1 HyperparametersThere are six hyperparameters in the HDP-PCFG GR model, which we set in the following manner:?</S>
			<S sid="261" ssid="26">= 1, ?T = 1 (uniform distribution over unar ies versus binaries), ?E = 1 (uniform distribution over terminal words), ?u(s) = ?b(s) = 1N(s) , whereN(s) is the number of different unary (binary) righthand sides of rules with left-hand side s in the treebank grammar.</S>
			<S sid="262" ssid="27">The two most important hyperpa rameters are ?U and ?B , which govern the sparsity of the right-hand side for unary and binary rules.</S>
			<S sid="263" ssid="28">We set ?U = ?B although more performance could probably be gained by tuning these individually.</S>
			<S sid="264" ssid="29">It turns out that there is not a single ?B that works for all truncation levels, as shown in Table 1.</S>
			<S sid="265" ssid="30">If the top-level distribution ? is uniform, the value of ?B corresponding to a uniform prior over pairs ofchildren subsymbols is K2.</S>
			<S sid="266" ssid="31">Interestingly, the opti mal ?B appears to be superlinear but subquadratic 695 truncation K 2 4 8 12 16 20 best ?B 16 12 20 28 48 80 uniform ?B 4 16 64 144 256 400 Table 1: For each truncation level, we report the ?B that yielded the highest F1 score on the development set.</S>
			<S sid="267" ssid="32">K PCFG PCFG (smoothed) HDP-PCFG F1 Size F1 Size F1 Size 1 60.47 2558 60.36 2597 60.5 2557 2 69.53 3788 69.38 4614 71.08 4264 8 74.32 4262 79.26 120598 79.15 50629 12 70.99 7297 78.8 160403 78.94 86386 16 66.99 19616 79.2 261444 78.24 131377 20 64.44 27593 79.27 369699 77.81 202767 Table 2: Shows development F1 and grammar sizes (the number of effective rules) as we increase the truncation K. in K. We used these values of ?B in the following experiments.</S>
			<S sid="268" ssid="33">3.2.2 Results The regime in which Bayesian inference is most important is when training data is scarce relative tothe complexity of the model.</S>
			<S sid="269" ssid="34">We train on just sec tion 2 of the Penn Treebank.</S>
			<S sid="270" ssid="35">Table 2 shows how the HDP-PCFG-GR can produce compact grammars that guard against overfitting.</S>
			<S sid="271" ssid="36">Without smoothing,ordinary PCFGs trained using EM improve as K in creases but start to overfit around K = 4.</S>
			<S sid="272" ssid="37">Simple add-1.01 smoothing prevents overfitting but at thecost of a sharp increase in grammar sizes.</S>
			<S sid="273" ssid="38">The HDP PCFG obtains comparable performance with a much smaller number of rules.We also trained on sections 2?21 to demon strate that our methods can scale up and achievebroadly comparable results to existing state-of-the art parsers.</S>
			<S sid="274" ssid="39">When using a truncation level of K = 16, the standard PCFG with smoothing obtains an F1 score of 88.36 using 706157 effective rules whilethe HDP-PCFG-GR obtains an F1 score of 87.08 us ing 428375 effective rules.</S>
			<S sid="275" ssid="40">We expect to see greaterbenefits from the HDP-PCFG with a larger trunca tion level.</S>
	</SECTION>
	<SECTION title="Related work. " number="4">
			<S sid="276" ssid="1">The question of how to select the appropriate gram mar complexity has been studied in earlier work.It is well known that more complex models nec essarily have higher likelihood and thus a penaltymust be imposed for more complex grammars.</S>
			<S sid="277" ssid="2">Examples of such penalized likelihood procedures in clude Stolcke and Omohundro (1994), which used an asymptotic Bayesian model selection criterion and Petrov et al (2006), which used a split-merge algorithm which procedurally determines when to switch between grammars of various complexities.</S>
			<S sid="278" ssid="3">These techniques are model selection techniquesthat use heuristics to choose among competing sta tistical models; in contrast, the HDP-PCFG relies on the Bayesian formalism to provide implicit control over model complexity within the framework of a single probabilistic model.Johnson et al (2006) also explored nonparametric grammars, but they do not give an inference algorithm for recursive grammars, e.g., grammars in cluding rules of the form A ? BC and B ? DA.</S>
			<S sid="279" ssid="4">Recursion is a crucial aspect of PCFGs and our inference algorithm does handle it.</S>
			<S sid="280" ssid="5">Finkel et al(2007) independently developed another nonpara metric model of grammars.</S>
			<S sid="281" ssid="6">Though their model is also based on hierarchical Dirichlet processes and issimilar to ours, they present a different inference al gorithm which is based on sampling.</S>
			<S sid="282" ssid="7">Kurihara and Sato (2004) and Kurihara and Sato (2006) applied variational inference to PCFGs.</S>
			<S sid="283" ssid="8">Their algorithm issimilar to ours, but they did not consider nonpara metric models.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="5">
			<S sid="284" ssid="1">We have presented the HDP-PCFG, a nonparametric Bayesian model for PCFGs, along with an efficient variational inference algorithm.</S>
			<S sid="285" ssid="2">While our primarycontribution is the elucidation of the model and algorithm, we have also explored some important empirical properties of the HDP-PCFG and also demon strated the potential of variational HDP-PCFGs on a full-scale parsing task.</S>
			<S sid="286" ssid="3">696</S>
	</SECTION>
</PAPER>
