Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics
Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.
We are the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.
We propose ROUGE, a semi automatic approach,  which is primarily based on n gram co-occurrence between automatic and human summaries.
