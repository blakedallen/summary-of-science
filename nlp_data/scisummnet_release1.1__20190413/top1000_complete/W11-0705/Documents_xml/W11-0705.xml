<PAPER>
  <S sid="0">Sentiment Analysis of Twitter Data</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We examine sentiment analysis on Twitter data.</S>
    <S sid="2" ssid="2">The contributions of this paper are: (1) We introduce POS-specific prior polarity features.</S>
    <S sid="3" ssid="3">(2) We explore the use of a tree kernel to obviate the need for tedious feature engineering.</S>
    <S sid="4" ssid="4">The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Microblogging websites have evolved to become a source of varied kind of information.</S>
    <S sid="6" ssid="2">This is due to nature of microblogs on which people post real time messages about their opinions on a variety of topics, discuss current issues, complain, and express positive sentiment for products they use in daily life.</S>
    <S sid="7" ssid="3">In fact, companies manufacturing such products have started to poll these microblogs to get a sense of general sentiment for their product.</S>
    <S sid="8" ssid="4">Many times these companies study user reactions and reply to users on microblogs.</S>
    <S sid="9" ssid="5">One challenge is to build technology to detect and summarize an overall sentiment.</S>
    <S sid="10" ssid="6">In this paper, we look at one such popular microblog called Twitter and build models for classifying &#8220;tweets&#8221; into positive, negative and neutral sentiment.</S>
    <S sid="11" ssid="7">We build models for two classification tasks: a binary task of classifying sentiment into positive and negative classes and a 3-way task of classifying sentiment into positive, negative and neutral classes.</S>
    <S sid="12" ssid="8">We experiment with three types of models: unigram model, a feature based model and a tree kernel based model.</S>
    <S sid="13" ssid="9">For the feature based model we use some of the features proposed in past literature and propose new features.</S>
    <S sid="14" ssid="10">For the tree kernel based model we design a new tree representation for tweets.</S>
    <S sid="15" ssid="11">We use a unigram model, previously shown to work well for sentiment analysis for Twitter data, as our baseline.</S>
    <S sid="16" ssid="12">Our experiments show that a unigram model is indeed a hard baseline achieving over 20% over the chance baseline for both classification tasks.</S>
    <S sid="17" ssid="13">Our feature based model that uses only 100 features achieves similar accuracy as the unigram model that uses over 10,000 features.</S>
    <S sid="18" ssid="14">Our tree kernel based model outperforms both these models by a significant margin.</S>
    <S sid="19" ssid="15">We also experiment with a combination of models: combining unigrams with our features and combining our features with the tree kernel.</S>
    <S sid="20" ssid="16">Both these combinations outperform the unigram baseline by over 4% for both classification tasks.</S>
    <S sid="21" ssid="17">In this paper, we present extensive feature analysis of the 100 features we propose.</S>
    <S sid="22" ssid="18">Our experiments show that features that have to do with Twitter-specific features (emoticons, hashtags etc.) add value to the classifier but only marginally.</S>
    <S sid="23" ssid="19">Features that combine prior polarity of words with their parts-of-speech tags are most important for both the classification tasks.</S>
    <S sid="24" ssid="20">Thus, we see that standard natural language processing tools are useful even in a genre which is quite different from the genre on which they were trained (newswire).</S>
    <S sid="25" ssid="21">Furthermore, we also show that the tree kernel model performs roughly as well as the best feature based models, even though it does not require detailed feature engineering.</S>
    <S sid="26" ssid="22">We use manually annotated Twitter data for our experiments.</S>
    <S sid="27" ssid="23">One advantage of this data, over previously used data-sets, is that the tweets are collected in a streaming fashion and therefore represent a true sample of actual tweets in terms of language use and content.</S>
    <S sid="28" ssid="24">Our new data set is available to other researchers.</S>
    <S sid="29" ssid="25">In this paper we also introduce two resources which are available (contact the first author): 1) a hand annotated dictionary for emoticons that maps emoticons to their polarity and 2) an acronym dictionary collected from the web with English translations of over 5000 frequently used acronyms.</S>
    <S sid="30" ssid="26">The rest of the paper is organized as follows.</S>
    <S sid="31" ssid="27">In section 2, we discuss classification tasks like sentiment analysis on micro-blog data.</S>
    <S sid="32" ssid="28">In section 3, we give details about the data.</S>
    <S sid="33" ssid="29">In section 4 we discuss our pre-processing technique and additional resources.</S>
    <S sid="34" ssid="30">In section 5 we present our prior polarity scoring scheme.</S>
    <S sid="35" ssid="31">In section 6 we present the design of our tree kernel.</S>
    <S sid="36" ssid="32">In section 7 we give details of our feature based approach.</S>
    <S sid="37" ssid="33">In section 8 we present our experiments and discuss the results.</S>
    <S sid="38" ssid="34">We conclude and give future directions of research in section 9.</S>
  </SECTION>
  <SECTION title="2 Literature Survey" number="2">
    <S sid="39" ssid="1">Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity.</S>
    <S sid="40" ssid="2">Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009).</S>
    <S sid="41" ssid="3">Microblog data like Twitter, on which users post real time reactions to and opinions about &#8220;everything&#8221;, poses newer and different challenges.</S>
    <S sid="42" ssid="4">Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010).</S>
    <S sid="43" ssid="5">Go et al. (2009) use distant learning to acquire sentiment data.</S>
    <S sid="44" ssid="6">They use tweets ending in positive emoticons like &#8220;:)&#8221; &#8220;:-)&#8221; as positive and negative emoticons like &#8220;:(&#8221; &#8220;:-(&#8221; as negative.</S>
    <S sid="45" ssid="7">They build models using Naive Bayes, MaxEnt and Support Vector Machines (SVM), and they report SVM outperforms other classifiers.</S>
    <S sid="46" ssid="8">In terms of feature space, they try a Unigram, Bigram model in conjunction with parts-of-speech (POS) features.</S>
    <S sid="47" ssid="9">They note that the unigram model outperforms all other models.</S>
    <S sid="48" ssid="10">Specifically, bigrams and POS features do not help.</S>
    <S sid="49" ssid="11">Pak and Paroubek (2010) collect data following a similar distant learning paradigm.</S>
    <S sid="50" ssid="12">They perform a different classification task though: subjective versus objective.</S>
    <S sid="51" ssid="13">For subjective data they collect the tweets ending with emoticons in the same manner as Go et al. (2009).</S>
    <S sid="52" ssid="14">For objective data they crawl twitter accounts of popular newspapers like &#8220;New York Times&#8221;, &#8220;Washington Posts&#8221; etc.</S>
    <S sid="53" ssid="15">They report that POS and bigrams both help (contrary to results presented by Go et al. (2009)).</S>
    <S sid="54" ssid="16">Both these approaches, however, are primarily based on ngram models.</S>
    <S sid="55" ssid="17">Moreover, the data they use for training and testing is collected by search queries and is therefore biased.</S>
    <S sid="56" ssid="18">In contrast, we present features that achieve a significant gain over a unigram baseline.</S>
    <S sid="57" ssid="19">In addition we explore a different method of data representation and report significant improvement over the unigram models.</S>
    <S sid="58" ssid="20">Another contribution of this paper is that we report results on manually annotated data that does not suffer from any known biases.</S>
    <S sid="59" ssid="21">Our data is a random sample of streaming tweets unlike data collected by using specific queries.</S>
    <S sid="60" ssid="22">The size of our hand-labeled data allows us to perform crossvalidation experiments and check for the variance in performance of the classifier across folds.</S>
    <S sid="61" ssid="23">Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010).</S>
    <S sid="62" ssid="24">They use polarity predictions from three websites as noisy labels to train a model and use 1000 manually labeled tweets for tuning and another 1000 manually labeled tweets for testing.</S>
    <S sid="63" ssid="25">They however do not mention how they collect their test data.</S>
    <S sid="64" ssid="26">They propose the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS of words.</S>
    <S sid="65" ssid="27">We extend their approach by using real valued prior polarity, and by combining prior polarity with POS.</S>
    <S sid="66" ssid="28">Our results show that the features that enhance the performance of our classifiers the most are features that combine prior polarity of words with their parts of speech.</S>
    <S sid="67" ssid="29">The tweet syntax features help but only marginally.</S>
    <S sid="68" ssid="30">Gamon (2004) perform sentiment analysis on feeadback data from Global Support Services survey.</S>
    <S sid="69" ssid="31">One aim of their paper is to analyze the role of linguistic features like POS tags.</S>
    <S sid="70" ssid="32">They perform extensive feature analysis and feature selection and demonstrate that abstract linguistic analysis features contributes to the classifier accuracy.</S>
    <S sid="71" ssid="33">In this paper we perform extensive feature analysis and show that the use of only 100 abstract linguistic features performs as well as a hard unigram baseline.</S>
  </SECTION>
  <SECTION title="3 Data Description" number="3">
    <S sid="72" ssid="1">Twitter is a social networking and microblogging service that allows users to post real time messages, called tweets.</S>
    <S sid="73" ssid="2">Tweets are short messages, restricted to 140 characters in length.</S>
    <S sid="74" ssid="3">Due to the nature of this microblogging service (quick and short messages), people use acronyms, make spelling mistakes, use emoticons and other characters that express special meanings.</S>
    <S sid="75" ssid="4">Following is a brief terminology associated with tweets.</S>
    <S sid="76" ssid="5">Emoticons: These are facial expressions pictorially represented using punctuation and letters; they express the user&#8217;s mood.</S>
    <S sid="77" ssid="6">Target: Users of Twitter use the &#8220;@&#8221; symbol to refer to other users on the microblog.</S>
    <S sid="78" ssid="7">Referring to other users in this manner automatically alerts them.</S>
    <S sid="79" ssid="8">Hashtags: Users usually use hashtags to mark topics.</S>
    <S sid="80" ssid="9">This is primarily done to increase the visibility of their tweets.</S>
    <S sid="81" ssid="10">We acquire 11,875 manually annotated Twitter data (tweets) from a commercial source.</S>
    <S sid="82" ssid="11">They have made part of their data publicly available.</S>
    <S sid="83" ssid="12">For information on how to obtain the data, see Acknowledgments section at the end of the paper.</S>
    <S sid="84" ssid="13">They collected the data by archiving the real-time stream.</S>
    <S sid="85" ssid="14">No language, location or any other kind of restriction was made during the streaming process.</S>
    <S sid="86" ssid="15">In fact, their collection consists of tweets in foreign languages.</S>
    <S sid="87" ssid="16">They use Google translate to convert it into English before the annotation process.</S>
    <S sid="88" ssid="17">Each tweet is labeled by a human annotator as positive, negative, neutral or junk.</S>
    <S sid="89" ssid="18">The &#8220;junk&#8221; label means that the tweet cannot be understood by a human annotator.</S>
    <S sid="90" ssid="19">A manual analysis of a random sample of tweets labeled as &#8220;junk&#8221; suggested that many of these tweets were those that were not translated well using Google translate.</S>
    <S sid="91" ssid="20">We eliminate the tweets with junk label for experiments.</S>
    <S sid="92" ssid="21">This leaves us with an unbalanced sample of 8,753 tweets.</S>
    <S sid="93" ssid="22">We use stratified sampling to get a balanced data-set of 5127 tweets (1709 tweets each from classes positive, negative and neutral).</S>
  </SECTION>
  <SECTION title="4 Resources and Pre-processing of data" number="4">
    <S sid="94" ssid="1">In this paper we introduce two new resources for pre-processing twitter data: 1) an emoticon dictionary and 2) an acronym dictionary.</S>
    <S sid="95" ssid="2">We prepare the emoticon dictionary by labeling 170 emoticons listed on Wikipedia1 with their emotional state.</S>
    <S sid="96" ssid="3">For example, &#8220;:)&#8221; is labeled as positive whereas &#8220;:=(&#8221; is labeled as negative.</S>
    <S sid="97" ssid="4">We assign each emoticon a label from the following set of labels: Extremely-positive, Extremely-negative, Positive, Negative, and Neutral.</S>
    <S sid="98" ssid="5">We compile an acronym dictionary from an online resource.2 The dictionary has translations for 5,184 acronyms.</S>
    <S sid="99" ssid="6">For example, lol is translated to laughing out loud.</S>
    <S sid="100" ssid="7">We pre-process all the tweets as follows: a) replace all the emoticons with a their sentiment polarity by looking up the emoticon dictionary, b) replace all URLs with a tag ||U||, c) replace targets (e.g.</S>
    <S sid="101" ssid="8">&#8220;@John&#8221;) with tag ||T||, d) replace all negations (e.g. not, no, never, n&#8217;t, cannot) by tag &#8220;NOT&#8221;, and e) replace a sequence of repeated characters by three characters, for example, convert coooooooool to coool.</S>
    <S sid="102" ssid="9">We do not replace the sequence by only two characters since we want to differentiate between the regular usage and emphasized usage of the word.</S>
    <S sid="103" ssid="10">Acronym English expansion gr8, gr8t great lol laughing out loud rotf rolling on the floor bff best friend forever We present some preliminary statistics about the data in Table 3.</S>
    <S sid="104" ssid="11">We use the Stanford tokenizer (Klein and Manning, 2003) to tokenize the tweets.</S>
    <S sid="105" ssid="12">We use a stop word dictionary3 to identify stop words.</S>
    <S sid="106" ssid="13">All the other words which are found in WordNet (Fellbaum, 1998) are counted as English words.</S>
    <S sid="107" ssid="14">We use the standard tagset defined by the Penn Treebank for identifying punctuation.</S>
    <S sid="108" ssid="15">We record the occurrence of three standard twitter tags: emoticons, URLs and targets.</S>
    <S sid="109" ssid="16">The remaining tokens are either non English words (like coool, zzz etc.) or other symbols.</S>
    <S sid="110" ssid="17">In Table 3 we see that 38.3% of the tokens are stop words, 30.1% of the tokens are found in WordNet and 1.2% tokens are negation words.</S>
    <S sid="111" ssid="18">11.8% of all the tokens are punctuation marks excluding exclamation marks which make up for 2.8% of all tokens.</S>
    <S sid="112" ssid="19">In total, 84.1% of all tokens are tokens that we expect to see in a typical English language text.</S>
    <S sid="113" ssid="20">There are 4.2% tags that are specific to Twitter which include emoticons, target, hastags and &#8220;RT&#8221; (retweet).</S>
    <S sid="114" ssid="21">The remaining 11.7% tokens are either words that cannot be found in WordNet (like Zzzzz, kewl) or special symbols which do not fall in the category of Twitter tags.</S>
  </SECTION>
  <SECTION title="5 Prior polarity scoring" number="5">
    <S sid="115" ssid="1">A number of our features are based on prior polarity of words.</S>
    <S sid="116" ssid="2">For obtaining the prior polarity of words, we take motivation from work by Agarwal et al. (2009).</S>
    <S sid="117" ssid="3">We use Dictionary of Affect in Language (DAL) (Whissel, 1989) and extend it using WordNet.</S>
    <S sid="118" ssid="4">This dictionary of about 8000 English language words assigns every word a pleasantness score (E R) between 1 (Negative) - 3 (Positive).</S>
    <S sid="119" ssid="5">We first normalize the scores by diving each score my the scale (which is equal to 3).</S>
    <S sid="120" ssid="6">We consider words with polarity less than 0.5 as negative, higher than 0.8 as positive and the rest as neutral.</S>
    <S sid="121" ssid="7">If a word is not directly found in the dictionary, we retrieve all synonyms from Wordnet.</S>
    <S sid="122" ssid="8">We then look for each of the synonyms in DAL.</S>
    <S sid="123" ssid="9">If any synonym is found in DAL, we assign the original word the same pleasantness score as its synonym.</S>
    <S sid="124" ssid="10">If none of the synonyms is present in DAL, the word is not associated with any prior polarity.</S>
    <S sid="125" ssid="11">For the given data we directly found prior polarity of 81.1% of the words.</S>
    <S sid="126" ssid="12">We find polarity of other 7.8% of the words by using WordNet.</S>
    <S sid="127" ssid="13">So we find prior polarity of about 88.9% of English language words.</S>
  </SECTION>
  <SECTION title="6 Design of Tree Kernel" number="6">
    <S sid="128" ssid="1">We design a tree representation of tweets to combine many categories of features in one succinct convenient representation.</S>
    <S sid="129" ssid="2">For calculating the similarity between two trees we use a Partial Tree (PT) kernel first proposed by Moschitti (2006).</S>
    <S sid="130" ssid="3">A PT kernel calculates the similarity between two trees by comparing all possible sub-trees.</S>
    <S sid="131" ssid="4">This tree kernel is an instance of a general class of convolution kernels.</S>
    <S sid="132" ssid="5">Convolution Kernels, first introduced by Haussler (1999), can be used to compare abstract objects, like strings, instead of feature vectors.</S>
    <S sid="133" ssid="6">This is because these kernels involve a recursive calculation over the &#8220;parts&#8221; of abstract object.</S>
    <S sid="134" ssid="7">This calculation is made computationally efficient by using Dynamic Programming techniques.</S>
    <S sid="135" ssid="8">By considering all possible combinations of fragments, tree kernels capture any possible correlation between features and categories of features.</S>
    <S sid="136" ssid="9">Figure 1 shows an example of the tree structure we design.</S>
    <S sid="137" ssid="10">This tree is for a synthesized tweet: @Fernando this isn&#8217;t a great day for playing the HARP!</S>
    <S sid="138" ssid="11">:).</S>
    <S sid="139" ssid="12">We use the following procedure to convert a tweet into a tree representation: Initialize the main tree to be &#8220;ROOT&#8221;.</S>
    <S sid="140" ssid="13">Then tokenize each tweet and for each token: a) if the token is a target, emoticon, exclamation mark, other punctuation mark, or a negation word, add a leaf node to the &#8220;ROOT&#8221; with the corresponding tag.</S>
    <S sid="141" ssid="14">For example, in the tree in Figure 1 we add tag ||T  ||(target) for &#8220;@Fernando&#8221;, add tag &#8220;NOT&#8221; for the token &#8220;n&#8217;t&#8221;, add tag &#8220;EXC&#8221; for the exclamation mark at the end of the sentence and add ||P ||for the emoticon representing positive mood. b) if the token is a stop word, we simply add the subtree &#8220; (STOP (&#8216;stop-word&#8217;))&#8221; to &#8220;ROOT&#8221;.</S>
    <S sid="142" ssid="15">For instance, we add a subtree corresponding to each of the stop words: this, is, and for. c) if the token is an English language word, we map the word to its part-of-speech tag, calculate the prior polarity of the word using the procedure described in section 5 and add the subtree (EW (&#8216;POS&#8217; &#8216;word&#8217; &#8216;prior polarity&#8217;)) to the &#8220;ROOT&#8221;.</S>
    <S sid="143" ssid="16">For example, we add the subtree (EW (JJ great POS)) for the word great.</S>
    <S sid="144" ssid="17">&#8220;EW&#8221; refers to English word. d) For any other token &lt;token&gt; we add subtree &#8220;(NE (&lt;token&gt;))&#8221; to the &#8220;ROOT&#8221;.</S>
    <S sid="145" ssid="18">&#8220;NE&#8221; refers to non-English.</S>
    <S sid="146" ssid="19">The PT tree kernel creates all possible subtrees and compares them to each other.</S>
    <S sid="147" ssid="20">These subtrees include subtrees in which non-adjacent branches become adjacent by excising other branches, though order is preserved.</S>
    <S sid="148" ssid="21">In Figure 1, we show some of the tree fragments that the PT kernel will attempt to compare with tree fragments from other trees.</S>
    <S sid="149" ssid="22">For example, given the tree (EW (JJ) (great) (POS)), the PT kernel will use (EW (JJ) (great) (POS)), (EW (great) (POS)), (EW (JJ) (POS)), (EW (JJ) (great)), (EW (JJ)), (EW (great)), (EW (POS)), (EW), (JJ), (great), and (POS).</S>
    <S sid="150" ssid="23">This means that the PT tree kernel attempts to use full information, and also abstracts away from specific information (such as the lexical item).</S>
    <S sid="151" ssid="24">In this manner, it is not necessary to create by hand features at all levels of abstraction.</S>
  </SECTION>
  <SECTION title="7 Our features" number="7">
    <S sid="152" ssid="1">We propose a set of features listed in Table 4 for our experiments.</S>
    <S sid="153" ssid="2">These are a total of 50 type of features.</S>
    <S sid="154" ssid="3">We calculate these features for the whole tweet and for the last one-third of the tweet.</S>
    <S sid="155" ssid="4">In total we get 100 additional features.</S>
    <S sid="156" ssid="5">We refer to these features as Senti-features throughout the paper.</S>
    <S sid="157" ssid="6">Our features can be divided into three broad categories: ones that are primarily counts of various features and therefore the value of the feature is a natural number E N. Second, features whose value is a real number E R. These are primarily features that capture the score retrieved from DAL.</S>
    <S sid="158" ssid="7">Thirdly, features whose values are boolean E B.</S>
    <S sid="159" ssid="8">These are bag of words, presence of exclamation marks and capitalized text.</S>
    <S sid="160" ssid="9">Each of these broad categories is divided into two subcategories: Polar features and Non-polar features.</S>
    <S sid="161" ssid="10">We refer to a feature as polar if we calculate its prior polarity either by looking it up in DAL (extended through WordNet) or in the emoticon dictionary.</S>
    <S sid="162" ssid="11">All other features which are not associated with any prior polarity fall in the Nonpolar category.</S>
    <S sid="163" ssid="12">Each of Polar and Non-polar features is further subdivided into two categories: POS and Other.</S>
    <S sid="164" ssid="13">POS refers to features that capture statistics about parts-of-speech of words and Other refers to all other types of features.</S>
    <S sid="165" ssid="14">In reference to Table 4, row f1 belongs to the category Polar POS and refers to the count of number of positive and negative parts-of-speech (POS) in a tweet, rows f2, f3, f4 belongs to the category Polar Other and refers to count of number of negation words, count of words that have positive and negative prior polarity, count of emoticons per polarity type, count of hashtags, capitalized words and words with exclamation marks associated with words that have prior polarity, row f5 belongs to the category Non-Polar POS and refers to counts of different parts-of-speech tags, rows f6, f7 belong to the category Non-Polar Other and refer to count of number of slangs, latin alphabets, and other words without polarity.</S>
    <S sid="166" ssid="15">It also relates to special terms such as the number of hashtags, URLs, targets and newlines.</S>
    <S sid="167" ssid="16">Row f8 belongs to the category Polar POS and captures the summation of prior polarity scores of words with POS of JJ, RB, VB and NN.</S>
    <S sid="168" ssid="17">Similarly, row f9 belongs to the category Polar Other and calculates the summation of prior polarity scores of all words, row f10 refers to the category Non-Polar Other and calculates the percentage of tweet that is capitalized.</S>
    <S sid="169" ssid="18">Finally, row f11 belongs to the category NonPolar Other and refers to presence of exclamation and presence of capitalized words as features.</S>
  </SECTION>
  <SECTION title="8 Experiments and Results" number="8">
    <S sid="170" ssid="1">In this section, we present experiments and results for two classification tasks: 1) Positive versus Negative and 2) Positive versus Negative versus Neutral.</S>
    <S sid="171" ssid="2">For each of the classification tasks we present three models, as well as results for two combinations of these models: For the unigram plus Senti-features model, we present feature analysis to gain insight about what kinds of features are adding most value to the model.</S>
    <S sid="172" ssid="3">We also present learning curves for each of the models and compare learning abilities of models when provided limited data.</S>
    <S sid="173" ssid="4">Experimental-Set-up: For all our experiments we use Support Vector Machines (SVM) and report averaged 5-fold cross-validation test results.</S>
    <S sid="174" ssid="5">We tune the C parameter for SVM using an embedded 5-fold cross-validation on the training data of each fold, i.e. for each fold, we first run 5-fold cross-validation only on the training data of that fold for different values of C. We pick the setting that yields the best cross-validation error and use that C for determining test error for that fold.</S>
    <S sid="175" ssid="6">As usual, the reported accuracies is the average over the five folds.</S>
    <S sid="176" ssid="7">This is a binary classification task with two classes of sentiment polarity: positive and negative.</S>
    <S sid="177" ssid="8">We use a balanced data-set of 1709 instances for each class and therefore the chance baseline is 50%.</S>
    <S sid="178" ssid="9">We use a unigram model as our baseline.</S>
    <S sid="179" ssid="10">Researchers report state-of-the-art performance for sentiment analysis on Twitter data using a unigram model (Go et al., 2009; Pak and Paroubek, 2010).</S>
    <S sid="180" ssid="11">Table 5 compares the performance of three models: unigram model, feature based model using only 100 Senti-features, and the tree kernel model.</S>
    <S sid="181" ssid="12">We report mean and standard deviation of 5-fold test accuracy.</S>
    <S sid="182" ssid="13">We observe that the tree kernels outperform the unigram and the Senti-features by 2.58% and 2.66% respectively.</S>
    <S sid="183" ssid="14">The 100 Senti-features described in Table 4 performs as well as the unigram model that uses about 10,000 features.</S>
    <S sid="184" ssid="15">We also experiment with combination of models.</S>
    <S sid="185" ssid="16">Combining unigrams with Senti-features outperforms the combination of kernels with Senti-features by 0.78%.</S>
    <S sid="186" ssid="17">This is our best performing system for the positive versus negative task, gaining about 4.04% absolute gain over a hard unigram baseline.</S>
    <S sid="187" ssid="18">Table 6 presents classifier accuracy and F1measure when features are added incrementally.</S>
    <S sid="188" ssid="19">We start with our baseline unigram model and subsequently add various sets of features.</S>
    <S sid="189" ssid="20">First, we add all non-polar features (rows f5, f6, f7, f10, f11 in Table 4) and observe no improvement in the performance.</S>
    <S sid="190" ssid="21">Next, we add all part-of-speech based features (rows f1, f8) and observe a gain of 3.49% over the unigram baseline.</S>
    <S sid="191" ssid="22">We see an additional increase in accuracy by 0.55% when we add other prior polarity features (rows f2, f3, f4, f9 in Table 4).</S>
    <S sid="192" ssid="23">From these experiments we conclude that the most important features in Senti-features are those that involve prior polarity of parts-of-speech.</S>
    <S sid="193" ssid="24">All other features play a marginal role in achieving the best performing system.</S>
    <S sid="194" ssid="25">In fact, we experimented by using unigrams with only prior polarity POS features and achieved a performance of 75.1%, which is only slightly lower than using all Senti-features.</S>
    <S sid="195" ssid="26">In terms of unigram features, we use Information Gain as the attribute evaluation metric to do feature selection.</S>
    <S sid="196" ssid="27">In Table 7 we present a list of unigrams that consistently appear as top 15 unigram features across all folds.</S>
    <S sid="197" ssid="28">Words having positive or negative prior polarity top the list.</S>
    <S sid="198" ssid="29">Emoticons also appear as important unigrams.</S>
    <S sid="199" ssid="30">Surprisingly though, the word for appeared as a top feature.</S>
    <S sid="200" ssid="31">A preliminary analysis revealed that the word for appears as frequently in positive tweets as it does in negative tweets.</S>
    <S sid="201" ssid="32">However, tweets containing phrases like for you and for me tend to be positive even in the absence of any other explicit prior polarity words.</S>
    <S sid="202" ssid="33">Owing to previous research, the URL appearing as a top feature is less surprising because Go et al. (2009) report that tweets containing URLs tend to be positive.</S>
    <S sid="203" ssid="34">The learning curve for the 2-way classification task is in Figure 2.</S>
    <S sid="204" ssid="35">The curve shows that when limited data is used the advantages in the performance of our best performing systems is even more pronounced.</S>
    <S sid="205" ssid="36">This implies that with limited amount of training data, simply using unigrams has a critical disadvantage, while both tree kernel and unigram model with our features exhibit promising performance.</S>
    <S sid="206" ssid="37">This is a 3-way classification task with classes of sentiment polarity: positive, negative and neutral.</S>
    <S sid="207" ssid="38">We use a balanced data-set of 1709 instances for each class and therefore the chance baseline is 33.33%.</S>
    <S sid="208" ssid="39">For this task the unigram model achieves a gain of 23.25% over chance baseline.</S>
    <S sid="209" ssid="40">Table 8 compares the performance of our three models.</S>
    <S sid="210" ssid="41">We report mean and standard deviation of 5-fold test accuracy.</S>
    <S sid="211" ssid="42">We observe that the tree kernels outperform the unigram and the Senti-features model by 4.02% and 4.29% absolute, respectively.</S>
    <S sid="212" ssid="43">We note that this difference is much more pronounced comparing to the two way classification task.</S>
    <S sid="213" ssid="44">Once again, our 100 Senti-features perform almost as well as the unigram baseline which has about 13,000 features.</S>
    <S sid="214" ssid="45">We also experiment with the combination of models.</S>
    <S sid="215" ssid="46">For this classification task the combination of tree kernel with Senti-features outperforms the combination of unigrams with Senti-features by a small margin.</S>
    <S sid="216" ssid="47">This is our best performing system for the 3-way classification task, gaining 4.25% over the unigram baseline.</S>
    <S sid="217" ssid="48">The learning curve for the 3-way classification task is similar to the curve of the 2-way classification task, and we omit it.</S>
    <S sid="218" ssid="49">Table 9 presents classifier accuracy and F1measure when features are added incrementally.</S>
    <S sid="219" ssid="50">We start with our baseline unigram model and subsequently add various sets of features.</S>
    <S sid="220" ssid="51">First, we add all non-polar features (rows f5, f6, f7, f10 in Table 4) and observe an small improvement in the performance.</S>
    <S sid="221" ssid="52">Next, we add all part-of-speech based features and observe a gain of 3.28% over the unigram baseline.</S>
    <S sid="222" ssid="53">We see an additional increase in accuracy by 0.64% when we add other prior polarity features (rows f2, f3, f4, f9 in Table 4).</S>
    <S sid="223" ssid="54">These results are in line with our observations for the 2-way classification task.</S>
    <S sid="224" ssid="55">Once again, the main contribution comes from features that involve prior polarity of parts-ofspeech.</S>
    <S sid="225" ssid="56">The top ranked unigram features for the 3-way classification task are mostly similar to that of the 2-way classification task, except several terms with neutral polarity appear to be discriminative features, such as to, have, and so.</S>
  </SECTION>
  <SECTION title="9 Conclusion" number="9">
    <S sid="226" ssid="1">We presented results for sentiment analysis on Twitter.</S>
    <S sid="227" ssid="2">We use previously proposed state-of-the-art unigram model as our baseline and report an overall gain of over 4% for two classification tasks: a binary, positive versus negative and a 3-way positive versus negative versus neutral.</S>
    <S sid="228" ssid="3">We presented a comprehensive set of experiments for both these tasks on manually annotated data that is a random sample of stream of tweets.</S>
    <S sid="229" ssid="4">We investigated two kinds of models: tree kernel and feature based models and demonstrate that both these models outperform the unigram baseline.</S>
    <S sid="230" ssid="5">For our feature-based approach, we do feature analysis which reveals that the most important features are those that combine the prior polarity of words and their parts-of-speech tags.</S>
    <S sid="231" ssid="6">We tentatively conclude that sentiment analysis for Twitter data is not that different from sentiment analysis for other genres.</S>
    <S sid="232" ssid="7">In future work, we will explore even richer linguistic analysis, for example, parsing, semantic analysis and topic modeling.</S>
  </SECTION>
  <SECTION title="10 Acknowledgments" number="10">
    <S sid="233" ssid="1">Agarwal and Rambow are funded by NSF grant IIS-0713548.</S>
    <S sid="234" ssid="2">Vovsha is funded by NSF grant IIS-0916200.</S>
    <S sid="235" ssid="3">We would like to thank NextGen Invent (NGI) Corporation for providing us with the Twitter data.</S>
    <S sid="236" ssid="4">Please contact Deepak Mittal (deepak.mittal@ngicorportion.com) about obtaining the data.</S>
  </SECTION>
</PAPER>
