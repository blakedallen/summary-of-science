<PAPER>
  <S sid="0">Word Reordering And A Dynamic Programming Beam Search Algorithm For Statistical Machine Translation</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP).</S>
    <S sid="2" ssid="2">The search algorithm uses the translation model presented in Brown et al. (1993).</S>
    <S sid="3" ssid="3">Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm.</S>
    <S sid="4" ssid="4">Word reordering restrictions especially useful for the translation direction German to English are presented.</S>
    <S sid="5" ssid="5">The restrictions are generalized, and a set offour parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions.</S>
    <S sid="6" ssid="6">The beam search procedure has been successfully tested on the Uerbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary).</S>
    <S sid="7" ssid="7">For the medium-sized Uerbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="8" ssid="1">IBM T. J. Watson Research Center RWTH Aachen In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP).</S>
    <S sid="9" ssid="2">The search algorithm uses the translation model presented in Brown et al. (1993).</S>
    <S sid="10" ssid="3">Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm.</S>
    <S sid="11" ssid="4">Word reordering restrictions especially useful for the translation direction German to English are presented.</S>
    <S sid="12" ssid="5">The restrictions are generalized, and a set offour parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions.</S>
    <S sid="13" ssid="6">The beam search procedure has been successfully tested on the Uerbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary).</S>
    <S sid="14" ssid="7">For the medium-sized Uerbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article.</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="15" ssid="1">This article is about a search procedure for statistical machine translation (MT).</S>
    <S sid="16" ssid="2">The task of the search procedure is to find the most likely translation given a source sentence and a set of model parameters.</S>
    <S sid="17" ssid="3">Here, we will use a trigram language model and the translation model presented in Brown et al. (1993).</S>
    <S sid="18" ssid="4">Since the number of possible translations of a given source sentence is enormous, we must find the best output without actually generating the set of all possible translations; instead we would like to focus on the most likely translation hypotheses during the search process.</S>
    <S sid="19" ssid="5">For this purpose, we present a data-driven beam search algorithm similar to the one used in speech recognition search algorithms (Ney et al. 1992).</S>
    <S sid="20" ssid="6">The major difference between the search problem in speech recognition and statistical MT is that MT must take into account the different word order for the source and the target language, which does not enter into speech recognition.</S>
    <S sid="21" ssid="7">Tillmann, Vogel, Ney, and Zubiaga (1997) proposes a dynamic programming (DP)&#8211;based search algorithm for statistical MT that monotonically translates the input sentence from left to right.</S>
    <S sid="22" ssid="8">The word order difference is dealt with using a suitable preprocessing step.</S>
    <S sid="23" ssid="9">Although the resulting search procedure is very fast, the preprocessing is language specific and requires a lot of manual work.</S>
    <S sid="24" ssid="10">Currently, most search algorithms for statistical MT proposed in the literature are based on the A* concept (Nilsson 1971).</S>
    <S sid="25" ssid="11">Here, the word reordering can be easily included in the search procedure, since the input sentence positions can be processed in any order.</S>
    <S sid="26" ssid="12">The work presented in Berger et al. (1996) that is based on the A* concept, however, introduces word reordering restrictions in order to reduce the overall search space.</S>
    <S sid="27" ssid="13">The search procedure presented in this article is based on a DP algorithm to solve the traveling-salesman problem (TSP).</S>
    <S sid="28" ssid="14">A data-driven beam search approach is presented on the basis of this DP-based algorithm.</S>
    <S sid="29" ssid="15">The cities in the TSP correspond to source positions of the input sentence.</S>
    <S sid="30" ssid="16">By imposing constraints on the possible word reorderings similar to that described in Berger et al. (1996), the DP-based approach becomes more effective: when the constraints are applied, the number of word reorderings is greatly reduced.</S>
    <S sid="31" ssid="17">The original reordering constraint in Berger et al. (1996) is shown to be a special case of a more general restriction scheme in which the word reordering constraints are expressed in terms of simple combinatorical restrictions on the processed sets of source sentence positions.1 A set of four parameters is given to control the word reordering.</S>
    <S sid="32" ssid="18">Additionally, a set of four states is introduced to deal with grammatical reordering restrictions (e.g., for the translation direction German to English, the word order difference between the two languages is mainly due to the German verb group.</S>
    <S sid="33" ssid="19">In combination with the reordering restrictions, a data-driven beam search organization for the search procedure is proposed.</S>
    <S sid="34" ssid="20">A beam search pruning technique is conceived that jointly processes partial hypotheses according to two criteria: (1) The partial hypotheses cover the same set of source sentence positions, and (2) the partial hypotheses cover sets C of source sentence positions of equal cardinality.</S>
    <S sid="35" ssid="21">A partial hypothesis is said to cover a set of source sentence positions when exactly the positions in the set have already been processed in the search process.</S>
    <S sid="36" ssid="22">To verify the effectiveness of the proposed techniques, we report and analyze results for two translation tasks: the German to English Verbmobil task and French to English Canadian Hansards task.</S>
    <S sid="37" ssid="23">The article is structured as follows.</S>
    <S sid="38" ssid="24">Section 2 gives a short introduction to the translation model used and reports on other approaches to the search problem in statistical MT.</S>
    <S sid="39" ssid="25">In Section 3, a DP-based search approach is presented, along with appropriate pruning techniques that yield an efficient beam search algorithm.</S>
    <S sid="40" ssid="26">Section 4 reports and analyzes translation results for the different translation directions.</S>
    <S sid="41" ssid="27">In Section 5, we conclude with a discussion of the achieved results.</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="42" ssid="1">In this article, we use the translation model presented in Brown et al. (1993), and the mathematical notation we use here is taken from that paper as well: a source string fJ1 = f1 &#183; &#183; &#183; fj &#183; &#183; &#183; fJ is to be translated into a target string eI1 = e1 &#183; &#183; &#183; ei &#183; &#183; &#183; eI.</S>
    <S sid="43" ssid="2">Here, I is the length of the target string, and J is the length of the source string.</S>
    <S sid="44" ssid="3">Among all possible target strings, we will choose the string with the highest probability as given by Bayes&#8217; Architecture of the statistical translation approach based on Bayes&#8217; decision rule.</S>
    <S sid="45" ssid="4">Pr(eI1) is the language model of the target language, whereas Pr l eI1) is the string translation model.</S>
    <S sid="46" ssid="5">The language model probability is computed using a trigram language model.</S>
    <S sid="47" ssid="6">The string translation probability Pr(fJ1  |eI1) is modeled using a series of five models of increasing complexity in training.</S>
    <S sid="48" ssid="7">Here, the model used for the translation experiments is the IBM-4 model.</S>
    <S sid="49" ssid="8">This model uses the same parameter set as the IBM-5 model, which in preliminary experiments did not yield better translation results.</S>
    <S sid="50" ssid="9">The actual implementation used during the experiments is described in AlOnaizan et al. (1999) and in Och and Ney (2000).</S>
    <S sid="51" ssid="10">The argmax operation denotes the search problem (i.e., the generation of the output sentence in the target language).</S>
    <S sid="52" ssid="11">The overall architecture of the statistical translation approach is summarized in Figure 1.</S>
    <S sid="53" ssid="12">In general, as shown in this figure, there may be additional transformations to make the translation task simpler for the algorithm.</S>
    <S sid="54" ssid="13">The transformations may range from simple word categorization to more complex preprocessing steps that require some parsing of the source string.</S>
    <S sid="55" ssid="14">In this article, however, we will use only word categorization as an explicit transformation step.</S>
    <S sid="56" ssid="15">In the search procedure both the language and the translation model are applied after the text transformation steps.</S>
    <S sid="57" ssid="16">The following &#8220;types&#8221; of parameters are used for the IBM-4 translation model: Lexicon probabilities: We use the lexicon probability p(f  |e) for translating the single target word e as the single source word f. A source word f may be translated by the &#8220;null&#8221; word e0 (i.e., it does not produce any target word e).</S>
    <S sid="58" ssid="17">A translation probability p(f  |e0) is trained along with the regular translation probabilities.</S>
    <S sid="59" ssid="18">Fertilities: A single target word e may be aligned to n = 0,1 or more source words.</S>
    <S sid="60" ssid="19">This is explicitly modeled by the fertility parameter &#966;(n  |e): the probability that the target word e is translated by n source words is &#966;(n  |e).</S>
    <S sid="61" ssid="20">The fertility for the &#8220;null&#8221; word is treated specially (for details see Brown et al. [1993]).</S>
    <S sid="62" ssid="21">Berger et al. (1996) describes the extension of a partial hypothesis by a pair of target words (e', e), where e' is not connected to any source word f. In this case, the so-called spontaneous target word e' is accounted for with the fertility.</S>
    <S sid="63" ssid="22">Here, the translation probability &#966;(0  |e') and notranslation probability p(f  |e').</S>
    <S sid="64" ssid="23">Class-based distortion probabilities: When covering a source sentence position j, we use distortion probabilities that depend on the previously covered source sentence positions (we say that a source sentence position j is covered for a partial hypothesis when it is taken account of in the translation process by generating a target word or the &#8220;null&#8221; word e0 ).</S>
    <S sid="65" ssid="24">In Brown et al. (1993), two types of distortion probabilities are distinguished: (1) the leftmost word of a set of source words f aligned to the same target word e (which is called the &#8220;head&#8221;) is placed, and (2) the remaining source words are placed.</S>
    <S sid="66" ssid="25">Two separate distributions are used for these two cases.</S>
    <S sid="67" ssid="26">For placing the &#8220;head&#8221; the center function center(i) (Brown et al. [1993] uses the notation Oi) is used: the average position of the source words with which the target word ei_1 is aligned.</S>
    <S sid="68" ssid="27">The distortion probabilities are class-based: They depend on the word class J7(f) of a covered source word f as well as on the word class E(e) of the previously generated target word e. The classes are automatically trained (Brown et al. 1992).</S>
    <S sid="69" ssid="28">When the IBM-4 model parameters are used during search, an input sentence can be processed one source position at a time in a certain order primarily determined by the distortion probabilities.</S>
    <S sid="70" ssid="29">We will use the following simplified set of translation model parameters: lexicon probabilities p(f  |e) and distortion probabilities p(j  |j',J).</S>
    <S sid="71" ssid="30">Here, j is the currently covered input sentence position and j' is the previously covered input sentence position.</S>
    <S sid="72" ssid="31">The input sentence length J is included, since we would like to think of the distortion probability as normalized according to J.</S>
    <S sid="73" ssid="32">No fertility probabilities or &#8220;null&#8221; word probabilities are used; thus each source word f is translated as exactly one target word e and each target word e is translated as exactly one source word f. The simplified notation will help us to focus on the most relevant details of the DP-based search procedure.</S>
    <S sid="74" ssid="33">The simplified set of parameters leads to an unrealistic assumption about the length of the source and target sentence, namely, I = J.</S>
    <S sid="75" ssid="34">During the translation experiments we will, of course, not make this assumption.</S>
    <S sid="76" ssid="35">The implementation details for using the full set of IBM-4 model parameters are given in Section 3.9.2.</S>
    <S sid="77" ssid="36">In this section, we give a short overview of search procedures used in statistical MT: Brown et al. (1990) and Brown et al.</S>
    <S sid="78" ssid="37">(1993) describe a statistical MT system that is based on the same statistical principles as those used in most speech recognition systems (Jelinek 1976).</S>
    <S sid="79" ssid="38">Berger et al. (1994) describes the French-to-English Candide translation system, which uses the translation model proposed in Brown et al.</S>
    <S sid="80" ssid="39">(1993).</S>
    <S sid="81" ssid="40">A detailed description of the decoder used in that system is given in Berger et al. (1996) but has never been published in a paper: Throughout the search process, partial hypotheses are maintained in a set of priority queues.</S>
    <S sid="82" ssid="41">There is a single priority queue for each subset of covered positions in the source string.</S>
    <S sid="83" ssid="42">In practice, the priority queues are initialized only on demand; far fewer than the full number of queues possible are actually used.</S>
    <S sid="84" ssid="43">The priority queues are limited in size, and only the 1,000 hypotheses with the highest probability are maintained.</S>
    <S sid="85" ssid="44">Each priority queue is assigned a threshold to select the hypotheses that are going to be extended, and the process of assigning these thresholds is rather complicated.</S>
    <S sid="86" ssid="45">A restriction on the possible word reorderings, which is described in Section 3.6, is applied.</S>
    <S sid="87" ssid="46">Wang and Waibel (1997) presents a search algorithm for the IBM-2 translation model based on the A&#8727; concept and multiple stacks.</S>
    <S sid="88" ssid="47">An extension of this algorithm is demonstrated in Wang and Waibel (1998).</S>
    <S sid="89" ssid="48">Here, a reshuffling step on top of the original decoder is used to handle more complex translation models (e.g., the IBM-3 model is added).</S>
    <S sid="90" ssid="49">Translation approaches that use the IBM-2 model parameters but are based on DP are presented in Garc&#180;&#305;a-Varea, Casacuberta, and Ney (1998) and Niessen et al. (1998).</S>
    <S sid="91" ssid="50">An approach based on the hidden Markov model alignments as used in speech recognition is presented in Tillmann, Vogel, Ney, and Zubiaga (1997) and Tillmann, Vogel, Ney, Zubiaga, and Sawaf (1997).</S>
    <S sid="92" ssid="51">This approach assumes that source and target language have the same word order, and word order differences are dealt with in a preprocessing stage.</S>
    <S sid="93" ssid="52">The work by Wu (1996) also uses the original IBM model parameters and obtains an efficient search algorithm by restricting the possible word reorderings using the so-called stochastic bracketing transduction grammar.</S>
    <S sid="94" ssid="53">Three different decoders for the IBM-4 translation model are compared in Germann et al. (2001).</S>
    <S sid="95" ssid="54">The first is a reimplementation of the stack-based decoder described in Berger et al. (1996).</S>
    <S sid="96" ssid="55">The second is a greedy decoder that starts with an approximate solution and then iteratively improves this first rough solution.</S>
    <S sid="97" ssid="56">The third converts the decoding problem into an integer program (IP), and a standard software package for solving IP is used.</S>
    <S sid="98" ssid="57">Although the last approach is guaranteed to find the optimal solution, it is tested only for input sentences of length eight or shorter.</S>
    <S sid="99" ssid="58">This article will present a DP-based beam search decoder for the IBM-4 translation model.</S>
    <S sid="100" ssid="59">The decoder is designed to carry out an almost full search with a small number of search errors and with little performance degradation as measured by the word error criterion.</S>
    <S sid="101" ssid="60">A preliminary version of the work presented here was published in Tillmann and Ney (2000).</S>
  </SECTION>
  <SECTION title="3." number="4">
    <S sid="102" ssid="1">To explicitly describe the word order difference between source and target language, Brown et al. (1993) introduced an alignment concept, in which a source position j is mapped to exactly one target position i: Regular alignment example for the translation direction German to English.</S>
    <S sid="103" ssid="2">For each German source word there is exactly one English target word on the alignment path.</S>
    <S sid="104" ssid="3">An example for this kind of alignment is given in Figure 2, in which each German source position j is mapped to an English target position i.</S>
    <S sid="105" ssid="4">In Brown et al. (1993), this alignment concept is used for model IBM-1 through model IBM-5.</S>
    <S sid="106" ssid="5">For search purposes, we use the inverted alignment concept as introduced in Niessen et al. (1998) and Ney et al.</S>
    <S sid="107" ssid="6">(2000).</S>
    <S sid="108" ssid="7">An inverted alignment is defined as follows: inverted alignment: i &#8594; j = bi Here, a target position i is mapped to a source position j.</S>
    <S sid="109" ssid="8">The coverage constraint for an inverted alignment is not expressed by the notation: Each source position j should be &#8220;hit&#8221; exactly once by the path of the inverted alignment bI1 = b1 &#183; &#183; &#183; bi &#183; &#183; &#183; bI.</S>
    <S sid="110" ssid="9">The advantage of the inverted alignment concept is that we can construct target sentence hypotheses from bottom to top along the positions of the target sentence.</S>
    <S sid="111" ssid="10">Using the inverted alignments in the maximum approximation, we rewrite equation (1) to obtain the following search criterion, in which we are looking for the most likely target Illustration of the transitions in the regular and in the inverted alignment model.</S>
    <S sid="112" ssid="11">The regular alignment model (left figure) is used to generate the sentence from left to right; the inverted alignment model (right figure) is used to generate the sentence from bottom to top.</S>
    <S sid="113" ssid="12">The following notation is used: ei&#8722;1, ei&#8722;2 are the immediate predecessor target words, ei is the word to be hypothesized, p(ei  |ei&#8722;1,ei&#8722;2) denotes the trigram language model probability, p(fbi  |ei) denotes the lexicon probability for translating the target word ei as source word fbi, and p(bi  |bi&#8722;1,J) is the distortion probability for covering source position bi after source position bi&#8722;1.</S>
    <S sid="114" ssid="13">Note that in equation (2) two products over i are merged into a single product over i.</S>
    <S sid="115" ssid="14">The translation probability p(f1J  |eI1) is computed in the maximum approximation using the distortion and the lexicon probabilities.</S>
    <S sid="116" ssid="15">Finally, p(J  |I) is the sentence length model, which will be dropped in the following (it is not used in the IBM-4 translation model).</S>
    <S sid="117" ssid="16">For each source sentence fJ1 to be translated, we are searching for the unknown mapping that optimizes equation (2): In Section 3.3, we will introduce an auxiliary quantity that can be evaluated recursively using DP to find this unknown mapping.</S>
    <S sid="118" ssid="17">We will explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have already been processed.</S>
    <S sid="119" ssid="18">Figure 3 illustrates the concept of the search algorithm using inverted alignments: Partial hypotheses are constructed from bottom to top along the positions of the target sentence.</S>
    <S sid="120" ssid="19">Partial hypotheses of length i-1 are extended to obtain partial hypotheses of the length i.</S>
    <S sid="121" ssid="20">Extending a partial hypothesis means covering a source sentence position j that has not yet been covered.</S>
    <S sid="122" ssid="21">For a given grid point in the translation lattice, the unknown target word sequence can be obtained by tracing back the translation decisions to the partial hypothesis at stage i = 1.</S>
    <S sid="123" ssid="22">The grid points are defined in Section 3.3.</S>
    <S sid="124" ssid="23">In the left part of the figure the regular alignment concept is shown for comparison purposes.</S>
    <S sid="125" ssid="24">Held and Karp (1962) presents a DP approach to solve the TSP, an optimization problem that is defined as follows: Given are a set of cities {1, ... , J} and for each pair of cities j,j' the cost djj, &gt; 0 for traveling from city j to city j'.</S>
    <S sid="126" ssid="25">We are looking for the shortest tour, starting and ending in city 1, that visits all cities in the set of cities exactly once.</S>
    <S sid="127" ssid="26">We are using the notation C for the set of cities, since it corresponds to a coverage set of processed source positions in MT.</S>
    <S sid="128" ssid="27">A straightforward way to find the shortest tour is by trying all possible permutations of the J cities.</S>
    <S sid="129" ssid="28">The resulting algorithm has a complexity of O(J!).</S>
    <S sid="130" ssid="29">DP can be used, however, to find the shortest tour in O(J2 &#183; 2J), which is a much smaller complexity for larger values of J.</S>
    <S sid="131" ssid="30">The approach recursively evaluates the quantity D(C, j): D(C, j) := costs of the partial tour starting in city 1, ending in city j, and visiting all cities in C Subsets of cities C of increasing cardinality c are processed.</S>
    <S sid="132" ssid="31">The algorithm, shown in Table 1, works because not all permutations of cities have to be considered explicitly.</S>
    <S sid="133" ssid="32">During the computation, for a pair (C, j), the order in which the cities in C have been visited can be ignored (except j); only the costs for the best path reaching j has to be stored.</S>
    <S sid="134" ssid="33">For the initialization the costs for starting from city 1 are set: D({k}, k) = d1k for each k E {2,..., |C|}.</S>
    <S sid="135" ssid="34">Then, subsets C of increasing cardinality are processed.</S>
    <S sid="136" ssid="35">Finally, the cost for the optimal tour is obtained in the second-to-last line of the algorithm.</S>
    <S sid="137" ssid="36">The optimal tour itself can be found using a back-pointer array in which the optimal decision for each grid point (C, j) is stored.</S>
    <S sid="138" ssid="37">Figure 4 illustrates the use of the algorithm by showing the &#8220;supergraph&#8221; that is searched in the Held and Karp algorithm for a TSP with J = 5 cities.</S>
    <S sid="139" ssid="38">When traversing the lattice from left to right following the different possibilities, a partial path to a node j corresponds to the subset C of all cities on that path together with the last visited Illustration of the algorithm by Held and Karp for a traveling salesman problem with J = 5 cities.</S>
    <S sid="140" ssid="39">Not all permutations of cities have to be evaluated explicitly.</S>
    <S sid="141" ssid="40">For a given subset of cities the order in which the cities have been visited can be ignored. city j.</S>
    <S sid="142" ssid="41">Of all the different paths merging into the node j, only the partial path with the smallest cost has to be retained for further computation.</S>
    <S sid="143" ssid="42">In this section, the Held and Karp algorithm is applied to statistical MT.</S>
    <S sid="144" ssid="43">Using the concept of inverted alignments as introduced in Section 3.1, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have already been processed.</S>
    <S sid="145" ssid="44">Here, the correspondence is according to the fact that each source sentence position has to be covered exactly once, fulfilling the coverage constraint.</S>
    <S sid="146" ssid="45">The cities of the more complex translation TSP correspond roughly to triples (e', e, j), the notation for which is given below.</S>
    <S sid="147" ssid="46">The final path output by the translation algorithm will contain exactly one triple (e', e, j) for each source position j.</S>
    <S sid="148" ssid="47">The algorithm processes subsets of partial hypotheses with coverage sets C of increasing cardinality c. For a trigram language model, the partial hypotheses are of the form (e', e, C, j), where e', e are the last two target words, C is a coverage set for the already covered source positions, and j is the last covered position.</S>
    <S sid="149" ssid="48">The target word sequence that ends in e', e is stored as a back pointer to the predecessor partial hypothesis (and recursively to its predecessor hypotheses) and is not shown in the notation.</S>
    <S sid="150" ssid="49">Each distance in the TSP now corresponds to the negative logarithm of the product of the translation, distortion, and language model probabilities.</S>
    <S sid="151" ssid="50">The following input: source language string f1 &#183; &#183; &#183; fj &#183; &#183; &#183; fJ Here, j' is the previously covered source sentence position and e', e'' are the predecessor words.</S>
    <S sid="152" ssid="51">The DP equation is evaluated recursively for each hypothesis (e', e, C, j).</S>
    <S sid="153" ssid="52">The resulting algorithm is depicted in Table 2.</S>
    <S sid="154" ssid="53">Some details concerning the initialization and the finding of the best target language string are presented in Section 3.4. p($  |e, e') is the trigram language probability for predicting the sentence boundary symbol $.</S>
    <S sid="155" ssid="54">The complexity of the algorithm is O(E3 &#183; J2 &#183; 2J), where E is the size of the target language vocabulary.</S>
    <S sid="156" ssid="55">The above search space is still too large to translate even a medium-length input sentence.</S>
    <S sid="157" ssid="56">On the other hand, only very restricted reorderings are necessary; for example, for the translation direction German to English, the word order difference is mostly restricted to the German verb group.</S>
    <S sid="158" ssid="57">The approach presented here assumes a mostly monotonic traversal of the source sentence positions from left to right.2 A small number of positions may be processed sooner than they would be in that monotonic traversal.</S>
    <S sid="159" ssid="58">Each source position then generates a certain number of target words.</S>
    <S sid="160" ssid="59">The restrictions are fully formalized in Section 3.5.</S>
    <S sid="161" ssid="60">A typical situation is shown in Figure 5.</S>
    <S sid="162" ssid="61">When translating the sentence monotonically from left to right, the translation of the German finite verb kann, which is the left verbal brace in this case, is skipped until the German noun phrase mein Kollege, which is the subject of the sentence, is translated.</S>
    <S sid="163" ssid="62">Then, the right verbal brace is translated: Word reordering for the translation direction German to English: The reordering is restricted to the German verb group.</S>
    <S sid="164" ssid="63">The infinitive besuchen and the negation particle nicht.</S>
    <S sid="165" ssid="64">The following restrictions are used: One position in the source sentence may be skipped for a distance of up to L = 4 source positions, and up to two source positions may be moved for a distance of at most R = 10 source positions (the notation L and R shows the relation to the handling of the left and right verbal brace).</S>
    <S sid="166" ssid="65">To formalize the approach, we introduce four verb group states S: Order in which the German source positions are covered for the German-to-English reordering example given in Figure 5.</S>
    <S sid="167" ssid="66">The states Move and Skip both allow a set of upcoming words to be processed sooner than would be the case in the monotonic traversal.</S>
    <S sid="168" ssid="67">The state Initial is entered whenever there are no uncovered positions to the left of the rightmost covered position.</S>
    <S sid="169" ssid="68">The sequence of states needed to carry out the word reordering example in Figure 5 is given in Figure 6.</S>
    <S sid="170" ssid="69">The 13 source sentence words are processed in the order shown.</S>
    <S sid="171" ssid="70">A formal specification of the state transitions is given in Section 3.5.</S>
    <S sid="172" ssid="71">Any number of consecutive German verb phrases in a sentence can be processed by the algorithm.</S>
    <S sid="173" ssid="72">The finite-state control presented here is obtained from a simple analysis of the Germanto-English word reordering problem and is not estimated from the training data.</S>
    <S sid="174" ssid="73">It can be viewed as an extension of the IBM-4 model distortion probabilities.</S>
    <S sid="175" ssid="74">Using the above states, we define partial hypothesis extensions of the following type: Not only the coverage set C and the positions j, j', but also the verb group states S, S', are taken into account.</S>
    <S sid="176" ssid="75">For the sake of brevity, we have omitted the target language words e, e' in the notation of the partial hypothesis extension.</S>
    <S sid="177" ssid="76">For each extension an uncovered position is added to the coverage set C of the partial hypothesis, and the verb group state S may change.</S>
    <S sid="178" ssid="77">A more detailed description of the partial hypothesis extension for a certain state S is given in the next section in a more general context.</S>
    <S sid="179" ssid="78">Covering the first uncovered position in the source sentence, we use the lanTillmann and Ney DP Beam Search for Statistical MT guage model probability p(e  |$,$).</S>
    <S sid="180" ssid="79">Here, $ is the sentence boundary symbol, which is thought to be at position 0 in the target sentence.</S>
    <S sid="181" ssid="80">The search starts in the hypothesis (Initial, {&#8709;}, 0).</S>
    <S sid="182" ssid="81">{&#8709;} denotes the empty set, where no source sentence position is covered.</S>
    <S sid="183" ssid="82">The following recursive equation is evaluated: The search ends in the hypotheses (Initial, {1, ... , J}, j); the last covered position may be in the range j &#8712; {J&#8722;L, ... , J}, because some source positions may have been skipped at the end of the input sentence.</S>
    <S sid="184" ssid="83">{1,. .</S>
    <S sid="185" ssid="84">.</S>
    <S sid="186" ssid="85">, J} denotes a coverage set including all positions from position 1 to position J.</S>
    <S sid="187" ssid="86">The final translation probability QF is where p($  |e,e') denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.</S>
    <S sid="188" ssid="87">QF can be obtained using an algorithm very similar to the one given in Table 2.</S>
    <S sid="189" ssid="88">The complexity of the verb group reordering for the translation direction German to English is O(E3 &#183; J &#183; (R2 &#183; L &#183; R)), as shown in Tillmann (2001).</S>
    <S sid="190" ssid="89">For the translation direction English to German, the word reordering can be restricted in a similar way as for the translation direction German to English.</S>
    <S sid="191" ssid="90">Again, the word order difference between the two languages is mainly due to the German verb group.</S>
    <S sid="192" ssid="91">During the translation process, the English verb group is decomposed as shown in Figure 7.</S>
    <S sid="193" ssid="92">When the sentence is translated monotonically from left to right, the translation of the English finite verb can is moved, and it is translated as the German left verbal brace before the English noun phrase my colleague, which is the subject of the sentence.</S>
    <S sid="194" ssid="93">The translations of the infinitive visit and of the negation particle not are skipped until later in the translation process.</S>
    <S sid="195" ssid="94">For this translation direction, the translation of one source sentence position may be moved for a distance of up to L = 4 source positions, and the translation of up to two source positions may be skipped for a distance of up to R = 10 source positions (we take over the L and R notation from the previous section).</S>
    <S sid="196" ssid="95">Thus, the role of the skipping and the moving are simply reversed with respect to their roles in German-to-English translation.</S>
    <S sid="197" ssid="96">For the example translation in Figure 7, the order in which the source sentence positions are covered is given in Figure 8.</S>
    <S sid="198" ssid="97">We generalize the two approaches for the different translation directions as follows: In both approaches, we assume that the source sentence is mainly processed monotonically.</S>
    <S sid="199" ssid="98">A small number of upcoming source sentence positions may be processed earlier than they would be in the monotonic traversal: The states Skip and Move are used as explained in the preceding section.</S>
    <S sid="200" ssid="99">The positions to be processed outside the monotonic traversal are restricted as follows: Word reordering for the translation direction English to German: The reordering is restricted to the English verb group.</S>
    <S sid="201" ssid="100">These restrictions will be fully formalized later in this section.</S>
    <S sid="202" ssid="101">In the state Move, some source sentence positions are &#8220;moved&#8221; from later in the sentence to earlier.</S>
    <S sid="203" ssid="102">After source sentence positions are moved, they are marked, and the translation of the sentence is continued monotonically, keeping track of the positions already covered.</S>
    <S sid="204" ssid="103">To formalize the approach, we introduce four reordering states S: To formalize the approach, the following notation is introduced: Order in which the English source positions are covered for the English-to-German reordering example given in Figure 7. rmax(C) is the rightmost covered and lmin(C) is the leftmost uncovered source position. u(C) is the number of &#8220;skipped&#8221; positions, and m(C) is the number of &#8220;moved&#8221; positions.</S>
    <S sid="205" ssid="104">The function card(&#183;) returns the cardinality of a set of source positions.</S>
    <S sid="206" ssid="105">The function w(C) describes the &#8220;window&#8221; size in which the word reordering takes place.</S>
    <S sid="207" ssid="106">A procedural description for the computation of the set of successor hypotheses for a given partial hypothesis (S, C, j) is given in Table 3.</S>
    <S sid="208" ssid="107">There are restrictions on the possible successor states: A partial hypothesis in state Skip cannot be expanded into a partial hypothesis in state Move and vice versa.</S>
    <S sid="209" ssid="108">If the coverage set for the newly generated hypothesis covers a contiguous initial block of source positions, the state Initial is entered.</S>
    <S sid="210" ssid="109">No other state S is considered as a successor state in this case (hence the use of the continue statement in the procedural description).</S>
    <S sid="211" ssid="110">The set of successor hypotheses Succ by which to extend the partial hypothesis (S, C, j) is computed using the constraints defined by the values for numskip, widthskip, nummove, and widthmove, as explained in the Appendix.</S>
    <S sid="212" ssid="111">In particular, a source position k is discarded for extension if the &#8220;window&#8221; restrictions are violated.</S>
    <S sid="213" ssid="112">Within the restrictions all possible successors are computed.</S>
    <S sid="214" ssid="113">It can be observed that the set of successors, as computed in Table 3, is never empty.</S>
    <S sid="215" ssid="114">Procedural description to compute the set Succ of successor hypotheses by which to extend a partial hypothesis (S, C, j). input: partial hypothesis (S, C, j) output: set Succ of successor hypotheses There is an asymmetry between the two reordering states Move and Skip: While in state Move, the algorithm is not allowed to cover the position lmin(C).</S>
    <S sid="216" ssid="115">It must first enter the state Cover to do so.</S>
    <S sid="217" ssid="116">In contrast, for the state Skip, the newly generated hypothesis always remains in the state Skip (until the state Initial is entered.)</S>
    <S sid="218" ssid="117">This is motivated by the word reordering for the German verb group.</S>
    <S sid="219" ssid="118">After the right verbal brace has been processed, no source words may be moved into the verbal brace from later in the sentence.</S>
    <S sid="220" ssid="119">There is a redundancy in the reorderings: The same reordering might be carried out using either the state Skip or Move, especially if widthskip and widthmove are about the same.</S>
    <S sid="221" ssid="120">The additional computational burden is alleviated somewhat by the fact that the pruning, as introduced in Section 3.8, does not distinguish hypotheses according to the states.</S>
    <S sid="222" ssid="121">A complexity analysis for different reordering constraints is given in Tillmann (2001).</S>
    <S sid="223" ssid="122">We now compare the new word reordering approach with the approach used in Berger et al. (1996).</S>
    <S sid="224" ssid="123">In the approach presented in this article, source sentence words are aligned with hypothesized target sentence words.3 When a source sentence word is aligned, we say its position is covered.</S>
    <S sid="225" ssid="124">During the search process, a partial hypothesis is extended by choosing an uncovered source sentence position, and this choice is restricted.</S>
    <S sid="226" ssid="125">Only one of the first n uncovered positions in a coverage set may be chosen, where n is set to 4.</S>
    <S sid="227" ssid="126">This choice is illustrated in Figure 9.</S>
    <S sid="228" ssid="127">In the figure, covered positions are marked by a filled circle, and uncovered positions are marked by an unfilled circle.</S>
    <S sid="229" ssid="128">Positions that may be covered next are marked by an unfilled square.</S>
    <S sid="230" ssid="129">The restrictions for a coverage set C can be expressed in terms of the expression u(C) defined in the previous section: The number of uncovered source sentence positions to the left of the rightmost covered position.</S>
    <S sid="231" ssid="130">Demanding u(C) &#8804; 3, we obtain the S3 restriction Illustration of the IBM-style reordering constraint. introduced in the Appendix.</S>
    <S sid="232" ssid="131">An upper bound of O(E3 &#183; J4) for the word reordering complexity is given in Tillmann (2001).</S>
    <S sid="233" ssid="132">In order to demonstrate the complexity of the proposed reordering constraints, we have modified our translation algorithm to show, for the different reordering constraints, the overall number of successor states generated by the algorithm given in Table 3.</S>
    <S sid="234" ssid="133">The number of successors shown in Figure 10 is counted for a pseudotranslation task in which a pseudo&#8211;source word x is translated into the identically pseudo&#8211; target word x.</S>
    <S sid="235" ssid="134">No actual optimization is carried out; the total number of successors is simply counted as the algorithm proceeds through subsets of increasing cardinality.</S>
    <S sid="236" ssid="135">The complexity differences for the different reordering constraints result from the different number of coverage subsets C and corresponding reordering states S allowed.</S>
    <S sid="237" ssid="136">For the different reordering constraints we obtain the following results (the abbreviations MON, GE, EG, and S3 are taken from the Appendix): Number of processed arcs for the pseudotranslation task as a function of the input sentence length J (y-axis is given in log scale).</S>
    <S sid="238" ssid="137">The complexity for the four different reordering constraints MON, GE, EG, and S3 is given.</S>
    <S sid="239" ssid="138">The complexity of the S3 constraint is close to J4.</S>
    <S sid="240" ssid="139">To speed up the search, a beam search strategy is used.</S>
    <S sid="241" ssid="140">There is a direct analogy to the data-driven search organization used in continuous-speech recognition (Ney et al. 1992).</S>
    <S sid="242" ssid="141">The full DP search algorithm proceeds cardinality-synchronously over subsets of source sentence positions of increasing cardinality.</S>
    <S sid="243" ssid="142">Using the beam search concept, the search can be focused on the most likely hypotheses.</S>
    <S sid="244" ssid="143">The hypotheses Qe,(e, C,j) are distinguished according to the coverage set C, with two kinds of pruning based on this coverage set: After the pruning is carried out, we retain for further consideration only hypotheses with a probability close to the maximum probability.</S>
    <S sid="245" ssid="144">The number of surviving hypotheses is controlled by four kinds of thresholds: For the coverage and the cardinality pruning, the probability Qe,(e, C, j) is adjusted to take into account the uncovered source sentence positions C&#175; = {1, ... , J}\C.</S>
    <S sid="246" ssid="145">To make this adjustment, for a source word f at an uncovered source position, we precompute an upper bound &#175;p(f) for the product of language model and lexicon probability: The above optimization is carried out only over the word trigrams (e, e', e'') that have actually been seen in the training data.</S>
    <S sid="247" ssid="146">Additionally, the observation pruning described below is applied to the possible translations e of a source word f. The upper bound is used in the beam search concept to increase the comparability between hypotheses covering different coverage sets.</S>
    <S sid="248" ssid="147">Even more benefit from the upper bound &#175;p(f) can be expected if the distortion and the fertility probabilities are taken into account (Tillmann 2001).</S>
    <S sid="249" ssid="148">Using the definition of &#175;p(f), the following modified probability Qe (e, C, j) is used to replace the original probability Qe (e,C,j), and all pruning is applied to the new probability: For the translation experiments, equation (3) is recursively evaluated over subsets of source positions of equal cardinality.</S>
    <S sid="250" ssid="149">For reasons of brevity, we omit the state description S in equation (3), since no separate pruning according to the states S is carried out.</S>
    <S sid="251" ssid="150">The set of surviving hypotheses for each cardinality c is referred to as the beam.</S>
    <S sid="252" ssid="151">The size of the beam for cardinality c depends on the ambiguity of the translation task for that cardinality.</S>
    <S sid="253" ssid="152">To fully exploit the speedup of the DP beam search, the search space is dynamically constructed as described in Tillmann, Vogel, Ney, Zubiaga, and Sawaf (1997), rather than using a static search space.</S>
    <S sid="254" ssid="153">To carry out the pruning, the maximum probabilities with respect to each coverage set C and cardinality c are computed: The coverage pruning threshold tC and the cardinality pruning threshold tc are used to prune active hypotheses.</S>
    <S sid="255" ssid="154">We call this pruning translation pruning.</S>
    <S sid="256" ssid="155">Hypotheses are pruned according to their translation probability: For the translation experiments presented in Section 4, the negative logarithms of the actual pruning thresholds tc and tC are reported.</S>
    <S sid="257" ssid="156">A hypothesis (e', e, C,j) is discarded if its probability is below the corresponding threshold.</S>
    <S sid="258" ssid="157">For the current experiments, the coverage and the cardinality threshold are constant for different coverage sets C and cardinalities c. Together with the translation pruning, histogram pruning is carried out: The overall number N(C) of active hypotheses for the coverage set C and the overall number N(c) of active hypotheses for all subsets of a given cardinality may not exceed a given number; again, different numbers are used for coverage and cardinality pruning.</S>
    <S sid="259" ssid="158">The coverage histogram pruning is denoted by nc, and the cardinality histogram pruning is denoted by nc: If the numbers of active hypotheses for each coverage set C and cardinality c, N(C) and N(c), exceed the above thresholds, only the partial hypotheses with the highest translation probabilities are retained (e.g., we may use nc = 1,000 for the coverage histogram pruning).</S>
    <S sid="260" ssid="159">The third type of pruning conducted observation pruning: The number of words that may be produced by a source word f is limited.</S>
    <S sid="261" ssid="160">For each source language word f the list of its possible translations e is sorted according to where puni(e) is the unigram probability of the target language word e. Only the best no target words e are hypothesized during the search process (e.g., during the experiments to hypothesize, the best no = 50 words was sufficient.</S>
    <S sid="262" ssid="161">In this section, we describe the implementation of the beam search algorithm presented in the previous sections and show how it is applied to the full set of IBM-4 model parameters.</S>
    <S sid="263" ssid="162">3.9.1 Baseline DP Implementation.</S>
    <S sid="264" ssid="163">The implementation described here is similar to that used in beam search speech recognition systems, as presented in Ney et al. (1992).</S>
    <S sid="265" ssid="164">The similarities are given mainly in the following: input sentences are processed mainly monotonically from left to right.</S>
    <S sid="266" ssid="165">The algorithm works cardinality-synchronously, meaning that all the hypotheses that are processed cover subsets of source sentence positions of equal cardinality c. Table 4 shows a two-list implementation of the search algorithm given in Table 2 in which the beam pruning is included.</S>
    <S sid="267" ssid="166">The two lists are referred to as S and Snew: S is the list of hypotheses that are currently expanded, and Snew is the list of newly Two-list implementation of a DP-based search algorithm for statistical MT. input: source string f1 &#183; &#183; &#183; fj &#183; &#183; &#183; fJ initial hypothesis lists: S = {($, $, {0}, 0)} generated hypotheses.</S>
    <S sid="268" ssid="167">The search procedure processes subsets of covered source sentence positions of increasing cardinality.</S>
    <S sid="269" ssid="168">The search starts with S = {($, $, {0}, 0)}, where $ denotes the sentence start symbol for the immediate two predecessor words and {0} denotes the empty coverage set, in which no source position is covered yet.</S>
    <S sid="270" ssid="169">For the initial search state, the position last covered is set to 0.</S>
    <S sid="271" ssid="170">A set S of active hypotheses is expanded for each cardinality c using lexicon model, language model, and distortion model probabilities.</S>
    <S sid="272" ssid="171">The newly generated hypotheses are added to the hypothesis set Snew; for hypotheses that are not distinguished according to our DP approach, only the best partial hypothesis is retained for further consideration.</S>
    <S sid="273" ssid="172">This so-called recombination is implemented as a set of simple lookup and update operations on the set Snew of partial hypotheses.</S>
    <S sid="274" ssid="173">During the partial hypothesis extensions, an anticipated pruning is carried out: Hypotheses are discarded before they are considered for recombination and are never added to Snew.</S>
    <S sid="275" ssid="174">(The anticipated pruning is not shown in Table 4.</S>
    <S sid="276" ssid="175">It is based on the pruning thresholds described in Section 3.8.)</S>
    <S sid="277" ssid="176">After the extension of all partial hypotheses in S, a pruning step is carried out for the hypotheses in the newly generated set Snew.</S>
    <S sid="278" ssid="177">The pruning is based on two simple sorting steps on the list of partial hypotheses Snew.</S>
    <S sid="279" ssid="178">(Instead of sorting the partial hypotheses, we might have used hashing.)</S>
    <S sid="280" ssid="179">First, the partial hypotheses are sorted according to their translation scores (within the implementation, all probabilities are converted into translation scores by taking the negative logarithm &#8722; log()).</S>
    <S sid="281" ssid="180">Cardinality pruning can then be carried out simply by running down the list of hypotheses, starting with the maximum-probability hypothesis, and applying the cardinality thresholds.</S>
    <S sid="282" ssid="181">Then, the partial hypotheses are sorted a second time according to their coverage set C and their translation score.</S>
    <S sid="283" ssid="182">After this sorting step, all partial hypotheses that cover the same subset of source sentence positions are located in consecutive fragments in the overall list of partial hypotheses.</S>
    <S sid="284" ssid="183">Coverage pruning is carried out in a single run over the list of partial hypotheses: For each fragment corresponding to the same coverage set C, the coverage pruning threshold is applied.</S>
    <S sid="285" ssid="184">The partial hypotheses that survive the two pruning stages are then written into the so-called bookkeeping array (Ney et al. 1992).</S>
    <S sid="286" ssid="185">For the next expansion step, the set S is set to the newly generated list of hypotheses.</S>
    <S sid="287" ssid="186">Finally, the target translation is constructed from the bookkeeping array.</S>
    <S sid="288" ssid="187">3.9.2 Details for IBM-4 Model.</S>
    <S sid="289" ssid="188">In this section, we outline how the DP-based beam search approach can be carried out using the full set of IBM-4 parameters.</S>
    <S sid="290" ssid="189">(More details can be found in Tillmann [2001] or in the cited papers.)</S>
    <S sid="291" ssid="190">First, the full set of IBM-4 parameters does not make the simplifying assumption given in Section 3.1, namely, that source and target sentences are of equal length: Either a target word e may be aligned with several source words (its fertility is greater than one) or a single source word may produce zero, one, or two target words, as described in Berger et al. (1996), or both.</S>
    <S sid="292" ssid="191">Zero target words are generated if f is aligned to the &#8220;null&#8221; word e0.</S>
    <S sid="293" ssid="192">Generating a single target word e is the regular case.</S>
    <S sid="294" ssid="193">Two target words (e',e'') may be generated.</S>
    <S sid="295" ssid="194">The costs for generating the target word e' are given by its fertility &#966;(0  |e') and the language model probability; no lexicon probability is used.</S>
    <S sid="296" ssid="195">During the experiments, we restrict ourselves to triples of target words (e, e', e'') actually seen in the training data.</S>
    <S sid="297" ssid="196">This approach is used for the French-to-English translation experiments presented in this article.</S>
    <S sid="298" ssid="197">Another approach for mapping a single source language word to several target language words involves preprocessing by the word-joining algorithm given in Tillmann (2001), which is similar to the approach presented in Och, Tillmann, and Ney (1999).</S>
    <S sid="299" ssid="198">Target words are joined during a training phase, and several joined target language words are dealt with as a new lexicon entry.</S>
    <S sid="300" ssid="199">This approach is used for the German-to-English translation experiments presented in this article.</S>
    <S sid="301" ssid="200">In order to deal with the IBM-4 fertility parameters within the DP-based concept, we adopt the distinction between open and closed hypotheses given in Berger et al. (1996).</S>
    <S sid="302" ssid="201">A hypothesis is said to be open if it is to be aligned with more source positions than it currently is (i.e., at least two).</S>
    <S sid="303" ssid="202">Otherwise it is called closed.</S>
    <S sid="304" ssid="203">The difference between open and closed is used to process the input sentence one position a time (for details see Tillmann 2001).</S>
    <S sid="305" ssid="204">The word reordering restrictions and the beam search pruning techniques are directly carried over to the full set of IBM-4 parameters, since they are based on restrictions on the coverage vectors C only.</S>
    <S sid="306" ssid="205">To ensure its correctness, the implementation was tested by carrying out forced alignments on 500 German-to-English training sentence pairs.</S>
    <S sid="307" ssid="206">In a forced alignment, the source sentence fJ1 and the target sentence eI1 are kept fixed, and a full search without re-ordering restrictions is carried out only over the unknown alignment aJ1.</S>
    <S sid="308" ssid="207">The language model probability is divided out, and the resulting probability is compared to the Viterbi probability as obtained by the training procedure.</S>
    <S sid="309" ssid="208">For 499 training sentences the Viterbi alignment probability as obtained by the forced-alignment search was exactly the same as the one produced by the training procedure.</S>
    <S sid="310" ssid="209">In one case the forcedalignment search did obtain a better Viterbi probability than the training procedure.</S>
  </SECTION>
  <SECTION title="4." number="5">
    <S sid="311" ssid="1">Translation experiments are carried out for the translation directions German to English and English to German (Verbmobil task) and for the translation directions French to English and English to French (Canadian Hansards task).</S>
    <S sid="312" ssid="2">Section 4.1 reports on the performance measures used.</S>
    <S sid="313" ssid="3">Section 4.2 shows translation results for the Verbmobil task.</S>
    <S sid="314" ssid="4">Sections 4.2.1 and 4.2.2 describe that task and the preprocessing steps applied.</S>
    <S sid="315" ssid="5">In Sections 4.2.3 through 4.2.5, the efficiency of the beam search pruning techniques is shown for German-to-English translation, as the most detailed experiments are conducted for that direction.</S>
    <S sid="316" ssid="6">Section 4.2.6 gives translation results for the translation direction English to German.</S>
    <S sid="317" ssid="7">In Section 4.3, translation results for the Canadian Hansards task are reported.</S>
    <S sid="318" ssid="8">To measure the performance of the translation methods, we use three types of automatic and easy-to-use measures of the translation errors.</S>
    <S sid="319" ssid="9">Additionally, a subjective evaluation involving human judges is carried out (Niessen et al. 2000).</S>
    <S sid="320" ssid="10">The following evaluation criteria are employed: performed to convert the generated string into the reference target string.</S>
    <S sid="321" ssid="11">This performance criterion is widely used in speech recognition.</S>
    <S sid="322" ssid="12">The minimum is computed using a DP algorithm and is typically referred to as edit or Levenshtein distance.</S>
    <S sid="323" ssid="13">evaluation measures subjective judgments by test persons are carried out (Niessen et al. 2000).</S>
    <S sid="324" ssid="14">The following scale for the error count per sentence is used in these subjective evaluations: Each translated sentence is judged by a human examiner according to the above error scale; several human judges may be involved in judging the same translated sentence.</S>
    <S sid="325" ssid="15">Subjective evaluation is carried out only for the Verbmobil TEST-147 test set.</S>
    <S sid="326" ssid="16">4.2.1 The Task and the Corpus.</S>
    <S sid="327" ssid="17">The translation system is tested on the Verbmobil task (Wahlster 2000).</S>
    <S sid="328" ssid="18">In that task, the goal is the translation of spontaneous speech in faceto-face situations for an appointment scheduling domain.</S>
    <S sid="329" ssid="19">We carry out experiments for both translation directions: German to English and English to German.</S>
    <S sid="330" ssid="20">Although the Verbmobil task is still a limited-domain task, it is rather difficult in terms of vocabulary size, namely, about 5,000 words or more for each of the two languages; second, the syntactic structures of the sentences are rather unrestricted.</S>
    <S sid="331" ssid="21">Although the ultimate goal of the Verbmobil project is the translation of spoken language, the input used for the translation experiments reported on in this article is mainly the (more or less) correct orthographic transcription of the spoken sentences.</S>
    <S sid="332" ssid="22">Thus, the effects of spontaneous speech are present in the corpus; the effect of speech recognition errors, however, is not covered.</S>
    <S sid="333" ssid="23">The corpus consists of 58,073 training pairs; its characteristics are given in Table 5.</S>
    <S sid="334" ssid="24">For the translation experiments, a trigram language model with a perplexity of 28.1 is used.</S>
    <S sid="335" ssid="25">The following two test corpora are used for the translation experiments: TEST-331: This test set consists of 331 test sentences.</S>
    <S sid="336" ssid="26">Only automatic evaluation is carried out on this test corpus: The WER and the mWER are computed.</S>
    <S sid="337" ssid="27">For each test sentence in the source language there is a range of acceptable reference translations (six on average) provided by a human translator, who is asked to produce word-to-word translations wherever it is possible.</S>
    <S sid="338" ssid="28">Part of the reference sentences are obtained by correcting automatic translations of the test sentences that are produced using the approach presented in this article with different reordering constraints.</S>
    <S sid="339" ssid="29">The other part is produced from the source sentences without looking at any of their translations.</S>
    <S sid="340" ssid="30">The TEST-331 test set is used as held-out data for parameter optimization (for the language mode scaling factor and for the distortion model scaling factor).</S>
    <S sid="341" ssid="31">Furthermore, the beam search experiments in which the effect of the different pruning thresholds is demonstrated are carried out on the TEST-331 test set.</S>
    <S sid="342" ssid="32">TEST-147: The second, separate test set consists of 147 test sentences.</S>
    <S sid="343" ssid="33">Translation results are given in terms of mWER and SSER.</S>
    <S sid="344" ssid="34">No parameter optimization is carried out on the TEST-147 test set; the parameter values as obtained from the experiments on the TEST-331 test set are used.</S>
    <S sid="345" ssid="35">4.2.2 Preprocessing Steps.</S>
    <S sid="346" ssid="36">To improve the translation performance the following preprocessing steps are carried out: Categorization: We use some categorization, which consists of replacing a single word by a category.</S>
    <S sid="347" ssid="37">The only words that are replaced by a category label are proper nouns denoting German cities.</S>
    <S sid="348" ssid="38">Using the new labeled corpus, all probability models are trained anew.</S>
    <S sid="349" ssid="39">To produce translations in the &#8220;normal&#8221; language, the categories are translated by rule and are inserted into the target sentence.</S>
    <S sid="350" ssid="40">Word joining: Target language words are joined using a method similar to the one described in Och, Tillmann, and Ney (1999).</S>
    <S sid="351" ssid="41">Words are joined to handle cases like the German compound noun &#8220;Zahnarzttermin&#8221; for the English &#8220;dentist&#8217;s appointment,&#8221; because a single word has to be mapped to two or more target words.</S>
    <S sid="352" ssid="42">The word joining is applied only to the target language words; the source language sentences remain unchanged.</S>
    <S sid="353" ssid="43">During the search process several joined target language words may be generated by a single source language word.</S>
    <S sid="354" ssid="44">Manual lexicon: To account for unseen words in the test sentences and to obtain a greater number of focused translation probabilities p(f  |e), we use a bilingual German-English dictionary.</S>
    <S sid="355" ssid="45">For each word e in the target vocabulary, we create a list of source translations f according to this dictionary.</S>
    <S sid="356" ssid="46">The translation probability pdic(f  |e) for the dictionary entry (f, e) is defined as where Ne is the number of source words listed as translations of the target word e. The dictionary probability pdic(f  |e) is linearly combined with the automatically trained translation probabilities paut(f  |e) to obtain smoothed probabilities p(f  |e): p(f  |e) = (1 &#8722; A) - pdic(f  |e) + A - paut(f  |e) For the translation experiments, the value of the interpolation parameter is fixed at A = 0.5.</S>
    <S sid="357" ssid="47">4.2.3 Effect of the Scaling Factors.</S>
    <S sid="358" ssid="48">In speech recognition, in which Bayes&#8217; decision rule is applied, a language model scaling factor &#945;LM is used; a typical value is &#945;LM ,: 15.</S>
    <S sid="359" ssid="49">This scaling factor is employed because the language model probabilities are more reliably estimated than the acoustic probabilities.</S>
    <S sid="360" ssid="50">Following this use of a language model scaling factor in speech recognition, such a factor is introduced into statistical MT, too.</S>
    <S sid="361" ssid="51">The optimization criterion in equation (1) is modified as follows: where p(eI1) is the language model probability of the target language sentence.</S>
    <S sid="362" ssid="52">In the experiments presented here, a trigram language model is used to compute p(eI1).</S>
    <S sid="363" ssid="53">The effect of the language model scaling factor &#945;LM is studied on the TEST-331 test set.</S>
    <S sid="364" ssid="54">A minimum mWER is obtained for &#945;LM = 0.8, as reported in Tillmann (2001).</S>
    <S sid="365" ssid="55">Unlike in speech recognition, the translation model probabilities seem to be estimated as reliably as the language model probabilities in statistical MT.</S>
    <S sid="366" ssid="56">A second scaling factor &#945;D is introduced for the distortion model probabilities p(j I j&#65533;,J).</S>
    <S sid="367" ssid="57">A minimum mWER is obtained for &#945;D = 0.4, as reported in Tillmann (2001).</S>
    <S sid="368" ssid="58">The WER and mWER on the TEST-331 test set increase significantly, if no distortion probability is used, for the case &#945;D = 0.0.</S>
    <S sid="369" ssid="59">The benefit of a distortion probability scaling factor of &#945;D = 0.4 comes from the fact that otherwise, a low distortion probability might suppress long-distant word reordering that is important for German-to-English verb group reordering.</S>
    <S sid="370" ssid="60">The setting &#945;LM = 0.8 and &#945;D = 0.4 is used for all subsequent translation results (including the translation direction English to German). mWER, and SSER on the TEST-147 test set as a function of three reordering constraints: MON, GE, and S3 (as discussed in the Appendix).</S>
    <S sid="371" ssid="61">The computing time is given in terms of central processing unit (CPU) time per sentence (on a 450 MHz Pentium III personal computer).</S>
    <S sid="372" ssid="62">For the SSER, it turns out that restricting the word reordering such that it may not cross punctuation marks improves translation performance significantly.</S>
    <S sid="373" ssid="63">The average length of the sentence fragments that are separated by punctuation marks is rather small: 4.5 words per fragment.</S>
    <S sid="374" ssid="64">A coverage pruning threshold of tc = 5.0 and an observation pruning of no = 50 are applied during the experiments.4 No other type of pruning is used.5 The MON constraint performs worst in terms of both mWER and SSER.</S>
    <S sid="375" ssid="65">The computing time is small, since no reordering is carried out.</S>
    <S sid="376" ssid="66">Constraints GE and S3 perform nearly identically in terms of both mWER and SSER.</S>
    <S sid="377" ssid="67">The GE constraint, however, works about three times as fast as the S3 constraint.</S>
    <S sid="378" ssid="68">Table 7 shows example translations obtained under the three different reordering constraints.</S>
    <S sid="379" ssid="69">Again, the MON reordering constraint performs worst.</S>
    <S sid="380" ssid="70">In the second and third translation examples, the S3 word reordering constraint performs worse than the GE reordering constraint, since it cannot take the word reordering due to the German verb group properly into account.</S>
    <S sid="381" ssid="71">The German finite verbs bin (second example) and k&#168;onnten (third example) are too far away from the personal pronouns ich and Sie (six and five source sentence positions, respectively) to be reordered properly.</S>
    <S sid="382" ssid="72">In the last example, the less restrictive S3 reordering constraint leads to a better translation; the GE translation is still acceptable, though. beam search pruning is demonstrated.</S>
    <S sid="383" ssid="73">Translation results on the TEST-331 test set are presented to evaluate the effectiveness of the pruning techniques.6 The quality of the search algorithm with respect to the GE and S3 reordering constraints is evaluated using two criteria: 1.</S>
    <S sid="384" ssid="74">The number of search errors for a certain combination of pruning thresholds is counted.</S>
    <S sid="385" ssid="75">A search error occurs for a test sentence if the final translation probability QF for a candidate translation eI1 as given in equation (4) is smaller than a reference probability for that test sentence.</S>
    <S sid="386" ssid="76">We will compute reference probabilities two ways, as explained below.</S>
    <S sid="387" ssid="77">Effect of the coverage pruning threshold tc on the number of search errors and mWER on the TEST-331 test set (no cardinality pruning carried out: t, = &#8734;).</S>
    <S sid="388" ssid="78">A cardinality histogram pruning of 200,000 is applied to restrict the maximum overall size of the search space.</S>
    <S sid="389" ssid="79">The negative logarithm of tc is reported. leads to a higher word error rate, since the optimal path through the translation lattice is missed, resulting in translation errors.</S>
    <S sid="390" ssid="80">Two automatically generated reference probabilities are used.</S>
    <S sid="391" ssid="81">These probabilities are computed separately for the reordering constraints GE and S3 (the difference is not shown in the notation, but will be clear from the context): Qref: A forced alignment is carried out between each of the test sentences and its corresponding reference translation; only a single reference translation for each test sentence is used.</S>
    <S sid="392" ssid="82">The probability obtained for the reference translation is denoted by Qref.</S>
    <S sid="393" ssid="83">QF&#8727;: A translation is carried out with conservatively large pruning thresholds, yielding a translation close to the one with the maximum translation probability.</S>
    <S sid="394" ssid="84">The translation probability for that translation is denoted by QF&#8727;.</S>
    <S sid="395" ssid="85">First, in a series of experiments we study the effect of the coverage and cardinality pruning for the reordering constraints GE and S3.</S>
    <S sid="396" ssid="86">(When we report on the different pruning thresholds, we will show the negative logarithm of those pruning thresholds.)</S>
    <S sid="397" ssid="87">The experiments are carried out on two different pruning &#8220;dimensions&#8221;: Both tables use an observation pruning of no = 50.</S>
    <S sid="398" ssid="88">The effect of the coverage pruning threshold tc is demonstrated in Table 8.</S>
    <S sid="399" ssid="89">For the translation experiments reported in this table, the cardinality pruning threshold is set to t, = oo; thus, no comparison between partial hypotheses that do not cover the same set C of source sentence positions is carried out.</S>
    <S sid="400" ssid="90">To restrict the overall size of the search space in terms of CPU time and memory requirements, a cardinality pruning of n, = 200,000 is applied.</S>
    <S sid="401" ssid="91">As can be seen from Table 8, mWER and the number of search errors decrease significantly as the coverage pruning threshold tc increases.</S>
    <S sid="402" ssid="92">For the GE reordering constraint, mWER decreases from 73.5% to 24.9%.</S>
    <S sid="403" ssid="93">For a coverage pruning threshold tc &#8805; 5.0, mWER remains nearly constant at 25.0%, although search errors still occur.</S>
    <S sid="404" ssid="94">For the S3 reordering constraint, mWER decreases from 70.0% to 28.3%.</S>
    <S sid="405" ssid="95">The largest coverage threshold tested for the S3 constraint is tc = 5.0, since for larger threshold values tc, the search procedure cannot be carried out because of memory and time restrictions.</S>
    <S sid="406" ssid="96">The number of search errors is reduced as the coverage pruning threshold is increased.</S>
    <S sid="407" ssid="97">It turns out to be difficult to verify search errors by looking at the reference translation probabilities Qref alone.</S>
    <S sid="408" ssid="98">The translation with the maximum translation probability seems to be quite narrowly defined.</S>
    <S sid="409" ssid="99">The coverage pruning is more effective for the GE constraint than for the S3 constraint, since the overall search space for the GE reordering is smaller.</S>
    <S sid="410" ssid="100">Table 9 shows the effect of the cardinality pruning threshold t, on mWER when no coverage pruning is carried out (a histogram coverage pruning of 1,000 is applied to restrict the overall size of the search space).</S>
    <S sid="411" ssid="101">The cardinality threshold t, has a strong effect on mWER, which decreases significantly as the cardinality threshold t, increases.</S>
    <S sid="412" ssid="102">For the GE reordering constraint, mWER decreases from 48.5% to 24.9%; for the S3 reordering constraint, mWER decreases from 51.4% to 28.2%.</S>
    <S sid="413" ssid="103">For the coverage threshold t = 15.0, the GE constraint works about four times as fast as the S3 constraint, since the overall search space for the S3 constraint is much larger.</S>
    <S sid="414" ssid="104">Although the overall search space is much larger for the S3 constraint, for smaller values of the coverage threshold tc &#8804; 5.0, the S3 constraint works as fast as the GE constraint or even faster, because only a very small portion of the overall search space is searched for small values of the cardinality pruning threshold t,.</S>
    <S sid="415" ssid="105">There is some computational overhead in expanding a partial hypothesis for the GE constraint because the finite-state control has to be handled.</S>
    <S sid="416" ssid="106">No results are obtained for the S3 constraint and the coverage threshold t, = 17.5 because of memory restrictions.</S>
    <S sid="417" ssid="107">The number of search errors is reduced as the cardinality pruning threshold is increased.</S>
    <S sid="418" ssid="108">Again, it is difficult to verify search errors by looking at the reference translation probabilities alone.</S>
    <S sid="419" ssid="109">Both coverage and cardinality pruning are more efficient for the GE reordering constraint than for the S3 reordering constraint.</S>
    <S sid="420" ssid="110">For the S3 constraint, no translation results are obtained for a coverage threshold t, &gt; 5.0 without cardinality pruning applied because of memory and computing time restrictions.</S>
    <S sid="421" ssid="111">For the GE constraint virtually a full search can be carried out where only observation pruning is applied: Identical target translations and translation probabilities are produced for the hypothesis files for the two cases (1) tc = 10.0, t, = &#8734;, and (2) tc = &#8734;, t, = 15.0.</S>
    <S sid="422" ssid="112">(Actually, for one test sentence in the TEST-331 test set, the translations are different, although the translation probabilities are exactly the same.)</S>
    <S sid="423" ssid="113">Since the pruning is carried out independently on two different pruning dimensions, no search errors will occur if the thresholds are further increased.</S>
    <S sid="424" ssid="114">Table 10 shows the effect of the observation pruning parameter no on mWER for the reordering constraint GE. mWER is significantly reduced by hypothesizing up to the best 50 target words a for a source language word f. mWER increases from 24.9% to 29.3% when the number of hypothesized words is decreased to only a single word.</S>
    <S sid="425" ssid="115">Table 11 demonstrates the effect of the combination of the coverage pruning threshold tc = 5.0 and the cardinality pruning threshold t, = 12.5, where the actual values are found in informal experiments: In a typical setting of the two parameters t, should be at least twice as big as tc.</S>
    <S sid="426" ssid="116">For the GE reordering constraint, the average computing time is about seven seconds per sentence without any loss in translation performance as measured in terms of mWER.</S>
    <S sid="427" ssid="117">For the S3 reordering constraint, the average computing time per sentence is 27 seconds.</S>
    <S sid="428" ssid="118">Again, the combination of coverage and cardinality pruning works more efficiently for the GE constraint.</S>
    <S sid="429" ssid="119">The memory requirement for the algorithm is about 100 MB.</S>
    <S sid="430" ssid="120">4.2.6 English-to-German Translation Experiments.</S>
    <S sid="431" ssid="121">A series of translation experiments for the translation direction English to German are also carried out.</S>
    <S sid="432" ssid="122">The results, given Demonstration of the combination of the two pruning thresholds tc = 5.0 and t, = 12.5 to speed up the search process for the two reordering constraints GE and S3 (no = 50).</S>
    <S sid="433" ssid="123">The translation performance is shown in terms of mWER on the TEST-331 test set. in terms of WER and PER, are shown in Table 12.</S>
    <S sid="434" ssid="124">For the English-to-German translation direction, a single reference translation for each test sentence is used to carry out the automatic evaluation.</S>
    <S sid="435" ssid="125">The translation task for the translation direction English to German is more difficult than for the translation direction German to English; the trigram language model perplexity increases from 38.3 to 68.2 on the TEST-331 test set, as can be seen in Table 5.</S>
    <S sid="436" ssid="126">No parameter optimization is carried out for this translation direction; the parameter settings are carried over from the results obtained in Table 11.</S>
    <S sid="437" ssid="127">The word error rates for the translation direction English to German are significantly higher than those for the translation direction German to English.</S>
    <S sid="438" ssid="128">There are several reasons for this: German vocabulary and perplexity are significantly larger than those for English, and only a single reference translation per test sentence is available for English-to-German translation.</S>
    <S sid="439" ssid="129">There is only a very small difference in terms of word error rates for the reordering constraints EG and S3; in particular, WER is 70.1% for both.</S>
    <S sid="440" ssid="130">The reordering constraint MON performs slightly worse: WER increases to 70.6%, and PER increases to 57.0%.</S>
    <S sid="441" ssid="131">Table 13 shows translation examples for the translation direction English to German.</S>
    <S sid="442" ssid="132">The MON constraint performs worst; there is no significant difference in quality of translations produced under the EG and the S3 constraints.</S>
    <S sid="443" ssid="133">4.3.1 The Task and the Corpus.</S>
    <S sid="444" ssid="134">The second corpus on which we perform translation experiments is the Hansard corpus.</S>
    <S sid="445" ssid="135">By law, the proceedings of the Canadian parliament are recorded in both French and English.</S>
    <S sid="446" ssid="136">(For historical reasons, these proceedings are called &#8220;Hansards.&#8221;) The remarks of the parliament members are written down in whichever of the two languages they use.</S>
    <S sid="447" ssid="137">They are then translated into the other language to produce complete sets of the proceedings, one in French and the other in English.</S>
    <S sid="448" ssid="138">The resulting bilingual data have been sentence-aligned using statistical methods (Brown et al. 1990).</S>
    <S sid="449" ssid="139">Originally, about three million sentences were selected.</S>
    <S sid="450" ssid="140">Here, we use a subset of the original training data; the details regarding this subset are given in Table 14.</S>
    <S sid="451" ssid="141">The Hansards corpus presents by far a more difficult task than the Verbmobil corpus in terms of vocabulary size and number of training sentences.</S>
    <S sid="452" ssid="142">The training and test sentences are less restrictive than for the Verbmobil task.</S>
    <S sid="453" ssid="143">For the translation experiments on the Hansards corpus, no word joining is carried out.</S>
    <S sid="454" ssid="144">Two target words can be produced by a single source word, as described in Section 3.9.2.</S>
    <S sid="455" ssid="145">4.3.2 Translation Results.</S>
    <S sid="456" ssid="146">As can be seen in Table 15 for the translation direction French to English and in Table 16 for the translation direction English to French, the word error rates are rather high compared to those for the Verbmobil task.</S>
    <S sid="457" ssid="147">The reason for the higher error rates is that, as noted in the previous section, the Hansards task is by far less restrictive than the Verbmobil task, and the vocabulary size is much larger.</S>
    <S sid="458" ssid="148">There is only a slight difference in performance between the MON and the S3 reordering constraints on the Hansards task.</S>
    <S sid="459" ssid="149">The computation time is also rather high compared to the Verbmobil task: For the S3 constraint, the average translation time is about 3 minutes per sentence for the translation direction English to French and about 10 minutes per sentence for the translation direction French to English.</S>
    <S sid="460" ssid="150">The following parameter setting is used for the experiment conducted here: tc = 5.0, t, = 10.0, nc = 250, and to = 12.</S>
    <S sid="461" ssid="151">(The actual parameters are chosen in informal experiments to obtain reasonable CPU times while permitting only a small number of search errors.)</S>
    <S sid="462" ssid="152">No cardinality histogram pruning is carried out.</S>
    <S sid="463" ssid="153">As for the Germanto-English translation experiments, word reordering is restricted so that it may not cross punctuation boundaries.</S>
    <S sid="464" ssid="154">The resulting fragment lengths are much larger for the translation direction English to French, and still larger for the translation direction French to English, when compared to the fragment lengths for the translation direction German to English, hence the high CPU times.</S>
    <S sid="465" ssid="155">In an additional experiment for the translation direction French to English and the reordering constraint S3, we find we can speed up the translation time to about 18 seconds per sentence by using the following parameter setting: tc = 3.0, t, = 7.5, nc = 20, n, = 400, and no = 5.</S>
    <S sid="466" ssid="156">For the resulting hypotheses file, PER increases only slightly, from 51.4% to 51.6%.</S>
    <S sid="467" ssid="157">Translation examples for the translation direction French to English under the S3 reordering constraint are given in Table 17.</S>
    <S sid="468" ssid="158">The French input sentences show some preprocessing that is carried out beforehand to simplify the translation task (e.g., des is transformed into de les and l&#8217;est is transformed into le est).</S>
    <S sid="469" ssid="159">The translations produced are rather approximative in some cases, although the general meaning is often preserved.</S>
  </SECTION>
  <SECTION title="5." number="6">
    <S sid="470" ssid="1">We have presented a DP-based beam search algorithm for the IBM-4 translation model.</S>
    <S sid="471" ssid="2">The approach is based on a DP solution to the TSP, and it gains efficiency by imposing constraints on the allowed word reorderings between source and target language.</S>
    <S sid="472" ssid="3">A data-driven search organization in conjunction with appropriate pruning techniques S3 I have the intention of speaking today about the many improvements in pensions for all Canadians especially those programs.</S>
    <S sid="473" ssid="4">Input Chacun en lui - m&#710;eme est tr`es complexe et le lien entre les deux le est encore davantage de sorte que pour beaucoup la situation pr&#180;esente est confuse.</S>
    <S sid="474" ssid="5">S3 Each in itself is very complex and the relationship between the two is more so much for the present situation is confused. is proposed.</S>
    <S sid="475" ssid="6">For the medium-sized Verbmobil task, a sentence can be translated in a few seconds on average, with a small number of search errors and no performance degradation as measured by the word error criterion used.</S>
    <S sid="476" ssid="7">Word reordering is parameterized using a set of four parameters, in such a way that it can easily be adopted to new translation directions.</S>
    <S sid="477" ssid="8">A finite-state control is added, and its usefulness is demonstrated for the translation direction German to English, in which the word order difference between the two languages is mainly due to the German verb group.</S>
    <S sid="478" ssid="9">Future work might aim at a tighter integration of the IBM-4 model distortion probabilities and the finite-state control; the finite-state control itself may be learned from training data.</S>
    <S sid="479" ssid="10">The applicability of the algorithm applied in the experiments in this article is not restricted to the IBM translation models or to the simplified translation model used in the description of the algorithm in Section 3.</S>
    <S sid="480" ssid="11">Since the efficiency of the beam search approach is based on restrictions on the allowed coverage vectors C alone, the approach may be used for different types of translation models as well (e.g., for the multiword-based translation model proposed in Och, Tillmann, and Ney [1999]).</S>
    <S sid="481" ssid="12">On the other hand, since the decoding problem for the IBM-4 translation model is provably NP-complete, as shown in Knight (1999) and Germann et al. (2001), word reordering restrictions as introduced in this article are essential for obtaining an efficient search algorithm that guarantees that a solution close to the optimal one will be found.</S>
    <S sid="482" ssid="13">To quantify the reordering restrictions in Section 3.5, the four non-negative numbers numskip, widthskip, nummove, and widthmove are used (widthskip corresponds to L, widthmove corresponds to R in Section 3.4; here, we use a more intuitive notation).</S>
    <S sid="483" ssid="14">Within the implementation of the DP search, the restrictions are provided to the algorithm as an input parameter of the following type: S numskip widthskip M nummove widthmove The meaning of the reordering string is as follows: The two numbers following S that are separated by an underscore describe the way words may be skipped; the two numbers following M that are separated by an underscore describe the way words may be moved during word reordering.</S>
    <S sid="484" ssid="15">The first number after S and M denotes the number of positions that may be skipped or moved, respectively (e.g., for the translation direction German to English [GE in the chart below], one position may be skipped and two positions may be moved).</S>
    <S sid="485" ssid="16">The second number after S and M restricts the distance a word may be skipped or moved, respectively.</S>
    <S sid="486" ssid="17">These &#8220;width&#8221; parameters restrict the word reordering to take place within a &#8220;window&#8221; of a certain size, established by the distance between the positions lmin(C) and rmax(C) as defined in Section 3.5.</S>
    <S sid="487" ssid="18">In the notation, either the substring headed by S or that headed by M (or both) may be omitted altogether to indicate that the corresponding reordering is not allowed.</S>
    <S sid="488" ssid="19">Any numerical value in the string may be set to INF, denoting that an arbitrary number of positions may be skipped/moved or that the moving or skipping distance may be arbitrarily large.</S>
    <S sid="489" ssid="20">The following reordering strings are used in this article: Word reordering Description string e The empty string denotes the reordering restriction in which (short: MON) no reordering is allowed.</S>
    <S sid="490" ssid="21">S 01 04 M 02 10 This string describes the German-to-English word reordering.</S>
    <S sid="491" ssid="22">(short: GE) Up to one word may be skipped for at most 4 positions, and up to two words may be moved up to 10 positions.</S>
    <S sid="492" ssid="23">S 02 10 M 01 04 This string describes the English-to-German word reordering.</S>
    <S sid="493" ssid="24">(short: EG) Up to two words may be skipped for at most 10 positions and up to one word may be moved for up to 4 positions.</S>
    <S sid="494" ssid="25">S 03 INF This string describes the IBM-style word reordering (short: S3) given in Section 3.6.</S>
    <S sid="495" ssid="26">Up to three words may be skipped for an unrestricted number of positions.</S>
    <S sid="496" ssid="27">No words may be moved.</S>
    <S sid="497" ssid="28">S INF INF or These strings denote the word re-ordering without M INF INF restrictions.</S>
    <S sid="498" ssid="29">(short: NO) The word reordering strings can be directly used as input parameters to the DP-based search procedure to test different reordering restrictions within a single implementation.</S>
    <S sid="499" ssid="30">Acknowledgments 01 IV 601 A) by the German Federal This work has been supported as part of the Ministry of Education, Science, Research Verbmobil project (contract number and Technology and as part of the Eutrans</S>
  </SECTION>
</PAPER>
