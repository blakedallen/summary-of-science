[
  {
    "citance_No": 1, 
    "citing_paper_id": "P04-1066", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Robert C., Moore", 
    "raw_text": "Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model", 
    "clean_text": "Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P04-1066", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Robert C., Moore", 
    "raw_text": "alignments, as described by Och and Ney (2003)", 
    "clean_text": "The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P04-1066", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Robert C., Moore", 
    "raw_text": "We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003)", 
    "clean_text": "We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P04-1066", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Robert C., Moore", 
    "raw_text": "It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models", 
    "clean_text": "It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P04-1066", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Robert C., Moore", 
    "raw_text": "However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since theViterbi word alignment that AER is based on is seldom used in applications of Model 1.5 Moreover, it is notable that while the versions of Model 1 having the lowest AER have dramatically higher precision than the standard version, they also have quite a bit lower recall", 
    "clean_text": "However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W12-0704", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Sara, Stymne", 
    "raw_text": "The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005)", 
    "clean_text": "The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W12-0704", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Sara, Stymne", 
    "raw_text": "The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977)", 
    "clean_text": "The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "H05-1085", 
    "citing_paper_authority": 34, 
    "citing_paper_authors": "Sharon, Goldwater | David, McClosky", 
    "raw_text": "Our translation models were trained usingGIZA++ (Och and Ney, 2003), which we modi 1Although we did not use it for the experiments in this paper, the PCEDT corpus does contain lemma information for the English data", 
    "clean_text": "Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "E12-1055", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Rico, Sennrich", 
    "raw_text": "To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training ,i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003)", 
    "clean_text": "To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "E12-1055", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Rico, Sennrich", 
    "raw_text": "The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide", 
    "clean_text": "The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "E12-1055", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Rico, Sennrich", 
    "raw_text": "All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for re sampling and significance testing. We compare three baselines and four translation model mixture techniques", 
    "clean_text": "All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W12-4202", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Rui, Wang | Petya, Osenova | Kiril, Simov", 
    "raw_text": "Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table", 
    "clean_text": "Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W12-3131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Michael, Denkowski | Greg, Hanneman | Alon, Lavie", 
    "raw_text": "Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003)", 
    "clean_text": "Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W12-3131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Michael, Denkowski | Greg, Hanneman | Alon, Lavie", 
    "raw_text": "The system translates from cased French to cased English; at no point do we lowercase data. The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008 )implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003)", 
    "clean_text": "The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W10-0704", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Qin, Gao | Stephan, Vogel", 
    "raw_text": "GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters", 
    "clean_text": "GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D10-1091", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Guillaume, Wisniewski | Alexandre, Allauzen | Fran&ccedil;ois, Yvon", 
    "raw_text": "In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights", 
    "clean_text": "In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P11-2029", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jinxi, Xu | Jinying, Chen", 
    "raw_text": "Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner", 
    "clean_text": "Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P11-2029", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jinxi, Xu | Jinying, Chen", 
    "raw_text": "The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003", 
    "clean_text": "The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P11-1104", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Zhanyi, Liu | Haifeng, Wang | Hua, Wu | Ting, Liu | Sheng, Li", 
    "raw_text": "Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection", 
    "clean_text": "Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P14-1082", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Maarten van, Gompel | Antal, van den Bosch", 
    "raw_text": "ItinvokesGIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Mod els and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003)", 
    "clean_text": "It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003).", 
    "keep_for_gold": 0
  }
]
