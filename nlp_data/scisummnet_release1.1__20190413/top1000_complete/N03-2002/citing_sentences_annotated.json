[
  {
    "citance_No": 1, 
    "citing_paper_id": "N04-4034", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Gang, Ji | Jeff A., Bilmes", 
    "raw_text": "This is reasonable since one can often predict a speaker? s word after it starts but before it completes. We score using the model P (wt|wt? 1, wt? 2, at) .1 Different back-off strategies, including different back-off paths as well as combination methods (Bilmes and Kirchhoff, 2003), were tried and here we present the best results", 
    "clean_text": "Different back-off strategies, including different back-off paths as well as combination methods (Bilmes and Kirchhoff, 2003), were tried and here we present the best results.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "N04-4034", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Gang, Ji | Jeff A., Bilmes", 
    "raw_text": "We compare an optimized four-gram, a three gram baseline, and various numbers of cluster sizes using our MCMI method and generalized back off (Bilmes and Kirchhoff, 2003), which, (again) with 500 clusters, achieves an 8.9% relative improvement over the trigram", 
    "clean_text": "We compare an optimized four-gram, a three gram baseline, and various numbers of cluster sizes using our MCMI method and generalized back off (Bilmes and Kirchhoff, 2003), which, (again) with 500 clusters, achieves an 8.9% relative improvement over the trigram.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P13-2037", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Heike, Adel | Ngoc Thang, Vu | Tanja, Schultz", 
    "raw_text": "In (Bilmes and Kirchhoff, 2003), it is shown that factored language models are able to outperform standard n-gram techniques in terms of perplexity", 
    "clean_text": "In (Bilmes and Kirchhoff, 2003), it is shown that factored language models are able to outperform standard n-gram techniques in terms of perplexity.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P10-1157", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Fran&ccedil;ois, Mairesse | Milica, Ga&scaron;i&#x107; | Filip, Jur&#x10D;&iacute;&#x10D;ek | Simon, Keizer | Blaise, Thomson | Kai, Yu | Steve, Young", 
    "raw_text": "t=1 P (rt|rt? 1, ht ,lt? 1 ,lt ,lt+1, s? t? 1, s? t, s ?t+1) (8) Conditional probability distributions are represented as factored language models smoothed using Witten-Bell interpolated back off smoothing (Bilmes and Kirchhoff, 2003), according to the back off graphs in Fig", 
    "clean_text": "Conditional probability distributions are represented as factored language models smoothed using Witten-Bell interpolated back off smoothing (Bilmes and Kirchhoff, 2003), according to the backoff graphs in Fig.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W07-0735", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Ond&#x159;ej, Bojar", 
    "raw_text": "Class-based LMs (Brown et al, 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario", 
    "clean_text": "Class-based LMs (Brown et al, 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "N06-1036", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Gang, Ji | Jeff A., Bilmes", 
    "raw_text": "In (Ji and Bilmes, 2005), for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a generalized back off procedures (Bilmes and Kirchhoff, 2003)", 
    "clean_text": "In (Ji and Bilmes, 2005), for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a generalized backoff procedures (Bilmes and Kirchhoff, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "N06-1036", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Gang, Ji | Jeff A., Bilmes", 
    "raw_text": "In deed, our DA-specific word models (implemented via back off) will also need to condition on the cur rent sub-DA, which at training time is unknown. We therefore have developed a procedure that al lows us to train generalized back off models (Bilmes and Kirchhoff, 2003), even when some or all of the variables involved in the model are hidden", 
    "clean_text": "We therefore have developed a procedure that allows us to train generalized backoff models (Bilmes and Kirchhoff, 2003), even when some or all of the variables involved in the model are hidden.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N06-1036", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Gang, Ji | Jeff A., Bilmes", 
    "raw_text": "Because all variables are observed when training our baseline, we use the SRILM toolkit (Stolcke, 2002), modified Kneser-Ney smoothing (Chen and Goodman, 1998), and factored extensions (Bilmesand Kirchhoff, 2003)", 
    "clean_text": "Because all variables are observed when training our baseline, we use the SRILM toolkit (Stolcke, 2002), modified Kneser-Ney smoothing (Chen and Goodman, 1998), and factored extensions (Bilmesand Kirchhoff, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N06-1036", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Gang, Ji | Jeff A., Bilmes", 
    "raw_text": "We use the SRILM toolkit with ex tensions (Bilmes and Kirchhoff, 2003) to train, and use GMTK (Bilmes and Zweig, 2002) for decoding", 
    "clean_text": "We use the SRILM toolkit with extensions (Bilmes and Kirchhoff, 2003) to train, and use GMTK (Bilmes and Zweig, 2002) for decoding.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D09-1116", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Denis, Filimonov | Mary P., Harper", 
    "raw_text": "The Factored Language Model (FLM) (Bilmes and Kirchhoff, 2003) offers a convenient view of the input data: it represents every word in a sentence as a tuple of factors", 
    "clean_text": "The Factored Language Model (FLM) (Bilmes and Kirchhoff, 2003) offers a convenient view of the input data: it represents every word in a sentence as a tuple of factors.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "N12-1043", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Thomas, Mueller | Hinrich, Schuetze | Helmut, Schmid", 
    "raw_text": "Both approaches are essentially a simple form of a factored language model (FLM) (Bilmes and Kirchhoff, 2003)", 
    "clean_text": "Both approaches are essentially a simple form of a factored language model (FLM) (Bilmes and Kirchhoff, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W05-0821", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Katrin, Kirchhoff | Mei, Yang", 
    "raw_text": "A factored language model (FLM) (Bilmes and Kirchhoff, 2003) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework", 
    "clean_text": "A factored language model (FLM) (Bilmes and Kirchhoff, 2003) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P11-2077", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Viet, Ha Thuc | Nicola, Cancedda", 
    "raw_text": "This work is related to several existing directions: generative factored language model, discriminative language models, on line passive-aggressive learning and confidence-weighted learning. Generative factored language models are pro posed by (Bilmes and Kirchhoff, 2003)", 
    "clean_text": "This work is related to several existing directions: generative factored language model, discriminative language models, online passive-aggressive learning and confidence-weighted learning. Generative factored language models are proposed by (Bilmes and Kirchhoff, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N06-2001", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Andrei, Alexandrescu | Katrin, Kirchhoff", 
    "raw_text": "A more powerful back off strategy is used in factored language models (FLMs) (Bilmes and Kirchhoff, 2003), which view a word as a vector of word features or? factors?: w= ?f1 ,f2,..", 
    "clean_text": "A more powerful back off strategy is used in factored language models (FLMs) (Bilmes and Kirchhoff, 2003), which view a word as a vector of word features or factors.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N10-1104", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Amr, El-Desoky | Ralf, Schl&uuml;&quot;ter | Hermann, Ney", 
    "raw_text": "Another approach is to use the factored language models (FLMs) which are powerful models that combine multiple sources of information and efficiently integrate them via a complex back off mechanism (Bilmes and Kirchhoff, 2003)", 
    "clean_text": "Another approach is to use the factored language models (FLMs) which are powerful models that combine multiple sources of information and efficiently integrate them via a complex back off mechanism (Bilmes and Kirchhoff, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W08-0510", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Hieu, Hoang | Philipp, Koehn", 
    "raw_text": "In addition, the framework exists to integrate language models, such as those described in (Bilmes and Kirchhoff 2003), which takes advantage of the factored representation within Moses", 
    "clean_text": "In addition, the framework exists to integrate language models, such as those described in (Bilmes and Kirchhoff 2003), which takes advantage of the factored representation within Moses.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P05-3012", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Mary Ellen, Foster | Michael, White | Andrea, Setzer | Roberta, Catizone", 
    "raw_text": "In COMIC, the OpenCCGrealiser uses factored language models (Bilmes and Kirchhoff, 2003) over words and multi modal co articulations to select the highest-scoring realisation licensed by the grammar that satisfies the specification given by the fission module", 
    "clean_text": "In COMIC, the OpenCC Grealiser uses factored language models (Bilmes and Kirchhoff, 2003) over words and multi modal co articulations to select the highest-scoring realisation licensed by the grammar that satisfies the specification given by the fission module.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W11-2124", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Jan, Niehues | Teresa, Herrmann | Stephan, Vogel | Alex, Waibel", 
    "raw_text": "They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process", 
    "clean_text": "They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W07-0702", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Alexandra, Birch | Miles, Osborne | Philipp, Koehn", 
    "raw_text": "Different back off paths are possible, and it would be interesting but prohibitively slow to apply a strategy similar to generalised parallel back off (Bilmesand Kirchhoff, 2003) which is used in factored language models", 
    "clean_text": "Different backoff paths are possible, and it would be interesting but prohibitively slow to apply a strategy similar to generalised parallel back off (Bilmesand Kirchhoff, 2003) which is used in factored language models.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P07-2045", 
    "citing_paper_authority": 746, 
    "citing_paper_authors": "Philipp, Koehn | Hieu, Hoang | Alexandra, Birch | Chris, Callison-Burch | Marcello, Federico | Nicola, Bertoldi | Brooke, Cowan | Wade, Shen | Christine, Moran | Richard, Zens | Chris, Dyer | Ond&#x159;ej, Bojar | Alexandra, Constantin | Evan, Herbst", 
    "raw_text": "Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006)", 
    "clean_text": "Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).", 
    "keep_for_gold": 0
  }
]
