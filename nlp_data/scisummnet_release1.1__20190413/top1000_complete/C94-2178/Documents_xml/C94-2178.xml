<PAPER>
	<S sid="0">K-Vec: A New Approach For Aligning Parallel Texts</S><ABSTRACT>
		<S sid="1" ssid="1">Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates (Hansards).</S>
		<S sid="2" ssid="2">Some of these methods generate a bilingual lexicon as a by-product.</S>
		<S sid="3" ssid="3">We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon.</S>
		<S sid="4" ssid="4">For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French.</S>
		<S sid="5" ssid="5">K-vec does not depend on sentence boundaries.</S>
	</ABSTRACT>
	<SECTION title="Motivation" number="1">
			<S sid="6" ssid="6">There have been quite a number of recent papers on parallel text: Brown et al(1990, 1991, 1993), Chen (1993), Church (1993), Church et al(1993), Dagan et al(1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al(1992), Warwick- Armstrong and Russell (1990), Wu (to appear).</S>
			<S sid="7" ssid="7">Most of this work has been focused on European language pairs, especially English-French.</S>
			<S sid="8" ssid="8">It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and English- Chinese.</S>
			<S sid="9" ssid="9">In previous work (Church et al 1993), we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980)), using charalign (Church, 1993), a method that looks for character sequences that are the same in both the source and target.</S>
			<S sid="10" ssid="10">The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement.</S>
			<S sid="11" ssid="11">In general, this approach doesn't work between languages uch as English and Japanese which are written in different alphabets.</S>
			<S sid="12" ssid="12">The AWK manual happens to contain a large number of examples and technical words that are the same in the English source and target Japanese.</S>
			<S sid="13" ssid="13">It remains an open question how we might be able to align a broader class of texts, especially those that are written in different character sets and share relatively few character sequences.</S>
			<S sid="14" ssid="14">The K-vec method attempts to address this question.</S>
	</SECTION>
	<SECTION title="The K-vec Algorithm. " number="2">
			<S sid="15" ssid="1">K-vec starts by estimating the lexicon.</S>
			<S sid="16" ssid="2">Consider the example: fisheries --~ p~ches.</S>
			<S sid="17" ssid="3">The K-vec algorithm will discover this fact by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French.</S>
			<S sid="18" ssid="4">The concordances for fisheries and p~ches are shown in Tables 1 and 2 (at the end of this paper).</S>
			<S sid="19" ssid="5">1 1.</S>
			<S sid="20" ssid="6">These tables were computed from a small fragment ofthe.</S>
			<S sid="21" ssid="7">Canadian Hansards that has been used in a number of other studies: Church (1993) and Simard et al(1992).</S>
			<S sid="22" ssid="8">The English text has 165,160 words and the French text has 185,615 words.</S>
			<S sid="23" ssid="9">1096 There are 19 instances of fisheries and 21 instances of p~ches.</S>
			<S sid="24" ssid="10">The numbers along the left hand edge show where the concordances were found in the texts.</S>
			<S sid="25" ssid="11">We want to know whether the distribution of numbers in Table 1 is similar to those in Table 2, and if so, we will suspect hat fisheries and p~ches are translations of one another.</S>
			<S sid="26" ssid="12">A quick look at the two tables suggests that the two distributions are probably very similar, though not quite identical.</S>
			<S sid="27" ssid="13">2 We use a simple representation f the distribution of fisheries and p~ches.</S>
			<S sid="28" ssid="14">The English text and the French text were each split into K pieces.</S>
			<S sid="29" ssid="15">Then we determine whether or not the word in question appears in each of the K pieces.</S>
			<S sid="30" ssid="16">Thus, we denote the distribution of fisheries in the English text with a K-dimensional binary vector, VU, and similarly, we denote the distribution of p~ches in the French text with a K-dimensional binary vector, Vp.</S>
			<S sid="31" ssid="17">The i th bit of Vf indicates whether or not Fisheries occurs in the i th piece of the English text, and similarly, the ith bit of Vp indicates whether or not p~ches occurs in the i th piece of the French text.</S>
			<S sid="32" ssid="18">If we take K be 10, the first three instances of fisheries in Table 1 fall into piece 2, and the remaining 16 fall into piece 8.</S>
			<S sid="33" ssid="19">Similarly, the first 4 instances of pgches in Table 2 fall into piece 2, and the remaining 17 fall into piece 8.</S>
			<S sid="34" ssid="20">Thus, VT= Vp = &lt;2 0,0,1,0,0,0,0,0,1,0 &gt; Now, we want to know if VT is similar to Vp, and if we find that it is, then we will suspect that fisheries ---&gt; p~ches.</S>
			<S sid="35" ssid="21">In this example, of course, the vectors are identical, so practically any reasonable similarity statistic ought to produce the desired result.</S>
			<S sid="36" ssid="22">3.</S>
			<S sid="37" ssid="23">fisheries is not file translation of lections Before describing how we estimate the similarity of Vf and Vp, let us see what would happen if we tried to compare fisheries with a completely unrelated word, eg., lections.</S>
			<S sid="38" ssid="24">(This word should be the translation of elections, not fisheries.)</S>
			<S sid="39" ssid="25">2.</S>
			<S sid="40" ssid="26">At most, fisheries can account for only 19 instances of.</S>
			<S sid="41" ssid="27">p~ches, leaving at least 2 instances ofp~ches unexplained.</S>
			<S sid="42" ssid="28">As can be seen in the concordances in Table 3, for K=10, the vector is &lt;1, 1, 0, 1, 1,0, 1, 0, 0, 0&gt;.</S>
			<S sid="43" ssid="29">By almost any measure of similarity one could imagine, this vector will be found to be quite different from the one for fisheries, and therefore, we will correctly discover that fisheries is not the translation of lections.</S>
			<S sid="44" ssid="30">To make this argument a little more precise, it might help to compare the contingency matrices in Tables 5 and 6.</S>
			<S sid="45" ssid="31">The contingency matrices show: (a) the number of pieces where both the English and French word were found, (b) the number of pieces where just the English word was found, (c) the number of pieces where just the French word was found, and (d) the number of peices where neither word was found.</S>
			<S sid="46" ssid="32">Table 4: A contingency matrix French English a b c d Table 5: fisheries vs. pgches p~ches fisheries 2 0 0 8 Table 6: fisheries vs. lections lections fisheries 0 2 4 4 In general, if the English and French words are good translations of one another, as in Table 5, then a should be large, and b and c should be small.</S>
			<S sid="47" ssid="33">In contrast, if the two words are not good translations of one another, as in Table 6, then a should be small, and b and c should be large.</S>
			<S sid="48" ssid="34">4.</S>
			<S sid="49" ssid="35">Mutual Information.</S>
			<S sid="50" ssid="36">Intuitively, these statements seem to be true, but we need to make them more precise.</S>
			<S sid="51" ssid="37">One could have chosen quite a number of similarity metrics for this purpose.</S>
			<S sid="52" ssid="38">We use mutual information: 1097 prob ( VI, Vp ) log2 prob(Vf) prob(Vp ) That is, we want to compare the probability of seeing fisheries and p~ches in the same piece to chance.</S>
			<S sid="53" ssid="39">The probability of seeing the two words in the same piece is simply: a prob(Vf, Vp) - a+b+c+d The marginal probabilities are: a+b prob(Vf ) - a+b+c+d a+c prob(Vp) = a+b+c+d For fisheries --~ p~ches, prob(Vf, Vp) =prob(Vf) =prob(Vp) =0.2.</S>
			<S sid="54" ssid="40">Thus, the mutual information is log25 or 2.32 bits, meaning that the joint probability is 5 times more likely than chance.</S>
			<S sid="55" ssid="41">In contrast, for fisheries ~ lections, prob ( V f, V p ) = O, prob(Vf) =0.5 and prob(Vp) = 0.4.</S>
			<S sid="56" ssid="42">Thus, the mutual information is log 2 0, meaning that the joint is infinitely less likely than chance.</S>
			<S sid="57" ssid="43">We conclude that it is quite likely that fisheries and p~ches are translations of one another, much more so than fisheries and lections.</S>
			<S sid="58" ssid="44">5.</S>
			<S sid="59" ssid="45">Significance.</S>
			<S sid="60" ssid="46">Unfortunately, mutual information is often unreliable when the counts are small.</S>
			<S sid="61" ssid="47">For example, there are lots of infrequent words.</S>
			<S sid="62" ssid="48">If we pick a pair of these words at random, there is a very large chance that they would receive a large mutual information value by chance.</S>
			<S sid="63" ssid="49">For example, let e be an English word that appeared just once and le t fbe a French word that appeared just once.</S>
			<S sid="64" ssid="50">Then, there a non-trivial chance (-~) that e andf will appear is in the same piece, as shown in Table 7.</S>
			<S sid="65" ssid="51">If this should happen, the mutual information estimate would be very large, i.e., logK, and probably misleading.</S>
			<S sid="66" ssid="52">Table 7: f e 1 0 0 9 In order to avoid this problem, we use a t-score to filter out insignificant mutual information values.</S>
			<S sid="67" ssid="53">prob ( Vf, Vp ) - prob (Vf) prob ( Vp ) t= 1 prob(Vf,gp) Using the numbers in Table 7, t=l , which is not significant.</S>
			<S sid="68" ssid="54">(A t of 1.65 or more would be significant at the p &gt; 0.95 confidence level.)</S>
			<S sid="69" ssid="55">Similarly, if e and f appeared in just two pieces 1 each, then there is approximately a ~ chance that they would both appear in the same two pieces, and then the mutual information score would be quite log, , ~--, but we probably wouldn't believe it high, Z. because the t-score would be only "~-.</S>
			<S sid="70" ssid="56">By this definition of significance, we need to see the two words in at least 3 different pieces before the result would be considered significant.</S>
			<S sid="71" ssid="57">This means, unfortunately, that we would reject fisheries --+ p~ches because we found them in only two pieces.</S>
			<S sid="72" ssid="58">The problem, of course, is that we don't have enough pieces.</S>
			<S sid="73" ssid="59">When K=10, there simply isn't enough resolution to see what's going on.</S>
			<S sid="74" ssid="60">At K=100, we obtain the contingency matrix shown in Table 8, and the t-score is significant (t=2.1).</S>
			<S sid="75" ssid="61">Table 8:K=100 p~ches fisheries 5 0 1 94 How do we choose K?</S>
			<S sid="76" ssid="62">As we have seen, if we choose too small a K, then the mutual information values will be unreliable.</S>
			<S sid="77" ssid="63">However, we can only increase K up to a point.</S>
			<S sid="78" ssid="64">If we set K to a ridiculously large value, say the size of the English text, then an English word and its translations are likely to fall in slightly different pieces due to random fluctuations and we would miss the signal.</S>
			<S sid="79" ssid="65">For this work, we set K to the square root of the size of the corpus.</S>
			<S sid="80" ssid="66">K should be thought of as a scale parameter.</S>
			<S sid="81" ssid="67">If we use too low a resolution, then everything turns into a blur and it is hard to see anything.</S>
			<S sid="82" ssid="68">But if we use too high a resolution, then we can miss the signal if 7098 it isn't just exactly where we are looking.</S>
			<S sid="83" ssid="69">Ideally, we would like to apply the K-vec algorithm to all pairs of English and French words, but unfortunately, there are too many such pairs to consider.</S>
			<S sid="84" ssid="70">We therefore limited the search to pairs of words in the frequency range: 3-10.</S>
			<S sid="85" ssid="71">This heuristic makes the search practical, and catches many interesting pairs) 6.</S>
			<S sid="86" ssid="72">Results.</S>
			<S sid="87" ssid="73">This algorithm was applied to a fragment of the Canadian Hansards that has been used in a number of other studies: Church (1993) and Simard et al (1992).</S>
			<S sid="88" ssid="74">The 30 significant pairs with the largest mutual information values are shown in Table 9.</S>
			<S sid="89" ssid="75">As can be seen, the results provide a quick-and- dirty estimate of a bilingual exicon.</S>
			<S sid="90" ssid="76">When the pair is not a direct translation, it is often the translation of a collocate, as illustrated by acheteur ~ Limited and Santd -~ Welfare.</S>
			<S sid="91" ssid="77">(Note that some words in Table 9 are spelled with same way in English and French; this information is not used by the K-vec algorithm).</S>
			<S sid="92" ssid="78">Using a scatter plot technique developed by Church and Helfman (1993) called dotplot, we can visulize the alignment, as illustrated in Figure 1.</S>
			<S sid="93" ssid="79">The source text (Nx bytes) is concatenated to the target text (Ny bytes) to form a single input sequence of Nx+Ny bytes.</S>
			<S sid="94" ssid="80">A dot is placed in position i,j whenever the input token at position i is the same as the input token at position j. The equality constraint is relaxed in Figure 2.</S>
			<S sid="95" ssid="81">A dot is placed in position i,j whenever the input token at position i is highly associated with the input token at position j as determined by the mutual information score of their respective K- vecs.</S>
			<S sid="96" ssid="82">In addition, it shows a detailed, magnified and rotated view of the diagonal line.</S>
			<S sid="97" ssid="83">The alignment program tracks this line with as much precision as possible.</S>
			<S sid="98" ssid="84">have been rejected anyways as insignificant.</S>
			<S sid="99" ssid="85">Table 9: K-vec results French English 3.2 Beauce Beauce.</S>
			<SUBSECTION>3.2 Comeau Comeau.</SUBSECTION>
			<S sid="100" ssid="86">3.2 1981 1981 3.0 Richmond Richmond.</S>
			<SUBSECTION>3.0 Rail VIA.</SUBSECTION>
			<S sid="101" ssid="87">3.0 p~ches Fisheries 2.8 Deans Deans.</S>
			<S sid="102" ssid="88">2.8 Prud Prud.</S>
			<S sid="103" ssid="89">2.8 Prud homme.</S>
			<S sid="104" ssid="90">2.7 acheteur Limited 2.7 Communications Communications.</S>
			<S sid="105" ssid="91">2.7 MacDonald MacDonald.</S>
			<S sid="106" ssid="92">2.6 Mazankowski Mazankowski.</S>
			<S sid="107" ssid="93">2.5 croisi~re nuclear 2.5 Sant6 Welfare.</S>
			<S sid="108" ssid="94">2.5 39 39 2.5 Johnston Johnston.</S>
			<S sid="109" ssid="95">2.5 essais nuclear 2.5 Universit6 University.</S>
			<S sid="110" ssid="96">2.5 bois lumber 2.5 Angus Angus.</S>
			<S sid="111" ssid="97">2.4 Angus VIA.</S>
			<S sid="112" ssid="98">2.4 Saskatoon University.</S>
			<S sid="113" ssid="99">2.4 agriculteurs farmers 2.4 inflation inflation 2.4 James James.</S>
			<S sid="114" ssid="100">2.4 Vanier Vanier.</S>
			<S sid="115" ssid="101">2.4 Sant6 Health.</S>
			<S sid="116" ssid="102">2.3 royale languages 2.3 grief grievance 7.</S>
			<S sid="117" ssid="103">Conclusions.</S>
			<S sid="118" ssid="104">The K-vec algorithm generates a quick-and-dirty estimate of a bilingual exicon.</S>
			<S sid="119" ssid="105">This estimate could be used as a starting point for a more detailed alignment algorithm such as word_align (Dagan et al, 1993).</S>
			<S sid="120" ssid="106">In this way, we might be able to apply word_align to a broader class of language combinations including possibly English-Japanese and English-Chinese.</S>
			<S sid="121" ssid="107">Currently, word_align depends on charalign (Church, 1993) to generate a starting point, which limits its applicability to European languages since char_align was designed for language pairs that share a common alphabet.</S>
	</SECTION>
</PAPER>
