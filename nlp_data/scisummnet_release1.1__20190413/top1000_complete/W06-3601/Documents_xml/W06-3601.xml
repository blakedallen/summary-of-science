<PAPER>
  <S sid="0">A Syntax-Directed Translator With Extended Domain Of Locality</S>
  <ABSTRACT>
    <S sid="1" ssid="1">SD translation schema (synchronous grammar) (string relation) A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language.</S>
    <S sid="2" ssid="2">We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility.</S>
    <S sid="3" ssid="3">We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.</S>
    <S sid="4" ssid="4">The model is then extended to the general log-linear framework in order to rescore with other fealike language models.</S>
    <S sid="5" ssid="5">We devise a simple-yet-effective algorithm to non-duplicate translations rescoring.</S>
    <S sid="6" ssid="6">Initial experimental results on English-to-Chinese translation are presented.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="7" ssid="1">A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language.</S>
    <S sid="8" ssid="2">We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility.</S>
    <S sid="9" ssid="3">We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.</S>
    <S sid="10" ssid="4">The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models.</S>
    <S sid="11" ssid="5">We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring.</S>
    <S sid="12" ssid="6">Initial experimental results on English-to-Chinese translation are presented.</S>
  </SECTION>
  <SECTION title="1 Introduction" number="2">
    <S sid="13" ssid="1">The concept of syntax-directed (SD) translation was originally proposed in compiling (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a tree representation that guides the generation of the object code.</S>
    <S sid="14" ssid="2">Following Aho and Ullman (1972), a translation, as a set of string pairs, can be specified by a syntax-directed translation schema (SDTS), which is essentially a synchronous context-free grammar (SCFG) that generates two languages simultaneously.</S>
    <S sid="15" ssid="3">An SDTS also induces a translator, a device that performs the transformation from input string to output string.</S>
    <S sid="16" ssid="4">In this context, an SD translator consists of two components, a sourcelanguage parser and a recursive converter which is usually modeled as a top-down tree-to-string transducer (G&#180;ecseg and Steinby, 1984).</S>
    <S sid="17" ssid="5">The relationship among these concepts is illustrated in Fig.</S>
    <S sid="18" ssid="6">1.</S>
    <S sid="19" ssid="7">This paper adapts the idea of syntax-directed translator to statistical machine translation (MT).</S>
    <S sid="20" ssid="8">We apply stochastic operations at each node of the source-language parse-tree and search for the best derivation (a sequence of translation steps) that converts the whole tree into some target-language string with the highest probability.</S>
    <S sid="21" ssid="9">However, the structural divergence across languages often results in nonisomorphic parse-trees that is beyond the power of SCFGs.</S>
    <S sid="22" ssid="10">For example, the S(VO) structure in English is translated into a VSO word-order in Arabic, an instance of complex reordering not captured by any SCFG (Fig.</S>
    <S sid="23" ssid="11">2).</S>
    <S sid="24" ssid="12">To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree.</S>
    <S sid="25" ssid="13">For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions.</S>
    <S sid="26" ssid="14">STSGs and STAGs generate more tree relations than SCFGs, e.g. the non-isomorphic tree pair in Fig.</S>
    <S sid="27" ssid="15">2.</S>
    <S sid="28" ssid="16">This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of onelevel context-free productions.</S>
    <S sid="29" ssid="17">Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004).</S>
    <S sid="30" ssid="18">Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side.</S>
    <S sid="31" ssid="19">Since an SD translator separates the sourcelanguage analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFGbased Treebank parser but focuses on the extended domain in the recursive converter.</S>
    <S sid="32" ssid="20">Following Galley et al. (2004), we use a special class of extended tree-to-string transducer (zRs for short) with multilevel left-hand-side (LHS) trees.1 Since the righthand-side (RHS) string can be viewed as a flat onelevel tree with the same nonterminal root from LHS (Fig.</S>
    <S sid="33" ssid="21">2), this framework is closely related to STSGs: they both have extended domain of locality on the source-side, while our framework remains as a CFG on the target-side.</S>
    <S sid="34" ssid="22">For instance, an equivalent zRs rule for the complex reordering in Fig.</S>
    <S sid="35" ssid="23">2 would be While Section 3 will define the model formally, we first proceed with an example translation from English to Chinese (note in particular that the inverted phrases between source and target): 1Throughout this paper, we will use LHS and source-side interchangeably (so are RHS and target-side).</S>
    <S sid="36" ssid="24">In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993).</S>
    <S sid="37" ssid="25">Figure 3 shows how the translator works.</S>
    <S sid="38" ssid="26">The English sentence (a) is first parsed into the tree in (b), which is then recursively converted into the Chinese string in (e) through five steps.</S>
    <S sid="39" ssid="27">First, at the root node, we apply the rule r1 which preserves the toplevel word-order and translates the English period into its Chinese counterpart: Then, the rule r2 grabs the whole sub-tree for &#8220;the gunman&#8221; and translates it as a phrase: (r2) NP-C ( DT (the) NN (gunman) ) &#8212;* qiangshou Now we get a &#8220;partial Chinese, partial English&#8221; sentence &#8220;qiangshou VP o&#8221; as shown in Fig.</S>
    <S sid="40" ssid="28">3 (c).</S>
    <S sid="41" ssid="29">Our recursion goes on to translate the VP sub-tree.</S>
    <S sid="42" ssid="30">Here we use the rule r3 for the passive construction: which captures the fact that the agent (NP-C, &#8220;the police&#8221;) and the verb (VBN, &#8220;killed&#8221;) are always inverted between English and Chinese in a passive voice.</S>
    <S sid="43" ssid="31">Finally, we apply rules r&#65533; and r5 which perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed Chinese string in (e).</S>
  </SECTION>
  <SECTION title="2 Previous Work" number="3">
    <S sid="44" ssid="1">It is helpful to compare this approach with recent efforts in statistical MT.</S>
    <S sid="45" ssid="2">Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order.</S>
    <S sid="46" ssid="3">This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages.</S>
    <S sid="47" ssid="4">Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems.</S>
    <S sid="48" ssid="5">Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model, so their translation schemata are syntax-directed as in Fig.</S>
    <S sid="49" ssid="6">1, but their translators are not: both systems do parsing and transformation in a joint search, essentially over a packed forest of parse-trees.</S>
    <S sid="50" ssid="7">To this end, their translators are not directed by a syntactic tree.</S>
    <S sid="51" ssid="8">Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level contextfree rule, while our approach decouples the sourcelanguage analyzer and the recursive converter, so that the latter can have an extended domain of locality.</S>
    <S sid="52" ssid="9">In addition, our translator also enjoys a speedup by this decoupling, with each of the two stages having a smaller search space.</S>
    <S sid="53" ssid="10">In fact, the recursive transfer step can be done by a a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000).</S>
    <S sid="54" ssid="11">In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable.</S>
    <S sid="55" ssid="12">There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work).</S>
    <S sid="56" ssid="13">Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997).</S>
    <S sid="57" ssid="14">Consider, again, the passive example in rule r3.</S>
    <S sid="58" ssid="15">In Chiang&#8217;s SCFG, there is only one nonterminal X, so a corresponding rule would be ( was X(1) by X(2), bei X(2) X(1) ) which can also pattern-match the English sentence: I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice.</S>
    <S sid="59" ssid="16">This produces very odd Chinese translation, because here &#8220;was A by B&#8221; in the English sentence is not a passive construction.</S>
    <S sid="60" ssid="17">By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C).</S>
    <S sid="61" ssid="18">This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to &#8220;by&#8221;).</S>
    <S sid="62" ssid="19">There are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005).</S>
    <S sid="63" ssid="20">Although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works).2 Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules.</S>
  </SECTION>
  <SECTION title="3 Extended Tree-to-String Tranducers" number="4">
    <S sid="64" ssid="1">In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs).</S>
    <S sid="65" ssid="2">We require each variable xi E X occurs exactly once in t and exactly once in s (linear and non-deleting).</S>
    <S sid="66" ssid="3">We denote &#961;(t) to be the root symbol of tree t. When writing these rules, we avoid notational overhead by introducing a short-hand form from Galley et al. (2004) that integrates the mapping into the tree, which is used throughout Section 1.</S>
    <S sid="67" ssid="4">Following TSG terminology (see Figure 2), we call these &#8220;variable nodes&#8221; such as x2:NP-C substitution nodes, since when applying a rule to a tree, these nodes will be matched with a sub-tree with the same root symbol.</S>
    <S sid="68" ssid="5">We also define |X  |to be the rank of the rule, i.e., the number of variables in it.</S>
    <S sid="69" ssid="6">For example, rules r1 and r3 in Section 1 are both of rank 2.</S>
    <S sid="70" ssid="7">If a rule has no variable, i.e., it is of rank zero, then it is called a purely lexical rule, which performs a phrasal translation as in phrase-based models.</S>
    <S sid="71" ssid="8">Rule r2, for instance, can be thought of as a phrase pair (the gunman, qiangshou).</S>
    <S sid="72" ssid="9">Informally speaking, a derivation in a transducer is a sequence of steps converting a source-language tree into a target-language string, with each step applying one tranduction rule.</S>
    <S sid="73" ssid="10">However, it can also be formalized as a tree, following the notion of derivation-tree in TAG (Joshi and Schabes, 1997): Definition 2.</S>
    <S sid="74" ssid="11">A derivation d, its source and target projections, noted &#163;(d) and C(d) respectively, are recursively defined as follows: derivation with the root symbol of its source projection matches the corresponding substitution node in r, i.e., &#961;(&#163;(di)) = &#966;(xi), then d = r(d1, ... , dm) is also a derivation, where &#163;(d) = [xi H &#163;(di)]t and C(d) = [xi H C(di)]s. Note that we use a short-hand notation [xi H yi]t to denote the result of substituting each xi with yi in t, where xi ranges over all variables in t. For example, Figure 4 shows two derivations for the sentence pair in Example (1).</S>
    <S sid="75" ssid="12">In both cases, the source projection is the English tree in Figure 3 (b), and the target projection is the Chinese translation.</S>
    <S sid="76" ssid="13">Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.</S>
    <S sid="77" ssid="14">Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one: where e is the English input string and c* is the best Chinese translation according to the translation model Pr(c  |e).</S>
    <S sid="78" ssid="15">We now marginalize over all English parse trees T (e) that yield the sentence e: Rather than taking the sum, we pick the best tree T* and factors the search into two separate steps: parsing (4) (a well-studied problem) and tree-to-string translation (5) (Section 5): In this sense, our approach can be considered as a Viterbi approximation of the computationally expensive joint search using (3) directly.</S>
    <S sid="79" ssid="16">Similarly, we now marginalize over all derivations that translates English tree T into some Chinese string and apply the Viterbi approximation again to search for the best derivation d*: Assuming different rules in a derivation are applied independently, we approximate Pr(d) as where the probability Pr(r) of the rule r is estimated by conditioning on the root symbol p(t(r)):</S>
  </SECTION>
  <SECTION title="4.2 Log-Linear Model" number="5">
    <S sid="80" ssid="1">Following Och and Ney (2002), we extend the direct model into a general log-linear framework in order to incorporate other features: where Pr(c) is the language model and e&#8722;&#955;|c |is the length penalty term based on |c|, the length of the translation.</S>
    <S sid="81" ssid="2">Parameters a, Q, and A are the weights of relevant features.</S>
    <S sid="82" ssid="3">Note that positive A prefers longer translations.</S>
    <S sid="83" ssid="4">We use a standard trigram model for Pr(c).</S>
  </SECTION>
  <SECTION title="5 Search Algorithms" number="6">
    <S sid="84" ssid="1">We first present a linear-time algorithm for searching the best derivation under the direct model, and then extend it to the log-linear case by a new variant of k-best parsing.</S>
    <S sid="85" ssid="2">Since our probability model is not based on the noisy channel, we do not call our search module a &#8220;decoder&#8221; as in most statistical MT work.</S>
    <S sid="86" ssid="3">Instead, readers who speak English but not Chinese can view it as an &#8220;encoder&#8221; (or encryptor), which corresponds exactly to our direct model.</S>
    <S sid="87" ssid="4">Given a fixed parse-tree T*, we are to search for the best derivation with the highest probability.</S>
    <S sid="88" ssid="5">This can be done by a simple top-down traversal (or depth-first search) from the root of T*: at each node q in T*, try each possible rule r whose Englishside pattern t(r) matches the subtree T*&#951; rooted at q, and recursively visit each descendant node qi in T*&#951; that corresponds to a variable in t(r).</S>
    <S sid="89" ssid="6">We then collect the resulting target-language strings and plug them into the Chinese-side s(r) of rule r, getting a translation for the subtree T*&#951;.</S>
    <S sid="90" ssid="7">We finally take the best of all translations.</S>
    <S sid="91" ssid="8">With the extended LHS of our transducer, there may be many different rules applicable at one tree node.</S>
    <S sid="92" ssid="9">For example, consider the VP subtree in Fig.</S>
    <S sid="93" ssid="10">3 (c), where both r3 and r6 can apply.</S>
    <S sid="94" ssid="11">As a result, the number of derivations is exponential in the size of the tree, since there are exponentially many decompositions of the tree for a given set of rules.</S>
    <S sid="95" ssid="12">This problem can be solved by memoization (Cormen et al., 2001): we cache each subtree that has been visited before, so that every tree node is visited at most once.</S>
    <S sid="96" ssid="13">This results in a dynamic programming algorithm that is guaranteed to run in O(npq) time where n is the size of the parse tree, p is the maximum number of rules applicable to one tree node, and q is the maximum size of an applicable rule.</S>
    <S sid="97" ssid="14">For a given rule-set, this algorithm runs in time linear to the length of the input sentence, since p and q are considered grammar constants, and n is proportional to the input length.</S>
    <S sid="98" ssid="15">The full pseudocode is worked out in Algorithm 1.</S>
    <S sid="99" ssid="16">A restricted version of this algorithm first appears in compiling for optimal code generation from expression-trees (Aho and Johnson, 1976).</S>
    <S sid="100" ssid="17">In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003).</S>
    <S sid="101" ssid="18">Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005).</S>
    <S sid="102" ssid="19">Under the log-linear model, one still prefers to search for the globally best derivation d*: However, integrating the n-gram model with the translation model in the search is computationally very expensive.</S>
    <S sid="103" ssid="20">As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the language model and length penalty.</S>
    <S sid="104" ssid="21">Like other instances of dynamic programming, Algorithm 1 can be viewed as a hypergraph search problem.</S>
    <S sid="105" ssid="22">To this end, we use an efficient algorithm by Huang and Chiang (2005, Algorithm 3) that solves the general k-best derivations problem in monotonic hypergraphs.</S>
    <S sid="106" ssid="23">It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, ..., kth derivations.</S>
    <S sid="107" ssid="24">Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules.</S>
    <S sid="108" ssid="25">In practice, this results in a very small ratio of unique strings among top-k derivations.</S>
    <S sid="109" ssid="26">To alleviate this problem, determinization techniques have been proposed by Mohri and Riley (2002) for finite-state automata and extended to tree automata by May and Knight (2006).</S>
    <S sid="110" ssid="27">These methods eliminate spurious ambiguity by effectively transforming the grammar into an equivalent deterministic form.</S>
    <S sid="111" ssid="28">However, this transformation often leads to a blow-up in forest size, which is exponential to the original size in the worst-case.</S>
    <S sid="112" ssid="29">So instead of determinization, here we present a simple-yet-effective extension to the Algorithm 3 of Huang and Chiang (2005) that guarantees to output unique translated strings: This method should work in general for any equivalence relation (say, same derived tree) that can be defined on derivations.</S>
  </SECTION>
  <SECTION title="6 Experiments" number="7">
    <S sid="113" ssid="1">Our experiments are on English-to-Chinese translation, the opposite direction to most of the recent work in SMT.</S>
    <S sid="114" ssid="2">We are not doing the reverse direction at this time partly due to the lack of a sufficiently good parser for Chinese.</S>
    <S sid="115" ssid="3">Our training set is a Chinese-English parallel corpus with 1.95M aligned sentences (28.3M words on the English side).</S>
    <S sid="116" ssid="4">We first word-align them by GIZA++, then parse the English side by a variant of Collins (1999) parser, and finally apply the rule-extraction algorithm of Galley et al. (2004).</S>
    <S sid="117" ssid="5">The resulting rule set has 24.7M xRs rules.</S>
    <S sid="118" ssid="6">We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a Chinese trigram model with Knesser-Ney smoothing on the Chinese side of the parallel corpus.</S>
    <S sid="119" ssid="7">Our evaluation data consists of 140 short sentences (&lt; 25 Chinese words) of the Xinhua portion of the NIST 2003 Chinese-to-English evaluation set.</S>
    <S sid="120" ssid="8">Since we are translating in the other direction, we use the first English reference as the source input and the Chinese as the single reference.</S>
    <S sid="121" ssid="9">We implemented our system as follows: for each input sentence, we first run Algorithm 1, which returns the 1-best translation and also builds the derivation forest of all translations for this sentence.</S>
    <S sid="122" ssid="10">Then we extract the top 5000 non-duplicate translated strings from this forest and rescore them with the trigram model and the length penalty.</S>
    <S sid="123" ssid="11">We compared our system with a state-of-the-art phrase-based system Pharaoh (Koehn, 2004) on the evaluation data.</S>
    <S sid="124" ssid="12">Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based).</S>
    <S sid="125" ssid="13">The BLEU scores are based on single reference and up to 4-gram precisions (r1n4).</S>
    <S sid="126" ssid="14">Feature weights of both systems are tuned on the same data set.3 For Pharaoh, we use the standard minimum error-rate training (Och, 2003); and for our system, since there are only two independent features (as we always fix &#945; = 1), we use a simple grid-based line-optimization along the language-model weight axis.</S>
    <S sid="127" ssid="15">For a given languagemodel weight Q, we use binary search to find the best length penalty A that leads to a length-ratio closest to 1 against the reference.</S>
    <S sid="128" ssid="16">The results are summarized in Table 1.</S>
    <S sid="129" ssid="17">The rescored translations are better than the 1-best results from the direct model, but still slightly worse than Pharaoh.</S>
  </SECTION>
  <SECTION title="7 Conclusion and On-going Work" number="8">
    <S sid="130" ssid="1">This paper presents an adaptation of the classic syntax-directed translation with linguisticallymotivated formalisms for statistical MT.</S>
    <S sid="131" ssid="2">Currently we are doing larger-scale experiments.</S>
    <S sid="132" ssid="3">We are also investigating more principled algorithms for integrating n-gram language models during the search, rather than k-best rescoring.</S>
    <S sid="133" ssid="4">Besides, we will extend this work to translating the top k parse trees, instead of committing to the 1-best tree, as parsing errors certainly affect translation quality.</S>
  </SECTION>
</PAPER>
