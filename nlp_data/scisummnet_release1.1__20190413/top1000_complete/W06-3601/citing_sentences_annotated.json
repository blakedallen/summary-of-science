[
  {
    "citance_No": 1, 
    "citing_paper_id": "D10-1027", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Liang, Huang | Haitao, Mi", 
    "raw_text": "In this framework, decoding without language model (? LM decoding) is simply a linear-time depth-first search with memoization (Huang et al, 2006), since a tree of n words is also of size O (n) and we visit every node only once", 
    "clean_text": "In this framework, decoding without language model (LM decoding) is simply a linear-time depth-first search with memoization (Huang et al, 2006), since a tree of n words is also of size O (n) and we visit every node only once.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P08-1064", 
    "citing_paper_authority": 42, 
    "citing_paper_authors": "Min, Zhang | Hongfei, Jiang | Aiti, Aw | Haizhou, Li | Chew Lim, Tan | Sheng, Li", 
    "raw_text": "Huang et al (2006) study a TSG-based tree-to-string alignment model", 
    "clean_text": "Huang et al (2006) study a TSG-based tree-to-string alignment model.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W08-0308", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "Liu et al (2006) and Huang et al (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance", 
    "clean_text": "Liu et al (2006) and Huang et al (2006) then used the TTS transducer on the task of Chinese-to-English and English-to-Chinese translation, respectively, and achieved decent performance.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W08-0308", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006)", 
    "clean_text": "The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W08-0308", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "denotes the best derivation of T. The implementation of a TTS transducer can be done either top down with memoiza tion to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006)", 
    "clean_text": "The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W08-0308", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "To speed up the decoding ,standard beam search is used. In Figure 3, BinaryCombine denotes the target size binarization (Huang et al, 2006) combination. The translation candidates of the template? s variables, as well as its terminals, are combined pair wise in the order they appear in the RHS of the template", 
    "clean_text": "To speed up the decoding ,standard beam search is used. In Figure 3, BinaryCombine denotes the target size binarization (Huang et al, 2006) combination. The translation candidates of the template's variables, as well as its terminals, are combined pair wise in the order they appear in the RHS of the template.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W08-0308", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source", 
    "clean_text": "Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D09-1136", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "Huang et al (2006) used character-basedBLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source", 
    "clean_text": "Huang et al (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P11-1086", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Ashish, Vaswani | Haitao, Mi | Liang, Huang | David, Chiang", 
    "raw_text": "Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead", 
    "clean_text": "Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "C10-1081", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006)", 
    "clean_text": "The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "C10-1081", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al, 2006)", 
    "clean_text": "It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "C10-1081", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source", 
    "clean_text": "Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P07-1019", 
    "citing_paper_authority": 99, 
    "citing_paper_authors": "Liang, Huang | David, Chiang", 
    "raw_text": "We push the idea behind this method further and make the following contributions in this paper:? We generalize cube pruning and adapt it to two systems very different from Hiero: a phrase based system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al, 2006)", 
    "clean_text": "We push the idea behind this method further and make the following contributions in this paper: We generalize cube pruning and adapt it to two systems very different from Hiero: a phrase based system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P07-1019", 
    "citing_paper_authority": 99, 
    "citing_paper_authors": "Liang, Huang | David, Chiang", 
    "raw_text": "We test our methods on two large-scale English-to Chinese translation systems: a phrase-based system and our tree-to-string system (Huang et al, 2006)", 
    "clean_text": "We test our methods on two large-scale English-to-Chinese translation systems: a phrase-based system and our tree-to-string system (Huang et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P07-1019", 
    "citing_paper_authority": 99, 
    "citing_paper_authors": "Liang, Huang | David, Chiang", 
    "raw_text": "Our data preparation follows Huang et al (2006): the training data is a parallel corpus of 28.3M words on the English side, and a trigram language model is trained on the Chinese side", 
    "clean_text": "Our data preparation follows Huang et al (2006): the training data is a parallel corpus of 28.3M words on the English side, and a trigram language model is trained on the Chinese side.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P07-1019", 
    "citing_paper_authority": 99, 
    "citing_paper_authors": "Liang, Huang | David, Chiang", 
    "raw_text": "We use the same test setas (Huang et al, 2006), which is a 140-sentence sub set of the NIST 2003 test set with 9? 36 words on the English side", 
    "clean_text": "We use the same test set as (Huang et al, 2006), which is a 140-sentence sub set of the NIST 2003 test set with 9-36 words on the English side.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P07-1019", 
    "citing_paper_authority": 99, 
    "citing_paper_authors": "Liang, Huang | David, Chiang", 
    "raw_text": "For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to? LM to estimate the lower-boundheuristics.4 This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time", 
    "clean_text": "For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to LM to estimate the lower-boundheuristics. This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "C10-1080", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yang, Liu | Qun, Liu", 
    "raw_text": "Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al, 2009b; Zhang et al, 2006) and tree parsing generally runs in linear time (Huang et al, 2006)", 
    "clean_text": "Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al, 2009b; Zhang et al, 2006) and tree parsing generally runs in linear time (Huang et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "N10-1014", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Adam, Pauls | Dan, Klein | David, Chiang | Kevin, Knight", 
    "raw_text": "Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al, 2006), the source side (Huang et al, 2006), or both (Liu et al, 2009)", 
    "clean_text": "Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al, 2006), the source side (Huang et al, 2006), or both (Liu et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P10-4002", 
    "citing_paper_authority": 67, 
    "citing_paper_authors": "Chris, Dyer | Adam, Lopez | Juri, Ganitkevitch | Jonathan, Weese | Ferhan, Ture | Philip, Blunsom | Hendra, Setiawan | Vladimir, Eidelman | Philip, Resnik", 
    "raw_text": "The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006)", 
    "clean_text": "The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006).", 
    "keep_for_gold": 0
  }
]
