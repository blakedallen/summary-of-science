<PAPER>
  <S sid="0">SPMT: Statistical Machine Translation With Syntactified Target Language Phrases</S>
  <ABSTRACT>
    <S sid="1" ssid="1">We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.</S>
    <S sid="2" ssid="2">The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="3" ssid="1">During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy.</S>
    <S sid="4" ssid="2">Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent.</S>
    <S sid="5" ssid="3">Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.</S>
    <S sid="6" ssid="4">And syntax-inspired formal models (Chiang, 2005), in spite of being trained on significantly less data, have shown promising results when compared on the same test sets with mature phrase-based systems.</S>
    <S sid="7" ssid="5">To our knowledge though, no previous research has demonstrated that a syntax-based statistical translation system could produce better results than a phrase-based system on a large-scale, well-established, open domain translation task.</S>
    <S sid="8" ssid="6">In this paper we present such a system.</S>
    <S sid="9" ssid="7">Our translation models rely upon and naturally exploit submodels (feature functions) that have been initially developed in phrase-based systems for choosing target translations of source language phrases, and use new, syntax-based translation and target language submodels for assembling target phrases into well-formed, grammatical outputs.</S>
    <S sid="10" ssid="8">After we introduce our models intuitively, we discuss their formal underpinning and parameter training in Section 2.</S>
    <S sid="11" ssid="9">In Section 3, we present our decoder and, in Section 4, we evaluate our models empirically.</S>
    <S sid="12" ssid="10">In Section 5, we conclude with a brief discussion.</S>
  </SECTION>
  <SECTION title="2 SPMT: statistical Machine Translation with Syntactified Phrases" number="2">
    <S sid="13" ssid="1">After being exposed to 100M+ words of parallel Chinese-English texts, current phrase-based statistical machine translation learners induce reasonably reliable phrase-based probabilistic dictionaries.</S>
    <S sid="14" ssid="2">For example, our baseline statistical phrasebased system learns that, with high probabilities, the Chinese phrases &#8220;ASTRO- -NAUTS&#8221;, &#8220;FRANCE AND RUSSIA&#8221; and &#8220;COMINGFROM&#8221; can be translated into English as &#8220;astronauts&#8221;/&#8220;cosmonauts&#8221;, &#8220;france and russia&#8221;/&#8220;france and russian&#8221; and &#8220;coming from&#8221;/&#8220;from&#8221;, respectively.</S>
    <S sid="15" ssid="3">1 Unfortunately, when given as input Chinese sentence 1, our phrase-based system produces the output shown in 2 and not the translation in 3, which correctly orders the phrasal translations into a grammatical sequence.</S>
    <S sid="16" ssid="4">We believe this happens because the distortion/reordering models that are used by state-of-the-art phrase-based systems, which exploit phrase movement and ngram target 'To increase readability, in this paper, we represent Chinese words using fully capitalized English glosses and English words using lowercased letters. language models (Och and Ney, 2004; Tillman, 2004), are too weak to help a phrase-based decoder reorder the target phrases into grammatical outputs.</S>
    <S sid="17" ssid="5">One method for increasing the ability of a decoder to reorder target language phrases is that of decorating them with syntactic constituent information.</S>
    <S sid="18" ssid="6">For example, we may make explicit that the Chinese phrase &#8220;ASTRO- -NAUTS&#8221; may be translated into English as a noun phrase, NP(NNS(astronauts)); that the phrase FRANCE AND RUSSIA may be translated into a complex nounphrase, NP(NP(NNP(france)) CC(and) NP(NNP(russia))); that the phrase COMINGFROM may be translated into a partially realized verb phrase that is looking for a noun phrase to its right in order to be fully realized, VP(VBG(coming) PP(IN(from) NP:x0)); and that the Chinese particle p-DE, when occurring between a Chinese string that was translated into a verb phrase to its left and another Chinese string that was translated into a noun phrase to its right, VP:x1 p-DE NP:x0, should be translated to nothing, while forcing the reordering of the two constituents, NP(NP:x0, VP:x1).</S>
    <S sid="19" ssid="7">If all these translation rules (labeled r1 to r4 in Figure 1) were available to a decoder that derives English parse trees starting from Chinese input strings, this decoder could produce derivations such as that shown in Figure 2.</S>
    <S sid="20" ssid="8">Because our approach uses translation rules with Syntactified target language Phrases (see Figure 1), we call it SPMT.</S>
    <S sid="21" ssid="9">We are interested to model a generative process that explains how English parse trees 7r and their associated English string yields E, foreign sentences, F, and word-level alignments, A, are produced.</S>
    <S sid="22" ssid="10">We assume that observed (7r, F, A) triplets are generated by a stochastic process similar to that used in Data Oriented Parsing models (Bonnema, 2002).</S>
    <S sid="23" ssid="11">For example, if we assume that the generative process has already produced the top NP node in Figure 2, then the corresponding partial English parse tree, foreign/source string, and word-level alignment could be generated by the rule derivation r4(r1, r3(r2)), where each rule is assumed to have some probability.</S>
    <S sid="24" ssid="12">The extended tree to string transducers introduced by Knight and Graehl (2005) provide a natural framework for expressing the tree to string transformations specific to our SPMT models.</S>
    <S sid="25" ssid="13">The transformation rules we plan to exploit are equivalent to one-state xRS top-down transducers with look ahead, which map subtree patterns to strings.</S>
    <S sid="26" ssid="14">For example, rule r3 in Figure 1 can be applied only when one is in a state that has a VP as its syntactic constituent and the tree pattern VP(VBG(coming) PP(IN(from) NP)) immediately underneath.</S>
    <S sid="27" ssid="15">The rule application outputs the string &#8220;COMINGFROM&#8221; as the transducer moves to the state co-indexed by x0; the outputs produced from the new state will be concatenated to the right of the string &#8220;COMINGFROM&#8221;.</S>
    <S sid="28" ssid="16">Since there are multiple derivations that could lead to the same outcome, the probability of a tuple (7r, F, A) is obtained by summing over all derivations Oi E O that are consistent with the tuple, c(0) _ (7r, F, A).</S>
    <S sid="29" ssid="17">The probability of each derivation BZ is given by the product of the probabilities of all the rules p(rj) in the derivation (see equation 4).</S>
    <S sid="30" ssid="18">In order to acquire the rules specific to our model and to induce their probabilities, we parse the English side of our corpus with an in-house implementation (Soricut, 2005) of Collins parsing models (Collins, 2003) and we word-align the parallel corpus with the Giza++2 implementation of the IBM models (Brown et al., 1993).</S>
    <S sid="31" ssid="19">We use the automatically derived (English-parse-tree, English-sentence, Foreign-sentence, Word-levelalignment) tuples in order to induce xRS rules for several models.</S>
    <S sid="32" ssid="20">In our simplest model, we assume that each tuple (7r, F, A) in our automatically annotated corpus could be produced by applying a combination of minimally syntactified, lexicalized, phrase-based compatible xRS rules, and minimal/necessary, non-lexicalized xRS rules.</S>
    <S sid="33" ssid="21">We call a rule non-lexicalized whenever it does not have any directly aligned source-to-target words.</S>
    <S sid="34" ssid="22">Rules r9&#8211;r12 in Figure 1 are examples of non-lexicalized rules.</S>
    <S sid="35" ssid="23">Minimally syntactified, lexicalized, phrasebased-compatible xRS rules are extracted via a simple algorithm that finds for each foreign phrase FZj , the smallest xRS rule that is consistent with the foreign phrase FZj , the English syntactic tree 7r, and the alignment A.</S>
    <S sid="36" ssid="24">The algorithm finds for each foreign/source phrase span its projected span on the English side and then traverses the English parse tree bottom up until it finds a node that subsumes the projected span.</S>
    <S sid="37" ssid="25">If this node has children that fall outside the projected span, then those children give rise to rules that have variables.</S>
    <S sid="38" ssid="26">For example, if the tuple shown in Figure 2 is in our training corpus, for the foreign/source phrases FRANCE, FRANCE AND, FRANCE AND RUSSIA, and ASTRO- -NAUTS, we extract the minimally syntactified, lexicalized phrase-based-compatible xRS rules r5, r6, r2, and r7 in Figure 1, respectively.</S>
    <S sid="39" ssid="27">Because, as in phrase-based MT, all our rules have continuous phrases on both the source and target language sides, we call these phrase-based compatible xRS rules.</S>
    <S sid="40" ssid="28">Since these lexicalized rules are not sufficient to explain an entire (7r, F, A) tuple, we also extract the required minimal/necessary, non-lexicalized xRS rules.</S>
    <S sid="41" ssid="29">The minimal non-lexicalized rules that are licensed by the tuple in Figure 2 are labeled r4, r9, r10, r11 and r12 in Figure 1.</S>
    <S sid="42" ssid="30">To obtain the non-lexicalized xRS rules, we compute the set of all minimal rules (lexicalized and non-lexicalized) by applying the algorithm proposed by Galley et al. (2006) and then remove the lexicalized rules.</S>
    <S sid="43" ssid="31">We remove the Galley et al.&#8217;s lexicalized rules because they are either already accounted for by the minimally syntactified, lexicalized, phrasebased-compatible xRS rules or they subsume noncontinuous source-target phrase pairs.</S>
    <S sid="44" ssid="32">It is worth mentioning that, in our framework, a rule is defined to be &#8220;minimal&#8221; with respect to a foreign/source language phrase, i.e., it is the minimal xRS rule that yields that source phrase.</S>
    <S sid="45" ssid="33">In contrast, in the work of Galley et al. (2004; 2006), a rule is defined to be minimal when it is necessary in order to explain a (7r, F, A) tuple.</S>
    <S sid="46" ssid="34">Under SPMT model 1, the tree in Figure 2 can be produced, for example, by the following derivation: r4(r9(r7),r3(r6(r12(rs)))).</S>
    <S sid="47" ssid="35">We hypothesize that composed rules, i.e., rules that can be decomposed via the application of a sequence of Model 1 rules may improve the performance of an SPMT system.</S>
    <S sid="48" ssid="36">For example, although the minimal Model 1 rules r11 and r13 are sufficient for building an English NP on top of two NPs separated by the Chinese conjunction AND, the composed rule r14 in Figure 1 accomplishes the same result in only one step.</S>
    <S sid="49" ssid="37">We hope that the composed rules could play in SPMT the same role that phrases play in string-based translation models.</S>
    <S sid="50" ssid="38">To test our hypothesis, we modify our rule extraction algorithm so that for every foreign phrase FZ , we extract not only a minimally syntactified, lexicalized xRS rule, but also one composed rule.</S>
    <S sid="51" ssid="39">The composed rule is obtained by extracting the rule licensed by the foreign/source phrase, alignment, English parse tree, and the first multi-child ancestor node of the root of the minimal rule.</S>
    <S sid="52" ssid="40">Our intuition is that composed rules that involve the application of more than two minimal rules are not reliable.</S>
    <S sid="53" ssid="41">For example, for the tuple in Figure 2, the composed rule that we extract given the foreign phrases AND and COMINGFROM are respectively labeled as rules r14 and r15 in Figure 1.</S>
    <S sid="54" ssid="42">Under the SPMT composed model 1, the tree in Figure 2 can be produced, for example, by the following derivation: r15(r9(r7), r14(r12(r5), r12(rs))).</S>
    <S sid="55" ssid="43">In many instances, the tuples (7r, F, A) in our training corpus exhibit alignment patterns that can be easily handled within a phrase-based SMT framework, but that become problematic in the SPMT models discussed until now.</S>
    <S sid="56" ssid="44">Consider, for example, the (7r, F, A) tuple fragment in Figure 3.</S>
    <S sid="57" ssid="45">When using a phrase-based translation model, one can easily extract the phrase pair (THE MUTUAL; the mutual) and use it during the phrase-based model estimation phrase and in decoding.</S>
    <S sid="58" ssid="46">However, within the xRS transducer framework that we use, it is impossible to extract an equivalent syntactified phrase translation rule that subsumes the same phrase pair because valid xRS translation rules cannot be multiheaded.</S>
    <S sid="59" ssid="47">When faced with this constraint, one has several options: Our SPMT Model 2 adopts the third option by rewriting on the fly the English parse tree for each foreign/source phrase and alignment that lead to non-syntactifiable phrase pairs.</S>
    <S sid="60" ssid="48">The rewriting process adds new rules to those that can be created under the SPMT model 1 constraints.</S>
    <S sid="61" ssid="49">The process creates one xRS rule that is headed by a pseudo, non-syntactic nonterminal symbol that subsumes the target phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals in order to obtain genuine parse trees.</S>
    <S sid="62" ssid="50">In this view, the foreign/source phrase THE MUTUAL and corresponding alignment in Figure 3 licenses the rules *NPB* NN(DT(the) JJ(mutual)) --+ THE MUTUAL and NPB(*NPB* NN:x0 NN:x1) --+ x0 x1 even though the foreign word UNDERSTANDING is aligned to an English word outside the NPB consituent.</S>
    <S sid="63" ssid="51">The name of the non-syntactic nonterminal reflects the intuition that the English phrase &#8220;the mutual&#8221; corresponds to a partially realized NPB that needs an NN to its right in order to be fully realized.</S>
    <S sid="64" ssid="52">Our hope is that the rules headed by pseudo nonterminals could make available to an SPMT system all the rules that are typically available to a phrase-based system; and that the sibling rules could provide a sufficiently robust generalization layer for integrating pseudo, partially realized constituents into the overall decoding process.</S>
    <S sid="65" ssid="53">The SPMT composed model 2 uses all rule types described in the previous models.</S>
    <S sid="66" ssid="54">For each model, we extract all rule instances that are licensed by a symmetrized Giza-aligned parallel corpus and the constraints we put on the model.</S>
    <S sid="67" ssid="55">We condition on the root node of each rule and use the rule counts f(r) and a basic maximum likelihood estimator to assign to each rule type a conditional probability (see equation 5).</S>
    <S sid="68" ssid="56">It is unlikely that this joint probability model can be discriminative enough to distinguish between good and bad translations.</S>
    <S sid="69" ssid="57">We are not too concerned though because, in practice, we decode using a larger set of submodels (feature functions).</S>
    <S sid="70" ssid="58">Given the way all our lexicalized xRS rules have been created, one can safely strip out the syntactic information and end up with phrase-to-phrase translation rules.</S>
    <S sid="71" ssid="59">For example, in string-to-string world, rule r5 in Figure 1 can be rewritten as &#8220;fiance --+ FRANCE&#8221;; and rule r6 can be rewritten as &#8220;fiance and --+ FRANCE AND&#8221;.</S>
    <S sid="72" ssid="60">When one analyzes the lexicalized xRS rules in this manner, it is easy to associate with them any of the submodel probability distributions that have been proven useful in statistical phrase-based MT.</S>
    <S sid="73" ssid="61">The non-lexicalized rules are assigned probability distributions under these submodels as well by simply assuming a NULL phrase for any missing lexicalized source or target phrase.</S>
    <S sid="74" ssid="62">In the experiments described in this paper, we use the following submodels (feature functions): Syntax-based-like submodels: All these models are combined log-linearly during decoding.</S>
    <S sid="75" ssid="63">The weights of the models are computed automatically using a variant of the Maximum Bleu training procedure proposed by Och (2003).</S>
    <S sid="76" ssid="64">The phrase-based-like submodels have been proved useful in phrase-based approaches to SMT (Och and Ney, 2004).</S>
    <S sid="77" ssid="65">The first two syntaxbased submodels implement a &#8220;fused&#8221; translation and lexical grounded distortion model (proot) and a syntax-based distortion model (pcfg).</S>
    <S sid="78" ssid="66">The indicator submodels are used to determine the extent to which our system prefers lexicalized vs. nonlexicalized rules; simple vs. composed rules; and high vs. low count rules.</S>
  </SECTION>
  <SECTION title="3 Decoding" number="3">
    <S sid="79" ssid="1">We decode with each of our SPMT models using a straightforward, bottom-up, CKY-style decoder that builds English syntactic constituents on the top of Chinese sentences.</S>
    <S sid="80" ssid="2">The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure (Zhang et al., 2006).</S>
    <S sid="81" ssid="3">The CKY-style decoder computes the probability of English syntactic constituents in a bottom up fashion, by log-linearly interpolating all the submodel scores described in Section 2.3.</S>
    <S sid="82" ssid="4">The decoder is capable of producing nbest derivations and nbest lists (Knight and Graehl, 2005), which are used for Maximum Bleu training (Och, 2003).</S>
    <S sid="83" ssid="5">When decoding the test corpus, the decoder returns the translation that has the most probable derivation; in other words, the sum operator in equation 4 is replaced with an argmax.</S>
    <S sid="84" ssid="6">Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al., 2006) when significantly different engines compete for producing the best outputs.</S>
    <S sid="85" ssid="7">In our case, combining multiple MT outputs is much simpler because the submodel probabilities across the four models described here are mostly identifical, with the exception of the root normalized and CFG-like submodels which are scaled differently &#8211; since Model 2 composed has, for example, more rules than Model 1, the root normalized and CFG-like submodels have smaller probabilities for identical rules in Model 2 composed than in Model 1.</S>
    <S sid="86" ssid="8">We compare these two probabilities across the submodels and we scale all model probabilities to be compatible with those of Model 2 composed.</S>
    <S sid="87" ssid="9">With this scaling procedure into place, we produce 6,000 non-unique nbest lists for all sentences in our development corpus, using all SPMT submodels.</S>
    <S sid="88" ssid="10">We concatenate the lists and we learn a new combination of weights that maximizes the Bleu score of the combined nbest list using the same development corpus we used for tuning the individual systems (Och, 2003).</S>
    <S sid="89" ssid="11">We use the new weights in order to rerank the nbest outputs on the test corpus.</S>
  </SECTION>
  <SECTION title="4 Experiments" number="4">
    <S sid="90" ssid="1">We evaluate our models on a Chinese to English machine translation task.</S>
    <S sid="91" ssid="2">We use the same training corpus, 138.7M words of parallel Chinese-English data released by LDC, in order to train several statistical-based MT systems: In all systems, we use a rule extraction algorithm that limits the size of the foreign/source phrases to four words.</S>
    <S sid="92" ssid="3">For all systems, we use a Kneser-Ney (1995) smoothed trigram language model trained on 2.3 billion words of English.</S>
    <S sid="93" ssid="4">As development data for the SPMT systems, we used the sentences in the 2002 NIST development corpus that are shorter than 20 words; we made this choice in order to finish all experiments in time for this submission.</S>
    <S sid="94" ssid="5">The PBMT system used all sentences in the 2002 NIST corpus for development.</S>
    <S sid="95" ssid="6">As test data, we used the 2003 NIST test set.</S>
    <S sid="96" ssid="7">Table 1 shows the number of string-to-string or tree-to-string rules extracted by each system and the performance on both the subset of sentences in the test corpus that were shorter than 20 words and the entire test corpus.</S>
    <S sid="97" ssid="8">The performance is measured using the Bleu metric (Papineni et al., 2002) on lowercased, tokenized outputs/references.</S>
    <S sid="98" ssid="9">The results show that the SPMT models clearly outperform the phrase-based systems &#8211; the 95% confidence intervals computed via bootstrap resampling in all cases are around 1 Bleu point.</S>
    <S sid="99" ssid="10">The results also show that the simple system combination procedure that we have employed is effective in our setting.</S>
    <S sid="100" ssid="11">The improvement on the development corpus transfers to the test setting as well.</S>
    <S sid="101" ssid="12">A visual inspection of the outputs shows significant differences between the outputs of the four models.</S>
    <S sid="102" ssid="13">The models that use composed rules prefer to produce outputs by using mostly lexicalized rules; in contrast, the simple M1 and M2 models produce outputs in which content is translated primarily using lexicalized rules and reorderings and word insertions are explained primarily by the non-lexical rules.</S>
    <S sid="103" ssid="14">It appears that the two strategies are complementary, succeeding and failing in different instances.</S>
    <S sid="104" ssid="15">We believe that this complementarity and the overcoming of some of the search errors in our decoder during the model rescoring phase explain the success of the system combination experiments.</S>
    <S sid="105" ssid="16">We suspect that our decoder still makes many search errors.</S>
    <S sid="106" ssid="17">In spite of this, the SPTM outputs are still significantly better than the PBMT outputs.</S>
    <S sid="107" ssid="18">We also tested whether the Bleu score improvements translate into improvements that can be perceived by humans.</S>
    <S sid="108" ssid="19">To this end, we randomly selected 138 sentences of less than 20 words from our development corpus; we expected the translation quality of sentences of this size to be easier to assess than that of sentences that are very long.</S>
    <S sid="109" ssid="20">We prepared a web-based evaluation interface that showed for each input sentence: The evaluated `MT systems&#8221; were the six systems shown in Table 1 and one of the reference translations.</S>
    <S sid="110" ssid="21">The reference translation presented as automatically produced output was selected from the set of four reference translations provided by NIST so as to be representative of human translation quality.</S>
    <S sid="111" ssid="22">More precisely, we chose the second best reference translation in the NIST corpus according to its Bleu score against the other three reference translations.</S>
    <S sid="112" ssid="23">The seven outputs were randomly shufied and presented to three English speakers for assessment.</S>
    <S sid="113" ssid="24">The judges who participated in our experiment were instructed to carefully read the three reference translations and seven machine translation outputs, and assign a score between 1 and 5 to each translation output on the basis of its quality.</S>
    <S sid="114" ssid="25">Human judges were told that the translation quality assessment should take into consideration both the grammatical iuency of the outputs and their translation adequacy.</S>
    <S sid="115" ssid="26">Table 2 shows the average scores obtained by each system according to each judge.</S>
    <S sid="116" ssid="27">For convenience, the table also shows the Bleu scores of all systems (including the human translations) on three reference translations.</S>
    <S sid="117" ssid="28">The results in Table 2 show that the human judges are remarkably consistent in preferring the syntax-based outputs over the phrase-based outputs.</S>
    <S sid="118" ssid="29">On a 1 to 5 quality scale, the difference between the phrase-based and syntax-based systems was, on average, between 0.2 and 0.3 points.</S>
    <S sid="119" ssid="30">All differences between the phrase-based baseline and the syntax-based outputs were statistically significant.</S>
    <S sid="120" ssid="31">For example, when comparing the phrasebased baseline against the combined system, the improvement in human scores was significant at P = 4.04e&#8722;6(t = 4.67, df = 413).</S>
    <S sid="121" ssid="32">The results also show that the LDC reference translations are far from being perfect.</S>
    <S sid="122" ssid="33">Although we selected from the four references the second best according to the Bleu metric, this human reference was judged to be at a quality level of only 4.67 on a scale from 1 to 5.</S>
    <S sid="123" ssid="34">Most of the translation errors were iuency errors.</S>
    <S sid="124" ssid="35">Although the human outputs had most of the time the right meaning, the syntax was sometimes incorrect.</S>
    <S sid="125" ssid="36">In order to give readers a iavor of the types of re-orderings enabled by the SPMT models, we present in Table 3, several translation outputs produced by the phrase-based baseline and the combined SPMT system.</S>
    <S sid="126" ssid="37">The outputs were selected to reflect both positive and negative effects of largescale re-orderings.</S>
  </SECTION>
  <SECTION title="5 Discussion" number="5">
    <S sid="127" ssid="1">The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).</S>
    <S sid="128" ssid="2">If we analyze these three models in terms of expressive power, the Galley et al. (2006) model is more expressive than the SPMT models, which in turn, are more expressive than Chiang&#8217;s model.</S>
    <S sid="129" ssid="3">The xRS formalism utilized by Galley et al. (2006) allows for the use of translation rules that have multi-level target tree annotations and discontinuous source language phrases.</S>
    <S sid="130" ssid="4">The SPMT models are less general: they use translation rules that have multi-level target tree annotations but require that the source language phrases are continuous.</S>
    <S sid="131" ssid="5">The Syncronous Grammar formalism utilized by Chiang is stricter than SPMT since it allows only for single-level target tree annotations.</S>
    <S sid="132" ssid="6">The parameters of the SPMT models presented in this paper are easier to estimate than those of Galley et al&#8217;s (2006) and can easily exploit and expand on previous research in phrase-based machine translation.</S>
    <S sid="133" ssid="7">Also, the SPMT models yield significantly fewer rules that the model of Galley et al. In contrast with the model proposed by Chiang, the SPMT models introduced in this paper are fully grounded in syntax; this makes them good candidates for exploring the impact that syntaxbased language models could have on translation performance.</S>
    <S sid="134" ssid="8">From a machine translation perspective, the SPMT translation model family we have proposed in this paper is promising.</S>
    <S sid="135" ssid="9">To our knowledge, we are the first to report results that show that a syntax-based system can produce results that are better than those produced by a strong phrasebased system in experimental conditions similar to those used in large-scale, well-established independent evaluations, such as those carried out annually by NIST.</S>
    <S sid="136" ssid="10">Although the number of syntax-based rules used by our models is smaller than the number of phrase-based rules used in our state-of-the-art baseline system, the SPMT models produce outputs of higher quality.</S>
    <S sid="137" ssid="11">This feature is encouraging because it shows that the syntactified translation rules learned in the SPMT models can generalize better than the phrase-based rules.</S>
    <S sid="138" ssid="12">We were also pleased to see that the Bleu score improvements going from the phrase- to the syntax-based models, as well as the Bleu improvements going from the simple syntax-based models to the combined models system are fully consistent with the human qualitative judgments in our subjective evaluations.</S>
    <S sid="139" ssid="13">This correlation suggests that we can continue to use the Bleu metric to further improve our models and systems.</S>
    <S sid="140" ssid="14">Acknowledgements.</S>
    <S sid="141" ssid="15">This research was partially supported by the National Institute of Standards and Technology&#8217;s Advanced Technology Program Award 70NANB4H3050 to Language Weaver Inc.</S>
  </SECTION>
</PAPER>
