<PAPER>
  <S sid="0" ssid="0">Towards History-based Grammars: Using Richer Models for Probabil ist ic Parsing* Ezra Black Fred Jelinek John Lafferty David M. Magerman Robert Mercer Salim Roukos IBM T.  J .</S>
  <S sid="1" ssid="1">Watson  Research  Center ABSTRACT We describe a generative probabilistic model of natural an- guage, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity.</S>
  <S sid="2" ssid="2">HBG incorpo- rates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.</S>
  <S sid="3" ssid="3">We use a corpus of bracketed sentences, called a Tree- bank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence.</S>
  <S sid="4" ssid="4">This stands in contrast o the usual approach of further grammar tailoring via the usual linguistic introspection i the hope of generating the correct parse.</S>
  <S sid="5" ssid="5">In head-to-head tests against one of the best existing robust probabflistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.</S>
  <S sid="6" ssid="6">In t roduct ion Almost any natural language sentence is ambiguous in structure, reference, or nuance of meaning.</S>
  <S sid="7" ssid="7">Humans overcome these apparent ambiguities by examining the con~ezt of the sentence.</S>
  <S sid="8" ssid="8">But what exactly is context?</S>
  <S sid="9" ssid="9">Frequently, the correct interpretation is apparent from the words or constituents immediately surrounding the phrase in question.</S>
  <S sid="10" ssid="10">This observation begs the following question: How much information about the context of a sentence or phrase is necessary and sufficient to de- termine its meaning?</S>
  <S sid="11" ssid="11">This question is at the crux of the debate among computational linguists about the ap- plication and implementation of statistical methods in natural anguage understanding.</S>
  <S sid="12" ssid="12">Previous work on disambiguation and probabilistic pars- ing has offered partial answers to this question.</S>
  <S sid="13" ssid="13">Hidden Markov models of words and their tags, introduced in [1] and [11] and popularized in the natural anguage commu- nity by Church [5], demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity.</S>
  <S sid="14" ssid="14">Hindle and Rooth [8] use a statistical measure of lexical asso- ciations to resolve structural ambiguities.</S>
  <S sid="15" ssid="15">Brent [2] ac- quires likely verb subcategorization patterns using the *Thanks to Philip Resnik and Stanley Chen for their valued input.</S>
  <S sid="16" ssid="16">frequencies of verb-object-preposition triples.</S>
  <S sid="17" ssid="17">Mager- man and Marcus [10] propose a model of context that combines the n-gram model with information from dom- inating constituents.</S>
  <S sid="18" ssid="18">All of these aspects of context are necessary for disambiguation, yet none is sufficient.</S>
  <S sid="19" ssid="19">We propose a probabilistic model of context for disam- biguation in parsing, HBG, which incorporates the intu- itions of these previous works into one unified framework.</S>
  <S sid="20" ssid="20">Let p(T, w~) be the joint probabil ity of generating the word string w~ and the parse tree T. Given w~, our parser chooses as its parse tree that tree T* for which T* = argmaxp(T ,  w~) (1) TeP(wN where "P(w~) is the set of all parses produced by the grammar for the sentence w~.</S>
  <S sid="21" ssid="21">Many aspects of the input sentence that might be relevant o the decision-making process participate in the probabilistic model, provid- ing a very rich if not the richest model of context ever attempted in a probabilistic parsing model.</S>
  <S sid="22" ssid="22">In this paper, we will motivate and define the HBG model, describe the task domain, give an overview of the grammar, describe the proposed HBG model, and present he results of experiments comparing HBG with an existing state-of-the-art model.</S>
  <S sid="23" ssid="23">Mot ivat ion  fo r  H is tory -based Grammars One goal of a parser is to produce a grammatical  inter- pretation of a sentence which represents the syntactic and semantic intent of the sentence.</S>
  <S sid="24" ssid="24">To achieve this goal, the parser must have a mechanism for estimating the coherence of an interpretation, both in isolation and in context.</S>
  <S sid="25" ssid="25">Probabilistic language models provide such a mechanism.</S>
  <S sid="26" ssid="26">A probabilistic language model attempts to estimate the probability of a sequence of sentences and their respec- tive interpretations (parse trees) occurring in the lan- guage, "P(S1 T1 $2 T2 ... S~ T~).</S>
  <S sid="27" ssid="27">The difficulty in applying probabilistic models to natu- 134 ral language is deciding what aspects of the sentence and the discourse are relevant o the model.</S>
  <S sid="28" ssid="28">Most previous probabilistic models of parsing assume the probabilities of sentences in a discourse are independent of other sen- tences.</S>
  <S sid="29" ssid="29">In fact, previous works have made much stronger independence assumptions.</S>
  <S sid="30" ssid="30">The P-CFG model consid- ers the probability of each constituent rule independent of all other constituents in the sentence.</S>
  <S sid="31" ssid="31">The Pearl [10] model includes a slightly richer model of context, allow- ing the probability of a constituent rule to depend upon the immediate parent of the rule and a part-of-speech tri- gram from the input sentence.</S>
  <S sid="32" ssid="32">But none of these models come close to incorporating enough context to disam- biguate many cases of ambiguity.</S>
  <S sid="33" ssid="33">A significant reason researchers have limited the contex- tual information used by their models is because of the difficulty in estimating very rich probabilistic models of context.</S>
  <S sid="34" ssid="34">In this work, we present a model, the history- based grammar model, which incorporates a very rich model of context, and we describe a technique for es- timating the parameters for this model using decision trees.</S>
  <S sid="35" ssid="35">The history-based grammar model provides a mechanism for taking advantage of contextual informa- tion from anywhere in the discourse history.</S>
  <S sid="36" ssid="36">Using deci- sion tree technology, any question which can be asked of the history (i.e.</S>
  <S sid="37" ssid="37">Is the subject of the previous entence animate?</S>
  <S sid="38" ssid="38">Was the previous sentence a question?</S>
  <S sid="39" ssid="39">can be incorporated into the language model.</S>
  <S sid="40" ssid="40">The History-based Grammar Model The history-based grammar model defines context of a parse tree in terms of the leftmost derivation of the tree.</S>
  <S sid="41" ssid="41">Following [7], we show in Figure 1 a context-free gram- mar (CFG) for anb ~ and the parse tree for the sentence aabb.</S>
  <S sid="42" ssid="42">The leftmost derivation of the tree T in Figure 1 is: S ~ ASB --% aSB --h aABB --% aaBB -% aabB -A aabb (2) where the rule used to expand the i-th node of the tree is denoted by ri.</S>
  <S sid="43" ssid="43">Note that we have indexed the non- terminal (NT) nodes of the tree with this leftmost order.</S>
  <S sid="44" ssid="44">We denote by t~ the sentential form obtained just before we expand node i.</S>
  <S sid="45" ssid="45">Hence, t~ corresponds to the senten- tial form aSB or equivalently to the string rlr2.</S>
  <S sid="46" ssid="46">In a leftmost derivation we produce the words in left-to-right order.</S>
  <S sid="47" ssid="47">Using the one-to-one correspondence b tween leftmost derivations and parse trees, we can rewrite the joint S ~ ASBIAB A ~ a B ~ b a a b b Figure i: Grammar and parse tree for aabb.</S>
  <S sid="48" ssid="48">probability in (1) as: p(T, w~) = ~IP(r~]tf-) i=1 In a probabilistic ontext-free grammar (P-CFG), the probability of an expansion at node i depends only on the identity of the non-terminal N~, i.e., p(ri ITS-) -- p(r~).</S>
  <S sid="49" ssid="49">Thus p(T, w~) = l~P( r , ) i----1 So in P-CFG the derivation order does not affect the probabilistic model 1.</S>
  <S sid="50" ssid="50">A tess crude approximation than the usual P-CFG is to use a decision tree to determine which aspects of the left- most derivation have a bearing on the probability of how node i will be expanded.</S>
  <S sid="51" ssid="51">In other words, the probabil- ity distribution p(rilt~" ) will be modeled by p(r~lE[t~]) where E[~] is the equivalence class of the history t as de- termined by the decision tree.</S>
  <S sid="52" ssid="52">This allows our probabilis- tic model to use any information anywhere in the partial derivation tree to determine the probability of different expansions of the i-th non-terminal.</S>
  <S sid="53" ssid="53">The use of deci- sion trees and a large bracketed corpus may shift some of the burden of identifying the intended parse from the grammarian to the statistical estimation methods.</S>
  <S sid="54" ssid="54">We refer to probabilistic methods based on the derivation as History-based Grammars (HBG).</S>
  <S sid="55" ssid="55">iNote the abuse of notat ion since we denote by p(r~) the con- ditional probabil ity of rewriting the non-terminal Ni.</S>
  <S sid="56" ssid="56">135 In this paper, we explored a restricted implementation of this model in which only the path from the current node to the root of the derivation along with the index of a branch (index of the child of a parent ) are examined in the decision tree model to build equivalence classes of histories.</S>
  <S sid="57" ssid="57">Other parts of the subtree are not examined in the implementation f HBG.</S>
  <S sid="58" ssid="58">Task  Domain We have chosen computer manuals as a task domain.</S>
  <S sid="59" ssid="59">We picked the most frequent 3000 words in a corpus of 600,000 words from 10 manuals as our vocabulary.</S>
  <S sid="60" ssid="60">We then extracted a few million words of sentences that are completely covered by this vocabulary from 40,000,000 words of computer manuals.</S>
  <S sid="61" ssid="61">A randomly chosen sen- tence from a sample of 5000 sentences from this corpus is: 396.</S>
  <S sid="62" ssid="62">It indicates whether a call completed suc- cessfully or if some error was detected that caused the call to fail.</S>
  <S sid="63" ssid="63">To define what we mean by a correct parse, we use a corpus of manually bracketed sentences at the University of Lancaster called the Treebank.</S>
  <S sid="64" ssid="64">The Treebank uses 17 non-terminal labels and 240 tags.</S>
  <S sid="65" ssid="65">The bracketing of the above sentence is shown in Figure 2.</S>
  <S sid="66" ssid="66">[N It_PPH1 N] [V indicates_VVZ [Fn [Fn&amp;whether_CSW IN a_AT1 call_NN1 N] [V completed_VVD successfully_RR V]Fn~] or_CC [Fn+ iLCSW IN some_DD error_NN1 N]@ IV was_VBDZ detected_VVN V] @[Fr that_CST [V caused_VVD IN the_AT call_NN1 N] [Ti to_TO fail_VVI Wi]V]Fr]Fn+] Fn]V]._.</S>
  <S sid="67" ssid="67">Figure 2: Sample bracketed sentence from Lancaster Treebank.</S>
  <S sid="68" ssid="68">A parse produced by the grammar is judged to be correct if it agrees with the Treebank parse structurally and the NT labels agree.</S>
  <S sid="69" ssid="69">The grammar has a significantly richer NT label set (more than 10000) than the Treebank but we have defined an equivalence mapping between the grammar NT labels and the Treebank NT labels.</S>
  <S sid="70" ssid="70">In this paper, we do not include the tags in the measure of a correct parse.</S>
  <S sid="71" ssid="71">We have used about 25,000 sentences to help the gram- marian develop the grammar with the goal that the cor- rect (as defined above) parse is among the proposed (by the grammar) parses for a sentence.</S>
  <S sid="72" ssid="72">Our most common test set consists of 1600 sentences that are never seen by the grammarian.</S>
  <S sid="73" ssid="73">The  Grammar The grammar used in this experiment is a broad- coverage, feature-based unification grammar.</S>
  <S sid="74" ssid="74">The gram- mar is context-free but uses unification to express rule templates for the the context-free productions.</S>
  <S sid="75" ssid="75">For ex- ample, the rule template: (3) : n unspec  : n corresponds to three CFG productions where the second feature : n is either s, p, or : n. This rule template may elicit up to 7 non-terminals.</S>
  <S sid="76" ssid="76">The grammar has 21 features whose range of values maybe from 2 to about 100 with a median of 8.</S>
  <S sid="77" ssid="77">There are 672 rule templates of which 400 are actually exercised when we parse a corpus of 15,000 sentences.</S>
  <S sid="78" ssid="78">The number of productions that are realized in this training corpus is several hundred thousand.</S>
  <S sid="79" ssid="79">P -CFG While a NT in the above grammar is a feature vector, we group several NTs into one class we call a mnemonic represented by the one NT that is the least specified in that class.</S>
  <S sid="80" ssid="80">For example, the mnemonic VBOPASTSG* corresponds to all NTs that unify with: pos=v ] v - type = be tense - aspect -- past (4) We use these mnemonics to label a parse tree and we also use them to estimate a P-CFG, where the probabil ity of rewriting a NT is given by the probability of rewrit- ing the mnemonic.</S>
  <S sid="81" ssid="81">So from a training set we induce a CFG from the actual mnemonic productions that are elicited in parsing the training corpus.</S>
  <S sid="82" ssid="82">Using the Inside- Outside algorithm, we can estimate P-CFG from a large corpus of text.</S>
  <S sid="83" ssid="83">But since we also have a large corpus of bracketed sentences, we can adapt the Inside-Outside algorithm to reestimate the probability parameters sub- ject to the constraint hat only parses consistent with the Treebank (where consistency is as defined earlier) 136 contribute to the reestimation.</S>
  <S sid="84" ssid="84">From a training run of 15,000 sentences we observed 87,704 mnemonic produc- tions, with 23,341 NT mnemonics of which 10,302 were lexical.</S>
  <S sid="85" ssid="85">Running on a test set of 760 sentences 32% of the rule templates were used, 7% of the lexical mnemon- ics, 10% of the constituent mnemonics, and 5% of the mnemonic productions actually contributed to parses of test sentences.</S>
  <S sid="86" ssid="86">Grammar  and  Mode l  Per fo rmance Met r i cs To evaluate the performance of a grammar and an ac- companying model, we use two types of measurements: ?</S>
  <S sid="87" ssid="87">the any-consistent rate, defined as the percentage of sentences for which the correct parse is proposed among the many parses that the grammar provides for a sentence.</S>
  <S sid="88" ssid="88">We also measure the parse base, which is defined as the geometric mean of the num- ber of proposed parses on a per word basis, to quan- tify the ambiguity of the grammar.</S>
  <S sid="89" ssid="89">the Viterbi rate defined as the percentage of sen- tences for which the most likely parse is consistent.</S>
  <S sid="90" ssid="90">The arty-consistent ra e is a measure of the grammars coverage of linguistic phenomena.</S>
  <S sid="91" ssid="91">The Viterbi rate eval- uates the grammars  coverage with the statistical model imposed on the grammar.</S>
  <S sid="92" ssid="92">The goal of probabilistic modelling is to produce a Viterbi rate close to the arty- consistent rate.</S>
  <S sid="93" ssid="93">The any-consistent rate is 90% when we require the structure and the labels to agree and 96% when unla- beled bracketing is required.</S>
  <S sid="94" ssid="94">These results are obtained on 760 sentences from 7 t0 17 words long from test ma- terial that has never been seen by the grammarian.</S>
  <S sid="95" ssid="95">The parse base is 1.35 parses/word.</S>
  <S sid="96" ssid="96">This translates to about 23 parses for a 12-word sentence.</S>
  <S sid="97" ssid="97">The unlabeled Viterbi rate stands at 64% and the labeled Viterbi rate is 60%.</S>
  <S sid="98" ssid="98">While we believe that the above Vitevbi rate is close if not the state-of-the-art performance, there is room for improvement by using a more refined statistical model to achieve the labeled arty-cortsistertt ra e of 90% with this grammar.</S>
  <S sid="99" ssid="99">There is a significant gap between the labeled Viterbi and arty-cortsistent ra es: 30 percentage points.</S>
  <S sid="100" ssid="100">Instead of the usual approach where a grammarian tries to fine tune the grammar in the hope of improving the Viterbi rate we use the combination of a large Treebank and the resulting derivation histories with a decision tree building algorithm to extract statistical parameters that would improve the Viterbi rate.</S>
  <S sid="101" ssid="101">The grammarians task remains that of improving the arty-consistertt ra e. The history-based grammar model is distinguished from the context-free grammar model in that each constituent structure depends not only on the input string, but also the entire history up to that point in the sentence.</S>
  <S sid="102" ssid="102">In HBGs, history is interpreted as any element of the out- put structure, or the parse tree, which has already been determined, including previous words, non-terminal cat- egories, constituent structure, and any other linguistic information which is generated as part of the parse struc- ture.</S>
  <S sid="103" ssid="103">The  HBG Mode l Unlike P-CFG which assigns a probability to a mnemonic production, the HBG model assigns a probability to a rule template.</S>
  <S sid="104" ssid="104">Because of this the HBG formulation al- lows one to handle any grammar formalism that has a derivation process.</S>
  <S sid="105" ssid="105">For the HBG model, we have defined about 50 syntactic categories, referred to as Syn, and about 50 semantic categories, referred to as Sem.</S>
  <S sid="106" ssid="106">Each NT (and therefore mnemonic) of the grammar has been assigned a syntactic (Syn) and a semantic (Sere) category.</S>
  <S sid="107" ssid="107">We also associate with a non-terminal a primary lexical head, denoted by H1, and a secondary lexical head, denoted by H2.</S>
  <S sid="108" ssid="108">2 When a rule is applied to a non-terminal, it indicates which child will generate the lexical primary head and which child will generate the secondary lexical head.</S>
  <S sid="109" ssid="109">The proposed generative model associates for each con- stituent in the parse tree the probability: p( Syn, Sere, R, H1, H2 ISynp, Sernp, Rp, Ipc, Hip, H2p) In HBG, we predict the syntactic and semantic labels of a constituent, its rewrite rule, and its two lexical heads using the labels of the parent constituent, the parents lexical heads, the parents rule Rp that lead to the con- stituent and the constituents index Ipc as a child of Rp.</S>
  <S sid="110" ssid="110">As we discuss in a later section, we have also used with success more information about the derivation tree than the immediate parent in conditioning the probability of expanding a constituent.</S>
  <S sid="111" ssid="111">We have approximated the above probability by the fol- lowing five factors: 1. p(Syn IRp, Ipc, Hip, Synp, Semp) ~The pr imary  lexical head H1 corresponds (roughly) to the lin- guistic not ion of a lexical head.</S>
  <S sid="112" ssid="112">The secondary lexicM head H2 has  no l inguistic parallel.</S>
  <S sid="113" ssid="113">It merely represents a word in the con- s t i tuent  besides the head which conta ins predict ive in format ion about  the const i tuent .</S>
  <S sid="114" ssid="114">137 2. p( Sem ISyn, Rp, Ip?, Hip, It2p, Synp, Serf+) 3. p( R ISyn, Sere, Rp, Ip?, Hip, H2p, Synp, Sern~ ) 4. p( ul [R, Syn, Sere, Rp, Ipo, Hip, H2p ) 5. p(H2 ]Hi, R, Syn, Sem, Rp, Ip~, Synp) While a different order for these predictions is possible, we only experimented with this one.</S>
  <S sid="115" ssid="115">Parameter  Es t imat ion We only have built a decision tree to the rule probabil- ity component (3) of the model.</S>
  <S sid="116" ssid="116">For the moment, we are using n-gram models with the usual deleted interpo- lation for smoothing for the other four components of the model.</S>
  <S sid="117" ssid="117">We have assigned bit strings to the syntactic and seman- tic categories and to the rules manually.</S>
  <S sid="118" ssid="118">Our retention is that bit strings differing in the least significant bit posi- tions correspond to categories of non-terminals or rules that are similar.</S>
  <S sid="119" ssid="119">We also have assigned bitstrings for the words in the vocabulary (the lexical heads) using automatic lustering algorithms using the bigram mu- tual information clustering algorithm (see [4]).</S>
  <S sid="120" ssid="120">Given the bitsting of a history, we then designed a decision tree for modeling the probability that a rule will be used for rewriting a node in the parse tree.</S>
  <S sid="121" ssid="121">Since the grammar produces parses which may be more detailed than the Treebank, the decision tree was built using a training set constructed in the following man- ner.</S>
  <S sid="122" ssid="122">Using the grammar with the P-CFG model we de- termined the most likely parse that is consistent with the Treebank and considered the resulting sentence-tree pair as an event.</S>
  <S sid="123" ssid="123">Note that the grammar parse will also provide the lexical head structure of the parse?</S>
  <S sid="124" ssid="124">Then, we extracted using leftmost derivation order tuples of a his- tory (truncated to the definition of a history in the HBG model) and the corresponding rule used in expanding a node.</S>
  <S sid="125" ssid="125">Using the resulting data set we built a decision tree by classifying histories to locally minimize the en- tropy of the rule template.</S>
  <S sid="126" ssid="126">With a training set of about 9000 sentence-tree pairs, we had about 240,000 tuples and we grew a tree with about 40,000 nodes.</S>
  <S sid="127" ssid="127">This required 18 hours on a 25 MIPS RISC-based machine and the resulting decision tree was nearly 100 megabytes.</S>
  <S sid="128" ssid="128">Immediate  vs.  Funct iona l  Parents The HBG model employs two types of parents, the im- mediate parent and the functional parent.</S>
  <S sid="129" ssid="129">The immedi- ate parent is the constituent that immediately dominates with R: PP i Syn:  PP Sem: Wi th -Data Hi:  l i s t H2 : w i th Sem: Data Hi: l i s t H2: a Syn:  N a Sem: Data Hi:  l i s t H2: * I l i s t Figure 3: Sample representation f "with a list" in HBG model.</S>
  <S sid="130" ssid="130">the constituent being predicted?</S>
  <S sid="131" ssid="131">If the immediate parent of a constituent has a different syntactic type from that of the constituent, hen the immediate parent is also the functional parent; otherwise, the functional parent is the functional parent of the immediate parent?</S>
  <S sid="132" ssid="132">The distinc- tion between functional parents and immediate parents arises primarily to cope with unit productions?</S>
  <S sid="133" ssid="133">When unit productions of the form XP2 ~ XP1 occur, the im- mediate parent of XP1 is XP2.</S>
  <S sid="134" ssid="134">But, in general, the con- stituent XP2 does not contain enough useful information for ambiguity resolution.</S>
  <S sid="135" ssid="135">In particular, when consider- ing only immediate parents, unit rules such as NP2 -+ NP1 prevent he probabilistic model from allowing the NP1 constituent to interact with the VP rule which is the functional parent of NP1.</S>
  <S sid="136" ssid="136">When the two parents are identical as it often hap- pens, the duplicate information will be ignored.</S>
  <S sid="137" ssid="137">How- ever, when they differ, the decision tree will select that parental context which best resolves ambiguities.</S>
  <S sid="138" ssid="138">138 Figure 3 shows an example of the representation of a history in HBG for the prepositional phrase "with a list."</S>
  <S sid="139" ssid="139">In this example, the immediate parent of the N1 node is the NBAR4 node and the functional parent of N1 is the PP1 node.</S>
  <S sid="140" ssid="140">Resu l ts We compared the performance of HBG to the "broad- coverage" probabilistic ontext-free grammar, P-CFG.</S>
  <S sid="141" ssid="141">The any-consisgen$ rate of the grammar is 90% on test sentences of 7 to 17 words.</S>
  <S sid="142" ssid="142">The Vigerbi rate of P-CFG is 60% on the same test corpus of 760 sentences used in our experiments.</S>
  <S sid="143" ssid="143">On the same test sentences, the HBC model has a Viterbi rate of 75%.</S>
  <S sid="144" ssid="144">This is a reduction of 37% in error rate.</S>
  <S sid="145" ssid="145">Accuracy P-CFG 59.8% HBG 74.6% Error Reduction 36.8% Figure 4: Parsing accuracy: P-CFG vs. HBG In developing HBG, we experimented with similar mod- els of varying complexity.</S>
  <S sid="146" ssid="146">One discovery made during this experimentation is that models which incorporated more context than HBG performed slightly worse than HBG.</S>
  <S sid="147" ssid="147">This suggests that the current raining corpus may not contain enough sentences to estimate richer models.</S>
  <S sid="148" ssid="148">Based on the results of these experiments, it appears likely that significantly increasing the size of the train- ing corpus should result in a corresponding improvement in the accuracy of HBG and richer HBG-like models.</S>
  <S sid="149" ssid="149">To check the value of the above detailed history, we tried the simpler model: 1. p(Ht IH~p, H=p, Rp, Ipc) 2. p(H2 IHx, Hxp, H2p, Rp, Ip~) 3. p(Sy  IH1, Rp, I 0) 4. p(sem ISy., H1, Rp, Ipc) 5. p(R ISyn, Sam, H1, H2) This model corresponds to a P-CFG with NTs that are the crude syntax and semantic ategories annotated with the lexical heads.</S>
  <S sid="150" ssid="150">The Viterbi rate in this case was 66%, a small improvement over the P-CFG model indicating the value of using more context from the derivation tree.</S>
  <S sid="151" ssid="151">Conclusions The success of the HBG model encourages future de- velopment of general history-based grammars as a more promising approach than the usual P-CFG.</S>
  <S sid="152" ssid="152">More ex- perimentation is needed with a larger Treebank than was used in this study and with different aspects of the derivation history.</S>
  <S sid="153" ssid="153">In addition, this paper illustrates a new approach to grammar development where the pars- ing problem is divided (and hopefully conquered) into two subproblems: one of grammar coverage for the gram- marian to address and the other of statistical modeling to increase the probability of picking the correct parse of a sentence.</S>
  <S sid="154" ssid="154">Baker, J. K., 1975.</S>
  <S sid="155" ssid="155">Stochastic Modeling for Automatic Speech Understanding.</S>
  <S sid="156" ssid="156">In Speech Recognition, edited by Raj Reddy, Academic Press, pp.</S>
  <S sid="157" ssid="157">Brent, M. R. 1991.</S>
  <S sid="158" ssid="158">Automatic Acquisition of Subcate- gorization Frames from Untagged Free-text Corpora.</S>
  <S sid="159" ssid="159">In Proceedings of the 29th Annual Meeting of the Associa- tion for Computational Linguistics.</S>
  <S sid="160" ssid="160">Berkeley, California.</S>
  <S sid="161" ssid="161">Brill, E., Magerman, D., Marcus, M., and Santorini, B.</S>
  <S sid="162" ssid="162">Deducing Linguistic Structure from the Statis- tics of Large Corpora.</S>
  <S sid="163" ssid="163">In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop.</S>
  <S sid="164" ssid="164">Hid- den Valley, Pennsylvania.</S>
  <S sid="165" ssid="165">Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C., and Mercer, R. L. Class-based n-gram Models of Natural Language.</S>
  <S sid="166" ssid="166">In Proceedings of the IBM Natural Language ITL, March, 1990.</S>
  <S sid="167" ssid="167">Church, K. 1988.</S>
  <S sid="168" ssid="168">A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text.</S>
  <S sid="169" ssid="169">In Proceedings of the Second Conference on Applied Natural Language Processing.</S>
  <S sid="170" ssid="170">Gale, W. A. and Church, K. 1990.</S>
  <S sid="171" ssid="171">Poor Estimates of Context are Worse than None.</S>
  <S sid="172" ssid="172">In Proceedings of the June 1990 DARPA Speech and Natural Language Work- shop.</S>
  <S sid="173" ssid="173">Hidden Valley, Pennsylvania.</S>
  <S sid="174" ssid="174">Harrison, M. A.</S>
  <S sid="175" ssid="175">Introduction to Formal Language Theory.</S>
  <S sid="176" ssid="176">Addison-Wesley Publishing Company.</S>
  <S sid="177" ssid="177">Hindle, D. and Rooth, M. 1990.</S>
  <S sid="178" ssid="178">Structural Ambiguity and Lexical Relations.</S>
  <S sid="179" ssid="179">In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop.</S>
  <S sid="180" ssid="180">Hid- den Valley, Pennsylvania.</S>
  <S sid="181" ssid="181">Jelinek, F. 1985.</S>
  <S sid="182" ssid="182">Self-organizing Language Modeling for Speech Recognition.</S>
  <S sid="183" ssid="183">Magerman, D. M. and Marcus, M. P. 1991.</S>
  <S sid="184" ssid="184">Pearl: A Probabilistic Chart Parser.</S>
  <S sid="185" ssid="185">In Proceedings of the Febru- ary 1991 DARPA Speech and Natural Language Work- shop.</S>
  <S sid="186" ssid="186">Asilomar, California.</S>
  <S sid="187" ssid="187">Derouault, A., and Merialdo, B., 1985.</S>
  <S sid="188" ssid="188">Probabilistie Grammar for Phonetic to French Transcription.</S>
  <S sid="189" ssid="189">ICASSP 85 Proceedings.</S>
  <S sid="190" ssid="190">Tampa, Florida, pp.</S>
  <S sid="191" ssid="191">Sharman, R. A., Jelinek, F., and Mercer, R. 1990.</S>
  <S sid="192" ssid="192">Gen- erating a Grammar for Statistical Training.</S>
  <S sid="193" ssid="193">In Proceed- ings of the June 1990 DARPA Speech and Natural Lan- guage Vv*orkshop.</S>
  <S sid="194" ssid="194">Hidden Valley, Pennsylvania.</S>
</PAPER>
