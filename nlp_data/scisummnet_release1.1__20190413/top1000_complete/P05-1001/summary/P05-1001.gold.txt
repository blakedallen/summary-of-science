A High-Performance Semi-Supervised Learning Method For Text Chunking
In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue.
Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning.
The idea is to find “what good classifiers are like” by learning from thousands of automatically generated auxiliary classification problems on unlabeled data.
By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.
The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German).
We utilize a multi task learner within our semi-supervised algorithm to learn feature representations which are useful across a large number of related tasks.
Our structural learning method uses alternating structural optimization (ASO).
For both computational and statistical reasons, we follow compute a low-dimensional linear approximation to the pivot predictor space.
