<PAPER>
  <S sid="0">Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary</S>
  <ABSTRACT>
    <S sid="1" ssid="1">This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity.</S>
    <S sid="2" ssid="2">We propose a new supervised learning method for PPattachment based on a semantically tagged corpus.</S>
    <S sid="3" ssid="3">Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags.</S>
    <S sid="4" ssid="4">We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods.</S>
  </ABSTRACT>
  <SECTION title="1." number="1">
    <S sid="5" ssid="1">The problem with successful resolution of ambiguous prepositional phrase attachment is that we need to employ various types of knowledge.</S>
    <S sid="6" ssid="2">Consider, for example, the following sentence:</S>
  </SECTION>
  <SECTION title="1." number="2">
    <S sid="7" ssid="1">The PP for children can be either adjectival and attach to the object noun books or adverbial and attach to the verb buy, leaving us with the ambiguity of two possible syntactic structures: adj) VP(VB---buy, NP(NNS&#8212;books, PP(IN=for,NP(NN--children)))) adv) VP(VB=buy, NP(NNS&#8212;books), PP(IN=for,NP(NN=children))).</S>
    <S sid="8" ssid="2">It is obvious that without some contextual information we cannot disambiguate such a sentence correctly.</S>
    <S sid="9" ssid="3">Consider, however, the next sentence:</S>
  </SECTION>
  <SECTION title="2." number="3">
    <S sid="10" ssid="1">In this case, we can almost certainly state that the PP is adverbial, i.e. attached to the verb.</S>
    <S sid="11" ssid="2">This resolution is based on our life time experience in which we much more often encounter the activity which can be described as &amp;quot;buying things for money&amp;quot; than entities described as &amp;quot;books for money&amp;quot;1 .</S>
    <S sid="12" ssid="3">At the moment, we do not have a computer database containing life time experiences, and therefore we have to find another way of how to decide the correct PP attachment.</S>
    <S sid="13" ssid="4">One of the solutions lies in the exploration of huge textual corpora, which can partially substitute world knowledge.</S>
    <S sid="14" ssid="5">Partially, because we do not know how wide a context, what type of general knowledge or how deep an inference has to be applied for a successful disambiguation.</S>
    <S sid="15" ssid="6">If we limit the context around the prepositional phrase to include only the verb, its object and the PP itself, the human performance on PP attachment is approximately 88.2% accurate decisions [RRR94].</S>
    <S sid="16" ssid="7">Because people are capable of utilising their world knowledge, the remaining inaccuracy must be attributed to the lack of a wider context2.</S>
    <S sid="17" ssid="8">Statistically, each preposition has a certain percentage of occurrences for each attachment, relying on which would provide us with approximately 72.7% of correct attachments [C&amp;B95].</S>
    <S sid="18" ssid="9">If we manage to partially substitute the world knowledge, the resulting accuracy would lie in the range between 72.7 and 88.2%.</S>
    <S sid="19" ssid="10">These are the boundaries we expect an automatic system to score within.</S>
    <S sid="20" ssid="11">Altman and Steedman [A&amp;S88] have shown that in many cases PP can be attached correctly only if the context of the current discourse is used.</S>
    <S sid="21" ssid="12">Using the discourse context is, however, extremely difficult because we do not have enough theoretical background to decide which bits of context are needed to correctly disambiguate and which are irrelevant.</S>
    <S sid="22" ssid="13">There have been numerous attempts to substitute context by superficial knowledge extracted from a large corpus.</S>
    <S sid="23" ssid="14">Pioneering research on corpus-based statistical PP attachment ambiguity resolution has been done by Hindle and Rooth in [H&amp;R93].</S>
    <S sid="24" ssid="15">They extracted over 200,000 verb-nounpreposition triples with unknown attachment decisions.</S>
    <S sid="25" ssid="16">An iterative, unsupervised method was then used to decide between adjectival and adverbial attachment in which the decision was based on comparing the co-occurence probabilities of the given preposition with the verb and with the noun in each quadruple.</S>
    <S sid="26" ssid="17">Another promising approach is the transformation-based rule derivation presented by Brill and Resnik in [B&amp;R941, which is a simple learning algorithm that derives a set of transformation rules.</S>
    <S sid="27" ssid="18">These rules are then used for PP attachment and therefore, unlike the statistical methods, it is unnecessary to store huge frequency tables.</S>
    <S sid="28" ssid="19">Brill and Resnik had reported 81.8% success of this method on 500 randomly-selected sentences.</S>
    <S sid="29" ssid="20">The current statistical state-of-the art method is the backed-off model proposed by Collins and Brooks in [C&amp;B95] which performs with 84.5% accuracy on stand-alone quadruples.</S>
    <S sid="30" ssid="21">Most of the methods, however, suffer from a sparse data problem.</S>
    <S sid="31" ssid="22">All are based on matching the words from the analysed sentence against the words in the training set.</S>
    <S sid="32" ssid="23">The problem is that only exact matches are allowed.</S>
    <S sid="33" ssid="24">The back-off model showed an overall accuracy of 84.5%, but the accuracy of full quadruple matches was 92.6%!</S>
    <S sid="34" ssid="25">Due to the sparse data problem, however, the full quadruple matches were quite rare, and contributed to the result in only 4.8% of cases.</S>
    <S sid="35" ssid="26">The accuracy for a match on three words was also still relatively high (90.1%), while for doubles and singles it dropped substantially [C&amp;B95].</S>
    <S sid="36" ssid="27">This originated our assumption that if the number of matches on four and three words was raised, the overall accuracy would increase as well.</S>
    <S sid="37" ssid="28">Because Collins and Brooks' backing-off model is very profound, we could not find a way of improving its accuracy unless we increased the percentage of full quadruple and triple matches by employing the semantic distance measure instead of word-string matching.</S>
    <S sid="38" ssid="29">We feel that the sentence Buy books for children should be matched with Buy magazines for children due to the small conceptual distance between books and magazines.</S>
    <S sid="39" ssid="30">What is unknown, however, is the limit distance for two concepts to be matched.</S>
    <S sid="40" ssid="31">Many nouns in the WordNet hierarchy share the same root (entity) and there is a danger of overgeneralisation.</S>
    <S sid="41" ssid="32">We will try to overcome this problem through the supervised learning algorithm described herein.</S>
    <S sid="42" ssid="33">Another problem is that most of the words are semantically ambiguous and unless disambiguated, it is difficult to establish distances between them.</S>
    <S sid="43" ssid="34">The PP attachment also depends on the selection of word senses and vice versa, as will be shown in the result section.</S>
    <S sid="44" ssid="35">A number of other researchers have explored corpus-based approaches to PP attachment that make use of word classes.</S>
    <S sid="45" ssid="36">For examples, Weischedel [W91] and Basil [B91] both describe the use of manually constructed, domain specific word classes together with corpus-based statistics in order to resolve PP attachment ambiguity.</S>
    <S sid="46" ssid="37">Because these papers describe results obtained on different corpora, however, it is difficult to make a performance comparison.</S>
    <S sid="47" ssid="38">We will now discuss the issues connected with matching two different words based on their semantic distance.</S>
    <S sid="48" ssid="39">Employing the notion of semantic similarity, it is necessary to address a number of problems.</S>
    <S sid="49" ssid="40">At first, we have to specify the semantic hierarchy.</S>
    <S sid="50" ssid="41">Second, we need to determine how to calculate the distance between two different concepts in the hierarchy.</S>
    <S sid="51" ssid="42">Finally we must determine how to select a sense of a word based on a context in which it appears.</S>
  </SECTION>
  <SECTION title="SEMANTIC HIERARCHY" number="4">
    <S sid="52" ssid="1">The hierarchy we chose for semantic matching is the semantic network of WordNet 1M190], [MI93].</S>
    <S sid="53" ssid="2">WordNet is a network of meanings connected by a variety of relations.</S>
    <S sid="54" ssid="3">WordNet presently contains approximately 95.000 different word forms organised into 70.100 word meanings, or sets of synonyms.</S>
    <S sid="55" ssid="4">It is divided into four categories (nouns, verbs, adjectives and adverbs), out of which we will be using only verbs and nouns.</S>
    <S sid="56" ssid="5">Nouns are organised as 11 topical hierarchies, where each root represents the most general concept for each topic.</S>
    <S sid="57" ssid="6">Verbs, which tend to be more polysemous and can change their meanings depending on the kind of the object they take, are formed into 15 groups and have altogether 337 possible roots.</S>
    <S sid="58" ssid="7">Verb hierarchies are more shallow than those of nouns, as nouns tend to be more easily organised by the is-a relation, while this is not always possible for verbs.</S>
  </SECTION>
  <SECTION title="SEMANTIC DISTANCE" number="5">
    <S sid="59" ssid="1">The traditional method of evaluating semantic distance between two meanings based merely on the length of the path between the nodes representing them, does not work well in WordNet, because the distance also depends on the depth at which the concepts appear in the hierarchy.</S>
    <S sid="60" ssid="2">For example, the root entity is directly followed by the concept of life...farm, while a sedan, a type of a car, is in terms of path more distant from the concept of express_train, although they are both vehicles and therefore closer concepts.</S>
    <S sid="61" ssid="3">In the case of verbs, the situation is even more complex, because many verbs do not share the same hierarchy, and therefore there is no direct path between the concepts they represent.</S>
    <S sid="62" ssid="4">There have been numerous attempts to defme a measure for semantic distance of WordNet contained concepts [RE95],[K&amp;E96], [SU95], [SU96], etc.</S>
    <S sid="63" ssid="5">For our purposes, we have based the semantic distance calculation on a combination of the path distance between two nodes and their depth.</S>
    <S sid="64" ssid="6">Having ascertained the nearest common ancestor in the hierarchy, we calculate the distance as an average of the distance of the two concepts to their nearest common ancestor divided by the depth in the WordNet Hierarchy:</S>
  </SECTION>
  <SECTION title="2 DI D2" number="6">
    <S sid="65" ssid="1">where L1, L2 are the lengths of paths between the concepts and the nearest common ancestor, and D1, D2 are the depths of each concept in the hierarchy (the distance to the root).</S>
    <S sid="66" ssid="2">The more abstract the concepts are (the higher in hierarchy), the bigger the distance.</S>
    <S sid="67" ssid="3">The same concepts have a distance equal to 0; concepts with no common ancestor have a distance equal to 1.</S>
    <S sid="68" ssid="4">Because the verb hierarchy is rather shallow and wide, the distance between many verbal concepts is often In order to determine the position of a word in the semantic hierarchy, we have to determine the meaning of the word from the context in which it appears.</S>
    <S sid="69" ssid="5">For example, the noun bank can take any of the nine meanings defined in WordNet (financial institution, building, ridge, container, slope, etc.).</S>
    <S sid="70" ssid="6">It is not a trivial problem and has been approached by many researchers [GCY92], [YA93], [B&amp;W94], [RE95], [YA95], [KAE96], fL196}, etc.</S>
    <S sid="71" ssid="7">We believe that the word sense disambiguation can be accompanied by PP attachment resolution, and that they complement each other.</S>
    <S sid="72" ssid="8">At the same time we would like to note, that PP attachment and sense disambiguation are heavily contextually dependent problems.</S>
    <S sid="73" ssid="9">Therefore, we know in advance that without incorporation of wide context, the full disambiguation will be never reached.</S>
  </SECTION>
  <SECTION title="2." number="7">
    <S sid="74" ssid="1">The supervised learning algorithm which we have devised for the PP attachment resolution, and which is discussed in Chapter 3, is based on the induction of a decision tree from a large set of training examples which contain verb-noun-preposition-noun quadruples with disambiguated senses.</S>
    <S sid="75" ssid="2">Unfortunately, at the time of writing this work, a sufficiently big corpus which was both syntactically analysed and semantically tagged did not exist.</S>
    <S sid="76" ssid="3">Therefore, we used the syntactically analysed corpus [MA931 and assigned the word senses ourselves.</S>
    <S sid="77" ssid="4">Manual assignment, however, in the case of a huge corpus would be beyond our capacity and therefore we devised an automatic method for an approximate word sense disambiguation based on the following notions: Determining the correct sense of an ambiguous word is highly dependent on the context in which the word occurs.</S>
    <S sid="78" ssid="5">Even without any sentential context, the human brain is capable of disambiguating word senses based on circumstances or experience3.</S>
    <S sid="79" ssid="6">In natural language processing, however, we rely mostly on the sentential contexts, Le. on the surrounding concepts and relations between them.</S>
    <S sid="80" ssid="7">These two problems arise: 1.</S>
    <S sid="81" ssid="8">The surrounding concepts are very often expressed by ambiguous words and a correct sense for these words also has to be determined.</S>
    <S sid="82" ssid="9">2.</S>
    <S sid="83" ssid="10">What relations and how deep an inference is needed for correct disambiguation is unknown.</S>
    <S sid="84" ssid="11">We based our word-sense disambiguating mechanism on the premise that two ambiguous words usually tend to stand for their most similar sense if they appear in the same context.</S>
    <S sid="85" ssid="12">In this chapter we present a similarity-based disambiguation method aimed at disambiguating sentences for subsequent PP-attachment resolution.</S>
    <S sid="86" ssid="13">Similar contextual situations (these include information on the PP-attachment) are found in the training corpora and are used for the sense disambiguation.</S>
    <S sid="87" ssid="14">If, for example, the verb buy (4 senses) appears in the sentence: The investor bought the company for 5 million dollars and somewhere else in the training corpus there is a sentence4: The investor purchased the company for 5 million dollars, we can take advantage of this similarity and disambiguate the verb &amp;quot;buy&amp;quot; to its sense that is nearest to the sense of the verb purchase, which is not ambiguous.</S>
    <S sid="88" ssid="15">The situation, however, might not be as simplistic as that, because such obvious matches are extremely rare even in a huge corpus.</S>
    <S sid="89" ssid="16">The first problem is that the sample verb in the training corpus may be also ambiguous.</S>
    <S sid="90" ssid="17">Which sense do we therefore choose?</S>
    <S sid="91" ssid="18">The second problem is that there may, in fact, be no exact match in the training corpus for the context surrounding words and their relations.</S>
    <S sid="92" ssid="19">To overcome both of these problems we have applied the concept of semantic distance discussed above.</S>
    <S sid="93" ssid="20">Every possible sense of all the related context words is evaluated and the best match.Chosen5.</S>
    <S sid="94" ssid="21">The proposed unsupervised similarity-based iterative algorithm for the word sense disambiguation of the training corpus looks as follows: &amp;quot;-for each quadruple Q in the training set: for each ambiguous word in the quadruple: * among the remaining quadruples find a set S of similar quadruples (those with quadruple distance &lt; SDT) for each non-empty set S: * choose the nearest similar quadruple from the set S * disambiguate the ambiguous word to the nearest sense of the corresponding word of the chosen nearest quadruple * increase the Similarity Distance Threshold SDT = SDT + 0.1 Until all the quadruples are disambiguated or SDT =3.</S>
    <S sid="95" ssid="22">The above algorithm can be described as iterative clustering, because at first, the nearest quadruples are matched and disambiguated.</S>
    <S sid="96" ssid="23">Then, the similarity distance threshold is raised, and the process repeats itself in the next iteration.</S>
    <S sid="97" ssid="24">If a word is not successfully disambiguated, it is assigned its first, Le. the most frequent sense.</S>
    <S sid="98" ssid="25">The reason for starting with the best matches is that these tend to provide better disambiguations.</S>
    <S sid="99" ssid="26">Consider, for example, the following set of quadruples: At first, the algorithm tries to disambiguate quadruple Q1 .</S>
    <S sid="100" ssid="27">Starting with the verb, the algorithm searches for other quadruples which have the quadruple distance (see below) smaller than the current similarity distance threshold.</S>
    <S sid="101" ssid="28">For SDT-0 this means only for quadruples with all the words with semantic distance 0, i.e. synonyms.</S>
    <S sid="102" ssid="29">There are no matches found for Q1 and the algorithm moves to Q2, fmding quadruple Q4 as the only one matching such criteria.</S>
    <S sid="103" ssid="30">The verb buy in Q2 is disambiguated to the sense which is nearest to the sense of purchase in Q4, i.e. min(dist(buy,purchase))&#8212;dist(BUY-1,PURCHASE-1).0.</S>
    <S sid="104" ssid="31">The noun company cannot be disambiguated, because the matched nearest quadruple Q4 contains the same noun and such a disambiguation is not allowed; the description million is monosemous.</S>
    <S sid="105" ssid="32">Same process is called for all the remaining quadruples but further disambigaution with SDT41 is not possible (the verb purchase in Q4 has only one sense in WordNet and therefore there is no need for disambiguation; the noun company cannot be disambiguated against the same word).</S>
    <S sid="106" ssid="33">The iteration threshold is increased by 0.1 and the algorithm starts again with the first quadruple.</S>
    <S sid="107" ssid="34">No match is found for Q1 for any word and we have to move to quadruple Q2.</S>
    <S sid="108" ssid="35">Its verb is already disambiguated, therefore the algorithm looks for all the quadruples which have the quadruple distance for nouns below the SDT of OA and which contain similar nouns (see definition of similar below).</S>
    <S sid="109" ssid="36">The quadruple Q3 satisfies this criteria.</S>
    <S sid="110" ssid="37">Distances of all the combinations of senses of the noun company and business are calculated and the nearest match chosen to disambiguate the noun company in Q2: min(dist(company,business)dist(COmPANY-1, BUS1NESS-10.083 The algorithm then proceeds to the next quadruple, i.e.</S>
    <S sid="111" ssid="38">Q3.</S>
    <S sid="112" ssid="39">There are two quadruples which satisfy the similarity threshold for verbs: Q2 and Q4 (Q6 is not considered, because its verb is identical and therefore not similar).</S>
    <S sid="113" ssid="40">The verb buy in Q2 is already disambiguated and the distance to both Q2 and Q4 is the same, i.e.</S>
    <S sid="114" ssid="41">: dqv(Q3,Q2)=dqv(Q3,Q4)&#8212;(0.252+0.083+0)/3-0.0485 where the minimum semantic distance between the nearest senses of the verb acquire and buy is: min(di st(acquire,buy)iist(ACQUI RE-1 ,BUY-1)4.25 The verb acquire is disambiguated to the sense nearest to the sense of the verb buy and the algorithm proceeds to the noun business in Q3.</S>
    <S sid="115" ssid="42">The same two quadruples fall below the SDT for nouns, as dqn(Q3,Q2)=dqv(Q3,Q4)=(0.25+0.007+0)/3.0857 and the noun business of Q3 is disambiguated to its sense nearest to the disambiguated sense of company in Q2.</S>
    <S sid="116" ssid="43">The verb in Q4 is monosemous, therefore the algorithm finds a set of similar quadruples for nouns (Q2 qualifies in spite if having the same noun (company), because it has already been disambiguated in the previous steps): Q2, Q3 and Q6.</S>
    <S sid="117" ssid="44">The nearest quadruple in this set is Q2 (dqn(Q4,Q2)=0) and the noun company in Q4 is disambiguated to the sense of the noun in Q2.</S>
    <S sid="118" ssid="45">The quadruple Q5 has no similar quadruples for the current SDT and therefore the next quadruple is Q6.</S>
    <S sid="119" ssid="46">Similarly to the above disambiguations, both its verb and noun are disambiguated.</S>
    <S sid="120" ssid="47">There is no further match for any quadruple and therefore SDT is increased to 0.2 and the algorithm starts with Q1 again (the quadruples Q2, Q3, Q4 and Q6 are already fully disambiguated).</S>
    <S sid="121" ssid="48">No matches are found for SDT4.2 for neither Q1 or Q5.</S>
    <S sid="122" ssid="49">The algorithm iterates until SDT=0.6 which enables the disambiguation of the noun plant in Q1 to its sense nearest to the noun facility in Q5: dqn(Q1,Q5)=(0+0.3752+1/)2.57 as min(dist(p/antfacifiry)=1:1ist(PLANT-1,FACIUTY-1)=0.375.</S>
    <S sid="123" ssid="50">Similarly, the noun facility in Q6 is disambiguated, whereas the descriptions in both Q1 and Q5 cannot be successfully disambiguated because only a very small set of quadruples was used in this example.</S>
    <S sid="124" ssid="51">In this case, both the description week and inspection would be assigned their most frequent senses, i.e. the first senses of WordNet.</S>
    <S sid="125" ssid="52">In case of a bigger training set, most of the quadruples get disambiguated, however, with increasing SDT the disambiguation quality decreases.</S>
    <S sid="126" ssid="53">The above example shows the importance of iteration, because starting with lower SDT guarantees better results.</S>
    <S sid="127" ssid="54">If, for example, there was no iteration cycle and the algorithm tried to disambiguate the quadruples in the order in which they appear, the quadruple Q1 would be matched with Q6 and all its words would be disambiguated to inappropriate senses.</S>
    <S sid="128" ssid="55">Such a wrong disambiguation would further force wrong disambiguations in other quadruples and the overall result would be substantially less accurate.</S>
    <S sid="129" ssid="56">Another advantage of this disambiguation mechanism is that the proper nouns, which usually refer to people or companies, can be also disambiguated.</S>
    <S sid="130" ssid="57">For example, an unknown name ARBY in quadruple: acquire ARBY for million is matched with disambiguated noun in Q6 and also disambiguated to the COMPANY-1 sense, rather than to,PERSON (note, that even if Q6 was not disambiguated, the COMPANY-1 sense of subsidiary is semantically closer to the sense company of ARBY and therefore, although possible, the disambiguation of ARBY to the first sense of subsidiary (PERSON) would be dismissed).</S>
    <S sid="131" ssid="58">Similarity Distance Threshold defines the limit matching distance between two quadruples.</S>
    <S sid="132" ssid="59">The matching distance between two quadruples Q1=v1-n1-p-di and Q2=v2-n2-p-d2 is defined as follows (v=verb, n=--noun, p=preposition, d=description noun): where P is the number of pairs of words in the quadruples which have a common semantic ancestor, Le.</S>
    <S sid="133" ssid="60">P 1, 2 or 3 (if there is no such a pair, Dq = 00) and its purpose is to give higher priority to matches on more words.</S>
    <S sid="134" ssid="61">The distance of the currendy disambiguated word is squared in order to have a bigger weight in the distance Da (the currently disambiguated word must be different from the corresponding word in the matched quadruple6 unless it has been previously disambiguated).</S>
    <S sid="135" ssid="62">The distance between two words D(w1,w2) is defined as the minimum semantic distance between all the possible senses of the words w1 and w2.</S>
    <S sid="136" ssid="63">Two quadruples are similar, if their distance is less or equal to the current Similarity Distance Threshold, and if the currently disambiguated word is similar to the corresponding word in the matched quadruple.</S>
    <S sid="137" ssid="64">Two words are similar if their semantic distance is less than 1.0 and if either their character strings are different or if one of the words has been previously disambiguated.</S>
  </SECTION>
  <SECTION title="3." number="8">
    <S sid="138" ssid="1">For the attachment of the prepositional phrases in unseen sentences, we have modified Quinlan's ID3 algorithm [Q86], [BR91] which belongs to the the family of inductive learning algorithms.</S>
    <S sid="139" ssid="2">Using a huge training set of classified examples, it uncovers the importance of the individual words (attributes) and creates a decision tree that is later used for classification of unseen examples7.</S>
    <S sid="140" ssid="3">The algorithm uses the concepts of the WordNet hierarchy as attribute values and creates the decision tree in the following way: Let T be a training set of classified quadruples.</S>
    <S sid="141" ssid="4">1..</S>
    <S sid="142" ssid="5">If all the examples in T are of the same PP attachment type (or satisfy the homogeneity termination condition, see below) then the result is a leaf labelled with this type, else Let us briefly explain each step of the algorithm.</S>
    <S sid="143" ssid="6">1.</S>
    <S sid="144" ssid="7">If the examples belong to the same class (set T is homogenous), the tree expansion terminates.</S>
    <S sid="145" ssid="8">However, such situation is very unlikely due to the non-perfect training data.</S>
    <S sid="146" ssid="9">Therefore, we relaxed the complete homogeneity condition by terminating the expansion when more than 77% of the examples in the set belonged to the same class (the value of 77% was set experimentally as it provided the best classification results).</S>
    <S sid="147" ssid="10">If the set T is still heterogeneous and there are no more attribute values to divide with, the tree is terminated and the leaf is marked by the majority class of the node.</S>
    <S sid="148" ssid="11">2.</S>
    <S sid="149" ssid="12">We consider the most informative attribute to be the one which splits the set T into the most homogenous subsets, i.e. subsets with either a high percentage of samples with adjectival attachments and a low percentage of adverbial ones, or vice-versa.</S>
    <S sid="150" ssid="13">The optimal split would be such that all the subsets would contain only samples of one attachment type.</S>
    <S sid="151" ssid="14">For each attribute A, we split the set into subsets, each associated with attribute value Aw and containing samples which were unifiable with value Aw (belong to the same WordNet class).</S>
    <S sid="152" ssid="15">Then, we calculate the overall heterogeneity (OH) of all these subsets as a weighted sum of their expected information: OH = -y p(A = A.</S>
    <S sid="153" ssid="16">)[p(PP ADvl A = A.</S>
    <S sid="154" ssid="17">)log2p(PIDADvl A = + p(PPADA A = A.,)log 2 p(PPADA A = A.</S>
    <S sid="155" ssid="18">)], where p(PPADvIA=Aw) and p(PADJIA=Aw) represent the conditional probabilities of the adverbial and adjectival attachments, respectively.</S>
    <S sid="156" ssid="19">The attribute with the lowest overall heterogeneity is selected for the decision tree expansion.</S>
    <S sid="157" ssid="20">In the following example (Figure 2) we Classification in this case means deciding whether the PP is adjectival or averbial.</S>
    <S sid="158" ssid="21">Verbs of all the node quadruples belong to the WordNet class V. nouns to the class N and descriptions p.the class D. We assume, in this example, that the WordNet hierarchy class V has three subclasses (VI, V2, V3), class N has two subclasses (NI, N2) and class D has also two subclasses (DI, D2)8.</S>
    <S sid="159" ssid="22">We use the values Vi, V2 and V3, NI and N2, and Dl and 132 as potential values of the attribute A. Splitting by verb results in three subnodes with an overall heterogeneity 0.56, splitting by noun in two subnodes with OH----0.99 and by description with OH=0.88.</S>
    <S sid="160" ssid="23">Therefore, in this case we would choose the verb as an attribute for the tree expansion.</S>
    <S sid="161" ssid="24">3.</S>
    <S sid="162" ssid="25">The attribute is either a verb, noun, or a description noun9.</S>
    <S sid="163" ssid="26">Its values correspond to the concept identificators (synsets) of WordNet.</S>
    <S sid="164" ssid="27">At the beginning of the tree induction, the top roots of the WordNet hierarchy are taken as attribute values for splitting the set of training examples.</S>
    <S sid="165" ssid="28">At first, all the training examples (separately for each preposition) are split into subsets which correspond to the topmost concepts of WordNet, which contains 11 topical roots for nouns and description nouns, and 337 for verbs (both nouns and verbs have hierarchical structure, although the hierarchy for verbs is shallower and wider).</S>
    <S sid="166" ssid="29">The training examples are grouped into subnodes according to the disambiguated senses of their content words.</S>
    <S sid="167" ssid="30">This means that quadruples with words that belong to the same top classes start at the same node.</S>
    <S sid="168" ssid="31">Each group is further split by the attribute, which provides less heterogeneous splitting (all verb, noun and description attributes are tried for each group and the one by which the current node can be split into the least heterogeneous set of subnodes is selected).</S>
    <S sid="169" ssid="32">Branches that lead to empty subnodes (as a result of not having a matching training example for the given attribute value) are pruned- This process repeats in all the emerging subnodes, using the attribute values which correspond to the WordNet hierarchy, moving from its top to its leaves.</S>
    <S sid="170" ssid="33">When splitting the set of training examples by the attribute A according to its values Avv, the emerging subsets contain those quadruples whose attribute A value is lower, in the WordNet hierarchy, i.e. belongs to the same class.</S>
    <S sid="171" ssid="34">If some quadruples had the attribute value equal to the values of A, an additional subset is added but its further splitting by the same attribute is prohibited.</S>
    <S sid="172" ssid="35">As soon as the decision tree is induced, classifying an unseen quadruple is a relatively simple procedure.</S>
    <S sid="173" ssid="36">At first, the word senses of the quadruple are disambiguated by the algorithm described in Chapter 2, which is modified to exclude the SDT iteration cycles.</S>
    <S sid="174" ssid="37">Then a path is traversed in the decision tree, starting at its root and ending at a leaf.</S>
    <S sid="175" ssid="38">At each internal node, we follow the branch labelled by the attribute value which is the semantic ancestor of the attribute value of the quadruple (i.e. the branch attribute value is a semantic ancestori0 of the value of the quadruple attribute).</S>
    <S sid="176" ssid="39">The quadruple is assigned the attachment type associated with the leaf, i.e. adjectival or adverbial.</S>
    <S sid="177" ssid="40">If no match is found for the attribute value of the quadruple at any given node, the quadruple is assigned the majority type of the current node.</S>
  </SECTION>
  <SECTION title="4 TRAINING AND TESTING DATA" number="9">
    <S sid="178" ssid="1">The training and testing data, extracted from the Penn Tree Bank [MA93], are identical to that used by IRRR94], [C&amp;B951 for comparison purposes&amp;quot;.</S>
    <S sid="179" ssid="2">The data contained 20801 training and 3097 testing quadruples with 51 prepositions and ensured that there was no implicit training of the method on the test set itself.</S>
    <S sid="180" ssid="3">We have processed the training data in the following way: cr converted a the verbs into lower cases e- converted all the words into base forms 0- replaced four digit numbers by 'year' el- replaced nouns ending by -ing and not in WordNet by 'action' 0- eliminated examples with verbs that are not in WordNet a, eliminated examples with lower-case nouns that are not in WordNet, except for pronouns, whose senses were substituted by universal pronoun synsets er the upper-case nouns were assigned their lower case equivalent senses plus the senses of 'company' and 'person' al- the upper case nouns not contained in WordNet were assigned the senses of 'company and *person' 0- disabled all the intransitive senses of verbs e- assigned all the words (yet ambiguous) the sets of WordNet senses (synsets) The above processing together with the elimination of double occurrences and contradicting examples, reduced the training set to 17577 quadruples, with an average quadruple ambiguity of 86, as of the ambiguity definition in section 1.2.</S>
  </SECTION>
  <SECTION title="5." number="10">
    <S sid="181" ssid="1">Because the induction of the decision tree for the PP attachment is based on a supervised learning from sense-tagged examples, it was necessary to sense-disambiguate the entire training set.</S>
    <S sid="182" ssid="2">This was done by the iterative algorithm described in Chapter 2.</S>
    <S sid="183" ssid="3">To form an approximate evaluation of the quality of this disambiguation, we have randomly selected 500 words, manually12 assigned sets of possible senses to them (sets, because without a full sentential context a full disambiguation is not always possible), and compared these with the automatic disambiguation.</S>
    <S sid="184" ssid="4">If the automatically chosen sense was present in the manually assigned set, the disambiguation was considered correct.</S>
    <S sid="185" ssid="5">Out of these 500 words 362 could be considered correctly disambiguated, which represents slightly over 72%.</S>
    <S sid="186" ssid="6">We can argue that the insufficient disambiguation context, sparse data problem and empirically set iteration step in the disambiguating algorithm lead to an unreliable disambiguation.</S>
    <S sid="187" ssid="7">However, it is necessary to maintain the understanding that it is the PP attachment rather than the sense disambiguation that is our primary goal.</S>
    <S sid="188" ssid="8">Additionally, because the words of the input sentences for the PP attachment are to be assigned senses in the same manner, the sense disambiguation error is concealed.</S>
    <S sid="189" ssid="9">Alhouhg the disambiguation of the training set is computationally the most expensive part of the system, it is done only once.</S>
    <S sid="190" ssid="10">The disambiguation of unseen (testing) examples is done by the same algorithm which is modified to exclude the SDT iteration cycles.</S>
    <S sid="191" ssid="11">It is therefore reasonably fast even for real-life applications.</S>
    <S sid="192" ssid="12">The PP attachment using the decision tree is extremely efficient and reliable.</S>
    <S sid="193" ssid="13">We have induced the decision tree separately for each preposition in the training corpus, covering the 51 most common prepositions.</S>
    <S sid="194" ssid="14">The induced decision trees are relatively shallow and the classification of unseen sentences is rapid.</S>
    <S sid="195" ssid="15">As shown in the following table, our algorithm appears to have surpassed many existing methods and is very close to human performance on the same testing data13.</S>
    <S sid="196" ssid="16">The fact that many words in both the training and the testing sets were not found in WordNet caused a reduction in the accuracy.</S>
    <S sid="197" ssid="17">This is because training examples with an error or with a word not found in WordNet could not fully participate on the decision tree induction.</S>
    <S sid="198" ssid="18">This reduced the original training set of 20801 quadruples to 17577.</S>
    <S sid="199" ssid="19">In the case of the testing set, many of the 3097 testing quadruples were also handicapped by having no entry in WordNet.</S>
    <S sid="200" ssid="20">Attachment of these had to be based on a partial quadruple and was usually assigned at a higher level of the decision tree, which reduced the overall accuracy.</S>
    <S sid="201" ssid="21">In order to conduct a fair comparison, however, we used the same testing set as the methods shown in the above table.</S>
    <S sid="202" ssid="22">If just the examples with full WordNet entries were used, the accuracy rose to 90.8%.</S>
    <S sid="203" ssid="23">Although the algorithm does not provide high enough accuracy from the point of view of word sense disambiguation, it is more important to bear in mind that our main goal is the PP attachment ambiguity resolution.</S>
    <S sid="204" ssid="24">The relatively low accuracy of the word sense disambiguation is compensated by the fact that the same sense disambiguation error is present in both the training set and the classified quadruple.</S>
    <S sid="205" ssid="25">The use of the same training set for both the PP attachment and the sense disambiguation provides a positive bias in favour of correct attachment.</S>
    <S sid="206" ssid="26">Until we have a sufficiently big enough word sense tagged corpus, we can only hypothesise on the importance of the correct sense disambiguation for the PP attachment.</S>
    <S sid="207" ssid="27">Experiments, however, show that if the positive bias between the word senses of the training set and the testing quadruples is removed, the accuracy of the PP attachment falls substantially.</S>
    <S sid="208" ssid="28">We have conducted an experiment, in which the disambiguated senses of the testing set were replaced by the most frequent senses, i.e. the first senses as defined in WordNet.</S>
    <S sid="209" ssid="29">This caused a substantial reduction of accuracy to 76.5%.</S>
    <S sid="210" ssid="30">The fact that our approximate disambiguation (algorithm in Chapter 2) leads to 88.1% correct PP attachment is partly to be attributed to the positive bias of disambiguation of the testing examples against the same training set which is also used for the decision tree induction.</S>
    <S sid="211" ssid="31">The disambiguation errors are thus hidden by their replication in both the training and the testing sets.</S>
    <S sid="212" ssid="32">As we have already mentioned, Collins and Brooks [C&amp;B951 based their method on matching the testing quadruples against the set of training examples.</S>
    <S sid="213" ssid="33">The decision on the attachment was made according to which attachment type had a higher count in the training corpus.</S>
    <S sid="214" ssid="34">If no match for the given quadruple was found, the algorithm backed-off to a combined frequency count of the occurences of matches on three words only, i.e. on the verb-noun-preposition, verb-prepositiondescription and noun-preposition-description.</S>
    <S sid="215" ssid="35">If no match was found on any of the three words combination, the algorithm backed-off to a combined match on two words, i.e. one of the content words with a Preposition.</S>
    <S sid="216" ssid="36">If there was further no match found on two words, the attachment type was assigned according to the prepositional statistics, or, if the preposition was not present in the training corpus, the quadruple was assigned the adjectival default.</S>
    <S sid="217" ssid="37">There was a substantial decrease of accuracy between the triples and doubles stage.</S>
    <S sid="218" ssid="38">Our algorithm, on the other hand, has substantially reduced the number of classifications based on fewer words.</S>
    <S sid="219" ssid="39">This is because at the top of the decision tree all of the semantic tops of all of the content words of the given quadruple are compared with the semantic generalisations of the training examples represented through the nodes of the decision tree.</S>
    <S sid="220" ssid="40">Only if the homogeneity termination condition is satisfied before all three content words are compared, the decision is based on less than a full quadruple.</S>
    <S sid="221" ssid="41">The decision tree therefore represents a very useful mechanism for determining the semantic level at which the decision on the PP attachment is made.</S>
    <S sid="222" ssid="42">Collins and Brooks' have also demonstrated the importance of low count events in training data by an experiment where all counts less than 5 were put to zero.</S>
    <S sid="223" ssid="43">This effectively made their algorithm ignore low count events which resulted in the decrease of accuracy from 84.1 to 81.6%.</S>
    <S sid="224" ssid="44">This important feature is maintained in our approach by small homogenous leaves at higher levels of the decision tree, which usually accommodate the low count training examples.</S>
    <S sid="225" ssid="45">Figure 3 shows an interesting aspect of learning the prepositional phrase attachment from a huge corpus.</S>
    <S sid="226" ssid="46">We have selected five most common prepositions and compared their learning curves.</S>
    <S sid="227" ssid="47">It turned out that for the size of a training set smaller than 1000 examples, learning is rather unreliable and dependent on the quality of the chosen quadruples.</S>
    <S sid="228" ssid="48">For a bigger training set, the accuracy grows with its size until a certain maximum accuracy level is reached.</S>
    <S sid="229" ssid="49">This level is different for different prepositions and we hypothesise that it can be broken only when a wider sentential or discourse context is used.</S>
    <S sid="230" ssid="50">Accuracy/Size 0 500 1000 1500 2000 2500 3000 3500 Training corpus size Our algorithm also provides a qualification certainty based on the heterogeneity of the decision tree leaves.</S>
    <S sid="231" ssid="51">The tree leaves are heterogeneous for two reasons: 1) the tree expansion is terminated when a node contains more than 77% of examples belonging to the same class, or, 2) when there are examples in the node that cannot be further divided because the tree has reached the bottom of the WordNet hierarchy.</S>
    <S sid="232" ssid="52">The Table 2 shows that the incorrect attachments usually occur with a lower certainty than the correct ones, i.e. most of the incorrect attachments are marked as less certain.</S>
    <S sid="233" ssid="53">&#8226; The prepositional statistics indicates that there were no matches found for the given quadruple and the attachment was decided based on the statistical frequency of the given preposition.</S>
    <S sid="234" ssid="54">Adjectival default was used in three cases when the preposition was not found in the training set.</S>
    <S sid="235" ssid="55">The certainty between 0.5 and 0.8 accounts mostly for the examples whose attachment was made through the decision tree, but there was either a small number of examples that participated on the creation of the tree branch or the examples were not sufficiently representative (e.g. contradictory examples).</S>
    <S sid="236" ssid="56">Most of the examples in this category possibly require a wider sentential context for further improvement of accuracy.</S>
    <S sid="237" ssid="57">The certainty bigger than 0.8 and smaller than .1.0 accounts for the situations when the decision was based on a leaf whose further expansion was terminated by the homogeneity termination condition or simply some noisy or incorrectly disambiguated examples were involved in its creation15.</S>
    <S sid="238" ssid="58">Examples, which did not reach the bottom of the decision tree and were assigned the majority class of the node from which there was no appropriate branch to follow, were all classified with certainty between 0.5 and 1.0.</S>
    <S sid="239" ssid="59">The decision with certainly 1.0 is always based on a homogenous leaf.</S>
    <S sid="240" ssid="60">It does not exhibit the highest accuracy because many of the homogenous leaves are formed from only very few examples and many of these are erroneous.</S>
    <S sid="241" ssid="61">As Figure 3 shows, each preposition has a different saturation accuracy which cannot be surpassed unless a wider sentential context is used.</S>
    <S sid="242" ssid="62">We believe, however, that a bigger corpus would provide better word-sense disambiguation which in turn would allow to increase the homogeneity limit for the termination of the tree expansion.</S>
    <S sid="243" ssid="63">Heterogeneous nodes, which force the expansion of the decision tree to unnecessary extent, are caused by I) examples with an error in the word sense disambiguation, or by 2) examples, that can be both adjectival and adverbial if taken out of context.</S>
    <S sid="244" ssid="64">The second case cannot be eliminated by a bigger training corpus, however, the reduction of noisy examples would contribute to an increase in accuracy mainly in the case of small nodes which can now contain more noisy examples than correct ones and thus force a wrong attachment.</S>
    <S sid="245" ssid="65">We feel that a bigger corpus, would provide us with an increase of accuracy of &amp;quot;certainty I&amp;quot; attachments, which partly includes attachments based on the small leaves.</S>
    <S sid="246" ssid="66">Also, we believe that a bigger training corpus would increase performance in the case of less frequent prepositions which do not have enough training examples to allow for induction of a reliable decision tree.</S>
  </SECTION>
  <SECTION title="6." number="11">
    <S sid="247" ssid="1">The most computationally expensive part of the system is the word sense disambiguation of the training corpus.</S>
    <S sid="248" ssid="2">This, however, is done only once and the disambiguated corpus is stored for future classifications of unseen quadruples.</S>
    <S sid="249" ssid="3">The above experiments confirmed the expectations that using the semantic information in combination with even a very limited context leads to a substantial improvement of NLP techniques.</S>
    <S sid="250" ssid="4">Although our method exhibits an accuracy close to the human performance, we feel that there is still a space for improvement, particularly in using a wider sentential context (human performance on full sentential context is over 93%), more training data and/or more accurate sense disambiguation technique.</S>
    <S sid="251" ssid="5">We believe that there is further space for elaboration of our method, in particular, it would be interesting to know the exact relations between the accuracy and the termination condition, and between the corpus size and the optimum termination condition separately for each preposition.</S>
    <S sid="252" ssid="6">At the moment, we are working on an implementation of the algorithm to work on with a wider sentential context and on its incorporation within a more complex NLP system.</S>
  </SECTION>
</PAPER>
