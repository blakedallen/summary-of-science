<PAPER>
	<S sid="0">Lexical Semantic Relatedness with Random Graph Walks</S><ABSTRACT>
		<S sid="1" ssid="1">Many systems for tasks such as question answering, multi-document summarization, and infor mation retrieval need robust numerical measures of lexical relatedness.</S>
		<S sid="2" ssid="2">Standard thesaurus-based measures of word pair similarity are based on only a single path between those words in the thesaurus graph.</S>
		<S sid="3" ssid="3">By contrast, we propose a newmodel of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the en tire graph.</S>
		<S sid="4" ssid="4">Our model uses a random walk over nodes and edges derived from WordNet links and corpus statistics.</S>
		<S sid="5" ssid="5">We treat the graph as aMarkov chain and compute a word-specific sta tionary distribution via a generalized PageRank algorithm.</S>
		<S sid="6" ssid="6">Semantic relatedness of a word pair is scored by a novel divergence measure, ZKL, that outperforms existing measures on certain classes of distributions.</S>
		<S sid="7" ssid="7">In our experiments, the resultingrelatedness measure is the WordNet-based measure most highly correlated with human similar ity judgments by rank ordering at ? = .90.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="8" ssid="8">Several kinds of Natural Language Processing systems need measures of semantic relatedness for arbitrary wordpairs.</S>
			<S sid="9" ssid="9">For example, document summarization and ques tion answering systems often use similarity scores to evaluate candidate sentence alignments, and informationretrieval systems use relatedness scores for query expan sion.</S>
			<S sid="10" ssid="10">Several popular algorithms calculate scores from information contained in WordNet (Fellbaum, 1998), an electronic dictionary where word senses are explicitly connected by zero or more semantic relationships.</S>
			<S sid="11" ssid="11">Thecentral challenge of these algorithms is to compute rea sonable relatedness scores for arbitrary word pairs given that few pairs are directly connected.</S>
			<S sid="12" ssid="12">Most pairs in WordNet share no direct semantic link, and for some the shortest connecting path can be surprising?even pairs that seem intuitively related, such ?furnace?</S>
			<S sid="13" ssid="13">and ?stove?</S>
			<S sid="14" ssid="14">share a lowest common ancestor in the hypernymy taxonomy (is-a links) all the way upat ?artifact?</S>
			<S sid="15" ssid="15">(a man-made object).</S>
			<S sid="16" ssid="16">Several existing algorithms compute relatedness only by traversing the hyper nymy taxonomy and find that ?furnace?</S>
			<S sid="17" ssid="17">and ?stove?</S>
			<S sid="18" ssid="18">are relatively unrelated.</S>
			<S sid="19" ssid="19">However, WordNet provides other types of semantic links in addition to hypernymy, such as meronymy (part/whole relationships), antonymy, andverb entailment, as well as implicit links defined by over lap in the text of definitional glosses.</S>
			<S sid="20" ssid="20">These links can provide valuable relatedness information.</S>
			<S sid="21" ssid="21">If we assume that relatedness is transitive across a wide variety of such links, then it is natural to follow paths such as furnace?</S>
			<S sid="22" ssid="22">crematory?gas oven?oven?kitchen appliance?stove and find a higher degree of relatedness between ?furnace?</S>
			<S sid="23" ssid="23">and ?stove.?</S>
			<S sid="24" ssid="24">This paper presents the application of random walkMarkov chain theory to measuring lexical semantic re latedness.</S>
			<S sid="25" ssid="25">A graph of words and concepts is constructedfrom WordNet.</S>
			<S sid="26" ssid="26">The random walk model posits the exis tence of a particle that roams this graph by stochastically following local semantic relational links.</S>
			<S sid="27" ssid="27">The particle is biased toward exploring the neighborhood around a target word, and is allowed to roam until the proportion of time it visits each node in the limit converges to a stationarydistribution.</S>
			<S sid="28" ssid="28">In this way we can compute distinct, word specific probability distributions over how often a particle visits all other nodes in the graph when ?starting?</S>
			<S sid="29" ssid="29">from a specific word.</S>
			<S sid="30" ssid="30">We compute the relatedness of two words as the similarity of their stationary distributions.The random walk brings with it two distinct advan tages.</S>
			<S sid="31" ssid="31">First, it enables the similarity measure to have a principled means of combination of multiple types of edges from WordNet.</S>
			<S sid="32" ssid="32">Second, by traversing all links, thewalk aggregates local similarity statistics across the en tire graph.</S>
			<S sid="33" ssid="33">The similarity scores produced by our method are, to our knowledge, the WordNet-based scores most highly correlated with human judgments.</S>
			<S sid="34" ssid="34">581</S>
	</SECTION>
	<SECTION title="Related work. " number="2">
			<S sid="35" ssid="1">Budanitsky and Hirst (2006) provide a survey of many WordNet-based measures of lexical similarity based on paths in the hypernym taxonomy.</S>
			<S sid="36" ssid="2">As an example, one of the best performing is the measure proposed by Jiang and Conrath (1997) (similar to the one proposed by (Lin,1991)), which finds the shortest path in the taxonomic hi erarchy between two candidate words before computing similarity as a function of the information content of thetwo words and their lowest common subsumer in the hi erarchy.</S>
			<S sid="37" ssid="3">We note the distinction between word similarityand word relatedness.</S>
			<S sid="38" ssid="4">Similarity is a special case of relat edness in that related words such as ?cat?</S>
			<S sid="39" ssid="5">and ?fur?</S>
			<S sid="40" ssid="6">share some semantic relationships (such as meronymy), but do not express the same likeness of form as would similarwords such as ?cat?</S>
			<S sid="41" ssid="7">and ?lion.?</S>
			<S sid="42" ssid="8">The Jiang-Conrath mea sure and most other measures that primarily make use of of hypernymy (is-a links) in the WordNet graph are better categorized as measures of similarity than of relatedness.</S>
			<S sid="43" ssid="9">Other measures have been proposed that utilize the text in WordNet?s definitional glosses, such as Extended Lesk(Banerjee and Pedersen, 2003) and later the Gloss Vectors (Patwardhan and Pedersen, 2006) method.</S>
			<S sid="44" ssid="10">These ap proaches are primarily based on comparing the ?bag of words?</S>
			<S sid="45" ssid="11">of two synsets?</S>
			<S sid="46" ssid="12">gloss text concatenated with the text of neighboring words?</S>
			<S sid="47" ssid="13">glosses in the taxonomy.</S>
			<S sid="48" ssid="14">As a result, these gloss-based methods measure relatedness.</S>
			<S sid="49" ssid="15">Our model captures some of this relatedness information by including weighted links based on gloss text.</S>
			<S sid="50" ssid="16">A variety of other measures of semantic relatedness have been proposed, including distributional similarity measures based on co-occurrence in a body of text?see (Weeds and Weir, 2005) for a survey.</S>
			<S sid="51" ssid="17">Other measures make use of alternative structured information resources than WordNet, such as Roget?s thesaurus (Jar masz and Szpakowicz, 2003).</S>
			<S sid="52" ssid="18">More recently, measures incorporating information from Wikipedia (Gabrilovich and Markovitch, 2007; Strube and Ponzetto, 2006) have reported stronger results on some tasks than have been achieved by existing measures based on shallower lexical resources.</S>
			<S sid="53" ssid="19">The results of our algorithm are competitivewith some Wikipedia algorithms while using only Word Net 2.1 as the underlying lexical resource.</S>
			<S sid="54" ssid="20">The approach presented here is generalizable to construction from any underlying semantic resource.PageRank is the most well-known example of a random walk Markov chain?see (Berkhin, 2005) for a sur vey.</S>
			<S sid="55" ssid="21">It uses the local hyperlink structure of the web to define a graph which it walks to aggregate popularityinformation for different pages.</S>
			<S sid="56" ssid="22">Recent work has ap plied random walks to NLP tasks such as PP attachment(Toutanova et al, 2004), word sense disambiguation (Mi halcea, 2005; Tarau et al, 2005), and query expansion (Collins-Thompson and Callan, 2005).</S>
			<S sid="57" ssid="23">However, to ourknowledge, the literature in NLP has only considered us ing one stationary distribution per specially-constructedgraph as a probability estimator.</S>
			<S sid="58" ssid="24">In this paper, we in troduce a measure of semantic relatedness based on thedivergence of the distinct stationary distributions result ing from random walks centered at different positions in the word graph.</S>
			<S sid="59" ssid="25">We believe we are the first to define such a measure.</S>
	</SECTION>
	<SECTION title="Random walks on WordNet. " number="3">
			<S sid="60" ssid="1">Our model is based on a random walk of a particle through a simple directed graphG = (V,E)whose nodes V and edges E are extracted from WordNet version 2.1.</S>
			<S sid="61" ssid="2">Formally, we define the probability n(t)i of finding the particle at node ni ? V at time t as the sum of all ways in which the particle could have reached ni from any other node at the previous time-step: n(t)i = ? nj?V n(t?1)j P (ni | nj)where P (ni | nj) is the conditional probability of moving to ni given that the particle is at nj . In partic ular, we construct the transition distribution such that P (ni | nj) &gt; 0 whenever WordNet specifies a local link relationship of the form j ? i. Note that this randomwalk is a Markov chain because the transition probabilities at time t are independent of the particle?s past trajec tory.</S>
			<S sid="62" ssid="3">The subsections that follow present the construction of the graph for our random walk from WordNet and the mathematics of computing the stationary distribution for a given word.</S>
			<S sid="63" ssid="4">3.1 Graph Construction.</S>
			<S sid="64" ssid="5">WordNet is itself a graph over synsets.</S>
			<S sid="65" ssid="6">A synset is best thought of as a concept evoked by one sense of one or more words.</S>
			<S sid="66" ssid="7">For instance, different senses of the word ?bank?</S>
			<S sid="67" ssid="8">take part in different synsets (e.g. a river bank versus a financial institution), and a single synset can be represented by multiple synonymous words, such as?middle?</S>
			<S sid="68" ssid="9">and ?center.?</S>
			<S sid="69" ssid="10">WordNet explicitly marks seman tic relationships between synsets, but we are additionally interested in representing relatedness between words.</S>
			<S sid="70" ssid="11">Wetherefore extract the following types of nodes fromWord Net: Synset Each WordNet synset has a corresponding node.For example, one node corresponds to the synset re ferred to by ?dog#n#3,?</S>
			<S sid="71" ssid="12">the third sense of dog as noun, whose meaning is ?an informal term for a man.?</S>
			<S sid="72" ssid="13">There are 117,597 Synset nodes.</S>
			<S sid="73" ssid="14">582TokenPOS One node is allocated to every word coupled with a part of speech, such as ?dog#n? mean ing dog as a noun.</S>
			<S sid="74" ssid="15">These nodes link to all the synsets they participate in, so that ?dog#n? links the Synset nodes for canine, hound, hot dog, etc. Collocations?multi-word expressions such as ?hot dog??that take part in a synsets are also represented by these nodes.</S>
			<S sid="75" ssid="16">There are 156,588 TokenPOS nodes.</S>
			<S sid="76" ssid="17">Token Every TokenPOS is connected to a Token node corresponding to the word when no part of speech information is present.</S>
			<S sid="77" ssid="18">For example, ?dog?</S>
			<S sid="78" ssid="19">links to ?dog#n? and ?dog#v?</S>
			<S sid="79" ssid="20">(meaning ?to chase?).</S>
			<S sid="80" ssid="21">There are 148,646 Token nodes.Synset nodes are connected with edges corresponding to many of the relationship types in Word Net.</S>
			<S sid="81" ssid="22">We use these WordNet relationships to form edges: hypernym/hyponym, instance/instance of, all holonym/meronym links, antonym, entails/entailed by, adjective satellite, causes/caused by, participle, pertainsto, derives/derived from, attribute/has attribute, and topical (but not regional or usage) domain links.</S>
			<S sid="82" ssid="23">By con struction, each edge created from a WordNet relationshipis guaranteed to have a corresponding edge in the oppo site direction.</S>
			<S sid="83" ssid="24">Edges that connect a TokenPOS to the Synsets using it are weighted based on a Bayesian estimate drawn from the SemCor frequency counts included in WordNet but with a non-uniform Dirichlet prior.</S>
			<S sid="84" ssid="25">Our edge weights are the SemCor frequency counts for each target Synset, with pseudo-counts of .1 for all Synsets, 1 for first sense ofeach word, and .1 for the first word in each Synset.</S>
			<S sid="85" ssid="26">Intuitively, this causes the particle to have a higher probabil ity of moving to more common senses of a TokenPOS; for example, the edges from ?dog#n? to ?dog#n#1?</S>
			<S sid="86" ssid="27">(canine) and ?dog#n#5?</S>
			<S sid="87" ssid="28">(hotdog) have un-normalized weights of43.2 and 0.1, respectively.</S>
			<S sid="88" ssid="29">The edges connecting a To ken to the TokenPOS nodes in which it can occur are alsoweighted by the sum of the weights of the outgoing TokenPOS?Synset links.</S>
			<S sid="89" ssid="30">Hence a walk starting at a com mon word like ?cat?</S>
			<S sid="90" ssid="31">is far more likely to follow a link to ?cat#n? than to rarities like ?cat#v?</S>
			<S sid="91" ssid="32">(to vomit).</S>
			<S sid="92" ssid="33">These edges are uni-directional; no edges are created from a Synset to a TokenPOS that can represent the Synset.</S>
			<S sid="93" ssid="34">In order for our graph construction to incorporatetextual gloss-based information, we also create uni directional edges from Synset nodes to the TokenPOS nodes for the words and collocations used in that synset?s gloss definition.</S>
			<S sid="94" ssid="35">This requires part-of-speech tagging the glosses, for which we use the Stanford maximum entropytagger (Toutanova et al, 2003).</S>
			<S sid="95" ssid="36">It is important to correctly weight these edges, because high-frequency stopwords such as ?by?</S>
			<S sid="96" ssid="37">and ?he?</S>
			<S sid="97" ssid="38">do not convey much in formation and might serve only to smear the probability mass across the whole graph.</S>
			<S sid="98" ssid="39">Gloss-based links to these nodes should therefore be down-weighted or removed.</S>
			<S sid="99" ssid="40">On the other hand, up-weighting extremely rare words such as by tf-idf scoring might also be inappropriate because such rare words would get extremely high scores,which is an undesirable trait in similarity search.</S>
			<S sid="100" ssid="41">(Haveliwala et al, 2002) and others have shown that a ?non monotonic document frequency?</S>
			<S sid="101" ssid="42">(NMDF) weighting can be more effective in such a setting.</S>
			<S sid="102" ssid="43">Because the frequency of words in the glosses is distributed by a power-law, we weight each word by its distance from the mean word count in log space.</S>
			<S sid="103" ssid="44">Formally, the weight wi for a word appearing ri times is wi = exp ( ?</S>
			<S sid="104" ssid="45">(log(ri)?</S>
			<S sid="105" ssid="46">?)2 2?2 ) where ? and ? are the mean and standard deviation ofthe logs of all word counts.</S>
			<S sid="106" ssid="47">This is a smooth approximation to the high and low frequency stop lists used effectively by other measures such as (Patwardhan and Ped ersen, 2006).</S>
			<S sid="107" ssid="48">We believe that because non-monotonic frequency scaling has no parameters and is data-driven,it could stand to be more widely adopted among gloss based lexical similarity measures.</S>
			<S sid="108" ssid="49">We also add bi-directional edges between Synsets whose word senses overlap with a common TokenPOS.</S>
			<S sid="109" ssid="50">These edges have raw weights given by the number ofTokenPOS nodes shared by the Synsets.</S>
			<S sid="110" ssid="51">The intuition be hind adding these edges is that WordNet often divides the meanings of words into fine-grained senses with similarmeanings, so there is likely to be some semantic relation ship between Synsets sharing a common TokenPOS.</S>
			<S sid="111" ssid="52">The final graph has 422,831 nodes and 5,133,281 edges.</S>
			<S sid="112" ssid="53">This graph is very sparse; fewer than 1 in 10,000node pairs are directly connected.</S>
			<S sid="113" ssid="54">When only the un weighted WordNet relationship edges are considered, the largest degree of any node is ?city#n#1?</S>
			<S sid="114" ssid="55">with 667 edges (mostly connecting to particular cities), followed by ?law#n#2?</S>
			<S sid="115" ssid="56">with 602 edges (mostly connecting to alarge number of domain terms such as ?dissenting opinion?</S>
			<S sid="116" ssid="57">and ?freedom of speech?), and each node is on aver age connected to 1.7 other nodes.</S>
			<S sid="117" ssid="58">When the gloss-based edges are considered separately, the highest degree nodesare those with the longest definitions; the maximum out degree is 56 and the average out-degree is 6.2.</S>
			<S sid="118" ssid="59">For the edges linking TokenPOS nodes to the Synsets in which they participate, TokenPOS nodes with many senses are the most connected; ?break#v? with 59 outgoing edges and ?make#v? with 49 outgoing edges have the highest out-degrees, with the average out-degree being 1.3.</S>
			<S sid="119" ssid="60">3.2 Computing the stationary distribution.</S>
			<S sid="120" ssid="61">Each of the K edge types presented above can be repre sented as separate transition matrix Ek ? RN?N where 583N is the total number of nodes.</S>
			<S sid="121" ssid="62">For each matrix, column j contains contains a normalized outgoing proba bility distribution,1 so the weight in cell (i, j) contains PK(ni | nj), the conditional probability of moving from node nj to node ni in edge type K. For many of the edge types, this is either 0 or 1, but for the weighted edges, these are real valued.</S>
			<S sid="122" ssid="63">The full transition matrixM is then the column normalized sum of all of the edge types: M?</S>
			<S sid="123" ssid="64">= ? k Ek M = (?</S>
			<S sid="124" ssid="65">?M? ? ?</S>
			<S sid="125" ssid="66">)?1 ? M? M is a distillation of relevant relatedness informationabout all nodes extracted from WordNet and is not tailored for computing a stationary distribution for any specific word.</S>
			<S sid="126" ssid="67">In order to compute the stationary distribu tion vdog#n for a walk centered around the TokenPOS ?dog#n,?</S>
			<S sid="127" ssid="68">we first define an initial distribution v(0)dog#n that places all the probability mass in the single vector entry corresponding to ?dog#n.?</S>
			<S sid="128" ssid="69">Then at every step of the walk, we will return to v(0) with probability ?.</S>
			<S sid="129" ssid="70">Intuitively, this return probability captures the notion that nodes close to?dog#n? should be given higher weight, and also guaran tees that the stationary distribution exists and is unique(Bremaud, 1999).</S>
			<S sid="130" ssid="71">The stationary distribution v is com puted via an iterative update algorithm: v(t) = ?v(0) + (1?</S>
			<S sid="131" ssid="72">?)Mv(t?1) Because the walk may return to the initial distribution v(0) at any step with probability ?, we found that v(t) converges to its unique stationary distribution v(?)</S>
			<S sid="132" ssid="73">in anumber of steps roughly proportional to ??1.</S>
			<S sid="133" ssid="74">We experi mented with a range of return probabilities and found that our results were relatively insensitive to this parameter.</S>
			<S sid="134" ssid="75">Our convergence criteria was ? ?v(t?1) ? v(t) ? ?</S>
			<S sid="135" ssid="76">1 &lt; 10?10, which, for our graph with a return probability of ? = .1,was met after about two dozen iterations.</S>
			<S sid="136" ssid="77">This computation takes under two seconds on a modern desktop ma chine.</S>
			<S sid="137" ssid="78">Note that because M is sparse, each iteration of theabove computation is linear in the total number of non zero entries in P , i.e. linear in the total number of edges.</S>
			<S sid="138" ssid="79">Introducing an edge type that is dense would dramatically increase running time.</S>
			<S sid="139" ssid="80">3.3 Model variants.</S>
			<S sid="140" ssid="81">For this paper, we consider three model variants that dif fer based on which subset of the edge types are included 1The frequency-count derived edges are normalized by thelargest column sum.</S>
			<S sid="141" ssid="82">This effectively preserves relative term fre quency information across the graph and causes some columns to sum to less than one.</S>
			<S sid="142" ssid="83">We interpret this lost mass as a link to ?nowhere.?</S>
			<S sid="143" ssid="84">in the transition matrix M . MarkovLink This variant includes the explicit WordNetrelations such as hypernymy and the edges representing overlap between the TokenPOS nodes con tained in Synsets.</S>
			<S sid="144" ssid="85">A particle walking through this graph reaches only Synset nodes and can step from one Synset to another whenever WordNet specifies a relationship between the Synsets or when the Synsets share a common word.</S>
			<S sid="145" ssid="86">There is a single connected component in this model variant.</S>
			<S sid="146" ssid="87">This model isloosely analogous to a smoothed version of the path based WordNet measures surveyed in (Budanitskyand Hirst, 2006) but differs in that it integrates multiple link types and aggregates relatedness informa tion across all paths in the graph.</S>
			<S sid="147" ssid="88">MarkovGloss This variant includes only the weighteduni-directional edges linking Synsets to the Token POS nodes contained in their gloss definitions, andthe edges from a TokenPOS node to the Synsets con taining it.</S>
			<S sid="148" ssid="89">The intuition behind this model variant is that the particle can move as if it were recursively looking up words in a dictionary, stepping from Synsets to the Synsets used to define them.</S>
			<S sid="149" ssid="90">Because WordNet?s gloss definitions are not sense-tagged,the particle must make an intermediate step to a To kenPOS contained in the gloss definition and then to a Synset representing a particular sense of that TokenPOS.</S>
			<S sid="150" ssid="91">The availability of sense-tagged glosseswould eliminate the noise introduced by this inter mediate step.</S>
			<S sid="151" ssid="92">The particle can reach both Synsets and TokenPOS nodes in this variant, but some parts of the graph are not reachable from other parts.</S>
			<S sid="152" ssid="93">This model incorporates much of the same information as the gloss-based WordNet measures (Banerjee and Pedersen, 2003; Patwardhan and Pedersen, 2006) but differs in that it considers many more glosses than just those in the immediate neighborhoods of the candidate words.</S>
			<S sid="153" ssid="94">MarkovJoined This variant is the natural combination of the above two; we construct the graph containing WordNet relation edges, Synset overlap edges, and gloss-based Synset to TokenPOS edges.</S>
			<S sid="154" ssid="95">Many of the characteristics of the model variants can be understood in terms of how much probability mass they assign to each node for a particular word-specific stationary distribution.</S>
			<S sid="155" ssid="96">Table 1 shows the highest scoringnodes in the word-specific stationary distributions cen tered around the Token node for ?wizard,?</S>
			<S sid="156" ssid="97">as computed bythe MarkovLink and MarkovGloss variants.</S>
			<S sid="157" ssid="98">In both variants, the ?wizard?</S>
			<S sid="158" ssid="99">Token?s only neighbors are the ?wiz ard#n? and ?wizard#a? TokenPOS nodes, and ?wizard#n? 584 MarkovLink MarkovGloss Node Probability Node Probability wizard 1.0E-1 wizard 1.3E-01 wizard#n 2.5E-3 wizard#n 2.9E-02 wizard#a 7.8E-5 wizard#a 9.1E-04 ace#n#3 4.2E-5 ace#n#3 1.1E-06 sorcerer#n#1 2.2E-6 sorcerer#n#1 5.8E-07 charming#a#2 2.2E-6 dazzlingly#r 2.4E-08 expert#n#1 1.1E-6 charming#a#2 1.6E-09 track star#n#1 1.1E-6 sorcery#n 2.6E-10 occultist#n#1 5.7E-7 magic#n 6.8E-12 Cagliostro#n#1 5.7E-7 magic#a 6.8E-12 star#v#2 5.5E-7 dazzlingly#r#1 4.3E-14 breeze_through#v#1 5.4E-7 dazzle#n 9.4E-16 magic#n#1 2.1E-8 beholder#n 9.4E-16 sorcery#n#1 2.1E-7 dazzle#v 9.4E-16 magician#n#1 1.9E-7 magic#n#1 5.1E-16Table 1: Highest scoring nodes in the stationary distri butions for ?wizard#n? as generated by the MarkovLinkmodel and the MarkovGloss model with return probabil ity 0.1.has a higher probability mass because of its higher SemCor usage counts.</S>
			<S sid="159" ssid="100">Likewise, the only possible steps per mitted in either variant from ?wizard#n? and ?wizard#a? are to the Synsets that can be expressed with those nodes: ?ace#n#3,?</S>
			<S sid="160" ssid="101">?sorcerer#n#1,?</S>
			<S sid="161" ssid="102">and ?charming#a#1.?</S>
			<S sid="162" ssid="103">Again, the amount of mass given to these nodes depends on the strength of these edge weights, which is determined by the SemCor usage counts.</S>
			<S sid="163" ssid="104">The highest probability nodes in the table are common because both model variants share the same initial links.However, the orders of the remaining nodes in the station ary distributions are different.</S>
			<S sid="164" ssid="105">In the MarkovLink variant, the random walk can only proceed to other Synsets usingWordNet relationship edges; ?track star#n#1?</S>
			<S sid="165" ssid="106">and ?ex pert#n#1?</S>
			<S sid="166" ssid="107">are first reached by following hyponym and hypernym edges from ?ace#n#1,?</S>
			<S sid="167" ssid="108">and ?occultist#n#1?</S>
			<S sid="168" ssid="109">and ?Cagliostro#n#1?</S>
			<S sid="169" ssid="110">are first reached with hypernym and instance edges from ?sorcerer#n#1.?</S>
			<S sid="170" ssid="111">The node?breeze through#v#1?</S>
			<S sid="171" ssid="112">is reached through a path follow ing derivational links with ?ace#n? and ?ace#v.?The MarkovGloss variant in table 1 shows how infor mation can be extracted solely from the textual glosses.</S>
			<S sid="172" ssid="113">Once the random walk reaches the first Synset nodes, itcan step to the TokenPOS nodes in their glosses; for example, ?ace#n#1?</S>
			<S sid="173" ssid="114">has the gloss ?someone who is daz zlingly skilled in any field.?</S>
			<S sid="174" ssid="115">Links to TokenPOS nodes that are very common in glosses are down-weighted with NMDF weighting, so ?someone#n? receives little mass while ?dazzlingly#r? receives more.</S>
			<S sid="175" ssid="116">From there, therandom walk can step to another Synset such as ?daz MarkovLink model MarkovGloss model Figure 1: Example stationary distributions plotted against each other for similar (top) and dissimilar (bottom) word pairs, using the MarkovLink (left) and MarkovGloss (right) model variants.</S>
			<S sid="176" ssid="117">zlingly#r#1,?</S>
			<S sid="177" ssid="118">and then on to other TokenPOS nodes used in its definition: ?in a manner or to a degree that dazzles the beholder.?Figure 1 demonstrates how two word-specific station ary distributions are more highly correlated if the words are related.</S>
			<S sid="178" ssid="119">In both model variants, random walks for related words are more likely to visit the same parts of the graph, and so assign higher probability to the same nodes.</S>
			<S sid="179" ssid="120">Figure 1 also shows that the MarkovGloss variantproduces distributions with a much wider range of proba bilities than the MarkovLink, which might be a source of difficulty in integrating the two model variants.</S>
			<S sid="180" ssid="121">Figure 2 shows the correlation between the stationary distributions produced by the two model variants for the same word.</S>
			<S sid="181" ssid="122">The log-log scale makes it possible to see the entire range of probabilities on the same axes, and shows that distributions produced by these two model variants share many of the same highest-probability words.</S>
			<S sid="182" ssid="123">A noteworthy property of the constructed graphs is thatword relatedness can be computed directly by compar ing walks that start at Token nodes.</S>
			<S sid="183" ssid="124">By contrast, existing WordNet-based measures require independent similarity judgments for all word senses relevant to a target wordpair (of which the maximum relatedness value is usu ally taken).</S>
			<S sid="184" ssid="125">Our algorithm lends itself to comparisonsbetween walks centered at a Synset node, or a Token POS node, or a Token node, or any mixed distributionthereof.</S>
			<S sid="185" ssid="126">And because the Synset nodes are strongly con nected, the model also admits direct comparison across parts of speech.</S>
			<S sid="186" ssid="127">585 Figure 2: Correlation of the stationary distributions for ?wizard#n,?</S>
			<S sid="187" ssid="128">produced by the MarkovLink variant (x-axis) and the MarkovGloss variant (y-axis).</S>
	</SECTION>
	<SECTION title="Similarity judgments. " number="4">
			<S sid="188" ssid="1">We have shown how to compute the word-specific sta tionary distribution from any starting distribution in thegraph.</S>
			<S sid="189" ssid="2">Now consider the task of deciding similarity be tween two words.</S>
			<S sid="190" ssid="3">Intuitively, if the random walk starting at the first word?s node and the random walk starting at the second word?s node tend to visit the same nodes, wewould like to consider them semantically related.</S>
			<S sid="191" ssid="4">Formally, we measure the divergence of their respective sta tionary distributions, p and q. A wide literature exists on similarity measures betweenprobability distributions.</S>
			<S sid="192" ssid="5">One standard choice is to con sider p and q to be vectors and measure the cosine ofthe angle between them, which is rank equivalent to Eu clidean distance.</S>
			<S sid="193" ssid="6">simcos(p, q) = ? i piqi ?p? ?q? Because p and q are probability distributions, we wouldalso expect a strong contender from the informationtheoretic measures based on Kullback-Leibler diver gence, defined as: DKL(p ? q) = ? i pi log pi qi Unfortunately, KL divergence is undefined if any qi is zero because those terms in the sum will have infinite weight.</S>
			<S sid="194" ssid="7">Several modifications to avoid this issue have been proposed in the literature.</S>
			<S sid="195" ssid="8">One is Jensen-Shannon divergence (Lin, 1991), a symmetric measure based onKL-divergence defined as the average of the KL diver gences of each distribution to their average distribution.Jensen-Shannon is well defined for all distributions be cause the average of pi and qi is non-zero whenever either number is. These measures and others are surveyed in (Lee, 2001), who finds that Jensen-Shannon is outperformed by the Skew divergence measure introduced by Lee in (1999).</S>
			<S sid="196" ssid="9">The skew divergence2 accounts for zeros in q by mixing in a small amount of p. s?(p, q) = D(p ? ?q + (1?</S>
			<S sid="197" ssid="10">?)p) = ? i pi log pi ?qi+(1??)piLee found that as ? ?</S>
			<S sid="198" ssid="11">1, the performance of skew divergence on natural language tasks improves.</S>
			<S sid="199" ssid="12">In partic ular, it outperforms most other models and even beatspure KL divergence modified to avoid zeros with sophis ticated smoothing models.</S>
			<S sid="200" ssid="13">In exploring the performanceof divergence measures on our model?s stationary distri butions, we observed the same phenomenon.</S>
			<S sid="201" ssid="14">Note thatin the limit as ? ?</S>
			<S sid="202" ssid="15">1, alpha skew is identically KL divergence.</S>
			<S sid="203" ssid="16">4.1 Zero-KL Divergence.</S>
			<S sid="204" ssid="17">In this section we introduce a novel measure of distribu tional divergence based on a reinterpretation of the skew divergence.</S>
			<S sid="205" ssid="18">Skew divergence avoids zeros in q by mixingin some of p, but its performance on many natural language tasks improves as it better approximates KL diver gence.</S>
			<S sid="206" ssid="19">We propose an alternative approximation to KL divergence called Zero-KL divergence, or ZKL.</S>
			<S sid="207" ssid="20">Whenqi is non-zero, we use exactly the term from KL diver gence.</S>
			<S sid="208" ssid="21">When qi = 0, we have a problem?in the limit as ? ?</S>
			<S sid="209" ssid="22">1, the corresponding term approaches infinity.</S>
			<S sid="210" ssid="23">We let ZKL use the Skew divergence value for these terms: pi log pi ?qi+(1??)pi . Because qi = 0 this simplifies to.</S>
			<S sid="211" ssid="24">pi log pi (1??)pi = pi log 11??</S>
			<S sid="212" ssid="25">Lee showed skew divergence?s best performance was for ? near to 1, so we formalize this intuition by choosing ? exponentially near to 1, i.e. we can choose our ? as 1?2??</S>
			<S sid="213" ssid="26">for some ? ?</S>
			<S sid="214" ssid="27">R+.</S>
			<S sid="215" ssid="28">Zero terms in the sum can now be written as pi log 12??</S>
			<S sid="216" ssid="29">= pi log 2 ? = pi ?.</S>
			<S sid="217" ssid="30">Note here an analogy to the case with qj &gt; 0 and where pj is exactly one order of magnitude greater than qj , i.e. pj = 2 ? qj . For such a term in the standard KL divergence, we would get pj log pj qj = pj log(2) = pj . Therefore, the ? term in skew divergence implicitly defines a parameter stating how many orders of magnitude smaller than pj to count qj if qj = 0.</S>
			<S sid="218" ssid="31">We define the Zero-KL divergence with respect to 2In Lee?s (1999) original presentation, skew divergence isdefined not as s?(p, q) but rather as s?(q, p).</S>
			<S sid="219" ssid="32">We reverse the ar gument order for consistency with the other measures discussed here.</S>
			<S sid="220" ssid="33">586 gamma: ZKL?(p, q) = ? i pi { log piqi qi 6= 0 ? qi = 0Note that this is exactly KL-divergence when KLdivergence is defined and, like skew divergence, approx imates KL divergence in the limit as ? ?</S>
			<S sid="221" ssid="34">A similar analysis of the skew divergence terms for when 0 &lt; qi  pi (and in particular with qi less than pi by more than a factor of 2??)</S>
			<S sid="222" ssid="35">shows that such a term in the skew divergence sum is again approximated by ? pi.</S>
			<S sid="223" ssid="36">ZKL does not have this property.</S>
			<S sid="224" ssid="37">Because ZKL is a better approximation to KL divergence and because they havethe same behavior in the limit, we expect ZKL?s performance to dominate that of skew divergence in many distributions.</S>
			<S sid="225" ssid="38">However, if there is a wide range in the ex ponent of noisy terms, the maximum possible penalty tosuch terms ascribed by skew divergence may be benefi cial.</S>
			<S sid="226" ssid="39">Figure 3 shows the relative performance of ZKL versus Jensen-Shannon, skew divergence, cosine similarity, and the Jaccard score (a measure from information retrieval) for correlations with human judgment on the MarkovLink model.</S>
			<S sid="227" ssid="40">ZKL consistently outperforms the other measures on distributions resulting from this model, but ZKL is not optimal on distributions generated by our other models.</S>
			<S sid="228" ssid="41">The next section explores this topic in more detail.</S>
	</SECTION>
	<SECTION title="Evaluation. " number="5">
			<S sid="229" ssid="1">Traditionally, there have been two primary types of evaluation for measures of semantic relatedness: one is correlation to human judgment, the other is the relative per formance gains of a task-driven system when it uses the measure.</S>
			<S sid="230" ssid="2">The evaluation here focuses on correlation with human judgments of relatedness.</S>
			<S sid="231" ssid="3">For consistency with previous literature, we use rank correlation (Spearman?s? coefficient) rather than linear correlation when comparing sets of relatedness judgments because the rank corre lation captures information about the relative ordering ofthe scores.</S>
			<S sid="232" ssid="4">However, it is worth noting that many applica tions that make use of lexical relatedness scores (e.g. as features to a machine learning algorithm) would better be served by scores on a linear scale with human judgments.</S>
			<S sid="233" ssid="5">Rubenstein and Goodenough (1965) solicited humanjudgments of semantic similarity for 65 pairs of com mon nouns on a scale of zero to four.</S>
			<S sid="234" ssid="6">Miller and Charles (1991) repeated their experiment on a subset of 29 nounpairs (out of 30 total) and found that although indi viduals varied among their judgments, in aggregate thescores were highly correlated with those found by Ruben stein and Goodenough (at ? = .944 by our calculation).Resnik (1999) replicated the Miller and Charles experiment and reported that the average per-subject linear cor relation on the dataset was around r = 0.90, providing a rough upper bound on any system?s linear correlation performance with respect to the Miller and Charles data.</S>
			<S sid="235" ssid="7">Figure 3 shows that the ZKL measure on the MarkovLink model has linear correlation coefficient r = .903?at the limit of human inter-annotator agreement.Recently, a larger set of word relatedness judg ments was obtained by (Finkelstein et al, 2002) in the WordSimilarity-353 (WS-353) collection.</S>
			<S sid="236" ssid="8">Despite the collection?s name, the study instructed participants to score word pairs for relatedness (on a scale of 0 to10), which is in contrast to the similarity judgments re quested of the Miller and Charles (MC) and Rubenstein and Goodenough (RG) participants.</S>
			<S sid="237" ssid="9">For this reason, the WordSimilarity-353 data contains many pairs that are not semantically similar but still receive high scores, such as ?computer-software?</S>
			<S sid="238" ssid="10">at 8.81.</S>
			<S sid="239" ssid="11">WS-353 contains pairs that include non-nouns, such as ?eat-drink,?</S>
			<S sid="240" ssid="12">one proper noun not appearing in WordNet (?Maradona-football?), and some pairs potentially subject to political bias.</S>
			<S sid="241" ssid="13">Again,the aggregate human judgments correlate well with ear lier data sets where they overlap?the 30 judgments that WordSimilarity-353 shares with the Miller and Charles data have ? = .939 and the 29 shared with Rubenstein and Goodenough have ? = .904 (by our calculations).</S>
			<S sid="242" ssid="14">We generated similarity scores for word pairs in all three data sets using the three variants of our walk model (MarkovLink, MarkovGloss, MarkovJoined) and with multiple distributional distance measures.</S>
			<S sid="243" ssid="15">We used the WordNet::Similarity package (Pedersen et al, 2004) to compute baseline scores for several existing measures, noting that one word pair was not processed in WS-353 because one of the words was missing from WordNet.The results are summarized in Table 2.</S>
			<S sid="244" ssid="16">These num bers differ slightly from previously reported scores due tovariations in the exact experimental setup, WordNet ver sion, and the method of breaking ties when computing?</S>
			<S sid="245" ssid="17">(here we break ties using the product-moment formu lation of Spearman?s rank correlation coefficient).</S>
			<S sid="246" ssid="18">It is worth noting that in their experiments, (Patwardhan and Pedersen, 2006) report that the Vector method has rank correlation coefficients of .91 and .90 for MC and RG, respectively, which are also top performing values.</S>
			<S sid="247" ssid="19">In our experiments, the MarkovLink model with ZKLdistance measure was the best performing model over all.</S>
			<S sid="248" ssid="20">MarkovGloss and MarkovJoined were also strong contenders but with the cosine measure instead of ZKL.One reason for this distinction is that the stationary distributions resulting from the MarkovLink model are non zero for all but the initial word nodes (i.e. non-zero for all Synset nodes).</S>
			<S sid="249" ssid="21">Consequently, ZKL?s re-estimate for the zero terms adds little information.</S>
			<S sid="250" ssid="22">By contrast, theMarkovGloss andMarkovJoined models include linksthat traverse from Synset nodes to TokenPOS nodes, re 587 Figure 3: Correlation with the Miller  Charles data sets by linear correlation (left) and rank correlation (right) for the MarkovLink model.</S>
			<S sid="251" ssid="23">All data points were based on one set of stationary distributions over the graph; only the divergence measure between those distributions is varied.</S>
			<S sid="252" ssid="24">Note that ZKL?</S>
			<S sid="253" ssid="25">dominates both graphs but skew divergence does well for increasing ?</S>
			<S sid="254" ssid="26">(computed as 1?</S>
			<S sid="255" ssid="27">2?).</S>
			<S sid="256" ssid="28">Gamma is swept over the range 0 to 1, then 1 through 20, then 20 through 40 at equal resolutions.</S>
			<S sid="257" ssid="29">Model MC Rank RG Rank WS-353 Rank MarkovLink (ZKL) .904 .817 .552 MarkovGloss (cosine) .841 .762 .467 MarkovJoined (cosine) .841 .838 .547 Gloss Vectors .888 .789 .445 Extended Lesk .869 .829 .511 Jiang-Conrath .653 .584 .195 Lin .625 .599 .216 Table 2: Spearman?s ? rank correlation coefficients withhuman judgments using ? = 2.0 for ZKL.</S>
			<S sid="258" ssid="30">Note that fig ure 3 demonstrates ZKL?s insensitivity with regard to the parameter setting for the MarkovLink model.</S>
			<S sid="259" ssid="31">sulting in a final stationary distribution with more (and more meaningful) zero/non-zero pairs.</S>
			<S sid="260" ssid="32">Hence the proper setting of gamma (or alpha for skew divergence) is of greater importance.</S>
			<S sid="261" ssid="33">ZKL?s performance improves with tuning of gamma, but cosine similarity remained the more robust measure for these distributions.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="6">
			<S sid="262" ssid="1">In this paper, we have introduced a new measure oflexical relatedness based on the divergence of the sta tionary distributions computed from random walks overgraphs extracted WordNet.</S>
			<S sid="263" ssid="2">We have explored the structural properties of extracted semantic graphs and characterized the distinctly different types of stationary distribu tions that result.</S>
			<S sid="264" ssid="3">We explored several distance measures on these distributions, including ZKL, a novel variant of KL-divergence.</S>
			<S sid="265" ssid="4">Our best relatedness measure is at the limit of human inter-annotator agreement and is one of the strongest measures of semantic relatedness that uses only WordNet as its underlying lexical resource.In future work, we hope to integrate other lexical resources such as Wikipedia into the walk.</S>
			<S sid="266" ssid="5">Incorporating more types of links from more resources will underline the importance of determining appropriate rela tive weights for all of the types of edges in the walk?s matrix.</S>
			<S sid="267" ssid="6">Even for WordNet, we believe that certain link types, such as antonyms, may be more or less appropriate for certain tasks and should weighted accordingly.</S>
			<S sid="268" ssid="7">And while our measure of lexical relatedness correlates well with human judgments, we hope to show performance gains in a real-word task from the use of our measure.</S>
			<S sid="269" ssid="8">Acknowledgments Thanks to Christopher D. Manning and Dan Jurafsky for their helpful comments and suggestions.</S>
			<S sid="270" ssid="9">We are also grateful to Siddharth Patwardhan and Ted Pedersen for assistance in comparing against their system.</S>
			<S sid="271" ssid="10">Thanks to Sushant Prakash, Rion Snow, and Varun Ganapathi fortheir advice on pursuing some of the ideas in this paper, and to our anonymous reviewers for their helpful cri tiques.</S>
			<S sid="272" ssid="11">Daniel Ramage was funded in part by an NDSEG fellowship.</S>
			<S sid="273" ssid="12">This work was also supported in part by the DTO AQUAINT Program, the DARPA GALE Program, and the ONR (MURI award N000140510388).</S>
			<S sid="274" ssid="13">588</S>
	</SECTION>
</PAPER>
