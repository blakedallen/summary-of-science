<PAPER>
  <S sid="0">Multilingual Dependency Analysis With A Two-Stage Discriminative Parser</S>
  <ABSTRACT>
    <S sid="1" ssid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
    <S sid="2" ssid="2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S>
    <S sid="3" ssid="3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S>
    <S sid="4" ssid="4">We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S>
    <S sid="6" ssid="2">With the availability of resources such as the Penn WSJ Treebank, much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S>
    <S sid="7" ssid="3">However, recently their has been a revived interest in parsing models that produce dependency graph representations of sentences, which model words and their arguments through directed edges (Hudson, 1984; Mel'&#711;cuk, 1988).</S>
    <S sid="8" ssid="4">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>
    <S sid="9" ssid="5">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S>
    <S sid="10" ssid="6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S>
    <S sid="11" ssid="7">This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005).</S>
    <S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
    <S sid="13" ssid="9">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al., 2006; Haji&#711;c et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B&#168;ohmov&#180;a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; D&#711;zeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003).</S>
    <S sid="14" ssid="10">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S>
    <S sid="15" ssid="11">For the remainder of this paper, we denote by x = x1,... xn a sentence with n words and by y a corresponding dependency graph.</S>
    <S sid="16" ssid="12">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
    <S sid="17" ssid="13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S>
    <S sid="18" ssid="14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 216&#8211;220, New York City, June 2006. c&#65533;2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use.</S>
  </SECTION>
  <SECTION title="2 Stage 1: Unlabeled Parsing" number="2">
    <S sid="19" ssid="1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S>
    <S sid="20" ssid="2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S>
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
    <S sid="23" ssid="5">That system uses MIRA, an online large-margin learning algorithm, to compute model parameters.</S>
    <S sid="24" ssid="6">Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.</S>
    <S sid="25" ssid="7">For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>
    <S sid="26" ssid="8">These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left, or a noun modifying a verb with another verb occurring between them.</S>
    <S sid="27" ssid="9">We augmented this model to incorporate morphological features derived from each token.</S>
    <S sid="28" ssid="10">Consider a proposed dependency of a dependent xj on the head xi, each with morphological features Mj and Mi respectively.</S>
    <S sid="29" ssid="11">We then add to the representation of the edge: Mi as head features, Mj as dependent features, and also each conjunction of a feature from both sets.</S>
    <S sid="30" ssid="12">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender, case, or number.</S>
    <S sid="31" ssid="13">Not all data sets in our experiments include morphological features, so we use them only when available.</S>
  </SECTION>
  <SECTION title="3 Stage 2: Label Classification" number="3">
    <S sid="32" ssid="1">The second stage takes the output parse y for sentence x and classifies each edge (i, j) E y with a particular label l(i,j).</S>
    <S sid="33" ssid="2">Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.</S>
    <S sid="34" ssid="3">However, the parser is fundamentally limited by the scope of local factorizations that make inference tractable.</S>
    <S sid="35" ssid="4">In our case this means we are forced only to consider features over single edges or pairs of edges.</S>
    <S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
    <S sid="37" ssid="6">The simplest labeler would be to take as input an edge (i, j) E y for sentence x and find the label with highest score, Doing this for each edge in the tree would produce the final output.</S>
    <S sid="38" ssid="7">Such a model could easily be trained using the provided training data for each language.</S>
    <S sid="39" ssid="8">However, it might be advantageous to know the labels of other nearby edges.</S>
    <S sid="40" ssid="9">For instance, if we consider a head xi with dependents xj1, ... , xjM, it is often the case that many of these dependencies will have correlated labels.</S>
    <S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
    <S sid="42" ssid="11">We attempted higher-order Markov factorizations but they did not improve performance uniformly across languages and training became significantly slower.</S>
    <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
    <S sid="44" ssid="13">We use the MIRA online learner to set the weights (Crammer and Singer, 2003; McDonald et al., 2005a) since we found it trained quickly and provide good performance.</S>
    <S sid="45" ssid="14">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>
    <S sid="46" ssid="15">Of course, we have to define a set of suitable features.</S>
    <S sid="47" ssid="16">We used the following: dependent have identical values?</S>
    <S sid="48" ssid="17">Is this the left/rightmost dependent for the head?</S>
    <S sid="49" ssid="18">Is this the first dependent to the left/right of the head?</S>
    <S sid="50" ssid="19">Various conjunctions of these were included based on performance on held-out data.</S>
    <S sid="51" ssid="20">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S>
    <S sid="52" ssid="21">Thus a joint model of parsing and labeling could not easily include them without some form of re-ranking or approximate parameter estimation.</S>
  </SECTION>
  <SECTION title="4 Results" number="4">
    <S sid="53" ssid="1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al., 2006).</S>
    <S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
    <S sid="55" ssid="3">Furthermore, for Arabic and Spanish, we used lemmas instead of inflected word forms, again based on performance on held-out data1.</S>
    <S sid="56" ssid="4">Results on the test set are given in Table 1.</S>
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
    <S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
    <S sid="59" ssid="7">Only Arabic, Turkish and Slovene have parsing accuracies significantly below 80%, and these languages have relatively small training sets and/or are highly inflected with little to no word order constraints.</S>
    <S sid="60" ssid="8">Furthermore, these results show that a twostage system can achieve a relatively high performance.</S>
    <S sid="61" ssid="9">In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
    <S sid="62" ssid="10">For the remainder of the paper we provide a general error analysis across a wide set of languages plus a detailed error analysis of Spanish and Arabic.</S>
  </SECTION>
  <SECTION title="5 General Error Analysis" number="5">
    <S sid="63" ssid="1">Our system has several components, including the ability to produce non-projective edges, sequential Japanese, Portuguese, Slovene, Spanish, Swedish and Turkish.</S>
    <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
    <S sid="65" ssid="3">The benefit of each of these is shown in Table 2.</S>
    <S sid="66" ssid="4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S>
    <S sid="67" ssid="5">This allowed us to train new models quickly.</S>
    <S sid="68" ssid="6">Table 2 shows that each component of our system does not change performance significantly (rows 24 versus row 1).</S>
    <S sid="69" ssid="7">However, if we only allow projective parses, do not use morphological features and label edges with a simple atomic classifier, the overall drop in performance becomes significant (row 5 versus row 1).</S>
    <S sid="70" ssid="8">Allowing non-projective parses helped with freer word order languages like Dutch (78.8%/74.7% to 83.6%/79.2%, unlabeled/labeled accuracy).</S>
    <S sid="71" ssid="9">Including rich morphology features naturally helped with highly inflected languages, in particular Spanish, Arabic, Turkish, Slovene and to a lesser extent Dutch and Portuguese.</S>
    <S sid="72" ssid="10">Derived morphological features improved accuracy in all these languages by 1-3% absolute.</S>
    <S sid="73" ssid="11">Sequential classification of labels had very little effect on overall labeled accuracy (79.4% to 79.7%)2.</S>
    <S sid="74" ssid="12">The major contribution was in helping to distinguish subjects, objects and other dependents of main verbs, which is the most common labeling error.</S>
    <S sid="75" ssid="13">This is not surprising since these edge labels typically are the most correlated (i.e., if you already know which noun dependent is the subject, then it should be easy to find the object).</S>
    <S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
    <S sid="77" ssid="15">Similar improvements are common across all languages, though not as dramatic.</S>
    <S sid="78" ssid="16">Even with this improvement, the labeling of verb dependents remains the highest source of error.</S>
  </SECTION>
  <SECTION title="6 Detailed Analysis" number="6">
    <S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
    <S sid="80" ssid="2">These words form 17% of the test corpus.</S>
    <S sid="81" ssid="3">Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%), adverbs (82%) and subordinating conjunctions (80%), for a total of another 23% of the test corpus.</S>
    <S sid="82" ssid="4">These weaknesses are not surprising, since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences, and prepositional phrase attachment.</S>
    <S sid="83" ssid="5">In a preliminary test of this hypothesis, we looked at all of the sentences from a development set in which a main verb is incorrectly attached.</S>
    <S sid="84" ssid="6">We confirmed that the main clause is often misidentified in multi-clause sentences, or that one of several conjoined clauses is incorrectly taken as the main clause.</S>
    <S sid="85" ssid="7">To test this further, we added features to count the number of commas and conjunctions between a dependent verb and its candidate head.</S>
    <S sid="86" ssid="8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S>
    <S sid="87" ssid="9">Unfortunately, accuracy for other word types decreases somewhat, resulting in no significant net accuracy change.</S>
    <S sid="88" ssid="10">Nevertheless, this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S>
    <S sid="89" ssid="11">Another common verb attachment error is a switch between head and dependent verb in phrasal verb forms like dejan intrigar or qiero decir, possibly because the non-finite verb in these cases is often a main verb in training sentences.</S>
    <S sid="90" ssid="12">We need to look more carefully at verb features that may be useful here, in particular features that distinguish finite and non-finite forms.</S>
    <S sid="91" ssid="13">In doing this preliminary analysis, we noticed some inconsistencies in the reference dependency structures.</S>
    <S sid="92" ssid="14">For example, in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambi&#180;en los hombres:..., decia&#8217;s head is given as decirlo, although the main verbs of relative clauses are normally dependent on what the relative modifies, in this case the article Lo.</S>
    <S sid="93" ssid="15">A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%), conjunctions (69%) and to a lesser extent verbs (73%).</S>
    <S sid="94" ssid="16">Similarly, for labeled accuracy, the hardest edges to label are for dependents of verbs, i.e., subjects, objects and adverbials.</S>
    <S sid="95" ssid="17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions, conjunctions and verbs, and the latter makes mistakes on edges into nouns (subject/objects).</S>
    <S sid="96" ssid="18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%), but since there is very little overlap in the kinds of errors each makes, overall labeled accuracy drops to 67%.</S>
    <S sid="97" ssid="19">This drop is not nearly as significant for other languages.</S>
    <S sid="98" ssid="20">Another source of potential error is that the average sentence length of Arabic is much higher than other languages (around 37 words/sentence).</S>
    <S sid="99" ssid="21">However, if we only look at performance for sentences of length less than 30, the labeled accuracy is still only 71%.</S>
    <S sid="100" ssid="22">The fact that Arabic has only 1500 training instances might also be problematic.</S>
    <S sid="101" ssid="23">For example if we train on 200, 400, 800 and the full training set, labeled accuracies are 54%, 60%, 62% and 67%.</S>
    <S sid="102" ssid="24">Clearly adding more data is improving performance.</S>
    <S sid="103" ssid="25">However, when compared to the performance of Slovene (1500 training instances) and Spanish (3300 instances), it appears that Arabic parsing is lagging.</S>
  </SECTION>
  <SECTION title="7 Conclusions" number="7">
    <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
    <S sid="105" ssid="2">In the future we plan to extend these models in two ways.</S>
    <S sid="106" ssid="3">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
    <S sid="107" ssid="4">It is our hypothesis that for languages with fine-grained label sets, joint parsing and labeling will improve performance.</S>
    <S sid="108" ssid="5">Second, we plan on integrating any available morphological features in a more principled manner.</S>
    <S sid="109" ssid="6">The current system simply includes all morphological bi-gram features.</S>
    <S sid="110" ssid="7">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S>
  </SECTION>
</PAPER>
