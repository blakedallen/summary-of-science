<PAPER>
  <S sid="0">Combining Distributional And Morphological Information For Part Of Speech Induction</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information.</S>
    <S sid="2" ssid="2">We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="3" ssid="1">The task studied in this paper is the unsupervised learning of parts-of-speech, that is to say lexical categories corresponding to traditional notions of, for example, nouns and verbs.</S>
    <S sid="4" ssid="2">As is often the case in machine learning of natural language, there are two parallel motivations: first a simple engineering one &#8212; the induction of these categories can help in smoothing and generalising other models, particularly in language modelling for speech recognition as explored by (Ney et al., 1994) and secondly a cognitive science motivation &#8212; exploring how evidence in the primary linguistic data can account for first language acquisition by infant children (Finch and Chater, 1992a; Finch and Chater, 1992b; Redington et al., 1998).</S>
    <S sid="5" ssid="3">At this early phase of learning, only limited sources of information can be used: primarily distributional evidence, about the contexts in which words occur, and morphological evidence, (more strictly phonotactic or orthotactic evidence) about the sequence of symbols (letters or phonemes) of which each word is formed.</S>
    <S sid="6" ssid="4">A number of different approaches have been presented for this task using exclusively distributional evidence to cluster the words together, starting with (Lamb, 1961) and these have been shown to produce good results in English, Japanese and Chinese.</S>
    <S sid="7" ssid="5">These languages have however rather simple morphology and thus words will tend to have higher frequency than in more morphologically complex languages.</S>
    <S sid="8" ssid="6">In this paper we will address two issues: first, whether the existing algorithms work adequately on a range of languages and secondly how we can incorporate morphological information.</S>
    <S sid="9" ssid="7">We are particularly interested in rare words: as (Rosenfeld, 2000, pp.1313-1314) points out, it is most important to cluster the infrequent words, as we will have reliable information about the frequent words; and yet it is these words that are most difficult to cluster.</S>
    <S sid="10" ssid="8">We accordingly focus both in our algorithms and our evaluation on how to cluster words effectively that occur only a few times (or not at all) in the training data.</S>
    <S sid="11" ssid="9">In addition we are interested primarily in inducing small numbers of clusters (at most 128) from comparatively small amounts of data using limited or no sources of external knowledge, and in approaches that will work across a wide range of languages, rather than inducing large numbers (say 1000) from hundreds of millions of words.</S>
    <S sid="12" ssid="10">Note this is different from the common task of guessing the word category of an unknown word given a pre-existing set of parts-of-speech, a task which has been studied extensively (Mikheev, 1997).</S>
    <S sid="13" ssid="11">Our approach will be to incorporate morphological information of a restricted form into a distributional clustering algorithm.</S>
    <S sid="14" ssid="12">In addition we will use a very limited sort of frequency information, since rare words tend to belong to open class categories.</S>
    <S sid="15" ssid="13">The input to the algorithm is a sequence of tokens, each of which is considered as a sequence of characters in a standard encoding.</S>
    <S sid="16" ssid="14">The rest of this paper is structured as follows: we will first discuss the evaluation of the models in some detail and present some simple experiments we have performed here (Section 2).</S>
    <S sid="17" ssid="15">We will then discuss the basic algorithm that is the starting point for our research in Section 3.</S>
    <S sid="18" ssid="16">Then we show how we can incorporate a limited form of morphological information into this algorithm in Section 4.</S>
    <S sid="19" ssid="17">Section 5 presents the results of our evaluations on a number of data sets drawn from typologically distinct languages.</S>
    <S sid="20" ssid="18">We then briefly discuss the use of ambiguous models or soft clustering in Section 6, and then finish with our conclusions and proposals for future work.</S>
  </SECTION>
  <SECTION title="2 Evaluation Discussion" number="2">
    <S sid="21" ssid="1">A number of different approaches to evaluation have been proposed in the past.</S>
    <S sid="22" ssid="2">First, early work used an informal evaluation of manually comparing the clusters or dendrograms produced by the algorithms with the authors' intuitive judgment of the lexical categories.</S>
    <S sid="23" ssid="3">This is inadequate for a number of obvious reasons &#8212; first it does not allow adequate comparison of different techniques, and secondly it restricts the languages that can easily be studied to those in which the researcher has competence thus limiting experimentation on a narrow range of languages.</S>
    <S sid="24" ssid="4">A second form of evaluation is to use some data that has been manually or semi-automatically annotated with part of speech (POS) tags, and to use some information theoretic measure to look at the correlation between the 'correct' data and the induced POS tags.</S>
    <S sid="25" ssid="5">Specifically, one could look at the conditional entropy of the gold standard tags given the induced tags.</S>
    <S sid="26" ssid="6">We use the symbol W to refer to the random variable related to the word, G for the associated gold standard tag, and T for the tag produced by one of our algorithms.</S>
    <S sid="27" ssid="7">Recall that Thus low conditional entropy means that the mutual information between the gold and induced tags will be high.</S>
    <S sid="28" ssid="8">If we have a random set of tags the mutual information will be zero and the conditional entropy will be the same as the entropy of the tag set.</S>
    <S sid="29" ssid="9">Again, this approach has several weaknesses: there is not a unique well-defined set of part-ofspeech tags, but rather many different possible sets that reflect rather arbitrary decisions by the annotators.</S>
    <S sid="30" ssid="10">To put the scores we present below in context, we note that using some data sets prepared for the AMALGAM project (Atwell et al., 2000) the conditional entropies between some data manually tagged with different tag sets varied from 0.22 (between Brown and LOB tag sets) to 1.3 (between LLC and Unix Parts tag sets).</S>
    <S sid="31" ssid="11">Secondly, because of the Zipfian distribution of word frequencies, simple baselines that assign each frequent word to a different class, can score rather highly, as we shall see below.</S>
    <S sid="32" ssid="12">A third evaluation is to use the derived classification in a class-based language model, and to measure the perplexity of the derived model.</S>
    <S sid="33" ssid="13">However it is not clear that this directly measures the linguistic plausibility of the classification.</S>
    <S sid="34" ssid="14">In particular many parts of speech (relative pronouns for example) represent long-distance combinatorial properties, and a simple finite-state model with local context (such as a class n-gram model (Brown et al., 1992)) will not measure this.</S>
    <S sid="35" ssid="15">We can also compare various simple baselines, to see how they perform according to these simple measures.</S>
    <S sid="36" ssid="16">Frequent word baseline take the n &#8212; 1 most frequent words and assign them each to a separate class, and put all remaining words in the remaining class.</S>
    <S sid="37" ssid="17">Word baseline each word is in its own class.</S>
    <S sid="38" ssid="18">We performed experiments on parts of the Wall Street Journal corpus, using the corpus tags.</S>
    <S sid="39" ssid="19">We chose sections 0 &#8212; 19, a total of about 500,000 words.</S>
    <S sid="40" ssid="20">Table 1 shows that the residual conditional entropy with the word baseline is only 0.12.</S>
    <S sid="41" ssid="21">This reflects lexical ambiguity.</S>
    <S sid="42" ssid="22">If all of the words were unambiguous, then the conditional entropy of the tag given the word would be zero.</S>
    <S sid="43" ssid="23">We are therefore justified in ignoring ambiguity for the moment, since it vastly improves the efficiency of the algorithms.</S>
    <S sid="44" ssid="24">Clearly as the number of clusters increases, the conditional entropy will decrease, as is demonstrated below.</S>
  </SECTION>
  <SECTION title="3 Basic algorithm" number="3">
    <S sid="45" ssid="1">The basic methods here have been studied in detail by (Ney et al., 1994), (Martin et al., 1998) and (Brown et al., 1992).</S>
    <S sid="46" ssid="2">We assume a vocabulary of words V = {W1, ... }.</S>
    <S sid="47" ssid="3">Our task is to learn a deterministic clustering, that is to say a class membership function g from V into the set of class labels , n}.</S>
    <S sid="48" ssid="4">This clustering can be used to define a number of simple statistical models.</S>
    <S sid="49" ssid="5">The objective function we try to maximise will be the likelihood of some model &#8212; i.e. the probability of the data with respect to the model.</S>
    <S sid="50" ssid="6">The simplest candidate for the model is the class bigram model, though the approach can also be extended to class trigram models.</S>
    <S sid="51" ssid="7">Suppose we have a corpus of length N, , wN.</S>
    <S sid="52" ssid="8">We can assume an additional sentence boundary token.</S>
    <S sid="53" ssid="9">Then the class bigram model defines the probability of the next word given the history as P(wi IOC') = P(wilg(wi))P(9(wi-1)1g(wi-2)) It is not computationally feasible to search through all possible partitions of the vocabulary to find the one with the highest value of the likelihood; we must therefore use some search algorithm that will give us a local optimum.</S>
    <S sid="54" ssid="10">We follow (Ney et al., 1994; Martin et al., 1998) and use an exchange algorithm similar to the k-means algorithm for clustering.</S>
    <S sid="55" ssid="11">This algorithm iteratively improves the likelihood of a given clustering by moving each word from its current cluster to the cluster that will give the maximum increase in likelihood, or leaving it in its original cluster if no improvement can be found.</S>
    <S sid="56" ssid="12">There are a number of different ways in which the initial clustering can be chosen; it has been found, and our own experiments have tended to confirm this, that the initialisation method has little effect on the final quality of the clusters but can have a marked effect on the speed of convergence of the algorithm.</S>
    <S sid="57" ssid="13">A more important variation for our purposes is how the rare words are treated.</S>
    <S sid="58" ssid="14">(Martin et al., 1998) leave all words with a frequency of less than 5 in a particular class, from which they may not be moved.</S>
  </SECTION>
  <SECTION title="4 Morphology" number="4">
    <S sid="59" ssid="1">The second sort of information is information about the sequence of letters or phones that form each word.</S>
    <S sid="60" ssid="2">To take a trivial example, if we encounter an unknown word, say &#163;212,000 then merely looking at the sequence of characters that compose it is enough to enable us to make a good guess as to its part of speech.</S>
    <S sid="61" ssid="3">Less trivially, if a word in English ends in -ing, then it is quite likely to be a present participle.</S>
    <S sid="62" ssid="4">We can distinguish this sort of information, which perhaps could better be called orthotactic or phonotactic information from a richer sort which incorporates relational information between the words &#8212; thus given a novel word that ends in &amp;quot;ing&amp;quot; such as &amp;quot;derailing&amp;quot; one could use the information that we had already seen the token &amp;quot;derailed&amp;quot; as additional evidence.</S>
    <S sid="63" ssid="5">One way to incorporate this simple source of information would be to use a mixture of string models alone, without distributional evidence.</S>
    <S sid="64" ssid="6">Some preliminary experiments not reported here established that this approach could only separate out the most basic differences, such as sequences of numbers.</S>
    <S sid="65" ssid="7">A more powerful approach is to combine the distributional information with the morphological information by composing the Ney-Essen clustering model with a model for the morphology within a Bayesian framework.</S>
    <S sid="66" ssid="8">We use the same formula for the probability of the data given the model, but include an additional term for the probability of the model, that depends on the strings used in each cluster.</S>
    <S sid="67" ssid="9">We wish to bias the algorithm so that it will put words that are morphologically similar in the same cluster.</S>
    <S sid="68" ssid="10">We can consider thus a generative process that produces sets of clusters as used before.</S>
    <S sid="69" ssid="11">Consider the vocabulary V to be a subset of E* where E is the set of characters or phonemes used, and let the model have for each cluster i a distribution over E* say P. Then we define the probability of the partition (the prior) as ignoring irrelevant normalisation constants.</S>
    <S sid="70" ssid="12">This will give a higher probability to partitions where morphologically similar strings are in the same cluster.</S>
    <S sid="71" ssid="13">The models we will use here for the cluster dependent word string probabilities will be letter Hidden Markov Models (HMMs).</S>
    <S sid="72" ssid="14">We decided to use HMMs rather than more powerful models, such as character trigram models, because we wanted models that were capable of modelling properties of the whole string; though in English and in other European languages, local statistics such as those used by n-gram models are adequate to capture most morphological regularities, in other languages this is not the case.</S>
    <S sid="73" ssid="15">Moreover, we wish to have comparatively weak models otherwise the algorithm will capture irrelevant orthotactic regularities &#8212; such as a class of words starting with &amp;quot;st&amp;quot; in English.</S>
    <S sid="74" ssid="16">In addition we can modify this to incorporate information about frequency.</S>
    <S sid="75" ssid="17">We know that rare words are more likely to be nouns, proper nouns or members of some other open word class rather than say pronouns or articles.</S>
    <S sid="76" ssid="18">We can do this simply by adding prior class probabilities ai to the above equation giving We can use the maximum likelihood estimates for ozi which are just the number of distinct types in cluster i, divided by the total number of types in the corpus.</S>
    <S sid="77" ssid="19">This just has the effect of discriminating between classes that will have lots of types (i.e. open class clusters) and clusters that tend to have few types (corresponding to closed class words).</S>
    <S sid="78" ssid="20">It is possible that in some languages there might be more subtle category related frequency effects, that could benefit from more complex models of frequency.</S>
  </SECTION>
  <SECTION title="5 Evaluation" number="5">
    <S sid="79" ssid="1">We used texts prepared for the MULTEXT-East project (Erjavec and Ide, 1998) which consists of data (George Orwell's novel 1984) in seven languages: the original English together with Romanian, Czech, Slovene, Bulgarian, Estonian, and Hungarian.</S>
    <S sid="80" ssid="2">These are summarised in Table 2.</S>
    <S sid="81" ssid="3">As can be seen they cover a wide range of language families; furthermore Bulgarian is written in Cyrillic, which slightly stretches the range.</S>
    <S sid="82" ssid="4">Token-type ratios range from 12.1 for English to 4.84 for Hungarian.</S>
    <S sid="83" ssid="5">The tags used are extremely fine-grained, and incorporate a great deal of information about case, gender and so on &#8212; in Hungarian for example 400 tags are used with 86 tags used only once.</S>
    <S sid="84" ssid="6">Table 3 shows the result of our cross-linguistic evaluation on this data.</S>
    <S sid="85" ssid="7">Since the data sets are so small we decided to use the conditional entropy evaluation.</S>
    <S sid="86" ssid="8">Here DO refers to the distributional clustering algorithm where all words are clustered; D5 leaves all words with frequency at most 5 in a seperate cluster, DM uses morphological information as well, DF uses frequency information and DMF uses morphological and frequency information.</S>
    <S sid="87" ssid="9">We evaluated it for all words, and also for words with frequency at most 5.</S>
    <S sid="88" ssid="10">We can see that the use of morphological information consistently improves the results on the rare words by a substantial margin.</S>
    <S sid="89" ssid="11">In some cases, however, a simpler algorithm performs better when all the words are considered &#8212; notably in Slovene and Estonian.</S>
    <S sid="90" ssid="12">We have also evaluated this method by comparing the perplexity of a class-based language model derived from these classes.</S>
    <S sid="91" ssid="13">We constructed a class bigram model, using absolute interpolation with a singleton generalised distribution for the transition weights, and using absolute discounting with backing off for the membership/output function.</S>
    <S sid="92" ssid="14">(Ney et al., 1994; Martin et al., 1998) We trained the model on sections 00-09 of the Penn Treebank, ( 518769 tokens including sentence boundaries and punctuation) and tested it on sections 10&#8212; l 9 (537639 tokens).</S>
    <S sid="93" ssid="15">We used the full vocabulary of the training and test sets together which was 45679, of which 14576 had frequency zero in the training data and thus had to be categorised based solely on their morphology and frequency.</S>
    <S sid="94" ssid="16">We did not reduce the vocabulary or change the capitalization in any way.</S>
    <S sid="95" ssid="17">We compared different models with varying numbers of clusters: 32 64 and 128.</S>
    <S sid="96" ssid="18">Table 4 shows the results of the perplexity evaluation on the WSJ data.</S>
    <S sid="97" ssid="19">As can be seen the models incorporating morphological information have slightly lower perplexity on the test data than the D5 model.</S>
    <S sid="98" ssid="20">Note that this is a global evaluation over all the words in the data, including words that do not occur in the training data at all.</S>
    <S sid="99" ssid="21">Figure 5 shows how the conditional entropy varies with respect to the frequency for these models.</S>
    <S sid="100" ssid="22">As can be seen the use of morphological information improves the preformance markedly for rare words, and that this effect reduces as the frequency increases.</S>
    <S sid="101" ssid="23">Note that the use of the frequency information worsens the performance for rare words according to this evaluation &#8212; this is because the rare words are much more tightly grouped into just a few clusters, thus the entropy of the cluster tags is lower.</S>
    <S sid="102" ssid="24">Table 5 shows a qualitative evaluation of some of the clusters produced by the best performing model for 64 clusters on the WSJ data set.</S>
    <S sid="103" ssid="25">We selected the 10 clusters with the largest number of zero frequency word types in.</S>
    <S sid="104" ssid="26">We examined each cluster and chose a simple regular expression to describe it, and calculated the precision and recall for words of all frequency, and for words of zero frequency.</S>
    <S sid="105" ssid="27">Note that several of the clusters capture syntactically salient morphological regularities: regular verb suffixes, noun suffixes and the presence of capitalisation are all detected, together with a class for numbers.</S>
    <S sid="106" ssid="28">In some cases these are split amongst more than one class, thus giving classes with high precision and low recall.</S>
    <S sid="107" ssid="29">We made no attempt to adjust the regular expressions to make these scores high &#8212; we merely present them as an aid to an intuitive understanding of the composition of these clusters.</S>
  </SECTION>
  <SECTION title="6 Ambiguous models" number="6">
    <S sid="108" ssid="1">Up until now we have considered only hard clusters, where each word is unambiguously assigned to a single class.</S>
    <S sid="109" ssid="2">Clearly, because of lexical ambiguity, we would like to be able to assign some words to more than one class.</S>
    <S sid="110" ssid="3">This is sometimes called soft clustering.</S>
    <S sid="111" ssid="4">Space does not permit an extensive analysis of the situation.</S>
    <S sid="112" ssid="5">We shall therefore report briefly on some experiments we have performed and our conclusions largely leaving this as an area for future research.</S>
    <S sid="113" ssid="6">(Jardino and Adda, 1994; SchUtze, 1997; Clark, 2000) have presented models that account for ambiguity to some extent.</S>
    <S sid="114" ssid="7">The most principled way is to use Hidden Markov Models: these provide the formal and technical apparatus required to train when the tags might be ambiguous.</S>
    <S sid="115" ssid="8">(Murakami et al., 1993) presents this idea together with a simple evaluation on English.</S>
    <S sid="116" ssid="9">We therefore extend our approach to allow ambiguous words, by changing our model from a deterministic to nondeterministic model.</S>
    <S sid="117" ssid="10">In this situation we want the states of the HMM to correspond to syntactic categories, and use the standard ExpectationMaximization (EM) algorithm to train it.</S>
    <S sid="118" ssid="11">To experiment with this we chose fullyconnected, randomly initialized Hidden Markov Models, with determined start and end states.</S>
    <S sid="119" ssid="12">We trained the model on the various sentences in the model, on WSJ data.</S>
    <S sid="120" ssid="13">With 5 substates, 20 iterations corpus, and then tagged the data with the most likely (Viterbi) tag sequence.</S>
    <S sid="121" ssid="14">We then evaluated the conditional entropy of the gold standard tags given the derived HMM tags.</S>
    <S sid="122" ssid="15">Table 6 shows the results of this evaluation on some English data for various numbers of states.</S>
    <S sid="123" ssid="16">As can be seen, increasing the number of states of the model does not reduce the conditional entropy of the gold standard tags; rather it increases the lexical ambiguity of the model H(TIW).</S>
    <S sid="124" ssid="17">This is because the states of the HMM will not necessarily correspond directly to syntactic categories &#8212; rather they correspond to sets of words that occur in particular positions &#8212; for example the model might have a state that corresponds to a noun that occurs before a main verb, and a separate state that corresponds to a noun after a main verb.</S>
    <S sid="125" ssid="18">One explanation for this is that the output function from each state of the HMM is a multinomial distribution over the vocabulary which is too powerful since it can memorise any set of words &#8212; thus there is no penalty for the same word being produced by many different states.</S>
    <S sid="126" ssid="19">This suggests a solution that is to replace the multinomial distribution by a weaker distribution such as the Hidden Markov Models we have used before.</S>
    <S sid="127" ssid="20">This gives us a two-level HMM: a HMM where each state corresponds to a word, and where the output function is a HMM where each state corresponds to a letter.</S>
    <S sid="128" ssid="21">This relates to two other approaches that we are aware of (Fine et al., 1998) and (Weber et al., 2001).</S>
    <S sid="129" ssid="22">Table 7 shows a simple evaluation of this approach; we can see that this does not suffer from the same drawback as the previous approach though the results are still poor compared to the other approaches, and in fact are consistently worse than the baselines of Table 1.</S>
    <S sid="130" ssid="23">The problem here is that we are restricted to using quite small HMMs which are insufficiently powerful to memorise large chunks of the vocabulary, and in addition the use of the Forward-Backward algorithm is more computationally expensive &#8212; by at least a factor of the number of states.</S>
  </SECTION>
  <SECTION title="7 Conclusion" number="7">
    <S sid="131" ssid="1">We have applied several different algorithms to the task of identifying parts of speech.</S>
    <S sid="132" ssid="2">We have demonstrated that the use of morphological information can improve the performance of the algorithm with rare words quite substantially.</S>
    <S sid="133" ssid="3">We have also demonstrated that a very simple use of frequency can provide further improvements.</S>
    <S sid="134" ssid="4">Additionally we have tested this on a wide range of languages.</S>
    <S sid="135" ssid="5">Intuitively we have used all of the different types of information available - when we encounter a new word, we know three things about it: first, the context that it has appeared in, secondly the string of characters that it is made of, and thirdly that it is a new word and therefore rare.</S>
  </SECTION>
  <SECTION title="7.1 Future work" number="8">
    <S sid="136" ssid="1">We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.</S>
    <S sid="137" ssid="2">We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).</S>
  </SECTION>
</PAPER>
