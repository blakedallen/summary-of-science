Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms
We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs).
The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates.
We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems.
We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.
We describe how the voted perceptron can be used to train maximum-entropy style taggers and also give a discussion of the theory behind the perceptron algorithm applied to ranking tasks.
Voted perceptron training attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model.
