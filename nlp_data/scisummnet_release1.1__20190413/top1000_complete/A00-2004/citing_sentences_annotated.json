[
  {
    "citance_No": 1, 
    "citing_paper_id": "P01-1064", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Masao, Utiyama | Hitoshi, Isahara", 
    "raw_text": "Choi (2000) used the rank of the cosine, rather than the cosine itself, to measure the similarity of sentences. The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3", 
    "clean_text": "Choi (2000) used the rank of the cosine, rather than the cosine itself, to measure the similarity of sentences. The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P01-1064", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Masao, Utiyama | Hitoshi, Isahara", 
    "raw_text": "(Choi, 2000) Table 1 gives the corpus statistics", 
    "clean_text": "(Choi, 2000) Table 1 gives the corpus statistics.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P01-1064", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Masao, Utiyama | Hitoshi, Isahara", 
    "raw_text": "(Choi, 2000) Segmentation accuracy was measured by the probabilistic error metric? proposed by Beeferman, et al (1999) .7 Low? indicates high accuedges in the minimum-cost path, then the resulting segmentation often contains very small segments consisting of only one or two sentences", 
    "clean_text": "(Choi, 2000) Segmentation accuracy was measured by the probabilistic error metric proposed by Beeferman, et al (1999) Low indicates high accuedges in the minimum-cost path, then the resulting segmentation often contains very small segments consisting of only one or two sentences.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P01-1064", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Masao, Utiyama | Hitoshi, Isahara", 
    "raw_text": "in Table 3 are slightly different from those listed in Table 6 of Choi? s paper (Choi, 2000)", 
    "clean_text": "in Table 3 are slightly different from those listed in Table 6 of Choi's paper (Choi, 2000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P01-1064", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Masao, Utiyama | Hitoshi, Isahara", 
    "raw_text": ", which are implemented in Java (Choi, 2000), due to the difference in programming languages", 
    "clean_text": "which are implemented in Java (Choi, 2000), due to the difference in programming languages.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W12-0703", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Martin, Riedl | Christian, Biemann", 
    "raw_text": "As dataset the Choi dataset (Choi, 2000) is used", 
    "clean_text": "As dataset the Choi dataset (Choi, 2000) is used.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W06-1320", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Maria, Georgescul | Alexander, Clark | Susan, Armstrong", 
    "raw_text": "Inthis paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001)", 
    "clean_text": "In this paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W06-1320", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Maria, Georgescul | Alexander, Clark | Susan, Armstrong", 
    "raw_text": "2.2 C99 Algorithm The C99 algorithm (Choi, 2000) makes a linear segmentation based on a divisive clustering strategy and the cosine similarity measure between any two minimal units", 
    "clean_text": "The C99 algorithm (Choi, 2000) makes a linear segmentation based on a divisive clustering strategy and the cosine similarity measure between any two minimal units.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W06-1320", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Maria, Georgescul | Alexander, Clark | Susan, Armstrong", 
    "raw_text": "Choi (2000) designed an artificial dataset, built by concatenating short pieces of texts that have been extracted from the Brown corpus", 
    "clean_text": "Choi (2000) designed an artificial dataset, built by concatenating short pieces of texts that have been extracted from the Brown corpus.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W06-1320", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Maria, Georgescul | Alexander, Clark | Susan, Armstrong", 
    "raw_text": "Percent error values are given in the figures and we used the following abbreviations: WD to denote WindowDiff error metric; TextSeg KA to denote the TextSeg algorithm (Utiyama and Isahara, 2001) when the average number of boundaries in the reference data was provided to the algorithm; C99 KA to denote the C99 algorithm (Choi, 2000) when the aver age number of boundaries in the reference data was provided to the algorithm; N0 to denote the algorithm proposing a segmentation with no boundaries; All to denote the algorithm proposing the degenerate segmentation all boundaries; RK to denote the algorithm that generates a random known segmentation; and RU to denote the algorithm that generates a random unknown segmentation", 
    "clean_text": "C99 KA to denote the C99 algorithm (Choi, 2000) when the aver age number of boundaries in the reference data was provided to the algorithm.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P06-1004", 
    "citing_paper_authority": 33, 
    "citing_paper_authors": "Igor, Malioutov | Regina, Barzilay", 
    "raw_text": "When documents exhibit sharp variations in lexical distribution, these algorithms are likely to detect segment boundaries accurately. For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks (Choi, 2000)", 
    "clean_text": "For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks (Choi, 2000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P06-1004", 
    "citing_paper_authority": 33, 
    "citing_paper_authors": "Igor, Malioutov | Regina, Barzilay", 
    "raw_text": "Therefore, we only use manual transcriptions of these lectures. Synthetic Corpus Also as part of our analysis, we used the synthetic corpus created byChoi (2000) which is commonly used in the evaluation of segmentation algorithms", 
    "clean_text": "Therefore, we only use manual transcriptions of these lectures. Synthetic Corpus Also as part of our analysis, we used the synthetic corpus created byChoi (2000) which is commonly used in the evaluation of segmentation algorithms.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P06-1004", 
    "citing_paper_authority": 33, 
    "citing_paper_authors": "Igor, Malioutov | Regina, Barzilay", 
    "raw_text": "We fol low Choi (2000) and compute the mean segment length used in determining the parameter k on each reference text separately. We also plot the Receiver Operating Character is tic (ROC) curve to gauge performance at a finer level of discrimination (Swets, 1988)", 
    "clean_text": "We follow Choi (2000) and compute the mean segment length used in determining the parameter k on each reference text separately. We also plot the Receiver Operating Character is tic (ROC) curve to gauge performance at a finer level of discrimination (Swets, 1988).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P06-1004", 
    "citing_paper_authority": 33, 
    "citing_paper_authors": "Igor, Malioutov | Regina, Barzilay", 
    "raw_text": "We test the system on a range of data sets, including the Physics and AI lectures and the synthetic corpus created by Choi (2000)", 
    "clean_text": "We test the system on a range of data sets, including the Physics and AI lectures and the synthetic corpus created by Choi (2000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P06-1004", 
    "citing_paper_authority": 33, 
    "citing_paper_authors": "Igor, Malioutov | Regina, Barzilay", 
    "raw_text": "Comparison with local dependency models We compare our system with the state-of-the-art similarity-based segmentation system developed by Choi (2000)", 
    "clean_text": "Comparison with local dependency models We compare our system with the state-of-the-art similarity-based segmentation system developed by Choi (2000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P06-1004", 
    "citing_paper_authority": 33, 
    "citing_paper_authors": "Igor, Malioutov | Regina, Barzilay", 
    "raw_text": "We use the publicly available implementation of the system and optimize the system on a range of mask-sizes and different parameter settings described in (Choi, 2000) on a held out development set of three lectures", 
    "clean_text": "We use the publicly available implementation of the system and optimize the system on a range of mask-sizes and different parameter settings described in (Choi, 2000) on a held out development set of three lectures.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "C08-1103", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Veselin, Stoyanov | Claire, Cardie", 
    "raw_text": "Existing methods for topic segmentation typically assume that fragments of text (e.g. sentences or sequences of words of a fixed length) with similar lexical distribution are about the same topic; the goal of these methods is to find the boundaries where the lexical distribution changes (e.g. Choi (2000), Malioutov and Barzilay (2006))", 
    "clean_text": "Existing methods for topic segmentation typically assume that fragments of text (e.g. sentences or sequences of words of a fixed length) with similar lexical distribution are about the same topic; the goal of these methods is to find the boundaries where the lexical distribution changes (e.g. Choi (2000), Malioutov and Barzilay (2006)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "C08-1103", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Veselin, Stoyanov | Claire, Cardie", 
    "raw_text": "Nevertheless, we will compare our topic identification approach to a state-of-the-art topic segmentation algorithm (Choi, 2000) in the evaluation", 
    "clean_text": "Nevertheless, we will compare our topic identification approach to a state-of-the-art topic segmentation algorithm (Choi, 2000) in the evaluation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "C08-1103", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Veselin, Stoyanov | Claire, Cardie", 
    "raw_text": "Choi 2000? Choi? s (2000) state-of-the-art approach to finding segment boundaries", 
    "clean_text": "Choi 2000 Choi's (2000) state-of-the-art approach to finding segment boundaries.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "C08-1103", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Veselin, Stoyanov | Claire, Cardie", 
    "raw_text": "Weuse the freely available C99 software described in Choi (2000), varying a parameter that allows us to control the average number of sentences per segment and reporting the best result on the test data", 
    "clean_text": "We use the freely available C99 software described in Choi (2000), varying a parameter that allows us to control the average number of sentences per segment and reporting the best result on the test data.", 
    "keep_for_gold": 0
  }
]
