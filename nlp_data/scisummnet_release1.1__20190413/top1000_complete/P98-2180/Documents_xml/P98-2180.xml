<PAPER>
  <S sid="0">MindNet: Acquiring and Structuring Semantic Information from Text</S>
  <ABSTRACT>
    <S sid="1" ssid="1">As a lexical knowledge base constructed automatically from the definitions and example sentences in two machine-readable dictionaries (MRDs), MindNet embodies several features that distinguish it from prior work with MRDs.</S>
    <S sid="2" ssid="2">It is, however, more than this static resource alone.</S>
    <S sid="3" ssid="3">MindNet represents a general methodology for acquiring, structuring, accessing, and exploiting semantic information from natural language text.</S>
    <S sid="4" ssid="4">This paper provides an overview of the distinguishing characteristics of MindNet, the steps involved in its creation, and its extension beyond dictionary text.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">In this paper, we provide a description of the salient characteristics and functionality of MindNet as it exists today, together with comparisons to related work.</S>
    <S sid="6" ssid="2">We conclude with a discussion on extending the MindNet methodology to the processing of other corpora (specifically, to the text of the Microsoft Encarta&#174; 98 Encyclopedia) and on future plans for MindNet.</S>
    <S sid="7" ssid="3">For additional details and background on the creation and use of MindNet, readers are referred to Richardson (1997), Vanderwende (1996), and Dolan et al. (1993).</S>
  </SECTION>
  <SECTION title="2 Full automation" number="2">
    <S sid="8" ssid="1">MindNet is produced by a fully automatic process, based on the use of a broad-coverage NL parser.</S>
    <S sid="9" ssid="2">A fresh version of MindNet is built regularly as part of a normal regression process.</S>
    <S sid="10" ssid="3">Problems introduced by daily changes to the underlying system or parsing grammar are quickly identified and fixed.</S>
    <S sid="11" ssid="4">Although there has been much research on the use of automatic methods for extracting information from dictionary definitions (e.g., Vossen 1995, Wilks et al. 1996), hand-coded knowledge bases, e.g.</S>
    <S sid="12" ssid="5">WordNet (Miller et al. 1990), continue to be the focus of ongoing research.</S>
    <S sid="13" ssid="6">The Euro WordNet project (Vossen 1996), although continuing in the WordNet tradition, includes a focus on semi-automated procedures for acquiring lexical content.</S>
    <S sid="14" ssid="7">Outside the realm of NLP, we believe that automatic procedures such as MindNet's provide the only credible prospect for acquiring world knowledge on the scale needed to support common-sense reasoning.</S>
    <S sid="15" ssid="8">At the same time, we acknowledge the potential need for the hand vetting of such information to insure accuracy and consistency in production level systems.</S>
  </SECTION>
  <SECTION title="3 Broad-coverage parsing" number="3">
    <S sid="16" ssid="1">The extraction of the semantic information contained in MindNet exploits the very same broadcoverage parser used in the Microsoft Word 97 grammar checker.</S>
    <S sid="17" ssid="2">This parser produces syntactic parse trees and deeper logical forms, to which rules are applied that generate corresponding structures of semantic relations.</S>
    <S sid="18" ssid="3">The parser has not been specially tuned to process dictionary definitions.</S>
    <S sid="19" ssid="4">All enhancements to the parser are geared to handle the immense variety of general text, of which dictionary definitions are simply a modest subset.</S>
    <S sid="20" ssid="5">There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al. 1985), specially constructed definition parsers (e.g., Wilks et al.</S>
    <S sid="21" ssid="6">1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993).</S>
    <S sid="22" ssid="7">However, none of these has succeeded in producing the breadth of semantic relations across entire dictionaries that has been produced for MindNet.</S>
    <S sid="23" ssid="8">Vanderwende (1996) describes in detail the methodology used in the extraction of the semantic relations comprising MindNet.</S>
    <S sid="24" ssid="9">A truly broad-coverage parser is an essential component of this process, and it is the basis for extending it to other sources of information such as encyclopedias and text corpora.</S>
  </SECTION>
  <SECTION title="4 Labeled, semantic relations" number="4">
    <S sid="25" ssid="1">The different types of labeled, semantic relations extracted by parsing for inclusion in MindNet are given in the table below: These relation types may be contrasted with simple co-occurrence statistics used to create network structures from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al. (1996).</S>
    <S sid="26" ssid="2">Labeled relations, while more difficult to obtain, provide greater power for resolving both structural attachment and word sense ambiguities.</S>
    <S sid="27" ssid="3">While many researchers have acknowledged the utility of labeled relations, they have been at times either unable (e.g., for lack of a sufficiently powerful parser) or unwilling (e.g., focused on purely statistical methods) to make the effort to obtain them.</S>
    <S sid="28" ssid="4">This deficiency limits the characterization of word pairs such as river/bank (Wilks et al. 1996) and write/pen (Veronis and Ide 1990) to simple relatedness, whereas the labeled relations of MindNet specify precisely the relations river&#8212;Part--&gt;bank and write&#8212;Means--&gt;pen.</S>
  </SECTION>
  <SECTION title="5 Semantic relation structures" number="5">
    <S sid="29" ssid="1">The automatic extraction of semantic relations (or semrels) from a definition or example sentence for MindNet produces a hierarchical structure of these relations, representing the entire definition or sentence from which they came.</S>
    <S sid="30" ssid="2">Such structures are stored in their entirety in MindNet and provide crucial context for some of the procedures described in later sections of this paper.</S>
    <S sid="31" ssid="3">The semrel structure for a definition of car is given in the figure below. car: &amp;quot;a vehicle with 3 or usu.</S>
    <S sid="32" ssid="4">4 wheels and driven by a motor, esp. one one for carrying people&amp;quot; Early dictionary-based work focused on the extraction of paradigmatic relations, in particular Hypernym relations (e.g., car&#8212;Hypernym-4vehicle).</S>
    <S sid="33" ssid="5">Almost exclusively, these relations, as well as other syntagmatic ones, have continued to take the form of relational triples (see Wilks et al. 1996).</S>
    <S sid="34" ssid="6">The larger contexts from which these relations have been taken have generally not been retained.</S>
    <S sid="35" ssid="7">For labeled relations, only a few researchers (recently, Barriere and Popowich 1996), have appeared to be interested in entire semantic structures extracted from dictionary definitions, though they have not reported extracting a significant number of them.</S>
  </SECTION>
  <SECTION title="6 Full inversion of structures" number="6">
    <S sid="36" ssid="1">After semrel structures are created, they are fully inverted and propagated throughout the entire MindNet database, being linked to every word that appears in them.</S>
    <S sid="37" ssid="2">Such an inverted structure, produced from a definition for motorist and linked to the entry for car (appearing as the root of the inverted structure), is shown in the figure below: Researchers who produced spreading activation networks from MRDs, including Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks.</S>
    <S sid="38" ssid="3">Words were not related backward to any of the headwords whose definitions mentioned them, and words co-occurring in the same definition were not related directly.</S>
    <S sid="39" ssid="4">In the fully inverted structures stored in MindNet, however, all words are cross-linked, no matter where they appear.</S>
    <S sid="40" ssid="5">The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993) that LKBs created from MRDs provide spotty coverage of a language at best.</S>
    <S sid="41" ssid="6">Experiments described elsewhere (Richardson 1997) demonstrate the comprehensive coverage of the information contained in MindNet.</S>
    <S sid="42" ssid="7">Some statistics indicating the size (rounded to the nearest thousand) of the current version of MindNet and the processing time required to create it are provided in the table below.</S>
    <S sid="43" ssid="8">The definitions and example sentences are from the Longman Dictionary of Contemporary English (LDOCE) and the American Heritage Dictionary, ri Edition (AHD3).</S>
  </SECTION>
  <SECTION title="7 Weighted paths" number="7">
    <S sid="44" ssid="1">Inverted semrel structures facilitate the access to direct and indirect relationships between the root word of each structure, which is the headword for the MindNet entry containing it, and every other word contained in the structures.</S>
    <S sid="45" ssid="2">These relationships, consisting of one or more semantic relations connected together, constitute semrel paths between two words.</S>
    <S sid="46" ssid="3">For example, the semrel path between car and person in Figure 2 above is: car(--Tobj--drive&#8212;Tsub--&gt;motorist&#8212;Hyp--&gt;person.</S>
    <S sid="47" ssid="4">An extended semrel path is a path created from subpaths in two different inverted semrel structures.</S>
    <S sid="48" ssid="5">For example, car and truck are not related directly by a semantic relation or by a semrel path from any single semrel structure.</S>
    <S sid="49" ssid="6">However, if one allows the joining of the semantic relations car&#8212;Hyp--vehicle and vehicle&#8249;-Hyp&#8212;truck, each from a different semrel structure, at the word vehicle, the semrel path car&#8212;Hyp-4vehiclet-Hyp--truck results.</S>
    <S sid="50" ssid="7">Adequately constrained, extended semrel paths have proven invaluable in determining the relationship between words in MindNet that would not otherwise be connected.</S>
    <S sid="51" ssid="8">Semrel paths are automatically assigned weights that reflect their salience.</S>
    <S sid="52" ssid="9">The weights in MindNet are based on the computation of averaged vertex probability, which gives preference to semantic relations occurring with middle frequency, and are described in detail in Richardson (1997).</S>
    <S sid="53" ssid="10">Weighting schemes with similar goals are found in work by Braden-Harder (1993) and Bookman (1994).</S>
  </SECTION>
  <SECTION title="8 Similarity and inference" number="8">
    <S sid="54" ssid="1">Many researchers, both in the dictionary- and corpus-based camps, have worked extensively on developing methods to identify similarity between words, since similarity determination is crucial to many word sense disambiguation and parametersmoothing/inference procedures.</S>
    <S sid="55" ssid="2">However, some researchers have failed to distinguish between substitutional similarity and general relatedness.</S>
    <S sid="56" ssid="3">The similarity procedure of MindNet focuses on measuring substitutional similarity, but a function is also provided for producing clusters of generally related words.</S>
    <S sid="57" ssid="4">Two general strategies have been described in the literature for identifying substitutional similarity.</S>
    <S sid="58" ssid="5">One is based on identifying direct, paradigmatic relations between the words, such as Hypernym or Synonym.</S>
    <S sid="59" ssid="6">For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et al. (1995) and Agirre and Rigau (1996).</S>
    <S sid="60" ssid="7">The other strategy is based on identifying syntagmatic relations with other words that similar words have in common.</S>
    <S sid="61" ssid="8">Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994).</S>
    <S sid="62" ssid="9">There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995).</S>
    <S sid="63" ssid="10">However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet.</S>
    <S sid="64" ssid="11">The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words.</S>
    <S sid="65" ssid="12">For example, some of the top semrel paths in MindNet between pen and pencil, are shown below: penf--Means--draw&#8212;Means--4pencil penf-Means&#8212;write&#8212;Means-&gt;pencil pen&#8212;Hyp-4instrument*-Hyp&#8212;pencil pen&#8212;Hyp--&gt;write&#8212;Means---*pencil pen&#8249;-Means&#8212;write(-Hyp--pencil pencil In the above example, a pattern of semrel symmetry clearly emerges in many of the paths.</S>
    <S sid="66" ssid="13">This observation of symmetry led to the hypothesis that similar words are typically connected in MindNet by semrel paths that frequently exhibit certain patterns of relations (exclusive of the words they actually connect), many patterns being symmetrical, but others not.</S>
    <S sid="67" ssid="14">Several experiments were performed in which word pairs from a thesaurus and an anti-thesaurus (the latter containing dissimilar words) were used in a training phase to identify semrel path patterns that indicate similarity.</S>
    <S sid="68" ssid="15">These path patterns were then used in a testing phase to determine the substitutional similarity or dissimilarity of unseen word pairs (algorithms are described in Richardson 1997).</S>
    <S sid="69" ssid="16">The results, summarized in the table below, demonstrate the strength of this integrated approach, which uniquely exploits both the paradigmatic and the syntagmatic relations in MindNet.</S>
    <S sid="70" ssid="17">Training: over 100,000 word pairs from a thesaurus and anti-thesaurus produced 285,000 semrel paths containing approx.</S>
    <S sid="71" ssid="18">13,500 unique path patterns.</S>
    <S sid="72" ssid="19">Testing: over 100,000 (different) word pairs from a thesaurus and anti-thesaurus were evaluated using the path patterns.</S>
    <S sid="73" ssid="20">Similar correct Dissimilar correct 84% 82% Human benchmark: random sample of 200 similar and dissimilar word pairs were evaluated by 5 humans and by MindNet: Similar correct Dissimilar correct This powerful similarity procedure may also be used to extend the coverage of the relations in MindNet.</S>
    <S sid="74" ssid="21">Equivalent to the use of similarity determination in corpus-based approaches to infer absent n-grams or triples (e.g., Dagan et al. 1994, Grishman and Sterling 1994), an inference procedure has been developed which allows semantic relations not presently in MindNet to be inferred from those that are.</S>
    <S sid="75" ssid="22">It also exploits the top-ranked paths between the words in the relation to be inferred.</S>
    <S sid="76" ssid="23">For example, if the relation watch&#8212;Means&#8212;gelescope were not in MindNet, it could be inferred by first finding the semrel paths between watch and telescope, examining those paths to see if another word appears in a Means relation with telescope, and then checking the similarity between that word and watch.</S>
    <S sid="77" ssid="24">As it turns out, the word observe satisfies these conditions in the path: watch&#8212;Hyp--observe&#8212;Means--telescope and therefore, it may be inferred that one can watch by Means of a telescope.</S>
    <S sid="78" ssid="25">The seamless integration of the inference and similarity procedures, both utilizing the weighted, extended paths derived from inverted semrel structures in MindNet, is a unique strength of this approach.</S>
  </SECTION>
  <SECTION title="9 Disambiguating MindNet" number="9">
    <S sid="79" ssid="1">An additional level of processing during the creation of MindNet seeks to provide sense identifiers on the words of semrel structures.</S>
    <S sid="80" ssid="2">Typically, word sense disambiguation (WSD) occurs during the parsing of definitions and example sentences, following the construction of logical forms (see Braden-Harder, 1993).</S>
    <S sid="81" ssid="3">Detailed information from the parse, both morphological and syntactic, sharply reduces the range of senses that can be plausibly assigned to each word.</S>
    <S sid="82" ssid="4">Other aspects of dictionary structure are also exploited, including domain information associated with particular senses (e.g., Baseball).</S>
    <S sid="83" ssid="5">In processing normal input text outside of the context of MindNet creation, WSD relies crucially on information from MindNet about how word senses are linked to one another.</S>
    <S sid="84" ssid="6">To help mitigate this bootstrapping problem during the initial construction of MindNet, we have experimented with a two-pass approach to WSD.</S>
    <S sid="85" ssid="7">During a first pass, a version of MindNet that does not include WSD is constructed.</S>
    <S sid="86" ssid="8">The result is a semantic network that nonetheless contains a great deal of &amp;quot;ambient&amp;quot; information about sense assignments.</S>
    <S sid="87" ssid="9">For instance, processing the definition spin 101: (of a spider or silkworm) to produce thread.., yields a semrel structure in which the sense node spin101 is linked by a Deep_Subject relation to the undisambiguated form spider.</S>
    <S sid="88" ssid="10">On the subsequent pass, this information can be exploited by WSD in assigning sense 101 to the word spin in unrelated definitions: wolf spider 100: any of various spiders...that...do not spin webs.</S>
    <S sid="89" ssid="11">This kind of bootstrapping reflects the broader nature of our approach, as discussed in the next section: a fully and accurately disambiguated MindNet allows us to bootstrap senses onto words encountered in free text outside the dictionary domain.</S>
    <S sid="90" ssid="12">The creation of MindNet was never intended to be an end unto itself.</S>
    <S sid="91" ssid="13">Instead, our emphasis has been on building a broad-coverage NLP understanding system.</S>
    <S sid="92" ssid="14">We consider the methodology for creating MindNet to consist of a set of general tools for acquiring, structuring, accessing, and exploiting semantic information from NL text.</S>
    <S sid="93" ssid="15">Our techniques for building MindNet are largely rule-based.</S>
    <S sid="94" ssid="16">However we arrive at these representations, though, the overall structure of MindNet can be regarded as crucially dependent on statistics.</S>
    <S sid="95" ssid="17">We have much more in common with traditional corpus-based approaches than a first glance might suggest.</S>
    <S sid="96" ssid="18">An advantage we have over these approaches, however, is the rich structure imposed by the parse, logical form, and word sense disambiguation components of our system.</S>
    <S sid="97" ssid="19">The statistics we use in the context of MindNet allow richer metrics because the data themselves are richer.</S>
    <S sid="98" ssid="20">Our first foray into the realm of processing free text with our methods has already been accomplished; Table 2 showed that some 58,000 example sentences from LDOCE and AHD3 were processed in the creation of our current MindNet.</S>
    <S sid="99" ssid="21">To put our hypothesis to a much more rigorous test, we have recently embarked on the assimilation of the entire text of the Microsoft Encarta&#174; 98 Encyclopedia.</S>
    <S sid="100" ssid="22">While this has presented several new challenges in terms of volume alone, we have nevertheless successfully completed a first pass and have produced and added semrel structures from the Encarta&#174; 98 text to MindNet.</S>
    <S sid="101" ssid="23">Statistics on that pass are given below: Besides our venture into additional English data, we fully intend to apply the same methodologies to text in other languages as well.</S>
    <S sid="102" ssid="24">We are currently developing NLP systems for 3 European and 3 Asian languages: French, German, and Spanish; Chinese, Japanese, and Korean.</S>
    <S sid="103" ssid="25">The syntactic parsers for some of these languages are already quite advanced and have been demonstrated publicly.</S>
    <S sid="104" ssid="26">As the systems for these languages mature, we will create corresponding MindNets, beginning, as we did in English, with the processing of machine-readable reference materials and then adding information gleaned from corpora.</S>
  </SECTION>
</PAPER>
