A Global Joint Model for Semantic Role Labeling
We present a model for semantic role labeling that effectively captures the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments.
We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases.
The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments.
We evaluate the gains from incorporating this joint information on the Propbank corpus, when using correct syntactic parse trees as input, and when using automatically derived parse trees.
The gains amount to 24.1% error reduction on all arguments and 36.8% on core arguments for gold-standard parse trees on Propbank.
For automatic parse trees, the error reductions are 8.3% and 10.3% on all and core arguments, respectively.
We also present results on the CoNLL 2005 shared task data set.
Additionally, we explore considering multiple syntactic analyses to cope with parser noise and uncertainty.
We present a re-ranking model to jointly learn the semantic roles of multiple constituents in the SRL task.
