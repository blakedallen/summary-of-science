[
  {
    "citance_No": 1, 
    "citing_paper_id": "P10-1120", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Amit, Dubey", 
    "raw_text": "Following Hale (2001) and Levy (2008), among others, the syntactic processor uses an incremental probabilistic Earley parser to compute a metric which correlates with increased reading difficulty", 
    "clean_text": "Following Hale (2001) and Levy (2008), among others, the syntactic processor uses an incremental probabilistic Earley parser to compute a metric which correlates with increased reading difficulty.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P10-1120", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Amit, Dubey", 
    "raw_text": "Using the prefix probability, we can compute the word-by-word Surprisal (Hale, 2001), by taking the log ratio of the previous word? s prefix probability against this word? s prefix probability: log (P (wi? 1..", 
    "clean_text": "Using the prefix probability, we can compute the word-by-word Surprisal (Hale, 2001), by taking the log ratio of the previous word's prefix probability against this word's prefix probability.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P10-1121", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Stephen, Wu | Asaf, Bachrach | Carlos, Cardenas | William, Schuler", 
    "raw_text": "Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity", 
    "clean_text": "Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P10-1121", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Stephen, Wu | Asaf, Bachrach | Carlos, Cardenas | William, Schuler", 
    "raw_text": "Surprisal (Hale, 2001) is then a straightforward calculation from the prefix probability", 
    "clean_text": "Surprisal (Hale, 2001) is then a straightforward calculation from the prefix probability.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W10-2009", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Harm, Brouwer | Hartmut, Fitz | John, Hoeks", 
    "raw_text": "Both Hale (2001) and Levy (2008) used a Probabilistic Context Free Grammar (PCFG) as a language model in their implementations of surprisal theory", 
    "clean_text": "Both Hale (2001) and Levy (2008) used a Probabilistic Context Free Grammar (PCFG) as a language model in their implementations of surprisal theory.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W10-2009", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Harm, Brouwer | Hartmut, Fitz | John, Hoeks", 
    "raw_text": "Hale (2001) pointed out that the ratio of the prefix probabilities P (w1..", 
    "clean_text": "Hale (2001) pointed out that the ratio of the prefix probabilities.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P08-2002", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marisa Ferrara, Boston | John, Hale | Reinhold, Kliegl | Shravan, Vasishth", 
    "raw_text": "Hale (2001) suggests this quantity as an index of psycholinguistic difficulty", 
    "clean_text": "Hale (2001) suggests this quantity as an index of psycholinguistic difficulty.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P10-1021", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Jeff, Mitchell | Mirella, Lapata | Vera, Demberg | Frank, Keller", 
    "raw_text": "Probably the best known measure of syn tactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words ,typically computed using a probabilistic context-free grammar", 
    "clean_text": "Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P10-1021", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Jeff, Mitchell | Mirella, Lapata | Vera, Demberg | Frank, Keller", 
    "raw_text": "The original formulation of surprisal (Hale 2001) used a probabilistic parser to calculate these probabilities, as the emphasis was on the processing costs incurred when parsing structurally ambiguous garden path sentences.1 Several variants of calculating surprisal have been developed in the literature since using different parsing strategies 1While hearing a sentence like The horse raced past the barn fell (Bever 1970), English speakers are inclined to interpreted horse as the subject of raced expecting the sentence to end at the word barn", 
    "clean_text": "The original formulation of surprisal (Hale 2001) used a probabilistic parser to calculate these probabilities, as the emphasis was on the processing costs incurred when parsing structurally ambiguous garden path sentences.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P06-1007", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jihyun, Park | Chris, Brew", 
    "raw_text": "Hale (2001) reported that a probabilistic Earley parser can make correct predictions of garden-path effects and the subject/object relative asymmetry", 
    "clean_text": "Hale (2001) reported that a probabilistic Earley parser can make correct predictions of garden-path effects and the subject/object relative asymmetry.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W10-2004", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Tim, Miller | William, Schuler", 
    "raw_text": "Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t? 1 and word t. In other words, it measures how much probability was lost in incorporating the next word into the current hypotheses", 
    "clean_text": "Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t1 and word t.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D12-1033", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Nikolaos, Engonopoulos | Philip, Gorinski | Asad B., Sayeed | Vera, Demberg", 
    "raw_text": "Hale (2001) showed that surprisal calculated froma probabilistic Earley parser correctly predicts well 356 known processing phenomena that were believed to emerge from structural ambiguities (e.g., garden paths) and Levy (2008) further demonstrated the relevance of surprisal to human sentence processing difficulty on a range of syntactic processing difficulty phenomena. There is existing work in correlating information theoretic measures of linguistic redundancy to the observed duration of speech units", 
    "clean_text": "Hale (2001) showed that surprisal calculated from a probabilistic Earley parser correctly predicts well 356 known processing phenomena that were believed to emerge from structural ambiguities (e.g., garden paths) and Levy (2008) further demonstrated the relevance of surprisal to human sentence processing difficulty on a range of syntactic processing difficulty phenomena.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W11-0612", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mattias, Nilsson | Joakim, Nivre", 
    "raw_text": "Thesurprisal (Hale, 2001) at word wi refers to the negative log probability of wi given the preceding words, computed using the prefix probabilities of the parser", 
    "clean_text": "The surprisal (Hale, 2001) at word wi refers to the negative log probability of wi given the preceding words, computed using the prefix probabilities of the parser.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P11-1106", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Roger, Levy", 
    "raw_text": "Hale (2001) demonstrated that surprisal thus predicts strong garden-pathing effects in the classic sentence The horse raced past the barn fell on basis of the overall rarity of reduced relative clauses alone", 
    "clean_text": "Hale (2001) demonstrated that surprisal thus predicts strong garden-pathing effects in the classic sentence.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P06-1053", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Amit, Dubey | Frank, Keller | Patrick, Sturt", 
    "raw_text": "An alternative is to keep track of all derivations, and predict difficulty at points where there is a large change in the shape of the probability distribution across adjacent parsing states (Hale, 2001)", 
    "clean_text": "An alternative is to keep track of all derivations, and predict difficulty at points where there is a large change in the shape of the probability distribution across adjacent parsing states (Hale, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P06-1053", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Amit, Dubey | Frank, Keller | Patrick, Sturt", 
    "raw_text": "This conclusion is strengthened when we turn to consider the performance of the parser on the standard Penn Treebank test set: the Within model showed a small increase in F-score over the PCFG baseline, while the copy model showed no such advantage.5All the models we proposed offer a broad coverage account of human parsing, not just a limited model on a hand-selected set of examples, such as the models proposed by Jurafsky (1996) and Hale (2001) (but see Crocker and Brants 2000)", 
    "clean_text": "This conclusion is strengthened when we turn to consider the performance of the parser on the standard Penn Treebank test set: the Within model showed a small increase in F-score over the PCFG baseline, while the copy model showed no such advantage.5All the models we proposed offer a broad coverage account of human parsing, not just a limited model on a hand-selected set of examples, such as the models proposed by Jurafsky (1996) and Hale (2001) (but see Crocker and Brants 2000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W10-2005", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Marisa Ferrara, Boston", 
    "raw_text": "Al though this does not match the interpretation of the experimental results followed in this paper, it leaves open the possibility that the feature could model the data according to a different measure of parsing difficulty, such as surprisal (Hale, 2001)", 
    "clean_text": "Although this does not match the interpretation of the experimental results followed in this paper, it leaves open the possibility that the feature could model the data according to a different measure of parsing difficulty, such as surprisal (Hale, 2001).", 
    "keep_for_gold": 0
  }
]
