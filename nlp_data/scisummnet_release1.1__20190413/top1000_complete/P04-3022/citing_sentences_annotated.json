[
  {
    "citance_No": 1, 
    "citing_paper_id": "P07-2040", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Toru, Hirano | Yoshihiro, Matsuo | Genichiro, Kikui", 
    "raw_text": "Theyuse two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004) .These methods have two problems which we consider in this paper", 
    "clean_text": "They use two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P07-2040", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Toru, Hirano | Yoshihiro, Matsuo | Genichiro, Kikui", 
    "raw_text": "tactic3 and word-based features, the path of the pairs of NEs in the parse tree and the word n gram between pairs of NEs (Kambhatla, 2004) 3", 
    "clean_text": "Supervised learning method using syntactic and word-based features, the path of the pairs of NEs in the parse tree and the word n gram between pairs of NEs (Kambhatla, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W06-1659", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Hany, Hassan | Ahmed, Hassan | Ossama, Emam", 
    "raw_text": "The approaches proposed to the ACE RDC task such as kernel methods (Zelenko et al, 2002) and Maximum Entropy methods (Kambhatla, 2004) required the availability of large set of human annotated corpora which are tagged with relation instances", 
    "clean_text": "The approaches proposed to the ACE RDC task such as kernel methods (Zelenko et al, 2002) and Maximum Entropy methods (Kambhatla, 2004) required the availability of large set of human annotated corpora which are tagged with relation instances.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W06-1659", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Hany, Hassan | Ahmed, Hassan | Ossama, Emam", 
    "raw_text": "We compare our results to a state-of-the-art supervised system similar to the system described in (Kambhatla, 2004)", 
    "clean_text": "We compare our results to a state-of-the-art supervised system similar to the system described in (Kambhatla, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "I08-2119", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Mengqiu, Wang", 
    "raw_text": "Kambhatla (2004 )tooka similar approach but used multivariate logistic regression (Kambhatla, 2004)", 
    "clean_text": "Kambhatla (2004) took a similar approach but used multivariate logistic regression (Kambhatla, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P06-1104", 
    "citing_paper_authority": 43, 
    "citing_paper_authors": "Min, Zhang | Jie, Zhang | Jian, Su | Guodong, Zhou", 
    "raw_text": "Methods (2002/2003 data) P (%) R (%) F Ours: composite kernel 2 (polynomial expansion) 77.3 (64.9) 65.6 (51.2) 70.9 (57.2) Zhou et al (2005): feature-based SVM 77.2 (63.1) 60.7 (49.5) 68.0 (55.5) Kambhatla (2004): feature-based ME () (63.5) () (45.2) () (52.8) Ours: tree kernel with entity information at node 76.1 (62.4) 62.6 (48.5) 68.7 (54.6) Bunescu and Mooney (2005): shortest path dependency kernel 65.5 () 43.8 () 52.5 () Culotta and Sorensen (2004): dependency kernel 67.1 () 35.0 () 45.8 () Table 3", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W10-0913", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Zareen, Syed | Evelyne, Viegas", 
    "raw_text": "Kambhatla (2004) developed a method for extracting relations by applying Maximum Entropy models to combine lexical, syntactic and semantic features and re port that they obtain improvement in results when they combine variety of features", 
    "clean_text": "Kambhatla (2004) developed a method for extracting relations by applying Maximum Entropy models to combine lexical, syntactic and semantic features and report that they obtain improvement in results when they combine variety of features.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P06-1017", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Jinxiu, Chen | Ji, Donghong | Chew Lim, Tan | Zhengyu, Niu", 
    "raw_text": "Relation Dectection Relation Detection and Classification on Types on Subtypes Method P R F P R F P R FCulotta and Soresen (2004) Tree kernel based 81.2 51.8 63.2 67.1 35.0 45.8- Kambhatla (2004) Feature based, Maxi mum Entropy--- 63.5 45.2 52.8 Zhou et al (2005) Feature based, SVM 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5 bootstrapping", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P06-2060", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Nanda, Kambhatla", 
    "raw_text": "Similar to our earlier work (Kambhatla, 2004), we used a combination of lexical, syntactic, and semantic features including all the words in between the two mentions, the entity types and subtypes of the two mentions, the number of words in between the two mentions, features derived from the small est parse fragment connecting the two mentions, etc. These features were held constant throughout these experiments", 
    "clean_text": "Similar to our earlier work (Kambhatla, 2004), we used a combination of lexical, syntactic, and semantic features including all the words in between the two mentions, the entity types and subtypes of the two mentions, the number of words in between the two mentions, features derived from the smallest parse fragment connecting the two mentions, etc.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D07-1076", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Guodong, Zhou | Min, Zhang | Ji, Donghong | Qiao-ming, Zhu", 
    "raw_text": "For the feature-based methods, Kambhatla (2004) employed Maximum Entropy models to combine diverse lexical, syntactic and semantic features in relation extraction, and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus", 
    "clean_text": "For the feature-based methods, Kambhatla (2004) employed Maximum Entropy models to combine diverse lexical, syntactic and semantic features in relation extraction, and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D07-1076", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Guodong, Zhou | Min, Zhang | Ji, Donghong | Qiao-ming, Zhu", 
    "raw_text": "Another problem is that, although they can explore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found difficult to well preserve structured information in the parse trees using the feature-based methods", 
    "clean_text": "Another problem is that, although they can explore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found difficult to well preserve structured information in the parse trees using the feature-based methods.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D07-1076", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Guodong, Zhou | Min, Zhang | Ji, Donghong | Qiao-ming, Zhu", 
    "raw_text": "System P (%) R (%) F Linear Kernel 78.2 (77.2) 63.4 (60.7) 70.1 (68.0) Context-Sensitive Convolution Tree Kernel 81.1 (80.1) 66.7 (63.8) 73.2 (71.0) Composite Kernel 82.2 (80.8) 70.2 (68.4) 75.8 (74.1) Table 3: Performance of the composite kernel via polynomial interpolation on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora Comparison with Other Systems ACE RDC 2003 P (%) R (%) F Ours: composite kernel 80.8 (65.2) 68.4 (54.9) 74.1 (59.6) Zhang et al (2006): composite kernel 77.3 (64.9) 65.6 (51.2) 70.9 (57.2) Ours: context-sensitive convolution tree kernel 80.1 (63.4) 63.8 (51.9) 71.0 (57.1) Zhang et al (2006): convolution tree kernel 76.1 (62.4) 62.6 (48.5) 68.7 (54.6) Bunescu et al (2005): shortest path dependency kernel 65.5 () 43.8 () 52.5 () Culotta et al (2004): dependency kernel 67.1 () 35.0 () 45.8 () Zhou et al (2005): feature-based 77.2 (63.1) 60.7 (49.5) 68.0 (55.5) Kambhatla (2004): feature-based (63.5) (45.2) (52.8) Table 4: Comparison of difference systems on the ACE RDC 2003 corpus over both 5 types (outside the parentheses) and 24 subtypes (inside the parentheses) ACE RDC 2004 P (%) R (%) F Ours: composite kernel 82.2 (70.3) 70.2 (62.2) 75.8 (66.0) Zhang et al (2006): composite kernel 76.1 (68.6) 68.4 (59.3) 72.1 (63.6) Zhao et al (2005) :8 composite kernel 69.2 () 70.5 () 70.4 () Ours: context-sensitive convolution tree kernel 81.1 (68.8) 66.7 (60.3) 73.2 (64.3) Zhang et al (2006): convolution tree kernel 72.5 () 56.7 () 63.6 () Table 5: Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types (outside the parentheses) and 23 subtypes (inside the parentheses) Finally, Tables 4 and 5 compare our system with other state-of-the-art systems9 on the ACE RDC 2003 and 2004 corpora, respectively", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P05-1053", 
    "citing_paper_authority": 50, 
    "citing_paper_authors": "Guodong, Zhou | Jian, Su | Jie, Zhang | Min, Zhang", 
    "raw_text": "Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree", 
    "clean_text": "Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P05-1053", 
    "citing_paper_authority": 50, 
    "citing_paper_authors": "Guodong, Zhou | Jian, Su | Jie, Zhang | Min, Zhang", 
    "raw_text": "Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect", 
    "clean_text": "Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P05-1053", 
    "citing_paper_authority": 50, 
    "citing_paper_authors": "Guodong, Zhou | Jian, Su | Jie, Zhang | Min, Zhang", 
    "raw_text": "Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes", 
    "clean_text": "Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "I08-1057", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Chen-Ming, Hung", 
    "raw_text": "(Culotta and Sorensen, 2004) extended this work to estimate kernel functions between augmented dependency trees, while (Kambhatla, 2004) combined lexical features, syntactic features, and semantic features in a maximum entropy model", 
    "clean_text": "(Culotta and Sorensen, 2004) extended this work to estimate kernel functions between augmented dependency trees, while (Kambhatla, 2004) combined lexical features, syntactic features, and semantic features in a maximum entropy model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "I08-1057", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Chen-Ming, Hung", 
    "raw_text": "However, the semantic features discussed in (Kambhatla, 2004) still focus on the word level instead of the conceptual level.LDA is an aspect model that represents documents as a set of topics instead of a bag-of-words", 
    "clean_text": "However, the semantic features discussed in (Kambhatla, 2004) still focus on the word level instead of the conceptual level.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "N06-1037", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Min, Zhang | Jie, Zhang | Jian, Su", 
    "raw_text": "Kambhatla (2004) employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text for relation extraction", 
    "clean_text": "Kambhatla (2004) employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text for relation extraction.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "N06-1037", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Min, Zhang | Jie, Zhang | Jian, Su", 
    "raw_text": "The features used in Kambhatla (2004) and Zhou et al (2005) have to be selected and carefully calibrated manually", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "N06-1037", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Min, Zhang | Jie, Zhang | Jian, Su", 
    "raw_text": "Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features", 
    "clean_text": "Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features.", 
    "keep_for_gold": 0
  }
]
