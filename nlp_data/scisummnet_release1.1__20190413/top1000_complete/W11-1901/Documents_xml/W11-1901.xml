<PAPER>
  <S sid="0">CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes</S>
  <ABSTRACT>
    <S sid="1" ssid="1">The CoNLL-2011 shared task involved predicting coreference using OntoNotes data.</S>
    <S sid="2" ssid="2">Resources in this field have tended to be limited to noun phrase coreference, often on a set of entities, such as entities.</S>
    <S sid="3" ssid="3">OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types.</S>
    <S sid="4" ssid="4">OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure.</S>
    <S sid="5" ssid="5">This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, and evaluation criteria, and presents and discusses the results achieved by the participating systems.</S>
    <S sid="6" ssid="6">Having a standard test set and evaluation parameters, all based on a new resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="7" ssid="1">The importance of coreference resolution for the entity/event detection task, namely identifying all mentions of entities and events in text and clustering them into equivalence classes, has been well recognized in the natural language processing community.</S>
    <S sid="8" ssid="2">Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data.</S>
    <S sid="9" ssid="3">Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules.</S>
    <S sid="10" ssid="4">A systematic study was then conducted using decision trees by Soon et al. (2001).</S>
    <S sid="11" ssid="5">Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010).</S>
    <S sid="12" ssid="6">Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007).</S>
    <S sid="13" ssid="7">Researchers continued finding novel ways of exploiting ontologies such as WordNet.</S>
    <S sid="14" ssid="8">Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006).</S>
    <S sid="15" ssid="9">In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs&#8217; distance, etc.</S>
    <S sid="16" ssid="10">A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject.</S>
    <S sid="17" ssid="11">Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC).</S>
    <S sid="18" ssid="12">These corpora were tagged with coreferring entities identified by noun phrases in the text.</S>
    <S sid="19" ssid="13">The de facto standard datasets for current coreference studies are the MUC (Hirschman and Chinchor, 1997; Chinchor, 2001; Chinchor and Sundheim, 2003) and the ACE1 (G. Doddington et al., 2000) corpora.</S>
    <S sid="20" ssid="14">The MUC corpora cover all noun phrases in text, but represent small training and test sets.</S>
    <S sid="21" ssid="15">The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities.</S>
    <S sid="22" ssid="16">They are also less consistent, in terms of inter-annotator agreement (ITA) (Hirschman et al., 1998).</S>
    <S sid="23" ssid="17">This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models.</S>
    <S sid="24" ssid="18">The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004).</S>
    <S sid="25" ssid="19">There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation &#8211; both of which seek to take information access technology to the next level &#8211; we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification.</S>
    <S sid="26" ssid="20">Identification and encoding of richer knowledge &#8211; possibly linked to knowledge sources &#8211; and development of learning algorithms that would effectively incorporate them is a necessary next step towards improving the current state of the art.</S>
    <S sid="27" ssid="21">The computational learning community, in general, is also witnessing a move towards evaluations based on joint inference, with the two previous CoNLL tasks (Surdeanu et al., 2008; Haji&#711;c et al., 2009) devoted to joint learning of syntactic and semantic dependencies.</S>
    <S sid="28" ssid="22">A principle ingredient for joint learning is the presence of multiple layers of semantic information.</S>
    <S sid="29" ssid="23">One fundamental question still remains, and that is &#8211; what would it take to improve the state of the art in coreference resolution that has not been attempted so far?</S>
    <S sid="30" ssid="24">Many different algorithms have been tried in the past 15 years, but one thing that is still lacking is a corpus comprehensively tagged on a large scale with consistent, multiple layers of semantic information.</S>
    <S sid="31" ssid="25">One of the many goals of the OntoNotes project2 (Hovy et al., 2006; Weischedel et al., 2011) is to explore whether it can fill this void and help push the progress further &#8211; not only in coreference, but with the various layers of semantics that it tries to capture.</S>
    <S sid="32" ssid="26">As one of its layers, it has created a corpus for general anaphoric coreference that covers entities and events not limited to noun phrases or a limited set of entity types.</S>
    <S sid="33" ssid="27">A small portion of this corpus from the newswire and broadcast news genres (-120k) was recently used for a SEMEVAL task (Recasens et al., 2010).</S>
    <S sid="34" ssid="28">As mentioned earlier, the coreference layer in OntoNotes constitutes just one part of a multi-layered, integrated annotation of shallow semantic structure in text with high interannotator agreement, which also provides a unique opportunity for performing joint inference over a substantial body of data.</S>
    <S sid="35" ssid="29">The remainder of this paper is organized as follows.</S>
    <S sid="36" ssid="30">Section 2 presents an overview of the OntoNotes corpus.</S>
    <S sid="37" ssid="31">Section 3 describes the coreference annotation in OntoNotes.</S>
    <S sid="38" ssid="32">Section 4 then describes the shared task, including the data provided and the evaluation criteria.</S>
    <S sid="39" ssid="33">Sections 5 and 6 then describe the participating system results and analyze the approaches, and Section 7 concludes.</S>
  </SECTION>
  <SECTION title="2 The OntoNotes Corpus" number="2">
    <S sid="40" ssid="1">The OntoNotes project has created a corpus of largescale, accurate, and integrated annotation of multiple levels of the shallow semantic structure in text.</S>
    <S sid="41" ssid="2">The idea is that this rich, integrated annotation covering many layers will allow for richer, cross-layer models enabling significantly better automatic semantic analysis.</S>
    <S sid="42" ssid="3">In addition to coreference, this data is also tagged with syntactic trees, high coverage verb and some noun propositions, partial verb and noun word senses, and 18 named entity types.</S>
    <S sid="43" ssid="4">However, such multi-layer annotations, with complex, cross-layer dependencies, demands a robust, efficient, scalable mechanism for storing them while providing efficient, convenient, integrated access to the the underlying structure.</S>
    <S sid="44" ssid="5">To this effect, it uses a relational database representation that captures both the inter- and intra-layer dependencies and also provides an object-oriented API for efficient, multitiered access to this data (Pradhan et al., 2007a).</S>
    <S sid="45" ssid="6">This should facilitate the creation of cross-layer features in integrated predictive models that will make use of these annotations.</S>
    <S sid="46" ssid="7">Although OntoNotes is a multi-lingual resource with all layers of annotation covering three languages: English, Chinese and Arabic, for the scope of this paper, we will just look at the English portion.</S>
    <S sid="47" ssid="8">Over the years of the development of this corpus, there were various priorities that came into play, and therefore not all the data in the English portion is annotated with all the different layers of annotation.</S>
    <S sid="48" ssid="9">There is a core portion, however, which is roughly 1.3M words which has been annotated with all the layers.</S>
    <S sid="49" ssid="10">It comprises &#8212;450k words from newswire, &#8212;150k from magazine articles, &#8212;200k from broadcast news, &#8212;200k from broadcast conversations and &#8212;200k web data.</S>
    <S sid="50" ssid="11">OntoNotes comprises the following layers of annotation:</S>
  </SECTION>
  <SECTION title="3 Coreference in OntoNotes" number="3">
    <S sid="51" ssid="1">General anaphoric coreference that spans a rich set of entities and events &#8211; not restricted to a few types, as has been characteristic of most coreference data available until now &#8211; has been tagged with a high degree of consistency.</S>
    <S sid="52" ssid="2">Attributive coreference is tagged separately from the more common identity coreference.</S>
    <S sid="53" ssid="3">Two different types of coreference are distinguished in the OntoNotes data: Identical (IDENT), and Appositive (APPOS).</S>
    <S sid="54" ssid="4">Appositives are treated separately because they function as attributions, as described further below.</S>
    <S sid="55" ssid="5">The IDENT type is used for anaphoric coreference, meaning links between pronominal, nominal, and named mentions of specific referents.</S>
    <S sid="56" ssid="6">It does not include mentions of generic, underspecified, or abstract entities.</S>
    <S sid="57" ssid="7">Coreference is annotated for all specific entities and events.</S>
    <S sid="58" ssid="8">There is no limit on the semantic types of NP entities that can be considered for coreference, and in particular, coreference is not limited to ACE types.</S>
    <S sid="59" ssid="9">The mentions over which IDENT coreference applies are typically pronominal, named, or definite nominal.</S>
    <S sid="60" ssid="10">The annotation process begins by automatically extracting all of the NP mentions from the Penn Treebank, though the annotators can also add additional mentions when appropriate.</S>
    <S sid="61" ssid="11">In the following two examples (and later ones), the phrases notated in bold form the links of an IDENT chain.</S>
    <S sid="62" ssid="12">Verbs are added as single-word spans if they can be coreferenced with a noun phrase or with another verb.</S>
    <S sid="63" ssid="13">The intent is to annotate the VP, but we mark the single-word head for convenience.</S>
    <S sid="64" ssid="14">This includes morphologically related nominalizations (3) and noun phrases that refer to the same event, even if they are lexically distinct from the verb (4).</S>
    <S sid="65" ssid="15">In the following two examples, only the chains related to the growth event are shown.</S>
    <S sid="66" ssid="16">All pronouns and demonstratives are linked to anything that they refer to, and pronouns in quoted speech are also marked.</S>
    <S sid="67" ssid="17">Expletive or pleonastic pronouns (it, there) are not considered for tagging, and generic you is not marked.</S>
    <S sid="68" ssid="18">In the following example, the pronoun you and it would not be marked.</S>
    <S sid="69" ssid="19">(In this and following examples, an asterisk (*) before a boldface phrase identifies entity/event mentions that would not be tagged as coreferent.)</S>
    <S sid="70" ssid="20">Generic nominal mentions can be linked with referring pronouns and other definite mentions, but are not linked to other generic nominal mentions.</S>
    <S sid="71" ssid="21">This would allow linking of the bracketed mentions in (6) and (7), but not (8). sell the PhacoFlex intraocular lens, the first foldable silicone lens available for *cataract surgery.</S>
    <S sid="72" ssid="22">The lens&#8217; foldability enables it to be inserted in smaller incisions than are now possible for *cataract surgery.</S>
    <S sid="73" ssid="23">Bare plurals, as in (6) and (7), are always considered generic.</S>
    <S sid="74" ssid="24">In example (9) below, there are two generic instances of parents.</S>
    <S sid="75" ssid="25">These are marked as distinct IDENT chains (with separate chains distinguished by subscripts X, Y and Z), each containing a generic and the related referring pronouns.</S>
    <S sid="76" ssid="26">(9) ParentsX should be involved with theirX children&#8217;s education at home, not in school.</S>
    <S sid="77" ssid="27">TheyX should see to it that theirX kids don&#8217;t play truant; theyX should make certain that the children spend enough time doing homework; theyX should scrutinize the report card.</S>
    <S sid="78" ssid="28">ParentsY are too likely to blame schools for the educational limitations of theirY children.</S>
    <S sid="79" ssid="29">If parentsZ are dissatisfied with a school, theyZ should have the option of switching to another.</S>
    <S sid="80" ssid="30">In (10) below, the verb &#8220;halve&#8221; cannot be linked to &#8220;a reduction of 50%&#8221;, since &#8220;a reduction&#8221; is indefinite.</S>
    <S sid="81" ssid="31">(10) Argentina said it will ask creditor banks to *halve its foreign debt of $64 billion &#8211; the third-highest in the developing world .</S>
    <S sid="82" ssid="32">Argentina aspires to reach *a reduction of 50% in the value of its external debt.</S>
    <S sid="83" ssid="33">Proper pre-modifiers can be coreferenced, but proper nouns that are in a morphologically adjectival form are treated as adjectives, and not coreferenced.</S>
    <S sid="84" ssid="34">For example, adjectival forms of GPEs such as Chinese in &#8220;the Chinese leader&#8221;, would not be linked.</S>
    <S sid="85" ssid="35">Thus we could coreference United States in &#8220;the United States policy&#8221; with another referent, but not American &#8220;the American policy.&#8221; GPEs and Nationality acronyms (e.g.</S>
    <S sid="86" ssid="36">U.S.S.R. or U.S.). are also considered adjectival.</S>
    <S sid="87" ssid="37">Pre-modifier acronyms can be coreferenced unless they refer to a nationality.</S>
    <S sid="88" ssid="38">Thus in the examples below, FBI can be coreferenced to other mentions, but U.S. cannot.</S>
    <S sid="89" ssid="39">Dates and monetary amounts can be considered part of a coreference chain even when they occur as pre-modifiers.</S>
    <S sid="90" ssid="40">(13) The current account deficit on France&#8217;s balance of payments narrowed to 1.48 billion French francs ($236.8 million) in August from a revised 2.1 billion francs in July, the Finance Ministry said.</S>
    <S sid="91" ssid="41">Previously, the July figure was estimated at a deficit of 613 million francs.</S>
    <S sid="92" ssid="42">(14) The company&#8217;s $150 offer was unexpected.</S>
    <S sid="93" ssid="43">The firm balked at the price.</S>
    <S sid="94" ssid="44">Attributes signaled by copular structures are not marked; these are attributes of the referent they modify, and their relationship to that referent will be captured through word sense and propositional argument tagging.</S>
    <S sid="95" ssid="45">Copular (or &#8217;linking&#8217;) verbs are those verbs that function as a copula and are followed by a subject complement.</S>
    <S sid="96" ssid="46">Some common copular verbs are: be, appear, feel, look, seem, remain, stay, become, end up, get.</S>
    <S sid="97" ssid="47">Subject complements following such verbs are considered attributes, and not linked.</S>
    <S sid="98" ssid="48">Since Called is copular, neither IDENT nor APPOS coreference is marked in the following case.</S>
    <S sid="99" ssid="49">Like copulas, small clause constructions are not marked.</S>
    <S sid="100" ssid="50">The following example is treated as if the copula were present (&#8220;John considers Fred to be an idiot&#8221;): Deictic expressions such as now, then, today, tomorrow, yesterday, etc. can be linked, as well as other temporal expressions that are relative to the time of the writing of the article, and which may therefore require knowledge of the time of the writing to resolve the coreference.</S>
    <S sid="101" ssid="51">Annotators were allowed to use knowledge from outside the text in resolving these cases.</S>
    <S sid="102" ssid="52">In the following example, the end of this period and that time can be coreferenced, as can this period and from three years to seven years.</S>
    <S sid="103" ssid="53">(19) The limit could range from three years to seven yearsX, depending on the composition of the management team and the nature of its strategic plan.</S>
    <S sid="104" ssid="54">At (the end of (this period)X)Y, the poison pill would be eliminated automatically, unless a new poison pill were approved by the then-current shareholders, who would have an opportunity to evaluate the corporation&#8217;s strategy and management team at that timeY.</S>
    <S sid="105" ssid="55">In multi-date temporal expressions, embedded dates are not separately connected to to other mentions of that date.</S>
    <S sid="106" ssid="56">For example in Nov. 2, 1999, Nov. would not be linked to another instance of November later in the text.</S>
    <S sid="107" ssid="57">Because they logically represent attributions, appositives are tagged separately from Identity coreference.</S>
    <S sid="108" ssid="58">They consist of a head, or referent (a noun phrase that points to a specific object/concept in the world), and one or more attributes of that referent.</S>
    <S sid="109" ssid="59">An appositive construction contains a noun phrase that modifies an immediately-adjacent noun phrase (separated only by a comma, colon, dash, or parenthesis).</S>
    <S sid="110" ssid="60">It often serves to rename or further define the first mention.</S>
    <S sid="111" ssid="61">Marking appositive constructions allows us to capture the attributed property even though there is no explicit copula.</S>
    <S sid="112" ssid="62">The head of each appositive construction is distinguished from the attribute according to the following heuristic specificity scale, in a decreasing order from top to bottom: This leads to the following cases: In cases where the two members of the appositive are equivalent in specificity, the left-most member of the appositive is marked as the head/referent.</S>
    <S sid="113" ssid="63">Definite NPs include NPs with a definite marker (the) as well as NPs with a possessive adjective (his).</S>
    <S sid="114" ssid="64">Thus the first element is the head in all of the following cases: In the specificity scale, specific names of diseases and technologies are classified as proper names, whether they are capitalized or not.</S>
    <S sid="115" ssid="65">When the entity to which an appositive refers is also mentioned elsewhere, only the single span containing the entire appositive construction is included in the larger IDENT chain.</S>
    <S sid="116" ssid="66">None of the nested NP spans are linked.</S>
    <S sid="117" ssid="67">In the example below, the entire span can be linked to later mentions to Richard Godown.</S>
    <S sid="118" ssid="68">The sub-spans are not included separately in the IDENT chain.</S>
    <S sid="119" ssid="69">In addition to the ones above, there are some special cases such as: &#8226; GPEs are linked to references to their governments, even when the references are nested NPs, or the modifier and head of a single NP.</S>
    <S sid="120" ssid="70">Table 1 shows the inter-annotator and annotatoradjudicator agreement on all the genres of OntoNotes.</S>
    <S sid="121" ssid="71">We also analyzed about 15K disagreements in various parts of the data, and grouped them into one of the categories shown in Figure 1.</S>
    <S sid="122" ssid="72">Figure 2 shows the distribution of these different types that were found in that sample.</S>
    <S sid="123" ssid="73">It can be seen that genuine ambiguity and annotator error are the biggest contributors &#8211; the latter of which is usually captured during adjudication, thus showing the increased agreement between the adjudicated version and the individual annotator version.</S>
  </SECTION>
  <SECTION title="4 CoNLL-2011 Coreference Task" number="4">
    <S sid="124" ssid="1">This section describes the CoNLL-2011 Coreference task, including its closed and open track versions, and characterizes the data used for the task and how it was prepared.</S>
    <S sid="125" ssid="2">Despite close to a two-decade history of evaluations on coreference tasks, variation in the evaluation criteria and in the training data used have made it difficult for researchers to be clear about the state of the art or to determine which particular areas require further attention.</S>
    <S sid="126" ssid="3">There are many different parameters involved in defining a coreference task.</S>
    <S sid="127" ssid="4">Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task.</S>
    <S sid="128" ssid="5">It can seem to be a very hard problem (Soon et al., 2001) or one that is somewhat easier (Culotta et al., 2007).</S>
    <S sid="129" ssid="6">Given the space constraints, we refer the reader to Stoyanov et al. (2009) for a detailed treatment of the issue.</S>
    <S sid="130" ssid="7">Limitations in the size and scope of the available datasets have also constrained research progress.</S>
    <S sid="131" ssid="8">The MUC and ACE corpora are the two that have been used most for reporting comparative results, but they differ in the types of entities and coreference annotated.</S>
    <S sid="132" ssid="9">The ACE corpus is also one that evolved over a period of almost five years, with different incarnations of the task definition and different corpus cross-sections on which performance numbers have been reported, making it hard to untangle and interpret the results.</S>
    <S sid="133" ssid="10">The availability of the OntoNotes data offered an opportunity to define a coreference task based on a larger, more broad-coverage corpus.</S>
    <S sid="134" ssid="11">We have tried to design the task so that it not only can support the current evaluation, but also can provide an ongoing resource for comparing different coreference algorithms and approaches.</S>
    <S sid="135" ssid="12">The CoNLL-2011 shared task was based on the English portion of the OntoNotes 4.0 data.</S>
    <S sid="136" ssid="13">The task was to automatically identify mentions of entities and events in text and to link the coreferring mentions together to form entity/event chains.</S>
    <S sid="137" ssid="14">The target coreference decisions could be made using automatically predicted information on the other structural layers including the parses, semantic roles, word senses, and named entities.</S>
    <S sid="138" ssid="15">As is customary for CoNLL tasks, there were two tracks, closed and open.</S>
    <S sid="139" ssid="16">For the closed track, systems were limited to using the distributed resources, in order to allow a fair comparison of algorithm performance, while the open track allowed for almost unrestricted use of external resources in addition to the provided data.</S>
    <S sid="140" ssid="17">In the closed track, systems were limited to the provided data, plus the use of two pre-specified external resources: i) WordNet and ii) a pre-computed number and gender table by Bergsma and Lin (2006).</S>
    <S sid="141" ssid="18">For the training and test data, in addition to the underlying text, predicted versions of all the supplementary layers of annotation were provided, where those predictions were derived using off-the-shelf tools (parsers, semantic role labelers, named entity taggers, etc.) as described in Section 4.4.2.</S>
    <S sid="142" ssid="19">For the training data, however, in addition to predicted values for the other layers, we also provided manual gold-standard annotations for all the layers.</S>
    <S sid="143" ssid="20">Participants were allowed to use either the gold-standard or predicted annotation for training their systems.</S>
    <S sid="144" ssid="21">They were also free to use the gold-standard data to train their own models for the various layers of annotation, if they judged that those would either provide more accurate predictions or alternative predictions for use as multiple views, or wished to use a lattice of predictions.</S>
    <S sid="145" ssid="22">More so than previous CoNLL tasks, coreference predictions depend on world knowledge, and many state-of-the-art systems use information from external resources such as WordNet, which can add a layer that helps the system to recognize semantic connections between the various lexicalized mentions in the text.</S>
    <S sid="146" ssid="23">Therefore, the use of WordNet was allowed, even for the closed track.</S>
    <S sid="147" ssid="24">Since word senses in OntoNotes are predominantly3 coarse-grained groupings of WordNet senses, systems could also map from the predicted or goldstandard word senses provided to the sets of underlying WordNet senses.</S>
    <S sid="148" ssid="25">Another significant piece of knowledge that is particularly useful for coreference but that is not available in the layers of OntoNotes is that of number and gender.</S>
    <S sid="149" ssid="26">There are many different ways of predicting these values, with differing accuracies, so in order to ensure that participants in the closed track were working from the same data, thus allowing clearer algorithmic comparisons, we specified a particular table of number and gender predictions generated by Bergsma and Lin (2006), for use during both training and testing.</S>
    <S sid="150" ssid="27">Following the recent CoNLL tradition, participants were allowed to use both the training and the development data for training the final model.</S>
    <S sid="151" ssid="28">In addition to resources available in the closed track, the open track, systems were allowed to use external resources such as Wikipedia, gazetteers etc.</S>
    <S sid="152" ssid="29">This track is mainly to get an idea of a performance ceiling on the task at the cost of not getting a comparison across all systems.</S>
    <S sid="153" ssid="30">Another advantage of the open track is that it might reduce the barriers to participation by allowing participants to field existing research systems that already depend on external resources &#8211; especially if there were hard dependencies on these resources.</S>
    <S sid="154" ssid="31">They can participate in the task with minimal or no modification to their existing system.</S>
    <S sid="155" ssid="32">Since there are no previously reported numbers on the full version of OntoNotes, we had to create a train/development/test partition.</S>
    <S sid="156" ssid="33">The only portion of OntoNotes that has a previously determined, widely used, standard split is the WSJ portion of the newswire data.</S>
    <S sid="157" ssid="34">For that subcorpus, we maintained the same partition.</S>
    <S sid="158" ssid="35">For all the other portions we created stratified training, development and test partitions over all the sources in OntoNotes using the procedure shown in Algorithm 1.</S>
    <S sid="159" ssid="36">The list of training, development and test document IDs can be found on the task webpage.4 This section gives details of the different annotation layers including the automatic models that were used to predict them, and describes the formats in which the data were provided to the participants.</S>
    <S sid="160" ssid="37">We will take a look at the manually annotated, or gold layers of information that were made available for the training data.</S>
    <S sid="161" ssid="38">Coreference The manual coreference annotation is stored as chains of linked mentions connecting multiple mentions of the same entity.</S>
    <S sid="162" ssid="39">Coreference is the only document-level phenomenon in OntoNotes, and the complexity of annotation increases nonlinearly with the length of a document.</S>
    <S sid="163" ssid="40">Unfortunately, some of the documents &#8211; especially ones in the broadcast conversation, weblogs, and telephone conversation genre &#8211; are very long which prohibited us from efficiently annotating them in entirety.</S>
    <S sid="164" ssid="41">These had to be split into smaller parts.</S>
    <S sid="165" ssid="42">We conducted a few passes to join some adjacent parts, but since some documents had as many as 17 parts, there are still multi-part documents in the corpus.</S>
    <S sid="166" ssid="43">Since the coreference chains are coherent only within each of these document parts, for this task, each such part is treated as a separate document.</S>
    <S sid="167" ssid="44">Another thing to note is that there were some cases of sub-token annotation in the corpus owing to the fact that tokens were not split at hyphens.</S>
    <S sid="168" ssid="45">Cases such as proWalMart had the sub-span WalMart linked with another instance of the same.</S>
    <S sid="169" ssid="46">The recent Treebank revision which split tokens at most hyphens, made a majority of these sub-token annotations go away.</S>
    <S sid="170" ssid="47">There were still some residual sub-token annotations.</S>
    <S sid="171" ssid="48">Since subtoken annotations cannot be represented in the CoNLL format, and they were a very small quantity &#8211; much less than even half a percent &#8211; we decided to ignore them.</S>
    <S sid="172" ssid="49">For various reasons, not all the documents in OntoNotes have been annotated with all the different layers of annotation, with full coverage.6 There is a core portion, however, which is roughly 1.3M words which has been annotated with all the layers.</S>
    <S sid="173" ssid="50">This is the portion that we used for the shared task.</S>
    <S sid="174" ssid="51">The number of documents in the corpus for this task, for each of the different genres, are shown in Table 2.</S>
    <S sid="175" ssid="52">Tables 3 and 4 shows the distribution of mentions by the syntactic categories, and the counts of entities, links and mentions in the corpus respectively.</S>
    <S sid="176" ssid="53">All of this data has been Treebanked and PropBanked either as part of the OntoNotes effort or some preceding effort.</S>
    <S sid="177" ssid="54">For comparison purposes, Table 2 also lists the number of documents in the MUC-6, MUC-7, and ACE (2000-2004) corpora.</S>
    <S sid="178" ssid="55">The MUC-6 data was taken from the Wall Street Journal, whereas the MUC-7 data was from the New York Times.</S>
    <S sid="179" ssid="56">The ACE data spanned many different genres similar to the ones in OntoNotes.</S>
    <S sid="180" ssid="57">Parse Trees This represents the syntactic layer that is a revised version of the Penn Treebank.</S>
    <S sid="181" ssid="58">For purposes of this task, traces were removed from the syntactic trees, since the CoNLL-style data format, being indexed by tokens, does not provide any good means of conveying that information.</S>
    <S sid="182" ssid="59">Function tags were also removed, since the parsers that we used for the predicted syntax layer did not provide them.</S>
    <S sid="183" ssid="60">One thing that needs to be dealt with in conversational data is the presence of disfluencies (restarts, etc.).</S>
    <S sid="184" ssid="61">In the original OntoNotes parses, these are marked using a special EDITED7 phrase tag &#8211; as was the case for the Switchboard Treebank.</S>
    <S sid="185" ssid="62">Given the frequency of disfluencies and the performance with which one can identify them automatically,8 a probable processing pipeline would filter them out before parsing.</S>
    <S sid="186" ssid="63">Since we did not have a readily available tagger for tagging disfluencies, we decided to remove them using oracle information available in the Treebank.</S>
    <S sid="187" ssid="64">Propositions The propositions in OntoNotes constitute PropBank semantic roles.</S>
    <S sid="188" ssid="65">Most of the verb predicates in the corpus have been annotated with their arguments.</S>
    <S sid="189" ssid="66">Recent enhancements to the PropBank to make it synchronize better with the Treebank (Babko-Malaya et al., 2006) have enhanced the information in the proposition by the addition of two types of LINKs that represent pragmatic coreference (LINK-PCR) and selectional preferences (LINKSLC).</S>
    <S sid="190" ssid="67">More details can be found in the addendum to the PropBank guidelines9 in the OntoNotes 4.0 re7There is another phrase type &#8211; EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases, so we decided not to remove that from the data. lease.</S>
    <S sid="191" ssid="68">Since the community is not used to this representation which relies heavily on the trace structure in the Treebank which we are excluding, we decided to unfold the LINKs back to their original representation as in the Release 1.0 of the Proposition Bank.</S>
    <S sid="192" ssid="69">This functionality is part of the OntoNotes DB Tool.10 Word Sense Gold word sense annotation was supplied using sense numbers as specified in the OntoNotes list of senses for each lemma.11 The sense inventories that were provided in the OntoNotes 4.0 release were not all mapped to the latest version 3.0 of WordNet, so we provided a revised version of the sense inventories, containing mapping to WordNet 3.0, on the task page for the participants.</S>
    <S sid="193" ssid="70">Named Entities Named Entities in OntoNotes data are specified using a catalog of 18 Name types.</S>
    <S sid="194" ssid="71">Other Layers Discourse plays a vital role in coreference resolution.</S>
    <S sid="195" ssid="72">In the case of broadcast conversation, or telephone conversation data, it partially manifests in the form of speakers of a given utterance, whereas in weblogs or newsgroups it does so as the writer, or commenter of a particular article or thread.</S>
    <S sid="196" ssid="73">This information provides an important clue for correctly linking anaphoric pronouns with the right antecedents.</S>
    <S sid="197" ssid="74">This information could be automatically deduced, but since it would add additional complexity to the already complex task, we decided to provide oracle information of this metadata both during training and testing.</S>
    <S sid="198" ssid="75">In other words, speaker and author identification was not treated as an annotation layer that needed to be predicted.</S>
    <S sid="199" ssid="76">This information was provided in the form of another column in the .conll table.</S>
    <S sid="200" ssid="77">There were some cases of interruptions and interjections that ideally would associate parts of a sentence to two different speakers, but since the frequency of this was quite small, we decided to make an assumption of one speaker/writer per sentence.</S>
    <S sid="201" ssid="78">The predicted annotation layers were derived using automatic models trained using cross-validation on other portions of OntoNotes data.</S>
    <S sid="202" ssid="79">As mentioned earlier, there are some portions of the OntoNotes corpus that have not been annotated for coreference but that have been annotated for other layers.</S>
    <S sid="203" ssid="80">For training models for each of the layers, where feasible, we used all the data that we could for that layer from the training portion of the entire OntoNotes release.</S>
    <S sid="204" ssid="81">Parse Trees Predicted parse trees were produced using the Charniak parser (Charniak and Johnson, 2005).12 Some additional tag types used in the OntoNotes trees were added to the parser&#8217;s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were appropriately extended.</S>
    <S sid="205" ssid="82">The parser was then re-trained on the training portion of the release 4.0 data using 10-fold crossvalidation.</S>
    <S sid="206" ssid="83">Table 5 shows the performance of the re-trained Charniak parser on the CoNLL-2011 test set.</S>
    <S sid="207" ssid="84">We did not get a chance to re-train the re-ranker, and since the stock re-ranker crashes when run on nbest parses containing NMLs, because it has not seen that tag in training, we could not make use of it.</S>
    <S sid="208" ssid="85">Word Sense We trained a word sense tagger using a SVM classifier and contextual word and part of speech features on all the training portion of the OntoNotes data.</S>
    <S sid="209" ssid="86">The OntoNotes 4.0 corpus comprises a total of 14,662 sense definitions across 4877 verb and noun lemmas13.</S>
    <S sid="210" ssid="87">The distribution of senses per lemma is as shown in Table 6.</S>
    <S sid="211" ssid="88">Table 7 shows the performance of this classifier over both the verbs and nouns in the CoNLL-2011 test set.</S>
    <S sid="212" ssid="89">Again this performance is not directly comparable to any reported in the literature before, and it seems lower then performances reported on previous versions of OntoNotes because this is over all the genres of OntoNotes, and aggregated over both verbs and nouns in the CoNLL-2011 test set.</S>
    <S sid="213" ssid="90">Propositions To predict propositional structure, ASSERT14 (Pradhan et al., 2005) was used, retrained also on all the training portion of the release 12http://bllip.cs.brown.edu/download/rerankingparserAug06.tar.gz 13The number of lemmas in Table 6 do not add up to this number because not all of them have examples in the training data, where the total number of instantiated senses amounts to 7933.</S>
    <S sid="214" ssid="91">4.0 data.</S>
    <S sid="215" ssid="92">Given time constraints, we had to perform two modifications: i) Instead of a single model that predicts all arguments including NULL arguments, we had to use the two-stage mode where the NULL arguments are first filtered out and the remaining NON-NULL arguments are classified into one of the argument types, and ii) The argument identification module used an ensemble of ten classifiers &#8211; each trained on a tenth of the training data and performed an unweighted voting among them.</S>
    <S sid="216" ssid="93">This should still give a close to state of the art performance given that the argument identification performance tends to start to be asymptotic around 10k training instances.</S>
    <S sid="217" ssid="94">At first glance, the performance on the newswire genre is much lower than what has been reported for WSJ Section 23.</S>
    <S sid="218" ssid="95">This could be attributed to two factors: i) the fact that we had to compromise on the training method, but more importantly because ii) the newswire in OntoNotes not only contains WSJ data, but also Xinhua news.</S>
    <S sid="219" ssid="96">One could try to verify using just the WSJ portion of the data, but it would be hard as it is not only a subset of the documents that the performance has been reported on previously, but also the annotation has been significantly revised; it includes propositions for be verbs missing from the original PropBank, and the training data is a subset of the original data as well.</S>
    <S sid="220" ssid="97">Table 8 shows the detailed performance numbers.</S>
    <S sid="221" ssid="98">In addition to automatically predicting the arguments, we also trained a classifier to tag PropBank frameset IDs in the data using the same word sense module as mentioned earlier.</S>
    <S sid="222" ssid="99">OntoNotes 4.0 contains a total of 7337 framesets across 5433 verb lemmas.15 An overwhelming number of them are monosemous, but the more frequent verbs tend to be polysemous.</S>
    <S sid="223" ssid="100">Table 9 gives the distribution of number of framesets per lemma in the PropBank layer of the OntoNotes 4.0 data.</S>
    <S sid="224" ssid="101">During automatic processing of the data, we tagged all the tokens that were tagged with a part of speech VBx.</S>
    <S sid="225" ssid="102">This means that there would be cases where the wrong token would be tagged with propositions.</S>
    <S sid="226" ssid="103">The CoNLL-2005 scorer was used to generate the scores.</S>
    <S sid="227" ssid="104">Named Entities BBN&#8217;s IdentiFinderTMsystem was used to predict the named entities.</S>
    <S sid="228" ssid="105">Given the 15The number of lemmas in Table 9 do not add up to this number because not all of them have examples in the training data, where the total number of instantiated senses amounts to 4229. time constraints, we could not re-train it on the OntoNotes data and so an existing, pre-trained model was used, therefore the results are not a good indicator of the model&#8217;s best performance.</S>
    <S sid="229" ssid="106">The pre-trained model had also used a somewhat different catalog of name types, which did not include the OntoNotes NORP type (for nationalities, organizations, religions, and political parties), so that category was never predicted.</S>
    <S sid="230" ssid="107">Table 10 shows the overall performance of the tagger on the CoNLL-2011 test set, as well as the performance broken down by individual name types.</S>
    <S sid="231" ssid="108">IdentiFinder performance has been reported to be in the low 90&#8217;s on WSJ test set.</S>
    <S sid="232" ssid="109">Other Layers As noted above, systems were allowed to make use of gender and number predictions for NPs using the table from Bergsma and Lin (Bergsma and Lin, 2006).</S>
    <S sid="233" ssid="110">In order to organize the multiple, rich layers of annotation, the OntoNotes project has created a database representation for the raw annotation layers along with a Python API to manipulate them (Pradhan et al., 2007a).</S>
    <S sid="234" ssid="111">In the OntoNotes distribution the data is organized as one file per layer, per document.</S>
    <S sid="235" ssid="112">The API requires a certain hierarchical structure with documents at the leaves inside a hierarchy of language, genre, source and section.</S>
    <S sid="236" ssid="113">It comes with various ways of cleanly querying and manipulating the data and allows convenient access to the sense inventory and propbank frame files instead of having to interpret the raw .xml versions.</S>
    <S sid="237" ssid="114">However, maintaining format consistency with earlier CoNLL tasks was deemed convenient for sites that already had tools configured to deal with that format.</S>
    <S sid="238" ssid="115">Therefore, in order to distribute the data so that one could make the best of both worlds, we created a new file type called .conll which logically served as another layer in addition to the .parse, .prop, .name and .coref layers.</S>
    <S sid="239" ssid="116">Each .conll file contained a merged representation of all the OntoNotes layers in the CoNLLstyle tabular format with one line per token, and with multiple columns for each token specifying the input annotation layers relevant to that token, with the final column specifying the target coreference layer.</S>
    <S sid="240" ssid="117">Because OntoNotes is not authorized to distribute the underlying text, and many of the layers contain inline annotation, we had to provide a skeletal form (.skel of the .conll file which was essentially the .conll file, but with the word column replaced with a dummy string.</S>
    <S sid="241" ssid="118">We provided an assembly script that participants could use to create a .conll file taking as input the .skel file and the top-level directory of the OntoNotes distribution that they had separately downloaded from the LDC16 Once the .conll file is created, it can be used to create the individual layers such as .parse, .name, .coref etc. using another set of scripts.</S>
    <S sid="242" ssid="119">Since the propositions and word sense layers are inherently standoff annotation, they were provided as is, and did not require that extra merging step.</S>
    <S sid="243" ssid="120">One thing thing that made this data creation process a bit tricky was the fact that we had dissected some of the trees for the conversation data to remove the EDITED phrases.</S>
    <S sid="244" ssid="121">Table 11 describes the data provided in each of the column of the .conll format.</S>
    <S sid="245" ssid="122">Figure 3 shows a sample from a .conll file.</S>
    <S sid="246" ssid="123">This section describes the evaluation criteria used.</S>
    <S sid="247" ssid="124">Unlike for propositions, word sense and named entities, where it is simply a matter of counting the correct answers, or for parsing, where there are several established metrics, evaluating the accuracy of coreference continues to be contentious.</S>
    <S sid="248" ssid="125">Various al16OntoNotes is deeply grateful to the Linguistic Data Consortium for making the source data freely available to the task participants. ternative metrics have been proposed, as mentioned below, which weight different features of a proposed coreference pattern differently.</S>
    <S sid="249" ssid="126">The choice is not clear in part because the value of a particular set of coreference predictions is integrally tied to the consuming application.</S>
    <S sid="250" ssid="127">A further issue in defining a coreference metric concerns the granularity of the mentions, and how closely the predicted mentions are required to match those in the gold standard for a coreference prediction to be counted as correct.</S>
    <S sid="251" ssid="128">Our evaluation criterion was in part driven by the OntoNotes data structures.</S>
    <S sid="252" ssid="129">OntoNotes coreference distinguishes between identity coreference and appositive coreference, treating the latter separately because it is already captured explicitly by other layers of the OntoNotes annotation.</S>
    <S sid="253" ssid="130">Thus we evaluated systems only on the identity coreference task, which links all categories of entities and events together into equivalent classes.</S>
    <S sid="254" ssid="131">The situation with mentions for OntoNotes is also different than it was for MUC or ACE.</S>
    <S sid="255" ssid="132">OntoNotes data does not explicitly identify the minimum extents of an entity mention, but it does include handtagged syntactic parses.</S>
    <S sid="256" ssid="133">Thus for the official evaluation, we decided to use the exact spans of mentions for determining correctness.</S>
    <S sid="257" ssid="134">The NP boundaries for the test data were pre-extracted from the handtagged Treebank for annotation, and events triggered by verb phrases were tagged using the verbs themselves.</S>
    <S sid="258" ssid="135">This choice means that scores for the CoNLL-2011 coreference task are likely to be lower than for coref evaluations based on MUC, where the mention spans are specified in the input,17 or those based on ACE data, where an approximate match is often allowed based on the specified head of the NP mention.</S>
    <S sid="259" ssid="136">As noted above, the choice of an evaluation metric for coreference has been a tricky issue and there does not appear to be any silver bullet approach that addresses all the concerns.</S>
    <S sid="260" ssid="137">Three metrics have been proposed for evaluating coreference performance over an unrestricted set of entity types: i) The link based MUC metric (Vilain et al., 1995), ii) The mention based B-CUBED metric (Bagga and Baldwin, 1998) and iii) The entity based CEAF (Constrained Entity Aligned F-measure) metric (Luo, 2005).</S>
    <S sid="261" ssid="138">Very recently BLANC (BiLateral Assessment of NounPhrase Coreference) measure (Recasens and Hovy, 17as is the case in this evaluation with Gold Mentions 2011) has been proposed as well.</S>
    <S sid="262" ssid="139">Each of the metric tries to address the shortcomings or biases of the earlier metrics.</S>
    <S sid="263" ssid="140">Given a set of key entities K, and a set of response entities R, with each entity comprising one or more mentions, each metric generates its variation of a precision and recall measure.</S>
    <S sid="264" ssid="141">The MUC measure if the oldest and most widely used.</S>
    <S sid="265" ssid="142">It focuses on the links (or, pairs of mentions) in the data.18 The number of common links between entities in K and R divided by the number of links in K represents the recall, whereas, precision is the number of common links between entities in K and R divided by the number of links in R. This metric prefers systems that have more mentions per entity &#8211; a system that creates a single entity of all the mentions will get a 100% recall without significant degradation in its precision.</S>
    <S sid="266" ssid="143">And, it ignores recall for singleton entities, or entities with only one mention.</S>
    <S sid="267" ssid="144">The B-CUBED metric tries to addresses MUCS&#8217;s shortcomings, by focusing on the mentions and computes recall and precision scores for each mention.</S>
    <S sid="268" ssid="145">If K is the key entity containing mention M, and R is the response entity containing mention M, then recall for the mention M is computed as |K&#8745;R| |K| and precision for the same is is computed as |K&#8745;R| |R|.</S>
    <S sid="269" ssid="146">Overall recall and precision are the average of the individual mention scores.</S>
    <S sid="270" ssid="147">CEAF aligns every response entity with at most one key entity by finding the best one-to-one mapping between the entities using an entity similarity metric.</S>
    <S sid="271" ssid="148">This is a maximum bipartite matching problem and can be solved by the Kuhn-Munkres algorithm.</S>
    <S sid="272" ssid="149">This is thus a entity based measure.</S>
    <S sid="273" ssid="150">Depending on the similarity, there are two variations &#8211; entity based CEAF &#8211; CEAF, and a mention based CEAF &#8211; CEAF,.</S>
    <S sid="274" ssid="151">Recall is the total similarity divided by the number of mentions in K, and precision is the total similarity divided by the number of mentions in R. Finally, BLANC uses a variation on the Rand index (Rand, 1971) suitable for evaluating coreference.</S>
    <S sid="275" ssid="152">There are a few other measures &#8211; one being the ACE value, but since this is specific to a restricted set of entities (ACE types), we did not consider it.</S>
    <S sid="276" ssid="153">In order to determine the best performing system in the shared task, we needed to associate a single number with each system.</S>
    <S sid="277" ssid="154">This could have been one of the metrics above, or some combination of more than one of them.</S>
    <S sid="278" ssid="155">The choice was not simple, and while we consulted various researchers in the field, hoping for a strong consensus, their conclusion seemed to be that each metric had its pros and cons.</S>
    <S sid="279" ssid="156">We settled on the MELA metric by Denis and Baldridge (2009), which takes a weighted average of three metrics: MUC, B-CUBED, and CEAF.</S>
    <S sid="280" ssid="157">The rationale for the combination is that each of the three metrics represents a different important dimension, the MUC measure being based on links, the B-CUBED based on mentions, and the CEAF based on entities.</S>
    <S sid="281" ssid="158">For a given task, a weighted average of the three might be optimal, but since we don&#8217;t have an end task in mind, we decided to use the unweighted mean of the three metrics as the score on which the winning system was judged.</S>
    <S sid="282" ssid="159">We decided to use CEAF, instead of CEAFm.</S>
    <S sid="283" ssid="160">We used the same core scorer implementation19 that was used for the SEMEVAL-2010 task, and which implemented all the different metrics.</S>
    <S sid="284" ssid="161">There were a couple of modifications done to this scorer after it was used for the SEMEVAL-2010 task.</S>
    <S sid="285" ssid="162">Since there are differences in the version used for CoNLL and the one available on the download site, and it is possible that the latter would be revised in the future, we have archived the version of the scorer on the CoNLL-2011 task webpage.20</S>
  </SECTION>
  <SECTION title="5 Systems and Results" number="5">
    <S sid="286" ssid="1">About 65 different groups demonstrated interest in the shared task by registering on the task webpage.</S>
    <S sid="287" ssid="2">Of these, 23 groups submitted system outputs on the test set during the evaluation week.</S>
    <S sid="288" ssid="3">18 groups submitted only closed track results, 3 groups only open track results, and 2 groups submitted both closed and open track results.</S>
    <S sid="289" ssid="4">2 participants in the closed track, did not write system papers, so we don&#8217;t use their results in the discussion.</S>
    <S sid="290" ssid="5">Their results will be reported on the task webpage.</S>
    <S sid="291" ssid="6">The official results for the 18 systems that submitted closed track outputs are shown in Table 12, with those for the 5 systems that submitted open track results in Table 13.</S>
    <S sid="292" ssid="7">The official ranking score, the arithmetic mean of the F-scores of MUC, B-CUBED and CEAF,, is shown in the rightmost column.</S>
    <S sid="293" ssid="8">For convenience, systems will be referred to here using the first portion of the full name, which is unique within each table.</S>
    <S sid="294" ssid="9">For completeness, the tables include the raw precision and recall scores from which the F-scores were derived.</S>
    <S sid="295" ssid="10">The tables also include two additional scores (BLANC and CEAFm) that did not factor into the official ranking score.</S>
    <S sid="296" ssid="11">Useful further analysis may be possible based on these results beyond the preliminary results presented here.</S>
    <S sid="297" ssid="12">As discussed previously in the task description, we will consider three different test input conditions: i) Predicted only (Official), ii) Predicted plus gold mention boundaries, and iii) Predicted plus gold mentions For the official test, beyond the raw source text, coreference systems were provided only with the predictions from automatic engines as to the other annotation layers (parses, semantic roles, word senses, and named entities).</S>
    <S sid="298" ssid="13">In this evaluation it is important to note that the mention detection score cannot be considered in isolation of the coreference task as has usually been the case.</S>
    <S sid="299" ssid="14">This is mainly owing to the fact that there are no singleton entities in the OntoNotes data.</S>
    <S sid="300" ssid="15">Most systems removed singletons from the response as a post-processing step, so not only will they not get credit for the singleton entities that they correctly removed from the data, but they will be penalized for the ones that they accidentally linked with another mention.</S>
    <S sid="301" ssid="16">What this number does indicate is the ceiling on recall that a system would have got in absence of being penalized for making mistakes in coreference resolution.</S>
    <S sid="302" ssid="17">A close look at the Table 12 indicates a possible outlier in case of the sapena system.</S>
    <S sid="303" ssid="18">The recall for this system is very high, and precision way lower than any other system.</S>
    <S sid="304" ssid="19">Further investigations uncovered that the reason for this aberrant behavior was that fact that this system opted to keep singletons in the response.</S>
    <S sid="305" ssid="20">By design, the scorer removes singletons that might be still present in the system, but it does so after the mention detection accuracy is computed.</S>
    <S sid="306" ssid="21">The official scores top out in the high 50&#8217;s.</S>
    <S sid="307" ssid="22">While this is lower than the figures cited in previous coreference evaluations, that is as expected, given that the task here includes predicting the underlying mentions and mention boundaries, the insistence on exact match, and given that the relatively easier appositive coreference cases are not included in this measure.</S>
    <S sid="308" ssid="23">The top-performing system (lee) had a score of 57.79 which is about 1.8 points higher than that of the second (sapena) and third (chang) ranking systems, which scored 55.99 and 55.96 respectively.</S>
    <S sid="309" ssid="24">Another 1.5 points separates them from the fourth best score of 54.53 (nugues).</S>
    <S sid="310" ssid="25">Thus the performance differences between the better-scoring systems were not large, with only about three points separating the top four systems.</S>
    <S sid="311" ssid="26">This becomes even clearer if we merge in the results of systems that participated only in the open track but that made relatively limited use of outside resources.21 Comparing that way, the cai system scores in the same ball park as the second rank systems (sapena and chang).</S>
    <S sid="312" ssid="27">The uryupina system similarly scores very close to nugues&#8217;s 54.53 Given that our choice of the official metric was somewhat arbitrary, if is also useful to look at the individual metrics, including the mention-based CEAFm and BLANC metrics that were not part of the official metric.</S>
    <S sid="313" ssid="28">The lee system which scored the best using the official metric does slightly worse than song on the MUC metric, and also does slightly worse than chang on the B-CUBED and BLANC metrics.</S>
    <S sid="314" ssid="29">However, it does much better than every other group on the entity-based CEAFe, and this is the primary reason for its 1.8 point advantage in the official score.</S>
    <S sid="315" ssid="30">If the CEAFe measure does indicate the accuracy of entities in the response, this suggests that the lee system is doing better on getting coherent entities than any other system.</S>
    <S sid="316" ssid="31">This could be partly due to the fact that that system is primarily a precision-based system that would tend to create purer entities.</S>
    <S sid="317" ssid="32">The CEAFe measure also seems to penalize other systems more harshly than do the other measures.</S>
    <S sid="318" ssid="33">We cannot compare these results to the ones obtained in the SEMEVAL-2010 coreference task using a small portion of OntoNotes data because it was only using nominal entities, and had heuristically added singleton mentions to the OntoNotes data22 21The cai system specifically mentions that, and the only resource that the uryupina system used outside of the closed track setting was the Stanford named entity tagger.</S>
    <S sid="319" ssid="34">22The documentation that comes with the SEMEVAL data package from LDC (LDC2011T01) states: &#8220;Only nominal mentions and identical (IDENT) types were taken from the OntoNotes coreference annotation, thus excluding coreference We also explored performance when the systems were provided with the gold mention boundaries, that is, with the exact spans (expressed in terms of token offsets) for all of the NP constituents in the human-annotated parse trees for the test data.</S>
    <S sid="320" ssid="35">Systems could use this additional data to ensure that the output mention spans in their entity chains would not clash with those in the answer set.</S>
    <S sid="321" ssid="36">Since this was a secondary evaluation, it was an optional element, and not all participants ran their systems on this task variation.</S>
    <S sid="322" ssid="37">The results for those systems that did participate in this optional task are shown in Tables 14 (closed track) and 15 (open track).</S>
    <S sid="323" ssid="38">Most of the better scoring systems did supply these results.</S>
    <S sid="324" ssid="39">While all systems did slightly better here in terms of raw scores, the performance was not much different from the official task, indicating that mention boundary errors resulting from problems in parsing do not contribute significantly to the final output.23 One side benefit of performing this supplemental evaluation was that it revealed a subtle bug in the automatic scoring routine that we were using that could double-count duplicate correct mentions in a given entity chain.</S>
    <S sid="325" ssid="40">These can occur, for example, if the system considers a unit-production NP-PRP combination as two mentions that identify the exact same token in the text, and reports them as separate mentions.</S>
    <S sid="326" ssid="41">Most systems had a filter in their processing that selected only one of these duplicate mentions, but the kobdani system considered both as potential mentions, and its developers tuned their algorithm using that flawed version of the scorer.</S>
    <S sid="327" ssid="42">When we fixed the scorer and re-evaluated all of the systems, the kobdani system was the only one whose score was affected significantly, dropping by about 8 points, which lowered that system&#8217;s rank from second to ninth.</S>
    <S sid="328" ssid="43">It is not clear how much of this was owing to the fact that the system&#8217;s paramrelations with verbs and appositives.</S>
    <S sid="329" ssid="44">Since OntoNotes is only annotated with multi-mention entities, singleton referential elements were identified heuristically: all NPs and possessive determiners were annotated as singletons excluding those functioning as appositives or as pre-modifiers but for NPs in the possessive case.</S>
    <S sid="330" ssid="45">In coordinated NPs, single constituents as well as the entire NPs were considered to be mentions.</S>
    <S sid="331" ssid="46">There is no reliable heuristic to automatically detect English expletive pronouns, thus they were (although inaccurately) also annotated as singletons.&#8221; 23It would be interesting to measure the overlap between the entity clusters for these two cases, to see whether there was any substantial difference in the mention chains, besides the expected differences in boundaries for individual mentions. eters had been tuned using the scorer with the bug, which double-credited duplicate mentions.</S>
    <S sid="332" ssid="47">To find out for sure, one would have to re-tune the system using the modified scorer.</S>
    <S sid="333" ssid="48">One difficulty with this supplementary evaluation using gold mention boundaries is that those boundaries alone provide only very partial information.</S>
    <S sid="334" ssid="49">For the roughly 10% of mentions that the automatic parser did not correctly identify, while the systems knew the correct boundaries, they had no hierarchical parser or semantic role label information, and they also had to further approximate the already heuristic head word identification.</S>
    <S sid="335" ssid="50">This incomplete data complicated the systems&#8217; task and also complicates interpretation of the results.</S>
    <S sid="336" ssid="51">The final supplementary condition that we explored was if the systems were supplied with the manuallyannotated spans for exactly those mentions that did participate in the gold standard coreference chains.</S>
    <S sid="337" ssid="52">This supplies significantly more information than the previous case, where exact spans were supplied for all NPs, since the gold mentions list here will also include verb headwords that are linked to event NPs, but will not include singleton mentions, which do not end up as part of any chain.</S>
    <S sid="338" ssid="53">The latter constraint makes this test seem somewhat artificial, since it directly reveals part of what the systems are designed to determine, but it still has some value in quantifying the impact that mention detection has on the overall task and what the results are if the mention detection is perfect.</S>
    <S sid="339" ssid="54">Since this was a logical extension of the task and since the data was available to the participants for the development set, a few of the sites did run experiments of this type.</S>
    <S sid="340" ssid="55">Therefore we decided to provide the gold mentions data to a few sites who had reported these scores, so that we could compute the performance on the test set.</S>
    <S sid="341" ssid="56">The results of these experiments are shown in Tables 16 and 17.</S>
    <S sid="342" ssid="57">The results show that performance does go up significantly, indicating that it is markedly easier for the systems to generate better entities given gold mentions.</S>
    <S sid="343" ssid="58">Although, ideally, one would expect a perfect mention detection score, it is the case that one of the two systems &#8211; lee &#8211; did not get a 100% Recall.</S>
    <S sid="344" ssid="59">This could possibly be owing to unlinked singletons that were removed in post-processing.</S>
    <S sid="345" ssid="60">The lee system developers also ran a further experiment where both gold mentions for the elements of the coreference chains and also gold annotations for all the other layers were available to the system.</S>
    <S sid="346" ssid="61">Surprisingly, the improvement in coreference performance from having gold annotation of the other layers was almost negligible.</S>
    <S sid="347" ssid="62">This suggests that either: i) the automatic models are predicting those layers well enough that switching to gold doesn&#8217;t make much difference; ii) information from the other layers does not provide much leverage for coreference resolution; or iii) current coreference models are not capable of utilizing the information from these other layers effectively.</S>
    <S sid="348" ssid="63">Given the performance numbers on the individual layers cited earlier, (i) seems unlikely, and we hope that further research in how best to leverage these layers will result in models that can benefit from them more definitively.</S>
    <S sid="349" ssid="64">In order to check how stringent the official, exact match scoring is, we also performed a relaxed scoring.</S>
    <S sid="350" ssid="65">Unlike ACE and MUC, the OntoNotes data does not have manually annotated minimum spans that a mention must contain to be considered correct.</S>
    <S sid="351" ssid="66">However, OntoNotes does have manual syntactic analysis in the form of the Treebank.</S>
    <S sid="352" ssid="67">Therefore, we decided to approximate the minimum spans by using the head words of the mentions using the gold standard syntax tree.</S>
    <S sid="353" ssid="68">If the response mention contained the head word and did not exceed the true mention boundary, then it was considered correct &#8211; both from the point of view of mention detection, and coreference resolution.</S>
    <S sid="354" ssid="69">The scores using this relaxed strategy for the open and closed track submissions using predicted data are shown in Tables 18 and 19.</S>
    <S sid="355" ssid="70">It can be observed that the relaxed, head word based, scoring does not improve performance very much.</S>
    <S sid="356" ssid="71">The only exception was the klenner system whose performance increased from 51.77 to 55.28.</S>
    <S sid="357" ssid="72">Overall, the ranking remained quite stable, though it did change for some adjacent systems which had very close exact match scores.</S>
    <S sid="358" ssid="73">In order to check how the systems did on various genres, we scored their performance per genre as well.</S>
    <S sid="359" ssid="74">Tables 20 and 21 summarize genre based performance for the closed and open track participants respectively.</S>
    <S sid="360" ssid="75">System performance does not seem to vary as much across the different genres as is normally the case with language processing tasks, which could suggest that coreference is relatively genre insensitive, or it is possible that scores are two low for the difference to be apparent.</S>
    <S sid="361" ssid="76">Comparisons are difficult, however, because the spoken genres were treated here with perfect speech recognition accuracy and perfect speaker turn information.</S>
    <S sid="362" ssid="77">Under more realistic application conditions, the spread in performance between genres might be greater.</S>
  </SECTION>
  <SECTION title="6 Approaches" number="6">
    <S sid="363" ssid="1">Tables 22 and 23 summarize the approaches of the participating systems along with some of the important dimensions.</S>
    <S sid="364" ssid="2">Most of the systems broke the problem into two phases, first identifying the potential mentions in the text and then linking the mentions to form coreference chains.</S>
    <S sid="365" ssid="3">Most participants also used rule-based approaches for mention detection, though two did use trained models.</S>
    <S sid="366" ssid="4">While trained morels seem able to better balance precision and recall, and thus to achieve a higher F-score on the mention task itself, their recall tends to be quite a bit lower than that achievable by rule-based systems designed to favor recall.</S>
    <S sid="367" ssid="5">This impacts coreference scores because the full coreference system has no way to recover if the mention detection stage misses a potentially anaphoric mention.</S>
    <S sid="368" ssid="6">Only one of the participating systems cai attempted to do joint mention detection and coreference resolution.</S>
    <S sid="369" ssid="7">While it did not happen to be among the top-performing systems, the difference in performance could be due to the richer features used by other systems rather than to the use of a joint model.</S>
    <S sid="370" ssid="8">Most systems represented the markable mentions internally in terms of the parse tree NP constituent span, but some systems used shared attribute models, where the attributes of the merged entity are determined collectively by heuristically merging the attribute types and values of the different constituent mentions.</S>
    <S sid="371" ssid="9">Various types of trained models were used for predicting coreference.</S>
    <S sid="372" ssid="10">It is interesting to note that some of the systems, including the best-performing one, used a completely rule-based approach even for this component.</S>
    <S sid="373" ssid="11">Most participants appear not to have focused much on eventive coreference, those coreference chains that build off verbs in the data.</S>
    <S sid="374" ssid="12">This usually meant that mentions that should have linked to the eventive verb were instead linked in with some other entity.</S>
    <S sid="375" ssid="13">Participants may have chosen not to focus on events because they pose unique challenges while making up only a small portion of the data.</S>
    <S sid="376" ssid="14">Roughly 91% of mentions in the data are NPs and pronouns.</S>
    <S sid="377" ssid="15">In the systems that used trained models, many systems used the approach described in Soon et al. (2001) for selecting the positive and negative training examples, while others used some of the alternative approaches that have been introduced in the research literature more recently.</S>
    <S sid="378" ssid="16">Many of the trained systems also were able to improve their performance by using feature selection, though things varied some depending on the example selection strategy and the classifier used.</S>
    <S sid="379" ssid="17">Almost half of the trained systems used the feature selection strategy from Soon et al. (2001) and found it beneficial.</S>
    <S sid="380" ssid="18">It is not clear whether the other systems did not explore this path, or whether it just did not prove as useful in their case.</S>
  </SECTION>
  <SECTION title="7 Conclusions" number="7">
    <S sid="381" ssid="1">In this paper we described the anaphoric coreference information and other layers of annotation in the Mention pairs with less than threshold (5) number of different attribute values are considered (22% out of 99% original are discarded) All mention pairs and longer of nested mentions with common head kept OntoNotes corpus, and presented the results from an evaluation on learning such unrestricted entities and events in text.</S>
    <S sid="382" ssid="2">The following represent our conclusions on reviewing the results: &#8226; Perhaps the most surprising finding was that the best-performing system (lee) was completely rule-based, rather than trained.</S>
    <S sid="383" ssid="3">This suggests that their rule-based approach was able to do a more effective job of combining the multiple sources of evidence than the trained systems.</S>
    <S sid="384" ssid="4">The features for coreference prediction are certainly more complex than for many other language processing tasks, which makes it more challenging to generate effective feature combinations.</S>
    <S sid="385" ssid="5">The rule-based approach used by the best-performing system seemed to benefit from a heuristic that captured the most confident links before considering less confident ones, and also made use of the information in the guidelines in a slightly more refined manner than other systems.</S>
    <S sid="386" ssid="6">They also included appositives and copular constructions in their calculations.</S>
    <S sid="387" ssid="7">Although OntoNotes does not count those as instances of IDENT coreference, using that information may have helped their system discover additional useful links.</S>
    <S sid="388" ssid="8">&#8226; It is interesting to note that the developers of the lee system also did the experiment of running their system using gold standard information on the individual layers, rather than automatic model predictions.</S>
    <S sid="389" ssid="9">The somewhat surprising result was that using perfect information for the other layers did not end up improving coreference performance much, if at all.</S>
    <S sid="390" ssid="10">It is not clear whether this means that: i) Automatic predictors for the individual layers are accurate enough already; ii) Information captured by those supplementary layers actually does not provide much leverage for resolving coreference; or iii) researchers have yet have found an effective way of capturing and utilizing the extra information provided by these layers.</S>
    <S sid="391" ssid="11">&#8226; It does seem that collecting information about an entity by merging information across the various attributes of the mentions that comprise it can be useful, though not all systems that attempted this achieved a benefit.</S>
    <S sid="392" ssid="12">&#8226; System performance did not seem to vary as much across the different genres as is normally the case with language processing tasks, which could suggest that coreference is relatively genre insensitive, or it is possible that scores are two low for the difference to be apparent.</S>
    <S sid="393" ssid="13">Comparisons are difficult, however, because the spoken genres were treated here with perfect speech recognition accuracy and perfect speaker turn information.</S>
    <S sid="394" ssid="14">Under more realistic application conditions, the spread in performance between genres might be greater.</S>
    <S sid="395" ssid="15">&#8226; It is noteworthy that systems did not seem to attempt the kind of joint inference that could make use of the full potential of various layers available in OntoNotes, but this could well have been owing to the limited time available for the shared task.</S>
    <S sid="396" ssid="16">&#8226; We had expected to see more attention paid to event coreference, which is a novel feature in this data, but again, given the time constraints and given that events represent only a small portion of the total, it is not surprising that most systems chose not to focus on it. versions of the CEAF metric &#8211; which tries to capture the goodness of the entities in the output &#8211; seem much lower than the other metric, though it is not clear whether that means that our systems are doing a poor job of creating coherent entities or whether that metric is just especially harsh.</S>
    <S sid="397" ssid="17">Finally, it is interesting to note that the problem of coreference does not seem to be following the same kind of learning curve that we are used to with other problems of this sort.</S>
    <S sid="398" ssid="18">While performance has improved somewhat, it is not clear how far we will be able to go given the strategies at hand, or whether new techniques will be needed to capture additional information from the texts or from world knowledge.</S>
    <S sid="399" ssid="19">We hope that this corpus and task will provide a useful resource for continued experimentation to help resolve this issue.</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="8">
    <S sid="400" ssid="1">We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.</S>
    <S sid="401" ssid="2">HR0011-06-C-0022.</S>
    <S sid="402" ssid="3">We would like to thank all the participants.</S>
    <S sid="403" ssid="4">Without their hard work, patience and perseverance this evaluation would not have been a success.</S>
    <S sid="404" ssid="5">We would also like to thank the Linguistic Data Consortium for making the OntoNotes 4.0 corpus freely and timely available to the participants.</S>
    <S sid="405" ssid="6">Emili Sapena, who graciously allowed the use of his scorer implementation, and made available enhancements and immediately fixed issues that were uncovered during the evaluation.</S>
    <S sid="406" ssid="7">Finally, we offer our special thanks to Llufs M`arquez and Joakim Nivre for their wonderful support and guidance without which this task would not have been successful.</S>
  </SECTION>
</PAPER>
