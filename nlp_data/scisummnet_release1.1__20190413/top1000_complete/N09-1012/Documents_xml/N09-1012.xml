<PAPER>
  <S sid="0">Improving Unsupervised Dependency Parsing with Richer Contexts and Smoothing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts.</S>
    <S sid="2" ssid="2">Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns.</S>
    <S sid="3" ssid="3">In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing.</S>
    <S sid="4" ssid="4">Our model produces state-of-theart results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="5" ssid="1">The last decade has seen great strides in statistical natural language parsing.</S>
    <S sid="6" ssid="2">Supervised and semisupervised methods now provide highly accurate parsers for a number of languages, but require training from corpora hand-annotated with parse trees.</S>
    <S sid="7" ssid="3">Unfortunately, manually annotating corpora with parse trees is expensive and time consuming so for languages and domains with minimal resources it is valuable to study methods for parsing without requiring annotated sentences.</S>
    <S sid="8" ssid="4">In this work, we focus on unsupervised dependency parsing.</S>
    <S sid="9" ssid="5">Our goal is to produce a directed graph of dependency relations (e.g.</S>
    <S sid="10" ssid="6">Figure 1) where each edge indicates a head-argument relation.</S>
    <S sid="11" ssid="7">Since the task is unsupervised, we are not given any examples of correct dependency graphs and only take words and their parts of speech as input.</S>
    <S sid="12" ssid="8">Most of the recent work in this area (Smith, 2006; Cohen et al., 2008) has focused on variants of the The big dog barks Dependency Model with Valence (DMV) by Klein and Manning (2004).</S>
    <S sid="13" ssid="9">DMV was the first unsupervised dependency grammar induction system to achieve accuracy above a right-branching baseline.</S>
    <S sid="14" ssid="10">However, DMV is not able to capture some of the more complex aspects of language.</S>
    <S sid="15" ssid="11">Borrowing some ideas from the supervised parsing literature, we present two new models: Extended Valence Grammar (EVG) and its lexicalized extension (L-EVG).</S>
    <S sid="16" ssid="12">The primary difference between EVG and DMV is that DMV uses valence information to determine the number of arguments a head takes but not their categories.</S>
    <S sid="17" ssid="13">In contrast, EVG allows different distributions over arguments for different valence slots.</S>
    <S sid="18" ssid="14">L-EVG extends EVG by conditioning on lexical information as well.</S>
    <S sid="19" ssid="15">This allows L-EVG to potentially capture subcategorizations.</S>
    <S sid="20" ssid="16">The downside of adding additional conditioning events is that we introduce data sparsity problems.</S>
    <S sid="21" ssid="17">Incorporating more valence and lexical information increases the number of parameters to estimate.</S>
    <S sid="22" ssid="18">A common solution to data sparsity in supervised parsing is to add smoothing.</S>
    <S sid="23" ssid="19">We show that smoothing can be employed in an unsupervised fashion as well, and show that mixing DMV, EVG, and L-EVG together produces state-ofthe-art results on this task.</S>
    <S sid="24" ssid="20">To our knowledge, this is the first time that grammars with differing levels of detail have been successfully combined for unsupervised dependency parsing.</S>
    <S sid="25" ssid="21">A brief overview of the paper follows.</S>
    <S sid="26" ssid="22">In Section 2, we discuss the relevant background.</S>
    <S sid="27" ssid="23">Section 3 presents how we will extend DMV with additional that satisfies the following properties: features.</S>
    <S sid="28" ssid="24">We describe smoothing in an unsupervised context in Section 4.</S>
    <S sid="29" ssid="25">In Section 5, we discuss search issues.</S>
    <S sid="30" ssid="26">We present our experiments in Section 6 and conclude in Section 7.</S>
  </SECTION>
  <SECTION title="2 Background" number="2">
    <S sid="31" ssid="1">In this paper, the observed variables will be a corpus of n sentences of text s = s1 ... sn, and for each word sij an associated part-of-speech &#964;ij.</S>
    <S sid="32" ssid="2">We denote the set of all words as Vw and the set of all parts-ofspeech as V&#964;.</S>
    <S sid="33" ssid="3">The hidden variables are parse trees t = t1 ... tn and parameters &#952;&#175; which specify a distribution over t. A dependency tree ti is a directed acyclic graph whose nodes are the words in si.</S>
    <S sid="34" ssid="4">The graph has a single incoming edge for each word in each sentence, except one called the root of ti.</S>
    <S sid="35" ssid="5">An edge from word i to word j means that word j is an argument of word i or alternatively, word i is the head of word j.</S>
    <S sid="36" ssid="6">Note that each word token may be the argument of at most one head, but a head may have several arguments.</S>
    <S sid="37" ssid="7">If parse tree ti can be drawn on a plane above the sentence with no crossing edges, it is called projective.</S>
    <S sid="38" ssid="8">Otherwise it is nonprojective.</S>
    <S sid="39" ssid="9">As in previous work, we restrict ourselves to projective dependency trees.</S>
    <S sid="40" ssid="10">The dependency models in this paper will be formulated as a particular kind of Probabilistic Context Free Grammar (PCFG), described below.</S>
    <S sid="41" ssid="11">3.</S>
    <S sid="42" ssid="12">If N1 &#8212;* &#946;1 and N2 &#8212;* &#946;2 are tied then the tying relation defines a one-to-one mapping between rules in RN, and RN2, and we say that N1 and N2 are tied nonterminals.</S>
    <S sid="43" ssid="13">As we see below, we can estimate tied PCFGs using standard techniques.</S>
    <S sid="44" ssid="14">Clearly, the tying relation also defines an equivalence class over nonterminals.</S>
    <S sid="45" ssid="15">The tying relation allows us to formulate the distributions over trees in terms of rule equivalence classes and nonterminal equivalence classes.</S>
    <S sid="46" ssid="16">Suppose R&#175; is the set of rule equivalence classes and JV&#175; is the set of nonterminal equivalence classes.</S>
    <S sid="47" ssid="17">Since all rules in an equivalence class r&#175; have the same probability (condition 1), and since all the nonterminals in an equivalence class N&#175; E JV&#175; have the same distribution over rule equivalence classes (condition 1 and 3), we can define the set of rule equivalence classes R&#175;N&#175; associated with a nonterminal equivalence class &#175;N, and a vector &#952;&#175; of probabilities, indexed by rule equivalence classes r&#175; E R&#175; . &#952;&#175;N&#175; refers to the subvector of &#952;&#175; associated with nonterminal equivalence class &#175;N, indexed by r&#175; E R&#175;&#175;N.</S>
    <S sid="48" ssid="18">Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S>
    <S sid="49" ssid="19">This will allow us to make independence assumptions for smoothing purposes without losing information, by giving analogous rules the same probability.</S>
    <S sid="50" ssid="20">Let G = (JV, T , S, R, &#952;) be a Probabilistic Context Free Grammar with nonterminal symbols JV, terminal symbols T, start symbol S E JV, set of productions R of the form N &#8212;* &#946;, N E JV, &#946; E (JV U T )*.</S>
    <S sid="51" ssid="21">Let RN indicate the subset of R whose left-hand sides are N. &#952; is a vector of length JRJ, indexed by productions N &#8212;* &#946; E R. &#952;N&#65533;&#946; specifies the probability that N rewrites to &#946;.</S>
    <S sid="52" ssid="22">We will let &#952;N indicate the subvector of &#952; corresponding to RN.</S>
    <S sid="53" ssid="23">A tied PCFG constrains a PCFG G with a tying relation, which is an equivalence relation over rules Let f(t, r) denote the number of times rule r appears in tree t, and let f(t, &#175;r) = ErE&#175;r f(t, r).</S>
    <S sid="54" ssid="24">We see that the complete data likelihood is That is, the likelihood is a product of multinomials, one for each nonterminal equivalence class, and there are no constraints placed on the parameters of these multinomials besides being positive and summing to one.</S>
    <S sid="55" ssid="25">This means that all the standard estimation methods (e.g.</S>
    <S sid="56" ssid="26">Expectation Maximization, Variational Bayes) extend directly to tied PCFGs.</S>
    <S sid="57" ssid="27">Maximum likelihood estimation provides a point estimate of &#175;&#952;.</S>
    <S sid="58" ssid="28">However, often we want to incorporate information about &#952;&#175; by modeling its prior distribution.</S>
    <S sid="59" ssid="29">As a prior, for each N&#175; E JV&#175; we will specify a Dirichlet distribution over &#952;N&#175; with hyperparameters &#945; &#175;N.</S>
    <S sid="60" ssid="30">The Dirichlet has the density function: Thus the prior over &#952; is a product of Dirichlets,which is conjugate to the PCFG likelihood function (Johnson et al., 2007).</S>
    <S sid="61" ssid="31">That is, the posterior P(&#175;&#952;|s, t, &#945;) is also a product of Dirichlets, also factoring into a Dirichlet for each nonterminal &#175;N, where the parameters &#945;&#175;r are augmented by the number of times rule r&#175; is observed in tree t: We can see that &#945;&#175;r acts as a pseudocount of the number of times r&#175; is observed prior to t. To make use of this prior, we use the Variational Bayes (VB) technique for PCFGs with Dirichlet Priors presented by Kurihara and Sato (2004).</S>
    <S sid="62" ssid="32">VB estimates a distribution over &#175;&#952;.</S>
    <S sid="63" ssid="33">In contrast, Expectation Maximization estimates merely a point estimate of &#175;&#952;.</S>
    <S sid="64" ssid="34">In VB, one estimates Q(t, &#175;&#952;), called the variational distribution, which approximates the posterior distribution P(t, &#175;&#952;|s, &#945;) by minimizing the KL divergence of P from Q.</S>
    <S sid="65" ssid="35">Minimizing the KL divergence, it turns out, is equivalent to maximizing a lower bound F of the log marginal likelihood log P(s|&#945;).</S>
    <S sid="66" ssid="36">The negative of the lower bound, &#8722;F, is sometimes called the free energy.</S>
    <S sid="67" ssid="37">As is typical in variational approaches, Kurihara and Sato (2004) make certain independence assumptions about the hidden variables in the variational posterior, which will make estimating it simpler.</S>
    <S sid="68" ssid="38">It factors Q(t, &#952;) = Q(t)Q( &#175;&#952;) = Qn i=1 Qi(ti)Q&#175;N&#8712;N&#175;Q( &#952; &#175;N).</S>
    <S sid="69" ssid="39">The goal is to recover &#952;), the estimate of the posterior distribution over parameters and Q(t), the estimate of the posterior distribution over trees.</S>
    <S sid="70" ssid="40">Finding a local maximum of F is done via an alternating maximization of Q(&#175;&#952;) and Q(t).</S>
    <S sid="71" ssid="41">Kurihara and Sato (2004) show that each &#952; &#175;N) is a Dirichlet distribution with parameters &#710;&#945;r = &#945;r + EQ(t)f(t, r).</S>
    <S sid="72" ssid="42">In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bilexical CFGs (Eisner and Satta, 1999).</S>
    <S sid="73" ssid="43">These allow us to use the fast Eisner and Satta (1999) parsing algorithm to compute the expectations required by VB in O(m3) time (Eisner and Blatz, 2007; Johnson, 2007) where m is the length of the sentence.1 In the split-head bilexical CFG framework, each nonterminal in the grammar is annotated with a terminal symbol.</S>
    <S sid="74" ssid="44">For dependency grammars, these annotations correspond to words and/or parts-ofspeech.</S>
    <S sid="75" ssid="45">Additionally, split-head bilexical CFGs require that each word sij in sentence si is represented in a split form by two terminals called its left part sijL and right part sijR.</S>
    <S sid="76" ssid="46">The set of these parts constitutes the terminal symbols of the grammar.</S>
    <S sid="77" ssid="47">This split-head property relates to a particular type of dependency grammar in which the left and right dependents of a head are generated independently.</S>
    <S sid="78" ssid="48">Note that like CFGs, split-head bilexical CFGs can be made probabilistic.</S>
    <S sid="79" ssid="49">The most successful recent work on dependency induction has focused on the Dependency Model with Valence (DMV) by Klein and Manning (2004).</S>
    <S sid="80" ssid="50">DMV is a generative model in which the head of the sentence is generated and then each head recursively generates its left and right dependents.</S>
    <S sid="81" ssid="51">The arguments of head H in direction d are generated by repeatedly deciding whether to generate another new argument or to stop and then generating the argument if required.</S>
    <S sid="82" ssid="52">The probability of deciding whether to generate another argument is conditioned on H, d and whether this would be the first argument (this is the sense in which it models valence).</S>
    <S sid="83" ssid="53">When DMV generates an argument, the part-of-speech of that argument A is generated given H and d. 1Efficiently parsable versions of split-head bilexical CFGs for the models described in this paper can be derived using the fold-unfold grammar transform (Eisner and Blatz, 2007; Johnson, 2007). tence.</S>
    <S sid="84" ssid="54">Note that these rules are for VH, A E VT so there is an instance of the first schema rule for each part-of-speech.</S>
    <S sid="85" ssid="55">YH splits words into their left and right components.</S>
    <S sid="86" ssid="56">LH encodes the stopping decision given that we have not generated any arguments so far.</S>
    <S sid="87" ssid="57">L&#8242;H encodes the same decision after generating one or more arguments.</S>
    <S sid="88" ssid="58">L1H represents the distribution over left attachments.</S>
    <S sid="89" ssid="59">To extract dependency relations from these parse trees, we scan for attachment rules (e.g., L1H &#8212;* YA L&#8242;H) and record that A depends on H. The schema omits the rules for right arguments since they are symmetric.</S>
    <S sid="90" ssid="60">We show a parse of &#8220;The big dog barks&#8221; in Figure 3.2 Much of the extensions to this work have focused on estimation procedures.</S>
    <S sid="91" ssid="61">Klein and Manning (2004) use Expectation Maximization to estimate the model parameters.</S>
    <S sid="92" ssid="62">Smith and Eisner (2005) and Smith (2006) investigate using Contrastive Estimation to estimate DMV.</S>
    <S sid="93" ssid="63">Contrastive Estimation maximizes the conditional probability of the observed sentences given a neighborhood of similar unseen sequences.</S>
    <S sid="94" ssid="64">The results of this approach vary widely based on regularization and neighborhood, but often outperforms EM.</S>
    <S sid="95" ssid="65">Smith (2006) also investigates two techniques for maximizing likelihood while incorporating the locality bias encoded in the harmonic initializer for DMV.</S>
    <S sid="96" ssid="66">One technique, skewed deterministic annealing, ameliorates the local maximum problem by flattening the likelihood and adding a bias towards the Klein and Manning initializer, which is decreased during learning.</S>
    <S sid="97" ssid="67">The second technique is structural annealing (Smith and Eisner, 2006; Smith, 2006) which penalizes long dependencies initially, gradually weakening the penalty during estimation.</S>
    <S sid="98" ssid="68">If hand-annotated dependencies on a held-out set are available for parameter selection, this performs far better than EM; however, performing parameter selection on a held-out set without the use of gold dependencies does not perform as well.</S>
    <S sid="99" ssid="69">Cohen et al. (2008) investigate using Bayesian Priors with DMV.</S>
    <S sid="100" ssid="70">The two priors they use are the Dirichlet (which we use here) and the Logistic Normal prior, which allows the model to capture correlations between different distributions.</S>
    <S sid="101" ssid="71">They initialize using the harmonic initializer of Klein and Manning (2004).</S>
    <S sid="102" ssid="72">They find that the Logistic Normal distribution performs much better than the Dirichlet with this initialization scheme.</S>
    <S sid="103" ssid="73">Cohen and Smith (2009), investigate (concurrently with our work) an extension of this, the Shared Logistic Normal prior, which allows different PCFG rule distributions to share components.</S>
    <S sid="104" ssid="74">They use this machinery to investigate smoothing the attachment distributions for (nouns/verbs), and for learning using multiple languages.</S>
  </SECTION>
  <SECTION title="3 Enriched Contexts" number="3">
    <S sid="105" ssid="1">DMV models the distribution over arguments identically without regard to their order.</S>
    <S sid="106" ssid="2">Instead, we propose to distinguish the distribution over the argument nearest the head from the distribution of subsequent arguments.</S>
    <S sid="107" ssid="3">3 Consider the following changes to the DMV grammar (results shown in Figure 4).</S>
    <S sid="108" ssid="4">First, we will introduce the rule L2H &#8212;* YA L&#8242;H to denote the decision of what argument to generate for positions not nearest to the head.</S>
    <S sid="109" ssid="5">Next, instead of having L&#8242;H expand to HL or L1H, we will expand it to L1H (attach to nearest argument and stop) or L2H (attach to nonnearest argument and continue).</S>
    <S sid="110" ssid="6">We call this the Extended Valence Grammar (EVG).</S>
    <S sid="111" ssid="7">As a concrete example, consider the phrase &#8220;the big hungry dog&#8221; (Figure 5).</S>
    <S sid="112" ssid="8">We would expect that distribution over the nearest left argument for &#8220;dog&#8221; to be different than farther left arguments.</S>
    <S sid="113" ssid="9">The figure shows that EVG allows these two distributions to be different (nonterminals L2dog and L1dog) whereas DMV forces them to be equivalent (both use L1dog as the nonterminal).</S>
    <S sid="114" ssid="10">All of the probabilistic models discussed thus far have incorporated only part-of-speech information (see Footnote 2).</S>
    <S sid="115" ssid="11">In supervised parsing of both dependencies and constituency, lexical information is critical (Collins, 1999).</S>
    <S sid="116" ssid="12">We incorporate lexical information into EVG (henceforth L-EVG) by extending the distributions over argument parts-of-speech A to condition on the head word h in addition to the head part-of-speech H, direction d and argument position v. The argument word a distribution is merely conditioned on part-of-speech A; we leave refining this model to future work.</S>
    <S sid="117" ssid="13">In order to incorporate lexicalization, we extend the EVG CFG to allow the nonterminals to be annotated with both the word and part-of-speech of the head.</S>
    <S sid="118" ssid="14">We first remove the old rules YH &#8212;* LH RH for each H E VT. Then we mark each nonterminal which is annotated with a part-of-speech as also annotated with its head, with a single exception: YH.</S>
    <S sid="119" ssid="15">We add a new nonterminal YH,h for each H E VT, h E Vw, and the rules YH &#8212;* YH,h and YH,h &#8212;* LH,h RH,h.</S>
    <S sid="120" ssid="16">The rule YH &#8212;* YH,h corresponds to selecting the word, given its part-ofspeech.</S>
  </SECTION>
  <SECTION title="4 Smoothing" number="4">
    <S sid="121" ssid="1">In supervised estimation one common smoothing technique is linear interpolation, (Jelinek, 1997).</S>
    <S sid="122" ssid="2">This section explains how linear interpolation can be represented using a PCFG with tied rule probabilities, and how one might estimate smoothing parameters in an unsupervised framework.</S>
    <S sid="123" ssid="3">In many probabilistic models it is common to estimate the distribution of some event x conditioned on some set of context information P(x|N(1) ... N(k)) by smoothing it with less complicated conditional distributions.</S>
    <S sid="124" ssid="4">Using linear interpolation we model P(x|N(1) ... N(k)) as a weighted average of two distributions &#955;1P1(x|N(1), ... , N(k)) + &#955;2P2(x|N(1), ... , N(k&#8722;1)), where the distribution P2 makes an independence assumption by dropping the conditioning event N(k).</S>
    <S sid="125" ssid="5">In a PCFG a nonterminal N can encode a collection of conditioning events N(1) ... N(k), and BN determines a distribution conditioned on N(1) ... N(k) over events represented by the rules r &#8712; RN.</S>
    <S sid="126" ssid="6">For example, in EVG the nonterminal L1 encodes three separate pieces of conditioning information: the direction d = left, the head part-of-speech H = NN, and the argument position v = 0; BL1&#8594;YJJ NNL represents the probability of generating JJ as the first left argument of NN.</S>
    <S sid="127" ssid="7">Suppose in EVG we are interested in smoothing P(A | d, H, v) with a component that excludes the head conditioning event.</S>
    <S sid="128" ssid="8">Using linear interpolation, this would be: We will estimate PCFG rules with linearly interpolated probabilities by creating a tied PCFG which is extended by adding rules that select between the main distribution P1 and the backoff distribution P2, and also rules that correspond to draws from those distributions.</S>
    <S sid="129" ssid="9">We will make use of tied rule probabilities to make the independence assumption in the backoff distribution.</S>
    <S sid="130" ssid="10">We still use the original grammar to parse the sentence.</S>
    <S sid="131" ssid="11">However, we estimate the parameters in the extended grammar and then translate them back into the original grammar for parsing.</S>
    <S sid="132" ssid="12">More formally, suppose B &#8838; N is a set of nonterminals (called the backoff set) with conditioning events N(1) ... N(k&#8722;1) in common (differing in a conditioning event N(k)), and with rule sets of the same cardinality.</S>
    <S sid="133" ssid="13">If G is our model&#8217;s PCFG, we can define a new tied PCFG G&#8242; = (N&#8242;, T , S, R&#8242;, O), where N&#8242; = N &#8746; {Nb&#8467;  |N &#8712; B, E &#8712; {1, 2}}, meaning for each nonterminal N in the backoff set we add two nonterminals Nb1, Nb2 representing each distribution P1 and P2.</S>
    <S sid="134" ssid="14">The new rule set R&#8242; = (&#8746;N&#8712;N&#8242;R&#8242;N) where for all N &#8712; B rule set R&#8242;N = IN &#8594; Nb&#8467;  |E &#8712; {1, 2}}, meaning at N in G&#8242; we decide which distribution P1, P2 to use; and for N &#8712; B and E &#8712; {1, 2} , R&#8242;Nb&#8467; = {Nb&#8467; &#8594; Q  |N &#8594; Q &#8712; RN} indicating a draw from distribution P&#8467;.</S>
    <S sid="135" ssid="15">For nonterminals N &#8712;6 B, R&#8242;N = RN.</S>
    <S sid="136" ssid="16">Finally, for each N, M &#8712; B we specify a tying relation between the rules inR&#8242;Nb2 and R&#8242;Mb2 , grouping together analogous rules.</S>
    <S sid="137" ssid="17">This has the effect of making an independence assumption about P2, namely that it ignores the conditioning event N(k), drawing from a common distribution each time a nonterminal Nb2 is rewritten.</S>
    <S sid="138" ssid="18">For example, in EVG to smooth P(A = DT | d = left, H = NN, v = 0) with P2(A = DT | d = left, v = 0) we define the backoff set to be {LH  |H &#8712; V&#964;}.</S>
    <S sid="139" ssid="19">In the extended grammar we define the tying relation to form rule equivalence classes by the argument they generate, i.e. for each argument A &#8712; V&#964;, we have a rule equivalence class {LFb2 &#8594; YA HL  |H &#8712; V&#964;}.</S>
    <S sid="140" ssid="20">We can see that in grammar G&#8242; each N &#8712; B eventually ends up rewriting to one of N&#8217;s expansions Q in G. There are two indirect paths, one through Nb1 and one through Nb2.</S>
    <S sid="141" ssid="21">Thus this defines the probability of N &#8594; Q in G, BN&#8594;&#946;, as the probability of rewriting N as Q in G&#8242; via Nb1 and Nb2.</S>
    <S sid="142" ssid="22">That is: BN&#8594;&#946; = ON&#8594;Nb1 ONb1&#8594;&#946; + ON&#8594;Nb2 ONb2&#8594;&#946; The example in Figure 6 shows the probability that L1 rewrites to Ybig dogL in grammar G. dog Typically when smoothing we need to incorporate the prior knowledge that conditioning events that have been seen fewer times should be more strongly smoothed.</S>
    <S sid="143" ssid="23">We accomplish this by setting the Dirichlet hyperparameters for each N &#8594; Nb1, N &#8594; Nb2 decision to (K, 2K), where K = |RNb1  |is the number of rewrite rules for A.</S>
    <S sid="144" ssid="24">This ensures that the model will only start to ignore the backoff distribuOur first experiments examine smoothing the distributions over an argument in the DMV and EVG models.</S>
    <S sid="145" ssid="25">In DMV we smooth the probability of argument A given head part-of-speech H and direction d with a distribution that ignores H. In EVG, which conditions on H, d and argument position v we back off two ways.</S>
    <S sid="146" ssid="26">The first is to ignore v and use backoff conditioning event H, d. This yields a backoff distribution with the same conditioning information as the argument distribution from DMV.</S>
    <S sid="147" ssid="27">We call this EVG smoothed-skip-val.</S>
    <S sid="148" ssid="28">The second possibility is to have the backoff distribution ignore the head part-of-speech H and use backoff conditioning event v, d. This assumes that arguments share a common distribution across heads.</S>
    <S sid="149" ssid="29">We call this EVG smoothed-skip-head.</S>
    <S sid="150" ssid="30">As we see below, backing off by ignoring the part-ofspeech of the head H worked better than ignoring the argument position v. For L-EVG we smooth the argument part-ofspeech distribution (conditioned on the head word) with the unlexicalized EVG smoothed-skip-head model.</S>
  </SECTION>
  <SECTION title="5 Initialization and Search issues" number="5">
    <S sid="151" ssid="1">Klein and Manning (2004) strongly emphasize the importance of smart initialization in getting good performance from DMV.</S>
    <S sid="152" ssid="2">The likelihood function is full of local maxima and different initial parameter values yield vastly different quality solutions.</S>
    <S sid="153" ssid="3">They offer what they call a &#8220;harmonic initializer&#8221; which initializes the attachment probabilities to favor arguments that appear more closely in the data.</S>
    <S sid="154" ssid="4">This starts EM in a state preferring shorter attachments.</S>
    <S sid="155" ssid="5">Since our goal is to expand the model to incorporate lexical information, we want an initialization scheme which does not depend on the details of DMV.</S>
    <S sid="156" ssid="6">The method we use is to create M sets of B random initial settings and to run VB some small number of iterations (40 in all our experiments) for each initial setting.</S>
    <S sid="157" ssid="7">For each of the M sets, the model with the best free energy of the B runs is then run out until convergence (as measured by likelihood of a held-out data set); the other models are pruned away.</S>
    <S sid="158" ssid="8">In this paper we use B = 20 and M = 50.</S>
    <S sid="159" ssid="9">For the bth setting, we draw a random sample from the prior &#175;&#952;(b).</S>
    <S sid="160" ssid="10">We set the initial Q(t) = P(t|s,&#175;&#952;(b)) which can be calculated using the Expectation-Maximization E-Step.</S>
    <S sid="161" ssid="11">Q(&#175;&#952;) is then initialized using the standard VB M-step.</S>
    <S sid="162" ssid="12">For the Lexicalized-EVG, we modify this procedure slightly, by first running MB smoothed EVG models for 40 iterations each and selecting the best model in each cohort as before; each L-EVG distribution is initialized from its corresponding EVG distribution.</S>
    <S sid="163" ssid="13">The new P(A|h, H, d, v) distributions are set initially to their corresponding P(A|H, d, v) values.</S>
  </SECTION>
  <SECTION title="6 Results" number="6">
    <S sid="164" ssid="1">We trained on the standard Penn Treebank WSJ corpus (Marcus et al., 1993).</S>
    <S sid="165" ssid="2">Following Klein and Manning (2002), sentences longer than 10 words after removing punctuation are ignored.</S>
    <S sid="166" ssid="3">We refer to this variant as WSJ10.</S>
    <S sid="167" ssid="4">Following Cohen et al. (2008), we train on sections 2-21, used 22 as a held-out development corpus, and present results evaluated on section 23.</S>
    <S sid="168" ssid="5">The models were all trained using Variational Bayes, and initialized as described in Section 5.</S>
    <S sid="169" ssid="6">To evaluate, we follow Cohen et al. (2008) in using the mean of the variational posterior Dirichlets as a point estimate &#175;&#952;&#8242;.</S>
    <S sid="170" ssid="7">For the unsmoothed models we decode by selecting the Viterbi parse given &#175;&#952;&#8242;, or argmaxtP (t|s, &#175;&#952;&#8242;).</S>
    <S sid="171" ssid="8">For the smoothed models we find the Viterbi parse of the unsmoothed CFG, but use the smoothed probabilities.</S>
    <S sid="172" ssid="9">We evaluate against the gold standard dependencies for section 23, which were extracted from the phrase structure trees using the standard rules by Yamada and Matsumoto (2003).</S>
    <S sid="173" ssid="10">We measure the percent accuracy of the directed dependency edges.</S>
    <S sid="174" ssid="11">For the lexicalized model, we replaced all words that were seen fewer than 100 times with &#8220;UNK.&#8221; We ran each of our systems 10 times, and report the average directed accuracy achieved.</S>
    <S sid="175" ssid="12">The results are shown in Table 1.</S>
    <S sid="176" ssid="13">We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S>
    <S sid="177" ssid="14">Looking at Table 1, we can first of all see the benefit of randomized initialization over the harmonic initializer for DMV.</S>
    <S sid="178" ssid="15">We can also see a large gain by adding smoothing to DMV, topping even the logistic normal prior.</S>
    <S sid="179" ssid="16">The unsmoothed EVG actually performs worse than unsmoothed DMV, but both smoothed versions improve even on smoothed DMV.</S>
    <S sid="180" ssid="17">Adding lexical information (L-EVG) yields a moderate further improvement.</S>
    <S sid="181" ssid="18">As the greatest improvement comes from moving to model EVG smoothed-skip-head, we show in Table 2 the most probable arguments for each val, dir, using the mean of the appropriate variational Dirichlet.</S>
    <S sid="182" ssid="19">For d = right, v = 1, P(A|v, d) largely seems to acts as a way of grouping together various verb types, while for d = left, v = 0 the model finds that nouns tend to act as the closest left argument. tion, according to smoothing distribution P(arg|dir, val) in EVG smoothed-skip-head model with lowest free energy.</S>
  </SECTION>
  <SECTION title="7 Conclusion" number="7">
    <S sid="183" ssid="1">We present a smoothing technique for unsupervised PCFG estimation which allows us to explore more sophisticated dependency grammars.</S>
    <S sid="184" ssid="2">Our method combines linear interpolation with a Bayesian prior that ensures the backoff distribution receives probability mass.</S>
    <S sid="185" ssid="3">Estimating the smoothed model requires running the standard Variational Bayes on an extended PCFG.</S>
    <S sid="186" ssid="4">We used this technique to estimate a series of dependency grammars which extend DMV with additional valence and lexical information.</S>
    <S sid="187" ssid="5">We found that both were helpful in learning English dependency grammars.</S>
    <S sid="188" ssid="6">Our L-EVG model gives the best reported accuracy to date on the WSJ10 corpus.</S>
    <S sid="189" ssid="7">Future work includes using lexical information more deeply in the model by conditioning argument words and valence on the lexical head.</S>
    <S sid="190" ssid="8">We suspect that successfully doing so will require using much larger datasets.</S>
    <S sid="191" ssid="9">We would also like to explore using our smoothing technique in other models such as HMMs.</S>
    <S sid="192" ssid="10">For instance, we could do unsupervised HMM part-of-speech induction by smooth a tritag model with a bitag model.</S>
    <S sid="193" ssid="11">Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S>
  </SECTION>
</PAPER>
