Improving Unsupervised Dependency Parsing with Richer Contexts and Smoothing
Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts.
Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns.
In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing.
Our model produces state-of-the-art results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points.
We use the lexical values with the frequency more than 100 and defining tied probabilistic context free grammar (PCFG) and Dirichlet priors, the accuracy is improved.
We also implement a sort of parameter tying for the E-DMV through a learning a back off distribution on child probabilities.
