<PAPER>
	<S sid="0">A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005</S><ABSTRACT>
		<S sid="1" ssid="1">We present a Chinese word seg mentation system submitted to the closed track of Sighan bakeoff 2005.</S>
		<S sid="2" ssid="2">Our segmenter was built using a condi tional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features.</S>
		<S sid="3" ssid="3">Because our morphological features were extracted from the training cor pora automatically, our system was not biased toward any particular variety of Mandarin.</S>
		<S sid="4" ssid="4">Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers.</S>
		<S sid="5" ssid="5">Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).</S>
	</ABSTRACT>
	<SECTION title="Introduction" number="1">
			<S sid="6" ssid="6">The 2005 Sighan Bakeoff included four dif ferent corpora, Academia Sinica (AS), City University of Hong Kong (HK), Peking Univer sity (PK), and Microsoft Research Asia (MSR), each of which has its own definition of a word.</S>
			<S sid="7" ssid="7">In the 2003 Sighan Bakeoff (Sproat &amp; Emer son 2003), no single model performed well on all corpora included in the task.</S>
			<S sid="8" ssid="8">Rather, systems tended to do well on corpora largely drawn from a set of similar Mandarin varieties to the one they were originally developed for.</S>
			<S sid="9" ssid="9">Across cor pora, variation is seen in both the lexicons and also in the word segmentation standards.</S>
			<S sid="10" ssid="10">We concluded that, for future systems, generaliza tion across such different Mandarin varieties is crucial.</S>
			<S sid="11" ssid="11">To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.</S>
	</SECTION>
	<SECTION title="Algorithm. " number="2">
			<S sid="12" ssid="1">Our system builds on research into condi tional random field (CRF), a statistical sequence modeling framework first introduced by Lafferty et al (2001).</S>
			<S sid="13" ssid="2">Work by Peng et al (2004) first used this framework for Chinese word segmen tation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.</S>
			<S sid="14" ssid="3">Gaussian priors were used to prevent overfitting and a quasi-Newton method was used for pa rameter optimization.</S>
			<S sid="15" ssid="4">The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: ( ) ( )??</S>
			<S sid="16" ssid="5">?Cc k c cXYkkXZ XYP f ,,exp)( 1| ??</S>
			<S sid="17" ssid="6">Y is the label sequence for the sentence, X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled.</S>
			<S sid="18" ssid="7">A CRF allows us to utilize a large number of n-gram features and different state sequence 168 based features and also provides an intuitive framework for the use of morphological features.</S>
	</SECTION>
	<SECTION title="Feature engineering. " number="3">
			<S sid="19" ssid="1">3.1 Features.</S>
			<S sid="20" ssid="2">The linguistic features used in our model fall into three categories: character identity n-grams,morphological and character reduplication fea tures.</S>
			<S sid="21" ssid="3">For each state, the character identity features (Ng &amp; Low 2004, Xue &amp; Shen 2003, Goh et al 2003) are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions.</S>
			<S sid="22" ssid="4">Specifically, we used four types of unigram feature functions, designated as C0 (current charac ter), C1 (next character), C-1 (previous character), C-2 (the character two characters back).</S>
			<S sid="23" ssid="5">Fur thermore, four types of bi-gram features were used, and are notationally designated here as conjunctions of the previously specified unigram features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0.</S>
			<S sid="24" ssid="6">Given that unknown words are normally more than one character long, when representing the morphological features as feature functions, such feature functions keyed off the morphological information extracted from both the proceeding state and the current state.</S>
			<S sid="25" ssid="7">Our morphological features are based upon the intuition re garding unknown word features given in Gao et al.</S>
			<S sid="26" ssid="8">(2004).</S>
			<S sid="27" ssid="9">Specifically, their idea was to use productive affixes and characters that only oc curred independently to predict boundaries of unknown words.</S>
			<S sid="28" ssid="10">To construct a table containing affixes of unknown words, rather than using threshold-filtered affix tables in a separate un known word model as was done in Gao et al (2004), we first extracted rare words from a corpus and then collected the first and last charac ters to construct the prefix and suffix tables.</S>
			<S sid="29" ssid="11">For the table of individual character words, we col lected an individual character word table for each corpus of the characters that always occurred alone as a separate word in the given cor pus.</S>
			<S sid="30" ssid="12">We also collected a list of bi-grams from each training corpus to distinguish known strings from unknown.</S>
			<S sid="31" ssid="13">Adopting all the features together in a model and using the automatically generated morphological tables prevented our system from manually overfitting the Mandarin varieties we are most familiar with.</S>
			<S sid="32" ssid="14">The tables are used in the following ways: 1) C-1+C0 unknown word feature functions were created for each specific pair of characters in the bi-gram tables.</S>
			<S sid="33" ssid="15">Such feature functions are active if the characters in the respective states match the corresponding feature function?s characters.</S>
			<S sid="34" ssid="16">These feature functions are designed to distinguish known strings from unknown.</S>
			<S sid="35" ssid="17">2) C-1, C0, and C1 individual character feature functions were created for each character in the individual character word table, and are likewise active if the respective character matches the feature function?s character.</S>
			<S sid="36" ssid="18">3) C-1 prefix feature functions are defined over characters in the prefix table, and fire if the character in the proceeding state matches the feature function?s character.</S>
			<S sid="37" ssid="19">4) C0 suffix feature functions are defined over suffix table characters, and fire if the char acter in the current state matches the feature function?s character.</S>
			<S sid="38" ssid="20">Additionally, we also use reduplication feature functions that are active based on the repetition of a given character.</S>
			<S sid="39" ssid="21">We used two such fea ture functions, one that fires if the previous and the current character, C-1 and C0, are identical and one that does so if the subsequent and the previous characters, C-1 and C1, are identical.</S>
			<S sid="40" ssid="22">Most features appeared in the first-order tem plates with a few of character identity features in the both zero-order and first-order templates.</S>
			<S sid="41" ssid="23">We also did normalization of punctuations due to the fact that Mandarin has a huge variety of punctuations.</S>
			<S sid="42" ssid="24">Table 1 shows the number of data features and lambda weights in each corpus.</S>
			<S sid="43" ssid="25">Table 1 The number of features in each corpus # of data features # of lambda weights AS 2,558,840 8,076,916 HK 2,308,067 7,481,164 PK 1,659,654 5,377,146 MSR 3,634,585 12,468,890 3.2 Experiments.</S>
			<S sid="44" ssid="26">3.2.1 Results on Sighan bakeoff 2003 Experiments done while developing this system showed that its performance was signifi cantly better than that of Peng et al (2004).</S>
			<S sid="45" ssid="27">As seen in Table 2, our system?s F-score was 0.863 on CTB (Chinese Treebank from Univer 169 sity of Pennsylvania) versus 0.849 F on Peng et al.</S>
			<S sid="46" ssid="28">(2004).</S>
			<S sid="47" ssid="29">We do not at present have a good understanding of which aspects of our system give it superior performance.</S>
			<S sid="48" ssid="30">Table 2 Comparisons of Peng et al (2004) and our F score on the closed track in Sighan bakeoff 2003 Sighan Bakeoff 2003 Our F-score F-score Peng et al (2004) CTB 0.863 0.849 AS 0.970 0.956 HK 0.947 0.928 PK 0.953 0.941 3.2.2 Results on Sighan bakeoff 2005 Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).</S>
			<S sid="49" ssid="31">This shows that our system successfully general ized and achieved state of the art performance on all four corpora.</S>
			<S sid="50" ssid="32">Table 3 Performance of the features cumulatively, starting with the n-gram.</S>
			<S sid="51" ssid="33">F-score AS HK PK MSR n-gram 0.943 0.946 0.950 0.961 n-gram (PU fixed) 0.953 +Unk&amp;redupl 0.947 0.943 0.950 0.964 +Unk&amp;redupl (PU fixed) 0.952 Table 3 lists our results on the four corpora.</S>
			<S sid="52" ssid="34">We give our results using just character identity based features; character identity features plus unknown words and reduplication features.</S>
			<S sid="53" ssid="35">Our unknown word features only helped on AS and MSR.</S>
			<S sid="54" ssid="36">Both of these corpora have words that have more characters than HK and PK.</S>
			<S sid="55" ssid="37">This in dicates that our unknown word features were more useful for corpora with segmentation stan dards that tend to result in longer words.</S>
			<S sid="56" ssid="38">In the HK corpus, when we added in un known word features, our performance dropped.</S>
			<S sid="57" ssid="39">However, we found that the testing data uses different punctuation than the training set.</S>
			<S sid="58" ssid="40">Our system could not distinguish new word characters from new punctuation, since having a com plete punctuation list is considered external knowledge for closed track systems.</S>
			<S sid="59" ssid="41">If the new punctuation were not unknown to us, our per formance on HK data would have gone up to 0.952 F and the unknown word features would have not hurt the system too much.</S>
			<S sid="60" ssid="42">Table 4 present recalls (R), precisions (P), f scores (F) and recalls on both unknown (Roov) and known words (Riv).</S>
			<S sid="61" ssid="43">Table 4 Detailed performances of each corpus R P F Roov Riv AS 0.950 0.943 0.947?</S>
			<S sid="62" ssid="44">0.718?</S>
			<S sid="63" ssid="45">0.960 HK 0.941 0.946 0.943?</S>
			<S sid="64" ssid="46">0.698?</S>
			<S sid="65" ssid="47">0.961 HK (PU-fix) 0.952 0.952 0.952 0.791 0.965 PK 0.946 0.954 0.950?</S>
			<S sid="66" ssid="48">0.787?</S>
			<S sid="67" ssid="49">0.956 MSR 0.962 0.966 0.964?</S>
			<S sid="68" ssid="50">0.717?</S>
			<S sid="69" ssid="51">0.968 3.3 Error analysis.</S>
			<S sid="70" ssid="52">Our system performed reasonably well on morphologically complex new words, such as ???</S>
			<S sid="71" ssid="53">(CABLE in AS) and ???</S>
			<S sid="72" ssid="54">(MUR DER CASE in PK), where ?</S>
			<S sid="73" ssid="55">(LINE) and ?(CASE) are suffixes.</S>
			<S sid="74" ssid="56">However, it over generalized to words with frequent suffixes such as ??</S>
			<S sid="75" ssid="57">(it should be ? ?</S>
			<S sid="76" ssid="58">?to burn some one?</S>
			<S sid="77" ssid="59">in PK) and ??</S>
			<S sid="78" ssid="60">(it should be?</S>
			<S sid="79" ssid="61">?to look backward?</S>
			<S sid="80" ssid="62">in PK).</S>
			<S sid="81" ssid="63">For the corpora that considered 4 character idioms as a word, our system combined most of new idioms together.</S>
			<S sid="82" ssid="64">This differs greatly from the results that one would likely obtain with a more traditional MaxMatch based technique, as such an algo rithm would segment novel idioms.</S>
			<S sid="83" ssid="65">One short coming of our system is that it is not robust enough to distinguish the difference between ordinal numbers and numbers with measure nouns.</S>
			<S sid="84" ssid="66">For example, ??</S>
			<S sid="85" ssid="67">(3rd year) and ??</S>
			<S sid="86" ssid="68">(three years) are not distinguishable to our system.</S>
			<S sid="87" ssid="69">In order to avoid this problem, it might require having more syntactic knowledge than was implicitly given in the training data.</S>
			<S sid="88" ssid="70">Finally, some errors are due to inconsistencies in the gold segmentation of non-hanzi char acter.</S>
			<S sid="89" ssid="71">For example, ?Pentium4?</S>
			<S sid="90" ssid="72">is a word, but ?PC133?</S>
			<S sid="91" ssid="73">is two words.</S>
			<S sid="92" ssid="74">Sometimes, ?8?</S>
			<S sid="93" ssid="75">is a word, but sometimes it is segmented into two words.</S>
			<S sid="94" ssid="76">170</S>
	</SECTION>
	<SECTION title="Conclusion. " number="4">
			<S sid="95" ssid="1">Our system used a conditional random field sequence model in conjunction with character identity features, morphological features and character reduplication features.</S>
			<S sid="96" ssid="2">We extracted our morphological information automatically to prevent overfitting Mandarin from particular Mandarin-speaking area.</S>
			<S sid="97" ssid="3">Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).</S>
	</SECTION>
	<SECTION title="Acknowledgment. " number="5">
			<S sid="98" ssid="1">Thanks to Kristina Toutanova for her generous help and to Jenny Rose Finkel who devel oped such a great conditional random field package.</S>
			<S sid="99" ssid="2">This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.</S>
	</SECTION>
</PAPER>
