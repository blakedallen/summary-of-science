[
  {
    "citance_No": 1, 
    "citing_paper_id": "P14-1099", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Shay B., Cohen | Michael John, Collins", 
    "raw_text": "The trees are binarized (Petrov et al, 2006) and for the EMalgorithm we use the initialization method described sec", 
    "clean_text": "The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1099", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Shay B., Cohen | Michael John, Collins", 
    "raw_text": "(2005) and Petrov et al (2006)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W10-1408", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Mohammed, Attia | Jennifer, Foster | Deirdre, Hogan | Joseph, Le Roux | Lamia, Tounsi | Josef, van Genabith", 
    "raw_text": "Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learntincrementally following an iterative process consisting of the repetition of three steps", 
    "clean_text": "Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D08-1012", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Slav, Petrov | Aria, Haghighi | Dan, Klein", 
    "raw_text": "We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods", 
    "clean_text": "We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D07-1094", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Slav, Petrov | Adam, Pauls | Dan, Klein", 
    "raw_text": "For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006)", 
    "clean_text": "For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D07-1094", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Slav, Petrov | Adam, Pauls | Dan, Klein", 
    "raw_text": "In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling ,analagous to the approach taken by Petrov et al (2006) for learning PCFGs", 
    "clean_text": "In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D07-1094", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Slav, Petrov | Adam, Pauls | Dan, Klein", 
    "raw_text": "Weap proximate the loss in data likelihood for a merge s1s2 ?swith the following likelihood ratio (Petrov et al, 2006):? (s1s2? s)=? sequences? t Pt (x, y) P (x, y). Here P (x, y) is the joint likelihood of an emission sequence x and associated state sequence y. This quantity can be recovered from the forward and backward probabilities using P (x, y)=? s: pi (s) =yt? t (s)?? t (s) .Pt (x, y) is an approximation to the same joint likelihood where states s1 and s2 are merged", 
    "clean_text": "We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N10-1095", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Mark, Johnson | Ahmet Engin, Ural", 
    "raw_text": "parser (Petrov et al, 2006)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P08-2026", 
    "citing_paper_authority": 23, 
    "citing_paper_authors": "David, McClosky | Eugene, Charniak", 
    "raw_text": "However, several very good cur rent parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006))", 
    "clean_text": "However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N12-1033", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Shane, Bergsma | Matt, Post | David, Yarowsky", 
    "raw_text": "Papers are then parsed via the Berke 1Via the open-source utility pdftotext 2Splitter from cogcomp.cs.illinois.edu/page/tools 328 Task Training Set: Dev Test Strict Lenient Set Set NativeL 2127 3963 450 477 Venue 2484 3991 400 421 Gender 2125 3497 400 409 Table 1: Number of documents for each task ley parser (Petrov et al, 2006), and part-of-speech (PoS) tagged using CRFTagger (Phan, 2006) .Training sets always comprise papers from 20012007, while test sets are created by randomly shuffling the 2008-2009 portion and then dividing it into development/test sets", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W10-1405", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Reut, Tsarfaty | Khalil, Sima'an", 
    "raw_text": "Wehave so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement", 
    "clean_text": "We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W12-3904", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Lucelene, Lopes | Roger, Granada | C&aacute;ssia, Trojahn | Carlos, Ramisch | Renata, Vieira | Aline, Villavicencio", 
    "raw_text": "Three well-known parsers were employed: Stanford parser (Klein and Manning, 2003) for texts in English, PALAVRAS (Bick, 2000) for texts in Portuguese, and Berkeley parser (Petrov et al, 2006) for texts in French", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P11-2125", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Gholamreza, Haffari | Marzieh, Razavi | Anoop, Sarkar", 
    "raw_text": "We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005) .We also provide an ensemble method for combining diverse cluster-based models", 
    "clean_text": "We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P11-2125", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Gholamreza, Haffari | Marzieh, Razavi | Anoop, Sarkar", 
    "raw_text": "In this paper, we obtain syn tactic clusters from the Berkeley parser (Petrov et al., 2006)", 
    "clean_text": "In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P11-2125", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Gholamreza, Haffari | Marzieh, Razavi | Anoop, Sarkar", 
    "raw_text": "Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006)", 
    "clean_text": "Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P10-1017", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Jason, Riesa | Daniel, Marcu", 
    "raw_text": "Togener ate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree", 
    "clean_text": "To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P11-1048", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Michael, Auli | Adam, Lopez", 
    "raw_text": "This enables us to com pare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank", 
    "clean_text": "This enables us to compare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P13-2110", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Zhiguo, Wang | Chengqing, Zong | Nianwen, Xue", 
    "raw_text": "We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006)", 
    "clean_text": "We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "C10-1151", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Muhua, Zhu | Jingbo, Zhu | Tong, Xiao", 
    "raw_text": "In contrast 1345 to previous work on n-best combination where multiple parsers, say, Collins parser (Collins, 1999) and Berkeley parser (Petrov et al, 2006) are trained on the same training data, n-best combination for heterogeneous parsing is instead allowed to use either a single parser or multiple parsers which are trained on heterogeneoustreebanks", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D07-1014", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "David A., Smith | Noah A., Smith", 
    "raw_text": "In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. Given the relative simplicity of clustering methods for context-free parsing to date (gains were found just by using Expectation-Maximization), we believe the fundamental reason clustering was not particularly helpful here is a structural one", 
    "clean_text": "In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate.", 
    "keep_for_gold": 0
  }
]
