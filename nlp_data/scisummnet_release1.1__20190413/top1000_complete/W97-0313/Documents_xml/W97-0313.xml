<PAPER>
  <S sid="0">A Corpus-Based Approach For Building Semantic Lexicons</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application.</S>
    <S sid="2" ssid="2">Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic.</S>
    <S sid="3" ssid="3">In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories.</S>
    <S sid="4" ssid="4">The input to the system is a small set of seed words for a category and a representative text corpus.</S>
    <S sid="5" ssid="5">The output is a ranked list of words that are associated with the category.</S>
    <S sid="6" ssid="6">A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon.</S>
    <S sid="7" ssid="7">In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="8" ssid="1">Semantic information can be helpful in almost all aspects of natural language understanding, including word sense disambiguation, selectional restrictions, attachment decisions, and discourse processing.</S>
    <S sid="9" ssid="2">Semantic knowledge can add a great deal of power and accuracy to natural language processing systems.</S>
    <S sid="10" ssid="3">But semantic information is difficult to obtain.</S>
    <S sid="11" ssid="4">In most cases, semantic knowledge is encoded manually for each application.</S>
    <S sid="12" ssid="5">There have been a few large-scale efforts to create broad semantic knowledge bases, such as WordNet (Miller, 1990) and Cyc (Lenat, Prakash, and Shepherd, 1986).</S>
    <S sid="13" ssid="6">While these efforts may be useful for some applications, we believe that they will never fully satisfy the need for semantic knowledge.</S>
    <S sid="14" ssid="7">Many domains are characterized by their own sublanguage containing terms and jargon specific to the field.</S>
    <S sid="15" ssid="8">Representing all sublanguages in a single knowledge base would be nearly impossible.</S>
    <S sid="16" ssid="9">Furthermore, domain-specific semantic lexicons are useful for minimizing ambiguity problems.</S>
    <S sid="17" ssid="10">Within the context of a restricted domain, many polysemous words have a strong preference for one word sense, so knowing the most probable word sense in a domain can strongly constrain the ambiguity.</S>
    <S sid="18" ssid="11">We have been experimenting with a corpusbased method for building semantic lexicons semiautomatically.</S>
    <S sid="19" ssid="12">Our system uses a text corpus and a small set of seed words for a category to identify other words that also belong to the category.</S>
    <S sid="20" ssid="13">The algorithm uses simple statistics and a bootstrapping mechanism to generate a ranked list of potential category words.</S>
    <S sid="21" ssid="14">A human then reviews the top words and selects the best ones for the dictionary.</S>
    <S sid="22" ssid="15">Our approach is geared toward fast semantic lexicon construction: given a handful of seed words for a category and a representative text corpus, one can build a semantic lexicon for a category in just a few minutes.</S>
    <S sid="23" ssid="16">In the first section, we describe the statistical bootstrapping algorithm for identifying candidate category words and ranking them.</S>
    <S sid="24" ssid="17">Next, we describe experimental results for five categories.</S>
    <S sid="25" ssid="18">Finally, we discuss our experiences with additional categories and seed word lists, and summarize our results.</S>
  </SECTION>
  <SECTION title="2 Generating a Semantic Lexicon" number="2">
    <S sid="26" ssid="1">Our work is based on the observation that category members are often surrounded by other category members in text, for example in conjunctions (lions and tigers and bears), lists (lions, tigers, bears...), appositives (the stallion, a white Arabian), and nominal compounds (Arabian stallion; tuna fish).</S>
    <S sid="27" ssid="2">Given a few category members, we wondered whether it would be possible to collect surrounding contexts and use statistics to identify other words that also belong to the category.</S>
    <S sid="28" ssid="3">Our approach was motivated by Yarowsky's word sense disambiguation algorithm (Yarowsky, 1992) and the notion of statistical salience, although our system uses somewhat different statistical measures and techniques.</S>
    <S sid="29" ssid="4">We begin with a small set of seed words for a category.</S>
    <S sid="30" ssid="5">We experimented with different numbers of seed words, but were surprised to find that only 5 seed words per category worked quite well.</S>
    <S sid="31" ssid="6">As an example, the seed word lists used in our experiments are shown below.</S>
    <S sid="32" ssid="7">Energy: fuel gas gasoline oil power Financial: bank banking currency dollar money Military: army commander infantry soldier troop Vehicle: airplane car jeep plane truck Weapon: bomb dynamite explosives gun rifle The input to our system is a text corpus and an initial set of seed words for each category.</S>
    <S sid="33" ssid="8">Ideally, the text corpus should contain many references to the category.</S>
    <S sid="34" ssid="9">Our approach is designed for domainspecific text processing, so the text corpus should be a representative sample of texts for the domain and the categories should be semantic classes associated with the domain.</S>
    <S sid="35" ssid="10">Given a text corpus and an initial seed word list for a category C, the algorithm for building a semantic lexicon is as follows: The context windows do not cut across sentence boundaries.</S>
    <S sid="36" ssid="11">Note that our context window is much narrower than those used by other researchers (Yarowsky, 1992).</S>
    <S sid="37" ssid="12">We experimented with larger window sizes and found that the narrow windows more consistently included words related to the target category.</S>
    <S sid="38" ssid="13">Note that this is not exactly a conditional probability because a single word occurrence can belong to more than one context window.</S>
    <S sid="39" ssid="14">For example, consider the sentence: I bought an AK-47 gun and an M-16 rifle.</S>
    <S sid="40" ssid="15">The word M-16 would be in the context windows for both gun and rifle even though there was just one occurrence of it in the sentence.</S>
    <S sid="41" ssid="16">Consequently, the category score for a word can be greater than 1.</S>
    <S sid="42" ssid="17">4.</S>
    <S sid="43" ssid="18">Next, we remove stopwords, numbers, and any words with a corpus frequency &lt; 5.</S>
    <S sid="44" ssid="19">We used a stopword list containing about 30 general nouns, mostly pronouns (e.g., I, he, she, they) and determiners (e.g., this, that, those).</S>
    <S sid="45" ssid="20">The stopwords and numbers are not specific to any category and are common across many domains, so we felt it was safe to remove them.</S>
    <S sid="46" ssid="21">The remaining nouns are sorted by category score and ranked so that the nouns most strongly associated with the category appear at the top.</S>
    <S sid="47" ssid="22">5.</S>
    <S sid="48" ssid="23">The top five nouns that are not already seed words are added to the seed word list dynamically.</S>
    <S sid="49" ssid="24">We then go back to Step 1 and repeat the process.</S>
    <S sid="50" ssid="25">This bootstrapping mechanism dynamically grows the seed word list so that each iteration produces a larger category context.</S>
    <S sid="51" ssid="26">In our experiments, the top five nouns were added automatically without any human intervention, but this sometimes allows non-category words to dilute the growing seed word list.</S>
    <S sid="52" ssid="27">A few inappropriate words are not likely to have much impact, but many inappropriate words or a few highly frequent words can weaken the feedback process.</S>
    <S sid="53" ssid="28">One could have a person verify that each word belongs to the target category before adding it to the seed word list, but this would require human interaction at each iteration of the feedback cycle.</S>
    <S sid="54" ssid="29">We decided to see how well the technique could work without this additional human interaction, but the potential benefits of human feedback still need to be investigated.</S>
    <S sid="55" ssid="30">After several iterations, the seed word list typically contains many relevant category words.</S>
    <S sid="56" ssid="31">But more importantly, the ranked list contains many additional category words, especially near the top.</S>
    <S sid="57" ssid="32">The number of iterations can make a big difference in the quality of the ranked list.</S>
    <S sid="58" ssid="33">Since new seed words are generated dynamically without manual review, the quality of the ranked list can deteriorate rapidly when too many non-category words become seed words.</S>
    <S sid="59" ssid="34">In our experiments, we found that about eight iterations usually worked well.</S>
    <S sid="60" ssid="35">The output of the system is the ranked list of nouns after the final iteration.</S>
    <S sid="61" ssid="36">The seed word list is thrown away.</S>
    <S sid="62" ssid="37">Note that the original seed words were already known to be category members, and the new seed words are already in the ranked list because that is how they were selected.2 Finally, a user must review the ranked list and identify the words that are true category members.</S>
    <S sid="63" ssid="38">How one defines a &amp;quot;true&amp;quot; category member is subjective and may depend on the specific application, so we leave this exercise to a person.</S>
    <S sid="64" ssid="39">Typically, the words near the top of the ranked list are highly associated with the category but the density of category words decreases as one proceeds down the list.</S>
    <S sid="65" ssid="40">The user may scan down the list until a sufficient number of category words is found, or as long as time permits.</S>
    <S sid="66" ssid="41">The words selected by the user are added to a permanent semantic lexicon with the appropriate category label.</S>
    <S sid="67" ssid="42">Our goal is to allow a user to build a semantic lexicon for one or more categories using only a small set of known category members as seed words and a text corpus.</S>
    <S sid="68" ssid="43">The output is a ranked list of potential category words that a user can review to create a semantic lexicon quickly.</S>
    <S sid="69" ssid="44">The success of this approach depends on the quality of the ranked list, especially the density of category members near the top.</S>
    <S sid="70" ssid="45">In the next section, we describe experiments to evaluate our system.</S>
    <S sid="71" ssid="46">2It is possible that a word may be near the top of the ranked list during one iteration (and subsequently become a seed word) but become buried at the bottom of the ranked list during later iterations.</S>
    <S sid="72" ssid="47">However, we have not observed this to be a problem so far.</S>
  </SECTION>
  <SECTION title="3 Experimental Results" number="3">
    <S sid="73" ssid="1">We performed experiments with five categories to evaluate the effectiveness and generality of our approach: energy, financial, military, vehicles, and weapons.</S>
    <S sid="74" ssid="2">The MUC-4 development corpus (1700 texts) was used as the text corpus (MUC-4 Proceedings, 1992).</S>
    <S sid="75" ssid="3">We chose these five categories because they represented relatively different semantic classes, they were prevalent in the MUC-4 corpus, and they seemed to be useful categories.</S>
    <S sid="76" ssid="4">For each category, we began with the seed word lists shown in Figure 1.</S>
    <S sid="77" ssid="5">We ran the bootstrapping algorithm for eight iterations, adding five new words to the seed word list after each cycle.</S>
    <S sid="78" ssid="6">After the final iteration, we had ranked lists of potential category words for each of the five categories.</S>
    <S sid="79" ssid="7">The top 45 words3 from each ranked list are shown in Figure 2.</S>
    <S sid="80" ssid="8">While the ranked lists are far from perfect, one can see that there are many category members near the top of each list.</S>
    <S sid="81" ssid="9">It is also apparent that a few additional heuristics could be used to remove many of the extraneous words.</S>
    <S sid="82" ssid="10">For example, our number processor failed to remove numbers with commas (e.g., 2,000).</S>
    <S sid="83" ssid="11">And the military category contains several ordinal numbers (e.g., 10th 3rd 1st) that could be easily identified and removed.</S>
    <S sid="84" ssid="12">But the key question is whether the ranked list contains many true category members.</S>
    <S sid="85" ssid="13">Since this is a subjective question, we set up an experiment involving human judges.</S>
    <S sid="86" ssid="14">For each category, we selected the top 200 words from its ranked list and presented them to a user.</S>
    <S sid="87" ssid="15">We presented the words in random order so that the user had no idea how our system had ranked the words.</S>
    <S sid="88" ssid="16">This was done to minimize contextual effects (e.g., seeing five category members in a row might make someone more inclined to judge the next word as relevant).</S>
    <S sid="89" ssid="17">Each category was judged by two people independently.4 The judges were asked to rate each word on a scale from 1 to 5 indicating how strongly it was associated with the category.</S>
    <S sid="90" ssid="18">Since category judgements can be highly subjective, we gave them guidelines to help establish uniform criteria.</S>
    <S sid="91" ssid="19">The instructions that were given to the judges are shown in Figure 3.</S>
    <S sid="92" ssid="20">We asked the judges to rate the words on a scale from 1 to 5 because different degrees of category membership might be acceptable for different applications.</S>
    <S sid="93" ssid="21">Some applications might require strict cat3 Note that some of these words are not nouns, such as boarded and U.S.-made.</S>
    <S sid="94" ssid="22">Our parser tags unknown words as nouns, so sometimes unknown words are mistakenly selected for context windows.</S>
    <S sid="95" ssid="23">'The judges were members of our research group but not the authors. aLimon-Covenas refers to an oil pipeline. aLa_Aurora refers to an airport.</S>
    <S sid="96" ssid="24">CRITERIA: On a scale of 0 to 5, rate each word's strength of association with the given category using the following criteria.</S>
    <S sid="97" ssid="25">We'll use the category ANIMAL as an example.</S>
    <S sid="98" ssid="26">5: CORE MEMBER OF THE CATEGORY: If a word is clearly a member of the category, then it deserves a 5.</S>
    <S sid="99" ssid="27">For example, dogs and sparrows are members of the ANIMAL category.</S>
  </SECTION>
  <SECTION title="4: SUBPART OF MEMBER OF THE CATEGORY:" number="4">
    <S sid="100" ssid="1">If a word refers to a part of something that is a member of the category, then it deserves a 4.</S>
    <S sid="101" ssid="2">For example, feathers and tails are parts of ANIMALS.</S>
  </SECTION>
  <SECTION title="3: STRONGLY ASSOCIATED WITH THE CATEGORY:" number="5">
    <S sid="102" ssid="1">If a word refers to something that is strongly associated with members of the category, but is not actually a member of the category itself, then it deserves a 3.</S>
    <S sid="103" ssid="2">For example, zoos and nests are strongly associated with ANIMALS.</S>
  </SECTION>
  <SECTION title="2: WEAKLY ASSOCIATED WITH THE CATEGORY:" number="6">
    <S sid="104" ssid="1">If a word refers to something that can be associated with members of the category, but is also associated with many other types of things, then it deserves a 2.</S>
    <S sid="105" ssid="2">For example, bowls and parks are weakly associated with ANIMALS.</S>
    <S sid="106" ssid="3">1: NO ASSOCIATION WITH THE CATEGORY: If a word has virtually no association with the category, then it deserves a 1.</S>
    <S sid="107" ssid="4">For example, tables and moons have virtually no association with ANIMALS.</S>
    <S sid="108" ssid="5">0: UNKNOWN WORD: If you do not know what a word means, then it should be labeled with a 0.</S>
    <S sid="109" ssid="6">IMPORTANT!</S>
    <S sid="110" ssid="7">Many words have several distinct meanings.</S>
    <S sid="111" ssid="8">For example, the word &amp;quot;horse&amp;quot; can refer to an animal, a piece of gymnastics equipment, or it can mean to fool around (e.g., &amp;quot;Don't horse around!&amp;quot;).</S>
    <S sid="112" ssid="9">If a word has ANY meaning associated with the given category, then only consider that meaning when assigning numbers.</S>
    <S sid="113" ssid="10">For example, the word &amp;quot;horse&amp;quot; would be a 5 because one of its meanings refers to an ANIMAL. egory membership, for example only words like gun, rifle, and bomb should be labeled as weapons.</S>
    <S sid="114" ssid="11">But from a practical perspective, subparts of category members might also be acceptable.</S>
    <S sid="115" ssid="12">For example, if a cartridge or trigger is mentioned in the context of an event, then one can infer that a gun was used.</S>
    <S sid="116" ssid="13">And for some applications, any word that is strongly associated with a category might be useful to include in the semantic lexicon.</S>
    <S sid="117" ssid="14">For example, words like ammunition or bullets are highly suggestive of a weapon.</S>
    <S sid="118" ssid="15">In the UMass/MUC-4 information extraction system (Lehnert et al., 1992), the words ammunition and bullets were defined as weapons, mainly for the purpose of selectional restrictions.</S>
    <S sid="119" ssid="16">The human judges estimated that it took them approximately 10-15 minutes, on average, to judge the 200 words for each category.</S>
    <S sid="120" ssid="17">Since the instructions allowed the users to assign a zero to a word if they did not know what it meant, we manually removed the zeros and assigned ratings that we thought were appropriate.</S>
    <S sid="121" ssid="18">We considered ignoring the zeros, but some of the categories would have been severely impacted.</S>
    <S sid="122" ssid="19">For example, many of the legitimate weapons (e.g., M-16 and AR-15) were not known to the judges.</S>
    <S sid="123" ssid="20">Fortunately, most of the unknown words were proper nouns with relatively unambiguous semantics, so we do not believe that this process compromised the integrity of the experiment.</S>
    <S sid="124" ssid="21">Finally, we graphed the results from the human judges.</S>
    <S sid="125" ssid="22">We counted the number of words judged as 5's by either judge, the number of words judged as 5's or 4's by either judge, the number of words judged as 5's, 4's, or 3's by either judge, and the number of words judged as either 5's, 4's, 3's, or 2's.</S>
    <S sid="126" ssid="23">We plotted the results after each 20 words, stepping down the ranked list, to see whether the words near the top of the list were more highly associated with the category than words farther down.</S>
    <S sid="127" ssid="24">We also wanted to see whether the number of category words leveled off or whether it continued to grow.</S>
    <S sid="128" ssid="25">The results from this experiment are shown in Figures 4-8.</S>
    <S sid="129" ssid="26">With the exception of the Energy category, we were able to find 25-45 words that were judged as 4's or 5's for each category.</S>
    <S sid="130" ssid="27">This was our strictest test because only true category members (or subparts of true category members) earned this rating.</S>
    <S sid="131" ssid="28">Although this might not seem like a lot of category words, 25-45 words is enough to produce a reasonable core semantic lexicon.</S>
    <S sid="132" ssid="29">For example, the words judged as 5's for each category are shown in Figure 9.</S>
    <S sid="133" ssid="30">Figure 9 illustrates an important benefit of the corpus-based approach.</S>
    <S sid="134" ssid="31">By sifting through a large text corpus, the algorithm can find many relevant category words that a user would probably not enter in a semantic lexicon on their own.</S>
    <S sid="135" ssid="32">For example, suppose a user wanted to build a dictionary of Vehicle words.</S>
    <S sid="136" ssid="33">Most people would probably define words such as car, truck, plane, and automobile.</S>
    <S sid="137" ssid="34">But it is doubtful that most people would think of words like gunships, fighter, carrier, and ambulances.</S>
    <S sid="138" ssid="35">The corpus-based algorithm is especially good at identifying words that are common in the text corpus even though they might not be commonly used in general.</S>
    <S sid="139" ssid="36">As another example, specific types of weapons (e.g., M-16, AR-15, M-60, or M-79) might not even be known to most users, but they are abundant in the MUC-4 corpus.</S>
    <S sid="140" ssid="37">If we consider all the words rated as 3's, 4's, or 5's, then we were able to find about 50-65 words for every category except Energy.</S>
    <S sid="141" ssid="38">Many of these words would be useful in a semantic dictionary for the category.</S>
    <S sid="142" ssid="39">For example, some of the words rated as 3's for the Vehicle category include: flight, flights, aviation, pilot, airport, and highways.</S>
    <S sid="143" ssid="40">Most of the words rated as 2's are not specific to the target category, but some of them might be useful for certain tasks.</S>
    <S sid="144" ssid="41">For example, some words judged as 2's for the Energy category are: spill, pole, tower, and fields.</S>
    <S sid="145" ssid="42">These words may appear in many different contexts, but in texts about Energy topics these words are likely to be relevant and probably should be defined in the dictionary.</S>
    <S sid="146" ssid="43">Therefore we expect that a user would likely keep some of these words in the semantic lexicon but would probably be very selective.</S>
    <S sid="147" ssid="44">Finally, the graphs show that most of the acquisition curves displayed positive slopes even at the end of the 200 words.</S>
    <S sid="148" ssid="45">This implies that more category words would likely have been found if the users had reviewed more than 200 words.</S>
    <S sid="149" ssid="46">The one exception, again, was the Energy category, which we will discuss in the next section.</S>
    <S sid="150" ssid="47">The size of the ranked lists ranged from 442 for the financial category to 919 for the military category, so it would be interesting to know how many category members would have been found if we had given the entire lists to our judges.</S>
  </SECTION>
  <SECTION title="4 Selecting Categories and Seed Words" number="7">
    <S sid="151" ssid="1">When we first began this work, we were unsure about what types of categories would be amenable to this approach.</S>
    <S sid="152" ssid="2">So we experimented with a number of different categories.</S>
    <S sid="153" ssid="3">Fortunately, most of them worked fairly well, but some of them did not.</S>
    <S sid="154" ssid="4">We do not claim to understand exactly what types of categories will work well and which ones will not, but our early experiences did shed some light on the strengths and weaknesses of this approach.</S>
    <S sid="155" ssid="5">In addition to the previous five categories, we also experimented with categories for Location, Commercial, and Person.</S>
    <S sid="156" ssid="6">The Location category performed very well using seed words such as city, town, and province.</S>
    <S sid="157" ssid="7">We didn't formally evaluate this category because most of the category words were proper nouns and we did not expect that our judges would know what they were.</S>
    <S sid="158" ssid="8">But it is worth noting that this category achieved good results, presumably because location names often cluster together in appositives, conjunctions, and nominal compounds.</S>
    <S sid="159" ssid="9">For the Commercial category, we chose seed words such as store, shop, and market.</S>
    <S sid="160" ssid="10">Only a few new commercial words were identified, such as hotel and restaurant.</S>
    <S sid="161" ssid="11">In retrospect, we realized that there were probably few words in the MUC-4 corpus that referred to commercial establishments.</S>
    <S sid="162" ssid="12">(The MUC-4 corpus mainly contains reports of terrorist and military events.)</S>
    <S sid="163" ssid="13">The relatively poor performance of the Energy category was probably due to the same problem.</S>
    <S sid="164" ssid="14">If a category is not well-represented in the corpus then it is doomed because inappropriate words become seed words in the early iterations and quickly derail the feedback loop.</S>
    <S sid="165" ssid="15">The Person category produced mixed results.</S>
    <S sid="166" ssid="16">Some good category words were found, such as rebel, advisers, criminal, and citizen.</S>
    <S sid="167" ssid="17">But many of the words referred to organizations (e.g., FMLN), groups (e.g., forces), and actions (e.g., attacks).</S>
    <S sid="168" ssid="18">Some of these words seemed reasonable, but it was hard to draw a line between specific references to people and concepts like organizations and groups that may or may not consist entirely of people.</S>
    <S sid="169" ssid="19">The large proportion of action words also diluted the list.</S>
    <S sid="170" ssid="20">More experiments are needed to better understand whether this category is inherently difficult or whether a more carefully chosen set of seed words would improve performance.</S>
    <S sid="171" ssid="21">More experiments are also needed to evaluate different seed word lists.</S>
    <S sid="172" ssid="22">The algorithm is clearly sensitive to the initial seed words, but the degree of sensitivity is unknown.</S>
    <S sid="173" ssid="23">For the five categories reported in this paper, we arbitrarily chose a few words that were central members of the category.</S>
    <S sid="174" ssid="24">Our initial seed words worked well enough that we did not experiment with them very much.</S>
    <S sid="175" ssid="25">But we did perform a few experiments varying the number of seed words.</S>
    <S sid="176" ssid="26">In general, we found that additional seed words tend to improve performance, but the results were not substantially different using five seed words or using ten.</S>
    <S sid="177" ssid="27">Of course, there is also a law of diminishing returns: using a seed word list containing 60 category words is almost like creating a semantic lexicon for the category by hand!</S>
  </SECTION>
  <SECTION title="5 Conclusions" number="8">
    <S sid="178" ssid="1">Building semantic lexicons will always be a subjective process, and the quality of a semantic lexicon is highly dependent on the task for which it will be used.</S>
    <S sid="179" ssid="2">But there is no question that semantic knowledge is essential for many problems in natural language processing.</S>
    <S sid="180" ssid="3">Most of the time semantic knowledge is defined manually for the target application, but several techniques have been developed for generating semantic knowledge automatically.</S>
    <S sid="181" ssid="4">Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994)).</S>
    <S sid="182" ssid="5">Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993)).</S>
    <S sid="183" ssid="6">Our task orientation is a bit different because we are trying to construct a semantic lexicon for a target category, instead of classifying unknown or polysemous words in context.</S>
    <S sid="184" ssid="7">To our knowledge, our system is the first one aimed at building semantic lexicons from raw text without using any additional semantic knowledge.</S>
    <S sid="185" ssid="8">The only lexical knowledge used by our parser is a part-of-speech dictionary for syntactic processing.</S>
    <S sid="186" ssid="9">Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993)).</S>
    <S sid="187" ssid="10">Our corpus-based approach is designed to support fast semantic lexicon construction.</S>
    <S sid="188" ssid="11">A user only needs to supply a representative text corpus and a small set of seed words for each target category.</S>
    <S sid="189" ssid="12">Our experiments suggest that a core semantic lexicon can be built for each category with only 10-15 minutes of human interaction.</S>
    <S sid="190" ssid="13">While more work needs to be done to refine this procedure and characterize the types of categories it can handle, we believe that this is a promising approach for corpus-based semantic knowledge acquisition.</S>
  </SECTION>
  <SECTION title="6 Acknowledgments" number="9">
    <S sid="191" ssid="1">This research was funded by NSF grant IRI-9509820 and the University of Utah Research Committee.</S>
    <S sid="192" ssid="2">We would like to thank David Bean, Jeff Lorenzen, and Kiri Wagstaff for their help in judging our category lists.</S>
  </SECTION>
</PAPER>
