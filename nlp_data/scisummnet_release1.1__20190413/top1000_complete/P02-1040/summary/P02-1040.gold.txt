Bleu: A Method For Automatic Evaluation Of Machine Translation
Human evaluations of machine translation are extensive but expensive.
Human evaluations can take months to finish and involve human labor that can not be reused.
We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
BLEU is a system for automatic evaluation of machine translation.
BLEU is based on measuring string level similarity between the reference translation and translation hypothesis.
