[
  {
    "citance_No": 1, 
    "citing_paper_id": "P02-1039", 
    "citing_paper_authority": 30, 
    "citing_paper_authors": "Kenji, Yamada | Kevin, Knight", 
    "raw_text": "As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002)", 
    "clean_text": "As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-2122", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Xiaolin, Wang | Masao, Utiyama | Andrew, Finch | Eiichiro, Sumita", 
    "raw_text": "For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality", 
    "clean_text": "For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N12-1026", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Taro, Watanabe", 
    "raw_text": "One of the standards for such tuning is minimum error rate training (MERT) (Och,2003), which directly minimize the loss of translation evaluation measures ,i.e. BLEU (Papineni et al,2002)", 
    "clean_text": "One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P10-2026", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Zhiyang, Wang | Yajuan, L&uuml; | Qun, Liu | Young-Sook, Hwang", 
    "raw_text": "We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003)", 
    "clean_text": "We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W10-1734", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Fabienne, Fritzinger | Alexander, Fraser", 
    "raw_text": "The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12", 
    "clean_text": "The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W03-0501", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "Bonnie Jean, Dorr | David, Zajic | Richard M., Schwartz", 
    "raw_text": "BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation", 
    "clean_text": "BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "N07-2006", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Matthias, Eck | Stephan, Vogel | Alex, Waibel", 
    "raw_text": "The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence inter val of [57.13, 61.09]", 
    "clean_text": "The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09].", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P03-1057", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Kenji, Imamura | Eiichiro, Sumita | Yuji, Matsumoto", 
    "raw_text": "We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper.BLEU measures the similarity between MT results and translation results made by humans (called 1In this paper, the number of rules denotes the number of unique pairs of source patterns and target patterns", 
    "clean_text": "We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N07-1022", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Yuk Wah, Wong | Raymond J., Mooney", 
    "raw_text": "We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002)", 
    "clean_text": "We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W07-0704", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Kemal, Oflazer | Ilknur, Durgar El-Kahlout", 
    "raw_text": "We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference transla tion.5The training set in the first row of 1 was limited to sentences on the Turkish side which had at most 90 tokens (root sand bound morphemes) in total in order to comply with requirements of the GIZA++ alignment tool", 
    "clean_text": "We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W11-2111", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Kristen, Parton | Joel R., Tetreault | Nitin, Madnani | Martin, Chodorow", 
    "raw_text": "BLEU (Papineni et al, 2002): Case-insensitive", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D10-1092", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Hideki, Isozaki | Tsutomu, Hirao | Kevin, Duh | Katsuhito, Sudoh | Hajime, Tsukada", 
    "raw_text": "BLEU (Papineni et al, 2002b; Papineni et al,2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric", 
    "clean_text": "BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W08-0312", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Abhaya, Agarwal | Alon, Lavie", 
    "raw_text": "Flexible Matching Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. Most of them, however, depend on finding exact matches between the words in two strings", 
    "clean_text": "Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W07-0409", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "H&eacute;l&egrave;ne, Bonneau-Maynard | Alexandre, Allauzen | Daniel, D&eacute;chelotte | Holger, Schwenk", 
    "raw_text": "A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002)", 
    "clean_text": "A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "C10-2144", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Yulia, Tsvetkov | Shuly, Wintner", 
    "raw_text": "The results show a statistically-significant (p &lt; 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores", 
    "clean_text": "The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W12-0113", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Santanu, Pal | Sivaji, Bandyopadhyay", 
    "raw_text": "Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002)", 
    "clean_text": "Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D10-1049", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Gabor, Angeli | Percy, Liang | Dan, Klein", 
    "raw_text": "Automatic Evaluation To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output", 
    "clean_text": "To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W04-1014", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Chiori, Hori | Tsutomu, Hirao | Hideki, Isozaki", 
    "raw_text": "To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation", 
    "clean_text": "To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "D10-1063", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mark, Hopkins | Greg, Langmead", 
    "raw_text": "For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output", 
    "clean_text": "For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P12-1003", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Prasanth, Kolachina | Nicola, Cancedda | Marc, Dymetman | Sriram, Venkatapathy", 
    "raw_text": "For acer tain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi", 
    "clean_text": "For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi.", 
    "keep_for_gold": 0
  }
]
