Two Languages are Better than One (for Syntactic Parsing)
We show that jointly parsing a bitext can substantially improve parse quality on both sides.
In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them.
Features include monolingual parse scores and various measures of syntactic divergence.
Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables.
The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F at predicting English side trees and 1.8 F at predicting Chinese side trees (the highest published numbers on these corpora).
Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.
In bitext parsing, we use feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy.
We use word alignment density features which measure how well the aligned entity pair matches up with alignments from an independent word aligner.
