Domain Adaptation With Structural Correspondence Learning
Discriminative learning methods are widely used in natural language processing.
These methods work best when their training and test data are drawn from the same distribution.
For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent.
In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain.
We introduce structural correspondence learning to automatically induce correspondences among features from different domains.
We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.
Following (Blitzer et al, 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al, 2005).
Our approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data.
We introduce SCL that is one feature representation approach that has been effective on certain high-dimensional NLP problems, including part-of-speech tagging and sentiment classification.
We apply the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP.
We append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain.
