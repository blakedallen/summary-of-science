Supervised And Unsupervised Learning For Sentence Compression
In Statistics-Based Summarization - Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.
The main difficulty in using this method is the lack of data; Knight and Marcu use a corpus of 1035 training sentences.
More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task.
Finally, we point out problems with modeling the task in this way.
They suggest areas for future research.
We approximate the rules of compression from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions.
We argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones.
We show that applying handcrafted rules for trimming sentences can improve both content and linguistic quality.
