<PAPER>
  <S sid="0">Pseudo-Projectivity A Polynomially Parsable Non-Projective Dependency Grammar</S>
  <ABSTRACT/>
  <SECTION title="1 Introduction" number="1">
    <S sid="1" ssid="1">Dependency grammar has a long tradition in syntactic theory, dating back to at least Tesniere's work from the thirties.'</S>
    <S sid="2" ssid="2">Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not.</S>
    <S sid="3" ssid="3">One problem that has posed an impediment to more wide-spread acceptance of dependency grammars is the fact that there is no computationally tractable version of dependency grammar which is not restricted to projective analyses.</S>
    <S sid="4" ssid="4">However, it is well known that there are some syntactic phenomena (such as wh-movement in English or clitic climbing in Romance) that require nonprojective analyses.</S>
    <S sid="5" ssid="5">In this paper, we present a form of projectivity which we call pseudoprojectivity, and we present a generative stringrewriting formalism that can generate pseudoprojective analyses and which is polynomially parsable.</S>
    <S sid="6" ssid="6">The paper is structured as follows.</S>
    <S sid="7" ssid="7">In Section 2, we introduce our notion of pseudoprojectivity.</S>
    <S sid="8" ssid="8">We briefly review a previously proposed formalization of projective dependency grammars in Section 3.</S>
    <S sid="9" ssid="9">In Section 4, we extend this formalism to handle pseudo-projectivity.</S>
    <S sid="10" ssid="10">We informally present a parser in Section 5.</S>
  </SECTION>
  <SECTION title="2 Linear and Syntactic Order of a Sentence" number="2">
    <S sid="11" ssid="1">We will use the following terminology and notation in this paper.</S>
    <S sid="12" ssid="2">The hierarchical order (dominance) between the nodes of a tree T will be represented with the symbol -&lt;T and Whenever they are unambiguous, the notations -&lt; and -&#8249; will be used.</S>
    <S sid="13" ssid="3">When x y, we will say that x is a descendent of y and y an ancestor of x.</S>
    <S sid="14" ssid="4">The projection of a node x, belonging to a tree T, is the set of the nodes y of T such that y x.</S>
    <S sid="15" ssid="5">An arc between two nodes y and x of a tree T, directed from y to x will be noted either (y, x) or The node x will be referred to as the dependent and y as the governor.</S>
    <S sid="16" ssid="6">The latter will be noted, when convenient, X+T (X+ when unambiguous).</S>
    <S sid="17" ssid="7">The notations t- and x+ are unambiguous because a node x has at most one governor in a tree.</S>
    <S sid="18" ssid="8">As usual, an ordered tree is a tree enriched with a linear order over the set of its nodes.</S>
    <S sid="19" ssid="9">Finally, if 1 is an arc of an ordered tree T, then Supp(1) represents the support of 1, i.e. the set of the nodes of T situated between the extremities of 1, extremities included.</S>
    <S sid="20" ssid="10">We will say that the elements of Supp(1) are covered by 1.</S>
    <S sid="21" ssid="11">The notion of projectivity was introduced by (Lecerf, 1960) and has received several different definitions since then.</S>
    <S sid="22" ssid="12">The definition given here is borrowed from (Marcus, 1965) and (Robinson, 1970): Definition: An arc t- is projective if and only if for every y covered by y x+ .</S>
    <S sid="23" ssid="13">A tree T is projective if and only if every arc of T is projective A projective tree has been represented in Figure 1.</S>
    <S sid="24" ssid="14">A projective dependency tree can be associated with a phrase structure tree whose constituents are the projections of the nodes of the dependency tree.</S>
    <S sid="25" ssid="15">Projectivity is therefore equivalent, in phrase structure markers, to continuity of constituent.</S>
    <S sid="26" ssid="16">The strong constraints introduced by the projectivity property on the relationship between hierarchical order and linear order allow us to describe word order of a projective dependency tree at a local level: in order to describe the linear position of a node, it is sufficient to describe its position towards its governor and sister nodes.</S>
    <S sid="27" ssid="17">The domain of locality of the linear order rules is therefore limited to a subtree of depth equal to one.</S>
    <S sid="28" ssid="18">It can be noted that this domain of locality is equal to the domain of locality of sub-categorization rules.</S>
    <S sid="29" ssid="19">Both rules can therefore be represented together as in (Gaffman, 1965) or separately as will be proposed in 3.</S>
    <S sid="30" ssid="20">Although most linguistic structures can be represented as projective trees, it is well known that projectivity is too strong a constraint for dependency trees, as shown by the example of Figure 2, which includes a non-projective arc (marked with a star).</S>
    <S sid="31" ssid="21">Who do you think she invited ?</S>
    <S sid="32" ssid="22">The non projective structures found in linguistics represent a small subset of the potential non projective structures.</S>
    <S sid="33" ssid="23">We will define a property (more exactly a family of properties), weaker than projectivity, called pseudo-projectivity, which describes a subset of the set of ordered dependency trees, containing the non-projective linguistic structures.</S>
    <S sid="34" ssid="24">In order to define pseudo-projectivity, we introduce an operation on dependency trees called lifting.</S>
    <S sid="35" ssid="25">When applied to a tree, this operation leads to the creation of a second tree, a lift of the first one.</S>
    <S sid="36" ssid="26">An ordered tree T' is a lift of the ordered tree T if and only if T and T' have the same nodes in the same order and for every node X, X+T &lt;TX&#177;T .</S>
    <S sid="37" ssid="27">We will say that the node x has been lifted from X+T (its syntactic governor) to X+Ti (its linear governor).</S>
    <S sid="38" ssid="28">Recall that the linear position of a node in a projective tree can be defined relative to its governor and its sisters.</S>
    <S sid="39" ssid="29">In order to define the linear order in a non projective tree, we will use a projective lift of the tree.</S>
    <S sid="40" ssid="30">In this case, the position of a node can be defined only with regards to its governor and sisters in the lift, i.e., its linear governor and sisters.</S>
    <S sid="41" ssid="31">Definition: An ordered tree T is said pseudo-projective if there exists a lift T' of tree T which is projective.</S>
    <S sid="42" ssid="32">If there is no restriction on the lifting, the previous definition is not very interesting since we can in fact take any non-projective tree and lift all nodes to the root node and obtain a projective tree.</S>
    <S sid="43" ssid="33">We will therefore constrain the lifting by a set of rules, called lifting rules.</S>
    <S sid="44" ssid="34">Consider a set of (syntactic) categories.</S>
    <S sid="45" ssid="35">The following definitions make sense only for trees whose nodes are labeled with categories.'</S>
    <S sid="46" ssid="36">The lifting rules are of the following form (LD, SG and LG are categories and w is a regular expression on the set of categories): This rule says that a node of category LD can be lifted from its syntactic governor of category SG to its linear governor of category LG through a path consisting of nodes of category C1, , Cn, where the string belongs to L(w).</S>
    <S sid="47" ssid="37">Every set of lifting rules defines a particular property of pseudo-projectivity by imposing particular constraints on the lifting.</S>
    <S sid="48" ssid="38">A 21t is possible to define pseudo-projectivity purely structurally (i.e. without referring to the labeling).</S>
    <S sid="49" ssid="39">For example, we can impose that each node x is lifted to the highest ancestor of x covered by t- ((Nasr, 1996)).</S>
    <S sid="50" ssid="40">The resulting pseudo-projectivity is a fairly weak extension to projectivity, which nevertheless covers major nonprojective linguistic structures.</S>
    <S sid="51" ssid="41">However, we do not pursue a purely structural definition of pseudo-projectivity in this paper. linguistic example of lifting rule is given in Section 4.</S>
    <S sid="52" ssid="42">The idea of building a projective tree by means of lifting appears in (Kunze, 1968) and is used by (Hudson, 1990) and (Hudson, unpublished).</S>
    <S sid="53" ssid="43">This idea can also be compared to the notion of word order domain (Reape, 1990; Broker and Neuhaus, 1997), to the Slash feature of GPSG and HPSG, to the functional uncertainty of LFG, and to the Move-a of GB theory.</S>
  </SECTION>
  <SECTION title="3 Projective Dependency Grammars Revisited" number="3">
    <S sid="54" ssid="1">We (informally) define a projective Dependency Grammar as a string-rewriting system3 by giving a set of categories such as N, V and Adv,4 a set of distinguished start categories (the root categories of well-formed trees), a mapping from strings to categories, and two types of rules: dependency rules which state hierarchical order (dominance) and LP rules which state linear order.</S>
    <S sid="55" ssid="2">The dependency rules are further subdivided into subcategorization rules (or s-rules) and modification rules (or m-rules).</S>
    <S sid="56" ssid="3">Here are some sample s-rules: LP rules are represented as regular expressions (actually, only a limited form of regular expressions) associated with each category.</S>
    <S sid="57" ssid="4">We use the hash sign (#) to denote the position of the governor (head).</S>
    <S sid="58" ssid="5">For example: 3We follow (Gaifman, 1965) throughout this paper by modeling a dependency grammar with a string-rewriting system.</S>
    <S sid="59" ssid="6">However, we will identify a derivation with its representation as a tree, and we will sometimes refer to symbols introduced in a rewrite step as &amp;quot;dependent nodes&amp;quot;.</S>
    <S sid="60" ssid="7">For a model of a DG based on tree-rewriting (in the spirit of Tree Adjoining Grammar (Joshi et al., 1975)), see (Nasr, 1995).</S>
    <S sid="61" ssid="8">We will call this system generative dependency grammar or GDG for short.</S>
    <S sid="62" ssid="9">Derivations in GDG are defined as follows.</S>
    <S sid="63" ssid="10">In a rewrite step, we choose a multiset of dependency rules (i.e., a set of instances of dependency rules) which contains exactly one srule and zero or more m-rules.</S>
    <S sid="64" ssid="11">The left-hand side nonterminal is the same as that we want to rewrite.</S>
    <S sid="65" ssid="12">Call this multiset the rewrite-multiset.</S>
    <S sid="66" ssid="13">In the rewriting operation, we introduce a multiset of new nonterminals and exactly one terminal symbol (the head).</S>
    <S sid="67" ssid="14">The rewriting operation then must meet the following three conditions: As an example, consider a grammar containing the three dependency rules di (rule 2), d2 (rule 3), and d3 (rule 4), as well as the LP rule pi (rule 5).</S>
    <S sid="68" ssid="15">In addition, we have some lexical mappings (they are obvious from the example), and the start symbol is Vfinite,&#177;.</S>
    <S sid="69" ssid="16">A sample derivation is shown in Figure 3, with the sentential form representation on top and the corresponding tree representation below.</S>
    <S sid="70" ssid="17">Using this kind of representation, we can derive a bottom-up parser in the following straightforward manner.5 Since syntactic and linear governors coincide, we can derive deterministic finite-state machines which capture both the dependency and the LP rules for a given governor category.</S>
    <S sid="71" ssid="18">We will refer to these FSMs as rule-FSMs, and if the governor is of category C, we will refer to a C-rule-FSM.</S>
    <S sid="72" ssid="19">In a rule-FSM, the transitions are labeled by categories, and the transition corresponding to the governor labeled by its category and a special mark (such as #).</S>
    <S sid="73" ssid="20">This transition is called the &amp;quot;head transition&amp;quot;.</S>
    <S sid="74" ssid="21">The entries in the parse matrix M are of the form (in, q), where in is a rule-FSM and q a state of it, except for the entries in squares M(i, i), 1 &lt; j&lt; n, which also contain category labels.</S>
    <S sid="75" ssid="22">Let WO &#8226; &#8226; wn be the input word.</S>
    <S sid="76" ssid="23">We initialize the parse matrix as follows.</S>
    <S sid="77" ssid="24">Let C be a category of word wi.</S>
    <S sid="78" ssid="25">First, we add C to M(i, i).</S>
    <S sid="79" ssid="26">Then, we add to M(i, i) every pair (7n, q) such that m is a rule-FSM with a transition labeled C from a start state and q the state reached after that transition.6 Embedded in the usual three loops on i, j, k, we add an entry (mi, q) to M(i, j) if (m1, qi) is in M (k, j), (m2, q2) is in M (i, k+1), q2 is a final state of m2, m2 is a C-rule-FSM, and mi transitions from qi to q on C (a non-head transition).</S>
    <S sid="80" ssid="27">There is a special case for the head transitions in mi: if k = i &#8212; 1, C is in M(i, i), mi is a Crule-FSM, and there is a head transition from qi to q in ml, then we add (mi, q) to M(i, j).</S>
    <S sid="81" ssid="28">The time complexity of the algorithm is 0(n3IGIQmax), where G is the number of ruleFSMs derived from the dependency and LP rules in the grammar and Qmax is the maximum number of states in any of the rule-FSMs.</S>
  </SECTION>
  <SECTION title="4 A Formalization of" number="4">
    <S sid="82" ssid="1">Recall that in a pseudo-projective tree, we make a distinction between a syntactic governor and a linear governor.</S>
    <S sid="83" ssid="2">A node can be &amp;quot;lifted&amp;quot; along a lifting path from being a dependent of its syntactic governor to being a dependent of its linear 'This type of parser has been proposed previously.</S>
    <S sid="84" ssid="3">See for example (Lombardi, 1996; Eisner, 1996), who also discuss Early-style parsers for projective dependency grammars.</S>
    <S sid="85" ssid="4">'We can use pre-computed top-down prediction to limit the number of pairs added. governor, which must be an ancestor of the governor.</S>
    <S sid="86" ssid="5">In defining a formal rewriting system for pseudo-projective trees, we will not attempt to model the &amp;quot;lifting&amp;quot; as a transformational step in the derivation.</S>
    <S sid="87" ssid="6">Rather, we will directly derive the &amp;quot;lifted&amp;quot; version of the tree, where a node is dependent of its linear governor.</S>
    <S sid="88" ssid="7">Thus, the derived structure resembles more a unistratal dependency representation like those used by (Hudson, 1990) than the multistratal representations of, for example, (Mel'euk, 1988).</S>
    <S sid="89" ssid="8">However, from a formal point of view, the distinction is not significant.</S>
    <S sid="90" ssid="9">In order to capture pseudo-projectivity, we will interpret rules of the form (2) (for subcategorization of arguments by a head) and (4) (for selection of a head by an adjunct) as introducing syntactic dependents which may lift to a higher linear governor.</S>
    <S sid="91" ssid="10">An LP rule of the form (5) orders all linear dependents of the linear governor, no matter whose syntactic dependents they are.</S>
    <S sid="92" ssid="11">In addition, we need a third type of rule, namely a lifting rule, or 1-rule (see 2.3).</S>
    <S sid="93" ssid="12">The 1-rule (1) can be rewrited on the following form: This rule resembles normal dependency rules but instead of introducing syntactic dependents of a category, it introduces a lifted dependent.</S>
    <S sid="94" ssid="13">Besides introducing a linear dependent LD, a 1-rule should make sure that the syntactic governor of LD will be introduced at a later stage of the derivation, and prevent it to introduce LD as its syntactic dependent, otherwise non projective nodes would be introduced twice, a first time by their linear governor and a second time by their syntactic governor.</S>
    <S sid="95" ssid="14">This condition is represented in the rule by means of a constraint on the categories found along the lifting path.</S>
    <S sid="96" ssid="15">This condition, which we call the lifting condition, is represented by the regular expression LG &#8226; w SG.</S>
    <S sid="97" ssid="16">The regular expression representing the lifting condition is enriched with a dot separating, on its left, the part of the lifting path which has already been introduced during the rewriting and on its right the part which is still to be introduced for the rewriting to be valid.</S>
    <S sid="98" ssid="17">The dot is an unperfect way of representing the current state in a finite state automaton equivalent to the regular expression.</S>
    <S sid="99" ssid="18">We can further notice that the lifting condition ends with a repetition of LD for reasons which will be made clear when discussing the rewriting process.</S>
    <S sid="100" ssid="19">A sentential form contains terminal strings and categories paired with a multiset of lifting conditions, called the lift multiset.</S>
    <S sid="101" ssid="20">The lift multiset associated to a category C contains 'transiting' lifting conditions: introduced by ancestors of C and passing across C. Three cases must be distinguished when rewriting a category C and its lifting multiset LM: &#8226; LM contains a single lifting condition which dot is situated to its right: LG w SG C..</S>
    <S sid="102" ssid="21">In such a case, C must be rewritten by the empty string.</S>
    <S sid="103" ssid="22">The situation of the dot at the right of the lifting condition indicates that C has been introduced by its syntactic governor although it has already been introduced by its linear governor earlier in the rewriting process.</S>
    <S sid="104" ssid="23">This is the reason why C has been added at the end of the lifting condition. of the 1-rules used in the rewriting operation.</S>
    <S sid="105" ssid="24">3.</S>
    <S sid="106" ssid="25">The lifting conditions contained in the lift multiset of all the newly introduced dependents D should be compatible with D, with the dot advanced appropriately.</S>
    <S sid="107" ssid="26">In addition, we require that, when we rewrite a category as a terminal, the lift multiset is empty.</S>
    <S sid="108" ssid="27">Let us consider an example.</S>
    <S sid="109" ssid="28">Suppose we have have a grammar containing the dependency rules di (rule 2), d2 (rule 3), and d3 (rule 4); the LP rule pi (rule 5) and p2: This rule says that an objective wh-noun with feature top:+ which depends on a verb with no further restrictions (the third V in the lifting path) can raise to any verb that dominates its immediate governor as long as the raising paths contains only verb with feature bridge:+, i.e., bridge verbs.</S>
    <S sid="110" ssid="29">A sample derivation is shown in Figure 4, with the sentential form representation on top and the corresponding tree representation below.</S>
    <S sid="111" ssid="30">We start our derivation with the start symbol Klause and rewrite it using dependency rules d2 and d3, and the lifting rule /1 which introduces an objective NP argument.</S>
    <S sid="112" ssid="31">The lifting condition of /I is passed to the V dependent but the dot remains at the left of Vbridge:+ because of the Kleene star.</S>
    <S sid="113" ssid="32">When we rewrite the embedded V, we choose to rewrite again with Klause, and the lifting condition is passed on to the next verb.</S>
    <S sid="114" ssid="33">This verb is a V+.</S>
    <S sid="115" ssid="34">&#8222;rans which requires a Nobj.</S>
    <S sid="116" ssid="35">The lifting condition is passed to Nobj and the dot is moved to the right of the regular expression, therefore Nobj is rewritten as the empty string.</S>
  </SECTION>
  <SECTION title="5 A Polynomial Parser for PP-GDG" number="5">
    <S sid="117" ssid="1">In this section, we show that pseudo-projective dependency grammars as defined in Section 2.3 are polynomially parsable.</S>
    <S sid="118" ssid="2">We can extend the bottom-up parser for GDG to a parser for PP-GDG in the following manner.</S>
    <S sid="119" ssid="3">In PP-GDG, syntactic and linear governors do not necessarily coincide, and we must keep track separately of linear precedence and of lifting (i.e., &amp;quot;long distance&amp;quot; syntactic dependence).</S>
    <S sid="120" ssid="4">The entries in the parse matrix M are of the form (m, q, LM), where m is a rule-FSM, q a state of m, and LM is a multiset of lifting conditions as defined in Section 4.</S>
    <S sid="121" ssid="5">An entry (m, q, LM) in a square M(i, j) of the parse matrix means that the sub-word wi &#8226; &#8226; &#8226; wj of the entry can be analyzed by in up to state q (i.e., it matches the beginning of an LP rule), but that nodes corresponding to the lifting rules in LM are being lifted from the subtrees spanning wz &#8226; &#8226; &#8226; wj.</S>
    <S sid="122" ssid="6">Put differently, in this bottomup view LM represents the set of nodes which have a syntactic governor in the subtree spanning ID, &#8226; &#8226; w3 and a lifting rule, but are still looking for a linear governor.</S>
    <S sid="123" ssid="7">Suppose we have an entry in the parse matrix M of the form (m, q, L).</S>
    <S sid="124" ssid="8">As we traverse the Crule-FSM m, we recognize one by one the linear dependents of a node of category C. Call this governor n. The action of adding a new entry to the parse matrix corresponds to adding a single new linear dependent to n. (While we are working on the C-rule-FSM 771 and are not yet in a final state, we have not yet recognized n itself.)</S>
    <S sid="125" ssid="9">Each new dependent 71' brings with it a multiset of nodes being lifted from the, subtree it is the root of.</S>
    <S sid="126" ssid="10">Call this multiset LM.</S>
    <S sid="127" ssid="11">The new entry will be (m, q', LMULM ) (where q is the state that m transitions to when n' is recognized as the next linear dependent.</S>
    <S sid="128" ssid="12">When we have reached a final state q of the rule-FSM m, we have recognized a complete subtree rooted in the new governor, 7/.</S>
    <S sid="129" ssid="13">Some of the dependent nodes of ?I will be both syntactic and linear dependents of n, and the others will be linear dependents of 7/, but lifted from a descendent of i.</S>
    <S sid="130" ssid="14">In addition, i may have syntactic dependents which are not realized as its own linear dependent and are lifted away.</S>
    <S sid="131" ssid="15">(No other options are possible.)</S>
    <S sid="132" ssid="16">Therefore, when we have reached the final state of a rule-FSM, we must connect up all nodes and lifting conditions before we can proceed to put an entry (in, q, L) in the parse matrix.</S>
    <S sid="133" ssid="17">This involves these steps: 1.</S>
    <S sid="134" ssid="18">For every lifting condition in LM, we ensure that it is compatible with the category of 77.</S>
    <S sid="135" ssid="19">This is done by moving the dot leftwards in accordance with the category of (The dot is moved leftwards since we are doing bottom-up recognition.)</S>
    <S sid="136" ssid="20">The obvious special provisions deal with the Kleene star and optional elements.</S>
    <S sid="137" ssid="21">If the category matches a catgeory with Kleene start in the lifting condition, we do not move the dot.</S>
    <S sid="138" ssid="22">If the category matches a category which is to the left of an optional category, or to the left of category with Kleene star, then we can move the dot to the left of that category.</S>
    <S sid="139" ssid="23">If the dot cannot be placed in accordance with the category of ij, then no new entry is made in the parse matrix for n. 2.</S>
    <S sid="140" ssid="24">We then choose a multiset of s-, m-, and 1rules whose left-hand side is the category of n. For every dependent of n introduced by an 1-rule, the dependent must be compatible with an instance of a lifting condition in LM (whose dot must be at its beginning, or seperated from the beginning by optional or categories only); the lifting condition is then removed from L. 3.</S>
    <S sid="141" ssid="25">If, after the above repositioning of the dot and the linking up of all linear dependents to lifting conditions, there are still lifting conditions in LM such that the dot is at the beginning of the lifting condition, then no new entry is made in the parse matrix for n. 4.</S>
    <S sid="142" ssid="26">For every syntactic dependent of rh we determine if it is a linear dependent of n which has not yet been identified as lifted.</S>
    <S sid="143" ssid="27">For each syntactic dependents which is not also a linear dependent, we check whether there is an applicable lifting rule.</S>
    <S sid="144" ssid="28">If not, no entry is made in the parse matrix for 77.</S>
    <S sid="145" ssid="29">If yes, we add the lifting rule to LM.</S>
    <S sid="146" ssid="30">This procedure determines a new multiset LM so we can add entry (in, q, LM) in the parse matrix.</S>
    <S sid="147" ssid="31">(In fact, it may determine several possible new multisets, resulting in multiple new entries.)</S>
    <S sid="148" ssid="32">The parse is complete if there is an entry (m, qm, 0) in square M(n, 1) of the parse matrix, where in is a C-rule-FSM for a start category and qm is a final state of M. If we keep backpointers at each step in the algorithm, we have a compact representation of the parse forest.</S>
    <S sid="149" ssid="33">The maximum number of entries in each square of the parse matrix is 0(GQ4), where G is the number of rule-FSMs corresponding to LP rules in the grammar, Q is the maximum number of states in any of the rule-FSMs, and L is the maximum number of states that the lifting rules can be in (i.e., the number of lifting conditions in the grammar multiplied by the maximum number of dot positions of any lifting condition).</S>
    <S sid="150" ssid="34">Note that the exponent is a grammar constant, but this number can be rather small since the lifting rules are not lexicalized - they are construction-specific, not lexemespecific.</S>
    <S sid="151" ssid="35">The time complexity of the algorithm is therefore 0(GQn3+2ILI).</S>
  </SECTION>
</PAPER>
