[
  {
    "citance_No": 1, 
    "citing_paper_id": "W06-1615", 
    "citing_paper_authority": 74, 
    "citing_paper_authors": "John, Blitzer | Ryan, McDonald | Fernando, Pereira", 
    "raw_text": "Chelba and Acero (2004) first traina classifier on the source data", 
    "clean_text": "Chelba and Acero (2004) first traina classifier on the source data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P08-2001", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Fernando, Batista | Nuno, Mamede | Isabel, Trancoso", 
    "raw_text": "(Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation", 
    "clean_text": "(Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P07-1033", 
    "citing_paper_authority": 98, 
    "citing_paper_authors": "Hal, Daum&eacute; III", 
    "raw_text": "The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004)", 
    "clean_text": "The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P07-1033", 
    "citing_paper_authority": 98, 
    "citing_paper_authors": "Hal, Daum&eacute; III", 
    "raw_text": "ChelbaandAcero (2004) describe this approach within the con text of a maximum entropy classifier, but the idea is more general", 
    "clean_text": "Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P07-1034", 
    "citing_paper_authority": 43, 
    "citing_paper_authors": "Jing, Jiang | ChengXiang, Zhai", 
    "raw_text": "Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data", 
    "clean_text": "Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P08-1029", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Andrew, Arnold | Ramesh, Nallapati | William W., Cohen", 
    "raw_text": "One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models 1 involves modifying the?? s of this Gaussian prior", 
    "clean_text": "One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P07-1056", 
    "citing_paper_authority": 96, 
    "citing_paper_authors": "John, Blitzer | Mark, Dredze | Fernando, Pereira", 
    "raw_text": "In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation", 
    "clean_text": "In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P07-1056", 
    "citing_paper_authority": 96, 
    "citing_paper_authors": "John, Blitzer | Mark, Dredze | Fernando, Pereira", 
    "raw_text": "This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation", 
    "clean_text": "This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N09-1068", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Jenny Rose, Finkel | Christopher D., Manning", 
    "raw_text": "Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior", 
    "clean_text": "Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N09-1032", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Hong Lei, Guo | Huijia, Zhu | Zhili, Guo | Xiaoxun, Zhang | Xian, Wu | Zhong, Su", 
    "raw_text": "Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data", 
    "clean_text": "Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "N06-1001", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Wei, Wang | Kevin, Knight | Daniel, Marcu", 
    "raw_text": "For example, Lita et al (2003) use a trigram language model estimated from a corpus with case information; Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases. Capitalization models presented in most previous approaches are monolingual because the models are estimated only from monolingual texts", 
    "clean_text": "Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N06-1001", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Wei, Wang | Kevin, Knight | Daniel, Marcu", 
    "raw_text": "Chelba and Acero (2004) frame capitalization asa sequence labeling problem, where, for each low MT Decoder Train Monolingual Capitalization Model Monolingual Cap Model Capitalization Lower Case Lower Case f Lower Case e Finput Eoutput Train Translation Model Train Language Model Translation Model Languagel Model{ F}{ E}{ f}{ e} Figure 1: The monolingual capitalization scheme employed by most statistical MT systems", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "D08-1072", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mark, Dredze | Koby, Crammer", 
    "raw_text": "Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume?, 2007) and adding the source classifier? s prediction as a feature (Chelba and Acero, 2004)", 
    "clean_text": "Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W10-2601", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Massimiliano, Ciaramita | Olivier, Chapelle", 
    "raw_text": "The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. Domain adaptation naturally invokes the existence of a specific task and data", 
    "clean_text": "The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text.", 
    "keep_for_gold": 0
  }
]
