<PAPER>
  <S sid="0">Introduction To The CoNLL-2000 Shared Task: Chunking</S>
  <ABSTRACT/>
  <SECTION title="1 Introduction" number="1">
    <S sid="1" ssid="1">Text chunking is a useful preprocessing step for parsing.</S>
    <S sid="2" ssid="2">There has been a large interest in recognizing non-overlapping noun phrases (Ramshaw and Marcus (1995) and follow-up papers) but relatively little has been written about identifying phrases of other syntactic categories.</S>
    <S sid="3" ssid="3">The CoNLL-2000 shared task attempts to fill this gap.</S>
  </SECTION>
  <SECTION title="2 Task description" number="2">
    <S sid="4" ssid="1">Text chunking consists of dividing a text into phrases in such a way that syntactically related words become member of the same phrase.</S>
    <S sid="5" ssid="2">These phrases are non-overlapping which means that one word can only be a member of one chunk Here is an example sentence: [NP He ] [vp reckons ] [NP the current account deficit] [vp will narrow] [pp to [NP only 1.8 billion] [pp in] [NP September] .</S>
    <S sid="6" ssid="3">Chunks have been represented as groups of words between square brackets.</S>
    <S sid="7" ssid="4">A tag next to the open bracket denotes the type of the chunk.</S>
    <S sid="8" ssid="5">As far as we know, there are no annotated corpora available which contain specific information about dividing sentences into chunks of words of arbitrary types.</S>
    <S sid="9" ssid="6">We have chosen to work with a corpus with parse information, the Wall Street Journal WSJ part of the Penn Treebank II corpus (Marcus et al., 1993), and to extract chunk information from the parse trees in this corpus.</S>
    <S sid="10" ssid="7">We will give a global description of the various chunk types in the next section.</S>
  </SECTION>
  <SECTION title="3 Chunk Types" number="3">
    <S sid="11" ssid="1">The chunk types are based on the syntactic category part (i.e. without function tag) of the bracket label in the Treebank (cf.</S>
    <S sid="12" ssid="2">Bies (1995) p.35).</S>
    <S sid="13" ssid="3">Roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.</S>
    <S sid="14" ssid="4">Some Treebank constituents do not have related chunks.</S>
    <S sid="15" ssid="5">The head of S (simple declarative clause) for example is normally thought to be the verb, but as the verb is already part of the VP chunk, no S chunk exists in our example sentence.</S>
    <S sid="16" ssid="6">Besides the head, a chunk also contains premodifiers (like determiners and adjectives in NPs), but no postmodifiers or arguments.</S>
    <S sid="17" ssid="7">This is why the PP chunk only contains the preposition, and not the argument NP, and the SBAR chunk consists of only the complementizer.</S>
    <S sid="18" ssid="8">There are several difficulties when converting trees into chunks.</S>
    <S sid="19" ssid="9">In the most simple case, a chunk is just a syntactic constituent without any further embedded constituents, like the NPs in our examples.</S>
    <S sid="20" ssid="10">In some cases, the chunk contains only what is left after other chunks have been removed from the constituent, cf.</S>
    <S sid="21" ssid="11">&amp;quot;(VP loves (NP Mary))&amp;quot; above, or ADJPs and PPs below.</S>
    <S sid="22" ssid="12">We will discuss some special cases during the following description of the individual chunk types.</S>
    <S sid="23" ssid="13">Our NP chunks are very similar to the ones of Ramshaw and Marcus (1995).</S>
    <S sid="24" ssid="14">Specifically, possessive NP constructions are split in front of the possessive marker (e.g.</S>
    <S sid="25" ssid="15">[NP Eastern Airlines] [NP creditors]) and the handling of coordinated NPs follows the Treebank annotators.</S>
    <S sid="26" ssid="16">However, as Ramshaw and Marcus do not describe the details of their conversion algorithm, results may differ in difficult cases, e.g. involving NAC and NX.1 An ADJP constituent inside an NP constituent becomes part of the NP chunk: (NP The (ADJP most volatile) form) [NP the most volatile form] In the Treebank, verb phrases are highly embedded; see e.g. the following sentence which contains four VP constituents.</S>
    <S sid="27" ssid="17">Following Ramshaw and Marcus' V-type chunks, this sentence will only contain one VP chunk: ((S (NP-SBJ-3 Mr. Icahn) (VP may not (VP want (S (NP-SBJ *-3) (VP to (VP sell ...))))) . ))</S>
    <S sid="28" ssid="18">[NP Mr. Icahn] [vp may not want to sell] It is still possible however to have one VP chunk directly follow another: [NP The impression [NP I] [vp have got] [vp is] [NP they] [vp 'd love to do] [PRT away] [pp with] [NP it].</S>
    <S sid="29" ssid="19">In this case the two VP constituents did not overlap in the Treebank.</S>
    <S sid="30" ssid="20">Adverbs/adverbial phrases become part of the VP chunk (as long as they are in front of the main verb): (VP could (ADVP very well) (VP show ... )) [vp could very well show] In contrast to Ramshaw and Marcus (1995), predicative adjectives of the verb are not part of the VP chunk, e.g. in &amp;quot;[NP they [vp are 1 [paw unhappy ]&amp;quot;.</S>
    <S sid="31" ssid="21">In inverted sentences, the auxiliary verb is not part of any verb phrase in the Treebank.</S>
    <S sid="32" ssid="22">Consequently it does not belong to any VP chunk: (NP-SBJ *-1) (VP to (VP be (ADJPPRD excellent)))))) , but ... [CONJP Not only ] does [NP your product [vp have to be [ADJP excellent , but ... ADVP chunks mostly correspond to ADVP constituents in the Treebank.</S>
    <S sid="33" ssid="23">However, ADVPs inside ADJPs or inside VPs if in front of the main verb are assimilated into the ADJP respectively VP chunk.</S>
    <S sid="34" ssid="24">On the other hand, ADVPs that contain an NP make two chunks: (ADVP-TMP (NP a year) earlier) [NP a year] [ADVP earlier] ADJPs inside NPs are assimilated into the NP.</S>
    <S sid="35" ssid="25">And parallel to ADVPs, ADJPs that contain an NP make two chunks: (ADJP-PRD (NP 68 years) old) [NP 68 years] [ADJP old] It would be interesting to see how changing these decisions (as can be done in the Treebank-to-chunk conversion script2) influences the chunking task.</S>
    <S sid="36" ssid="26">Most PP chunks just consist of one word (the preposition) with the part-of-speech tag IN.</S>
    <S sid="37" ssid="27">This does not mean, though, that finding PP chunks is completely trivial.</S>
    <S sid="38" ssid="28">INs can also constitute an SBAR chunk (see below) and some PP chunks contain more than one word.</S>
    <S sid="39" ssid="29">This is the case with fixed multi-word prepositions such as such as, because of, due to, with prepositions preceded by a modifier: well above, just after, even in, particularly among or with coordinated prepositions: inside and outside.</S>
    <S sid="40" ssid="30">We think that PPs behave sufficiently differently from NPs in a sentence for not wanting to group them into one class (as Ramshaw and Marcus did in their N-type chunks), and that on the other hand tagging all NP chunks inside a PP as I-PP would only confuse the chunker.</S>
    <S sid="41" ssid="31">We therefore chose not to handle the recognition of true PPs (prep.+NP) during this first chunking step.</S>
    <S sid="42" ssid="32">SBAR chunks mostly consist of one word (the complementizer) with the part-of-speech tag IN, but like multi-word prepositions, there are also multi-word complementizers: even though, so that, just as, even if, as if, only if.</S>
    <S sid="43" ssid="33">Conjunctions can consist of more than one word as well: as well as, instead of, rather than, not only, but also.</S>
    <S sid="44" ssid="34">One-word conjunctions (like and, or) are not annotated as CONJP in the Treebank, and are consequently no CONJP chunks in our data.</S>
    <S sid="45" ssid="35">The Treebank uses the PRT constituent to annotate verb particles, and our PRT chunk does the same.</S>
    <S sid="46" ssid="36">The only multi-word particle is on and off This chunk type should be easy to recognize as it should coincide with the partof-speech tag RP, but through tagging errors it is sometimes also assigned IN (preposition) or RB (adverb).</S>
    <S sid="47" ssid="37">INTJ is an interjection phrase/chunk like no, oh, hello, alas, good grief!.</S>
    <S sid="48" ssid="38">It is quite rare.</S>
    <S sid="49" ssid="39">The list marker LST is even rarer.</S>
    <S sid="50" ssid="40">Examples are 1., 2., 3., first, second, a, b, c. It might consist of two words: the number and the period.</S>
    <S sid="51" ssid="41">The UCP chunk is reminiscent of the UCP (unlike coordinated phrase) constituent in the Treebank.</S>
    <S sid="52" ssid="42">Arguably, the conjunction is the head of the UCP, so most UCP chunks consist of conjunctions like and and or.</S>
    <S sid="53" ssid="43">UCPs are the rarest chunks and are probably not very useful for other NLP tasks.</S>
    <S sid="54" ssid="44">Tokens outside any chunk are mostly punctuation signs and the conjunctions in ordinary coordinated phrases.</S>
    <S sid="55" ssid="45">The word not may also be outside of any chunk.</S>
    <S sid="56" ssid="46">This happens in two cases: Either not is not inside the VP constituent in the Treebank annotation e.g. in ... (VP have (VP told (NP-1 clients) (S (NP-SBJ *-1) not (VP to (VP ship (NP anything)))))) or not is not followed by another verb (because the main verb is a form of to be).</S>
    <S sid="57" ssid="47">As the right chunk boundary is defined by the chunk's head, i.e. the main verb in this case, not is then in fact a postmodifier and as such not included in the chunk: &amp;quot;... [SBAR that ] [NP there ] [vp were ] n't [NP any major problems] .&amp;quot; All chunks were automatically extracted from the parsed version of the Treebank, guided by the tree structure, the syntactic constituent labels, the part-of-speech tags and by knowledge about which tags can be heads of which constituents.</S>
    <S sid="58" ssid="48">However, some trees are very complex and some annotations are inconsistent.</S>
    <S sid="59" ssid="49">What to think about a VP in which the main verb is tagged as NN (common noun)?</S>
    <S sid="60" ssid="50">Either we allow NNs as heads of VPs (not very elegant but which is what we did) or we have a VP without a head.</S>
    <S sid="61" ssid="51">The first solution might also introduce errors elsewhere... As Ramshaw and Marcus (1995) already noted: &amp;quot;While this automatic derivation process introduced a small percentage of errors on its own, it was the only practical way both to provide the amount of training data required and to allow for fully-automatic testing.&amp;quot;</S>
  </SECTION>
  <SECTION title="4 Data and Evaluation" number="4">
    <S sid="62" ssid="1">For the CoNLL shared task, we have chosen to work with the same sections of the Penn Treebank as the widely used data set for base noun phrase recognition (Ramshaw and Marcus, 1995): WSJ sections 15-18 of the Penn Treebank as training material and section 20 as test materia13.</S>
    <S sid="63" ssid="2">The chunks in the data were selected to match the descriptions in the previous section.</S>
    <S sid="64" ssid="3">An overview of the chunk types in the training data can be found in table 1.</S>
    <S sid="65" ssid="4">De data sets contain tokens (words and punctuation marks), information about the location of sentence boundaries and information about chunk boundaries.</S>
    <S sid="66" ssid="5">Additionally, a partof-speech (POS) tag was assigned to each token by a standard POS tagger (Brill (1994) trained on the Penn Treebank).</S>
    <S sid="67" ssid="6">We used these POS tags rather than the Treebank ones in order to make sure that the performance rates obtained for this data are realistic estimates for data for which no treebank POS tags are available.</S>
    <S sid="68" ssid="7">In our example sentence in section 2, we have used brackets for encoding text chunks.</S>
    <S sid="69" ssid="8">In the data sets we have represented chunks with three types of tags: B-X first word of a chunk of type X I-X non-initial word in an X chunk 0 word outside of any chunk This representation type is based on a representation proposed by Ramshaw and Marcus (1995) for noun phrase chunks.</S>
    <S sid="70" ssid="9">The three tag groups are sufficient for encoding the chunks in the data since these are non-overlapping.</S>
    <S sid="71" ssid="10">Using these chunk tags makes it possible to approach the chunking task as a word classification task.</S>
    <S sid="72" ssid="11">We can use chunk tags for representing our example sentence in the following way: The output of a chunk recognizer may contain inconsistencies in the chunk tags in case a word tagged I-X follows a word tagged 0 or I-Y, with X and Y being different.</S>
    <S sid="73" ssid="12">These inconsistencies can be resolved by assuming that such I-X tags start a new chunk.</S>
    <S sid="74" ssid="13">The performance on this task is measured with three rates.</S>
    <S sid="75" ssid="14">First, the percentage of detected phrases that are correct (precision).</S>
    <S sid="76" ssid="15">Second, the percentage of phrases in the data that were found by the chunker (recall).</S>
    <S sid="77" ssid="16">And third, the Fo=1 rate which is equal to (132+1)*precision*recall / (02*precision+recall) with 0.1 (van Rijsbergen, 1975).</S>
    <S sid="78" ssid="17">The latter rate has been used as the target for optimization4.</S>
  </SECTION>
  <SECTION title="5 Results" number="5">
    <S sid="79" ssid="1">The eleven systems that have been applied to the CoNLL-2000 shared task can be divided in four groups: Vilain and Day (2000) approached the shared task in three different ways.</S>
    <S sid="80" ssid="2">The most successful was an application of the Alembic parser which uses transformation-based rules.</S>
    <S sid="81" ssid="3">Johansson (2000) uses context-sensitive and contextfree rules for transforming part-of-speech (POS) tag sequences to chunk tag sequences.</S>
    <S sid="82" ssid="4">Dejean (2000) has applied the theory refinement system ALLiS to the shared task.</S>
    <S sid="83" ssid="5">In order to obtain a system which could process XML formatted data while using context information, he has used three extra tools.</S>
    <S sid="84" ssid="6">Veenstra and Van den Bosch (2000) examined different parameter settings of a memory-based learning algorithm.</S>
    <S sid="85" ssid="7">They found that modified value difference metric applied to POS information only worked best.</S>
    <S sid="86" ssid="8">A large number of the systems applied to the CoNLL-2000 shared task uses statistical methods.</S>
    <S sid="87" ssid="9">Pla, Molina and Prieto (2000) use a finite-state version of Markov Models.</S>
    <S sid="88" ssid="10">They started with using POS information only and obtained a better performance when lexical information was used.</S>
    <S sid="89" ssid="11">Zhou, Tey and Su (2000) implemented a chunk tagger based on HMMs.</S>
    <S sid="90" ssid="12">The initial performance of the tagger was improved by a post-process correction method based on error driven learning and by incorporating chunk probabilities generated by a memory-based learning process.</S>
    <S sid="91" ssid="13">The two other statistical systems use maximum-entropy based methods.</S>
    <S sid="92" ssid="14">Osborne (2000) trained Ratnaparkhi's maximum-entropy POS tagger to output chunk tags.</S>
    <S sid="93" ssid="15">Koeling (2000) used a standard maximum-entropy learner for generating chunk tags from words and POS tags.</S>
    <S sid="94" ssid="16">Both have tested different feature combinations before finding an optimal one and their final results are close to each other.</S>
    <S sid="95" ssid="17">Three systems use system combination.</S>
    <S sid="96" ssid="18">Tjong Kim Sang (2000) trained and tested five memory-based learning systems to produce different representations of the chunk tags.</S>
    <S sid="97" ssid="19">A combination of the five by majority voting performed better than the individual parts.</S>
    <S sid="98" ssid="20">Van Halteren (2000) used Weighted Probability Distribution Voting (WPDV) for combining the results of four WPDV chunk taggers and a memory-based chunk tagger.</S>
    <S sid="99" ssid="21">Again the combination outperformed the individual systems.</S>
    <S sid="100" ssid="22">Kudoh and Matsumoto (2000) created 231 support vector machine classifiers to predict the unique pairs of chunk tags.</S>
    <S sid="101" ssid="23">The results of the classifiers were combined by a dynamic programming algorithm.</S>
    <S sid="102" ssid="24">The performance of the systems can be found in Table 2.</S>
    <S sid="103" ssid="25">A baseline performance was obtained by selecting the chunk tag most frequently associated with a POS tag.</S>
    <S sid="104" ssid="26">All systems outperform the baseline.</S>
    <S sid="105" ssid="27">The majority of the systems reached an Fo=1 score between 91.50 and 92.50.</S>
    <S sid="106" ssid="28">Two approaches performed a lot better: the combination system WPDV used by Van Halteren and the Support Vector Machines used by Kudoh and Matsumoto.</S>
  </SECTION>
  <SECTION title="6 Related Work" number="6">
    <S sid="107" ssid="1">In the early nineties, Abney (1991) proposed to approach parsing by starting with finding related chunks of words.</S>
    <S sid="108" ssid="2">By then, Church (1988) had already reported on recognition of base noun phrases with statistical methods.</S>
    <S sid="109" ssid="3">Ramshaw and Marcus (1995) approached chunking by using a machine learning method.</S>
    <S sid="110" ssid="4">Their work has inspired many others to study the application of learning methods to noun phrase chunking5.</S>
    <S sid="111" ssid="5">Other chunk types have not received the same attention as NP chunks.</S>
    <S sid="112" ssid="6">The most complete work is Buchholz et al. (1999), which presents results for NP, VP, PP, ADJP and ADVP chunks.</S>
    <S sid="113" ssid="7">Veenstra (1999) works with NP, VP and PP chunks.</S>
    <S sid="114" ssid="8">Both he and Buchholz et al. use data generated by the script that produced the CoNLL-2000 shared task data sets.</S>
    <S sid="115" ssid="9">Ratnaparkhi (1998) has recognized arbitrary chunks as part of a parsing task but did not report on the chunking performance.</S>
    <S sid="116" ssid="10">Part of the Sparkle project has concentrated on finding various sorts of chunks for the different languages 'An elaborate overview of the work done on noun phrase chunking can be found on http://lcg-www.uia. ac.bererikt/research/np-chunking.html (Carroll et al., 1997).</S>
  </SECTION>
  <SECTION title="7 Concluding Remarks" number="7">
    <S sid="117" ssid="1">We have presented an introduction to the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking.</S>
    <S sid="118" ssid="2">For this task we have generated training and test data from the Penn Treebank.</S>
    <S sid="119" ssid="3">This data has been processed by eleven systems.</S>
    <S sid="120" ssid="4">The best performing system was a combination of Support Vector Machines submitted by Taku Kudoh and Yuji Matsumoto.</S>
    <S sid="121" ssid="5">It obtained an Fo=1 score of 93.48 on this task.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="8">
    <S sid="122" ssid="1">We would like to thank the members of the CNTS - Language Technology Group in Antwerp, Belgium and the members of the ILK group in Tilburg, The Netherlands for valuable discussions and comments.</S>
    <S sid="123" ssid="2">Tjong Kim Sang is funded by the European TMR network Learning Computational Grammars.</S>
    <S sid="124" ssid="3">Buchholz is supported by the Netherlands Organization for Scientific Research (NWO).</S>
  </SECTION>
</PAPER>
