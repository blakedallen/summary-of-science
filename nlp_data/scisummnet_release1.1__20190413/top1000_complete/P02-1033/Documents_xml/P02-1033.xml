<PAPER>
  <S sid="0">An Unsupervised Method For Word Sense Tagging Using Parallel Corpora</S>
  <ABSTRACT>
    <S sid="1" ssid="1">tion using statistical models of Roget's categories on large corpora.</S>
    <S sid="2" ssid="2">In of the</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="3" ssid="1">second, achieving sense tagging using that same sense inventory for the second language, thus creating a sense-tagged corpus and automatically making a connection to the first language's sense inventory.</S>
    <S sid="4" ssid="2">In this paper we focus primarily ort the first goal.</S>
    <S sid="5" ssid="3">The crux of this research is the observation that translations can serve as a source of sense distinctions (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Dyvik, 1998; Ide, 2000; Resnik and Yarowsky, 1999).</S>
    <S sid="6" ssid="4">A word that has multiple senses in one language is often translated as distinct words in another language, with the particular choice depending ort the translator and the contextualized meaning; thus the corresponding translation can be thought of as a sense indicator for the instance of the word in its context.</S>
    <S sid="7" ssid="5">Looking at parallel translations, it becomes evident that two factors are at play.</S>
    <S sid="8" ssid="6">On the one hand, instances of a word/sense combination are translated with some consistency into a relatively small handful of words in the second language.</S>
    <S sid="9" ssid="7">On the other hand, that handful of words is rarely a singleton set evert for a single word/sense, because the preferences of different translators and the demands of context produce semantically similar words that differ in their nuances.</S>
    <S sid="10" ssid="8">For example, in a French-English parallel corpus, the French word catastrophe could be found in correspondence to English disaster in one instance, and to tragedy in another.</S>
    <S sid="11" ssid="9">Each of those English words is itself ambiguous e.g., tragedy can refer to a kind of play (as opposed to comedy) but we can take advantage of the fact that both English word instances appeared in correspondence with catastrophe to infer that they share some common element of meaning, and we can use that inference in deciding which of the English senses was intended.</S>
    <S sid="12" ssid="10">Having done so, we can go further: we can project the English word sense chosen for this instance of tragedy to the French word catastrophe in this context, thus tagging the two languages in tandem with a single sense inventory.</S>
    <S sid="13" ssid="11">The remainder of this paper is organized as follows.</S>
    <S sid="14" ssid="12">Section 2 describes the approach.</S>
    <S sid="15" ssid="13">Section 3 lays out evaluation experiments, using SENSEVAL-2 data, showing the results of several different variations of the approach and comparing performance with other SENSEVAL-2 systems.</S>
    <S sid="16" ssid="14">Section 4 contains discussion and we conclude in Section 5.</S>
  </SECTION>
  <SECTION title="2 Approach" number="2">
    <S sid="17" ssid="1">For the sake of exposition, let us assume that we are working with art English-French parallel corpus and that we are using art English sense inventory.'</S>
    <S sid="18" ssid="2">Although there is no necessary assumption of directionality in translation, we will sometimes refer to the English language corpus as the target corpus and the French corpus as the source corpus, which corresponds to the characterization, in the previous section, of the French word (catastrophe) being translated into two different words (disaster and tragedy) in two diferent contexts.</S>
    <S sid="19" ssid="3">The process we described can be viewed more abstractly as follows: forming target sets that were translated into the same orthographic form in the source corpus.</S>
    <S sid="20" ssid="4">The first step of the process assumes a sentence- or segment-aligned parallel corpus; suitable data are now available for many languages via organizations such LDC and ELRA and the Web is a promising source of data in new language pairs and in new genres (Nie et al., 1999; Resnik, 1999a).</S>
    <S sid="21" ssid="5">After identifying and tokenizing sentences, we obtain word-level alignments for the parallel corpus using the GIZA++ implementation of the IBM statistical MT models (Och and Ney, 2000).</S>
    <S sid="22" ssid="6">For each French word instance f, we collect the word instance e with which it is aligned.</S>
    <S sid="23" ssid="7">Positions of the word instances are recorded so that in later stages we can project the eventual semantic annotation ort e to f. For example, the alignment of The accident was a tragedy with L 'accident etait une catastrophe might associate these two instances of catastrophe and tragedy. lit the second step, we collect for each word type F the set of all English word types with which it is aligned anywhere in the corpus, which we call the target set for F. For example, the target set for French catastrophe might contain English word types disaster, tragedy, and situation, the last of these arising because some translator chose to render la catastrophe in English as the awful situation.</S>
    <S sid="24" ssid="8">In extracting correspondences we take advantage of WordNet to identify English nominal compounds in order to help reduce the number of ambiguous terms in the target set.2 For example, without nominal compound identification on the English side, the target set for French abeille will contain bee, winch is ambiguous (SPELLING-BEE VS. INSECT).</S>
    <S sid="25" ssid="9">With compound identification, the target set for abeille still contains bee, but it is also rich in unambiguous terms like alkali_bee, honey_bee, and gueen_bee.</S>
    <S sid="26" ssid="10">In the semantic similarity computation, the presence of these monosemous words provides strong reinforcement for the INSECT sense of bee.</S>
    <S sid="27" ssid="11">Moreover, it enables us to tag instances of bee with their more specific compound-noun senses when they appear within a compound that is known to the sense inventory. lit the third step, the target set is treated as a problem of monolingual sense disambiguation with respect to the target-language sense inventory.</S>
    <S sid="28" ssid="12">Consider the target set {disaster, tragedy, situation}: to the human reader, the juxtaposition of these words within a single set automatically brings certain senses 2We used a small set of compound-matching rules considering a window of two tokens to the right and left, and also used the &amp;quot;satellite&amp;quot; annotations in SENSEVAL data as part of our preprocessing. to the foreground.</S>
    <S sid="29" ssid="13">The same intuitive idea is exploited by Resnik's (1999b) algorithm for disambiguating groups of related nouns, which we apply here.</S>
    <S sid="30" ssid="14">For a target set {el, ..., en}, the algorithm considers each pair of words (e,, ei)(j and identifies which senses of the two words are most similar semantically.</S>
    <S sid="31" ssid="15">Those senses are then reinforced by an amount corresponding to that degree of similarity.3 After comparison across all pairs, each word sense s,j, of word e, ends up having associated with it a confidence c(s,j,) E [0, 1] that reflects how much reinforcement sense s,j, received based on the other words in the set.</S>
    <S sid="32" ssid="16">In our example, the KIND-OF-DRAMA sense of tragedy would have received little support from the senses of the other two words in the set; on the other hand, the CALAMITY sense would have been reinforced and therefore would receive higher confidence.</S>
    <S sid="33" ssid="17">At the end of the third step, we highlight the significance of variability in translation: since the method relies on semantic similarities between multiple items in a target set, the target set must contain at least two members.</S>
    <S sid="34" ssid="18">If throughout the parallel corpus the translator always chose to translate the French word catastrophe to tragedy, the target set for catastrophe will contain only a single element.</S>
    <S sid="35" ssid="19">Our algorithm will have no basis for assigning reinforcement differently to different senses, and as a result, none of these instances of tragedy the ones corresponding to catastrophe will be tagged.</S>
    <S sid="36" ssid="20">At this point we take advantage of the bookkeeping information recorded earlier.</S>
    <S sid="37" ssid="21">We know which instances of tragedy are associated with the target set {disaster, tragedy, situation} , and so those instances can be labeled with the most confident sense (CALAMITY) or, for that matter, with the confidence distribution over all possible senses as determined by the noun-group disambiguation algorithm.</S>
    <S sid="38" ssid="22">In the fourth and final step, we take advantage of the English-side tagging and the wordlevel alignment to project the sense tags on 3Since we use WordNet as our sense inventory, we also adopt the information-theoretic measure of semantic similarity based on that taxonomy.</S>
    <S sid="39" ssid="23">English to the corresponding words in French.</S>
    <S sid="40" ssid="24">For example, the tagging The accident was a tragedy/cALAmnv would yield L'accident etait une catastrophe/CALAMITY.</S>
    <S sid="41" ssid="25">As a result, a large number of French words will receive tags from the English sense inventory.</S>
  </SECTION>
  <SECTION title="3 Evaluation" number="3">
    <S sid="42" ssid="1">In order to provide a useful formal evaluation of this approach for English sense disambiguation, there were three requirements.</S>
    <S sid="43" ssid="2">We needed: Meeting all three requirements simultaneously presented something of a challenge.</S>
    <S sid="44" ssid="3">There are a few human-tagged English corpora available for word sense disambiguation, but most are relatively small by model-training standards and none have associated translations in other languages.</S>
    <S sid="45" ssid="4">Conversely, there are some parallel corpora large enough for training alignment models, but to our knowledge none of these have been even partially sense tagged.</S>
    <S sid="46" ssid="5">To solve this problem, we adopted a &amp;quot;pseudotranslation&amp;quot; approach (Diab, 2000).</S>
    <S sid="47" ssid="6">A suitably large English corpus is constructed, containing as a subset an English corpus for which we have art existing set of associated gold-standard sense tags.</S>
    <S sid="48" ssid="7">The entire corpus, including the subset, is translated using commercial MT technology, producing an artificial parallel corpus.</S>
    <S sid="49" ssid="8">This corpus is then used as described in Section 2, and the quality of sense tagging on the English gold-standard subset is assessed using community-wide evaluation standards, with results suitable for inter-system comparison with other algorithms that have been tested ort the same data.</S>
    <S sid="50" ssid="9">The pseudo-translation approach has advantages and disadvantages.</S>
    <S sid="51" ssid="10">On the one hand, using commercial MT systems does not necessarily result in performance figures representing what could be obtained with better quality human translations.</S>
    <S sid="52" ssid="11">On the other hand, a pseudotranslated corpus is far easier to produce, and this approach to evaluation allows for controlled experimentation using English paired with multiple languages.</S>
    <S sid="53" ssid="12">We used the the English &amp;quot;all words&amp;quot; portion of the SENSEVAL-2 test data (henceforth 5V2AW) as our gold-standard English subset.</S>
    <S sid="54" ssid="13">The corpus comprises three documents from the Wall Street Journal, totaling 242 lines with 5826 tokens in all.</S>
    <S sid="55" ssid="14">To fill out this English-side corpus, we added the raw unannotated texts of the Brown Corpus (BC) (Francis and KuCera, 1982), the SENSEVAL-1 Corpus (SV1), the SENSEVAL2 English Lexical Sample test, trial and training corpora (5V2-LS), and Wall Street Journal (WSJ) sections 18-24 from the Penn Treebank.</S>
    <S sid="56" ssid="15">We will refer to this unwieldy merged corpus with the unwieldy but informative label BCSV1SV2WSJ.</S>
    <S sid="57" ssid="16">Table 1 shows the sizes of the component corpora.</S>
    <S sid="58" ssid="17">Two different commercially available MT systems were used for the pseudo-translations: Globalink Pro 6.4 (GL) and Systran Professional Premium (SYS).</S>
    <S sid="59" ssid="18">The motivation behind using two MT systems stems from a desire to more closely approximate the variability of human translation in a very large corpus, where one translator would be unlikely to have performed the entire task, and to help offset the possible tendency of any single MT system to be unnaturally consistent in its lexical selection.</S>
    <S sid="60" ssid="19">The English BCSV1SV2WSJ was translated into French and Spanish, resulting in four parallel corpora: BCSV1SV2WSJ paired with the French GL translation (yielding parallel corpus FRGL), with French SYS translation (FRSYS), with Spanish GL (SPGL), and with Spanish SYS (SPSYS).4 Each of the four parallel corpora just described (FRGL, FRSYS, SPGL, SPSYS) represents a separate experimental variant.</S>
    <S sid="61" ssid="20">Consistent with Diab (2000), we added one more variant for each language in order to more closely approach the variability associated with multiple translations: in Step 2 we combined the target sets from the two MT systems.</S>
    <S sid="62" ssid="21">For example, if the word types shore, bank are in the target set of orilla in SPGL, and coast, bank, and shore are in the target set for orilla in SPSYS, the union of the target sets is taken and the result is a merged target set for orilla containing {bank, coast, shore}.</S>
    <S sid="63" ssid="22">These last two variations are labeled MFRGLSYS and MSPGLSYS.</S>
    <S sid="64" ssid="23">We restricted our experiments to disambiguation of nouns, for which there were 1071 instances in 5V2-AW not marked &amp;quot;unassignable&amp;quot; by SENSEVAL'S human annotators.</S>
    <S sid="65" ssid="24">Nouns were identified on the basis of human-assigned partof-speech tags where available (BC, WSJ and 5V2-AW) and using the Brill tagger elsewhere (Brill, 1993).</S>
    <S sid="66" ssid="25">The choice of 5V2-AW as our gold standard corpus determined our choice of sense inventory: SENSEVAL-2 produced a gold standard for the English &amp;quot;all words&amp;quot; task using a pre-release of WordNet 1.7 (Fellbaum, 1998), and we restricted our attention to the noun taxonomy.</S>
    <S sid="67" ssid="26">Because the algorithm for disambiguating noun groupings returns a confidence value for every sense of a word, some threshold or other criterion is needed to decide which sense or senses to actually assign.</S>
    <S sid="68" ssid="27">We simply assign the sense tag that scored the maximum confidence level, or all such tags, equally weighted, if there is a tie.</S>
    <S sid="69" ssid="28">(The SENSEVAL evaluation measures allow for partial credit.)</S>
    <S sid="70" ssid="29">This criterion is fairly sensitive to noise in target sets; for example, in a real corpus the French catastrophe is aligned with English {catastrophe, disaster, shocker, tragedy}.</S>
    <S sid="71" ssid="30">Shocker is art outlier in this set and its presence affects the overall confidence score assignment for all the words in the set.</S>
    <S sid="72" ssid="31">We observed that this is similar to what happens when the French word underlying the target set is homonymous; such cases are part of our discussion in Section 4.</S>
    <S sid="73" ssid="32">We evaluated the algorithm's performance using the standard SENSEVAL-2 evaluation software, obtaining figures for precision and recall for sense tagging the nouns in our gold standard.</S>
    <S sid="74" ssid="33">In this evaluation, partial credit is given in cases where a system assigns multiple sense tags.5 We report results using the &amp;quot;fine-grained&amp;quot; scoring variant; this is the strictest variant, which sometimes requires systems to discern among WordNet senses that even linguists have a difficult time distinguishing.</S>
    <S sid="75" ssid="34">Table 2 summarizes the results, and Figure 1 shows our algorithm's results (triangles) compared to the performance of the 21 SENSEVAL-2 English All Words participants, when the evaluation is restricted to the same set of noun test instances.6 Hollow circles represent supervised systems and filled circles represent unsupervised systems.'</S>
    <S sid="76" ssid="35">Of the systems that are unsupervised, and can therefore be included in a fair comparison, only one is clearly better ort both precision and recall.</S>
  </SECTION>
  <SECTION title="4 Discussion" number="4">
    <S sid="77" ssid="1">The results show that the performance of our approach is comparable or superior to most other unsupervised systems, even though it is based on cross-language lexical correspondences, a radically different source of evidence, and even though those correspondences were derived from machine translations rather than clean human translations.</S>
    <S sid="78" ssid="2">Here we briefly consider issues that bear on recall and precision, respectively.</S>
    <S sid="79" ssid="3">Some of the sentences in the test corpus could not be automatically aligned because our aligner discards sentence pairs that are longer than a pre-defined limit.</S>
    <S sid="80" ssid="4">For these sentences, therefore, no attempt could be made at disambiguation.</S>
    <S sid="81" ssid="5">Future experiments will attempt to increase the acceptable sentence length, as limited by real memory, and to break longer sentence pairs into logical sub-parts for alignment.</S>
    <S sid="82" ssid="6">A second issue that affects recall is the lack of variability in pseudo-translations.</S>
    <S sid="83" ssid="7">Of the English nouns that are aligned with sourcelanguage words, approximately 35% are always aligned with the same word, rendering them untaggable using an approach based ort semantic similarity within target sets.</S>
    <S sid="84" ssid="8">Some cases may reflect preserved ambiguity in the language pair e.g.</S>
    <S sid="85" ssid="9">French interet and English interest are ambiguous in similar ways and others may simply reflect the fact that commercial MT systems are just not very creative or context sensitive in their lexical choices.</S>
    <S sid="86" ssid="10">It should be possible to increase variability by extending the corpus to include human-translated parallel text, or by combining evidence from multiple or more distantly related source languages in the spirit of Resnik and Yarowsky (1999).</S>
    <S sid="87" ssid="11">On inspecting the target sets qualitatively, we find that they contain many outliers, largely owing to noisy alignment.</S>
    <S sid="88" ssid="12">The problem worsens when the outliers are monosemous, since a monosemous word with a misleading sense will erroneously bias the sense tag assignment for the other target set words.</S>
    <S sid="89" ssid="13">For example, the word types adolescence, idol, teen, and teenager form a target set for the French source word adolescence, and the presence of idol has a negative impact ort the sense assignment for the other members of the set.</S>
    <S sid="90" ssid="14">In addition, semantically distant words can align with the same source word; e.g., amorce in French may align with initiation, bait, and cap, which are all correct translations in suitable contexts but provide Ito suitable basis for semantic reinforcement.</S>
    <S sid="91" ssid="15">These problems reflect the algorithm's implicit assumption that the source words are monosemous, reflected in its attempt to have every word in a target set influence the semantics of every other word.</S>
    <S sid="92" ssid="16">Inspecting the data produces many counterexamples, e.g.</S>
    <S sid="93" ssid="17">French canon (cannon, cannonball, canon, theologian) bandes (band, gang, mob, strip, streak, tape), and baie (bay, berry, cove).</S>
    <S sid="94" ssid="18">A sensible alternative would be apply automatic clustering techniques to the target sets (e.g.</S>
    <S sid="95" ssid="19">(Diab and Finch, 2000; Schiitze, 1992)), providing target sub-clusters of words that should be treated as related, with no crosscluster reinforcement.</S>
    <S sid="96" ssid="20">For example, the target set for French canon would have two coherent sub-clusters containing {cannon, cannonball} and {canon, theologian)}, respectively.</S>
    <S sid="97" ssid="21">Manual inspection of target sets in our experiments suggests that when target sets are semantically coherent e.g. adversaires (antagonists, opponents, contestants), accident: (accident, crash, wreck) sense assignment is generally highly accurate.</S>
  </SECTION>
  <SECTION title="5 Conclusions" number="5">
    <S sid="98" ssid="1">This paper presents art unsupervised approach to word sense disambiguation that exploits translations as a proxy for semantic annotation across languages.</S>
    <S sid="99" ssid="2">The observation behind the approach, that words having the same translation often share some dimension of meaning, leads to art algorithm in which the correct sense of a word is reinforced by the semantic similarity of other words with which it shares those dimensions of meaning.</S>
    <S sid="100" ssid="3">Performance using this algorithm has been rigorously evaluated and is comparable with other unsupervised WSD systems, based ori fair comparison using community-wide test data.</S>
    <S sid="101" ssid="4">Because it achieves this performance using crosslanguage data alone, it is likely that improved results can be obtained by also taking advantage of monolingual contextual evidence.</S>
    <S sid="102" ssid="5">Although in the end all unsupervised systems are likely to produce precision results inferior to the best supervised algorithms, they are often more practical to apply in a broad-vocabulary setting.</S>
    <S sid="103" ssid="6">Moreover, noisy annotations can serve as seeds both for monolingual supervised methods and for bootstrapping cross-linguistic sense disambiguation and sense inventories, complementing other research ori the complex problem of mapping sense tags cross linguistically (e.g.</S>
    <S sid="104" ssid="7">(Alonge et al., 1998; Rodriguez et al., 1998; Vossen et al., 1999)).</S>
  </SECTION>
  <SECTION title="Acknowledgments" number="6">
    <S sid="105" ssid="1">This work has been supported, in part, by ONR MiTRI Contract FCP0.810548265, NSA RD-02-5700, and DARPA/ITO Cooperative Agreement N660010028910.</S>
    <S sid="106" ssid="2">The authors would like to thank the anonymous reviewers for their comments, Rebecca Hwa and Okan Kolak for helpful assistance and discussion, Franz Josef Och for his help with GIZA++, Adwait Ratnaparkhi for the use of MXTERMINATOR, and our collaborators at Johns Hopkins for the use of their computing facilities in parts of this work.</S>
  </SECTION>
</PAPER>
