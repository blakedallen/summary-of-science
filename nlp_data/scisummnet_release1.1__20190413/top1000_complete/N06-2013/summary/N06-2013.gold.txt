Arabic Preprocessing Schemes For Statistical Machine Translation
In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality.
Our results show that given large amounts of training data, splitting off only proclitics performs best.
However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation.
Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data.
We show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size.
