<PAPER>
  <S sid="0">A Regression Model of Adjective-Noun Compositionality in Distributional Semantics</S>
  <ABSTRACT>
    <S sid="1" ssid="1">In this paper we explore the computational modelling of compositionality in distributional models of semantics.</S>
    <S sid="2" ssid="2">In particular, we model the semantic composition of pairs of adjacent English Adjecand Nouns from the National We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens.</S>
    <S sid="3" ssid="3">We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model.</S>
    <S sid="4" ssid="4">We propose two evaluation methods for the implemented models.</S>
    <S sid="5" ssid="5">Our study leads to the conclusion that regression-based models of compositionality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research.</S>
  </ABSTRACT>
  <SECTION title="1 Introduction" number="1">
    <S sid="6" ssid="1">Word-space vector models or distributional models of semantics (henceforth DSMs), are computational models that build contextual semantic representations for lexical items from corpus data.</S>
    <S sid="7" ssid="2">DSMs have been successfully used in the recent years for a number of different computational tasks involving semantic relations between words (e.g. synonym identification, computation of semantic similarity, modelling selectional preferences, etc., for a thorough discussion of the field, cf.</S>
    <S sid="8" ssid="3">Sahlgren, 2006).</S>
    <S sid="9" ssid="4">The theoretical foundation of DSMs is to be found in the &#8220;distributional hypothesis of meaning&#8221;, attributed to Z. Harris, which maintains that meaning is susceptible to distributional analysis and, in particular, that differences in meaning between words or morphemes in a language correlate with differences in their distribution (Harris 1970, pp.</S>
    <S sid="10" ssid="5">784&#8211;787).</S>
    <S sid="11" ssid="6">While the vector-based representation of word meaning has been used for a long time in computational linguistics, the techniques that are currently used have not seen much development with regards to one of the main aspects of semantics in natural language: compositionality.</S>
    <S sid="12" ssid="7">To be fair, the study of semantic compositionality in DSMs has seen a slight revival in the recent times, cf.</S>
    <S sid="13" ssid="8">Widdows (2008), Mitchell &amp; Lapata (2008), Giesbrecht (2009), Baroni &amp; Lenci (2009), who propose various DSM approaches to represent argument structure, subject-verb and verb-object co-selection.</S>
    <S sid="14" ssid="9">Current approaches to compositionality in DSMs are based on the application of a simple geometric operation on the basis of individual vectors (vector addition, pointwisemultiplication of corresponding dimensions, tensor product) which should in principle approximate the composition of any two given vectors.</S>
    <S sid="15" ssid="10">On the contrary, since the the very nature of compositionality depends on the semantic relation being instantiated in a syntactic structure, we propose that the composition of vector representations must be modelled as a relation-specific phenomenon.</S>
    <S sid="16" ssid="11">In particular, we propose that the usual procedures from machine learning tasks must be implemented also in the search for semantic compositionality in DSM.</S>
    <S sid="17" ssid="12">In this paper we present work in progress on the computational modelling of compositionality in a data-set of English Adjective-Noun pairs extracted from the BNC.</S>
    <S sid="18" ssid="13">We extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and, finally, a multinomial multiple regression model by Partial Least Squares Regression (PLSR).</S>
  </SECTION>
  <SECTION title="2 Compositionality of meaning in DSMs" number="2">
    <S sid="19" ssid="1">Previous work in the field has produced a small number of operations to represent the composition of vectorial representations of word meaning.</S>
    <S sid="20" ssid="2">In particular, given two independent vectors v1 and v2, the semantically compositional result v3 is modelled by: In the DSM literature, the additive model has become a de facto standard approach to approximate the composed meaning of a group of words (or a document) as the sum of their vectors (which results in the centroid of the starting vectors).</S>
    <S sid="21" ssid="3">This has been successfully applied to document-based applications such as the computation of document similarity in information retrieval.</S>
    <S sid="22" ssid="4">Mitchell &amp; Lapata (2008) indicate that the various variations of the pointwise-multiplication model perform better than simple additive models in term similarity tasks (variations included combination with simple addition and adding weights to individual vector components).</S>
    <S sid="23" ssid="5">Widdows (2008) Obtain results indicating that both the tensor product and the convolution product perform better than the simple additive model.</S>
    <S sid="24" ssid="6">For the sake of simplifying the implementation of evaluation methods, in this paper we will compare the first two approaches, vector addition and vector pointwise-multiplication, with regression modelling by partial least squares.</S>
  </SECTION>
  <SECTION title="3 Partial least squares regression of compositionality" number="3">
    <S sid="25" ssid="1">We assume that the composition of meaning in DSMs is a function mapping two or more independent vectors in a multidimensional space to a newly composed vector the same space and, further, we assume that semantic composition is dependent on the syntactic structure being instantiated in natural language.1 Assuming that each dimension in the starting vectors v1 and v2 is a candidate predictor, and that each dimension in the composed vector v3 is a dependent variable, vector-based semantic compositionality can be formulated as a problem of multivariate multiple regression.</S>
    <S sid="26" ssid="2">This is, in principle, a tractable problem that can be solved by standard machine learning techniques such as multilayer perceptrons or support vector machines.</S>
    <S sid="27" ssid="3">However, given that sequences of words tend to be of very low frequency (and thus difficult to represent in a DSM), suitable data sets will inevitably suffer the curse of dimensionality: we will often have many more variables (dimensions) than observations.</S>
    <S sid="28" ssid="4">Partial Least Squares Regression (PLSR) is a multivariate regression technique that has been designed specifically to tackle such situations with high dimensionality and limited data.</S>
    <S sid="29" ssid="5">PLSR is widely used in in unrelated fields such as spectroscopy, medical chemistry, brain-imaging and marketing (Mevik &amp; Wehrens, 2007).</S>
  </SECTION>
  <SECTION title="4 Materials and tools" number="4">
    <S sid="30" ssid="1">We use a general-purpose vector space extracted from the British National Corpus.</S>
    <S sid="31" ssid="2">We used the Infomap software to collect co-occurrence statistics for lemmas within a rectangular 5L&#8211;5R window.</S>
    <S sid="32" ssid="3">The corpus was pre-processed to represent frequent Adjective-Noun lemma pairs as a single token (e.g. while in the original corpus the A-N phrase nice house consists in two separate lemmas (nice and house), in the processed corpus it appears as a single entry nice_house).</S>
    <S sid="33" ssid="4">The corpus was also processed by stop-word removal.</S>
    <S sid="34" ssid="5">We extracted a list of A-N candidate pairs with simple regex-based queries targeting adjacent sequences composed of [Det/Art&#8211;A&#8211;N] (e.g. that little house).</S>
    <S sid="35" ssid="6">We filtered the candidate list by frequency (&gt; 400) obtaining 1,380 different A-N pairs.</S>
    <S sid="36" ssid="7">The vector space was built with the 40,000 most frequent tokens in the corpus (a cut-off point that included all the extracted A-N pairs).</S>
    <S sid="37" ssid="8">The original dimensions were the 3,000 most frequent content words in the BNC.</S>
    <S sid="38" ssid="9">The vector space was reduced to the first 500 &#8220;latent&#8221; dimensions by SVD as implemented by the Infomap software.</S>
    <S sid="39" ssid="10">Thus, the resulting space consists in a matrix with 40, 000 x 500 dimensions.</S>
    <S sid="40" ssid="11">We then extracted the vector representation for each A-N candidate as well as for each independent constituent, e.g. vectors for nice_house (v3), as well as for nice (v1) and house (v2) were saved.</S>
    <S sid="41" ssid="12">The resulting vector subspace was imported into the R statistical computing environment for the subsequent model building and evaluation.</S>
    <S sid="42" ssid="13">In particular, we produced our regression analysis with the pls package (Mevik &amp; Wehrens, 2007), which implements PLSR and a number of very useful functions for cross-validation, prediction, error analysis, etc.</S>
    <S sid="43" ssid="14">By simply combining the vector representations of the independent Adjectives and Nouns in our data-set (v1 and v2) we built an additive prediction model (v1 + v2) and a simplified pointwise multiplicative prediction model (v1 x v2) for each candidate pair.</S>
    <S sid="44" ssid="15">We also fitted a PLSR model using v1 and v2 as predictors and the corresponding observed pair v3 as dependent variable.</S>
    <S sid="45" ssid="16">The data were divided into a training set (1,000 A-N pairs) and a testing set (the remaining 380 A-N pairs).</S>
    <S sid="46" ssid="17">The model&#8217;s parameters were estimated by performing 10-fold cross-validation during the training phase.</S>
    <S sid="47" ssid="18">In what follows we briefly evaluate the three resulting models of compositionality.</S>
  </SECTION>
  <SECTION title="5 Evaluation" number="5">
    <S sid="48" ssid="1">In order to evaluate the three models of compositionality that were built, we devised two different procedures based on the Euclidean measure of geometric distance.</S>
    <S sid="49" ssid="2">The first method draws a direct comparison of the different predicted vectors for each candidate A-N pair by computing the Euclidean distance between the observed vector and the modelled predictions.</S>
    <S sid="50" ssid="3">We also inspect a general distance matrix for the whole compositionality subspace, i.e. all the observed vectors and all the predicted vectors.</S>
    <S sid="51" ssid="4">We extract the 10 nearest neighbours for the 380 Adjective-Noun pairs in the test set and look for the intended predicted vectors in each case.</S>
    <S sid="52" ssid="5">The idea here is that the best models should produce predictions that are as close as possible to the originally observed A-N vector.</S>
    <S sid="53" ssid="6">Our second evaluation method uses the 10 nearest neighbours of each of the observed A-N pairs in the test set as gold-standard (excluding any modelled predictions), and compares them with the 10 nearest neighbours of each of the corresponding predictions as generated by the models.</S>
    <S sid="54" ssid="7">The aim is to assess if the predictions made by each model share any top-10 neighbours with their corresponding gold-standard.</S>
    <S sid="55" ssid="8">We award 1 point for every shared neighbour.</S>
    <S sid="56" ssid="9">We calculated the Euclidean distance between each observed A-N pair and the corresponding prediction made by each model.</S>
    <S sid="57" ssid="10">On general inspection, it is clear that the approximation of A-N compositional vectors made by PLSR is considerably closer than those produced by the additive and multiplicative models, cf.</S>
    <S sid="58" ssid="11">Table 1.</S>
    <S sid="59" ssid="12">We also computed in detail which of the three predicted composed vectors was closest to the corresponding observation.</S>
    <S sid="60" ssid="13">To this effect we extracted the 10 nearest neighbours for each A-N pair in the test set using the whole compositionality subspace (all the predicted and the original vectors).</S>
    <S sid="61" ssid="14">In 94 cases out of 380, the PLSR intended prediction was the nearest neighbour.</S>
    <S sid="62" ssid="15">Cumulatively, PLSR&#8217;s predictions were in the top-10 nearest neighbour list in 219 out of 380 cases (57.6%).</S>
    <S sid="63" ssid="16">The other models&#8217; performance in this test was negligible, cf.</S>
    <S sid="64" ssid="17">Table 2.</S>
    <S sid="65" ssid="18">Overall, 223 items in the test set had at least one predicted vector in the top-10 list; of these, 219 (98%) were generated by PLSR and the remaining 4 (1%) by the multiplicative model.</S>
    <S sid="66" ssid="19">Since the main use of DSMs is to extract similar vectors from a multidimensional space (representing related documents, distributional synonyms, etc.</S>
    <S sid="67" ssid="20">), we would like to test if the modelling of semantic compositionality is able to produce predictions that are as similar as possible to the originally observed data.</S>
    <S sid="68" ssid="21">A very desirable result would be if any predicted compositional A-N vector could be reliably used instead of the extracted bigram.</S>
    <S sid="69" ssid="22">This could only be achieved if a model&#8217;s predictions show a similar distributional behaviour with respect to the observed vector.</S>
    <S sid="70" ssid="23">To test this idea using our data, we took the 10 nearest neighbours of each of the observed AN pairs in the test set as gold standard.</S>
    <S sid="71" ssid="24">These gold neighbours were extracted from the observation testing subspace, thus excluding any modelled predictions.</S>
    <S sid="72" ssid="25">This is a very restrictive setting: it means that the gold standard for each of the 380 test items is composed of the 10 nearest neighbours from the same 380 items (which may turn out to be not very close at all).</S>
    <S sid="73" ssid="26">We then extracted the 10 nearest neighbours for each of the three modelled predictions, but this time the subspace included all predictions, as well as all the original observations (380 x 4 = 1520 items).</S>
    <S sid="74" ssid="27">Finally, we tested if the predictions made by each model shared any top-10 neighbours with their corresponding gold-standard.</S>
    <S sid="75" ssid="28">We awarded 1 point for every shared neighbour.</S>
    <S sid="76" ssid="29">The results obtained with these evaluation settings were very poor.</S>
    <S sid="77" ssid="30">Only the additive model scored points (48), although the performance was rather disappointing (maximum potential score for the test was 3,800 points).</S>
    <S sid="78" ssid="31">Both the pointwise multiplicative model and the PLSR model failed to retrieve any of the gold standard neighbours.</S>
    <S sid="79" ssid="32">This poor results can be attributed to the very restrictive nature of our gold standard and, also, to the asymmetrical composition of the compared data (gold standard: 3,800 neighbours from a pool of just 380 different items; prediction space: 11,400 neighbours from a pool of 1,520 items).</S>
    <S sid="80" ssid="33">However, given the that DSMs are known for their ability to extract similar items from the same space, we decided to relax our test settings by awarding points not only to shared neighbours, but also to the same model&#8217;s predictions of those neighbours.</S>
    <S sid="81" ssid="34">Thus, given a target neighbour such as good_deal, in our second setting we awarded points not only to the gold standard good_deal, but also to the predictions good_deal_ADD, good_deal_MUL and good_deal_PLSR when evaluating each corresponding model.</S>
    <S sid="82" ssid="35">With these settings the compared spaces become less asymmetrical (gold standard: 7,600 neighbours from a pool of just 380 different items plus predictions; prediction space: 11,400 neighbours from a pool of 1,520 items).</S>
    <S sid="83" ssid="36">The obtained results show a great improvement (max. potential score 7,600 points): Once again, the additive model showed the best performance, followed by PLSR.</S>
    <S sid="84" ssid="37">The multiplicative model&#8217;s performance was negligible.</S>
    <S sid="85" ssid="38">While carrying out these experiments, an unexpected fact became evident.</S>
    <S sid="86" ssid="39">Each of the models in turn produces predictions that are relatively close to each other, regardless of the independent words that were used to calculate the compositional vectors.</S>
    <S sid="87" ssid="40">This has the consequence that the nearest neighbour lists for each model&#8217;s predictions are, by and large, populated by items generated in the same model, as shown in Table 4.</S>
    <S sid="88" ssid="41">Neighbours of predictions from the multiplicative model are all multiplicative.</S>
    <S sid="89" ssid="42">The additive model has the most varied set of neighbours, but the majority of them are additive-neighbours.</S>
    <S sid="90" ssid="43">PLSR shows a mixed behaviour.</S>
    <S sid="91" ssid="44">However, PLSR produced neighbours that find their way into the neighbour sets of both the additive model and the observations.</S>
    <S sid="92" ssid="45">These remarks point in the same direction: every model is a simplified and specialised version of the original space, somewhat more orderly than the observed data, and may give different results depending on the task at stake.</S>
    <S sid="93" ssid="46">PLSR (and to a lesser extent also the multiplicative model) is particularly efficient as generator of neighbours for real vectors, a characteristic that could be applied to guess distributional synonyms of unseen A-N pairs.</S>
    <S sid="94" ssid="47">On the other hand, the additive model (and to a lesser extent PLSR) is especially successful in attracting gold standard neighbours.</S>
    <S sid="95" ssid="48">Overall, even at this experimental stage, PLSR is clearly the model that produces the most consistent results.</S>
  </SECTION>
  <SECTION title="6 Concluding remarks" number="6">
    <S sid="96" ssid="1">This paper proposed a novel method to model the compositionality of meaning in distributional models of semantics.</S>
    <S sid="97" ssid="2">The method, Partial Least Squares Regression, is well known in other dataintensive fields of research, but to our knowledge had never been put to work in computational distributional semantics.</S>
    <S sid="98" ssid="3">Its main advantage is the fact that it is designed to approximate functions in problems of multivariate multiple regression where the number of observations is relatively small if compared to the number of variables (dimensions).</S>
    <S sid="99" ssid="4">We built a DSM targeting a type of semantic composition that has not been treated extensively in the literature before, adjacent A-N pairs.</S>
    <S sid="100" ssid="5">The model built by PLSR performed better than both a simple additive model and a multiplicative model in the first proposed evaluation method.</S>
    <S sid="101" ssid="6">Our second evaluation test (using comparison to a gold standard) gave mixed results: the best performance was obtained by the simple additive model, with PLSR coming in second place.</S>
    <S sid="102" ssid="7">This is work in progress, but the results look very promising.</S>
    <S sid="103" ssid="8">Future developments will certainly focus on the creation of better evaluation methods, as well as on extending the experiments to other techniques (e.g. convolution product as discussed by Widdows, 2008 and Giesbrecht, 2009).</S>
    <S sid="104" ssid="9">Another important issue that we still have not touched is the role played by lexical association (collocations) in the prediction models.</S>
    <S sid="105" ssid="10">We would like to make sure that we are not modelling the compositionality of noncompositional examples.</S>
    <S sid="106" ssid="11">A last word on the view of semantic compositionality suggested by our approach.</S>
    <S sid="107" ssid="12">Modelling compositionality as a machine learning task implies that a great number of different &#8220;types&#8221; of composition (functions combining vectors) may be learned from natural language samples.</S>
    <S sid="108" ssid="13">In principle, any semantic relation instantiated by any syntactic structure could be learned if sufficient data is provided.</S>
    <S sid="109" ssid="14">This approach must be confronted with other linguistic phenomena, also of greater complexity than just a set of bigrams.</S>
    <S sid="110" ssid="15">Finally, we might wonder if there is an upper limit to the number of compositionality functions that we need to learn in natural language, or if there are types of functions that are more difficult, or even impossible, to learn.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="7">
    <S sid="111" ssid="1">Thanks are due to Marco Baroni, Stefan Evert, Roberto Zamparelli and the three anonymous reviewers for their assistance and helpful comments.</S>
  </SECTION>
</PAPER>
