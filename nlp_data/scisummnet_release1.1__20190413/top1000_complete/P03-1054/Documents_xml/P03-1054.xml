<PAPER>
  <S sid="0">Accurate Unlexicalized Parsing</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Figure 3: Size and devset performance of the cumulatively annotated models, starting with the markovized baseline.</S>
    <S sid="2" ssid="2">The two columns show the change in the baseline for each annotation introduced, both cumulatively and for each single annotation applied to the baseline in isolation.</S>
    <S sid="3" ssid="3">history models similar in intent to those described in Ron et al. (1994).</S>
    <S sid="4" ssid="4">For variable horizontal histories, we did not split intermediate states below 10 occurrences of a symbol.</S>
    <S sid="5" ssid="5">For example, if the symbol too rare, we would colit to For vertical histories, we used a cutoff which included both frequency and mutual information between the history and the expansions (this was not appropriate for the horizontal because unreliable at such low counts).</S>
    <S sid="6" ssid="6">Figure 2 shows parsing accuracies as well as the number of symbols in each markovization.</S>
    <S sid="7" ssid="7">These symbol counts include all the intermediate states which represent partially completed constituents.</S>
    <S sid="8" ssid="8">The general trend is that, in the absence of further annotation, more vertical annotation is better &#8211; even exhaustive grandparent annotation.</S>
    <S sid="9" ssid="9">This is not true for horizontal markovization, where the variableorder second-order model was superior.</S>
    <S sid="10" ssid="10">The best has an 79.74, already a substantial improvement over the baseline.</S>
    <S sid="11" ssid="11">In the remaining sections, we discuss other annotations which increasingly split the symbol space.</S>
    <S sid="12" ssid="12">Since we expressly do not smooth the grammar, not all splits are guaranteed to be beneficial, and not all sets of useful splits are guaranteed to co-exist well. particular, while markovization is good on its own, it has a large number of states and does not tolerate further splitting well.</S>
    <S sid="13" ssid="13">Therefore, base all further exploration on the ROOT S&amp;quot;ROOT 4: An error which can be resolved with the (incorrect baseline parse shown). grammar.</S>
    <S sid="14" ssid="14">Although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories.</S>
    <S sid="15" ssid="15">3 External vs. Internal Annotation The two major previous annotation strategies, parent annotation and head lexicalization, can be seen as instances of external and internal annotation, respectively.</S>
    <S sid="16" ssid="16">Parent annotation lets us indicate an important feature of the external environment of a node which influences the internal expansion of that node.</S>
    <S sid="17" ssid="17">On the other hand, lexicalization is a (radical) method of marking a distinctive aspect of the otherwise hidden internal contents of a node which influence the external distribution.</S>
    <S sid="18" ssid="18">Both kinds of annotation can be useful.</S>
    <S sid="19" ssid="19">To identify split states, we suffixes of the form mark internal content and mark external features.</S>
    <S sid="20" ssid="20">To illustrate the difference, consider unary productions.</S>
    <S sid="21" ssid="21">In the raw grammar, there are many unaries, and once any major category is constructed over a span, most others become constructible as well using unary chains (see Klein and Manning (2001) for discussion).</S>
    <S sid="22" ssid="22">Such chains are rare in real treebank trees: unary rewrites only appear in very specific for example of verbs where an empty, controlled subject.</S>
    <S sid="23" ssid="23">Figure 4 shows an erroneous output of the parser, using the baseline markovized grammar.</S>
    <S sid="24" ssid="24">Intuitively, there are several reasons this parse should be ruled out, but is that the lower which is intended prifor of communication verbs, is not a unary rewrite position (such complements usually have subjects).</S>
    <S sid="25" ssid="25">It would therefore be natural to annotate the trees so as to confine unary productions to the contexts in which they are actually ap- We tried two annotations.</S>
    <S sid="26" ssid="26">First, .</S>
    <S sid="27" ssid="27">NP&amp;quot;S VP&amp;quot;S NP&amp;quot;VP VBD , NN S&#710;VP .</S>
    <S sid="28" ssid="28">VP&#710;S QP , NP&amp;quot;VP $ CD CD VBG 444.9 million including , CONJP NP&amp;quot;NP NP&amp;quot;NP $ QP JJ NN , RB RB IN net interest down slightly from CD $ 450.7 million was Revenue $ CD (with a any nonterminal node which has only one child.</S>
    <S sid="29" ssid="29">In isolation, this resulted in an absolute gain of 0.55% (see figure 3).</S>
    <S sid="30" ssid="30">The same sentence, parsed using only the baseline and is parsed correctly, because the in the incorrect parse ends with an very low marked nodes had no siblings with It was similar to solo benefit (0.01% worse), but provided far less marginal benefit on top of later features (none at all on top of our top models), and was One restricted place where external unary annotation was very useful, however, was at the preterminal level, where internal annotation was meaningless.</S>
    <S sid="31" ssid="31">One distributionally salient tag conflation in the Penn treebank is the identification of demonstraand regular determiners based on whether they were only captured this distinction.</S>
    <S sid="32" ssid="32">The same external unary annotation was even more efwhen applied to adverbs disfor example, well Beyond these cases, unary tag marking was detrimen- The 78.86%.</S>
    <S sid="33" ssid="33">4 Tag Splitting The idea that part-of-speech tags are not fine-grained enough to abstract away from specific-word behaviour is a cornerstone of lexicalization.</S>
    <S sid="34" ssid="34">The for example, showed that the determiners which occur alone are usefully distinguished from those which occur with other nomimaterial.</S>
    <S sid="35" ssid="35">This marks the with a single bit about their immediate external context: whether there are sisters.</S>
    <S sid="36" ssid="36">Given the success of parent annotation for nonterminals, it makes sense to parent antags, as well In fact, as figure 3 shows, exhaustively marking all preterminals with their parent category was the most effective single annotation we tried.</S>
    <S sid="37" ssid="37">Why should this be useful?</S>
    <S sid="38" ssid="38">Most tags have a canonical category.</S>
    <S sid="39" ssid="39">For example, occur under (only 234 of 70855 do not, mostly mistakes).</S>
    <S sid="40" ssid="40">However, when a tag that when we show such trees, we generally only show one annotation on top of the baseline at a time.</S>
    <S sid="41" ssid="41">Moreover, we do not explicitly show the binarization implicit by the horizontal markovization. two are not equivalent even given infinite data.</S>
    <S sid="42" ssid="42">5: An error resolved with the (of the (a) the incorrect baseline parse and (b) the correct resolves this error. somewhat regularly occurs in a non-canonical position, its distribution is usually distinct.</S>
    <S sid="43" ssid="43">For example, most common adverbs directly under and Under they are and Under and and so on. substantially, to 80.62%.</S>
    <S sid="44" ssid="44">In addition to the adverb case, the Penn tag set conflates various grammatical distinctions that are commonly made in traditional and generative grammar, and from which a parser could hope to get useful information.</S>
    <S sid="45" ssid="45">For example, subordinating conas, complementizers prepositions in, all get the tag of these distinctions are captured by conjunctions occur under under but are not (both subordinating conjunctions and complementizers appear Also, there are exclusively nounprepositions predominantly verbones and so on.</S>
    <S sid="46" ssid="46">The annotation a linguistically motivated 6-way split the and brought the total to 81.19%.</S>
    <S sid="47" ssid="47">Figure 5 shows an example error in the baseline is equally well fixed by either In this case, the more common nominal of preferred unless the is annoto allow prefer We also got value from three other annotations which subcategorized tags for specific lexemes. we split off auxiliary verbs with the which appends all forms all forms of More miconjunction tags to indicate is an extended uniform version of the partial auxiliary annotation of Charniak (1997), wherein all auxiliaries are as a added to gerund auxiliaries and VP&amp;quot;S VP&amp;quot;S TO VP&amp;quot;VP TO&amp;quot;VP VP&amp;quot;VP to VB PP&amp;quot;VP to VB&amp;quot;VP SBAR&amp;quot;VP see NP&amp;quot;PP see IN&amp;quot;SBAR S&amp;quot;SBAR IN (a) (b) NNS NN if VP&amp;quot;S VBZ&amp;quot;VP works works NN&amp;quot;NP advertising advertising or not they were the strings &amp;, each of which have distinctly different distributions from other conjunctions.</S>
    <S sid="48" ssid="48">Finally, we gave the percent sign (%) its own tag, in line with the dollar sign ($) already having its own.</S>
    <S sid="49" ssid="49">Together these three anbrought the 81.81%.</S>
    <S sid="50" ssid="50">5 What is an Unlexicalized Grammar?</S>
    <S sid="51" ssid="51">Around this point, we must address exactly what we by an To the extent that go about subcategorizing many of them might come to represent a single word.</S>
    <S sid="52" ssid="52">One might thus feel that the approach of this paper is to walk down a slippery slope, and that we are merely arguing degrees.</S>
    <S sid="53" ssid="53">However, we believe that there is a fundamental qualitative distinction, grounded in linguistic practice, between what we see as permitted an unlexicalized against what one finds hopes to exploit in lexicalized The division rests on the traditional distinction between words closed-class words) and open class or lexical words).</S>
    <S sid="54" ssid="54">It is standard practice in linguistics, dating back decades, to annotate phrasal nodes with important functiondistinctions, for example to have a a whereas content words are not part of grammatical structure, and one would not have sperules or constraints for an for example.</S>
    <S sid="55" ssid="55">We follow this approach in our model: various closed classes are subcategorized to better represent important distinctions, and important features commonly expressed by function words are annotated phrasal nodes (such as whether a finite, or a participle, or an infinitive clause).</S>
    <S sid="56" ssid="56">However, no use is made of lexical class words, to provide either or bilexical At any rate, we have kept ourselves honest by estimating our models exclusively by maximum likelihood estimation over our subcategorized grammar, without any form of interpolation or shrinkage to unsubcategorized categories (although we do rules, as explained above).</S>
    <S sid="57" ssid="57">This effecshould be noted that we started with four tags in the Penn tagset that rewrite as a single word: and some of the punctuation tags, which rewrite as barely more.</S>
    <S sid="58" ssid="58">To the extent that we subcategorize tags, there will be more such cases, but many of them already exist in other tag sets.</S>
    <S sid="59" ssid="59">For instance, many tag sets, such as the Brown and tagsets give a separate sets of tags to each form of verbal auxiliaries and most of which rewrite as only a single word (and any corresponding contractions).</S>
    <S sid="60" ssid="60">6: An error resolved with the (a) incorrect baseline parse and (b) the correct tively means that the subcategories that we break off must themselves be very frequent in the language.</S>
    <S sid="61" ssid="61">In such a framework, if we try to annotate categories with any detailed lexical information, many sentences either entirely fail to parse, or have only extremely weird parses.</S>
    <S sid="62" ssid="62">The resulting battle against sparsity means that we can only afford to make a few distinctions which have major distributional impact.</S>
    <S sid="63" ssid="63">Even with the individual-lexeme annotations in this section, the grammar still has only 9255 states compared to the 7619 of the baseline model.</S>
    <S sid="64" ssid="64">6 Annotations Already in the Treebank At this point, one might wonder as to the wisdom of stripping off all treebank functional tags, only to heuristically add other such markings back in to the grammar.</S>
    <S sid="65" ssid="65">By and large, the treebank out-of-the tags, such as have negative utility.</S>
    <S sid="66" ssid="66">Recall that the raw treebank gramwith no annotation or markovization, had an of 72.62% on our development set.</S>
    <S sid="67" ssid="67">With the functional annotation left in, this drops to 71.49%.</S>
    <S sid="68" ssid="68">The v markovization baseline of 77.77% dropped even further, all the way to 72.87%, when these annotations were included.</S>
    <S sid="69" ssid="69">Nonetheless, some distinctions present in the raw trees were valuable.</S>
    <S sid="70" ssid="70">For example, an an could be either a temporal a For the annotation we retained the on and, furthermore, propathe tag down to the tag of the head of the This is illustrated in figure 6, which also shows an of its utility, clarifying that last night is not a plausible compound and facilitating the othunusual high attachment of the smaller the cumulative 82.25%.</S>
    <S sid="71" ssid="71">Note that this technique of pushing the functional tags down to preterminals might be useful more generfor example, locative expand roughly the VP&#710;VP VP&#710;VP VB to NP&#710;VP appear NN&#710;TMP NP&#710;NP PP&#710;NP JJ PP&#710;NP NP&#710;NP appear NP&#710;VP VB NP-TMP&#710;VP to IN NNS last night JJ times three NNP NN on CD on times three last CNN night NP&#710;PP NNP CNN NP&#710;PP IN NNS CD (a) (b) ROOT Distance S&#710;ROOT S&#710;ROOT 7: An error resolved with the (a) incorrect baseline parse and (b) the correct way as all other (usually as but do tend to have different prepositions below A second kind of information in the original trees is the presence of empty elements.</S>
    <S sid="72" ssid="72">Following (1999), the annotation nodes which have an empty subject (i.e., raising and constructions).</S>
    <S sid="73" ssid="73">This brought 82.28%.</S>
    <S sid="74" ssid="74">7 Head Annotation The notion that the head word of a constituent can affect its behavior is a useful one.</S>
    <S sid="75" ssid="75">However, often the head tag is as good (or better) an indicator of how constituent will We found several head annotations to be particularly effective.</S>
    <S sid="76" ssid="76">First, poshave a very different distribution than &#8211; in particular, are only used in the treebank when the leftmost child is possessive (as opposed to other imaginable uses like for York which is left flat).</S>
    <S sid="77" ssid="77">To address this, all possessive This brought total 83.06%.</S>
    <S sid="78" ssid="78">Second, the is very overloaded in the Penn treebank, most severely in that there is no distinction between finite and in- An example of the damage this conflation can do is given in figure 7, where one needs to capture the fact that present-tense verbs do not take bare infinitive To allow the finite/non-finite distinction, and other verb distinctions, all with their head tag, merging all finite forms to a sintag In particular, this also accomplished This was extremely bringing the cumulative 85.72%, 2.66% absolute improvement (more than its solo improvement over the baseline). is part of the explanation of why (Charniak, 2000) finds that early generation of head tags as in (Collins, 1999) is so beneficial.</S>
    <S sid="79" ssid="79">The rest of the benefit is presumably in the availability of the tags for smoothing purposes.</S>
    <S sid="80" ssid="80">Error analysis at this point suggested that many remaining errors were attachment level and conjunction scope.</S>
    <S sid="81" ssid="81">While these kinds of errors are undoubtedly profitable targets for lexical preference, most attachment mistakes were overly high attachments, indicating that the overall right-branching tendency of English was not being captured.</S>
    <S sid="82" ssid="82">Indeed, this tenis a difficult trend to capture in a because often the high and low attachments involve the very same rules.</S>
    <S sid="83" ssid="83">Even if not, attachment height is modeled by a it is somehow explicitly encoded into category labels.</S>
    <S sid="84" ssid="84">More complex parsing models have indirectly overcome this by modeling distance (rather than height). distance is difficult to encode in a &#8211; marking nodes with the size of their yields masmultiplies the state Therefore, we wish to find indirect indicators that distinguish high from low ones.</S>
    <S sid="85" ssid="85">In the case of two a with the question of whether the a second modifier of the leftmost should attach lower, inside the first the important distinction is usually that the lower site is a base Collins (1999) captures this by introducing the notion of a base in any dominates only preterminals is with a Further, if an not have non-base it is given one with a unary production.</S>
    <S sid="86" ssid="86">This was helpful, but substantially less than marking base the unary, whose presence actually erased a useful indicator &#8211; base are more frequent in subject position than object position, for example.</S>
    <S sid="87" ssid="87">In isolation, the Collins method actually hurt the base- (absolute cost to 0.37%), while skipping the unary insertion added an absolute 0.73% to the and brought the cumulative 86.04%. the case of attachment of a an eiabove or inside a relative clause, the high is distinct from the low one in that the already modified one contains a verb (and the low one may be base well).</S>
    <S sid="88" ssid="88">This is a partial explanation of the utility of verbal distance in Collins (1999).</S>
    <S sid="89" ssid="89">To inability to encode distance naturally in a naive somewhat ironic.</S>
    <S sid="90" ssid="90">In the heart of any the fundamental table entry or chart item is a label over a span, for exan position 0 to position 5.</S>
    <S sid="91" ssid="91">The concrete use of a grammar rule is to take two adjacent span-marked labels and them (for example and into Yet, only the labels are used to score the combination.</S>
    <S sid="92" ssid="92">&#8220; DT &#8220; NP&#710;S VBZ VP&#710;VP VP&#710;S !</S>
    <S sid="93" ssid="93">.</S>
    <S sid="94" ssid="94">&#8221; &#8221; &#8220; &#8220; NP&#710;S DT NP&#710;VP &#8221; .</S>
    <S sid="95" ssid="95">!</S>
    <S sid="96" ssid="96">&#8221; VP&#710;S-VBF VBZ (a) (b) NP&#710;VP buying This is This NN NN panic buying LP LR Exact CB 0 CB Magerman (1995) 84.9 84.6 1.26 56.6 Collins (1996) 86.3 85.8 1.14 59.9 this paper 86.9 85.7 86.3 30.9 1.10 60.3 Charniak (1997) 87.4 87.5 1.00 62.1 Collins (1999) 88.7 88.6 0.90 67.1 Figure 8: Results of the final model on the test set (section 23). this, all nodes which any verbal node with a This the cumulative 86.91%.</S>
    <S sid="97" ssid="97">We also tried marking nodes which dominated prepositions and/or conjunctions, but these features did not help the cumulative hill-climb.</S>
    <S sid="98" ssid="98">The final distance/depth feature we used was an explicit attempt to model depth, rather than use distance and linear intervention as a proxy.</S>
    <S sid="99" ssid="99">With we marked all which contained their right periphery (i.e., as a rightmost descendant).</S>
    <S sid="100" ssid="100">This captured some further attachment trends, and brought us to a final develop- 87.04%.</S>
    <S sid="101" ssid="101">9 Final Results We took the final model and used it to parse section 23 of the treebank.</S>
    <S sid="102" ssid="102">Figure 8 shows the re- The test set 86.32% for words, already higher than early lexicalized models, though of course lower than the state-of-the-art parsers.</S>
    <S sid="103" ssid="103">10 Conclusion The advantages of unlexicalized grammars are clear enough &#8211; easy to estimate, easy to parse with, and timeand space-efficient.</S>
    <S sid="104" ssid="104">However, the dismal performance of basic unannotated unlexicalized grammars has generally rendered those advantages irrelevant.</S>
    <S sid="105" ssid="105">Here, we have shown that, surprisingly, the maximum-likelihood estimate of a compact unlexiparse on par with early lexicalized parsers.</S>
    <S sid="106" ssid="106">We do not want to argue that lexical selection is not a worthwhile component of a state-ofthe-art parser &#8211; certain attachments, at least, require it &#8211; though perhaps its necessity has been overstated.</S>
    <S sid="107" ssid="107">Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well.</S>
  </ABSTRACT>
  <SECTION title="" number="1">
    <S sid="108" ssid="1">categories appearing in the Penn treebank.</S>
    <S sid="109" ssid="2">Charniak (2000) shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating &#8220;base NPs&#8221; from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP.</S>
    <S sid="110" ssid="3">While he gives incomplete experimental results as to their efficacy, we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization.</S>
    <S sid="111" ssid="4">In this paper, we show that the parsing performance that can be achieved by an unlexicalized PCFG is far higher than has previously been demonstrated, and is, indeed, much higher than community wisdom has thought possible.</S>
    <S sid="112" ssid="5">We describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla PCFG and state-of-the-art lexicalized models.</S>
    <S sid="113" ssid="6">Specifically, we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of Magerman (1995) and Collins (1996) (though not more recent models, such as Charniak (1997) or Collins (1999)).</S>
    <S sid="114" ssid="7">One benefit of this result is a much-strengthened lower bound on the capacity of an unlexicalized PCFG.</S>
    <S sid="115" ssid="8">To the extent that no such strong baseline has been provided, the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing, rather than looking critically at where lexicalized probabilities are both needed to make the right decision and available in the training data.</S>
    <S sid="116" ssid="9">Secondly, this result affirms the value of linguistic analysis for feature discovery.</S>
    <S sid="117" ssid="10">The result has other uses and advantages: an unlexicalized PCFG is easier to interpret, reason about, and improve than the more complex lexicalized models.</S>
    <S sid="118" ssid="11">The grammar representation is much more compact, no longer requiring large structures that store lexicalized probabilities.</S>
    <S sid="119" ssid="12">The parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories, for example dividing verb phrases into finite and non-finite verb phrases, rather than in the modern restricted usage where the term refers only to the syntactic argument frames of predicators.</S>
    <S sid="120" ssid="13">4O(n3) vs. O(n5) for a naive implementation, or vs. O(n4) if using the clever approach of Eisner and Satta (1999). constants.</S>
    <S sid="121" ssid="14">An unlexicalized PCFG parser is much simpler to build and optimize, including both standard code optimization techniques and the investigation of methods for search space pruning (Caraballo and Charniak, 1998; Charniak et al., 1998).</S>
    <S sid="122" ssid="15">It is not our goal to argue against the use of lexicalized probabilities in high-performance probabilistic parsing.</S>
    <S sid="123" ssid="16">It has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities, and a parser should make use of such information where possible.</S>
    <S sid="124" ssid="17">We focus here on using unlexicalized, structural context because we feel that this information has been underexploited and underappreciated.</S>
    <S sid="125" ssid="18">We see this investigation as only one part of the foundation for state-of-the-art parsing which employs both lexical and structural conditioning.</S>
  </SECTION>
  <SECTION title="1 Experimental Setup" number="2">
    <S sid="126" ssid="1">To facilitate comparison with previous work, we trained our models on sections 2&#8211;21 of the WSJ section of the Penn treebank.</S>
    <S sid="127" ssid="2">We used the first 20 files (393 sentences) of section 22 as a development set (devset).</S>
    <S sid="128" ssid="3">This set is small enough that there is noticeable variance in individual results, but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill-climb.</S>
    <S sid="129" ssid="4">All of section 23 was used as a test set for the final model.</S>
    <S sid="130" ssid="5">For each model, input trees were annotated or transformed in some way, as in Johnson (1998).</S>
    <S sid="131" ssid="6">Given a set of transformed trees, we viewed the local trees as grammar rewrite rules in the standard way, and used (unsmoothed) maximum-likelihood estimates for rule probabilities.5 To parse the grammar, we used a simple array-based Java implementation of a generalized CKY parser, which, for our final best model, was able to exhaustively parse all sentences in section 23 in 1GB of memory, taking approximately 3 sec for average length sentences.6 The traditional starting point for unlexicalized parsing is the raw n-ary treebank grammar read from training trees (after removing functional tags and null elements).</S>
    <S sid="132" ssid="7">This basic grammar is imperfect in two well-known ways.</S>
    <S sid="133" ssid="8">First, the category symbols are too coarse to adequately render the expansions independent of the contexts.</S>
    <S sid="134" ssid="9">For example, subject NP expansions are very different from object NP expansions: a subject NP is 8.7 times more likely than an object NP to expand as just a pronoun.</S>
    <S sid="135" ssid="10">Having separate symbols for subject and object NPs allows this variation to be captured and used to improve parse scoring.</S>
    <S sid="136" ssid="11">One way of capturing this kind of external context is to use parent annotation, as presented in Johnson (1998).</S>
    <S sid="137" ssid="12">For example, NPs with S parents (like subjects) will be marked NP&amp;quot;S, while NPs with VP parents (like objects) will be NP&amp;quot;VP.</S>
    <S sid="138" ssid="13">The second basic deficiency is that many rule types have been seen only once (and therefore have their probabilities overestimated), and many rules which occur in test sentences will never have been seen in training (and therefore have their probabilities underestimated &#8211; see Collins (1999) for analysis).</S>
    <S sid="139" ssid="14">Note that in parsing with the unsplit grammar, not having seen a rule doesn&#8217;t mean one gets a parse failure, but rather a possibly very weird parse (Charniak, 1996).</S>
    <S sid="140" ssid="15">One successful method of combating sparsity is to markovize the rules (Collins, 1999).</S>
    <S sid="141" ssid="16">In particular, we follow that work in markovizing out from the head child, despite the grammar being unlexicalized, because this seems the best way to capture the traditional linguistic insight that phrases are organized around a head (Radford, 1988).</S>
    <S sid="142" ssid="17">Both parent annotation (adding context) and RHS markovization (removing it) can be seen as two instances of the same idea.</S>
    <S sid="143" ssid="18">In parsing, every node has a vertical history, including the node itself, parent, grandparent, and so on.</S>
    <S sid="144" ssid="19">A reasonable assumption is that only the past v vertical ancestors matter to the current expansion.</S>
    <S sid="145" ssid="20">Similarly, only the previous h horizontal ancestors matter (we assume that the head child always matters).</S>
    <S sid="146" ssid="21">It is a historical accident that the default notion of a treebank PCFG grammar takes v = 1 (only the current node matters vertically) and h = oc (rule right hand sides do not decompose at all).</S>
    <S sid="147" ssid="22">On this view, it is unsurprising that increasing v and decreasing h have historically helped.</S>
    <S sid="148" ssid="23">As an example, consider the case of v = 1, h = 1.</S>
    <S sid="149" ssid="24">If we start with the rule VP &#8594; VBZ NP PP PP, it will be broken into several stages, each a binary or unary rule, which conceptually represent a head-outward generation of the right hand size, as shown in figure 1.</S>
    <S sid="150" ssid="25">The bottom layer will be a unary over the head declaring the goal: (VP: [VBZ]) &#8594; VBZ.</S>
    <S sid="151" ssid="26">The square brackets indicate that the VBZ is the head, while the angle brackets (X) indicates that the symbol (X) is an intermediate symbol (equivalently, an active or incomplete state).</S>
    <S sid="152" ssid="27">The next layer up will generate the first rightward sibling of the head child: (VP: [VBZ]... NP) &#8594; (VP: [VBZ]) NP.</S>
    <S sid="153" ssid="28">Next, the PP is generated: (VP: [VBZ]... PP) &#8594; (VP: [VBZ]... NP) PP.</S>
    <S sid="154" ssid="29">We would then branch off left siblings if there were any.7 Finally, we have another unary to finish the VP.</S>
    <S sid="155" ssid="30">Note that while it is convenient to think of this as a head-outward process, these are just PCFG rewrites, and so the actual scores attached to each rule will correspond to a downward generation order.</S>
    <S sid="156" ssid="31">Figure 2 presents a grid of horizontal and vertical markovizations of the grammar.</S>
    <S sid="157" ssid="32">The raw treebank grammar corresponds to v = 1, h = oc (the upper right corner), while the parent annotation in (Johnson, 1998) corresponds to v = 2, h = oc, and the second-order model in Collins (1999), is broadly a smoothed version of v = 2, h = 2.</S>
    <S sid="158" ssid="33">In addition to exact nth-order models, we tried variablenotated models, starting with the markovized baseline.</S>
    <S sid="159" ssid="34">The right two columns show the change in F1 from the baseline for each annotation introduced, both cumulatively and for each single annotation applied to the baseline in isolation.</S>
    <S sid="160" ssid="35">history models similar in intent to those described in Ron et al. (1994).</S>
    <S sid="161" ssid="36">For variable horizontal histories, we did not split intermediate states below 10 occurrences of a symbol.</S>
    <S sid="162" ssid="37">For example, if the symbol (VP: [VBZ]... PP PP) were too rare, we would collapse it to (VP: [VBZ]... PP).</S>
    <S sid="163" ssid="38">For vertical histories, we used a cutoff which included both frequency and mutual information between the history and the expansions (this was not appropriate for the horizontal case because MI is unreliable at such low counts).</S>
    <S sid="164" ssid="39">Figure 2 shows parsing accuracies as well as the number of symbols in each markovization.</S>
    <S sid="165" ssid="40">These symbol counts include all the intermediate states which represent partially completed constituents.</S>
    <S sid="166" ssid="41">The general trend is that, in the absence of further annotation, more vertical annotation is better &#8211; even exhaustive grandparent annotation.</S>
    <S sid="167" ssid="42">This is not true for horizontal markovization, where the variableorder second-order model was superior.</S>
    <S sid="168" ssid="43">The best entry, v = 3, h &lt; 2, has an F1 of 79.74, already a substantial improvement over the baseline.</S>
    <S sid="169" ssid="44">In the remaining sections, we discuss other annotations which increasingly split the symbol space.</S>
    <S sid="170" ssid="45">Since we expressly do not smooth the grammar, not all splits are guaranteed to be beneficial, and not all sets of useful splits are guaranteed to co-exist well.</S>
    <S sid="171" ssid="46">In particular, while v = 3, h &lt; 2 markovization is good on its own, it has a large number of states and does not tolerate further splitting well.</S>
    <S sid="172" ssid="47">Therefore, we base all further exploration on the v &lt; 2, h &lt; 2 grammar.</S>
    <S sid="173" ssid="48">Although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories.</S>
  </SECTION>
  <SECTION title="3 External vs. Internal Annotation" number="3">
    <S sid="174" ssid="1">The two major previous annotation strategies, parent annotation and head lexicalization, can be seen as instances of external and internal annotation, respectively.</S>
    <S sid="175" ssid="2">Parent annotation lets us indicate an important feature of the external environment of a node which influences the internal expansion of that node.</S>
    <S sid="176" ssid="3">On the other hand, lexicalization is a (radical) method of marking a distinctive aspect of the otherwise hidden internal contents of a node which influence the external distribution.</S>
    <S sid="177" ssid="4">Both kinds of annotation can be useful.</S>
    <S sid="178" ssid="5">To identify split states, we add suffixes of the form -X to mark internal content features, and &amp;quot;X to mark external features.</S>
    <S sid="179" ssid="6">To illustrate the difference, consider unary productions.</S>
    <S sid="180" ssid="7">In the raw grammar, there are many unaries, and once any major category is constructed over a span, most others become constructible as well using unary chains (see Klein and Manning (2001) for discussion).</S>
    <S sid="181" ssid="8">Such chains are rare in real treebank trees: unary rewrites only appear in very specific contexts, for example S complements of verbs where the S has an empty, controlled subject.</S>
    <S sid="182" ssid="9">Figure 4 shows an erroneous output of the parser, using the baseline markovized grammar.</S>
    <S sid="183" ssid="10">Intuitively, there are several reasons this parse should be ruled out, but one is that the lower S slot, which is intended primarily for S complements of communication verbs, is not a unary rewrite position (such complements usually have subjects).</S>
    <S sid="184" ssid="11">It would therefore be natural to annotate the trees so as to confine unary productions to the contexts in which they are actually appropriate.</S>
    <S sid="185" ssid="12">We tried two annotations.</S>
    <S sid="186" ssid="13">First, UNARYINTERNAL marks (with a -U) any nonterminal node which has only one child.</S>
    <S sid="187" ssid="14">In isolation, this resulted in an absolute gain of 0.55% (see figure 3).</S>
    <S sid="188" ssid="15">The same sentence, parsed using only the baseline and UNARY-INTERNAL, is parsed correctly, because the VP rewrite in the incorrect parse ends with an S&amp;quot;VPU with very low probability.8 Alternately, UNARY-EXTERNAL, marked nodes which had no siblings with &amp;quot;U.</S>
    <S sid="189" ssid="16">It was similar to UNARY-INTERNAL in solo benefit (0.01% worse), but provided far less marginal benefit on top of other later features (none at all on top of UNARYINTERNAL for our top models), and was discarded.9 One restricted place where external unary annotation was very useful, however, was at the preterminal level, where internal annotation was meaningless.</S>
    <S sid="190" ssid="17">One distributionally salient tag conflation in the Penn treebank is the identification of demonstratives (that, those) and regular determiners (the, a).</S>
    <S sid="191" ssid="18">Splitting DT tags based on whether they were only children (UNARY-DT) captured this distinction.</S>
    <S sid="192" ssid="19">The same external unary annotation was even more effective when applied to adverbs (UNARY-RB), distinguishing, for example, as well from also).</S>
    <S sid="193" ssid="20">Beyond these cases, unary tag marking was detrimental.</S>
    <S sid="194" ssid="21">The F1 after UNARY-INTERNAL, UNARY-DT, and UNARY-RB was 78.86%.</S>
  </SECTION>
  <SECTION title="4 Tag Splitting" number="4">
    <S sid="195" ssid="1">The idea that part-of-speech tags are not fine-grained enough to abstract away from specific-word behaviour is a cornerstone of lexicalization.</S>
    <S sid="196" ssid="2">The UNARY-DT annotation, for example, showed that the determiners which occur alone are usefully distinguished from those which occur with other nominal material.</S>
    <S sid="197" ssid="3">This marks the DT nodes with a single bit about their immediate external context: whether there are sisters.</S>
    <S sid="198" ssid="4">Given the success of parent annotation for nonterminals, it makes sense to parent annotate tags, as well (TAG-PA).</S>
    <S sid="199" ssid="5">In fact, as figure 3 shows, exhaustively marking all preterminals with their parent category was the most effective single annotation we tried.</S>
    <S sid="200" ssid="6">Why should this be useful?</S>
    <S sid="201" ssid="7">Most tags have a canonical category.</S>
    <S sid="202" ssid="8">For example, NNS tags occur under NP nodes (only 234 of 70855 do not, mostly mistakes).</S>
    <S sid="203" ssid="9">However, when a tag somewhat regularly occurs in a non-canonical position, its distribution is usually distinct.</S>
    <S sid="204" ssid="10">For example, the most common adverbs directly under ADVP are also (1599) and now (544).</S>
    <S sid="205" ssid="11">Under VP, they are n&#8217;t (3779) and not (922).</S>
    <S sid="206" ssid="12">Under NP, only (215) and just (132), and so on.</S>
    <S sid="207" ssid="13">TAG-PA brought F1 up substantially, to 80.62%.</S>
    <S sid="208" ssid="14">In addition to the adverb case, the Penn tag set conflates various grammatical distinctions that are commonly made in traditional and generative grammar, and from which a parser could hope to get useful information.</S>
    <S sid="209" ssid="15">For example, subordinating conjunctions (while, as, if), complementizers (that, for), and prepositions (of, in, from) all get the tag IN.</S>
    <S sid="210" ssid="16">Many of these distinctions are captured by TAGPA (subordinating conjunctions occur under S and prepositions under PP), but are not (both subordinating conjunctions and complementizers appear under SBAR).</S>
    <S sid="211" ssid="17">Also, there are exclusively nounmodifying prepositions (of), predominantly verbmodifying ones (as), and so on.</S>
    <S sid="212" ssid="18">The annotation SPLIT-IN does a linguistically motivated 6-way split of the IN tag, and brought the total to 81.19%.</S>
    <S sid="213" ssid="19">Figure 5 shows an example error in the baseline which is equally well fixed by either TAG-PA or SPLIT-IN.</S>
    <S sid="214" ssid="20">In this case, the more common nominal use of works is preferred unless the IN tag is annotated to allow if to prefer S complements.</S>
    <S sid="215" ssid="21">We also got value from three other annotations which subcategorized tags for specific lexemes.</S>
    <S sid="216" ssid="22">First we split off auxiliary verbs with the SPLITAUX annotation, which appends &amp;quot;BE to all forms of be and &amp;quot;HAVE to all forms of have.10 More minorly, SPLIT-CC marked conjunction tags to indicate whether or not they were the strings [Bb]ut or &amp;, each of which have distinctly different distributions from other conjunctions.</S>
    <S sid="217" ssid="23">Finally, we gave the percent sign (%) its own tag, in line with the dollar sign ($) already having its own.</S>
    <S sid="218" ssid="24">Together these three annotations brought the F1 to 81.81%.</S>
  </SECTION>
  <SECTION title="5 What is an Unlexicalized Grammar?" number="5">
    <S sid="219" ssid="1">Around this point, we must address exactly what we mean by an unlexicalized PCFG.</S>
    <S sid="220" ssid="2">To the extent that we go about subcategorizing POS categories, many of them might come to represent a single word.</S>
    <S sid="221" ssid="3">One might thus feel that the approach of this paper is to walk down a slippery slope, and that we are merely arguing degrees.</S>
    <S sid="222" ssid="4">However, we believe that there is a fundamental qualitative distinction, grounded in linguistic practice, between what we see as permitted in an unlexicalized PCFG as against what one finds and hopes to exploit in lexicalized PCFGs.</S>
    <S sid="223" ssid="5">The division rests on the traditional distinction between function words (or closed-class words) and content words (or open class or lexical words).</S>
    <S sid="224" ssid="6">It is standard practice in linguistics, dating back decades, to annotate phrasal nodes with important functionword distinctions, for example to have a CP[for] or a PP[to], whereas content words are not part of grammatical structure, and one would not have special rules or constraints for an NP[stocks], for example.</S>
    <S sid="225" ssid="7">We follow this approach in our model: various closed classes are subcategorized to better represent important distinctions, and important features commonly expressed by function words are annotated onto phrasal nodes (such as whether a VP is finite, or a participle, or an infinitive clause).</S>
    <S sid="226" ssid="8">However, no use is made of lexical class words, to provide either monolexical or bilexical probabilities.11 At any rate, we have kept ourselves honest by estimating our models exclusively by maximum likelihood estimation over our subcategorized grammar, without any form of interpolation or shrinkage to unsubcategorized categories (although we do markovize rules, as explained above).</S>
    <S sid="227" ssid="9">This effectively means that the subcategories that we break off must themselves be very frequent in the language.</S>
    <S sid="228" ssid="10">In such a framework, if we try to annotate categories with any detailed lexical information, many sentences either entirely fail to parse, or have only extremely weird parses.</S>
    <S sid="229" ssid="11">The resulting battle against sparsity means that we can only afford to make a few distinctions which have major distributional impact.</S>
    <S sid="230" ssid="12">Even with the individual-lexeme annotations in this section, the grammar still has only 9255 states compared to the 7619 of the baseline model.</S>
  </SECTION>
  <SECTION title="6 Annotations Already in the Treebank" number="6">
    <S sid="231" ssid="1">At this point, one might wonder as to the wisdom of stripping off all treebank functional tags, only to heuristically add other such markings back in to the grammar.</S>
    <S sid="232" ssid="2">By and large, the treebank out-of-the package tags, such as PP-LOC or ADVP-TMP, have negative utility.</S>
    <S sid="233" ssid="3">Recall that the raw treebank grammar, with no annotation or markovization, had an F1 of 72.62% on our development set.</S>
    <S sid="234" ssid="4">With the functional annotation left in, this drops to 71.49%.</S>
    <S sid="235" ssid="5">The h &lt; 2, v &lt; 1 markovization baseline of 77.77% dropped even further, all the way to 72.87%, when these annotations were included.</S>
    <S sid="236" ssid="6">Nonetheless, some distinctions present in the raw treebank trees were valuable.</S>
    <S sid="237" ssid="7">For example, an NP with an S parent could be either a temporal NP or a subject.</S>
    <S sid="238" ssid="8">For the annotation TMP-NP, we retained the original -TMP tags on NPs, and, furthermore, propagated the tag down to the tag of the head of the NP.</S>
    <S sid="239" ssid="9">This is illustrated in figure 6, which also shows an example of its utility, clarifying that CNN last night is not a plausible compound and facilitating the otherwise unusual high attachment of the smaller NP.</S>
    <S sid="240" ssid="10">TMP-NP brought the cumulative F1 to 82.25%.</S>
    <S sid="241" ssid="11">Note that this technique of pushing the functional tags down to preterminals might be useful more generally; for example, locative PPs expand roughly the same way as all other PPs (usually as IN NP), but they do tend to have different prepositions below IN.</S>
    <S sid="242" ssid="12">A second kind of information in the original trees is the presence of empty elements.</S>
    <S sid="243" ssid="13">Following Collins (1999), the annotation GAPPED-S marks S nodes which have an empty subject (i.e., raising and control constructions).</S>
    <S sid="244" ssid="14">This brought F1 to 82.28%.</S>
  </SECTION>
  <SECTION title="7 Head Annotation" number="7">
    <S sid="245" ssid="1">The notion that the head word of a constituent can affect its behavior is a useful one.</S>
    <S sid="246" ssid="2">However, often the head tag is as good (or better) an indicator of how a constituent will behave.12 We found several head annotations to be particularly effective.</S>
    <S sid="247" ssid="3">First, possessive NPs have a very different distribution than other NPs &#8211; in particular, NP &#8594; NP &#945; rules are only used in the treebank when the leftmost child is possessive (as opposed to other imaginable uses like for New York lawyers, which is left flat).</S>
    <S sid="248" ssid="4">To address this, POSS-NP marked all possessive NPs.</S>
    <S sid="249" ssid="5">This brought the total F1 to 83.06%.</S>
    <S sid="250" ssid="6">Second, the VP symbol is very overloaded in the Penn treebank, most severely in that there is no distinction between finite and infinitival VPs.</S>
    <S sid="251" ssid="7">An example of the damage this conflation can do is given in figure 7, where one needs to capture the fact that present-tense verbs do not generally take bare infinitive VP complements.</S>
    <S sid="252" ssid="8">To allow the finite/non-finite distinction, and other verb type distinctions, SPLIT-VP annotated all VP nodes with their head tag, merging all finite forms to a single tag VBF.</S>
    <S sid="253" ssid="9">In particular, this also accomplished Charniak&#8217;s gerund-VP marking.</S>
    <S sid="254" ssid="10">This was extremely useful, bringing the cumulative F1 to 85.72%, 2.66% absolute improvement (more than its solo improvement over the baseline).</S>
    <S sid="255" ssid="11">Error analysis at this point suggested that many remaining errors were attachment level and conjunction scope.</S>
    <S sid="256" ssid="12">While these kinds of errors are undoubtedly profitable targets for lexical preference, most attachment mistakes were overly high attachments, indicating that the overall right-branching tendency of English was not being captured.</S>
    <S sid="257" ssid="13">Indeed, this tendency is a difficult trend to capture in a PCFG because often the high and low attachments involve the very same rules.</S>
    <S sid="258" ssid="14">Even if not, attachment height is not modeled by a PCFG unless it is somehow explicitly encoded into category labels.</S>
    <S sid="259" ssid="15">More complex parsing models have indirectly overcome this by modeling distance (rather than height).</S>
    <S sid="260" ssid="16">Linear distance is difficult to encode in a PCFG &#8211; marking nodes with the size of their yields massively multiplies the state space.13 Therefore, we wish to find indirect indicators that distinguish high attachments from low ones.</S>
    <S sid="261" ssid="17">In the case of two PPs following a NP, with the question of whether the second PP is a second modifier of the leftmost NP or should attach lower, inside the first PP, the important distinction is usually that the lower site is a non-recursive base NP.</S>
    <S sid="262" ssid="18">Collins (1999) captures this notion by introducing the notion of a base NP, in which any NP which dominates only preterminals is marked with a -B.</S>
    <S sid="263" ssid="19">Further, if an NP-B does not have a non-base NP parent, it is given one with a unary production.</S>
    <S sid="264" ssid="20">This was helpful, but substantially less effective than marking base NPs without introducing the unary, whose presence actually erased a useful internal indicator &#8211; base NPs are more frequent in subject position than object position, for example.</S>
    <S sid="265" ssid="21">In isolation, the Collins method actually hurt the baseline (absolute cost to F1 of 0.37%), while skipping the unary insertion added an absolute 0.73% to the baseline, and brought the cumulative F1 to 86.04%.</S>
    <S sid="266" ssid="22">In the case of attachment of a PP to an NP either above or inside a relative clause, the high NP is distinct from the low one in that the already modified one contains a verb (and the low one may be a base NP as well).</S>
    <S sid="267" ssid="23">This is a partial explanation of the utility of verbal distance in Collins (1999).</S>
    <S sid="268" ssid="24">To 13The inability to encode distance naturally in a naive PCFG is somewhat ironic.</S>
    <S sid="269" ssid="25">In the heart of any PCFG parser, the fundamental table entry or chart item is a label over a span, for example an NP from position 0 to position 5.</S>
    <S sid="270" ssid="26">The concrete use of a grammar rule is to take two adjacent span-marked labels and combine them (for example NP[0,5] and VP[5,12] into S[0,12]).</S>
    <S sid="271" ssid="27">Yet, only the labels are used to score the combination. capture this, DOMINATES-V marks all nodes which dominate any verbal node (V*, MD) with a -V. This brought the cumulative F1 to 86.91%.</S>
    <S sid="272" ssid="28">We also tried marking nodes which dominated prepositions and/or conjunctions, but these features did not help the cumulative hill-climb.</S>
    <S sid="273" ssid="29">The final distance/depth feature we used was an explicit attempt to model depth, rather than use distance and linear intervention as a proxy.</S>
    <S sid="274" ssid="30">With RIGHT-REC-NP, we marked all NPs which contained another NP on their right periphery (i.e., as a rightmost descendant).</S>
    <S sid="275" ssid="31">This captured some further attachment trends, and brought us to a final development F1 of 87.04%.</S>
  </SECTION>
  <SECTION title="9 Final Results" number="8">
    <S sid="276" ssid="1">We took the final model and used it to parse section 23 of the treebank.</S>
    <S sid="277" ssid="2">Figure 8 shows the results.</S>
    <S sid="278" ssid="3">The test set F1 is 86.32% for &lt; 40 words, already higher than early lexicalized models, though of course lower than the state-of-the-art parsers.</S>
  </SECTION>
  <SECTION title="10 Conclusion" number="9">
    <S sid="279" ssid="1">The advantages of unlexicalized grammars are clear enough &#8211; easy to estimate, easy to parse with, and time- and space-efficient.</S>
    <S sid="280" ssid="2">However, the dismal performance of basic unannotated unlexicalized grammars has generally rendered those advantages irrelevant.</S>
    <S sid="281" ssid="3">Here, we have shown that, surprisingly, the maximum-likelihood estimate of a compact unlexicalized PCFG can parse on par with early lexicalized parsers.</S>
    <S sid="282" ssid="4">We do not want to argue that lexical selection is not a worthwhile component of a state-ofthe-art parser &#8211; certain attachments, at least, require it &#8211; though perhaps its necessity has been overstated.</S>
    <S sid="283" ssid="5">Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well.</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="10">
    <S sid="284" ssid="1">This paper is based on work supported in part by the National Science Foundation under Grant No.</S>
    <S sid="285" ssid="2">IIS0085896, and in part by an IBM Faculty Partnership Award to the second author.</S>
  </SECTION>
</PAPER>
