[
  {
    "citance_No": 1, 
    "citing_paper_id": "W07-0718", 
    "citing_paper_authority": 55, 
    "citing_paper_authors": "Chris, Callison-Burch | Cameron, Fordyce | Philipp, Koehn | Christof, Monz | Josh, Schroeder", 
    "raw_text": "Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007)", 
    "clean_text": "Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P13-2073", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Ahmed, El Kholy | Nizar, Habash | Gregor, Leusch | Evgeny, Matusov | Hassan, Sawaf", 
    "raw_text": "We evaluate using BLEU4 (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007)", 
    "clean_text": "We evaluate using BLEU4 (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W10-0102", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Vamshi, Ambati | Stephan, Vogel | Jaime G., Carbonnell", 
    "raw_text": "The translation accuracy reported in Table 3, as measured by BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007), also shows significant improvement and approaches the quality achieved using gold standard data", 
    "clean_text": "The translation accuracy reported in Table 3, as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007), also shows significant improvement and approaches the quality achieved using gold standard data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W10-1760", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Michael, Paul | Andrew, Finch | Eiichiro, Sumita", 
    "raw_text": "For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics ,i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007)", 
    "clean_text": "For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W09-2301", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Greg, Hanneman | Alon, Lavie", 
    "raw_text": "We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006)", 
    "clean_text": "We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W08-0329", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Antti-Veikko I., Rosti | Bing, Zhang | Spyros, Matsoukas | Richard M., Schwartz", 
    "raw_text": "The translation quality is measured by three MT evaluation metrics: TER (Snover et al, 2006), BLEU (Papineni et al, 2002), and METEOR (Lavie and Agarwal, 2007)", 
    "clean_text": "The translation quality is measured by three MT evaluation metrics: TER (Snover et al, 2006), BLEU (Papineni et al, 2002), and METEOR (Lavie and Agarwal, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W08-0324", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Greg, Hanneman | Edmund, Huber | Abhaya, Agarwal | Vamshi, Ambati | Alok, Parlikar | Erik, Peterson | Alon, Lavie", 
    "raw_text": "We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006)", 
    "clean_text": "We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P11-1094", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Y. Albert, Park | Roger, Levy", 
    "raw_text": "For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007)", 
    "clean_text": "For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W08-1808", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Richard, Shaw | Ben, Solway | Robert J., Gaizauskas | Mark A., Greenwood", 
    "raw_text": "We considered a variety of tools like ROUGE (Lin, 2004) and ME TEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task", 
    "clean_text": "We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P09-1018", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Hua, Wu | Haifeng, Wang", 
    "raw_text": "In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of non consecutive bi grams with a gap size of m (1? m? 2) 13-14 longest common subsequences15-19 n-gram precision against a target corpus (1? n? 5) Table 1: Feature sets for regression learning can easily retrain the learner under different conditions, therefore enabling our method to be applied to sentence-level translation selection fro many sets of translation systems without any additional human work", 
    "clean_text": "In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of nonconsecutive bigrams with a gap size of m (1? m? 2) 13-14 longest common subsequences 15-19 n-gram precision against a target corpus (1? n? 5) Table 1: Feature sets for regression learning can easily retrain the learner under different conditions, therefore enabling our method to be applied to sentence-level translation selection fro many sets of translation systems without any additional human work.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P10-2067", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Vamshi, Ambati | Stephan, Vogel | Jaime G., Carbonnell", 
    "raw_text": "The translation accuracy as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) also shows improvement over baseline and approaches gold standard quality", 
    "clean_text": "The translation accuracy as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) also shows improvement over baseline and approaches gold standard quality.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W10-0710", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Vamshi, Ambati | Stephan, Vogel", 
    "raw_text": "This problem is similar to the task of automatic translation output evaluation and so we use METEOR (Lavie and Agarwal,2007), an automatic MT evaluation metric for com paring two sentences", 
    "clean_text": "This problem is similar to the task of automatic translation output evaluation and so we use METEOR (Lavie and Agarwal,2007), an automatic MT evaluation metric for com paring two sentences.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N10-1031", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Michael, Denkowski | Alon, Lavie", 
    "raw_text": "The final METEOR score is then calculated: Score= (1? Pen)? Fmean The free parameters?,?, and? can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007)", 
    "clean_text": "The free parameters can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N10-1031", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Michael, Denkowski | Alon, Lavie", 
    "raw_text": "Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008)", 
    "clean_text": "Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N10-1031", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Michael, Denkowski | Alon, Lavie", 
    "raw_text": "Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics: BLEU (Papineni et al, 2002) with a maximum N gram length of 4, TER (Snover et al, 2006), versions of METEOR based on release 0.7 tuned for adequacy and fluency (METEOR-0.7-af) (Lavie and Agarwal, 2007), ranking (METEOR-0.7-rank) (Agarwal and Lavie, 2008), and HTER (METEOR-0.7-hter)", 
    "clean_text": "Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics: BLEU (Papineni et al, 2002) with a maximum N gram length of 4, TER (Snover et al, 2006), versions of METEOR based on release 0.7 tuned for adequacy and fluency (METEOR-0.7-af) (Lavie and Agarwal, 2007), ranking (METEOR-0.7-rank) (Agarwal and Lavie, 2008), and HTER (METEOR-0.7-hter).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W11-2203", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Marianna, Apidianaki", 
    "raw_text": "In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric? s correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010)", 
    "clean_text": "In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D08-1065", 
    "citing_paper_authority": 38, 
    "citing_paper_authors": "Roy W., Tromble | Shankar, Kumar | Franz Josef, Och | Wolfgang, Macherey", 
    "raw_text": "In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007) .In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hyper graph that rep resents a generalized translation lattice with word sand hidden tree structures", 
    "clean_text": "In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with word sand hidden tree structures.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W10-1734", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Fabienne, Fritzinger | Alexander, Fraser", 
    "raw_text": "The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12", 
    "clean_text": "The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P08-1022", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Dominic, Espinosa | Michael, White | Dennis, Mehay", 
    "raw_text": "Moreover, the overall BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCGbank) are higher for the hyper tagger-seeded realizer than for the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in OpenCCGusing a corpus-derived grammar", 
    "clean_text": "Moreover, the overall BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCG bank) are higher for the hyper tagger-seeded realizer than for the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in Open CCG using a corpus-derived grammar.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W11-1604", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Sander, Wubben | Erwin, Marsi | Antal, van den Bosch | Emiel, Krahmer", 
    "raw_text": "In stead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available", 
    "clean_text": "Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available.", 
    "keep_for_gold": 0
  }
]
