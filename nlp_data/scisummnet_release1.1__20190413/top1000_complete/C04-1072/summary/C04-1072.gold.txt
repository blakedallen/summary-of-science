ORANGE: A Method For Evaluating Automatic Evaluation Metrics For Machine Translation
Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson’s product moment correlation coefficient or  Spearman’s rank order correlation coefficient between human scores and automatic scores.
However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency.
Unfortunately, these judgments are often inconsistent and very expensive to acquire.
In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations.
We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE.
BLEU is smoothed (Lin and Och, 2004b), and it considers only matching up to bi grams because this has higher correlations with human judgments than when higher-ordered n-grams are included.
Smoothed per-sentence BLEU was used as a similarity metric.
