Comparative Experiments On Disambiguating Word Senses: An Illustration Of The Role Of Bias In Machine Learning
This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context.
The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.
The specific problem tested involves disambiguating six senses of the word "line" using the words in the current and proceeding sentence as context.
The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this observed difference.
We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems.
We argue that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination.
Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination.
