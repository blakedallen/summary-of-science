[
  {
    "citance_No": 1, 
    "citing_paper_id": "W12-2109", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Ann, Irvine | Jonathan, Weese | Chris, Callison-Burch", 
    "raw_text": "More recently, Han and Baldwin (2011) use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms", 
    "clean_text": "We use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P13-1114", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Congle, Zhang | Tyler, Baldwin | Howard, Ho | Benny, Kimelfeld | Yunyao, Li", 
    "raw_text": "Hanand Baldwin (2011) use a classifier to detect ill formed words, and then generate correction candidates based on morphophonemic similarity", 
    "clean_text": "Hanand Baldwin (2011) use a classifier to detect ill formed words, and then generate correction candidates based on morphophonemic similarity.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P13-1114", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Congle, Zhang | Tyler, Baldwin | Howard, Ho | Benny, Kimelfeld | Yunyao, Li", 
    "raw_text": "Google: Output of the Google spell checker.w2wN: The output of the word-to-word normalization of Han and Baldwin (2011)", 
    "clean_text": "w2wN: The output of the word-to-word normalization of Han and Baldwin (2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P13-1114", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Congle, Zhang | Tyler, Baldwin | Howard, Ho | Benny, Kimelfeld | Yunyao, Li", 
    "raw_text": "Our results use the metrics of Section 5.1. 5.2.1 Twitter To evaluate the performance on Twitter data ,weuse the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011)", 
    "clean_text": "5.2.1 Twitter To evaluate the performance on Twitter data, we use the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P14-2112", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Miles, Osborne | Ashwin, Lall | Benjamin, van Durme", 
    "raw_text": "We expect that our language model could improve other Social Media tasks, for example lexical normalisation (Han and Baldwin, 2011) or even event detection (Lin et al., 2011)", 
    "clean_text": "We expect that our language model could improve other Social Media tasks, for example lexical normalisation (Han and Baldwin, 2011) or even event detection (Lin et al., 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P12-3006", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Ai Ti, Aw | Lian Hau, Lee", 
    "raw_text": "Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity", 
    "clean_text": "Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D12-1039", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Bo, Han | Paul, Cook | Timothy, Baldwin", 
    "raw_text": "Recently, Han and Baldwin (2011) and Gouwsetal2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised", 
    "clean_text": "Recently, Han and Baldwin (2011) and Gouwsetal2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D12-1039", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Bo, Han | Paul, Cook | Timothy, Baldwin", 
    "raw_text": "Theyapproach lexical variant detection by using a con text fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al2011) .However, the lexical variant detection of both methods is rather unreliable, indicating the challenge of this aspect of normalisation", 
    "clean_text": "They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "D12-1039", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Bo, Han | Paul, Cook | Timothy, Baldwin", 
    "raw_text": "In contrast to the normalisation dictionaries of Han and Baldwin (2011) and Gouws et al2011) which focus on very frequent lexical variants, we focus on moderate frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalisation lexicons that are complementary to those currently available", 
    "clean_text": "In contrast to the normalisation dictionaries of Han and Baldwin (2011) and Gouws et al 2011) which focus on very frequent lexical variants, we focus on moderate frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalisation lexicons that are complementary to those currently available.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D12-1039", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Bo, Han | Paul, Cook | Timothy, Baldwin", 
    "raw_text": "To further narrow the search space, we only consider IV words which are morphophonemic ally similar to the OOV type, following settings in Han and Baldwin (2011) .3 1https: //dev.twitter.com/docs/streaming-api/methods 2http: //aspell.net/ 3We only consider IV words within an edit distance of 2 or a phonemic edit distance of 1 from the OOV type, and we further 424In order to evaluate the generated pairs, we randomly selected 1000 OOV words from the 10 mil lion tweet corpus", 
    "clean_text": "To further narrow the search space, we only consider IV words which are morphophonemic ally similar to the OOV type, following settings in Han and Baldwin (2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D12-1039", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Bo, Han | Paul, Cook | Timothy, Baldwin", 
    "raw_text": "Given the re-ranked pairs from Section 5, here weapply them to a token-level normalisation task using the normalisation dataset of Han and Baldwin (2011)", 
    "clean_text": "Given the re-ranked pairs from Section 5, here we apply them to a token-level normalisation task using the normalisation dataset of Han and Baldwin (2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D12-1039", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Bo, Han | Paul, Cook | Timothy, Baldwin", 
    "raw_text": "In addition, the contribution of these dictionaries in hybrid nor malisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs", 
    "clean_text": "In addition, the contribution of these dictionaries in hybrid normalisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "D12-1039", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Bo, Han | Paul, Cook | Timothy, Baldwin", 
    "raw_text": "HB-dict+S-dictandGHM-dict+S-dict, on the other hand, improve sub 428 Method Precision Recall F-Score False Alarm Word Error Rate C-dict 0.474 0.218 0.299 0.298 0.103 DM-dict 0.727 0.106 0.185 0.145 0.102 S-dict 0.700 0.179 0.285 0.162 0.097 HB-dict 0.915 0.435 0.590 0.048 0.066 GHM-dict 0.982 0.319 0.482 0.000 0.076 HB-dict+S-dict 0.840 0.601 0.701 0.090 0.052 GHM-dict+S-dict 0.863 0.498 0.632 0.072 0.061 HB-dict+GHM-dict 0.920 0.465 0.618 0.045 0.063 HB-dict+GHM-dict+S-dict 0.847 0.630 0.723 0.086 0.049 GHM-dict+GHM-norm 0.338 0.578 0.427 0.458 0.135 HB-dict+GHM-dict+S-dict+GHM-norm 0.406 0.715 0.518 0.468 0.124 HB-dict+HB-norm 0.515 0.771 0.618 0.332 0.081 HB-dict+GHM-dict+S-dict+HB-norm 0.527 0.789 0.632 0.332 0.079Table 3: Normalisation results using our derived dictionaries (contextual similarity (C-dict); double metaphone rendering (DM-dict); string subsequence kernel scores (S-dict)), the dictionary of Gouws et al2011) (GHM-dict), the Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries", 
    "clean_text": "the Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D12-1039", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Bo, Han | Paul, Cook | Timothy, Baldwin", 
    "raw_text": "In addition, we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm)", 
    "clean_text": "In addition, we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "D12-1039", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Bo, Han | Paul, Cook | Timothy, Baldwin", 
    "raw_text": "6.2.3 Hybrid Approaches The methods of Gouws et al2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionary based approaches; this is largely caused by lexical variant detection errors.8 Using all dictionaries in combination with these methods? HB-dict+GHM-dict+S-dict+GHM-norm and HBdict+GHM-dict+S-dict+HB-norm? gives some improvements, but the false alarm rates remain high", 
    "clean_text": "6.2.3 Hybrid Approaches The methods of Gouws et al2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionary based approaches; this is largely caused by lexical variant detection errors.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "E12-2014", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Masud, Moshtaghi | Shanika, Karunasekera | Aaron, Harwood | Timothy, Baldwin | Paul, Cook | Bo, Han", 
    "raw_text": "The present lexical normalisation used by our system is the dictionary lookup method of Hanand Baldwin (2011) which normalises noisy to kens only when the normalised form is known with high confidence (e.g. you for u)", 
    "clean_text": "The present lexical normalisation used by our system is the dictionary lookup method of Hanand Baldwin (2011) which normalises noisy tokens only when the normalised form is known with high confidence (e.g. you for u).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "E12-2014", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Masud, Moshtaghi | Shanika, Karunasekera | Aaron, Harwood | Timothy, Baldwin | Paul, Cook | Bo, Han", 
    "raw_text": "Ultimately, however, we are interested in performing context sensitive lexical normalisation, based on a reimplementation of the method of Han and Baldwin (2011)", 
    "clean_text": "Ultimately, however, we are interested in performing context sensitive lexical normalisation, based on a reimplementation of the method of Han and Baldwin (2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P12-1109", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Xiao, Jiang | Fei, Liu | Fuliang, Weng", 
    "raw_text": "(Hanand Baldwin, 2011) reported an average of 127 candidates per nonstandard token with the correct-word coverage of 84%", 
    "clean_text": "(Hanand Baldwin, 2011) reported an average of 127 candidates per nonstandard token with the correct-word coverage of 84%.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P12-1109", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Xiao, Jiang | Fei, Liu | Fuliang, Weng", 
    "raw_text": "(Han and Baldwin, 2011 )developed classifiers for detecting the ill-formed word sand generated corrections based on the morphophonemic similarity", 
    "clean_text": "(Han and Baldwin, 2011) developed classifiers for detecting the ill-formed word sand generated corrections based on the morphophonemic similarity.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P12-1109", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Xiao, Jiang | Fei, Liu | Fuliang, Weng", 
    "raw_text": "Reference Tokensw/ Multi-cands of cands (1) SMS Around 2007n/a 303 1.32% 100% (Choudhury et al, 2007) (2) Twitter Nov 2009? Feb 2010 6150 3802 3.87% 99.34% (Liu et al, 2011) (3) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011) (4) Twitter Aug 2010? Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011) Table 3: Statistics of different SMS and Twitter data sets", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]
