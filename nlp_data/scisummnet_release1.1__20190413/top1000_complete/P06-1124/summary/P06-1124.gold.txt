A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes
We propose a new hierarchical Bayesian n-gram model of natural languages.
Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages.
We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models.
Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney.
We priovide a Bayesian interpretation to smoothing techniques, such as Kneser-Ney and Witten-Bell back-off schemes.
Nonparametric Bayesian modeling is able to provide priors that are especially suitable for tasks in NLP.
While the Dirichlet process is simply the Pitman Yor process with d= 0, we find that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language.
