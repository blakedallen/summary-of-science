[
  {
    "citance_No": 1, 
    "citing_paper_id": "N12-1005", 
    "citing_paper_authority": 27, 
    "citing_paper_authors": "Fran&ccedil;ois, Yvon | Hai-Son, Le | Alexandre, Allauzen", 
    "raw_text": "Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesianinterpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events", 
    "clean_text": "Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "N12-1043", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Thomas, Mueller | Hinrich, Schuetze | Helmut, Schmid", 
    "raw_text": "Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), re current neural networks (Mikolov et al, 2010 )andFLMs in their general, more powerful form", 
    "clean_text": "Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al, 2010) and FLMs in their general, more powerful form.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P11-2036", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Hiroyuki, Shindo | Akinori, Fujino | Masaaki, Nagata", 
    "raw_text": "statistics: a few initial trees are often used for derivation while many are rarely used, and this is shown empirically to be well-suited for natural language (Teh, 2006b; Johnson and Goldwater, 2009)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P11-2036", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Hiroyuki, Shindo | Akinori, Fujino | Masaaki, Nagata", 
    "raw_text": "The hyper parameters of our model are updated with the auxiliary variable technique (Teh, 2006a)", 
    "clean_text": "The hyper parameters of our model are updated with the auxiliary variable technique (Teh, 2006a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P11-2005", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Greg, Durrett | Dan, Klein", 
    "raw_text": "The results of section 2.1 point to a remarkably pervasive phenomenon of growing empirical discounts, except in the case of extremely similar corpora. Growing discounts of this sort were previously suggested by the model of Teh (2006)", 
    "clean_text": "Growing discounts of this sort were previously suggested by the model of Teh (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D12-1021", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Abby, Levenberg | Philip, Blunsom | Chris, Dyer", 
    "raw_text": "individual rules directly using the process described by Teh (2006)", 
    "clean_text": "Although the PYP has no known analytical form, we can marginalise out the GX's and reason about individual rules directly using the process described by Teh (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P11-1087", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P11-1087", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "Research in language modelling (Teh, 2006b; Goldwater et al, 2006a) and parsing (Cohn et al., 2010) has shown that models employing Pitman-Yor priors can significantly outperform the more frequently used Dirichlet priors, especially where complex hierarchical relationships exist between latent variables", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P11-1087", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006a; Goldwater et al, 2006b), and has been shown to be a generalisation of Kneser-Ney smoothing, widely recognised as the best smoothing method for language modelling (Chen and Good man, 1996) .The model is depicted in the plate diagram in Figure 1", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D10-1076", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Hai-Son, Le | Alexandre, Allauzen | Guillaume, Wisniewski | Fran&ccedil;ois, Yvon", 
    "raw_text": "Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events", 
    "clean_text": "Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P13-1077", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Trevor, Cohn | Gholamreza, Haffari", 
    "raw_text": "Themodel is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrase pairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a Pitman Yor process prior, which is capable of modellinga diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006)", 
    "clean_text": "We develop a Bayesian approach using a Pitman Yor process prior, which is capable of modellinga diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W09-0210", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Andreas, Vlachos | Anna, Korhonen | Zoubin, Ghahramani", 
    "raw_text": "Recentwork has applied Bayesian non-parametric mod els to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. Recently, Vlachos et al (2008) applied the basic models of this class, Dirichlet Process Mixture Models (DPMMs) (Neal, 2000), to a typical learning task in NLP: lexical-semantic verb clustering", 
    "clean_text": "Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N12-1045", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Kairit, Sirts | Tanel, Alumae", 
    "raw_text": "Nonparametric Bayesian modeling has recently be come very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006)", 
    "clean_text": "Nonparametric Bayesian modeling has recently become very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P11-1064", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Graham, Neubig | Taro, Watanabe | Eiichiro, Sumita | Shinsuke, Mori | Tatsuya, Kawahara", 
    "raw_text": "In FLAT, only minimal phrases generated after Px outputs the terminal symbol TERM are generated from Pt, and thus only minimal phrases are memorized by the model. While the Dirichlet process is simply the Pitman Yor process with d= 0, it has been shown that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language (Teh, 2006)", 
    "clean_text": "While the Dirichlet process is simply the Pitman Yor process with d= 0, it has been shown that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language (Teh, 2006).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P11-1064", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Graham, Neubig | Taro, Watanabe | Eiichiro, Sumita | Shinsuke, Mori | Tatsuya, Kawahara", 
    "raw_text": "The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solution to this problem", 
    "clean_text": "The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solution to this problem.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "E12-3008", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jan A., Botha", 
    "raw_text": "Similarly, if the seating dynamics are constrained such that each dish is only served once (tw= 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006)", 
    "clean_text": "Similarly, if the seating dynamics are constrained such that each dish is only served once (tw= 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P13-1033", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yang, Feng | Trevor, Cohn", 
    "raw_text": "served with the dish h in o? n and K? h is the number of tables served with the dish h in t? n. The hierarchical PYP (hPYP; Teh (2006)) is an extension of the PYP in which the base distribution G0 is itself a PYP distribution", 
    "clean_text": "The hierarchical PYP (hPYP; Teh (2006)) is an extension of the PYP in which the base distribution G0 is itself a PYP distribution.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P13-1033", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yang, Feng | Trevor, Cohn", 
    "raw_text": "There is not space for a complete treatment of the hPYP and the particulars of inference; we refer the interested reader to Teh (2006)", 
    "clean_text": "There is not space for a complete treatment of the hPYP and the particulars of inference; we refer the interested reader to Teh (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W10-2809", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Andreas, Vlachos | Zoubin, Ghahramani | Ted, Briscoe", 
    "raw_text": "Recent work has applied such mod els to various tasks with promising results ,e.g. Teh (2006) and Cohn et al (2009)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W12-2701", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Fran&ccedil;ois, Yvon | Hai-Son, Le | Alexandre, Allauzen", 
    "raw_text": "Among these, smoothing techniques, such as Good-Turing, Witten-Bell and Kneser-Ney smoothing schemes (see (Chen and Goodman, 1996) for an empirical overview and (Teh, 2006) for a Bayesian interpretation) are used to compute estimates for the probability of unseen events, which are needed to achieve state-of-the-art performance in large-scale settings", 
    "clean_text": "Among these, smoothing techniques, such as Good-Turing, Witten-Bell and Kneser-Ney smoothing schemes (see (Chen and Goodman, 1996) for an empirical overview and (Teh, 2006) for a Bayesian interpretation) are used to compute estimates for the probability of unseen events, which are needed to achieve state-of-the-art performance in large-scale settings.", 
    "keep_for_gold": 0
  }
]
