<PAPER>
  <S sid="0">Introduction To The CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition</S>
  <ABSTRACT/>
  <SECTION title="1 Introduction" number="1">
    <S sid="1" ssid="1">Named entities are phrases that contain the names of persons, organizations, locations, times and quantities.</S>
    <S sid="2" ssid="2">Example: [PER Wolff ] , currently a journalist in [LOC Argentina ] , played with [PER Del Bosque ] in the final years of the seventies in [ORG Real Madrid ] .</S>
    <S sid="3" ssid="3">This sentence contains four named entities: Wol&#65533; and Del Bosque are persons, Argentina is a location and Real Madrid is a organization.</S>
    <S sid="4" ssid="4">The shared task of CoNLL-2002 concerns language-independent named entity recognition.</S>
    <S sid="5" ssid="5">We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.</S>
    <S sid="6" ssid="6">The participants of the shared task have been offered training and test data for two European languages: Spanish and Dutch.</S>
    <S sid="7" ssid="7">They have used the data for developing a named-entity recognition system that includes a machine learning component.</S>
    <S sid="8" ssid="8">The organizers of the shared task were especially interested in approaches that make use of additional nonannotated data for improving their performance.</S>
  </SECTION>
  <SECTION title="2 Data and Evaluation" number="2">
    <S sid="9" ssid="1">The CoNLL-2002 named entity data consists of six files covering two languages: Spanish and Dutch'.</S>
    <S sid="10" ssid="2">Each of the languages has a training file, a development file and a test file.</S>
    <S sid="11" ssid="3">The learning methods will be trained with the training data.</S>
    <S sid="12" ssid="4">The development data can be used for tuning the parameters of the learning methods.</S>
    <S sid="13" ssid="5">When the best parameters are found, the method can be trained on the training data and tested on the test data.</S>
    <S sid="14" ssid="6">The results of the different learning methods on the test sets will be compared in the evaluation of the shared task.</S>
    <S sid="15" ssid="7">The split between development data and test data has been chosen to avoid that systems are being tuned to the test data.</S>
    <S sid="16" ssid="8">All data files contain one word per line with empty lines representing sentence boundaries.</S>
    <S sid="17" ssid="9">Additionally each line contains a tag which states whether the word is inside a named entity or not.</S>
    <S sid="18" ssid="10">The tag also encodes the type of named entity.</S>
    <S sid="19" ssid="11">Here is a part of the example sentence: Words tagged with O are outside of named entities.</S>
    <S sid="20" ssid="12">The B-XXX tag is used for the first word in a named entity of type XXX and IXXX is used for all other words in named entities of type XXX.</S>
    <S sid="21" ssid="13">The data contains entities of four types: persons (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC).</S>
    <S sid="22" ssid="14">The tagging scheme is a variant of the IOB scheme originally put forward by Ramshaw and Marcus (1995).</S>
    <S sid="23" ssid="15">We assume that named entities are non-recursive and non-overlapping.</S>
    <S sid="24" ssid="16">In case a named entity is embedded in another named entity usually only the top level entity will be marked.</S>
    <S sid="25" ssid="17">The Spanish data is a collection of news wire articles made available by the Spanish EFE News Agency.</S>
    <S sid="26" ssid="18">The articles are from May 2000.</S>
    <S sid="27" ssid="19">The annotation was carried out by the TALP Research Center2 of the Technical University of Catalonia (UPC) and the Center of Language and Computation (CLiC3) of the University of Barcelona (UB), and funded by the European Commission through the NAMIC project (IST1999-12392).</S>
    <S sid="28" ssid="20">The data contains words and entity tags only.</S>
    <S sid="29" ssid="21">The training, development and test data files contain 273037, 54837 and 53049 lines respectively.</S>
    <S sid="30" ssid="22">The Dutch data consist of four editions of the Belgian newspaper &amp;quot;De Morgen&amp;quot; of 2000 (June 2, July 1, August 1 and September 1).</S>
    <S sid="31" ssid="23">The data was annotated as a part of the Atranos project4 at the University of Antwerp in Belgium, Europe.</S>
    <S sid="32" ssid="24">The annotator has followed the MITRE and SAIC guidelines for named entity recognition (Chinchor et al., 1999) as well as possible.</S>
    <S sid="33" ssid="25">The data consists of words, entity tags and partof-speech tags which have been derived by a Dutch part-of-speech tagger (Daelemans et al., 1996).</S>
    <S sid="34" ssid="26">Additionally the article boundaries in the text have been marked explicitly with lines containing the tag -DOCSTART-.</S>
    <S sid="35" ssid="27">The training, development and test data files contain 218737, 40656 and 74189 lines respectively.</S>
    <S sid="36" ssid="28">The performance in this task is measured with F,3=1 rate which is equal to (32+1)*precision*recall / ('32*precision+recall) with 3=1 (van Rijsbergen, 1975).</S>
    <S sid="37" ssid="29">Precision is the percentage of named entities found by the learning system that are correct.</S>
    <S sid="38" ssid="30">Recall is the percentage of named entities present in the corpus that are found by the system.</S>
    <S sid="39" ssid="31">A named entity is correct only if it is an exact match of the corresponding entity in the data file.</S>
  </SECTION>
  <SECTION title="3 Results" number="3">
    <S sid="40" ssid="1">Twelve systems have participated in this shared task.</S>
    <S sid="41" ssid="2">The results for the test sets for Spanish and Dutch can be found in Table 1.</S>
    <S sid="42" ssid="3">A baseline rate was computed for both sets.</S>
    <S sid="43" ssid="4">It was produced by a system which only identified entities which had a unique class in the training data.</S>
    <S sid="44" ssid="5">If a phrase was part of more than one entity, the system would select the longest one.</S>
    <S sid="45" ssid="6">All systems that participated in the shared task have outperformed the baseline system.</S>
    <S sid="46" ssid="7">McNamee and Mayfield (2002) have applied support vector machines to the data of the shared task.</S>
    <S sid="47" ssid="8">Their system used many binary features for representing words (almost 9000).</S>
    <S sid="48" ssid="9">They have evaluated different parameter settings of the system and have selected a cascaded approach in which first entity boundaries were predicted and then entity classes (Spanish test set: F,3=1=60.97; Dutch test set: F,3=1=59.52).</S>
    <S sid="49" ssid="10">Black and Vasilakopoulos (2002) have evaluated two approaches to the shared task.</S>
    <S sid="50" ssid="11">The first was a transformation-based method which generated in rules in a single pass rather than in many passes.</S>
    <S sid="51" ssid="12">The second method was a decision tree method.</S>
    <S sid="52" ssid="13">They found that the transformation-based method consistently outperformed the decision trees (Spanish test set: F,3=1=67.49; Dutch test set: F,3=1=56.43) Tsukamoto, Mitsuishi and Sassano (2002) used a stacked AdaBoost classifier for finding named entities.</S>
    <S sid="53" ssid="14">They found that cascading classifiers helped improved performance.</S>
    <S sid="54" ssid="15">Their final system consisted of a cascade of five learners each of which performed 10,000 boosting rounds (Spanish test set: F,3=1=71.49; Dutch test set: F,3=1=60.93) Malouf (2002) tested different models with the shared task data: a statistical baseline model, a Hidden Markov Model and maximum entropy models with different features.</S>
    <S sid="55" ssid="16">The latter proved to perform best.</S>
    <S sid="56" ssid="17">The maximum entropy models benefited from extra feature which encoded capitalization information, positional information and information about the current word being part of a person name earlier in the text.</S>
    <S sid="57" ssid="18">However, incorporating a list of person names in the training process did not help (Spanish test set: F,3=1=73.66; Dutch test set: F,3=1=68.08) Jansche (2002) employed a first-order Markov model as a named entity recognizer.</S>
    <S sid="58" ssid="19">His system used two separate passes, one for extracting entity boundaries and one for classifying entities.</S>
    <S sid="59" ssid="20">He evaluated different features in both subprocesses.</S>
    <S sid="60" ssid="21">The categorization process was trained separately from the extraction process but that did not seem to have harmed overall performance (Spanish test set: F,3=1=73.89; Dutch test set: F,3=1=69.68) Patrick, Whitelaw and Munro (2002) present SLINERC, a language-independent named entity recognizer.</S>
    <S sid="61" ssid="22">The system uses tries as well as character n-grams for encoding word-internal and contextual information.</S>
    <S sid="62" ssid="23">Additionally, it relies on lists of entities which have been compiled from the training data.</S>
    <S sid="63" ssid="24">The overall system consists of six stages, three regarding entity recognition and three for entity categorization.</S>
    <S sid="64" ssid="25">Stages use the output of previous stages for obtaining an improved performance (Spanish test set: F,3=1=73.92; Dutch test set: F,3=1=71.36) Tjong Kim Sang (2002) has applied a memory-based learner to the data of the shared task.</S>
    <S sid="65" ssid="26">He used a two-stage processing strategy as well: first identifying entities and then classifying them.</S>
    <S sid="66" ssid="27">Apart from the base classifier, his system made use of three extra techniques for boosting performance: cascading classifiers (stacking), feature selection and system combination.</S>
    <S sid="67" ssid="28">Each of these techniques were shown to be useful (Spanish test set: F,3=1=75.78; Dutch test set: F,3=1=70.67).</S>
    <S sid="68" ssid="29">Burger, Henderson and Morgan (2002) have evaluated three approaches to finding named entities.</S>
    <S sid="69" ssid="30">They started with a baseline system which consisted of an HMM-based phrase tagger.</S>
    <S sid="70" ssid="31">They gave the tagger access to a list of approximately 250,000 named entities and the performance improved.</S>
    <S sid="71" ssid="32">After this several smoothed word classes derived from the available data were incorporated into the training process.</S>
    <S sid="72" ssid="33">The system performed better with the derived word lists than with the external named entity lists (Spanish test set: F,3=1=75.78; Dutch test set: F,3=1=72.57).</S>
    <S sid="73" ssid="34">Cucerzan and Yarowsky (2002) approached the shared task by using word-internal and contextual information stored in character-based tries.</S>
    <S sid="74" ssid="35">Their system obtained good results by using part-of-speech tag information and employing the one sense per discourse principle.</S>
    <S sid="75" ssid="36">The authors expect a performance increase when the system has access to external entity lists but have not presented the results of this in detail (Spanish test set: F,3=1=77.15; Dutch test set: F/3=1=72.31).</S>
    <S sid="76" ssid="37">Wu, Ngai, Carpuat, Larsen and Yang (2002) have applied AdaBoost.MH to the shared task data and compared the performance with that of a maximum entropy-based named entity tagger.</S>
    <S sid="77" ssid="38">Their system used lexical and part-of-speech information, contextual and word-internal clues, capitalization information, knowledge about entity classes of previous occurrences of words and a small external list of named entity words.</S>
    <S sid="78" ssid="39">The boosting techniques operated on decision stumps, decision trees of depth one.</S>
    <S sid="79" ssid="40">They outperformed the maximum entropy-based named entity tagger (Spanish test set: F,3=1=76.61; Dutch test set: F,3=1=75.36).</S>
    <S sid="80" ssid="41">Florian (2002) employed three stacked learners for named entity recognition: transformation-based learning for obtaining base-level non-typed named entities, Snow for improving the quality of these entities and the forward-backward algorithm for finding categories for the named entities.</S>
    <S sid="81" ssid="42">The combination of the three algorithms showed a substantially improved performance when compared with a single algorithm and an algorithm pair (Spanish test set: F,3=1=79.05; Dutch test set: F,3=1=74.99).</S>
    <S sid="82" ssid="43">Carreras, Marquez and Padro (2002) have approached the shared task by using AdaBoost applied to fixed-depth decision trees.</S>
    <S sid="83" ssid="44">Their system used many different input features contextual information, word-internal clues, previous entity classes, part-of-speech tags (Dutch only) and external word lists (Spanish only).</S>
    <S sid="84" ssid="45">It processed the data in two stages: first entity recognition and then classification.</S>
    <S sid="85" ssid="46">Their system obtained the best results in this shared task for both the Spanish and Dutch test data sets (Spanish test set: F,3=1=81.39; Dutch test set: F,3=1=77.05).</S>
  </SECTION>
  <SECTION title="4 Concluding Remarks" number="4">
    <S sid="86" ssid="1">We have described the CoNLL-2002 shared task: language-independent named entity recognition.</S>
    <S sid="87" ssid="2">Twelve different systems have been applied to data covering two Western European languages: Spanish and Dutch.</S>
    <S sid="88" ssid="3">A boosted decision tree method obtained the best performance on both data sets (Carreras et al., 2002).</S>
  </SECTION>
  <SECTION title="Acknowledgements" number="5">
    <S sid="89" ssid="1">Tjong Kim Sang is supported by IWT STWW as a researcher in the ATRANOS project.</S>
  </SECTION>
</PAPER>
