Paraphrasing For Automatic Evaluation
This paper studies the impact of paraphrases on the accuracy of automatic evaluation.
Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.
We apply our paraphrasing method in the context of machine translation evaluation.
Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation.
We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.
We show that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy.
