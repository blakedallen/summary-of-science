<PAPER>
  <S sid="0" ssid="0">Evaluating WordNet-based Measures of Lexical Semantic Relatedness Alexander Budanitsky?</S>
  <S sid="1" ssid="1">University of Toronto Graeme Hirst?</S>
  <S sid="2" ssid="2">University of Toronto The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed.</S>
  <S sid="3" ssid="3">We evaluate five of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors.</S>
  <S sid="4" ssid="4">An information-content?based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik.</S>
  <S sid="5" ssid="5">In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness.</S>
  <S sid="6" ssid="6">Introduction The need to determine semantic relatedness or its inverse, semantic distance, be- tween two lexically expressed concepts is a problem that pervades much of natural language processing.</S>
  <S sid="7" ssid="7">Measures of relatedness or distance are used in such applica- tions as word sense disambiguation, determining the structure of texts, text sum- marization and annotation, information extraction and retrieval, automatic indexing, lexical selection, and the automatic correction of word errors in text.</S>
  <S sid="8" ssid="8">It?s important to note that semantic relatedness is a more general concept than similarity; similar entities are semantically related by virtue of their similarity (bank?trust company), but dissimilar entities may also be semantically related by lexical relationships such as meronymy (car?wheel) and antonymy (hot?cold), or just by any kind of functional rela- tionship or frequent association (pencil?paper, penguin?Antarctica, rain?flood).</S>
  <S sid="9" ssid="9">Computa- tional applications typically require relatedness rather than just similarity; for example, money and river are cues to the in-context meaning of bank that are just as good as trust company.</S>
  <S sid="10" ssid="10">However, it is frequently unclear how to assess the relative merits of the many competing approaches that have been proposed for determining lexical semantic re- latedness.</S>
  <S sid="11" ssid="11">Given a measure of relatedness, how can we tell whether it is a good one or a poor one?</S>
  <S sid="12" ssid="12">Given two measures, how can we tell whether one is bet- ter than the other, and under what conditions it is better?</S>
  <S sid="13" ssid="13">And what is it that makes some measures better than others?</S>
  <S sid="14" ssid="14">Our purpose in this paper is to compare the performance of a number of measures of semantic relatedness that have been proposed for use in applications in natural language processing and information retrieval.</S>
  <S sid="15" ssid="15">Department of Computer Science, Toronto, Ontario, Canada M5S 3G4; {abm, gh}@cs.toronto.edu.</S>
  <S sid="16" ssid="16">Submission received: 8 July 2004; revised submission received: 30 July 2005; accepted for publication: 20 August 2005. ?</S>
  <S sid="17" ssid="17">2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 1 1.1 Terminology and Notation In the literature related to this topic, at least three different terms are used by differ- ent authors or sometimes interchangeably by the same authors: semantic relatedness, similarity, and semantic distance.</S>
  <S sid="18" ssid="18">Resnik (1995) attempts to demonstrate the distinction between the first two by way of example.</S>
  <S sid="19" ssid="19">?Cars and gasoline?, he writes, ?would seem to be more closely related than, say, cars and bicycles, but the latter pair are certainly more similar.?</S>
  <S sid="20" ssid="20">Similarity is thus a special case of semantic relatedness, and we adopt this perspective in this paper.</S>
  <S sid="21" ssid="21">Among other relationships that the notion of relatedness encompasses are the various kinds of meronymy, antonymy, functional association, and other ?non-classical relations?</S>
  <S sid="22" ssid="22">(Morris and Hirst 2004).</S>
  <S sid="23" ssid="23">The term semantic distance may cause even more confusion, as it can be used when talking about either just similarity or relatedness in general.</S>
  <S sid="24" ssid="24">Two concepts are ?close?</S>
  <S sid="25" ssid="25">to one another if their similarity or their relatedness is high, and otherwise they are ?distant?.</S>
  <S sid="26" ssid="26">Most of the time, these two uses are consistent with one another, but not always; antonymous concepts are dissimilar and hence distant in one sense, and yet are strongly related semantically and hence close in the other sense.</S>
  <S sid="27" ssid="27">We would thus have very much preferred to be able to adhere to the view of semantic distance as the inverse of semantic relatedness, not merely of similarity, in the present paper.</S>
  <S sid="28" ssid="28">Unfortunately, because of the sheer number of methods measuring similarity, as well as those measuring distance as the ?opposite?</S>
  <S sid="29" ssid="29">of similarity, this would have made for an awkward presentation.</S>
  <S sid="30" ssid="30">Therefore, we have to ask the reader to rely on context when interpreting what exactly the expressions semantic distance, semantically distant, and semantically close mean in each particular case.</S>
  <S sid="31" ssid="31">Various approaches presented below speak of concepts and words.</S>
  <S sid="32" ssid="32">As a means of acknowledging the polysemy of language, in this paper the term concept will refer to a particular sense of a given word.</S>
  <S sid="33" ssid="33">We want to be very clear that, throughout this paper, when we say that two words are ?similar?, this is a short way of saying that they denote similar concepts; we are not talking about similarity of distributional or co-occurrence behavior of the words, for which the term word similarity has also been used (Dagan 2000; Dagan, Lee, and Pereira 1999).</S>
  <S sid="34" ssid="34">While similarity of denotation might be inferred from similarity of distributional or co-occurrence behavior (Dagan 2000; Weeds 2003), the two are distinct ideas.</S>
  <S sid="35" ssid="35">We return to the relationship between them in Section 6.2.</S>
  <S sid="36" ssid="36">When we refer to hierarchies and networks of concepts, we will use both the terms link and edge to refer to the relationships between nodes; we prefer the former term when our view emphasizes the taxonomic aspect or the meaning of the network, and the latter when our view emphasizes algorithmic or graph-theoretic aspects.</S>
  <S sid="37" ssid="37">In running text, examples of concepts are typeset in sans-serif font, whereas examples of words are given in italics; in formulas, concepts and words will usually be denoted by c and w, with various subscripts.</S>
  <S sid="38" ssid="38">For the sake of uniformity of presentation, we have taken the liberty of altering the original notation accordingly in some other authors?</S>
  <S sid="39" ssid="39">Lexical Resource?based Approaches to Measuring Semantic Relatedness All approaches to measuring semantic relatedness that use a lexical resource construe the resource, in one way or another, as a network or directed graph, and then base the measure of relatedness on properties of paths in this graph.</S>
  <S sid="40" ssid="40">14 Budanitsky and Hirst Lexical Semantic Relatedness 2.1 Dictionary-based Approaches Kozima and Furugori (1993) turned the Longman Dictionary of Contemporary English (LDOCE) (Procter 1978) into a network by creating a node for every headword and linking each node to the nodes for all the words used in its definition.</S>
  <S sid="41" ssid="41">The 2851- word controlled defining vocabulary of LDOCE thus becomes the densest part of the network: the remaining nodes, which represent the headwords outside of the defining vocabulary, can be pictured as being situated at the fringe of the network, as they are linked only to defining-vocabulary nodes and not to each other.</S>
  <S sid="42" ssid="42">In this network, the similarity function simKF between words of the defining vocabulary is computed by means of spreading activation on this network.</S>
  <S sid="43" ssid="43">The function is extended to the rest of LDOCE by representing each word as a list W = {w1, .</S>
  <S sid="44" ssid="44">, wr} of the words in its definition; thus, for instance, simKF(linguistics, stylistics) = simKF({the, study, of, language, in, general, and, of, particular, languages, and, their, structure, and, grammar, and, history}, {the, study, of, style, in, written, or, spoken, language}) Kozima and Ito (1997) built on this work to derive a context-sensitive, or dynamic, measure that takes into account the ?associative direction?</S>
  <S sid="45" ssid="45">of a given word pair.</S>
  <S sid="46" ssid="46">For example, the context {car, bus} imposes the associative direction of vehicle (close words are then likely to include taxi, railway, airplane, etc.</S>
  <S sid="47" ssid="47">), whereas the context {car, engine} imposes the direction of components of car (tire, seat, headlight, etc.).</S>
  <S sid="48" ssid="48">2.2 Approaches Based on Roget-structured Thesauri Roget-structured thesauri, such as Roget?s Thesaurus itself, the Macquarie Thesaurus (Bernard 1986), and others, group words in a structure based on categories within which there are several levels of finer clustering.</S>
  <S sid="49" ssid="49">The categories themselves are grouped into a number of broad, loosely defined classes.</S>
  <S sid="50" ssid="50">However, while the classes and categories are named, the finer divisions are not; the words are clustered without attempting to explicitly indicate how and why they are related.</S>
  <S sid="51" ssid="51">The user?s main access is through the index, which contains category numbers along with labels representative of those categories for each word.</S>
  <S sid="52" ssid="52">Polysemes are implicitly disambiguated, to a certain extent, by the other words in their cluster and in their index entry.</S>
  <S sid="53" ssid="53">Closely related concepts might or might not be physically close in the thesaurus: ?Physical closeness has some importance .</S>
  <S sid="54" ssid="54">but words in the index of the thesaurus often have widely scattered categories, and each category often points to a widely scattered selection of cate- gories?</S>
  <S sid="55" ssid="55">(Morris and Hirst 1991).</S>
  <S sid="56" ssid="56">Methods of semantic distance that are based on Roget- structured thesauri therefore rely not only on the category structure but also on the index and on the pointers within categories that cross-reference other categories.</S>
  <S sid="57" ssid="57">In part as a consequence of this, typically no numerical value for semantic distance can be obtained: rather, algorithms using the thesaurus compute a distance implicitly and return a boolean value of ?close?</S>
  <S sid="58" ssid="58">or ?not close?.</S>
  <S sid="59" ssid="59">Working with an abridged version of Roget?s Thesaurus, Morris and Hirst (1991) identified five types of semantic relations between words.</S>
  <S sid="60" ssid="60">In their approach, two words 15 Computational Linguistics Volume 32, Number 1 were deemed to be related to one another, or semantically close, if their base forms satisfy any one of the following conditions: 1.</S>
  <S sid="61" ssid="61">They have a category in common in their index entries.</S>
  <S sid="62" ssid="62">One has a category in its index entry that contains a pointer to a category of the other.</S>
  <S sid="63" ssid="63">One is either a label in the other?s index entry or is in a category of the other.</S>
  <S sid="64" ssid="64">They are both contained in the same subcategory.</S>
  <S sid="65" ssid="65">They both have categories in their index entries that point to a common category.</S>
  <S sid="66" ssid="66">These relations account for such pairings as wife and married, car and driving, blind and see, reality and theoretically, brutal and terrified.</S>
  <S sid="67" ssid="67">(However, different editions of Roget?s Thesaurus yield somewhat different sets of relations.)</S>
  <S sid="68" ssid="68">Of the five types of relations, perhaps the most intuitively plausible ones ?</S>
  <S sid="69" ssid="69">the first two in the list above ?</S>
  <S sid="70" ssid="70">were found to validate over 90% of the intuitive lexical relationships that the authors used as a benchmark in their experiments.</S>
  <S sid="71" ssid="71">Jarmasz and Szpakowicz (2003) also implemented a similarity measure with Roget?s Thesaurus; but because this measure is based strictly on hierachy rather than the index structure, we discuss it in Section 2.4 below.</S>
  <S sid="72" ssid="72">2.3 Approaches Using WordNet and Other Semantic Networks Most of the methods discussed in the remainder of Section 2 use WordNet (Fellbaum 1998), a broad coverage lexical network of English words.</S>
  <S sid="73" ssid="73">Nouns, verbs, adjectives, and adverbs are each organized into networks of synonym sets (synsets) that each represent one underlying lexical concept and are interlinked with a variety of relations.</S>
  <S sid="74" ssid="74">(A polysemous word will appear in one synset for each of its senses.)</S>
  <S sid="75" ssid="75">In the first versions of WordNet (those numbered 1.x), the networks for the four different parts of speech were not linked to one another.1 The noun network of WordNet was the first to be richly developed, and most of the researchers whose work we will discuss below therefore limited themselves to this network.2 The backbone of the noun network is the subsumption hierarchy (hyponymy/ hypernymy), which accounts for close to 80% of the relations.</S>
  <S sid="76" ssid="76">At the top of the hier- archy are 11 abstract concepts, termed unique beginners, such as entity (?something having concrete existence; living or nonliving?)</S>
  <S sid="77" ssid="77">and psychological feature (?a feature of the mental life of a living organism?).</S>
  <S sid="78" ssid="78">The maximum depth of the noun hierarchy is 16 nodes.</S>
  <S sid="79" ssid="79">The nine types of relations defined on the noun subnetwork, in addition to the synonymy relation that is implicit in each node are: the hyponymy (IS-A) relation, 1 We began this work with WordNet 1.5, and stayed with this version despite newer releases in order to maintain strict comparability.</S>
  <S sid="80" ssid="80">Our experiments were complete before WordNet 2.0 was released.</S>
  <S sid="81" ssid="81">2 It seems to have been tacitly assumed by these researchers that results would generalize to the network hierarchies of other parts of speech.</S>
  <S sid="82" ssid="82">Nonetheless, Resnik and Diab (2000) caution that the properties of verbs and nouns might be different enough that they should be treated as separate problems, and recent research by Banerjee and Pedersen (2003) supports this assumption: they found that in a word-sense disambiguation task, their gloss-overlap measure of semantic relatedness (see Section 6.1 below) performed far worse on verbs (and slightly worse on adjectives) than it did on nouns.</S>
  <S sid="83" ssid="83">16 Budanitsky and Hirst Lexical Semantic Relatedness and its inverse, hypernymy; six meronymic (PART-OF) relations ?</S>
  <S sid="84" ssid="84">COMPONENT-OF, MEMBER-OF and SUBSTANCE-OF and their inverses; and antonymy, the COMPLEMENT- OF relation.</S>
  <S sid="85" ssid="85"></S>
  <S sid="86" ssid="86">We stipulate a global root root above the 11 unique beginners to ensure the existence of a path between any two nodes.</S>
  <S sid="87" ssid="87"></S>
  <S sid="88" ssid="88"></S>
  <S sid="89" ssid="89"></S>
  <S sid="90" ssid="90">That is, the relatedness of two words is equal to that of the most-related pair of concepts that they denote.</S>
  <S sid="91" ssid="91">2.4 Computing Taxonomic Path Length A simple way to compute semantic relatedness in a taxonomy such as WordNet is to view it as a graph and identify relatedness with path length between the concepts: ?The shorter the path from one node to another, the more similar they are?</S>
  <S sid="92" ssid="92">This approach was taken, for example, by Rada and colleagues (Rada et al.</S>
  <S sid="93" ssid="93">1989; Rada and Bicknell 1989), not on WordNet but on MeSH (Medical Subject Headings), a semantic hierarchy of terms used for indexing articles in the bibliographic retrieval system Medline.</S>
  <S sid="94" ssid="94">The network?s 15,000 terms form a nine-level hierarchy based on the BROADER-THAN relationship.</S>
  <S sid="95" ssid="95">The principal assumption of Rada and colleagues was that ?the number of edges between terms in the MeSH hierarchy is a measure of conceptual distance between terms?.</S>
  <S sid="96" ssid="96">Despite the simplicity of this distance function, the authors were able to obtain surprisingly good results in their information retrieval task.</S>
  <S sid="97" ssid="97">In part, their success can be explained by the observation of Lee, Kim, and Lee (1993) that while ?in the context of .</S>
  <S sid="98" ssid="98">semantic networks, shortest path lengths between two concepts are not sufficient to represent conceptual distance between those concepts .</S>
  <S sid="99" ssid="99">when the paths are restricted to IS-A links, the shortest path length does measure conceptual distance.?</S>
  <S sid="100" ssid="100">Another component of their success is certainly the specificity of the domain, which ensures relative homogeneity of the hierarchy.</S>
  <S sid="101" ssid="101">Notwithstanding these qualifications, Jarmasz and Szpakowicz (2003) also achieved good results with Roget?s Thesaurus by ignoring the index and treating the thesaurus as a simple hierarchy of clusters.</S>
  <S sid="102" ssid="102">They computed semantic similarity between two words as the length of the shortest path between them.</S>
  <S sid="103" ssid="103">The words were not explicitly disambiguated.</S>
  <S sid="104" ssid="104">17 Computational Linguistics Volume 32, Number 1 Hirst and St-Onge (1998; St-Onge 1995) adapted Morris and Hirst?s (1991) seman- tic distance algorithm from Roget?s Thesaurus to WordNet.3 They distinguished two strengths of semantic relations in WordNet.</S>
  <S sid="105" ssid="105">Two words are strongly related if one of the following holds: 1.</S>
  <S sid="106" ssid="106">They have a synset in common (for example, human and person).</S>
  <S sid="107" ssid="107">They are associated with two different synsets that are connected by the antonymy relation (for example, precursor and successor).</S>
  <S sid="108" ssid="108">One of the words is a compound (or a phrase) that includes the other and ?there is any kind of link at all between a synset associated with each word?</S>
  <S sid="109" ssid="109">(for example, school and private school).</S>
  <S sid="110" ssid="110">Two words are said to be in a medium-strong, or regular, relation if there exists an allowable path connecting a synset associated with each word (for example, carrot and apple).</S>
  <S sid="111" ssid="111">A path is allowable if it contains no more than five links and conforms to one of eight patterns, the intuition behind which is that ?the longer the path and the more changes of direction, the lower the weight?.</S>
  <S sid="112" ssid="112">The details of the patterns are outside of the scope of this paper; all we need to know for the purposes of subsequent discussion is that an allowable path may include more than one link and that the directions of links on the same path may vary (among upward (hypernymy and meronymy), downward (hyponymy and holonymy) and horizontal (antonymy)).</S>
  <S sid="113" ssid="113"></S>
  <S sid="114" ssid="114">turns(c1, c2) (2) where C and k are constants (in practice, they used C = 8 and k = 1), and turns(c1, c2) is the number of times the path between c1 and c2 changes direction.</S>
  <S sid="115" ssid="115">2.5 Scaling the Network Despite its apparent simplicity, a widely acknowledged problem with the edge- counting approach is that it typically ?relies on the notion that links in the taxonomy represent uniform distances?, which is typically not true: ?there is a wide variability in the ?distance?</S>
  <S sid="116" ssid="116">covered by a single taxonomic link, particularly when certain sub- taxonomies (e.g., biological categories) are much denser than others?</S>
  <S sid="117" ssid="117">For instance, in WordNet, the link rabbit ears IS-A television antenna covers an intuitively narrow distance, whereas white elephant IS-A possession covers an intuitively wide one.</S>
  <S sid="118" ssid="118">The approaches discussed below are attempts undertaken by various researchers to overcome this problem.</S>
  <S sid="119" ssid="119">2.5.1 Sussna?s Depth-relative Scaling.</S>
  <S sid="120" ssid="120">Sussna?s (1993, 1997) approach to scaling is based on his observation that sibling-concepts deep in a taxonomy appear to be more closely related to one another than those higher up.</S>
  <S sid="121" ssid="121">His method construes each edge 3 The original ideas and definitions of Hirst and St-Onge (1998) (including those for the direction of links ?</S>
  <S sid="122" ssid="122">see below) were intended to apply to all parts of speech and the entire range of relations featured in the WordNet ontology (which include cause, pertinence, also see, etc.).</S>
  <S sid="123" ssid="123">Like other researchers, however, they had to resort to the noun subnetwork only.</S>
  <S sid="124" ssid="124">In what follows, therefore, we will use appropriately restricted versions of their notions.</S>
  <S sid="125" ssid="125">18 Budanitsky and Hirst Lexical Semantic Relatedness in the WordNet noun network as consisting of two directed edges representing inverse relations.</S>
  <S sid="126" ssid="126">Each relation r has a weight or a range [minr; maxr] of weights associated with it: for example, hypernymy, hyponymy, holonymy, and meronymy have weights between minr = 1 and maxr = 2.4 The weight of each edge of type r from some node c1 is reduced by a factor that depends on the number of edges, edgesr, of the same type leaving c1: wt(c1 ?r) = maxr ?</S>
  <S sid="127" ssid="127">minr edgesr(c1) (3) The distance between two adjacent nodes c1 and c2 is then the average of the weights on each direction of the edge, scaled by the depth of the nodes: distS(c1, c2) = wt(c1 ?r) + wt(c2 ?r? )</S>
  <S sid="128" ssid="128">max{depth(c1), depth(c2)} (4) where r is the relation that holds between c1 and c2 and r?</S>
  <S sid="129" ssid="129">is its inverse (i.e., the relation that holds between c2 and c1).</S>
  <S sid="130" ssid="130">Finally, the semantic distance between two arbitrary nodes ci and cj is the sum of the distances between the pairs of adjacent nodes along the shortest path connecting them.</S>
  <S sid="131" ssid="131">2.5.2 Wu and Palmer?s Conceptual Similarity.</S>
  <S sid="132" ssid="132">In a paper on translating English verbs into Mandarin Chinese, Wu and Palmer (1994) introduce a scaled metric for what they call conceptual similarity between a pair of concepts c1 and c2 in a hierarchy as simWP(c1, c2) = 2 ?</S>
  <S sid="133" ssid="133">depth(lso(c1, c2)) len(c1, lso(c1, c2)) + len(c2, lso(c1, c2)) + 2 ?</S>
  <S sid="134" ssid="134">depth(lso(c1, c2)) (5) Note that depth(lso(c1, c2)) is the ?global?</S>
  <S sid="135" ssid="135">depth in the hierarchy; its role as a scaling factor can be seen more clearly if we recast equation 5 from similarity into distance: distWP(c1, c2) = 1 ?</S>
  <S sid="136" ssid="136">simWP(c1, c2) = len(c1, lso(c1, c2)) + len(c2, lso(c1, c2)) len(c1, lso(c1, c2)) + len(c2, lso(c1, c2)) + 2 ?</S>
  <S sid="137" ssid="137">depth(lso(c1, c2)) (6) 2.5.3 Leacock and Chodorow?s Normalized Path Length.</S>
  <S sid="138" ssid="138">Leacock and Chodorow (1998) proposed the following formula for computing the scaled semantic similarity between concepts c1 and c2 in WordNet: simLC(c1, c2) = ?</S>
  <S sid="139" ssid="139">log len(c1, c2) 2 ?</S>
  <S sid="140" ssid="140">max c?WordNet depth(c) (7) Here, the denominator includes the maximum depth of the hierarchy.</S>
  <S sid="141" ssid="141">4 Sussna?s experiments proved the precise details of the weighting scheme to be material only in fine-tuning the performance.</S>
  <S sid="142" ssid="142">19 Computational Linguistics Volume 32, Number 1 2.6 Information-based and Integrated Approaches Like the methods in the preceding subsection, the final group of approaches that we present attempt to counter problems inherent in a general ontology by incorporating an additional, and qualitatively different, knowledge source, namely information from a corpus.</S>
  <S sid="143" ssid="143">2.6.1 Resnik?s Information-based Approach.</S>
  <S sid="144" ssid="144">The key idea underlying Resnik?s (1995) approach is the intuition that one criterion of similarity between two concepts is ?the extent to which they share information in common?, which in an IS-A taxonomy can be determined by inspecting the relative position of the most-specific concept that subsumes them both.</S>
  <S sid="145" ssid="145">This intuition seems to be indirectly captured by edge-counting methods (such as that of Rada and colleagues; section 2.4 above), in that ?if the minimal path of IS-A links between two nodes is long, that means it is necessary to go high in the taxonomy, to more abstract concepts, in order to find a least upper bound?.</S>
  <S sid="146" ssid="146">An example given by Resnik is the difference in the relative positions of the most-specific subsumer of nickel and dime ?</S>
  <S sid="147" ssid="147">and that of nickel and credit card ?</S>
  <S sid="148" ssid="148">medium of exchange, as seen in Figure 1.</S>
  <S sid="149" ssid="149">In mathematical terms, for any concept c in the taxonomy, let p(c) be the probabil- ity of encountering an instance of concept c. Following the standard definition from information theory, the information content of c, IC(c), is then ?</S>
  <S sid="150" ssid="150">Thus, we can define the semantic similarity of a pair of concepts c1 and c2, as simR(c1, c2) = ?</S>
  <S sid="151" ssid="151">log p(lso(c1, c2)).</S>
  <S sid="152" ssid="152">(8) Notice that p is monotonic as one moves up the taxonomy: if c1 IS-A c2 then p(c1) ?</S>
  <S sid="153" ssid="153">For example, whenever we encounter a nickel, we have encountered a coin (Figure 1), so p(nickel) ?</S>
  <S sid="154" ssid="154">As a consequence, the higher the position of the most-specific subsumer for given two concepts in the taxonomy (i.e., the more abstract it is), the lower their similarity.</S>
  <S sid="155" ssid="155">In particular, if the taxonomy has a unique top node, its probability will be 1, so if the most-specific subsumer of a pair of concepts is the top node, their similarity will be ?</S>
  <S sid="156" ssid="156">log(1) = 0, as desired.</S>
  <S sid="157" ssid="157">Figure 1 Fragment of the WordNet taxonomy, showing most-specific subsumers of nickel and dime and of nickel and credit card.</S>
  <S sid="158" ssid="158">Solid lines represent IS-A links; dashed lines indicate that some intervening nodes have been omitted.</S>
  <S sid="159" ssid="159">Adapted from Resnik (1995).</S>
  <S sid="160" ssid="160">20 Budanitsky and Hirst Lexical Semantic Relatedness In Resnik?s experiments, the probabilities of concepts in the taxonomy were esti- mated from noun frequencies gathered from the one-million-word Brown Corpus of American English (Francis and Kuc?era 1982).</S>
  <S sid="161" ssid="161">The key characteristic of his counting method is that an individual occurrence of any noun in the corpus ?was counted as an occurrence of each taxonomic class containing it?.</S>
  <S sid="162" ssid="162">For example, an occurrence of the noun nickel was, in accordance with Figure 1, counted towards the frequency of nickel, coin, and so forth.</S>
  <S sid="163" ssid="163">Notice that, as a consequence of using raw (non-disambiguated) data, encountering a polysemous word contributes to the counts of all its senses.</S>
  <S sid="164" ssid="164">So in the case of nickel, the counts of both the coin and the metal senses will be increased.</S>
  <S sid="165" ssid="165">Formally, p(c) = ?</S>
  <S sid="166" ssid="166">w?W(c) count(w) N (9) where W(c) is the set of words (nouns) in the corpus whose senses are subsumed by concept c, and N is the total number of word (noun) tokens in the corpus that are also present in WordNet.</S>
  <S sid="167" ssid="167">Thus Resnik?s approach attempts to deal with the problem of varying link distances (see Section 2.5) by generally downplaying the role of network edges in the determi- nation of the degree of semantic proximity: Edges are used solely for locating super- ordinates of a pair of concepts; in particular, the number of links does not figure in any of the formulas pertaining to the method; and numerical evidence comes from corpus statistics, which are associated with nodes.</S>
  <S sid="168" ssid="168">This rather selective use of the structure of the taxonomy has its drawbacks, one of which is the indistinguishability, in terms of semantic distance, of any two pairs of concepts having the same most-specific subsumer.</S>
  <S sid="169" ssid="169">For example, in Figure 1, we find that simR(money, credit) = simR(dime, credit card), be- cause in each case the lso is medium of exchange, whereas, for an edge-based method such as Leacock and Chodorow?s (Section 2.5.3), clearly this is not so, as the number of edges in each case is different.</S>
  <S sid="170" ssid="170">2.6.2 Jiang and Conrath?s Combined Approach.</S>
  <S sid="171" ssid="171">Reacting to the disadvantages of Resnik?s method, Jiang and Conrath?s (1997) idea was to synthesize edge- and node- based techniques by restoring network edges to their dominant role in similarity com- putations, and using corpus statistics as a secondary, corrective factor.</S>
  <S sid="172" ssid="172">A complete exegesis of their work is presented by Budanitsky (1999); here we summarize only their conclusions.</S>
  <S sid="173" ssid="173">In the framework of the IS-A hierarchy, Jiang and Conrath postulated that the semantic distance of the link connecting a child-concept c to its parent-concept par(c) is proportional to the conditional probability p(c | par(c)) of encountering an instance of c given an instance of par(c).</S>
  <S sid="174" ssid="174">More specifically, distJC(c, par(c)) = ?</S>
  <S sid="175" ssid="175">log p(c | par(c)) (10) By definition, p(c | par(c)) = p(c&amp;par(c)) p(par(c)) (11) 21 Computational Linguistics Volume 32, Number 1 If we adopt Resnik?s scheme for assigning probabilities to concepts (Section 2.6.1), then p(c&amp;par(c)) = p(c), since any instance of a child is automatically an instance of its parent.</S>
  <S sid="176" ssid="176">Then, p(c|par(c)) = p(c) p(par(c)) (12) and, recalling the definition of information content, distJC(c, par(c)) = IC(c) ?</S>
  <S sid="177" ssid="177">IC(par(c)) (13) Given this as the measure of semantic distance from a node to its immediate parent, the semantic distance between an arbitrary pair of nodes was taken, as per common practice, to be the sum of the distances along the shortest path that connects the nodes: distJC(c1, c2) = ?</S>
  <S sid="178" ssid="178"></S>
  <S sid="179" ssid="179">The node lso(c1, c2) is removed from Path(c1, c2) in (14)), because it has no parent in the set.</S>
  <S sid="180" ssid="180">Ex- panding the sum in the right-hand side of equation (14), plugging in the expression for parent?child distance from equation (13), and performing necessary eliminations results in the following final formula for the semantic distance between concepts c1 and c2: distJC(c1, c2) = IC(c1) + IC(c2) ?</S>
  <S sid="181" ssid="181">IC(lso(c1, c2)) (15) = 2 log p(lso(c1, c2)) ?</S>
  <S sid="182" ssid="182">(log p(c1) + log p(c2)) (16) 2.6.3 Lin?s Universal Similarity Measure.</S>
  <S sid="183" ssid="183">Noticing that all of the similarity measures known to him were tied to a particular application, domain, or resource, Lin (1998b) attempted to define a measure of similarity that would be both universal (applicable to arbitrary objects and ?not presuming any form of knowledge representation?)</S>
  <S sid="184" ssid="184">and theoretically justified (?derived from a set of assumptions?, instead of ?directly by a formula?, so that ?if the assumptions are deemed reasonable, the similarity measure necessarily follows?).</S>
  <S sid="185" ssid="185">He used the following three intuitions as a basis: 1.</S>
  <S sid="186" ssid="186">The similarity between arbitrary objects A and B is related to their commonality; the more commonality they share, the more similar they are.</S>
  <S sid="187" ssid="187">The similarity between A and B is related to the differences between them; the more differences they have, the less similar they are.</S>
  <S sid="188" ssid="188">The maximum similarity between A and B is reached when A and B are identical, no matter how much commonality they share.</S>
  <S sid="189" ssid="189">Lin defined the commonality between A and B as the information content of ?the proposition that states the commonalities?</S>
  <S sid="190" ssid="190">between them, formally IC(comm(A, B)) (17) 22 Budanitsky and Hirst Lexical Semantic Relatedness and the difference between A and B as IC(descr(A, B)) ?</S>
  <S sid="191" ssid="191">IC(comm(A, B)) (18) where descr(A, B) is a proposition describing what A and B are.</S>
  <S sid="192" ssid="192">Given these assumptions and definitions and the apparatus of information theory, Lin proved the following: Similarity Theorem: The similarity between A and B is measured by the ratio between the amount of information needed to state their commonality and the information needed to fully describe what they are: simL(A, B) = log p(comm(A, B)) log p(descr(A, B)) (19) His measure of similarity between two concepts in a taxonomy is a corollary of this theorem: simL(c1, c2) = 2 ?</S>
  <S sid="193" ssid="193">log p(lso(c1, c2)) log p(c1) + log p(c2) (20) where the probabilities p(c) are determined in a manner analogous to Resnik?s p(c) (equation (9)).</S>
  <S sid="194" ssid="194">Evaluation Methods How can we reason about and evaluate computational measures of semantic related- ness?</S>
  <S sid="195" ssid="195">Three kinds of approaches are prevalent in the literature.</S>
  <S sid="196" ssid="196">The first kind (Wei 1993; Lin 1998b) is a (chiefly) theoretical examination of a pro- posed measure for those mathematical properties thought desirable, such as whether it is a metric (or the inverse of a metric), whether it has singularities, whether its parameter-projections are smooth functions, and so on.</S>
  <S sid="197" ssid="197">In our opinion, such analyses act at best as a coarse filter in the comparison of a set of measures and an even coarser one in the assessment of a single measure.</S>
  <S sid="198" ssid="198">The second kind of evaluation is comparison with human judgments.</S>
  <S sid="199" ssid="199">Insofar as human judgments of similarity and relatedness are deemed to be correct by definition, this clearly gives the best assessment of the ?goodness?</S>
  <S sid="200" ssid="200">Its main drawback lies in the difficulty of obtaining a large set of reliable, subject-independent judgments for comparison ?</S>
  <S sid="201" ssid="201">designing a psycholinguistic experiment, validating its results, and so on.</S>
  <S sid="202" ssid="202">(In Section 4.1 below, we will employ the rather limited data that such experiments have obtained to date.)</S>
  <S sid="203" ssid="203">The third approach is to evaluate the measures with respect to their performance in the framework of a particular application.</S>
  <S sid="204" ssid="204">If some particular NLP system requires a measure of semantic relatedness, we can compare different measures by seeing which one the system is most effective with, while holding all other aspects of the system constant.</S>
  <S sid="205" ssid="205">In the remainder of this paper, we will use the second and the third methods to compare several different measures (Sections 4 and 5 respectively).</S>
  <S sid="206" ssid="206">We focus on measures that use WordNet (Fellbaum 1998) as their knowledge source (to keep that as a constant) and that permit straightforward implementation as functions in a 23 Computational Linguistics Volume 32, Number 1 programming language.</S>
  <S sid="207" ssid="207">Therefore, we select the following five measures: Hirst and St-Onge?s (Section 2.4), Jiang and Conrath?s (Section 2.6.2), Leacock and Chodorow?s (Section 2.5.3), Lin?s (Section 2.6.3), and Resnik?s (Section 2.6.1).5 The first is claimed as a measure of semantic relatedness because it uses all noun relations in WordNet; the others are claimed only as measures of similarity because they use only the hyponymy relation.</S>
  <S sid="208" ssid="208">We implemented each measure, and used the Brown Corpus as the basis for the frequency counts needed in the information-based approaches.6 4.</S>
  <S sid="209" ssid="209">Comparison with Human Ratings of Semantic Relatedness In this section we compare the five chosen measures by how well they reflect human judgments of semantic relatedness.</S>
  <S sid="210" ssid="210">In addition, we will use the data that we obtain in this section to set closeness thresholds for the application-based evaluation of each measure in Section 5.</S>
  <S sid="211" ssid="211">4.1 Data As part of an investigation into ?the relationship between similarity of context and similarity of meaning (synonymy)?, Rubenstein and Goodenough (1965) obtained ?synonymy judgements?</S>
  <S sid="212" ssid="212">from 51 human subjects on 65 pairs of words.</S>
  <S sid="213" ssid="213">The pairs ranged from ?highly synonymous?</S>
  <S sid="214" ssid="214">to ?semantically unrelated?, and the subjects were asked to rate them, on the scale of 0.0 to 4.0, according to their ?similarity of mean- ing?</S>
  <S sid="215" ssid="215">(see Table 1, columns 2 and 3).</S>
  <S sid="216" ssid="216">For a similar study, Miller and Charles (1991) chose 30 pairs from the original 65, taking 10 from the ?high level (between 3 and 4. .</S>
  <S sid="217" ssid="217">), 10 from the intermediate level (between 1 and 3), and 10 from the low level (0 to 1) of semantic similarity?, and then obtained similarity judgments from 38 sub- jects, given the same instructions as above, on those 30 pairs (see Table 2, columns 2 and 3).7 4.2 Method For each of our five implemented measures, we obtained similarity or relatedness scores for the human-rated pairs.</S>
  <S sid="218" ssid="218">Where either or both of the words had more than one synset in WordNet, we took the most-related pair of synsets.</S>
  <S sid="219" ssid="219">For the measures of Resnik, Jiang and Conrath, and Lin, this replicates and extends a study by each of the original authors of their own measure.</S>
  <S sid="220" ssid="220">5 We also attempted to implement Sussna?s (1993, 1997) measure (Section 2.5.1), but ran into problems because a key element depended closely on the particulars of an earlier version of WordNet; see (Budanitsky 1999) for details.</S>
  <S sid="221" ssid="221">We did not include Wu and Palmer?s measure (Section 2.5.2) because Lin (1998b) has shown it to be a special case of his measure in which all child?parent probabilities are equal.</S>
  <S sid="222" ssid="222">6 In their original experiments, Lin and Jiang and Conrath used SemCor, a sense-tagged subset of the Brown Corpus, as their empirical data; but we decided to follow Resnik in using the full and untagged corpus.</S>
  <S sid="223" ssid="223">While this means trading accuracy for size, we believe that using a non-disambiguated corpus constitutes a more-general approach, as the availability and size of disambiguated texts such as SemCor is highly limited.</S>
  <S sid="224" ssid="224">7 As a result of a typographical error that occurred in the course of either Miller and Charles?s actual experiments or in its publication, the Rubenstein?Goodenough pair cord?smile became chord?smile.</S>
  <S sid="225" ssid="225">Probably because of the comparable degree of (dis)similarity, the error was not discovered and the latter pair has been used in all subsequent work.</S>
  <S sid="226" ssid="226">24 Budanitsky and Hirst Lexical Semantic Relatedness Table 1 Human and computer ratings of the Rubenstein?Goodenough set of word pairs (part 1 of 2).</S>
  <S sid="227" ssid="227"># Pair Humans relHS distJC simLC simL simR 1 cord smile 0.02 0 19.6 1.38 0.09 1.17 2 rooster voyage 0.04 0 26.9 0.91 0.00 0.00 3 noon string 0.04 0 22.6 1.50 0.00 0.00 4 fruit furnace 0.05 0 18.5 2.28 0.14 1.85 5 autograph shore 0.06 0 22.7 1.38 0.00 0.00 6 automobile wizard 0.11 0 17.8 1.50 0.09 0.97 7 mound stove 0.14 0 17.2 2.28 0.22 2.90 8 grin implement 0.18 0 16.6 1.28 0.00 0.00 9 asylum fruit 0.19 0 19.5 2.28 0.14 1.85 10 asylum monk 0.39 0 25.6 1.62 0.07 0.97 11 graveyard madhouse 0.42 0 29.7 1.18 0.00 0.00 12 glass magician 0.44 0 22.8 1.91 0.07 0.97 13 boy rooster 0.44 0 17.8 1.50 0.21 2.38 14 cushion jewel 0.45 0 22.9 2.28 0.13 1.85 15 monk slave 0.57 94 18.9 2.76 0.21 2.53 16 asylum cemetery 0.79 0 28.1 1.50 0.00 0.00 17 coast forest 0.85 0 20.2 2.28 0.12 1.50 18 grin lad 0.88 0 20.8 1.28 0.00 0.00 19 shore woodland 0.90 93 19.3 2.50 0.13 1.50 20 monk oracle 0.91 0 22.7 2.08 0.18 2.53 21 boy sage 0.96 93 19.9 2.50 0.20 2.53 22 automobile cushion 0.97 98 15.0 2.08 0.27 2.90 23 mound shore 0.97 91 12.4 2.76 0.49 6.19 24 lad wizard 0.99 94 16.5 2.76 0.23 2.53 25 forest graveyard 1.00 0 24.5 1.76 0.00 0.00 26 food rooster 1.09 0 17.4 1.38 0.10 0.97 27 cemetery woodland 1.18 0 25.0 1.76 0.00 0.00 28 shore voyage 1.22 0 23.7 1.38 0.00 0.00 29 bird woodland 1.24 0 18.1 2.08 0.13 1.50 30 coast hill 1.26 94 10.8 2.76 0.53 6.19 31 furnace implement 1.37 93 15.8 2.50 0.18 1.85 32 crane rooster 1.41 0 12.8 2.08 0.58 8.88 33 hill woodland 1.48 93 18.2 2.50 0.14 1.50 34 car journey 1.55 0 16.3 1.28 0.00 0.00 35 cemetery mound 1.69 0 23.8 1.91 0.00 0.00 36 glass jewel 1.78 0 22.0 2.08 0.14 1.85 37 magician oracle 1.82 98 1.0 3.50 0.96 13.58 38 crane implement 2.37 94 15.6 2.76 0.27 2.90 39 brother lad 2.41 94 16.3 2.76 0.23 2.53 40 sage wizard 2.46 93 22.8 2.50 0.18 2.53 41 oracle sage 2.61 0 26.2 2.08 0.16 2.53 42 bird crane 2.63 97 7.4 3.08 0.70 8.88 43 bird cock 2.63 150 5.4 4.08 0.76 8.88 44 food fruit 2.69 0 10.2 2.28 0.22 1.50 45 brother monk 2.74 93 19.2 2.50 0.20 2.53 46 asylum madhouse 3.04 150 0.2 4.08 0.99 15.70 47 furnace stove 3.11 0 20.5 2.08 0.13 1.85 48 magician wizard 3.21 200 0.00 5.08 1.00 13.58 49 hill mound 3.29 200 0.00 5.08 1.00 12.08 50 cord string 3.41 150 2.2 4.08 0.89 9.25 51 glass tumbler 3.45 150 5.9 4.08 0.79 11.34 52 grin smile 3.46 200 0.0 5.08 1.00 10.41 53 serf slave 3.46 0 19.8 2.28 0.34 5.28 54 journey voyage 3.58 150 5.2 4.08 0.74 7.71 25 Computational Linguistics Volume 32, Number 1 Table 1 (cont.)</S>
  <S sid="228" ssid="228"># Pair Humans relHS distJC simLC simL simR 55 autograph signature 3.59 150 2.4 4.08 0.92 14.29 56 coast shore 3.60 150 0.8 4.08 0.96 11.12 57 forest woodland 3.65 200 0.0 5.08 1.00 11.23 58 implement tool 3.66 150 1.1 4.08 0.91 6.20 59 cock rooster 3.68 200 0.0 5.08 1.00 14.29 60 boy lad 3.82 150 5.3 4.08 0.72 8.29 61 cushion pillow 3.84 150 0.7 4.08 0.97 13.58 62 cemetery graveyard 3.88 200 0.0 5.08 1.00 13.76 63 automobile car 3.92 200 0.0 5.08 1.00 8.62 64 midday noon 3.94 200 0.0 5.08 1.00 15.96 65 gem jewel 3.94 200 0.0 5.08 1.00 14.38 4.3 Results The mean ratings from Rubenstein and Goodenough?s and Miller and Charles?s original experiments (labeled ?Humans?)</S>
  <S sid="229" ssid="229">and the ratings of the Rubenstein?Goodenough and Miller?Charles word pairs produced by (our implementations of) the Hirst?St-Onge, Jiang?Conrath, Leacock?Chodorow, Lin, and Resnik measures of relatedness are given in Tables 1 and 2, and in Figures 2 and 3.8 4.4 Discussion When comparing two sets of ratings, we are interested in the strength of the linear asso- ciation between two quantitative variables, so we follow Resnik (1995) in summarizing the comparison results by means of the coefficient of correlation of each computational measure with the human ratings; see Table 3.</S>
  <S sid="230" ssid="230">(For Jiang and Conrath?s measure, the coefficients are negative because their measure returns distance rather than similarity; so for convenience, we show absolute values in the table.</S>
  <S sid="231" ssid="231">)9 4.4.1 Comparison to Upper Bound.</S>
  <S sid="232" ssid="232">To get an idea of the upper bound on perfor- mance of a computational measure, we can again refer to human performance.</S>
  <S sid="233" ssid="233">We have such an upper bound for the Miller and Charles word pairs (but not for the complete set of Rubenstein and Goodenough pairs): Resnik (1995) replicated Miller and 8 We have kept the original orderings of the pairs: from dissimilar to similar for the Rubenstein?</S>
  <S sid="234" ssid="234">Goodenough data and from similar to dissimilar for Miller?Charles.</S>
  <S sid="235" ssid="235">This explains why the two groups of graphs (Figures 2 and 3) as wholes have the opposite directions.</S>
  <S sid="236" ssid="236">Notice that because distJC measures distance, the Jiang?Conrath plot has a slope opposite to the rest of each group.</S>
  <S sid="237" ssid="237">9 Resnik (1995), Jiang and Conrath (1997), and Lin (1998b) report the coefficients of correlation between their measures and the Miller?Charles ratings to be 0.7911, 0.8282, and 0.8339, respectively, which differ slightly from the corresponding figures in Table 3.</S>
  <S sid="238" ssid="238">These discrepancies can be explained by possible minor differences in implementation (e.g., the compound-word recognition mechanism used in collecting the frequency data), differences between the versions of WordNet used in the experiments (Resnik), and differences in the corpora used to obtain the frequency data (Jiang and Conrath, Lin).</S>
  <S sid="239" ssid="239">Also, the coefficients reported by Resnik and Lin are actually based on only 28 out of the 30 Miller?Charles pairs because of a noun missing from an earlier version of WordNet.</S>
  <S sid="240" ssid="240">Jarmasz and Szpakowicz (2003) repeated the experiment, obtaining similar results to ours in some cases and markedly different results in others; in their experiment, the correlations obtained with their measure that uses the hierarchy of Roget?s Thesaurus exceeded those of all the WordNet measures.</S>
  <S sid="241" ssid="241">26 Budanitsky and Hirst Lexical Semantic Relatedness Table 2 Human and computer ratings of the Miller?Charles set of word pairs.</S>
  <S sid="242" ssid="242"># Pair Humans relHS distJC simLC simL simR 1 car automobile 3.92 200 0.0 5.08 1.00 8.62 2 gem jewel 3.84 200 0.0 5.08 1.00 14.38 3 journey voyage 3.84 150 5.2 4.08 0.74 7.71 4 boy lad 3.76 150 5.3 4.08 0.72 8.29 5 coast shore 3.70 150 0.9 4.08 0.96 11.12 6 asylum madhouse 3.61 150 0.2 4.08 0.99 15.70 7 magician wizard 3.50 200 0.0 5.08 1.00 13.58 8 midday noon 3.42 200 0.0 5.08 1.00 15.96 9 furnace stove 3.11 0 20.5 2.08 0.13 1.85 10 food fruit 3.08 0 10.2 2.28 0.22 1.50 11 bird cock 3.05 150 5.4 4.08 0.76 8.88 12 bird crane 2.97 97 7.4 3.08 0.70 8.88 13 tool implement 2.95 150 1.1 4.08 0.91 6.20 14 brother monk 2.82 93 19.2 2.50 0.20 2.53 15 lad brother 1.66 94 16.3 2.76 0.23 2.53 16 crane implement 1.68 94 15.7 2.76 0.27 2.90 17 journey car 1.16 0 16.3 1.28 0.00 0.00 18 monk oracle 1.10 0 22.7 2.08 0.18 2.53 19 cemetery woodland 0.95 0 25.0 1.76 0.00 0.00 20 food rooster 0.89 0 17.4 1.38 0.10 0.97 21 coast hill 0.87 94 10.9 2.76 0.53 6.19 22 forest graveyard 0.84 0 24.6 1.76 0.00 0.00 23 shore woodland 0.63 93 19.3 2.50 0.13 1.50 24 monk slave 0.55 94 18.9 2.76 0.21 2.53 25 coast forest 0.42 0 20.2 2.28 0.12 1.50 26 lad wizard 0.42 94 16.5 2.76 0.23 2.53 27 chord smile 0.13 0 20.2 1.62 0.18 2.23 28 glass magician 0.11 0 22.8 1.91 0.07 0.97 29 rooster voyage 0.08 0 26.9 0.91 0.00 0.00 30 noon string 0.08 0 22.6 1.50 0.00 0.00 Charles?s experiment with 10 subjects and found that the average correlation with the Miller?Charles mean ratings over his subjects was 0.8848.</S>
  <S sid="243" ssid="243">While the difference between the (absolute) values of the highest and lowest correlation coefficients in the ?M&amp;C?</S>
  <S sid="244" ssid="244">column of Table 3 is of the order of 0.1, all of the coefficients compare quite favorably with this estimate of the upper bound; furthermore, the difference diminishes almost twofold as we consider the larger Rubenstein?Goodenough dataset (column ?R&amp;G?</S>
  <S sid="245" ssid="245">of Table 3).10 In fact, the measures are divided in their reaction to increasing the size of the dataset: the correlations of relHS, simLC, and simR improve but those of distJC and simL deteriorate.</S>
  <S sid="246" ssid="246">This division might not be arbitrary: the last two depend on the same three quantities, log p(c1), log p(c2), and log p(lso(c1, c2)) (see equations (16) and (20)).</S>
  <S sid="247" ssid="247">(In fact, the coefficient for simR, which depends on only one of the three quantities, log p(lso(c1, c2)), improves only in the third digit.)</S>
  <S sid="248" ssid="248">However, with the present paucity of evidence, this connection remains hypothetical.</S>
  <S sid="249" ssid="249">10 None of the differences in either column are statistically significant at the .05 level.</S>
  <S sid="250" ssid="250">27 Computational Linguistics Volume 32, Number 1 4.4.2 Differences in the Performance and Behavior of the Measures.</S>
  <S sid="251" ssid="251">We now examine the results of each of the measures and the differences between them.</S>
  <S sid="252" ssid="252">To do this, we will sometimes look at differences in their behavior on individual word pairs.</S>
  <S sid="253" ssid="253">Looking at the graphs in Figures 2 and 3, we see that the discrete nature of the Hirst?St-Onge and Leacock?Chodorow measures is much more apparent than that of the others: i.e., the values that they can take on are just a fixed number of levels.</S>
  <S sid="254" ssid="254">This is, of course, a result of their being based on the same highly discrete factor: the path length.</S>
  <S sid="255" ssid="255">As a matter of fact, a more substantial correspondence between the two measures can be recognized from the graphs and explained in the same way.</S>
  <S sid="256" ssid="256">In each dataset, the upper portions of the two graphs are identical: namely, the sets of pairs affording the highest and second-highest values of the two measures (relHS ?</S>
  <S sid="257" ssid="257">150, simLC &gt; 4).</S>
  <S sid="258" ssid="258">This happens because these sets are composed of WordNet synonym and parent-child pairs, respectively.11 Further down the Y-axis, we find that for the Miller?Charles data, the two graphs still follow each other quite closely in the middle region (2.4?3.2 for simLC and 90?100 for relHS).</S>
  <S sid="259" ssid="259">For the larger set of Rubenstein and Goodenough?s, however, differences appear.</S>
  <S sid="260" ssid="260">The pair automobile?cushion (#22), for instance, is ranked even with magician?oracle (#37) by the Hirst?St-Onge measure but far below both magician?oracle (#37) and bird?crane (#42) by Leacock?Chodorow (and, in fact, by all the other measures).</S>
  <S sid="261" ssid="261">The cause of such a high ranking in the former case is the following meronymic connection in WordNet: automobile/.</S>
  <S sid="262" ssid="262">/car HAS-A suspension/suspension system (?a system of springs or shock absorbers connecting the wheels and axles to the chassis of a wheeled vehicle?)</S>
  <S sid="263" ssid="263">HAS-A cushion/shock absorber/shock (?a mechanical damper; absorbs energy of sudden impulses?).</S>
  <S sid="264" ssid="264">Since relHS is the only measure that takes meronymy (and other WordNet relations beyond IS-A) into account, no other measure detected this connection ?</S>
  <S sid="265" ssid="265">nor did the hu- man judges, whose task was to assess similarity, not generic relatedness; see Section 4.1).</S>
  <S sid="266" ssid="266">Finally, at the bottom portion of these two graphs, the picture becomes very dif- ferent, because relHS assigns all weakly-related pairs the value of zero.</S>
  <S sid="267" ssid="267">(In fact, it is this cut-off that we believe to be largely responsible for the relatively low ranking of the correlation coefficient of the Hirst?St-Onge measure.)</S>
  <S sid="268" ssid="268">In contrast, two other measures, Resnik?s and Lin?s, behave quite similarly to each other in the low-similarity region.</S>
  <S sid="269" ssid="269">In particular, their sets of zero-similarity pairs are identical, because the definitions of both measures include the term log p(lso(c1, c2)), which is zero for the pairs in question.12 For instance, for the pair rooster?voyage (M&amp;C #29, R&amp;G #2), the synsets rooster and voyage have different ?unique beginners?, and hence their lso ?</S>
  <S sid="270" ssid="270">in fact their sole common subsumer ?</S>
  <S sid="271" ssid="271">is the (fake) global root (see Section 2.5.3), which is the only concept whose probability is 1: cock/rooster (?adult male chicken?)</S>
  <S sid="272" ssid="272">IS-A domestic fowl/.</S>
  <S sid="273" ssid="273">/poultry IS-A .</S>
  <S sid="274" ssid="274">IS-A bird IS-A .</S>
  <S sid="275" ssid="275">IS-A animal/animate being/.</S>
  <S sid="276" ssid="276">/fauna IS-A life form/.</S>
  <S sid="277" ssid="277">/living thing (?any living entity?)</S>
  <S sid="278" ssid="278">IS-A entity (?something having concrete existence; living or nonliving?)</S>
  <S sid="279" ssid="279">IS-A global root, 11 More generally, the inverse image of the second highest value for simLC is a proper subset of that for relHS, for the latter would also include all the antonym and meronym?holonym pairs.</S>
  <S sid="280" ssid="280">The two datasets at hand, however, do not contain any instances from these categories.</S>
  <S sid="281" ssid="281">footnote 11), the former set actually constitutes a proper subset of the latter, as simL(c1, c2) will also be zero if either concept does not occur in the frequency-corpus (see equation (20)).</S>
  <S sid="282" ssid="282">However, no such instances appear in the data.</S>
  <S sid="283" ssid="283">28 Budanitsky and Hirst Lexical Semantic Relatedness Figure 2 Human and computer ratings of the Rubenstein?Goodenough set of word pairs, with sparse bands marked (see text).</S>
  <S sid="284" ssid="284">From left to right and top to bottom: The word pairs rated by (a) Rubenstein and Goodenough?s subjects; (b) by the Hirst?St-Onge similarity measure; (c) by the Jiang?Conrath distance measure; (d) by the Leacock?Chodorow similarity measure; (e) by the Lin similarity measure; and (f) by the Resnik similarity measure.</S>
  <S sid="285" ssid="285">29 Computational Linguistics Volume 32, Number 1 Figure 3 Human and computer ratings of the Miller?Charles set of word pairs, with sparse bands marked (see text).</S>
  <S sid="286" ssid="286">From left to right and top to bottom: The word pairs rated by (a) Miller and Charles?s subjects; (b) by the Hirst?St-Onge similarity measure; (c) by the Jiang?Conrath distance measure; (d) by the Leacock?Chodorow similarity measure; (e) by the Lin similarity measure; and (f) by the Resnik similarity measure.</S>
  <S sid="287" ssid="287">30 Budanitsky and Hirst Lexical Semantic Relatedness Table 3 The absolute values of the coefficients of correlation between human ratings of similarity (by Miller and Charles and by Rubenstein and Goodenough) and the five computational measures.</S>
  <S sid="288" ssid="288">Measure M&amp;C R&amp;G ?????????????????????</S>
  <S sid="289" ssid="289">Hirst and St-Onge, relHS .744 .786 Jiang and Conrath, distJC .850 .781 Leacock and Chodorow, simLC .816 .838 Lin, simL .829 .819 Resnik, simR .774 .779 voyage IS-A journey/journeying IS-A travel/.</S>
  <S sid="290" ssid="290">/traveling IS-A change of location/.</S>
  <S sid="291" ssid="291">/motion IS-A change (?the act of changing something?)</S>
  <S sid="292" ssid="292">IS-A action (?something done (usually as opposed to something said)?)</S>
  <S sid="293" ssid="293">IS-A act/human action/human activity (?something that people do or cause to happen?)</S>
  <S sid="294" ssid="294">IS-A global root.</S>
  <S sid="295" ssid="295">Analogously, although perhaps somewhat more surprisingly for a human reader, the same is true of the pair asylum?cemetery (R&amp;G #16): asylum/insane asylum/.</S>
  <S sid="296" ssid="296">/mental hospital IS-A hospital/infirmary IS-A medical building (?a building where medicine is practiced?)</S>
  <S sid="297" ssid="297">IS-A building/edifice IS-A .</S>
  <S sid="298" ssid="298">IS-A artifact/ artefact (?a man-made object?)</S>
  <S sid="299" ssid="299">IS-A object/inanimate object/physical object (?a nonliving entity?)</S>
  <S sid="300" ssid="300">IS-A entity IS-A global root, cemetery/graveyard/.</S>
  <S sid="301" ssid="301">/necropolis (?a tract of land used for burials?)</S>
  <S sid="302" ssid="302">IS-A site (?the piece of land on which something is located (or is to be located)?)</S>
  <S sid="303" ssid="303">IS-A position/place (?the particular portion of space occupied by a physical object?)</S>
  <S sid="304" ssid="304">IS-A location (?a point or extent in space?)</S>
  <S sid="305" ssid="305">IS-A global root.</S>
  <S sid="306" ssid="306">Looking back at the high-similarity portion of the graphs, but now taking into con- sideration the other three measures, we can make a couple more observations.</S>
  <S sid="307" ssid="307">First, the graphs of all of the measures except Resnik?s exhibit a ?line?</S>
  <S sid="308" ssid="308">of synonyms (comprising four points for the Miller?Charles dataset and nine points for Rubenstein?Goodenough) at the top (bottom for Jiang and Conrath?s measure).</S>
  <S sid="309" ssid="309">In the case of Resnik?s measure, simR(c, c) = ?</S>
  <S sid="310" ssid="310">log p(lso(c, c)) = ?</S>
  <S sid="311" ssid="311">log p(c) (see equation (8)), and hence the similarity of a concept to itself varies from one concept to another.</S>
  <S sid="312" ssid="312">Second, these ?lines?</S>
  <S sid="313" ssid="313">are not continuous, as one might expect from the graphs of the human judgments: For the Miller?Charles set, for instance, the line includes pairs 1, 2, 7, and 8, but omits pairs 3?6.</S>
  <S sid="314" ssid="314">This peculiarity is due entirely to WordNet, according to which gem and jewel (#2) and magician and wizard (#7) are synonyms, whereas journey and voyage (#3), boy and lad (#4), and even asylum and madhouse (#6) are not, but rather are related by IS-A: voyage (?a journey to some distant place?)</S>
  <S sid="315" ssid="315">IS-A journey/journeying (?the act of traveling from one place to another?</S>
  <S sid="316" ssid="316">), lad/laddie/cub/sonny/sonny boy (?a male child (a familiar term of address to a boy)?)</S>
  <S sid="317" ssid="317">IS-A boy/male child/child (?a young male person?</S>
  <S sid="318" ssid="318">), madhouse/nuthouse/.</S>
  <S sid="319" ssid="319">/sanatorium (?pejorative terms for an insane asylum?)</S>
  <S sid="320" ssid="320">IS-A asylum/insane asylum/.</S>
  <S sid="321" ssid="321">/mental hospital (?a hospital for mentally incompetent or unbalanced persons?).</S>
  <S sid="322" ssid="322">31 Computational Linguistics Volume 32, Number 1 Although, as we saw above, already for two measures the details of their medium- similarity regions differ, there appears to be an interesting commonality at the level of general structure: in the vicinity of sim = 2, the plots of human similarity ratings for both the Miller?Charles and the Rubenstein?Goodenough word pairs display a very clear horizontal band that contains no points.</S>
  <S sid="323" ssid="323">For the Miller?Charles data (Figure 3), the band separates the pair crane?implement (#16) from brother?monk (#14),13 and for the Rubenstein-Goodenough set (Figure 2), it separates magician?oracle (#37) from crane?</S>
  <S sid="324" ssid="324">implement (#38).</S>
  <S sid="325" ssid="325">On the graphs of the computed ratings, these empty bands correspond to regions with at most a few points?no more than two points for the Miller?Charles set and no more than four for the Rubenstein?Goodenough set.</S>
  <S sid="326" ssid="326">These regions are shown in Figures 3(b)?</S>
  <S sid="327" ssid="327">(f) and 2(b)?(f).</S>
  <S sid="328" ssid="328">This commonality among the measures suggests that if we were to partition the set of all word pairs into those that are deemed to be related and those that are deemed unrelated, the boundary between the two subsets for each measure (and for the human judgments, for that matter) would lie somewhere within these regions.</S>
  <S sid="329" ssid="329">4.4.3 The Limitations of this Analysis.</S>
  <S sid="330" ssid="330">While comparison with human judgments is the ideal way to evaluate a measure of similarity or semantic relatedness, in practice the tiny amount of data available (and only for similarity, not relatedness) is quite inadequate.</S>
  <S sid="331" ssid="331">But constructing a large-enough set of pairs and obtaining human judgments on them would be a very large task.14 Even more importantly, there are serious methodological problems with this ap- proach.</S>
  <S sid="332" ssid="332">It was implicit in the Rubenstein?Goodenough and Miller?Charles experiments that subjects were to use the dominant sense of the target words or mutually triggering related senses.</S>
  <S sid="333" ssid="333">But often what we are really interested in is the relationship between the concepts for which the words are merely surrogates; the human judgments that we need are of the relatedness of word-senses, not words.</S>
  <S sid="334" ssid="334">So the experimental situation would need to set up contexts that bias the sense selection for each target word and yet don?t bias the subject?s judgment of their a priori relationship, an almost self-contradictory situation.15 5.</S>
  <S sid="335" ssid="335">An Application-based Evaluation of Measures of Relatedness We now turn to a different approach to the evaluation of similarity and relatedness measures that tries to overcome the problems of comparison to human judgments that were described in the previous section.</S>
  <S sid="336" ssid="336">Here, we compare the measures through the 13 For some reason, Miller and Charles, while generally ordering their pairs from least to most similar, put crane?implement (#16) after lad?brother(#15), even though the former was rated more similar.</S>
  <S sid="337" ssid="337">14 Evgeniy Gabrilovich has recently made available a dataset of similarity judgments of 353 English word pairs that were used by Finkelstein et al.</S>
  <S sid="338" ssid="338">Unfortunately, this set is still very small, and, as Jarmasz and Szpakowicz (2003) point out, is culturally and politically biased.</S>
  <S sid="339" ssid="339">And the scarcely larger set of synonymy norms for nouns created by Whitten, Suter, and Frank (1979) covers only words with quite closely related senses, and hence is not useful here either.</S>
  <S sid="340" ssid="340">15 In their creation of a set of synonymy norms for nouns, Whitten, Suter, and Frank (1979) observed frequent artifacts stemming from the order of presentation of the stimuli that seem to be due to the practical impossibility of forcing a context of interpretation in the experimental setting.</S>
  <S sid="341" ssid="341">32 Budanitsky and Hirst Lexical Semantic Relatedness performance of an application that uses them: the detection and correction of real-word spelling errors in open-class words, i.e., malapropisms.</S>
  <S sid="342" ssid="342">While malapropism correction is also a useful application in its own right, it is particularly appropriate for evaluating measures of semantic relatedness.</S>
  <S sid="343" ssid="343">Naturally occurring coherent texts, by their nature, contain many instances of related pairs of words (Halliday and Hasan 1976; Morris and Hirst 1991; Hoey 1991; Morris and Hirst 2004).</S>
  <S sid="344" ssid="344">That is, they implicitly contain human judgments of relatedness that we could use in the evaluation of our relatedness measures.</S>
  <S sid="345" ssid="345">But, of course, we don?t know in practice just which pairs of words in a text are and aren?t related.</S>
  <S sid="346" ssid="346">We can get around this problem, however, by deliberately perturbing the coherence of the text ?</S>
  <S sid="347" ssid="347">that is, introduding semantic anomalies such as malapropisms ?</S>
  <S sid="348" ssid="348">and looking at the ability of the different relatedness measures to detect and correct the perturbations.</S>
  <S sid="349" ssid="349">5.1 Malapropism Detection and Correction as a Testbed Our malapropism corrector (Hirst and Budanitsky 2005) is based on the idea behind that of Hirst and St-Onge (1998): Look for semantic anomalies that can be removed by small changes to spelling.16 Words are (crudely) disambiguated where possible by accepting senses that are semantically related to possible senses of other nearby words.</S>
  <S sid="350" ssid="350">If all senses of any open-class, non?stop-list word that occurs only once in the text are found to be semantically unrelated to accepted senses of all other nearby words, but some sense of a spelling variation of that word would be related (or is identical to another token in the context), then it is hypothesized that the original word is an error and the variation is what the writer intended; a user would be warned of this possibility.</S>
  <S sid="351" ssid="351">For example, if no nearby word in a text is related to diary but one or more are related to dairy, we suggest to the user that it is the latter that was intended.</S>
  <S sid="352" ssid="352">The exact window size implied by ?nearby?</S>
  <S sid="353" ssid="353">is a parameter to the algorithm, as is the precise definition of spelling variation; see Hirst and Budanitsky (2005).</S>
  <S sid="354" ssid="354"></S>
  <S sid="355" ssid="355"></S>
  <S sid="356" ssid="356">While the performance of the malapropism corrector is inherently limited by these assumptions, we can nonetheless evaluate measures of semantic relatedness by com- paring their effect on its performance, as its limitations affect all measures equally.</S>
  <S sid="357" ssid="357">Regardless of the degree of adequacy of its performance, it is a ?level playing field?</S>
  <S sid="358" ssid="358">for 16 Although it shares underlying assumptions, our algorithm differs from that of Hirst and St-Onge in its mechanisms.</S>
  <S sid="359" ssid="359">In particular, Hirst and St-Onge?s algorithm was based on lexical chains (Morris and Hirst 1991), whereas our algorithm regards regions of text as bags of words.</S>
  <S sid="360" ssid="360">17 In fact, there is a semantic bias in human typing errors (Fromkin 1980), but not in the malapropism generator to be described below.</S>
  <S sid="361" ssid="361">33 Computational Linguistics Volume 32, Number 1 comparison of the measures.</S>
  <S sid="362" ssid="362">Hirst and Budanitsky (2005) discuss the practical aspects of the method and compare it with other approaches to the same problem.</S>
  <S sid="363" ssid="363">5.2 Method To test the measures in this application, we need a sufficiently large corpus of malapropisms in their context, each identified and annotated with its correction.</S>
  <S sid="364" ssid="364">Since no such corpus of naturally occurring malapropisms exists, we created one artificially.</S>
  <S sid="365" ssid="365">Following Hirst and St-Onge (1998), we took 500 articles from the Wall Street Journal corpus and, after removing proper nouns and stop-list words from consideration, replaced one word in every 200 with a spelling variation, choosing always WordNet nouns with at least one spelling variation.18 For example, in a sentence beginning To win the case, which was filed shortly after the indictment and is pending in Manhattan federal court .</S>
  <S sid="366" ssid="366">, the word case was replaced by cage.</S>
  <S sid="367" ssid="367">This gave us a corpus with 1,408 malapropisms among 107,233 candidates.19 We then tried to detect and correct the malapropisms by the algorithm outlined above, using in turn each of the five measures of semantic relatedness.</S>
  <S sid="368" ssid="368">For each, we used four different search scopes, i.e., window sizes: just the paragraph containing the target word (scope = 1); that paragraph plus one or two adjacent paragraphs on each side (scope = 3 and 5); and the complete article (scope = MAX).</S>
  <S sid="369" ssid="369">We also needed to set a threshold of ?relatedness?</S>
  <S sid="370" ssid="370">for each of the measures.</S>
  <S sid="371" ssid="371">This is because the malapropism-detection algorithm requires a boolean related?unrelated judgment, but each of the measures that we tested instead returns a numerical value of relatedness or similarity, and nothing in the measure (except for the Hirst?St-Onge measure) indicates which values count as ?close?.</S>
  <S sid="372" ssid="372">Moreover, the values from the dif- ferent measures are incommensurate.</S>
  <S sid="373" ssid="373">We therefore set the threshold of relatedness of each measure at the value at which it separated the higher level of the Rubenstein?</S>
  <S sid="374" ssid="374">Goodenough pairs (the near-synonyms) from the lower level, as we described in Sec- tion 4.4.2.</S>
  <S sid="375" ssid="375">5.3 Results Malapropism detection was viewed as a retrieval task and evaluated in terms of preci- sion, recall, and F-measure.</S>
  <S sid="376" ssid="376">Observe that semantic relatedness is used at two different places in the algorithm ?</S>
  <S sid="377" ssid="377">to judge whether an original word of the text is related to any nearby word and to judge whether a spelling variation is related ?</S>
  <S sid="378" ssid="378">and success in malapropism detection requires success at both stages.</S>
  <S sid="379" ssid="379">For the first stage, we say that a word is suspected of being a malapropism (and the word is a suspect) if it is judged to be unrelated to other words nearby; the word is a correct suspect if it is indeed a malapropism and a false suspect if it isn?t.</S>
  <S sid="380" ssid="380">At the second stage, we say that, given a suspect, an alarm is raised when a spelling variation of the suspect is judged to be related to a nearby word or words; and if an alarm word is a malapropism, we say that the alarm is a true alarm and that the malapropism has been detected; otherwise, it is a false alarm.</S>
  <S sid="381" ssid="381">Then we can define precision (P), recall (R), and F-measure (F) for 18 Articles too small to warrant such a replacement (19 in total) were excluded from further consideration.</S>
  <S sid="382" ssid="382">19 We assume that the original Wall Street Journal, being carefully edited text, contains essentially no malapropisms of its own.</S>
  <S sid="383" ssid="383">34 Budanitsky and Hirst Lexical Semantic Relatedness suspicion (S), involving only the first stage, and detection (D), involving both stages, as follows: Suspicion: PS = number of correct suspects number of suspects (21) RS = number of correct suspects number of malapropisms in text (22) FS = 2 ?</S>
  <S sid="384" ssid="384">RS PS + RS (23) Detection: PD = number of true alarms number of alarms (24) RD = number of true alarms number of malapropisms in text (25) FD = 2 ?</S>
  <S sid="385" ssid="385">RD PD + RD (26) The precision, recall, and F values are computed as the mean values of these statistics across our collection of 481 articles, which constitute a random sample from the population of all WSJ articles.</S>
  <S sid="386" ssid="386">All the comparisons that we make below, except for comparisons to baseline, are performed with the Bonferroni multiple-comparison technique (Agresti and Finlay 1997), with an overall significance level of .05.</S>
  <S sid="387" ssid="387">5.3.1 Suspicion.</S>
  <S sid="388" ssid="388">We look first at the results for suspicion ?</S>
  <S sid="389" ssid="389">just identifying words that have no semantically related word nearby.</S>
  <S sid="390" ssid="390">Obviously, the chance of finding some word that is judged to be related to the target word will increase with the size of the scope of the search (with a large enough scope, e.g., a complete book, we would probably find a relative for just about any word).</S>
  <S sid="391" ssid="391">So we expect recall to decrease as scope increases, because some relationships will be found even for malapropisms (i.e., there will be more false negatives).</S>
  <S sid="392" ssid="392">But we expect that precision will increase with scope, as it becomes more likely that (genuine) relationships will be found for non-malapropisms (i.e., there will be fewer false positives), and this factor will outweigh the decrease in the overall number of suspects found.</S>
  <S sid="393" ssid="393">Table 4 and Figure 4 show suspicion precision, recall, and F for each of the 5 ?</S>
  <S sid="394" ssid="394">4 combinations of measure and scope.</S>
  <S sid="395" ssid="395">The values of precision range from 3.3% (Resnik, scope = 1) to 11% (Jiang?Conrath, scope = MAX), with a mean of 6.2%, increasing with scope, as expected, for all measures except Hirst?St-Onge.</S>
  <S sid="396" ssid="396">More specifically, differences in precision are statistically significant for the difference between scope = 5 and scope = MAX for Leacock?Chodorow and between 1 and larger scopes for Lin, Resnik, and Jiang?Conrath; there are no significant differences for Hirst?St-Onge, which hence ap- pears flat overall.</S>
  <S sid="397" ssid="397">The values of recall range from just under 6% (Hirst?St-Onge, scope = MAX) to more than 72% (Resnik, scope = 1), with a mean of 39.7%, decreasing with scope, as expected.</S>
  <S sid="398" ssid="398">All differences in recall are statistically significant, except between scope = 3 and scope = 5 for all measures other than Resnik?s.</S>
  <S sid="399" ssid="399">F ranges from 5% (Hirst?</S>
  <S sid="400" ssid="400">St-Onge, scope = MAX) to 14% (Jiang?Conrath, scope = 5), with a mean of just under 35 Computational Linguistics Volume 32, Number 1 Table 4 Precision (PS), recall (RS), and F-measure (FS) for malapropism suspicion with five measures of semantic relatedness, varying the scope of the search for related words to 1, 3, or 5 paragraphs or the complete news article (MAX).</S>
  <S sid="401" ssid="401">Measure Scope PS RS FS??????????????????????</S>
  <S sid="402" ssid="402">Hirst?St-Onge 1 .056 .298 .091 3 .067 .159 .089 5 .069 .114 .079 MAX .051 .059 .049 Jiang?Conrath 1 .064 .536 .112 3 .086 .383 .135 5 .097 .326 .141 MAX .111 .233 .137 Leacock?Chodorow 1 .042 .702 .079 3 .052 .535 .094 5 .058 .463 .101 MAX .073 .356 .115 Lin 1 .047 .579 .086 3 .062 .421 .105 5 .067 .3</S>
</PAPER>
