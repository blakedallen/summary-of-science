<PAPER>
  <S sid="0">Models For The Semantic Classification Of Noun Phrases</S>
  <ABSTRACT>
    <S sid="1" ssid="1">Roles.</S>
    <S sid="2" ssid="2">In 28(3).</S>
    <S sid="3" ssid="3">Relation no.</S>
    <S sid="4" ssid="4">1 2 3 6 7 11 13 15 16 21 25 the rest 0.06103 0.11268 0.00939 0.04225 0.39437 0.01878 0.03286 0.25822 0.04694 0.01878 0.00469 0 Table 5: Sample row from the conditional probability table where the feature pair is entity-entity.</S>
    <S sid="5" ssid="5">The numbers in the top row identify the semantic relations (as in Table 4).</S>
    <S sid="6" ssid="6">Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.</S>
  </ABSTRACT>
  <SECTION title="1 Problem description" number="1">
    <S sid="7" ssid="1">This paper is about the automatic labeling of semantic relations in noun phrases (NPs).</S>
    <S sid="8" ssid="2">The semantic relations are the underlying relations between two concepts expressed by words or phrases.</S>
    <S sid="9" ssid="3">We distinguish here between semantic relations and semantic roles.</S>
    <S sid="10" ssid="4">Semantic roles are always between verbs (or nouns derived from verbs) and other constituents (run quickly, went to the store, computer maker), whereas semantic relations can occur between any constituents, for example in complex nominals (malaria mosquito (CAUSE)), genitives (girl&#8217;s mouth (PART-WHOLE)), prepositional phrases attached to nouns (man at the store (LOCATIVE)), or discourse level (The bus was late.</S>
    <S sid="11" ssid="5">As a result, I missed my appointment (CAUSE)).</S>
    <S sid="12" ssid="6">Thus, in a sense, semantic relations are more general than semantic roles and many semantic role types will appear on our list of semantic relations.</S>
    <S sid="13" ssid="7">The following NP level constructions are considered here (cf. the classifications provided by (Quirk et al.1985) and (Semmelmeyer and Bolander 1992)): (1) Compound Nominals consisting of two consecutive nouns (eg night club - a TEMPORAL relation - indicating that club functions at night), (2) Adjective Noun constructions where the adjectival modifier is derived from a noun (eg musical clock - a MAKE/PRODUCE relation), (3) Genitives (eg the door of the car - a PART-WHOLE relation), and (4) Adjective phrases (cf.</S>
    <S sid="14" ssid="8">(Semmelmeyer and Bolander 1992)) in which the modifier noun is expressed by a prepositional phrase which functions as an adjective (eg toy in the box - a LOCATION relation).</S>
    <S sid="15" ssid="9">Example: &#8220;Saturday&#8217;s snowfall topped a one-day record in Hartford, Connecticut, with the total of 12.5 inches, the weather service said.</S>
    <S sid="16" ssid="10">The storm claimed its fatality Thursday, when a car which was driven by a college student skidded on an interstate overpass in the mountains of Virginia and hit a concrete barrier, police said&#8221;.</S>
    <S sid="17" ssid="11">(www.cnn.com - &#8220;Record-setting Northeast snowstorm winding down&#8221;, Sunday, December 7, 2003).</S>
    <S sid="18" ssid="12">There are several semantic relations at the noun phrase level: (1) Saturday&#8217;s snowfall is a genitive encoding a TEMPORAL relation, (2) one-day record is a TOPIC noun compound indicating that record is about one-day snowing - an ellipsis here, (3) record in Hartford is an adjective phrase in a LOCATION relation, (4) total of 12.5 inches is an of-genitive that expresses MEASURE, (5) weather service is a noun compound in a TOPIC relation, (6) car which was driven by a college student encodes a THEME semantic role in an adjectival clause, (7) college student is a compound nominal in a PART-WHOLE/MEMBER-OF relation, (8) interstate overpass is a LOCATION noun compound, (9) mountains of Virginia is an of-genitive showing a PART-WHOLE/PLACE-AREA and LOCATION relation, (10) concrete barrier is a noun compound encoding PART-WHOLE/STUFF-OF.</S>
    <S sid="19" ssid="13">After many iterations over a period of time we identified a set of semantic relations that cover a large majority of text semantics.</S>
    <S sid="20" ssid="14">Table 1 lists these relations, their definitions, examples, and some references.</S>
    <S sid="21" ssid="15">Most of the time, the semantic relations are encoded by lexico-syntactic patterns that are highly ambiguous.</S>
    <S sid="22" ssid="16">One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge.</S>
    <S sid="23" ssid="17">Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct.</S>
    <S sid="24" ssid="18">This is called semantic blend (Quirk et al.1985).</S>
    <S sid="25" ssid="19">For example, the expression &#8220;Texas city&#8221; contains both a LOCATION as well as a PART-WHOLE relation.</S>
    <S sid="26" ssid="20">Other researchers have identified other sets of semantic relations (Levi 1979), (Uanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002).</S>
    <S sid="27" ssid="21">Our list contains the most frequently used semantic relations we have observed on a large corpus.</S>
    <S sid="28" ssid="22">Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals.</S>
    <S sid="29" ssid="23">The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds.</S>
    <S sid="30" ssid="24">Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993).</S>
    <S sid="31" ssid="25">Another popular approach focuses on the interpretation of the underlying semantics.</S>
    <S sid="32" ssid="26">Many researchers that followed this approach relied mostly on hand-coded rules (Finin 1980), (Uanderwende 1994).</S>
    <S sid="33" ssid="27">More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text.</S>
    <S sid="34" ssid="28">(Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques.</S>
  </SECTION>
  <SECTION title="2 Approach" number="2">
    <S sid="35" ssid="1">We approach the problem top-down, namely identify and study first the characteristics or feature vectors of each noun phrase linguistic pattern, then develop models for their semantic classification.</S>
    <S sid="36" ssid="2">This is in contrast to our prior approach ( (Girju, Badulescu, and Moldovan 2003a)) when we studied one relation at a time, and learned constraints to identify only that relation.</S>
    <S sid="37" ssid="3">We study the distribution of the semantic relations across different NP patterns and analyze the similarities and differences among resulting semantic spaces.</S>
    <S sid="38" ssid="4">We define a semantic space as the set of semantic relations an NP construction can encode.</S>
    <S sid="39" ssid="5">We aim at uncovering the general aspects that govern the NP semantics, and thus delineate the semantic space within clusters of semantic relations.</S>
    <S sid="40" ssid="6">This process has the advantage of reducing the annotation effort, a time consuming activity.</S>
    <S sid="41" ssid="7">Instead of manually annotating a corpus for each semantic relation, we do it only for each syntactic pattern and get a clear view of its semantic space.</S>
    <S sid="42" ssid="8">This syntactico-semantic approach allows us to explore various NP semantic classification models in a unified way.</S>
    <S sid="43" ssid="9">This approach stemmed from our desire to answer questions such as: It is well understood and agreed in linguistics that concepts can be represented in many ways using various constructions at different syntactic levels.</S>
    <S sid="44" ssid="10">This is in part why we decided to take the syntactico-semantic approach that analyzes semantic relations at different syntactic levels of representation.</S>
    <S sid="45" ssid="11">In this paper we focus only on the behavior of semantic relations at NP level.</S>
    <S sid="46" ssid="12">A thorough understanding of the syntactic and semantic characteristics of NPs provides valuable insights into defining the most representative feature vectors that ultimately drive the discriminating learning models.</S>
    <S sid="47" ssid="13">Levi (Levi 1979) defines complex nominals (CNs) as expressions that have a head noun preceded by one or more modifying nouns, or by adjectives derived from nouns (usually called denominal adjectives).</S>
    <S sid="48" ssid="14">Most importantly for us, each sequence of nouns, or possibly adjectives and nouns, has a particular meaning as a whole carrying an implicit semantic relation; for example, &#8220;spoon handle&#8221; (PART-WHOLE) or &#8220;musical clock&#8221; (MAKE/PRODUCE).</S>
    <S sid="49" ssid="15">CNs have been studied intensively in linguistics, psycho-linguistics, philosophy, and computational linguistics for a long time.</S>
    <S sid="50" ssid="16">The semantic interpretation of CNs proves to be very difficult for a number of reasons.</S>
    <S sid="51" ssid="17">(1) Sometimes the meaning changes with the head (eg &#8220;musical clock&#8221; MAKE/PRODUCE, &#8220;musical creation&#8221; THEME), other times with the modifier (eg &#8220;GM car&#8221; MAKE/PRODUCE, &#8220;family car&#8221; POSSESSION).</S>
    <S sid="52" ssid="18">(2) CNs&#8217; interpretation is knowledge intensive and can be idiosyncratic.</S>
    <S sid="53" ssid="19">For example, in order to interpret correctly &#8220;GM car&#8221; we have to know that GM is a car-producing company.</S>
    <S sid="54" ssid="20">(3) There can be many possible semantic relations between a given pair of word constituents.</S>
    <S sid="55" ssid="21">For example, &#8220;USA city&#8221; can be regarded as a LOCATION as well as a PART-WHOLE relation.</S>
    <S sid="56" ssid="22">(4) Interpretation of CNs can be highly context-dependent.</S>
    <S sid="57" ssid="23">For example, &#8220;apple juice seat&#8221; can be defined as &#8220;seat with apple juice on the table in front of it&#8221; (cf.</S>
    <S sid="58" ssid="24">(Downing 1977)).</S>
    <S sid="59" ssid="25">The semantic interpretation of genitive constructions is considered problematic by linguists because they involve an implicit relation that seems to allow for a large variety of relational interpretations; for example: &#8220;John&#8217;s car&#8221;-POSSESSOR-POSSESSEE, &#8220;Mary&#8217;s brother&#8221;-KINSHIP, &#8220;last year&#8217;s exhibition&#8221;-TEMPORAL, &#8220;a picture of my nice&#8221;-DEPICTION-DEPICTED, and &#8220;the desert&#8217;s oasis&#8221;-PART-WHOLE/PLACE-AREA.</S>
    <S sid="60" ssid="26">A characteristic of these constructions is that they are very productive, as the construction can be given various interpretations depending on the context.</S>
    <S sid="61" ssid="27">One such example is &#8220;Kate&#8217;s book&#8221; that can mean the book Kate owns, the book Kate wrote, or the book Kate is very fond of.</S>
    <S sid="62" ssid="28">Thus, the features that contribute to the semantic interpretation of genitives are: the nouns&#8217; semantic classes, the type of genitives, discourse and pragmatic information.</S>
    <S sid="63" ssid="29">Adjective Phrases are prepositional phrases attached to nouns acting as adjectives (cf.</S>
    <S sid="64" ssid="30">(Semmelmeyer and Bolander 1992)).</S>
    <S sid="65" ssid="31">Prepositions play an important role both syntactically and semantically.</S>
    <S sid="66" ssid="32">Semantically speaking, prepositional constructions can encode various semantic relations, their interpretations being provided most of the time by the underlying context.</S>
    <S sid="67" ssid="33">For instance, the preposition &#8220;with&#8221; can encode different semantic relations: (1) It was the girl with blue eyes (MERONYMY), The conclusion for us is that in addition to the nouns semantic classes, the preposition and the context play important roles here.</S>
    <S sid="68" ssid="34">In order to focus our research, we will concentrate for now only on noun - noun or adjective - noun compositional constructions at NP level, ie those whose meaning can be derived from the meaning of the constituent nouns (&#8220;door knob&#8221;, &#8220;cup of wine&#8221;).</S>
    <S sid="69" ssid="35">We don&#8217;t consider metaphorical names (eg, &#8220;ladyfinger&#8221;), metonymies (eg, &#8220;Vietnam veteran&#8221;), proper names (eg, &#8220;John Doe&#8221;), and NPs with coordinate structures in which neither noun is the head (eg, &#8220;player-coach&#8221;).</S>
    <S sid="70" ssid="36">However, we check if the constructions are non-compositional (lexicalized) (the meaning is a matter of convention; e.g., &#8220;soap opera&#8221;, &#8220;sea lion&#8221;), but only for statistical purposes.</S>
    <S sid="71" ssid="37">Fortunately, some of these can be identified with the help of lexicons.</S>
    <S sid="72" ssid="38">In order to provide a unified approach for the detection of semantic relations at different NP levels, we analyzed the syntactic and semantic behavior of these constructions on a large open-domain corpora of examples.</S>
    <S sid="73" ssid="39">Our intention is to answer questions like: (1) What are the semantic relations encoded by the NP-level constructions?, (2) What is their distribution on a large corpus?, (3) Is there a common subset of semantic relations that can be fully paraphrased by all types ofNP constructions?, (4) How many NPs are lexicalized?</S>
    <S sid="74" ssid="40">We have assembled a corpus from two sources: Wall Street Journal articles from TREC-9, and eXtended WordNet glosses (XWN) (http://xwn.hlt.utdallas.edu).</S>
    <S sid="75" ssid="41">We used XWN 2.0 since all its glosses are syntactically parsed and their words semantically disambiguated which saved us considerable amount of time.</S>
    <S sid="76" ssid="42">Table 2 shows for each syntactic category the number of randomly selected sentences from each corpus, the number of instances found in these sentences, and finally the number of instances that our group managed to annotate by hand.</S>
    <S sid="77" ssid="43">The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation from those listed in Table 1.</S>
  </SECTION>
  <SECTION title="Inter-annotator Agreement" number="3">
    <S sid="78" ssid="1">The annotators, four PhD students in Computational Semantics worked in groups of two, each group focusing on one half of the corpora to annotate.</S>
    <S sid="79" ssid="2">Noun - noun (adjective - noun, respectively) sequences of words were extracted using the Lauer heuristic (Lauer 1995) which looks for consecutive pairs of nouns that are neither preceded nor succeeded by a noun after each sentence was syntactically parsed with Charniak parser (Charniak 2001) (for XWN we used the gold parse trees).</S>
    <S sid="80" ssid="3">Moreover, they were provided with the sentence in which the pairs occurred along with their corresponding WordNet senses.</S>
    <S sid="81" ssid="4">Whenever the annotators found an example encoding a semantic relation other than those provided or they didn&#8217;t know what interpretation to give, they had to tag it as &#8220;OTHERS&#8221;.</S>
    <S sid="82" ssid="5">Besides the type of relation, the annotators were asked to provide information about the order of the modifier and the head nouns in the syntactic constructions if applicable.</S>
    <S sid="83" ssid="6">For instance, in &#8220;owner of car&#8221;-POSSESSION the possessor owner is followed by the possessee car, while in &#8220;car ofJohn&#8221;-POSSESSION/R the order is reversed.</S>
    <S sid="84" ssid="7">On average, 30% of the training examples had the nouns in reverse order.</S>
    <S sid="85" ssid="8">Most of the time, one instance was tagged with one semantic relation, but there were also situations in which an example could belong to more than one relation in the same context.</S>
    <S sid="86" ssid="9">For example, the genitive &#8220;city of USA&#8221; was tagged as a PART-WHOLE/PLACE-AREA relation and as a LOCATION relation.</S>
    <S sid="87" ssid="10">Overall, there were 608 such cases in the training corpora.</S>
    <S sid="88" ssid="11">Moreover, the annotators were asked to indicate if the instance was lexicalized or not.</S>
    <S sid="89" ssid="12">Also, the judges tagged the NP nouns in the training corpus with their corresponding WordNet senses.</S>
    <S sid="90" ssid="13">The annotators&#8217; agreement was measured using the Kappa statistics, one of the most frequently used measure of inter-annotator agreement for classification tasks: , where is the proportion of times the raters agree and is the probability of agreement by chance.</S>
    <S sid="91" ssid="14">The K coefficient is 1 if there is a total agreement among the annotators, and 0 if there is no agreement other than that expected to occur by chance.</S>
    <S sid="92" ssid="15">Table 3 shows the semantic relations inter-annotator agreement on both training and test corpora for each NP construction.</S>
    <S sid="93" ssid="16">For each construction, the corpus was splint into 80/20 training/testing ratio after agreement.</S>
    <S sid="94" ssid="17">We computed the K coefficient only for those instances tagged with one of the 35 semantic relations.</S>
    <S sid="95" ssid="18">For each pattern, we also computed the number of pairs that were tagged with OTHERS by both annotators, over the number of examples classified in this category by at least one of the judges, averaged by the number of patterns considered.</S>
    <S sid="96" ssid="19">The K coefficient shows a fair to good level of agreement for the training and testing data on the set of 35 relations, taking into consideration the task difficulty.</S>
    <S sid="97" ssid="20">This can be explained by the instructions the annotators received prior to annotation and by their expertise in lexical semantics.</S>
    <S sid="98" ssid="21">There were many heated discussions as well.</S>
    <S sid="99" ssid="22">Even noun phrase constructions are very productive allowing for a large number of possible interpretations, Table 4 shows that a relatively small set of 35 semantic relations covers a significant part of the semantic distribution of these constructions on a large open-domain corpus.</S>
    <S sid="100" ssid="23">Moreover, the distribution of these relations is dependent on the type of NP construction, each type encoding a particular subset.</S>
    <S sid="101" ssid="24">For example, in the case of of-genitives, there were 21 relations found from the total of 35 relations considered.</S>
    <S sid="102" ssid="25">The most frequently occurring relations were PART-WHOLE, ATTRIBUTE-HOLDER, POSSESSION, LOCATION, SOURCE, TOPIC, and THEME.</S>
    <S sid="103" ssid="26">By comparing the subsets of semantic relations in each column we can notice that these semantic spaces are not identical, proving our initial intuition that the NP constructions cannot be alternative ways of packing the same information.</S>
    <S sid="104" ssid="27">Table 4 also shows that there is a subset of semantic relations that can be fully encoded by all types of NP constructions.</S>
    <S sid="105" ssid="28">The statistics about the lexicalized examples are as follows: N-N (30.01%), Adj-N (0%), s-genitive (0%), of-genitive (0%), adjective phrase (1%).</S>
    <S sid="106" ssid="29">From the 30.01% lexicalized noun compounds , 18% were proper names.</S>
    <S sid="107" ssid="30">This simple analysis leads to the important conclusion that the NP constructions must be treated separately as their semantic content is different.</S>
    <S sid="108" ssid="31">This observation is also partially consistent with other recent work in linguistics and computational linguistics on the grammatical variation of the English genitives, noun compounds, and adjective phrases.</S>
    <S sid="109" ssid="32">We can draw from here the following conclusions: Given each NP syntactic construction considered, the goal is to develop a procedure for the automatic labeling of the semantic relations they encode.</S>
    <S sid="110" ssid="33">The semantic relation derives from the lexical, syntactic, semantic and contextual features of each NP construction.</S>
    <S sid="111" ssid="34">Semantic classification of syntactic patterns in general can be formulated as a learning problem, and thus benefit from the theoretical foundation and experience gained with various learning paradigms.</S>
    <S sid="112" ssid="35">This is a multi-class classification problem since the output can be one of the semantic relations in the set.</S>
    <S sid="113" ssid="36">We cast this as a supervised learning problem where input/ output pairs are available as training data.</S>
    <S sid="114" ssid="37">An important first step is to map the characteristics of each NP construction (usually not numerical) into feature vectors.</S>
    <S sid="115" ssid="38">Let&#8217;s define with the feature vector of an instance and let be the space of all instances; ie .</S>
    <S sid="116" ssid="39">The multi-class classification is performed by a function that maps the feature space into a semantic space , , where is the set of semantic relations from Table 1, ie .</S>
    <S sid="117" ssid="40">Let be the training set of examples or instances where is the number of examples each accompanied by its semantic relation label .</S>
    <S sid="118" ssid="41">The problem is to decide which semantic relation to assign to a new, unseen example .</S>
    <S sid="119" ssid="42">In order to classify a given set of examples (members of ), one needs some kind of measure of the similarity (or the difference) between any two given members of .</S>
    <S sid="120" ssid="43">Most of the times it is difficult to explicitly define this function, since can contain features with numerical as well as non-numerical values.</S>
    <S sid="121" ssid="44">Note that the features, thus space , vary from an NP pattern to another and the classification function will be pattern dependent.</S>
    <S sid="122" ssid="45">The novelty of this learning problem is the feature space and the nature ofthe discriminating An essential aspect of our approach below is the word sense disambiguation (WSD) of the content words (nouns, verbs, adjectives and adverbs).</S>
    <S sid="123" ssid="46">Using a stateof-the-art open-text WSD system, each word is mapped into its corresponding WordNet 2.0 sense.</S>
    <S sid="124" ssid="47">When disambiguating each word, the WSD algorithm takes into account the surrounding words, and this is one important way through which context gets to play a role in the semantic classification of NPs.</S>
    <S sid="125" ssid="48">So far, we have identified and experimented with the following NP features: specifies the WordNet synset of the modifier noun.</S>
    <S sid="126" ssid="49">In case the modifier is a denominal adjective, we take the synset of the noun from which the adjective is derived.</S>
    <S sid="127" ssid="50">Example: &#8220;musical clock&#8221; - MAKE/PRODUCE, and &#8220;electric clock&#8221;- INSTRUMENT.</S>
    <S sid="128" ssid="51">Several learning models can be used to provide the discriminating function .</S>
    <S sid="129" ssid="52">So far we have experimented with three models: (1) semantic scattering, (2) decision trees, and (3) naive Bayes.</S>
    <S sid="130" ssid="53">The first is described below, the other two are fairly well known from the machine learning literature.</S>
    <S sid="131" ssid="54">Semantic Scattering.</S>
    <S sid="132" ssid="55">This is a new model developed by us particularly useful for the classification of compound nominals without nominalization.</S>
    <S sid="133" ssid="56">The semantic relation in this case derives from the semantics of the two noun concepts participating in these constructions as well as the surrounding context.</S>
    <S sid="134" ssid="57">Model Formulation.</S>
    <S sid="135" ssid="58">Let us define with and the sets of semantic class features (ie, function derived for each syntactic pattern.</S>
    <S sid="136" ssid="59">WordNet synsets) of the NP modifiers and, respectively NP heads (ie features 2 and 1).</S>
    <S sid="137" ssid="60">The compound nominal semantics is distinctly specified by the feature pair , written shortly as .</S>
    <S sid="138" ssid="61">Given feature pair , the probability of a semantic relation r is , defined as the ratio between the number of occurrences of a relation r in the presence of feature pair over the number of occurrences of feature pair in the corpus.</S>
    <S sid="139" ssid="62">The most probable relation is Since the number of possible noun synsets combinations is large, it is difficult to measure the quantities and on a training corpus to calculate .</S>
    <S sid="140" ssid="63">One way of approximating the feature vector is to perform a semantic generalization, by replacing the synsets with their most general hypernyms, followed by a series of specializations for the purpose of eliminating ambiguities in the training data.</S>
    <S sid="141" ssid="64">There are 9 noun hierarchies, thus only 81 possible combinations at the most general level.</S>
    <S sid="142" ssid="65">Table 5 shows a row of the probability matrix for .</S>
    <S sid="143" ssid="66">Each entry, for which there is more than one relation, is scattered into other subclasses through an iterative process till there is only one semantic relation per line.</S>
    <S sid="144" ssid="67">This can be achieved by specializing the feature pair&#8217;s semantic classes with their immediate WordNet hyponyms.</S>
    <S sid="145" ssid="68">The iterative process stops when new training data does not bring any improvements (see Table 6).</S>
    <S sid="146" ssid="69">The f-measure results obtained so far are summarized in Table 7.</S>
    <S sid="147" ssid="70">Overall, these results are very encouraging given the complexity of the problem.</S>
  </SECTION>
  <SECTION title="2.5.5 Error Analysis" number="4">
    <S sid="148" ssid="1">An important way of improving the performance of a system is to do a detailed error analysis of the results.</S>
    <S sid="149" ssid="2">We have analyzed the sources of errors in each case and found out that most of them are due to (in decreasing order of importance): (1) errors in automatic sense disambiguation, (2) missing combinations of features that occur in testing but not in the training data, (3) levels of specialization are too high, (4) errors caused by metonymy, (6) errors in the modifier-head order, and others.</S>
    <S sid="150" ssid="3">These errors could be substantially decreased with more research effort.</S>
    <S sid="151" ssid="4">A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non-nominalizations.</S>
    <S sid="152" ssid="5">The reason is that nominalization noun phrases seem to call for a different set of learning features than the non-nominalization noun phrases, taking advantage of the underlying verbargument structure.</S>
    <S sid="153" ssid="6">Details about this approach are provided in (Girju et al. 2004)).</S>
  </SECTION>
  <SECTION title="3 Applications" number="5">
    <S sid="154" ssid="1">Semantic relations occur with high frequency in open text, and thus, their discovery is paramount for many applications.</S>
    <S sid="155" ssid="2">One important application is Question Answering.</S>
    <S sid="156" ssid="3">A powerful method of answering more difficult questions is to associate to each question the semantic relation that reflects the meaning of that question and then search for that semantic relation over the candidates of semantically tagged paragraphs.</S>
    <S sid="157" ssid="4">Here is an example.</S>
    <S sid="158" ssid="5">Q.</S>
    <S sid="159" ssid="6">Where have nuclear incidents occurred?</S>
    <S sid="160" ssid="7">From the question stem word where, we know the question asks for a LOCATION which is found in the complex nominal &#8220;Three Mile Island&#8221;-LOCATION of the sentence &#8220;The Three Mile Island nuclear incident caused a DOE policy crisis&#8221;, leading to the correct answer &#8220;Three Mile Island&#8221;.</S>
    <S sid="161" ssid="8">Q.</S>
    <S sid="162" ssid="9">What did the factory in Howell Michigan make?</S>
    <S sid="163" ssid="10">The verb make tells us to look for a MAKE/PRODUCE relation which is found in the complex nominal &#8220;car factory&#8221;-MAKE/PRODUCE of the text: &#8220;The car factory in Howell Michigan closed on Dec 22, 1991&#8221; which leads to answer car.</S>
    <S sid="164" ssid="11">Another important application is building semantically rich ontologies.</S>
    <S sid="165" ssid="12">Last but not least, the discovery of text semantic relations can improve syntactic parsing and even WSD which in turn affects directly the accuracy of other NLP modules and applications.</S>
    <S sid="166" ssid="13">We consider these applications for future work.</S>
  </SECTION>
</PAPER>
