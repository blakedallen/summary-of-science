Learning Semantic Correspondences with Less Supervision
A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.
To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.
We show that our model generalizes across three domains of increasing difficulty--Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.
We propose a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. We use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts.
