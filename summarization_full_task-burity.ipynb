{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization - Full task 'keep it simple'\n",
    "\n",
    "Scientific papers summarization task divided into 4 steps:\n",
    "\n",
    "**Step 0 - Download and parse the data** \\\n",
    "**Step 1 - Cited text spans identification** \\\n",
    "**Step 2 - Prepare data for train and inference** \\\n",
    "**Step 3 - Summarize with Pre-Trained Pegasus (no fine-tuning)** \\\n",
    "**Step 4 - Fine-Tune Pre-Trained Pegasus**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Download the dataset\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# For regular expressions\n",
    "import re\n",
    "# For handling string\n",
    "import string\n",
    "# For performing mathematical operations\n",
    "import math\n",
    "# Importing spacy\n",
    "import spacy\n",
    "# Importing json to read input\n",
    "import json\n",
    "# Importing rouge for evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import html\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import statistics as stats\n",
    "import time\n",
    "\n",
    "from scipy import spatial\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "\n",
    "# for turn text into sentences\n",
    "import nltk.data\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import html\n",
    "from lxml import etree\n",
    "import unidecode\n",
    "\n",
    "from scipy import stats as s\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "import nltk \n",
    "import glob, os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.etree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "plot_dims = (16, 16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# url = \"https://cs.stanford.edu/~myasu/projects/scisumm_net/scisummnet_release1.1__20190413.zip\"\n",
    "# response = requests.get(url)\n",
    "# with zipfile.ZipFile(io.BytesIO(response.content)) as zipObj:\n",
    "#     # Extract all the contents of zip file in different directory\n",
    "#     zipObj.extractall(\"nlp_data\")\n",
    "#     print(\"File is unzipped in nlp_data folder\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pip\n",
    "# from pip._internal import main as pipmain\n",
    "\n",
    "# pipmain(['install', 'datasets'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Download and parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "DATA_DIR = \"data/nlp_data/scisummnet_release1.1__20190413/top1000_complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all raw text, break all papers into two parts -- Abstract and rest of document\n",
    "#first get all filepaths\n",
    "xmlfiles = []\n",
    "citations = []\n",
    "golden_summaries = []\n",
    "for subdir, dirs, files in os.walk(DATA_DIR):\n",
    "    for filename in files:\n",
    "        filepath = subdir + os.sep + filename\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            xmlfiles.append(filepath)\n",
    "        if filepath.endswith(\".json\"):\n",
    "            citations.append(filepath)\n",
    "        if filepath.endswith(\".txt\"):\n",
    "            golden_summaries.append(filepath)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next parse all XML documents\n",
    "\n",
    "def parse_xml_abstract(fp):\n",
    "    \"\"\" parse an XML journal article into an abstract and the rest of the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(fp)\n",
    "    except Exception as e:\n",
    "        return \"\",\"\",str(e)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    ab = []\n",
    "    bod = []\n",
    "    \n",
    "    for child in root:\n",
    "        if child.tag == \"ABSTRACT\":\n",
    "            for block in child:\n",
    "                ab.append(block.text)\n",
    "        else:\n",
    "            for block in child:\n",
    "                bod.append(block.text)\n",
    "                \n",
    "    #convert from list --> string\n",
    "    abstract = \"\\n\".join(ab)\n",
    "    body = \"\\n\".join(bod)\n",
    "    \n",
    "    #decode html entities\n",
    "    abstract = html.unescape(abstract)\n",
    "    body = html.unescape(body)\n",
    "    \n",
    "    return abstract,body,\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>golden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present a method for extracting parts of ob...</td>\n",
       "      <td>We present a method of extracting parts of obj...</td>\n",
       "      <td>[Berland and Charniak (1999) use Hearst style ...</td>\n",
       "      <td>Finding Parts In Very Large Corpora\\nWe presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>[The program takes the output of char_align (C...</td>\n",
       "      <td>The Mathematics Of Statistical Machine Transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Previous work has shown that Chinese word segm...</td>\n",
       "      <td>Word segmentation is considered an important f...</td>\n",
       "      <td>[Chinese word segmentation is done by the Stan...</td>\n",
       "      <td>Optimizing Chinese Word Segmentation for Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We examine the viability of building large pol...</td>\n",
       "      <td>Polarity lexicons are large lists of phrases t...</td>\n",
       "      <td>[Recent work in this area includes Velikovich ...</td>\n",
       "      <td>The viability of web-derived polarity lexicons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extracting semantic relationships between enti...</td>\n",
       "      <td>Extraction of semantic relationships between e...</td>\n",
       "      <td>[They use two kinds of features: syntactic one...</td>\n",
       "      <td>Combining Lexical Syntactic And Semantic Featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>[In addition, Niessen and Ney (2004) decompose...</td>\n",
       "      <td>Statistical Machine Translation With Scarce Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>We have developed a new program called alignin...</td>\n",
       "      <td>Aligning parallel texts has recently received ...</td>\n",
       "      <td>[There have been quite a number of recent pape...</td>\n",
       "      <td>Robust Bilingual Word Alignment For Machine Ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>We present an approach to pronoun resolution b...</td>\n",
       "      <td>Pronoun resolution is a difficult but vital pa...</td>\n",
       "      <td>[, We follow the closed track setting where sy...</td>\n",
       "      <td>Bootstrapping Path-Based Pronoun Resolution\\nW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>We use logical inference techniques for recogn...</td>\n",
       "      <td>Recognising textual entailment (RTE) is the ta...</td>\n",
       "      <td>[However, this method does not work for realwo...</td>\n",
       "      <td>Recognising Textual Entailment With Logical In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>This paper deals with two important ambiguitie...</td>\n",
       "      <td>The problem with successful resolution of ambi...</td>\n",
       "      <td>[The state of the art is a supervised algorith...</td>\n",
       "      <td>Corpus Based PP Attachment Ambiguity Resolutio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1009 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     We present a method for extracting parts of ob...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Previous work has shown that Chinese word segm...   \n",
       "3     We examine the viability of building large pol...   \n",
       "4     Extracting semantic relationships between enti...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  We have developed a new program called alignin...   \n",
       "1006  We present an approach to pronoun resolution b...   \n",
       "1007  We use logical inference techniques for recogn...   \n",
       "1008  This paper deals with two important ambiguitie...   \n",
       "\n",
       "                                                   body  \\\n",
       "0     We present a method of extracting parts of obj...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Word segmentation is considered an important f...   \n",
       "3     Polarity lexicons are large lists of phrases t...   \n",
       "4     Extraction of semantic relationships between e...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  Aligning parallel texts has recently received ...   \n",
       "1006  Pronoun resolution is a difficult but vital pa...   \n",
       "1007  Recognising textual entailment (RTE) is the ta...   \n",
       "1008  The problem with successful resolution of ambi...   \n",
       "\n",
       "                                              citations  \\\n",
       "0     [Berland and Charniak (1999) use Hearst style ...   \n",
       "1     [The program takes the output of char_align (C...   \n",
       "2     [Chinese word segmentation is done by the Stan...   \n",
       "3     [Recent work in this area includes Velikovich ...   \n",
       "4     [They use two kinds of features: syntactic one...   \n",
       "...                                                 ...   \n",
       "1004  [In addition, Niessen and Ney (2004) decompose...   \n",
       "1005  [There have been quite a number of recent pape...   \n",
       "1006  [, We follow the closed track setting where sy...   \n",
       "1007  [However, this method does not work for realwo...   \n",
       "1008  [The state of the art is a supervised algorith...   \n",
       "\n",
       "                                                 golden  \n",
       "0     Finding Parts In Very Large Corpora\\nWe presen...  \n",
       "1     The Mathematics Of Statistical Machine Transla...  \n",
       "2     Optimizing Chinese Word Segmentation for Machi...  \n",
       "3     The viability of web-derived polarity lexicons...  \n",
       "4     Combining Lexical Syntactic And Semantic Featu...  \n",
       "...                                                 ...  \n",
       "1004  Statistical Machine Translation With Scarce Re...  \n",
       "1005  Robust Bilingual Word Alignment For Machine Ai...  \n",
       "1006  Bootstrapping Path-Based Pronoun Resolution\\nW...  \n",
       "1007  Recognising Textual Entailment With Logical In...  \n",
       "1008  Corpus Based PP Attachment Ambiguity Resolutio...  \n",
       "\n",
       "[1009 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create DF with papers and citations\n",
    "\n",
    "raw_cols = []\n",
    "golden = []\n",
    "for fpn in range(len(xmlfiles)):\n",
    "    ab,bod,err = parse_xml_abstract(xmlfiles[fpn])\n",
    "    if err:\n",
    "        #print(fp, err)\n",
    "        continue\n",
    "    f = open(citations[fpn]) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    only_text = []\n",
    "    for entry in data:\n",
    "        only_text.append(entry['clean_text'])\n",
    "#     print(only_text)\n",
    "    \n",
    "    f2 = open(golden_summaries[fpn],\"r+\") \n",
    "    golden = f2.read()\n",
    "\n",
    "    \n",
    "    raw_cols.append([ab,bod,only_text, golden])\n",
    "\n",
    "df = pd.DataFrame(raw_cols, columns=[\"abstract\",\"body\",\"citations\", \"golden\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Cited text spans identification\n",
    "\n",
    "Finding the top x sentences in the body that have the largest similarity (as per ROUGE score) with the citations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1 - Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT BEING USED\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkQuality(sentence):\n",
    "    \n",
    "    '''Check the quality of body sentences, to classify each of them as eligible (or not) to be a cited text span'''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    length = len(tokenized_text)\n",
    "    alpha = 0\n",
    "    nonAlpha = 0\n",
    "    found = 0\n",
    "    nonFound = 0\n",
    "    upper = 0\n",
    "    stop = 0\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    for token in tokenized_text:\n",
    "        #print(token)\n",
    "        if len(token)==1:\n",
    "            if token.isalpha():\n",
    "                # count number of single alpha chars\n",
    "                alpha+=1\n",
    "            else:\n",
    "                # count number of not single non-alpha chars\n",
    "                nonAlpha+=1\n",
    "        else:\n",
    "            if(token.lower() in stop_words):\n",
    "                # count number of stop words\n",
    "                stop+=1\n",
    "            else:\n",
    "                lemma = lemmatizer.lemmatize(token.lower())\n",
    "                # count number of Synsets for alpha tokens\n",
    "                if token.isalpha():\n",
    "                    if (len(wn.synsets(lemma))>0):\n",
    "                        found+=1 # alpha tokens that have Synsets\n",
    "                    else:\n",
    "                        nonFound+=1 # alpha tokens that dont have Synsets\n",
    "                else:\n",
    "                    nonAlpha+=1 #non alpha tokens\n",
    "    \n",
    "    # calculate shares \n",
    "    alphaS = alpha/length\n",
    "    nonAlphaS = nonAlpha/length\n",
    "    foundS = found/length\n",
    "    nonFoundS = nonFound/length\n",
    "#     upperS = upper/length\n",
    "    stopS = stop/length\n",
    "    \n",
    "    good_quality = True\n",
    "    \n",
    "    if(nonFoundS>0.2):\n",
    "        good_quality = False\n",
    "    if(foundS<0.1):\n",
    "        good_quality = False\n",
    "    if(alphaS>0.2):\n",
    "        good_quality = False\n",
    "    if(nonAlphaS>0.5):\n",
    "        good_quality = False\n",
    "    if len(tokenized_text)< 6:\n",
    "        good_quality = False\n",
    "    \n",
    "    if (\"equation\" in sentence.lower()) | (\"section\" in sentence.lower()) | (\"table\" in sentence.lower()) | (\"figure\" in sentence.lower()) | (\"=\" in sentence) | (\">\" in sentence) | (\"<\" in sentence) | (\"p(\" in sentence.lower()):\n",
    "        good_quality = False\n",
    "    \n",
    "    # remove sentences that has dates\n",
    "    match = re.match(r'.*([1-3][0-9]{3})', sentence)\n",
    "    if match is not None:\n",
    "        good_quality = False \n",
    "        \n",
    "    return good_quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cited_text_spans_ids(df,n_sent = 3):\n",
    "\n",
    "    '''Find (n_sent) body sentences most similar to citation sentences \n",
    "    inputs: \n",
    "        df: dataframe with simularity scores between each body sentence and citation \n",
    "        n_set: number of body sentences to output\n",
    "    output:\n",
    "        list of best ranked body sentetences ids\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    # empty list where we'll store best scored body sentences \n",
    "    body_sent_all_ids = []\n",
    "    body_sent_id = []\n",
    "     \n",
    "    # create an empty dataframe (num rows = num sentences in citations;  num col = n_sent)\n",
    "    # we'll store here the body sentences ids with largest similarity\n",
    "    sim_df_max = pd.DataFrame(0.0, index=[j for j in range(len(citations_sentences))], columns= [j for j in range(n_sent)])\n",
    "    \n",
    "    for ns in range(n_sent):\n",
    "        # get the indexes that maximize similarity for each column (i.e., for each citation sentence)    \n",
    "        sim_df_max[ns] = sim_df.idxmax(axis=0, skipna=True)\n",
    "        # reset the largest values to zero, so we can retreive other top values in the loop\n",
    "        sim_df[ns][sim_df_max[ns]] = 0\n",
    "\n",
    "    \n",
    "    # loop over the number of body sentences we want to retrieve\n",
    "    for ns in range(n_sent): \n",
    "        # turn all columns into a list of best scored body sentences ids  \n",
    "        li = sim_df_max[ns].tolist() \n",
    "        body_sent_all_ids = body_sent_all_ids + li\n",
    "    \n",
    "#     print(body_sent_all_ids)\n",
    "    for ns in range(n_sent):   \n",
    "        # append best scored sentence id over all citations sentences\n",
    "        best_sent_id_single = int(s.mode(body_sent_all_ids)[0])\n",
    "        body_sent_id.append(best_sent_id_single)\n",
    "        # reset the largest values to zero, so we can retreive other top values in the loop                    \n",
    "        body_sent_all_ids = list(filter(lambda a: a != best_sent_id_single, body_sent_all_ids)) \n",
    "#         print(body_sent_all_ids)\n",
    "#         if len(body_sent_all_ids) == 0:\n",
    "#             break\n",
    "\n",
    "    return body_sent_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2 - Select 'x' body sentences to \"represent\" citations as model inputs - i.e., find cited text spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an empty column in the original dataframe, in which we'll store the cited text spans for each paper \n",
    "df['cited_text_spans'] = \"\"\n",
    "\n",
    "## instantiate nlp tools to read the data\n",
    "# vectorizer = Vectorizer() - NOT BEING USED - USED FOR COSINE SIMILARITY\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "## instantiate rouge score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# start time to keep track of the timing\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for paper_id in df.index: # [i for i in df.index if i >79]:\n",
    "    \n",
    "    body_sentences = []\n",
    "    \n",
    "    # split body into sentences\n",
    "    body_sentences_tok = df.body[paper_id].replace(\"et al.\", \"et al\").replace(\"e.g.\", \"eg\").replace(\"eg.\", \"eg\").replace(\"i.e.\", \"ie\").replace(\"ie.\", \"ie\")\n",
    "    body_sentences_tok = body_sentences_tok.replace(\".1\", \". \").replace(\".2\", \". \").replace(\".3\", \".\").replace(\".4\", \". \").replace(\".5\", \". \").replace(\".6\", \". \").replace(\".7\", \". \").replace(\".8\", \". \").replace(\".9\", \". \")\n",
    "\n",
    "    # tokenize body sentences\n",
    "    body_sentences_tok = tokenizer.tokenize(body_sentences_tok)\n",
    "        \n",
    "    # truncate too large body sentences (larger than 512 throws an error - rare cases) and remove \"?\"\n",
    "    body_sentences_tok = [s.replace(\"?\", \"\") for s in body_sentences_tok if len(s) <= 512]\n",
    "    \n",
    "    # remove papers that are too small even before quality check\n",
    "    if len(body_sentences_tok) < 10:\n",
    "        continue\n",
    "        \n",
    "    # get citations sentences (discarding empty entries)\n",
    "    citations_sentences = list(filter(None, df.citations[paper_id]))\n",
    "    \n",
    "    # clean citations\n",
    "    for c in range(len(citations_sentences)):\n",
    "        citations_sentences[c] = re.sub(r\"\\s\\([A-Z][a-z]+,\\s[A-Z][a-z]?\\.[^\\)]*,\\s\\d{4}\\)\", \"\", citations_sentences[c])\n",
    "    \n",
    "#     print(paper_id, \"size after quality check:\", len(body_sentences_tok))\n",
    "    \n",
    "    # truncate the center of the body for large bodies\n",
    "    if len(body_sentences_tok) > 300:\n",
    "        body_sentences_tok = body_sentences_tok[:150] + body_sentences_tok[-150:]\n",
    "#         print(\"truncated\", paper_id, \"new size:\", len(body_sentences_tok))\n",
    "        \n",
    "    # find body sentences with quality\n",
    "    for i in range(len(body_sentences_tok)):\n",
    "        if len(body_sentences_tok[i])>0:\n",
    "            if checkQuality(body_sentences_tok[i]):\n",
    "                body_sentences.append(body_sentences_tok[i])\n",
    "                \n",
    "#     print(\"step 1 done\")\n",
    "\n",
    "    # create an empty dataframe (num rows = num sentences in body;  num col = num sentences in citations)\n",
    "    sim_df = pd.DataFrame(0.0, index=[j for j in range(len(body_sentences))], columns=[j for j in range(len(citations_sentences))])\n",
    "\n",
    "# #     # vectorize body sentences\n",
    "# #     vectorizer.bert(body_sentences)\n",
    "# #     vectors_bert_body = vectorizer.vectors\n",
    "\n",
    "# #     # vectorize citations sentences\n",
    "# #     vectorizer.bert(citations_sentences)\n",
    "# #     vectors_bert_citations = vectorizer.vectors\n",
    "\n",
    "#     print(\"step 2 done\")\n",
    "\n",
    "    # nested loop: over body sentences and citations sentences\n",
    "    for body_sentence_id in range(len(body_sentences)):\n",
    "        for citation_sentence_id in range(len(citations_sentences)):\n",
    "            # fill empty dataframe with sim measure\n",
    "#             sim_df[citation_sentence_id][body_sentence_id] = spatial.distance.cosine(vectors_bert_body[body_sentence_id], vectors_bert_citations[citation_sentence_id])\n",
    "            scores = scorer.score(body_sentences[body_sentence_id],citations_sentences[citation_sentence_id] )\n",
    "            sim_df[citation_sentence_id][body_sentence_id] = 100*scores['rouge2'][2]\n",
    "    \n",
    "        \n",
    "#     print(\"step 3 done\")\n",
    "    # get selected body sentences ids - if rouge 2 doesnt work (usually because it yields too many socre =0), \n",
    "    # retry with rouge 1\n",
    "    try:\n",
    "        sent_ids = cited_text_spans_ids(sim_df,3)\n",
    "        \n",
    "    except: \n",
    "        for body_sentence_id in range(len(body_sentences)):\n",
    "            for citation_sentence_id in range(len(citations_sentences)):\n",
    "            # fill empty dataframe with sim measure\n",
    "#             sim_df[citation_sentence_id][body_sentence_id] = spatial.distance.cosine(vectors_bert_body[body_sentence_id], vectors_bert_citations[citation_sentence_id])\n",
    "                scores = scorer.score(body_sentences[body_sentence_id],citations_sentences[citation_sentence_id] )\n",
    "                sim_df[citation_sentence_id][body_sentence_id] = 100*scores['rouge1'][2]\n",
    "#             print(body_sentence_id,citation_sentence_id,100*scores['rouge1'][2]  )\n",
    "        sent_ids = cited_text_spans_ids(sim_df,3)\n",
    "  \n",
    "    \n",
    "#     print(\"step 4 done\")\n",
    "    # fill our large/original dataframe with the best scored body sentences \n",
    "    df.cited_text_spans[paper_id] = [body_sentences[b] for b in sent_ids]\n",
    "    \n",
    "    # keep track of time to run \n",
    "    if paper_id % 250 == 0:\n",
    "        print(paper_id, \"secs to run:\", time.time() - start)\n",
    "        start = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle(\"df.pkl\")\n",
    "df = pd.read_pickle(\"df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3 - Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The work was motivated by the needs of the ILEX system for generating descriptions of museum artefacts (in particular, 20th Century jewellery) [Mellish et al 98].', 'This paper presents some initial experiments using stochastic search methods for aspects of text planning.', 'In this task, one is given a set of facts all of which should be included in a text and a set of relations between facts, some of which can be included in the text.']\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mellish et al (1998) investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations.',\n",
       " 'Mellish et al (1998) (and subsequently Karamanis and Manurung 2002) advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts.',\n",
       " 'Following previous work (Mellish et al, 1998) we used a single fitness function that scored candidates based on their coherence.',\n",
       " 'The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose.',\n",
       " 'For example, the measure from (Mellish et al, 1998) looks at the entire discourse up to the current transition for some of their cost factors.',\n",
       " 'Mellish et al (1998) advocate stochastic search as an alternative to exhaustively examining the search space.',\n",
       " 'As in the case of Mellish et al (1998) we construct an acceptable ordering rather than the best possible one.',\n",
       " 'Mellish et al (1998) made the point that even this restricted approach would soon become intractable with more than a small set of facts when one allows weak RST relations such as Joint and Elaboration into the model.',\n",
       " 'In the late 1990s, Chris Mellish implemented the first stochastic text planner (Mellish et al 1998).',\n",
       " 'The evaluation function of Mellish et al (1998) also was calculated over a sum of local features of the tree, although a wider set of features were involved.',\n",
       " 'For instance, the evaluation function of Mellish et al (1998) assigned +3 for each instance of subject-repetition.',\n",
       " 'Genetic algorithms are also used in [Mellish et al, 1998] where the authors state the problem of given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how one can arrange this material so as to yield the best possible text.',\n",
       " 'Mellish et al (1998) advocate stochastic search methods for document structuring.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check quality of cited text spans\n",
    "# pick any number up to the total number of papers  \n",
    "paper_id = 80 \n",
    "\n",
    "print(df.cited_text_spans[paper_id])\n",
    "print('------')\n",
    "list(filter(None, df.citations[paper_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data size: 1009\n",
      "Number of papers with no cited text spans: 67\n",
      "Share of papers with no cited text spans (%): 6.640237859266601\n"
     ]
    }
   ],
   "source": [
    "# check the number of papers with no cited text spans\n",
    "# -> this can occur because some papers are too small, or have few sentences considered of high quality, \n",
    "#   and were discarded\n",
    "\n",
    "# pick any number up to the total number of papers  \n",
    "size_all_data = df.shape[0]\n",
    "size_no_data = df[df.cited_text_spans == \"\"].shape[0]\n",
    "print(\"Full data size:\", size_all_data)\n",
    "print(\"Number of papers with no cited text spans:\", size_no_data)\n",
    "print(\"Share of papers with no cited text spans (%):\", 100*size_no_data/size_all_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Prepare data for train and inference \n",
    "\n",
    "In this step we will do the following:\n",
    " \n",
    "Step 2.1 - Define model inputs and 'labels' \\\n",
    "Step 2.2 - Clean the data (important specially if we are summarizing the body) \\\n",
    "Step 2.3 - Divide the data into train, validation and test \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 - Define model inputs and 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract', 'body', 'citations', 'golden', 'cited_text_spans', 'model_input', 'model_output']\n",
      "---------\n",
      "We present a method for extracting parts of objects from wholes (e.g. \"speedometer\" from \"car\"). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon.We present a method of extracting parts of objects from wholes (eg\n",
      "\"speedometer\" from \"car\"). Our first goal is to find lexical patterns that tend to indicate part-whole relations. In this paper we use the more colloquial \"part-of\" terminology.\n"
     ]
    }
   ],
   "source": [
    "# create a new df named data in case you mess it up\n",
    "data = df.copy()\n",
    "data['model_input'] = ''\n",
    "data['model_output'] = ''\n",
    "\n",
    "# inputs - body only? abstract + cts?\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data['model_input'][i] = data['abstract'][i].replace(\"\\n\", \" \") + \" \".join(data['cited_text_spans'][i])\n",
    "    \n",
    "# output  \n",
    "data['model_output'] = data['golden'] \n",
    "\n",
    "# sanity checks\n",
    "print(list(data))\n",
    "print(\"---------\")\n",
    "print(data['model_input'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 - Clean the data\n",
    "Using 'checkQuality' function to select only those input sentences with high quality. \\\n",
    "See other idea for data cleansing: https://www.kaggle.com/sandeepbhogaraju/text-summarization-with-seq2seq-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA:\n",
    "# we can use the checkQuality function to select only those input sentences with high quality\n",
    "\n",
    "def cleanInput(df, truncate_center = 300000):\n",
    "    \n",
    "    ''' Clean input so that low quality sentences don't feed the model\n",
    "        \n",
    "        Inputs: \n",
    "            \n",
    "            df: model data. It has to have a column named 'model_input' \n",
    "        \n",
    "            truncate_center: If the number of sentences in model input is larger than 'truncate_center', \n",
    "                truncate the center of model input - i.e., get only the \n",
    "                first and last x (= truncate_center/2) sentences.\n",
    "        \n",
    "        Output: df with 'cleaned' 'model_input' column \n",
    "        \n",
    "     '''\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    for paper_id in df.index:\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        input_sentences = []\n",
    "\n",
    "        # split input into sentences\n",
    "        input_sentences_tok = df.model_input[paper_id].replace(\"et al.\", \"et al\").replace(\"e.g.\", \"eg\").replace(\"eg.\", \"eg\").replace(\"i.e.\", \"ie\").replace(\"ie.\", \"ie\")\n",
    "        input_sentences_tok = input_sentences_tok.replace(\".1\", \". \").replace(\".2\", \". \").replace(\".3\", \".\").replace(\".4\", \". \").replace(\".5\", \". \").replace(\".6\", \". \").replace(\".7\", \". \").replace(\".8\", \". \").replace(\".9\", \". \")\n",
    "\n",
    "        # tokenize input sentences\n",
    "        input_sentences_tok = tokenizer.tokenize(input_sentences_tok)\n",
    "\n",
    "        # truncate too large input sentences (larger than 512 throws an error - rare cases) and remove \"?\"\n",
    "        input_sentences_tok = [s.replace(\"?\", \"\") for s in input_sentences_tok if len(s) <= 512]\n",
    "        \n",
    "        \n",
    "        # truncate the center of the body for large bodies\n",
    "        if len(input_sentences_tok) > truncate_center:\n",
    "            input_sentences_tok = input_sentences_tok[:int(truncate_center/2)] + input_sentences_tok[-int(truncate_center/2):]\n",
    "\n",
    "        # find input sentences with quality\n",
    "        for i in range(len(input_sentences_tok)):\n",
    "            if len(input_sentences_tok[i])>0:\n",
    "                if checkQuality(input_sentences_tok[i]):\n",
    "                    input_sentences.append(input_sentences_tok[i])\n",
    "\n",
    "        # fill our large/original dataframe with the cleaned input sentences \n",
    "        df.model_input[paper_id] = \" \".join(input_sentences)\n",
    "        \n",
    "        # keep track of time to run \n",
    "        if paper_id % 50 == 0:\n",
    "#             print(paper_id, \"secs to run:\", time.time() - start)\n",
    "            start = time.time()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>golden</th>\n",
       "      <th>cited_text_spans</th>\n",
       "      <th>model_input</th>\n",
       "      <th>model_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present a method for extracting parts of ob...</td>\n",
       "      <td>We present a method of extracting parts of obj...</td>\n",
       "      <td>[Berland and Charniak (1999) use Hearst style ...</td>\n",
       "      <td>Finding Parts In Very Large Corpora\\nWe presen...</td>\n",
       "      <td>[We present a method of extracting parts of ob...</td>\n",
       "      <td>We present a method for extracting parts of ob...</td>\n",
       "      <td>Finding Parts In Very Large Corpora\\nWe presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>[The program takes the output of char_align (C...</td>\n",
       "      <td>The Mathematics Of Statistical Machine Transla...</td>\n",
       "      <td>[Like Brown et al, they consider only the simu...</td>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>The Mathematics Of Statistical Machine Transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Previous work has shown that Chinese word segm...</td>\n",
       "      <td>Word segmentation is considered an important f...</td>\n",
       "      <td>[Chinese word segmentation is done by the Stan...</td>\n",
       "      <td>Optimizing Chinese Word Segmentation for Machi...</td>\n",
       "      <td>[In order to contrast with the simple maximum ...</td>\n",
       "      <td>Previous work has shown that Chinese word segm...</td>\n",
       "      <td>Optimizing Chinese Word Segmentation for Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We examine the viability of building large pol...</td>\n",
       "      <td>Polarity lexicons are large lists of phrases t...</td>\n",
       "      <td>[Recent work in this area includes Velikovich ...</td>\n",
       "      <td>The viability of web-derived polarity lexicons...</td>\n",
       "      <td>[Thus, even though the web-derived lexicon is ...</td>\n",
       "      <td>We examine the viability of building large pol...</td>\n",
       "      <td>The viability of web-derived polarity lexicons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extracting semantic relationships between enti...</td>\n",
       "      <td>Extraction of semantic relationships between e...</td>\n",
       "      <td>[They use two kinds of features: syntactic one...</td>\n",
       "      <td>Combining Lexical Syntactic And Semantic Featu...</td>\n",
       "      <td>[We build Maximum Entropy models for extractin...</td>\n",
       "      <td>Extracting semantic relationships between enti...</td>\n",
       "      <td>Combining Lexical Syntactic And Semantic Featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>[In addition, Niessen and Ney (2004) decompose...</td>\n",
       "      <td>Statistical Machine Translation With Scarce Re...</td>\n",
       "      <td>[In statistical machine translation, correspon...</td>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>Statistical Machine Translation With Scarce Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>We have developed a new program called alignin...</td>\n",
       "      <td>Aligning parallel texts has recently received ...</td>\n",
       "      <td>[There have been quite a number of recent pape...</td>\n",
       "      <td>Robust Bilingual Word Alignment For Machine Ai...</td>\n",
       "      <td>[Brown et al estimate t(fle) on the basis of a...</td>\n",
       "      <td>We have developed a new program called alignin...</td>\n",
       "      <td>Robust Bilingual Word Alignment For Machine Ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>We present an approach to pronoun resolution b...</td>\n",
       "      <td>Pronoun resolution is a difficult but vital pa...</td>\n",
       "      <td>[, We follow the closed track setting where sy...</td>\n",
       "      <td>Bootstrapping Path-Based Pronoun Resolution\\nW...</td>\n",
       "      <td>[The noun-pronoun path coreference can be used...</td>\n",
       "      <td>We present an approach to pronoun resolution b...</td>\n",
       "      <td>Bootstrapping Path-Based Pronoun Resolution\\nW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>We use logical inference techniques for recogn...</td>\n",
       "      <td>Recognising textual entailment (RTE) is the ta...</td>\n",
       "      <td>[However, this method does not work for realwo...</td>\n",
       "      <td>Recognising Textual Entailment With Logical In...</td>\n",
       "      <td>[Another attractive property of a model builde...</td>\n",
       "      <td>We use logical inference techniques for recogn...</td>\n",
       "      <td>Recognising Textual Entailment With Logical In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>This paper deals with two important ambiguitie...</td>\n",
       "      <td>The problem with successful resolution of ambi...</td>\n",
       "      <td>[The state of the art is a supervised algorith...</td>\n",
       "      <td>Corpus Based PP Attachment Ambiguity Resolutio...</td>\n",
       "      <td>[The current statistical state-of-the art meth...</td>\n",
       "      <td>This paper deals with two important ambiguitie...</td>\n",
       "      <td>Corpus Based PP Attachment Ambiguity Resolutio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1009 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     We present a method for extracting parts of ob...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Previous work has shown that Chinese word segm...   \n",
       "3     We examine the viability of building large pol...   \n",
       "4     Extracting semantic relationships between enti...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  We have developed a new program called alignin...   \n",
       "1006  We present an approach to pronoun resolution b...   \n",
       "1007  We use logical inference techniques for recogn...   \n",
       "1008  This paper deals with two important ambiguitie...   \n",
       "\n",
       "                                                   body  \\\n",
       "0     We present a method of extracting parts of obj...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Word segmentation is considered an important f...   \n",
       "3     Polarity lexicons are large lists of phrases t...   \n",
       "4     Extraction of semantic relationships between e...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  Aligning parallel texts has recently received ...   \n",
       "1006  Pronoun resolution is a difficult but vital pa...   \n",
       "1007  Recognising textual entailment (RTE) is the ta...   \n",
       "1008  The problem with successful resolution of ambi...   \n",
       "\n",
       "                                              citations  \\\n",
       "0     [Berland and Charniak (1999) use Hearst style ...   \n",
       "1     [The program takes the output of char_align (C...   \n",
       "2     [Chinese word segmentation is done by the Stan...   \n",
       "3     [Recent work in this area includes Velikovich ...   \n",
       "4     [They use two kinds of features: syntactic one...   \n",
       "...                                                 ...   \n",
       "1004  [In addition, Niessen and Ney (2004) decompose...   \n",
       "1005  [There have been quite a number of recent pape...   \n",
       "1006  [, We follow the closed track setting where sy...   \n",
       "1007  [However, this method does not work for realwo...   \n",
       "1008  [The state of the art is a supervised algorith...   \n",
       "\n",
       "                                                 golden  \\\n",
       "0     Finding Parts In Very Large Corpora\\nWe presen...   \n",
       "1     The Mathematics Of Statistical Machine Transla...   \n",
       "2     Optimizing Chinese Word Segmentation for Machi...   \n",
       "3     The viability of web-derived polarity lexicons...   \n",
       "4     Combining Lexical Syntactic And Semantic Featu...   \n",
       "...                                                 ...   \n",
       "1004  Statistical Machine Translation With Scarce Re...   \n",
       "1005  Robust Bilingual Word Alignment For Machine Ai...   \n",
       "1006  Bootstrapping Path-Based Pronoun Resolution\\nW...   \n",
       "1007  Recognising Textual Entailment With Logical In...   \n",
       "1008  Corpus Based PP Attachment Ambiguity Resolutio...   \n",
       "\n",
       "                                       cited_text_spans  \\\n",
       "0     [We present a method of extracting parts of ob...   \n",
       "1     [Like Brown et al, they consider only the simu...   \n",
       "2     [In order to contrast with the simple maximum ...   \n",
       "3     [Thus, even though the web-derived lexicon is ...   \n",
       "4     [We build Maximum Entropy models for extractin...   \n",
       "...                                                 ...   \n",
       "1004  [In statistical machine translation, correspon...   \n",
       "1005  [Brown et al estimate t(fle) on the basis of a...   \n",
       "1006  [The noun-pronoun path coreference can be used...   \n",
       "1007  [Another attractive property of a model builde...   \n",
       "1008  [The current statistical state-of-the art meth...   \n",
       "\n",
       "                                            model_input  \\\n",
       "0     We present a method for extracting parts of ob...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Previous work has shown that Chinese word segm...   \n",
       "3     We examine the viability of building large pol...   \n",
       "4     Extracting semantic relationships between enti...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  We have developed a new program called alignin...   \n",
       "1006  We present an approach to pronoun resolution b...   \n",
       "1007  We use logical inference techniques for recogn...   \n",
       "1008  This paper deals with two important ambiguitie...   \n",
       "\n",
       "                                           model_output  \n",
       "0     Finding Parts In Very Large Corpora\\nWe presen...  \n",
       "1     The Mathematics Of Statistical Machine Transla...  \n",
       "2     Optimizing Chinese Word Segmentation for Machi...  \n",
       "3     The viability of web-derived polarity lexicons...  \n",
       "4     Combining Lexical Syntactic And Semantic Featu...  \n",
       "...                                                 ...  \n",
       "1004  Statistical Machine Translation With Scarce Re...  \n",
       "1005  Robust Bilingual Word Alignment For Machine Ai...  \n",
       "1006  Bootstrapping Path-Based Pronoun Resolution\\nW...  \n",
       "1007  Recognising Textual Entailment With Logical In...  \n",
       "1008  Corpus Based PP Attachment Ambiguity Resolutio...  \n",
       "\n",
       "[1009 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cleanInput(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3 - Divide the data into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "\n",
    "train_pct = 0.9\n",
    "test_pct = 0.05\n",
    "\n",
    "data = data.sample(len(data), random_state=20)\n",
    "train_sub = int(len(data) * train_pct)\n",
    "test_sub = int(len(data) * test_pct) + train_sub\n",
    "\n",
    "train_df = data[0:train_sub]\n",
    "test_df = data[train_sub:test_sub]\n",
    "val_df = data[test_sub:]\n",
    "\n",
    "train_input = list(train_df['model_input'])\n",
    "test_input = list(test_df['model_input'])\n",
    "val_input = list(val_df['model_input'])\n",
    "\n",
    "train_output = list(train_df['model_output'])\n",
    "test_output = list(test_df['model_output'])\n",
    "val_output = list(val_df['model_output'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>golden</th>\n",
       "      <th>cited_text_spans</th>\n",
       "      <th>model_input</th>\n",
       "      <th>model_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>Conditional random fields (Lafferty et al., 20...</td>\n",
       "      <td>Finding linguistic structure in raw text is no...</td>\n",
       "      <td>[Recent work by Smith and Eisner (2005) on con...</td>\n",
       "      <td>Contrastive Estimation: Training Log-Linear Mo...</td>\n",
       "      <td>[2 as contrastive estimation (CE)., Finding li...</td>\n",
       "      <td>CRFs allowing the incorporation of arbifeature...</td>\n",
       "      <td>Contrastive Estimation: Training Log-Linear Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Recognizing analogies, synonyms, anto nyms, an...</td>\n",
       "      <td>A pair of words (petrify:stone) is analogous t...</td>\n",
       "      <td>[Language modeling (Chen and Goodman, 1996), n...</td>\n",
       "      <td>A Uniform Approach to Analogies Synonyms Anton...</td>\n",
       "      <td>[A pair of words (petrify:stone) is analogous ...</td>\n",
       "      <td>Recognizing analogies, synonyms, anto nyms, an...</td>\n",
       "      <td>A Uniform Approach to Analogies Synonyms Anton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>We seek a knowledge-free method for inducing m...</td>\n",
       "      <td>A multiword unit (MWU) is a connected collocat...</td>\n",
       "      <td>[Schone and Jurafsky (2001) applied LSA to the...</td>\n",
       "      <td>Is Knowledge-Free Induction Of Multiword Unit ...</td>\n",
       "      <td>[In other words, MWUs are typically non-compos...</td>\n",
       "      <td>We seek a knowledge-free method for inducing m...</td>\n",
       "      <td>Is Knowledge-Free Induction Of Multiword Unit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>hanb@student.unimelb.edu.au tb@ldwin.net Abstr...</td>\n",
       "      <td>Twitter and other micro-blogging services are ...</td>\n",
       "      <td>[We use unsupervised methods to build a pipeli...</td>\n",
       "      <td>Lexical Normalisation of Short Text Messages: ...</td>\n",
       "      <td>[Additionally, we evaluate using the BLEU scor...</td>\n",
       "      <td>hanb@student.unimelb.edu.au tb@ldwin.net Abstr...</td>\n",
       "      <td>Lexical Normalisation of Short Text Messages: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>In this paper we present a novel, customizable...</td>\n",
       "      <td>The goal of recent Information Extraction (IE)...</td>\n",
       "      <td>[, Indeed, the analysis produced by existing s...</td>\n",
       "      <td>Using Predicate-Argument Structures For Inform...</td>\n",
       "      <td>[Regardless of the syntactic frame or verb sen...</td>\n",
       "      <td>In this paper we present a novel, customizable...</td>\n",
       "      <td>Using Predicate-Argument Structures For Inform...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              abstract  \\\n",
       "457  Conditional random fields (Lafferty et al., 20...   \n",
       "798  Recognizing analogies, synonyms, anto nyms, an...   \n",
       "391  We seek a knowledge-free method for inducing m...   \n",
       "376  hanb@student.unimelb.edu.au tb@ldwin.net Abstr...   \n",
       "387  In this paper we present a novel, customizable...   \n",
       "\n",
       "                                                  body  \\\n",
       "457  Finding linguistic structure in raw text is no...   \n",
       "798  A pair of words (petrify:stone) is analogous t...   \n",
       "391  A multiword unit (MWU) is a connected collocat...   \n",
       "376  Twitter and other micro-blogging services are ...   \n",
       "387  The goal of recent Information Extraction (IE)...   \n",
       "\n",
       "                                             citations  \\\n",
       "457  [Recent work by Smith and Eisner (2005) on con...   \n",
       "798  [Language modeling (Chen and Goodman, 1996), n...   \n",
       "391  [Schone and Jurafsky (2001) applied LSA to the...   \n",
       "376  [We use unsupervised methods to build a pipeli...   \n",
       "387  [, Indeed, the analysis produced by existing s...   \n",
       "\n",
       "                                                golden  \\\n",
       "457  Contrastive Estimation: Training Log-Linear Mo...   \n",
       "798  A Uniform Approach to Analogies Synonyms Anton...   \n",
       "391  Is Knowledge-Free Induction Of Multiword Unit ...   \n",
       "376  Lexical Normalisation of Short Text Messages: ...   \n",
       "387  Using Predicate-Argument Structures For Inform...   \n",
       "\n",
       "                                      cited_text_spans  \\\n",
       "457  [2 as contrastive estimation (CE)., Finding li...   \n",
       "798  [A pair of words (petrify:stone) is analogous ...   \n",
       "391  [In other words, MWUs are typically non-compos...   \n",
       "376  [Additionally, we evaluate using the BLEU scor...   \n",
       "387  [Regardless of the syntactic frame or verb sen...   \n",
       "\n",
       "                                           model_input  \\\n",
       "457  CRFs allowing the incorporation of arbifeature...   \n",
       "798  Recognizing analogies, synonyms, anto nyms, an...   \n",
       "391  We seek a knowledge-free method for inducing m...   \n",
       "376  hanb@student.unimelb.edu.au tb@ldwin.net Abstr...   \n",
       "387  In this paper we present a novel, customizable...   \n",
       "\n",
       "                                          model_output  \n",
       "457  Contrastive Estimation: Training Log-Linear Mo...  \n",
       "798  A Uniform Approach to Analogies Synonyms Anton...  \n",
       "391  Is Knowledge-Free Induction Of Multiword Unit ...  \n",
       "376  Lexical Normalisation of Short Text Messages: ...  \n",
       "387  Using Predicate-Argument Structures For Inform...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Summarize with Pre-Trained Pegasus (no fine-tuning)\n",
    "    \n",
    "In this step we will do the following:\n",
    " \n",
    "Step 3.1 - Instantiate the model\\\n",
    "Step 3.2 - Instatiate the metric of interest \\\n",
    "Step 3.3 - Compute the summaries\\\n",
    "Step 3.4 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 - Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "model_name = 'google/pegasus-large'\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "# This is the PEGASUS Model with a language modeling head. Can be used for summarization. \n",
    "# This model inherits from PreTrainedModel. \n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # OTHER OPTIONS BELOW"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OTHER OPTIONS: Possible pre-trained models\n",
    "![image.png](attachment:image.png)\n",
    "Source: https://pub.towardsai.net/summarization-using-pegasus-model-with-the-transformers-library-553cd0dc5c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 - Instatiate the metric of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate rouge score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3 - Compute the summaries and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32.089011907577515\n",
      "5 154.72424602508545\n",
      "10 247.23304510116577\n",
      "15 181.95827889442444\n",
      "20 282.8203761577606\n",
      "25 221.4812240600586\n"
     ]
    }
   ],
   "source": [
    "#INFERENCE\n",
    "\n",
    "## Sample with no fine-tunning \n",
    "\n",
    "import time\n",
    "\n",
    "# create empry lists to store rouge scores\n",
    "r1_precision = []\n",
    "r2_precision = []\n",
    "rL_precision = []\n",
    "\n",
    "r1_recall = []\n",
    "r2_recall = []\n",
    "rL_recall = []\n",
    "\n",
    "r1_fmeasure = []\n",
    "r2_fmeasure = []\n",
    "rL_fmeasure = []\n",
    "\n",
    "val_model_output = []\n",
    "\n",
    "# start counting seconds to keep track of time \n",
    "start = time.time()\n",
    "\n",
    "# loop over validation set\n",
    "for sample_id in range(len(val_input)):\n",
    "    \n",
    "    # get input (body OR abstract + cited text spans) - scisummnet uses abstract, we want to use body\n",
    "    sample_input = val_input[sample_id]\n",
    "    \n",
    "    # tokenize it\n",
    "    inputs = tokenizer([sample_input], max_length=1024, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # 'max_length': Pad to a maximum length specified with the argument max_length \n",
    "    # or to the maximum acceptable input length for the model if that argument is not provided.\n",
    "\n",
    "    # generate Summary\n",
    "    summary_ids = model.generate(inputs['input_ids'])\n",
    "    \n",
    "    # decode summary\n",
    "    sample_output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    \n",
    "    # store summary\n",
    "    val_model_output = val_model_output + sample_output\n",
    "    \n",
    "    # get reference (gold) summary \n",
    "    sample_reference = val_output[sample_id]\n",
    "    \n",
    "    #calculate rouge score\n",
    "    scores = scorer.score(str(sample_reference), str(sample_output))\n",
    "    \n",
    "    r1_precision.append(scores['rouge1'][0])\n",
    "    r1_recall.append(scores['rouge1'][1])\n",
    "    r1_fmeasure.append(scores['rouge1'][2])\n",
    "    \n",
    "    r2_precision.append(scores['rouge2'][0])\n",
    "    r2_recall.append(scores['rouge2'][1])\n",
    "    r2_fmeasure.append(scores['rouge2'][2])\n",
    "    \n",
    "    rL_precision.append(scores['rougeL'][0])\n",
    "    rL_recall.append(scores['rougeL'][1])\n",
    "    rL_fmeasure.append(scores['rougeL'][2])\n",
    "    \n",
    "    if sample_id % 5 == 0:\n",
    "        print(sample_id, time.time() - start)\n",
    "        start = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4 - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute score statistics\n",
    "\n",
    "all_socores = {'R1': [stats.mean(r1_precision), stats.mean(r1_recall), stats.mean(r1_fmeasure)],\n",
    "        'R2': [stats.mean(r2_precision), stats.mean(r2_recall), stats.mean(r2_fmeasure)],\n",
    "        'RL': [stats.mean(rL_precision), stats.mean(rL_recall), stats.mean(rL_fmeasure)]      \n",
    "        }\n",
    "\n",
    "all_socores_df = pd.DataFrame(all_socores, columns = ['R1', 'R2', 'RL'], index=['precision','recall','fmeasure'])\n",
    "\n",
    "print(all_socores_df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check quality of cited text spans\n",
    "# pick any number up to the total number of papers in the validation set\n",
    "paper_id = 30 \n",
    "\n",
    "print(val_input[paper_id])\n",
    "print('------')\n",
    "list(val_model_output[paper_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Fine-Tune Pre-Trained Pegasus\n",
    "\n",
    "In this step we will do the following:\n",
    " \n",
    "Step 4.1 - Instantiate the model (the same as in step 3, so I'll skip it) \\\n",
    "Step 4.2 - Define metrics of interest \\\n",
    "Step 4.3 - Tokenize text data and wrap it into a torch Dataset \\\n",
    "Step 4.4 - Train and evaluate the model \\\n",
    "Step 4.5 - Save the model \\\n",
    "Step 4.6 - Instantiate the fine-tuned model \\\n",
    "Step 4.7 - Compute the summaries and evaluate \\\n",
    "\n",
    "Helpful resources: \\\n",
    "https://towardsdatascience.com/how-to-perform-abstractive-summarization-with-pegasus-3dd74e48bafb \\\n",
    "https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_summarization.py \\\n",
    "https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1 - Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same as in step 3, so I'll skip it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2 - Define metrics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Ignore pad token for loss: Replace -100 in the labels as we can't decode them.\n",
    "    #     if data_args.ignore_pad_token_for_loss:\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3 - Tokenize text data and wrap it into a torch Dataset\n",
    "\n",
    "Since we gonna use Trainer from Transformers library, it expects our dataset as a torch.utils.data.Dataset, so we made a simple class that implements __len__() method that returns number of samples, and __getitem__() method to return a data sample at a specific index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(model_name, \n",
    "                 train_texts, train_labels, \n",
    "                 val_texts=None, val_labels=None, \n",
    "                 test_texts=None, test_labels=None):\n",
    "  \"\"\"\n",
    "  Prepare input data for model fine-tuning\n",
    "  \"\"\"\n",
    "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  prepare_val = False if val_texts is None or val_labels is None else True\n",
    "  prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "  def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    decodings = tokenizer(labels,  truncation=True, padding=True)\n",
    "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "    return dataset_tokenized\n",
    "\n",
    "  train_dataset = tokenize_data(train_texts, train_labels)\n",
    "  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename variables\n",
    "train_texts, train_labels = train_input, train_output \n",
    "val_texts, val_labels = val_input, val_output \n",
    "\n",
    "# prepare data\n",
    "train_dataset, val_dataset, _ = prepare_data(model_name, train_texts, train_labels, val_texts, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.4 - Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "# define Training Arguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,   # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "    logging_steps=1  ,               # log & save weights each logging_steps\n",
    "    eval_steps=1,                    # number of update steps before evaluation\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    "    predict_with_generate = True     # whether to use generate to calculate generative metrics (ROUGE, BLEU). \n",
    "\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                     # the instantiated 🤗 Transformers model to be trained\n",
    "    tokenizer = tokenizer,           # the instantiated 🤗 Transformers tokenizer to be trained  \n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    eval_dataset=val_dataset,        # evaluation dataset\n",
    "    compute_metrics=compute_metrics  # pass metric function\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.5 - Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('fine_tuned')\n",
    "tokenizer.save_pretrained('fine_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.6 - Instantiate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the new model\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained('fine_tuned')\n",
    "# The PEGASUS Model with a language modeling head. Can be used for summarization. \n",
    "# This model inherits from PreTrainedModel. \n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained('fine_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.7 - Compute the summaries and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFERENCE\n",
    "\n",
    "## With fine-tunning \n",
    "\n",
    "# create empry lists to store rouge scores\n",
    "r1_precision = []\n",
    "r2_precision = []\n",
    "rL_precision = []\n",
    "\n",
    "r1_recall = []\n",
    "r2_recall = []\n",
    "rL_recall = []\n",
    "\n",
    "r1_fmeasure = []\n",
    "r2_fmeasure = []\n",
    "rL_fmeasure = []\n",
    "\n",
    "val_model_output = []\n",
    "\n",
    "# start counting seconds to keep track of time \n",
    "start = time.time()\n",
    "\n",
    "# loop over validation set\n",
    "for sample_id in range(len(val_input)):\n",
    "    \n",
    "    # get input (body OR abstract + cited text spans) - scisummnet uses abstract, we want to use body\n",
    "    sample_input = val_input[sample_id]\n",
    "    \n",
    "    # tokenize it\n",
    "    inputs = tokenizer([sample_input], max_length=1024, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # 'max_length': Pad to a maximum length specified with the argument max_length \n",
    "    # or to the maximum acceptable input length for the model if that argument is not provided.\n",
    "\n",
    "    # generate Summary\n",
    "    summary_ids = model.generate(inputs['input_ids'])\n",
    "    \n",
    "    # decode summary\n",
    "    sample_output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    \n",
    "    # store summary\n",
    "    val_model_output = val_model_output + sample_output\n",
    "    \n",
    "    # get reference (gold) summary \n",
    "    sample_reference = val_output[sample_id]\n",
    "    \n",
    "    #calculate rouge score\n",
    "    scores = scorer.score(str(sample_reference), str(sample_output))\n",
    "    \n",
    "    r1_precision.append(scores['rouge1'][0])\n",
    "    r1_recall.append(scores['rouge1'][1])\n",
    "    r1_fmeasure.append(scores['rouge1'][2])\n",
    "    \n",
    "    r2_precision.append(scores['rouge2'][0])\n",
    "    r2_recall.append(scores['rouge2'][1])\n",
    "    r2_fmeasure.append(scores['rouge2'][2])\n",
    "    \n",
    "    rL_precision.append(scores['rougeL'][0])\n",
    "    rL_recall.append(scores['rougeL'][1])\n",
    "    rL_fmeasure.append(scores['rougeL'][2])\n",
    "    \n",
    "    if sample_id % 5 == 0:\n",
    "        print(sample_id, time.time() - start)\n",
    "        start = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute score statistics\n",
    "\n",
    "all_socores = {'R1': [stats.mean(r1_precision), stats.mean(r1_recall), stats.mean(r1_fmeasure)],\n",
    "        'R2': [stats.mean(r2_precision), stats.mean(r2_recall), stats.mean(r2_fmeasure)],\n",
    "        'RL': [stats.mean(rL_precision), stats.mean(rL_recall), stats.mean(rL_fmeasure)]      \n",
    "        }\n",
    "\n",
    "all_socores_df = pd.DataFrame(all_socores, columns = ['R1', 'R2', 'RL'], index=['precision','recall','fmeasure'])\n",
    "\n",
    "print(all_socores_df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "# from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# def prepare_fine_tuning(model_name, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n",
    "#   \"\"\"\n",
    "#   Prepare configurations and base model for fine-tuning\n",
    "#   \"\"\"\n",
    "#   torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#   model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "#   if freeze_encoder:\n",
    "#     for param in model.model.encoder.parameters():\n",
    "#       param.requires_grad = False\n",
    "\n",
    "#   if val_dataset is not None:\n",
    "#     training_args = TrainingArguments(\n",
    "#       output_dir=output_dir,           # output directory\n",
    "#       do_train=True,  \n",
    "#       num_train_epochs=5,              # total number of training epochs\n",
    "#       per_device_train_batch_size=16,  # batch size per device during training, can increase if memory allows\n",
    "#       per_device_eval_batch_size=32,   # batch size for evaluation, can increase if memory allows\n",
    "#       save_steps=1,                  # number of updates steps before checkpoint saves\n",
    "#       save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "#       evaluation_strategy='steps',     # evaluation strategy to adopt during training\n",
    "#       eval_steps=1,                  # number of update steps before evaluation\n",
    "#       warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#       weight_decay=0.01,               # strength of weight decay\n",
    "#       logging_dir='./logs',            # directory for storing logs\n",
    "#       logging_steps=32,\n",
    "#       gradient_accumulation_steps=1,  \n",
    "# #     output_dir=\"./checkpoints\",\n",
    "# #     per_device_train_batch_size=1,\n",
    "# #     do_train=True,\n",
    "# #     # fp16=True,  # This has a known bug with t5\n",
    "# #     gradient_accumulation_steps=1,\n",
    "# #     logging_steps=1,\n",
    "# #     save_steps=1,\n",
    "# #     overwrite_output_dir=True,\n",
    "# #     save_total_limit=10,\n",
    "#     )\n",
    "\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#       model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "#       args=training_args,                  # training arguments, defined above\n",
    "#       train_dataset=train_dataset,         # training dataset\n",
    "#       eval_dataset=val_dataset             # evaluation dataset\n",
    "#     )\n",
    "\n",
    "#   else:\n",
    "#     training_args = TrainingArguments(\n",
    "# #       output_dir=output_dir,           # output directory\n",
    "# #       num_train_epochs=5,              # total number of training epochs\n",
    "# #       per_device_train_batch_size=1,  # batch size per device during training, can increase if memory allows\n",
    "# #       save_steps=16,                  # number of updates steps before checkpoint saves\n",
    "# #       save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "# #       warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "# #       weight_decay=0.01,               # strength of weight decay\n",
    "# #       logging_dir='./logs',            # directory for storing logs\n",
    "# #       logging_steps=16,\n",
    "# #       gradient_accumulation_steps=16,  \n",
    "#     output_dir=\"./checkpoints\",\n",
    "#     per_device_train_batch_size=1,\n",
    "#     do_train=True,\n",
    "#     # fp16=True,  # This has a known bug with t5\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     logging_steps=1,\n",
    "#     save_steps=1,\n",
    "#     overwrite_output_dir=True,\n",
    "#     save_total_limit=10,\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#       model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "#       args=training_args,                  # training arguments, defined above\n",
    "#       train_dataset=train_dataset,         # training dataset\n",
    "#     )\n",
    "\n",
    "#   return trainer\n",
    "\n",
    "\n",
    "# # Train\n",
    "\n",
    "# trainer = prepare_fine_tuning(model_name, train_dataset, val_dataset)\n",
    "# trainer.train()\n",
    "\n",
    "# ### Step 4.6 - Instantiate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
