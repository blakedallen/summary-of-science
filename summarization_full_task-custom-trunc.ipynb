{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization - Full task 'keep it simple'\n",
    "\n",
    "Scientific papers summarization task divided into 4 steps:\n",
    "\n",
    "**Step 0 - Download and parse the data** \\\n",
    "**Step 1 - Cited text spans identification** \\\n",
    "**Step 2 - Prepare data for train and inference** \\\n",
    "**Step 3 - Summarize with Pre-Trained Pegasus (no fine-tuning)** \\\n",
    "**Step 4 - Fine-Tune Pre-Trained Pegasus**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "    #Download the dataset\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# For regular expressions\n",
    "import re\n",
    "# For handling string\n",
    "import string\n",
    "# For performing mathematical operations\n",
    "import math\n",
    "# Importing spacy\n",
    "import spacy\n",
    "# Importing json to read input\n",
    "import json\n",
    "# Importing rouge for evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import html\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import statistics as stats\n",
    "import time\n",
    "\n",
    "from scipy import spatial\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "\n",
    "# for turn text into sentences\n",
    "import nltk.data\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import html\n",
    "from lxml import etree\n",
    "import unidecode\n",
    "\n",
    "from scipy import stats as s\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "import nltk \n",
    "import glob, os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.etree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "plot_dims = (16, 16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# url = \"https://cs.stanford.edu/~myasu/projects/scisumm_net/scisummnet_release1.1__20190413.zip\"\n",
    "# response = requests.get(url)\n",
    "# with zipfile.ZipFile(io.BytesIO(response.content)) as zipObj:\n",
    "#     # Extract all the contents of zip file in different directory\n",
    "#     zipObj.extractall(\"nlp_data\")\n",
    "#     print(\"File is unzipped in nlp_data folder\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pip\n",
    "# from pip._internal import main as pipmain\n",
    "\n",
    "# pipmain(['install', 'datasets'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Download and parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "DATA_DIR = \"data/nlp_data/scisummnet_release1.1__20190413/top1000_complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all raw text, break all papers into two parts -- Abstract and rest of document\n",
    "#first get all filepaths\n",
    "xmlfiles = []\n",
    "citations = []\n",
    "golden_summaries = []\n",
    "for subdir, dirs, files in os.walk(DATA_DIR):\n",
    "    for filename in files:\n",
    "        filepath = subdir + os.sep + filename\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            xmlfiles.append(filepath)\n",
    "        if filepath.endswith(\".json\"):\n",
    "            citations.append(filepath)\n",
    "        if filepath.endswith(\".txt\"):\n",
    "            golden_summaries.append(filepath)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next parse all XML documents\n",
    "\n",
    "def parse_xml_abstract(fp):\n",
    "    \"\"\" parse an XML journal article into an abstract and the rest of the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(fp)\n",
    "    except Exception as e:\n",
    "        return \"\",\"\",str(e)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    ab = []\n",
    "    bod = []\n",
    "    \n",
    "    for child in root:\n",
    "        if child.tag == \"ABSTRACT\":\n",
    "            for block in child:\n",
    "                ab.append(block.text)\n",
    "        else:\n",
    "            for block in child:\n",
    "                bod.append(block.text)\n",
    "                \n",
    "    #convert from list --> string\n",
    "    abstract = \"\\n\".join(ab)\n",
    "    body = \"\\n\".join(bod)\n",
    "    \n",
    "    #decode html entities\n",
    "    abstract = html.unescape(abstract)\n",
    "    body = html.unescape(body)\n",
    "    \n",
    "    return abstract,body,\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>golden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present a method for extracting parts of ob...</td>\n",
       "      <td>We present a method of extracting parts of obj...</td>\n",
       "      <td>[Berland and Charniak (1999) use Hearst style ...</td>\n",
       "      <td>Finding Parts In Very Large Corpora\\nWe presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>[The program takes the output of char_align (C...</td>\n",
       "      <td>The Mathematics Of Statistical Machine Transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Previous work has shown that Chinese word segm...</td>\n",
       "      <td>Word segmentation is considered an important f...</td>\n",
       "      <td>[Chinese word segmentation is done by the Stan...</td>\n",
       "      <td>Optimizing Chinese Word Segmentation for Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We examine the viability of building large pol...</td>\n",
       "      <td>Polarity lexicons are large lists of phrases t...</td>\n",
       "      <td>[Recent work in this area includes Velikovich ...</td>\n",
       "      <td>The viability of web-derived polarity lexicons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extracting semantic relationships between enti...</td>\n",
       "      <td>Extraction of semantic relationships between e...</td>\n",
       "      <td>[They use two kinds of features: syntactic one...</td>\n",
       "      <td>Combining Lexical Syntactic And Semantic Featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>[In addition, Niessen and Ney (2004) decompose...</td>\n",
       "      <td>Statistical Machine Translation With Scarce Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>We have developed a new program called alignin...</td>\n",
       "      <td>Aligning parallel texts has recently received ...</td>\n",
       "      <td>[There have been quite a number of recent pape...</td>\n",
       "      <td>Robust Bilingual Word Alignment For Machine Ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>We present an approach to pronoun resolution b...</td>\n",
       "      <td>Pronoun resolution is a difficult but vital pa...</td>\n",
       "      <td>[, We follow the closed track setting where sy...</td>\n",
       "      <td>Bootstrapping Path-Based Pronoun Resolution\\nW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>We use logical inference techniques for recogn...</td>\n",
       "      <td>Recognising textual entailment (RTE) is the ta...</td>\n",
       "      <td>[However, this method does not work for realwo...</td>\n",
       "      <td>Recognising Textual Entailment With Logical In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>This paper deals with two important ambiguitie...</td>\n",
       "      <td>The problem with successful resolution of ambi...</td>\n",
       "      <td>[The state of the art is a supervised algorith...</td>\n",
       "      <td>Corpus Based PP Attachment Ambiguity Resolutio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1009 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     We present a method for extracting parts of ob...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Previous work has shown that Chinese word segm...   \n",
       "3     We examine the viability of building large pol...   \n",
       "4     Extracting semantic relationships between enti...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  We have developed a new program called alignin...   \n",
       "1006  We present an approach to pronoun resolution b...   \n",
       "1007  We use logical inference techniques for recogn...   \n",
       "1008  This paper deals with two important ambiguitie...   \n",
       "\n",
       "                                                   body  \\\n",
       "0     We present a method of extracting parts of obj...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Word segmentation is considered an important f...   \n",
       "3     Polarity lexicons are large lists of phrases t...   \n",
       "4     Extraction of semantic relationships between e...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  Aligning parallel texts has recently received ...   \n",
       "1006  Pronoun resolution is a difficult but vital pa...   \n",
       "1007  Recognising textual entailment (RTE) is the ta...   \n",
       "1008  The problem with successful resolution of ambi...   \n",
       "\n",
       "                                              citations  \\\n",
       "0     [Berland and Charniak (1999) use Hearst style ...   \n",
       "1     [The program takes the output of char_align (C...   \n",
       "2     [Chinese word segmentation is done by the Stan...   \n",
       "3     [Recent work in this area includes Velikovich ...   \n",
       "4     [They use two kinds of features: syntactic one...   \n",
       "...                                                 ...   \n",
       "1004  [In addition, Niessen and Ney (2004) decompose...   \n",
       "1005  [There have been quite a number of recent pape...   \n",
       "1006  [, We follow the closed track setting where sy...   \n",
       "1007  [However, this method does not work for realwo...   \n",
       "1008  [The state of the art is a supervised algorith...   \n",
       "\n",
       "                                                 golden  \n",
       "0     Finding Parts In Very Large Corpora\\nWe presen...  \n",
       "1     The Mathematics Of Statistical Machine Transla...  \n",
       "2     Optimizing Chinese Word Segmentation for Machi...  \n",
       "3     The viability of web-derived polarity lexicons...  \n",
       "4     Combining Lexical Syntactic And Semantic Featu...  \n",
       "...                                                 ...  \n",
       "1004  Statistical Machine Translation With Scarce Re...  \n",
       "1005  Robust Bilingual Word Alignment For Machine Ai...  \n",
       "1006  Bootstrapping Path-Based Pronoun Resolution\\nW...  \n",
       "1007  Recognising Textual Entailment With Logical In...  \n",
       "1008  Corpus Based PP Attachment Ambiguity Resolutio...  \n",
       "\n",
       "[1009 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create DF with papers and citations\n",
    "\n",
    "raw_cols = []\n",
    "golden = []\n",
    "for fpn in range(len(xmlfiles)):\n",
    "    ab,bod,err = parse_xml_abstract(xmlfiles[fpn])\n",
    "    if err:\n",
    "        #print(fp, err)\n",
    "        continue\n",
    "    f = open(citations[fpn]) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    only_text = []\n",
    "    for entry in data:\n",
    "        only_text.append(entry['clean_text'])\n",
    "#     print(only_text)\n",
    "    \n",
    "    f2 = open(golden_summaries[fpn],\"r+\") \n",
    "    golden = f2.read()\n",
    "\n",
    "    \n",
    "    raw_cols.append([ab,bod,only_text, golden])\n",
    "\n",
    "df = pd.DataFrame(raw_cols, columns=[\"abstract\",\"body\",\"citations\", \"golden\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>golden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present a method for extracting parts of ob...</td>\n",
       "      <td>We present a method of extracting parts of obj...</td>\n",
       "      <td>[Berland and Charniak (1999) use Hearst style ...</td>\n",
       "      <td>Finding Parts In Very Large Corpora\\nWe presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>[The program takes the output of char_align (C...</td>\n",
       "      <td>The Mathematics Of Statistical Machine Transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Previous work has shown that Chinese word segm...</td>\n",
       "      <td>Word segmentation is considered an important f...</td>\n",
       "      <td>[Chinese word segmentation is done by the Stan...</td>\n",
       "      <td>Optimizing Chinese Word Segmentation for Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We examine the viability of building large pol...</td>\n",
       "      <td>Polarity lexicons are large lists of phrases t...</td>\n",
       "      <td>[Recent work in this area includes Velikovich ...</td>\n",
       "      <td>The viability of web-derived polarity lexicons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extracting semantic relationships between enti...</td>\n",
       "      <td>Extraction of semantic relationships between e...</td>\n",
       "      <td>[They use two kinds of features: syntactic one...</td>\n",
       "      <td>Combining Lexical Syntactic And Semantic Featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>are at least two kinds of similarity. similari...</td>\n",
       "      <td>There are at least two kinds of similarity.\\nR...</td>\n",
       "      <td>[Veale (2004) used WordNet to answer 374 multi...</td>\n",
       "      <td>Similarity of Semantic Relations\\nThere are at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>j schroeder ed ac uk Abstract This paper analy...</td>\n",
       "      <td>This paper presents the results the shared tas...</td>\n",
       "      <td>[Tests were run on the ACL WSMT 2008 test set ...</td>\n",
       "      <td>Further Meta-Evaluation of Machine Translation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In adding syntax to statistical MT, there is a...</td>\n",
       "      <td>The statistical revolution in machine translat...</td>\n",
       "      <td>[In Marton and Resnik (2008), hiero variables ...</td>\n",
       "      <td>Soft Syntactic Constraints for Hierarchical Ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Although disjunction has been used in several ...</td>\n",
       "      <td>Disjunction has been used in several unificati...</td>\n",
       "      <td>[It is well-known that disjunctive unification...</td>\n",
       "      <td>A Unification Method For Disjunctive Feature D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Corpus-based approaches to word sense identifi...</td>\n",
       "      <td>Corpus-based approaches to word sense identifi...</td>\n",
       "      <td>[Leacock et al (1998), Agirre and Lopezde Laca...</td>\n",
       "      <td>Using Corpus Statistics And WordNet Relations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Syntactic parsing requires a fine balance betw...</td>\n",
       "      <td>Dependency-based representations have become i...</td>\n",
       "      <td>[First, well-nestedness is interesting as a ge...</td>\n",
       "      <td>Mildly Non-Projective Dependency Structures\\nS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>present a system for identifying the semantic ...</td>\n",
       "      <td>We present a system for identifying the semant...</td>\n",
       "      <td>[Gildea and Jurafsky (2002) describe a statist...</td>\n",
       "      <td>Automatic Labeling Of Semantic Roles\\nWe prese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>This paper compares a number of generative pro...</td>\n",
       "      <td>The currently best single-model statistical pa...</td>\n",
       "      <td>[These parsers are trained and evaluated using...</td>\n",
       "      <td>Generative Models For Statistical Parsing With...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tagged Dependency -.— Tagged Adjacency -e— L. ...</td>\n",
       "      <td>If parsing is taken to be the first step in ta...</td>\n",
       "      <td>[A concise review of this research area can be...</td>\n",
       "      <td>Corpus Statistics Meet The Noun Compound: Some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Of the 1600 IBM sentences that have been pars...</td>\n",
       "      <td>Building A Large Annotated Corpus Of English: ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             abstract  \\\n",
       "0   We present a method for extracting parts of ob...   \n",
       "1   We describe a series of five statistical model...   \n",
       "2   Previous work has shown that Chinese word segm...   \n",
       "3   We examine the viability of building large pol...   \n",
       "4   Extracting semantic relationships between enti...   \n",
       "5   are at least two kinds of similarity. similari...   \n",
       "6   j schroeder ed ac uk Abstract This paper analy...   \n",
       "7   In adding syntax to statistical MT, there is a...   \n",
       "8   Although disjunction has been used in several ...   \n",
       "9   Corpus-based approaches to word sense identifi...   \n",
       "10  Syntactic parsing requires a fine balance betw...   \n",
       "11  present a system for identifying the semantic ...   \n",
       "12  This paper compares a number of generative pro...   \n",
       "13  Tagged Dependency -.— Tagged Adjacency -e— L. ...   \n",
       "14                                                      \n",
       "\n",
       "                                                 body  \\\n",
       "0   We present a method of extracting parts of obj...   \n",
       "1   We describe a series of five statistical model...   \n",
       "2   Word segmentation is considered an important f...   \n",
       "3   Polarity lexicons are large lists of phrases t...   \n",
       "4   Extraction of semantic relationships between e...   \n",
       "5   There are at least two kinds of similarity.\\nR...   \n",
       "6   This paper presents the results the shared tas...   \n",
       "7   The statistical revolution in machine translat...   \n",
       "8   Disjunction has been used in several unificati...   \n",
       "9   Corpus-based approaches to word sense identifi...   \n",
       "10  Dependency-based representations have become i...   \n",
       "11  We present a system for identifying the semant...   \n",
       "12  The currently best single-model statistical pa...   \n",
       "13  If parsing is taken to be the first step in ta...   \n",
       "14                                                      \n",
       "\n",
       "                                            citations  \\\n",
       "0   [Berland and Charniak (1999) use Hearst style ...   \n",
       "1   [The program takes the output of char_align (C...   \n",
       "2   [Chinese word segmentation is done by the Stan...   \n",
       "3   [Recent work in this area includes Velikovich ...   \n",
       "4   [They use two kinds of features: syntactic one...   \n",
       "5   [Veale (2004) used WordNet to answer 374 multi...   \n",
       "6   [Tests were run on the ACL WSMT 2008 test set ...   \n",
       "7   [In Marton and Resnik (2008), hiero variables ...   \n",
       "8   [It is well-known that disjunctive unification...   \n",
       "9   [Leacock et al (1998), Agirre and Lopezde Laca...   \n",
       "10  [First, well-nestedness is interesting as a ge...   \n",
       "11  [Gildea and Jurafsky (2002) describe a statist...   \n",
       "12  [These parsers are trained and evaluated using...   \n",
       "13  [A concise review of this research area can be...   \n",
       "14  [Of the 1600 IBM sentences that have been pars...   \n",
       "\n",
       "                                               golden  \n",
       "0   Finding Parts In Very Large Corpora\\nWe presen...  \n",
       "1   The Mathematics Of Statistical Machine Transla...  \n",
       "2   Optimizing Chinese Word Segmentation for Machi...  \n",
       "3   The viability of web-derived polarity lexicons...  \n",
       "4   Combining Lexical Syntactic And Semantic Featu...  \n",
       "5   Similarity of Semantic Relations\\nThere are at...  \n",
       "6   Further Meta-Evaluation of Machine Translation...  \n",
       "7   Soft Syntactic Constraints for Hierarchical Ph...  \n",
       "8   A Unification Method For Disjunctive Feature D...  \n",
       "9   Using Corpus Statistics And WordNet Relations ...  \n",
       "10  Mildly Non-Projective Dependency Structures\\nS...  \n",
       "11  Automatic Labeling Of Semantic Roles\\nWe prese...  \n",
       "12  Generative Models For Statistical Parsing With...  \n",
       "13  Corpus Statistics Meet The Noun Compound: Some...  \n",
       "14  Building A Large Annotated Corpus Of English: ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Cited text spans identification\n",
    "\n",
    "Finding the top x sentences in the body that have the largest similarity (as per ROUGE score) with the citations \n",
    "\n",
    "Step 1.1 - Define helper functions\\\n",
    "Step 1.2 - Select body sentences with good quality \\\n",
    "Step 1.3 - Select 'x' body sentences to \"represent\" citations as model inputs - i.e., find cited text spans \\\n",
    "Step 1.4 - Check the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1 - Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT BEING USED\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkQuality(sentence):\n",
    "    \n",
    "    '''Check the quality of body sentences, to classify each of them as eligible (or not) to be a cited text span'''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    length = len(tokenized_text)\n",
    "    alpha = 0\n",
    "    nonAlpha = 0\n",
    "    found = 0\n",
    "    nonFound = 0\n",
    "    upper = 0\n",
    "    stop = 0\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    for token in tokenized_text:\n",
    "        #print(token)\n",
    "        if len(token)==1:\n",
    "            if token.isalpha():\n",
    "                # count number of single alpha chars\n",
    "                alpha+=1\n",
    "            else:\n",
    "                # count number of not single non-alpha chars\n",
    "                nonAlpha+=1\n",
    "        else:\n",
    "            if(token.lower() in stop_words):\n",
    "                # count number of stop words\n",
    "                stop+=1\n",
    "            else:\n",
    "                lemma = lemmatizer.lemmatize(token.lower())\n",
    "                # count number of Synsets for alpha tokens\n",
    "                if token.isalpha():\n",
    "                    if (len(wn.synsets(lemma))>0):\n",
    "                        found+=1 # alpha tokens that have Synsets\n",
    "                    else:\n",
    "                        nonFound+=1 # alpha tokens that dont have Synsets\n",
    "                else:\n",
    "                    nonAlpha+=1 #non alpha tokens\n",
    "    \n",
    "    # calculate shares \n",
    "    alphaS = alpha/length\n",
    "    nonAlphaS = nonAlpha/length\n",
    "    foundS = found/length\n",
    "    nonFoundS = nonFound/length\n",
    "#     upperS = upper/length\n",
    "    stopS = stop/length\n",
    "    \n",
    "    good_quality = True\n",
    "    \n",
    "    if(nonFoundS>0.2):\n",
    "        good_quality = False\n",
    "    if(foundS<0.1):\n",
    "        good_quality = False\n",
    "    if(alphaS>0.2):\n",
    "        good_quality = False\n",
    "    if(nonAlphaS>0.5):\n",
    "        good_quality = False\n",
    "    if len(tokenized_text)< 6:\n",
    "        good_quality = False\n",
    "    \n",
    "    if (\"equation\" in sentence.lower()) | (\"section\" in sentence.lower()) | (\"table\" in sentence.lower()) | (\"figure\" in sentence.lower()) | (\"=\" in sentence) | (\">\" in sentence) | (\"<\" in sentence) | (\"p(\" in sentence.lower()):\n",
    "        good_quality = False\n",
    "    \n",
    "    # remove sentences that has dates\n",
    "    match = re.match(r'.*([1-3][0-9]{3})', sentence)\n",
    "    if match is not None:\n",
    "        good_quality = False \n",
    "        \n",
    "    return good_quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cited_text_spans_ids(df,n_sent = 3):\n",
    "\n",
    "    '''Find (n_sent) body sentences most similar to citation sentences \n",
    "    inputs: \n",
    "        df: dataframe with simularity scores between each body sentence and citation \n",
    "        n_set: number of body sentences to output\n",
    "    output:\n",
    "        list of best ranked body sentetences ids\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    # empty list where we'll store best scored body sentences \n",
    "    body_sent_all_ids = []\n",
    "    body_sent_id = []\n",
    "     \n",
    "    # create an empty dataframe (num rows = num sentences in citations;  num col = n_sent)\n",
    "    # we'll store here the body sentences ids with largest similarity\n",
    "    sim_df_max = pd.DataFrame(0.0, index=[j for j in range(len(citations_sentences))], columns= [j for j in range(n_sent)])\n",
    "    \n",
    "    for ns in range(n_sent):\n",
    "        # get the indexes that maximize similarity for each column (i.e., for each citation sentence)    \n",
    "        sim_df_max[ns] = sim_df.idxmax(axis=0, skipna=True)\n",
    "        # reset the largest values to zero, so we can retreive other top values in the loop\n",
    "        sim_df[ns][sim_df_max[ns]] = 0\n",
    "        \n",
    "#     print(sim_df_max)\n",
    "#     print('done')\n",
    "\n",
    "    # loop over the number of body sentences we want to retrieve\n",
    "    for ns in range(n_sent): \n",
    "        # turn all columns into a list of best scored body sentences ids  \n",
    "        li = sim_df_max[ns].tolist() \n",
    "#         print(li)\n",
    "        body_sent_all_ids = body_sent_all_ids + li\n",
    "#         print(body_sent_all_ids)\n",
    "    \n",
    "#     print(body_sent_all_ids)\n",
    "    for ns in range(n_sent):   \n",
    "        # append best scored sentence id over all citations sentences\n",
    "#         print(type(body_sent_all_ids))\n",
    "#         print(body_sent_all_ids)\n",
    "#         print(s.mode(body_sent_all_ids))\n",
    "        best_sent_id_single = int(s.mode(body_sent_all_ids)[0])\n",
    "        body_sent_id.append(best_sent_id_single)\n",
    "        # reset the largest values to zero, so we can retreive other top values in the loop                    \n",
    "        body_sent_all_ids = list(filter(lambda a: a != best_sent_id_single, body_sent_all_ids)) \n",
    "#         print(body_sent_all_ids)\n",
    "#         if len(body_sent_all_ids) == 0:\n",
    "#             break\n",
    "\n",
    "    return body_sent_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Step 1.2 - Select body sentences with good quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 secs to run: 2.1284050941467285\n",
      "250 secs to run: 27.846195936203003\n",
      "500 secs to run: 26.506773948669434\n",
      "750 secs to run: 24.662267208099365\n",
      "1000 secs to run: 26.148672819137573\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "df['body_good_quality'] = \"\"\n",
    "\n",
    "good_quality_size_share = []\n",
    "\n",
    "for paper_id in df.index: # [i for i in df.index if i >79]:\n",
    "    \n",
    "    body_sentences = []\n",
    "    \n",
    "    # remove meaningless dots and references\n",
    "    body_sentences_tok = df.body[paper_id].replace(\"et al.\", \"et al\").replace(\"e.g.\", \"eg\").replace(\"eg.\", \"eg\").replace(\"i.e.\", \"ie\").replace(\"ie.\", \"ie\")\n",
    "    body_sentences_tok = body_sentences_tok.replace(\".1\", \". \").replace(\".2\", \". \").replace(\".3\", \".\").replace(\".4\", \". \").replace(\".5\", \". \").replace(\".6\", \". \").replace(\".7\", \". \").replace(\".8\", \". \").replace(\".9\", \". \")\n",
    "    body_sentences_tok = body_sentences_tok.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # tokenize body sentences\n",
    "    body_sentences_tok = tokenizer.tokenize(body_sentences_tok)\n",
    "\n",
    "    # truncate too large body sentences (larger than 512 throws an error - rare cases) and remove \"?\"\n",
    "    body_sentences_tok = [s.replace(\"?\", \"\") for s in body_sentences_tok if len(s) <= 512]\n",
    "    \n",
    "#     print(body_sentences_tok)\n",
    "    \n",
    "#     remove papers that are too small even before quality check\n",
    "    if len(body_sentences_tok) < 10:\n",
    "        continue\n",
    "    \n",
    "    len_total = len(body_sentences_tok)\n",
    "    # find body sentences with quality\n",
    "    for i in range(len(body_sentences_tok)):\n",
    "        if len(body_sentences_tok[i])>0:\n",
    "            if checkQuality(body_sentences_tok[i]):\n",
    "                body_sentences.append(body_sentences_tok[i])\n",
    "    len_clean = len(body_sentences)\n",
    "    \n",
    "    # store share of size reduction by quality check\n",
    "    if len_total > 0:\n",
    "        good_quality_size_share.append(len_clean/len_total*100) \n",
    "    \n",
    "    # fill our large/original dataframe with the best scored body sentences \n",
    "    df['body_good_quality'][paper_id] = \" \".join(body_sentences)\n",
    "    \n",
    "    # keep track of time to run \n",
    "    if paper_id % 250 == 0:\n",
    "        print(paper_id, \"secs to run:\", time.time() - start)\n",
    "        start = time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, clean text is 67.04721012937098 % of the size of full text\n"
     ]
    }
   ],
   "source": [
    "lst = good_quality_size_share\n",
    "\n",
    "print(\"On average, clean text is\", sum(lst) / len(lst), \"% of the size of full text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3 - Select 'x' body sentences to \"represent\" citations as model inputs - i.e., find cited text spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an empty column in the original dataframe, in which we'll store the cited text spans for each paper \n",
    "df['cited_text_spans'] = \"\"\n",
    "\n",
    "## instantiate nlp tools to read the data\n",
    "# vectorizer = Vectorizer() - NOT BEING USED - USED FOR COSINE SIMILARITY\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "## instantiate rouge score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# start time to keep track of the timing\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 secs to run: 2.0177600383758545\n",
      "250 secs to run: 579.6857738494873\n",
      "500 secs to run: 599.5049519538879\n",
      "750 secs to run: 538.4577572345734\n",
      "1000 secs to run: 2488.3857820034027\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# loop over df rows and find cited text spans\n",
    "\n",
    "for paper_id in df.index: # [i for i in df.index if i >79]:\n",
    "    \n",
    "    body_sentences = tokenizer.tokenize(df['body_good_quality'][paper_id])\n",
    "    \n",
    "    if len(body_sentences) < 10:\n",
    "        continue\n",
    "    \n",
    "#     print(body_sentences)\n",
    "    \n",
    "    citations_sentences = df['citations'][paper_id]\n",
    "        \n",
    "#     print(citations_sentences)\n",
    "    \n",
    "    # create an empty dataframe (num rows = num sentences in body;  num col = num sentences in citations)\n",
    "    sim_df = pd.DataFrame(0.0, index=[j for j in range(len(body_sentences))], columns=[j for j in range(len(citations_sentences))])\n",
    "\n",
    "    # nested loop: over body sentences and citations sentences\n",
    "    for body_sentence_id in range(len(body_sentences)):\n",
    "        for citation_sentence_id in range(len(citations_sentences)):\n",
    "            # fill empty dataframe with sim measure\n",
    "#             sim_df[citation_sentence_id][body_sentence_id] = spatial.distance.cosine(vectors_bert_body[body_sentence_id], vectors_bert_citations[citation_sentence_id])\n",
    "            scores = scorer.score(body_sentences[body_sentence_id],citations_sentences[citation_sentence_id] )\n",
    "            sim_df[citation_sentence_id][body_sentence_id] = 100*scores['rouge2'][2]\n",
    "    \n",
    "        \n",
    "    # get selected body sentences ids - if rouge 2 doesnt work (usually because it yields too many socre =0), \n",
    "    # retry with rouge 1\n",
    "    try:\n",
    "        sent_ids = cited_text_spans_ids(sim_df,3)\n",
    "        \n",
    "    except: \n",
    "        for body_sentence_id in range(len(body_sentences)):\n",
    "            for citation_sentence_id in range(len(citations_sentences)):\n",
    "            # fill empty dataframe with sim measure\n",
    "#             sim_df[citation_sentence_id][body_sentence_id] = spatial.distance.cosine(vectors_bert_body[body_sentence_id], vectors_bert_citations[citation_sentence_id])\n",
    "                scores = scorer.score(body_sentences[body_sentence_id],citations_sentences[citation_sentence_id] )\n",
    "                sim_df[citation_sentence_id][body_sentence_id] = 100*scores['rouge1'][2]\n",
    "#             print(body_sentence_id,citation_sentence_id,100*scores['rouge1'][2]  )\n",
    "#         print(sim_df)\n",
    "        sent_ids = cited_text_spans_ids(sim_df,3)\n",
    "  \n",
    "    # fill our large/original dataframe with the best scored body sentences \n",
    "    df.cited_text_spans[paper_id] = [body_sentences[b] for b in sent_ids]\n",
    "    \n",
    "    # keep track of time to run \n",
    "    if paper_id % 250 == 0:\n",
    "        print(paper_id, \"secs to run:\", time.time() - start)\n",
    "        start = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df.pkl\")\n",
    "# df = pd.read_pickle(\"df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4 - Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The work was motivated by the needs of the ILEX system for generating descriptions of museum artefacts (in particular, 20th Century jewellery) [Mellish et al 98].', 'This paper presents some initial experiments using stochastic search methods for aspects of text planning.', 'In this task, one is given a set of facts all of which should be included in a text and a set of relations between facts, some of which can be included in the text.']\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mellish et al (1998) investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations.',\n",
       " 'Mellish et al (1998) (and subsequently Karamanis and Manurung 2002) advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts.',\n",
       " 'Following previous work (Mellish et al, 1998) we used a single fitness function that scored candidates based on their coherence.',\n",
       " 'The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose.',\n",
       " 'For example, the measure from (Mellish et al, 1998) looks at the entire discourse up to the current transition for some of their cost factors.',\n",
       " 'Mellish et al (1998) advocate stochastic search as an alternative to exhaustively examining the search space.',\n",
       " 'As in the case of Mellish et al (1998) we construct an acceptable ordering rather than the best possible one.',\n",
       " 'Mellish et al (1998) made the point that even this restricted approach would soon become intractable with more than a small set of facts when one allows weak RST relations such as Joint and Elaboration into the model.',\n",
       " 'In the late 1990s, Chris Mellish implemented the first stochastic text planner (Mellish et al 1998).',\n",
       " 'The evaluation function of Mellish et al (1998) also was calculated over a sum of local features of the tree, although a wider set of features were involved.',\n",
       " 'For instance, the evaluation function of Mellish et al (1998) assigned +3 for each instance of subject-repetition.',\n",
       " 'Genetic algorithms are also used in [Mellish et al, 1998] where the authors state the problem of given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how one can arrange this material so as to yield the best possible text.',\n",
       " 'Mellish et al (1998) advocate stochastic search methods for document structuring.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check quality of cited text spans\n",
    "# pick any number up to the total number of papers  \n",
    "paper_id = 80 \n",
    "\n",
    "print(df.cited_text_spans[paper_id])\n",
    "print('----------------------------------------------')\n",
    "list(filter(None, df.citations[paper_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data size: 1009\n",
      "Number of papers with no cited text spans: 71\n",
      "Share of papers with no cited text spans (%): 7.0366699702675914\n"
     ]
    }
   ],
   "source": [
    "# check the number of papers with no cited text spans\n",
    "# -> this can occur because some papers are too small, or have few sentences considered of high quality, \n",
    "#   and were discarded\n",
    "\n",
    "# pick any number up to the total number of papers  \n",
    "size_all_data = df.shape[0]\n",
    "size_no_data = df[df.cited_text_spans == \"\"].shape[0]\n",
    "print(\"Full data size:\", size_all_data)\n",
    "print(\"Number of papers with no cited text spans:\", size_no_data)\n",
    "print(\"Share of papers with no cited text spans (%):\", 100*size_no_data/size_all_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Prepare data for train and inference \n",
    "\n",
    "In this step we will do the following:\n",
    " \n",
    "Step 2.1 - Define maximun input size and define a function to get most important sentences \\\n",
    "Step 2.2 - Define model inputs and 'labels' \\\n",
    "Step 2.3 - Clean the data (important specially if we are summarizing the body) \\\n",
    "Step 2.4 - Divide the data into train, validation and test \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 - Define maximun input size and define a function to get most important sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maximun input size\n",
    "max_input_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_important_sentences(document, max_size = max_input_size):\n",
    "    '''\n",
    "    If the number of sentences in a document is larger than max_input_size, select sentences with \n",
    "    largest similarity with all other sentences in the document. \n",
    "    \n",
    "    Inputs:\n",
    "        document: str, sentences in the original order of the document\n",
    "        max_size = number of sentences to be collected from the document\n",
    "    Output: \n",
    "        document: list of sentences text with 'max_size' sentences\n",
    "    '''\n",
    "    \n",
    "#     if type(document) == \"str\":\n",
    "            \n",
    "#     document = tokenizer.tokenize(document)\n",
    "    \n",
    "#     print(document)\n",
    "    \n",
    "    # create an empty dataframe (num rows = num sentences in doc;  num col = num sentences in doc)\n",
    "    sim_df = pd.DataFrame(0.0, index=[j for j in range(len(document))], columns=[j for j in range(len(document))])\n",
    "\n",
    "    # nested loop: over each pair of document sentences\n",
    "    for column_sentence_id in range(len(document)):\n",
    "        for row_sentence_id in range(len(document)):\n",
    "            # fill empty dataframe with sim measure\n",
    "            if column_sentence_id > row_sentence_id:\n",
    "                scores = scorer.score(document[column_sentence_id],document[row_sentence_id] )\n",
    "                sim_df[column_sentence_id][row_sentence_id] = 100*scores['rouge1'][2]\n",
    "    \n",
    "        \n",
    "#     print(sim_df)\n",
    "    sim_df_sum = list(sim_df.sum())\n",
    "    \n",
    "    index = sorted(range(len(sim_df_sum)), key=lambda k: sim_df_sum[k], reverse = True)\n",
    "    \n",
    "    most_important = \" \".join([document[b] for b in range(len(document)) if index[b] <= max_size])\n",
    "    \n",
    "    return most_important\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def most_important_sentences_vs_doc(document, max_size = max_input_size):\n",
    "    '''\n",
    "    If the number of sentences in a document is larger than max_input_size, select sentences with \n",
    "    largest similarity with the rest of the document. \n",
    "    \n",
    "    Inputs:\n",
    "        document: str, sentences in the original order of the document\n",
    "        max_size = number of sentences to be collected from the document\n",
    "    Output: \n",
    "        document: list of sentences text with 'max_size' sentences\n",
    "    '''\n",
    "    \n",
    "#     if type(document) == \"str\":\n",
    "            \n",
    "#     document = tokenizer.tokenize(document)\n",
    "    \n",
    "#     print(document)\n",
    "    \n",
    "    # create an empty dataframe (num rows = num sentences in doc;  num col = num sentences in doc)\n",
    "    sim_df = pd.DataFrame(0.0, index=[j for j in range(1)], columns=[j for j in range(len(document))])\n",
    "\n",
    "    # nested loop: over each pair of document sentences\n",
    "    for column_sentence_id in range(len(document)):\n",
    "\n",
    "    # fill empty dataframe with sim measure\n",
    "        document_ex = copy.deepcopy(document)   \n",
    "        del document_ex[column_sentence_id]\n",
    "        document_ex = \" \".join(document_ex)\n",
    "        scores = scorer.score(document[column_sentence_id],document_ex)\n",
    "        sim_df[column_sentence_id][0] = 100*scores['rouge1'][2]\n",
    "\n",
    "        \n",
    "#     print(sim_df)\n",
    "    sim_df_sum = list(sim_df.sum())\n",
    "    \n",
    "    index = sorted(range(len(sim_df_sum)), key=lambda k: sim_df_sum[k], reverse = True)\n",
    "    \n",
    "    most_important = \" \".join([document[b] for b in range(len(document)) if index[b] <= max_size])\n",
    "    \n",
    "    return most_important\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 - Define model inputs and 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract', 'body', 'citations', 'golden', 'body_good_quality', 'cited_text_spans', 'model_input', 'model_output']\n",
      "---------\n",
      "We present a method of extracting parts of objects from wholes (eg \"speedometer\" from \"car\"). To be more precise, given a single word denoting some entity that has recognizable parts, the system finds and rank-orders other words that may denote parts of the entity in question. Thus the relation found is strictly speaking between words, a relation Miller [1] calls \"meronymy.\" In this paper we use the more colloquial \"part-of\" terminology. We produce words with 55% accuracy for the top 50 words ranked by the system, given a very large corpus. Lacking an objective definition of the part-of relation, we use the majority judgment of five human subjects to decide which proposed parts are correct. The program's output could be scanned by an enduser and added to an existing ontology (eg, WordNet), or used as a part of a rough semantic lexicon. To the best of our knowledge, there is no published work on automatically finding parts from unlabeled corpora. Casting our nets wider, the work most similar to what we present here is that by Hearst [2] on acquisition of hyponyms (\"isa\" relations). In that paper Hearst (a) finds lexical correlates to the hyponym relations by looking in text for cases where known hyponyms appear in proximity (eg, in the construction (NP, NP and (NP other NN)) as in \"boats, cars, and other vehicles\"), (b) tests the proposed patterns for validity, and (c) uses them to extract relations from a corpus. In this paper we apply much the same methodology to the part-of relation. Indeed, in [2] Hearst states that she tried to apply this strategy to the part-of relation, but failed. We comment later on the differences in our approach that we believe were most important to our comparative success. Looking more widely still, there is an evergrowing literature on the use of statistical/corpusbased techniques in the automatic acquisition of lexical-semantic knowledge ([3-8]). We take it as axiomatic that such knowledge is tremendously useful in a wide variety of tasks, from lower-level tasks like noun-phrase reference, and parsing to user-level tasks such as web searches, question answering, and digesting. Certainly the large number of projects that use WordNet [1] would support this contention. And although WordNet is hand-built, there is general agreement that corpus-based methods have an advantage in the relative completeness of their coverage, particularly when used as supplements to the more laborintensive methods. Webster's Dictionary defines \"part\" as \"one of the often indefinite or unequal subdivisions into which something is or is regarded as divided and which together constitute the whole.\" The vagueness of this definition translates into a lack of guidance on exactly what constitutes a part, which in turn translates into some doubts about evaluating the results of any procedure that claims to find them. More specifically, note that the definition does not claim that parts must be physical objects. Thus, say, \"novel\" might have \"plot\" as a part. In this study we handle this problem by asking informants which words in a list are parts of some target word, and then declaring majority opinion to be correct. We give more details on this aspect of the study later. Here we simply note that while our subjects often disagreed, there was fair consensus that what might count as a part depends on the nature of the word: a physical object yields physical parts, an institution yields its members, and a concept yields its characteristics and processes. In other words, \"floor\" is part of \"building\" and \"plot\" is part of \"book.\" Our first goal is to find lexical patterns that tend to indicate part-whole relations. Following Hearst [2], we find possible patterns by taking two words that are in a part-whole relation (e.g, basement and building) and finding sentences in our corpus (we used the North American News Corpus (NANC) from LDC) that have these words within close proximity. The first few such sentences are: ... the basement of the building. ... the basement in question is in a four-story apartment building ... ... the basement of the apartment building. We assume here that parts and wholes are represented by individual lexical items (more specifically, as head nouns of noun-phrases) as opposed to complete noun phrases, or as a sequence of \"important\" noun modifiers together with the head. This occasionally causes problems, eg, \"conditioner\" was marked by our informants as not part of \"car\", whereas \"air conditioner\" probably would have made it into a part list. Nevertheless, in most cases head nouns have worked quite well on their own. We evaluated these patterns by observing how they performed in an experiment on a single example. The relatively poor performance of patterns C and E was anticipated, as many things occur \"in\" cars (or buildings, etc.) Pattern D is not so obviously bad as it differs from the plural case of pattern B only in the lack of the determiner \"the\" or \"a\". However, this difference proves critical in that pattern D tends to pick up \"counting\" nouns such as \"truckload.\" We use the LDC North American News Corpus (NANC). which is a compilation of the wire output of several US newspapers. The total corpus is about 100,000,000 words. We ran our program on the whole data set, which takes roughly four hours on our network. The bulk of that time (around 90%) is spent tagging the corpus. As is typical in this sort of work, we assume that our evidence (occurrences of patterns A and B) is independently and identically distributed (iid). We have found this assumption reasonable, but its breakdown has led to a few errors. In particular, a drawback of the NANC is the occurrence of repeated articles; since the corpus consists of all of the articles that come over the wire, some days include multiple, updated versions of the same story, containing identical paragraphs or sentences. We wrote programs to weed out such cases, but ultimately found them of little use. First, \"update\" articles still have substantial variation, so there is a continuum between these and articles that are simply on the same topic. Second, our data is so sparse that any such repeats are very unlikely to manifest themselves as repeated examples of part-type patterns. Nevertheless since two or three occurrences of a word can make it rank highly, our results have a few anomalies that stem from failure of the lid assumption (eg, quite appropriately, \"clunker\"). Our seeds are one word (such as \"car\") and its plural. We do not claim that all single words would fare as well as our seeds, as we picked highly probable words for our corpus (such as \"building\" and \"hospital\") that we thought would have parts that might also be mentioned therein. With enough text, one could probably get reasonable results with any noun that met these criteria. The program has three phases. The first identifies and records all occurrences of patterns A and B in our corpus. The second filters out all words ending with \"ing\", \"ness\", or \"ity\", since these suffixes typically occur in words that denote a quality rather than a physical object. Finally we order the possible parts by the likelihood that they are true parts according to some appropriate metric. We took some care in the selection of this metric. (Here and in what follows w denotes the outcome of the random variable generating wholes, and p the outcome for parts. However, in making this intuitive idea someone more precise we found two closely related versions: We call metrics based on the first of these \"loosely conditioned\" and those based on the second \"strongly conditioned\". While invariance with respect to frequency is generally a good property, such invariant metrics can lead to bad results when used with sparse data. Thus this metric must be tempered to take into account the quantity of data that supports its conclusion. We need a metric that combines these two desiderata in a natural way. We tried two such metrics. The second metric is proposed by Johnson (personal communication). We call this new test the \"significant-difference\" test, or sigdiff. The first group contains the words found for the method we perceive as the most accurate, sigdiff and strong conditioning. The other groups show the differences between them and the first group. The + category means that this method adds the word to its list, — means the opposite. For example, \"back\" is on the sigdiff-loose list but not the sigdiff-strong list. In general, sigdiff worked better than surprise and strong conditioning worked better than loose conditioning. In both cases the less favored methods tend to promote words that are less specific (\"back\" over \"airbag\", \"use\" over \"radiator\"). Furthermore, the combination of sigdiff and strong conditioning worked better than either by itself. Thus all results in this paper, unless explicitly noted otherwise, were gathered using sigdiff and strong conditioning combined. We tested five subjects (all of whom were unaware of our goals) for their concept of a \"part.\" We asked them to rate sets of 100 words, of which 50 were in our final results set. The score of individual words vary greatly but there was relative consensus on most words. We put an asterisk next to words that the majority subjects marked as correct. Lacking a formal definition of part, we can only define those words as correct and the rest as wrong. While the scoring is admittedly not perfect', it provides an adequate reference result. There we show the number of correct part words in the top 10, 20, 30, 40, and 50 parts for each seed (eg, for \"book\", 8 of the top 10 are parts, and 14 of the top 20). Overall, about 55% of the top 50 words for each seed are parts, and about 70% of the top 20 for each seed. The reader should also note that we tried one ambiguous word, \"plant\" to see what would happen. Our program finds parts corresponding to both senses, though given the nature of our text, the industrial use is more common. Our subjects marked both kinds of parts as correct, but even so, this produced the weakest part list of the six words we tried. As a baseline we also tried using as our \"pattern\" the head nouns that immediately surround our target word. We then applied the same \"strong conditioning, sigdiff\" statistical test to rank the candidates. Of the top 50 candidates for each target, only 8% were parts, as opposed to the 55% for our program. We also compared out parts list to those of WordNet. There are definite tradeoffs, although we would argue that our top20 set is both more specific and more comprehensive. More generally, all WordNet parts occur somewhere before 500, with the exception of \"tailfin\", which never occurs with car. It would seem that our program would be a good tool for expanding Wordnet, as a person can to the entire statistical NLP group at Brown, and scan and mark the list of part words in a few minutes. particularly to Mark Johnson, Brian Roark, Gideon Mann, and Ana-Maria Popescu who provided invaluable help on the project. The program presented here can find parts of objects given a word denoting the whole object and a large corpus of unmarked text. The program is about 55% accurate for the top 50 proposed parts for each of six examples upon which we tested it. There does not seem to be a single cause for the 45% of the cases that are mistakes. We present here a few problems that have caught our attention. Idiomatic phrases like \"a jalopy of a car\" or \"the son of a gun\" provide problems that are not easily weeded out. Depending on the data, these phrases can be as prevalent as the legitimate parts. In some cases problems arose because of tagger mistakes. For example, \"re-enactment\" would be found as part of a \"car\" using pattern B in the phrase \"the re-enactment of the car crash\" if \"crash\" is tagged as a verb. The program had some tendency to find qualities of objects. For example, \"driveability\" is strongly correlated with car. We try to weed out most of the qualities by removing words with the suffixes \"ness\", \"ing\", and \"ity.\" The most persistent problem is sparse data, which is the source of most of the noise. More data would almost certainly allow us to produce better lists, both because the statistics we are currently collecting would be more accurate, but also because larger numbers would allow us to find other reliable indicators. For example, idiomatic phrases might be recognized as such. So we see \"jalopy of a car\" (two times) but not, of course, \"the car's jalopy\". Words that appear in only one of the two patterns are suspect, but to use this rule we need sufficient counts on the good words to be sure we have a representative sample. At 100 million words, the NANC is not exactly small, but we were able to process it in about four hours with the machines at our disposal, so still larger corpora would not be out of the question. Finally, as noted above, Hearst [2] tried to find parts in corpora but did not achieve good results. She does not say what procedures were used, but assuming that the work closely paralleled her work on hyponyms, we suspect that our relative success was due to our very large corpus and the use of more refined statistical measures for ranking the output.\n"
     ]
    }
   ],
   "source": [
    "# create a new df named data in case you mess it up\n",
    "data = df.copy()\n",
    "data['model_input'] = ''\n",
    "data['model_output'] = ''\n",
    "\n",
    "# inputs - body? abstract? - cts will be added later\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data['model_input'][i] = data['body_good_quality'][i].replace(\"\\n\", \" \")\n",
    "    \n",
    "# output  \n",
    "data['model_output'] = data['golden'] \n",
    "\n",
    "# sanity checks\n",
    "print(list(data))\n",
    "print(\"---------\")\n",
    "print(data['model_input'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 // size: 110 // secs to run: 0.003364086151123047\n",
      "1 // size: 446 // secs to run: 4106.343423128128\n",
      "2 // size: 135 // secs to run: 0.005137920379638672\n",
      "3 // size: 108 // secs to run: 0.003550291061401367\n",
      "4 // size: 55 // secs to run: 0.001893758773803711\n",
      "5 // size: 382 // secs to run: 68.22395896911621\n",
      "6 // size: 197 // secs to run: 0.007094860076904297\n",
      "7 // size: 84 // secs to run: 0.0030362606048583984\n",
      "8 // size: 103 // secs to run: 0.003571033477783203\n",
      "9 // size: 254 // secs to run: 0.008100748062133789\n",
      "10 // size: 91 // secs to run: 0.003047943115234375\n",
      "11 // size: 444 // secs to run: 134.18049883842468\n",
      "12 // size: 15 // secs to run: 0.0006730556488037109\n",
      "13 // size: 109 // secs to run: 0.002925872802734375\n",
      "14 // size: 0 // secs to run: 2.288818359375e-05\n",
      "15 // size: 0 // secs to run: 1.3828277587890625e-05\n",
      "16 // size: 273 // secs to run: 0.009342193603515625\n",
      "17 // size: 278 // secs to run: 0.01079869270324707\n",
      "18 // size: 147 // secs to run: 0.004688739776611328\n",
      "19 // size: 163 // secs to run: 0.004824161529541016\n",
      "20 // size: 103 // secs to run: 0.0030410289764404297\n",
      "21 // size: 0 // secs to run: 2.7894973754882812e-05\n",
      "22 // size: 134 // secs to run: 0.0035822391510009766\n",
      "23 // size: 128 // secs to run: 0.003631114959716797\n",
      "24 // size: 150 // secs to run: 0.004850864410400391\n",
      "25 // size: 86 // secs to run: 0.002569437026977539\n",
      "26 // size: 40 // secs to run: 0.0012249946594238281\n",
      "27 // size: 64 // secs to run: 0.0020341873168945312\n",
      "28 // size: 102 // secs to run: 0.0028121471405029297\n",
      "29 // size: 72 // secs to run: 0.002090930938720703\n",
      "30 // size: 91 // secs to run: 0.002961874008178711\n",
      "31 // size: 103 // secs to run: 0.003442049026489258\n",
      "32 // size: 0 // secs to run: 3.910064697265625e-05\n",
      "33 // size: 444 // secs to run: 121.84097003936768\n",
      "34 // size: 86 // secs to run: 0.0030269622802734375\n",
      "35 // size: 0 // secs to run: 2.6702880859375e-05\n",
      "36 // size: 74 // secs to run: 0.0031042098999023438\n",
      "37 // size: 118 // secs to run: 0.004500150680541992\n",
      "38 // size: 201 // secs to run: 0.011021852493286133\n",
      "39 // size: 178 // secs to run: 0.00820302963256836\n",
      "40 // size: 78 // secs to run: 0.0034308433532714844\n",
      "41 // size: 91 // secs to run: 0.003690004348754883\n",
      "42 // size: 235 // secs to run: 0.010050058364868164\n",
      "43 // size: 140 // secs to run: 0.006577968597412109\n",
      "44 // size: 130 // secs to run: 0.006354093551635742\n",
      "45 // size: 140 // secs to run: 0.006908893585205078\n",
      "46 // size: 56 // secs to run: 0.002457857131958008\n",
      "47 // size: 155 // secs to run: 0.00774383544921875\n",
      "48 // size: 142 // secs to run: 0.006810903549194336\n",
      "49 // size: 136 // secs to run: 0.00621795654296875\n",
      "50 // size: 327 // secs to run: 54.00212001800537\n",
      "51 // size: 55 // secs to run: 0.0017747879028320312\n",
      "52 // size: 143 // secs to run: 0.004683971405029297\n",
      "53 // size: 166 // secs to run: 0.0052258968353271484\n",
      "54 // size: 120 // secs to run: 0.004118919372558594\n",
      "55 // size: 132 // secs to run: 0.0041942596435546875\n",
      "56 // size: 165 // secs to run: 0.004662036895751953\n",
      "57 // size: 145 // secs to run: 0.0045871734619140625\n",
      "58 // size: 115 // secs to run: 0.003340005874633789\n",
      "59 // size: 92 // secs to run: 0.0030257701873779297\n",
      "60 // size: 166 // secs to run: 0.005618095397949219\n",
      "61 // size: 144 // secs to run: 0.0044460296630859375\n",
      "62 // size: 19 // secs to run: 0.0005860328674316406\n",
      "63 // size: 112 // secs to run: 0.003662109375\n",
      "64 // size: 114 // secs to run: 0.003820180892944336\n",
      "65 // size: 131 // secs to run: 0.0040891170501708984\n",
      "66 // size: 87 // secs to run: 0.0031728744506835938\n",
      "67 // size: 106 // secs to run: 0.0037550926208496094\n",
      "68 // size: 156 // secs to run: 0.004782915115356445\n",
      "69 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "70 // size: 143 // secs to run: 0.004450798034667969\n",
      "71 // size: 87 // secs to run: 0.0029408931732177734\n",
      "72 // size: 94 // secs to run: 0.003241300582885742\n",
      "73 // size: 139 // secs to run: 0.004122257232666016\n",
      "74 // size: 92 // secs to run: 0.0029840469360351562\n",
      "75 // size: 121 // secs to run: 0.003902912139892578\n",
      "76 // size: 258 // secs to run: 0.009016990661621094\n",
      "77 // size: 127 // secs to run: 0.003905057907104492\n",
      "78 // size: 113 // secs to run: 0.003152132034301758\n",
      "79 // size: 6 // secs to run: 0.00020194053649902344\n",
      "80 // size: 151 // secs to run: 0.00446009635925293\n",
      "81 // size: 265 // secs to run: 0.010825872421264648\n",
      "82 // size: 86 // secs to run: 0.0025818347930908203\n",
      "83 // size: 95 // secs to run: 0.0031130313873291016\n",
      "84 // size: 127 // secs to run: 0.004250049591064453\n",
      "85 // size: 82 // secs to run: 0.0027611255645751953\n",
      "86 // size: 129 // secs to run: 0.004232883453369141\n",
      "87 // size: 142 // secs to run: 0.0042858123779296875\n",
      "88 // size: 111 // secs to run: 0.0033321380615234375\n",
      "89 // size: 0 // secs to run: 3.123283386230469e-05\n",
      "90 // size: 107 // secs to run: 0.0032041072845458984\n",
      "91 // size: 54 // secs to run: 0.001940011978149414\n",
      "92 // size: 170 // secs to run: 0.0046689510345458984\n",
      "93 // size: 223 // secs to run: 0.010022878646850586\n",
      "94 // size: 113 // secs to run: 0.0033969879150390625\n",
      "95 // size: 110 // secs to run: 0.003445148468017578\n",
      "96 // size: 236 // secs to run: 0.008875846862792969\n",
      "97 // size: 200 // secs to run: 0.005848884582519531\n",
      "98 // size: 126 // secs to run: 0.0038847923278808594\n",
      "99 // size: 124 // secs to run: 0.004006862640380859\n",
      "100 // size: 84 // secs to run: 0.002808094024658203\n",
      "101 // size: 392 // secs to run: 63.74543285369873\n",
      "102 // size: 63 // secs to run: 0.0020868778228759766\n",
      "103 // size: 97 // secs to run: 0.0028672218322753906\n",
      "104 // size: 171 // secs to run: 0.004463911056518555\n",
      "105 // size: 208 // secs to run: 0.005254983901977539\n",
      "106 // size: 94 // secs to run: 0.002743959426879883\n",
      "107 // size: 137 // secs to run: 0.003899812698364258\n",
      "108 // size: 11 // secs to run: 0.00036787986755371094\n",
      "109 // size: 125 // secs to run: 0.00420689582824707\n",
      "110 // size: 143 // secs to run: 0.004068136215209961\n",
      "111 // size: 175 // secs to run: 0.004683971405029297\n",
      "112 // size: 109 // secs to run: 0.002961874008178711\n",
      "113 // size: 123 // secs to run: 0.003966093063354492\n",
      "114 // size: 110 // secs to run: 0.0033338069915771484\n",
      "115 // size: 94 // secs to run: 0.002768993377685547\n",
      "116 // size: 128 // secs to run: 0.0049800872802734375\n",
      "117 // size: 101 // secs to run: 0.0031998157501220703\n",
      "118 // size: 127 // secs to run: 0.004317283630371094\n",
      "119 // size: 112 // secs to run: 0.004025936126708984\n",
      "120 // size: 98 // secs to run: 0.0027887821197509766\n",
      "121 // size: 132 // secs to run: 0.00444483757019043\n",
      "122 // size: 93 // secs to run: 0.0029480457305908203\n",
      "123 // size: 114 // secs to run: 0.0032989978790283203\n",
      "124 // size: 0 // secs to run: 1.9311904907226562e-05\n",
      "125 // size: 165 // secs to run: 0.004557132720947266\n",
      "126 // size: 160 // secs to run: 0.005545854568481445\n",
      "127 // size: 119 // secs to run: 0.003314971923828125\n",
      "128 // size: 190 // secs to run: 0.006826162338256836\n",
      "129 // size: 297 // secs to run: 0.01081705093383789\n",
      "130 // size: 104 // secs to run: 0.003256082534790039\n",
      "131 // size: 90 // secs to run: 0.002397775650024414\n",
      "132 // size: 40 // secs to run: 0.0010650157928466797\n",
      "133 // size: 107 // secs to run: 0.0027360916137695312\n",
      "134 // size: 66 // secs to run: 0.0019538402557373047\n",
      "135 // size: 112 // secs to run: 0.0033540725708007812\n",
      "136 // size: 0 // secs to run: 1.9788742065429688e-05\n",
      "137 // size: 88 // secs to run: 0.003290891647338867\n",
      "138 // size: 0 // secs to run: 2.1696090698242188e-05\n",
      "139 // size: 0 // secs to run: 1.52587890625e-05\n",
      "140 // size: 130 // secs to run: 0.0037178993225097656\n",
      "141 // size: 114 // secs to run: 0.0034148693084716797\n",
      "142 // size: 73 // secs to run: 0.002130270004272461\n",
      "143 // size: 151 // secs to run: 0.004700183868408203\n",
      "144 // size: 161 // secs to run: 0.004817962646484375\n",
      "145 // size: 130 // secs to run: 0.004310131072998047\n",
      "146 // size: 131 // secs to run: 0.003721952438354492\n",
      "147 // size: 115 // secs to run: 0.002833127975463867\n",
      "148 // size: 148 // secs to run: 0.00399327278137207\n",
      "149 // size: 113 // secs to run: 0.003184795379638672\n",
      "150 // size: 152 // secs to run: 0.004356861114501953\n",
      "151 // size: 91 // secs to run: 0.0038709640502929688\n",
      "152 // size: 87 // secs to run: 0.0036749839782714844\n",
      "153 // size: 87 // secs to run: 0.0037038326263427734\n",
      "154 // size: 77 // secs to run: 0.002360105514526367\n",
      "155 // size: 114 // secs to run: 0.0033218860626220703\n",
      "156 // size: 50 // secs to run: 0.0016350746154785156\n",
      "157 // size: 146 // secs to run: 0.0044329166412353516\n",
      "158 // size: 106 // secs to run: 0.003905057907104492\n",
      "159 // size: 100 // secs to run: 0.0033800601959228516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 // size: 312 // secs to run: 51.64738893508911\n",
      "161 // size: 115 // secs to run: 0.005026817321777344\n",
      "162 // size: 9 // secs to run: 0.00030994415283203125\n",
      "163 // size: 159 // secs to run: 0.0037841796875\n",
      "164 // size: 14 // secs to run: 0.0004508495330810547\n",
      "165 // size: 96 // secs to run: 0.0031228065490722656\n",
      "166 // size: 122 // secs to run: 0.0037109851837158203\n",
      "167 // size: 119 // secs to run: 0.0035698413848876953\n",
      "168 // size: 89 // secs to run: 0.0027518272399902344\n",
      "169 // size: 116 // secs to run: 0.003535747528076172\n",
      "170 // size: 185 // secs to run: 0.005162954330444336\n",
      "171 // size: 193 // secs to run: 0.005425930023193359\n",
      "172 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "173 // size: 132 // secs to run: 0.003329038619995117\n",
      "174 // size: 128 // secs to run: 0.004154682159423828\n",
      "175 // size: 294 // secs to run: 0.010671138763427734\n",
      "176 // size: 130 // secs to run: 0.003632783889770508\n",
      "177 // size: 262 // secs to run: 0.01164102554321289\n",
      "178 // size: 120 // secs to run: 0.003242969512939453\n",
      "179 // size: 136 // secs to run: 0.003898143768310547\n",
      "180 // size: 138 // secs to run: 0.003946065902709961\n",
      "181 // size: 80 // secs to run: 0.002115011215209961\n",
      "182 // size: 94 // secs to run: 0.002359628677368164\n",
      "183 // size: 0 // secs to run: 1.9311904907226562e-05\n",
      "184 // size: 109 // secs to run: 0.0030679702758789062\n",
      "185 // size: 481 // secs to run: 104.75692486763\n",
      "186 // size: 122 // secs to run: 0.0039370059967041016\n",
      "187 // size: 100 // secs to run: 0.003081083297729492\n",
      "188 // size: 323 // secs to run: 62.736976861953735\n",
      "189 // size: 220 // secs to run: 0.0074498653411865234\n",
      "190 // size: 114 // secs to run: 0.0036079883575439453\n",
      "191 // size: 124 // secs to run: 0.004088163375854492\n",
      "192 // size: 101 // secs to run: 0.0030939579010009766\n",
      "193 // size: 119 // secs to run: 0.004333972930908203\n",
      "194 // size: 171 // secs to run: 0.005143165588378906\n",
      "195 // size: 63 // secs to run: 0.0020279884338378906\n",
      "196 // size: 119 // secs to run: 0.004029989242553711\n",
      "197 // size: 43 // secs to run: 0.001178741455078125\n",
      "198 // size: 72 // secs to run: 0.0024759769439697266\n",
      "199 // size: 358 // secs to run: 82.36857914924622\n",
      "200 // size: 221 // secs to run: 0.007245063781738281\n",
      "201 // size: 0 // secs to run: 5.2928924560546875e-05\n",
      "202 // size: 126 // secs to run: 0.003990888595581055\n",
      "203 // size: 164 // secs to run: 0.004730224609375\n",
      "204 // size: 103 // secs to run: 0.003240823745727539\n",
      "205 // size: 86 // secs to run: 0.002933979034423828\n",
      "206 // size: 20 // secs to run: 0.0005598068237304688\n",
      "207 // size: 133 // secs to run: 0.0038301944732666016\n",
      "208 // size: 51 // secs to run: 0.0015590190887451172\n",
      "209 // size: 168 // secs to run: 0.006234884262084961\n",
      "210 // size: 45 // secs to run: 0.0013370513916015625\n",
      "211 // size: 115 // secs to run: 0.003910064697265625\n",
      "212 // size: 114 // secs to run: 0.0037670135498046875\n",
      "213 // size: 108 // secs to run: 0.00344085693359375\n",
      "214 // size: 207 // secs to run: 0.0068819522857666016\n",
      "215 // size: 109 // secs to run: 0.003679037094116211\n",
      "216 // size: 0 // secs to run: 4.029273986816406e-05\n",
      "217 // size: 0 // secs to run: 1.71661376953125e-05\n",
      "218 // size: 133 // secs to run: 0.005220890045166016\n",
      "219 // size: 41 // secs to run: 0.0014147758483886719\n",
      "220 // size: 108 // secs to run: 0.003988027572631836\n",
      "221 // size: 158 // secs to run: 0.004902839660644531\n",
      "222 // size: 128 // secs to run: 0.004841804504394531\n",
      "223 // size: 121 // secs to run: 0.004492044448852539\n",
      "224 // size: 346 // secs to run: 48.816832065582275\n",
      "225 // size: 133 // secs to run: 0.003734111785888672\n",
      "226 // size: 147 // secs to run: 0.005315065383911133\n",
      "227 // size: 102 // secs to run: 0.0030541419982910156\n",
      "228 // size: 137 // secs to run: 0.0045070648193359375\n",
      "229 // size: 109 // secs to run: 0.0031960010528564453\n",
      "230 // size: 82 // secs to run: 0.002307891845703125\n",
      "231 // size: 96 // secs to run: 0.003470897674560547\n",
      "232 // size: 89 // secs to run: 0.002772808074951172\n",
      "233 // size: 79 // secs to run: 0.0026731491088867188\n",
      "234 // size: 122 // secs to run: 0.0039577484130859375\n",
      "235 // size: 270 // secs to run: 0.008795976638793945\n",
      "236 // size: 112 // secs to run: 0.004368782043457031\n",
      "237 // size: 127 // secs to run: 0.003496885299682617\n",
      "238 // size: 120 // secs to run: 0.0036292076110839844\n",
      "239 // size: 315 // secs to run: 65.79947304725647\n",
      "240 // size: 143 // secs to run: 0.00432896614074707\n",
      "241 // size: 174 // secs to run: 0.005029201507568359\n",
      "242 // size: 69 // secs to run: 0.0022368431091308594\n",
      "243 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "244 // size: 88 // secs to run: 0.0027298927307128906\n",
      "245 // size: 152 // secs to run: 0.003972053527832031\n",
      "246 // size: 53 // secs to run: 0.0018248558044433594\n",
      "247 // size: 315 // secs to run: 64.82581806182861\n",
      "248 // size: 91 // secs to run: 0.0032501220703125\n",
      "249 // size: 79 // secs to run: 0.002237081527709961\n",
      "250 // size: 141 // secs to run: 0.0037457942962646484\n",
      "251 // size: 106 // secs to run: 0.002774953842163086\n",
      "252 // size: 104 // secs to run: 0.002789020538330078\n",
      "253 // size: 0 // secs to run: 1.7881393432617188e-05\n",
      "254 // size: 107 // secs to run: 0.0031490325927734375\n",
      "255 // size: 115 // secs to run: 0.0037178993225097656\n",
      "256 // size: 134 // secs to run: 0.004288911819458008\n",
      "257 // size: 30 // secs to run: 0.0010287761688232422\n",
      "258 // size: 180 // secs to run: 0.00522303581237793\n",
      "259 // size: 345 // secs to run: 65.21686005592346\n",
      "260 // size: 79 // secs to run: 0.0035049915313720703\n",
      "261 // size: 119 // secs to run: 0.005468845367431641\n",
      "262 // size: 139 // secs to run: 0.006017923355102539\n",
      "263 // size: 209 // secs to run: 0.008596181869506836\n",
      "264 // size: 92 // secs to run: 0.00372314453125\n",
      "265 // size: 281 // secs to run: 0.013874292373657227\n",
      "266 // size: 102 // secs to run: 0.003961086273193359\n",
      "267 // size: 180 // secs to run: 0.00849294662475586\n",
      "268 // size: 74 // secs to run: 0.003228902816772461\n",
      "269 // size: 0 // secs to run: 5.2928924560546875e-05\n",
      "270 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "271 // size: 78 // secs to run: 0.003336191177368164\n",
      "272 // size: 108 // secs to run: 0.004194974899291992\n",
      "273 // size: 133 // secs to run: 0.004759073257446289\n",
      "274 // size: 161 // secs to run: 0.009097099304199219\n",
      "275 // size: 0 // secs to run: 6.67572021484375e-05\n",
      "276 // size: 97 // secs to run: 0.0036432743072509766\n",
      "277 // size: 143 // secs to run: 0.0066509246826171875\n",
      "278 // size: 359 // secs to run: 64.81764721870422\n",
      "279 // size: 87 // secs to run: 0.002897024154663086\n",
      "280 // size: 118 // secs to run: 0.003719806671142578\n",
      "281 // size: 262 // secs to run: 0.009252071380615234\n",
      "282 // size: 97 // secs to run: 0.003298044204711914\n",
      "283 // size: 58 // secs to run: 0.0016410350799560547\n",
      "284 // size: 52 // secs to run: 0.0014829635620117188\n",
      "285 // size: 173 // secs to run: 0.005296945571899414\n",
      "286 // size: 146 // secs to run: 0.0048389434814453125\n",
      "287 // size: 131 // secs to run: 0.004050731658935547\n",
      "288 // size: 57 // secs to run: 0.0017788410186767578\n",
      "289 // size: 279 // secs to run: 0.009122133255004883\n",
      "290 // size: 92 // secs to run: 0.003261089324951172\n",
      "291 // size: 437 // secs to run: 92.58390593528748\n",
      "292 // size: 37 // secs to run: 0.0013260841369628906\n",
      "293 // size: 108 // secs to run: 0.00335693359375\n",
      "294 // size: 117 // secs to run: 0.0038290023803710938\n",
      "295 // size: 131 // secs to run: 0.004461050033569336\n",
      "296 // size: 102 // secs to run: 0.003609895706176758\n",
      "297 // size: 183 // secs to run: 0.006167173385620117\n",
      "298 // size: 131 // secs to run: 0.004251956939697266\n",
      "299 // size: 85 // secs to run: 0.0028159618377685547\n",
      "300 // size: 116 // secs to run: 0.0037589073181152344\n",
      "301 // size: 83 // secs to run: 0.0029082298278808594\n",
      "302 // size: 141 // secs to run: 0.004180908203125\n",
      "303 // size: 155 // secs to run: 0.005023956298828125\n",
      "304 // size: 108 // secs to run: 0.003350973129272461\n",
      "305 // size: 117 // secs to run: 0.003932952880859375\n",
      "306 // size: 144 // secs to run: 0.0044782161712646484\n",
      "307 // size: 142 // secs to run: 0.004663944244384766\n",
      "308 // size: 135 // secs to run: 0.004473209381103516\n",
      "309 // size: 66 // secs to run: 0.002156972885131836\n",
      "310 // size: 155 // secs to run: 0.004681110382080078\n",
      "311 // size: 97 // secs to run: 0.0031890869140625\n",
      "312 // size: 8 // secs to run: 0.00030112266540527344\n",
      "313 // size: 98 // secs to run: 0.003058910369873047\n",
      "314 // size: 187 // secs to run: 0.006094694137573242\n",
      "315 // size: 287 // secs to run: 0.00970602035522461\n",
      "316 // size: 81 // secs to run: 0.0027451515197753906\n",
      "317 // size: 152 // secs to run: 0.0050051212310791016\n",
      "318 // size: 0 // secs to run: 2.5033950805664062e-05\n",
      "319 // size: 173 // secs to run: 0.006455183029174805\n",
      "320 // size: 38 // secs to run: 0.001322031021118164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321 // size: 314 // secs to run: 161.4649977684021\n",
      "322 // size: 101 // secs to run: 0.0030219554901123047\n",
      "323 // size: 79 // secs to run: 0.002783060073852539\n",
      "324 // size: 88 // secs to run: 0.0029082298278808594\n",
      "325 // size: 85 // secs to run: 0.0028579235076904297\n",
      "326 // size: 138 // secs to run: 0.004126071929931641\n",
      "327 // size: 35 // secs to run: 0.001039266586303711\n",
      "328 // size: 192 // secs to run: 0.006578922271728516\n",
      "329 // size: 0 // secs to run: 2.574920654296875e-05\n",
      "330 // size: 108 // secs to run: 0.003043651580810547\n",
      "331 // size: 186 // secs to run: 0.005055904388427734\n",
      "332 // size: 97 // secs to run: 0.002974987030029297\n",
      "333 // size: 112 // secs to run: 0.0030829906463623047\n",
      "334 // size: 101 // secs to run: 0.0030090808868408203\n",
      "335 // size: 385 // secs to run: 64.088054895401\n",
      "336 // size: 15 // secs to run: 0.00048422813415527344\n",
      "337 // size: 125 // secs to run: 0.003772258758544922\n",
      "338 // size: 107 // secs to run: 0.0034837722778320312\n",
      "339 // size: 134 // secs to run: 0.004178047180175781\n",
      "340 // size: 99 // secs to run: 0.0031843185424804688\n",
      "341 // size: 91 // secs to run: 0.003165006637573242\n",
      "342 // size: 97 // secs to run: 0.0035598278045654297\n",
      "343 // size: 272 // secs to run: 0.008252143859863281\n",
      "344 // size: 150 // secs to run: 0.0040781497955322266\n",
      "345 // size: 122 // secs to run: 0.003312826156616211\n",
      "346 // size: 200 // secs to run: 0.005192995071411133\n",
      "347 // size: 89 // secs to run: 0.0027730464935302734\n",
      "348 // size: 60 // secs to run: 0.0019919872283935547\n",
      "349 // size: 71 // secs to run: 0.002132892608642578\n",
      "350 // size: 110 // secs to run: 0.003384113311767578\n",
      "351 // size: 0 // secs to run: 1.9788742065429688e-05\n",
      "352 // size: 124 // secs to run: 0.0036468505859375\n",
      "353 // size: 102 // secs to run: 0.0035979747772216797\n",
      "354 // size: 87 // secs to run: 0.0026040077209472656\n",
      "355 // size: 148 // secs to run: 0.004700899124145508\n",
      "356 // size: 0 // secs to run: 2.3126602172851562e-05\n",
      "357 // size: 146 // secs to run: 0.00426793098449707\n",
      "358 // size: 109 // secs to run: 0.0035178661346435547\n",
      "359 // size: 68 // secs to run: 0.0023653507232666016\n",
      "360 // size: 111 // secs to run: 0.0034558773040771484\n",
      "361 // size: 55 // secs to run: 0.0015938282012939453\n",
      "362 // size: 146 // secs to run: 0.004125356674194336\n",
      "363 // size: 254 // secs to run: 0.008411884307861328\n",
      "364 // size: 190 // secs to run: 0.005324125289916992\n",
      "365 // size: 0 // secs to run: 5.3882598876953125e-05\n",
      "366 // size: 150 // secs to run: 0.004185199737548828\n",
      "367 // size: 215 // secs to run: 0.005892038345336914\n",
      "368 // size: 384 // secs to run: 77.38046598434448\n",
      "369 // size: 128 // secs to run: 0.003779172897338867\n",
      "370 // size: 596 // secs to run: 206.69489693641663\n",
      "371 // size: 147 // secs to run: 0.005299806594848633\n",
      "372 // size: 177 // secs to run: 0.005992889404296875\n",
      "373 // size: 140 // secs to run: 0.005122184753417969\n",
      "374 // size: 123 // secs to run: 0.003779888153076172\n",
      "375 // size: 147 // secs to run: 0.004803895950317383\n",
      "376 // size: 156 // secs to run: 0.0066068172454833984\n",
      "377 // size: 170 // secs to run: 0.005478858947753906\n",
      "378 // size: 143 // secs to run: 0.004468202590942383\n",
      "379 // size: 44 // secs to run: 0.0014760494232177734\n",
      "380 // size: 117 // secs to run: 0.003695964813232422\n",
      "381 // size: 225 // secs to run: 0.008517026901245117\n",
      "382 // size: 327 // secs to run: 65.75141501426697\n",
      "383 // size: 63 // secs to run: 0.0019412040710449219\n",
      "384 // size: 159 // secs to run: 0.004311084747314453\n",
      "385 // size: 136 // secs to run: 0.004382133483886719\n",
      "386 // size: 157 // secs to run: 0.004591941833496094\n",
      "387 // size: 116 // secs to run: 0.0035190582275390625\n",
      "388 // size: 120 // secs to run: 0.003933906555175781\n",
      "389 // size: 101 // secs to run: 0.003595113754272461\n",
      "390 // size: 81 // secs to run: 0.0024950504302978516\n",
      "391 // size: 147 // secs to run: 0.0042569637298583984\n",
      "392 // size: 64 // secs to run: 0.0019867420196533203\n",
      "393 // size: 137 // secs to run: 0.004483938217163086\n",
      "394 // size: 213 // secs to run: 0.005727052688598633\n",
      "395 // size: 142 // secs to run: 0.004397153854370117\n",
      "396 // size: 104 // secs to run: 0.0028159618377685547\n",
      "397 // size: 116 // secs to run: 0.0031502246856689453\n",
      "398 // size: 68 // secs to run: 0.0021059513092041016\n",
      "399 // size: 0 // secs to run: 1.7881393432617188e-05\n",
      "400 // size: 166 // secs to run: 0.004902839660644531\n",
      "401 // size: 125 // secs to run: 0.0036399364471435547\n",
      "402 // size: 100 // secs to run: 0.003014802932739258\n",
      "403 // size: 112 // secs to run: 0.004042863845825195\n",
      "404 // size: 156 // secs to run: 0.0042150020599365234\n",
      "405 // size: 166 // secs to run: 0.004152059555053711\n",
      "406 // size: 81 // secs to run: 0.002161741256713867\n",
      "407 // size: 83 // secs to run: 0.00214385986328125\n",
      "408 // size: 148 // secs to run: 0.003709077835083008\n",
      "409 // size: 190 // secs to run: 0.004848957061767578\n",
      "410 // size: 64 // secs to run: 0.0018270015716552734\n",
      "411 // size: 110 // secs to run: 0.0034089088439941406\n",
      "412 // size: 150 // secs to run: 0.0045282840728759766\n",
      "413 // size: 308 // secs to run: 45.0228910446167\n",
      "414 // size: 141 // secs to run: 0.004033088684082031\n",
      "415 // size: 314 // secs to run: 47.215234994888306\n",
      "416 // size: 9 // secs to run: 0.0003650188446044922\n",
      "417 // size: 0 // secs to run: 1.9073486328125e-05\n",
      "418 // size: 0 // secs to run: 1.5020370483398438e-05\n",
      "419 // size: 85 // secs to run: 0.002892017364501953\n",
      "420 // size: 82 // secs to run: 0.0029799938201904297\n",
      "421 // size: 128 // secs to run: 0.003629922866821289\n",
      "422 // size: 258 // secs to run: 0.00832509994506836\n",
      "423 // size: 60 // secs to run: 0.0020411014556884766\n",
      "424 // size: 143 // secs to run: 0.004609823226928711\n",
      "425 // size: 125 // secs to run: 0.00403285026550293\n",
      "426 // size: 93 // secs to run: 0.0026330947875976562\n",
      "427 // size: 0 // secs to run: 2.1219253540039062e-05\n",
      "428 // size: 215 // secs to run: 0.008843183517456055\n",
      "429 // size: 240 // secs to run: 0.0075833797454833984\n",
      "430 // size: 89 // secs to run: 0.0027136802673339844\n",
      "431 // size: 471 // secs to run: 90.09282207489014\n",
      "432 // size: 41 // secs to run: 0.0014939308166503906\n",
      "433 // size: 65 // secs to run: 0.001699209213256836\n",
      "434 // size: 102 // secs to run: 0.003077268600463867\n",
      "435 // size: 93 // secs to run: 0.0026519298553466797\n",
      "436 // size: 109 // secs to run: 0.003923892974853516\n",
      "437 // size: 394 // secs to run: 71.09720969200134\n",
      "438 // size: 555 // secs to run: 205.84522104263306\n",
      "439 // size: 117 // secs to run: 0.0038950443267822266\n",
      "440 // size: 151 // secs to run: 0.004441022872924805\n",
      "441 // size: 66 // secs to run: 0.002192974090576172\n",
      "442 // size: 155 // secs to run: 0.004724025726318359\n",
      "443 // size: 81 // secs to run: 0.002669811248779297\n",
      "444 // size: 149 // secs to run: 0.0049228668212890625\n",
      "445 // size: 104 // secs to run: 0.0033071041107177734\n",
      "446 // size: 158 // secs to run: 0.00559687614440918\n",
      "447 // size: 87 // secs to run: 0.002918243408203125\n",
      "448 // size: 97 // secs to run: 0.003126859664916992\n",
      "449 // size: 427 // secs to run: 108.14143896102905\n",
      "450 // size: 91 // secs to run: 0.003244161605834961\n",
      "451 // size: 128 // secs to run: 0.004151105880737305\n",
      "452 // size: 62 // secs to run: 0.002106904983520508\n",
      "453 // size: 162 // secs to run: 0.006227970123291016\n",
      "454 // size: 62 // secs to run: 0.0024938583374023438\n",
      "455 // size: 0 // secs to run: 2.47955322265625e-05\n",
      "456 // size: 187 // secs to run: 0.00613093376159668\n",
      "457 // size: 153 // secs to run: 0.004713296890258789\n",
      "458 // size: 117 // secs to run: 0.0036957263946533203\n",
      "459 // size: 65 // secs to run: 0.0019261837005615234\n",
      "460 // size: 232 // secs to run: 0.007709980010986328\n",
      "461 // size: 122 // secs to run: 0.004096031188964844\n",
      "462 // size: 170 // secs to run: 0.004537105560302734\n",
      "463 // size: 85 // secs to run: 0.0032291412353515625\n",
      "464 // size: 144 // secs to run: 0.004534006118774414\n",
      "465 // size: 98 // secs to run: 0.0033690929412841797\n",
      "466 // size: 104 // secs to run: 0.003274202346801758\n",
      "467 // size: 138 // secs to run: 0.00432276725769043\n",
      "468 // size: 150 // secs to run: 0.004482746124267578\n",
      "469 // size: 112 // secs to run: 0.0036537647247314453\n",
      "470 // size: 163 // secs to run: 0.0048100948333740234\n",
      "471 // size: 123 // secs to run: 0.004255056381225586\n",
      "472 // size: 145 // secs to run: 0.00450897216796875\n",
      "473 // size: 111 // secs to run: 0.003525972366333008\n",
      "474 // size: 100 // secs to run: 0.00319671630859375\n",
      "475 // size: 188 // secs to run: 0.0060541629791259766\n",
      "476 // size: 111 // secs to run: 0.003468036651611328\n",
      "477 // size: 66 // secs to run: 0.002154111862182617\n",
      "478 // size: 140 // secs to run: 0.0043201446533203125\n",
      "479 // size: 94 // secs to run: 0.0031707286834716797\n",
      "480 // size: 113 // secs to run: 0.003589153289794922\n",
      "481 // size: 104 // secs to run: 0.0031816959381103516\n",
      "482 // size: 125 // secs to run: 0.0038073062896728516\n",
      "483 // size: 96 // secs to run: 0.002955198287963867\n",
      "484 // size: 89 // secs to run: 0.0025670528411865234\n",
      "485 // size: 126 // secs to run: 0.003743886947631836\n",
      "486 // size: 119 // secs to run: 0.003706216812133789\n",
      "487 // size: 122 // secs to run: 0.0037860870361328125\n",
      "488 // size: 171 // secs to run: 0.0050411224365234375\n",
      "489 // size: 141 // secs to run: 0.003971099853515625\n",
      "490 // size: 144 // secs to run: 0.004486083984375\n",
      "491 // size: 105 // secs to run: 0.0038650035858154297\n",
      "492 // size: 95 // secs to run: 0.002809762954711914\n",
      "493 // size: 155 // secs to run: 0.0044748783111572266\n",
      "494 // size: 95 // secs to run: 0.0027420520782470703\n",
      "495 // size: 157 // secs to run: 0.0050199031829833984\n",
      "496 // size: 118 // secs to run: 0.003670930862426758\n",
      "497 // size: 229 // secs to run: 0.008385896682739258\n",
      "498 // size: 84 // secs to run: 0.0029549598693847656\n",
      "499 // size: 123 // secs to run: 0.0036499500274658203\n",
      "500 // size: 93 // secs to run: 0.0030241012573242188\n",
      "501 // size: 0 // secs to run: 2.3126602172851562e-05\n",
      "502 // size: 0 // secs to run: 1.5020370483398438e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 // size: 396 // secs to run: 84.31925773620605\n",
      "504 // size: 171 // secs to run: 0.006103992462158203\n",
      "505 // size: 99 // secs to run: 0.003542184829711914\n",
      "506 // size: 120 // secs to run: 0.004431962966918945\n",
      "507 // size: 113 // secs to run: 0.0037679672241210938\n",
      "508 // size: 124 // secs to run: 0.004384040832519531\n",
      "509 // size: 128 // secs to run: 0.004082918167114258\n",
      "510 // size: 135 // secs to run: 0.0046350955963134766\n",
      "511 // size: 269 // secs to run: 0.009486198425292969\n",
      "512 // size: 86 // secs to run: 0.0026602745056152344\n",
      "513 // size: 71 // secs to run: 0.001986980438232422\n",
      "514 // size: 102 // secs to run: 0.003248929977416992\n",
      "515 // size: 102 // secs to run: 0.003414154052734375\n",
      "516 // size: 357 // secs to run: 73.16676473617554\n",
      "517 // size: 154 // secs to run: 0.004732847213745117\n",
      "518 // size: 153 // secs to run: 0.00488591194152832\n",
      "519 // size: 82 // secs to run: 0.002702951431274414\n",
      "520 // size: 96 // secs to run: 0.0031709671020507812\n",
      "521 // size: 101 // secs to run: 0.0031108856201171875\n",
      "522 // size: 168 // secs to run: 0.0048449039459228516\n",
      "523 // size: 121 // secs to run: 0.0038819313049316406\n",
      "524 // size: 249 // secs to run: 0.008919000625610352\n",
      "525 // size: 175 // secs to run: 0.005110979080200195\n",
      "526 // size: 414 // secs to run: 87.57382297515869\n",
      "527 // size: 80 // secs to run: 0.003113985061645508\n",
      "528 // size: 107 // secs to run: 0.003863096237182617\n",
      "529 // size: 147 // secs to run: 0.004664897918701172\n",
      "530 // size: 149 // secs to run: 0.005196094512939453\n",
      "531 // size: 76 // secs to run: 0.0028421878814697266\n",
      "532 // size: 105 // secs to run: 0.003837108612060547\n",
      "533 // size: 150 // secs to run: 0.005369901657104492\n",
      "534 // size: 0 // secs to run: 2.384185791015625e-05\n",
      "535 // size: 68 // secs to run: 0.0022041797637939453\n",
      "536 // size: 109 // secs to run: 0.003729104995727539\n",
      "537 // size: 163 // secs to run: 0.00527191162109375\n",
      "538 // size: 139 // secs to run: 0.0045566558837890625\n",
      "539 // size: 128 // secs to run: 0.004115104675292969\n",
      "540 // size: 155 // secs to run: 0.00475001335144043\n",
      "541 // size: 181 // secs to run: 0.006597042083740234\n",
      "542 // size: 185 // secs to run: 0.006803035736083984\n",
      "543 // size: 184 // secs to run: 0.005588054656982422\n",
      "544 // size: 130 // secs to run: 0.004314899444580078\n",
      "545 // size: 127 // secs to run: 0.003945112228393555\n",
      "546 // size: 100 // secs to run: 0.0034499168395996094\n",
      "547 // size: 107 // secs to run: 0.004110097885131836\n",
      "548 // size: 117 // secs to run: 0.0034637451171875\n",
      "549 // size: 44 // secs to run: 0.0014901161193847656\n",
      "550 // size: 46 // secs to run: 0.0014050006866455078\n",
      "551 // size: 106 // secs to run: 0.003596067428588867\n",
      "552 // size: 161 // secs to run: 0.0050580501556396484\n",
      "553 // size: 147 // secs to run: 0.004821062088012695\n",
      "554 // size: 0 // secs to run: 2.47955322265625e-05\n",
      "555 // size: 74 // secs to run: 0.0024302005767822266\n",
      "556 // size: 121 // secs to run: 0.004034996032714844\n",
      "557 // size: 0 // secs to run: 4.1961669921875e-05\n",
      "558 // size: 172 // secs to run: 0.005333423614501953\n",
      "559 // size: 142 // secs to run: 0.004418134689331055\n",
      "560 // size: 102 // secs to run: 0.0036139488220214844\n",
      "561 // size: 91 // secs to run: 0.003031015396118164\n",
      "562 // size: 0 // secs to run: 8.20159912109375e-05\n",
      "563 // size: 97 // secs to run: 0.004229068756103516\n",
      "564 // size: 170 // secs to run: 0.006749153137207031\n",
      "565 // size: 132 // secs to run: 0.005151987075805664\n",
      "566 // size: 128 // secs to run: 0.004326343536376953\n",
      "567 // size: 160 // secs to run: 0.0066051483154296875\n",
      "568 // size: 112 // secs to run: 0.003942966461181641\n",
      "569 // size: 131 // secs to run: 0.004587888717651367\n",
      "570 // size: 156 // secs to run: 0.006665945053100586\n",
      "571 // size: 0 // secs to run: 5.1975250244140625e-05\n",
      "572 // size: 50 // secs to run: 0.0018291473388671875\n",
      "573 // size: 55 // secs to run: 0.0020132064819335938\n",
      "574 // size: 119 // secs to run: 0.004342079162597656\n",
      "575 // size: 73 // secs to run: 0.0028870105743408203\n",
      "576 // size: 95 // secs to run: 0.003381013870239258\n",
      "577 // size: 93 // secs to run: 0.0032629966735839844\n",
      "578 // size: 128 // secs to run: 0.004128932952880859\n",
      "579 // size: 82 // secs to run: 0.0027458667755126953\n",
      "580 // size: 157 // secs to run: 0.0056629180908203125\n",
      "581 // size: 134 // secs to run: 0.004465818405151367\n",
      "582 // size: 161 // secs to run: 0.004857063293457031\n",
      "583 // size: 213 // secs to run: 0.008981943130493164\n",
      "584 // size: 267 // secs to run: 0.013057947158813477\n",
      "585 // size: 97 // secs to run: 0.0037009716033935547\n",
      "586 // size: 439 // secs to run: 97.72446727752686\n",
      "587 // size: 147 // secs to run: 0.0057220458984375\n",
      "588 // size: 94 // secs to run: 0.0044329166412353516\n",
      "589 // size: 65 // secs to run: 0.002725839614868164\n",
      "590 // size: 136 // secs to run: 0.006173849105834961\n",
      "591 // size: 175 // secs to run: 0.008540153503417969\n",
      "592 // size: 56 // secs to run: 0.002521991729736328\n",
      "593 // size: 103 // secs to run: 0.004766941070556641\n",
      "594 // size: 132 // secs to run: 0.0065460205078125\n",
      "595 // size: 79 // secs to run: 0.0038809776306152344\n",
      "596 // size: 109 // secs to run: 0.005103111267089844\n",
      "597 // size: 129 // secs to run: 0.0057599544525146484\n",
      "598 // size: 162 // secs to run: 0.00709986686706543\n",
      "599 // size: 0 // secs to run: 4.887580871582031e-05\n",
      "600 // size: 0 // secs to run: 2.09808349609375e-05\n",
      "601 // size: 133 // secs to run: 0.004732847213745117\n",
      "602 // size: 85 // secs to run: 0.0032150745391845703\n",
      "603 // size: 314 // secs to run: 48.01888704299927\n",
      "604 // size: 131 // secs to run: 0.004451751708984375\n",
      "605 // size: 137 // secs to run: 0.004600048065185547\n",
      "606 // size: 104 // secs to run: 0.0032520294189453125\n",
      "607 // size: 163 // secs to run: 0.005811929702758789\n",
      "608 // size: 101 // secs to run: 0.0037240982055664062\n",
      "609 // size: 20 // secs to run: 0.00074005126953125\n",
      "610 // size: 130 // secs to run: 0.003959178924560547\n",
      "611 // size: 49 // secs to run: 0.0017709732055664062\n",
      "612 // size: 120 // secs to run: 0.0038919448852539062\n",
      "613 // size: 145 // secs to run: 0.0048389434814453125\n",
      "614 // size: 135 // secs to run: 0.005034923553466797\n",
      "615 // size: 159 // secs to run: 0.005402088165283203\n",
      "616 // size: 117 // secs to run: 0.0038728713989257812\n",
      "617 // size: 135 // secs to run: 0.004204988479614258\n",
      "618 // size: 153 // secs to run: 0.005502939224243164\n",
      "619 // size: 77 // secs to run: 0.002744913101196289\n",
      "620 // size: 110 // secs to run: 0.004311084747314453\n",
      "621 // size: 113 // secs to run: 0.004611968994140625\n",
      "622 // size: 163 // secs to run: 0.005669116973876953\n",
      "623 // size: 110 // secs to run: 0.0040111541748046875\n",
      "624 // size: 98 // secs to run: 0.0037391185760498047\n",
      "625 // size: 111 // secs to run: 0.003859996795654297\n",
      "626 // size: 110 // secs to run: 0.004166126251220703\n",
      "627 // size: 87 // secs to run: 0.003509998321533203\n",
      "628 // size: 0 // secs to run: 2.6941299438476562e-05\n",
      "629 // size: 102 // secs to run: 0.003428936004638672\n",
      "630 // size: 138 // secs to run: 0.004225015640258789\n",
      "631 // size: 204 // secs to run: 0.009254932403564453\n",
      "632 // size: 105 // secs to run: 0.0035042762756347656\n",
      "633 // size: 303 // secs to run: 48.22198987007141\n",
      "634 // size: 107 // secs to run: 0.0041239261627197266\n",
      "635 // size: 122 // secs to run: 0.004162788391113281\n",
      "636 // size: 112 // secs to run: 0.0038809776306152344\n",
      "637 // size: 276 // secs to run: 0.010542154312133789\n",
      "638 // size: 104 // secs to run: 0.003760099411010742\n",
      "639 // size: 70 // secs to run: 0.002656698226928711\n",
      "640 // size: 400 // secs to run: 70.54875588417053\n",
      "641 // size: 108 // secs to run: 0.004309892654418945\n",
      "642 // size: 47 // secs to run: 0.001795053482055664\n",
      "643 // size: 311 // secs to run: 44.43792009353638\n",
      "644 // size: 104 // secs to run: 0.0029468536376953125\n",
      "645 // size: 0 // secs to run: 2.002716064453125e-05\n",
      "646 // size: 152 // secs to run: 0.0042629241943359375\n",
      "647 // size: 94 // secs to run: 0.004309177398681641\n",
      "648 // size: 100 // secs to run: 0.0037152767181396484\n",
      "649 // size: 65 // secs to run: 0.00214385986328125\n",
      "650 // size: 61 // secs to run: 0.0022127628326416016\n",
      "651 // size: 20 // secs to run: 0.0005941390991210938\n",
      "652 // size: 376 // secs to run: 87.61375617980957\n",
      "653 // size: 0 // secs to run: 4.124641418457031e-05\n",
      "654 // size: 52 // secs to run: 0.0018589496612548828\n",
      "655 // size: 176 // secs to run: 0.0053288936614990234\n",
      "656 // size: 183 // secs to run: 0.005605936050415039\n",
      "657 // size: 152 // secs to run: 0.0054399967193603516\n",
      "658 // size: 124 // secs to run: 0.004110097885131836\n",
      "659 // size: 165 // secs to run: 0.00505375862121582\n",
      "660 // size: 100 // secs to run: 0.0035469532012939453\n",
      "661 // size: 133 // secs to run: 0.004801034927368164\n",
      "662 // size: 112 // secs to run: 0.003705263137817383\n",
      "663 // size: 158 // secs to run: 0.004853963851928711\n",
      "664 // size: 101 // secs to run: 0.0035469532012939453\n",
      "665 // size: 0 // secs to run: 2.9802322387695312e-05\n",
      "666 // size: 86 // secs to run: 0.002456188201904297\n",
      "667 // size: 0 // secs to run: 3.600120544433594e-05\n",
      "668 // size: 100 // secs to run: 0.0035524368286132812\n",
      "669 // size: 112 // secs to run: 0.003214120864868164\n",
      "670 // size: 114 // secs to run: 0.004094839096069336\n",
      "671 // size: 64 // secs to run: 0.0019121170043945312\n",
      "672 // size: 59 // secs to run: 0.0018329620361328125\n",
      "673 // size: 79 // secs to run: 0.0026578903198242188\n",
      "674 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "675 // size: 0 // secs to run: 1.5020370483398438e-05\n",
      "676 // size: 141 // secs to run: 0.004292011260986328\n",
      "677 // size: 73 // secs to run: 0.002549886703491211\n",
      "678 // size: 112 // secs to run: 0.0038118362426757812\n",
      "679 // size: 78 // secs to run: 0.0027608871459960938\n",
      "680 // size: 71 // secs to run: 0.0019690990447998047\n",
      "681 // size: 121 // secs to run: 0.0038259029388427734\n",
      "682 // size: 106 // secs to run: 0.003513813018798828\n",
      "683 // size: 110 // secs to run: 0.0037789344787597656\n",
      "684 // size: 0 // secs to run: 3.886222839355469e-05\n",
      "685 // size: 151 // secs to run: 0.005265951156616211\n",
      "686 // size: 92 // secs to run: 0.0029761791229248047\n",
      "687 // size: 0 // secs to run: 3.910064697265625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688 // size: 469 // secs to run: 115.26401686668396\n",
      "689 // size: 123 // secs to run: 0.0036149024963378906\n",
      "690 // size: 210 // secs to run: 0.007405996322631836\n",
      "691 // size: 105 // secs to run: 0.003490924835205078\n",
      "692 // size: 84 // secs to run: 0.002892017364501953\n",
      "693 // size: 92 // secs to run: 0.0031201839447021484\n",
      "694 // size: 117 // secs to run: 0.003933906555175781\n",
      "695 // size: 116 // secs to run: 0.004125118255615234\n",
      "696 // size: 0 // secs to run: 2.4080276489257812e-05\n",
      "697 // size: 69 // secs to run: 0.002107858657836914\n",
      "698 // size: 141 // secs to run: 0.004855155944824219\n",
      "699 // size: 170 // secs to run: 0.005795001983642578\n",
      "700 // size: 0 // secs to run: 3.1948089599609375e-05\n",
      "701 // size: 59 // secs to run: 0.002192258834838867\n",
      "702 // size: 101 // secs to run: 0.0032720565795898438\n",
      "703 // size: 121 // secs to run: 0.004426002502441406\n",
      "704 // size: 100 // secs to run: 0.0032660961151123047\n",
      "705 // size: 138 // secs to run: 0.0044171810150146484\n",
      "706 // size: 239 // secs to run: 0.010162115097045898\n",
      "707 // size: 66 // secs to run: 0.002817869186401367\n",
      "708 // size: 109 // secs to run: 0.003696918487548828\n",
      "709 // size: 156 // secs to run: 0.004774808883666992\n",
      "710 // size: 120 // secs to run: 0.0037009716033935547\n",
      "711 // size: 0 // secs to run: 2.09808349609375e-05\n",
      "712 // size: 79 // secs to run: 0.0027468204498291016\n",
      "713 // size: 116 // secs to run: 0.0038039684295654297\n",
      "714 // size: 117 // secs to run: 0.0034341812133789062\n",
      "715 // size: 151 // secs to run: 0.004926919937133789\n",
      "716 // size: 99 // secs to run: 0.0033140182495117188\n",
      "717 // size: 148 // secs to run: 0.004658222198486328\n",
      "718 // size: 87 // secs to run: 0.003004789352416992\n",
      "719 // size: 55 // secs to run: 0.0014872550964355469\n",
      "720 // size: 124 // secs to run: 0.0039937496185302734\n",
      "721 // size: 133 // secs to run: 0.0044558048248291016\n",
      "722 // size: 48 // secs to run: 0.0017039775848388672\n",
      "723 // size: 153 // secs to run: 0.004135847091674805\n",
      "724 // size: 282 // secs to run: 0.009305715560913086\n",
      "725 // size: 71 // secs to run: 0.002599000930786133\n",
      "726 // size: 122 // secs to run: 0.004117012023925781\n",
      "727 // size: 93 // secs to run: 0.0028429031372070312\n",
      "728 // size: 213 // secs to run: 0.008209228515625\n",
      "729 // size: 167 // secs to run: 0.005017995834350586\n",
      "730 // size: 85 // secs to run: 0.0025789737701416016\n",
      "731 // size: 172 // secs to run: 0.006743907928466797\n",
      "732 // size: 325 // secs to run: 61.79215693473816\n",
      "733 // size: 83 // secs to run: 0.0026302337646484375\n",
      "734 // size: 172 // secs to run: 0.00459599494934082\n",
      "735 // size: 0 // secs to run: 2.1457672119140625e-05\n",
      "736 // size: 244 // secs to run: 0.00824594497680664\n",
      "737 // size: 153 // secs to run: 0.004764080047607422\n",
      "738 // size: 49 // secs to run: 0.001615285873413086\n",
      "739 // size: 111 // secs to run: 0.0032739639282226562\n",
      "740 // size: 294 // secs to run: 0.011301755905151367\n",
      "741 // size: 60 // secs to run: 0.0018260478973388672\n",
      "742 // size: 173 // secs to run: 0.0051457881927490234\n",
      "743 // size: 178 // secs to run: 0.006849050521850586\n",
      "744 // size: 46 // secs to run: 0.0017251968383789062\n",
      "745 // size: 122 // secs to run: 0.0042188167572021484\n",
      "746 // size: 89 // secs to run: 0.0028753280639648438\n",
      "747 // size: 45 // secs to run: 0.0016911029815673828\n",
      "748 // size: 90 // secs to run: 0.0028159618377685547\n",
      "749 // size: 150 // secs to run: 0.004782199859619141\n",
      "750 // size: 158 // secs to run: 0.004815816879272461\n",
      "751 // size: 157 // secs to run: 0.005079984664916992\n",
      "752 // size: 153 // secs to run: 0.004494905471801758\n",
      "753 // size: 81 // secs to run: 0.002691984176635742\n",
      "754 // size: 74 // secs to run: 0.002741098403930664\n",
      "755 // size: 139 // secs to run: 0.0042111873626708984\n",
      "756 // size: 53 // secs to run: 0.0019278526306152344\n",
      "757 // size: 143 // secs to run: 0.00465703010559082\n",
      "758 // size: 161 // secs to run: 0.006012916564941406\n",
      "759 // size: 163 // secs to run: 0.004930973052978516\n",
      "760 // size: 182 // secs to run: 0.005405902862548828\n",
      "761 // size: 120 // secs to run: 0.0034148693084716797\n",
      "762 // size: 123 // secs to run: 0.003928184509277344\n",
      "763 // size: 81 // secs to run: 0.002866983413696289\n",
      "764 // size: 143 // secs to run: 0.0047719478607177734\n",
      "765 // size: 110 // secs to run: 0.0038499832153320312\n",
      "766 // size: 124 // secs to run: 0.004057168960571289\n",
      "767 // size: 333 // secs to run: 67.33693099021912\n",
      "768 // size: 61 // secs to run: 0.002131938934326172\n",
      "769 // size: 496 // secs to run: 202.43593502044678\n",
      "770 // size: 0 // secs to run: 4.673004150390625e-05\n",
      "771 // size: 134 // secs to run: 0.004439830780029297\n",
      "772 // size: 121 // secs to run: 0.004530191421508789\n",
      "773 // size: 628 // secs to run: 263.62728810310364\n",
      "774 // size: 120 // secs to run: 0.004464149475097656\n",
      "775 // size: 0 // secs to run: 3.0994415283203125e-05\n",
      "776 // size: 130 // secs to run: 0.004004240036010742\n",
      "777 // size: 148 // secs to run: 0.007688045501708984\n",
      "778 // size: 142 // secs to run: 0.0071620941162109375\n",
      "779 // size: 113 // secs to run: 0.003760814666748047\n",
      "780 // size: 88 // secs to run: 0.003422975540161133\n",
      "781 // size: 101 // secs to run: 0.002814054489135742\n",
      "782 // size: 152 // secs to run: 0.004292011260986328\n",
      "783 // size: 107 // secs to run: 0.003863096237182617\n",
      "784 // size: 143 // secs to run: 0.006272077560424805\n",
      "785 // size: 118 // secs to run: 0.004166841506958008\n",
      "786 // size: 111 // secs to run: 0.003507852554321289\n",
      "787 // size: 145 // secs to run: 0.004336833953857422\n",
      "788 // size: 161 // secs to run: 0.004546165466308594\n",
      "789 // size: 117 // secs to run: 0.0031669139862060547\n",
      "790 // size: 126 // secs to run: 0.003609180450439453\n",
      "791 // size: 172 // secs to run: 0.008803844451904297\n",
      "792 // size: 116 // secs to run: 0.0036742687225341797\n",
      "793 // size: 149 // secs to run: 0.004812002182006836\n",
      "794 // size: 157 // secs to run: 0.004824161529541016\n",
      "795 // size: 256 // secs to run: 0.008674144744873047\n",
      "796 // size: 93 // secs to run: 0.0032782554626464844\n",
      "797 // size: 105 // secs to run: 0.004303932189941406\n",
      "798 // size: 125 // secs to run: 0.004348039627075195\n",
      "799 // size: 68 // secs to run: 0.002791881561279297\n",
      "800 // size: 52 // secs to run: 0.0018758773803710938\n",
      "801 // size: 98 // secs to run: 0.003699064254760742\n",
      "802 // size: 114 // secs to run: 0.004500150680541992\n",
      "803 // size: 0 // secs to run: 5.91278076171875e-05\n",
      "804 // size: 0 // secs to run: 4.410743713378906e-05\n",
      "805 // size: 147 // secs to run: 0.007858753204345703\n",
      "806 // size: 118 // secs to run: 0.004639148712158203\n",
      "807 // size: 135 // secs to run: 0.007995843887329102\n",
      "808 // size: 107 // secs to run: 0.004374980926513672\n",
      "809 // size: 136 // secs to run: 0.006178140640258789\n",
      "810 // size: 135 // secs to run: 0.005944013595581055\n",
      "811 // size: 144 // secs to run: 0.007171154022216797\n",
      "812 // size: 126 // secs to run: 0.006698131561279297\n",
      "813 // size: 103 // secs to run: 0.004940032958984375\n",
      "814 // size: 120 // secs to run: 0.005841970443725586\n",
      "815 // size: 145 // secs to run: 0.007523059844970703\n",
      "816 // size: 122 // secs to run: 0.00613093376159668\n",
      "817 // size: 180 // secs to run: 0.008049249649047852\n",
      "818 // size: 150 // secs to run: 0.006100177764892578\n",
      "819 // size: 119 // secs to run: 0.007261037826538086\n",
      "820 // size: 61 // secs to run: 0.002588033676147461\n",
      "821 // size: 116 // secs to run: 0.004715681076049805\n",
      "822 // size: 122 // secs to run: 0.006415843963623047\n",
      "823 // size: 68 // secs to run: 0.0029098987579345703\n",
      "824 // size: 117 // secs to run: 0.005082130432128906\n",
      "825 // size: 607 // secs to run: 211.01729917526245\n",
      "826 // size: 130 // secs to run: 0.005300045013427734\n",
      "827 // size: 92 // secs to run: 0.0038945674896240234\n",
      "828 // size: 115 // secs to run: 0.003908872604370117\n",
      "829 // size: 0 // secs to run: 4.00543212890625e-05\n",
      "830 // size: 119 // secs to run: 0.004068136215209961\n",
      "831 // size: 104 // secs to run: 0.0040781497955322266\n",
      "832 // size: 41 // secs to run: 0.0017349720001220703\n",
      "833 // size: 115 // secs to run: 0.004128932952880859\n",
      "834 // size: 71 // secs to run: 0.0025768280029296875\n",
      "835 // size: 0 // secs to run: 3.504753112792969e-05\n",
      "836 // size: 138 // secs to run: 0.004754781723022461\n",
      "837 // size: 122 // secs to run: 0.004144906997680664\n",
      "838 // size: 114 // secs to run: 0.004105806350708008\n",
      "839 // size: 95 // secs to run: 0.003027200698852539\n",
      "840 // size: 119 // secs to run: 0.003599882125854492\n",
      "841 // size: 124 // secs to run: 0.003609895706176758\n",
      "842 // size: 127 // secs to run: 0.0038809776306152344\n",
      "843 // size: 82 // secs to run: 0.0025599002838134766\n",
      "844 // size: 121 // secs to run: 0.003718852996826172\n",
      "845 // size: 144 // secs to run: 0.003618001937866211\n",
      "846 // size: 104 // secs to run: 0.003159761428833008\n",
      "847 // size: 190 // secs to run: 0.005452156066894531\n",
      "848 // size: 133 // secs to run: 0.004829883575439453\n",
      "849 // size: 136 // secs to run: 0.004866123199462891\n",
      "850 // size: 111 // secs to run: 0.003467082977294922\n",
      "851 // size: 116 // secs to run: 0.0033638477325439453\n",
      "852 // size: 147 // secs to run: 0.004682064056396484\n",
      "853 // size: 115 // secs to run: 0.0039119720458984375\n",
      "854 // size: 95 // secs to run: 0.002969026565551758\n",
      "855 // size: 57 // secs to run: 0.001661062240600586\n",
      "856 // size: 44 // secs to run: 0.0013828277587890625\n",
      "857 // size: 156 // secs to run: 0.00430607795715332\n",
      "858 // size: 192 // secs to run: 0.005273103713989258\n",
      "859 // size: 69 // secs to run: 0.002238035202026367\n",
      "860 // size: 111 // secs to run: 0.003365755081176758\n",
      "861 // size: 109 // secs to run: 0.003197908401489258\n",
      "862 // size: 148 // secs to run: 0.00422978401184082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "863 // size: 397 // secs to run: 69.15798497200012\n",
      "864 // size: 100 // secs to run: 0.0036020278930664062\n",
      "865 // size: 96 // secs to run: 0.004008769989013672\n",
      "866 // size: 72 // secs to run: 0.00257110595703125\n",
      "867 // size: 0 // secs to run: 2.5033950805664062e-05\n",
      "868 // size: 70 // secs to run: 0.002404928207397461\n",
      "869 // size: 88 // secs to run: 0.0031249523162841797\n",
      "870 // size: 0 // secs to run: 2.384185791015625e-05\n",
      "871 // size: 86 // secs to run: 0.0031402111053466797\n",
      "872 // size: 164 // secs to run: 0.005818843841552734\n",
      "873 // size: 158 // secs to run: 0.005589962005615234\n",
      "874 // size: 129 // secs to run: 0.004536151885986328\n",
      "875 // size: 146 // secs to run: 0.006754875183105469\n",
      "876 // size: 115 // secs to run: 0.004125118255615234\n",
      "877 // size: 399 // secs to run: 92.81442999839783\n",
      "878 // size: 122 // secs to run: 0.004319906234741211\n",
      "879 // size: 117 // secs to run: 0.00391697883605957\n",
      "880 // size: 48 // secs to run: 0.0015201568603515625\n",
      "881 // size: 511 // secs to run: 150.59640383720398\n",
      "882 // size: 0 // secs to run: 5.507469177246094e-05\n",
      "883 // size: 86 // secs to run: 0.0035309791564941406\n",
      "884 // size: 150 // secs to run: 0.005105018615722656\n",
      "885 // size: 122 // secs to run: 0.004806995391845703\n",
      "886 // size: 83 // secs to run: 0.0030059814453125\n",
      "887 // size: 71 // secs to run: 0.0024738311767578125\n",
      "888 // size: 125 // secs to run: 0.004516124725341797\n",
      "889 // size: 106 // secs to run: 0.0043871402740478516\n",
      "890 // size: 150 // secs to run: 0.005852937698364258\n",
      "891 // size: 74 // secs to run: 0.0036721229553222656\n",
      "892 // size: 99 // secs to run: 0.004177093505859375\n",
      "893 // size: 131 // secs to run: 0.006474018096923828\n",
      "894 // size: 174 // secs to run: 0.007024049758911133\n",
      "895 // size: 93 // secs to run: 0.0034401416778564453\n",
      "896 // size: 427 // secs to run: 93.41176509857178\n",
      "897 // size: 0 // secs to run: 4.57763671875e-05\n",
      "898 // size: 0 // secs to run: 1.8835067749023438e-05\n",
      "899 // size: 147 // secs to run: 0.005155801773071289\n",
      "900 // size: 104 // secs to run: 0.0034668445587158203\n",
      "901 // size: 78 // secs to run: 0.0026938915252685547\n",
      "902 // size: 118 // secs to run: 0.004083871841430664\n",
      "903 // size: 357 // secs to run: 68.75369501113892\n",
      "904 // size: 322 // secs to run: 75.33346581459045\n",
      "905 // size: 108 // secs to run: 0.0040740966796875\n",
      "906 // size: 285 // secs to run: 0.011823892593383789\n",
      "907 // size: 116 // secs to run: 0.00434112548828125\n",
      "908 // size: 149 // secs to run: 0.0060329437255859375\n",
      "909 // size: 71 // secs to run: 0.0029561519622802734\n",
      "910 // size: 140 // secs to run: 0.00553584098815918\n",
      "911 // size: 88 // secs to run: 0.0038030147552490234\n",
      "912 // size: 94 // secs to run: 0.0037069320678710938\n",
      "913 // size: 133 // secs to run: 0.004716157913208008\n",
      "914 // size: 264 // secs to run: 0.013582944869995117\n",
      "915 // size: 125 // secs to run: 0.005493879318237305\n",
      "916 // size: 97 // secs to run: 0.0037271976470947266\n",
      "917 // size: 107 // secs to run: 0.004214048385620117\n",
      "918 // size: 108 // secs to run: 0.004173994064331055\n",
      "919 // size: 81 // secs to run: 0.0033037662506103516\n",
      "920 // size: 179 // secs to run: 0.007496833801269531\n",
      "921 // size: 68 // secs to run: 0.0031752586364746094\n",
      "922 // size: 194 // secs to run: 0.008156776428222656\n",
      "923 // size: 54 // secs to run: 0.002384185791015625\n",
      "924 // size: 368 // secs to run: 70.64612913131714\n",
      "925 // size: 0 // secs to run: 4.100799560546875e-05\n",
      "926 // size: 119 // secs to run: 0.003980875015258789\n",
      "927 // size: 96 // secs to run: 0.0027320384979248047\n",
      "928 // size: 110 // secs to run: 0.0036270618438720703\n",
      "929 // size: 139 // secs to run: 0.004122018814086914\n",
      "930 // size: 85 // secs to run: 0.003117799758911133\n",
      "931 // size: 63 // secs to run: 0.002026081085205078\n",
      "932 // size: 156 // secs to run: 0.0047719478607177734\n",
      "933 // size: 86 // secs to run: 0.0029959678649902344\n",
      "934 // size: 66 // secs to run: 0.0022170543670654297\n",
      "935 // size: 42 // secs to run: 0.0014147758483886719\n",
      "936 // size: 91 // secs to run: 0.0027098655700683594\n",
      "937 // size: 153 // secs to run: 0.004321098327636719\n",
      "938 // size: 76 // secs to run: 0.0022430419921875\n",
      "939 // size: 69 // secs to run: 0.002590179443359375\n",
      "940 // size: 311 // secs to run: 46.396209955215454\n",
      "941 // size: 141 // secs to run: 0.0041429996490478516\n",
      "942 // size: 100 // secs to run: 0.0030939579010009766\n",
      "943 // size: 16 // secs to run: 0.0006160736083984375\n",
      "944 // size: 131 // secs to run: 0.003675699234008789\n",
      "945 // size: 330 // secs to run: 57.4215669631958\n",
      "946 // size: 121 // secs to run: 0.0042040348052978516\n",
      "947 // size: 177 // secs to run: 0.0059163570404052734\n",
      "948 // size: 126 // secs to run: 0.0043866634368896484\n",
      "949 // size: 117 // secs to run: 0.004119157791137695\n",
      "950 // size: 193 // secs to run: 0.00615692138671875\n",
      "951 // size: 92 // secs to run: 0.00286102294921875\n",
      "952 // size: 181 // secs to run: 0.007091999053955078\n",
      "953 // size: 151 // secs to run: 0.005772590637207031\n",
      "954 // size: 109 // secs to run: 0.003656148910522461\n",
      "955 // size: 127 // secs to run: 0.004335880279541016\n",
      "956 // size: 135 // secs to run: 0.0043430328369140625\n",
      "957 // size: 170 // secs to run: 0.004957914352416992\n",
      "958 // size: 120 // secs to run: 0.0037071704864501953\n",
      "959 // size: 102 // secs to run: 0.0035851001739501953\n",
      "960 // size: 370 // secs to run: 77.83367824554443\n",
      "961 // size: 137 // secs to run: 0.004050016403198242\n",
      "962 // size: 98 // secs to run: 0.0028488636016845703\n",
      "963 // size: 162 // secs to run: 0.004628181457519531\n",
      "964 // size: 110 // secs to run: 0.003983974456787109\n",
      "965 // size: 92 // secs to run: 0.0038399696350097656\n",
      "966 // size: 100 // secs to run: 0.003077983856201172\n",
      "967 // size: 127 // secs to run: 0.0038499832153320312\n",
      "968 // size: 109 // secs to run: 0.0033752918243408203\n",
      "969 // size: 145 // secs to run: 0.004603862762451172\n",
      "970 // size: 154 // secs to run: 0.0042150020599365234\n",
      "971 // size: 194 // secs to run: 0.007794857025146484\n",
      "972 // size: 82 // secs to run: 0.0024340152740478516\n",
      "973 // size: 74 // secs to run: 0.002178668975830078\n",
      "974 // size: 139 // secs to run: 0.004098176956176758\n",
      "975 // size: 68 // secs to run: 0.00238800048828125\n",
      "976 // size: 122 // secs to run: 0.0034339427947998047\n",
      "977 // size: 161 // secs to run: 0.0065958499908447266\n",
      "978 // size: 141 // secs to run: 0.003793001174926758\n",
      "979 // size: 175 // secs to run: 0.004842996597290039\n",
      "980 // size: 146 // secs to run: 0.005258083343505859\n",
      "981 // size: 135 // secs to run: 0.0033638477325439453\n",
      "982 // size: 280 // secs to run: 0.012620925903320312\n",
      "983 // size: 131 // secs to run: 0.0033521652221679688\n",
      "984 // size: 85 // secs to run: 0.0027968883514404297\n",
      "985 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "986 // size: 83 // secs to run: 0.002363920211791992\n",
      "987 // size: 372 // secs to run: 75.60843396186829\n",
      "988 // size: 133 // secs to run: 0.004787921905517578\n",
      "989 // size: 145 // secs to run: 0.005685091018676758\n",
      "990 // size: 122 // secs to run: 0.004819154739379883\n",
      "991 // size: 119 // secs to run: 0.00419306755065918\n",
      "992 // size: 138 // secs to run: 0.0051839351654052734\n",
      "993 // size: 0 // secs to run: 4.7206878662109375e-05\n",
      "994 // size: 104 // secs to run: 0.003961086273193359\n",
      "995 // size: 144 // secs to run: 0.0068171024322509766\n",
      "996 // size: 102 // secs to run: 0.004441976547241211\n",
      "997 // size: 118 // secs to run: 0.004415035247802734\n",
      "998 // size: 124 // secs to run: 0.004888057708740234\n",
      "999 // size: 116 // secs to run: 0.004739999771118164\n",
      "1000 // size: 84 // secs to run: 0.003092050552368164\n",
      "1001 // size: 127 // secs to run: 0.005232095718383789\n",
      "1002 // size: 117 // secs to run: 0.004604339599609375\n",
      "1003 // size: 115 // secs to run: 0.005164146423339844\n",
      "1004 // size: 253 // secs to run: 0.010155916213989258\n",
      "1005 // size: 133 // secs to run: 0.005305767059326172\n",
      "1006 // size: 161 // secs to run: 0.006143093109130859\n",
      "1007 // size: 131 // secs to run: 0.005001068115234375\n",
      "1008 // size: 223 // secs to run: 0.009670019149780273\n"
     ]
    }
   ],
   "source": [
    "# custom truncate: if input is too large, get only the most important sentences\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    input = tokenizer.tokenize(data['model_input'][i])\n",
    "#     print(input)\n",
    "    if len(input) > max_input_size:\n",
    "        data['model_input'][i] = most_important_sentences_vs_doc(input)\n",
    "    \n",
    "    # keep track of time to run \n",
    "    if i % 1 == 0:\n",
    "        print(i, \"// size:\", len(input), \"// secs to run:\", time.time() - start)\n",
    "        start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector offrequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM. There are at least two kinds of similarity. When two words have a high degree of attributional similarity, we call them synonyms. When two word pairs have a high degree of relational similarity, we say they are analogous. Verbal analogies are often written in the form A:B::C:D, meaning A is to B as C is to D; for example, traffic:street::water:riverbed. Traffic flows over a street; water flows over a riverbed. A street carries traffic; a riverbed carries water. There is a high degree of relational similarity between the word pair traffic:street and the word pair water:riverbed. In analogies such as mason:stone::carpenter:wood, it seems that relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. This article builds on the Vector Space Model (VSM) of information retrieval. The documents are ranked in order of decreasing attributional similarity between the query and each document. The algorithm learns from a large corpus of unlabeled, unstructured text, without supervision. (2) Singular Value Decomposition (SVD) is used to smooth the frequency data. (3) Given a word pair such as traffic:street, LRA considers transformations of the word pair, generated by replacing one of the words by synonyms, such as traffic:road or traffic:highway. In the educational testing literature, the first pair (mason:stone) is called the stem of the analogy. The correct choice is called the solution and the incorrect choices are distractors. We evaluate LRA by testing its ability to select the solution and avoid the distractors. LRA achieves an accuracy of about 56%. On these same questions, the VSM attained 47%. The problem is to classify a noun-modifier pair, such as “laser printer,” according to the semantic relation between the head noun (printer) and the modifier (laser). We approach the task of classifying semantic relations in noun-modifier pairs as a supervised learning problem. The 600 pairs are divided into training and testing sets and a testing pair is classified according to the label of its single nearest neighbor in the training set. LRA is used to measure distance (ie, similarity, nearness). LRA achieves an accuracy of 39. % on the 30-class problem and 58.0% on the 5-class problem. On the same 600 noun-modifier pairs, the VSM had accuracies of 27. Attributes are used to state properties of objects; relations express relations between objects or propositions. For example, large can be viewed as an attribute of X, LARGE(X), or a relation between X and some standard Y, LARGER THAN(X, Y). The amount of attributional similarity between two words, A and B, depends on the degree of correspondence between the properties of A and B. For example, dog and wolf have a relatively high degree of attributional similarity. For example, dog:bark and cat:meow have a relatively high degree of relational similarity. Both of these are types of attributional similarity, since they are based on correspondence between attributes (eg, bees and honey are both found in hives; deer and ponies are both mammals). It’s important to note that semantic relatedness is a more general concept than similarity; similar entities are usually assumed to be related by virtue of their likeness (bank–trust company), but dissimilar entities may also be semantically related by lexical relationships such as meronymy (car–wheel) and antonymy (hot–cold), or just by any kind of functional relationship or frequent association (pencil–paper, penguin–Antarctica). As these examples show, semantic relatedness is the same as attributional similarity (eg, hot and cold are both kinds of temperature, pencil and paper are both used for writing). Here we prefer to use the term attributional similarity because it emphasizes the contrast with relational similarity. The term semantic relatedness may lead to confusion when the term relational similarity is also under discussion. Thus semantic similarity is a specific type of attributional similarity. The term semantic similarity is misleading, because it refers to a type of attributional similarity, yet relational similarity is not any less semantic than attributional similarity. Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). The results support the claim that lexicon-based algorithms have no advantage over corpus-based algorithms for recognizing synonymy. Stem: Levied To evaluate this approach, we applied several measures of attributional similarity to our collection of 374 SAT questions. The performance of the algorithms was measured by precision, recall, and F, defined as follows: Note that recall is the same as percent correct (for multiple-choice questions, with only zero or one guesses allowed per question, but not in general). When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped. We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not enough near analogies for attributional similarity to perform as well as relational similarity. The problem of recognizing word analogies is, given a stem word pair and a finite list of choice word pairs, selecting the choice that is most analogous to the stem. Argus could only solve the limited set of analogy questions that its programmer had anticipated. The goal of computational modeling of analogy making is to understand how people form complex, structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (expressing relations between relations). The analogical mapping connects source domain relations to target domain relations. The basic objects in the target model are the electrons and the nucleus. The planets and the sun have various attributes, such as mass(sun) and mass(planet), and various relations, such as revolve(planet, sun) and attracts(sun, planet). Likewise, the nucleus and the electrons have attributes, such as charge(electron) and charge(nucleus), and relations, such as revolve(electron, nucleus) and attracts(nucleus, electron). SME maps revolve(planet, sun) to revolve(electron, nucleus) and attracts(sun, planet) to attracts(nucleus, electron). Each individual connection (eg, from revolve(planet, sun) to revolve(electron, nucleus)) in an analogical mapping implies that the connected relations are similar; thus, SMT requires a measure of relational similarity in order to form maps. However, the focus of research in analogy making has been on the mapping process as a whole, rather than measuring the similarity between any two particular relations; hence, the similarity measures used in SME at the level of individual connections are somewhat rudimentary. We believe that a more sophisticated measure of relational similarity, such as LRA, may enhance the performance of SME. Likewise, the focus of our work here is on the similarity between particular relations, and we ignore systematic mapping between sets of relations, so LRA may also be enhanced by integration with SME. The task of classifying semantic relations is to identify the relation between a pair of words. Often the pairs are restricted to noun-modifier pairs, but there are many interesting relations, such as antonymy, that do not occur in noun-modifier pairs. However, noun-modifier pairs are interesting due to their high frequency in English. For instance, WordNet 2.0 contains more than 26,000 noun-modifier pairs, although many common noun-modifiers are not in WordNet, especially technical terms. They trained a neural network to distinguish 13 classes of semantic relations. None of these approaches explicitly involved measuring relational similarity, but any classification of semantic relations necessarily employs some implicit notion of relational similarity since members of the same class must be relationally similar to some extent. The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. For example, reptile haven was paraphrased as haven for reptiles. We believe that the intended sense of a polysemous word is determined by its semantic relations with the other words in the surrounding text. If we can identify the semantic relations between the given word and its context, then we can disambiguate the given word. For example, consider the word plant. Out of context, plant could refer to an industrial plant or a living organism. In this case, the decision may not be clear, since industrial plants often produce food and living organisms often serve as food. It would be very helpful to know the relation between food and plant in this example. In the phrase “food for the plant,” the relation between food and plant strongly suggests that the plant is a living organism, since industrial plants do not need food. In the text “food at the plant,” the relation strongly suggests that the plant is an industrial plant, since living organisms are not usually considered as locations. The problem of relation extraction is, given an input document and a specific relation R, to extract all pairs of entities (if any) that have the relation R in the document. ), except that information extraction focuses on the relation between a specific pair of entities in a specific document, rather than a general pair of words in general text. Therefore an algorithm for classifying semantic relations should be useful for information extraction. Each example would be represented by a vector of pattern frequencies. Given a specific document discussing John Smith and Hardcom Corporation, we could construct a vector representing the relation between these two entities and then measure the relational similarity between this unlabeled vector and each of our labeled training vectors. It would seem that there is a problem here because the training vectors would be relatively dense, since they would presumably be derived from a large corpus, but the new unlabeled vector for John Smith and Hardcom Corporation would be very sparse, since these entities might be mentioned only once in the given document. However, this is not a new problem for the VSM; it is the standard situation when the VSM is used for information retrieval. A query to a search engine is represented by a very sparse vector, whereas a document is represented by a relatively dense vector. Instead of manually generating new rules or patterns for each new semantic relation, it is possible to automatically learn a measure of relational similarity that can handle arbitrary semantic relations. A nearest neighbor algorithm can then use this relational similarity measure to learn to classify according to any set of classes of relations, given the appropriate labeled training data. However, they supplement their manual rules with automatically learned constraints, to increase the precision of the rules. He proposes to use the algorithm for analogical information retrieval. For example, the query Muslim church should return mosque and the query Hindu bible should return the Vedas. A measure of relational similarity is applicable to this task. (The pair Muslim:mosque has a high relational similarity to the pair Christian:church.) Each cluster of words in one corpus is coupled one-to-one with a cluster in the other corpus. For example, one experiment used a corpus of Buddhist documents and a corpus of Christian documents. A cluster of words such as {Hindu, Mahayana, Zen, ...} from the Buddhist corpus was coupled with a cluster of words such as {Catholic, Protestant, ...} from the Christian corpus. Thus the algorithm appears to have discovered an analogical mapping between Buddhist schools and traditions and Christian schools and traditions. This is interesting work, but it is not directly applicable to SAT analogies, because it discovers analogies between clusters of words rather than individual words. The task of identifying semantic roles is to label the parts of a sentence according to their semantic roles. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors. For a given query, the search engine sorts the matching documents in order of decreasing cosine. Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning. The relations R1 and R2 are not given to us; our task is to infer these hidden (latent) relations and then compare them. These phrases are then used as queries for a search engine and the number of hits (matching documents) is recorded for each query. This process yields a vector of 128 numbers. If the number of hits for a query is x, then the corresponding element in the vector r is log(x + 1). Since there are five choices for each question, the expected score for random guessing is 20%. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Secti on 6. A testing pair is classified by searching for its single nearest neighbor in the labeled training data. Thus their corpus was the set of all Web pages indexed by AltaVista. AltaVista also changed their policy toward automated searching, which is now forbidden. The WMTS is a distributed (multiprocessor) search engine, designed primarily for passage retrieval (although document retrieval is possible, as a special case of passage retrieval). The text and index require approximately one terabyte of disk space. Although AltaVista only gives a rough estimate of the number of matching documents, the WMTS gives exact counts of the number of matching passages. The performance of LRA significantly surpasses this combined system, but there is no real contest between these approaches, because we can simply add LRA to the combination, as a fourteenth module. LRA takes as input a set of word pairs and produces as output a measure of the relational similarity between any two of the input pairs. LRA relies on three resources, a search engine with a very large corpus of text, a broad-coverage thesaurus of synonyms, and an efficient implementation of SVD. We first present a short description of the core algorithm. The motivation for the alternate pairs is to handle cases where the original pairs cooccur rarely in the corpus. Before applying SVD, the vectors are completely non-negative, which implies that the cosine can only range from 0 to +1, but SVD introduces negative values, so it is possible for the cosine to be negative, although we have never observed this in our experiments. The Web pages cover a very wide range of topics, styles, genres, quality, and writing skill. The WMTS is well suited to LRA, because the WMTS scales well to large corpora (one terabyte, in our case), it gives exact frequency counts (unlike most Web search engines), it is designed for passage retrieval (rather than document retrieval), and it has a powerful query syntax. This thesaurus is available through an on-line interactive demonstration or it can be downloaded. We used the on-line demonstration, since the downloadable version seems to contain fewer words. For each word in the input set of word pairs, we automatically query the on-line demonstration and fetch the resulting list of synonyms. Words were then clustered into synonym sets, based on the similarity of their grammatical relations. Two words were judged to be highly similar when they tended to have the same kinds of grammatical relations with the same sets of words. Given a word and its part of speech, Lin’s thesaurus provides a list of words, sorted in order of decreasing attributional similarity. WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses. In LRA, SVD is used to reduce noise and compensate for sparseness. Sort the alternate pairs by the frequency of their phrases. This step tends to eliminate alternates that have no clear semantic relation. Alternate forms of the original pair quart:volume. The first column shows the original pair and the alternate pairs. The second column shows Lin’s similarity score for the alternate word compared to the original word. For example, the similarity between quart and pint is 0. The third column shows the frequency of the pair in the WMTS corpus. The phrases cannot have more than max phrase words and there must be at least one word between the two members of the word pair. These phrases give us information about the semantic relations between the words in each pair. A phrase with no words between the two members of the word pair would give us very little information about the semantic relations (other than that the words occur together with a certain frequency in a certain order). Find patterns: For each phrase found in the previous step, build patterns from the intervening words. At least one word must occur between quart and volume. At most max phrase words can appear in a phrase. quarts liquid volume volume in quarts quarts of volume volume capacity quarts quarts in volume volume being about two quarts quart total volume volume of milk in quarts quart of spray volume volume include measures like quart replace only one word). If a phrase is n words long, there are n − 2 intervening words between the members of the given word pair (eg, between quart and volume). Thus a phrase with n words generates 2(n−2) patterns. For each pattern, count the number of pairs (originals and alternates) with phrases that match the pattern (a wild card must match exactly one word). Typically there will be millions of patterns, so it is not feasible to keep them all. more weight to columns (patterns) with frequencies that vary substantially from one row (word pair) to the next, and less weight to columns that are uniform. We also apply the log transformation to frequencies, log(xi,j + 1). (Entropy is calculated with the original frequency values, before the log transformation is applied.) approximates the original matrix X, in the sense that it minimizes the � � approximation errors. We may think of this matrix UkEkVTk as a “smoothed” or “compressed” version of the original matrix. In the subsequent steps, we will be calculating cosines for row vectors. For this purpose, we can simplify calculations by dropping V. The cosine of two vectors is their dot product, after they have been normalized to unit length. The matrix XXT contains the dot products of all of the row vectors. We can find the dot product of the ith and jth row vectors by looking at the cell in row i, column j of the matrix XXT. This matrix has the same number of rows as X, but only k columns (instead of 2 x num patterns columns; in our experiments, that is 300 columns instead of 8,000). We can compare two word pairs by calculating the cosine of the corresponding row vectors in UkEk. The row vector for each word pair has been projected from the original 8,000 dimensional space into a new 300 dimensional space. Therefore we have (num filter + 1)2 ways to compare a version of A:B with a version of C:D. Look for the row vectors in UkEk that correspond to the versions of A:B and the versions of C:D and calculate the (num filter + 1)2 cosines (in our experiments, there are 16 cosines). For example, suppose A:B is quart:volume and C:D is mile:distance. Calculate relational similarity: The relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. The requirement that the cosine must be greater than or equal to the original cosine is a way of filtering out poor analogies, which may be introduced in step 1 and may have slipped through the filtering in step 2. Steps 11 and 12 can be repeated for each two input pairs that are to be compared. This completes the description of LRA. The choice pair with the highest average cosine (the choice with the largest value in column 1), choice (b), is the solution for this question; LRA answers the question correctly. As another point of reference, consider the simple strategy of always guessing the choice with the highest co-occurrence frequency. The idea here is that the words in the solution pair may occur together frequently, because there is presumably a clear and meaningful relation between the solution words, whereas the distractors may only occur together rarely because they have no meaningful relation. This strategy is signifcantly worse than random guessing. The opposite strategy, always guessing the choice pair with the lowest co-occurrence frequency, is also worse than random guessing (but not significantly). It appears that the designers of the SAT questions deliberately chose distractors that would thwart these two strategies. With 374 questions and six word pairs per question (one stem and five choices), there are 2,244 pairs in the input set. In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 8,976 pairs. In step 5, for each pair A:B, we add B:A, yielding 17,952 pairs. However, some pairs are dropped because they correspond to zero vectors (they do not appear together in a window of five words in the WMTS corpus). Also, a few words do not appear in Lin’s thesaurus, and some word pairs appear twice in the SAT questions (eg, lion:cat). All of the steps used a single CPU on a desktop computer, except step 3, finding the phrases for each word pair, which used a 16 CPU Beowulf cluster. Most of the other steps are parallelizable; with a bit of programming effort, they could also be executed on the Beowulf cluster. All CPUs (both desktop and cluster) were 2. The desktop computer had 2 GB of RAM and the cluster had a total of 16 GB of RAM. We generated the VSM-WMTS results by adapting the VSM to the WMTS. The pairwise differences in precision between LRA and the two VSM variations are also significant, but the difference in precision between the two VSM variations (42. %) is not significant. Although VSM-AV has a corpus 10 times larger than LRA’s, LRA still performs better than VSM-AV. Comparing VSM-AV to VSM-WMTS, the smaller corpus has reduced the score of the VSM, but much of the drop is due to the larger number of questions that were skipped (34 for VSM-WMTS versus 5 for VSM-AV). With the smaller corpus, many more of the input word pairs simply do not appear together in short phrases in the corpus. LRA is able to answer as many questions as VSM-AV, although it uses the same corpus as VSM-WMTS, because Lin’s thesaurus allows LRA to substitute synonyms for words that are not in the corpus. The SAT I test consists of 78 verbal questions and 60 math questions (there is also an SAT II test, covering specific subjects, such as chemistry). Analogy questions are only a subset of the 78 verbal SAT questions. If we assume that the difficulty of our 374 analogy questions is comparable to the difficulty of the 78 verbal SAT I questions, then we can estimate that the average college-bound senior would correctly answer about 57% of the 374 analogy questions. On this subset of the questions, LRA has a recall of 61. %, compared to a recall of 51. % on the other 184 questions. This indicates that we may be underestimating how well LRA performs, relative to college-bound senior high school students. The parameter values were determined by trying a small number of possible values on a small set of questions that were set aside. Since LRA is intended to be an unsupervised learning algorithm, we did not attempt to tune the parameter values to maximize the precision and recall on the 374 SAT questions. We hypothesized that LRA is relatively insensitive to the values of the parameters. and vary each parameter, one at a time, while holding the remaining parameters fixed at their baseline values. This supports the hypothesis that the algorithm is not sensitive to the parameter values. Although a full run of LRA on the 374 SAT questions takes 9 days, for some of the parameters it is possible to reuse cached data from previous runs. We limited the experiments with num sim and max phrase because caching was not as helpful for these parameters, so experimenting with them required several weeks. However, we hypothesize that the drop in performance would be significant with a larger set of word pairs. More word pairs would increase the sample size, which would decrease the 95% confidence interval, which would likely show that SVD is making a significant contribution. Furthermore, more word pairs would increase the matrix size, which would give SVD more leverage. We are currently gathering more SAT questions to test this hypothesis. %), but the drop in precision is not significant. When the synonym component is dropped, the number of skipped questions rises from 4 to 22, which demonstrates the value of the synonym component of LRA for compensating for sparse data. Again, we believe that a larger sample size would show that the drop in precision is significant. If we eliminate both synonyms and SVD from LRA, all that distinguishes LRA from VSM-WMTS is the patterns (step 4). We can see the value of the automatically generated patterns by comparing LRA without synonyms and SVD (column 4) to VSM-WMTS (column 5). The ablation experiments support the value of the patterns (step 4) and synonyms (step 1) in LRA, but the contribution of SVD (step 9) has not been proven, although we believe more data will support its effectiveness. Nonetheless, the three components together result in a 16% increase in F (compare column 1 to 5). We know a priori that, if A:B::C:D, then B:A::D:C. For example, mason is to stone as carpenter is to wood implies stone is to mason as wood is to carpenter. The matrix is designed so that the row vector for A:B is different from the row vector for B:A only by a permutation of the elements. To discover the consequences of this design decision, we altered steps 5 and 6 so that symmetry is no longer preserved. In step 5, for each word pair A:B that appears in the input set, we only have one row. Thus the number of rows in the matrix dropped from 17,232 to 8,616. In step 6, we no longer have two columns for each pattern P, one for “word1 P word2” and another for “word2 P word1.” However, to be fair, we kept the total number of columns at 8,000. In step 4, we selected the top 8,000 patterns (instead of the top 4,000), distinguishing the pattern “word1 P word2” from the pattern “word2 P word1” (instead of considering them equivalent). Thus a pattern P with a high frequency is likely to appear in two columns, in both possible orders, but a lower frequency pattern might appear in only one column, in only one possible order. These changes resulted in a slight decrease in performance. % to 55.% and precision dropped from 56. The decrease is not statistically significant. In step 12 of LRA, the relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. That is, the average includes only those alternates that are “better” than the originals. Taking all alternates instead of the better alternates, recall drops from 56. % and precision drops from 56. The idea here is that we will only pay attention to the N most important patterns in r; the remaining patterns will be ignored. If most of the semantic content is in the N largest elements of r, then setting the remaining elements to zero should have relatively little impact. The precision and recall are significantly below the baseline LRA until N ≥ 300 (95% confidence, Fisher Exact Test). In other words, for a typical SAT analogy question, we need to examine the top 300 patterns to explain why LRA selected one choice instead of another. We are currently working on an extension of LRA that will explain with a single pattern why one choice is better than another. If we require an exact match, 50 of the 64 manual patterns can be found in the automatic patterns. If we are lenient about wildcards, and count the pattern not the as matching * not the (for example), then 60 of the 64 manual patterns appear within the automatic patterns. This suggests that the improvement in performance with the automatic patterns is due to the increased quantity of patterns, rather than a qualitative difference in the patterns. Both of these patterns are included in the 4,000 patterns automatically generated by LRA. No adjustments were made to tune LRA to the noun-modifier pairs. LRA is used as a distance (nearness) measure in a single nearest neighbor supervised learning algorithm. We make use of this grouping in the following experiments. The following experiments use single nearest neighbor classification with leave-one-out cross-validation. For leave-one-out cross-validation, the testing set consists of a single noun-modifier pair and the training set consists of the 599 remaining noun-modifiers. The data set is split 600 times, so that each noun-modifier gets a turn as the testing word pair. The predicted class of the testing pair is the class of the single nearest neighbor in the training set. As the measure of nearness, we use LRA to calculate the relational similarity between the testing pair and the training pairs. The factor of 16 comes from the alternate pairs, step 11 in LRA. Macroaveraging calculates the precision, recall, and F for each class separately, and then calculates the average across all classes. Macroaveraging gives equal weight to all classes, but microaveraging gives more weight to larger classes. We use macroaveraging (giving equal weight to all classes), because we have no reason to believe that the class sizes in the data set reflect the actual distribution of the classes in a real corpus. Classification with 30 distinct classes is a hard problem. For example, agent and beneficiary both collapse to participant. On the 30 class problem, LRA with the single nearest neighbor algorithm achieves an accuracy of 39. Always guessing the majority class would result in an accuracy of 8. On the 5 class problem, the accuracy is 58.0% (348/600) and the macroaveraged F is 54. Always guessing the majority class would give an accuracy of 43.% (260/600). Another limitation is speed; it took almost 9 days for LRA to answer 374 analogy questions. However, with progress in computer hardware, speed will gradually become less of a concern. It may also be possible to precompute much of the information for LRA, although this would require substantial changes to the algorithm. The difference in performance between VSM-AV and VSM-WMTS shows that VSM is sensitive to the size of the corpus. Although LRA is able to surpass VSM-AV when the WMTS corpus is only about one tenth the size of the AV corpus, it seems likely that LRA would perform better with a larger corpus. The WMTS corpus requires one terabyte of hard disk space, but progress in hardware will likely make 10 or even 100 terabytes affordable in the relatively near future. For noun-modifier classification, more labeled data should yield performance improvements. With 600 noun-modifier pairs and 30 classes, the average class has only 20 examples. We expect that the accuracy would improve substantially with 5 or 10 times more examples. Another issue with noun-modifier classification is the choice of classification scheme for the semantic relations. It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. SVD is only one of many methods for handling sparse, noisy data. In step 4 of LRA, we simply select the top num patterns most frequent patterns and discard the remaining patterns. Perhaps a more sophisticated selection algorithm would improve the performance of LRA. We have tried a variety of ways of selecting patterns, but it seems that the method of selection has little impact on performance. This article has introduced a new method for calculating relational similarity, Latent Relational Analysis. The experiments demonstrate that LRA performs better than the VSM approach, when evaluated with SAT word analogy questions and with the task of classifying noun-modifier expressions. The VSM approach represents the relation between a pair of words with a vector, in which the elements are based on the frequencies of 64 hand-built patterns in a large corpus. LRA extends this approach in three ways: (1) The patterns are generated dynamically from the corpus, (2) SVD is used to smooth the data, and (3) a thesaurus is used to explore variations of the word pairs. We have presented several examples of the many potential applications for measures of relational similarity. Just as attributional similarity measures have proven to have many practical uses, we expect that relational similarity measures will soon become widely used. We believe that relational similarity plays a fundamental role in the mind and therefore relational similarity measures could be crucial for artificial intelligence. In future work, we plan to investigate some potential applications for LRA. It is possible that the error rate of LRA is still too high for practical applications, but the fact that LRA matches average human performance on SAT analogy questions is encouraging. Thanks to Michael Littman for sharing the 374 SAT analogy questions and for inspiring me to tackle them. Thanks to Dekang Lin for making his Dependency-Based Word Similarity lexicon available online. Thanks to Ted Pedersen for making his WordNet::Similarity package available. Thanks to Joel Martin for comments on the article. Thanks to the anonymous reviewers of Computational Linguistics for their very helpful comments and suggestions.\n",
      "---------------------------------------------\n",
      "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector offrequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM. There are at least two kinds of similarity. When two words have a high degree of attributional similarity, we call them synonyms. When two word pairs have a high degree of relational similarity, we say they are analogous. Verbal analogies are often written in the form A:B::C:D, meaning A is to B as C is to D; for example, traffic:street::water:riverbed. Traffic flows over a street; water flows over a riverbed. A street carries traffic; a riverbed carries water. There is a high degree of relational similarity between the word pair traffic:street and the word pair water:riverbed. In analogies such as mason:stone::carpenter:wood, it seems that relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. This article builds on the Vector Space Model (VSM) of information retrieval. The documents are ranked in order of decreasing attributional similarity between the query and each document. They used a vector of frequencies of patterns in a corpus to represent the relation between a pair of words. The algorithm learns from a large corpus of unlabeled, unstructured text, without supervision. (2) Singular Value Decomposition (SVD) is used to smooth the frequency data. (3) Given a word pair such as traffic:street, LRA considers transformations of the word pair, generated by replacing one of the words by synonyms, such as traffic:road or traffic:highway. In the educational testing literature, the first pair (mason:stone) is called the stem of the analogy. The correct choice is called the solution and the incorrect choices are distractors. We evaluate LRA by testing its ability to select the solution and avoid the distractors. The average performance of collegebound senior high school students on verbal SAT questions corresponds to an accuracy of about 57%. LRA achieves an accuracy of about 56%. On these same questions, the VSM attained 47%. The problem is to classify a noun-modifier pair, such as “laser printer,” according to the semantic relation between the head noun (printer) and the modifier (laser). The 600 pairs have been manually labeled with 30 classes of semantic relations. For example, “laser printer” is classified as instrument; the printer uses the laser as an instrument for printing. We approach the task of classifying semantic relations in noun-modifier pairs as a supervised learning problem. The 600 pairs are divided into training and testing sets and a testing pair is classified according to the label of its single nearest neighbor in the training set. LRA is used to measure distance (ie, similarity, nearness). LRA achieves an accuracy of 39. % on the 30-class problem and 58.0% on the 5-class problem. On the same 600 noun-modifier pairs, the VSM had accuracies of 27. Attributes are used to state properties of objects; relations express relations between objects or propositions. For example, large can be viewed as an attribute of X, LARGE(X), or a relation between X and some standard Y, LARGER THAN(X, Y). The amount of attributional similarity between two words, A and B, depends on the degree of correspondence between the properties of A and B. For example, dog and wolf have a relatively high degree of attributional similarity. For example, dog:bark and cat:meow have a relatively high degree of relational similarity. Both of these are types of attributional similarity, since they are based on correspondence between attributes (eg, bees and honey are both found in hives; deer and ponies are both mammals). It’s important to note that semantic relatedness is a more general concept than similarity; similar entities are usually assumed to be related by virtue of their likeness (bank–trust company), but dissimilar entities may also be semantically related by lexical relationships such as meronymy (car–wheel) and antonymy (hot–cold), or just by any kind of functional relationship or frequent association (pencil–paper, penguin–Antarctica). As these examples show, semantic relatedness is the same as attributional similarity (eg, hot and cold are both kinds of temperature, pencil and paper are both used for writing). Here we prefer to use the term attributional similarity because it emphasizes the contrast with relational similarity. The term semantic relatedness may lead to confusion when the term relational similarity is also under discussion. Thus semantic similarity is a specific type of attributional similarity. The term semantic similarity is misleading, because it refers to a type of attributional similarity, yet relational similarity is not any less semantic than attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). The results support the claim that lexicon-based algorithms have no advantage over corpus-based algorithms for recognizing synonymy. It seems possible that SAT analogy questions might consist largely of near analogies, in which case they can be solved using attributional similarity measures. An example of a typical TOEFL question, from the collection of 80 questions. Stem: Levied To evaluate this approach, we applied several measures of attributional similarity to our collection of 374 SAT questions. The performance of the algorithms was measured by precision, recall, and F, defined as follows: Note that recall is the same as percent correct (for multiple-choice questions, with only zero or one guesses allowed per question, but not in general). When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped. We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not enough near analogies for attributional similarity to perform as well as relational similarity. The problem of recognizing word analogies is, given a stem word pair and a finite list of choice word pairs, selecting the choice that is most analogous to the stem. Argus could only solve the limited set of analogy questions that its programmer had anticipated. Argus was based on a spreading activation model and did not explicitly attempt to measure relational similarity. The final output of the system was based on a weighted combination of the outputs of each individual module. The VSM was evaluated on a set of 374 SAT questions, achieving a score of 47%. The goal of computational modeling of analogy making is to understand how people form complex, structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (expressing relations between relations). The analogical mapping connects source domain relations to target domain relations. The solar system is the source domain and Rutherford’s model of the atom is the target domain. The basic objects in the source model are the planets and the sun. The basic objects in the target model are the electrons and the nucleus. The planets and the sun have various attributes, such as mass(sun) and mass(planet), and various relations, such as revolve(planet, sun) and attracts(sun, planet). Likewise, the nucleus and the electrons have attributes, such as charge(electron) and charge(nucleus), and relations, such as revolve(electron, nucleus) and attracts(nucleus, electron). SME maps revolve(planet, sun) to revolve(electron, nucleus) and attracts(sun, planet) to attracts(nucleus, electron). Each individual connection (eg, from revolve(planet, sun) to revolve(electron, nucleus)) in an analogical mapping implies that the connected relations are similar; thus, SMT requires a measure of relational similarity in order to form maps. However, the focus of research in analogy making has been on the mapping process as a whole, rather than measuring the similarity between any two particular relations; hence, the similarity measures used in SME at the level of individual connections are somewhat rudimentary. We believe that a more sophisticated measure of relational similarity, such as LRA, may enhance the performance of SME. Likewise, the focus of our work here is on the similarity between particular relations, and we ignore systematic mapping between sets of relations, so LRA may also be enhanced by integration with SME. The task of classifying semantic relations is to identify the relation between a pair of words. Often the pairs are restricted to noun-modifier pairs, but there are many interesting relations, such as antonymy, that do not occur in noun-modifier pairs. However, noun-modifier pairs are interesting due to their high frequency in English. For instance, WordNet 2.0 contains more than 26,000 noun-modifier pairs, although many common noun-modifiers are not in WordNet, especially technical terms. They trained a neural network to distinguish 13 classes of semantic relations. None of these approaches explicitly involved measuring relational similarity, but any classification of semantic relations necessarily employs some implicit notion of relational similarity since members of the same class must be relationally similar to some extent. The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. For example, reptile haven was paraphrased as haven for reptiles. We believe that the intended sense of a polysemous word is determined by its semantic relations with the other words in the surrounding text. If we can identify the semantic relations between the given word and its context, then we can disambiguate the given word. For example, consider the word plant. Out of context, plant could refer to an industrial plant or a living organism. Suppose plant appears in some text near food. In this case, the decision may not be clear, since industrial plants often produce food and living organisms often serve as food. It would be very helpful to know the relation between food and plant in this example. In the phrase “food for the plant,” the relation between food and plant strongly suggests that the plant is a living organism, since industrial plants do not need food. In the text “food at the plant,” the relation strongly suggests that the plant is an industrial plant, since living organisms are not usually considered as locations. The problem of relation extraction is, given an input document and a specific relation R, to extract all pairs of entities (if any) that have the relation R in the document. ), except that information extraction focuses on the relation between a specific pair of entities in a specific document, rather than a general pair of words in general text. Therefore an algorithm for classifying semantic relations should be useful for information extraction. Each example would be represented by a vector of pattern frequencies. Given a specific document discussing John Smith and Hardcom Corporation, we could construct a vector representing the relation between these two entities and then measure the relational similarity between this unlabeled vector and each of our labeled training vectors. It would seem that there is a problem here because the training vectors would be relatively dense, since they would presumably be derived from a large corpus, but the new unlabeled vector for John Smith and Hardcom Corporation would be very sparse, since these entities might be mentioned only once in the given document. However, this is not a new problem for the VSM; it is the standard situation when the VSM is used for information retrieval. A query to a search engine is represented by a very sparse vector, whereas a document is represented by a relatively dense vector. As defined in the Text Retrieval Conference (TREC) QA track, the task is to answer simple questions, such as “Where have nuclear incidents occurred”, by retrieving a relevant document from a large corpus and then extracting a short string from the document, such as The Three Mile Island nuclear incident caused a DOE policy crisis. They argue that the desired semantic relation can easily be inferred from the surface form of the question. A question of the form “Where... ” is likely to be looking for entities with a location relation and a question of the form “What did ... make” is likely to be looking for entities with a product relation. These algorithms could be used to automatically generate a thesaurus or dictionary, but we would like to handle more relations than hyponymy and meronymy. LRA does not use a predefined set of patterns; it learns patterns from a large corpus. Instead of manually generating new rules or patterns for each new semantic relation, it is possible to automatically learn a measure of relational similarity that can handle arbitrary semantic relations. A nearest neighbor algorithm can then use this relational similarity measure to learn to classify according to any set of classes of relations, given the appropriate labeled training data. However, they supplement their manual rules with automatically learned constraints, to increase the precision of the rules. He proposes to use the algorithm for analogical information retrieval. For example, the query Muslim church should return mosque and the query Hindu bible should return the Vedas. The algorithm was designed with a focus on analogies of the form adjective:noun::adjective:noun, such as Christian:church::Muslim:mosque. A measure of relational similarity is applicable to this task. (The pair Muslim:mosque has a high relational similarity to the pair Christian:church.) Each cluster of words in one corpus is coupled one-to-one with a cluster in the other corpus. For example, one experiment used a corpus of Buddhist documents and a corpus of Christian documents. A cluster of words such as {Hindu, Mahayana, Zen, ...} from the Buddhist corpus was coupled with a cluster of words such as {Catholic, Protestant, ...} from the Christian corpus. Thus the algorithm appears to have discovered an analogical mapping between Buddhist schools and traditions and Christian schools and traditions. This is interesting work, but it is not directly applicable to SAT analogies, because it discovers analogies between clusters of words rather than individual words. The task of identifying semantic roles is to label the parts of a sentence according to their semantic roles. We believe that it may be helpful to view semantic frames and their semantic roles as sets of semantic relations; thus, a measure of relational similarity should help us to identify semantic roles. ), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech. In the VSM approach to information retrieval, queries and documents are represented by vectors. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors. For a given query, the search engine sorts the matching documents in order of decreasing cosine. Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning. Let R1 be the semantic relation (or set of relations) between a pair of words, A and B, and let R2 be the semantic relation (or set of relations) between another pair, C and D. We wish to measure the relational similarity between R1 and R2. The relations R1 and R2 are not given to us; our task is to infer these hidden (latent) relations and then compare them. These phrases are then used as queries for a search engine and the number of hits (matching documents) is recorded for each query. This process yields a vector of 128 numbers. If the number of hits for a query is x, then the corresponding element in the vector r is log(x + 1). Since there are five choices for each question, the expected score for random guessing is 20%. To answer a multiple-choice analogy question, vectors are created for the stem pair and each choice pair, and then cosines are calculated for the angles between the stem pair and each choice pair. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Secti on 6. A testing pair is classified by searching for its single nearest neighbor in the labeled training data. The best guess is the label for the training pair with the highest cosine. Thus their corpus was the set of all Web pages indexed by AltaVista. AltaVista also changed their policy toward automated searching, which is now forbidden. The WMTS is a distributed (multiprocessor) search engine, designed primarily for passage retrieval (although document retrieval is possible, as a special case of passage retrieval). The text and index require approximately one terabyte of disk space. Although AltaVista only gives a rough estimate of the number of matching documents, the WMTS gives exact counts of the number of matching passages. The performance of LRA significantly surpasses this combined system, but there is no real contest between these approaches, because we can simply add LRA to the combination, as a fourteenth module. LRA takes as input a set of word pairs and produces as output a measure of the relational similarity between any two of the input pairs. LRA relies on three resources, a search engine with a very large corpus of text, a broad-coverage thesaurus of synonyms, and an efficient implementation of SVD. We first present a short description of the core algorithm. The motivation for the alternate pairs is to handle cases where the original pairs cooccur rarely in the corpus. The hope is that we can find near analogies for the original pairs, such that the near analogies co-occur more frequently in the corpus. The danger is that the alternates may have different relations from the originals. The filtering steps above aim to reduce this risk. In our experiments, the input set contains from 600 to 2,244 word pairs. Before applying SVD, the vectors are completely non-negative, which implies that the cosine can only range from 0 to +1, but SVD introduces negative values, so it is possible for the cosine to be negative, although we have never observed this in our experiments. The Web pages cover a very wide range of topics, styles, genres, quality, and writing skill. The WMTS is well suited to LRA, because the WMTS scales well to large corpora (one terabyte, in our case), it gives exact frequency counts (unlike most Web search engines), it is designed for passage retrieval (rather than document retrieval), and it has a powerful query syntax. This thesaurus is available through an on-line interactive demonstration or it can be downloaded. We used the on-line demonstration, since the downloadable version seems to contain fewer words. For each word in the input set of word pairs, we automatically query the on-line demonstration and fetch the resulting list of synonyms. As a courtesy to other users of Lin’s on-line system, we insert a 20-second delay between each two queries. The parser was used to extract pairs of words and their grammatical relations. Words were then clustered into synonym sets, based on the similarity of their grammatical relations. Two words were judged to be highly similar when they tended to have the same kinds of grammatical relations with the same sets of words. Given a word and its part of speech, Lin’s thesaurus provides a list of words, sorted in order of decreasing attributional similarity. This sorting is convenient for LRA, since it makes it possible to focus on words with higher attributional similarity and ignore the rest. WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses. In LRA, SVD is used to reduce noise and compensate for sparseness. We will go through each step of LRA, using an example to illustrate the steps. Since there are six word pairs per question (the stem and five choices), the input consists of 2,244 word pairs. The LRA algorithm consists of the following 12 steps: alternates as follows. For each alternate pair, send a query to the WMTS, to find the frequency of phrases that begin with one member of the pair and end with the other. Sort the alternate pairs by the frequency of their phrases. This step tends to eliminate alternates that have no clear semantic relation. Alternate forms of the original pair quart:volume. The first column shows the original pair and the alternate pairs. The second column shows Lin’s similarity score for the alternate word compared to the original word. For example, the similarity between quart and pint is 0. The third column shows the frequency of the pair in the WMTS corpus. The fourth column shows the pairs that pass the filtering step (ie, step 2). The phrases cannot have more than max phrase words and there must be at least one word between the two members of the word pair. These phrases give us information about the semantic relations between the words in each pair. A phrase with no words between the two members of the word pair would give us very little information about the semantic relations (other than that the words occur together with a certain frequency in a certain order). Find patterns: For each phrase found in the previous step, build patterns from the intervening words. A pattern is constructed by replacing any or all or none of the intervening words with wild cards (one wild card can Some examples of phrases that contain quart:volume. Suffixes are ignored when searching for matching phrases in the WMTS corpus. At least one word must occur between quart and volume. At most max phrase words can appear in a phrase. quarts liquid volume volume in quarts quarts of volume volume capacity quarts quarts in volume volume being about two quarts quart total volume volume of milk in quarts quart of spray volume volume include measures like quart replace only one word). If a phrase is n words long, there are n − 2 intervening words between the members of the given word pair (eg, between quart and volume). Thus a phrase with n words generates 2(n−2) patterns. For each pattern, count the number of pairs (originals and alternates) with phrases that match the pattern (a wild card must match exactly one word). Typically there will be millions of patterns, so it is not feasible to keep them all. more weight to columns (patterns) with frequencies that vary substantially from one row (word pair) to the next, and less weight to columns that are uniform. We also apply the log transformation to frequencies, log(xi,j + 1). (Entropy is calculated with the original frequency values, before the log transformation is applied.) For all i and all j, replace the original value xi,j in X by the new value wj log(xi,j + 1). approximates the original matrix X, in the sense that it minimizes the � � approximation errors. We may think of this matrix UkEkVTk as a “smoothed” or “compressed” version of the original matrix. In the subsequent steps, we will be calculating cosines for row vectors. For this purpose, we can simplify calculations by dropping V. The cosine of two vectors is their dot product, after they have been normalized to unit length. The matrix XXT contains the dot products of all of the row vectors. We can find the dot product of the ith and jth row vectors by looking at the cell in row i, column j of the matrix XXT. This matrix has the same number of rows as X, but only k columns (instead of 2 x num patterns columns; in our experiments, that is 300 columns instead of 8,000). We can compare two word pairs by calculating the cosine of the corresponding row vectors in UkEk. The row vector for each word pair has been projected from the original 8,000 dimensional space into a new 300 dimensional space. Therefore we have (num filter + 1)2 ways to compare a version of A:B with a version of C:D. Look for the row vectors in UkEk that correspond to the versions of A:B and the versions of C:D and calculate the (num filter + 1)2 cosines (in our experiments, there are 16 cosines). For example, suppose A:B is quart:volume and C:D is mile:distance. Calculate relational similarity: The relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. The requirement that the cosine must be greater than or equal to the original cosine is a way of filtering out poor analogies, which may be introduced in step 1 and may have slipped through the filtering in step 2. Averaging the cosines, as opposed to taking their maximum, is intended to provide some resistance to noise. For these two pairs, the average of the selected cosines is 0. Steps 11 and 12 can be repeated for each two input pairs that are to be compared. This completes the description of LRA. The choice pair with the highest average cosine (the choice with the largest value in column 1), choice (b), is the solution for this question; LRA answers the question correctly. For comparison, column 2 gives the cosines for the original pairs and column 3 gives the highest cosine. For this particular SAT question, there is one choice that has the highest cosine for all three columns, choice (b), although this is not true in general. Note that the gap between the first choice (b) and the second choice (d) is largest for the average cosines (column 1). This suggests that the average of the cosines (column 1) is better at discriminating the correct choice than either the original cosine (column 2) or the highest cosine (column 3). LRA correctly answered 210 of the 374 questions; 160 questions were answered incorrectly and 4 questions were skipped, because the stem pair and its alternates were represented by zero vectors. As another point of reference, consider the simple strategy of always guessing the choice with the highest co-occurrence frequency. The idea here is that the words in the solution pair may occur together frequently, because there is presumably a clear and meaningful relation between the solution words, whereas the distractors may only occur together rarely because they have no meaningful relation. This strategy is signifcantly worse than random guessing. The opposite strategy, always guessing the choice pair with the lowest co-occurrence frequency, is also worse than random guessing (but not significantly). It appears that the designers of the SAT questions deliberately chose distractors that would thwart these two strategies. With 374 questions and six word pairs per question (one stem and five choices), there are 2,244 pairs in the input set. In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 8,976 pairs. In step 5, for each pair A:B, we add B:A, yielding 17,952 pairs. However, some pairs are dropped because they correspond to zero vectors (they do not appear together in a window of five words in the WMTS corpus). Also, a few words do not appear in Lin’s thesaurus, and some word pairs appear twice in the SAT questions (eg, lion:cat). The sparse matrix (step 7) has 17,232 rows (word pairs) and 8,000 columns (patterns), with a density of 5. % (percentage of nonzero values). All of the steps used a single CPU on a desktop computer, except step 3, finding the phrases for each word pair, which used a 16 CPU Beowulf cluster. Most of the other steps are parallelizable; with a bit of programming effort, they could also be executed on the Beowulf cluster. All CPUs (both desktop and cluster) were 2. The desktop computer had 2 GB of RAM and the cluster had a total of 16 GB of RAM. We generated the VSM-WMTS results by adapting the VSM to the WMTS. The pairwise differences in precision between LRA and the two VSM variations are also significant, but the difference in precision between the two VSM variations (42. %) is not significant. Although VSM-AV has a corpus 10 times larger than LRA’s, LRA still performs better than VSM-AV. Comparing VSM-AV to VSM-WMTS, the smaller corpus has reduced the score of the VSM, but much of the drop is due to the larger number of questions that were skipped (34 for VSM-WMTS versus 5 for VSM-AV). With the smaller corpus, many more of the input word pairs simply do not appear together in short phrases in the corpus. LRA is able to answer as many questions as VSM-AV, although it uses the same corpus as VSM-WMTS, because Lin’s thesaurus allows LRA to substitute synonyms for words that are not in the corpus. Since the WMTS is running locally, there is no need for delays. VSM-WMTS processed the questions in only one day. The SAT I test consists of 78 verbal questions and 60 math questions (there is also an SAT II test, covering specific subjects, such as chemistry). Analogy questions are only a subset of the 78 verbal SAT questions. If we assume that the difficulty of our 374 analogy questions is comparable to the difficulty of the 78 verbal SAT I questions, then we can estimate that the average college-bound senior would correctly answer about 57% of the 374 analogy questions. On this subset of the questions, LRA has a recall of 61. %, compared to a recall of 51. % on the other 184 questions. This indicates that we may be underestimating how well LRA performs, relative to college-bound senior high school students. There is no significant difference between LRA and human performance, but VSM-AV and VSM-WMTS are significantly below human-level performance. The parameter values were determined by trying a small number of possible values on a small set of questions that were set aside. Since LRA is intended to be an unsupervised learning algorithm, we did not attempt to tune the parameter values to maximize the precision and recall on the 374 SAT questions. We hypothesized that LRA is relatively insensitive to the values of the parameters. and vary each parameter, one at a time, while holding the remaining parameters fixed at their baseline values. This supports the hypothesis that the algorithm is not sensitive to the parameter values. Although a full run of LRA on the 374 SAT questions takes 9 days, for some of the parameters it is possible to reuse cached data from previous runs. We limited the experiments with num sim and max phrase because caching was not as helpful for these parameters, so experimenting with them required several weeks. However, we hypothesize that the drop in performance would be significant with a larger set of word pairs. More word pairs would increase the sample size, which would decrease the 95% confidence interval, which would likely show that SVD is making a significant contribution. Furthermore, more word pairs would increase the matrix size, which would give SVD more leverage. We are currently gathering more SAT questions to test this hypothesis. %), but the drop in precision is not significant. When the synonym component is dropped, the number of skipped questions rises from 4 to 22, which demonstrates the value of the synonym component of LRA for compensating for sparse data. Again, we believe that a larger sample size would show that the drop in precision is significant. If we eliminate both synonyms and SVD from LRA, all that distinguishes LRA from VSM-WMTS is the patterns (step 4). We can see the value of the automatically generated patterns by comparing LRA without synonyms and SVD (column 4) to VSM-WMTS (column 5). The ablation experiments support the value of the patterns (step 4) and synonyms (step 1) in LRA, but the contribution of SVD (step 9) has not been proven, although we believe more data will support its effectiveness. Nonetheless, the three components together result in a 16% increase in F (compare column 1 to 5). We know a priori that, if A:B::C:D, then B:A::D:C. For example, mason is to stone as carpenter is to wood implies stone is to mason as wood is to carpenter. The matrix is designed so that the row vector for A:B is different from the row vector for B:A only by a permutation of the elements. To discover the consequences of this design decision, we altered steps 5 and 6 so that symmetry is no longer preserved. In step 5, for each word pair A:B that appears in the input set, we only have one row. Thus the number of rows in the matrix dropped from 17,232 to 8,616. In step 6, we no longer have two columns for each pattern P, one for “word1 P word2” and another for “word2 P word1.” However, to be fair, we kept the total number of columns at 8,000. In step 4, we selected the top 8,000 patterns (instead of the top 4,000), distinguishing the pattern “word1 P word2” from the pattern “word2 P word1” (instead of considering them equivalent). Thus a pattern P with a high frequency is likely to appear in two columns, in both possible orders, but a lower frequency pattern might appear in only one column, in only one possible order. These changes resulted in a slight decrease in performance. % to 55.% and precision dropped from 56. The decrease is not statistically significant. In step 12 of LRA, the relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. That is, the average includes only those alternates that are “better” than the originals. Taking all alternates instead of the better alternates, recall drops from 56. % and precision drops from 56. It would be convenient if inspection of r gave us a simple explanation or description of the relation between A and B. For example, suppose the word pair ostrich:bird maps to the row vector r. It would be pleasing to look in r and find that the largest element corresponds to the pattern “is the largest” (ie, “ostrich is the largest bird”). Unfortunately, inspection of r reveals no such convenient patterns. We hypothesize that the semantic content of a vector is distributed over the whole vector; it is not concentrated in a few elements. To test this hypothesis, we modified step 10 of LRA. Instead of projecting the 8,000 dimensional vectors into the 300 dimensional space UkEk, we use the matrix UkEkVTk . This matrix yields the same cosines as UkEk, but preserves the original 8,000 dimensions, making it easier to interpret the row vectors. For each row vector in UkEkVTk , we select the N largest values and set all other values to zero. The idea here is that we will only pay attention to the N most important patterns in r; the remaining patterns will be ignored. If most of the semantic content is in the N largest elements of r, then setting the remaining elements to zero should have relatively little impact. The precision and recall are significantly below the baseline LRA until N ≥ 300 (95% confidence, Fisher Exact Test). In other words, for a typical SAT analogy question, we need to examine the top 300 patterns to explain why LRA selected one choice instead of another. We are currently working on an extension of LRA that will explain with a single pattern why one choice is better than another. We have had some promising results, but this work is not yet mature. However, we can confidently claim that interpreting the vectors is not trivial. It may be interesting to see how many of the manually generated patterns appear within the automatically generated patterns. If we require an exact match, 50 of the 64 manual patterns can be found in the automatic patterns. If we are lenient about wildcards, and count the pattern not the as matching * not the (for example), then 60 of the 64 manual patterns appear within the automatic patterns. This suggests that the improvement in performance with the automatic patterns is due to the increased quantity of patterns, rather than a qualitative difference in the patterns. Both of these patterns are included in the 4,000 patterns automatically generated by LRA. No adjustments were made to tune LRA to the noun-modifier pairs. LRA is used as a distance (nearness) measure in a single nearest neighbor supervised learning algorithm. This data set includes information about the part of speech and WordNet synset (synonym set; ie, word sense tag) of each word, but our algorithm does not use this information. These were relations that are typically expressed with longer phrases (three or more words), rather than noun-modifier word pairs. For example, in flu virus, the head noun (H) is virus and the modifier (M) is flu (*). In English, the modifier (typically a noun or adjective) usually precedes the head noun. In the description of purpose, V represents an arbitrary verb. In concert hall, the hall is for presenting concerts (V is present) or holding concerts (V is hold) (†). We make use of this grouping in the following experiments. The following experiments use single nearest neighbor classification with leave-one-out cross-validation. For leave-one-out cross-validation, the testing set consists of a single noun-modifier pair and the training set consists of the 599 remaining noun-modifiers. The data set is split 600 times, so that each noun-modifier gets a turn as the testing word pair. The predicted class of the testing pair is the class of the single nearest neighbor in the training set. As the measure of nearness, we use LRA to calculate the relational similarity between the testing pair and the training pairs. The factor of 16 comes from the alternate pairs, step 11 in LRA. There are 600 word pairs in the input set for LRA. In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 2,400 pairs. In step 5, for each pair A:B, we add B:A, yielding 4,800 pairs. However, some pairs are dropped because they correspond to zero vectors and a few words do not appear in Lin’s thesaurus. The sparse matrix (step 7) has 4,748 rows and 8,000 columns, with a density of 8. Macroaveraging calculates the precision, recall, and F for each class separately, and then calculates the average across all classes. Microaveraging combines the true positive, false positive, and false negative counts for all of the classes, and then calculates precision, recall, and F from the combined counts. Macroaveraging gives equal weight to all classes, but microaveraging gives more weight to larger classes. We use macroaveraging (giving equal weight to all classes), because we have no reason to believe that the class sizes in the data set reflect the actual distribution of the classes in a real corpus. Classification with 30 distinct classes is a hard problem. For example, agent and beneficiary both collapse to participant. On the 30 class problem, LRA with the single nearest neighbor algorithm achieves an accuracy of 39. Always guessing the majority class would result in an accuracy of 8. On the 5 class problem, the accuracy is 58.0% (348/600) and the macroaveraged F is 54. Always guessing the majority class would give an accuracy of 43.% (260/600). Another limitation is speed; it took almost 9 days for LRA to answer 374 analogy questions. However, with progress in computer hardware, speed will gradually become less of a concern. Also, the software has not been optimized for speed; there are several places where the efficiency could be increased and many operations are parallelizable. It may also be possible to precompute much of the information for LRA, although this would require substantial changes to the algorithm. The difference in performance between VSM-AV and VSM-WMTS shows that VSM is sensitive to the size of the corpus. Although LRA is able to surpass VSM-AV when the WMTS corpus is only about one tenth the size of the AV corpus, it seems likely that LRA would perform better with a larger corpus. The WMTS corpus requires one terabyte of hard disk space, but progress in hardware will likely make 10 or even 100 terabytes affordable in the relatively near future. For noun-modifier classification, more labeled data should yield performance improvements. With 600 noun-modifier pairs and 30 classes, the average class has only 20 examples. We expect that the accuracy would improve substantially with 5 or 10 times more examples. Unfortunately, it is time consuming and expensive to acquire hand-labeled data. Another issue with noun-modifier classification is the choice of classification scheme for the semantic relations. It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. SVD is only one of many methods for handling sparse, noisy data. In step 4 of LRA, we simply select the top num patterns most frequent patterns and discard the remaining patterns. Perhaps a more sophisticated selection algorithm would improve the performance of LRA. We have tried a variety of ways of selecting patterns, but it seems that the method of selection has little impact on performance. We hypothesize that the distributed vector representation is not sensitive to the selection method, but it is possible that future work will find a method that yields significant improvement in performance. This article has introduced a new method for calculating relational similarity, Latent Relational Analysis. The experiments demonstrate that LRA performs better than the VSM approach, when evaluated with SAT word analogy questions and with the task of classifying noun-modifier expressions. The VSM approach represents the relation between a pair of words with a vector, in which the elements are based on the frequencies of 64 hand-built patterns in a large corpus. LRA extends this approach in three ways: (1) The patterns are generated dynamically from the corpus, (2) SVD is used to smooth the data, and (3) a thesaurus is used to explore variations of the word pairs. We have presented several examples of the many potential applications for measures of relational similarity. Just as attributional similarity measures have proven to have many practical uses, we expect that relational similarity measures will soon become widely used. We believe that relational similarity plays a fundamental role in the mind and therefore relational similarity measures could be crucial for artificial intelligence. In future work, we plan to investigate some potential applications for LRA. It is possible that the error rate of LRA is still too high for practical applications, but the fact that LRA matches average human performance on SAT analogy questions is encouraging. Thanks to Michael Littman for sharing the 374 SAT analogy questions and for inspiring me to tackle them. Thanks to Egidio Terra, Charlie Clarke, and the School of Computer Science of the University of Waterloo, for giving us a copy of the Waterloo MultiText System and their Terabyte Corpus. Thanks to Dekang Lin for making his Dependency-Based Word Similarity lexicon available online. Thanks to Ted Pedersen for making his WordNet::Similarity package available. Thanks to Joel Martin for comments on the article. Thanks to the anonymous reviewers of Computational Linguistics for their very helpful comments and suggestions.\n"
     ]
    }
   ],
   "source": [
    "# viasually check the results\n",
    "\n",
    "paper_id = 5\n",
    "print(data.model_input[paper_id])\n",
    "print(\"---------------------------------------------\")\n",
    "print(data.body_good_quality[paper_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, add cited text spans \n",
    "for i in range(len(data)):\n",
    "    data['model_input'][i] = data['model_input'][i] + \" \".join(data['cited_text_spans'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3 - Clean the data\n",
    "Using 'checkQuality' function to select only those input sentences with high quality. \\\n",
    "This step only makes a difference if we use abstract or body with no previous quality check. \\\n",
    "See other idea for data cleansing: https://www.kaggle.com/sandeepbhogaraju/text-summarization-with-seq2seq-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA:\n",
    "# we can use the checkQuality function to select only those input sentences with high quality\n",
    "\n",
    "def cleanInput(df, truncate_center = 300):\n",
    "    \n",
    "    ''' Clean input so that low quality sentences don't feed the model\n",
    "        \n",
    "        Inputs: \n",
    "            \n",
    "            df: model data. It has to have a column named 'model_input' \n",
    "        \n",
    "            truncate_center: If the number of sentences in model input is larger than 'truncate_center', \n",
    "                truncate the center of model input - i.e., get only the \n",
    "                first and last x (= truncate_center/2) sentences.\n",
    "        \n",
    "        Output: df with 'cleaned' 'model_input' column \n",
    "        \n",
    "     '''\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    for paper_id in df.index:\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        input_sentences = []\n",
    "\n",
    "        # split input into sentences\n",
    "        input_sentences_tok = df.model_input[paper_id].replace(\"et al.\", \"et al\").replace(\"e.g.\", \"eg\").replace(\"eg.\", \"eg\").replace(\"i.e.\", \"ie\").replace(\"ie.\", \"ie\")\n",
    "        input_sentences_tok = input_sentences_tok.replace(\".1\", \". \").replace(\".2\", \". \").replace(\".3\", \".\").replace(\".4\", \". \").replace(\".5\", \". \").replace(\".6\", \". \").replace(\".7\", \". \").replace(\".8\", \". \").replace(\".9\", \". \")\n",
    "\n",
    "        # tokenize input sentences\n",
    "        input_sentences_tok = tokenizer.tokenize(input_sentences_tok)\n",
    "\n",
    "        # truncate too large input sentences (larger than 512 throws an error - rare cases) and remove \"?\"\n",
    "        input_sentences_tok = [s.replace(\"?\", \"\") for s in input_sentences_tok if len(s) <= 512]\n",
    "        \n",
    "        \n",
    "        # truncate the center of the body for large bodies\n",
    "        if len(input_sentences_tok) > truncate_center:\n",
    "            input_sentences_tok = input_sentences_tok[:int(truncate_center/2)] + input_sentences_tok[-int(truncate_center/2):]\n",
    "\n",
    "        # find input sentences with quality\n",
    "        for i in range(len(input_sentences_tok)):\n",
    "            if len(input_sentences_tok[i])>0:\n",
    "                if checkQuality(input_sentences_tok[i]):\n",
    "                    input_sentences.append(input_sentences_tok[i])\n",
    "\n",
    "        # fill our large/original dataframe with the cleaned input sentences \n",
    "        df.model_input[paper_id] = \" \".join(input_sentences)\n",
    "        \n",
    "        # keep track of time to run \n",
    "        if paper_id % 50 == 0:\n",
    "#             print(paper_id, \"secs to run:\", time.time() - start)\n",
    "            start = time.time()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this step is only necessary if we use abstract or body with no previous quality check\n",
    "# data = cleanInput(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"data_organized300_rouge1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4 - Divide the data into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "\n",
    "train_pct = 0.9\n",
    "test_pct = 0.05\n",
    "\n",
    "data = data.sample(len(data), random_state=20)\n",
    "train_sub = int(len(data) * train_pct)\n",
    "test_sub = int(len(data) * test_pct) + train_sub\n",
    "\n",
    "train_df = data[0:train_sub]\n",
    "test_df = data[train_sub:test_sub]\n",
    "val_df = data[test_sub:]\n",
    "\n",
    "train_input = list(train_df['model_input'])\n",
    "test_input = list(test_df['model_input'])\n",
    "val_input = list(val_df['model_input'])\n",
    "\n",
    "train_output = list(train_df['model_output'])\n",
    "test_output = list(test_df['model_output'])\n",
    "val_output = list(val_df['model_output'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>golden</th>\n",
       "      <th>body_good_quality</th>\n",
       "      <th>cited_text_spans</th>\n",
       "      <th>model_input</th>\n",
       "      <th>model_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>Conditional random fields (Lafferty et al., 20...</td>\n",
       "      <td>Finding linguistic structure in raw text is no...</td>\n",
       "      <td>[Recent work by Smith and Eisner (2005) on con...</td>\n",
       "      <td>Contrastive Estimation: Training Log-Linear Mo...</td>\n",
       "      <td>Finding linguistic structure in raw text is no...</td>\n",
       "      <td>[Finding linguistic structure in raw text is n...</td>\n",
       "      <td>Finding linguistic structure in raw text is no...</td>\n",
       "      <td>Contrastive Estimation: Training Log-Linear Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Recognizing analogies, synonyms, anto nyms, an...</td>\n",
       "      <td>A pair of words (petrify:stone) is analogous t...</td>\n",
       "      <td>[Language modeling (Chen and Goodman, 1996), n...</td>\n",
       "      <td>A Uniform Approach to Analogies Synonyms Anton...</td>\n",
       "      <td>A pair of words (petrify:stone) is analogous t...</td>\n",
       "      <td>[A pair of words (petrify:stone) is analogous ...</td>\n",
       "      <td>A pair of words (petrify:stone) is analogous t...</td>\n",
       "      <td>A Uniform Approach to Analogies Synonyms Anton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>We seek a knowledge-free method for inducing m...</td>\n",
       "      <td>A multiword unit (MWU) is a connected collocat...</td>\n",
       "      <td>[Schone and Jurafsky (2001) applied LSA to the...</td>\n",
       "      <td>Is Knowledge-Free Induction Of Multiword Unit ...</td>\n",
       "      <td>In other words, MWUs are typically non-composi...</td>\n",
       "      <td>[In other words, MWUs are typically non-compos...</td>\n",
       "      <td>In other words, MWUs are typically non-composi...</td>\n",
       "      <td>Is Knowledge-Free Induction Of Multiword Unit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>hanb@student.unimelb.edu.au tb@ldwin.net Abstr...</td>\n",
       "      <td>Twitter and other micro-blogging services are ...</td>\n",
       "      <td>[We use unsupervised methods to build a pipeli...</td>\n",
       "      <td>Lexical Normalisation of Short Text Messages: ...</td>\n",
       "      <td>The quality of messages varies significantly, ...</td>\n",
       "      <td>[Additionally, we evaluate using the BLEU scor...</td>\n",
       "      <td>The quality of messages varies significantly, ...</td>\n",
       "      <td>Lexical Normalisation of Short Text Messages: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>In this paper we present a novel, customizable...</td>\n",
       "      <td>The goal of recent Information Extraction (IE)...</td>\n",
       "      <td>[, Indeed, the analysis produced by existing s...</td>\n",
       "      <td>Using Predicate-Argument Structures For Inform...</td>\n",
       "      <td>The goal of recent Information Extraction (IE)...</td>\n",
       "      <td>[The goal of recent Information Extraction (IE...</td>\n",
       "      <td>The goal of recent Information Extraction (IE)...</td>\n",
       "      <td>Using Predicate-Argument Structures For Inform...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              abstract  \\\n",
       "457  Conditional random fields (Lafferty et al., 20...   \n",
       "798  Recognizing analogies, synonyms, anto nyms, an...   \n",
       "391  We seek a knowledge-free method for inducing m...   \n",
       "376  hanb@student.unimelb.edu.au tb@ldwin.net Abstr...   \n",
       "387  In this paper we present a novel, customizable...   \n",
       "\n",
       "                                                  body  \\\n",
       "457  Finding linguistic structure in raw text is no...   \n",
       "798  A pair of words (petrify:stone) is analogous t...   \n",
       "391  A multiword unit (MWU) is a connected collocat...   \n",
       "376  Twitter and other micro-blogging services are ...   \n",
       "387  The goal of recent Information Extraction (IE)...   \n",
       "\n",
       "                                             citations  \\\n",
       "457  [Recent work by Smith and Eisner (2005) on con...   \n",
       "798  [Language modeling (Chen and Goodman, 1996), n...   \n",
       "391  [Schone and Jurafsky (2001) applied LSA to the...   \n",
       "376  [We use unsupervised methods to build a pipeli...   \n",
       "387  [, Indeed, the analysis produced by existing s...   \n",
       "\n",
       "                                                golden  \\\n",
       "457  Contrastive Estimation: Training Log-Linear Mo...   \n",
       "798  A Uniform Approach to Analogies Synonyms Anton...   \n",
       "391  Is Knowledge-Free Induction Of Multiword Unit ...   \n",
       "376  Lexical Normalisation of Short Text Messages: ...   \n",
       "387  Using Predicate-Argument Structures For Inform...   \n",
       "\n",
       "                                     body_good_quality  \\\n",
       "457  Finding linguistic structure in raw text is no...   \n",
       "798  A pair of words (petrify:stone) is analogous t...   \n",
       "391  In other words, MWUs are typically non-composi...   \n",
       "376  The quality of messages varies significantly, ...   \n",
       "387  The goal of recent Information Extraction (IE)...   \n",
       "\n",
       "                                      cited_text_spans  \\\n",
       "457  [Finding linguistic structure in raw text is n...   \n",
       "798  [A pair of words (petrify:stone) is analogous ...   \n",
       "391  [In other words, MWUs are typically non-compos...   \n",
       "376  [Additionally, we evaluate using the BLEU scor...   \n",
       "387  [The goal of recent Information Extraction (IE...   \n",
       "\n",
       "                                           model_input  \\\n",
       "457  Finding linguistic structure in raw text is no...   \n",
       "798  A pair of words (petrify:stone) is analogous t...   \n",
       "391  In other words, MWUs are typically non-composi...   \n",
       "376  The quality of messages varies significantly, ...   \n",
       "387  The goal of recent Information Extraction (IE)...   \n",
       "\n",
       "                                          model_output  \n",
       "457  Contrastive Estimation: Training Log-Linear Mo...  \n",
       "798  A Uniform Approach to Analogies Synonyms Anton...  \n",
       "391  Is Knowledge-Free Induction Of Multiword Unit ...  \n",
       "376  Lexical Normalisation of Short Text Messages: ...  \n",
       "387  Using Predicate-Argument Structures For Inform...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Summarize with Pre-Trained Pegasus (no fine-tuning)\n",
    "    \n",
    "In this step we will do the following:\n",
    " \n",
    "Step 3.1 - Instantiate the model\\\n",
    "Step 3.2 - Instatiate the metric of interest \\\n",
    "Step 3.3 - Compute the summaries\\\n",
    "Step 3.4 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 - Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# model_name = 'google/pegasus-large'\n",
    "\n",
    "model_name = 'fine_tuned'\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "# This is the PEGASUS Model with a language modeling head. Can be used for summarization. \n",
    "# This model inherits from PreTrainedModel. \n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # OTHER OPTIONS BELOW"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OTHER OPTIONS: Possible pre-trained models\n",
    "![image.png](attachment:image.png)\n",
    "Source: https://pub.towardsai.net/summarization-using-pegasus-model-with-the-transformers-library-553cd0dc5c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 - Instatiate the metric of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate rouge score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3 - Compute the summaries and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 65.41269397735596\n",
      "5 419.13114500045776\n",
      "10 3082.1335821151733\n",
      "15 603.0696787834167\n",
      "20 438.3871431350708\n",
      "25 403.2465629577637\n",
      "30 493.0546989440918\n",
      "35 535.3676171302795\n",
      "40 471.82842993736267\n",
      "45 389.94857120513916\n",
      "50 414.92228984832764\n"
     ]
    }
   ],
   "source": [
    "#INFERENCE\n",
    "\n",
    "## Sample with no fine-tunning \n",
    "\n",
    "import time\n",
    "\n",
    "# create empry lists to store rouge scores\n",
    "r1_precision = []\n",
    "r2_precision = []\n",
    "rL_precision = []\n",
    "\n",
    "r1_recall = []\n",
    "r2_recall = []\n",
    "rL_recall = []\n",
    "\n",
    "r1_fmeasure = []\n",
    "r2_fmeasure = []\n",
    "rL_fmeasure = []\n",
    "\n",
    "val_model_output = []\n",
    "\n",
    "# start counting seconds to keep track of time \n",
    "start = time.time()\n",
    "\n",
    "# loop over validation set\n",
    "for sample_id in range(len(val_input)):\n",
    "    \n",
    "    # get input (body OR abstract + cited text spans) - scisummnet uses abstract, we want to use body\n",
    "    sample_input = val_input[sample_id]\n",
    "    \n",
    "    # tokenize it\n",
    "    inputs = tokenizer([sample_input], max_length=1024, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # 'max_length': Pad to a maximum length specified with the argument max_length \n",
    "    # or to the maximum acceptable input length for the model if that argument is not provided.\n",
    "\n",
    "    # generate Summary\n",
    "    summary_ids = model.generate(inputs['input_ids'])\n",
    "    \n",
    "    # decode summary\n",
    "    sample_output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    \n",
    "    # store summary\n",
    "    val_model_output = val_model_output + sample_output\n",
    "    \n",
    "    # get reference (gold) summary \n",
    "    sample_reference = val_output[sample_id]\n",
    "    \n",
    "    #calculate rouge score\n",
    "    scores = scorer.score(str(sample_reference), str(sample_output))\n",
    "    \n",
    "    r1_precision.append(scores['rouge1'][0])\n",
    "    r1_recall.append(scores['rouge1'][1])\n",
    "    r1_fmeasure.append(scores['rouge1'][2])\n",
    "    \n",
    "    r2_precision.append(scores['rouge2'][0])\n",
    "    r2_recall.append(scores['rouge2'][1])\n",
    "    r2_fmeasure.append(scores['rouge2'][2])\n",
    "    \n",
    "    rL_precision.append(scores['rougeL'][0])\n",
    "    rL_recall.append(scores['rougeL'][1])\n",
    "    rL_fmeasure.append(scores['rougeL'][2])\n",
    "    \n",
    "    if sample_id % 5 == 0:\n",
    "        print(sample_id, time.time() - start)\n",
    "        start = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4 - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 R1        R2        RL\n",
      "precision  0.427665  0.127138  0.234739\n",
      "recall     0.280574  0.077877  0.147930\n",
      "fmeasure   0.325131  0.092175  0.173313\n"
     ]
    }
   ],
   "source": [
    "# compute score statistics\n",
    "\n",
    "all_socores = {'R1': [stats.mean(r1_precision), stats.mean(r1_recall), stats.mean(r1_fmeasure)],\n",
    "        'R2': [stats.mean(r2_precision), stats.mean(r2_recall), stats.mean(r2_fmeasure)],\n",
    "        'RL': [stats.mean(rL_precision), stats.mean(rL_recall), stats.mean(rL_fmeasure)]      \n",
    "        }\n",
    "\n",
    "all_socores_df = pd.DataFrame(all_socores, columns = ['R1', 'R2', 'RL'], index=['precision','recall','fmeasure'])\n",
    "\n",
    "print(all_socores_df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input:\n",
      "This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences. To discriminate senses, an understander can consider a diversity of information, including syntactic tags, word frequencies, collocations, semantic context, role-related expectations, and syntactic restrictions. However, current approaches make use of only small subsets of this information. Here we will describe how to use the whole range of information. Our discussion will include how the preference cues relate to general lexical and conceptual knowledge and to more specialized knowledge of collocations and contexts. We will describe a method of combining cues on the basis of their individual specificity, rather than a fixed ranking among cue-types. We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences. Many problems in applied natural language processing — including information retrieval, database generation from text, and machine translation — hinge on relating words to other words that are similar in meaning. Current approaches to these applications are often word-based — that is, they treat words in the input as strings, mapping them directly to other words. However, the fact that many words have multiple senses and different words often have similar meanings limits the accuracy of such systems. An alternative is to use a knowledge representation, or interlingua, to reflect text content, thereby separating text representation from the individual words. These approaches can, in principle, be more accurate than word-based approaches, but have not been sufficiently robust to perform any practical text processing task. Their lack of robustness is generally due to the difficulty in building knowledge bases that are sufficient for broad-scale processing. But a synthesis is possible. Robustness need not be sacrificed, however, because this tagging does not require a full-blown semantic analysis. Suppose that the input is (1): The agreement reached by the state and the EPA provides for the safe storage of the waste. The analysis would provide an application with the following information. The primary goal of this paper, then, is to describe in detail methods and knowledge that will enable a language analyzer to tag each word with its sense. The principles that make up the approach are completely general, however, and not just specific to TRUMP. Because determining this information accurately is knowledge-intensive, the analyzer should be as flexible as possible, requiring a minimum amount of customization for different domains. One way to gain such flexibility is give the system enough generic information about word senses and semantic relations so that it will be able to handle texts spanning more than a single domain. While having an extensive grammar and lexicon is essential for any system's domain independence, this increased flexibility also introduces degrees of ambiguity not frequently addressed by current NLP work. Typically, the system will have to choose from several senses for each word. For example, we found that TRUMP's base of nearly 10,000 root senses and 10,000 derivations provides an average of approximately four senses for each word of a sentence taken from the Wall Street Journal. No simple recipe can resolve the general problem of lexical ambiguity. Although semantic context and selectional restrictions provide good cues to disambiguation, they are neither reliable enough, nor available quickly enough, to be used alone. The approach incorporates a number of innovations, including: Although improvements to our system are ongoing, it already interprets arbitrary text and makes coarse word sense selections reasonably well. No other system, to our knowledge, has been as successful. We will now review word sense discrimination and the determination of role relations. Afterward, we will discuss the results of our methods and the avenues for improvement that remain. The problem of word sense discrimination is to choose, for a particular word in a particular context, which of its possible senses is the \"correct\" one for the context. Information about senses can come from a wide variety of sources: Of course, not all these cues will be equally useful. We have found that, in general, the most important sources of information for word sense discrimination are syntactic tags, morphology, collocations, and word associations. Role-related expectations are also important, but to a slightly lesser degree. Syntactic tags are very important, because knowing the intended part of speech is often enough to identify the correct sense. Morphology is also a strong cue to discrimination because certain sense—affix combinations are preferred, deprecated, or forbidden. The verb agree can mean either 'concur,' 'benefit,' or 'be equivalent' and, in general, adding the affix -ment to a verb creates a noun corresponding either to an act, or to its result, its object, or its associated state. However, of the twelve possible combinations of root sense and affix sense, in practice only four occur: agreement can refer only to the act, object, or result in the case of the 'concur' sense of agree or the state in the case of the 'equivalence' sense of agree. For example, when paired with increase, the preposition in clearly denotes a patient rather than a temporal or spatial location, or a direction. Word associations such as bank/ money similarly create a bias for the related senses. Despite their apparent strength, however, the preferences created by these cues are not absolute, as other cues may defeat them. For example, although normally the collocation wait on means 'serve' (Mary waited on John), the failure of a role-related expectation, such as that the BENEFICIARY be animate, can override this preference (Mary waited on the steps). The selection of a role relationship can both influence and be influenced by the selection of word senses, because preferences partially constrain the various combinations of a role, its holder, and the filler. Preferences based on role-related expectations are often only a weak cue because they are primarily for verbs and not normally very restrictive. Although generally a weak cue, role-related preferences are quite valuable for the disambiguation of prepositions. In our view, prepositions should be treated essentially the same as other words in the lexicon. Because the meaning of a preposition actually names a relation, relation-based cues are a good source of information for disambiguating them. The problem of determining role relationships entangles word sense discrimination with the problem of syntactic attachment. The attachment problem is a direct result of the ambiguity in determining whether a concept is related to an adjacent object, or to some enveloping structure that incorporates the adjacent object. Such rules can be either syntactic, semantic, or pragmatic. Syntactic rules attempt to solve the attachment problem independent of the sense discrimination problem. Semantic rules, by contrast, intertwine the problems of discrimination and attachment; one must examine all combinations of senses and attachments to locate the semantically best one. Such rules normally also collapse the attachment problem into the conceptual role filling problem. Pragmatic rules also intermingle sense discrimination and attachment, but consider the context of the utterance. The accuracy of systems with fixed-order rules is limited by the fact that it is not always possible to strictly order a set of rules independent of the context. The existence of a conceptually similar object in the context (such as \"the morning trial\") can also create an expectation for the grouping \"hearing in the afternoon,\" as in Example 3 below. The judge had to leave town for the day. He found a replacement to take over his morning trial, but couldn't find anyone else that was available. He called the courthouse and cancelled the hearing in the afternoon. The presence of different lexical items or different objects in the discourse focus may strengthen or weaken the information provided by an individual rule. But first, we need to discuss the sources of knowledge that enable a system to identify these cues. To identify preference cues such as morphology, word frequency, collocations, semantic contexts, syntactic expectations, and conceptual relations in unrestricted texts, a system needs a large amount of knowledge in each category. In most cases, this just means that the understander's lexicon and conceptual hierarchy must include preference information, although processing concerns suggest moving some information out of these structures and into data modules specific to a particular process, such as identifying collocations. TRUMP obtains the necessary knowledge from a moderately sized lexicon (8,775 unique roots), specifically designed for use in language understanding, and a hierarchy of nearly 1,000 higher-level concepts, overlaid with approximately 40 concept-cluster definitions. It also uses a library of over 1,400 collocational patterns. We will consider each in turn. Development of TRUMP's current lexicon followed an experiment with a moderatelysized, commercially available lexicon (10,000 unique roots), which demonstrated many substantive problems in applying lexical resources to text processing. Although the lexicon had good morphological and grammatical coverage, as well as a thesaurus-based semantic representation of word meanings, it lacked reasonable information for discriminating senses. The current lexicon, although roughly the same size as the earlier one, has been designed to better meet the needs of producing semantic representations of text. The lexicon features a hierarchy of 1,000 parent concepts for encoding semantic preferences and restrictions, sense-based morphology and subcategorization, a distinction between primary and secondary senses and senses that require particular \"triggers\" or appear only in specific contexts, and a broad range of collocational information. (An alternative would have been to give up discriminating senses that the lexicon does not distinguish; cf. At this time, the lexicon contains about 13,000 senses and 10,000 explicit derivations. Each lexical entry provides information about the morphological preferences, sense preferences, and syntactic cues associated with a root, its senses, and their possible derivations. An entry also links words to the conceptual hierarchy by naming the conceptual parent of each sense. If necessary, an entry can also specify the composition of common phrases, such as collocations, that have the root as their head. TRUMP's lexicon combines a core lexicon with dynamic lexicons linked to specialized conceptual domains, collocations, and concretions. The core lexicon contains the generic, or context-independent, senses of each word. The system considers these senses whenever a word appears in the input. The dynamic lexicons contain word senses that normally appear only within a particular context; these senses are considered only when that context is active. This distinction is a product of experience; it is conceivable that a formerly dynamic sense may become static, as when military terms creep into everyday language. The partitioning of the lexicon into static and dynamic components reduces the number of senses the system must consider in situations where the context does not trigger some dynamic sense. As a result, the lexical retrieval mechanism never forces the system to use a sense just because the domain has preselected it. The core lexicon, by design, includes only coarse distinctions between word senses. This means that, for a task such as generating databases from text, task-specific processing or inference must augment the core lexical knowledge, but problems of considering many nuances of meaning or low-frequency senses are avoided. For example, the financial sense of issue (eg, a new security) falls under the same core sense as the latest issue of a magazine. The 'progeny' and 'exit' senses of issue are omitted from the lexicon. The idea is to preserve in the core lexicon only the common, coarse distinctions among senses (cf. Each entry has a part of speech, : POS, and a set of core senses, : SENSES. Each sense has a : TYPE field that indicates *primary* for a preferred (primary) sense and *secondary* for a deprecated (secondary) sense. The general rule for determining the : TYPE of a sense is that secondary senses are those that the semantic interpreter should not select without specific contextual information, such as the failure of some selectional restriction pertaining to the primary sense. For example, the word yard can mean an enclosed area, a workplace, or a unit of measure, but in the empty context, the enclosed-area sense is assumed. This classification makes clear the relative frequency of the senses. The :PAR field links each word sense to its immediate parent in the semantic hierarchy. Explicit derivations, such as \"period-ic-al-x,\" are indicated by roots followed by endings and additional type specifiers. These derivative lexical entries do \"double duty\" in the lexicon: an application program can use the derivation as well as the semantics of the derivative form. The : ASSOC field, not currently used in processing, includes the lexicographer's choice of synonym or closely related words for each sense. The : SYNTAX field encodes syntactic constraints and subcategorizations for each sense. When senses share constraints (not the case in this example), they can be encoded at the level of the word entry. The grammatical knowledge base of the system relates these subcategories to semantic roles. The : G-DERIV and :S-DERIV fields mark morphological derivations. The former, which is NIL in the case of issue to indicate no derivations, encodes the derivations at the word root level, while the latter encodes them at the sense preference level. For example, the : S-DERIV constraint allows issuance to derive from either of the first two senses of the verb, with issuer and issuable deriving only from the 'giving' sense. The lexical entries for issue. The derivation triples encode the form of each affix, the resulting syntactic category (usually redundant), and the \"semantic transformation\" that applies between the core sense and the resulting sense. For example, the triple (-er noun tr_act or) in the entry for issue says that an issuer plays the ACTOR role of the first sense of the verb issue. Because derivations often apply to multiple senses and often result in different semantic transformations (for example, the ending -ion can indicate the act of performing some action, the object of the action, or the result of the action), a lexical entry can mark certain interpretations of a morphological derivation as primary or secondary. situations, the dynamic lexicons contain senses that are active only in a particular context. Although these senses require triggers, a sense and its trigger may occur just as frequently as a core sense. For example, TRUMP's military lexicon contains a sense of engage that means 'attack.' However, the system does not consider this sense unless the military domain is active. Similarly, the collocational lexicon contains senses triggered by well-known patterns of words; for example, the sequence take effect activates a sense of take meaning 'transpire.' Concretions activate specializations of the abstract sense of a word when it occurs with an object of a specific type. For example, in the core lexicon, the verb project has the abstract sense 'transfer'; however, if its object is a sound, the system activates a sense corresponding to a 'communication event,' as in She projected her voice. Encoding these specializations in the core lexicon would be problematic, because then a system would be forced to resolve such nuances of meaning even when there was not enough information to do so. Dynamic lexicons can provide much finer distinctions among senses than the core lexicon, because they do not increase the amount of ambiguity when their triggering context is inactive. Together, the core and dynamic lexicons provide the information necessary to recognize morphological preferences, sense preferences, and syntactic cues. They also provide some of the information required to verify and interpret collocations. The concept hierarchy serves several purposes. First, it associates word senses that are siblings or otherwise closely related in the hierarchy, thus providing a thesaurus for information retrieval and other tasks (cf. In a sense tagging system, these associations can help determine the semantic context. Third, it allows role-based preferences, wherever possible, to apply to groups of word senses rather than just individual lexical entries. c-recording is the parent concept for activities involving the storage of information, namely, the following verb senses: book2 cataloguel clockl compilel date3 documentl enter3 indexl inputl keyl logl recordl In a concept definition, the :PAR fields link the concept to its immediate parents in the hierarchy. The :ASSOC field links the derived instances of the given concept to their places in the hierarchy. The conceptual definition of c-made-of -rel. Each :ROLE-PLAY indicates the parent's name for a role along with the concept's specialization of it. For example, c-re cording specializes its inherited OBJECT role as PATIENT. The :RELS and : PREF fields identify which combinations of concept, role, and filler an understander should expect (and hence prefer). TRUMP's hierarchy also allows the specification of such preferences from the perspective of the filler, where they can be made more general. The hierarchy also permits the specification of such preferences from the perspective of the relation underlying a role. Although collocations seem to have a semantic basis, many collocations are best recognized by their syntactic form. Thus, for current purposes, we limit the use of the term \"collocation\" to sense preferences that result from these well-defined syntactic constructions. The top ten co-occurences with take. and the verb-complement combination make the team are both collocation-inducing expressions. Excluded from this classification are unstructured associations among senses that establish the general semantic context, for example, courtroom/defendant. Collocations often introduce dynamic word senses, ie, ones that behave compositionally, but occur only in the context of the expression, making it inappropriate for the system to consider them outside that context. For example, the collocation hang from triggers a sense of from that marks an INSTRUMENT. In other cases, a collocation simply creates preferences for selected core senses, as in the pairing of the 'opportunity' sense of break with the 'cause-to-have' sense of give in give her a break. There is also a class of collocations that introduce a noncompositional sense for the entire expression, for example, the collocation take place invokes a sense 'transpire.' To recognize collocations during preprocessing, TRUMP uses a set of patterns, each of which lists the root words or syntactic categories that make up the collocation. Initially, we acquired patterns for verb-object collocations by analyzing lists of root word pairs that were weighted for relative co-occurrence in a corpus of articles from the Dow Jones News Service (cf. Note that the collocation \"take action\" appears both in its active form (third in the list), as well as its passive, actions were taken (fifth in the list). From an examination of these lists and the contexts in which the pairs appeared in the corpus, we constructed the patterns used by TRUMP to identify collocations. Then, using the patterns as a guide, we added lexical entries for each collocation. That is, instead of expressing collocations as word-templates, the lexicon groups together collocations that combine the same sense of the head verb with particular senses or higher-level concepts (cf. This approach better addresses the fact that collocations do have a semantic basis, capturing general forms such as give him or her (some temporal object), which underlies the collocations give month, give minute, and give time. The last source of sense preferences we need to consider is the semantic context. Since language producers select senses that group together semantically, a language analyzer should prefer senses that share a semantic association. Thus, our system provides a cluster mechanism for representing and identifying groups of senses that are associated in some unspecified way. A cluster is a set of the senses associated with some central concept. A cluster may contain concepts or other clusters. TRUMP's knowledge base contains three types of clusters: categorial, functional, and situational. The simplest type of cluster is the categorial cluster. These clusters consist of the sets of all senses sharing a particular conceptual parent. Since the conceptual hierarchy already encodes these clusters implicitly, we need not write formal cluster definitions for them. Obviously, a sense will belong to a number of categorial clusters, one for each element of its parent chain. The second type of cluster is the functional cluster. These consist of the sets of all senses sharing a specified functional relationship. For example, our system has a small number of part-whole clusters that list the parts associated with the object named by the cluster. The third type of cluster, the situational cluster, encodes general relationships among senses on the basis of their being associated with a common setting, event, The definition of the cluster cl-egg. The definition of the cluster cl-courtroom. Since a cluster's usefulness is inversely proportional to its size, these clusters normally include only senses that do not occur outside the clustered context or that strongly suggest the clustered context when they occur with some other member of the cluster. Thus, situational clusters are centered upon fairly specific ideas and may correspondingly be very specific with respect to their elements. It is not unusual for a word to be contained in a cluster while its synonyms are not. Situational clusters capture the associations found in generic descriptions (cf. (As mentioned above, the conceptual hierarchy is the best place for encoding known role-related expectations.) In fact, clusters capture most of the useful associations found in scripts or semantic networks, but lack many of the disadvantages of using networks. For example, because clusters do not specify what the exact nature of any association is, learning new clusters from previously processed sentences would be fairly straightforward, in contrast to learning new fragments of network. The relevant difference is that a cluster is cautious because it must explicitly specify all its elements. A marker passer takes the opposite stance, however, considering all paths up, down, and across the network unless it is explicitly constrained. Thus a marker passer might find the following dubious path from the 'written object' sense of book to the 'part-of-a-plant' sense of leaf: [book made-of paper] [paper made-from wood] [tree made-of wood] [tree has-part leaf] whereas no cluster would link these entities, unless there had been some prior evidence of a connection. From the lexical entries, the underlying concept hierarchy, and the specialized entries for collocation and clusters just described, a language analyzer can extract the information that establishes preferences among senses. There is a wide variety of information about which sense is the correct one, and the challenge is to decide when and how to use this information. The danger of a combinatorial explosion of possibilities makes it advantageous to try to resolve ambiguities as early as possible. Then, the parse and semantic interpretation of the text will provide the cues necessary to complete the task of resolution. Without actually parsing a text, a preprocessor can identify for each word its morphology,2 its syntactic tag or tags,3 and whether it is part of a collocation; for each sense, it can identify whether the sense is preferred or deprecated and whether it is supported by a cluster. To identify whether the input satisfies the expectations created by syntactic cues or whether it satisfies role-related expectations, the system must first perform some syntactic analysis of the input. Identifying these properties must come after parsing, because recognizing them requires both the structural cues provided by parsing and a semantic analysis of the text. In our system, processing occurs in three phases: morphology, preprocessing, and parsing and semantic interpretation. Analysis of a text begins with the identification of the morphological features of each word and the retrieval of the (core) senses of each word. Then, the input passes through a special preprocessor that identifies parse-independent semantic preferences (ie, syntactic tags, collocations, and clusters) and makes a preliminary selection of word senses. This selection process eliminates those core senses that are obviously inappropriate and triggers certain The system architecture. In the third phase, TRUMP attempts to parse the input and at the same time produce a \"preferred\" semantic interpretation for it. Since the preferred interpretation also fixes the preferred sense of each word, it is at this point that the text can be given semantic tags, thus allowing sense-based information retrieval. Afterward we will discuss how the system combines the preferences it identifies. The first step in processing an input text is to determine the root, syntactic features, and affixes of each word. This information is necessary both for retrieving the word's lexical entries and for the syntactic tagging of the text during preprocessing. Once morphological analysis of a word is complete, the system retrieves (or derives) the corresponding senses and establishes initial semantic preferences for the primary senses. The entry also says that derivations (listed in the :S-DERIV field) agree1+ment and agree2-1-able are preferred, derivations agreel±able and agree3+ment are deprecated, and all other sense-affix combinations (excepting inflections) have been disallowed. During morphological analysis, the system retrieves only the most general senses. It waits until the preprocessor or the parser identifies supporting evidence before it retrieves word senses specific to a context, such as a domain, a situation, or a collocation. In most cases this approach helps reduce the amount of ambiguity. \"multiple meanings are activated in frequency-coded order\" and that low-frequency senses are handled by a second retrieval process that accumulates evidence for those senses and activates them as necessary. Once the system determines the morphological analysis of each word, the next step in preprocessing is to try to determine the correct part of speech for the word. Stochastic taggers look at small groups of words and pick the most likely assignment of tags, determined by the frequency of alternative syntactic patterns in similar texts. To allow for the fact that the tagger may err, as part of the tagging process the system makes a second pass through the text to remove some systematic errors that result from biases common to statistical approaches. You really need the Campbell Soups of the world to be interested in your magazine. In this second pass, the system applies a few rules derived from our grammar and resets the tags where necessary. For example, to correct for the noun versus verb overgeneralization, whenever a word that can be either a noun or a verb gets tagged as just a noun, the corrector lets it remain ambiguous unless it is immediately preceded by a determiner (a good clue for nouns), or it is immediately preceded by a plural noun or a preposition, or is immediately followed by a determiner (three clues that suggest a word may be a verb). The system is able to correct for all the systematic errors we have identified thus far using just nine rules of this sort. After tagging, the preprocessor eliminates all senses corresponding to unselected parts of speech. Following the syntactic filtering of senses, TRUMP's preprocessor identifies collocations and establishes semantic preferences for the senses associated with them. In this stage of preprocessing, the system recognizes the following types of collocations: To recognize a collocation, the preprocessor relies on a set of simple patterns, which match the general syntactic context in which the collocation occurs. For example, the system recognizes the collocation \"take profit\" found in Example 6 with the pattern (TAKE (DET) PROFIT). (Triggers typically correspond to the phrasal head of a collocation, but for more complex patterns, such as verb-complement clauses, both parts of the collocation must be present.) The system's matching procedures allow for punctuation and verb-complement inversion. If the triggers are found and the match is successful, the preprocessor has a choice of subsequent actions, depending on how cautious it is supposed to be. In its aggressive mode, it updates the representations of the matched words, adding any triggered senses and preferences for the collocated senses. It also deletes any unsupported, deprecated senses. In its cautious mode, it just adds the word senses associated with the pattern to a dynamic store. Once stored, these senses are then available for the parser to use after it verifies the syntactic constraints of the collocation; if it is successful, it will add preferences for the appropriate senses. Early identification of triggered senses enables the system to use them for cluster matching in the next stage. A cluster is active if it contains any of the senses under consideration for other words in the current paragraph. The system may also activate certain clusters to represent the general topic of the text. (For purposes of cluster matching, the sense list for each word will include all the special and noncompositional senses activated during the previous stage of preprocessing, as well as any domain-specific senses that are not yet active.) This triggering of senses on the basis of conceptual context forms the final step of the preprocessing phase. Once preprocessing is complete, the parsing phase begins. In this phase, TRUMP attempts to build syntactic structures, while calling on the semantic interpreter to build and rate alternative interpretations for each structure proposed. These semantic evaluations then guide the parser's evaluation of syntactic structures. They may also influence the actual progression of the parse. For example, if a structure is found to have incoherent semantics, the parser immediately eliminates it (and all structures that might contain it) from further consideration. As suggested above, the system builds semantic interpretations incrementally. For each proposed combination of syntactic structures, there is a corresponding combination of semantic structures. It is the job of the semantic interpreter to identify the possible relations that link the structures being combined, identify the preferences associated with each possible combination of head, role (relation), and filler (the argument or modifier), and then rank competing semantic interpretations. For each proposed combination, knowledge sources may contribute the following preferences: certain syntactic form. (For example, the sense meaning 'to care for,' in She tends plants or She tends to plants occurs with an NP or PP object, whereas the sense of tend meaning 'to have a tendency' as in She tends to lose things requires a clausal object.) • preferences associated with the semantic \"fit\" between any two of the head, the role, and the filler, for example: filler and role eg, foods make good fillers for the PATIENT role of eating activities; filler and head eg, colors make good modifiers of physical objects; head and role eg, monetary objects expect to be qualified by some QUANTITY. Given the number of preference cues available for discriminating word senses, an understander must face the question of what to do if they conflict. For example, in the sentence Mary took a picture to Bob, the fact that photography does not normally have a destination (negative role-related information) should override the support for the 'photograph' interpretation of took a picture given by collocation analysis. A particular source of information may also support more than one possible interpretation, but to different degrees. For example, cigarette filter may correspond either to something that filters out cigarettes or to something that is part of a cigarette, but the latter relation is more likely. The system assigns each preference cue a strength, an integer value between +10 and -10, and then sums these strengths to find the sense with the highest rating. The strength of a particular cue depends on its type and on the degree to which the expectations underlying it are satisfied. For cues that are polar — for example, a sense is either low or high frequency — a value must be chosen experimentally, depending on the strength of the cue compared with others. For example, the system assigns frequency information (the primary-secondary distinction) a score close to zero because this information tends to be significant only when other preferences are inconclusive. For cues that have an inherent extent -- for example, the conceptual category specified by a role preference subsumes a set of elements that can be counted — the cue strength is a function of the magnitude of the extent, that is, its specificity. TRUMP's specificity function maps the number of elements subsumed by the concept onto the range 0 to +10. The function assigns concepts with few members a high value and concepts with many members a low value. In either case, a concept that subsumes only a few senses is stronger information than a concept that subsumes more. The preference score for a complex concept, formed by combining simpler concepts with the connectives AND, OR, and NOT, is a function of the number of senses subsumed by both, either, or neither concept, respectively. (If a sense belongs to more than one active cluster, then only the most specific one is considered.) The exact details of the function (ie, the range of magnitudes corresponding to each specificity class) necessarily depend on the size and organization of one's concept hierarchy. For example, one would assign specificity value 1 to any concept with more members than any immediate specialization of the most abstract concept. When a preference cue matches the input, the cue strength is its specificity value; when a concept fails to match the input, the strength is a negative value whose magnitude is usually the specificity of the concept, but it is not always this straightforward. Rating the evidence associated with a preference failure is a subtle problem, because there are different types of preference failure to take into account. Failure to meet a general preference is always significant, whereas failure to meet a very specific preference is only strong information when a slight relaxation of the preference does not eliminate the failure. This presents a bit of a paradox: the greater the specificity of a concept, the more information there is about it, but the less information there may be about a corresponding preference. The specificity function in this case returns a value whose magnitude is the same as the specificity of the complement of the concept (ie, the positive specificity less the maximum specificity, 10.) The result is a negative number whose absolute value is greater than it would be by default. For example, if a preference is for the concept c-obj ect, which has a positive specificity of 1, and this concept fails to match the input, then the preference value for the cue will be —9. On the other hand, a very specific preference usually pinpoints the expected entity, ie, the dead giveaway pairings of role and filler. Thus, it is quite common for these preferences to overspecify the underlying constraint; for example, cut may expect a tool as an INSTRUMENT, but almost any physical object will suffice. When a slight relaxation of the preference is satisfiable, a system should take the cautious route, and assume it has a case of overspecification and is at worst a weak failure. Again, the specificity function returns a negative value with magnitude equivalent to the specificity of the complement of the concept, but this time the result will be a negative number whose absolute value is less than it would be by default. When this approach fails, a system can safely assume that the entity under consideration is \"obviously inappropriate\" for a relatively strong expectation, and return the default value. The default value for a concept that is neither especially general nor specific and that fails to match the input is just —1 times the positive specificity of the concept. The strategy of favoring the most specific information has several advantages. Generally, the more specific information there is, the more complete, and hence more trustworthy, the information is. Thus, when there is a clear semantic distinction between the senses and the system has the information necessary to identify it, a clear distinction usually emerges in the ratings. This result provides a simple, sensible means of balancing syntactic and semantic preferences. To see how the cue strengths of frequency information, morphological preferences, collocations, clusters, syntactic preferences, and role-related preferences interact with one another to produce the final ranking of senses, consider the problem of deciding the correct sense of reached in Example 1 (repeated below): Example 1 The agreement reached by the state and the EPA provides for the safe storage of the waste. The sense reach3 has the highest total score. Role-related preferences of reachl for the preposition by. Although the system favors the 'communication' sense of reach in the VP, for the final result, it must balance this information with that provided by the relationship between agreement and the verb phrase. By the end of the parse, the 'event-change' sense comes to take precedence: Role-related preferences of reach4 for the preposition by. The main cause of this weakness is that (in our system) the role that agreement would fill, DESTINATION, has no special preference for being associated with a c-de st-event — many events allow a DESTINATION role. The strengths of individual components of each interpretation contribute to, but do not determine, the strength of the final interpretation, because there are also strengths associated with how well the individual components fit together. No additional weights are necessary, because the specificity values the system uses are a direct measure of strength. Our goal has been a natural language system that can effectively analyze an arbitrary input at least to the level of word sense tagging. Although we have not yet fully accomplished this goal, our results are quite encouraging. Using a lexicon of approximately 10,000 roots and 10,000 derivations, the system shows excellent lexical and morphological coverage. When tested on a sample of 25,000 words of text from the Wall Street Journal, the system covered 98% of non-proper noun, non-abbreviated word occurrences (and 91% of all words). The semantic interpreter is able to discriminate senses even when the parser cannot produce a single correct parse. One show will ask viewers to vote on their favorite all-time players through telephone polls. Each word is tagged with its part of speech and sense number along with a parent concept. For example, the tag [changing verb_3 (c-replacing)] shows that the input word is changing, the preferred sense is number 3 of the verb, and this sense falls under the concept c-replacing in the hierarchy. This tagging was produced even though the parser was unable to construct a complete and correct syntactic representation of the text. Since most semantic preferences appear at this level (and those that do not, do not depend on syntactic analysis), the results of this tagging are encouraging. This example also shows some of the limitations of our system in practice. The system is unable to recognize the collocation \"hold on to\" in the first sentence, because it lacks a pattern for it. The system also lacks patterns for the collocations \"vote on\" and \"all-time players\" that occur in the second sentence, and as a result, mistakenly tags on as c-temporal-proximity-rel rather than something more appropriate, such as c-purpose-r el. These difficulties point out the need for even more knowledge. It is encouraging to note that, even if our encoding scheme is not entirely \"correct\" according to human intuition, as long as it is consistent, in theory it should lead to capabilities that are no worse, with zero customization, than word-based methods for information retrieval. However, having access to sense tags allows for easy improvement by more knowledge-intensive methods. To date we have been unable to get a meaningful quantitative assessment of the accuracy of the system's sense tagging. We made an unsuccessful attempt at evaluating the accuracy of sense-tagging over a corpus. First, we discovered that a human \"expert\" had great difficulty identifying each sense, and that this task was far more tedious than manual part-of-speech tagging or bracketing. Improving the quality of our sense tagging requires a fair amount of straightforward but time-consuming work. This needed work includes filling a number of gaps in our knowledge sources. Our next step is to evaluate the effect of text coding on an information retrieval task, by applying traditional term-weighted statistical retrieval methods to the recoded text. One intriguing aspect of this approach is that errors in distinguishing sense preferences should not be too costly in this task, so long as the program is fairly consistent in its disambiguation of terms in both the source texts and the input queries. Having access to a large amount of information and being able to use it effectively are essential for understanding unrestricted texts, such as newspaper articles. We have developed a substantial knowledge base for text processing, including a word sensebased lexicon that contains both core senses and dynamically triggered entries. We have also created a number of concept-cluster definitions describing common semantic contexts and a conceptual hierarchy that acts as a sense-disambiguated thesaurus. Our approach to word sense discrimination uses information drawn from the knowledge base and the structure of the text, combining the strongest, most obvious sense preferences created by syntactic tags, word frequencies, collocations, semantic context (clusters), selectional restrictions, and syntactic cues. To apply this information most efficiently, the approach introduces a preprocessing phase that uses preference information available prior to parsing to eliminate some of the lexical ambiguity and establish baseline preferences. Then, during parsing, the system combines the baseline preferences with preferences created by selectional restrictions and syntactic cues to identify preferred interpretations. The preference combination mechanism of the system uses dynamic measures of strength based on specificity, rather than relying on some fixed, ordered set of rules. There are some encouraging results from applying the system to sense tagging of arbitrary text. We expect to evaluate our approach on tasks in information retrieval, and, later, machine translation, to determine the likelihood of achieving substantive improvements through sense-based semantic analysis. I am grateful to Paul Jacobs for his comments and his encouragement of my work on natural language processing at GE; to George Krupka for helping me integrate my work with TRUMP, and for continuing to improve the system; to Graeme Hirst for his many comments and suggestions on this article; and to Jan Wiebe and Evan Steeg for their comments on earlier drafts. I acknowledge the financial support of the General Electric Company, the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences. The :PAR field links each word sense to its immediate parent in the semantic hierarchy. Collocations often introduce dynamic word senses, ie, ones that behave compositionally, but occur only in the context of the expression, making it inappropriate for the system to consider them outside that context.\n",
      "-------------------------------------------------------------\n",
      "Model output:\n",
      "To discriminate senses, an understander can consider a diversity of information, including syntactic tags, word senses, collocations, semantic context, role-related expectations, and syntactic restrictions. We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences.\n"
     ]
    }
   ],
   "source": [
    "# visually check quality of cited text spans\n",
    "# pick any number up to the total number of papers in the validation set\n",
    "paper_id = 0\n",
    "\n",
    "print(\"Model input:\")\n",
    "print(val_input[paper_id])\n",
    "print('-------------------------------------------------------------')\n",
    "print(\"Model output:\")\n",
    "print(val_model_output[paper_id])\n",
    "# print(compute_metrics2([val_model_output[paper_id], val_model_output[paper_id]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Fine-Tune Pre-Trained Pegasus\n",
    "\n",
    "In this step we will do the following:\n",
    " \n",
    "Step 4.0 - Reduce train size if necessary\n",
    "Step 4.1 - Instantiate the model (the same as in step 3, so I'll skip it) \\\n",
    "Step 4.2 - Define metrics of interest \\\n",
    "Step 4.3 - Tokenize text data and wrap it into a torch Dataset \\\n",
    "Step 4.4 - Train and evaluate the model \\\n",
    "Step 4.5 - Save the model \\\n",
    "Step 4.6 - Instantiate the fine-tuned model \\\n",
    "Step 4.7 - Compute the summaries and evaluate \\\n",
    "\n",
    "Helpful resources: \\\n",
    "https://towardsdatascience.com/how-to-perform-abstractive-summarization-with-pegasus-3dd74e48bafb (github link inside)\\\n",
    "https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_summarization.py \\\n",
    "https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.0 - Reduce train size if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(20)\n",
    "# number of samples in train\n",
    "n = 20\n",
    "\n",
    "train_input = random.sample(train_input, n)\n",
    "\n",
    "\n",
    "train_output = random.sample(train_output, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1 - Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same as in step 3, so I'll skip it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2 - Define metrics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Ignore pad token for loss: Replace -100 in the labels as we can't decode them.\n",
    "    #     if data_args.ignore_pad_token_for_loss:\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "def compute_metrics2(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = preds\n",
    "\n",
    "    # Ignore pad token for loss: Replace -100 in the labels as we can't decode them.\n",
    "    #     if data_args.ignore_pad_token_for_loss:\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = labels\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3 - Tokenize text data and wrap it into a torch Dataset\n",
    "\n",
    "Since we gonna use Trainer from Transformers library, it expects our dataset as a torch.utils.data.Dataset, so we made a simple class that implements __len__() method that returns number of samples, and __getitem__() method to return a data sample at a specific index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(model_name, \n",
    "                 train_texts, train_labels, \n",
    "                 val_texts=None, val_labels=None, \n",
    "                 test_texts=None, test_labels=None):\n",
    "  \"\"\"\n",
    "  Prepare input data for model fine-tuning\n",
    "  \"\"\"\n",
    "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  prepare_val = False if val_texts is None or val_labels is None else True\n",
    "  prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "  def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    decodings = tokenizer(labels,  truncation=True, padding=True)\n",
    "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "    return dataset_tokenized\n",
    "\n",
    "  train_dataset = tokenize_data(train_texts, train_labels)\n",
    "  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename variables\n",
    "train_texts, train_labels = train_input, train_output \n",
    "val_texts, val_labels = val_input, val_output \n",
    "\n",
    "# prepare data\n",
    "train_dataset, val_dataset, _ = prepare_data(model_name, train_texts, train_labels, val_texts, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.4 - Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 13:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.382540</td>\n",
       "      <td>36.182700</td>\n",
       "      <td>22.920200</td>\n",
       "      <td>29.920100</td>\n",
       "      <td>35.113200</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>154.186900</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.382067</td>\n",
       "      <td>36.182700</td>\n",
       "      <td>22.920200</td>\n",
       "      <td>29.920100</td>\n",
       "      <td>35.113200</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>153.820100</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.380956</td>\n",
       "      <td>36.182700</td>\n",
       "      <td>22.920200</td>\n",
       "      <td>29.920100</td>\n",
       "      <td>35.113200</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>154.241100</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=8.511229832967123, metrics={'train_runtime': 905.9725, 'train_samples_per_second': 0.003, 'total_flos': 21041862672384, 'epoch': 3.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "# define Training Arguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "    logging_steps=5  ,               # log & save weights each logging_steps\n",
    "    eval_steps=1,                    # number of update steps before evaluation\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    "    predict_with_generate = True     # whether to use generate to calculate generative metrics (ROUGE, BLEU). \n",
    "\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                     # the instantiated 🤗 Transformers model to be trained\n",
    "    tokenizer = tokenizer,           # the instantiated 🤗 Transformers tokenizer to be trained  \n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    eval_dataset=val_dataset,        # evaluation dataset\n",
    "    compute_metrics=compute_metrics  # pass metric function\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 8.380955696105957,\n",
       " 'eval_rouge1': 36.1827,\n",
       " 'eval_rouge2': 22.9202,\n",
       " 'eval_rougeL': 29.9201,\n",
       " 'eval_rougeLsum': 35.1132,\n",
       " 'eval_gen_len': 82.0,\n",
       " 'eval_runtime': 151.2394,\n",
       " 'eval_samples_per_second': 0.013,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.5 - Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('fine_tuned')\n",
    "tokenizer.save_pretrained('fine_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.6 - Instantiate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the new model\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained('fine_tuned')\n",
    "# The PEGASUS Model with a language modeling head. Can be used for summarization. \n",
    "# This model inherits from PreTrainedModel. \n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained('fine_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.7 - Compute the summaries and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFERENCE\n",
    "\n",
    "## Sample with no fine-tunning \n",
    "\n",
    "import time\n",
    "\n",
    "# create empry lists to store rouge scores\n",
    "r1_precision = []\n",
    "r2_precision = []\n",
    "rL_precision = []\n",
    "\n",
    "r1_recall = []\n",
    "r2_recall = []\n",
    "rL_recall = []\n",
    "\n",
    "r1_fmeasure = []\n",
    "r2_fmeasure = []\n",
    "rL_fmeasure = []\n",
    "\n",
    "val_model_output = []\n",
    "\n",
    "# start counting seconds to keep track of time \n",
    "start = time.time()\n",
    "\n",
    "# loop over validation set\n",
    "for sample_id in range(len(val_input)):\n",
    "    \n",
    "    # get input (body OR abstract + cited text spans) - scisummnet uses abstract, we want to use body\n",
    "    sample_input = val_input[sample_id]\n",
    "    \n",
    "    # tokenize it\n",
    "    inputs = tokenizer([sample_input], max_length=1024, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # 'max_length': Pad to a maximum length specified with the argument max_length \n",
    "    # or to the maximum acceptable input length for the model if that argument is not provided.\n",
    "\n",
    "    # generate Summary\n",
    "    summary_ids = model.generate(inputs['input_ids'])\n",
    "    \n",
    "    # decode summary\n",
    "    sample_output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    \n",
    "    # store summary\n",
    "    val_model_output = val_model_output + sample_output\n",
    "    \n",
    "    # get reference (gold) summary \n",
    "    sample_reference = val_output[sample_id]\n",
    "    \n",
    "    #calculate rouge score\n",
    "    scores = scorer.score(str(sample_reference), str(sample_output))\n",
    "    \n",
    "    r1_precision.append(scores['rouge1'][0])\n",
    "    r1_recall.append(scores['rouge1'][1])\n",
    "    r1_fmeasure.append(scores['rouge1'][2])\n",
    "    \n",
    "    r2_precision.append(scores['rouge2'][0])\n",
    "    r2_recall.append(scores['rouge2'][1])\n",
    "    r2_fmeasure.append(scores['rouge2'][2])\n",
    "    \n",
    "    rL_precision.append(scores['rougeL'][0])\n",
    "    rL_recall.append(scores['rougeL'][1])\n",
    "    rL_fmeasure.append(scores['rougeL'][2])\n",
    "    \n",
    "    if sample_id % 5 == 0:\n",
    "        print(sample_id, time.time() - start)\n",
    "        start = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute score statistics\n",
    "\n",
    "all_socores = {'R1': [stats.mean(r1_precision), stats.mean(r1_recall), stats.mean(r1_fmeasure)],\n",
    "        'R2': [stats.mean(r2_precision), stats.mean(r2_recall), stats.mean(r2_fmeasure)],\n",
    "        'RL': [stats.mean(rL_precision), stats.mean(rL_recall), stats.mean(rL_fmeasure)]      \n",
    "        }\n",
    "\n",
    "all_socores_df = pd.DataFrame(all_socores, columns = ['R1', 'R2', 'RL'], index=['precision','recall','fmeasure'])\n",
    "\n",
    "print(all_socores_df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check quality of cited text spans\n",
    "# pick any number up to the total number of papers in the validation set\n",
    "paper_id = 25\n",
    "\n",
    "print(\"Model input:\")\n",
    "print(val_input[paper_id])\n",
    "print('------')\n",
    "print(\"Model output:\")\n",
    "print(val_model_output[paper_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "# from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# def prepare_fine_tuning(model_name, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n",
    "#   \"\"\"\n",
    "#   Prepare configurations and base model for fine-tuning\n",
    "#   \"\"\"\n",
    "#   torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#   model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "#   if freeze_encoder:\n",
    "#     for param in model.model.encoder.parameters():\n",
    "#       param.requires_grad = False\n",
    "\n",
    "#   if val_dataset is not None:\n",
    "#     training_args = TrainingArguments(\n",
    "#       output_dir=output_dir,           # output directory\n",
    "#       do_train=True,  \n",
    "#       num_train_epochs=5,              # total number of training epochs\n",
    "#       per_device_train_batch_size=16,  # batch size per device during training, can increase if memory allows\n",
    "#       per_device_eval_batch_size=32,   # batch size for evaluation, can increase if memory allows\n",
    "#       save_steps=1,                  # number of updates steps before checkpoint saves\n",
    "#       save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "#       evaluation_strategy='steps',     # evaluation strategy to adopt during training\n",
    "#       eval_steps=1,                  # number of update steps before evaluation\n",
    "#       warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#       weight_decay=0.01,               # strength of weight decay\n",
    "#       logging_dir='./logs',            # directory for storing logs\n",
    "#       logging_steps=32,\n",
    "#       gradient_accumulation_steps=1,  \n",
    "# #     output_dir=\"./checkpoints\",\n",
    "# #     per_device_train_batch_size=1,\n",
    "# #     do_train=True,\n",
    "# #     # fp16=True,  # This has a known bug with t5\n",
    "# #     gradient_accumulation_steps=1,\n",
    "# #     logging_steps=1,\n",
    "# #     save_steps=1,\n",
    "# #     overwrite_output_dir=True,\n",
    "# #     save_total_limit=10,\n",
    "#     )\n",
    "\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#       model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "#       args=training_args,                  # training arguments, defined above\n",
    "#       train_dataset=train_dataset,         # training dataset\n",
    "#       eval_dataset=val_dataset             # evaluation dataset\n",
    "#     )\n",
    "\n",
    "#   else:\n",
    "#     training_args = TrainingArguments(\n",
    "# #       output_dir=output_dir,           # output directory\n",
    "# #       num_train_epochs=5,              # total number of training epochs\n",
    "# #       per_device_train_batch_size=1,  # batch size per device during training, can increase if memory allows\n",
    "# #       save_steps=16,                  # number of updates steps before checkpoint saves\n",
    "# #       save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "# #       warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "# #       weight_decay=0.01,               # strength of weight decay\n",
    "# #       logging_dir='./logs',            # directory for storing logs\n",
    "# #       logging_steps=16,\n",
    "# #       gradient_accumulation_steps=16,  \n",
    "#     output_dir=\"./checkpoints\",\n",
    "#     per_device_train_batch_size=1,\n",
    "#     do_train=True,\n",
    "#     # fp16=True,  # This has a known bug with t5\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     logging_steps=1,\n",
    "#     save_steps=1,\n",
    "#     overwrite_output_dir=True,\n",
    "#     save_total_limit=10,\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#       model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "#       args=training_args,                  # training arguments, defined above\n",
    "#       train_dataset=train_dataset,         # training dataset\n",
    "#     )\n",
    "\n",
    "#   return trainer\n",
    "\n",
    "\n",
    "# # Train\n",
    "\n",
    "# trainer = prepare_fine_tuning(model_name, train_dataset, val_dataset)\n",
    "# trainer.train()\n",
    "\n",
    "# ### Step 4.6 - Instantiate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
