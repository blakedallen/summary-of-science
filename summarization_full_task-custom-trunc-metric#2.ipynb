{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization - Full task 'keep it simple'\n",
    "\n",
    "Scientific papers summarization task divided into 4 steps:\n",
    "\n",
    "**Step 0 - Download and parse the data** \\\n",
    "**Step 1 - Cited text spans identification** \\\n",
    "**Step 2 - Prepare data for train and inference** \\\n",
    "**Step 3 - Summarize with Pre-Trained Pegasus (no fine-tuning)** \\\n",
    "**Step 4 - Fine-Tune Pre-Trained Pegasus**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priscillaburity/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "    #Download the dataset\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# For regular expressions\n",
    "import re\n",
    "# For handling string\n",
    "import string\n",
    "# For performing mathematical operations\n",
    "import math\n",
    "# Importing spacy\n",
    "import spacy\n",
    "# Importing json to read input\n",
    "import json\n",
    "# Importing rouge for evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import html\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import statistics as stats\n",
    "import time\n",
    "\n",
    "from scipy import spatial\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "\n",
    "# for turn text into sentences\n",
    "import nltk.data\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import html\n",
    "from lxml import etree\n",
    "import unidecode\n",
    "\n",
    "from scipy import stats as s\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "import nltk \n",
    "import glob, os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.etree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "plot_dims = (16, 16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# url = \"https://cs.stanford.edu/~myasu/projects/scisumm_net/scisummnet_release1.1__20190413.zip\"\n",
    "# response = requests.get(url)\n",
    "# with zipfile.ZipFile(io.BytesIO(response.content)) as zipObj:\n",
    "#     # Extract all the contents of zip file in different directory\n",
    "#     zipObj.extractall(\"nlp_data\")\n",
    "#     print(\"File is unzipped in nlp_data folder\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pip\n",
    "# from pip._internal import main as pipmain\n",
    "\n",
    "# pipmain(['install', 'datasets'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Download and parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "DATA_DIR = \"data/nlp_data/scisummnet_release1.1__20190413/top1000_complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all raw text, break all papers into two parts -- Abstract and rest of document\n",
    "#first get all filepaths\n",
    "xmlfiles = []\n",
    "citations = []\n",
    "golden_summaries = []\n",
    "for subdir, dirs, files in os.walk(DATA_DIR):\n",
    "    for filename in files:\n",
    "        filepath = subdir + os.sep + filename\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            xmlfiles.append(filepath)\n",
    "        if filepath.endswith(\".json\"):\n",
    "            citations.append(filepath)\n",
    "        if filepath.endswith(\".txt\"):\n",
    "            golden_summaries.append(filepath)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next parse all XML documents\n",
    "\n",
    "def parse_xml_abstract(fp):\n",
    "    \"\"\" parse an XML journal article into an abstract and the rest of the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(fp)\n",
    "    except Exception as e:\n",
    "        return \"\",\"\",str(e)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    ab = []\n",
    "    bod = []\n",
    "    \n",
    "    for child in root:\n",
    "        if child.tag == \"ABSTRACT\":\n",
    "            for block in child:\n",
    "                ab.append(block.text)\n",
    "        else:\n",
    "            for block in child:\n",
    "                bod.append(block.text)\n",
    "                \n",
    "    #convert from list --> string\n",
    "    abstract = \"\\n\".join(ab)\n",
    "    body = \"\\n\".join(bod)\n",
    "    \n",
    "    #decode html entities\n",
    "    abstract = html.unescape(abstract)\n",
    "    body = html.unescape(body)\n",
    "    \n",
    "    return abstract,body,\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>golden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present a method for extracting parts of ob...</td>\n",
       "      <td>We present a method of extracting parts of obj...</td>\n",
       "      <td>[Berland and Charniak (1999) use Hearst style ...</td>\n",
       "      <td>Finding Parts In Very Large Corpora\\nWe presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>[The program takes the output of char_align (C...</td>\n",
       "      <td>The Mathematics Of Statistical Machine Transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Previous work has shown that Chinese word segm...</td>\n",
       "      <td>Word segmentation is considered an important f...</td>\n",
       "      <td>[Chinese word segmentation is done by the Stan...</td>\n",
       "      <td>Optimizing Chinese Word Segmentation for Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We examine the viability of building large pol...</td>\n",
       "      <td>Polarity lexicons are large lists of phrases t...</td>\n",
       "      <td>[Recent work in this area includes Velikovich ...</td>\n",
       "      <td>The viability of web-derived polarity lexicons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extracting semantic relationships between enti...</td>\n",
       "      <td>Extraction of semantic relationships between e...</td>\n",
       "      <td>[They use two kinds of features: syntactic one...</td>\n",
       "      <td>Combining Lexical Syntactic And Semantic Featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>In statistical machine translation, correspond...</td>\n",
       "      <td>[In addition, Niessen and Ney (2004) decompose...</td>\n",
       "      <td>Statistical Machine Translation With Scarce Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>We have developed a new program called alignin...</td>\n",
       "      <td>Aligning parallel texts has recently received ...</td>\n",
       "      <td>[There have been quite a number of recent pape...</td>\n",
       "      <td>Robust Bilingual Word Alignment For Machine Ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>We present an approach to pronoun resolution b...</td>\n",
       "      <td>Pronoun resolution is a difficult but vital pa...</td>\n",
       "      <td>[, We follow the closed track setting where sy...</td>\n",
       "      <td>Bootstrapping Path-Based Pronoun Resolution\\nW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>We use logical inference techniques for recogn...</td>\n",
       "      <td>Recognising textual entailment (RTE) is the ta...</td>\n",
       "      <td>[However, this method does not work for realwo...</td>\n",
       "      <td>Recognising Textual Entailment With Logical In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>This paper deals with two important ambiguitie...</td>\n",
       "      <td>The problem with successful resolution of ambi...</td>\n",
       "      <td>[The state of the art is a supervised algorith...</td>\n",
       "      <td>Corpus Based PP Attachment Ambiguity Resolutio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1009 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     We present a method for extracting parts of ob...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Previous work has shown that Chinese word segm...   \n",
       "3     We examine the viability of building large pol...   \n",
       "4     Extracting semantic relationships between enti...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  We have developed a new program called alignin...   \n",
       "1006  We present an approach to pronoun resolution b...   \n",
       "1007  We use logical inference techniques for recogn...   \n",
       "1008  This paper deals with two important ambiguitie...   \n",
       "\n",
       "                                                   body  \\\n",
       "0     We present a method of extracting parts of obj...   \n",
       "1     We describe a series of five statistical model...   \n",
       "2     Word segmentation is considered an important f...   \n",
       "3     Polarity lexicons are large lists of phrases t...   \n",
       "4     Extraction of semantic relationships between e...   \n",
       "...                                                 ...   \n",
       "1004  In statistical machine translation, correspond...   \n",
       "1005  Aligning parallel texts has recently received ...   \n",
       "1006  Pronoun resolution is a difficult but vital pa...   \n",
       "1007  Recognising textual entailment (RTE) is the ta...   \n",
       "1008  The problem with successful resolution of ambi...   \n",
       "\n",
       "                                              citations  \\\n",
       "0     [Berland and Charniak (1999) use Hearst style ...   \n",
       "1     [The program takes the output of char_align (C...   \n",
       "2     [Chinese word segmentation is done by the Stan...   \n",
       "3     [Recent work in this area includes Velikovich ...   \n",
       "4     [They use two kinds of features: syntactic one...   \n",
       "...                                                 ...   \n",
       "1004  [In addition, Niessen and Ney (2004) decompose...   \n",
       "1005  [There have been quite a number of recent pape...   \n",
       "1006  [, We follow the closed track setting where sy...   \n",
       "1007  [However, this method does not work for realwo...   \n",
       "1008  [The state of the art is a supervised algorith...   \n",
       "\n",
       "                                                 golden  \n",
       "0     Finding Parts In Very Large Corpora\\nWe presen...  \n",
       "1     The Mathematics Of Statistical Machine Transla...  \n",
       "2     Optimizing Chinese Word Segmentation for Machi...  \n",
       "3     The viability of web-derived polarity lexicons...  \n",
       "4     Combining Lexical Syntactic And Semantic Featu...  \n",
       "...                                                 ...  \n",
       "1004  Statistical Machine Translation With Scarce Re...  \n",
       "1005  Robust Bilingual Word Alignment For Machine Ai...  \n",
       "1006  Bootstrapping Path-Based Pronoun Resolution\\nW...  \n",
       "1007  Recognising Textual Entailment With Logical In...  \n",
       "1008  Corpus Based PP Attachment Ambiguity Resolutio...  \n",
       "\n",
       "[1009 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create DF with papers and citations\n",
    "\n",
    "raw_cols = []\n",
    "golden = []\n",
    "for fpn in range(len(xmlfiles)):\n",
    "    ab,bod,err = parse_xml_abstract(xmlfiles[fpn])\n",
    "    if err:\n",
    "        #print(fp, err)\n",
    "        continue\n",
    "    f = open(citations[fpn]) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    only_text = []\n",
    "    for entry in data:\n",
    "        only_text.append(entry['clean_text'])\n",
    "#     print(only_text)\n",
    "    \n",
    "    f2 = open(golden_summaries[fpn],\"r+\") \n",
    "    golden = f2.read()\n",
    "\n",
    "    \n",
    "    raw_cols.append([ab,bod,only_text, golden])\n",
    "\n",
    "df = pd.DataFrame(raw_cols, columns=[\"abstract\",\"body\",\"citations\", \"golden\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>golden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present a method for extracting parts of ob...</td>\n",
       "      <td>We present a method of extracting parts of obj...</td>\n",
       "      <td>[Berland and Charniak (1999) use Hearst style ...</td>\n",
       "      <td>Finding Parts In Very Large Corpora\\nWe presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>We describe a series of five statistical model...</td>\n",
       "      <td>[The program takes the output of char_align (C...</td>\n",
       "      <td>The Mathematics Of Statistical Machine Transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Previous work has shown that Chinese word segm...</td>\n",
       "      <td>Word segmentation is considered an important f...</td>\n",
       "      <td>[Chinese word segmentation is done by the Stan...</td>\n",
       "      <td>Optimizing Chinese Word Segmentation for Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We examine the viability of building large pol...</td>\n",
       "      <td>Polarity lexicons are large lists of phrases t...</td>\n",
       "      <td>[Recent work in this area includes Velikovich ...</td>\n",
       "      <td>The viability of web-derived polarity lexicons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extracting semantic relationships between enti...</td>\n",
       "      <td>Extraction of semantic relationships between e...</td>\n",
       "      <td>[They use two kinds of features: syntactic one...</td>\n",
       "      <td>Combining Lexical Syntactic And Semantic Featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>are at least two kinds of similarity. similari...</td>\n",
       "      <td>There are at least two kinds of similarity.\\nR...</td>\n",
       "      <td>[Veale (2004) used WordNet to answer 374 multi...</td>\n",
       "      <td>Similarity of Semantic Relations\\nThere are at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>j schroeder ed ac uk Abstract This paper analy...</td>\n",
       "      <td>This paper presents the results the shared tas...</td>\n",
       "      <td>[Tests were run on the ACL WSMT 2008 test set ...</td>\n",
       "      <td>Further Meta-Evaluation of Machine Translation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In adding syntax to statistical MT, there is a...</td>\n",
       "      <td>The statistical revolution in machine translat...</td>\n",
       "      <td>[In Marton and Resnik (2008), hiero variables ...</td>\n",
       "      <td>Soft Syntactic Constraints for Hierarchical Ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Although disjunction has been used in several ...</td>\n",
       "      <td>Disjunction has been used in several unificati...</td>\n",
       "      <td>[It is well-known that disjunctive unification...</td>\n",
       "      <td>A Unification Method For Disjunctive Feature D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Corpus-based approaches to word sense identifi...</td>\n",
       "      <td>Corpus-based approaches to word sense identifi...</td>\n",
       "      <td>[Leacock et al (1998), Agirre and Lopezde Laca...</td>\n",
       "      <td>Using Corpus Statistics And WordNet Relations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Syntactic parsing requires a fine balance betw...</td>\n",
       "      <td>Dependency-based representations have become i...</td>\n",
       "      <td>[First, well-nestedness is interesting as a ge...</td>\n",
       "      <td>Mildly Non-Projective Dependency Structures\\nS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>present a system for identifying the semantic ...</td>\n",
       "      <td>We present a system for identifying the semant...</td>\n",
       "      <td>[Gildea and Jurafsky (2002) describe a statist...</td>\n",
       "      <td>Automatic Labeling Of Semantic Roles\\nWe prese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>This paper compares a number of generative pro...</td>\n",
       "      <td>The currently best single-model statistical pa...</td>\n",
       "      <td>[These parsers are trained and evaluated using...</td>\n",
       "      <td>Generative Models For Statistical Parsing With...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tagged Dependency -.— Tagged Adjacency -e— L. ...</td>\n",
       "      <td>If parsing is taken to be the first step in ta...</td>\n",
       "      <td>[A concise review of this research area can be...</td>\n",
       "      <td>Corpus Statistics Meet The Noun Compound: Some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Of the 1600 IBM sentences that have been pars...</td>\n",
       "      <td>Building A Large Annotated Corpus Of English: ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             abstract  \\\n",
       "0   We present a method for extracting parts of ob...   \n",
       "1   We describe a series of five statistical model...   \n",
       "2   Previous work has shown that Chinese word segm...   \n",
       "3   We examine the viability of building large pol...   \n",
       "4   Extracting semantic relationships between enti...   \n",
       "5   are at least two kinds of similarity. similari...   \n",
       "6   j schroeder ed ac uk Abstract This paper analy...   \n",
       "7   In adding syntax to statistical MT, there is a...   \n",
       "8   Although disjunction has been used in several ...   \n",
       "9   Corpus-based approaches to word sense identifi...   \n",
       "10  Syntactic parsing requires a fine balance betw...   \n",
       "11  present a system for identifying the semantic ...   \n",
       "12  This paper compares a number of generative pro...   \n",
       "13  Tagged Dependency -.— Tagged Adjacency -e— L. ...   \n",
       "14                                                      \n",
       "\n",
       "                                                 body  \\\n",
       "0   We present a method of extracting parts of obj...   \n",
       "1   We describe a series of five statistical model...   \n",
       "2   Word segmentation is considered an important f...   \n",
       "3   Polarity lexicons are large lists of phrases t...   \n",
       "4   Extraction of semantic relationships between e...   \n",
       "5   There are at least two kinds of similarity.\\nR...   \n",
       "6   This paper presents the results the shared tas...   \n",
       "7   The statistical revolution in machine translat...   \n",
       "8   Disjunction has been used in several unificati...   \n",
       "9   Corpus-based approaches to word sense identifi...   \n",
       "10  Dependency-based representations have become i...   \n",
       "11  We present a system for identifying the semant...   \n",
       "12  The currently best single-model statistical pa...   \n",
       "13  If parsing is taken to be the first step in ta...   \n",
       "14                                                      \n",
       "\n",
       "                                            citations  \\\n",
       "0   [Berland and Charniak (1999) use Hearst style ...   \n",
       "1   [The program takes the output of char_align (C...   \n",
       "2   [Chinese word segmentation is done by the Stan...   \n",
       "3   [Recent work in this area includes Velikovich ...   \n",
       "4   [They use two kinds of features: syntactic one...   \n",
       "5   [Veale (2004) used WordNet to answer 374 multi...   \n",
       "6   [Tests were run on the ACL WSMT 2008 test set ...   \n",
       "7   [In Marton and Resnik (2008), hiero variables ...   \n",
       "8   [It is well-known that disjunctive unification...   \n",
       "9   [Leacock et al (1998), Agirre and Lopezde Laca...   \n",
       "10  [First, well-nestedness is interesting as a ge...   \n",
       "11  [Gildea and Jurafsky (2002) describe a statist...   \n",
       "12  [These parsers are trained and evaluated using...   \n",
       "13  [A concise review of this research area can be...   \n",
       "14  [Of the 1600 IBM sentences that have been pars...   \n",
       "\n",
       "                                               golden  \n",
       "0   Finding Parts In Very Large Corpora\\nWe presen...  \n",
       "1   The Mathematics Of Statistical Machine Transla...  \n",
       "2   Optimizing Chinese Word Segmentation for Machi...  \n",
       "3   The viability of web-derived polarity lexicons...  \n",
       "4   Combining Lexical Syntactic And Semantic Featu...  \n",
       "5   Similarity of Semantic Relations\\nThere are at...  \n",
       "6   Further Meta-Evaluation of Machine Translation...  \n",
       "7   Soft Syntactic Constraints for Hierarchical Ph...  \n",
       "8   A Unification Method For Disjunctive Feature D...  \n",
       "9   Using Corpus Statistics And WordNet Relations ...  \n",
       "10  Mildly Non-Projective Dependency Structures\\nS...  \n",
       "11  Automatic Labeling Of Semantic Roles\\nWe prese...  \n",
       "12  Generative Models For Statistical Parsing With...  \n",
       "13  Corpus Statistics Meet The Noun Compound: Some...  \n",
       "14  Building A Large Annotated Corpus Of English: ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Cited text spans identification\n",
    "\n",
    "Finding the top x sentences in the body that have the largest similarity (as per ROUGE score) with the citations \n",
    "\n",
    "Step 1.1 - Define helper functions\\\n",
    "Step 1.2 - Select body sentences with good quality \\\n",
    "Step 1.3 - Select 'x' body sentences to \"represent\" citations as model inputs - i.e., find cited text spans \\\n",
    "Step 1.4 - Check the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1 - Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT BEING USED\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkQuality(sentence):\n",
    "    \n",
    "    '''Check the quality of body sentences, to classify each of them as eligible (or not) to be a cited text span'''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    length = len(tokenized_text)\n",
    "    alpha = 0\n",
    "    nonAlpha = 0\n",
    "    found = 0\n",
    "    nonFound = 0\n",
    "    upper = 0\n",
    "    stop = 0\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    for token in tokenized_text:\n",
    "        #print(token)\n",
    "        if len(token)==1:\n",
    "            if token.isalpha():\n",
    "                # count number of single alpha chars\n",
    "                alpha+=1\n",
    "            else:\n",
    "                # count number of not single non-alpha chars\n",
    "                nonAlpha+=1\n",
    "        else:\n",
    "            if(token.lower() in stop_words):\n",
    "                # count number of stop words\n",
    "                stop+=1\n",
    "            else:\n",
    "                lemma = lemmatizer.lemmatize(token.lower())\n",
    "                # count number of Synsets for alpha tokens\n",
    "                if token.isalpha():\n",
    "                    if (len(wn.synsets(lemma))>0):\n",
    "                        found+=1 # alpha tokens that have Synsets\n",
    "                    else:\n",
    "                        nonFound+=1 # alpha tokens that dont have Synsets\n",
    "                else:\n",
    "                    nonAlpha+=1 #non alpha tokens\n",
    "    \n",
    "    # calculate shares \n",
    "    alphaS = alpha/length\n",
    "    nonAlphaS = nonAlpha/length\n",
    "    foundS = found/length\n",
    "    nonFoundS = nonFound/length\n",
    "#     upperS = upper/length\n",
    "    stopS = stop/length\n",
    "    \n",
    "    good_quality = True\n",
    "    \n",
    "    if(nonFoundS>0.2):\n",
    "        good_quality = False\n",
    "    if(foundS<0.1):\n",
    "        good_quality = False\n",
    "    if(alphaS>0.2):\n",
    "        good_quality = False\n",
    "    if(nonAlphaS>0.5):\n",
    "        good_quality = False\n",
    "    if len(tokenized_text)< 6:\n",
    "        good_quality = False\n",
    "    \n",
    "    if (\"equation\" in sentence.lower()) | (\"section\" in sentence.lower()) | (\"table\" in sentence.lower()) | (\"figure\" in sentence.lower()) | (\"=\" in sentence) | (\">\" in sentence) | (\"<\" in sentence) | (\"p(\" in sentence.lower()):\n",
    "        good_quality = False\n",
    "    \n",
    "    # remove sentences that has dates\n",
    "    match = re.match(r'.*([1-3][0-9]{3})', sentence)\n",
    "    if match is not None:\n",
    "        good_quality = False \n",
    "        \n",
    "    return good_quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cited_text_spans_ids(df,n_sent = 3):\n",
    "\n",
    "    '''Find (n_sent) body sentences most similar to citation sentences \n",
    "    inputs: \n",
    "        df: dataframe with simularity scores between each body sentence and citation \n",
    "        n_set: number of body sentences to output\n",
    "    output:\n",
    "        list of best ranked body sentetences ids\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    # empty list where we'll store best scored body sentences \n",
    "    body_sent_all_ids = []\n",
    "    body_sent_id = []\n",
    "     \n",
    "    # create an empty dataframe (num rows = num sentences in citations;  num col = n_sent)\n",
    "    # we'll store here the body sentences ids with largest similarity\n",
    "    sim_df_max = pd.DataFrame(0.0, index=[j for j in range(len(citations_sentences))], columns= [j for j in range(n_sent)])\n",
    "    \n",
    "    for ns in range(n_sent):\n",
    "        # get the indexes that maximize similarity for each column (i.e., for each citation sentence)    \n",
    "        sim_df_max[ns] = sim_df.idxmax(axis=0, skipna=True)\n",
    "        # reset the largest values to zero, so we can retreive other top values in the loop\n",
    "        sim_df[ns][sim_df_max[ns]] = 0\n",
    "        \n",
    "#     print(sim_df_max)\n",
    "#     print('done')\n",
    "\n",
    "    # loop over the number of body sentences we want to retrieve\n",
    "    for ns in range(n_sent): \n",
    "        # turn all columns into a list of best scored body sentences ids  \n",
    "        li = sim_df_max[ns].tolist() \n",
    "#         print(li)\n",
    "        body_sent_all_ids = body_sent_all_ids + li\n",
    "#         print(body_sent_all_ids)\n",
    "    \n",
    "#     print(body_sent_all_ids)\n",
    "    for ns in range(n_sent):   \n",
    "        # append best scored sentence id over all citations sentences\n",
    "#         print(type(body_sent_all_ids))\n",
    "#         print(body_sent_all_ids)\n",
    "#         print(s.mode(body_sent_all_ids))\n",
    "        best_sent_id_single = int(s.mode(body_sent_all_ids)[0])\n",
    "        body_sent_id.append(best_sent_id_single)\n",
    "        # reset the largest values to zero, so we can retreive other top values in the loop                    \n",
    "        body_sent_all_ids = list(filter(lambda a: a != best_sent_id_single, body_sent_all_ids)) \n",
    "#         print(body_sent_all_ids)\n",
    "#         if len(body_sent_all_ids) == 0:\n",
    "#             break\n",
    "\n",
    "    return body_sent_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Step 1.2 - Select body sentences with good quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 secs to run: 0.2884509563446045\n",
      "250 secs to run: 25.556607007980347\n",
      "500 secs to run: 27.228420972824097\n",
      "750 secs to run: 27.094300031661987\n",
      "1000 secs to run: 29.710898876190186\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "df['body_good_quality'] = \"\"\n",
    "\n",
    "good_quality_size_share = []\n",
    "\n",
    "for paper_id in df.index: # [i for i in df.index if i >79]:\n",
    "    \n",
    "    body_sentences = []\n",
    "    \n",
    "    # remove meaningless dots and references\n",
    "    body_sentences_tok = df.body[paper_id].replace(\"et al.\", \"et al\").replace(\"e.g.\", \"eg\").replace(\"eg.\", \"eg\").replace(\"i.e.\", \"ie\").replace(\"ie.\", \"ie\")\n",
    "    body_sentences_tok = body_sentences_tok.replace(\".1\", \". \").replace(\".2\", \". \").replace(\".3\", \".\").replace(\".4\", \". \").replace(\".5\", \". \").replace(\".6\", \". \").replace(\".7\", \". \").replace(\".8\", \". \").replace(\".9\", \". \")\n",
    "    body_sentences_tok = body_sentences_tok.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # tokenize body sentences\n",
    "    body_sentences_tok = tokenizer.tokenize(body_sentences_tok)\n",
    "\n",
    "    # truncate too large body sentences (larger than 512 throws an error - rare cases) and remove \"?\"\n",
    "    body_sentences_tok = [s.replace(\"?\", \"\") for s in body_sentences_tok if len(s) <= 512]\n",
    "    \n",
    "#     print(body_sentences_tok)\n",
    "    \n",
    "#     remove papers that are too small even before quality check\n",
    "    if len(body_sentences_tok) < 10:\n",
    "        continue\n",
    "    \n",
    "    len_total = len(body_sentences_tok)\n",
    "    # find body sentences with quality\n",
    "    for i in range(len(body_sentences_tok)):\n",
    "        if len(body_sentences_tok[i])>0:\n",
    "            if checkQuality(body_sentences_tok[i]):\n",
    "                body_sentences.append(body_sentences_tok[i])\n",
    "    len_clean = len(body_sentences)\n",
    "    \n",
    "    # store share of size reduction by quality check\n",
    "    if len_total > 0:\n",
    "        good_quality_size_share.append(len_clean/len_total*100) \n",
    "    \n",
    "    # fill our large/original dataframe with the best scored body sentences \n",
    "    df['body_good_quality'][paper_id] = \" \".join(body_sentences)\n",
    "    \n",
    "    # keep track of time to run \n",
    "    if paper_id % 250 == 0:\n",
    "        print(paper_id, \"secs to run:\", time.time() - start)\n",
    "        start = time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, clean text is 67.04721012937098 % of the size of full text\n"
     ]
    }
   ],
   "source": [
    "lst = good_quality_size_share\n",
    "\n",
    "print(\"On average, clean text is\", sum(lst) / len(lst), \"% of the size of full text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3 - Select 'x' body sentences to \"represent\" citations as model inputs - i.e., find cited text spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an empty column in the original dataframe, in which we'll store the cited text spans for each paper \n",
    "df['cited_text_spans'] = \"\"\n",
    "\n",
    "## instantiate nlp tools to read the data\n",
    "# vectorizer = Vectorizer() - NOT BEING USED - USED FOR COSINE SIMILARITY\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "## instantiate rouge score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# start time to keep track of the timing\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 secs to run: 2.0849616527557373\n",
      "250 secs to run: 1077.2591478824615\n",
      "500 secs to run: 902.4701681137085\n",
      "750 secs to run: 537.009349822998\n",
      "1000 secs to run: 601.0326092243195\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# loop over df rows and find cited text spans\n",
    "\n",
    "for paper_id in df.index: # [i for i in df.index if i >79]:\n",
    "    \n",
    "    body_sentences = tokenizer.tokenize(df['body_good_quality'][paper_id])\n",
    "    \n",
    "    if len(body_sentences) < 10:\n",
    "        continue\n",
    "    \n",
    "#     print(body_sentences)\n",
    "    \n",
    "    citations_sentences = df['citations'][paper_id]\n",
    "        \n",
    "#     print(citations_sentences)\n",
    "    \n",
    "    # create an empty dataframe (num rows = num sentences in body;  num col = num sentences in citations)\n",
    "    sim_df = pd.DataFrame(0.0, index=[j for j in range(len(body_sentences))], columns=[j for j in range(len(citations_sentences))])\n",
    "\n",
    "    # nested loop: over body sentences and citations sentences\n",
    "    for body_sentence_id in range(len(body_sentences)):\n",
    "        for citation_sentence_id in range(len(citations_sentences)):\n",
    "            # fill empty dataframe with sim measure\n",
    "#             sim_df[citation_sentence_id][body_sentence_id] = spatial.distance.cosine(vectors_bert_body[body_sentence_id], vectors_bert_citations[citation_sentence_id])\n",
    "            scores = scorer.score(body_sentences[body_sentence_id],citations_sentences[citation_sentence_id] )\n",
    "            sim_df[citation_sentence_id][body_sentence_id] = 100*scores['rouge2'][2]\n",
    "    \n",
    "        \n",
    "    # get selected body sentences ids - if rouge 2 doesnt work (usually because it yields too many socre =0), \n",
    "    # retry with rouge 1\n",
    "    try:\n",
    "        sent_ids = cited_text_spans_ids(sim_df,3)\n",
    "        \n",
    "    except: \n",
    "        for body_sentence_id in range(len(body_sentences)):\n",
    "            for citation_sentence_id in range(len(citations_sentences)):\n",
    "            # fill empty dataframe with sim measure\n",
    "#             sim_df[citation_sentence_id][body_sentence_id] = spatial.distance.cosine(vectors_bert_body[body_sentence_id], vectors_bert_citations[citation_sentence_id])\n",
    "                scores = scorer.score(body_sentences[body_sentence_id],citations_sentences[citation_sentence_id] )\n",
    "                sim_df[citation_sentence_id][body_sentence_id] = 100*scores['rouge1'][2]\n",
    "#             print(body_sentence_id,citation_sentence_id,100*scores['rouge1'][2]  )\n",
    "#         print(sim_df)\n",
    "        sent_ids = cited_text_spans_ids(sim_df,3)\n",
    "  \n",
    "    # fill our large/original dataframe with the best scored body sentences \n",
    "    df.cited_text_spans[paper_id] = [body_sentences[b] for b in sent_ids]\n",
    "    \n",
    "    # keep track of time to run \n",
    "    if paper_id % 250 == 0:\n",
    "        print(paper_id, \"secs to run:\", time.time() - start)\n",
    "        start = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df.pkl\")\n",
    "# df = pd.read_pickle(\"df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4 - Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The work was motivated by the needs of the ILEX system for generating descriptions of museum artefacts (in particular, 20th Century jewellery) [Mellish et al 98].', 'This paper presents some initial experiments using stochastic search methods for aspects of text planning.', 'In this task, one is given a set of facts all of which should be included in a text and a set of relations between facts, some of which can be included in the text.']\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mellish et al (1998) investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations.',\n",
       " 'Mellish et al (1998) (and subsequently Karamanis and Manurung 2002) advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts.',\n",
       " 'Following previous work (Mellish et al, 1998) we used a single fitness function that scored candidates based on their coherence.',\n",
       " 'The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose.',\n",
       " 'For example, the measure from (Mellish et al, 1998) looks at the entire discourse up to the current transition for some of their cost factors.',\n",
       " 'Mellish et al (1998) advocate stochastic search as an alternative to exhaustively examining the search space.',\n",
       " 'As in the case of Mellish et al (1998) we construct an acceptable ordering rather than the best possible one.',\n",
       " 'Mellish et al (1998) made the point that even this restricted approach would soon become intractable with more than a small set of facts when one allows weak RST relations such as Joint and Elaboration into the model.',\n",
       " 'In the late 1990s, Chris Mellish implemented the first stochastic text planner (Mellish et al 1998).',\n",
       " 'The evaluation function of Mellish et al (1998) also was calculated over a sum of local features of the tree, although a wider set of features were involved.',\n",
       " 'For instance, the evaluation function of Mellish et al (1998) assigned +3 for each instance of subject-repetition.',\n",
       " 'Genetic algorithms are also used in [Mellish et al, 1998] where the authors state the problem of given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how one can arrange this material so as to yield the best possible text.',\n",
       " 'Mellish et al (1998) advocate stochastic search methods for document structuring.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check quality of cited text spans\n",
    "# pick any number up to the total number of papers  \n",
    "paper_id = 80 \n",
    "\n",
    "print(df.cited_text_spans[paper_id])\n",
    "print('----------------------------------------------')\n",
    "list(filter(None, df.citations[paper_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data size: 1009\n",
      "Number of papers with no cited text spans: 71\n",
      "Share of papers with no cited text spans (%): 7.0366699702675914\n"
     ]
    }
   ],
   "source": [
    "# check the number of papers with no cited text spans\n",
    "# -> this can occur because some papers are too small, or have few sentences considered of high quality, \n",
    "#   and were discarded\n",
    "\n",
    "# pick any number up to the total number of papers  \n",
    "size_all_data = df.shape[0]\n",
    "size_no_data = df[df.cited_text_spans == \"\"].shape[0]\n",
    "print(\"Full data size:\", size_all_data)\n",
    "print(\"Number of papers with no cited text spans:\", size_no_data)\n",
    "print(\"Share of papers with no cited text spans (%):\", 100*size_no_data/size_all_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Prepare data for train and inference \n",
    "\n",
    "In this step we will do the following:\n",
    " \n",
    "Step 2.1 - Define maximun input size and define a function to get most important sentences \\\n",
    "Step 2.2 - Define model inputs and 'labels' \\\n",
    "Step 2.3 - Clean the data (important specially if we are summarizing the body) \\\n",
    "Step 2.4 - Divide the data into train, validation and test \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 - Define maximun input size and define a function to get most important sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maximun input size\n",
    "max_input_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_important_sentences(document, max_size = max_input_size):\n",
    "    '''\n",
    "    If the number of sentences in a document is larger than max_input_size, select sentences with \n",
    "    largest similarity with all other sentences in the document. \n",
    "    \n",
    "    Inputs:\n",
    "        document: str, sentences in the original order of the document\n",
    "        max_size = number of sentences to be collected from the document\n",
    "    Output: \n",
    "        document: list of sentences text with 'max_size' sentences\n",
    "    '''\n",
    "    \n",
    "#     if type(document) == \"str\":\n",
    "            \n",
    "#     document = tokenizer.tokenize(document)\n",
    "    \n",
    "#     print(document)\n",
    "    \n",
    "    # create an empty dataframe (num rows = num sentences in doc;  num col = num sentences in doc)\n",
    "    sim_df = pd.DataFrame(0.0, index=[j for j in range(len(document))], columns=[j for j in range(len(document))])\n",
    "\n",
    "    # nested loop: over each pair of document sentences\n",
    "    for column_sentence_id in range(len(document)):\n",
    "        for row_sentence_id in range(len(document)):\n",
    "            # fill empty dataframe with sim measure\n",
    "            if column_sentence_id > row_sentence_id:\n",
    "                scores = scorer.score(document[column_sentence_id],document[row_sentence_id] )\n",
    "                sim_df[column_sentence_id][row_sentence_id] = 100*scores['rouge1'][2]\n",
    "    \n",
    "        \n",
    "#     print(sim_df)\n",
    "    sim_df_sum = list(sim_df.sum())\n",
    "    \n",
    "    index = sorted(range(len(sim_df_sum)), key=lambda k: sim_df_sum[k], reverse = True)\n",
    "    \n",
    "    most_important = \" \".join([document[b] for b in range(len(document)) if index[b] <= max_size])\n",
    "    \n",
    "    return most_important\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def most_important_sentences_vs_doc(document, max_size = max_input_size):\n",
    "    '''\n",
    "    If the number of sentences in a document is larger than max_input_size, select sentences with \n",
    "    largest similarity with the rest of the document. \n",
    "    \n",
    "    Inputs:\n",
    "        document: str, sentences in the original order of the document\n",
    "        max_size = number of sentences to be collected from the document\n",
    "    Output: \n",
    "        document: list of sentences text with 'max_size' sentences\n",
    "    '''\n",
    "    \n",
    "#     if type(document) == \"str\":\n",
    "            \n",
    "#     document = tokenizer.tokenize(document)\n",
    "    \n",
    "#     print(document)\n",
    "    \n",
    "    # create an empty dataframe (num rows = num sentences in doc;  num col = num sentences in doc)\n",
    "    sim_df = pd.DataFrame(0.0, index=[j for j in range(1)], columns=[j for j in range(len(document))])\n",
    "\n",
    "    # nested loop: over each pair of document sentences\n",
    "    for column_sentence_id in range(len(document)):\n",
    "\n",
    "    # fill empty dataframe with sim measure\n",
    "        document_ex = copy.deepcopy(document)   \n",
    "        del document_ex[column_sentence_id]\n",
    "        document_ex = \" \".join(document_ex)\n",
    "        scores = scorer.score(document[column_sentence_id],document_ex)\n",
    "        sim_df[column_sentence_id][0] = 100*scores['rouge1'][2]\n",
    "\n",
    "        \n",
    "#     print(sim_df)\n",
    "    sim_df_sum = list(sim_df.sum())\n",
    "    \n",
    "    index = sorted(range(len(sim_df_sum)), key=lambda k: sim_df_sum[k], reverse = True)\n",
    "    \n",
    "    most_important = \" \".join([document[b] for b in range(len(document)) if index[b] <= max_size])\n",
    "    \n",
    "    return most_important\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 - Define model inputs and 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract', 'body', 'citations', 'golden', 'body_good_quality', 'cited_text_spans', 'model_input', 'model_output']\n",
      "---------\n",
      "We present a method of extracting parts of objects from wholes (eg \"speedometer\" from \"car\"). To be more precise, given a single word denoting some entity that has recognizable parts, the system finds and rank-orders other words that may denote parts of the entity in question. Thus the relation found is strictly speaking between words, a relation Miller [1] calls \"meronymy.\" In this paper we use the more colloquial \"part-of\" terminology. We produce words with 55% accuracy for the top 50 words ranked by the system, given a very large corpus. Lacking an objective definition of the part-of relation, we use the majority judgment of five human subjects to decide which proposed parts are correct. The program's output could be scanned by an enduser and added to an existing ontology (eg, WordNet), or used as a part of a rough semantic lexicon. To the best of our knowledge, there is no published work on automatically finding parts from unlabeled corpora. Casting our nets wider, the work most similar to what we present here is that by Hearst [2] on acquisition of hyponyms (\"isa\" relations). In that paper Hearst (a) finds lexical correlates to the hyponym relations by looking in text for cases where known hyponyms appear in proximity (eg, in the construction (NP, NP and (NP other NN)) as in \"boats, cars, and other vehicles\"), (b) tests the proposed patterns for validity, and (c) uses them to extract relations from a corpus. In this paper we apply much the same methodology to the part-of relation. Indeed, in [2] Hearst states that she tried to apply this strategy to the part-of relation, but failed. We comment later on the differences in our approach that we believe were most important to our comparative success. Looking more widely still, there is an evergrowing literature on the use of statistical/corpusbased techniques in the automatic acquisition of lexical-semantic knowledge ([3-8]). We take it as axiomatic that such knowledge is tremendously useful in a wide variety of tasks, from lower-level tasks like noun-phrase reference, and parsing to user-level tasks such as web searches, question answering, and digesting. Certainly the large number of projects that use WordNet [1] would support this contention. And although WordNet is hand-built, there is general agreement that corpus-based methods have an advantage in the relative completeness of their coverage, particularly when used as supplements to the more laborintensive methods. Webster's Dictionary defines \"part\" as \"one of the often indefinite or unequal subdivisions into which something is or is regarded as divided and which together constitute the whole.\" The vagueness of this definition translates into a lack of guidance on exactly what constitutes a part, which in turn translates into some doubts about evaluating the results of any procedure that claims to find them. More specifically, note that the definition does not claim that parts must be physical objects. Thus, say, \"novel\" might have \"plot\" as a part. In this study we handle this problem by asking informants which words in a list are parts of some target word, and then declaring majority opinion to be correct. We give more details on this aspect of the study later. Here we simply note that while our subjects often disagreed, there was fair consensus that what might count as a part depends on the nature of the word: a physical object yields physical parts, an institution yields its members, and a concept yields its characteristics and processes. In other words, \"floor\" is part of \"building\" and \"plot\" is part of \"book.\" Our first goal is to find lexical patterns that tend to indicate part-whole relations. Following Hearst [2], we find possible patterns by taking two words that are in a part-whole relation (e.g, basement and building) and finding sentences in our corpus (we used the North American News Corpus (NANC) from LDC) that have these words within close proximity. The first few such sentences are: ... the basement of the building. ... the basement in question is in a four-story apartment building ... ... the basement of the apartment building. We assume here that parts and wholes are represented by individual lexical items (more specifically, as head nouns of noun-phrases) as opposed to complete noun phrases, or as a sequence of \"important\" noun modifiers together with the head. This occasionally causes problems, eg, \"conditioner\" was marked by our informants as not part of \"car\", whereas \"air conditioner\" probably would have made it into a part list. Nevertheless, in most cases head nouns have worked quite well on their own. We evaluated these patterns by observing how they performed in an experiment on a single example. The relatively poor performance of patterns C and E was anticipated, as many things occur \"in\" cars (or buildings, etc.) Pattern D is not so obviously bad as it differs from the plural case of pattern B only in the lack of the determiner \"the\" or \"a\". However, this difference proves critical in that pattern D tends to pick up \"counting\" nouns such as \"truckload.\" We use the LDC North American News Corpus (NANC). which is a compilation of the wire output of several US newspapers. The total corpus is about 100,000,000 words. We ran our program on the whole data set, which takes roughly four hours on our network. The bulk of that time (around 90%) is spent tagging the corpus. As is typical in this sort of work, we assume that our evidence (occurrences of patterns A and B) is independently and identically distributed (iid). We have found this assumption reasonable, but its breakdown has led to a few errors. In particular, a drawback of the NANC is the occurrence of repeated articles; since the corpus consists of all of the articles that come over the wire, some days include multiple, updated versions of the same story, containing identical paragraphs or sentences. We wrote programs to weed out such cases, but ultimately found them of little use. First, \"update\" articles still have substantial variation, so there is a continuum between these and articles that are simply on the same topic. Second, our data is so sparse that any such repeats are very unlikely to manifest themselves as repeated examples of part-type patterns. Nevertheless since two or three occurrences of a word can make it rank highly, our results have a few anomalies that stem from failure of the lid assumption (eg, quite appropriately, \"clunker\"). Our seeds are one word (such as \"car\") and its plural. We do not claim that all single words would fare as well as our seeds, as we picked highly probable words for our corpus (such as \"building\" and \"hospital\") that we thought would have parts that might also be mentioned therein. With enough text, one could probably get reasonable results with any noun that met these criteria. The program has three phases. The first identifies and records all occurrences of patterns A and B in our corpus. The second filters out all words ending with \"ing\", \"ness\", or \"ity\", since these suffixes typically occur in words that denote a quality rather than a physical object. Finally we order the possible parts by the likelihood that they are true parts according to some appropriate metric. We took some care in the selection of this metric. (Here and in what follows w denotes the outcome of the random variable generating wholes, and p the outcome for parts. However, in making this intuitive idea someone more precise we found two closely related versions: We call metrics based on the first of these \"loosely conditioned\" and those based on the second \"strongly conditioned\". While invariance with respect to frequency is generally a good property, such invariant metrics can lead to bad results when used with sparse data. Thus this metric must be tempered to take into account the quantity of data that supports its conclusion. We need a metric that combines these two desiderata in a natural way. We tried two such metrics. The second metric is proposed by Johnson (personal communication). We call this new test the \"significant-difference\" test, or sigdiff. The first group contains the words found for the method we perceive as the most accurate, sigdiff and strong conditioning. The other groups show the differences between them and the first group. The + category means that this method adds the word to its list, — means the opposite. For example, \"back\" is on the sigdiff-loose list but not the sigdiff-strong list. In general, sigdiff worked better than surprise and strong conditioning worked better than loose conditioning. In both cases the less favored methods tend to promote words that are less specific (\"back\" over \"airbag\", \"use\" over \"radiator\"). Furthermore, the combination of sigdiff and strong conditioning worked better than either by itself. Thus all results in this paper, unless explicitly noted otherwise, were gathered using sigdiff and strong conditioning combined. We tested five subjects (all of whom were unaware of our goals) for their concept of a \"part.\" We asked them to rate sets of 100 words, of which 50 were in our final results set. The score of individual words vary greatly but there was relative consensus on most words. We put an asterisk next to words that the majority subjects marked as correct. Lacking a formal definition of part, we can only define those words as correct and the rest as wrong. While the scoring is admittedly not perfect', it provides an adequate reference result. There we show the number of correct part words in the top 10, 20, 30, 40, and 50 parts for each seed (eg, for \"book\", 8 of the top 10 are parts, and 14 of the top 20). Overall, about 55% of the top 50 words for each seed are parts, and about 70% of the top 20 for each seed. The reader should also note that we tried one ambiguous word, \"plant\" to see what would happen. Our program finds parts corresponding to both senses, though given the nature of our text, the industrial use is more common. Our subjects marked both kinds of parts as correct, but even so, this produced the weakest part list of the six words we tried. As a baseline we also tried using as our \"pattern\" the head nouns that immediately surround our target word. We then applied the same \"strong conditioning, sigdiff\" statistical test to rank the candidates. Of the top 50 candidates for each target, only 8% were parts, as opposed to the 55% for our program. We also compared out parts list to those of WordNet. There are definite tradeoffs, although we would argue that our top20 set is both more specific and more comprehensive. More generally, all WordNet parts occur somewhere before 500, with the exception of \"tailfin\", which never occurs with car. It would seem that our program would be a good tool for expanding Wordnet, as a person can to the entire statistical NLP group at Brown, and scan and mark the list of part words in a few minutes. particularly to Mark Johnson, Brian Roark, Gideon Mann, and Ana-Maria Popescu who provided invaluable help on the project. The program presented here can find parts of objects given a word denoting the whole object and a large corpus of unmarked text. The program is about 55% accurate for the top 50 proposed parts for each of six examples upon which we tested it. There does not seem to be a single cause for the 45% of the cases that are mistakes. We present here a few problems that have caught our attention. Idiomatic phrases like \"a jalopy of a car\" or \"the son of a gun\" provide problems that are not easily weeded out. Depending on the data, these phrases can be as prevalent as the legitimate parts. In some cases problems arose because of tagger mistakes. For example, \"re-enactment\" would be found as part of a \"car\" using pattern B in the phrase \"the re-enactment of the car crash\" if \"crash\" is tagged as a verb. The program had some tendency to find qualities of objects. For example, \"driveability\" is strongly correlated with car. We try to weed out most of the qualities by removing words with the suffixes \"ness\", \"ing\", and \"ity.\" The most persistent problem is sparse data, which is the source of most of the noise. More data would almost certainly allow us to produce better lists, both because the statistics we are currently collecting would be more accurate, but also because larger numbers would allow us to find other reliable indicators. For example, idiomatic phrases might be recognized as such. So we see \"jalopy of a car\" (two times) but not, of course, \"the car's jalopy\". Words that appear in only one of the two patterns are suspect, but to use this rule we need sufficient counts on the good words to be sure we have a representative sample. At 100 million words, the NANC is not exactly small, but we were able to process it in about four hours with the machines at our disposal, so still larger corpora would not be out of the question. Finally, as noted above, Hearst [2] tried to find parts in corpora but did not achieve good results. She does not say what procedures were used, but assuming that the work closely paralleled her work on hyponyms, we suspect that our relative success was due to our very large corpus and the use of more refined statistical measures for ranking the output.\n"
     ]
    }
   ],
   "source": [
    "# create a new df named data in case you mess it up\n",
    "data = df.copy()\n",
    "data['model_input'] = ''\n",
    "data['model_output'] = ''\n",
    "\n",
    "# inputs - body? abstract? - cts will be added later\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data['model_input'][i] = data['body_good_quality'][i].replace(\"\\n\", \" \")\n",
    "    \n",
    "# output  \n",
    "data['model_output'] = data['golden'] \n",
    "\n",
    "# sanity checks\n",
    "print(list(data))\n",
    "print(\"---------\")\n",
    "print(data['model_input'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 // size: 110 // secs to run: 0.00397801399230957\n",
      "1 // size: 446 // secs to run: 99.08491492271423\n",
      "2 // size: 135 // secs to run: 0.004148960113525391\n",
      "3 // size: 108 // secs to run: 0.003512144088745117\n",
      "4 // size: 55 // secs to run: 0.0026128292083740234\n",
      "5 // size: 382 // secs to run: 60.1707968711853\n",
      "6 // size: 197 // secs to run: 0.0074269771575927734\n",
      "7 // size: 84 // secs to run: 0.0033469200134277344\n",
      "8 // size: 103 // secs to run: 0.003863096237182617\n",
      "9 // size: 254 // secs to run: 0.008996009826660156\n",
      "10 // size: 91 // secs to run: 0.003368854522705078\n",
      "11 // size: 444 // secs to run: 126.01718807220459\n",
      "12 // size: 15 // secs to run: 0.0010020732879638672\n",
      "13 // size: 109 // secs to run: 0.004237174987792969\n",
      "14 // size: 0 // secs to run: 9.608268737792969e-05\n",
      "15 // size: 0 // secs to run: 3.5762786865234375e-05\n",
      "16 // size: 273 // secs to run: 0.012371063232421875\n",
      "17 // size: 278 // secs to run: 0.013036012649536133\n",
      "18 // size: 147 // secs to run: 0.0067882537841796875\n",
      "19 // size: 163 // secs to run: 0.005970954895019531\n",
      "20 // size: 103 // secs to run: 0.0036449432373046875\n",
      "21 // size: 0 // secs to run: 4.076957702636719e-05\n",
      "22 // size: 134 // secs to run: 0.004571199417114258\n",
      "23 // size: 128 // secs to run: 0.00467991828918457\n",
      "24 // size: 150 // secs to run: 0.0075719356536865234\n",
      "25 // size: 86 // secs to run: 0.0030870437622070312\n",
      "26 // size: 40 // secs to run: 0.0014939308166503906\n",
      "27 // size: 64 // secs to run: 0.0028870105743408203\n",
      "28 // size: 102 // secs to run: 0.0035619735717773438\n",
      "29 // size: 72 // secs to run: 0.0026459693908691406\n",
      "30 // size: 91 // secs to run: 0.003610849380493164\n",
      "31 // size: 103 // secs to run: 0.003720998764038086\n",
      "32 // size: 0 // secs to run: 4.315376281738281e-05\n",
      "33 // size: 444 // secs to run: 124.23714423179626\n",
      "34 // size: 86 // secs to run: 0.0024209022521972656\n",
      "35 // size: 0 // secs to run: 2.1696090698242188e-05\n",
      "36 // size: 74 // secs to run: 0.002643108367919922\n",
      "37 // size: 118 // secs to run: 0.0052220821380615234\n",
      "38 // size: 201 // secs to run: 0.0076868534088134766\n",
      "39 // size: 178 // secs to run: 0.005568027496337891\n",
      "40 // size: 78 // secs to run: 0.0025701522827148438\n",
      "41 // size: 91 // secs to run: 0.0026488304138183594\n",
      "42 // size: 235 // secs to run: 0.008387088775634766\n",
      "43 // size: 140 // secs to run: 0.004405021667480469\n",
      "44 // size: 130 // secs to run: 0.003565073013305664\n",
      "45 // size: 140 // secs to run: 0.005361795425415039\n",
      "46 // size: 56 // secs to run: 0.0026357173919677734\n",
      "47 // size: 155 // secs to run: 0.0072748661041259766\n",
      "48 // size: 142 // secs to run: 0.005012989044189453\n",
      "49 // size: 136 // secs to run: 0.0036559104919433594\n",
      "50 // size: 327 // secs to run: 53.34400415420532\n",
      "51 // size: 55 // secs to run: 0.002368927001953125\n",
      "52 // size: 143 // secs to run: 0.007158994674682617\n",
      "53 // size: 166 // secs to run: 0.008096933364868164\n",
      "54 // size: 120 // secs to run: 0.00581812858581543\n",
      "55 // size: 132 // secs to run: 0.0060808658599853516\n",
      "56 // size: 165 // secs to run: 0.006824970245361328\n",
      "57 // size: 145 // secs to run: 0.007031917572021484\n",
      "58 // size: 115 // secs to run: 0.004571199417114258\n",
      "59 // size: 92 // secs to run: 0.004098176956176758\n",
      "60 // size: 166 // secs to run: 0.008754253387451172\n",
      "61 // size: 144 // secs to run: 0.006515979766845703\n",
      "62 // size: 19 // secs to run: 0.0008442401885986328\n",
      "63 // size: 112 // secs to run: 0.006367921829223633\n",
      "64 // size: 114 // secs to run: 0.005280971527099609\n",
      "65 // size: 131 // secs to run: 0.007046937942504883\n",
      "66 // size: 87 // secs to run: 0.004286050796508789\n",
      "67 // size: 106 // secs to run: 0.005216121673583984\n",
      "68 // size: 156 // secs to run: 0.007642030715942383\n",
      "69 // size: 0 // secs to run: 5.793571472167969e-05\n",
      "70 // size: 143 // secs to run: 0.006559848785400391\n",
      "71 // size: 87 // secs to run: 0.003629922866821289\n",
      "72 // size: 94 // secs to run: 0.003695964813232422\n",
      "73 // size: 139 // secs to run: 0.0048291683197021484\n",
      "74 // size: 92 // secs to run: 0.003641843795776367\n",
      "75 // size: 121 // secs to run: 0.00431513786315918\n",
      "76 // size: 258 // secs to run: 0.01085519790649414\n",
      "77 // size: 127 // secs to run: 0.005504131317138672\n",
      "78 // size: 113 // secs to run: 0.004335880279541016\n",
      "79 // size: 6 // secs to run: 0.00026106834411621094\n",
      "80 // size: 151 // secs to run: 0.0059947967529296875\n",
      "81 // size: 265 // secs to run: 0.012964010238647461\n",
      "82 // size: 86 // secs to run: 0.0031659603118896484\n",
      "83 // size: 95 // secs to run: 0.004498958587646484\n",
      "84 // size: 127 // secs to run: 0.0050389766693115234\n",
      "85 // size: 82 // secs to run: 0.003050088882446289\n",
      "86 // size: 129 // secs to run: 0.0048770904541015625\n",
      "87 // size: 142 // secs to run: 0.00485682487487793\n",
      "88 // size: 111 // secs to run: 0.0037686824798583984\n",
      "89 // size: 0 // secs to run: 2.6941299438476562e-05\n",
      "90 // size: 107 // secs to run: 0.003551006317138672\n",
      "91 // size: 54 // secs to run: 0.0022430419921875\n",
      "92 // size: 170 // secs to run: 0.005903959274291992\n",
      "93 // size: 223 // secs to run: 0.008153915405273438\n",
      "94 // size: 113 // secs to run: 0.00406193733215332\n",
      "95 // size: 110 // secs to run: 0.004209995269775391\n",
      "96 // size: 236 // secs to run: 0.008479833602905273\n",
      "97 // size: 200 // secs to run: 0.011825799942016602\n",
      "98 // size: 126 // secs to run: 0.005083322525024414\n",
      "99 // size: 124 // secs to run: 0.004953145980834961\n",
      "100 // size: 84 // secs to run: 0.003434896469116211\n",
      "101 // size: 392 // secs to run: 64.79476189613342\n",
      "102 // size: 63 // secs to run: 0.0022001266479492188\n",
      "103 // size: 97 // secs to run: 0.004034996032714844\n",
      "104 // size: 171 // secs to run: 0.0049190521240234375\n",
      "105 // size: 208 // secs to run: 0.009109020233154297\n",
      "106 // size: 94 // secs to run: 0.00308990478515625\n",
      "107 // size: 137 // secs to run: 0.004492044448852539\n",
      "108 // size: 11 // secs to run: 0.00039315223693847656\n",
      "109 // size: 125 // secs to run: 0.003933906555175781\n",
      "110 // size: 143 // secs to run: 0.004475116729736328\n",
      "111 // size: 175 // secs to run: 0.006094217300415039\n",
      "112 // size: 109 // secs to run: 0.0034637451171875\n",
      "113 // size: 123 // secs to run: 0.004188060760498047\n",
      "114 // size: 110 // secs to run: 0.003676891326904297\n",
      "115 // size: 94 // secs to run: 0.0029799938201904297\n",
      "116 // size: 128 // secs to run: 0.0038619041442871094\n",
      "117 // size: 101 // secs to run: 0.0032029151916503906\n",
      "118 // size: 127 // secs to run: 0.004061222076416016\n",
      "119 // size: 112 // secs to run: 0.004152059555053711\n",
      "120 // size: 98 // secs to run: 0.0030059814453125\n",
      "121 // size: 132 // secs to run: 0.00417017936706543\n",
      "122 // size: 93 // secs to run: 0.0038919448852539062\n",
      "123 // size: 114 // secs to run: 0.00362396240234375\n",
      "124 // size: 0 // secs to run: 2.3126602172851562e-05\n",
      "125 // size: 165 // secs to run: 0.005777835845947266\n",
      "126 // size: 160 // secs to run: 0.0056400299072265625\n",
      "127 // size: 119 // secs to run: 0.00414586067199707\n",
      "128 // size: 190 // secs to run: 0.008436203002929688\n",
      "129 // size: 297 // secs to run: 0.009940147399902344\n",
      "130 // size: 104 // secs to run: 0.0037109851837158203\n",
      "131 // size: 90 // secs to run: 0.0026810169219970703\n",
      "132 // size: 40 // secs to run: 0.001194000244140625\n",
      "133 // size: 107 // secs to run: 0.003142833709716797\n",
      "134 // size: 66 // secs to run: 0.002228975296020508\n",
      "135 // size: 112 // secs to run: 0.003576993942260742\n",
      "136 // size: 0 // secs to run: 2.288818359375e-05\n",
      "137 // size: 88 // secs to run: 0.004909038543701172\n",
      "138 // size: 0 // secs to run: 5.2928924560546875e-05\n",
      "139 // size: 0 // secs to run: 1.7881393432617188e-05\n",
      "140 // size: 130 // secs to run: 0.0039081573486328125\n",
      "141 // size: 114 // secs to run: 0.0034067630767822266\n",
      "142 // size: 73 // secs to run: 0.0021889209747314453\n",
      "143 // size: 151 // secs to run: 0.004709959030151367\n",
      "144 // size: 161 // secs to run: 0.004719972610473633\n",
      "145 // size: 130 // secs to run: 0.00424504280090332\n",
      "146 // size: 131 // secs to run: 0.003912925720214844\n",
      "147 // size: 115 // secs to run: 0.003164052963256836\n",
      "148 // size: 148 // secs to run: 0.0044558048248291016\n",
      "149 // size: 113 // secs to run: 0.003565073013305664\n",
      "150 // size: 152 // secs to run: 0.00892186164855957\n",
      "151 // size: 91 // secs to run: 0.0032320022583007812\n",
      "152 // size: 87 // secs to run: 0.003114938735961914\n",
      "153 // size: 87 // secs to run: 0.003233671188354492\n",
      "154 // size: 77 // secs to run: 0.0024650096893310547\n",
      "155 // size: 114 // secs to run: 0.0038721561431884766\n",
      "156 // size: 50 // secs to run: 0.001821279525756836\n",
      "157 // size: 146 // secs to run: 0.0048367977142333984\n",
      "158 // size: 106 // secs to run: 0.0041849613189697266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 // size: 100 // secs to run: 0.0032320022583007812\n",
      "160 // size: 312 // secs to run: 50.11865973472595\n",
      "161 // size: 115 // secs to run: 0.004057168960571289\n",
      "162 // size: 9 // secs to run: 0.0003719329833984375\n",
      "163 // size: 159 // secs to run: 0.0041351318359375\n",
      "164 // size: 14 // secs to run: 0.0004730224609375\n",
      "165 // size: 96 // secs to run: 0.0033218860626220703\n",
      "166 // size: 122 // secs to run: 0.0037560462951660156\n",
      "167 // size: 119 // secs to run: 0.004912853240966797\n",
      "168 // size: 89 // secs to run: 0.0027778148651123047\n",
      "169 // size: 116 // secs to run: 0.003302335739135742\n",
      "170 // size: 185 // secs to run: 0.004905223846435547\n",
      "171 // size: 193 // secs to run: 0.005395174026489258\n",
      "172 // size: 0 // secs to run: 2.3126602172851562e-05\n",
      "173 // size: 132 // secs to run: 0.0036067962646484375\n",
      "174 // size: 128 // secs to run: 0.004288911819458008\n",
      "175 // size: 294 // secs to run: 0.01003122329711914\n",
      "176 // size: 130 // secs to run: 0.003566741943359375\n",
      "177 // size: 262 // secs to run: 0.010315895080566406\n",
      "178 // size: 120 // secs to run: 0.0034983158111572266\n",
      "179 // size: 136 // secs to run: 0.0042078495025634766\n",
      "180 // size: 138 // secs to run: 0.004233837127685547\n",
      "181 // size: 80 // secs to run: 0.0023849010467529297\n",
      "182 // size: 94 // secs to run: 0.0024328231811523438\n",
      "183 // size: 0 // secs to run: 1.9073486328125e-05\n",
      "184 // size: 109 // secs to run: 0.0033769607543945312\n",
      "185 // size: 481 // secs to run: 108.499764919281\n",
      "186 // size: 122 // secs to run: 0.004566192626953125\n",
      "187 // size: 100 // secs to run: 0.003490924835205078\n",
      "188 // size: 323 // secs to run: 66.50658893585205\n",
      "189 // size: 220 // secs to run: 0.008193731307983398\n",
      "190 // size: 114 // secs to run: 0.004669904708862305\n",
      "191 // size: 124 // secs to run: 0.0046269893646240234\n",
      "192 // size: 101 // secs to run: 0.0034618377685546875\n",
      "193 // size: 119 // secs to run: 0.004972934722900391\n",
      "194 // size: 171 // secs to run: 0.007667064666748047\n",
      "195 // size: 63 // secs to run: 0.0023202896118164062\n",
      "196 // size: 119 // secs to run: 0.0047380924224853516\n",
      "197 // size: 43 // secs to run: 0.001943826675415039\n",
      "198 // size: 72 // secs to run: 0.0028150081634521484\n",
      "199 // size: 358 // secs to run: 77.94874215126038\n",
      "200 // size: 221 // secs to run: 0.007771015167236328\n",
      "201 // size: 0 // secs to run: 3.1948089599609375e-05\n",
      "202 // size: 126 // secs to run: 0.004492044448852539\n",
      "203 // size: 164 // secs to run: 0.007868051528930664\n",
      "204 // size: 103 // secs to run: 0.0036559104919433594\n",
      "205 // size: 86 // secs to run: 0.0032880306243896484\n",
      "206 // size: 20 // secs to run: 0.0006058216094970703\n",
      "207 // size: 133 // secs to run: 0.004557132720947266\n",
      "208 // size: 51 // secs to run: 0.001901865005493164\n",
      "209 // size: 168 // secs to run: 0.006496906280517578\n",
      "210 // size: 45 // secs to run: 0.0016436576843261719\n",
      "211 // size: 115 // secs to run: 0.004343986511230469\n",
      "212 // size: 114 // secs to run: 0.0041429996490478516\n",
      "213 // size: 108 // secs to run: 0.0037810802459716797\n",
      "214 // size: 207 // secs to run: 0.008115053176879883\n",
      "215 // size: 109 // secs to run: 0.004080772399902344\n",
      "216 // size: 0 // secs to run: 2.5987625122070312e-05\n",
      "217 // size: 0 // secs to run: 1.8835067749023438e-05\n",
      "218 // size: 133 // secs to run: 0.005400896072387695\n",
      "219 // size: 41 // secs to run: 0.0014400482177734375\n",
      "220 // size: 108 // secs to run: 0.004719972610473633\n",
      "221 // size: 158 // secs to run: 0.004979848861694336\n",
      "222 // size: 128 // secs to run: 0.004919767379760742\n",
      "223 // size: 121 // secs to run: 0.0046138763427734375\n",
      "224 // size: 346 // secs to run: 52.36598181724548\n",
      "225 // size: 133 // secs to run: 0.005869150161743164\n",
      "226 // size: 147 // secs to run: 0.008368253707885742\n",
      "227 // size: 102 // secs to run: 0.004685163497924805\n",
      "228 // size: 137 // secs to run: 0.005589962005615234\n",
      "229 // size: 109 // secs to run: 0.005717039108276367\n",
      "230 // size: 82 // secs to run: 0.003377199172973633\n",
      "231 // size: 96 // secs to run: 0.004419088363647461\n",
      "232 // size: 89 // secs to run: 0.004091978073120117\n",
      "233 // size: 79 // secs to run: 0.0041849613189697266\n",
      "234 // size: 122 // secs to run: 0.006184101104736328\n",
      "235 // size: 270 // secs to run: 0.012425899505615234\n",
      "236 // size: 112 // secs to run: 0.005018949508666992\n",
      "237 // size: 127 // secs to run: 0.005136966705322266\n",
      "238 // size: 120 // secs to run: 0.005755186080932617\n",
      "239 // size: 315 // secs to run: 68.25234627723694\n",
      "240 // size: 143 // secs to run: 0.004310131072998047\n",
      "241 // size: 174 // secs to run: 0.005445003509521484\n",
      "242 // size: 69 // secs to run: 0.00238800048828125\n",
      "243 // size: 0 // secs to run: 2.5272369384765625e-05\n",
      "244 // size: 88 // secs to run: 0.0035300254821777344\n",
      "245 // size: 152 // secs to run: 0.004209995269775391\n",
      "246 // size: 53 // secs to run: 0.0035288333892822266\n",
      "247 // size: 315 // secs to run: 64.35564088821411\n",
      "248 // size: 91 // secs to run: 0.0044708251953125\n",
      "249 // size: 79 // secs to run: 0.002694845199584961\n",
      "250 // size: 141 // secs to run: 0.004208803176879883\n",
      "251 // size: 106 // secs to run: 0.0032126903533935547\n",
      "252 // size: 104 // secs to run: 0.003099679946899414\n",
      "253 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "254 // size: 107 // secs to run: 0.0033211708068847656\n",
      "255 // size: 115 // secs to run: 0.0037658214569091797\n",
      "256 // size: 134 // secs to run: 0.004338264465332031\n",
      "257 // size: 30 // secs to run: 0.0010509490966796875\n",
      "258 // size: 180 // secs to run: 0.005124807357788086\n",
      "259 // size: 345 // secs to run: 67.29903221130371\n",
      "260 // size: 79 // secs to run: 0.002880096435546875\n",
      "261 // size: 119 // secs to run: 0.004846096038818359\n",
      "262 // size: 139 // secs to run: 0.0045740604400634766\n",
      "263 // size: 209 // secs to run: 0.007655143737792969\n",
      "264 // size: 92 // secs to run: 0.0028181076049804688\n",
      "265 // size: 281 // secs to run: 0.009281158447265625\n",
      "266 // size: 102 // secs to run: 0.004023075103759766\n",
      "267 // size: 180 // secs to run: 0.007580995559692383\n",
      "268 // size: 74 // secs to run: 0.0024890899658203125\n",
      "269 // size: 0 // secs to run: 2.7894973754882812e-05\n",
      "270 // size: 0 // secs to run: 1.5735626220703125e-05\n",
      "271 // size: 78 // secs to run: 0.002585887908935547\n",
      "272 // size: 108 // secs to run: 0.0031647682189941406\n",
      "273 // size: 133 // secs to run: 0.003795146942138672\n",
      "274 // size: 161 // secs to run: 0.004904747009277344\n",
      "275 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "276 // size: 97 // secs to run: 0.0027501583099365234\n",
      "277 // size: 143 // secs to run: 0.004763126373291016\n",
      "278 // size: 359 // secs to run: 65.37999367713928\n",
      "279 // size: 87 // secs to run: 0.0025746822357177734\n",
      "280 // size: 118 // secs to run: 0.0033788681030273438\n",
      "281 // size: 262 // secs to run: 0.008985042572021484\n",
      "282 // size: 97 // secs to run: 0.0031900405883789062\n",
      "283 // size: 58 // secs to run: 0.0015821456909179688\n",
      "284 // size: 52 // secs to run: 0.0018360614776611328\n",
      "285 // size: 173 // secs to run: 0.00518035888671875\n",
      "286 // size: 146 // secs to run: 0.0044972896575927734\n",
      "287 // size: 131 // secs to run: 0.00386810302734375\n",
      "288 // size: 57 // secs to run: 0.001728057861328125\n",
      "289 // size: 279 // secs to run: 0.008929967880249023\n",
      "290 // size: 92 // secs to run: 0.004225969314575195\n",
      "291 // size: 437 // secs to run: 95.13417220115662\n",
      "292 // size: 37 // secs to run: 0.0013740062713623047\n",
      "293 // size: 108 // secs to run: 0.0034453868865966797\n",
      "294 // size: 117 // secs to run: 0.003567218780517578\n",
      "295 // size: 131 // secs to run: 0.0054531097412109375\n",
      "296 // size: 102 // secs to run: 0.003348112106323242\n",
      "297 // size: 183 // secs to run: 0.0060312747955322266\n",
      "298 // size: 131 // secs to run: 0.004557132720947266\n",
      "299 // size: 85 // secs to run: 0.0032498836517333984\n",
      "300 // size: 116 // secs to run: 0.0038809776306152344\n",
      "301 // size: 83 // secs to run: 0.002753734588623047\n",
      "302 // size: 141 // secs to run: 0.003837108612060547\n",
      "303 // size: 155 // secs to run: 0.004958152770996094\n",
      "304 // size: 108 // secs to run: 0.0030679702758789062\n",
      "305 // size: 117 // secs to run: 0.0036449432373046875\n",
      "306 // size: 144 // secs to run: 0.004173755645751953\n",
      "307 // size: 142 // secs to run: 0.004204988479614258\n",
      "308 // size: 135 // secs to run: 0.004666805267333984\n",
      "309 // size: 66 // secs to run: 0.0018968582153320312\n",
      "310 // size: 155 // secs to run: 0.004246950149536133\n",
      "311 // size: 97 // secs to run: 0.0030548572540283203\n",
      "312 // size: 8 // secs to run: 0.0002739429473876953\n",
      "313 // size: 98 // secs to run: 0.002791166305541992\n",
      "314 // size: 187 // secs to run: 0.006599903106689453\n",
      "315 // size: 287 // secs to run: 0.009656906127929688\n",
      "316 // size: 81 // secs to run: 0.0029740333557128906\n",
      "317 // size: 152 // secs to run: 0.010177135467529297\n",
      "318 // size: 0 // secs to run: 5.602836608886719e-05\n",
      "319 // size: 173 // secs to run: 0.0047550201416015625\n",
      "320 // size: 38 // secs to run: 0.0015418529510498047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321 // size: 314 // secs to run: 52.17355704307556\n",
      "322 // size: 101 // secs to run: 0.0031659603118896484\n",
      "323 // size: 79 // secs to run: 0.0027480125427246094\n",
      "324 // size: 88 // secs to run: 0.0028753280639648438\n",
      "325 // size: 85 // secs to run: 0.002923727035522461\n",
      "326 // size: 138 // secs to run: 0.0036978721618652344\n",
      "327 // size: 35 // secs to run: 0.0010356903076171875\n",
      "328 // size: 192 // secs to run: 0.006430864334106445\n",
      "329 // size: 0 // secs to run: 2.5272369384765625e-05\n",
      "330 // size: 108 // secs to run: 0.003084897994995117\n",
      "331 // size: 186 // secs to run: 0.005231142044067383\n",
      "332 // size: 97 // secs to run: 0.0029687881469726562\n",
      "333 // size: 112 // secs to run: 0.0033597946166992188\n",
      "334 // size: 101 // secs to run: 0.0033211708068847656\n",
      "335 // size: 385 // secs to run: 66.31256604194641\n",
      "336 // size: 15 // secs to run: 0.0005059242248535156\n",
      "337 // size: 125 // secs to run: 0.0038766860961914062\n",
      "338 // size: 107 // secs to run: 0.003596782684326172\n",
      "339 // size: 134 // secs to run: 0.004413127899169922\n",
      "340 // size: 99 // secs to run: 0.003281831741333008\n",
      "341 // size: 91 // secs to run: 0.0032041072845458984\n",
      "342 // size: 97 // secs to run: 0.0036101341247558594\n",
      "343 // size: 272 // secs to run: 0.011077880859375\n",
      "344 // size: 150 // secs to run: 0.004730701446533203\n",
      "345 // size: 122 // secs to run: 0.0038051605224609375\n",
      "346 // size: 200 // secs to run: 0.005957126617431641\n",
      "347 // size: 89 // secs to run: 0.003074169158935547\n",
      "348 // size: 60 // secs to run: 0.002285003662109375\n",
      "349 // size: 71 // secs to run: 0.002456188201904297\n",
      "350 // size: 110 // secs to run: 0.003998994827270508\n",
      "351 // size: 0 // secs to run: 3.0040740966796875e-05\n",
      "352 // size: 124 // secs to run: 0.0038869380950927734\n",
      "353 // size: 102 // secs to run: 0.0034601688385009766\n",
      "354 // size: 87 // secs to run: 0.002686023712158203\n",
      "355 // size: 148 // secs to run: 0.00484013557434082\n",
      "356 // size: 0 // secs to run: 2.9087066650390625e-05\n",
      "357 // size: 146 // secs to run: 0.004424333572387695\n",
      "358 // size: 109 // secs to run: 0.003506898880004883\n",
      "359 // size: 68 // secs to run: 0.0023260116577148438\n",
      "360 // size: 111 // secs to run: 0.003902912139892578\n",
      "361 // size: 55 // secs to run: 0.0017578601837158203\n",
      "362 // size: 146 // secs to run: 0.0049970149993896484\n",
      "363 // size: 254 // secs to run: 0.01221609115600586\n",
      "364 // size: 190 // secs to run: 0.006004810333251953\n",
      "365 // size: 0 // secs to run: 3.0994415283203125e-05\n",
      "366 // size: 150 // secs to run: 0.004739999771118164\n",
      "367 // size: 215 // secs to run: 0.00812983512878418\n",
      "368 // size: 384 // secs to run: 79.59024500846863\n",
      "369 // size: 128 // secs to run: 0.003535032272338867\n",
      "370 // size: 596 // secs to run: 210.0054850578308\n",
      "371 // size: 147 // secs to run: 0.00542902946472168\n",
      "372 // size: 177 // secs to run: 0.006845951080322266\n",
      "373 // size: 140 // secs to run: 0.005378007888793945\n",
      "374 // size: 123 // secs to run: 0.004174947738647461\n",
      "375 // size: 147 // secs to run: 0.0043981075286865234\n",
      "376 // size: 156 // secs to run: 0.004667043685913086\n",
      "377 // size: 170 // secs to run: 0.004822969436645508\n",
      "378 // size: 143 // secs to run: 0.004112720489501953\n",
      "379 // size: 44 // secs to run: 0.0014028549194335938\n",
      "380 // size: 117 // secs to run: 0.0034890174865722656\n",
      "381 // size: 225 // secs to run: 0.007765054702758789\n",
      "382 // size: 327 // secs to run: 67.33596515655518\n",
      "383 // size: 63 // secs to run: 0.0026209354400634766\n",
      "384 // size: 159 // secs to run: 0.0055599212646484375\n",
      "385 // size: 136 // secs to run: 0.005542278289794922\n",
      "386 // size: 157 // secs to run: 0.005714893341064453\n",
      "387 // size: 116 // secs to run: 0.004543781280517578\n",
      "388 // size: 120 // secs to run: 0.007024288177490234\n",
      "389 // size: 101 // secs to run: 0.004662990570068359\n",
      "390 // size: 81 // secs to run: 0.0031981468200683594\n",
      "391 // size: 147 // secs to run: 0.005674123764038086\n",
      "392 // size: 64 // secs to run: 0.002691030502319336\n",
      "393 // size: 137 // secs to run: 0.00603485107421875\n",
      "394 // size: 213 // secs to run: 0.009783744812011719\n",
      "395 // size: 142 // secs to run: 0.007122039794921875\n",
      "396 // size: 104 // secs to run: 0.004490852355957031\n",
      "397 // size: 116 // secs to run: 0.0060198307037353516\n",
      "398 // size: 68 // secs to run: 0.0033698081970214844\n",
      "399 // size: 0 // secs to run: 6.079673767089844e-05\n",
      "400 // size: 166 // secs to run: 0.008477210998535156\n",
      "401 // size: 125 // secs to run: 0.005609989166259766\n",
      "402 // size: 100 // secs to run: 0.004119873046875\n",
      "403 // size: 112 // secs to run: 0.004992008209228516\n",
      "404 // size: 156 // secs to run: 0.008013010025024414\n",
      "405 // size: 166 // secs to run: 0.006947994232177734\n",
      "406 // size: 81 // secs to run: 0.0033941268920898438\n",
      "407 // size: 83 // secs to run: 0.003524303436279297\n",
      "408 // size: 148 // secs to run: 0.0055010318756103516\n",
      "409 // size: 190 // secs to run: 0.007628917694091797\n",
      "410 // size: 64 // secs to run: 0.0027599334716796875\n",
      "411 // size: 110 // secs to run: 0.005030155181884766\n",
      "412 // size: 150 // secs to run: 0.008110761642456055\n",
      "413 // size: 308 // secs to run: 46.19667625427246\n",
      "414 // size: 141 // secs to run: 0.004042863845825195\n",
      "415 // size: 314 // secs to run: 49.59560203552246\n",
      "416 // size: 9 // secs to run: 0.00037097930908203125\n",
      "417 // size: 0 // secs to run: 3.266334533691406e-05\n",
      "418 // size: 0 // secs to run: 1.430511474609375e-05\n",
      "419 // size: 85 // secs to run: 0.002791166305541992\n",
      "420 // size: 82 // secs to run: 0.0028901100158691406\n",
      "421 // size: 128 // secs to run: 0.003773212432861328\n",
      "422 // size: 258 // secs to run: 0.008430004119873047\n",
      "423 // size: 60 // secs to run: 0.0019118785858154297\n",
      "424 // size: 143 // secs to run: 0.004328012466430664\n",
      "425 // size: 125 // secs to run: 0.0037658214569091797\n",
      "426 // size: 93 // secs to run: 0.0025169849395751953\n",
      "427 // size: 0 // secs to run: 2.002716064453125e-05\n",
      "428 // size: 215 // secs to run: 0.00780487060546875\n",
      "429 // size: 240 // secs to run: 0.009221076965332031\n",
      "430 // size: 89 // secs to run: 0.002804279327392578\n",
      "431 // size: 471 // secs to run: 91.25808620452881\n",
      "432 // size: 41 // secs to run: 0.0016252994537353516\n",
      "433 // size: 65 // secs to run: 0.0017600059509277344\n",
      "434 // size: 102 // secs to run: 0.0029349327087402344\n",
      "435 // size: 93 // secs to run: 0.0024399757385253906\n",
      "436 // size: 109 // secs to run: 0.0039560794830322266\n",
      "437 // size: 394 // secs to run: 74.16888499259949\n",
      "438 // size: 555 // secs to run: 202.12562894821167\n",
      "439 // size: 117 // secs to run: 0.003970146179199219\n",
      "440 // size: 151 // secs to run: 0.004650115966796875\n",
      "441 // size: 66 // secs to run: 0.0022361278533935547\n",
      "442 // size: 155 // secs to run: 0.005002021789550781\n",
      "443 // size: 81 // secs to run: 0.002949953079223633\n",
      "444 // size: 149 // secs to run: 0.005420684814453125\n",
      "445 // size: 104 // secs to run: 0.003446817398071289\n",
      "446 // size: 158 // secs to run: 0.005393266677856445\n",
      "447 // size: 87 // secs to run: 0.0029761791229248047\n",
      "448 // size: 97 // secs to run: 0.0031740665435791016\n",
      "449 // size: 427 // secs to run: 108.13948202133179\n",
      "450 // size: 91 // secs to run: 0.0030019283294677734\n",
      "451 // size: 128 // secs to run: 0.003934144973754883\n",
      "452 // size: 62 // secs to run: 0.001987934112548828\n",
      "453 // size: 162 // secs to run: 0.005342006683349609\n",
      "454 // size: 62 // secs to run: 0.0024590492248535156\n",
      "455 // size: 0 // secs to run: 3.314018249511719e-05\n",
      "456 // size: 187 // secs to run: 0.008352994918823242\n",
      "457 // size: 153 // secs to run: 0.004888057708740234\n",
      "458 // size: 117 // secs to run: 0.0038619041442871094\n",
      "459 // size: 65 // secs to run: 0.001983165740966797\n",
      "460 // size: 232 // secs to run: 0.008723258972167969\n",
      "461 // size: 122 // secs to run: 0.004475831985473633\n",
      "462 // size: 170 // secs to run: 0.004762887954711914\n",
      "463 // size: 85 // secs to run: 0.003498077392578125\n",
      "464 // size: 144 // secs to run: 0.00480198860168457\n",
      "465 // size: 98 // secs to run: 0.003606081008911133\n",
      "466 // size: 104 // secs to run: 0.003442049026489258\n",
      "467 // size: 138 // secs to run: 0.004535198211669922\n",
      "468 // size: 150 // secs to run: 0.004594087600708008\n",
      "469 // size: 112 // secs to run: 0.0037620067596435547\n",
      "470 // size: 163 // secs to run: 0.004929065704345703\n",
      "471 // size: 123 // secs to run: 0.004542827606201172\n",
      "472 // size: 145 // secs to run: 0.0046651363372802734\n",
      "473 // size: 111 // secs to run: 0.0036716461181640625\n",
      "474 // size: 100 // secs to run: 0.003331899642944336\n",
      "475 // size: 188 // secs to run: 0.008957386016845703\n",
      "476 // size: 111 // secs to run: 0.003606081008911133\n",
      "477 // size: 66 // secs to run: 0.002268075942993164\n",
      "478 // size: 140 // secs to run: 0.0045299530029296875\n",
      "479 // size: 94 // secs to run: 0.0032761096954345703\n",
      "480 // size: 113 // secs to run: 0.0037529468536376953\n",
      "481 // size: 104 // secs to run: 0.003268003463745117\n",
      "482 // size: 125 // secs to run: 0.003991127014160156\n",
      "483 // size: 96 // secs to run: 0.003156900405883789\n",
      "484 // size: 89 // secs to run: 0.002680063247680664\n",
      "485 // size: 126 // secs to run: 0.003904104232788086\n",
      "486 // size: 119 // secs to run: 0.003899097442626953\n",
      "487 // size: 122 // secs to run: 0.003957033157348633\n",
      "488 // size: 171 // secs to run: 0.005816936492919922\n",
      "489 // size: 141 // secs to run: 0.004180192947387695\n",
      "490 // size: 144 // secs to run: 0.004739284515380859\n",
      "491 // size: 105 // secs to run: 0.004064083099365234\n",
      "492 // size: 95 // secs to run: 0.002956867218017578\n",
      "493 // size: 155 // secs to run: 0.00473785400390625\n",
      "494 // size: 95 // secs to run: 0.002889871597290039\n",
      "495 // size: 157 // secs to run: 0.005273103713989258\n",
      "496 // size: 118 // secs to run: 0.0038361549377441406\n",
      "497 // size: 229 // secs to run: 0.0076770782470703125\n",
      "498 // size: 84 // secs to run: 0.003103017807006836\n",
      "499 // size: 123 // secs to run: 0.003799915313720703\n",
      "500 // size: 93 // secs to run: 0.0031380653381347656\n",
      "501 // size: 0 // secs to run: 2.2172927856445312e-05\n",
      "502 // size: 0 // secs to run: 1.621246337890625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 // size: 396 // secs to run: 82.2067129611969\n",
      "504 // size: 171 // secs to run: 0.0056650638580322266\n",
      "505 // size: 99 // secs to run: 0.0032777786254882812\n",
      "506 // size: 120 // secs to run: 0.0041887760162353516\n",
      "507 // size: 113 // secs to run: 0.0034770965576171875\n",
      "508 // size: 124 // secs to run: 0.004297018051147461\n",
      "509 // size: 128 // secs to run: 0.004094362258911133\n",
      "510 // size: 135 // secs to run: 0.004683017730712891\n",
      "511 // size: 269 // secs to run: 0.00948190689086914\n",
      "512 // size: 86 // secs to run: 0.002768993377685547\n",
      "513 // size: 71 // secs to run: 0.0021522045135498047\n",
      "514 // size: 102 // secs to run: 0.0033779144287109375\n",
      "515 // size: 102 // secs to run: 0.003468036651611328\n",
      "516 // size: 357 // secs to run: 72.91860699653625\n",
      "517 // size: 154 // secs to run: 0.005471944808959961\n",
      "518 // size: 153 // secs to run: 0.0057561397552490234\n",
      "519 // size: 82 // secs to run: 0.0031311511993408203\n",
      "520 // size: 96 // secs to run: 0.0038068294525146484\n",
      "521 // size: 101 // secs to run: 0.0035037994384765625\n",
      "522 // size: 168 // secs to run: 0.00613713264465332\n",
      "523 // size: 121 // secs to run: 0.005918025970458984\n",
      "524 // size: 249 // secs to run: 0.011022090911865234\n",
      "525 // size: 175 // secs to run: 0.007519960403442383\n",
      "526 // size: 414 // secs to run: 87.03281998634338\n",
      "527 // size: 80 // secs to run: 0.003215312957763672\n",
      "528 // size: 107 // secs to run: 0.003993034362792969\n",
      "529 // size: 147 // secs to run: 0.00498509407043457\n",
      "530 // size: 149 // secs to run: 0.00569605827331543\n",
      "531 // size: 76 // secs to run: 0.0029239654541015625\n",
      "532 // size: 105 // secs to run: 0.004004955291748047\n",
      "533 // size: 150 // secs to run: 0.005976676940917969\n",
      "534 // size: 0 // secs to run: 4.8160552978515625e-05\n",
      "535 // size: 68 // secs to run: 0.0024356842041015625\n",
      "536 // size: 109 // secs to run: 0.004410982131958008\n",
      "537 // size: 163 // secs to run: 0.007227182388305664\n",
      "538 // size: 139 // secs to run: 0.005055904388427734\n",
      "539 // size: 128 // secs to run: 0.005221843719482422\n",
      "540 // size: 155 // secs to run: 0.005856990814208984\n",
      "541 // size: 181 // secs to run: 0.007642984390258789\n",
      "542 // size: 185 // secs to run: 0.007493019104003906\n",
      "543 // size: 184 // secs to run: 0.008497953414916992\n",
      "544 // size: 130 // secs to run: 0.005646228790283203\n",
      "545 // size: 127 // secs to run: 0.0046482086181640625\n",
      "546 // size: 100 // secs to run: 0.003952980041503906\n",
      "547 // size: 107 // secs to run: 0.004772186279296875\n",
      "548 // size: 117 // secs to run: 0.0040509700775146484\n",
      "549 // size: 44 // secs to run: 0.0020058155059814453\n",
      "550 // size: 46 // secs to run: 0.0018279552459716797\n",
      "551 // size: 106 // secs to run: 0.005054950714111328\n",
      "552 // size: 161 // secs to run: 0.007684230804443359\n",
      "553 // size: 147 // secs to run: 0.007130861282348633\n",
      "554 // size: 0 // secs to run: 0.00013303756713867188\n",
      "555 // size: 74 // secs to run: 0.003470182418823242\n",
      "556 // size: 121 // secs to run: 0.0052187442779541016\n",
      "557 // size: 0 // secs to run: 5.698204040527344e-05\n",
      "558 // size: 172 // secs to run: 0.008059024810791016\n",
      "559 // size: 142 // secs to run: 0.005816221237182617\n",
      "560 // size: 102 // secs to run: 0.004801034927368164\n",
      "561 // size: 91 // secs to run: 0.003846883773803711\n",
      "562 // size: 0 // secs to run: 5.1975250244140625e-05\n",
      "563 // size: 97 // secs to run: 0.004562854766845703\n",
      "564 // size: 170 // secs to run: 0.007075071334838867\n",
      "565 // size: 132 // secs to run: 0.005551815032958984\n",
      "566 // size: 128 // secs to run: 0.004967927932739258\n",
      "567 // size: 160 // secs to run: 0.008436918258666992\n",
      "568 // size: 112 // secs to run: 0.00438690185546875\n",
      "569 // size: 131 // secs to run: 0.006295204162597656\n",
      "570 // size: 156 // secs to run: 0.007314920425415039\n",
      "571 // size: 0 // secs to run: 6.413459777832031e-05\n",
      "572 // size: 50 // secs to run: 0.0020678043365478516\n",
      "573 // size: 55 // secs to run: 0.002351045608520508\n",
      "574 // size: 119 // secs to run: 0.0048160552978515625\n",
      "575 // size: 73 // secs to run: 0.003309011459350586\n",
      "576 // size: 95 // secs to run: 0.003945827484130859\n",
      "577 // size: 93 // secs to run: 0.0037698745727539062\n",
      "578 // size: 128 // secs to run: 0.004855632781982422\n",
      "579 // size: 82 // secs to run: 0.0032482147216796875\n",
      "580 // size: 157 // secs to run: 0.006555795669555664\n",
      "581 // size: 134 // secs to run: 0.005081892013549805\n",
      "582 // size: 161 // secs to run: 0.006265878677368164\n",
      "583 // size: 213 // secs to run: 0.011049985885620117\n",
      "584 // size: 267 // secs to run: 0.01383209228515625\n",
      "585 // size: 97 // secs to run: 0.005304098129272461\n",
      "586 // size: 439 // secs to run: 98.3873040676117\n",
      "587 // size: 147 // secs to run: 0.0043621063232421875\n",
      "588 // size: 94 // secs to run: 0.003489971160888672\n",
      "589 // size: 65 // secs to run: 0.002225160598754883\n",
      "590 // size: 136 // secs to run: 0.004850864410400391\n",
      "591 // size: 175 // secs to run: 0.005872249603271484\n",
      "592 // size: 56 // secs to run: 0.0019440650939941406\n",
      "593 // size: 103 // secs to run: 0.003448963165283203\n",
      "594 // size: 132 // secs to run: 0.004269123077392578\n",
      "595 // size: 79 // secs to run: 0.003077268600463867\n",
      "596 // size: 109 // secs to run: 0.0037360191345214844\n",
      "597 // size: 129 // secs to run: 0.0046138763427734375\n",
      "598 // size: 162 // secs to run: 0.005441904067993164\n",
      "599 // size: 0 // secs to run: 2.288818359375e-05\n",
      "600 // size: 0 // secs to run: 1.5735626220703125e-05\n",
      "601 // size: 133 // secs to run: 0.003938913345336914\n",
      "602 // size: 85 // secs to run: 0.0026743412017822266\n",
      "603 // size: 314 // secs to run: 47.35736799240112\n",
      "604 // size: 131 // secs to run: 0.004229068756103516\n",
      "605 // size: 137 // secs to run: 0.004124879837036133\n",
      "606 // size: 104 // secs to run: 0.0030422210693359375\n",
      "607 // size: 163 // secs to run: 0.005616903305053711\n",
      "608 // size: 101 // secs to run: 0.003603219985961914\n",
      "609 // size: 20 // secs to run: 0.0007071495056152344\n",
      "610 // size: 130 // secs to run: 0.004884958267211914\n",
      "611 // size: 49 // secs to run: 0.0018410682678222656\n",
      "612 // size: 120 // secs to run: 0.003823995590209961\n",
      "613 // size: 145 // secs to run: 0.0049707889556884766\n",
      "614 // size: 135 // secs to run: 0.0045549869537353516\n",
      "615 // size: 159 // secs to run: 0.005133152008056641\n",
      "616 // size: 117 // secs to run: 0.003715038299560547\n",
      "617 // size: 135 // secs to run: 0.0037381649017333984\n",
      "618 // size: 153 // secs to run: 0.004930734634399414\n",
      "619 // size: 77 // secs to run: 0.0024318695068359375\n",
      "620 // size: 110 // secs to run: 0.0037059783935546875\n",
      "621 // size: 113 // secs to run: 0.003846883773803711\n",
      "622 // size: 163 // secs to run: 0.00502324104309082\n",
      "623 // size: 110 // secs to run: 0.003648042678833008\n",
      "624 // size: 98 // secs to run: 0.0033881664276123047\n",
      "625 // size: 111 // secs to run: 0.003461122512817383\n",
      "626 // size: 110 // secs to run: 0.0038900375366210938\n",
      "627 // size: 87 // secs to run: 0.003320932388305664\n",
      "628 // size: 0 // secs to run: 2.288818359375e-05\n",
      "629 // size: 102 // secs to run: 0.0032498836517333984\n",
      "630 // size: 138 // secs to run: 0.0040740966796875\n",
      "631 // size: 204 // secs to run: 0.008929014205932617\n",
      "632 // size: 105 // secs to run: 0.003401041030883789\n",
      "633 // size: 303 // secs to run: 45.89636301994324\n",
      "634 // size: 107 // secs to run: 0.0038781166076660156\n",
      "635 // size: 122 // secs to run: 0.00360107421875\n",
      "636 // size: 112 // secs to run: 0.003538846969604492\n",
      "637 // size: 276 // secs to run: 0.009879112243652344\n",
      "638 // size: 104 // secs to run: 0.0031027793884277344\n",
      "639 // size: 70 // secs to run: 0.0023381710052490234\n",
      "640 // size: 400 // secs to run: 66.48014163970947\n",
      "641 // size: 108 // secs to run: 0.003975868225097656\n",
      "642 // size: 47 // secs to run: 0.0017819404602050781\n",
      "643 // size: 311 // secs to run: 40.54214382171631\n",
      "644 // size: 104 // secs to run: 0.003242015838623047\n",
      "645 // size: 0 // secs to run: 2.5987625122070312e-05\n",
      "646 // size: 152 // secs to run: 0.004907846450805664\n",
      "647 // size: 94 // secs to run: 0.0032989978790283203\n",
      "648 // size: 100 // secs to run: 0.002816915512084961\n",
      "649 // size: 65 // secs to run: 0.0023190975189208984\n",
      "650 // size: 61 // secs to run: 0.003049135208129883\n",
      "651 // size: 20 // secs to run: 0.0006668567657470703\n",
      "652 // size: 376 // secs to run: 92.17106771469116\n",
      "653 // size: 0 // secs to run: 4.315376281738281e-05\n",
      "654 // size: 52 // secs to run: 0.0018379688262939453\n",
      "655 // size: 176 // secs to run: 0.00508880615234375\n",
      "656 // size: 183 // secs to run: 0.005240917205810547\n",
      "657 // size: 152 // secs to run: 0.005045175552368164\n",
      "658 // size: 124 // secs to run: 0.0038781166076660156\n",
      "659 // size: 165 // secs to run: 0.004909992218017578\n",
      "660 // size: 100 // secs to run: 0.003526926040649414\n",
      "661 // size: 133 // secs to run: 0.004302024841308594\n",
      "662 // size: 112 // secs to run: 0.0036668777465820312\n",
      "663 // size: 158 // secs to run: 0.004776954650878906\n",
      "664 // size: 101 // secs to run: 0.003423929214477539\n",
      "665 // size: 0 // secs to run: 2.3126602172851562e-05\n",
      "666 // size: 86 // secs to run: 0.0024743080139160156\n",
      "667 // size: 0 // secs to run: 2.2172927856445312e-05\n",
      "668 // size: 100 // secs to run: 0.00335693359375\n",
      "669 // size: 112 // secs to run: 0.0028939247131347656\n",
      "670 // size: 114 // secs to run: 0.003704071044921875\n",
      "671 // size: 64 // secs to run: 0.0016989707946777344\n",
      "672 // size: 59 // secs to run: 0.0016510486602783203\n",
      "673 // size: 79 // secs to run: 0.002457141876220703\n",
      "674 // size: 0 // secs to run: 2.09808349609375e-05\n",
      "675 // size: 0 // secs to run: 2.4080276489257812e-05\n",
      "676 // size: 141 // secs to run: 0.003879070281982422\n",
      "677 // size: 73 // secs to run: 0.0023322105407714844\n",
      "678 // size: 112 // secs to run: 0.0034720897674560547\n",
      "679 // size: 78 // secs to run: 0.002443075180053711\n",
      "680 // size: 71 // secs to run: 0.0017998218536376953\n",
      "681 // size: 121 // secs to run: 0.003676176071166992\n",
      "682 // size: 106 // secs to run: 0.003178834915161133\n",
      "683 // size: 110 // secs to run: 0.003422975540161133\n",
      "684 // size: 0 // secs to run: 2.002716064453125e-05\n",
      "685 // size: 151 // secs to run: 0.004570960998535156\n",
      "686 // size: 92 // secs to run: 0.002649068832397461\n",
      "687 // size: 0 // secs to run: 2.0265579223632812e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688 // size: 469 // secs to run: 118.647451877594\n",
      "689 // size: 123 // secs to run: 0.0038902759552001953\n",
      "690 // size: 210 // secs to run: 0.007477998733520508\n",
      "691 // size: 105 // secs to run: 0.003484964370727539\n",
      "692 // size: 84 // secs to run: 0.0028700828552246094\n",
      "693 // size: 92 // secs to run: 0.003047943115234375\n",
      "694 // size: 117 // secs to run: 0.004106998443603516\n",
      "695 // size: 116 // secs to run: 0.003824949264526367\n",
      "696 // size: 0 // secs to run: 2.8848648071289062e-05\n",
      "697 // size: 69 // secs to run: 0.0020542144775390625\n",
      "698 // size: 141 // secs to run: 0.004665851593017578\n",
      "699 // size: 170 // secs to run: 0.00683903694152832\n",
      "700 // size: 0 // secs to run: 2.5987625122070312e-05\n",
      "701 // size: 59 // secs to run: 0.002092123031616211\n",
      "702 // size: 101 // secs to run: 0.003242969512939453\n",
      "703 // size: 121 // secs to run: 0.004110097885131836\n",
      "704 // size: 100 // secs to run: 0.0033483505249023438\n",
      "705 // size: 138 // secs to run: 0.00450897216796875\n",
      "706 // size: 239 // secs to run: 0.007953882217407227\n",
      "707 // size: 66 // secs to run: 0.0030269622802734375\n",
      "708 // size: 109 // secs to run: 0.003480195999145508\n",
      "709 // size: 156 // secs to run: 0.005083322525024414\n",
      "710 // size: 120 // secs to run: 0.003579854965209961\n",
      "711 // size: 0 // secs to run: 2.09808349609375e-05\n",
      "712 // size: 79 // secs to run: 0.0028328895568847656\n",
      "713 // size: 116 // secs to run: 0.004094123840332031\n",
      "714 // size: 117 // secs to run: 0.003593921661376953\n",
      "715 // size: 151 // secs to run: 0.004949808120727539\n",
      "716 // size: 99 // secs to run: 0.0033190250396728516\n",
      "717 // size: 148 // secs to run: 0.004523038864135742\n",
      "718 // size: 87 // secs to run: 0.0031898021697998047\n",
      "719 // size: 55 // secs to run: 0.0014688968658447266\n",
      "720 // size: 124 // secs to run: 0.003851175308227539\n",
      "721 // size: 133 // secs to run: 0.005755186080932617\n",
      "722 // size: 48 // secs to run: 0.0017170906066894531\n",
      "723 // size: 153 // secs to run: 0.004266977310180664\n",
      "724 // size: 282 // secs to run: 0.010848045349121094\n",
      "725 // size: 71 // secs to run: 0.0028638839721679688\n",
      "726 // size: 122 // secs to run: 0.00416874885559082\n",
      "727 // size: 93 // secs to run: 0.0029098987579345703\n",
      "728 // size: 213 // secs to run: 0.008256912231445312\n",
      "729 // size: 167 // secs to run: 0.004717826843261719\n",
      "730 // size: 85 // secs to run: 0.002841949462890625\n",
      "731 // size: 172 // secs to run: 0.0056650638580322266\n",
      "732 // size: 325 // secs to run: 62.649539947509766\n",
      "733 // size: 83 // secs to run: 0.002705812454223633\n",
      "734 // size: 172 // secs to run: 0.00476384162902832\n",
      "735 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "736 // size: 244 // secs to run: 0.009377717971801758\n",
      "737 // size: 153 // secs to run: 0.004902839660644531\n",
      "738 // size: 49 // secs to run: 0.001628875732421875\n",
      "739 // size: 111 // secs to run: 0.003407001495361328\n",
      "740 // size: 294 // secs to run: 0.0100860595703125\n",
      "741 // size: 60 // secs to run: 0.0018491744995117188\n",
      "742 // size: 173 // secs to run: 0.005125761032104492\n",
      "743 // size: 178 // secs to run: 0.005785942077636719\n",
      "744 // size: 46 // secs to run: 0.001628875732421875\n",
      "745 // size: 122 // secs to run: 0.004014253616333008\n",
      "746 // size: 89 // secs to run: 0.0027158260345458984\n",
      "747 // size: 45 // secs to run: 0.0016450881958007812\n",
      "748 // size: 90 // secs to run: 0.002716064453125\n",
      "749 // size: 150 // secs to run: 0.004839181900024414\n",
      "750 // size: 158 // secs to run: 0.0046961307525634766\n",
      "751 // size: 157 // secs to run: 0.004948139190673828\n",
      "752 // size: 153 // secs to run: 0.0046329498291015625\n",
      "753 // size: 81 // secs to run: 0.0026917457580566406\n",
      "754 // size: 74 // secs to run: 0.0027048587799072266\n",
      "755 // size: 139 // secs to run: 0.004270792007446289\n",
      "756 // size: 53 // secs to run: 0.0021390914916992188\n",
      "757 // size: 143 // secs to run: 0.00496673583984375\n",
      "758 // size: 161 // secs to run: 0.006102085113525391\n",
      "759 // size: 163 // secs to run: 0.005120992660522461\n",
      "760 // size: 182 // secs to run: 0.0054972171783447266\n",
      "761 // size: 120 // secs to run: 0.0036499500274658203\n",
      "762 // size: 123 // secs to run: 0.0040130615234375\n",
      "763 // size: 81 // secs to run: 0.0029599666595458984\n",
      "764 // size: 143 // secs to run: 0.004831075668334961\n",
      "765 // size: 110 // secs to run: 0.0038700103759765625\n",
      "766 // size: 124 // secs to run: 0.004163026809692383\n",
      "767 // size: 333 // secs to run: 69.07913494110107\n",
      "768 // size: 61 // secs to run: 0.0023260116577148438\n",
      "769 // size: 496 // secs to run: 197.54414176940918\n",
      "770 // size: 0 // secs to run: 4.100799560546875e-05\n",
      "771 // size: 134 // secs to run: 0.003576993942260742\n",
      "772 // size: 121 // secs to run: 0.0037102699279785156\n",
      "773 // size: 628 // secs to run: 277.0675277709961\n",
      "774 // size: 120 // secs to run: 0.0046122074127197266\n",
      "775 // size: 0 // secs to run: 4.982948303222656e-05\n",
      "776 // size: 130 // secs to run: 0.004164695739746094\n",
      "777 // size: 148 // secs to run: 0.0073740482330322266\n",
      "778 // size: 142 // secs to run: 0.004943132400512695\n",
      "779 // size: 113 // secs to run: 0.0035347938537597656\n",
      "780 // size: 88 // secs to run: 0.0027201175689697266\n",
      "781 // size: 101 // secs to run: 0.0027132034301757812\n",
      "782 // size: 152 // secs to run: 0.0040130615234375\n",
      "783 // size: 107 // secs to run: 0.003529071807861328\n",
      "784 // size: 143 // secs to run: 0.004572868347167969\n",
      "785 // size: 118 // secs to run: 0.003949165344238281\n",
      "786 // size: 111 // secs to run: 0.0031881332397460938\n",
      "787 // size: 145 // secs to run: 0.004042148590087891\n",
      "788 // size: 161 // secs to run: 0.004673004150390625\n",
      "789 // size: 117 // secs to run: 0.0035309791564941406\n",
      "790 // size: 126 // secs to run: 0.003896951675415039\n",
      "791 // size: 172 // secs to run: 0.00549626350402832\n",
      "792 // size: 116 // secs to run: 0.004075050354003906\n",
      "793 // size: 149 // secs to run: 0.005003213882446289\n",
      "794 // size: 157 // secs to run: 0.0047070980072021484\n",
      "795 // size: 256 // secs to run: 0.008853912353515625\n",
      "796 // size: 93 // secs to run: 0.0025320053100585938\n",
      "797 // size: 105 // secs to run: 0.0029578208923339844\n",
      "798 // size: 125 // secs to run: 0.003255128860473633\n",
      "799 // size: 68 // secs to run: 0.0022509098052978516\n",
      "800 // size: 52 // secs to run: 0.0015418529510498047\n",
      "801 // size: 98 // secs to run: 0.0030808448791503906\n",
      "802 // size: 114 // secs to run: 0.0036690235137939453\n",
      "803 // size: 0 // secs to run: 2.3126602172851562e-05\n",
      "804 // size: 0 // secs to run: 1.52587890625e-05\n",
      "805 // size: 147 // secs to run: 0.0043182373046875\n",
      "806 // size: 118 // secs to run: 0.003715991973876953\n",
      "807 // size: 135 // secs to run: 0.004193782806396484\n",
      "808 // size: 107 // secs to run: 0.003650665283203125\n",
      "809 // size: 136 // secs to run: 0.004039764404296875\n",
      "810 // size: 135 // secs to run: 0.003476858139038086\n",
      "811 // size: 144 // secs to run: 0.004244089126586914\n",
      "812 // size: 126 // secs to run: 0.004343748092651367\n",
      "813 // size: 103 // secs to run: 0.003777742385864258\n",
      "814 // size: 120 // secs to run: 0.003901958465576172\n",
      "815 // size: 145 // secs to run: 0.004924774169921875\n",
      "816 // size: 122 // secs to run: 0.00419306755065918\n",
      "817 // size: 180 // secs to run: 0.0057260990142822266\n",
      "818 // size: 150 // secs to run: 0.0049779415130615234\n",
      "819 // size: 119 // secs to run: 0.00491023063659668\n",
      "820 // size: 61 // secs to run: 0.0020999908447265625\n",
      "821 // size: 116 // secs to run: 0.003160715103149414\n",
      "822 // size: 122 // secs to run: 0.0032660961151123047\n",
      "823 // size: 68 // secs to run: 0.001806020736694336\n",
      "824 // size: 117 // secs to run: 0.0033309459686279297\n",
      "825 // size: 607 // secs to run: 238.51483702659607\n",
      "826 // size: 130 // secs to run: 0.004120826721191406\n",
      "827 // size: 92 // secs to run: 0.003242969512939453\n",
      "828 // size: 115 // secs to run: 0.0034410953521728516\n",
      "829 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "830 // size: 119 // secs to run: 0.003507852554321289\n",
      "831 // size: 104 // secs to run: 0.0034911632537841797\n",
      "832 // size: 41 // secs to run: 0.0014078617095947266\n",
      "833 // size: 115 // secs to run: 0.0035851001739501953\n",
      "834 // size: 71 // secs to run: 0.0022840499877929688\n",
      "835 // size: 0 // secs to run: 2.09808349609375e-05\n",
      "836 // size: 138 // secs to run: 0.0042231082916259766\n",
      "837 // size: 122 // secs to run: 0.0037271976470947266\n",
      "838 // size: 114 // secs to run: 0.003592252731323242\n",
      "839 // size: 95 // secs to run: 0.002574920654296875\n",
      "840 // size: 119 // secs to run: 0.0034008026123046875\n",
      "841 // size: 124 // secs to run: 0.0040740966796875\n",
      "842 // size: 127 // secs to run: 0.003644227981567383\n",
      "843 // size: 82 // secs to run: 0.002698183059692383\n",
      "844 // size: 121 // secs to run: 0.003838062286376953\n",
      "845 // size: 144 // secs to run: 0.005033969879150391\n",
      "846 // size: 104 // secs to run: 0.003206014633178711\n",
      "847 // size: 190 // secs to run: 0.0049359798431396484\n",
      "848 // size: 133 // secs to run: 0.003751993179321289\n",
      "849 // size: 136 // secs to run: 0.0044269561767578125\n",
      "850 // size: 111 // secs to run: 0.003256082534790039\n",
      "851 // size: 116 // secs to run: 0.0033979415893554688\n",
      "852 // size: 147 // secs to run: 0.005155086517333984\n",
      "853 // size: 115 // secs to run: 0.003973960876464844\n",
      "854 // size: 95 // secs to run: 0.0026350021362304688\n",
      "855 // size: 57 // secs to run: 0.0016298294067382812\n",
      "856 // size: 44 // secs to run: 0.001338958740234375\n",
      "857 // size: 156 // secs to run: 0.004476785659790039\n",
      "858 // size: 192 // secs to run: 0.004770040512084961\n",
      "859 // size: 69 // secs to run: 0.0017769336700439453\n",
      "860 // size: 111 // secs to run: 0.003439188003540039\n",
      "861 // size: 109 // secs to run: 0.003222942352294922\n",
      "862 // size: 148 // secs to run: 0.004152059555053711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "863 // size: 397 // secs to run: 69.33651900291443\n",
      "864 // size: 100 // secs to run: 0.002897977828979492\n",
      "865 // size: 96 // secs to run: 0.003568887710571289\n",
      "866 // size: 72 // secs to run: 0.002323150634765625\n",
      "867 // size: 0 // secs to run: 2.7894973754882812e-05\n",
      "868 // size: 70 // secs to run: 0.0019741058349609375\n",
      "869 // size: 88 // secs to run: 0.0032749176025390625\n",
      "870 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "871 // size: 86 // secs to run: 0.002769947052001953\n",
      "872 // size: 164 // secs to run: 0.004871845245361328\n",
      "873 // size: 158 // secs to run: 0.0044591426849365234\n",
      "874 // size: 129 // secs to run: 0.004282951354980469\n",
      "875 // size: 146 // secs to run: 0.00509190559387207\n",
      "876 // size: 115 // secs to run: 0.004077911376953125\n",
      "877 // size: 399 // secs to run: 90.79249572753906\n",
      "878 // size: 122 // secs to run: 0.005072832107543945\n",
      "879 // size: 117 // secs to run: 0.0040667057037353516\n",
      "880 // size: 48 // secs to run: 0.0018868446350097656\n",
      "881 // size: 511 // secs to run: 145.26139092445374\n",
      "882 // size: 0 // secs to run: 4.124641418457031e-05\n",
      "883 // size: 86 // secs to run: 0.0026199817657470703\n",
      "884 // size: 150 // secs to run: 0.0037508010864257812\n",
      "885 // size: 122 // secs to run: 0.003751039505004883\n",
      "886 // size: 83 // secs to run: 0.0026731491088867188\n",
      "887 // size: 71 // secs to run: 0.0018818378448486328\n",
      "888 // size: 125 // secs to run: 0.0034842491149902344\n",
      "889 // size: 106 // secs to run: 0.0033740997314453125\n",
      "890 // size: 150 // secs to run: 0.004292011260986328\n",
      "891 // size: 74 // secs to run: 0.0025720596313476562\n",
      "892 // size: 99 // secs to run: 0.0029001235961914062\n",
      "893 // size: 131 // secs to run: 0.0037660598754882812\n",
      "894 // size: 174 // secs to run: 0.004469394683837891\n",
      "895 // size: 93 // secs to run: 0.002889394760131836\n",
      "896 // size: 427 // secs to run: 89.93366885185242\n",
      "897 // size: 0 // secs to run: 6.794929504394531e-05\n",
      "898 // size: 0 // secs to run: 2.3126602172851562e-05\n",
      "899 // size: 147 // secs to run: 0.007570028305053711\n",
      "900 // size: 104 // secs to run: 0.004460811614990234\n",
      "901 // size: 78 // secs to run: 0.0033931732177734375\n",
      "902 // size: 118 // secs to run: 0.007134914398193359\n",
      "903 // size: 357 // secs to run: 65.52842092514038\n",
      "904 // size: 322 // secs to run: 74.16242003440857\n",
      "905 // size: 108 // secs to run: 0.003835916519165039\n",
      "906 // size: 285 // secs to run: 0.010192155838012695\n",
      "907 // size: 116 // secs to run: 0.003849029541015625\n",
      "908 // size: 149 // secs to run: 0.004723072052001953\n",
      "909 // size: 71 // secs to run: 0.002438068389892578\n",
      "910 // size: 140 // secs to run: 0.004383087158203125\n",
      "911 // size: 88 // secs to run: 0.0031151771545410156\n",
      "912 // size: 94 // secs to run: 0.0031147003173828125\n",
      "913 // size: 133 // secs to run: 0.003860950469970703\n",
      "914 // size: 264 // secs to run: 0.009593963623046875\n",
      "915 // size: 125 // secs to run: 0.004134178161621094\n",
      "916 // size: 97 // secs to run: 0.003161191940307617\n",
      "917 // size: 107 // secs to run: 0.003300189971923828\n",
      "918 // size: 108 // secs to run: 0.003383159637451172\n",
      "919 // size: 81 // secs to run: 0.002508878707885742\n",
      "920 // size: 179 // secs to run: 0.005321979522705078\n",
      "921 // size: 68 // secs to run: 0.0023391246795654297\n",
      "922 // size: 194 // secs to run: 0.0072209835052490234\n",
      "923 // size: 54 // secs to run: 0.001981019973754883\n",
      "924 // size: 368 // secs to run: 68.6613039970398\n",
      "925 // size: 0 // secs to run: 4.315376281738281e-05\n",
      "926 // size: 119 // secs to run: 0.004113197326660156\n",
      "927 // size: 96 // secs to run: 0.002931833267211914\n",
      "928 // size: 110 // secs to run: 0.0038950443267822266\n",
      "929 // size: 139 // secs to run: 0.004779815673828125\n",
      "930 // size: 85 // secs to run: 0.003329038619995117\n",
      "931 // size: 63 // secs to run: 0.0021059513092041016\n",
      "932 // size: 156 // secs to run: 0.005136251449584961\n",
      "933 // size: 86 // secs to run: 0.002930879592895508\n",
      "934 // size: 66 // secs to run: 0.002390146255493164\n",
      "935 // size: 42 // secs to run: 0.0014951229095458984\n",
      "936 // size: 91 // secs to run: 0.0028450489044189453\n",
      "937 // size: 153 // secs to run: 0.004572868347167969\n",
      "938 // size: 76 // secs to run: 0.0023279190063476562\n",
      "939 // size: 69 // secs to run: 0.0027539730072021484\n",
      "940 // size: 311 // secs to run: 44.15077018737793\n",
      "941 // size: 141 // secs to run: 0.004474163055419922\n",
      "942 // size: 100 // secs to run: 0.003451824188232422\n",
      "943 // size: 16 // secs to run: 0.0006740093231201172\n",
      "944 // size: 131 // secs to run: 0.0041370391845703125\n",
      "945 // size: 330 // secs to run: 56.44244408607483\n",
      "946 // size: 121 // secs to run: 0.003766775131225586\n",
      "947 // size: 177 // secs to run: 0.0054931640625\n",
      "948 // size: 126 // secs to run: 0.004127025604248047\n",
      "949 // size: 117 // secs to run: 0.003754854202270508\n",
      "950 // size: 193 // secs to run: 0.005611896514892578\n",
      "951 // size: 92 // secs to run: 0.0026748180389404297\n",
      "952 // size: 181 // secs to run: 0.005444049835205078\n",
      "953 // size: 151 // secs to run: 0.004960060119628906\n",
      "954 // size: 109 // secs to run: 0.003512859344482422\n",
      "955 // size: 127 // secs to run: 0.004194021224975586\n",
      "956 // size: 135 // secs to run: 0.0039958953857421875\n",
      "957 // size: 170 // secs to run: 0.004751920700073242\n",
      "958 // size: 120 // secs to run: 0.0035529136657714844\n",
      "959 // size: 102 // secs to run: 0.0034241676330566406\n",
      "960 // size: 370 // secs to run: 75.322350025177\n",
      "961 // size: 137 // secs to run: 0.0043888092041015625\n",
      "962 // size: 98 // secs to run: 0.002913951873779297\n",
      "963 // size: 162 // secs to run: 0.004159212112426758\n",
      "964 // size: 110 // secs to run: 0.0029561519622802734\n",
      "965 // size: 92 // secs to run: 0.0026068687438964844\n",
      "966 // size: 100 // secs to run: 0.003061056137084961\n",
      "967 // size: 127 // secs to run: 0.003913402557373047\n",
      "968 // size: 109 // secs to run: 0.003242969512939453\n",
      "969 // size: 145 // secs to run: 0.004434108734130859\n",
      "970 // size: 154 // secs to run: 0.004488229751586914\n",
      "971 // size: 194 // secs to run: 0.008051872253417969\n",
      "972 // size: 82 // secs to run: 0.0023469924926757812\n",
      "973 // size: 74 // secs to run: 0.0021479129791259766\n",
      "974 // size: 139 // secs to run: 0.0038499832153320312\n",
      "975 // size: 68 // secs to run: 0.002251148223876953\n",
      "976 // size: 122 // secs to run: 0.0032482147216796875\n",
      "977 // size: 161 // secs to run: 0.004803895950317383\n",
      "978 // size: 141 // secs to run: 0.004095792770385742\n",
      "979 // size: 175 // secs to run: 0.005299091339111328\n",
      "980 // size: 146 // secs to run: 0.005250692367553711\n",
      "981 // size: 135 // secs to run: 0.003754138946533203\n",
      "982 // size: 280 // secs to run: 0.011447906494140625\n",
      "983 // size: 131 // secs to run: 0.0036630630493164062\n",
      "984 // size: 85 // secs to run: 0.002882242202758789\n",
      "985 // size: 0 // secs to run: 2.193450927734375e-05\n",
      "986 // size: 83 // secs to run: 0.0026340484619140625\n",
      "987 // size: 372 // secs to run: 69.31349992752075\n",
      "988 // size: 133 // secs to run: 0.0039539337158203125\n",
      "989 // size: 145 // secs to run: 0.004105091094970703\n",
      "990 // size: 122 // secs to run: 0.004010915756225586\n",
      "991 // size: 119 // secs to run: 0.0035240650177001953\n",
      "992 // size: 138 // secs to run: 0.004122018814086914\n",
      "993 // size: 0 // secs to run: 2.288818359375e-05\n",
      "994 // size: 104 // secs to run: 0.003341197967529297\n",
      "995 // size: 144 // secs to run: 0.004391193389892578\n",
      "996 // size: 102 // secs to run: 0.0032808780670166016\n",
      "997 // size: 118 // secs to run: 0.0034389495849609375\n",
      "998 // size: 124 // secs to run: 0.004003047943115234\n",
      "999 // size: 116 // secs to run: 0.0037169456481933594\n",
      "1000 // size: 84 // secs to run: 0.0024688243865966797\n",
      "1001 // size: 127 // secs to run: 0.0040628910064697266\n",
      "1002 // size: 117 // secs to run: 0.0039310455322265625\n",
      "1003 // size: 115 // secs to run: 0.004288911819458008\n",
      "1004 // size: 253 // secs to run: 0.009285926818847656\n",
      "1005 // size: 133 // secs to run: 0.0041561126708984375\n",
      "1006 // size: 161 // secs to run: 0.00490117073059082\n",
      "1007 // size: 131 // secs to run: 0.004046201705932617\n",
      "1008 // size: 223 // secs to run: 0.00799107551574707\n"
     ]
    }
   ],
   "source": [
    "# custom truncate: if input is too large, get only the most important sentences\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    input = tokenizer.tokenize(data['model_input'][i])\n",
    "#     print(input)\n",
    "    if len(input) > max_input_size:\n",
    "        data['model_input'][i] = most_important_sentences_vs_doc(input)\n",
    "    \n",
    "    # keep track of time to run \n",
    "    if i % 1 == 0:\n",
    "        print(i, \"// size:\", len(input), \"// secs to run:\", time.time() - start)\n",
    "        start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector offrequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM. There are at least two kinds of similarity. When two words have a high degree of attributional similarity, we call them synonyms. When two word pairs have a high degree of relational similarity, we say they are analogous. Verbal analogies are often written in the form A:B::C:D, meaning A is to B as C is to D; for example, traffic:street::water:riverbed. Traffic flows over a street; water flows over a riverbed. A street carries traffic; a riverbed carries water. There is a high degree of relational similarity between the word pair traffic:street and the word pair water:riverbed. In analogies such as mason:stone::carpenter:wood, it seems that relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. This article builds on the Vector Space Model (VSM) of information retrieval. The documents are ranked in order of decreasing attributional similarity between the query and each document. The algorithm learns from a large corpus of unlabeled, unstructured text, without supervision. (2) Singular Value Decomposition (SVD) is used to smooth the frequency data. (3) Given a word pair such as traffic:street, LRA considers transformations of the word pair, generated by replacing one of the words by synonyms, such as traffic:road or traffic:highway. In the educational testing literature, the first pair (mason:stone) is called the stem of the analogy. The correct choice is called the solution and the incorrect choices are distractors. We evaluate LRA by testing its ability to select the solution and avoid the distractors. LRA achieves an accuracy of about 56%. On these same questions, the VSM attained 47%. The problem is to classify a noun-modifier pair, such as “laser printer,” according to the semantic relation between the head noun (printer) and the modifier (laser). We approach the task of classifying semantic relations in noun-modifier pairs as a supervised learning problem. The 600 pairs are divided into training and testing sets and a testing pair is classified according to the label of its single nearest neighbor in the training set. LRA is used to measure distance (ie, similarity, nearness). LRA achieves an accuracy of 39. % on the 30-class problem and 58.0% on the 5-class problem. On the same 600 noun-modifier pairs, the VSM had accuracies of 27. Attributes are used to state properties of objects; relations express relations between objects or propositions. For example, large can be viewed as an attribute of X, LARGE(X), or a relation between X and some standard Y, LARGER THAN(X, Y). The amount of attributional similarity between two words, A and B, depends on the degree of correspondence between the properties of A and B. For example, dog and wolf have a relatively high degree of attributional similarity. For example, dog:bark and cat:meow have a relatively high degree of relational similarity. Both of these are types of attributional similarity, since they are based on correspondence between attributes (eg, bees and honey are both found in hives; deer and ponies are both mammals). It’s important to note that semantic relatedness is a more general concept than similarity; similar entities are usually assumed to be related by virtue of their likeness (bank–trust company), but dissimilar entities may also be semantically related by lexical relationships such as meronymy (car–wheel) and antonymy (hot–cold), or just by any kind of functional relationship or frequent association (pencil–paper, penguin–Antarctica). As these examples show, semantic relatedness is the same as attributional similarity (eg, hot and cold are both kinds of temperature, pencil and paper are both used for writing). Here we prefer to use the term attributional similarity because it emphasizes the contrast with relational similarity. The term semantic relatedness may lead to confusion when the term relational similarity is also under discussion. Thus semantic similarity is a specific type of attributional similarity. The term semantic similarity is misleading, because it refers to a type of attributional similarity, yet relational similarity is not any less semantic than attributional similarity. Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). The results support the claim that lexicon-based algorithms have no advantage over corpus-based algorithms for recognizing synonymy. Stem: Levied To evaluate this approach, we applied several measures of attributional similarity to our collection of 374 SAT questions. The performance of the algorithms was measured by precision, recall, and F, defined as follows: Note that recall is the same as percent correct (for multiple-choice questions, with only zero or one guesses allowed per question, but not in general). When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped. We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not enough near analogies for attributional similarity to perform as well as relational similarity. The problem of recognizing word analogies is, given a stem word pair and a finite list of choice word pairs, selecting the choice that is most analogous to the stem. Argus could only solve the limited set of analogy questions that its programmer had anticipated. The goal of computational modeling of analogy making is to understand how people form complex, structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (expressing relations between relations). The analogical mapping connects source domain relations to target domain relations. The basic objects in the target model are the electrons and the nucleus. The planets and the sun have various attributes, such as mass(sun) and mass(planet), and various relations, such as revolve(planet, sun) and attracts(sun, planet). Likewise, the nucleus and the electrons have attributes, such as charge(electron) and charge(nucleus), and relations, such as revolve(electron, nucleus) and attracts(nucleus, electron). SME maps revolve(planet, sun) to revolve(electron, nucleus) and attracts(sun, planet) to attracts(nucleus, electron). Each individual connection (eg, from revolve(planet, sun) to revolve(electron, nucleus)) in an analogical mapping implies that the connected relations are similar; thus, SMT requires a measure of relational similarity in order to form maps. However, the focus of research in analogy making has been on the mapping process as a whole, rather than measuring the similarity between any two particular relations; hence, the similarity measures used in SME at the level of individual connections are somewhat rudimentary. We believe that a more sophisticated measure of relational similarity, such as LRA, may enhance the performance of SME. Likewise, the focus of our work here is on the similarity between particular relations, and we ignore systematic mapping between sets of relations, so LRA may also be enhanced by integration with SME. The task of classifying semantic relations is to identify the relation between a pair of words. Often the pairs are restricted to noun-modifier pairs, but there are many interesting relations, such as antonymy, that do not occur in noun-modifier pairs. However, noun-modifier pairs are interesting due to their high frequency in English. For instance, WordNet 2.0 contains more than 26,000 noun-modifier pairs, although many common noun-modifiers are not in WordNet, especially technical terms. They trained a neural network to distinguish 13 classes of semantic relations. None of these approaches explicitly involved measuring relational similarity, but any classification of semantic relations necessarily employs some implicit notion of relational similarity since members of the same class must be relationally similar to some extent. The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. For example, reptile haven was paraphrased as haven for reptiles. We believe that the intended sense of a polysemous word is determined by its semantic relations with the other words in the surrounding text. If we can identify the semantic relations between the given word and its context, then we can disambiguate the given word. For example, consider the word plant. Out of context, plant could refer to an industrial plant or a living organism. In this case, the decision may not be clear, since industrial plants often produce food and living organisms often serve as food. It would be very helpful to know the relation between food and plant in this example. In the phrase “food for the plant,” the relation between food and plant strongly suggests that the plant is a living organism, since industrial plants do not need food. In the text “food at the plant,” the relation strongly suggests that the plant is an industrial plant, since living organisms are not usually considered as locations. The problem of relation extraction is, given an input document and a specific relation R, to extract all pairs of entities (if any) that have the relation R in the document. ), except that information extraction focuses on the relation between a specific pair of entities in a specific document, rather than a general pair of words in general text. Therefore an algorithm for classifying semantic relations should be useful for information extraction. Each example would be represented by a vector of pattern frequencies. Given a specific document discussing John Smith and Hardcom Corporation, we could construct a vector representing the relation between these two entities and then measure the relational similarity between this unlabeled vector and each of our labeled training vectors. It would seem that there is a problem here because the training vectors would be relatively dense, since they would presumably be derived from a large corpus, but the new unlabeled vector for John Smith and Hardcom Corporation would be very sparse, since these entities might be mentioned only once in the given document. However, this is not a new problem for the VSM; it is the standard situation when the VSM is used for information retrieval. A query to a search engine is represented by a very sparse vector, whereas a document is represented by a relatively dense vector. Instead of manually generating new rules or patterns for each new semantic relation, it is possible to automatically learn a measure of relational similarity that can handle arbitrary semantic relations. A nearest neighbor algorithm can then use this relational similarity measure to learn to classify according to any set of classes of relations, given the appropriate labeled training data. However, they supplement their manual rules with automatically learned constraints, to increase the precision of the rules. He proposes to use the algorithm for analogical information retrieval. For example, the query Muslim church should return mosque and the query Hindu bible should return the Vedas. A measure of relational similarity is applicable to this task. (The pair Muslim:mosque has a high relational similarity to the pair Christian:church.) Each cluster of words in one corpus is coupled one-to-one with a cluster in the other corpus. For example, one experiment used a corpus of Buddhist documents and a corpus of Christian documents. A cluster of words such as {Hindu, Mahayana, Zen, ...} from the Buddhist corpus was coupled with a cluster of words such as {Catholic, Protestant, ...} from the Christian corpus. Thus the algorithm appears to have discovered an analogical mapping between Buddhist schools and traditions and Christian schools and traditions. This is interesting work, but it is not directly applicable to SAT analogies, because it discovers analogies between clusters of words rather than individual words. The task of identifying semantic roles is to label the parts of a sentence according to their semantic roles. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors. For a given query, the search engine sorts the matching documents in order of decreasing cosine. Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning. The relations R1 and R2 are not given to us; our task is to infer these hidden (latent) relations and then compare them. These phrases are then used as queries for a search engine and the number of hits (matching documents) is recorded for each query. This process yields a vector of 128 numbers. If the number of hits for a query is x, then the corresponding element in the vector r is log(x + 1). Since there are five choices for each question, the expected score for random guessing is 20%. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Secti on 6. A testing pair is classified by searching for its single nearest neighbor in the labeled training data. Thus their corpus was the set of all Web pages indexed by AltaVista. AltaVista also changed their policy toward automated searching, which is now forbidden. The WMTS is a distributed (multiprocessor) search engine, designed primarily for passage retrieval (although document retrieval is possible, as a special case of passage retrieval). The text and index require approximately one terabyte of disk space. Although AltaVista only gives a rough estimate of the number of matching documents, the WMTS gives exact counts of the number of matching passages. The performance of LRA significantly surpasses this combined system, but there is no real contest between these approaches, because we can simply add LRA to the combination, as a fourteenth module. LRA takes as input a set of word pairs and produces as output a measure of the relational similarity between any two of the input pairs. LRA relies on three resources, a search engine with a very large corpus of text, a broad-coverage thesaurus of synonyms, and an efficient implementation of SVD. We first present a short description of the core algorithm. The motivation for the alternate pairs is to handle cases where the original pairs cooccur rarely in the corpus. Before applying SVD, the vectors are completely non-negative, which implies that the cosine can only range from 0 to +1, but SVD introduces negative values, so it is possible for the cosine to be negative, although we have never observed this in our experiments. The Web pages cover a very wide range of topics, styles, genres, quality, and writing skill. The WMTS is well suited to LRA, because the WMTS scales well to large corpora (one terabyte, in our case), it gives exact frequency counts (unlike most Web search engines), it is designed for passage retrieval (rather than document retrieval), and it has a powerful query syntax. This thesaurus is available through an on-line interactive demonstration or it can be downloaded. We used the on-line demonstration, since the downloadable version seems to contain fewer words. For each word in the input set of word pairs, we automatically query the on-line demonstration and fetch the resulting list of synonyms. Words were then clustered into synonym sets, based on the similarity of their grammatical relations. Two words were judged to be highly similar when they tended to have the same kinds of grammatical relations with the same sets of words. Given a word and its part of speech, Lin’s thesaurus provides a list of words, sorted in order of decreasing attributional similarity. WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses. In LRA, SVD is used to reduce noise and compensate for sparseness. Sort the alternate pairs by the frequency of their phrases. This step tends to eliminate alternates that have no clear semantic relation. Alternate forms of the original pair quart:volume. The first column shows the original pair and the alternate pairs. The second column shows Lin’s similarity score for the alternate word compared to the original word. For example, the similarity between quart and pint is 0. The third column shows the frequency of the pair in the WMTS corpus. The phrases cannot have more than max phrase words and there must be at least one word between the two members of the word pair. These phrases give us information about the semantic relations between the words in each pair. A phrase with no words between the two members of the word pair would give us very little information about the semantic relations (other than that the words occur together with a certain frequency in a certain order). Find patterns: For each phrase found in the previous step, build patterns from the intervening words. At least one word must occur between quart and volume. At most max phrase words can appear in a phrase. quarts liquid volume volume in quarts quarts of volume volume capacity quarts quarts in volume volume being about two quarts quart total volume volume of milk in quarts quart of spray volume volume include measures like quart replace only one word). If a phrase is n words long, there are n − 2 intervening words between the members of the given word pair (eg, between quart and volume). Thus a phrase with n words generates 2(n−2) patterns. For each pattern, count the number of pairs (originals and alternates) with phrases that match the pattern (a wild card must match exactly one word). Typically there will be millions of patterns, so it is not feasible to keep them all. more weight to columns (patterns) with frequencies that vary substantially from one row (word pair) to the next, and less weight to columns that are uniform. We also apply the log transformation to frequencies, log(xi,j + 1). (Entropy is calculated with the original frequency values, before the log transformation is applied.) approximates the original matrix X, in the sense that it minimizes the � � approximation errors. We may think of this matrix UkEkVTk as a “smoothed” or “compressed” version of the original matrix. In the subsequent steps, we will be calculating cosines for row vectors. For this purpose, we can simplify calculations by dropping V. The cosine of two vectors is their dot product, after they have been normalized to unit length. The matrix XXT contains the dot products of all of the row vectors. We can find the dot product of the ith and jth row vectors by looking at the cell in row i, column j of the matrix XXT. This matrix has the same number of rows as X, but only k columns (instead of 2 x num patterns columns; in our experiments, that is 300 columns instead of 8,000). We can compare two word pairs by calculating the cosine of the corresponding row vectors in UkEk. The row vector for each word pair has been projected from the original 8,000 dimensional space into a new 300 dimensional space. Therefore we have (num filter + 1)2 ways to compare a version of A:B with a version of C:D. Look for the row vectors in UkEk that correspond to the versions of A:B and the versions of C:D and calculate the (num filter + 1)2 cosines (in our experiments, there are 16 cosines). For example, suppose A:B is quart:volume and C:D is mile:distance. Calculate relational similarity: The relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. The requirement that the cosine must be greater than or equal to the original cosine is a way of filtering out poor analogies, which may be introduced in step 1 and may have slipped through the filtering in step 2. Steps 11 and 12 can be repeated for each two input pairs that are to be compared. This completes the description of LRA. The choice pair with the highest average cosine (the choice with the largest value in column 1), choice (b), is the solution for this question; LRA answers the question correctly. As another point of reference, consider the simple strategy of always guessing the choice with the highest co-occurrence frequency. The idea here is that the words in the solution pair may occur together frequently, because there is presumably a clear and meaningful relation between the solution words, whereas the distractors may only occur together rarely because they have no meaningful relation. This strategy is signifcantly worse than random guessing. The opposite strategy, always guessing the choice pair with the lowest co-occurrence frequency, is also worse than random guessing (but not significantly). It appears that the designers of the SAT questions deliberately chose distractors that would thwart these two strategies. With 374 questions and six word pairs per question (one stem and five choices), there are 2,244 pairs in the input set. In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 8,976 pairs. In step 5, for each pair A:B, we add B:A, yielding 17,952 pairs. However, some pairs are dropped because they correspond to zero vectors (they do not appear together in a window of five words in the WMTS corpus). Also, a few words do not appear in Lin’s thesaurus, and some word pairs appear twice in the SAT questions (eg, lion:cat). All of the steps used a single CPU on a desktop computer, except step 3, finding the phrases for each word pair, which used a 16 CPU Beowulf cluster. Most of the other steps are parallelizable; with a bit of programming effort, they could also be executed on the Beowulf cluster. All CPUs (both desktop and cluster) were 2. The desktop computer had 2 GB of RAM and the cluster had a total of 16 GB of RAM. We generated the VSM-WMTS results by adapting the VSM to the WMTS. The pairwise differences in precision between LRA and the two VSM variations are also significant, but the difference in precision between the two VSM variations (42. %) is not significant. Although VSM-AV has a corpus 10 times larger than LRA’s, LRA still performs better than VSM-AV. Comparing VSM-AV to VSM-WMTS, the smaller corpus has reduced the score of the VSM, but much of the drop is due to the larger number of questions that were skipped (34 for VSM-WMTS versus 5 for VSM-AV). With the smaller corpus, many more of the input word pairs simply do not appear together in short phrases in the corpus. LRA is able to answer as many questions as VSM-AV, although it uses the same corpus as VSM-WMTS, because Lin’s thesaurus allows LRA to substitute synonyms for words that are not in the corpus. The SAT I test consists of 78 verbal questions and 60 math questions (there is also an SAT II test, covering specific subjects, such as chemistry). Analogy questions are only a subset of the 78 verbal SAT questions. If we assume that the difficulty of our 374 analogy questions is comparable to the difficulty of the 78 verbal SAT I questions, then we can estimate that the average college-bound senior would correctly answer about 57% of the 374 analogy questions. On this subset of the questions, LRA has a recall of 61. %, compared to a recall of 51. % on the other 184 questions. This indicates that we may be underestimating how well LRA performs, relative to college-bound senior high school students. The parameter values were determined by trying a small number of possible values on a small set of questions that were set aside. Since LRA is intended to be an unsupervised learning algorithm, we did not attempt to tune the parameter values to maximize the precision and recall on the 374 SAT questions. We hypothesized that LRA is relatively insensitive to the values of the parameters. and vary each parameter, one at a time, while holding the remaining parameters fixed at their baseline values. This supports the hypothesis that the algorithm is not sensitive to the parameter values. Although a full run of LRA on the 374 SAT questions takes 9 days, for some of the parameters it is possible to reuse cached data from previous runs. We limited the experiments with num sim and max phrase because caching was not as helpful for these parameters, so experimenting with them required several weeks. However, we hypothesize that the drop in performance would be significant with a larger set of word pairs. More word pairs would increase the sample size, which would decrease the 95% confidence interval, which would likely show that SVD is making a significant contribution. Furthermore, more word pairs would increase the matrix size, which would give SVD more leverage. We are currently gathering more SAT questions to test this hypothesis. %), but the drop in precision is not significant. When the synonym component is dropped, the number of skipped questions rises from 4 to 22, which demonstrates the value of the synonym component of LRA for compensating for sparse data. Again, we believe that a larger sample size would show that the drop in precision is significant. If we eliminate both synonyms and SVD from LRA, all that distinguishes LRA from VSM-WMTS is the patterns (step 4). We can see the value of the automatically generated patterns by comparing LRA without synonyms and SVD (column 4) to VSM-WMTS (column 5). The ablation experiments support the value of the patterns (step 4) and synonyms (step 1) in LRA, but the contribution of SVD (step 9) has not been proven, although we believe more data will support its effectiveness. Nonetheless, the three components together result in a 16% increase in F (compare column 1 to 5). We know a priori that, if A:B::C:D, then B:A::D:C. For example, mason is to stone as carpenter is to wood implies stone is to mason as wood is to carpenter. The matrix is designed so that the row vector for A:B is different from the row vector for B:A only by a permutation of the elements. To discover the consequences of this design decision, we altered steps 5 and 6 so that symmetry is no longer preserved. In step 5, for each word pair A:B that appears in the input set, we only have one row. Thus the number of rows in the matrix dropped from 17,232 to 8,616. In step 6, we no longer have two columns for each pattern P, one for “word1 P word2” and another for “word2 P word1.” However, to be fair, we kept the total number of columns at 8,000. In step 4, we selected the top 8,000 patterns (instead of the top 4,000), distinguishing the pattern “word1 P word2” from the pattern “word2 P word1” (instead of considering them equivalent). Thus a pattern P with a high frequency is likely to appear in two columns, in both possible orders, but a lower frequency pattern might appear in only one column, in only one possible order. These changes resulted in a slight decrease in performance. % to 55.% and precision dropped from 56. The decrease is not statistically significant. In step 12 of LRA, the relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. That is, the average includes only those alternates that are “better” than the originals. Taking all alternates instead of the better alternates, recall drops from 56. % and precision drops from 56. The idea here is that we will only pay attention to the N most important patterns in r; the remaining patterns will be ignored. If most of the semantic content is in the N largest elements of r, then setting the remaining elements to zero should have relatively little impact. The precision and recall are significantly below the baseline LRA until N ≥ 300 (95% confidence, Fisher Exact Test). In other words, for a typical SAT analogy question, we need to examine the top 300 patterns to explain why LRA selected one choice instead of another. We are currently working on an extension of LRA that will explain with a single pattern why one choice is better than another. If we require an exact match, 50 of the 64 manual patterns can be found in the automatic patterns. If we are lenient about wildcards, and count the pattern not the as matching * not the (for example), then 60 of the 64 manual patterns appear within the automatic patterns. This suggests that the improvement in performance with the automatic patterns is due to the increased quantity of patterns, rather than a qualitative difference in the patterns. Both of these patterns are included in the 4,000 patterns automatically generated by LRA. No adjustments were made to tune LRA to the noun-modifier pairs. LRA is used as a distance (nearness) measure in a single nearest neighbor supervised learning algorithm. We make use of this grouping in the following experiments. The following experiments use single nearest neighbor classification with leave-one-out cross-validation. For leave-one-out cross-validation, the testing set consists of a single noun-modifier pair and the training set consists of the 599 remaining noun-modifiers. The data set is split 600 times, so that each noun-modifier gets a turn as the testing word pair. The predicted class of the testing pair is the class of the single nearest neighbor in the training set. As the measure of nearness, we use LRA to calculate the relational similarity between the testing pair and the training pairs. The factor of 16 comes from the alternate pairs, step 11 in LRA. Macroaveraging calculates the precision, recall, and F for each class separately, and then calculates the average across all classes. Macroaveraging gives equal weight to all classes, but microaveraging gives more weight to larger classes. We use macroaveraging (giving equal weight to all classes), because we have no reason to believe that the class sizes in the data set reflect the actual distribution of the classes in a real corpus. Classification with 30 distinct classes is a hard problem. For example, agent and beneficiary both collapse to participant. On the 30 class problem, LRA with the single nearest neighbor algorithm achieves an accuracy of 39. Always guessing the majority class would result in an accuracy of 8. On the 5 class problem, the accuracy is 58.0% (348/600) and the macroaveraged F is 54. Always guessing the majority class would give an accuracy of 43.% (260/600). Another limitation is speed; it took almost 9 days for LRA to answer 374 analogy questions. However, with progress in computer hardware, speed will gradually become less of a concern. It may also be possible to precompute much of the information for LRA, although this would require substantial changes to the algorithm. The difference in performance between VSM-AV and VSM-WMTS shows that VSM is sensitive to the size of the corpus. Although LRA is able to surpass VSM-AV when the WMTS corpus is only about one tenth the size of the AV corpus, it seems likely that LRA would perform better with a larger corpus. The WMTS corpus requires one terabyte of hard disk space, but progress in hardware will likely make 10 or even 100 terabytes affordable in the relatively near future. For noun-modifier classification, more labeled data should yield performance improvements. With 600 noun-modifier pairs and 30 classes, the average class has only 20 examples. We expect that the accuracy would improve substantially with 5 or 10 times more examples. Another issue with noun-modifier classification is the choice of classification scheme for the semantic relations. It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. SVD is only one of many methods for handling sparse, noisy data. In step 4 of LRA, we simply select the top num patterns most frequent patterns and discard the remaining patterns. Perhaps a more sophisticated selection algorithm would improve the performance of LRA. We have tried a variety of ways of selecting patterns, but it seems that the method of selection has little impact on performance. This article has introduced a new method for calculating relational similarity, Latent Relational Analysis. The experiments demonstrate that LRA performs better than the VSM approach, when evaluated with SAT word analogy questions and with the task of classifying noun-modifier expressions. The VSM approach represents the relation between a pair of words with a vector, in which the elements are based on the frequencies of 64 hand-built patterns in a large corpus. LRA extends this approach in three ways: (1) The patterns are generated dynamically from the corpus, (2) SVD is used to smooth the data, and (3) a thesaurus is used to explore variations of the word pairs. We have presented several examples of the many potential applications for measures of relational similarity. Just as attributional similarity measures have proven to have many practical uses, we expect that relational similarity measures will soon become widely used. We believe that relational similarity plays a fundamental role in the mind and therefore relational similarity measures could be crucial for artificial intelligence. In future work, we plan to investigate some potential applications for LRA. It is possible that the error rate of LRA is still too high for practical applications, but the fact that LRA matches average human performance on SAT analogy questions is encouraging. Thanks to Michael Littman for sharing the 374 SAT analogy questions and for inspiring me to tackle them. Thanks to Dekang Lin for making his Dependency-Based Word Similarity lexicon available online. Thanks to Ted Pedersen for making his WordNet::Similarity package available. Thanks to Joel Martin for comments on the article. Thanks to the anonymous reviewers of Computational Linguistics for their very helpful comments and suggestions.\n",
      "---------------------------------------------\n",
      "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector offrequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM. There are at least two kinds of similarity. When two words have a high degree of attributional similarity, we call them synonyms. When two word pairs have a high degree of relational similarity, we say they are analogous. Verbal analogies are often written in the form A:B::C:D, meaning A is to B as C is to D; for example, traffic:street::water:riverbed. Traffic flows over a street; water flows over a riverbed. A street carries traffic; a riverbed carries water. There is a high degree of relational similarity between the word pair traffic:street and the word pair water:riverbed. In analogies such as mason:stone::carpenter:wood, it seems that relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. This article builds on the Vector Space Model (VSM) of information retrieval. The documents are ranked in order of decreasing attributional similarity between the query and each document. They used a vector of frequencies of patterns in a corpus to represent the relation between a pair of words. The algorithm learns from a large corpus of unlabeled, unstructured text, without supervision. (2) Singular Value Decomposition (SVD) is used to smooth the frequency data. (3) Given a word pair such as traffic:street, LRA considers transformations of the word pair, generated by replacing one of the words by synonyms, such as traffic:road or traffic:highway. In the educational testing literature, the first pair (mason:stone) is called the stem of the analogy. The correct choice is called the solution and the incorrect choices are distractors. We evaluate LRA by testing its ability to select the solution and avoid the distractors. The average performance of collegebound senior high school students on verbal SAT questions corresponds to an accuracy of about 57%. LRA achieves an accuracy of about 56%. On these same questions, the VSM attained 47%. The problem is to classify a noun-modifier pair, such as “laser printer,” according to the semantic relation between the head noun (printer) and the modifier (laser). The 600 pairs have been manually labeled with 30 classes of semantic relations. For example, “laser printer” is classified as instrument; the printer uses the laser as an instrument for printing. We approach the task of classifying semantic relations in noun-modifier pairs as a supervised learning problem. The 600 pairs are divided into training and testing sets and a testing pair is classified according to the label of its single nearest neighbor in the training set. LRA is used to measure distance (ie, similarity, nearness). LRA achieves an accuracy of 39. % on the 30-class problem and 58.0% on the 5-class problem. On the same 600 noun-modifier pairs, the VSM had accuracies of 27. Attributes are used to state properties of objects; relations express relations between objects or propositions. For example, large can be viewed as an attribute of X, LARGE(X), or a relation between X and some standard Y, LARGER THAN(X, Y). The amount of attributional similarity between two words, A and B, depends on the degree of correspondence between the properties of A and B. For example, dog and wolf have a relatively high degree of attributional similarity. For example, dog:bark and cat:meow have a relatively high degree of relational similarity. Both of these are types of attributional similarity, since they are based on correspondence between attributes (eg, bees and honey are both found in hives; deer and ponies are both mammals). It’s important to note that semantic relatedness is a more general concept than similarity; similar entities are usually assumed to be related by virtue of their likeness (bank–trust company), but dissimilar entities may also be semantically related by lexical relationships such as meronymy (car–wheel) and antonymy (hot–cold), or just by any kind of functional relationship or frequent association (pencil–paper, penguin–Antarctica). As these examples show, semantic relatedness is the same as attributional similarity (eg, hot and cold are both kinds of temperature, pencil and paper are both used for writing). Here we prefer to use the term attributional similarity because it emphasizes the contrast with relational similarity. The term semantic relatedness may lead to confusion when the term relational similarity is also under discussion. Thus semantic similarity is a specific type of attributional similarity. The term semantic similarity is misleading, because it refers to a type of attributional similarity, yet relational similarity is not any less semantic than attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). The results support the claim that lexicon-based algorithms have no advantage over corpus-based algorithms for recognizing synonymy. It seems possible that SAT analogy questions might consist largely of near analogies, in which case they can be solved using attributional similarity measures. An example of a typical TOEFL question, from the collection of 80 questions. Stem: Levied To evaluate this approach, we applied several measures of attributional similarity to our collection of 374 SAT questions. The performance of the algorithms was measured by precision, recall, and F, defined as follows: Note that recall is the same as percent correct (for multiple-choice questions, with only zero or one guesses allowed per question, but not in general). When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped. We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not enough near analogies for attributional similarity to perform as well as relational similarity. The problem of recognizing word analogies is, given a stem word pair and a finite list of choice word pairs, selecting the choice that is most analogous to the stem. Argus could only solve the limited set of analogy questions that its programmer had anticipated. Argus was based on a spreading activation model and did not explicitly attempt to measure relational similarity. The final output of the system was based on a weighted combination of the outputs of each individual module. The VSM was evaluated on a set of 374 SAT questions, achieving a score of 47%. The goal of computational modeling of analogy making is to understand how people form complex, structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (expressing relations between relations). The analogical mapping connects source domain relations to target domain relations. The solar system is the source domain and Rutherford’s model of the atom is the target domain. The basic objects in the source model are the planets and the sun. The basic objects in the target model are the electrons and the nucleus. The planets and the sun have various attributes, such as mass(sun) and mass(planet), and various relations, such as revolve(planet, sun) and attracts(sun, planet). Likewise, the nucleus and the electrons have attributes, such as charge(electron) and charge(nucleus), and relations, such as revolve(electron, nucleus) and attracts(nucleus, electron). SME maps revolve(planet, sun) to revolve(electron, nucleus) and attracts(sun, planet) to attracts(nucleus, electron). Each individual connection (eg, from revolve(planet, sun) to revolve(electron, nucleus)) in an analogical mapping implies that the connected relations are similar; thus, SMT requires a measure of relational similarity in order to form maps. However, the focus of research in analogy making has been on the mapping process as a whole, rather than measuring the similarity between any two particular relations; hence, the similarity measures used in SME at the level of individual connections are somewhat rudimentary. We believe that a more sophisticated measure of relational similarity, such as LRA, may enhance the performance of SME. Likewise, the focus of our work here is on the similarity between particular relations, and we ignore systematic mapping between sets of relations, so LRA may also be enhanced by integration with SME. The task of classifying semantic relations is to identify the relation between a pair of words. Often the pairs are restricted to noun-modifier pairs, but there are many interesting relations, such as antonymy, that do not occur in noun-modifier pairs. However, noun-modifier pairs are interesting due to their high frequency in English. For instance, WordNet 2.0 contains more than 26,000 noun-modifier pairs, although many common noun-modifiers are not in WordNet, especially technical terms. They trained a neural network to distinguish 13 classes of semantic relations. None of these approaches explicitly involved measuring relational similarity, but any classification of semantic relations necessarily employs some implicit notion of relational similarity since members of the same class must be relationally similar to some extent. The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. For example, reptile haven was paraphrased as haven for reptiles. We believe that the intended sense of a polysemous word is determined by its semantic relations with the other words in the surrounding text. If we can identify the semantic relations between the given word and its context, then we can disambiguate the given word. For example, consider the word plant. Out of context, plant could refer to an industrial plant or a living organism. Suppose plant appears in some text near food. In this case, the decision may not be clear, since industrial plants often produce food and living organisms often serve as food. It would be very helpful to know the relation between food and plant in this example. In the phrase “food for the plant,” the relation between food and plant strongly suggests that the plant is a living organism, since industrial plants do not need food. In the text “food at the plant,” the relation strongly suggests that the plant is an industrial plant, since living organisms are not usually considered as locations. The problem of relation extraction is, given an input document and a specific relation R, to extract all pairs of entities (if any) that have the relation R in the document. ), except that information extraction focuses on the relation between a specific pair of entities in a specific document, rather than a general pair of words in general text. Therefore an algorithm for classifying semantic relations should be useful for information extraction. Each example would be represented by a vector of pattern frequencies. Given a specific document discussing John Smith and Hardcom Corporation, we could construct a vector representing the relation between these two entities and then measure the relational similarity between this unlabeled vector and each of our labeled training vectors. It would seem that there is a problem here because the training vectors would be relatively dense, since they would presumably be derived from a large corpus, but the new unlabeled vector for John Smith and Hardcom Corporation would be very sparse, since these entities might be mentioned only once in the given document. However, this is not a new problem for the VSM; it is the standard situation when the VSM is used for information retrieval. A query to a search engine is represented by a very sparse vector, whereas a document is represented by a relatively dense vector. As defined in the Text Retrieval Conference (TREC) QA track, the task is to answer simple questions, such as “Where have nuclear incidents occurred”, by retrieving a relevant document from a large corpus and then extracting a short string from the document, such as The Three Mile Island nuclear incident caused a DOE policy crisis. They argue that the desired semantic relation can easily be inferred from the surface form of the question. A question of the form “Where... ” is likely to be looking for entities with a location relation and a question of the form “What did ... make” is likely to be looking for entities with a product relation. These algorithms could be used to automatically generate a thesaurus or dictionary, but we would like to handle more relations than hyponymy and meronymy. LRA does not use a predefined set of patterns; it learns patterns from a large corpus. Instead of manually generating new rules or patterns for each new semantic relation, it is possible to automatically learn a measure of relational similarity that can handle arbitrary semantic relations. A nearest neighbor algorithm can then use this relational similarity measure to learn to classify according to any set of classes of relations, given the appropriate labeled training data. However, they supplement their manual rules with automatically learned constraints, to increase the precision of the rules. He proposes to use the algorithm for analogical information retrieval. For example, the query Muslim church should return mosque and the query Hindu bible should return the Vedas. The algorithm was designed with a focus on analogies of the form adjective:noun::adjective:noun, such as Christian:church::Muslim:mosque. A measure of relational similarity is applicable to this task. (The pair Muslim:mosque has a high relational similarity to the pair Christian:church.) Each cluster of words in one corpus is coupled one-to-one with a cluster in the other corpus. For example, one experiment used a corpus of Buddhist documents and a corpus of Christian documents. A cluster of words such as {Hindu, Mahayana, Zen, ...} from the Buddhist corpus was coupled with a cluster of words such as {Catholic, Protestant, ...} from the Christian corpus. Thus the algorithm appears to have discovered an analogical mapping between Buddhist schools and traditions and Christian schools and traditions. This is interesting work, but it is not directly applicable to SAT analogies, because it discovers analogies between clusters of words rather than individual words. The task of identifying semantic roles is to label the parts of a sentence according to their semantic roles. We believe that it may be helpful to view semantic frames and their semantic roles as sets of semantic relations; thus, a measure of relational similarity should help us to identify semantic roles. ), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech. In the VSM approach to information retrieval, queries and documents are represented by vectors. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors. For a given query, the search engine sorts the matching documents in order of decreasing cosine. Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning. Let R1 be the semantic relation (or set of relations) between a pair of words, A and B, and let R2 be the semantic relation (or set of relations) between another pair, C and D. We wish to measure the relational similarity between R1 and R2. The relations R1 and R2 are not given to us; our task is to infer these hidden (latent) relations and then compare them. These phrases are then used as queries for a search engine and the number of hits (matching documents) is recorded for each query. This process yields a vector of 128 numbers. If the number of hits for a query is x, then the corresponding element in the vector r is log(x + 1). Since there are five choices for each question, the expected score for random guessing is 20%. To answer a multiple-choice analogy question, vectors are created for the stem pair and each choice pair, and then cosines are calculated for the angles between the stem pair and each choice pair. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Secti on 6. A testing pair is classified by searching for its single nearest neighbor in the labeled training data. The best guess is the label for the training pair with the highest cosine. Thus their corpus was the set of all Web pages indexed by AltaVista. AltaVista also changed their policy toward automated searching, which is now forbidden. The WMTS is a distributed (multiprocessor) search engine, designed primarily for passage retrieval (although document retrieval is possible, as a special case of passage retrieval). The text and index require approximately one terabyte of disk space. Although AltaVista only gives a rough estimate of the number of matching documents, the WMTS gives exact counts of the number of matching passages. The performance of LRA significantly surpasses this combined system, but there is no real contest between these approaches, because we can simply add LRA to the combination, as a fourteenth module. LRA takes as input a set of word pairs and produces as output a measure of the relational similarity between any two of the input pairs. LRA relies on three resources, a search engine with a very large corpus of text, a broad-coverage thesaurus of synonyms, and an efficient implementation of SVD. We first present a short description of the core algorithm. The motivation for the alternate pairs is to handle cases where the original pairs cooccur rarely in the corpus. The hope is that we can find near analogies for the original pairs, such that the near analogies co-occur more frequently in the corpus. The danger is that the alternates may have different relations from the originals. The filtering steps above aim to reduce this risk. In our experiments, the input set contains from 600 to 2,244 word pairs. Before applying SVD, the vectors are completely non-negative, which implies that the cosine can only range from 0 to +1, but SVD introduces negative values, so it is possible for the cosine to be negative, although we have never observed this in our experiments. The Web pages cover a very wide range of topics, styles, genres, quality, and writing skill. The WMTS is well suited to LRA, because the WMTS scales well to large corpora (one terabyte, in our case), it gives exact frequency counts (unlike most Web search engines), it is designed for passage retrieval (rather than document retrieval), and it has a powerful query syntax. This thesaurus is available through an on-line interactive demonstration or it can be downloaded. We used the on-line demonstration, since the downloadable version seems to contain fewer words. For each word in the input set of word pairs, we automatically query the on-line demonstration and fetch the resulting list of synonyms. As a courtesy to other users of Lin’s on-line system, we insert a 20-second delay between each two queries. The parser was used to extract pairs of words and their grammatical relations. Words were then clustered into synonym sets, based on the similarity of their grammatical relations. Two words were judged to be highly similar when they tended to have the same kinds of grammatical relations with the same sets of words. Given a word and its part of speech, Lin’s thesaurus provides a list of words, sorted in order of decreasing attributional similarity. This sorting is convenient for LRA, since it makes it possible to focus on words with higher attributional similarity and ignore the rest. WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses. In LRA, SVD is used to reduce noise and compensate for sparseness. We will go through each step of LRA, using an example to illustrate the steps. Since there are six word pairs per question (the stem and five choices), the input consists of 2,244 word pairs. The LRA algorithm consists of the following 12 steps: alternates as follows. For each alternate pair, send a query to the WMTS, to find the frequency of phrases that begin with one member of the pair and end with the other. Sort the alternate pairs by the frequency of their phrases. This step tends to eliminate alternates that have no clear semantic relation. Alternate forms of the original pair quart:volume. The first column shows the original pair and the alternate pairs. The second column shows Lin’s similarity score for the alternate word compared to the original word. For example, the similarity between quart and pint is 0. The third column shows the frequency of the pair in the WMTS corpus. The fourth column shows the pairs that pass the filtering step (ie, step 2). The phrases cannot have more than max phrase words and there must be at least one word between the two members of the word pair. These phrases give us information about the semantic relations between the words in each pair. A phrase with no words between the two members of the word pair would give us very little information about the semantic relations (other than that the words occur together with a certain frequency in a certain order). Find patterns: For each phrase found in the previous step, build patterns from the intervening words. A pattern is constructed by replacing any or all or none of the intervening words with wild cards (one wild card can Some examples of phrases that contain quart:volume. Suffixes are ignored when searching for matching phrases in the WMTS corpus. At least one word must occur between quart and volume. At most max phrase words can appear in a phrase. quarts liquid volume volume in quarts quarts of volume volume capacity quarts quarts in volume volume being about two quarts quart total volume volume of milk in quarts quart of spray volume volume include measures like quart replace only one word). If a phrase is n words long, there are n − 2 intervening words between the members of the given word pair (eg, between quart and volume). Thus a phrase with n words generates 2(n−2) patterns. For each pattern, count the number of pairs (originals and alternates) with phrases that match the pattern (a wild card must match exactly one word). Typically there will be millions of patterns, so it is not feasible to keep them all. more weight to columns (patterns) with frequencies that vary substantially from one row (word pair) to the next, and less weight to columns that are uniform. We also apply the log transformation to frequencies, log(xi,j + 1). (Entropy is calculated with the original frequency values, before the log transformation is applied.) For all i and all j, replace the original value xi,j in X by the new value wj log(xi,j + 1). approximates the original matrix X, in the sense that it minimizes the � � approximation errors. We may think of this matrix UkEkVTk as a “smoothed” or “compressed” version of the original matrix. In the subsequent steps, we will be calculating cosines for row vectors. For this purpose, we can simplify calculations by dropping V. The cosine of two vectors is their dot product, after they have been normalized to unit length. The matrix XXT contains the dot products of all of the row vectors. We can find the dot product of the ith and jth row vectors by looking at the cell in row i, column j of the matrix XXT. This matrix has the same number of rows as X, but only k columns (instead of 2 x num patterns columns; in our experiments, that is 300 columns instead of 8,000). We can compare two word pairs by calculating the cosine of the corresponding row vectors in UkEk. The row vector for each word pair has been projected from the original 8,000 dimensional space into a new 300 dimensional space. Therefore we have (num filter + 1)2 ways to compare a version of A:B with a version of C:D. Look for the row vectors in UkEk that correspond to the versions of A:B and the versions of C:D and calculate the (num filter + 1)2 cosines (in our experiments, there are 16 cosines). For example, suppose A:B is quart:volume and C:D is mile:distance. Calculate relational similarity: The relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. The requirement that the cosine must be greater than or equal to the original cosine is a way of filtering out poor analogies, which may be introduced in step 1 and may have slipped through the filtering in step 2. Averaging the cosines, as opposed to taking their maximum, is intended to provide some resistance to noise. For these two pairs, the average of the selected cosines is 0. Steps 11 and 12 can be repeated for each two input pairs that are to be compared. This completes the description of LRA. The choice pair with the highest average cosine (the choice with the largest value in column 1), choice (b), is the solution for this question; LRA answers the question correctly. For comparison, column 2 gives the cosines for the original pairs and column 3 gives the highest cosine. For this particular SAT question, there is one choice that has the highest cosine for all three columns, choice (b), although this is not true in general. Note that the gap between the first choice (b) and the second choice (d) is largest for the average cosines (column 1). This suggests that the average of the cosines (column 1) is better at discriminating the correct choice than either the original cosine (column 2) or the highest cosine (column 3). LRA correctly answered 210 of the 374 questions; 160 questions were answered incorrectly and 4 questions were skipped, because the stem pair and its alternates were represented by zero vectors. As another point of reference, consider the simple strategy of always guessing the choice with the highest co-occurrence frequency. The idea here is that the words in the solution pair may occur together frequently, because there is presumably a clear and meaningful relation between the solution words, whereas the distractors may only occur together rarely because they have no meaningful relation. This strategy is signifcantly worse than random guessing. The opposite strategy, always guessing the choice pair with the lowest co-occurrence frequency, is also worse than random guessing (but not significantly). It appears that the designers of the SAT questions deliberately chose distractors that would thwart these two strategies. With 374 questions and six word pairs per question (one stem and five choices), there are 2,244 pairs in the input set. In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 8,976 pairs. In step 5, for each pair A:B, we add B:A, yielding 17,952 pairs. However, some pairs are dropped because they correspond to zero vectors (they do not appear together in a window of five words in the WMTS corpus). Also, a few words do not appear in Lin’s thesaurus, and some word pairs appear twice in the SAT questions (eg, lion:cat). The sparse matrix (step 7) has 17,232 rows (word pairs) and 8,000 columns (patterns), with a density of 5. % (percentage of nonzero values). All of the steps used a single CPU on a desktop computer, except step 3, finding the phrases for each word pair, which used a 16 CPU Beowulf cluster. Most of the other steps are parallelizable; with a bit of programming effort, they could also be executed on the Beowulf cluster. All CPUs (both desktop and cluster) were 2. The desktop computer had 2 GB of RAM and the cluster had a total of 16 GB of RAM. We generated the VSM-WMTS results by adapting the VSM to the WMTS. The pairwise differences in precision between LRA and the two VSM variations are also significant, but the difference in precision between the two VSM variations (42. %) is not significant. Although VSM-AV has a corpus 10 times larger than LRA’s, LRA still performs better than VSM-AV. Comparing VSM-AV to VSM-WMTS, the smaller corpus has reduced the score of the VSM, but much of the drop is due to the larger number of questions that were skipped (34 for VSM-WMTS versus 5 for VSM-AV). With the smaller corpus, many more of the input word pairs simply do not appear together in short phrases in the corpus. LRA is able to answer as many questions as VSM-AV, although it uses the same corpus as VSM-WMTS, because Lin’s thesaurus allows LRA to substitute synonyms for words that are not in the corpus. Since the WMTS is running locally, there is no need for delays. VSM-WMTS processed the questions in only one day. The SAT I test consists of 78 verbal questions and 60 math questions (there is also an SAT II test, covering specific subjects, such as chemistry). Analogy questions are only a subset of the 78 verbal SAT questions. If we assume that the difficulty of our 374 analogy questions is comparable to the difficulty of the 78 verbal SAT I questions, then we can estimate that the average college-bound senior would correctly answer about 57% of the 374 analogy questions. On this subset of the questions, LRA has a recall of 61. %, compared to a recall of 51. % on the other 184 questions. This indicates that we may be underestimating how well LRA performs, relative to college-bound senior high school students. There is no significant difference between LRA and human performance, but VSM-AV and VSM-WMTS are significantly below human-level performance. The parameter values were determined by trying a small number of possible values on a small set of questions that were set aside. Since LRA is intended to be an unsupervised learning algorithm, we did not attempt to tune the parameter values to maximize the precision and recall on the 374 SAT questions. We hypothesized that LRA is relatively insensitive to the values of the parameters. and vary each parameter, one at a time, while holding the remaining parameters fixed at their baseline values. This supports the hypothesis that the algorithm is not sensitive to the parameter values. Although a full run of LRA on the 374 SAT questions takes 9 days, for some of the parameters it is possible to reuse cached data from previous runs. We limited the experiments with num sim and max phrase because caching was not as helpful for these parameters, so experimenting with them required several weeks. However, we hypothesize that the drop in performance would be significant with a larger set of word pairs. More word pairs would increase the sample size, which would decrease the 95% confidence interval, which would likely show that SVD is making a significant contribution. Furthermore, more word pairs would increase the matrix size, which would give SVD more leverage. We are currently gathering more SAT questions to test this hypothesis. %), but the drop in precision is not significant. When the synonym component is dropped, the number of skipped questions rises from 4 to 22, which demonstrates the value of the synonym component of LRA for compensating for sparse data. Again, we believe that a larger sample size would show that the drop in precision is significant. If we eliminate both synonyms and SVD from LRA, all that distinguishes LRA from VSM-WMTS is the patterns (step 4). We can see the value of the automatically generated patterns by comparing LRA without synonyms and SVD (column 4) to VSM-WMTS (column 5). The ablation experiments support the value of the patterns (step 4) and synonyms (step 1) in LRA, but the contribution of SVD (step 9) has not been proven, although we believe more data will support its effectiveness. Nonetheless, the three components together result in a 16% increase in F (compare column 1 to 5). We know a priori that, if A:B::C:D, then B:A::D:C. For example, mason is to stone as carpenter is to wood implies stone is to mason as wood is to carpenter. The matrix is designed so that the row vector for A:B is different from the row vector for B:A only by a permutation of the elements. To discover the consequences of this design decision, we altered steps 5 and 6 so that symmetry is no longer preserved. In step 5, for each word pair A:B that appears in the input set, we only have one row. Thus the number of rows in the matrix dropped from 17,232 to 8,616. In step 6, we no longer have two columns for each pattern P, one for “word1 P word2” and another for “word2 P word1.” However, to be fair, we kept the total number of columns at 8,000. In step 4, we selected the top 8,000 patterns (instead of the top 4,000), distinguishing the pattern “word1 P word2” from the pattern “word2 P word1” (instead of considering them equivalent). Thus a pattern P with a high frequency is likely to appear in two columns, in both possible orders, but a lower frequency pattern might appear in only one column, in only one possible order. These changes resulted in a slight decrease in performance. % to 55.% and precision dropped from 56. The decrease is not statistically significant. In step 12 of LRA, the relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. That is, the average includes only those alternates that are “better” than the originals. Taking all alternates instead of the better alternates, recall drops from 56. % and precision drops from 56. It would be convenient if inspection of r gave us a simple explanation or description of the relation between A and B. For example, suppose the word pair ostrich:bird maps to the row vector r. It would be pleasing to look in r and find that the largest element corresponds to the pattern “is the largest” (ie, “ostrich is the largest bird”). Unfortunately, inspection of r reveals no such convenient patterns. We hypothesize that the semantic content of a vector is distributed over the whole vector; it is not concentrated in a few elements. To test this hypothesis, we modified step 10 of LRA. Instead of projecting the 8,000 dimensional vectors into the 300 dimensional space UkEk, we use the matrix UkEkVTk . This matrix yields the same cosines as UkEk, but preserves the original 8,000 dimensions, making it easier to interpret the row vectors. For each row vector in UkEkVTk , we select the N largest values and set all other values to zero. The idea here is that we will only pay attention to the N most important patterns in r; the remaining patterns will be ignored. If most of the semantic content is in the N largest elements of r, then setting the remaining elements to zero should have relatively little impact. The precision and recall are significantly below the baseline LRA until N ≥ 300 (95% confidence, Fisher Exact Test). In other words, for a typical SAT analogy question, we need to examine the top 300 patterns to explain why LRA selected one choice instead of another. We are currently working on an extension of LRA that will explain with a single pattern why one choice is better than another. We have had some promising results, but this work is not yet mature. However, we can confidently claim that interpreting the vectors is not trivial. It may be interesting to see how many of the manually generated patterns appear within the automatically generated patterns. If we require an exact match, 50 of the 64 manual patterns can be found in the automatic patterns. If we are lenient about wildcards, and count the pattern not the as matching * not the (for example), then 60 of the 64 manual patterns appear within the automatic patterns. This suggests that the improvement in performance with the automatic patterns is due to the increased quantity of patterns, rather than a qualitative difference in the patterns. Both of these patterns are included in the 4,000 patterns automatically generated by LRA. No adjustments were made to tune LRA to the noun-modifier pairs. LRA is used as a distance (nearness) measure in a single nearest neighbor supervised learning algorithm. This data set includes information about the part of speech and WordNet synset (synonym set; ie, word sense tag) of each word, but our algorithm does not use this information. These were relations that are typically expressed with longer phrases (three or more words), rather than noun-modifier word pairs. For example, in flu virus, the head noun (H) is virus and the modifier (M) is flu (*). In English, the modifier (typically a noun or adjective) usually precedes the head noun. In the description of purpose, V represents an arbitrary verb. In concert hall, the hall is for presenting concerts (V is present) or holding concerts (V is hold) (†). We make use of this grouping in the following experiments. The following experiments use single nearest neighbor classification with leave-one-out cross-validation. For leave-one-out cross-validation, the testing set consists of a single noun-modifier pair and the training set consists of the 599 remaining noun-modifiers. The data set is split 600 times, so that each noun-modifier gets a turn as the testing word pair. The predicted class of the testing pair is the class of the single nearest neighbor in the training set. As the measure of nearness, we use LRA to calculate the relational similarity between the testing pair and the training pairs. The factor of 16 comes from the alternate pairs, step 11 in LRA. There are 600 word pairs in the input set for LRA. In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 2,400 pairs. In step 5, for each pair A:B, we add B:A, yielding 4,800 pairs. However, some pairs are dropped because they correspond to zero vectors and a few words do not appear in Lin’s thesaurus. The sparse matrix (step 7) has 4,748 rows and 8,000 columns, with a density of 8. Macroaveraging calculates the precision, recall, and F for each class separately, and then calculates the average across all classes. Microaveraging combines the true positive, false positive, and false negative counts for all of the classes, and then calculates precision, recall, and F from the combined counts. Macroaveraging gives equal weight to all classes, but microaveraging gives more weight to larger classes. We use macroaveraging (giving equal weight to all classes), because we have no reason to believe that the class sizes in the data set reflect the actual distribution of the classes in a real corpus. Classification with 30 distinct classes is a hard problem. For example, agent and beneficiary both collapse to participant. On the 30 class problem, LRA with the single nearest neighbor algorithm achieves an accuracy of 39. Always guessing the majority class would result in an accuracy of 8. On the 5 class problem, the accuracy is 58.0% (348/600) and the macroaveraged F is 54. Always guessing the majority class would give an accuracy of 43.% (260/600). Another limitation is speed; it took almost 9 days for LRA to answer 374 analogy questions. However, with progress in computer hardware, speed will gradually become less of a concern. Also, the software has not been optimized for speed; there are several places where the efficiency could be increased and many operations are parallelizable. It may also be possible to precompute much of the information for LRA, although this would require substantial changes to the algorithm. The difference in performance between VSM-AV and VSM-WMTS shows that VSM is sensitive to the size of the corpus. Although LRA is able to surpass VSM-AV when the WMTS corpus is only about one tenth the size of the AV corpus, it seems likely that LRA would perform better with a larger corpus. The WMTS corpus requires one terabyte of hard disk space, but progress in hardware will likely make 10 or even 100 terabytes affordable in the relatively near future. For noun-modifier classification, more labeled data should yield performance improvements. With 600 noun-modifier pairs and 30 classes, the average class has only 20 examples. We expect that the accuracy would improve substantially with 5 or 10 times more examples. Unfortunately, it is time consuming and expensive to acquire hand-labeled data. Another issue with noun-modifier classification is the choice of classification scheme for the semantic relations. It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. SVD is only one of many methods for handling sparse, noisy data. In step 4 of LRA, we simply select the top num patterns most frequent patterns and discard the remaining patterns. Perhaps a more sophisticated selection algorithm would improve the performance of LRA. We have tried a variety of ways of selecting patterns, but it seems that the method of selection has little impact on performance. We hypothesize that the distributed vector representation is not sensitive to the selection method, but it is possible that future work will find a method that yields significant improvement in performance. This article has introduced a new method for calculating relational similarity, Latent Relational Analysis. The experiments demonstrate that LRA performs better than the VSM approach, when evaluated with SAT word analogy questions and with the task of classifying noun-modifier expressions. The VSM approach represents the relation between a pair of words with a vector, in which the elements are based on the frequencies of 64 hand-built patterns in a large corpus. LRA extends this approach in three ways: (1) The patterns are generated dynamically from the corpus, (2) SVD is used to smooth the data, and (3) a thesaurus is used to explore variations of the word pairs. We have presented several examples of the many potential applications for measures of relational similarity. Just as attributional similarity measures have proven to have many practical uses, we expect that relational similarity measures will soon become widely used. We believe that relational similarity plays a fundamental role in the mind and therefore relational similarity measures could be crucial for artificial intelligence. In future work, we plan to investigate some potential applications for LRA. It is possible that the error rate of LRA is still too high for practical applications, but the fact that LRA matches average human performance on SAT analogy questions is encouraging. Thanks to Michael Littman for sharing the 374 SAT analogy questions and for inspiring me to tackle them. Thanks to Egidio Terra, Charlie Clarke, and the School of Computer Science of the University of Waterloo, for giving us a copy of the Waterloo MultiText System and their Terabyte Corpus. Thanks to Dekang Lin for making his Dependency-Based Word Similarity lexicon available online. Thanks to Ted Pedersen for making his WordNet::Similarity package available. Thanks to Joel Martin for comments on the article. Thanks to the anonymous reviewers of Computational Linguistics for their very helpful comments and suggestions.\n"
     ]
    }
   ],
   "source": [
    "# viasually check the results\n",
    "\n",
    "paper_id = 5\n",
    "print(data.model_input[paper_id])\n",
    "print(\"---------------------------------------------\")\n",
    "print(data.body_good_quality[paper_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, add cited text spans \n",
    "for i in range(len(data)):\n",
    "    data['model_input'][i] = data['model_input'][i] + \" \".join(data['cited_text_spans'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3 - Clean the data\n",
    "Using 'checkQuality' function to select only those input sentences with high quality. \\\n",
    "This step only makes a difference if we use abstract or body with no previous quality check. \\\n",
    "See other idea for data cleansing: https://www.kaggle.com/sandeepbhogaraju/text-summarization-with-seq2seq-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA:\n",
    "# we can use the checkQuality function to select only those input sentences with high quality\n",
    "\n",
    "def cleanInput(df, truncate_center = 300):\n",
    "    \n",
    "    ''' Clean input so that low quality sentences don't feed the model\n",
    "        \n",
    "        Inputs: \n",
    "            \n",
    "            df: model data. It has to have a column named 'model_input' \n",
    "        \n",
    "            truncate_center: If the number of sentences in model input is larger than 'truncate_center', \n",
    "                truncate the center of model input - i.e., get only the \n",
    "                first and last x (= truncate_center/2) sentences.\n",
    "        \n",
    "        Output: df with 'cleaned' 'model_input' column \n",
    "        \n",
    "     '''\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    for paper_id in df.index:\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        input_sentences = []\n",
    "\n",
    "        # split input into sentences\n",
    "        input_sentences_tok = df.model_input[paper_id].replace(\"et al.\", \"et al\").replace(\"e.g.\", \"eg\").replace(\"eg.\", \"eg\").replace(\"i.e.\", \"ie\").replace(\"ie.\", \"ie\")\n",
    "        input_sentences_tok = input_sentences_tok.replace(\".1\", \". \").replace(\".2\", \". \").replace(\".3\", \".\").replace(\".4\", \". \").replace(\".5\", \". \").replace(\".6\", \". \").replace(\".7\", \". \").replace(\".8\", \". \").replace(\".9\", \". \")\n",
    "\n",
    "        # tokenize input sentences\n",
    "        input_sentences_tok = tokenizer.tokenize(input_sentences_tok)\n",
    "\n",
    "        # truncate too large input sentences (larger than 512 throws an error - rare cases) and remove \"?\"\n",
    "        input_sentences_tok = [s.replace(\"?\", \"\") for s in input_sentences_tok if len(s) <= 512]\n",
    "        \n",
    "        \n",
    "        # truncate the center of the body for large bodies\n",
    "        if len(input_sentences_tok) > truncate_center:\n",
    "            input_sentences_tok = input_sentences_tok[:int(truncate_center/2)] + input_sentences_tok[-int(truncate_center/2):]\n",
    "\n",
    "        # find input sentences with quality\n",
    "        for i in range(len(input_sentences_tok)):\n",
    "            if len(input_sentences_tok[i])>0:\n",
    "                if checkQuality(input_sentences_tok[i]):\n",
    "                    input_sentences.append(input_sentences_tok[i])\n",
    "\n",
    "        # fill our large/original dataframe with the cleaned input sentences \n",
    "        df.model_input[paper_id] = \" \".join(input_sentences)\n",
    "        \n",
    "        # keep track of time to run \n",
    "        if paper_id % 50 == 0:\n",
    "#             print(paper_id, \"secs to run:\", time.time() - start)\n",
    "            start = time.time()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this step is only necessary if we use abstract or body with no previous quality check\n",
    "# data = cleanInput(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"data_organized300_rouge1_vsdoc.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4 - Divide the data into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "\n",
    "train_pct = 0.9\n",
    "test_pct = 0.05\n",
    "\n",
    "data = data.sample(len(data), random_state=20)\n",
    "train_sub = int(len(data) * train_pct)\n",
    "test_sub = int(len(data) * test_pct) + train_sub\n",
    "\n",
    "train_df = data[0:train_sub]\n",
    "test_df = data[train_sub:test_sub]\n",
    "val_df = data[test_sub:]\n",
    "\n",
    "train_input = list(train_df['model_input'])\n",
    "test_input = list(test_df['model_input'])\n",
    "val_input = list(val_df['model_input'])\n",
    "\n",
    "train_output = list(train_df['model_output'])\n",
    "test_output = list(test_df['model_output'])\n",
    "val_output = list(val_df['model_output'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>citations</th>\n",
       "      <th>golden</th>\n",
       "      <th>body_good_quality</th>\n",
       "      <th>cited_text_spans</th>\n",
       "      <th>model_input</th>\n",
       "      <th>model_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>Conditional random fields (Lafferty et al., 20...</td>\n",
       "      <td>Finding linguistic structure in raw text is no...</td>\n",
       "      <td>[Recent work by Smith and Eisner (2005) on con...</td>\n",
       "      <td>Contrastive Estimation: Training Log-Linear Mo...</td>\n",
       "      <td>Finding linguistic structure in raw text is no...</td>\n",
       "      <td>[Finding linguistic structure in raw text is n...</td>\n",
       "      <td>Finding linguistic structure in raw text is no...</td>\n",
       "      <td>Contrastive Estimation: Training Log-Linear Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Recognizing analogies, synonyms, anto nyms, an...</td>\n",
       "      <td>A pair of words (petrify:stone) is analogous t...</td>\n",
       "      <td>[Language modeling (Chen and Goodman, 1996), n...</td>\n",
       "      <td>A Uniform Approach to Analogies Synonyms Anton...</td>\n",
       "      <td>A pair of words (petrify:stone) is analogous t...</td>\n",
       "      <td>[A pair of words (petrify:stone) is analogous ...</td>\n",
       "      <td>A pair of words (petrify:stone) is analogous t...</td>\n",
       "      <td>A Uniform Approach to Analogies Synonyms Anton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>We seek a knowledge-free method for inducing m...</td>\n",
       "      <td>A multiword unit (MWU) is a connected collocat...</td>\n",
       "      <td>[Schone and Jurafsky (2001) applied LSA to the...</td>\n",
       "      <td>Is Knowledge-Free Induction Of Multiword Unit ...</td>\n",
       "      <td>In other words, MWUs are typically non-composi...</td>\n",
       "      <td>[In other words, MWUs are typically non-compos...</td>\n",
       "      <td>In other words, MWUs are typically non-composi...</td>\n",
       "      <td>Is Knowledge-Free Induction Of Multiword Unit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>hanb@student.unimelb.edu.au tb@ldwin.net Abstr...</td>\n",
       "      <td>Twitter and other micro-blogging services are ...</td>\n",
       "      <td>[We use unsupervised methods to build a pipeli...</td>\n",
       "      <td>Lexical Normalisation of Short Text Messages: ...</td>\n",
       "      <td>The quality of messages varies significantly, ...</td>\n",
       "      <td>[Additionally, we evaluate using the BLEU scor...</td>\n",
       "      <td>The quality of messages varies significantly, ...</td>\n",
       "      <td>Lexical Normalisation of Short Text Messages: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>In this paper we present a novel, customizable...</td>\n",
       "      <td>The goal of recent Information Extraction (IE)...</td>\n",
       "      <td>[, Indeed, the analysis produced by existing s...</td>\n",
       "      <td>Using Predicate-Argument Structures For Inform...</td>\n",
       "      <td>The goal of recent Information Extraction (IE)...</td>\n",
       "      <td>[The goal of recent Information Extraction (IE...</td>\n",
       "      <td>The goal of recent Information Extraction (IE)...</td>\n",
       "      <td>Using Predicate-Argument Structures For Inform...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              abstract  \\\n",
       "457  Conditional random fields (Lafferty et al., 20...   \n",
       "798  Recognizing analogies, synonyms, anto nyms, an...   \n",
       "391  We seek a knowledge-free method for inducing m...   \n",
       "376  hanb@student.unimelb.edu.au tb@ldwin.net Abstr...   \n",
       "387  In this paper we present a novel, customizable...   \n",
       "\n",
       "                                                  body  \\\n",
       "457  Finding linguistic structure in raw text is no...   \n",
       "798  A pair of words (petrify:stone) is analogous t...   \n",
       "391  A multiword unit (MWU) is a connected collocat...   \n",
       "376  Twitter and other micro-blogging services are ...   \n",
       "387  The goal of recent Information Extraction (IE)...   \n",
       "\n",
       "                                             citations  \\\n",
       "457  [Recent work by Smith and Eisner (2005) on con...   \n",
       "798  [Language modeling (Chen and Goodman, 1996), n...   \n",
       "391  [Schone and Jurafsky (2001) applied LSA to the...   \n",
       "376  [We use unsupervised methods to build a pipeli...   \n",
       "387  [, Indeed, the analysis produced by existing s...   \n",
       "\n",
       "                                                golden  \\\n",
       "457  Contrastive Estimation: Training Log-Linear Mo...   \n",
       "798  A Uniform Approach to Analogies Synonyms Anton...   \n",
       "391  Is Knowledge-Free Induction Of Multiword Unit ...   \n",
       "376  Lexical Normalisation of Short Text Messages: ...   \n",
       "387  Using Predicate-Argument Structures For Inform...   \n",
       "\n",
       "                                     body_good_quality  \\\n",
       "457  Finding linguistic structure in raw text is no...   \n",
       "798  A pair of words (petrify:stone) is analogous t...   \n",
       "391  In other words, MWUs are typically non-composi...   \n",
       "376  The quality of messages varies significantly, ...   \n",
       "387  The goal of recent Information Extraction (IE)...   \n",
       "\n",
       "                                      cited_text_spans  \\\n",
       "457  [Finding linguistic structure in raw text is n...   \n",
       "798  [A pair of words (petrify:stone) is analogous ...   \n",
       "391  [In other words, MWUs are typically non-compos...   \n",
       "376  [Additionally, we evaluate using the BLEU scor...   \n",
       "387  [The goal of recent Information Extraction (IE...   \n",
       "\n",
       "                                           model_input  \\\n",
       "457  Finding linguistic structure in raw text is no...   \n",
       "798  A pair of words (petrify:stone) is analogous t...   \n",
       "391  In other words, MWUs are typically non-composi...   \n",
       "376  The quality of messages varies significantly, ...   \n",
       "387  The goal of recent Information Extraction (IE)...   \n",
       "\n",
       "                                          model_output  \n",
       "457  Contrastive Estimation: Training Log-Linear Mo...  \n",
       "798  A Uniform Approach to Analogies Synonyms Anton...  \n",
       "391  Is Knowledge-Free Induction Of Multiword Unit ...  \n",
       "376  Lexical Normalisation of Short Text Messages: ...  \n",
       "387  Using Predicate-Argument Structures For Inform...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Summarize with Pre-Trained Pegasus (no fine-tuning)\n",
    "    \n",
    "In this step we will do the following:\n",
    " \n",
    "Step 3.1 - Instantiate the model\\\n",
    "Step 3.2 - Instatiate the metric of interest \\\n",
    "Step 3.3 - Compute the summaries\\\n",
    "Step 3.4 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 - Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# model_name = 'google/pegasus-large'\n",
    "\n",
    "model_name = 'fine_tuned'\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "# This is the PEGASUS Model with a language modeling head. Can be used for summarization. \n",
    "# This model inherits from PreTrainedModel. \n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # OTHER OPTIONS BELOW"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OTHER OPTIONS: Possible pre-trained models\n",
    "![image.png](attachment:image.png)\n",
    "Source: https://pub.towardsai.net/summarization-using-pegasus-model-with-the-transformers-library-553cd0dc5c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 - Instatiate the metric of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate rouge score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3 - Compute the summaries and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-c19c64d6e59a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#     score_result = compute_metrics2([inputs, sample_output])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#     scores = scorer.score(\" \".join(decoded_preds), \" \".join(decoded_labels))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_model_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_reference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/rouge_score/rouge_scorer.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, target, prediction)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtarget_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stemmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mprediction_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stemmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/rouge_score/tokenize.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text, stemmer)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m# Convert everything to lowercase.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m   \u001b[0;31m# Replace any non-alpha-numeric characters with spaces.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[^a-z0-9]+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#INFERENCE\n",
    "\n",
    "## Sample with no fine-tunning \n",
    "\n",
    "import time\n",
    "\n",
    "# create empry lists to store rouge scores\n",
    "r1_precision = []\n",
    "r2_precision = []\n",
    "rL_precision = []\n",
    "\n",
    "r1_recall = []\n",
    "r2_recall = []\n",
    "rL_recall = []\n",
    "\n",
    "r1_fmeasure = []\n",
    "r2_fmeasure = []\n",
    "rL_fmeasure = []\n",
    "\n",
    "val_model_output = []\n",
    "\n",
    "# start counting seconds to keep track of time \n",
    "start = time.time()\n",
    "\n",
    "# loop over validation set\n",
    "for sample_id in range(len(val_input)):\n",
    "    \n",
    "    # get input (body OR abstract + cited text spans) - scisummnet uses abstract, we want to use body\n",
    "    sample_input = val_input[sample_id]\n",
    "    \n",
    "    # tokenize it\n",
    "    inputs = tokenizer([sample_input], max_length=1024, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # 'max_length': Pad to a maximum length specified with the argument max_length \n",
    "    # or to the maximum acceptable input length for the model if that argument is not provided.\n",
    "\n",
    "    # generate Summary\n",
    "    summary_ids = model.generate(inputs['input_ids'])\n",
    "    \n",
    "    # decode summary\n",
    "    sample_output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    \n",
    "    # store summary\n",
    "    val_model_output = val_model_output + sample_output\n",
    "    \n",
    "    # get reference (gold) summary \n",
    "    sample_reference = val_output[sample_id]\n",
    "    \n",
    "    # Some simple post-processing\n",
    "#     decoded_preds, decoded_labels = postprocess_text(val_model_output, sample_reference)\n",
    "\n",
    "    #calculate rouge score\n",
    "#     score_result = compute_metrics2([inputs, sample_output])\n",
    "#     scores = scorer.score(\" \".join(decoded_preds), \" \".join(decoded_labels))\n",
    "    scores = scorer.score(val_model_output, sample_reference)  \n",
    "\n",
    "\n",
    "    \n",
    "    r1_precision.append(scores['rouge1'][0])\n",
    "    r1_recall.append(scores['rouge1'][1])\n",
    "    r1_fmeasure.append(scores['rouge1'][2])\n",
    "    \n",
    "    r2_precision.append(scores['rouge2'][0])\n",
    "    r2_recall.append(scores['rouge2'][1])\n",
    "    r2_fmeasure.append(scores['rouge2'][2])\n",
    "    \n",
    "    rL_precision.append(scores['rougeL'][0])\n",
    "    rL_recall.append(scores['rougeL'][1])\n",
    "    rL_fmeasure.append(scores['rougeL'][2])\n",
    "    \n",
    "    if sample_id % 5 == 0:\n",
    "        print(sample_id, time.time() - start)\n",
    "        start = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4 - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 R1   R2        RL\n",
      "precision  0.002606  0.0  0.002606\n",
      "recall     0.050000  0.0  0.050000\n",
      "fmeasure   0.004955  0.0  0.004955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute score statistics\n",
    "\n",
    "all_socores = {'R1': [stats.mean(r1_precision), stats.mean(r1_recall), stats.mean(r1_fmeasure)],\n",
    "        'R2': [stats.mean(r2_precision), stats.mean(r2_recall), stats.mean(r2_fmeasure)],\n",
    "        'RL': [stats.mean(rL_precision), stats.mean(rL_recall), stats.mean(rL_fmeasure)]      \n",
    "        }\n",
    "\n",
    "all_socores_df = pd.DataFrame(all_socores, columns = ['R1', 'R2', 'RL'], index=['precision','recall','fmeasure'])\n",
    "\n",
    "print(all_socores_df)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visually check quality of cited text spans\n",
    "# pick any number up to the total number of papers in the validation set\n",
    "paper_id = 0\n",
    "\n",
    "# print(\"Model input:\")\n",
    "# print(val_input[paper_id])\n",
    "# print('-------------------------------------------------------------')\n",
    "# print(\"Model output:\")\n",
    "# print(val_model_output[paper_id])\n",
    "print(compute_metrics2([val_model_output[paper_id], val_model_output[paper_id]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Fine-Tune Pre-Trained Pegasus\n",
    "\n",
    "In this step we will do the following:\n",
    " \n",
    "Step 4.0 - Reduce train size if necessary \\\n",
    "Step 4.1 - Instantiate the model (the same as in step 3, so I'll skip it) \\\n",
    "Step 4.2 - Define metrics of interest \\\n",
    "Step 4.3 - Tokenize text data and wrap it into a torch Dataset \\\n",
    "Step 4.4 - Train and evaluate the model \\\n",
    "Step 4.5 - Save the model \\\n",
    "Step 4.6 - Instantiate the fine-tuned model \\\n",
    "Step 4.7 - Compute the summaries and evaluate \\\n",
    "\n",
    "Helpful resources: \\\n",
    "https://towardsdatascience.com/how-to-perform-abstractive-summarization-with-pegasus-3dd74e48bafb (github link inside)\\\n",
    "https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_summarization.py \\\n",
    "https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.0 - Reduce train size if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(20)\n",
    "# number of samples in train\n",
    "n = 20\n",
    "\n",
    "train_input = random.sample(train_input, n)\n",
    "\n",
    "\n",
    "train_output = random.sample(train_output, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1 - Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same as in step 3, so I'll skip it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2 - Define metrics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Ignore pad token for loss: Replace -100 in the labels as we can't decode them.\n",
    "    #     if data_args.ignore_pad_token_for_loss:\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "def compute_metrics2(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = preds\n",
    "\n",
    "    # Ignore pad token for loss: Replace -100 in the labels as we can't decode them.\n",
    "    #     if data_args.ignore_pad_token_for_loss:\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = labels\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3 - Tokenize text data and wrap it into a torch Dataset\n",
    "\n",
    "Since we gonna use Trainer from Transformers library, it expects our dataset as a torch.utils.data.Dataset, so we made a simple class that implements __len__() method that returns number of samples, and __getitem__() method to return a data sample at a specific index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(model_name, \n",
    "                 train_texts, train_labels, \n",
    "                 val_texts=None, val_labels=None, \n",
    "                 test_texts=None, test_labels=None):\n",
    "  \"\"\"\n",
    "  Prepare input data for model fine-tuning\n",
    "  \"\"\"\n",
    "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  prepare_val = False if val_texts is None or val_labels is None else True\n",
    "  prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "  def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    decodings = tokenizer(labels,  truncation=True, padding=True)\n",
    "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "    return dataset_tokenized\n",
    "\n",
    "  train_dataset = tokenize_data(train_texts, train_labels)\n",
    "  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename variables\n",
    "train_texts, train_labels = train_input, train_output \n",
    "val_texts, val_labels = val_input, val_output \n",
    "\n",
    "# prepare data\n",
    "train_dataset, val_dataset, _ = prepare_data(model_name, train_texts, train_labels, val_texts, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.4 - Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "# define Training Arguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "    logging_steps=5  ,               # log & save weights each logging_steps\n",
    "    eval_steps=1,                    # number of update steps before evaluation\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    "    predict_with_generate = True     # whether to use generate to calculate generative metrics (ROUGE, BLEU). \n",
    "\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                     # the instantiated 🤗 Transformers model to be trained\n",
    "    tokenizer = tokenizer,           # the instantiated 🤗 Transformers tokenizer to be trained  \n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    eval_dataset=val_dataset,        # evaluation dataset\n",
    "    compute_metrics=compute_metrics  # pass metric function\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.5 - Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('fine_tuned')\n",
    "tokenizer.save_pretrained('fine_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.6 - Instantiate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the new model\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained('fine_tuned')\n",
    "# The PEGASUS Model with a language modeling head. Can be used for summarization. \n",
    "# This model inherits from PreTrainedModel. \n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained('fine_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.7 - Compute the summaries and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFERENCE\n",
    "\n",
    "## Sample with no fine-tunning \n",
    "\n",
    "import time\n",
    "\n",
    "# create empry lists to store rouge scores\n",
    "r1_precision = []\n",
    "r2_precision = []\n",
    "rL_precision = []\n",
    "\n",
    "r1_recall = []\n",
    "r2_recall = []\n",
    "rL_recall = []\n",
    "\n",
    "r1_fmeasure = []\n",
    "r2_fmeasure = []\n",
    "rL_fmeasure = []\n",
    "\n",
    "val_model_output = []\n",
    "\n",
    "# start counting seconds to keep track of time \n",
    "start = time.time()\n",
    "\n",
    "# loop over validation set\n",
    "for sample_id in range(len(val_input)):\n",
    "    \n",
    "    # get input (body OR abstract + cited text spans) - scisummnet uses abstract, we want to use body\n",
    "    sample_input = val_input[sample_id]\n",
    "    \n",
    "    # tokenize it\n",
    "    inputs = tokenizer([sample_input], max_length=1024, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # 'max_length': Pad to a maximum length specified with the argument max_length \n",
    "    # or to the maximum acceptable input length for the model if that argument is not provided.\n",
    "\n",
    "    # generate Summary\n",
    "    summary_ids = model.generate(inputs['input_ids'])\n",
    "    \n",
    "    # decode summary\n",
    "    sample_output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    \n",
    "    # store summary\n",
    "    val_model_output = val_model_output + sample_output\n",
    "    \n",
    "    # get reference (gold) summary \n",
    "    sample_reference = val_output[sample_id]\n",
    "    \n",
    "    #calculate rouge score\n",
    "    scores = scorer.score(str(sample_reference), str(sample_output))\n",
    "    \n",
    "    r1_precision.append(scores['rouge1'][0])\n",
    "    r1_recall.append(scores['rouge1'][1])\n",
    "    r1_fmeasure.append(scores['rouge1'][2])\n",
    "    \n",
    "    r2_precision.append(scores['rouge2'][0])\n",
    "    r2_recall.append(scores['rouge2'][1])\n",
    "    r2_fmeasure.append(scores['rouge2'][2])\n",
    "    \n",
    "    rL_precision.append(scores['rougeL'][0])\n",
    "    rL_recall.append(scores['rougeL'][1])\n",
    "    rL_fmeasure.append(scores['rougeL'][2])\n",
    "    \n",
    "    if sample_id % 5 == 0:\n",
    "        print(sample_id, time.time() - start)\n",
    "        start = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute score statistics\n",
    "\n",
    "all_socores = {'R1': [stats.mean(r1_precision), stats.mean(r1_recall), stats.mean(r1_fmeasure)],\n",
    "        'R2': [stats.mean(r2_precision), stats.mean(r2_recall), stats.mean(r2_fmeasure)],\n",
    "        'RL': [stats.mean(rL_precision), stats.mean(rL_recall), stats.mean(rL_fmeasure)]      \n",
    "        }\n",
    "\n",
    "all_socores_df = pd.DataFrame(all_socores, columns = ['R1', 'R2', 'RL'], index=['precision','recall','fmeasure'])\n",
    "\n",
    "print(all_socores_df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check quality of cited text spans\n",
    "# pick any number up to the total number of papers in the validation set\n",
    "paper_id = 25\n",
    "\n",
    "print(\"Model input:\")\n",
    "print(val_input[paper_id])\n",
    "print('------')\n",
    "print(\"Model output:\")\n",
    "print(val_model_output[paper_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "# from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# def prepare_fine_tuning(model_name, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n",
    "#   \"\"\"\n",
    "#   Prepare configurations and base model for fine-tuning\n",
    "#   \"\"\"\n",
    "#   torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#   model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "#   if freeze_encoder:\n",
    "#     for param in model.model.encoder.parameters():\n",
    "#       param.requires_grad = False\n",
    "\n",
    "#   if val_dataset is not None:\n",
    "#     training_args = TrainingArguments(\n",
    "#       output_dir=output_dir,           # output directory\n",
    "#       do_train=True,  \n",
    "#       num_train_epochs=5,              # total number of training epochs\n",
    "#       per_device_train_batch_size=16,  # batch size per device during training, can increase if memory allows\n",
    "#       per_device_eval_batch_size=32,   # batch size for evaluation, can increase if memory allows\n",
    "#       save_steps=1,                  # number of updates steps before checkpoint saves\n",
    "#       save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "#       evaluation_strategy='steps',     # evaluation strategy to adopt during training\n",
    "#       eval_steps=1,                  # number of update steps before evaluation\n",
    "#       warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#       weight_decay=0.01,               # strength of weight decay\n",
    "#       logging_dir='./logs',            # directory for storing logs\n",
    "#       logging_steps=32,\n",
    "#       gradient_accumulation_steps=1,  \n",
    "# #     output_dir=\"./checkpoints\",\n",
    "# #     per_device_train_batch_size=1,\n",
    "# #     do_train=True,\n",
    "# #     # fp16=True,  # This has a known bug with t5\n",
    "# #     gradient_accumulation_steps=1,\n",
    "# #     logging_steps=1,\n",
    "# #     save_steps=1,\n",
    "# #     overwrite_output_dir=True,\n",
    "# #     save_total_limit=10,\n",
    "#     )\n",
    "\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#       model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "#       args=training_args,                  # training arguments, defined above\n",
    "#       train_dataset=train_dataset,         # training dataset\n",
    "#       eval_dataset=val_dataset             # evaluation dataset\n",
    "#     )\n",
    "\n",
    "#   else:\n",
    "#     training_args = TrainingArguments(\n",
    "# #       output_dir=output_dir,           # output directory\n",
    "# #       num_train_epochs=5,              # total number of training epochs\n",
    "# #       per_device_train_batch_size=1,  # batch size per device during training, can increase if memory allows\n",
    "# #       save_steps=16,                  # number of updates steps before checkpoint saves\n",
    "# #       save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "# #       warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "# #       weight_decay=0.01,               # strength of weight decay\n",
    "# #       logging_dir='./logs',            # directory for storing logs\n",
    "# #       logging_steps=16,\n",
    "# #       gradient_accumulation_steps=16,  \n",
    "#     output_dir=\"./checkpoints\",\n",
    "#     per_device_train_batch_size=1,\n",
    "#     do_train=True,\n",
    "#     # fp16=True,  # This has a known bug with t5\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     logging_steps=1,\n",
    "#     save_steps=1,\n",
    "#     overwrite_output_dir=True,\n",
    "#     save_total_limit=10,\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#       model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "#       args=training_args,                  # training arguments, defined above\n",
    "#       train_dataset=train_dataset,         # training dataset\n",
    "#     )\n",
    "\n",
    "#   return trainer\n",
    "\n",
    "\n",
    "# # Train\n",
    "\n",
    "# trainer = prepare_fine_tuning(model_name, train_dataset, val_dataset)\n",
    "# trainer.train()\n",
    "\n",
    "# ### Step 4.6 - Instantiate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
