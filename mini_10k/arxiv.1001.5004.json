{"id": "http://arxiv.org/abs/1001.5004v1", "guidislink": true, "updated": "2010-01-27T19:33:12Z", "updated_parsed": [2010, 1, 27, 19, 33, 12, 2, 27, 0], "published": "2010-01-27T19:33:12Z", "published_parsed": [2010, 1, 27, 19, 33, 12, 2, 27, 0], "title": "\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and\n  efficient nature of Statistics", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1001.3281%2C1001.2010%2C1001.1674%2C1001.0758%2C1001.4270%2C1001.4741%2C1001.0238%2C1001.2059%2C1001.5064%2C1001.1119%2C1001.2101%2C1001.4253%2C1001.1322%2C1001.1256%2C1001.4371%2C1001.3018%2C1001.1784%2C1001.4136%2C1001.5004%2C1001.1495%2C1001.2961%2C1001.2277%2C1001.5150%2C1001.2272%2C1001.4431%2C1001.5443%2C1001.1578%2C1001.2503%2C1001.4693%2C1001.2323%2C1001.0038%2C1001.1029%2C1001.4806%2C1001.4275%2C1001.4357%2C1001.4240%2C1001.5033%2C1001.0527%2C1001.2789%2C1001.0387%2C1001.3324%2C1001.1291%2C1001.3761%2C1001.1812%2C1001.4175%2C1001.0806%2C1001.2746%2C1001.5106%2C1001.2508%2C1001.1627%2C1001.4076%2C1001.1890%2C1001.2927%2C1001.5382%2C1001.4646%2C1001.0763%2C1001.0507%2C1001.1556%2C1001.4760%2C1001.4764%2C1001.4835%2C1001.5455%2C1001.4982%2C1001.2807%2C1001.4840%2C1001.4264%2C1001.2345%2C1001.3849%2C1001.2294%2C1001.3647%2C1001.1926%2C1001.0098%2C1001.0828%2C1001.1486%2C1001.0224%2C1001.2724%2C1001.2463%2C1001.4730%2C1001.1614%2C1001.5028%2C1001.3573%2C1001.0999%2C1001.2125%2C1001.2617%2C1001.2530%2C1001.3316%2C1001.0465%2C1001.2167%2C1001.0776%2C1001.0384%2C1001.2086%2C1001.4943%2C1001.1651%2C1001.5392%2C1001.0308%2C1001.3381%2C1001.3128%2C1001.4819%2C1001.1086%2C1001.2563%2C1001.5112&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and\n  efficient nature of Statistics"}, "summary": "Unlike the Probability Theory based on additivity, Statistical Inference\nseems to hesitate between \"Additivity\" and a so-called \"Maxitivity\" approach.\nAfter a brief overview of three types of principles for any (parametric)\nstatistical theory and the proof that these principles are mutually exclusive,\nthe paper shows that two kinds of support measures are conceivable, an additive\none and a maxitive one (based on maximization operators). Unfortunately, none\nof them is able to cope with the ignorance part of the statistical experiment\nand, in the meantime, with the partial information given through the structure\nof the data. To conclude, the author promotes the combined use of both\napproaches, as an efficient middle-of-the-road position for the statistician.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1001.3281%2C1001.2010%2C1001.1674%2C1001.0758%2C1001.4270%2C1001.4741%2C1001.0238%2C1001.2059%2C1001.5064%2C1001.1119%2C1001.2101%2C1001.4253%2C1001.1322%2C1001.1256%2C1001.4371%2C1001.3018%2C1001.1784%2C1001.4136%2C1001.5004%2C1001.1495%2C1001.2961%2C1001.2277%2C1001.5150%2C1001.2272%2C1001.4431%2C1001.5443%2C1001.1578%2C1001.2503%2C1001.4693%2C1001.2323%2C1001.0038%2C1001.1029%2C1001.4806%2C1001.4275%2C1001.4357%2C1001.4240%2C1001.5033%2C1001.0527%2C1001.2789%2C1001.0387%2C1001.3324%2C1001.1291%2C1001.3761%2C1001.1812%2C1001.4175%2C1001.0806%2C1001.2746%2C1001.5106%2C1001.2508%2C1001.1627%2C1001.4076%2C1001.1890%2C1001.2927%2C1001.5382%2C1001.4646%2C1001.0763%2C1001.0507%2C1001.1556%2C1001.4760%2C1001.4764%2C1001.4835%2C1001.5455%2C1001.4982%2C1001.2807%2C1001.4840%2C1001.4264%2C1001.2345%2C1001.3849%2C1001.2294%2C1001.3647%2C1001.1926%2C1001.0098%2C1001.0828%2C1001.1486%2C1001.0224%2C1001.2724%2C1001.2463%2C1001.4730%2C1001.1614%2C1001.5028%2C1001.3573%2C1001.0999%2C1001.2125%2C1001.2617%2C1001.2530%2C1001.3316%2C1001.0465%2C1001.2167%2C1001.0776%2C1001.0384%2C1001.2086%2C1001.4943%2C1001.1651%2C1001.5392%2C1001.0308%2C1001.3381%2C1001.3128%2C1001.4819%2C1001.1086%2C1001.2563%2C1001.5112&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Unlike the Probability Theory based on additivity, Statistical Inference\nseems to hesitate between \"Additivity\" and a so-called \"Maxitivity\" approach.\nAfter a brief overview of three types of principles for any (parametric)\nstatistical theory and the proof that these principles are mutually exclusive,\nthe paper shows that two kinds of support measures are conceivable, an additive\none and a maxitive one (based on maximization operators). Unfortunately, none\nof them is able to cope with the ignorance part of the statistical experiment\nand, in the meantime, with the partial information given through the structure\nof the data. To conclude, the author promotes the combined use of both\napproaches, as an efficient middle-of-the-road position for the statistician."}, "authors": ["M. R\u00e9mon"], "author_detail": {"name": "M. R\u00e9mon"}, "author": "M. R\u00e9mon", "arxiv_comment": "19 pages, 40 references", "links": [{"href": "http://arxiv.org/abs/1001.5004v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1001.5004v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1001.5004v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1001.5004v1", "journal_reference": null, "doi": null, "fulltext": "\"Additivity\" versus \"Maxitivity\" at the heart of the\n\narXiv:1001.5004v1 [stat.ME] 27 Jan 2010\n\nparadoxical and efficient nature of Statistics\n\nM. R\u00e9mon\nDepartment of Mathematics, Namur University, B-5000 Namur, Belgium\nmarcel.remon@fundp.ac.be\n\nSummary.\nUnlike the Probability Theory based on additivity, Statistical Inference seems to hesitate between \"Additivity\" and a so-called \"Maxitivity\" approach. After a brief overview of three types of\nprinciples for any (parametric) statistical theory and the proof that these principles are mutually\nexclusive, the paper shows that two kinds of support measures are conceivable, an additive\none and a maxitive one (based on maximization operators). Unfortunately, none of them is\nable to cope with the ignorance part of the statistical experiment and, in the meantime, with\nthe partial information given through the structure of the data. To conclude, the author promotes the combined use of both approaches, as an efficient middle-of-the-road position for the\nstatistician.\nR\u00e9sum\u00e9.\nContrairement \u00e0 la th\u00e9orie de probabilit\u00e9 qui est fond\u00e9e sur l'additivit\u00e9, l'inf\u00e9rence statistique\nsemble h\u00e9siter entre \"l'Additivit\u00e9\" et ce que d'aucun appelle la \"Maxitivit\u00e9\". Apr\u00e8s un bref survol des trois cat\u00e9gories de principes applicables \u00e0 toute th\u00e9orie (param\u00e9trique) statistique et\nla d\u00e9monstration que ceux-ci sont mutuellement exclusifs, le papier montre que deux types\nde mesure de support sont envisageables, \u00e0 savoir une mesure additive ou une mesure maxitive (bas\u00e9e sur des op\u00e9rateurs de maximisation). Malheureusement, aucune n'est capable\n\n\f2\n\nM. R\u00e9mon\nd'appr\u00e9hender correctement la part d'ignorance contenue dans l'exp\u00e9rience statistique et,\ndans le m\u00eame temps, l'information partielle d\u00e9livr\u00e9e par la structure des donn\u00e9es. En conclusion, l'auteur propose l'utilisation combin\u00e9e des deux approches comme une position efficace\net m\u00e9diane pour le statisticien.\nKeywords: Principles of statistics, Support measures, Maxitivity property, Statistical paradoxes\n\n1. Introduction\nIn this paper, we restrict ourselves to parametric statistical models. Doing that, we know\nthat we leave aside a large part of statistics. But we think that equivalent reflections can\nbe done for non parametric statistics. Each statistical theory tries to answer the same\nbasic question : \"What can we say about the underlying hypotheses, from the observed data\ninformation we get ?\" The different schools of inference have succeeded in giving an answer\nto the question, as long as one accepts some principles related to these schools. Classical\napproach is best if one looks for long-run properties. Bayesian inference should be chosen if\none has meaningful proper prior information over the parameter space. Structural inference\nis to be used for transformation models, etc. But there is not always evident prior to choose\nand the Bayesian approach is therefore difficult to apply; or the data come from a unique\nand non replicable experiment and the Classical approach is no longer appropriate.\n\nIt would be naive to believe that a single inference theory could be suitable for all inference\nproblems. In that sense, we totally agree with Kalbfleisch and Sprott (1970) : \"In fact, the\nmain criticism to be directed at the study of statistical inference today is the slavish adherence\nto rigid dogmas and principles (e.g. Bayes theorem, likelihood principle, admissibility, etc.)\nwhich is characteristic of the various schools of inference * * * To claim that all problems of\ninference have been, or even can be, solved by one overriding principle seems to us naive.\"\n\n\f\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and efficient nature of Statistics\n\n3\n\nThis appears to close definitively the search for a general statistical inference school. What\nwe try to do in this paper, is to go deeper in the formal understanding of such a failure.\nWe do that by focusing on the opposition between the way to handle ignorance on one side\nand structural data information on the other side. For that, it is important to look at the\nprinciples underlying the various schools of inference.\n\n2. Three sets of principles : a brief overview of Statistical principles\n\n2.1. Notations\nWe represent a parametric statistical experiment by means of the following model :\n\n\u2212M(X, \u0398, p\u03b8 (x), \u03bc(x)) where\n\nX is the sample space,\n\u0398 the parameter space,\np\u03b8 (x) the density family with respect to \u03bc(x), and\n\u03bc(x) a \u03c3-finite measure over X\n(usually the Lebesgue or the countable measure)\n\n\u2212Plus the knowledge of the observed data \"x \u2208 E\".\n\nMany principles will not be mentioned here because they are mere consequences of general\nprinciples such as the Likelihood or Invariance ones. We think, for instance, of the Mathematical Equivalence principle (Birnbaum (1964)), which states that our inference should\nbe independent from any one-to-one transformation of the sample space. This principle is\na corollary of the Likelihood principle.\n\n\f4\n\nM. R\u00e9mon\n\n2.2. First set : invariance concerning the parameter space\n2.2.1. The (Strong) Invariance principle I\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\nI : Let M1 (X, \u0398, p\u03b8 (x), \u03bc(x)) and M2 (X , \u0398 , p\u03b8\u2032 (x ), \u03bc (x )) be two different models for\n\u2032\n\n\u2032\n\nthe same experiment, connected by two functions f : X \u2192 X and g : \u0398 \u2192 \u0398 such that\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\np\u03b8 ({x : f (x) = xo }) = pg(\u03b8) (xo ) and \u03bc({x : f (x) = xo }) = \u03bc (xo ) \u2200\u03b8 \u2208 \u0398 and \u2200xo \u2208 X .\nThe Invariance principle states that equivalent inference about g(\u03b8) should be made from\n\u2032\n\nthe first model given the knowledge \"f (x) = xo \" as from the second model given the same\n\u2032\n\n\u2032\n\nobservation \"x = xo \".\nTo better understand the Invariance principle, let us consider N (\u03bc, \u03c3 2 ), the Normal model.\nSuppose we only observe the value of the standard deviation s2 = s2o . The Invariance\nprinciple states that inference about \u03c3 given the observed s2o should be the same whether\none uses the Normal model N (\u03bc, \u03c3 2 ) or the Chi-square distribution for\n\ns2\n\u03c32 .\n\nThe Invariance\n\nprinciple requires that statistical inference does not depend on the choice of parameterization\nfor the model. A consequence of this principle is the invariance of inference under one-toone transformation of the parameter. This principle is advocated by many authors, see for\ninstance Hartigan (1967). It is at the heart of the paradoxes studied by Dawid et al. (1973)\nagainst Bayesian and Structural inference. See also the old Bertrand-Von Mises paradox\nabout the choice of the ratio \"wine-water\" versus \"water-wine\" as our parameterization\nwithin an uniform model (Von Mises (1939)).\n\n2.3. Second set : invariance concerning the sample space\n2.3.1. The Censoring principle CE\nCE : For any specified outcome xo of an experiment M(X, \u0398, p\u03b8 (x), \u03bc(x)), our statistical\nevidence is fully characterized by the function p\u03b8 (xo ), \u03b8 \u2208 \u0398, without further reference\n\n\f\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and efficient nature of Statistics\n\n5\n\nto M or xo , i.e. all our statistical information is contained in the likelihood function\n(Birnbaum (1964)).\n\nCE was first proposed by Pratt (1962) by means of an example : if an accurate voltmeter\ngave a reading of 87, does it matter, for the interpretation of this reading (assumed errorfree), whether the meter's range was bounded by 1,000 or by 100 ?\n\n2.3.2. The Stopping Rule principle ST\nST : The Stopping Rule principle states that the sampling design is irrelevant to statistical\ninference at the stage of data analysis.\n\nThis principle is formally equivalent to the Censoring principle, if one considers the following\nstopping rule : stop the experiment as soon as \"xo \" is observed. ST can be accepted if\none is working with an experiment which will be performed once only. It is certainly\nnot a satisfying principle for long-run sampling experiment, which is the basis of classical\ninference.\n\n2.3.3. The (Strong) Likelihood principle L\nL : Suppose a statistical experiment is characterized by two different models with com\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\nmon parameter space : M1 (X, \u0398, p\u03b8 (x), \u03bc(x)) and M2 (X , \u0398, p\u03b8 (x ), \u03bc (x )) such that\n\u2032\n\n\u2032\n\n\u2032\n\n\u2032\n\np\u03b8 (xo ) = c * p\u03b8 (xo ) for each \u03b8 in \u0398, for some xo in X, xo in X and for constant c 6= 0. Then\n\u2032\n\nL states that the same inference should be made about \u03b8 whatever xo or xo is observed.\nThe Likelihood principle says that all the relevant information for inference about \u03b8 is\ncontained in the sole knowledge of the relative likelihood function. This principle is advocated and criticized by many statisticians. See for instance Fisher (1950), Birnbaum (1962,\n\n\f6\n\nM. R\u00e9mon\n\n1964), Barnard (1967, 1973), Basu (1973), Berger (1985) or Berger and Wolpert (1988).\nBirnbaum (1964) proved that L is equivalent to the Sufficiency principle S (see next section)\n& CE or to S & ST .\n\n2.4. Third set : Reduction-type principles\n2.4.1. The Reduction principle R\nR : In logic, if A \u21d2 C and B \u21d2 C, then (A \u222a B) \u21d2 C. In statistics, one has a similar\nprinciple. Let I(A) be the inferential information contained in the observation A [or some\nstatistical inference made from A]. If the data A and B lead to the same inference I(A) =\nI(B), one should perform equivalent inference from the observation of A \u222a B : i.e. if\nA \u21d2 I(A) and B \u21d2 I(B) = I(A), then (A \u222a B) \u21d2 I(A \u222a B) = I(A) = I(B).\n\nThis Reduction principle was proposed by Dawid (1977). It gives a general framework for\nall the (partial) Sufficiency or Conditionality principles.\n\n2.4.2. The Sufficiency principle S\nS : In an experiment M(X, \u0398, p\u03b8 (x), \u03bc(x)), we get the same information about \u03b8, if we\nobserve the realization xo or only its realization through a sufficient statistic T (xo ) = to .\n\nThis principle is certainly the most widely accepted principle in statistics. Together with\nCE or ST , it implies that the likelihood function is only relevant for inference up to a\nproportional constant.\n\n\f\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and efficient nature of Statistics\n\n7\n\n2.4.3. The Conditionality principle CO\nCO : Suppose we have an experiment M(X, \u0398, p\u03b8 (x), \u03bc(x)) and a maximal ancillary statistic T (x). T is ancillary if p\u03b8 (T (x)) is independent of \u03b8. Then our inference about \u03b8 should\nbe done through the conditional probability p\u03b8 (x|T (x)).\n\nCO was studied, among others, by Cox (1958) and Barndorff-Nielsen (1971, 1973). Birnbaum (1962)\nproved that the Sufficiency principle S together with the Conditionality principle CO implies\nthe Likelihood principle L.\n\n2.4.4. The Partial Nonformation principles PN\nPS [Partial Sufficiency principles] : Let T (x) be a partial sufficient statistic, in some\nspecified sense, like B-, S-, M-, K-, I- or L-sufficiency.\n\nSee Barndorff-Nielsen (1971),\n\nR\u00e9mon (1984), Cano Sanchez et al. (1989) or Jorgensen (1993) for definitions of partial\nsufficiency. All the Partial Sufficiency principles state that one gets the same inferential\ninformation about some parameter of interest from the knowledge of \"xo \" or \"T (xo )\", and\nthat one has to do inference through the marginal distribution P\u03b8 (T (x)).\n\nEquivalent Partial Conditionality principles PC require that our inference should be done\nthrough the conditional distribution given the observation of some B-, S-, ... ancillary statistic. Barndorff-Nielsen (1978) introduced the concept of nonformation which generalizes\nboth notions of partial sufficiency and partial ancillarity, and leads to Partial Nonformation\nprinciples PN .\n\n2.5. Summary\nAll these statistical principles can be summarized in three principles :\n\n\f8\n\nM. R\u00e9mon\n\n\u2022 The Invariance principle I about the choice of parameterization;\n\u2022 The Likelihood principle L about the choice for the reference sample space, which\nis equivalent to the Censoring CE or Stopping Rule principle ST together with the\nSufficiency principle S;\n\u2022 The Reduction principle R which generalizes the Partial Nonformation principles PN\nabout the kind of information one has to consider in the data (\"xo \" or \"T (xo )\" ?).\nThe next section will discuss the logic of ignorance versus structural information with respect\nto the best choice for a support measure over the hypotheses space \u0398.\n\n3. Is there a good choice for a support measure with respect to the hypotheses ?\n3.1. Introduction\nIn this paper, we choose the general terms of support measure to express the support the\nobservation data give to some unknown hypothesis. When looking for support measures\nin the theories of ignorance or uncertainty, one finds a lot of propositions. Let us just\nmention here the ancient Laplace's (1812) inverse probability theory (see Dale (1999) or\nFienberg (2006)), Dempster-Shafer's belief function (Shafer, 1976), the classical Bayesian\na posteriori probability, the structural inference (Fraser, 1968), the theory of possibility\n(Zadeh, 1978; Dubois and Prade, 2007), the plausibility measures (Friedman and Halpern, 1995)\nor the recent general uncertainty theory (Zadeh, 2005).\n\n3.2. The case of the non informative Bayesian priors\nIt is well known that additive priors, as proposed by the Bayesian theory, are not suitable\nfor expressing absence of knowledge about hypotheses. See Shafer (1976) : if we have no\n\n\f\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and efficient nature of Statistics\n\n9\n\ninformation about three hypotheses H1 , H2 and H3 , we cannot say that we have a better\nknowledge about H1 \u222a H2 with respect to H3 because we can add these small pieces of\n(non)information. Another example is the one proposed by Bernardo (1979) : we toss a\ncoin and we wish to do inference about its bias through the parameter of interest \u03c6 = |\u03b8\u2212 21 |,\nwhere \u03b8 is the probability of observing \"Head\". We know that the coin is either fair\n(H1 : \u03b8 =\n\n1\n2 ),\n\ndouble-headed (H2 : \u03b8 = 1) or double-tailed (H3 : \u03b8 = 0). We observe\n\nx0 =\"Head\" \u222a \"Tail\", i.e. we have no information coming from the data. The likelihood\nfunction is then l(\u03b8|x0 ) = 1 \u2200\u03b8 \u2208 \u0398. If we express our ignorance about \u03b8 through an\nadditive uniform measure : p(Hi ) =\n\n1\n3, i\n\n= 1, 2, 3, we therefore state that the hypothesis\n\nH1 : \u03c6 = 0 is twice less likely than the hypothesis HA : \u03c6 6= 0. This contradicts the situation\nof ignorance.\n\nDawid et al. (1973) and Stone (1976) have proposed many paradoxes against the additive\nnature of the Bayesian prior, especially in the context of lack of information. Jeffreys (1939)\nworked a lot to find non informative Bayesian priors. In his paper about the history of\nBayesian Inference, Fienberg (2006) writes that trying to 'derive \"objective\" priors that\nexpressed ignorance or lack of knowledge' is like trying 'to grasp the holy grail that had\neluded statisticians since the days of Laplace'. In fact, we can broaden the scope of the\nincompatibility between ignorance and additivity, to situations where partial information\nis available, i.e. to any kind of support measure.\n\n3.3. The incompatibility between \"additivity\" and the logic of ignorance\nOur knowledge (a priori or a posteriori) about the \"true\" unknown hypothesis \u03b80 \u2208 \u0398 can\nbe in some way informative. This does not mean that our support measure about this\nhypothesis behaves like a probability measure. Let us define the support measure describing\n\n\f10\n\nM. R\u00e9mon\n\nthe likelihood the observation E gives to the hypothesis \u03b8 \u2208 \u03981 by S[\u03b8 \u2208 \u03981 |E]. A support\nmeasure, like any plausibility measure (Friedman and Halpern, 1995), has to satisfy three\n\"axioms\" :\n\n\u2022\n\nS[\u03b8 \u2208 \u03981 |E] = 0 if E \u21d2 (\u03b8 6\u2208 \u03981 )\n\n\u2022\n\nS[\u03b8 \u2208 \u03981 |E] = 1 if E \u21d2 (\u03b8 \u2208 \u03981 )\n\n\u2022\n\n\u03982 \u2286 \u03981 \u21d2 S[\u03b8 \u2208 \u03982 |E] \u2264 S[\u03b8 \u2208 \u03981 |E]\n[monotonicity of the support function]\n\nThe problem for choosing a support measure on \u0398 is that this measure should always handle\na part of ignorance. Indeed, even when it is an a posteriori support measure over \u0398, there\nwill be hypotheses \u03b8i with equivalent support from the observed data (through the likelihood function, for instance), and the support measure will have to manage this ignorance\nbetween these \u03b8i . Once again, like in the Bernardo's coin example, this cannot be done\nby an additive support measure. Let us prove this incompatibility as a consequence of the\nInvariance I and Likelihood L principles.\n\nSuppose that we express our statistical information about \u03b8 by means of an additive posterior support measure S[\u03b8|E]. Because of the Invariance I and Likelihood L, two \u03b8-values, \u03b81\nand \u03b82 , having the same relative likelihood cannot be distinguished. To prove that, one has\njust to consider the function g(\u03b8) used in I as the permutation of \u03b81 and \u03b82 . I implies that\n\u03bc1 \u2261 S[\u03b81 |E] = S[\u03b82 |E] \u2261 \u03bc2 . The logic of ignorance requires equivalent inference for \u03b81 \u222a \u03b82\nas for \u03b81 . Considering S[\u03b8|E] as additive, one gets : S[\u03b81 \u222a \u03b82 |E] = \u03bc1 + \u03bc2 = S[\u03b81 |E] = \u03bc1 .\nHence, \u03bc1 = \u03bc2 = 0, which is far from convincing. All this reasoning about the consequences\nof I and L was already mentioned, in similar terms, by Hartigan (1967).\n\n\f\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and efficient nature of Statistics\n\n11\n\nAs it is clear that additive support measures are incompatible with I and L, one can think\nthat non-additive support measures, like the ones proposed in the possibility theory, will be\nthe correct choice. The next section shows that support measures built on maximization\n(or minimization) are also to be questioned.\n\n3.4. The case of the possibility measure and its \"Maxitivity\" property\nThe theory of possibility, as well as the theory of plausibility, proposes measures defined\nin terms of maximization or minimization. Dubois and Prade (2007) have introduced the\npretty terms of \"Maxitivity\" and \"Maxitive measure\" in reference to the additivity property\nof probability measures. For instance, the possibility measure for the state A \u2286 S is denoted\nby \u03a0(A) and defined by :\n\n\u03a0(A)\n\n=\n\nsup \u03c0(s)\ns\u2208A\n\nwhere \u03c0 : S \u2192 [0, 1] is a possibility distribution for the states s \u2208 S\n\nA necessity measure can be defined for A \u2286 S by N (A) = inf s\u2208A (1 \u2212 \u03c0(s)) = 1 \u2212 \u03a0(A).\nOne get the following \"maxitivity\" properties :\n\n\u03a0(A \u222a B) =\n\nmax(\u03a0(A), \u03a0(B))\n\nN (A \u2229 B) =\n\nmin(N (A), N (B))\n\nSee Dubois and Prade (2007) and Sigarretta et al. (2007) for detailed explanations about\npossibility and plausibility measures. Let us note that \u03a0\u2217 (A) \u2261 (\u03a0(A) + N (Ac ))/2 is\nstill a possibility measure with the additional properties that \u03a0\u2217 (A) = 0 is equivalent to\nthe impossibility of A, and \u03a0\u2217 (A) = 1 to the certainty of A. This can be interesting for\ncomparison with a posteriori Bayesian probability, but it will not be used here.\n\n\f12\n\nM. R\u00e9mon\n\n3.5. The impossibility of a \"maxitive\" support measure satisfying I, L and R\nLet us define our support measure in the framework of the theory of possibility, but in\nrelation to the relative likelihood function l(\u03b8; E) :\n\nS[\u03b8 \u2208 \u03980 |E]\n\nsup l(\u03b8; E)\n\n=\n\n\u03b8\u2208\u03980\n\nsup p\u03b8 (E)\n\u03b8\u2208\u03980\n\n=\n\nsup p\u03b8 (E)\n\u03b8\u2208\u0398\n\nIt is clear that such a possibility measure satisfies the Invariance and Likelihood principles.\nHowever, the Reduction principle is not satisfied. Indeed, such a support measure based on\nthe sole likelihood function is incompatible with the Reduction principle, as far as partial\nsufficiency principle is concerned. Let us consider the Bernardo's (1979) coin example again.\nRemember that our parameter of interest is \u03c6 = |\u03b8\u2212 21 | where \u03b8 is the probability of observing\n\"Head\", and that the coin is known to be either fair (H1 : \u03b8 = 21 ), double-headed (H2 : \u03b8 = 1)\nor double-tailed (H3 : \u03b8 = 0). This time, we observe x0 =\"Head\". The likelihood function\nis l(\u03b8|x0 ) = \u03b8 \u2200\u03b8 \u2208 \u0398. So, by definition of our support measure, one gets :\n\nS[\u03b8|\"Head\"] = 1 \u2212 S[\u03b8|\"Tail\"] =\n\n\u03b8\n\nS[H1 |\"Head\"] = S[H1 |\"Tail\"] =\n\n1\n2\n\n[The support measure for fairness]\n\nS[H2 \u222a H3 |\"Head\"] = S[H2 \u222a H3 |\"Tail\"] =\n\n1\n\n[The support measure for unfairness]\n\nWe see that the likelihood, as well as our \"maxitive\" support measure, puts its highest\nsupport towards the unfairness of the coin, whatever the first toss gives as a result. We\nsee also that there is an invariant structure in the model, concerning our parameter of\ninterest. Indeed, the minimal G-sufficient statistic with respect to \u03c6, see Barnard (1963), is\nT (\"Head\") = T (\"Tail\") = T (\"Head\" or \"Tail\"). Thus, from R (or PS) and the fact that\n\n\f\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and efficient nature of Statistics\n\n13\n\nthe Marginal likelihood l(\u03b8|\"Head\" or \"Tail\") = 1 \u2200\u03b8 \u2208 \u0398, we should have :\nR\n\nS[H1 |\"Head\"] = S[H1 |\"Tail\"] = S[H1 |\"Head\" or \"Tail\"] = 1\nR\n\nS[H2 \u222a H3 |\"Head\"] = S[H2 \u222a H3 |\"Tail\"] = S[H2 \u222a H3 |\"Head\" or \"Tail\"] = 1\nThis is clearly a better situation in terms of inferential support, as the first toss of a coin\ngives no information at all about the fairness or unfairness of a coin.\n\nThis example shows the impossibility for a \"maxitive\" support measure to satisfy the Reduction principle, as this last one, through structural invariance and partial sufficiency,\nintroduces Marginal likelihood function in the scene of inference. And therefore, an additive operation in terms of likelihood. Moreover, as we observed in the coin example, single\nand marginal likelihood functions can express totally different support with respect to the\nhypotheses. Which one should we prefer ? Our choice will, de facto, contradict either the\nReduction or the Likelihood principle.\n\n3.6. Summary\nIn this section, we have seen that neither the additivity nor the maxitivity approach is\n\"the\" solution for our support measure. The first one cannot handle properly the ignorance\npresent in any statistical problem, while the second one cannot cope with its structural\ninvariance (for instance, a location-scale structure emerging with the asymptotic normal\nmodel when the number of observations increases).\n\u2022 The additive Bayesian posterior approach satisfies the Likelihood L and Reduction R\nprinciples, but not the Invariance I principle. R will be valid under the condition that\na reference parameterization is chosen as well as a proper prior distribution over the\nparameter space \u0398. This extra information is required if paradoxes are to be avoided\n\n\f14\n\nM. R\u00e9mon\n\n(Dawid et al. (1973); Stone (1976)).\n\n\u2022 The maxitive Maximized or Profile Likelihood approach (Barndorff-Nielsen and Cox (1994))\nsatisfies the Invariance I and Likelihood L principles, but not the Reduction R principle. The Generalized Likelihood Ratio tests are based on this type of support measure,\nas well as the Maximum Likelihood point estimation. The extra information needed\nhere to avoid paradoxes is the long-run behavior of the model, as well as its structural\ninvariance (Stein (1956); Barnard (1965); Berger and Wolpert (1988)).\n\n\u2022 The mixed additive-maxitive Marginal or Conditional Inference approaches have not\nyet been considered in this paper. The Marginal (or Conditional) Likelihood approach is defined in the same way as the Profile Likelihood approach, but using the\nmarginal [respectively conditional] likelihood function l(\u03b8; T (x)) [l(\u03b8; x|T (x))] instead\nof the simple likelihood function l(\u03b8; x), as we did in Bernardo's example. The Invariance I and Reduction R principles will be satisfied here, but not the Likelihood\nL principle. The problem here is the definition of what is a partial nonformative\nstatistic T (x) (Barndorff-Nielsen (1978); R\u00e9mon (1984); Cano Sanchez et al. (1989);\nZhu and Reid (1994); Barndorff-Nielsen and Cox (1994)). The use of a marginal or\nconditional likelihood function requires extra information as the knowledge of the stopping rule because the marginal or conditional density can differ from one stopping rule\nto another. L is no longer valid for this type of inference.\n\nBayesian, Profile Likelihood and Marginal/Conditional Likelihood inferences are three major approaches corresponding to the possible two-by-two combinations of our general principles. Other inference methods can be classified in the same way, depending on the list of\nprinciples they satisfy. But none will be able to satisfy all these principles, as there is an\n\n\f\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and efficient nature of Statistics\n\n15\n\ninternal incompatibility between them. This incompatibility can be seen as a dilemma between an additive or a maxitive approach for dealing with the ignorance and the structural\ninformation contained in the data.\n\n4. Conclusions : the dilemma between \"Additivity\" and \"Maxitivity\"\n\nOur point of view is that discussion about Statistical Schools of Inference should not focus\nso much on the kind of principle one keeps or rejects, or even by-passes thanks to some well\nchosen extra information. Indeed, any inference theory seems to miss some information,\nas extra information is always needed to avoid paradoxes. Statisticians should be more\naware of and worried by the mathematical properties of the support measure they wish to\nuse. Here comes the dilemma between the \"additivity\" and the \"maxitivity\" of our support\nmeasure.\n\nOne can think that most statisticians will prefer an additive approach, by similarity with\nthe probability theory, but this is not so clear. Indeed, the core of the point estimation\nis done in a maxitive environment. And if they have to compare hypotheses, they will\nnormally use likelihood ratios, which are based on maxitive support measures. We think\nthat neither the maxitive nor the additive approach should be promoted as the sole possible\napproach.\n\nOur point of view is that statisticians should use both perspectives, in a dialogal process, like\nin the Marginal/Conditional Likelihood approach. EM algorithm (Dempster et al. (1977))\nis also a good example of this combined use of \"maxitive\" and \"additive\" operators. This\ndouble nature of the support measure is, for us, the characteristic of statistical inference,\nas statisticians should consider themselves as staying in the middle of the road, trying\n\n\f16\n\nM. R\u00e9mon\n\nto reconcile the logic of ignorance (related to \"Maxitivity\") and the logic of information\n(linked to \"Additivity\"). This is also the source of the efficiency of many statistical ad hoc\nmethods.\n\nReferences\nBarnard, G. A. (1963). Some logical aspects of the fiducial argument. Statist. Soc. Ser.\nB 25, 111\u2013115.\nBarnard, G. A. (1965). Misleading likelihood functions. Communication to Prof. D.R. Cox.\nBarnard, G. A. (1967). The use of likelihood function in statistical practice. In Porc. Fifth\nBerkeley Symp., Volume 1, pp. 27\u201340.\nBarnard, G. A. (1973). On likelihood (with discussion). In O. Barndorff-Nielsen and al.\n(Eds.), Conference on Foundational Questions in Statistical Inference, Volume 1 of Aarhus\nMemoirs, pp. 355\u2013366.\nBarndorff-Nielsen, O. E. (1973). Exponential Families and Conditioning. New York: Wiley.\nBarndorff-Nielsen, O. E. (1978). Information and Exponential Families in Statistical Theory.\nChichester: Wiley.\nBarndorff-Nielsen, O. E. (nov. 1971). On conditional statistical inference. Technical report,\nAarhus report.\nBarndorff-Nielsen, O. E. and D. R. Cox (1994). Inference and Asymptotics. Monographs\non Statistics and Applied Probability. London: Chapman and Hall.\nBasu, D. (1973). Statistical information and likelihood. In O. E. Barndorff-Nielsen and al.\n\n\f\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and efficient nature of Statistics\n\n17\n\n(Eds.), Conference on foundational questions in statistical inference, Volume 1 of Aarhus\nMemoirs, pp. 139\u2013247.\nBerger, J. O. (1985). In defense of the likelihood principle, axiomatics and coherency. In\nL. D. V. Bernardo J., De Groot M. A. and S. A. F. M. (Eds.), Bayesian Statistics,\nVolume 2, pp. 33\u201366. New York: North Holland.\nBerger, J. O. and R. Wolpert (1988). The Likelihood Principle (2nd ed.), Volume 9 of IMS\nMonograph. California: Hayward.\nBernardo, J. (1979). Reference posterior distribution for bayesian inference (with discussion). J. Roy. Statist. Soc. Ser. B 41, 113\u2013147.\nBirnbaum, A. (1962). On the foundations of statistical inference. J. Amer. Statist. Ass. 57,\n269\u2013326.\nBirnbaum, A. (1964). The anomalous concept of statistical evidence. Technical Report\nIMM NYU 332, Courant Institute of Math. Sciences, New York University.\nCano Sanchez, J. A., A. Hern\u00e1ndez Bastida, and E. Moreno Bas (1989). On l-sufficiency\nconcept of partial sufficiency. Statistica 49, 519\u2013528.\nCox, D. R. (1958). Some problems connected with statistical inference.\n\nAnn. Math.\n\nStatist. 29, 357\u2013372.\nDale, A. I. (1999). A History of Inverse Probability from Thomas Bayes to Karl Pearson.\nSpringer-Verlag.\nDawid, A. P. (1977). Conformity of inference patterns. In Barra and al. (Eds.), Recent\nDevelopments in Statistics, pp. 245\u2013256. Amsterdam: North Holland.\n\n\f18\n\nM. R\u00e9mon\n\nDawid, A. P., M. Stone, and J. Zidek (1973). Marginalization paradoxes in bayesian and\nstructural inference (with discussion). J. Roy. Statist. Soc. Ser. B 35, 189\u2013233.\nDempster, A. P., N. M. Laird, and D. B. Rubin (1977). Maximum likelihood from incomplete\ndata with the em algorithm (with discussion). J. Roy. Statist. Soc. Ser. B 39, 1\u201338.\nDubois, D. and H. Prade (2007). Possibility theory. Scholarpedia 2 (10), 2074.\nFienberg, S. E. (2006). When did bayesian inference become \"bayesian\" ?\n\nBayesian\n\nAnalysis 1 (1), 1\u201340.\nFisher, R. A. (1950). Two new properties of mathematical likelihood (1934). In Contributions to Mathematical Statistics. New York: Wiley.\nFraser, D. A. S. (1968). The Structure of Inference. New York: Wiley.\nFriedman, N. and J. Y. Halpern (1995). Plausibility measures : A user's guide. In Proc.\nEleventh Conf. on Uncertainty in Artificial Intelligence (UAI95).\nHartigan, J. (1967). The likelihood and invariance principles. J. Roy. Statist. Soc. Ser.\nB 29, 533\u2013539.\nJeffreys, H. (1939). Theory of Probability. Oxford University Press.\nJorgensen, B. (1993). The rules of conditional inference : is there a universal definition\nof nonformation ?\n\nTechnical report, The Department of Statistics, The University of\n\nBritish Columbia, Vancouver.\nKalbfleisch, J. D. and D. A. S. Sprott (1970). Application of likelihood methods to models\ninvolving large number of parameters (with discussion). J. Roy. Statist. Soc. Ser. B 32,\n175\u2013209.\n\n\f\"Additivity\" versus \"Maxitivity\" at the heart of the paradoxical and efficient nature of Statistics\n\n19\n\nLaplace, P. S. (1812). Th\u00e9orie Analytique des Probabilit\u00e9s. Courcier.\nPratt, J. W. (1962). In discussion of birmbaum (1962). J. Amer. Statist. Ass. 57, 314\u2013315.\nR\u00e9mon, M. (1984). On a concept of partial sufficiency : L\u2013sufficiency. International Statistical Review 52, 127\u2013135.\nShafer, G. (1976). A Mathematical Theory of Statistical Evidence. University Press, Princeton.\nSigarretta, J. M., P. Ruesga, and M. Rodriguez (2007). On mathematical foundations of\nthe plausibility theory. International Mathematical Forum 2 (27), 1319\u20131328.\nStein, C. (1956). Inadmissibility of the usual estimator for the mean of a multivariate\nnormal distribution. In 3rd Berkeley Symposium, Volume 1, pp. 197\u2013206.\nStone, M. (1976). Strong inconstency from uniform priors (with discussion). J. Amer.\nStatist. Ass. 71, 114\u2013125.\nVon Mises, R. (1939). Probability, Statistics and Truth. Hodge and Co. Press. Translated\nby Neyman, Sholl and Rabinowitsch.\nZadeh, L. (1978). Fuzzy sets as the basis for a theory of possibility. Fuzzy Sets and Systems 1,\n3\u201328.\nZadeh, L. A. (2005). Towards a generalized theory of uncertainty (gtu) - an outline. Information Sciences 172, 1\u201340.\nZhu, Y. and N. Reid (1994). Information, ancillarity and sufficiency in the presence of\nnuisance of parameters. Canadian J. Stat. 22, 111\u2013123.\n\n\f"}