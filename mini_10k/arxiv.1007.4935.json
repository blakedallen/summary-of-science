{"id": "http://arxiv.org/abs/1007.4935v3", "guidislink": true, "updated": "2011-01-01T15:12:13Z", "updated_parsed": [2011, 1, 1, 15, 12, 13, 5, 1, 0], "published": "2010-07-28T12:02:39Z", "published_parsed": [2010, 7, 28, 12, 2, 39, 2, 209, 0], "title": "Optimal Base Encodings for Pseudo-Boolean Constraints", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1007.1546%2C1007.4955%2C1007.4766%2C1007.5092%2C1007.1859%2C1007.5128%2C1007.4302%2C1007.2501%2C1007.2044%2C1007.1835%2C1007.1911%2C1007.3399%2C1007.4417%2C1007.5331%2C1007.4434%2C1007.3991%2C1007.3766%2C1007.1674%2C1007.2884%2C1007.3218%2C1007.4402%2C1007.0238%2C1007.3380%2C1007.5225%2C1007.3694%2C1007.3120%2C1007.3234%2C1007.5353%2C1007.5056%2C1007.4204%2C1007.2310%2C1007.1876%2C1007.3836%2C1007.3742%2C1007.0632%2C1007.3313%2C1007.2989%2C1007.3199%2C1007.4992%2C1007.4517%2C1007.4222%2C1007.5425%2C1007.0977%2C1007.2245%2C1007.3021%2C1007.2029%2C1007.4545%2C1007.0442%2C1007.0024%2C1007.0646%2C1007.1598%2C1007.2823%2C1007.0535%2C1007.3666%2C1007.3564%2C1007.2609%2C1007.5169%2C1007.3811%2C1007.3878%2C1007.0957%2C1007.4618%2C1007.3875%2C1007.2071%2C1007.1011%2C1007.1532%2C1007.3884%2C1007.0930%2C1007.4574%2C1007.0374%2C1007.5272%2C1007.1745%2C1007.4848%2C1007.1579%2C1007.0025%2C1007.4639%2C1007.3804%2C1007.4973%2C1007.0122%2C1007.3366%2C1007.3122%2C1007.3192%2C1007.0743%2C1007.4150%2C1007.4669%2C1007.0281%2C1007.0421%2C1007.1099%2C1007.0574%2C1007.4935%2C1007.0063%2C1007.5168%2C1007.0924%2C1007.0630%2C1007.5450%2C1007.2963%2C1007.2478%2C1007.0783%2C1007.0498%2C1007.0404%2C1007.0332%2C1007.2210&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Optimal Base Encodings for Pseudo-Boolean Constraints"}, "summary": "This paper formalizes the optimal base problem, presents an algorithm to\nsolve it, and describes its application to the encoding of Pseudo-Boolean\nconstraints to SAT. We demonstrate the impact of integrating our algorithm\nwithin the Pseudo-Boolean constraint solver MINISAT+. Experimentation indicates\nthat our algorithm scales to bases involving numbers up to 1,000,000, improving\non the restriction in MINISAT+ to prime numbers up to 17. We show that, while\nfor many examples primes up to 17 do suffice, encoding with respect to optimal\nbases reduces the CNF sizes and improves the subsequent SAT solving time for\nmany examples.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1007.1546%2C1007.4955%2C1007.4766%2C1007.5092%2C1007.1859%2C1007.5128%2C1007.4302%2C1007.2501%2C1007.2044%2C1007.1835%2C1007.1911%2C1007.3399%2C1007.4417%2C1007.5331%2C1007.4434%2C1007.3991%2C1007.3766%2C1007.1674%2C1007.2884%2C1007.3218%2C1007.4402%2C1007.0238%2C1007.3380%2C1007.5225%2C1007.3694%2C1007.3120%2C1007.3234%2C1007.5353%2C1007.5056%2C1007.4204%2C1007.2310%2C1007.1876%2C1007.3836%2C1007.3742%2C1007.0632%2C1007.3313%2C1007.2989%2C1007.3199%2C1007.4992%2C1007.4517%2C1007.4222%2C1007.5425%2C1007.0977%2C1007.2245%2C1007.3021%2C1007.2029%2C1007.4545%2C1007.0442%2C1007.0024%2C1007.0646%2C1007.1598%2C1007.2823%2C1007.0535%2C1007.3666%2C1007.3564%2C1007.2609%2C1007.5169%2C1007.3811%2C1007.3878%2C1007.0957%2C1007.4618%2C1007.3875%2C1007.2071%2C1007.1011%2C1007.1532%2C1007.3884%2C1007.0930%2C1007.4574%2C1007.0374%2C1007.5272%2C1007.1745%2C1007.4848%2C1007.1579%2C1007.0025%2C1007.4639%2C1007.3804%2C1007.4973%2C1007.0122%2C1007.3366%2C1007.3122%2C1007.3192%2C1007.0743%2C1007.4150%2C1007.4669%2C1007.0281%2C1007.0421%2C1007.1099%2C1007.0574%2C1007.4935%2C1007.0063%2C1007.5168%2C1007.0924%2C1007.0630%2C1007.5450%2C1007.2963%2C1007.2478%2C1007.0783%2C1007.0498%2C1007.0404%2C1007.0332%2C1007.2210&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper formalizes the optimal base problem, presents an algorithm to\nsolve it, and describes its application to the encoding of Pseudo-Boolean\nconstraints to SAT. We demonstrate the impact of integrating our algorithm\nwithin the Pseudo-Boolean constraint solver MINISAT+. Experimentation indicates\nthat our algorithm scales to bases involving numbers up to 1,000,000, improving\non the restriction in MINISAT+ to prime numbers up to 17. We show that, while\nfor many examples primes up to 17 do suffice, encoding with respect to optimal\nbases reduces the CNF sizes and improves the subsequent SAT solving time for\nmany examples."}, "authors": ["Michael Codish", "Yoav Fekete", "Carsten Fuhs", "Peter Schneider-Kamp"], "author_detail": {"name": "Peter Schneider-Kamp"}, "author": "Peter Schneider-Kamp", "links": [{"href": "http://arxiv.org/abs/1007.4935v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1007.4935v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1007.4935v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1007.4935v3", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Optimal Base Encodings for\nPseudo-Boolean Constraints?\nMichael Codish1 , Yoav Fekete1 , Carsten Fuhs2 , and Peter Schneider-Kamp3\n\narXiv:1007.4935v3 [cs.DM] 1 Jan 2011\n\n1\n\nDepartment of Computer Science, Ben Gurion University of the Negev, Israel\n2\nLuFG Informatik 2, RWTH Aachen University, Germany\n3\nIMADA, University of Southern Denmark, Denmark\n\nAbstract. This paper formalizes the optimal base problem, presents\nan algorithm to solve it, and describes its application to the encoding of Pseudo-Boolean constraints to SAT. We demonstrate the impact of integrating our algorithm within the Pseudo-Boolean constraint\nsolver MiniSat+ . Experimentation indicates that our algorithm scales\nto bases involving numbers up to 1,000,000, improving on the restriction\nin MiniSat+ to prime numbers up to 17. We show that, while for many\nexamples primes up to 17 do suffice, encoding with respect to optimal\nbases reduces the CNF sizes and improves the subsequent SAT solving\ntime for many examples.\n\n1\n\nIntroduction\n\nThe optimal base problem is all about finding an efficient representation for a\ngiven collection of positive integers. One measure for the efficiency of such a\nrepresentation is the sum of the digits of the numbers. Consider for example\nthe decimal numbers S = {16, 30, 54, 60}. The sum of their digits is 25. Taking\nbinary representation we have S(2) = {10000, 11110, 110110, 111100} and the\nsum of digits is 13, which is smaller. Taking ternary representation gives S(3) =\n{121, 1010, 2000, 2020} with an even smaller sum of digits, 12. Considering the\nmixed radix base B = h3, 5, 2, 2i, the numbers are represented as S(B) = {101,\n1000, 1130, 10000} and the sum of the digits is 9. The optimal base problem is to\nfind a (possibly mixed radix) base for a given sequence of numbers to minimize\nthe size of the representation of the numbers. When measuring size as \"sum\nof digits\", the base B is indeed optimal for the numbers of S. In this paper we\npresent the optimal base problem and illustrate why it is relevant to the encoding\nof Pseudo-Boolean constraints to SAT. We also present an algorithm and show\nthat our implementation is superior to current implementations.\nPseudo-Boolean constraints take the form a1 x1 +a2 x2 +* * *+an xn \u2265 k, where\na1 , . . . , an are integer coefficients, x1 , . . . , xn are Boolean literals (i.e., Boolean\nvariables or their negation), and k is an integer. We assume that constraints are\nin Pseudo-Boolean normal form [3], that is, the coefficients ai and k are always\npositive and Boolean variables occur at most once in a1 x1 + a2 x2 + * * * + an xn .\n?\n\nSupported by GIF grant 966-116.6 and the Danish Natural Science Research Council.\n\n\fPseudo-Boolean constraints are well studied and arise in many different contexts,\nfor example in verification [6] and in operations research [5]. Typically we are\ninterested in the satisfiability of a conjunction of Pseudo-Boolean constraints.\nSince 2005 there is a series of Pseudo-Boolean Evaluations [10] which aim to\nassess the state of the art in the field of Pseudo-Boolean solvers. We adopt these\ncompetition problems as a benchmark for the techniques proposed in this paper.\nPseudo-Boolean constraint satisfaction problems are often reduced to SAT.\nMany works describe techniques to encode these constraints to propositional formulas [1, 2, 8]. The Pseudo-Boolean solver MiniSat+ ([8], cf. http://minisat.\nse) chooses between three techniques to generate SAT encodings for PseudoBoolean constraints. These convert the constraint to: (a) a BDD structure, (b) a\nnetwork of sorters, and (c) a network of (binary) adders. The network of adders\nis the most concise encoding, but it has the weakest propagation properties and\noften leads to higher SAT solving times than the BDD based encoding, which, on\nthe other hand, generates the largest encoding. The encoding based on sorting\nnetworks is often the one applied and is the one we consider in this paper.\nTo demonstrate how sorters can be used to\nx5\ny8\nx5\ny7\ntranslate Pseudo-Boolean constraints, consider the\nx5\ny6\nconstraint \u03c8 = x1 + x2 + x3 + 2x4 + 3x5 \u2265 4 where\nx4\ny5\nx4\ny4 = 1\nthe sum of the coefficients is 8. On the right, we\nx3\ny3 = 1\nillustrate an 8 \u00d7 8 sorter where x1 , x2 , x3 are each\nx2\ny2 = 1\nx1\ny1 = 1\nfed into a single input, x4 into two of the inputs,\nand x5 into three of the inputs. The outputs are\ny1 , . . . , y8 . First, we represent the sorting network as a Boolean formula, \u03c6, which\nin general, for n inputs, will be of size O(n log2 n) [4]. Then, to assert \u03c8 we take\nthe conjunction of \u03c6 with the formula y1 \u2227 y2 \u2227 y3 \u2227 y4 .\nBut what happens if the coefficients in a constraint are larger than in this\nexample? How should we encode 16x1 + 30x2 + 54x3 + 30x4 + 60x5 \u2265 87? How\nshould we handle very large coefficients (larger than 1,000,000)? To this end, the\nauthors in [8] generalize the above idea and propose to decompose the constraint\ninto a number of interconnected sorting networks. Each sorter represents a digit\nin a mixed radix base. This construction is governed by the choice of a suitable\nmixed radix base and the objective is to find a base which minimizes the size of\nthe sorting networks. Here the optimal base problem comes in, as the size of the\nnetworks is directly related to the size of the representation of the coefficients.\nWe consider the sum of the digits (of coefficients) and other measures for the size\nof the representations and study their influence on the quality of the encoding.\nIn MiniSat+ the search for an optimal base is performed using a brute force\nalgorithm and the resulting base is constructed from prime numbers up to 17.\nThe starting point for this paper is the following remark from [8] (Footnote 8):\nThis is an ad-hoc solution that should be improved in the future. Finding\nthe optimal base is a challenging optimization problem in its own right.\nIn this paper we take the challenge and present an algorithm which scales to find\nan optimal base consisting of elements with values up to 1,000,000. We illustrate\nthat in many cases finding a better base leads also to better SAT solving time.\n2\n\n\fSection 2 provides preliminary definitions and formalizes the optimal base\nproblem. Section 3 describes how MiniSat+ decomposes a Pseudo-Boolean constraint with respect to a given mixed radix base to generate a corresponding\npropositional encoding, so that the constraint has a solution precisely when\nthe encoding has a model. Section 4 is about (three) alternative measures with\nrespect to which an optimal base can be found. Sections 5\u20137 introduce our algorithm based on classic AI search methods (such as cost underapproximation) in\nthree steps: Heuristic pruning, best-first branch and bound, and base abstraction. Sections 8 and 9 present an experimental evaluation and some related work.\nSection 10 concludes. Proofs are given in the appendix.\n\n2\n\nOptimal Base Problems\n\nIn the classic base r radix system, positive integers are represented as finite\nsequences of digits d = hd0 , . . . , dk i where for each digit 0 \u2264 di < r, and for\nthe most significant digit, dk > 0. The integer value associated with d is v =\nd0 + d1 r + d2 r2 + * * * + dk rk . A mixed radix system is a generalization where a\nbase is an infinite radix sequence B = hr0 , r1 , r2 , . . .i of integers where for each\nradix, ri > 1 and for each digit, 0 \u2264 di < ri . The integer value associated with d\nis v = d0 w0 + d1 w1 + d2 w2 + * * * + dk wk where w0 = 1 and for i \u2265 0, wi+1 = wi ri .\nThe sequence weights(B) = hw0 , w1 , w2 , . . .i specifies the weighted contribution\nof each digit position and is called the weight sequence of B. A finite mixed radix\nbase is a finite sequence B = hr0 , r1 , . . . , rk\u22121 i with the same restrictions as for\nthe infinite case except that numbers always have k + 1 digits (possibly padded\nwith zeroes) and there is no bound on the value of the most significant digit, dk .\nIn this paper we focus on the representation of finite multisets of natural\nnumbers in finite mixed radix bases. Let Base denote the set of finite mixed\nradix bases and ms(N) the set of finite non-empty multisets of natural numbers.\nWe often view multisets as ordered (and hence refer to their first element, second\nelement, etc.). For a finite sequence or multiset S of natural numbers, we denote\nth\nits length by |S|, its maximal element\nQ by max(S), its i element by S(i),\nQ and the\nmultiplication of its elements by S (if S is the empty sequence then S = 1).\nIf a base consists of prime numbers only, then we say that it is a prime base.\nThe set of prime bases is denoted Base p .\nLet B \u2208 Base with |B| = k. We denote by v(B) = hd0 , d1 , . . . , dk i the representation of a natural number v in base B. The most significant digit of v(B) ,\ndenoted msd(v(B) ), is dk . If msd(v(B) ) = 0 then we say that B is redundant for\nv. Let S \u2208 ms(N) with |S| = n. We denote the n \u00d7 (k + 1) matrix of digits of\nelements from S in base B as S(B) . Namely, the ith row in S(B) is the vector\nS(i)(B) . The most significant digit column of S(B) is the k + 1 column of the\nT\n, then we say that B\nmatrix and denoted msd(S(B) ). If msd(S(B) ) = h0, . . . , 0iQ\nis redundant for S. This is equivalently characterized by B > max(S).\nDefinition 1 (non-redundant bases). Let\n\b S \u2208 ms(N).\nQ We denote the set\nB \u2264 max(S) . The\nof non-redundant bases for S, Base(S) = B \u2208 Base\nset of non-redundant prime bases for S is denoted Base p (S). The set of non3\n\n\fredundant (prime) bases for S, containing elements no larger than `, is denoted\nBase ` (S) (Base `p (S)). The set of bases in Base(S)/Base ` (S)/Base `p (S), is often\nviewed as a tree with root h i (the empty base) and an edge from B to B 0 if and\nonly if B 0 is obtained from B by extending it with a single integer value.\nDefinition 2 (sum digits). Let S \u2208 ms(N) and B \u2208 Base. The sum of the\ndigits of the numbers from S in base B is denoted sum digits(S(B) ).\nExample 3. The usual binary \"base 2\" and ternary \"base 3\" are represented as\nthe infinite sequences B1 = h2, 2, 2, . . .i and B2 = h3, 3, 3, . . .i. The finite sequence\nB3 = h3, 5, 2, 2i and the empty sequence B4 = h i are also bases. The empty base\nis often called the \"unary base\" (every number in this base has a single digit).\nLet S = {16, 30, 54, 60}. Then, sum digits(S(B1 ) ) = 13, sum digits(S(B2 ) ) = 12,\nsum digits(S(B3 ) ) = 9, and sum digits(S(B4 ) ) = 160.\nLet S \u2208 ms(N). A cost function for S is a function cost S : Base \u2192 R which\nassociates bases with real numbers. An example is cost S (B) = sum digits(S(B) ).\nIn this paper we are concerned with the following optimal base problem.\nDefinition 4 (optimal base problem). Let S \u2208 ms(N) and cost S a cost\nfunction. We say that a base B is an optimal base for S with respect to cost S , if\nfor all bases B 0 , cost S (B) \u2264 cost S (B 0 ). The corresponding optimal base problem\nis to find an optimal base B for S.\nThe following two lemmata confirm that for the sum digits cost function, we\nmay restrict attention to non-redundant bases involving prime numbers only.\nLemma 5. Let S \u2208 ms(N) and consider the sum digits cost function. Then, S\nhas an optimal base in Base(S).\nLemma 6. Let S \u2208 ms(N) and consider the sum digits cost function. Then, S\nhas an optimal base in Base p (S).\nHow hard is it to solve an instance of the optimal base problem (namely,\nfor S \u2208 ms(N))? The following lemma provides a polynomial (in max(S)) upper\nbound on the size of the search space. This in turn suggests a pseudo-polynomial\ntime brute force algorithm (to traverse the search space).\nLemma 7. Let S \u2208 ms(N) with m = max(S). Then, Base(S) \u2264 m1+\u03c1 where\n\u03c1 = \u03b6 \u22121 (2) \u2248 1.73 and where \u03b6 is the Riemann zeta function.\nProof. Chor et al. prove in [7] that the number of ordered factorizations of a\nnatural number n is lessPthan n\u03c1 . The number of bases for all of the numbers in\nS is hence bounded by n\u2264m n\u03c1 , which is bounded by m1+\u03c1 .\n\n3\n\nEncoding Pseudo-Boolean Constraints\n\nThis section presents the construction underlying the sorter based encoding of\nPseudo-Boolean constraints applied in MiniSat+ [8]. It is governed by the choice\nof a mixed radix base B, the optimal selection of which is the topic of this paper.\nThe construction sets up a series of sorting networks to encode the digits, in base\n4\n\n\fB, of the sum of the terms on the left side of a constraint \u03c8 = a1 x1 +a2 x2 +* * *+\nan xn \u2265 k. The encoding then compares these digits with those from k(B) from\nthe right side. We present the construction, step by step, through an example\nwhere B = h2, 3, 3i and \u03c8 = 2x1 + 2x2 + 2x3 + 2x4 + 5x5 + 18x6 \u2265 23.\n\uf8eb\n\uf8f6\nStep one - representation in base:\n0100\n\nThe coefficients of \u03c8 form a multiset S = {2, 2, 2, 2, 5, 18}\nand their representation in base B, a 6 \u00d7 4 matrix, S(B) ,\ndepicted on the right. The rows of the matrix correspond\nto the representation of the coefficients in base B.\n\nS(B)\n\n\uf8ec 00 11 00 00 \uf8f7\n\uf8f7\n=\uf8ec\n\uf8ed0 1 0 0\uf8f8\n1200\n0001\n\nStep two - counting: Representing the coefficients as four digit numbers in\nbase B = h2, 3, 3i and considering the values weights(B) = h1, 2, 6, 18i of the\ndigit positions, we obtain a decomposition for the left side of \u03c8:\n2x1 + 2x2 + 2x3 + 2x4 + 5x5 + 18x6 =\n1 * (x5 ) + 2 * (x1 + x2 + x3 + x4 + 2x5 ) + 6 * (0) + 18 * (x6 )\nTo encode the sums at each digit position (1, 2, 6, 18), we set up a series of four\nsorting networks as depicted below. Given values for the variables, the sorted\noutputs from these netx1\nx2\nworks represented unary\nx3\nx4\nnumbers d1 , d2 , d3 , d4 such\nx5\nx5\nx5\nx6\nthat the left side of \u03c8\ntakes the value 1 * d1 + 2 *\ncount 10 s\ncount 20 s\ncount 60 s\ncount 180 s\nd2 + 6 * d3 + 18 * d4 .\nStep three - converting to base: For the outputs d1 , d2 , d3 , d4 to represent\nthe digits of a number in base B = h2, 3, 3i, we need to encode also the \"carry\"\noperation from each digit position to the next. The first 3 outputs must represent valid digits for B, i.e., unary numbers less than h2, 3, 3i respectively.\nIn our example the single potential violation to this restriction is d2 , which\nis represented in 6 bits. To this end we add two components to the encoding: (1) each third output of the second network (y3 and y6 in the diagram)\nis fed into the third network as an additional (carry) input; and (2) clauses\nare added to encode that the output of the second network is to be considered modulo 3. We call these additional clauses a normalizer. The normalizer\ndefines two outputs R =\nx1\ny6\ny6\nx2\ny5\nhr1 , r2 i and introduces\nx3\ny4\nx4\ny3\ny3\nclauses specifying that\nx5\ny2\nr1\nx5\nx5\ny1\nr2\nx6\nthe (unary) value of\n0\n0\n0\nR equals the (unary)\ncount 2 s\ncount 6 s\ncount 180 s\ncount 1 s\nvalue of d2 mod 3.\nStep four - comparison: The outputs from these four units now specify a\nnumber in base B, each digit represented in unary notation. This number is now\ncompared (via an encoding of the lexicographic order) to 23(B) (the value from\nthe right-hand side of \u03c8).\n5\n\n\f4\n\nMeasures of Optimality\n\nWe now return to the objective of this paper: For a given Pseudo-Boolean constraint, how can we choose a mixed radix base with respect to which the encoding\nof the constraint via sorting networks will be optimal? We consider here three\nalternative cost functions with respect to which an optimal base can be found.\nThese cost functions capture with increasing degree of precision the actual size\nof the encodings.\nThe first cost function, sum digits as introduced in Definition 2, provides a\ncoarse measure on the size of the encoding. It approximates (from below) the\ntotal number of input bits in the network of sorting networks underlying the\nencoding. An advantage in using this cost function is that there always exists an\noptimal base which is prime. The disadvantage is that it ignores the carry bits\nin the construction, and as such is not always a precise measure for optimality.\nIn [8], the authors propose to apply a cost function which considers also the\ncarry bits. This is the second cost function we consider and we call it sum carry.\nDefinition 8 (cost function: sum carry). Let S \u2208 ms(N), B \u2208 Base with\n|B| = k and S(B) = (aij ) the corresponding n \u00d7 (k + 1) matrix of digits. Denote the sequences\nPns\u0304 = hs0 , s1 , . . . , sk i (sums) and c\u0304 = hc0 , c1 , . . . , ck i (carries)\ndefined by: sj = i=1 aij for 0 \u2264 j \u2264 k, c0 = 0, and cj+1 = (sj +cj ) div B(j) for\nk\n0 \u2264 j \u2264 k. The \"sum of digits with\nX\ncarry\" cost function is defined by the\n(sj + cj )\nsum carry(S(B) ) =\nequation on the right.\nj=0\nThe following example illustrates the sum carry cost function and that it\nprovides a better measure of base optimality for the (size of the) encoding of\nPseudo-Boolean constraints.\nExample 9.\bConsider the encoding of a Pseudo-Boolean constraint with coefficients S = 1, 3, 4, 8, 18, 18 with respect to bases: B1 = h2, 3, 3i, B2 = h3, 2, 3i,\nand B3 = h2, 2, 2, 2i. Figure 1 depicts the sizes of the sorting networks for each\nof these bases. The upper tables illustrate the representation of the coefficients\nin the corresponding bases. In the lower tables, the rows labeled \"sum\" indicate\nthe number of bits per network and (to their right) their total sum which is the\nsum digits cost. With respect to the sum digits cost function, all three bases are\noptimal for S, with a total of 9 inputs. The algorithm might as well return B3 .\nThe rows labeled \"carry\" indicate the number of carry bits in each of the\nconstructions and (to their right) their totals. With respect to the sum carry\ncost function, bases B1 and B2 are optimal for S, with a total of 9 + 2 = 11 bits\nwhile B3 involves 9 + 5 = 14 bits. The algorithm might as well return B1 .\nThe following example shows that when searching for an optimal base with\nrespect to the sum carry cost function, one must consider also non-prime bases.\nExample 10. Consider again the Pseudo Boolean constraint \u03c8 = 2x1 + 2x2 +\n2x3 + 2x4 + 5x5 + 18x6 \u2265 23 from Section 3. The encoding with respect to\n6\n\n\fB1 = h2, 3, 3i\nS\n1\n3\n4\n8\n18\n18\nsum\ncarry\ncomp\n\n1's 2's\n1\n0\n1\n1\n0\n2\n0\n1\n0\n0\n0\n0\n2\n0\n1\n\n4\n1\n9\n\nB2 = h3, 2, 3i\n\n6's 18's\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n1\n\n2\n0\n1\n\nS\n1\n3\n4\n8\n18\n18\n9\n2\n12\n\nsum\ncarry\ncomp\n\n1's 3's\n1\n0\n0\n1\n1\n1\n2\n0\n0\n0\n0\n0\n4\n0\n5\n\nB3 = h2, 2, 2, 2i\n\n6's 18's\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n\n2\n1\n3\n\n1\n1\n1\n\n2\n0\n1\n\nS\n1\n3\n4\n8\n18\n18\n9\n2\n10\n\nsum\ncarry\ncomp\n\n1's 2's\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n2\n0\n1\n\n3\n1\n5\n\n4's\n0\n0\n1\n0\n0\n0\n\n8's 16's\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n\n1\n2\n3\n\n1\n1\n1\n\n2\n1\n3\n\n9\n5\n13\n\nFig. 1. Number of inputs/carries/comparators when encoding S = {1, 3, 4, 8, 18, 18}\nand three bases B1 = h2, 3, 3i, B2 = h3, 2, 3i, and B3 = h2, 2, 2, 2i .\n\nB1 = h2, 3, 3i results in 4 sorting networks with 10 inputs from the coefficients\nand 2 carries. So a total of 12 bits. The encoding with respect to B2 = h2, 9i is\nsmaller. It has the same 10 inputs from the coefficients but no carry bits. Base\nB2 is optimal and non-prime.\nWe consider a third cost function which we call the num comp cost function.\nSorting networks are constructed from \"comparators\" [9] and in the encoding\neach comparator is modeled using six CNF clauses. This function counts the\nnumber of comparators in the construction. Let f (n) denote the number of\ncomparators in an n \u00d7 n sorting network. For small values of 0 \u2264 n \u2264 8, f (n)\ntakes the values 0, 0, 1, 3, 5, 9, 12, 16 and 19 respectively which correspond to the\nsizes of the optimal networks of these sizes [9]. For larger values, the construction\nuses Batcher's odd-even sorting networks [4] for which f (n) = n * dlog2 ne *\n(dlog2 ne \u2212 1)/4 + n \u2212 1.\nDefinition 11 (cost function: num comp). Consider the same setting as in\nDefinition 8. Then,\nk\nX\nnum comp(S(B) ) =\nf (sj + cj )\nj=0\n\nExample 12. Consider again the setting of Example 9. In Figure 1 the rows labeled \"comp\" indicate the number of comparators in each of the sorting networks\nand their totals. The construction with the minimal number of comparators is\nthat obtained with respect to the base B2 = h3, 2, 3i with 10 comparators.\nIt is interesting to remark the following relationship between the three cost\nfunctions: The sum digits function is the most \"abstract\". It is only based on\nthe representation of numbers in a mixed radix base. The sum carry function\nconsiders also properties of addition in mixed-radix bases (resulting in the carry\nbits). Finally, the num comp function considers also implementation details of\nthe odd-even sorting networks applied in the underlying MiniSat+ construction.\nIn Section 8 we evaluate how the alternative choices for a cost function influence\nthe size and quality of the encodings obtained with respect to corresponding\noptimal bases.\n7\n\n\f5\n\nOptimal Base Search I: Heuristic Pruning\n\nThis section introduces a simple, heuristic-based, depth-first, tree search algorithm to solve the optimal base problem. The search space is the domain of\nnon-redundant bases as presented in Definition 1. The starting point is the brute\nforce algorithm applied in MiniSat+ . For a sequence of integers S, MiniSat+\napplies a depth-first traversal of Base 17\np (S) to find the base with the optimal\nvalue for the cost function cost S (B) = sum carry(S(B) ).\nOur first contribution is to introduce a heuristic function and to identify\nbranches in the search space which can be pruned early on in the search. Each\ntree node B encountered during the traversal is inspected to check if given the\nbest node encountered so far, bestB, it is possible to determine that all descendants of B are guaranteed to be less optimal than bestB. In this case, the\nsubtree rooted at B may be pruned. The resulting algorithm improves on the one\nof MiniSat+ and provides the basis for the further improvements introduced in\nSections 6 and 7. We need first a definition.\nDefinition 13 (base extension, partial cost, and admissible heuristic).\nLet S \u2208 ms(N), B, B 0 \u2208 Base(S), and cost S a cost function. We say that: (1)\nB 0 extends B, denoted B 0 \u001f B, if B is a prefix of B 0 , (2) \u2202cost S is a partial\ncost function for cost S if \u2200B 0 \u001f B. cost S (B 0 ) \u2265 \u2202cost S (B), and (3) hS is an\nadmissible heuristic function for cost S and \u2202cost S if \u2200B 0 \u001f B. cost S (B 0 ) \u2265\n\u2202cost S (B 0 ) + hS (B 0 ) \u2265 \u2202cost S (B) + hS (B).\nThe intuition is that \u2202cost S (B) signifies a part of the cost of B which will be a\npart of the cost of any extension of B, and that hS (B) is an under-approximation\non the additional cost of extending B (in any way) given the partial cost of B. We\ndenote cost \u03b1\nS (B) = \u2202cost S (B) + hS (B). If \u2202cost S is a partial cost function and\nhS is an admissible heuristic function, then cost \u03b1\nS (B) is an under-approximation\nof cost S (B). The next lemma provides the basis for heuristic pruning using the\nthree cost functions introduced above.\nLemma 14. The following are admissible heuristics for the cases when:\nP\n1. cost S (B) = sum digits(S(B) ): \u2202cost S (B) = cost S (B) \u2212 P msd(S(B) ).\n2. cost S (B) = sum carry(S(B) ): \u2202cost S (B) = cost S (B) \u2212 msd(S(B) ).\n3. cost S (B) = num comp(S(B) ): \u2202cost S (B) = cost S (B) \u2212 f (s|B| + c|B| ).\n\b\nQ\nIn the first two settings we take hS (B) =\nx\u2208S x\u2265 B .\nIn the case of num comp we take the trivial heuristic estimate hS (B) = 0\nThe algorithm, which we call dfsHP for depth-first search with heuristic pruning, is now stated as Figure 2 where the input to the algorithm is a multiset\nof integers S and the output is an optimal base. The algorithm applies a depthfirst traversal of Base(S) in search of an optimal base. We assume given: a cost\nfunction cost S , a partial cost function \u2202cost S and an admissible heuristic hS .\nWe denote cost \u03b1\nS (B) = \u2202cost S (B) + hS (B). The abstract data type base has\ntwo operations: extend(int) and extenders(multiset). For a base B and an\n8\n\n\f/*input*/\n/*init*/\n/*dfs*/\n\n/*output*/\n\nmultiset S\nbase bestB = h2, 2, ..., 2i\ndepth-first traverse Base(S)\nat each node B, for the next value p \u2208 B.extenders(S) do\nbase newB = B.extend(p)\nif (cost \u03b1\nS (newB) > cost S (bestB)) prune\nelse if (cost S (newB) < cost S (bestB)) bestB = newB\nreturn bestB;\n\nFig. 2. dfsHP: depth-first search for an optimal base with heuristic pruning\n\ninteger p, B.extend(p) is the base obtained by extending B by p. For a multiset\nS, B.extenders(S) is the set of integer valuesQp by which B can be extended\nto a non-redundant base for S, i.e., such that\nB.extend(p) \u2264 max(S). The\ndefinition of this operation may have additional arguments to indicate if we seek\na prime base or one containing elements no larger than `.\nInitialization (/*init*/ in the figure) assigns to the variable bestB a finite\nbinary base of size blog2 (max(S))c. This variable will always denote the best base\nencountered so far (or the initial finite binary base). Throughout the traversal,\nwhen visiting a node newB we first check if the subtree rooted at newB should be\npruned. If this is not the case, then we check if a better \"best base so far\" has\nbeen found. Once the entire (with pruning) search space has been considered,\nthe optimal base is in bestB.\nTo establish a bound on the complexity of the algorithm, denote the number of different integers in S by s and m = max(S). The algorithm has space\ncomplexity O(log(m)), for the depth first search on a tree with height bound by\nlog(m) (an element of Base(S) will have at most log2 (m) elements). For each\nbase considered during the traversal, we have to calculate costS which incurs\na cost of O(s). To see why, consider that when extending a base B by a new\nelement giving base B 0 , the first columns of S(B 0 ) are the same as those in S(B)\n(and thus also the costs incurred by them). Only the cost incurred by the most\nsignificant digit column of S(B) needs to be recomputed for S(B 0 ) due to base\nextension of B to B 0 . Performing the computation for this column, we compute a\nnew digit for the s different values in S. Finally, by Lemma 7, there are O(m2.73 )\nbases and therefore, the total runtime is O(s \u2217 m2.73 ). Given that s \u2264 m, we can\nconclude that runtime is bounded by O(m3.73 ).\n\n6\n\nOptimal Base Search II: Branch and Bound\n\nIn this section we further improve the search algorithm for an optimal base. The\nsearch algorithm is, as before, a traversal of the search space using the same\npartial cost and heuristic functions as before to prune the tree. The difference\nis that instead of a depth first search, we maintain a priority queue of nodes for\nexpansion and apply a best-first, branch and bound search strategy.\nFigure 3 illustrates our enhanced search algorithm. We call it B&B. The abstract data type priority queue maintains bases prioritized by the value of\n9\n\n\fbase findBase(multiset S)\n\b\n/*1*/\nbase bestB = h2, 2, ..., 2i; priority queue Q = h i ;\n/*2*/\nwhile (Q 6= {} && cost \u03b1\nS (Q.peek()) < cost S (bestB))\n/*3*/\nbase B = Q.popMin();\n/*4*/\nforeach (p \u2208 B.extenders(S))\n/*5*/\nbase newB = B.extend(p);\n/*6*/\nif (cost \u03b1\nS (newB) \u2264 cost S (bestB))\n/*7*/\n{ Q.push(newB); if (cost S (newB) < cost S (bestB)) bestB = newB; }\n/*8*/\nreturn bestB;\nFig. 3. Algorithm B&B: best-first, branch and bound\n\ncost \u03b1\nS . Operations popMin(), push(base) and peek() (peeks at the minimal entry) are the usual. The reason to box the text \"priority queue\" in the figure\nwill become apparent in the next section.\nOn line /*1*/ in the figure, we initialize the variable bestB to a finite binary\nbase of size blog2 (max(S))c (same as in Figure 2) and initialize the queue to\ncontain the root of the search space (the empty base). As long as there are still\nnodes to be expanded in the queue that are potentially interesting (line /*2*/),\nwe select (at line /*3*/) the best candidate base B from the frontier of the tree\nin construction for further expansion. Now the search tree is expanded for each\nof the relevant integers (calculated at line /*4*/). For each child newB of B (line\n/*5*/), we check if pruning at newB should occur (line /*6*/) and if not we check\nif a better bound has been found (line /*7*/) Finally, when the loop terminates,\nwe have found the optimal base and return it (line /*8*/).\n\n7\n\nOptimal Base Search III: Search Modulo Product\n\nThis section introduces an abstraction on the search space, classifying bases\naccording to their product. Instead of maintaining (during the search) a priority\nqueue of all bases (nodes) that still need to be explored, we maintain a special\npriority queue in which there will only ever be at most one base with the same\nproduct.\nthe queue will never contain two different bases B1 and B2 such\nQ So, Q\nthat B1 = B2 . In case a second base, with the same product as one already\nin, is inserted to the queue, then only the base with the minimal value of cost \u03b1\nS\nis maintained on the queue. We call this type of priority queue a hashed priority\nqueue because it can conveniently be implemented as a hash table.\nThe intuition comes from a study of the sum digits cost function for which\nwe can prove Q\nthe following\nProperty 1 on bases: Consider two bases B1 and\nQ\n\u03b1\nB2 such that B1 = B2 and such that cost \u03b1\nS (B1 ) \u2264 cost S (B2 ). Then for any\nextension of B1 and of B2 by the same sequence C, cost S (B1 C) \u2264 cost S (B2 C).\nIn particular, if one of B1 or B2 can be extended to an optimal base, then B1\ncan. A direct implication is that when maintaining the frontier of the search\nspace as a priority queue, we only need one representative of the class of bases\nwhich have the same product (the one with the minimal value of cost \u03b1\nS ).\n10\n\n\fA second Property 2 is more subtle and true for any cost function that has\nthe first property: Assume that in the algorithm described as Figure 3 we at\nsome stage remove a base B1 from the priority\nQ queue.QThis implies that if in the\nfuture we encounter any base B2 such that B1 = B2 , then we can be sure\nthat cost S (B1 ) \u2264 cost S (B2 ) and immediately prune the search tree from B2 .\nOur third and final algorithm, which we call hashB&B (best-first, branch\nand bound, with hash priority queue) is identical to the algorithm presented in\nFigure 3, except that the the boxed priority queue introduced at line /*1*/ is\nreplaced by a hash priority queue .\nThe abstract data type hash priority queue maintains bases prioritized by\nthe value of cost \u03b1\nS . Operations popMin() and peek() are as usual. Operation\npush(B\n)\nworks\nas follows: (a) if there is no base B2 in the queue such that\n1Q\nQ\n\u03b1\nB1 = B2 , then add B1 . Otherwise, (b) if cost \u03b1\nS (B2 ) \u2264 cost S (B1 ) then do\nnot add B1 . Otherwise, (c) remove B2 from the queue and add B1 .\nTheorem 15.\n(1) The sum digits cost function satisfies Property 1; and (2) the hashB&B\nalgorithm finds an optimal base for any cost function which satisfies Property 1.\nWe conjecture that the other cost functions do not satisfy Property 1, and\nhence cannot guarantee that the hashB&B algorithm always finds an optimal base.\nHowever, in our extensive experimentation, all bases found (when searching for\nan optimal prime base) are indeed optimal.\nA direct implication of the above improvements is that we can now provide\na tighter bound on the complexity of the search algorithm. Let us denote the\nnumber of different integers in S by s and m = max(S). First note that in\nthe worst case the hashed priority queue will contain m elements (one for each\npossible value of a base product, which is never more than m). Assuming that\nwe use a Fibonacci Heap, we have a O(log(m)) cost (amortized) per popMin()\noperation and in total a O(m \u2217 log(m)) cost for popping elements off the queue\nduring the search for an optimal base.\nNow focus on the costQof operations performed when extracting a base B\nfrom the queue. Denoting B = q, B has at most m/q children (integers which\nextend it). For each child we have to calculate costS which incurs a cost of O(s)\nand possibly to insert it to the queue. Pushing an element onto a hashed priority\nqueue (in all three cases) is a constant time operation (amortized), and hence\nthe total cost for dealing with a child is O(s).\nFinally, consider the total number of children created during the search which\ncorresponds to the following sum:\nm\nm\nX\nX\nO(\nm/q) = O(m\n1/q) = O(m \u2217 log(m))\nq=1\n\nq=1\n\nSo, in total we get O(m \u2217 log(m)) + O(m \u2217 log(m) \u2217 s) \u2264 O(m2 \u2217 log(m)). When\nwe restrict the extenders to be prime numbers then we can further improve this\nbound to O(m2 \u2217 log(log(m))) by reasoning about the density of the primes. A\nproof can be found in the appendix.\n11\n\n\f8\n\nExperiments\n\nExperiments are performed using an extension to MiniSat+ [8] where the only\nchange to the tool is to plug in our optimal base algorithm. The reader is invited\nto experiment with the implementation via its web interface.4 All experiments\nare performed on a Quad-Opteron 848 at 2.2 GHz, 16 GB RAM, running Linux.\nOur benchmark suite originates from 1945 Pseudo-Boolean Evaluation [10]\ninstances from the years 2006\u20132009 containing a total of 74,442,661 individual\nPseudo-Boolean\nconstraints. After normalizing and removing constraints with\n\b\n0, 1 coefficients we are left with 115,891 different optimal base problems where\nthe maximal coefficient is 231 \u2212 1. We then focus on 734 PB instances where at\nleast one optimal base problem from the instance yields a base with an element\nthat is non-prime or greater than 17. When solving PB instances, in all experiments, a 30 minute timeout is imposed as in the Pseudo-Boolean competitions.\nWhen solving an optimal base problem, a 10 minute timeout is applied.\nExperiment 1 (Impact of optimal bases): The first experiment illustrates the advantage in searching for an optimal base for Pseudo-Boolean solving. We compare\nsizes and solving times when encoding w.r.t. the binary base vs. w.r.t. an optimal\nbase (using the hashB&B algorithm with the num comp cost function). Encoding\nw.r.t. the binary base, we solve 435 PB instances (within the time limit) with an\naverage time of 146 seconds and average CNF size of 1.2 million clauses. Using\nan optimal base, we solve 445 instances with an average time of 108 seconds,\nand average CNF size of 0.8 million clauses.\nExperiment 2 (Base search time): Here we focus on the search time for an optimal base in six configurations using the sum carry cost function. Configurations\nM17, dfsHP17, and B&B17, are respectively, the MiniSat+ implementation, our\ndfsHP and our B&B algorithms, all three searching for an optimal base from\nBase 17\np , i.e., with prime elements up to 17. Configurations hashB&B1,000,000,\nhashB&B10,000, and hashB&B17 are our hashB&B algorithm searching for a base\nfrom Base `p with bounds of ` = 1,000,000, ` = 10,000, and ` = 17, respectively.\nResults are summarized in Fig. 4 which is obtained as follows. We cluster\nthe optimal base problems according to the values dlog1.9745 M e where M is the\nmaximal coefficient in a problem. Then, for each cluster we take the average runtime for the problems in the cluster. The value 1.9745 is chosen to minimize the\nstandard deviation from the averages (over all clusters). These are the points on\nthe graphs. Configuration M17 times out on 28 problems. For dfsHP17, the maximal search time is 200 seconds. Configuration B&B17 times out for 1 problem.\nThe hashB&B configurations have maximal runtimes of 350 seconds, 14 seconds\nand 0.16 seconds, respectively for the bounds 1,000,000, 10,000 and 17.\nFig. 4 shows that: (left) even with primes up to 1,000,000, hashB&B is faster\nthan the algorithm from MiniSat+ with the limit of 17; and (right) even with\nprimes up to 10,000, the search time using hashB&B is essentially negligible.\n4\n\nhttp://aprove.informatik.rwth-aachen.de/forms/unified_form_PBB.asp\n\n12\n\n\fFig. 4. Experiment 2: the 3 slowest configurations (left) (from back to front) M17(blue),\nhashB&B1,000,000(orange) and dfsHP17(yellow). The 4 fastest configurations (right)\n(from back to front) dfsHP17(yellow), B&B17(green), hashB&B10,000(brown) and\nhashB&B17(azure). Note the change of scale for the y-axis with 50k ms on the left\nand 8k ms on the right. Configuration dfsHP17 (yellow) is lowest on left and highest\non right, setting the reference point to compare the two graphs.\n\nExperiment 3 (Impact on PB solving): Fig. 5 illustrates the influence of improved\nbase search on SAT solving for PB Constraints. Both graphs depict the number\nof instances solved (the x-axis) within a time limit (the y-axis). On the left, total\nsolving time (with base search), and on the right, SAT solving time only.\n1400000\n1200000\n1000000\n800000\n600000\n\nMinisat+ 17\nBinary base\nhashB&B sum\nof digits\n10,000\nhashB&B carry\ncost 10,000\nhashB&B\ncomp cost\n10,000\n\n1600000\n\nSolve time (ms)\n\nTotal time (ms)\n\n1600000\n\n1400000\n1200000\n1000000\n800000\n600000\n\n400000\n\n400000\n\n200000\n\n200000\n\n0\n391\n\n397\n\n403\n\n409\n\n415\n\n421\n\n427\n\n433\n\n439\n\n445\n\n0\n391\n\nMinisat+ 17\nBinary base\nhashB&B sum\nof digits\n10,000\nhashB&B carry\ncost 10,000\nhashB&B\ncomp cost\n10,000\n\n397\n\n403\n\n409\n\n415\n\n421\n\n427\n\n433\n\n439\n\n445\n\nNumber of instances solved\n\nNumber of instances solved\n\nFig. 5. Experiment 3: total times (left), solving times (right)\n\nBoth graphs consider the 734 instances of interest and compare SAT solving\ntimes with bases found using five configurations. The first is MiniSat+ with\nconfiguration M17, the second is with respect to the binary base, the third to\n(S) with cost functions:\nfifth are hashB&B searching for bases from Base 10,000\np\nsum digits, sum carry, and num comp, respectively. The average total/solve\nrun-times (in sec) are 150/140, 146/146, 122/121, 116/115 and 108/107 (left\nto right). The total number of instances solved are 431, 435, 442, 442 and 445\n(left to right). The average CNF sizes (in millions of clauses) for the entire test\nset/the set where all algorithms solved/the set where no algorithm solved are\n7.7/1.0/18, 9.5/1.2/23, 8.4/1.1/20, 7.2/0.8/17 and 7.2/0.8/17 (left to right).\nThe graphs of Fig. 5 and average solving times clearly show: (1) SAT solving\ntime dominates base finding time, (2) MiniSat+ is outperformed by the trivial\nbinary base, (3) total solving times with our algorithms are faster than with the\nbinary base, and (4) the most specific cost function (comparator cost) outperforms the other cost functions both in solving time and size. Finally, note that\nsum of digits with its nice mathematical properties, simplicity, and application\nindependence solves as many instances as cost carry.\n13\n\n\fNumber of Clauses\n\nExperiment 4 (Impact of high\n100000\nprime factors): This experiment is about the effects of\n80000\nrestricting the maximal prime\n60000\nvalue in a base (i.e. the value\n` = 17 of MiniSat+ ). An\n40000\nanalysis of the our benchmark suite indicates that coef20000\nficients with small prime factors are overrepresented. To\n0\n1\n6\n11\n16\n21\n26\n31\n36\n41\nintroduce instances where coNumber of Instances\nefficients have larger prime\nfactors we select 43 instances\nFig. 6. Experiment 4: Number (x-axis) of instances\nfrom the suite and multiply\nencoded within number of clauses (y-axis) on 4\ntheir coefficients to introduce configurations. From top line to bottom: (yellow)\nthe prime factor 31 raised to ` = 17, i = 5, (red) ` = 17, i = 2, (green) ` = 31,\nthe power i \u2208 {0, . . . , 5}. We i = 5, and (blue) ` \u2208 {17, 31}, i = 0.\nalso introduce a slack variable\nto avoid gcd-based simplification. This gives us a collection of 258 new instances.\nWe used the B&B algorithm with the sum carry cost function applying the limit\n` = 17 (as in MiniSat+ ) and ` = 31. Results indicate that for ` = 31, both CNF\nsize and SAT-solving time are independent of the factor 31i introduced for i > 0.\nHowever, for ` = 17, both measures increase as the power i increases. Results on\nCNF sizes are reported in Fig. 6 which plots for 4 different settings the number\nof instances encoded (x-axis) within a CNF with that many clauses (y-axis).\n\n9\n\nRelated Work\n\nRecent work [2] encodes Pseudo-Boolean constraints via \"totalizers\" similar to\nsorting networks, determined by the representation of the coefficients in an underlying base. Here the authors choose the standard base 2 representation of\nnumbers. It is straightforward to generalize their approach for an arbitrary mixed\nbase, and our algorithm is directly applicable. In [11] the author considers the\nsum digits cost function and analyzes the size of representing the natural numbers up to n with (a particular class of) mixed radix bases. Our Lemma 6 may\nlead to a contribution in that context.\n\n10\n\nConclusion\n\nIt has been recognized now for some years that decomposing the coefficients in\na Pseudo-Boolean constraint with respect to a mixed radix base can lead to\nsmaller SAT encodings. However, it remained an open problem to determine if\nit is feasible to find such an optimal base for constraints with large coefficients.\nIn lack of a better solution, the implementation in the MiniSat+ tool applies a\nbrute force search considering prime base elements less than 17.\n14\n\n\fTo close this open problem, we first formalize the optimal base problem and\nthen significantly improve the search algorithm currently applied in MiniSat+ .\nOur algorithm scales and easily finds optimal bases with elements up to 1,000,000.\nWe also illustrate that, for the measure of optimality applied in MiniSat+ , one\nmust consider also non-prime base elements. However, choosing the more simple\nsum digits measure, it is sufficient to restrict the search to prime bases.\nWith the implementation of our search algorithm it is possible, for the first\ntime, to study the influence of basing SAT encodings on optimal bases. We show\nthat for a wide range of benchmarks, MiniSat+ does actually find an optimal\nbase consisting of elements less than 17. We also show that many Pseudo-Boolean\ninstances have optimal bases with larger elements and that this does influence\nthe subsequent CNF sizes and SAT solving times, especially when coefficients\ncontain larger prime factors.\nAcknowledgement We thank Daniel Berend and Carmel Domshlak for useful\ndiscussions.\n\nReferences\n1. O. Bailleux, Y. Boufkhad, and O. Roussel. A translation of pseudo boolean constraints to SAT. Journal on Satisfiability, Boolean Modeling and Computation\n(JSAT), 2(1-4):191\u2013200, 2006.\n2. O. Bailleux, Y. Boufkhad, and O. Roussel. New encodings of pseudo-boolean\nconstraints into CNF. In Proc. Theory and Applications of Satisfiability Testing\n(SAT '09), pages 181\u2013194, 2009.\n3. P. Barth. Logic-based 0-1 constraint programming. Kluwer Academic Publishers,\nNorwell, MA, USA, 1996.\n4. K. E. Batcher. Sorting networks and their applications. In AFIPS Spring Joint\nComputing Conference, volume 32 of AFIPS Conference Proceedings, pages 307\u2013\n314. Thomson Book Company, Washington D.C., 1968.\n5. R. E. Bixby, E. A. Boyd, and R. R. Indovina. MIPLIB: A test set of mixed integer\nprogramming problems. SIAM News, 25:16, 1992.\n6. R. E. Bryant, S. K. Lahiri, and S. A. Seshia. Deciding CLU logic formulas via\nboolean and pseudo-boolean encodings. In Proc. Intl. Workshop on Constraints in\nFormal Verification (CFV '02), 2002.\n7. B. Chor, P. Lemke, and Z. Mador. On the number of ordered factorizations of\nnatural numbers. Discrete Mathematics, 214(1-3):123\u2013133, 2000.\n8. N. E\u00e9n and N. S\u00f6rensson. Translating pseudo-boolean constraints into SAT.\nJournal on Satisfiability, Boolean Modeling and Computation (JSAT), 2(1-4):1\u2013\n26, 2006.\n9. D. Knuth. The Art of Computer Programming, Volume III: Sorting and Searching.\nAddison-Wesley, 1973.\n10. V. M. Manquinho and O. Roussel. The first evaluation of Pseudo-Boolean solvers\n(PB'05). Journal on Satisfiability, Boolean Modeling and Computation (JSAT),\n2(1-4):103\u2013143, 2006.\n11. N. Sidorov. Sum-of-digits function for certain nonstationary bases. Journal of\nMathematical Sciences, 96(5):3609\u20133615, 1999.\n\n15\n\n\fA\nA.1\n\nAppendix: Proofs\nProving Lemma 5\n\nLemma 5. Let S \u2208 ms(N) and consider the sum digits cost function. Then, S\nhas an optimal base in Base(S).\nProof. Let S \u2208 ms(N) and let B \u2208 Base be an optimal base for S with\nQ\nB > max(S). Let B 0 be the base obtained by removing the last element\nfrom B. We show that msd(S(B) ) = h0, . . . , 0i and that sum digits(S(B) ) =\nsum digits(S(B 0 ) ). The claim then follows. Assume falsely that for v \u2208 S,\nQ|B|\nmsd(v(B) ) >Q0. Then we get the contradiction v = i=0 v(B) (i)*weights(B)(i) \u2265\nmsd(v(B) ) * B > msd(v(B) ) * max(S) \u2265 v. From the definition of sum digits\nand the above contradiction we deduce that sum digits(B) = sum digits(B 0 ).\nIt is straightforward to generalize Lemma 5 for the other cost functions considered in this paper.\n\nA.2\n\nProving Lemma 6\n\nProposition 16 (Unique base representation). Let B = hr0 , . . . , rk\u22121 i be a\nbase with weights(B) = hw0 , . . . , wk i and let v \u2208 N. Then, the unique representation of v in B is obtained as v(B) = hd0 , . . . , dk i such that: (1) d0 = mod(v, r0 ),\n(2) di = mod(div(v, wi ), ri ) for 0 < i < k, and (3) dk = div(v, wk ).\nProof. (sketch) We have to show that for i < k, 0 \u2264 di < ri and that v =\nPk\ni=0 di *wi . The first property follows directly from the construction. The second\nis elaborated on below.\nProposition 17 (Base factoring). Let v \u2208 N and let B1 = hr0 , . . . , rk\u22121 i and\n0\nB2 = hr00 , . . . , rk\u22122\ni be bases which are identical except that at some position\n0 \u2264 p < k \u2212 1, two consecutive base elements in B1 are replaced in B2 by their\nmultiplication. Formally: ri0 = ri for 0 \u2264 i < p, rp0 = rp * rp+1 , and ri0 = ri+1 for\np < i < k \u2212 1. Then, the sum of the digits in v(B2 ) is greater or equal to the sum\nof the digits in v(B1 ) .\nProof. (sketch) We first observe that v(B2 ) (i) = v(B1 ) (i) for 0 \u2264 i < p and\nPk\u22121\nv(B2 ) (i) = v(B1 ) (i+1) for p < i < k\u22121. So, it remains to show that i=0 v(B2 ) (i)\u2212\nPk\ni=0 v(B1 ) (i) = v(B2 ) (p)\u2212v(B1 ) (p)\u2212v(B1 ) (p+1) \u2265 0. We elaborate on this below.\n\n16\n\n\fLemma 6. Let S \u2208 ms(N) and consider the sum digits cost function. Then, S\nhas an optimal base in Base p (S).\nProof. Let B2 be a base with k \u2212 1 elements of the form\nB2 = hr1 , . . . , rp\u22121 , (rp * rp+1 ), rp+2 , . . . , rk\u22121 i\nwhere the element (rp *rp+1 ) at position p is non-prime and rp , rp+1 > 1. So, taking B1 = hr1 , . . . , rp\u22121 , rp , rp+1 , rp+2 , . . . , rk\u22121 i, we are in the setting of Proposition 17. The result follows:\nsum digits(S(B2 ) ) \u2212 sum digits(S(B1 ) ) =\n\nX k\u22121\nX\nv\u2208S i=0\n\n=\n\nX\n\nv(B2 ) (i) \u2212\n\nk\nXX\n\nv(B1 ) (i)\n\nv\u2208S i=0\n\n(v(B2 ) (p) \u2212 v(B1 ) (p) \u2212 v(B1 ) (p + 1))\n\nv\u2208S\n\n\u22650\nA.3\n\nProving Lemma 14\n\nConsider the notation of Definition 8. Let S \u2208 ms(N), B \u2208 Base with |B| = k\nB\nB\nand S(B) = (aij ). Denote the sequences s\u0304(B) = P\nhsB\n0 , s1 , . . . , sk i (sums) and\nn\nB B\nB\nB\nc\u0304(B) = hc0 , c1 , . . . , ck i (carries) defined by: sj = i=1 aij for 0 \u2264 j \u2264 k, cB\n0 =\nB\nB\nB\n(j)\n=\n0, and cB\n=\n(s\n+\nc\n)\ndiv\nB(j)\nfor\n0\n\u2264\nj\n\u2264\nk.\nWe\ndenote\nalso\ninputs\nj+1\nj\nj\nS\nB\nsB\n+\nc\n.\nj\u22121\nj\u22121\nProposition 18. Let S \u2208 ms(N) and B,B 0 bases such that B 0 \u001f B. Then, for\nB0\n1 \u2264 j \u2264 |B|, inputsB\nS (j) = inputsS (j).\nProposition 19. Let 0 6= v \u2208 N, and B a base. Then for every 0 \u2264 i \u2264 |B|\nsuch that v \u2265 weights(B)(i) there exists i \u2264 j \u2264 |B| such that v(B) (j) > 0.\nProposition 20. Let f : R 7\u2192 R be any monotonically increasing function such\nthat for x, y \u2208 R+ , f (x+y) \u2265 f (x)+y. Let S \u2208 ms(N). Then hcostfS , \u2202cost fS , hfS i\ngiven by:\nP|B|+1\nf (inputsB\nS (j))\n\u0010j=1\n\u0011\nP\n|B|\nB\nB\n2. \u2202cost fS (B) =\nj=1 f (inputsS (j)) + f (div(inputsS (|B|), B(|B| \u2212 1))).\n\b\nQ\n3. hfS (B) =\nx\u2208S x\u2265 B .\n1. costfS (B) =\n\nis a heuristic cost function.\nProof. Let f and S be as defined above and B, B 0 bases such that B 0 \u001f B,\n|B| = k, and |B 0 | = k 0 . For every base B 00 we can see that hfS (B 00 ) \u2265 0 (the\nsize of a finite set is a natural number). Therefore it is sufficient to prove that\n17\n\n\fcost S (B 0 ) \u2265 \u2202cost S (B) + hS (B). From Proposition 18 we get that inputsB\nS (k) =\n0\ninputsB\n(k).\nFrom\nthe\ndefinition\nof\ninputs\nand\nof\nf\nwe\nsee\nthat\nS\n0\n\nf (inputsB\nS (k + 1)) = f (\n\nX\n\n0\n\n0\nv(B 0 ) (k) + div(inputsB\nS (k), B (k \u2212 1)))\n\nv\u2208S\n\n= f(\n\nX\n\nv(B 0 ) (k) + div(inputsB\nS (k), B(k \u2212 1)))\n\nv\u2208S\n\nX\n\n\u2265\n\nv(B 0 ) (k) + f (div(inputsB\nS (k), B(k \u2212 1)))\n\nv\u2208S\n\n\b\nFor k \u2264 j\b\u2264 k 0 , denote nz(j) = v \u2208 S v(B 0 ) (j) > 0 .\nQ\nLet v \u2208 x \u2208 S x \u2265 B = weights(B 0 )(k) . By Proposition 19 there exists\nk \u2264 j \u2264 k 0 such that v(B 0 ) (j) > 0 and therefore v \u2208 nz(j). This implies that\n|\n\n\b\n\nQ\n\nx\u2208S x\u2265\n\nB\n\n| + f (div(inputsB\nS (k), B(k \u2212 1))) \u2264\n\n0\n\nk\nX\nX\n\nv(B 0 ) (j) + f (div(inputsB\nS (k), B(k \u2212 1))) =\n\nj=k v\u2208nz(j)\n0\n\nk X\nX\n\nv(B 0 ) (j) +\n\nf (div(inputsB\nS (k), B(k\n\nj=k v\u2208S\n\n\u2212 1))) \u2264\n\n0\nkX\n+1\n\n0\n\nf (inputsB\nS (j))\n\nj=k+1\n\nIn total we have\ncost S (B 0 ) \u2212 \u2202cost S (B) \u2212 hS (B) =\n|B 0 |+1\n\nX\n\n0\n\nB\nf (inputsB\nS (j)) \u2212 f (div(inputsS (|B|), B(|B| \u2212 1))) \u2212\n\nj=|B|+1\n\n\b\n\nx\u2208S x\u2265\n\nQ\n\nB\n\n\u22650\n\nThe proof that \u2202cost S (B 0 ) + hS (B 0 ) \u2265 \u2202cost S (B) + hS (B) is similar noting that\n|nz(k 0 )| = hS (B 0 ).\nProof. (of Lemma 14)\nIf f (x) = x, then costfS = sum carry. By Proposition 20 both definitions give\na heuristic cost function. The proof for the case of num comp and sum digits\nis of the same structure as Proposition 20. The case of sum carry is the most\ncomplicated one.\nA.4\n\nProving Theorem 15\n\nDefinition 21. (Property 1) A heuristic cost function hcost S , \u2202cost S , hS i is\ncalled baseQmul equivalent\nif for every S \u2208 ms(N) and for every bases B1 , B2\nQ\n\u03b1\nsuch that B1 = B2 and cost \u03b1\nS (B1 ) \u2264 cost S (B2 ) the following holds:\n18\n\n\f1. for any extension of B1 and of B2 by same base C, cost S (B1 C) \u2264 cost S (B2 C).\nIn the following propositions we refer to the hashB&B search algorithm of\nSection 7.\nProposition 22. Let S \u2208 ms(N) and let hcost S , \u2202cost S , hS i be a base mul equivalent heuristic cost function.\nevery base B1 extracted from the queue and for\nQ For Q\n\u03b1\nevery base B2 such that B2 = B1 then cost \u03b1\nS B1 \u2264 cost S B2 .\nProof. Let\n\nQ\n\nB1 = k. By complete induction on k.\n\nBase: For k = 1 we have only the empty base and the claim is trivially true.\nStep: Assume that the claim is true for every i < k and assume that\nQ during\nthe run of the algorithm we extract a base B1 from Q\nthe queue with B1 = k.\nAssume falsely that there exists a base B2 such that B2 = k and cost \u03b1\nS (B2 ) <\ncost \u03b1\n(B\n).This\nmeans\nthat\nB\nwas\nnot\nevaluated\nyet\n(otherwise\nB\n1\n2\n1 = B2 ).\nS\nTherefore the father of B2 (in the tree of bases) was never extracted from the\nqueue. Let A2 be the closest ancestor of B2 that was extracted. Denote by C2\nthe child of A2 which is also the ancestor of B2 (potentially B2 itself). So, C2\nwas evaluated. Observe that A2 and C2 are unique because the search space (of\n\u03b1\n\u03b1\nbases) is a tree and B2 \u001f C2 . So, cost \u03b1\nS (B1 ) \u2264 cost S (C2 ) \u2264 cost S (B2 ) and that\nis a contradiction to the existence of B2 .\nThis proves that Property 1 derives Property 2.\nProposition 23. Let v \u2208 N and B,B 0 bases such that B 0 \u001f B. Then, for 0 \u2264\nj \u2264 |B| \u2212 1, v(B 0 ) (i) = v(B) (i).\n\nTheorem 15.\n(1) The sum digits cost function satisfies Property 1; and (2) the hashB&B\nalgorithm finds an optimal base for any cost function which satisfies Property 1.\nProof.\nQ\nQ\n1) Let S \u2208 ms(N), B1 , B2 \u2208 Base and\nB1 =\nB2 \u2208 N. We prove that if\n\u03b1\ncost \u03b1\nS (B1 ) \u2264 cost S (B2 ) then for any base extension C, cost S (B1 C) \u2264 cost S (B2 C).\nThe proof is by complete induction on |C|.\nBase: For |C| = 1, first notice that hS (B1 ) = hS (B2 ). This follows directly\nfrom the definition of admissible heuristics (for the sum digits case). Hence,\n19\n\n\f\u2202cost S (B1 ) \u2264 \u2202cost S (B2 ). From Propositions 16 and 23, we have that\ncostS (B1 C) =\n\n1 C|\nX |B\nX\n\nv(B1 C)\n\nv\u2208S i=0\n\n=\n\n1 |\u22121\nX |BX\n\nv\u2208S\n\nv(B1 ) +\n\ni=0\n\n1 C|\nX\nX |B\n\nv(B1 C)\n\nv\u2208S i=|B1 |\n\n= \u2202cost S (B1 ) +\n\nX\n\n(mod(div(v,\n\nY\n\nB1 ), C(0)) + div(v,\n\nY\n\nB1 C))\n\nv\u2208S\n\n= \u2264 \u2202cost S (B2 ) +\n\nX\n\n(mod(div(v,\n\nY\n\nB2 ), C(0)) + div(v,\n\nY\n\nB2 C))\n\nv\u2208S\n\n=\n\n2 |\u22121\nX |BX\n\nv\u2208S\n\n=\n\nv(B2 ) +\n\ni=0\n\n2 C|\nX |B\nX\n\n2 C|\nX\nX |B\n\nv(B2 C)\n\nv\u2208S i=|B2 |\n\nv(B2 C)\n\nv\u2208S i=0\n\n= cost S (B2 C)\n\nStep: For |C| = k > 1, we define C = C 0 hpi. So by the complete induction\n0\n0\n0\nassumption\nfor |C\nQ\nQ | = k \u2212 1 we get that costS (B1 C ) \u2264 costS (B2 C ). By the fact\nthat B1 C 0 = B2 C 0 we can deduce that msd(S(B1 C 0 ) ) = msd(S(B2 C 0 ) ). By\nthe definition of admissible heuristics for sum digits:\nX\n\u2202cost S (B1 C 0 ) +\nmsd(S(B1 C 0 ) ) = costS (B1 C 0 ) \u2264 costS (B2 C 0 )\nX\n= \u2202cost S (B2 C 0 ) +\nmsd(S(B2 C 0 ) )\nTherefore, \u2202cost S (B1 C 0 ) \u2264 \u2202cost S (B2 C 0 ). Combining it with the fact that\n\u03b1\n0\n0\nhS (B1 C 0 ) = hS (B2 C 0 ) we have that cost \u03b1\nS (B1 C ) \u2264 cost S (B2 C ). Finally from\n0\nthe inductive assumption we get that cost S (B1 C hpi) \u2264 cost S (B2 C 0 hpi).\n2) Let hcost S , \u2202cost S , hs i a base mul equivalent heuristic cost function and\nS \u2208 ms(N). We denote by bestB the best base found by the algorithm at\neach point of the run. Let B be the first base extracted from the queue such\nthat cost \u03b1\nS (B) \u2265 cost S (bestB). This is the condition that terminates the main\nloop of the algorithm, so we need to prove that bestB is the optimal base\nfor S. Assume falsely that there exists an optimal base B 0 = F.hbi such that\ncost S (B 0 ) < cost S (bestB). Let A be the nearest ancestor of B 0 such that the its\nbase equivalence class representative R was extracted from queue (R 6= B 0 otherwise cost S (bestB) \u2264 cost S (B 0 )). By Proposition 22 we know that cost \u03b1\nS (R) \u2264\ncost \u03b1\nS (A) and by Property 1 that for any base C cost S (RC) \u2264 cost S (AC). In\nparticular for the case where AC = A.hbiC 0 = B 0 . By choice of A we get that\nthe equivalence class representative of A.hbi was not extracted (and it is the\nsame class of R.hbi). Therefore, cost S (B 0 ) \u2265 cost S (R.hbiC 0 ) \u2265 cost \u03b1\nS (R.hbi) \u2265\ncost \u03b1\nS (B), which is a contradiction.\n20\n\n\fA.5\n\nOn the complexity of the hashB&B algorithm for prime bases\n\nTheorem 24. Let S \u2208 ms(N) with m = max(S). Then, the complexity of the\nhashB&B algorithm for prime bases is O(m2 \u2217 log(log(m))).\nProof. We use the prime number theorem which states that the density of the\nprimes near x is O(x/ log(x)). The number of prime bases evaluated in the worst\ncase scenario is:\nO(\n\nm\nm\nX\nX\n1\n(m/i)\n) = O(m\n)\nlog(m/i)\ni\n*\nlog(m/i)\ni=1\n\nk=1\n\nBut\nm\nX\ni=1\n\nk\n\nlog(m)\n2\nX X\n1\n1\n=\ni * log(m/i)\ni * log(m/i)\nk\u22121\nk=1 i=2\n\nlog(m)\n\n\u2264\n\nX\n\nk\n\n2\nX\n\nk=1 i=2k\u22121\nlog(m)\n\n=\n\nX\nk=1\nlog(m)\n\n=\n\nX\nk=1\n\n1\n2k\u22121 * (log(m) \u2212 log(i))\n\n2k\u22121\n2k\u22121 * (log(m) \u2212 k + 1)\n1\n(log(m) \u2212 k + 1)\n\nlog(m)\n\n=\n\nX 1\n= O(log(log(m)))\nk\n\nk=1\n\nAnd so the total number of bases evaluated during a run of the algorithm is\nbounded by O(m*log(log(m))) and the overall complexity is O(m2 *log(log(m))).\n\nA.6\n\nInteger division and modulus\n\nLet a, b \u2208 N with b > 0. It is standard to define div(a, b) and mod(a, b) as natural\nnumbers such that a = div(a, b) * b + mod(a, b) and 0 \u2264 mod(a, b) < b. In our\nproofs we note that div(a, b) is the maximal number such that there exists r \u2208 N\nwith a = div(a, b) * b + r.\nProposition 25. Let a, b, c \u2208 N with b, c > 0. Then div(a, b*c) = div(div(a, b), c).\nProof. By definition if k = div(a, b * c), k 0 = div(a, b) and k 00 = div(k 0 , c). Then,\na = k * c * b + r, a = k 0 * b + r0 and k 0 = k 00 * c + r00 . Now, k * c \u2264 k 0 because\notherwise it would be a contradiction to the maximality of k 0 . If k * c = k 0\nthen div(div(a, b), c) = div(k * c, c) = k = div(a, b * c). Assume that k * c < k 0 .\n21\n\n\fThen a = k 0 * b + r0 = k 00 * c * b + r00 * b + r0 and from that we deduce k 00 \u2264 k\n(otherwise it would be a contradiction to the maximality of k). On the other\nhand, 0 < k 0 \u2212 k * c = k 00 * c + r00 \u2212 k * c \u2194 k * c \u2212 r00 < k 00 * c. From the definition of\nmodulus we get that r00 < c and so (k \u2212 1) * c < k * c \u2212 r00 < k 00 * c, and therefore\nk \u2212 1 < k 00 . In total we get that k \u2212 1 < k 00 \u2264 k and because k and k 00 are natural\nnumbers we get the equality.\n\nProposition 26. Let a, b, c \u2208 N such that b, c > 0. Then, mod(a, b * c) =\nmod(a, b) + mod(div(a, b), c) * c.\nProof. Let r = mod(a, b * c), r0 = mod(a, b), k 0 = div(b, c), and r00 = mod(k 0 , c).\nBy definition we can see that a = k * b * c + r, a = k 0 * b + r0 , and k 0 = k 00 * c + r00 .\nTherefore mod(a, b * c) = mod(a, b) + mod(div(a, b), c) \u2194 r = r0 + r00 * b \u2194\na \u2212 k * b * c = a \u2212 k 0 * b + (k 0 \u2212 k 00 * c) * b \u2194 \u2212k * c = \u2212k 0 + (k 0 \u2212 k 00 * c) \u2194 k * c = k 00 * c\n\u2194 k = k 00 \u2194 div(a, b * c) = div(k 0 , c) \u2194 div(a, b * c) = div(div(a, b), c) and this is\ntrue by Proposition 25.\n\nA.7\n\nThe rest of the details\n\nFor the sake of readability, we write weights(B) = B\u0303.\nProof. (of Proposition 16 by induction on |B|).\nBase (length is 1): B = hbi and hence B\u0303 = h1, bi, v(B) (0) = mod(v, b), v(B) (1) =\ndiv(v, b) and by definition of div and mod we can see that v = div(v, b) \u2217 b +\nmod(v, b).\nStep: Assume the assumption is true for every base B such that |B| = k \u2212 1. Let\n|B| = k such that B = hb0 , b1 , . . . , bk\u22121 i. Define the base B 0 = hb0 b1 , b2 , . . . , bk\u22121 i\nwith |B 0 | = k \u2212 1. We can see that for 0 < i < k \u2212 1, B 0 (i) = B(i + 1), and that\nfor 0 < i < k, B\u0303 0 (i) = B\u0303(i + 1). From this we can see that for 0 < i < k \u2212 1,\nv(B 0 ) (i) = mod(div(v, B\u0303 0 (i)), B 0 (i)) = mod(div(v, B\u0303(i + 1)), B(i + 1)) = v(B) (i +\n1) and also that v(B 0 ) (k \u2212 1) = div(v, B\u0303 0 (k \u2212 1)) = div(v, B\u0303(k)) = v(B) (k).\nPk\u22121\nBack to the main claim by the induction we know that v = i=0 v(B 0 ) (i)B\u0303 0 (i).\nPk\nPk\nTherefore i=0 v(B) (i)B\u0303(i) = v(B) (0)B\u0303(0) + v(B) (1)B\u0303(1) + i=2 v(B) (i)B\u0303(i) =\nPk\u22121\nv(B) (0)B\u0303(0) + v(B) (1)B\u0303(1) + i=1 v(B 0 ) (i)B\u0303 0 (i) = v(B) (0)B\u0303(0) + v(B) (1)B\u0303(1) +\nv \u2212 v(B 0 ) (0)B\u0303 0 (0).\nBy Proposition 26 we can see that 0 = mod(v, b0 ) + mod(div(v, b0 ), b1 ) * b0 \u2212\nmod(v, b0 * b1 ) = v(B) (0)B\u0303(0) + v(B) (1)B\u0303(1) \u2212 v(B 0 ) (0)B\u0303 0 (0). And therefore we\nPk\nget that i=0 v(B) (i)B\u0303(i) = v.\n\n22\n\n\fProof. (of Proposition 18)\nFirst we notice that because B 0 \u001f B then for 0 \u2264 i < |B| we get that B(i) =\nB 0 (i) and therefore B\u0303(i) = B\u0303 0 (i). By Proposition 16 we see that v(B) (i) =\nv(B 0 ) (i). By definition of S(B) and by Proposition 16 and the definition of inputsB\nS\nwe prove this proposition by induction on 0 < i \u2264 |B|.\nP\nP\nB0\n1. Base case: inputsB\nS (1) =\nv\u2208S v(B) (0) =\nv\u2208S v(B 0 ) (0) = inputsS (1)\nB0\n2. Step: Assume P\nthat inputsB\nS (i \u2212 1) = inputsS (i \u2212 1).\nB\ninputsS (i) = v\u2208S v(B) (i \u2212 1) + div(inputsB\nS (i \u2212 1), B(i \u2212 2)) =\nP\n0\nB\n0\nB0\nv\u2208S v(B 0 ) (i \u2212 1) + div(inputsS (i \u2212 1), B (i \u2212 2)) = inputsS (i)\nProof. (of Proposition 19)\nLet v and B be as defined above. Let 0 \u2264 i \u2264 |B| such that v \u2265 B\u0303(i). If there\nexists an i \u2264 j \u2264 |B| such that v = B\u0303(j) then we get that div(v, B\u0303(j)) = 1\nand in any case (j < |B| or j = |B|) v(B) (j) = 1. Otherwise let i \u2264 j \u2264 |B|\nbe the maximal index such that v > B\u0303(j). If j = |B| then div(v, B\u0303(|B|)) > 0.\nConsider the case when j < |B| Then div(v, B\u0303(j)) = k such that k * B\u0303(j) \u2264\nk * B\u0303(j) + r = v < B\u0303(j) * B(j) and so by dividing both sides by B\u0303(j) we get that\ndiv(v, B\u0303(j)) < B(j), which means that v(B) (j) = mod(div(v, B\u0303(j)), B(j)) > 0 .\nProof. (of Proposition 17)\nThe following proof is deeply based on the definition of v(B) and Proposition 16.\nLet v, B1 and B2 be as defined above.\n1. For 0 \u2264 i < p by definition B1 (i) = B2 (i) which means that B\u03031 (i) = B\u03032 (i)\nand therefore v(B1 ) (i) = v(B2 ) (i). For p < i < k \u2212 1 again we get that\nB2 (i) = B1 (i + 1) and B\u03032 (i) = B\u03031 (i + 1) which again means that v(B2 ) (i) =\nv(B1 ) (i + 1)).\n2. We can see that B\u03031 (p) = B\u03032 (p). By Proposition 16 we know that\nPk\nPk\u22121\n0 = v \u2212 v = i=0 vB1 (i) * B\u03031 (i) \u2212 i=0 vB2 (i) * B\u03032 (i) =\nv(B1 ) (p) * B\u03031 (p) + v(B1 ) (p + 1) * B\u03031 (p + 1) \u2212 v(B2 ) (p) * B\u03032 (p) =\nv(B1 ) (p) * B\u03031 (p) + v(B1 ) (p + 1) * B\u03031 (p) * a \u2212 v(B2 ) (p) * B\u03031 (p)\nTherefore v(B2 ) (p) = v(B1 ) (p) + v(B1 ) (p + 1) * a\nBecause a > 1 we deduce that v(B2 ) (p) \u2265 v(B1 ) (p) + v(B1 ) (p + 1)\nAnd\nPk\u22121in total we get\nPk that\nv\n(i)\n\u2212\ni=0 v(B1 ) (i) = v(B2 ) (p) \u2212 v(B1 ) (p) \u2212 v(B1 ) (p + 1) \u2265 0\ni=0 (B2 )\nProof. (of Proposition 23)\nLet v \u2208 N and B,B 0 bases such that B 0 \u001f B. Let 0 \u2264 i \u2264 |B| \u2212 1 (|B| > 0,\notherwise there is no such index). For i = 0 by Proposition 16 we get that\nv(B 0 ) (0) = mod(v, B 0 (0)) = mod(v, B(0)) = v(B) (0). If i > 0 then by Proposition 16 and definition of B\u0303 we can deduce v(B 0 ) (i) = mod(div(v, B\u0303 0 (i)), B 0 (i)) =\nmod(div(v, B\u0303(i)), B(i)) = v(B) (i).\n\n23\n\n\f"}