{"id": "http://arxiv.org/abs/math/0702819v1", "guidislink": true, "updated": "2007-02-27T09:39:26Z", "updated_parsed": [2007, 2, 27, 9, 39, 26, 1, 58, 0], "published": "2007-02-27T09:39:26Z", "published_parsed": [2007, 2, 27, 9, 39, 26, 1, 58, 0], "title": "Multi-armed bandit problem with precedence relations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0702560%2Cmath%2F0702700%2Cmath%2F0702570%2Cmath%2F0702540%2Cmath%2F0702413%2Cmath%2F0702544%2Cmath%2F0702596%2Cmath%2F0702597%2Cmath%2F0702535%2Cmath%2F0702149%2Cmath%2F0702633%2Cmath%2F0702372%2Cmath%2F0702167%2Cmath%2F0702485%2Cmath%2F0702758%2Cmath%2F0702759%2Cmath%2F0702068%2Cmath%2F0702401%2Cmath%2F0702546%2Cmath%2F0702795%2Cmath%2F0702074%2Cmath%2F0702417%2Cmath%2F0702649%2Cmath%2F0702333%2Cmath%2F0702011%2Cmath%2F0702531%2Cmath%2F0702361%2Cmath%2F0702050%2Cmath%2F0702304%2Cmath%2F0702312%2Cmath%2F0702255%2Cmath%2F0702721%2Cmath%2F0702487%2Cmath%2F0702534%2Cmath%2F0702836%2Cmath%2F0702623%2Cmath%2F0702794%2Cmath%2F0702657%2Cmath%2F0702821%2Cmath%2F0702809%2Cmath%2F0702502%2Cmath%2F0702874%2Cmath%2F0702696%2Cmath%2F0702797%2Cmath%2F0702096%2Cmath%2F0702562%2Cmath%2F0702341%2Cmath%2F0702118%2Cmath%2F0702586%2Cmath%2F0702711%2Cmath%2F0702866%2Cmath%2F0702819%2Cmath%2F0702394%2Cmath%2F0702309%2Cmath%2F0702439%2Cmath%2F0702837%2Cmath%2F0702582%2Cmath%2F0702630%2Cmath%2F0702681%2Cmath%2F0702785%2Cmath%2F0702645%2Cmath%2F0702296%2Cmath%2F0702331%2Cmath%2F0702165%2Cmath%2F0702121%2Cmath%2F0702511%2Cmath%2F0702063%2Cmath%2F0702719%2Cmath%2F0702323%2Cmath%2F0702811%2Cmath%2F0702169%2Cmath%2F0702112%2Cmath%2F0702896%2Cmath%2F0702442%2Cmath%2F0702421%2Cmath%2F0702520%2Cmath%2F0702065%2Cmath%2F0702363%2Cmath%2F0702541%2Cmath%2F0702207%2Cmath%2F0702529%2Cmath%2F0702680%2Cmath%2F0702175%2Cmath%2F0702292%2Cmath%2F0702430%2Cmath%2F0702578%2Cmath%2F0702614%2Cmath%2F0702443%2Cmath%2F0702114%2Cmath%2F0702426%2Cmath%2F0702796%2Cmath%2F0702263%2Cmath%2F0702643%2Cmath%2F0702001%2Cmath%2F0702594%2Cmath%2F0702026%2Cmath%2F0702481%2Cmath%2F0702337%2Cmath%2F0702764%2Cmath%2F0702757%2Cmath%2F0702835&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Multi-armed bandit problem with precedence relations"}, "summary": "Consider a multi-phase project management problem where the decision maker\nneeds to deal with two issues: (a) how to allocate resources to projects within\neach phase, and (b) when to enter the next phase, so that the total expected\nreward is as large as possible. We formulate the problem as a multi-armed\nbandit problem with precedence relations. In Chan, Fuh and Hu (2005), a class\nof asymptotically optimal arm-pulling strategies is constructed to minimize the\nshortfall from perfect information payoff. Here we further explore optimality\nproperties of the proposed strategies. First, we show that the efficiency\nbenchmark, which is given by the regret lower bound, reduces to those in Lai\nand Robbins (1985), Hu and Wei (1989), and Fuh and Hu (2000). This implies that\nthe proposed strategy is also optimal under the settings of aforementioned\npapers. Secondly, we establish the super-efficiency of proposed strategies when\nthe bad set is empty. Thirdly, we show that they are still optimal with\nconstant switching cost between arms. In addition, we prove that the Wald's\nequation holds for Markov chains under Harris recurrent condition, which is an\nimportant tool in studying the efficiency of the proposed strategies.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0702560%2Cmath%2F0702700%2Cmath%2F0702570%2Cmath%2F0702540%2Cmath%2F0702413%2Cmath%2F0702544%2Cmath%2F0702596%2Cmath%2F0702597%2Cmath%2F0702535%2Cmath%2F0702149%2Cmath%2F0702633%2Cmath%2F0702372%2Cmath%2F0702167%2Cmath%2F0702485%2Cmath%2F0702758%2Cmath%2F0702759%2Cmath%2F0702068%2Cmath%2F0702401%2Cmath%2F0702546%2Cmath%2F0702795%2Cmath%2F0702074%2Cmath%2F0702417%2Cmath%2F0702649%2Cmath%2F0702333%2Cmath%2F0702011%2Cmath%2F0702531%2Cmath%2F0702361%2Cmath%2F0702050%2Cmath%2F0702304%2Cmath%2F0702312%2Cmath%2F0702255%2Cmath%2F0702721%2Cmath%2F0702487%2Cmath%2F0702534%2Cmath%2F0702836%2Cmath%2F0702623%2Cmath%2F0702794%2Cmath%2F0702657%2Cmath%2F0702821%2Cmath%2F0702809%2Cmath%2F0702502%2Cmath%2F0702874%2Cmath%2F0702696%2Cmath%2F0702797%2Cmath%2F0702096%2Cmath%2F0702562%2Cmath%2F0702341%2Cmath%2F0702118%2Cmath%2F0702586%2Cmath%2F0702711%2Cmath%2F0702866%2Cmath%2F0702819%2Cmath%2F0702394%2Cmath%2F0702309%2Cmath%2F0702439%2Cmath%2F0702837%2Cmath%2F0702582%2Cmath%2F0702630%2Cmath%2F0702681%2Cmath%2F0702785%2Cmath%2F0702645%2Cmath%2F0702296%2Cmath%2F0702331%2Cmath%2F0702165%2Cmath%2F0702121%2Cmath%2F0702511%2Cmath%2F0702063%2Cmath%2F0702719%2Cmath%2F0702323%2Cmath%2F0702811%2Cmath%2F0702169%2Cmath%2F0702112%2Cmath%2F0702896%2Cmath%2F0702442%2Cmath%2F0702421%2Cmath%2F0702520%2Cmath%2F0702065%2Cmath%2F0702363%2Cmath%2F0702541%2Cmath%2F0702207%2Cmath%2F0702529%2Cmath%2F0702680%2Cmath%2F0702175%2Cmath%2F0702292%2Cmath%2F0702430%2Cmath%2F0702578%2Cmath%2F0702614%2Cmath%2F0702443%2Cmath%2F0702114%2Cmath%2F0702426%2Cmath%2F0702796%2Cmath%2F0702263%2Cmath%2F0702643%2Cmath%2F0702001%2Cmath%2F0702594%2Cmath%2F0702026%2Cmath%2F0702481%2Cmath%2F0702337%2Cmath%2F0702764%2Cmath%2F0702757%2Cmath%2F0702835&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Consider a multi-phase project management problem where the decision maker\nneeds to deal with two issues: (a) how to allocate resources to projects within\neach phase, and (b) when to enter the next phase, so that the total expected\nreward is as large as possible. We formulate the problem as a multi-armed\nbandit problem with precedence relations. In Chan, Fuh and Hu (2005), a class\nof asymptotically optimal arm-pulling strategies is constructed to minimize the\nshortfall from perfect information payoff. Here we further explore optimality\nproperties of the proposed strategies. First, we show that the efficiency\nbenchmark, which is given by the regret lower bound, reduces to those in Lai\nand Robbins (1985), Hu and Wei (1989), and Fuh and Hu (2000). This implies that\nthe proposed strategy is also optimal under the settings of aforementioned\npapers. Secondly, we establish the super-efficiency of proposed strategies when\nthe bad set is empty. Thirdly, we show that they are still optimal with\nconstant switching cost between arms. In addition, we prove that the Wald's\nequation holds for Markov chains under Harris recurrent condition, which is an\nimportant tool in studying the efficiency of the proposed strategies."}, "authors": ["Hock Peng Chan", "Cheng-Der Fuh", "Inchi Hu"], "author_detail": {"name": "Inchi Hu"}, "author": "Inchi Hu", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/074921706000001067", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/math/0702819v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0702819v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published at http://dx.doi.org/10.1214/074921706000001067 in the IMS\n  Lecture Notes Monograph Series\n  (http://www.imstat.org/publications/lecnotes.htm) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62L05 (Primary) 62N99 (Secondary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0702819v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0702819v1", "journal_reference": "IMS Lecture Notes Monograph Series 2006, Vol. 52, 223-235", "doi": "10.1214/074921706000001067", "fulltext": "IMS Lecture Notes\u2013Monograph Series\nTime Series and Related Topics\nVol. 52 (2006) 223\u2013235\nc Institute of Mathematical Statistics, 2006\nDOI: 10.1214/074921706000001067\n\narXiv:math/0702819v1 [math.ST] 27 Feb 2007\n\nMulti-armed bandit problem with\nprecedence relations\nHock Peng Chan1,\u2217 , Cheng-Der Fuh2,\u2020 and Inchi Hu3,\u2021\nNational University of Singapore, National Central University, Academia Sinica and\nHong Kong University of Science and Technology\nAbstract: Consider a multi-phase project management problem where the\ndecision maker needs to deal with two issues: (a) how to allocate resources to\nprojects within each phase, and (b) when to enter the next phase, so that the\ntotal expected reward is as large as possible. We formulate the problem as a\nmulti-armed bandit problem with precedence relations. In Chan, Fuh and Hu\n(2005), a class of asymptotically optimal arm-pulling strategies is constructed\nto minimize the shortfall from perfect information payoff. Here we further\nexplore optimality properties of the proposed strategies. First, we show that\nthe efficiency benchmark, which is given by the regret lower bound, reduces to\nthose in Lai and Robbins (1985), Hu and Wei (1989), and Fuh and Hu (2000).\nThis implies that the proposed strategy is also optimal under the settings of\naforementioned papers. Secondly, we establish the super-efficiency of proposed\nstrategies when the bad set is empty. Thirdly, we show that they are still\noptimal with constant switching cost between arms. In addition, we prove that\nthe Wald's equation holds for Markov chains under Harris recurrent condition,\nwhich is an important tool in studying the efficiency of the proposed strategies.\n\n1. Introduction\nSuppose there are U = J1 + * * * + JI statistical populations, \u03a011 , \u03a012 , . . . , \u03a0IJI .\nPulling arm ij once corresponds to taking an observation from population \u03a0ij . The\nobservations from \u03a0ij form a Markov chain on a state space D with transition\nprobability density function pij (x, y, \u03b8) with respect to a \u03c3-finite measure Q, where\n\u03b8 is an unknown parameter belonging to a parameter space \u0398. The stationary\nprobability distribution for the Markov chain exists and has probability density\nfunction \u03c0ij (*, \u03b8).\nAt each step, we are required to sample one of the statistical populations obeying\nthe partial order ij \u0016 i\u2032 j \u2032 \u21d4 i \u2264 i\u2032 . An adaptive policy is a sampling rule that\ndictates, at each step, which population should be sampled based on observations\nbefore that step. We can represent a policy as a sequence of random variables\n\u03c6 = {\u03c6t |\u03c6t\u22121 \u0016 \u03c6t , t = 1, 2, . . .} taking values in {ij|i = 1, . . . , I; j = 1, . . . , Ji }\nsuch that the event {\u03c6t = ij} 'take an observation from \u03a0ij at step t' belongs to\nthe \u03c3-field generated by \u03c61 , X1 , . . . , \u03c6t\u22121 , Xt\u22121 , where Xt denotes the state of the\npopulation being sampled at t-th step.\n1 National\n\nUniversity of Singapore, e-mail: stachp@nus.edu.sg\nSinica, e-mail: stcheng@stat.sinica.edu.tw\n3 Hong Kong University of Science and Technology, e-mail: imichu@ust.hk\n\u2217 Research supported by grants from the National University of Singapore.\n\u2020 Research partially supported by the National Science Council of ROC.\n\u2021 Research partially supported by Hong Kong Research Grant Council.\nAMS 2000 subject classifications: primary 62L05; secondary 62N99.\nKeywords and phrases: Markov chains, multi-armed bandits, Kullback\u2013Leibler number, likelihood ratio, optimal stopping, scheduling, single-machine job sequencing, Wald's equation.\n2 Academia\n\n223\n\n\f224\n\nH. P. Chan, C.-D. Fuh and I. Hu\n\nLet the initial state of \u03a0ij be distributed according to \u03bdij (*; \u03b8). Throughout this\npaper, we shall use the notation E\u03b8 (P\u03b8 ) to denote expectation (probability) with\nrespect to the initial distribution \u03bdij (*; \u03b8); similarly, E\u03c0(\u03b8) to denote expectation\nwith respect to the stationary distribution \u03c0ij (*; \u03b8). We shall assume that Vij =\n{x \u2208 D : \u03bdij (x; \u03b8) > 0} does not dependRon \u03b8 and vij := inf x\u2208Vij inf \u03b8,\u03b8\u2032 \u2208\u0398 [\u03bdij (x; \u03b8)/\n\u03bdij (x; \u03b8\u2032 )] > 0 for all i, j. Suppose that x\u2208D |g(x)|\u03c0ij (x; \u03b8)Q(dx) < \u221e. Let\nZ\ng(x)\u03c0ij (x; \u03b8)Q(dx)\n\u03bcij (\u03b8) =\nx\u2208D\n\nbe the mean reward under stationary distribution \u03c0ij when \u03a0ij is sampled once.\nLet N be the total sample size from all populations, and\nTN (ij) =\n\n(1.1)\n\nN\nX\n\n1{\u03c6t =ij}\n\nt=1\n\nbe the sample size from \u03a0ij and 1 denotes the indicator function. It follows that\nthe total reward equals\n(1.2)\n\nWN (\u03b8) :=\n\nJi\nN X\nI X\nX\n\nE\u03b8 {E\u03b8 [Xt 1{\u03c6t =ij} |Ft\u22121 ]}.\n\nt=1 i=1 j=1\n\nIn the case of independent rewards, that is, when pij (x, y, ; \u03b8) = pij (y; \u03b8) for all\nP I P Ji\ni, j, x, y and \u03b8, WN (\u03b8) =\nj=1 \u03bcij (\u03b8)E\u03b8 TN (ij). We shall show in the Api=1\npendix that for Markovian rewards, under regularity conditions A3-A4 (see Section\n2.1), there exists a constant C0 < \u221e independent of \u03b8 \u2208 \u0398, N > 0 and the strategy\n\u03c6 such that\n(1.3)\n\nWN (\u03b8) \u2212\n\nJi\nI X\nX\n\n\u03bcij (\u03b8)E\u03b8 TN (ij) \u2264 C0 .\n\ni=1 j=1\n\nIn light of (1.3), maximizing WN (\u03b8) is asymptotically equivalent [up to a O(1)\nterm] to minimizing the regret\nX\n(1.4) RN (\u03b8) := N \u03bc\u2217 (\u03b8) \u2212 WN (\u03b8) =\n[\u03bc\u2217 (\u03b8) \u2212 \u03bcij (\u03b8)]E\u03b8 TN (ij),\nij:\u03bcij (\u03b8)<\u03bc\u2217 (\u03b8)\n\nwhere \u03bc\u2217 (\u03b8) := max1\u2264i\u2264I max1\u2264j\u2264Ji \u03bcij (\u03b8).\nBecause adaptive strategies \u03c6 that are optimal for all \u03b8 \u2208 \u0398 and large N in\ngeneral do not exist, we consider the class of all (asymptotically) uniformly good\nadaptive strategies under the partial order constraint \u0016, satisfying\n(1.5)\n\nRN (\u03b8) = o(N \u03b1 ),\n\nfor all \u03b1 > 0 and \u03b8 \u2208 \u0398.\n\nSuch strategies have regret that does not increase too rapidly for any \u03b8 \u2208 \u0398. We\nwould like to find a strategy that minimizes the increasing rate of the regret within\nthe class of uniformly good adaptive strategies under the partial order constraint \u0016.\nThe rest of the article is organized as follows. In Section 2, we present the assumptions and introduce the concept of bad sets. The regret lower bound is investigated\nin Section 3. We also prove that the regret lower bound specializes to other lower\nbounds obtained by previous authors under less general settings. Section 4 contains the super efficiency result when the bad sets are empty. The optimality of the\nproposed strategies under constant switching cost is investigated in Section 5. The\nlast section includes the proof of Wald's equation for Markov random walks under\nHarris recurrence condition.\n\n\fMulti-armed bandit problem with precedence relations\n\n225\n\n2. The assumption and bad sets\nDenote the Kullback-Leibler information number by\nZ\nZ\nh p (x, y; \u03b8) i\nij\n\u2032\n(2.1) Iij (\u03b8, \u03b8 ) =\npij (x, y; \u03b8)\u03c0ij (x; \u03b8)Q(dy)Q(dx).\nlog\n\u2032\np\nij (x, y; \u03b8 )\nx\u2208D y\u2208D\nThen, 0 \u2264 Iij (\u03b8, \u03b8\u2032 ) \u2264 \u221e. We shall assume that Iij (\u03b8, \u03b8\u2032 ) < \u221e for all i, j and\n\u03b8, \u03b8\u2032 \u2208 \u0398. Let \u03bci (\u03b8) = max1\u2264j\u2264Ji \u03bcij (\u03b8) be the largest reward in the i-th group of\narms, and\n(2.2) \u0398i = {\u03b8 \u2208 \u0398 : \u03bci (\u03b8) > \u03bci\u2032 (\u03b8) for all i\u2032 < i and \u03bci (\u03b8) \u2265 \u03bci\u2032 (\u03b8) for all i\u2032 \u2265 i}\nbe the set of parameter values such that the first optimal job is in group i. Let\n\u0398ij = {\u03b8 \u2208 \u0398i : \u03bcij (\u03b8) = \u03bci (\u03b8)}\n\n(2.3)\n\nbe the parameter set such that arm ij is one of the first optimal ones. Each \u03b8 \u2208 \u0398\nbelongs to exactly one \u0398i but may belong to more than one \u0398ij . Let\n\u0398\u2217i = {\u03b8 \u2208 \u0398 : \u03bci (\u03b8) > \u03bci\u2032 (\u03b8) for all i\u2032 6= i}\n\n(2.4)\n\nbe the parameter set in which all the optimal arms lie in group i. Clearly, \u0398\u2217i \u2282 \u0398i\nbut the reverse relation is not necessarily true.\n2.1. The assumptions\nWe now state a set of assumptions that will be used to prove the optimality results.\nLet \u0398 be a compact subset of Rd for some d \u2265 1.\nA1. \u03bcij (*) are finite and continuous on \u0398 for all i, j. Moreover, no arm group is\nredundant in the sense that \u0398\u2217i 6= \u2205 for all i = 1, . . . , I.\nP J1\n\u2032\n\u2032\n\u2032\nA2.\nj=1 I1j (\u03b8, \u03b8 ) > 0 for all \u03b8 6= \u03b8 and inf \u03b8 \u2032 \u2208\u0398ij Iij (\u03b8, \u03b8 ) > 0 for all 1 \u2264 i <\nI, 1 \u2264 j \u2264 Ji and \u03b8 \u2208 \u222al>i \u0398l .\nA3. For each j = 1, . . . , Ji , i = 1, . . . , I and \u03b8 \u2208 \u0398, {Xijt , t \u2265 0} is a Markov\nchain on a state space D with \u03c3-algebra D, irreducible with respect to a\nmaximal irreducible measure on (D, D) and aperiodic. Furthermore, Xijt is\nHarris recurrent in the sense that there exists a set Gij \u2208 D, \u03b1ij > 0 and\nprobability measure \u03c6ij on Gij such that Pij\u03b8 {Xijt \u2208 Gij i.o.|Xij0 = x} = 1\nfor all x \u2208 D and\n(2.5) Pij\u03b8 {Xij1 \u2208 A|Xij0 = x} \u2265 \u03b1ij \u03c6ij (A)\n\nfor all x \u2208 Gij and A \u2208 D.\n\nA4. There exist constants 0 < b\u0304 < 1, b > 0 and drift functions Vij : D \u2192 [1, \u221e)\nsuch that for all j = 1, . . . , Ji and i = 1, . . . , I,\nsup |g(x)|/Vij (x) < \u221e,\n\n(2.6)\n\nx\u2208D\n\nand for all x \u2208 D, \u03b8 \u2208 \u0398,\n(2.7)\n\nPij\u03b8 Vij (x) \u2264 (1 \u2212 b\u0304)Vij (x) + b1Gij (x),\n\n\f226\n\nH. P. Chan, C.-D. Fuh and I. Hu\n\nR\nwhere Gij satisfies (2.5) and Pij\u03b8 Vij (x) = D Vij (y)Pij\u03b8 (x, dy). Moreover, we\nrequire that\nZ\nVij (x)\u03bdij (dx; \u03b8)Q(dx) < \u221e and Vij\u2217 := sup Vij (x) < \u221e.\n(2.8)\nx\u2208Gij\n\nD\n\nLet lij (x, y; \u03b8, \u03b8\u2032 ) = log[pij (x, y; \u03b8)/pij (x, y; \u03b8\u2032 )] be the log likelihood ratio be\u2032\ntween Pij\u03b8 and Pij\u03b8 and N\u03b4 (\u03b8) = {\u03b8\u2032 : k\u03b8 \u2212 \u03b8\u2032 k < \u03b4} a ball of radius \u03b4 around\n\u03b8, where k * k denotes Euclidean norm.\nA5. There exists \u03b4 > 0 such that for all \u03b8, \u03b8\u2032 \u2208 \u0398,\n(2.9)\n\nK\u03b8,\u03b8\u2032 := sup\n\nx\u2208D\n\ne ij0 = x]\nE\u03b8 [sup\u03b8\u0303\u2208N\u03b4 (\u03b8\u2032 ) l2ij (Xij0 , Xij1 ; \u03b8, \u03b8)|X\nVij (x)\n\n<\u221e\n\nfor all j = 1, . . . , Ji , i = 1, . . . , I. Moreover,\n(2.10)\n\nsup\n\u03b8\u0303\u2208N\u03b4\u2032 (\u03b8 \u2032 )\n\nfor all x, y \u2208 D and \u03b8\u2032 \u2208 \u0398.\n\ne \u2192 0 as \u03b4 \u2032 \u2192 0\n|lij (x, y; \u03b8\u2032 , \u03b8)|\n\nAssumption A1 is a mild regularity condition to exclude unrealistic models. A2\nis a positive information criterion: the first inequality makes sure that information\nis available in the first arm group to estimate \u03b8; while the second inequality allows\nus to collect information in the i-th arm group for moving to the next group when\n\u03b8 \u2208 \u0398l for some l > i. Assumption A3 is a recurrence condition and A4 is a drift\ncondition. These two conditions are used to guarantee the stability of the Markov\nchain so that the strong law of large numbers and Wald's equation hold. A5 is a\nfinite second moment condition that allows us to bound the probability that the\nMLE of \u03b8 lies outside a small neighborhood of \u03b8. This bound is important for us\nto determine the level of unequal allocation of observations that can be permitted\nin the testing stage of our procedure. The proof of the asymptotic lower bound in\nTheorem 1 requires only A1-A3; while additional A4 and A5 are required for the\nconstruction of efficient strategies attaining the lower bound.\n2.2. Bad sets\nThe bad set is a useful concept for understanding the learning required within the\ngroup containing optimal arms. It is associated with the asymptotic lower bound\ndescribed in Section 3 and is used explicitly in constructing the asymptotically\nefficient strategy. For \u03b8 \u2208 \u0398l , define J(\u03b8) = {j : \u03bc\u2217 (\u03b8) = \u03bclj (\u03b8)} as the set of\noptimal jobs in group l. Hence \u03b8 \u2208 \u0398lj if and only if j \u2208 J(\u03b8). We also define\nthe bad set, the set of 'bad' parameter values associated with \u03b8, as all \u03b8\u2032 \u2208 \u0398l\nwhich cannot be distinguished from \u03b8 by processing any of the optimal jobs lj.\nSpecifically,\no\nn\n[\n\u0001\n\u0398lj : Ilj (\u03b8, \u03b8\u2032 ) = 0 for all j \u2208 J(\u03b8) .\n(2.11)\nBl (\u03b8) = \u03b8\u2032 \u2208 \u0398l \\\nj\u2208J(\u03b8)\n\nThe bad set Bl (\u03b8) is the intersection of two parameter sets. One set consists of\nparameter values that have different optimal arms from those for \u03b8. The other set\ncontains parameter values that cannot be distinguished from sampling the optimal\n\n\fMulti-armed bandit problem with precedence relations\n\n227\n\narm for \u03b8. When a parameter value is in the intersection, sampling from arms that\nare non-optimal for \u03b8 is required.\nWe note that if Ilj (\u03b8, \u03b8\u2032 ) = 0, then the transition probabilities of Xljt are identical under both \u03b8 and \u03b8\u2032 . If \u03b8\u2032 \u2208 Bl (\u03b8), then by definition, \u03b8\u2032 6\u2208 \u222aj\u2208J(\u03b8) \u0398lj and hence\nJ(\u03b8\u2032 ) \u2229 J(\u03b8) = \u2205. Let j \u2208 J(\u03b8) and j \u2032 \u2208 J(\u03b8\u2032 ). Then \u03bclj \u2032 (\u03b8\u2032 ) > \u03bclj (\u03b8\u2032 ) = \u03bclj (\u03b8) >\n\u03bclj \u2032 (\u03b8). Thus\nIlj \u2032 (\u03b8, \u03b8\u2032 ) > 0 for all \u03b8\u2032 \u2208 Bl (\u03b8) and j \u2032 \u2208 J(\u03b8\u2032 ).\n\n(2.12)\n\nThe interpretation of (2.12) is as follows. Although we cannot distinguish \u03b8 from\n\u03b8\u2032 \u2208 Bl (\u03b8) when sampling the optimal arm for \u03b8, we can distinguish them by\nsampling the optimal job for \u03b8\u2032 . This fact explains the necessity of processing nonoptimal arms to collect information.\n3. The regret lower bound\nThe following theorem gives an asymptotic lower bound for the regret (1.4) of\nuniformly good adaptive strategies under the partial order constraint \u0016. The proof\ncan be found in [1]. We will discuss the relation of the lower bound with those in\n[6, 7] and [3].\nTheorem 1. Assume A1-A3 and let \u03b8 \u2208 \u0398l . For any uniformly good adaptive\nstrategy \u03c6 under the partial order constraint \u0016,\nlim inf RN (\u03b8)/log N \u2265 z(\u03b8, l),\n\n(3.1)\n\nN \u2192\u221e\n\nwhere z(\u03b8, l) is the minimum value of the following minimization problem.\n(3.2)\n\nMinimize\n\nJi\nXX\n\n[\u03bc\u2217 (\u03b8) \u2212 \u03bcij (\u03b8)]zij (\u03b8) +\n\ni<l j=1\n\nX\n\n[\u03bc\u2217 (\u03b8) \u2212 \u03bclj (\u03b8)]zlj (\u03b8),\n\nj \u2208J(\u03b8)\n/\n\nsubject to zij (\u03b8) \u2265 0, j = 1, . . . , Ji , if i < l, j \u2208\n/ J(\u03b8), if i = l,\nand\n\uf8f1\nP 1\n\uf8f4\ninf \u03b8\u2032 \u2208\u03981 { Jj=1\nI1j (\u03b8, \u03b8\u2032 )z1j (\u03b8)} \u2265 1,\n\uf8f4\n\uf8f4\nP\nP J2\n\uf8f4\nJ1\n\uf8f4\n\uf8f4\ninf \u03b8\u2032 \u2208\u03982 { j=1 I1j (\u03b8, \u03b8\u2032 )z1j (\u03b8) + j=1\nI2j (\u03b8, \u03b8\u2032 )z2j (\u03b8)} \u2265 1,\n\uf8f4\n\uf8f2\n..\n(3.3)\n.\n\uf8f4\n\uf8f4\nPJ1\nPJl\u22121\n\uf8f4inf \u2032\n\u2032\n\uf8f4\nI(l\u22121)j (\u03b8, \u03b8\u2032 )z(l\u22121)j (\u03b8)} \u2265 1,\n\u03b8 \u2208\u0398l\u22121 {\n\uf8f4\nj=1 I1j (\u03b8, \u03b8 )z1j (\u03b8) + * * * +\n\uf8f4\nP\nP\nPj=1\n\uf8f4\nJi\n\uf8f3\n\u2032\nIlj (\u03b8, \u03b8\u2032 )zlj (\u03b8)} \u2265 1.\ninf \u03b8\u2032 \u2208Bl (\u03b8) { i<l j=1 Iij (\u03b8, \u03b8 )zij (\u03b8) + j \u2208J(\u03b8)\n/\nCorollary 1. When there is only one group of arms, (3.1) reduces to the lower\nbound (1.11) of Lai and Robbins [7].\n\nProof. When there is only group of arms, only the last inequality of (3.3) is needed\nand it takes the form\n\n(3.4)\n\ninf\n\n\u03b8 \u2032 \u2208B(\u03b8)\n\nX\n\nj \u2208J(\u03b8)\n/\n\nIj (\u03b8, \u03b8\u2032 )zj (\u03b8) \u2265 1.\n\n\f228\n\nH. P. Chan, C.-D. Fuh and I. Hu\n\nIn [7], it is proved that\nE\u03b8 TN (j) \u2265\n\n(3.5)\n\nlog N\nfor all j \u2208\n/ J(\u03b8),\nI(\u03b8j , \u03b8\u2217 )\n\nwhere \u03b8\u2217 = max1\u2264i\u2264k \u03b8i . Note that in [7], all jobs belong to the same family of\nprobability distributions with different parameter values, and thus the KL information number does not depend on the job label but only the parameter value. Let\nE\u03b8 TN (j)/ log N = zj (\u03b8), then (3.5) is the same as\nzj (\u03b8)I(\u03b8j , \u03b8\u2217 ) \u2265 1 for all j \u2208\n/ J(\u03b8).\n\n(3.6)\n\nWe first show that (3.4) \u21d2 (3.5). Because (3.4) implies that for all \u03b8\u2032 \u2208 B(\u03b8)\nX\nI(\u03b8j , \u03b8j\u2032 )zj (\u03b8) \u2265 1.\n(3.7)\nj \u2208J(\u03b8)\n/\n\nIf \u03b8\u2032 = (\u03b81\u2032 , . . . , \u03b8k\u2032 ) \u2208 B(\u03b8), then \u03b8\u2217 = \u03b8j \u2217 = \u03b8j\u2032 \u2217 and max1\u2264i\u2264k \u03b8i\u2032 > \u03b8\u2217 . Suppose we\nchoose a sequence of \u03b8\u2032 \u2208 B(\u03b8) such that there is only one component \u03b8j\u2032 approaching\n\u03b8\u2217 from above and other components \u03b8j\u2032 \u2032 , j \u2032 \u2208\n/ J(\u03b8), all have the same values as the\ncorresponding components of \u03b8. Taking infimum over this sequence of \u03b8\u2032 \u2208 B(\u03b8) in\n(3.7), we obtain (3.6). This complete the proof of (3.4) \u21d2 (3.5).\nTo prove (3.5) \u21d2 (3.4), we assume that (3.4) does not hold. That is, there exist\na \u03b8\u2032 \u2208 B(\u03b8) such that\nX\nI(\u03b8j , \u03b8j\u2032 )zj (\u03b8) < 1.\nj \u2208J(\u03b8)\n/\n\n\u2032\n\n\u2032\n\u2032\nBecause \u03b8 \u2208 B(\u03b8), there exists at least one component \u03b8j\u2217\nof \u03b8\u2032 such that \u03b8j\u2217\n> \u03b8\u2217 .\nThen the preceding inequality and the property of exponential families imply that\n\nzj \u2217 (\u03b8)I(\u03b8j \u2217 , \u03b8\u2217 ) < zj \u2217 (\u03b8)I(\u03b8j \u2217 , \u03b8j\u2032 \u2217 ) < 1,\nand thus (3.6) does not hold. This establishes (3.5) \u21d2 (3.4) and the proof is complete.\nCorollary 2. When there is only one arm in each group, then (3.1) reduces to the\nlower bound (1.17) of Hu and Wei [6].\nProof. In Hu and Wei [6], the set \u0398i are intervals of R. Thus the infimum over \u0398i\nis achieved at the end points of the intervals. Furthermore, because there is only\none arm in each group, the bad sets are all empty and therefore the last inequality\nin (3.3) is not needed. In view of these facts, it is straightforward to show that\nthe systems of inequalities (3.3) reduces to (1.14) of Hu and Wei [6]. The proof is\ncomplete.\nCorollary 3. When there is only one arm in each group, the lower bound (3.1)\nreduces to (3.2) of Fuh and Hu [3].\nProof. The assumptions A3 and A4 of Fuh and Hu [3] correspond to the regularity\ncondition A1 and the positive information criterion A2 in Section 2, respectively.\nThe A1, A2 and A5 of Fuh and Hu are essentially the same as Harris recurrence\ncondition A3, the drift condition A4, and the finite second moment condition A5\nof this paper, respectively.\nNote that the definition of bad sets in [3] is different from that of this paper.\nIn [3], the bad set consists of all those parameter values having optimal arm not\n\n\fMulti-armed bandit problem with precedence relations\n\n229\n\nin the same group and cannot be distinguished when sampling from the optimal\narm. Here the bad set consists of parameter values that has different optimal arm\n(but still in the same group), and cannot be distinguished when sampling from the\noptimal arm(s). If we adopt the definition (2.11), then it is clear that the bad sets\nare all empty under the setting of [3].\nThe infimums in Problem A of Fuh and Hu [3] is taken over the union of \u0398i and\nthe corresponding bad set. Because the bad sets in [3] are all empty as we point\nout earlier, the infimums is actually taken over \u0398i . With this understanding, it is\nstraightforward to verify that the lower bound (3.1) reduces to (3.2) of Fuh and\nHu [3].\n4. Super efficiency\nThe strategy in the allocation of the observations is as follows. For the rationale\nof the proposed strategy and more detailed discussion, please see [1]. Let n0 and\nn1 be positive integers that increase to infinity with respect to N and satisfies\nn0 = o(log N ) and n1 = o(n0 ).\n1. Estimation. Select n0 observations from each arm in group 1 and let \u03b8b be the\nmaximum likelihood estimate (MLE) of \u03b8 defined by\n(4.1)\n\nL(\u03b8) =\n\nn0\nJ X\nX\nj=1 t=1\n\nlog p1j (X1j(t\u22121) , X1jt ; \u03b8), \u03b8b = arg max L(\u03b8).\n\u03b8\u2208\u0398\n\nb \u2229 \u0398i 6= \u2205}. Select an adjusted MLE estimate \u03b8ba \u2208 N\u03b4/2 (\u03b8)\nb\u2229\nLet l = min{i : N\u03b4/2 (\u03b8)\n\u0398l , (where \u03b4 \u2192 0 as N \u2192 \u221e at a rate to be specified in Theorem 1 below), in the\nfollowing manner. Let | * | denote the number of elements in a finite set and\nb \u2229 \u0398l }.\nJ = max{|J(\u03b8\u2032 )| : \u03b8\u2032 \u2208 N\u03b4/2 (\u03b8)\n\n(4.2)\nWe require that\n\nb \u2229 \u0398l : |J(\u03b8)| = J}.\n\u03b8ba \u2208 H := {\u03b8 \u2208 N\u03b4/2 (\u03b8)\n\n(4.3)\n\nThe motivation behind considering an adjusted MLE is to estimate J(\u03b8) and the\nset \u0398i that \u03b8 belongs to consistently. This has implications in the experimentation\nb need not be consistent for J(\u03b8) and if\nphase. We note that if |J(\u03b8)| > 1, then J(\u03b8)\n\u2217\n\u0398i lies on \u0398i \\ \u0398i [see (2.2) and (2.4)], then \u03b8b need not be consistently inside \u0398i .\nConversely, the probability that J(\u03b8ba ) = J(\u03b8) and \u03b8ba lying inside \u0398i tends to 1 as\nN \u2192 \u221e.\nLet\nBl (\u03b8; \u03b4) = \u222a\u03b8\u2032 \u2208H Bl (\u03b8\u2032 )\n\n(4.4)\n\nand let {b\nzij }1\u2264i\u2264l,1\u2264j\u2264Ji minimize\n(4.5)\n\nJi\nXX\ni<l j=1\n\n[\u03bc\u2217 (\u03b8ba ) \u2212 \u03bcij (\u03b8ba )]zij +\n\nX\n\nj6\u2208J(b\n\u03b8a )\n\n[\u03bc\u2217 (\u03b8ba ) \u2212 \u03bclj (\u03b8ba )]zlj\n\n\f230\n\nH. P. Chan, C.-D. Fuh and I. Hu\n\nsubject to the constraints\n\uf8f1\nPJ1\n\u2032\n\uf8f4\n\uf8f4 inf \u03b8\u2032 \u2208\u03981 {Pj=1 I1j (\u03b8ba , \u03b8 )z1j } \u2265P1,\n\uf8f4\n\uf8f4\nJ1\nJ2\n\uf8f4\n\uf8f4\n{ j=1\ninf \u2032\nI1j (\u03b8ba , \u03b8\u2032 )z1j + j=1\nI2j (\u03b8ba , \u03b8\u2032 )z2j } \u2265 1,\n\uf8f4\n\uf8f2 \u03b8 \u2208\u03982\n..\n(4.6)\n\uf8f4.\nP 1\nPJl\u22121\n\uf8f4\n\uf8f4\n\uf8f4\ninf \u03b8\u2032 \u2208\u0398l\u22121 { Jj=1\nI1j (\u03b8ba , \u03b8\u2032 )z1j + * * * + j=1\nI(l\u22121)j (\u03b8ba , \u03b8\u2032 )z(l\u22121)j } \u2265 1,\n\uf8f4\n\uf8f4\nP\nP\nP\n\uf8f4\nJ\ni\n\uf8f3 inf\n{ i<l j=1\nIij (\u03b8ba , \u03b8\u2032 )zij + j \u2208J(\nI (\u03b8b , \u03b8\u2032 )zlj } \u2265 1.\n\u03b8 \u2032 \u2208Bl (b\n\u03b8;\u03b4)\n/ b\n\u03b8a ) lj a\n\nLet k = 1.\n\n2. Experimentation. If k \u2264 l, select \u230ab\nzkj log N \u230b observations from arm kj, where\n\u230a*\u230b denotes the greatest integer function. If k > l, we skip the experimentation stage.\nb \u03b4) is empty, then the last inequality in (4.6) is automatically\nWe note that if Bl (\u03b8;\nb \u03b4) is\nsatisfied and hence we can select zbl1 = * * * = zblJl = 0. In other words, if Bl (\u03b8;\nempty, then the experimentation stage is also skipped over for k = l.\n\n3. Testing. Start with a full set {k1, . . . , kJk } of unrejected jobs. The rejection\nof a job is based on the following test statistic. Let Fk , 1 \u2264 k \u2264 I, be a probability\ndistribution with positive probability on all open subsets of \u222aIi=k \u0398i . Define\n(4.7)\nR\nQk QJi\nQnij\nt=1 pij (Xij(t\u22121) , Xijt ; \u03b8) dFk (\u03b8)\ni=1\nj=1 \u03bdij (Xij0 ; \u03b8)\n\u222aIi=k \u0398i\nUk (n; \u03bb) =\nQk QJi\nQnij\nt=1 pij (Xij(t\u22121) , Xijt ; \u03bb)\ni=1\nj=1 \u03bdij (Xij0 ; \u03bb)\n\nfor all \u03bb \u2208 \u0398k .\n(a) If \u03b8b \u2208 \u222ai>k \u0398i : Add one observation from each unrejected job. Reject parameter \u03bb if Uk (n; \u03bb) \u2265 N . Reject a job kj if all \u03bb \u2208 \u0398kj have been rejected at some\npoint in the testing stage. If there is a job in group k left unrejected and the total\nnumber of observations is less than N , repeat 3(a). Otherwise go to step 4.\n\nb and\n(b) If \u03b8b \u2208 \u0398k : Add n1 observations from each unrejected job kj, j \u2208 J(\u03b8)\nb Reject a job kj if all \u03bb \u2208 \u0398kj\none observation from each unrejected job kj, j 6\u2208 J(\u03b8).\nhave been rejected at some point in the testing phase. If there is a job in group k\nleft unrejected and the total number of observations is less than N , repeat 3(b).\nOtherwise, go to step 4.\n(c) If \u03b8b \u2208 \u222ai<k \u0398i : Adopt the procedure of 3(a).\n\n4. Moving to the next group and termination. The strategy terminates once N\nobservations have been collected. Otherwise, if k < I, increment k by 1 and go to\nb =\nstep 2; if k = I, select all remaining observations from a job Ij satisfying \u03bcIj (\u03b8)\nb\nmax1\u2264h\u2264JI \u03bcIh (\u03b8).\n\nIn [1] Theorem 2, it was established that when Bl (\u03b8) is non-empty, then the\nasymptotic lower bound of the regret is attained with the procedure above. We\nshall show that the same procedure is not only asymptotically optimal but also\nthe regret from the optimal group will be o(log N ) when Bl (\u03b8) = \u2205 as oppose to\nO(log N ) when Bl (\u03b8) 6= \u2205. An important key step required in our proof is the\nconsistency result\n(4.8)\n\nb \u03b4) = \u2205} \u2192 1 as N \u2192 \u221e\nP\u03b8 {Bl (\u03b8,\n\nunder the empty bad set assumption.\n\n\fMulti-armed bandit problem with precedence relations\n\n231\n\nTheorem 2. Let \u03b8 \u2208 \u0398l . Assume A1-A5 and (1.5) . Let n0 \u2192 \u221e with n0 =\no(log N ) and n1 \u2192 \u221e such that n1 = o(n0 ). There exists \u03b4(= \u03b4N ) \u2193 0 as N \u2192 \u221e\nsuch that\n(4.9)\n\nP\u03b8 {\u03b8b \u2208 \u0398 \\ N\u03b4 (\u03b8)} = o(n\u22121\n1 ) as n1 \u2192 \u221e.\n\nMoreover, if Bl (\u03b8) = \u2205, then (4.8) holds and\n(4.10)\n\nJl\nX\n\nE\u03b8 TN (lj) = o(log N ).\n\nj=1\n\nHence\n(4.11)\n\nlim RN (\u03b8)/ log N = z(\u03b8, l).\n\nN \u2192\u221e\n\nProof. The consistency of \u03b8b in (4.9) follows from A2 and (4.5) of Chan, Fuh and\nHu [1]. We shall now prove (4.8). Since \u03b4 \u2193 0 and \u03b8b is consistent for \u03b8, it suffices\nfrom the definition of Bl (\u03b8; \u03b4) in (4.4) to show that there exists \u03b40 > 0 such that\n(4.12)\n\ne = J.\ne = \u2205 for all \u03b8e \u2208 N\u03b40 (\u03b8) \u2229 \u0398l with |J(\u03b8)|\nBl (\u03b8)\n\ne \u2282 J(\u03b8)\nWe observe from the continuity of \u03bclj that there exists \u03b41 > 0 such that J(\u03b8)\ne = |J(\u03b8)|, then it must be\nfor all \u03b8e \u2208 N\u03b41 (\u03b8) \u2229 \u0398l . Hence it follows that if |J(\u03b8)|\ne\ntrue that J(\u03b8) = J(\u03b8). We see from the definition of bad sets in (2.11) that for each\n\u03b8\u2032 \u2208 \u0398l \\ (\u222aj\u2208J(\u03b8) \u0398lj ), Ilj (\u03b8, \u03b8\u2032 ) > 0 for some j \u2208 J(\u03b8) and hence by the continuity\ne \u03b8\u2032 ) > 0\nof the Kullback-Leibler information, there exists \u03b42 > 0 such that Ilj (\u03b8,\ne\nwhenever \u03b8 \u2208 N\u03b42 (\u03b8). Select \u03b40 = min{\u03b41 , \u03b42 }. Then (4.12) holds.\nWe shall next show (4.10). By (4.8) and since the experimentation stage is\nb \u03b4) = \u2205, it suffices to show that the expected total\nskipped over when k = l and Bl (\u03b8,\nnumber of observations taken from inferior arms in the testing stage is o(log N ).\ne \u2282 J(\u03b8) for all\nDefine pN = P\u03b8 {J(\u03b8ba ) = J(\u03b8)}. Then by (4.3), (4.9) and as J(\u03b8)\n\u22121\ne\n\u03b8 \u2208 N\u03b41 (\u03b8) for some \u03b41 > 0, 1 \u2212 pN = o(n1 ). By (2.16) and the assumption\nBl (\u03b8) = \u2205, at least one optimal arm will provide positive information against each\n\u03b8\u2032 6\u2208 \u222aj\u2208J(\u03b8) \u0398j . By A3-A5 and (6.4), (6.5) of Chan, Fuh and Hu [1], (an expected)\nO(log N ) number of observations from arms with positive information is required\nto reject each \u03b8\u2032 \u2208 \u0398l \\ (\u222aj\u2208J(\u03b8) \u0398lj ). Hence O(n\u22121\n1 log N ) number of recursions is\ninvolved when J(\u03b8) = J(\u03b8ba ) because at least n1 observations in each recursion has\npositive information. Similarly, O(log N ) recursions is needed when J(\u03b8) 6= J(\u03b8ba )\nbecause at least one observation in each recursion has positive information. The\nnumber of observations from inferior arms in each recursion is O(1) if J(\u03b8ba ) = J(\u03b8)\nand O(n1 ) otherwise. Hence the expected number of observations from inferior arms\nduring the recursion steps in the testing phase is\n(4.13)\n\npN O(n\u22121\n1 log N ) + (1 \u2212 pN )O(n1 log N ) = o(log N ).\n\nThe asymptotic result (4.11) follows from (4.10) and the proof of Chan, Fuh and\nHu [1] Theorem 2.\nFor the special case l = 1, it follows from (4.11) that RN (\u03b8) = o(log N ) occurs. In\n[2] and [10], a uniformly good procedure was proposed that satisfies RN (\u03b8) = O(1)\nwhen \u0398 is finite and I = 1.\n\n\f232\n\nH. P. Chan, C.-D. Fuh and I. Hu\n\n5. The switching cost\nLet a(\u03b8) > 0 be the switching cost between two arms and are not both optimal\nwhen the underlying parameter is \u03b8. It is assumed here that there is no switching\ncost when both arms are optimal. Then\nLN (\u03b8) := a(\u03b8)E\u03b8\n\n\u22121\n\u0010 NX\n\n1{\u03c6t 6=\u03c6t+1 ,min[\u03bc\u03c6t (\u03b8),\u03bc\u03c6t+1 (\u03b8)]<\u03bc\u2217 (\u03b8)}\n\nt=1\n\n\u0011\n\nis the average switching cost of a procedure. It is also desirable that this cost is\nasymptotically negligible compared to the regret as N \u2192 \u221e.\nTheorem 3. Under Assumptions A1 - A5, the strategy \u03c6\u2217 has average switching\ncost\n(5.1)\n\nLN (\u03b8) = o(log N ) as N \u2192 \u221e.\n\nHence, the strategy is asymptotically optimal when there is switching cost.\nProof. In the estimation stage it is require to take n0 observations from each arm\nin group 1. We can take the n0 observations in batches and switch only J1 \u2212 1\ntimes. Therefore the switching cost from estimation stage is a(\u03b8)(J1 \u2212 1). In the\nexperimentation stage, we need to allocate at most zbkj log N observations to arm kj.\nAgain this can be done in batches and thus the switching cost from experimentation\nstage is at most a(\u03b8)(Jk \u2212 1). In the testing stage, it is shown in (6.12) of Chan, Fuh\nand Hu [1], that the expected total number of observations is o(log N ) and thus\nthe switching cost is no more than o(log N ). Adding the switching costs from the\nestimation, experimentation, and testing stages together, shows that the total cost\ndue to switching is o(log N ). However, the regret lower bound is O(log N ), which\nimplies that the switching cost constitutes a negligible part of the total regret as\nn \u2192 \u221e. This completes the proof that the proposed strategy is still asymptotically\noptimal with constant cost per switch.\n6. Extension of Wald's equation to Markovian rewards\nAs we will be focusing on a single arm ij and fixed parameters \u03b80 , \u03b8q such that\n\u03bc := Iij (\u03b80 , \u03b8q ) > 0 we will drop some of the references to i, j, \u03b80 , \u03b8q and q in\nthis section. This applies also to the notations in assumptions A3-A5. Moreover, we\nshall use the notation E(*) as a short form of E\u03b80 (*) and Ex (*) as a short form of\nE\u03b80 (*|X0 = x). Let Sn = \u03be1 + * * * + \u03ben , where \u03bek = log[pij (Xk\u22121 , Xk ; \u03b80 )/pij (Xk\u22121 ,\nXk ; \u03b8q )] has stationary mean \u03bc under P\u03b80 and let \u03c4 be a stopping-time. We shall\nshow that\n(6.1)\n\nES\u03c4 = \u03bc(E\u03c4 ) \u2212 E[\u03b3(X\u03c4 )] + E[\u03b3(X0 )]\n\nfor some function \u03b3 to be specified in Lemma 1. In Lemma 2, we show that the\nconditions on V in A4-A5 lead to bounds on \u03b3(x) and by applying Lemma 3, we\nobtain\n(6.2)\n\nE|\u03b3(X\u03c4 )| + E|\u03b3(X0 )| = o(E\u03c4 ).\n\nSubstituting (6.2) back into (6.1), Wald's equation\n(6.3)\n\nES\u03c4 = [\u03bc + o(1)]E\u03c4\n\n\fMulti-armed bandit problem with precedence relations\n\n233\n\nis established for Markovian rewards. Under uniform recurrence condition, Fuh and\nLai [4] established Wald's equation based on perturbation theory for the transition\noperator. The Wald's equation was proved under the assumption that the solution\nfor the Poisson equation exists in [5] based on Poisson equation for the transition\noperator. In this section, we apply the idea of regeneration epoch to derive the\nWald's equation for Markov random walks.\nBy (2.5), we can augment the Markov additive process and create a split chain\ncontaining an atom, so that increments in Sn between visits to the atom are independent. More specifically, we construct stopping-times 0 < \u03ba(1) < \u03ba(2) < * * *\nusing an auxiliary randomization procedure such that\n\n(6.4)\n\nP {Xn+1 \u2208 A, \u03ba(i) = n + 1|Xn = x, \u03ba(i) > n \u2265 \u03ba(i \u2212 1)}\n(\n\u03b1\u03c6(A) x \u2208 G,\n=\n0\notherwise.\n\nThen by Lemma 3.1 of Ney and Nummelin [9],\n(i) {\u03ba(i + 1) \u2212 \u03ba(i) : i = 1, 2, . . .} are i.i.d. random variables.\n(ii) the random blocks {X\u03ba(i) , . . . , X\u03ba(i+1)\u22121 }, i = 1, 2, . . . , are independent and\n(iii) P {X\u03ba(i) \u2208 A|F\u03ba(i)\u22121 } = \u03c6(A), where Fn =\u03c3-field generated by {X0 , . . . , Xn }.\nBy (ii)-(iii), E\u03c6 (S\u03ba \u2212 \u03ba\u03bc) = 0. Define \u03ba = \u03ba(1). We shall use the notation \"n =\natom\" to denote n = \u03ba(i) for some i.\nLemma 1. Let \u03b3(x) = Ex (S\u03ba \u2212 \u03ba\u03bc). Then Zn = (Sn \u2212 n\u03bc) + \u03b3(Xn ) is a martingale\nwith respect to Fn . Hence (6.1) holds.\nProof. We can express\nZn = E(SUn \u2212 Un \u03bc|Fn )\n\nwhere Un = inf{m > n : m = atom}.\n\nIf Xn = xn 6\u2208 G, then by (6.4), Un > n + 1. Hence Un+1 = Un and\nE(Zn+1 |Fn ) = Zn\n\n(6.5)\n\nbecause Fn+1 \u2283 Fn . If Xn = xn \u2208 G, then by (6.4) and (ii),\nE(Zn+1 |Fn ) \u2212 Zn = E[(SUn+1 \u2212 SUn ) + (Un+1 \u2212 Un )|Fn ] = \u03b1E\u03c6 (S\u03ba \u2212 \u03ba\u03bc) = 0\nand hence (6.5) also holds.\nLemma 2. Under A3-A5,\n|\u03b3(x)| \u2264 \u03b2 \u22121 [V (x) + b + (V \u2217 + b)V \u2217 (\u03b1\u22121 + 1)](K + 1 + |\u03bc|),\nwhere \u03b1 satisfies (2.5), V \u2217 is defined in A4 and K is defined in (2.9).\nProof. By (2.9),\n(6.6)\n\nV (x) \u2265 K \u22121 Ex \u03be12 \u2265 K \u22121 (Ex |\u03be1 | \u2212 1).\n\nLet 0 < \u03c3(1) < \u03c3(2) < * * * be the hitting times of the set G and let \u03c3 = \u03c3(1). Let\n(6.7)\n\nmn (A) = Ex\n\n\u03ba\nhX\n\nn=1\n\nV (Xn )1{Xn \u2208A}\n\ni\n\n\f234\n\nH. P. Chan, C.-D. Fuh and I. Hu\n\nfor all measurable set A \u2282 D. By (2.7),\nEx [V (Xn )1{\u03c3\u2265n} ] \u2264 (1 \u2212 \u03b2)Ex [V (Xn\u22121 )1{\u03c3\u2265n\u22121} ], n \u2265 2\nand\nEx [V (X1 )] \u2264 V (x) + b.\nHence by induction,\n(6.8)\n\nEx\n\n\u03c3\nhX\n\nn=1\n\n\u221e\ni\nX\nV (Xn ) \u2264 [V (x) + b]\n(1 \u2212 \u03b2)n\u22121 = [V (x) + b]/\u03b2.\nn=1\n\nBy (6.7)-(6.8), and as V \u2265 1,\nmn (D) = Ex\n\n\u03c3\nnX\n\nV (Xn ) +\n\nn=1\n\n= Ex\n\n\u03c3\nnX\n\nn=1\n\nV (Xn ) +\n\n\u221e h\nX\n\nk=1\n\u221e\nX\n\nk=1\n\n\u2264 \u03b2 \u22121 [V (x) + b] + Ex\n\n\u03c3(k+1)\n\nX\n\ni\no\nV (Xn ) 1{\u03ba>\u03c3(k)}\n\nn=\u03c3(k)+1\n\u03c3\nhX\n\nEX\u03c3(k)\n\n\u221e\nnX\n\nn=1\n\ni\no\nV (Xn ) 1{\u03ba>\u03c3(k)}\n\n\u03b2 \u22121 [V (X\u03c3(k) ) + b]1{\u03ba>\u03c3(k)}\n\nk=1\n\n\u2264 \u03b2 \u22121 [V (x) + b] + \u03b2 \u22121 (V \u2217 + b)mn (G).\n\n(6.9)\n\no\n\nBut by (6.4), mn (G) \u2264 V \u2217 (\u03b1\u22121 + 1). Since \u03b3(x) \u2264 (K + 1 + |\u03bc|)mn (D), Lemma 2\nholds.\nLet Wi = |\u03b3(X\u03ba(i) )| + * * * + |\u03b3(X\u03ba(i+1)\u22121 )|, for i \u2265 1. Then by A3-A5, Lemma\n4 and its proof, and (i)-(iii), W1 , W2 , . . . are i.i.d. with finite mean while by (2.8),\nW0 := |\u03b3(X0 )| + * * * + |\u03b3(X\u03ba(1)\u22121 )| also has finite mean.\nLemma 3. Let Mn = max1\u2264k\u2264n Wk . Then for any stopping-time \u03c4 ,\n(6.10)\n\nE(M\u03c4 ) = o(E\u03c4 ).\n\nProof. Let \u03b4 > 0 and let c(= c\u03b4 ) > 0 be large enough such that E[(W1 \u2212 c)+ ] \u2264 \u03b4.\nWe shall show that\nZn = (Mn \u2228 c) \u2212 n\u03b4\nis a supermartingale. Indeed for any \u03bb \u2265 0,\nE[Mn+1 \u2228 c|Mn \u2228 c = c + \u03bb] = c + \u03bb + E[(Wn+1 \u2212 c \u2212 \u03bb)+ ] \u2264 c + \u03bb + \u03b4\nand the claim is shown. Hence EZ\u03c4 \u2264 EZ0 = c and it follows that E(M\u03c4 ) \u2264\nE(M\u03c4 \u2228 c) \u2264 \u03b4(E\u03c4 ) + c. Lemma 3 then follows by letting \u03b4 \u2193 0.\n7. Appendix\nProof. Proof of (1.3) Let Xijt denotes the tth observation taken from arm ij. Then\n(7.1)\n\nWN (\u03b8) \u2212\n\nI X\nJi\nX\ni=1 j=1\n\n\u03bcij (\u03b8)E\u03b8 TN (ij) \u2264\n\n\u221e\nI X\nJi X\nX\ni=1 j=1 t=1\n\n|E\u03b8 g(Xijt ) \u2212 \u03bcij (\u03b8)|.\n\n\fMulti-armed bandit problem with precedence relations\n\nFor any signed measure \u03bb on (D, D), let\nk\u03bbkVij =\n\n(7.2)\n\nsup\nh:|h|\u2264Vij\n\nZ\n\n235\n\nh(x)\u03bb(dx) .\n\nIt follows from Meyn and Tweedie ([8], p.367 and Theorem 16.0.1) that under A3\nand the geometric drift condition (2.7),\n(7.3)\n\n\u03c9ij :=\n\nsup\n\n\u221e\nX\n\n\u03b8\u2208\u0398,x\u2208D t=1\n\n\u03b8\nPijt\n(x, *)\n\n\u03b8\nkPijt\n(x, *) \u2212 \u03c0ij (\u03b8)kVij /Vij (x) < \u221e,\n\nwhere\ndenotes the distribution of Xijt conditioned on Xij0 = x and \u03c0ij (\u03b8)\ndenotes the stationary distribution of Xijt under parameter \u03b8. By (2.6), there exists\n\u03ba > 0 such that \u03ba|g(x)| \u2264 Vij (x) for all x \u2208 D and hence it follows from (7.2) and\n(7.3) that\n(7.4)\n\n\u03ba\n\n\u221e\nX\n\n|E\u03b8,x g(Xijt ) \u2212 \u03bcij (\u03b8)| \u2264 \u03c9ij Vij (x),\n\nt=1\n\nwhere E\u03b8,x denotes expectation with respect to P\u03b8 and intial distribution Xij0 = x.\nIn general, for any initial distribution \u03bdij (*; \u03b8), it follows from (2.8) and (7.4)\nthat\nZ X\n\u221e\n\u221e\nX\n|E\u03b8,x g(Xijt ) \u2212 \u03bcij (\u03b8)|\u03bdij (x; \u03b8)Q(dx) < \u221e\n|E\u03b8 g(Xijt ) \u2212 \u03bcij (\u03b8)| \u2264\nt=1\n\nt=1\n\nuniformly over \u03b8 \u2208 \u0398 and hence (1.3) follows from (7.1).\n\nReferences\n[1] Chan, H. P., Fuh, C. D. and Hu, I. (2005). Optimal strategies for a class\nof sequential control problems with precedence relations. Annals of Statistics,\nto appear.\n[2] Feldman, D. (1962). Contributions to the \"two-armed bandit\" problem. Annals of Mathematical Statistics 33 847\u2013856. MR0145625\n[3] Fuh, C.D. and Hu, I. (2000). Asymptotically efficient strategies for a stochastic scheduling problem with order constraints. Annals of Statistics 28 1670\u2013\n1695. MR1835036\n[4] Fuh, C. D. and Lai, T. L. (1998). Wald's equations, first passage times\nand moments of ladder variables in Markov random walks. Journal of Applied\nProbability 35 566\u2013580. MR1659504\n[5] Fuh, C. D. and Zhang, C. H. (2000). Poisson equation, moment inequalities\nand r-quick convergence for Markov random walks. Stochastic Processes and\ntheir Applications 87 53\u201367. MR1751164\n[6] Hu, I. and Wei, C. Z. (1989). Irreversible adaptive allocation rules. Annals\nof Statistics17 801\u2013823. MR0994269\n[7] Lai, T. L. and Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics 6 4\u201322. MR0776826\n[8] Meyn, S. P. and Tweedie, R. L. (1993). Markov Chain and Stochastic\nStability. Springer-Verlag, New York. MR1287609\n[9] Ney, P. and Nummelin, E. (1987). Markov additive processes I: eigenvalue\nproperties and limit theorems. Annals of Probability 15 561\u2013592. MR0885131\n[10] Rodman, L. (1978). On the many-armed bandit problem. Annals of Probability 6 491\u2013498. MR0494728\n\n\f"}