{"id": "http://arxiv.org/abs/cs/0307070v1", "guidislink": true, "updated": "2003-07-30T21:36:03Z", "updated_parsed": [2003, 7, 30, 21, 36, 3, 2, 211, 0], "published": "2003-07-30T21:36:03Z", "published_parsed": [2003, 7, 30, 21, 36, 3, 2, 211, 0], "title": "Modeling Belief in Dynamic Systems, Part I: Foundations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0307058%2Ccs%2F0307059%2Ccs%2F0307044%2Ccs%2F0307060%2Ccs%2F0307042%2Ccs%2F0307070%2Ccs%2F0307009%2Ccs%2F0307019%2Ccs%2F0307028%2Ccs%2F0307050%2Ccs%2F0307023%2Ccs%2F0307049%2Ccs%2F0307017%2Ccs%2F0307029%2Ccs%2F0307067%2Ccs%2F0307045%2Ccs%2F0307020%2Ccs%2F0307038%2Ccs%2F0307071%2Ccs%2F0307052%2Ccs%2F0307043%2Ccs%2F0307055%2Ccs%2F0307063%2Ccs%2F0307011%2Ccs%2F0307026%2Ccs%2F0307030%2Ccs%2F0307068%2Ccs%2F0307032%2Ccs%2F0307024%2Ccs%2F0307061%2Ccs%2F0307014%2Ccs%2F0307036%2Ccs%2F0307040%2Ccs%2F0307046%2Ccs%2F0307013%2Ccs%2F0307051%2Ccs%2F0307039%2Ccs%2F0307005%2Ccs%2F0307047%2Ccs%2F0307015%2Ccs%2F0307025%2Ccs%2F0307016%2Ccs%2F0307022%2Ccs%2F0307056%2Ccs%2F0307033%2Ccs%2F0307057%2Ccs%2F0307002%2Ccs%2F0307007%2Ccs%2F0307053%2Ccs%2F0307069%2Ccs%2F0307012%2Ccs%2F0307037%2Ccs%2F0307065%2Ccs%2F0502019%2Ccs%2F0502034%2Ccs%2F0502087%2Ccs%2F0502062%2Ccs%2F0502094%2Ccs%2F0502012%2Ccs%2F0502003%2Ccs%2F0502022%2Ccs%2F0502068%2Ccs%2F0502046%2Ccs%2F0502025%2Ccs%2F0502042%2Ccs%2F0502054%2Ccs%2F0502001%2Ccs%2F0502072%2Ccs%2F0502013%2Ccs%2F0502051%2Ccs%2F0502041%2Ccs%2F0502011%2Ccs%2F0502006%2Ccs%2F0502049%2Ccs%2F0502091%2Ccs%2F0502080%2Ccs%2F0502092%2Ccs%2F0502002%2Ccs%2F0502063%2Ccs%2F0502059%2Ccs%2F0502027%2Ccs%2F0502081%2Ccs%2F0502018%2Ccs%2F0502032%2Ccs%2F0502095%2Ccs%2F0502052%2Ccs%2F0502014%2Ccs%2F0502028%2Ccs%2F0502017%2Ccs%2F0502016%2Ccs%2F0502023%2Ccs%2F0502009%2Ccs%2F0502083%2Ccs%2F0502085%2Ccs%2F0502021%2Ccs%2F0502076%2Ccs%2F0502005%2Ccs%2F0502020%2Ccs%2F0502096%2Ccs%2F0502026%2Ccs%2F0502073&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Modeling Belief in Dynamic Systems, Part I: Foundations"}, "summary": "Belief change is a fundamental problem in AI: Agents constantly have to\nupdate their beliefs to accommodate new observations. In recent years, there\nhas been much work on axiomatic characterizations of belief change. We claim\nthat a better understanding of belief change can be gained from examining\nappropriate semantic models. In this paper we propose a general framework in\nwhich to model belief change. We begin by defining belief in terms of knowledge\nand plausibility: an agent believes p if he knows that p is more plausible than\nits negation. We then consider some properties defining the interaction between\nknowledge and plausibility, and show how these properties affect the properties\nof belief. In particular, we show that by assuming two of the most natural\nproperties, belief becomes a KD45 operator. Finally, we add time to the\npicture. This gives us a framework in which we can talk about knowledge,\nplausibility (and hence belief), and time, which extends the framework of\nHalpern and Fagin for modeling knowledge in multi-agent systems. We then\nexamine the problem of ``minimal change''. This notion can be captured by using\nprior plausibilities, an analogue to prior probabilities, which can be updated\nby ``conditioning''. We show by example that conditioning on a plausibility\nmeasure can capture many scenarios of interest. In a companion paper, we show\nhow the two best-studied scenarios of belief change, belief revisionand belief\nupdate, fit into our framework.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0307058%2Ccs%2F0307059%2Ccs%2F0307044%2Ccs%2F0307060%2Ccs%2F0307042%2Ccs%2F0307070%2Ccs%2F0307009%2Ccs%2F0307019%2Ccs%2F0307028%2Ccs%2F0307050%2Ccs%2F0307023%2Ccs%2F0307049%2Ccs%2F0307017%2Ccs%2F0307029%2Ccs%2F0307067%2Ccs%2F0307045%2Ccs%2F0307020%2Ccs%2F0307038%2Ccs%2F0307071%2Ccs%2F0307052%2Ccs%2F0307043%2Ccs%2F0307055%2Ccs%2F0307063%2Ccs%2F0307011%2Ccs%2F0307026%2Ccs%2F0307030%2Ccs%2F0307068%2Ccs%2F0307032%2Ccs%2F0307024%2Ccs%2F0307061%2Ccs%2F0307014%2Ccs%2F0307036%2Ccs%2F0307040%2Ccs%2F0307046%2Ccs%2F0307013%2Ccs%2F0307051%2Ccs%2F0307039%2Ccs%2F0307005%2Ccs%2F0307047%2Ccs%2F0307015%2Ccs%2F0307025%2Ccs%2F0307016%2Ccs%2F0307022%2Ccs%2F0307056%2Ccs%2F0307033%2Ccs%2F0307057%2Ccs%2F0307002%2Ccs%2F0307007%2Ccs%2F0307053%2Ccs%2F0307069%2Ccs%2F0307012%2Ccs%2F0307037%2Ccs%2F0307065%2Ccs%2F0502019%2Ccs%2F0502034%2Ccs%2F0502087%2Ccs%2F0502062%2Ccs%2F0502094%2Ccs%2F0502012%2Ccs%2F0502003%2Ccs%2F0502022%2Ccs%2F0502068%2Ccs%2F0502046%2Ccs%2F0502025%2Ccs%2F0502042%2Ccs%2F0502054%2Ccs%2F0502001%2Ccs%2F0502072%2Ccs%2F0502013%2Ccs%2F0502051%2Ccs%2F0502041%2Ccs%2F0502011%2Ccs%2F0502006%2Ccs%2F0502049%2Ccs%2F0502091%2Ccs%2F0502080%2Ccs%2F0502092%2Ccs%2F0502002%2Ccs%2F0502063%2Ccs%2F0502059%2Ccs%2F0502027%2Ccs%2F0502081%2Ccs%2F0502018%2Ccs%2F0502032%2Ccs%2F0502095%2Ccs%2F0502052%2Ccs%2F0502014%2Ccs%2F0502028%2Ccs%2F0502017%2Ccs%2F0502016%2Ccs%2F0502023%2Ccs%2F0502009%2Ccs%2F0502083%2Ccs%2F0502085%2Ccs%2F0502021%2Ccs%2F0502076%2Ccs%2F0502005%2Ccs%2F0502020%2Ccs%2F0502096%2Ccs%2F0502026%2Ccs%2F0502073&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Belief change is a fundamental problem in AI: Agents constantly have to\nupdate their beliefs to accommodate new observations. In recent years, there\nhas been much work on axiomatic characterizations of belief change. We claim\nthat a better understanding of belief change can be gained from examining\nappropriate semantic models. In this paper we propose a general framework in\nwhich to model belief change. We begin by defining belief in terms of knowledge\nand plausibility: an agent believes p if he knows that p is more plausible than\nits negation. We then consider some properties defining the interaction between\nknowledge and plausibility, and show how these properties affect the properties\nof belief. In particular, we show that by assuming two of the most natural\nproperties, belief becomes a KD45 operator. Finally, we add time to the\npicture. This gives us a framework in which we can talk about knowledge,\nplausibility (and hence belief), and time, which extends the framework of\nHalpern and Fagin for modeling knowledge in multi-agent systems. We then\nexamine the problem of ``minimal change''. This notion can be captured by using\nprior plausibilities, an analogue to prior probabilities, which can be updated\nby ``conditioning''. We show by example that conditioning on a plausibility\nmeasure can capture many scenarios of interest. In a companion paper, we show\nhow the two best-studied scenarios of belief change, belief revisionand belief\nupdate, fit into our framework."}, "authors": ["Nir Friedman", "Joseph Y. Halpern"], "author_detail": {"name": "Joseph Y. Halpern"}, "author": "Joseph Y. Halpern", "links": [{"href": "http://arxiv.org/abs/cs/0307070v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0307070v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.4; F.2.1", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0307070v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0307070v1", "arxiv_comment": null, "journal_reference": "Aritificial Intelligence 95:2, 1997, pp. 257-316", "doi": null, "fulltext": "Modeling Belief in Dynamic Systems.\nPart I: Foundations\n\narXiv:cs/0307070v1 [cs.AI] 30 Jul 2003\n\nNir Friedman\nComputer Science Division, 387 Soda Hall, University of California, Berkeley, CA\n94720, nir@cs.berkeley.edu, http://www.cs.berkeley.edu/\u00f1ir\n\nJoseph Y. Halpern\nComputer Science Department, Cornell University, Ithaca, NY 14853,\nhalpern@cs.cornell.edu, http://www.cs.cornell.edu/home/halpern\n\nAbstract\nBelief change is a fundamental problem in AI: Agents constantly have to update\ntheir beliefs to accommodate new observations. In recent years, there has been\nmuch work on axiomatic characterizations of belief change. We claim that a better\nunderstanding of belief change can be gained from examining appropriate semantic\nmodels. In this paper we propose a general framework in which to model belief\nchange. We begin by defining belief in terms of knowledge and plausibility: an agent\nbelieves \u03c6 if he knows that \u03c6 is more plausible than \u00ac\u03c6. We then consider some\nproperties defining the interaction between knowledge and plausibility, and show\nhow these properties affect the properties of belief. In particular, we show that\nby assuming two of the most natural properties, belief becomes a KD45 operator.\nFinally, we add time to the picture. This gives us a framework in which we can\ntalk about knowledge, plausibility (and hence belief), and time, which extends the\nframework of Halpern and Fagin for modeling knowledge in multi-agent systems.\nWe then examine the problem of \"minimal change\". This notion can be captured by\nusing prior plausibilities, an analogue to prior probabilities, which can be updated\nby \"conditioning\". We show by example that conditioning on a plausibility measure\ncan capture many scenarios of interest. In a companion paper, we show how the two\nbest-studied scenarios of belief change, belief revision and belief update, fit into our\nframework.\n\n\u22c6 Some of this work was done while both authors were at the IBM Almaden Research Center.\nThe first author was also at Stanford while much of the work was done. IBM and Stanford's\nsupport are gratefully acknowledged. The work was also supported in part by the Air Force Office\nof Scientific Research (AFSC), under Contract F49620-91-C-0080 and grant F94620-96-1-0323 and\nby NSF under grants IRI-95-03109 and IRI-96-25901. A preliminary version of this paper appears\n\nPreprint submitted to Elsevier Preprint\n\n11 September 2018\n\n\f1\n\nIntroduction\n\nIn order to act in the world we must make assumptions, such as \"the corridor is clear\" or\n\"my car is parked where I left it\". These assumptions, however, are defeasible. We can easily\nimagine situations where the corridor is blocked, or where the car is stolen. We call the\nlogical consequences of such defeasible assumptions beliefs. As time passes, we constantly\nobtain new information that might cause us to make additional assumptions or withdraw\nsome of our previous assumptions. The problem of belief change is to understand how beliefs\nshould change.\nThe study of belief change has been an active area in philosophy and in artificial intelligence [G\u00e4r88,KM91a]. In the literature, two instances of this general phenomenon have been\nstudied in detail: Belief revision [AGM85,G\u00e4r88] attempts to describe how an agent should\naccommodate a new belief (possibly inconsistent with his other beliefs) about a static world.\nBelief update [KM91a], on the other hand, attempts to describe how an agent should change\nhis beliefs as a result of learning about a change in the world. Belief revision and belief update\ndescribe only two of the many ways in which beliefs can change. Our goal is to construct a\nframework to reason about belief change in general. This paper describes the details of that\nframework. In a companion paper [FH97a] we consider the special cases of belief revision\nand update in more detail.\nPerhaps the most straightforward approach to belief change is to simply represent an agent's\nbeliefs as a closed set of formulas in some language and then put constraints on how these\nbeliefs can change. This is essentially the approach taken in [AGM85,G\u00e4r88,KM91a]; as their\nresults show, much can be done with this framework. The main problem with this approach\nis that it does not provide a good semantics for belief. As we hope to show in this paper\nand in [FH97a], such a semantics can give us a much deeper understanding of how and why\nbeliefs change. Moreover, this semantics provides the tools to deal with complicating factors\nsuch actions, external events, and multiple agents.\nOne standard approach to giving semantics to beliefs is to put a preference ordering on\nthe set of worlds that the agent considers possible. Intuitively, such an ordering captures\nthe relative likelihood of worlds. Various authors [Bou92,GP92,KM91a,Spo88] have then\ninterpreted \"the agent believes \u03c6\" as \"\u03c6 is true in the most plausible worlds that the agent\nconsiders possible\". An alternative approach is to put a probability measure over the set\nof possible worlds. Then we can interpret \"the agent believes \u03c6\" as \"the probability of \u03c6 is\nclose to 1\" [Pea89]. We examine a new approach to modeling uncertainty based on plausibility\nmeasures, introduced in [FH95,FH97b], where a plausibility measure just associates with an\nevent (i.e., a set of possible worlds) its plausibility, an element in some partially ordered set.\nThis approach is easily seen to generalize other approaches to modeling uncertainty, such\nin Proceedings of the 5th Conference on Theoretical Aspects of Reasoning About Knowledge, 1994,\npp. 44-64, under the title \"A knowledge-based framework for belief change, Part I: Foundations\".\nThis version is almost identical to one that will appear in Artificial Intelligence.\n\n2\n\n\fas probability measures, belief functions, and preference orderings. We interpret the \"agent\nbelieves \u03c6\" as \"the plausibility of \u03c6 is greater than that of \u00ac\u03c6\". As we show, this is often\n(but not always) equivalent to \"\u03c6 is true in the most plausible worlds\".\nBy modeling beliefs in this way, there is an assumption that the plausibility measure is part of\nthe agent's epistemic state. (This assumption is actually made explicitly in [Bou92,KLM90].)\nThis implies that the plausibility measure is subjective, that is, it describes the agent's\nestimate of the plausibility of each event. But actually, an even stronger assumption is\nbeing made: namely, that the agent's epistemic state is characterized by a single plausibility\nmeasure. We feel that this latter assumption makes the models less expressive than they\nought to be. In particular, they cannot represent a situation where the agent is not sure\nabout what is plausible, such as \"Alice does not know that it typically does not rain in\nSan Francisco in the summer\". To capture this, we need to allow Alice to consider several\nplausibility measures possible; in some it typically does not rain and in others it typically\ndoes. 1 As we shall see, this extra expressive power is necessary to capture some interesting\nscenarios of belief change.\nTo deal with this, in addition to plausibility measures, we add a standard accessibility relation\nto represent knowledge. Once we have knowledge in the picture, we define belief by saying\nthat an agent believes \u03c6 if she knows that \u03c6 is typically true. That is, according to all the\nplausibility measures she considers possible, \u03c6 is more plausible than \u00ac\u03c6.\nThe properties of belief depend on how the plausibility measure interacts with the accessibility relation that defines knowledge. We study these interactions, keeping in mind that\nplausibility generalizes probability. In view of this, it is perhaps not surprising that many\nof the issues studied by Fagin and Halpern [FH94a] when considering the interaction of\nknowledge and probability also arise in our framework. There are, however, a number of new\nissues that arise in our framework due to the interaction between knowledge and belief. As\nwe shall see, if we take what are perhaps the most natural restrictions on this interaction, our\nnotion of belief is characterized by the axioms of the modal logic KD45 (where an agent has\ncomplete introspective knowledge about her beliefs, but may have false beliefs). Moreover,\nthe interaction between knowledge and belief satisfies the standard properties considered\nby Kraus and Lehmann [KL88]. Although our major goal is not an abstract study of the\nproperties of knowledge and belief, we view the fact that we have a concrete interpretation\nunder which these properties can be studied to be an important side-benefit of our approach.\nHaving a notion of belief is not enough in order to study belief change. We want a framework\nthat captures the beliefs of the agent before and after the change. This is achieved by\nintroducing time explicitly into the framework. The resulting framework is an extension of\nthe framework of Halpern and Fagin [HF89] for modeling knowledge in multi-agent systems,\nand allows to talk about knowledge, plausibility (and hence belief), and time. This framework\nis analogous to combination of knowledge, probability and time studied in [HT93]. As we\n1\n\nIn fact, this issue is discussed by Boutilier [Bou92], although his framework does not allow him\nto represent such a situation.\n\n3\n\n\fshow by example, having knowledge, plausibility, and time represented explicitly gives us a\npowerful and expressive framework for capturing belief change.\nThis framework is particularly suited to studying how plausibility changes over time. One\nimportant intuition we would like to capture is that of minimal change. Suppose an agent\ngets new information at time t. Certainly we would expect his plausibility assessment (and\nhis beliefs) at time t + 1 to incorporate this new information; otherwise, we would expect his\nassessment at time t + 1 to have changed minimally from his assessment at time t. In probabilistic reasoning, it can be argued that conditioning captures this intuition. Conditioning\nincorporates the new information by giving it probability 1. Moreover, the relative probability\nof all events consistent with the new information is the same before and after conditioning,\nso, in this sense, conditioning changes things minimally. We focus here on a plausibilistic\nanalogue of conditioning and argue that it captures the intuition of minimal change in plausibilities. We can then proceed much in the spirit of the Bayesian approach, but starting with\na prior plausibility and conditioning. As we show, many situations previously studied in the\nliterature, such as diagnostic reasoning [Rei87], can be easily captured by using such prior\nplausibilities. Moreover, as we show in a companion paper [FH97a], belief revision and belief\nupdate-which both attempt to capture intuitions involving minimal change in beliefs-can\nbe captured in our framework by conditioning on an appropriate prior plausibility measure.\nThinking in terms of priors also gives us insight into other representations of belief change,\nsuch as those of [Bou94b,GP92,LS94].\nThe rest of this paper is organized as follows. In the next section, we review the syntax and\nsemantics of the standard approach to modeling knowledge using Kripke structures and show\nhow plausibility can be added to the framework. Much of our technical discussion of axiomatizations and decision procedures is closely related to that of [FH94a]. In Section 3.1, we\npresent our full framework which adds plausibility to the framework of [HF89] for modeling\nknowledge (and time) in multi-agent systems. In Section 4 we introduce prior plausibilities\nand show how they can be used. We conclude in Section 5 with some discussion of the general\napproach. Proofs of theorems are given in Appendix A.\n\n2\n\nKnowledge and Plausibility\n\nIn this section, we briefly review the standard models for knowledge and beliefs (see [HM92]\nfor further motivation and details), describe a notion of plausibility, and then show how to\ncombine the two notions. Finally, we compare the derived notion of belief with previous work\non the subject.\n4\n\n\f2.1 The Logic of Knowledge\n\nWe start by examining the standard models for knowledge and belief. The syntax for the logic\nof knowledge is simple: we start with primitive propositions and close off under conjunction,\nnegation, and the modal operators K1 , . . . , Kn . A formula such as Ki \u03c6 is read \"agent i knows\n\u03c6\". The logic of belief is the result of replacing the Ki operator by Bi . The formula, Bi \u03c6 is\nread \"agent i believes \u03c6\". The resulting languages are denoted LK and LB , respectively.\nThe semantics for these languages is given by means of Kripke structures. A Kripke structure\nfor knowledge (or belief) is a tuple (W, \u03c0, K1 , . . . , Kn ), where W is a set of possible worlds,\n\u03c0(w) is a truth assignment to the primitive propositions at world w \u2208 W , and the Ki 's are\naccessibility relations on the worlds in W . For convenience, we define Ki (w) = {w \u2032 : (w, w \u2032) \u2208\nKi }. Intuitively, Ki (w) describes the set of worlds that agent i considers possible in w. We\nsay that agent i knows (or believes) \u03c6 at world w, if all the worlds Ki (w) satisfy \u03c6.\nWe assign truth values to formulas at each world in the structure. We write (M, w) |= \u03c6 if\nthe formula \u03c6 is true at a world w in the Kripke structure M.\n\u2022\n\u2022\n\u2022\n\u2022\n\n(M, w) |= p for a primitive proposition p if \u03c0(w)(p) = true,\n(M, w) |= \u00ac\u03c6 if (M, w) 6|= \u03c6,\n(M, w) |= \u03c6 \u2227 \u03c8 if (M, w) |= \u03c6 and (M, w) |= \u03c8,\n(M, w) |= Ki \u03c6 if (M, w \u2032 ) |= \u03c6 for all w \u2032 \u2208 Ki (w).\n\nThe last clause captures the intuition that \u03c6 is known exactly when it is true in all possible\nworlds. When considering the language of beliefs LB , we typically use Bi rather than Ki to\ndenote the accessibility relations. The truth condition for Bi \u03c6 is exactly the same as for Ki \u03c6.\nLet MK be the class of Kripke structures described above. We say that \u03c6 \u2208 LK is valid\nin some M \u2208 MK if (M, w) |= \u03c6 for all w in M. We say that \u03c6 \u2208 LK is valid in MK if\nit is valid in all models M \u2208 MK . We say that \u03c6 is satisfiable in MK if there is a model\nM \u2208 MK and world w such that (M, w) |= \u03c6.\nThe definition of Kripke structure does not put any restriction on the Ki relations. By\nimposing conditions on the Ki relations we get additional properties of knowledge (or belief).\nThese properties are captured by systems of axioms that describe the valid formulas in classes\nof structures that satisfy various constraints of interest. We briefly describe these systems and\nthe corresponding constraints on the accessibility relations. Consider the following axioms\nand rules:\nK1.\nK2.\nK3.\nK4.\n\nAll substitution instances of propositional tautologies\nKi \u03c6 \u2227 Ki (\u03c6 \u21d2 \u03c8) \u21d2 Ki \u03c8\nKi \u03c6 \u21d2 \u03c6\nKi \u03c6 \u21d2 Ki Ki \u03c6\n5\n\n\fK5. \u00acKi \u03c6 \u21d2 Ki \u00acKi \u03c6\nK6. \u00acKi false\nRK1. From \u03c6 and \u03c6 \u21d2 \u03c8 infer \u03c8\nRK2. From \u03c6 infer Ki \u03c6\nThe system K contains the axioms K1 and K2 and the rules of inference RK1 and RK2. By\nadding axioms K4 and K5 we get system K45; if in addition we add axiom K6 we get system\nKD45; if instead we add axiom K3 to K45 we get the axiom system known as S5.\nWe now relate these axiom systems with restrictions on the accessibility relations. We start\nwith some definitions. A relation R on W is Euclidean if (x, y), (x, z) \u2208 R implies that\n(y, z) \u2208 R, for all x, y and z in W ; it is reflexive if (x, x) \u2208 R for all x \u2208 W ; it is serial if for\nall x \u2208 W there is a y such that (x, y) \u2208 R; and it is transitive if (x, y), (y, z) \u2208 R implies that\n(x, z) \u2208 R, for x, y and z in W . Let Met\nK be the set of Kripke structures with Euclidean and\nest\ntransitive accessibility relations, MK be the subset of Met\nK where the accessibility relations\net\nare also serial, and Mert\nbe\nthe\nsubset\nof\nM\nwhere\nthe\naccessibility relations are also\nK\nK\ntransitive.\nTheorem 1 [HM92] The axiom system K (resp. K45, KD45, S5) is a sound and complete\nert\nest\naxiomatization of LK with respect to MK (resp. Met\nK , MK , MK ).\nIn this paper, we use the multi-agent systems formalism of [FHMV95] to model knowledge;\nthis means that knowledge satisfies the axioms of S5. (We provide some motivation for this\nchoice below; see [FHMV95] for further discussion.)\nThis implies that if an agent knows \u03c6, then \u03c6 is true (K3) and that the agent is introspective-\nhe knows what he knows and does not know (K4 and K5). Belief, on the other hand, is\ntypically viewed as defeasible. Thus, it does not necessarily satisfy K3. It may satisfy a weaker\nproperty, such as K6, which says that the agent does not believe inconsistent formulas. Like\nknowledge, belief is taken to be introspective, as it satisfies K4 and K5. Thus, in the literature,\nbelief has typically been take to satisfy K45 or KD45; we do the same here. According to\nTheorem 1, this means that the notion of knowledge we use is characterized by Mert\nK while\nest 2\net\nbelief is characterized by MK or MK .\n\n2.2 Plausibility Measures\nMost non-probabilistic approaches to belief change require (explicitly or implicitly) that\nthe agent has some ordering over possible alternatives. For example, the agent might have a\npreference ordering over possible worlds [Bou94b,Gro88,KM91b] or an entrenchment ordering\n2\n\nAs is well known, a relation is reflexive, Euclidean and transitive if and only if it is an equivalence\nrelation (i.e., reflexive, symmetric and transitive). Thus, Mert\nK consists of these structures where\nthe Ki 's are equivalence relations.\n\n6\n\n\fover formulas [GM88]. This ordering dictates how the agent's beliefs change. For example, in\n[Gro88], the new beliefs are characterized by the most preferred worlds that are consistent\nwith the new observation, while in [GM88] beliefs are discarded according to their degree of\nentrenchment until it is consistent to add the new observation to the resulting set of beliefs.\nKeeping this insight in mind, we now describe plausibility measures [FH95,FH97b]. This is\na notion for handling uncertainty that generalizes previous approaches, including various\nnotions of preference ordering. We briefly review the relevant definitions and results here.\nRecall that a probability space is a tuple (W, F , Pr), where W is a set of worlds, F is\nan algebra of measurable subsets of W (that is, a set of subsets closed under union and\ncomplementation to which we assign probability), and Pr is a probability measure, that is, a\nfunction mapping each set in F to a number in [0, 1] satisfying the well-known probability\naxioms (Pr(\u2205) = 0, Pr(W ) = 1, and Pr(A \u222a B) = Pr(A) + Pr(B), if A and B are disjoint).\nA plausibility space is a direct generalization of a probability space. We simply replace the\nprobability measure Pr by a plausibility measure Pl, which, rather than mapping sets in F\nto numbers in [0, 1], maps them to elements in some arbitrary partially ordered set. We read\nPl(A) as \"the plausibility of set A\". If Pl(A) \u2264 Pl(B), then B is at least as plausible as A.\nFormally, a plausibility space is a tuple S = (W, F , Pl), where W is a set of worlds, F is\nan algebra of subsets of W , and Pl maps sets in F to some domain D of plausibility values\npartially ordered by a relation \u2264D (so that \u2264D is reflexive, transitive, and anti-symmetric).\nWe assume that D is pointed : that is, it contains two special elements \u22a4D and \u22a5D such that\n\u22a5D \u2264D d \u2264D \u22a4D for all d \u2208 D; we further assume that Pl(W ) = \u22a4D and Pl(\u2205) =\u22a5D . As\nusual, we define the ordering <D by taking d1 <D d2 if d1 \u2264D d2 and d1 6= d2 . We omit the\nsubscript D from \u2264D , <D , \u22a4D and \u22a5D whenever it is clear from context.\nSince we want a set to be at least as plausible as any of its subsets, we require\nA1 If A \u2286 B, then Pl(A) \u2264 Pl(B).\nSome brief remarks on this definition: We have deliberately suppressed the domain D of\nplausibility values from the tuple S, since for the purposes of this paper, only the ordering\ninduced by \u2264 on the subsets in F is relevant. The algebra F also does not play a significant\nrole in this paper. Unless we say otherwise, we assume F contains all subsets of interest and\nsuppress mention of F , denoting a plausibility space as a pair (W, Pl).\nClearly plausibility spaces generalize probability spaces. We now briefly discuss a few other\nnotions of uncertainty that they generalize:\n\u2022 A belief function B on W is a function B : 2W \u2192 [0, 1] satisfying certain axioms [Sha76].\nThese axioms certainly imply property A1, so a belief function is a plausibility measure.\n\u2022 A fuzzy measure (or a Sugeno measure) f on W [WK92] is a function f : 2W 7\u2192 [0, 1],\nthat satisfies A1 and some continuity constraints. A possibility measure [DP90] Poss is a\nfuzzy measure such that Poss(W ) = 1, Poss(\u2205) = 0, and Poss(A) = supw\u2208A (Poss({w}).\n\u2022 An ordinal ranking (or \u03ba-ranking) on W (as defined by [GP92], based on ideas that go\n7\n\n\fback to [Spo88]) is a function \u03ba : 2W \u2192 IN \u2217 , where IN \u2217 = IN \u222a {\u221e}, such that \u03ba(W ) = 0,\n\u03ba(\u2205) = \u221e, and \u03ba(A) = minw\u2208A (\u03ba({w})). Intuitively, an ordinal ranking assigns a degree\nof surprise to each subset of worlds in W , where 0 means unsurprising and higher numbers\ndenote greater surprise. It is easy to see that if \u03ba is a ranking on W , then (W, \u03ba) is a\nplausibility space, where x \u2264IN \u2217 y if and only if y \u2264 x under the usual ordering on the\nordinals.\n\u2022 A preference ordering on W is a partial order \u227a over W [KLM90,Sho87]. Intuitively, w \u227a w \u2032\nholds if w is preferred to w \u2032 . Preference orders have been used to provide semantics for\ndefault (i.e., conditional) statements. In [FH97b] we show how to map preference orders\non W to plausibility measures on W in a way that preserves the ordering of events of the\nform {w} as well as the truth values of defaults. We review these results below.\n\u2022 A parametrized probability distribution (PPD) on W is a sequence {Pri : i \u2265 0} of\nprobability measures over W . Such structures provide semantics for defaults in \u01eb-semantics\n[Pea89,GMP93]. In [FH97b] we show how to map PPDs into plausibility structures in a\nway that preserves the truth-values of conditionals (again, see discussion below).\n2.3 The Logic of Conditionals\nOur goal is to describe the agent's beliefs in terms of plausibility. To do this, we describe\nhow to evaluate statements of the form B\u03c6 given a plausibility space. In fact, we examine\na richer logical language that also allows us to describe how the agent compares different\nalternatives. This is the logic of conditionals. Conditionals are statements of the form \u03c6 \u2192 \u03c8,\nread \"given \u03c6, \u03c8 is plausible\" or \"given \u03c6, then by default \u03c8\". The syntax of the logic of\nconditionals is simple: we start with primitive propositions and close off under conjunction,\nnegation and the modal operator \u2192. The resulting language is denoted LC .\nMany semantics have been proposed in the literature for conditionals. Most of them involve\nstructures of the form (W, X, \u03c0), where W is a set of possible worlds, \u03c0(w) is a truth assignment to primitive propositions, and X is some \"measure\" on W such as a preference\nordering, a \u03ba-ranking, or a possibility measure. We now describe some of the proposals in\nthe literature, and then show how they can be viewed as using plausibility measures. Given\na structure (W, X, \u03c0), let [[\u03c6]] \u2286 W be the set of worlds satisfying \u03c6.\n\u2022 A possibility structure is a tuple (W, Poss, \u03c0), where Poss is a possibility measure on W .\nIt satisfies a conditional \u03c6 \u2192 \u03c8 if either Poss([[\u03c6]]) = 0 or Poss([[\u03c6 \u2227 \u03c8]]) > Poss([[\u03c6 \u2227 \u00ac\u03c8]])\n[DP91]. That is, either \u03c6 is impossible, in which case the conditional holds vacuously, or\n\u03c6 \u2227 \u03c8 is more possible than \u03c6 \u2227 \u00ac\u03c8.\n\u2022 A \u03ba-structure is a tuple (W, \u03ba, \u03c0), where \u03ba is an ordinal ranking on W . It satisfies a\nconditional \u03c6 \u2192 \u03c8 if either \u03ba([[\u03c6]]) = \u221e or \u03ba([[\u03c6 \u2227 \u03c8]]) < \u03ba([[\u03c6 \u2227 \u00ac\u03c8]]) [GP92].\n\u2022 A preferential structure is a tuple (W, \u227a, \u03c0), where \u227a is a partial order on W . The intuition\n[Sho87] is that a preferential structure satisfies a conditional \u03c6 \u2192 \u03c8 if all the most preferred\nworlds (i.e., the minimal worlds according to \u227a) in [[\u03c6]] satisfy \u03c8. However, there may be\nno minimal worlds in [[\u03c6]]. This can happen if [[\u03c6]] contains an infinite descending sequence\n8\n\n\f. . . \u227a w2 \u227a w1 . What do we do in these structures? There are a number of options: the first\nis to assume that, for each formula \u03c6, there are minimal worlds in [[\u03c6]]; this is the assumption\nactually made in [KLM90], where it is called the smoothness assumption. A yet more\ngeneral definition-one that works even if \u227a is not smooth-is given in [Lew73,Bou94a].\nRoughly speaking, \u03c6 \u2192 \u03c8 is true if, from a certain point on, whenever \u03c6 is true, so is \u03c8.\nMore formally,\n(W, \u227a, \u03c0) satisfies \u03c6 \u2192 \u03c8, if for every world w1 \u2208 [[\u03c6]], there is a world w2 such that (a)\nw2 \u0016 w1 (so that w2 is at least as normal as w1 ), (b) w2 \u2208 [[\u03c6 \u2227 \u03c8]], and (c) for all worlds\nw3 \u227a w2 , we have w3 \u2208 [[\u03c6 \u21d2 \u03c8]] (so any world more normal than w2 that satisfies \u03c6\nalso satisfies \u03c8).\nIt is easy to verify that this definition is equivalent to the earlier one if \u227a is smooth.\n\u2022 A PPD structure is a tuple (W, {Pri : i \u2265 0}, \u03c0), where {Pri } is PPD over W . Intuitively,\nit satisfies a conditional \u03c6 \u2192 \u03c8 if the conditional probability \u03c8 given \u03c6 goes to 1 in the\nlimit. Formally, \u03c6 \u2192 \u03c8 is satisfied if limi\u2192\u221e Pri ([[\u03c8]]|[[\u03c8]]) = 1 [GMP93] (where Pri ([[\u03c8]]|[[\u03c6]])\nis taken to be 1 if Pri ([[\u03c6]]) = 0).\nIn [FH97b] we use plausibility to provide semantics for conditionals and show that our\ndefinition generalizes the definition in the various approaches we just described. We briefly\nreview the definitions and results here.\nA plausibility structure is a tuple PL = (W,Pl, \u03c0), where Pl is a plausibility measure on W .\nConditionals are evaluated according to a rule that is essentially that used in possibility\nstructures:\n\u2022 PL |= \u03c6 \u2192 \u03c8 if either Pl([[\u03c6]]) =\u22a5 or Pl([[\u03c6 \u2227 \u03c8]]) > Pl([[\u03c6 \u2227 \u00ac\u03c8]]).\nIntuitively, \u03c6 \u2192 \u03c8 holds vacuously if \u03c6 is impossible; otherwise, it holds if \u03c6 \u2227 \u03c8 is more\nplausible than \u03c6 \u2227 \u00ac\u03c8. It is easy to see that this semantics for conditionals generalizes\nthe semantics of conditionals in possibility structures and \u03ba-structures. The following result\nshows that it also generalizes the semantics of conditionals in preferential structures and\nPPD structures.\nProposition 2 [FH97b]\n(a) If \u227a is a preference ordering on W , then there is a plausibility measure Pl\u227a on W such\nthat (W, \u227a, \u03c0) |= \u03c6 \u2192 \u03c8 if and only if (W, Pl\u227a , \u03c0) |= \u03c6 \u2192 \u03c8.\n(b) If P P = {Pri } is a PPD on W , then there is a plausibility measure PlP P such that\n(W, {Pri }, \u03c0) |= \u03c6 \u2192 \u03c8 if and only if (W, PlP P , \u03c0) |= \u03c6 \u2192 \u03c8.\nWe briefly describe the construction of Pl\u227a and PlP P here, since we use them in the sequel.\nGiven a preference order \u227a on W , let D0 be the domain of plausibility values consisting of\none element dw for every element w \u2208 W . We define a partial order on D0 using \u227a: dv < dw\nif w \u227a v. (Recall that w \u227a w \u2032 denotes that w is preferred to w \u2032 .) We then take D to be\nthe smallest set containing D0 that is closed under least upper bounds (so that every set of\nelements in D has a least upper bound in D). For a subset A of W , we can then define Pl\u227a (A)\n9\n\n\fto be the least upper bound of {dw : w \u2208 A}. Since D is closed under least upper bounds,\nPl\u227a (A) is well defined. As shown in [FH97b], this choice of Pl\u227a satisfies Proposition 2.\nThe construction in the case of PPD's is even more straightforward. Given a PPD P P =\n{P ri } on W , we define PlP P as follows:\nPlP P (A) \u2264 PlP P (B) if and only if limi\u2192\u221e Pri (B|A \u222a B) = 1.\nA straightforward argument shows that this choice of PlP P satisfies Proposition 2.\nThese results show that our semantics for conditionals in plausibility structures generalizes\nthe various approaches examined in the literature. Does it capture our intuitions about\nconditionals? In the AI literature, there has been discussion of the right properties of default\nstatements (which are essentially conditionals). While there has been little consensus on\nwhat the \"right\" properties for defaults should be, there has been some consensus on a\nreasonable \"core\" of inference rules for default reasoning. This core, known as the KLM\nproperties [KLM90], consists of the following axiom and rules of inference:\nLLE. From \u03c6 \u21d4 \u03c6\u2032 and \u03c6 \u2192 \u03c8 infer \u03c6\u2032 \u2192 \u03c8 (left logical equivalence)\nRW. From \u03c8 \u21d2 \u03c8 \u2032 and \u03c6 \u2192 \u03c8 infer \u03c6 \u2192 \u03c8 \u2032 (right weakening)\nREF. \u03c6 \u2192 \u03c6 (reflexivity)\nAND. From \u03c6 \u2192 \u03c81 and \u03c6 \u2192 \u03c82 infer \u03c6 \u2192 \u03c81 \u2227 \u03c82\nOR. From \u03c61 \u2192 \u03c8 and \u03c62 \u2192 \u03c8 infer \u03c61 \u2228 \u03c62 \u2192 \u03c8\nCM. From \u03c6 \u2192 \u03c81 and \u03c6 \u2192 \u03c82 infer \u03c6 \u2227 \u03c81 \u2192 \u03c82 (cautious monotonicity)\nLLE states that the syntactic form of the antecedent is irrelevant. Thus, if \u03c61 and \u03c62 are\nequivalent, we can deduce \u03c62 \u2192 \u03c8 from \u03c61 \u2192 \u03c8. RW describes a similar property of the\nconsequent: If \u03c8 (logically) entails \u03c8 \u2032 , then we can deduce \u03c6 \u2192 \u03c8 \u2032 from \u03c6 \u2192 \u03c8. This allows us\nto can combine default and logical reasoning. REF states that \u03c6 is always a default conclusion\nof \u03c6. AND states that we can combine two default conclusions: If we can conclude by default\nboth \u03c81 and \u03c82 from \u03c6, we can also conclude \u03c81 \u2227 \u03c82 from \u03c6. OR states that we are allowed\nto reason by cases: If the same default conclusion follows from each of two antecedents, then\nit also follows from their disjunction. CM states that if \u03c81 and \u03c82 are two default conclusions\nof \u03c6, then discovering that \u03c81 holds when \u03c6 holds (as would be expected, given the default)\nshould not cause us to retract the default conclusion \u03c82 .\nDo conditionals in plausibility structures satisfy the KLM properties? In general, the answer\nis no. It is almost immediate from the definition that a probability measure Pr is also a\nplausibility measure. Notice that Pr([[\u03c6\u2227\u03c8]]) > Pr([[\u03c6\u2227\u00ac\u03c8]]) if and only if Pr([[\u03c8]] | [[\u03c6]]) > 1/2.\nExpanding the semantics of conditionals, we get that \u03c6 \u2192 \u03c8 holds in Pr exactly if Pr([[\u03c6]]) = 0\nor Pr([[\u03c8]] | [[\u03c6]]) > 1/2. It is easy to see that this definition does not satisfy the AND rule:\nit is not in general the case that \u03c6 \u2192 \u03c81 and \u03c6 \u2192 \u03c82 together imply \u03c6 \u2192 (\u03c81 \u2227 \u03c82 ), since\nPr(A1 | B) > 1/2 and Pr(A2 | B) > 1/2 do not imply Pr(A1 \u2229 A2 |B) > 1/2. Since the AND\nrule is a fundamental feature of qualitative reasoning, we would like to restrict to plausibility\n10\n\n\fstructures where it holds. In [FH97b] we show that the following condition is necessary and\nsufficient to guarantee that the And rule holds:\nA2 If A, B, and C are pairwise disjoint sets, Pl(A \u222a B) > Pl(C), and Pl(A \u222a C) > Pl(B),\nthen Pl(A) > Pl(B \u222a C).\nIt turns out that conditionals in plausibility structures that satisfy A2 also satisfy LLE,\nRW, and CM. They also satisfy OR when one of the conditionals \u03c61 \u2192 \u03c8 and \u03c62 \u2192 \u03c8 is\nsatisfied non-vacuously (that is, in a plausibility measure Pl such that either Pl([[\u03c61 ]]) > \u22a5\nor Pl([[\u03c62 ]]) > \u22a5). To satisfy OR in general we need another condition:\nA3 If Pl(A) = Pl(B) =\u22a5, then Pl(A \u222a B) =\u22a5.\nA3 also has a nice axiomatic characterization. Let N\u03c6 be an abbreviation for \u00ac\u03c6 \u2192 false.\n(This operator is called the \"outer modality\" in [Lew73].) Expanding the definition of \u2192,\nwe get that N\u03c6 holds at w if and only if Pl([[\u00ac\u03c6]]) =\u22a5. Thus, N\u03c6 holds if \u00ac\u03c6 is considered\ncompletely implausible. We can think of the N modality as the plausibilistic version of\nnecessity. It is easy to show that A3 corresponds to an AND rule for N. It holds exactly if\n(N\u03c6 \u2227 N\u03c8) \u21d2 N(\u03c6 \u2227 \u03c8).\nA plausibility space (W, Pl) is qualitative if it satisfies A2 and A3. A plausibility structure\n(W, Pl, \u03c0) is qualitative if (W, Pl) is a qualitative plausibility space. In [FH97b] we show that,\nin a very general sense, qualitative plausibility structures capture default reasoning. More\nprecisely, we show that the KLM properties are sound with respect to a class of plausibility\nstructures if and only if the class consists of qualitative plausibility structures. We also show\nthat a very weak condition is necessary and sufficient in order for the KLM properties to be\ncomplete axiomatization of the language of default entailment considered in [KLM90]. These\nresults help explain why so many different approaches to giving semantics to conditionals\nare characterized by the KLM properties. In addition, as we shall see, it also shows that if\nwe want belief to have some reasonable properties, then we need to restrict to qualitative\nplausibility measures.\n2.4 Combining Knowledge and Plausibility\nWe now define a logic that combines knowledge and plausibility. Let LKC be the language\nobtained by starting with primitive propositions, and closing off under conjunction, negation,\nand the operators Ki and \u2192i , i = 1, . . . , n. Note that we have a different conditional operator\nfor each agent. We read \u03c6 \u2192i \u03c8 as \"according to agent i's plausibility measure, \u03c6 typically\nimplies \u03c8\".\nA (Kripke) structure (for knowledge and plausibility) is a tuple (W, \u03c0, K1 , . . . , Kn , P1 , . . . , Pn )\nwhere W , \u03c0 and Ki are just as in Kripke structures for knowledge, while Pi is a plausibility\nassignment, a function that assigns a plausibility space to agent i at each world. Intuitively,\nthe structure Pi (w) = (W(w,i) , Pl(w,i) ) captures agent i's plausibility measure in the world w.\n11\n\n\fFor now we allow W(w,i) to be an arbitrary subset of W . We discuss some possible restrictions\non W(w,i) below. It is reasonable to ask at this point where the plausibility spaces Pi (w) are\ncoming from, and why we need a different one for each agent at each world. The answer to\nthis question depends very much on the intended application. We defer further discussion of\nthis issue until later.\nWe can now give semantics to formulas in LKC in Kripke structures for knowledge and\nplausibility. This is done in a recursive way using the rules specified above for LK and LC .\nStatements of the form Ki \u03c6 are evaluated according to Ki :\n\u2022 (M, w) |= Ki \u03c6 if (M, w \u2032 ) |= \u03c6 for all w \u2032 \u2208 Ki (w).\nStatements of the form \u03c6 \u2192i \u03c8 are evaluated according to Pi . Let [[\u03c6]](w,i) = {w \u2032 \u2208 W(w,i) :\n(M, w \u2032 ) |= \u03c6}.\n\u2022 (M, w) |= \u03c6 \u2192i \u03c8 if either Pl(w,i) ([[\u03c6]](w,i) ) =\u22a5 or Pl(w,i) ([[\u03c6\u2227\u03c8]](w,i) ) > Pl(w,i) ([[\u03c6\u2227\u00ac\u03c8]](w,i) ).\nWe now define beliefs. Recall that true \u2192i \u03c6 means that \u03c6 is more plausible than \u00ac\u03c6\naccording to agent's i plausibility measure. We might say that in this case the agent believes\n\u03c6. However, recall that the agent can have different plausibility assessments at different\nworlds. Thus, there can be a model M, and worlds w, w \u2032 such that (w, w \u2032) \u2208 Ki , but (M, w) |=\ntrue \u2192i \u03c6 while (M, w \u2032 ) |= \u00ac(true \u2192i \u03c6). (In Example 5, we show why this extra expressive\npower is necessary.) That is, \u03c6 is more plausible than \u00ac\u03c6 in one of the worlds the agent\nconsiders possible, but not in another. Since our intention is that the agent should not\ndistinguish between accessible worlds, we would like the agent to have the same beliefs in all\nthe worlds he considers possible. We say that an agent believes \u03c6 if he knows that \u03c6 is more\nplausible than \u00ac\u03c6 in all the worlds he considers possible. Thus, we define Bi \u03c6, read \"agent\ni believes \u03c6\", as an abbreviation for Ki (true \u2192i \u03c6).\n\n2.5 Example: Circuit Diagnosis\nThe following example illustrates some of the expressive power of this language. Although\nit only involves one agent and only one plausibility measure in any given structure, it can\neasily be extended to allow for many agents with different plausibility measures.\nThe circuit diagnosis problem has been well studied in the literature (see [DH88] for an\noverview). Consider a circuit that contains n logical components c1 , . . . , cn and k lines\nl1 , . . . , lk . As a concrete example, consider the circuit of Figure 1. 3 The diagnosis task is\nto identify which components are faulty. The agent can set the values of input lines of the\ncircuit and observe the output values. The agent then compares the actual output values to\nthe expected output values and attempts to locate faulty components.\n3\n\nThe \"full adder\" example is often used in the diagnosis literature. In our discussion here we\nloosely follow the examples of Reiter [Rei87].\n\n12\n\n\fl1\nl2\n\nl4\n\nX1\n\nl3\n\nX2\n\nA2\n\nA1\n\nl5\n\nl7\n\nl6\n\nO1\n\nl8\n\nFig. 1. A full adder. X1 and X2 are XOR gates, A1 and A2 are AND gates, and O1 is an OR gate.\n\nWe model this situation using the tools we presented in the previous sections. We start by\ndescribing the agent's knowledge using a Kripke structure. We then construct two possible plausibility measures over worlds in this Kripke structures, and examine the resulting\nknowledge and belief.\n\nKnowledge We model the agent's knowledge about the circuit using the Kripke strucK\nture Mdiag\n= (Wdiag , \u03c0diag , Kdiag ). Each possible world w \u2208 Wdiag is composed of two parts:\nfault(w), the failure set-that is, the set of faulty components in w, and value(w), the value\nof all the lines in the circuit. We consider only worlds where the components that are not in\nthe failure sets perform as expected. For example, in the circuit of Figure 1, if the AND gate\nA1 is not faulty, then we require that l5 has value \"high\" if and only if both l1 and l2 have\nthe value \"high\". Most accounts of diagnosis assume that there is a logical theory \u2206 that\ndescribes the properties of the device. To capture our intuition, it must be the case that w\nis a possible world in M if and only if fault(w) and value(w) are together consistent with \u2206.\nThe most straightforward language for reasoning about faults is the following: let \u03a6diag =\n{faulty(c1 ), . . . , faulty(cn ), hi(l1 ), . . . , hi(lk )} be the set of propositions, where each faulty(ci )\ndenotes that component i is faulty and hi(li ) denotes that line i in a \"high\" state. We then\ndefine the interpretation \u03c0diag in the obvious way: \u03c0diag (w)(faulty(ci )) = true if ci \u2208 fault(w),\nand \u03c0diag (w)(hi(li )) = true if hli , 1i \u2208 value(w).\nNext, we need to define the agent's knowledge. We define ow \u2286 value(w) to be the values of\nthose lines the agent sets or observes. The agent knows which tests he has performed and\nthe results he observed. Therefore, we have (w, w \u2032) \u2208 Kdiag if ow = ow\u2032 . For example, suppose\nthe agent observes hi(l1 ) \u2227 hi(l2 ) \u2227 hi(l3 ) \u2227 hi(l7 ) \u2227 hi(l8 ). The agent then considers possible all\nworlds where the same observations hold. Since these observations are consistent with the\ncorrect behavior of the circuit, one of these worlds has an empty failure set. However, other\nworlds are possible. For example, it might be that the AND gate A2 is faulty. This would\nnot affect the outputs in this case, since if A1 is non-faulty, then its output is \"high\", and\nthus, O1 's output is \"high\" regardless of A2 's output.\n13\n\n\fNow suppose that the agent observes hi(l1 ) \u2227 \u00achi(l2 ) \u2227 hi(l3 ) \u2227 hi(l7 ) \u2227 \u00achi(l8 ). These observations imply that the circuit is faulty. (If l1 and l3 are \"high\" and l2 is \"low\", then the correct\nvalues for l7 and l8 should be \"low\" and \"high\", respectively.) In this case there are several\npossible failure sets, including {X1 }, {X2 , O1 }, and {X2 , A2 }.\nIn general, there is more than one explanation for the observed faulty behavior. Thus, the\nagent can not know exactly which components are faulty, but he may have beliefs on that\nscore.\n\nPlausibility To model the agent's beliefs, we need to decide on the plausibility measure\nthe agent has at any world. We assume that only failure sets are relevant for determining\na world's plausibility. Thus, we start by constructing a plausibility measure over possible\nfailures of the circuit. We assume that failures of individual components are independent of\none another. If we also assume that the likelihood of each component failing is the same, we\ncan construct a preference ordering on failure set as follows: If f1 and f2 are two failure sets,\nwe say that f1 is preferred to f2 if |f1 | < |f2 |, that is, if f1 consists of fewer faulty components\nthan f2 . This preference ordering induces a plausibility measure using the construction of\nProposition 2. In this measure Pl(F1 ) < Pl(F2 ) if minf \u2208F1 (|f |) < minf \u2208F2 (|f |).\nWe can construct the same plausibility measure based on probabilistic arguments using\nPPDs. Suppose that the probability of a single component failing is \u01eb. Since we have assumed\nthat failures are independent, it follows that the probability of a failure set f is \u01eb|f | (1\u2212\u01eb)(n\u2212|f |) ,\nsince there are |f | components that fail, and n\u2212|f | components that do not fail. To model the\nbehavior of small but unknown failure probability, we can consider the PPD (Pr0 , Pr1 , . . .),\nwhere in Prm the probability of a single failure is 1/(m + 1). It is not hard to check that\nlimm\u2192\u221e Prm (F2 )/ Prm (F1 ) = 0 if and only if Pl(F2 ) < Pl(F1 ) in the plausibility measure\ndescribed above. Interestingly, this plausibility measure is almost identical to the \u03ba-ranking\nin which \u03ba({f }) = |f |. The only difference is that if |f1 | = |f2 |, Pl({f1 }) is incomparable to\nPl({f2 }) in the plausibility measure we constructed, while they are equal according to the\n\u03ba-ranking.\nIn some situations it might be unreasonable to assume that all components have equal failure\nprobability. Thus, we might assume that for each component ci there is a probability \u01ebi of\nfailure. If we assume independence, then given ~\u01eb = (\u01eb1 , . . . , \u01ebn ), the probability of a failure\nset f is \u03a0ci \u2208f \u01ebi \u03a0ci 6\u2208f (1 \u2212 \u01ebi ). We can construct a PPD that captures the effect of the \u01ebi 's\ngetting smaller, but at possibly different rates: Suppose g is a bijection from IN m to IN.\nIf m\n~ = (m1 , . . . , mn ), let Prg(m)\n~ be the distribution where the probability of ci failing is\n1/(mi + 1), for i = 1, . . . , n. In this case, we get that limm\u2192\u221e Prm (f2 )/ Prm (f1 ) = 0 if and\nonly if f2 is a strict subset of f1 , i.e., if f1 contains all the components in f2 and more. Since\nwe do not assume any relations among the failure probabilities of different components, it\nis not possible to compare failure sets unless one is a subset of the other. Thus, we can\ndefine f \u227a f \u2032 if f \u2282 f \u2032 . Using the construction of Proposition 2, we can again consider the\nplausibility measure Pl induced by \u227a. It is not hard to see that Pl(F1 ) \u2264 Pl(F2 ) if for every\nfailure set f1 \u2208 F1 \u2212 F2 there is some f2 \u2208 F2 such that f2 \u227a f1 . As our construction shows,\n14\n\n\fthis plausibility measure can be induced by either a preference ordering or a PPD; however,\nit cannot be captured by a \u03ba-ranking or a possibility measure, since the ordering on failure\nsets is partial.\n\nBeliefs We now have the required components to examine the agent's beliefs. Using the\ntwo plausibility measures we just described, we can construct two possible structures Mdiag,1\nand Mdiag,2 . In both structures we set W(w,1) = Kdiag (w), and in both Mdiag,1 and Mdiag,2 the\nplausibility measure is induced from a preference ordering on failures (using the construction\nof Proposition 2). In Mdiag,1 , we take the plausibility measure to be such that Pl(w,1) ({w}) \u2265\nPl(w,1) ({w \u2032}) if and only if |fault(w)| \u2264 |fault(w \u2032)|, and in Mdiag,2 so that Pl(w,1) ({w}) \u2265\nPl(w,1) ({w \u2032}) if and only if fault(w) \u2286 fault(w \u2032). It is easy to see that, in both structures,\nif there is a world w in which these observations occur and where fault(w) = \u2205, then the\nagent believes that the circuit is faultless. If the agent detects an error, he believes that\nit is caused by one of the minimal explanations of his observations, where the notion of\nminimality differs in the two structures. We now make this statement more precise. Let\nf be a failure set. Let Df be the formula that denotes that f is the failure set, so that\n(M, w) |= Df if and only if fault(w) = f . The agent believes that f is a possible diagnosis\n(i.e., an explanation of his observations) if \u00acB1 \u00acDf . The set of diagnoses the agent considers\npossible is Bel(M, w) = {f : (M, w) |= \u00acB1 \u00acDf }. We say that a failure set f is consistent\nwith an observation o if it is possible to observe o when f occurs, i.e., if there is a world w\nin W such that fault(w) = f and ow = o. 4\nProposition 3 (a) Bel(Mdiag,1 , w) contains all failure sets f that are consistent with ow\nsuch that there is no failure set f \u2032 with |f \u2032 | < |f | which is consistent with ow .\n(b) Bel(Mdiag,2 , w) contains all failure sets f that are consistent with ow such that there is\nno failure set f \u2032 with f \u2032 \u2282 f which is consistent with ow .\n\nPROOF. Straightforward; left to the reader. \u2737\nThus, both Bel(Mdiag,1 , w) and Bel(Mdiag,2 , w) consist of minimal sets of failure sets consistent\nwith ow , for different notions of minimality. In the case of Mdiag,1 , \"minimality\" means\n\"of minimal cardinality\", while in the case of Mdiag,2 , it means \"minimal in terms of set\ncontainment\". This proposition shows that Mdiag,1 and Mdiag,2 capture standard assumptions\nmade in model-based diagnosis; Mdiag,1 captures the assumptions made in [de 90], while\nMdiag,2 captures the assumptions made in [Rei87]. More concretely, in our example, if the\nagent observes hi(l1 ) \u2227 \u00achi(l2 ) \u2227 hi(l3 ) \u2227 hi(l7 ) \u2227 \u00achi(l8 ), then in Mdiag,1 she would believe that\nX1 is faulty, since {X1 } is the only diagnosis with cardinality one. On the other hand, in\nMdiag,2 she would believe that one of the three minimal diagnoses occurred: {X1 }, {X2 , O1 }\nor {X2 , A2 }.\n4\n\nNote that if \u2206 is a theory that describes the properties of circuit, then a failure f is consistent\nwith observation o, if and only if f and o are consistent according to \u2206.\n\n15\n\n\f2.6 Properties of Knowledge and Plausibility\nKripke structures for knowledge and plausibility are quite similar to the Kripke structures for\nknowledge and probability introduced by Fagin and Halpern [FH94a]. The only difference\nis that in Kripke structures for knowledge and probability, Pi (w) is a probability space\nrather than a plausibility space. Fagin and Halpern explore various natural restrictions on\nthe interactions between the probability spaces Pi (w) and the accessibility relations Ki .\nHere we investigate restrictions on the interaction between the plausibility spaces and the\naccessibility relations. Not surprisingly, some of these conditions are exact analogues to\nconditions investigated by Fagin and Halpern.\nGiven our interest in the KLM properties, we will be interested in structures that satisfy the\nfollowing condition:\nQUAL Pi (w) is qualitative for all worlds w and agents i.\nThe same arguments that show that A2 gives us the AND rule also show that it gives us\nproperty K2 for beliefs. More precisely, we have the following result.\nTheorem 4 If M satisfies QUAL, then for all worlds w in M, we have\n(a) (M, w) |= ((\u03c3 \u2192i \u03c6) \u2227 (\u03c3 \u2192i \u03c8)) \u21d2 (\u03c3 \u2192i (\u03c6 \u2227 \u03c8))\n(b) (M, w) |= Bi \u03c6 \u2227 Bi \u03c8 \u21d2 Bi (\u03c6 \u2227 \u03c8)\n(c) (M, w) |= Bi \u03c6 \u2227 Bi (\u03c6 \u2227 \u03c8) \u21d2 Bi \u03c8.\n\nPROOF. Straightforward; left to the reader. \u2737\n\nIn view of this result, we typically assume that QUAL holds whenever we want to reason\nabout belief.\nThe set W(w,i) consists of all worlds to which agent i assigns some degree of plausibility in\nworld w. We would not expect the agent to place a positive probability on worlds that he\nconsiders impossible. Similarly, he would not want to consider as plausible (even remotely)\na world he knows to be impossible. This intuition leads us to the following condition, called\nCONS for consistency (following [FH94a]):\nCONS W(w,i) \u2286 Ki (w) for all worlds w and all agents i. 5\n5\n\nWe remark that CONS is inappropriate if we use \u2192 to model, not plausibility, but counterfactual\nconditions, as is done by Lewis [Lew73]. If CONS holds, then it is easy to see that Ki \u03c6 \u21d2 Ki (\u00ac\u03c6 \u2192i\n\u03c8) is valid, for all \u03c8. That is, if agent i knows \u03c6, then he knows that in the most plausible worlds\nwhere \u00ac\u03c6 is true, \u03c8 is vacuously true, because there are no plausible worlds where \u00ac\u03c6 is true. On\nthe other hand, under the counterfactual reading, it makes perfect sense to say \"I know the match\n\n16\n\n\fA consequence of assuming CONS is a stronger connection between knowledge and belief.\nSince CONS implies that the most plausible worlds are in Ki (w), it follows that if the agent\nknows \u03c6 he also believes \u03c6. (Indeed, as we shall see, this condition characterizes CONS.)\nIn probability theory, the agent assigns probability 1 to the set of all worlds. Since 1 > 0, this\nmeans the agent assigns non-zero probability to some sets of worlds. It is possible to have\n\u22a4 = \u22a5 in plausibility spaces. If this happens, the agent considers all sets to be completely\nimplausible. The following condition, called NORM for normality (following [Lew73]), says\nthis does not happen:\nNORM P(w, i) is normal, that is, \u22a4(w,i) >\u22a5(w,i) , for all worlds w and all agents i.\nWe can strengthen this condition somewhat to one that says that the agent never considers\nthe real world implausible. This suggests the following condition: Pl(w,i) ({w}) >\u22a5. Stating\nthis condition, however, leads to a technical problem. Recall that Pl(w,i) is defined over the\nset of measurable subsets of W(w,i) . In general, however, singletons may not be measurable.\nThus, we examine a slightly weaker condition which we call REF for reflexive (following\n[Lew73]):\nREF For all worlds w and all agents i,\n\u2022 w \u2208 W(w,i) , and\n\u2022 Pl(w,i) (A) >\u22a5 for all A \u2208 F(w,i) such that w \u2208 A.\nAs we said in the introduction, much of the previous work using conditionals assumed (implicitly or explicitly) that the agent considers only one plausibility measure possible. This\namounts to assuming that the plausibility measure is a function of the agent's epistemic\nstate. This is captured by an assumption called SDP (following [FH94a]) for state determined plausibilities:\nSDP For all worlds w and w \u2032 and all agents i, if (w, w \u2032) \u2208 Ki then Pi (w) = Pi (w \u2032).\nIt is easy to see that SDP implies that an agent knows his plausibility measure. In particular,\nas we shall see, with SDP we have that \u03c6 \u2192i \u03c8 implies Ki (\u03c6 \u2192i \u03c8).\nIt is easy to verify that the structures described in the diagnosis example of Section 2.5\nsatisfy CONS, REF, and SDP. As mentioned in the introduction, SDP is not appropriate in\nall situations; at times we may want to allow the agent to consider possible several plausibility\nmeasures. To capture this, we need to generalize SDP. The following example might help\nmotivate the formal definition.\nExample 5 This is a variation of the Liar's Paradox. On a small Pacific island there are\ntwo tribes, the Rightfeet and the Leftfeet. The Rightfeet are known to usually tell the truth,\nwhile the Leftfeet are known to usually lie. Alice is a visitor to the island. She encounters a\nnative, Bob, and discusses with him various aspects of life on the island. Now, Alice does not\nis dry, but it is not the case that if it were wet, then it would light if it were struck.\"\n\n17\n\n\fknow to what tribe Bob belongs. Thus, she considers it possible both that Bob is a Rightfoot\nand that he is a Leftfoot. In the first case, she should believe what he tells her and in the\nsecond she should be skeptical.\nOne possible way of capturing this situation is by partitioning the worlds Alice considers\npossible into two sets, according to Bob's tribe. Let WR (resp. WL ) be the set of worlds\nthat Alice considers possible where Bob is a Rightfoot (resp. Leftfoot). As the discussion\nabove suggests, Alice's plausibility measure at the worlds of WR gives greater plausibility\nto worlds where Bob is telling the truth than to worlds where Bob is lying; the opposite\nsituation holds at worlds of WL . In such a structure, the formula \u00acKAlice \u00ac(tell(\u03c6) \u2192Alice\n\u00ac\u03c6) \u2227 \u00acKAlice \u00ac(tell(\u03c6) \u2192Alice \u03c6) is satisfiable, where tell(\u03c6) is the formula that holds when\nBob tells Alice \u03c6. On the other hand, in structures satisfying SDP, this formula is satisfiable\nonly when tell(\u03c6) has plausibility \u22a5 in all the worlds that Alice considers possible.\nWhile this example may seem contrived, in many situations it is possible to extract parameters such as Leftfoot and Rightfoot that determine which conditional statements are true.\nFor example, when we introduce time into the picture (in Section 3.1), these parameters\nmight be the agent's own actions in the future. Such a partition allows us to make statements such as \"I do not know whether \u03c6 is plausible or not, but I know that if I do a, then\n\u03c6 is plausible\", where \u03c6 is some statement about the future. If the agent does not know the\nvalue of these parameters, she will not necessarily know which conditionals are true at a\ngiven world (as was the case in the example above).\nExample 5 motivates the condition called uniformity.\nUNIF For all worlds w and agents i, if w \u2032 \u2208 W(w,i) then Pi (w) = Pi (w \u2032). 6\nIt is not hard to show that UNIF holds if and only if, for each agent i, we can partition\nthe set of possible worlds in such a way that for each cell C in the partition, there is a\nplausibility space (WC , PlC ) such that WC \u2286 C and Pi (w) = (WC , PlC ) for all worlds w \u2208 C.\nMoreover, if CONS also holds, then this partition refines the partition induced by the agent's\nknowledge, i.e., if C is a cell in the partition and w is some world C, then C \u2286 Ki (w). It\neasily follows that SDP and CONS together imply UNIF.\nWhen we model uncertainty about the relative plausibility of different worlds this way it\nis reasonable to demand that the plausibility measure totally orders all events; i.e., it is a\nranking. The RANK assumption is:\nRANK For all worlds w and agents i, Pi (w) is a ranking, that is, for all sets A, B \u2286 Ww\neither Plw (A) \u2264 Plw (B) or Plw (B) \u2264 Plw (A), and Plw (A \u222a B) = max(Plw (A), Plw (B)).\nNote that \u03ba-rankings and possibility measures are two examples of rankings. Additionally,\nrational preference orderings of [KLM90] are essentially rankings in the sense that for each\n6\n\nThis condition is not the same as uniformity as defined in [Lew73]; rather, it corresponds in the\nLewis terminology to absoluteness.\n\n18\n\n\frational preference ordering we can construct a ranking that satisfies exactly the same conditional statements [Fri97,FH97b].\nWhile rankings are quite natural, they have often been rejected as being too inexpressive\n[Gin86]. In a ranking there is a total order on events. The standard argument for partial orders\nis as follows: In general, an agent may not be able to determine the relative plausibility of a\nand b. If the plausibility measure is a ranking, the agent is forced to make this determination;\nwith a partial order, he is not. This argument loses much of its force in our framework, once\nwe combine knowledge and plausibility. As we said above, the agent's ignorance can be\nmodeled by allowing him to consider (at least) two rankings possible, one in which a is more\nplausible than b, and one in which b is more plausible that a. The agent then believes neither\nthat a is more plausible than b nor that b is more plausible than a.\n\n2.7 Knowledge and Belief\nHow reasonable is the notion of belief we have defined? In this section, we compare it to\nother notions considered in the literature.\nRecall that LB be the language where the only modal operators are B1 , . . . , Bn . Let LKB be\nthe language where we have K1 , . . . , Kn and B1 , . . . , Bn (but no \u2192i operators). It is not hard\nto see (and will follow from our proofs below) that to get belief to satisfy even minimal such\nas K2, we need the AND rule to hold. Thus, in this section, we restrict attention to Kripke\nstructures for knowledge and plausibility that satisfy QUAL. We then want to investigate\nthe impact of adding additional assumptions. Let M be the set of all Kripke structures for\nknowledge and plausibility that satisfy QUAL, and let MCONS (resp. MCONS,NORM ) be the\nstructures satisfying QUAL and CONS (resp. QUAL, CONS and NORM).\nWork on belief and knowledge in the literature [HM92,Hin62,Lev84] has focused on the modal\nsystems S5, KD45, K45, and K with semantics based on Kripke structures as described\nin Section 2.1. Before we examine the properties of belief in our approach, we relate our\nsemantics of belief (in terms of plausibility) to the more standard Kripke approach, which\npresumes that belief is defined in terms of a binary relation Bi . Can we define a relation Bi\nin terms of Ki and Pi such that (M, w) |= Bi \u03c6 if and only if (M, v) |= \u03c6 for all v \u2208 Bi (w)?\nWe show that this is possible in some structures, but not in general.\nLet S = (W, Pl) be a qualitative plausibility space. We say that A \u2286 W is a set of most\nplausible worlds if Pl(A) > Pl(A) (where A is the complement of A, i.e., W \u2212 A) and for\nall B \u2282 A, Pl(B) 6> Pl(B). That is, A is a minimal set of worlds that is more plausible\nthan its complement. It is easy to verify that if such a set exists, then it must be unique.\nTo see this, suppose that A and A\u2032 are both sets of most plausible worlds. We now show\nthat Pl(A \u2229 A\u2032 ) > Pl(A \u2229 A\u2032 ). Since A and A\u2032 are both most plausible sets of worlds, this\nwill show that we must have A = A\u2032 . To see that Pl(A \u2229 A\u2032 ) > Pl(A \u2229 A\u2032 ), first note\nthat A \u2229 A\u2032 , A \u2212 A\u2032 and A are pairwise disjoint. Since A and A\u2032 are most plausible sets\n19\n\n\fof worlds, we have that Pl((A \u2229 A\u2032 ) \u222a (A \u2212 A\u2032 )) = Pl(A) > Pl(A) and Pl((A \u2229 A\u2032 ) \u222a A) \u2265\nPl((A \u2229 A\u2032 ) \u222a (A\u2032 \u2212 A)) = Pl(A\u2032 ) > Pl(A\u2032 ) \u2265 Pl(A \u2212 A\u2032 ). We can apply A2 to get that\nPl(A \u2229 A\u2032 ) > Pl((A \u2212 A\u2032 ) \u222a A) = Pl(A \u2229 A\u2032 ).\nIn finite plausibility structures (that is, ones with only finitely many worlds), it is easy to see\nthat there is always a (unique) set of most plausible worlds. In general, however, a set of most\nplausible worlds does not necessarily exist. For example, consider the space S0 = (W, Pl),\nwhere W = {wi : i \u2265 0} and Pl is defined as follows: Pl(A) = \u221e if A contains an infinite\nnumber of worlds, and Pl(A) = maxwi\u2208A (i) otherwise. Suppose that Pl(A) > Pl(A). A must\nbe finite, for otherwise Pl(A) = \u221e. Thus, A must be infinite. Suppose wi \u2208 A. It is easy\nto see that A \u2212 {wi } is infinite and A \u2212 {wi } is finite. Thus, Pl(A \u2212 {wi }) > Pl(A \u2212 {wi }).\nThis shows that there does not exist a set of most plausible worlds in S.\nIf there is no set of most plausible worlds, then we may not be able to find a relation Bi\nthat characterizes agent i's beliefs. For example, consider the structure M = (W, \u03c0, K1 , P1 ),\nwhere W = {wi : i \u2265 0} is the set of worlds described in S0 above; \u03c0 assigns truth values\nto primitive propositions p1 , p2 , . . . in such a way that \u03c0(wi )(pj ) = true if and only if j \u2265 i;\nK1 is the complete accessibility relation K1 = W \u00d7 W ; and P1 (wi ) is the space S0 described\nabove. It is not hard to verify that (M, w0 ) |= B1 \u03c6 if and only if [[\u00ac\u03c6]](w0 ,i) is a finite set,\ni.e., there is an index i such that for all j \u2265 i, we have (M, wj ) |= \u03c6. Thus, (M, w0 ) |= B1 pj\nfor all j \u2265 0. Yet there are no worlds in the model that satisfy all the propositions pj at\nonce. Thus, there is no accessibility relation B1 that characterizes agent 1's beliefs in w0 .\nOn the other hand, we can show that if there is always a set of most plausible worlds, then\nwe can characterize the agents' beliefs by an accessibility relation. Let S = (W, Pl) be a\nplausibility space. Define MP(S) to be the set of most plausible worlds in S if it exists, and\n\u2205 if Pl(W ) = \u22a5. Otherwise MP(S) is not defined.\nProposition 6 Let M be a Kripke structure for knowledge and plausibility. If MP(Pi (w \u2032 ))\nis defined for all w \u2032 \u2208 Ki (w), then (M, w) |= Bi \u03c6 if and only if (M, w \u2032\u2032 ) |= \u03c6 for all w \u2032\u2032 \u2208\n\u222aw\u2032 \u2208Ki (w) MP(Pi (w \u2032)).\n\nPROOF. Straightforward; left to the reader. \u2737\n\nThis proposition implies that, if most plausible sets of worlds always exist in M, then we can\nset Bi (w) = \u222aw\u2032 \u2208Ki (w) MP(Pi (w \u2032 )) and recover the usual Kripke-style semantics for belief.\nThis discussion shows that our model of belief is more general than the classical Kripkestructure account of beliefs, since there are models where the agent's beliefs are not determined by a set of accessible worlds. However, as we shall see, this does not lead to new\nproperties of beliefs in LB . Roughly speaking, this is because we have a finite model property:\na formula in LB is satisfiable if and only if it is satisfiable in a finite model (see Theorem 13\nbelow). It is easy to verify that in a finite model MP(Pi (w)) is always defined. We note,\n20\n\n\fhowever, that this finite model property is no longer true when we consider the interaction\nof beliefs with other modalities, such as time, or when we examine the first-order case. In\nthese situations, the two models of beliefs are not equivalent. Plausibility is strictly more\nexpressive; see [FHK96].\nWe now examine the formal properties of belief and knowledge in structures of knowledge\nand plausibility. We start by restricting our attention to LB . As we show below, the modal\nsystem K precisely characterizes the valid formulas of LB in the class M. However, in the\nliterature, belief has typically been taken to be characterized by the modal system K45 or\nKD45, not K. We get K45 by restricting to models that satisfy CONS, and KD45 by further\nrestricting to models that satisfy NORM. Thus, the two requirements that are most natural,\nat least if we have a probabilistic intuition for plausibility, are already enough to make Bi a\nKD45 operator.\nTheorem 7 K (resp., K45, KD45) is a sound and complete axiomatization for LB with\nrespect to M (resp., MCONS , MCONS,NORM ).\n\nPROOF. See Appendix A.1. \u2737\n\nWe now consider knowledge and belief together. This combination has been investigated in\nthe literature [KL88,Voo92]. In particular, Kraus and Lehmann [KL88] define Kripke structures for knowledge and belief that have two accessibility relations, one characterizing the\nworlds that are knowledge-accessible and one characterizing worlds that are belief-accessible.\nKi and Bi are defined, as usual, in terms of these relations. They argue that the two accessibility relations must be coherent in the sense that the agent knows what she believes and\nbelieves what she knows to be true. Kraus and Lehmann describe restrictions on the interaction between the two relations that force this coherence. They show that in the resulting\nstructures, the interactions between knowledge and belief are characterized by the following\naxioms.\nKB1. Bi \u03c6 \u21d2 Ki Bi \u03c6\nKB2. Ki \u03c6 \u21d2 Bi \u03c6\nIt turns out that KB1 holds in M and KB2 is a consequence of CONS. To see this, recall\nthat Bi \u03c6 \u2261 Ki (true \u2192 \u03c6). Using positive introspection for knowledge (axiom K4), we derive\nthat Bi \u03c6 \u21d2 Ki Ki (true \u2192 \u03c6). This is equivalent to axiom KB1. When M satisfies CONS, we\nhave that W(w,i) \u2286 Ki (w). If (M, w) |= Ki \u03c6, then all worlds in Ki (w) satisfy \u03c6. This implies\nthat there are no worlds satisfying \u00ac\u03c6 in W(w,i) , and thus Bi \u03c6 must hold. Thus, KB2 must\nhold.\nWe now state this formally. Let AXKB consist of the S5 axioms for the operators Ki , the K\naxioms for the operators Bi , together with KB1; let AXKB,CONS consist of AXKB together\n21\n\n\fwith the K4 and K5 axioms for Bi and KB2; and let AXKB,CONS,NORM consist of AXKB,CONS\ntogether with the K6 axiom for Bi .\nTheorem 8 AXKB (resp., AXKB,CONS , AXKB,CONS,NORM ) is a sound and complete axiomatization of LKB with respect to M (resp., MCONS, MCONS,NORM ).\n\nPROOF. See Appendix A.1. \u2737\n\nAs an immediate corollary, we get that there is a close relationship between our framework\nand that of [KL88]. Let KL be the logic of Kraus and Lehmann:\nCorollary 9 For any \u03c6 \u2208 LKB , KL |= \u03c6 if and only if MCONS,NORM |= \u03c6.\nWe now relate to three other notions of beliefs in the literature-those of Moses and Shoham\n[MS93], Voorbraak [Voo92], and Lamarre and Shoham [LS94].\nMoses and Shoham [MS93] also view belief as being derived from knowledge. The intuition\nthat they try to capture is that once the agent makes a defeasible assumption, the rest of\nhis beliefs should follow from his knowledge. In this sense, Moses and Shoham can be viewed\nas focusing on the implications of an assumption and not on how it was obtained. We can\nunderstand their notion as saying that \u03c6 is believed if it is known to be true in the most\nplausible worlds. But for them, plausibility is not defined by an ordering. Rather, it is defined\nin terms of a formula, which can be thought of as characterizing the most plausible worlds.\nMore formally, for a fixed formula \u03b1, they define Bi\u03b1 \u03c6 to be an abbreviation for Ki (\u03b1 \u21d2 \u03c6). 7\nThe following result relates our notion of belief to that of Moses and Shoham.\nLemma 10 Let M be a propositional Kripke structure of knowledge and plausibility satisfying CONS and SDP. Suppose that w, i, and \u03b1 are such that the most plausible worlds in\nPi (w) are exactly those worlds in Ki (w) that satisfy \u03b1, i.e., MP(Pi (w)) = {w \u2032 \u2208 Ki (w) :\n(M, w \u2032 ) |= \u03b1}. Then for any formula \u03c6 \u2208 LKB that includes only the modalities Ki and Bi ,\n(M, w) |= \u03c6 if and only if (M, w) |= \u03c6\u2217 , where \u03c6\u2217 is the result of recursively replacing each\nsubformula of the form Bi \u03c8 in \u03c6 by Ki (\u03b1 \u21d2 \u03c8 \u2217 ).\n\nPROOF. See Appendix A.1. \u2737\n\nVoorbraak [Voo92] distinguishes two notions of knowledge: objective knowledge and true\njustified belief . He then studies the interaction of both notions of knowledge with beliefs. The\nintuition we assign to knowledge is similar to Voorbraak's intuition for objective knowledge.\n7\n\nShoham and Moses also examine two variants of this definition. These mainly deal with the\ncases where \u03b1 is inconsistent with the agent's knowledge. For simplicity, we assume here that \u03b1 is\nconsistent with the agent's knowledge.\n\n22\n\n\fHowever, Voorbraak objects to the axiom Ki \u03c6 \u21d2 Bi \u03c6, and suggests Bi \u03c6 \u21d2 Bi Ki \u03c6. The\ndifference lies in the interpretation of belief. Voorbraak's notion of belief is stronger than\nours. His view is that the agent cannot distinguish what he believes from what he knows\n(indeed, he believes that what he believes is the same as what he knows). Our notion of\nbelief is weaker, in that we allow agents to be aware of the defeasibility of their beliefs.\nLamarre and Shoham [LS94] investigate the notion of knowledge as justified true belief using\na framework that is very similar to ours. They start with an explicit preference ordering\nover possible worlds, and then define B \u03b1 \u03c6 to read \"given evidence \u03b1, \u03c6 holds in the most\nplausible \u03b1-worlds\". Their formal account of B \u03b1 \u03c6 is exactly \u03b1 \u2192i \u03c6 in our notation. Unlike\nus, they examine a notion of knowledge as \"belief stable under incorporation of correct\nfacts\", which is rather different then our notion of objective knowledge. Thus, while the\ntechnical construction is similar, the resulting framework is substantially different. Lamarre\nand Shoham take plausibility to be the only primitive, and use it to determine both knowledge\nand belief. We take both knowledge and plausibility to be primitive, and use them to define\nbelief.\n2.8 Axiomatizing the Language of Knowledge and Plausibility\nUp to now, we have considered just the restricted language LKB . We now present sound\nand complete axiomatizations for the full language LKC . The technical details are much in\nthe spirit of the axiomatizations presented in [FH94a] for knowledge and probability. Our\ncomplete axiomatization for M consists of two \"modules\": a complete axiomatization for\nknowledge (i.e., S5) and a complete axiomatization for conditionals. In the general case,\nthere are no axioms connecting knowledge and plausibility. For each of the conditions we\nconsider, we provide an axiom that characterizes it. The axioms characterizing NORM, REF,\nRANK, and UNIF are taken from [Lew73] and [Bur81] (see also [Fri97,FH97b]), while the\naxioms for CONS and SDP (and also UNIF) correspond directly to the axioms suggested\nin [FH94a] for their probabilistic counterparts. We also provide complete characterizations\nof the complexity of the validity problem for all the logics considered, based on complexity\nresults for knowledge [HM92] and for conditionals [FH96a].\nThe axiom system can be modularized into three components: propositional reasoning, reasoning about knowledge, and reasoning about conditionals. The component for propositional\nreasoning consists of K1 and RK1 (from Section 2.1); the component for reasoning about\nknowledge consists of K2\u2013K5 and RK2 (from Section 2.1); the component for reasoning\nabout conditionals consists of the standard axioms and rules for conditional logic C1\u2013C4,\nRC1, and RC2 described in [Fri97,FH97b] following [Bur81,Lew73]:\nC1.\nC2.\nC3.\nC4.\n\n\u03c6\u2192\u03c6\n((\u03c6 \u2192 \u03c81 ) \u2227 (\u03c6 \u2192 \u03c82 )) \u21d2 (\u03c6 \u2192 (\u03c81 \u2227 \u03c82 ))\n((\u03c61 \u2192 \u03c8) \u2227 (\u03c62 \u2192 \u03c8)) \u21d2 ((\u03c61 \u2228 \u03c62 ) \u2192 \u03c8)\n((\u03c61 \u2192 \u03c62 ) \u2227 (\u03c61 \u2192 \u03c8)) \u21d2 ((\u03c61 \u2227 \u03c62 ) \u2192 \u03c8)\n23\n\n\fR1. From \u03c6 and \u03c6 \u21d2 \u03c8 infer \u03c8\nRC1. From \u03c6 \u21d4 \u03c6\u2032 infer (\u03c6 \u2192 \u03c8) \u21d2 (\u03c6\u2032 \u2192 \u03c8)\nRC2. From \u03c8 \u21d2 \u03c8 \u2032 infer (\u03c6 \u2192 \u03c8) \u21d2 (\u03c6 \u2192 \u03c8 \u2032 )\nLet AX consist of K1\u2013K5, C1\u2013C4, RK1, RK2, RC1, and RC2.\nTheorem 11 AX is a sound and complete axiomatization for LKC with respect to M.\n\nPROOF. See Appendix A.2. \u2737\n\nWe now capture the conditions described above-CONS, NORM, REF, SDP, UNIF, and\nRANK-axiomatically.\nRANK, NORM, REF, and UNIF correspond the axioms C5\u2013C8, respectively, from [Fri97,FH97b]:\nC5.\nC6.\nC7.\nC8.\n\n\u03c6 \u2192 \u03c8 \u2227 \u00ac(\u03c6 \u2192 \u00ac\u03be) \u21d2 \u03c6 \u2227 \u03be \u2192 \u03c8\n\u00ac(true \u2192 false).\nN\u03c6 \u2192 \u03c6\n[(\u03c6 \u2192 \u03c8) \u21d2 N(\u03c6 \u2192 \u03c8)] \u2227 [\u00ac(\u03c6 \u2192 \u03c8) \u21d2 N\u00ac(\u03c6 \u2192 \u03c8)]\n\nCONS and SDP correspond to the following axioms, respectively;\nC9. Ki \u03c6 \u21d2 Ni \u03c6\nC10. (\u03c6 \u2192i \u03c8) \u21d2 Ki (\u03c6 \u2192i \u03c8)\nIt is interesting to note that the axioms for CONS and UNIF are derived from the axioms\ndefined in [FH94a] by replacing w(\u03c6) = 1 (the probability of \u03c6 is 1) by Ni \u03c6, which has\na similar reading. We show that adding the appropriate axioms to AX gives a sound and\ncomplete axiomatization of the logic with respect to the class of structures satisfying the\ncorresponding conditions.\nTheorem 12 Let A be a subset of {RANK, NORM, REF, UNIF, CONS, SDP} and let A\nbe the corresponding subset of {C5, C6, C7, C8, C9, C10}. Then AX \u222a A is a sound and\ncomplete axiomatization with respect to the structures in M satisfying A.\n\nPROOF. See Appendix A.2. \u2737\n\nWe now consider the complexity of the validity problem. Our results are based on a combination of results for complexity of epistemic logics [HM92] and conditional logics [FH96a].\nAgain, the technical details are much in the spirit of those in [FH94a].\n24\n\n\fWe start with few results that will be useful in our discussion of complexity. As is often the\ncase in modal logics, we can prove a \"small model property\" for our logic: if a formula is\nsatisfiable at all, it is satisfiable in a small model. Let Sub(\u03c6) be the set of subformulas in \u03c6.\nIt is easy to see that an upper bound on |Sub(\u03c6)| is the number of symbols in \u03c6.\nTheorem 13 Let A be a subset of {CONS, NORM, REF, SDP, UNIF, RANK}. The formula\n\u03c6 is satisfiable in a Kripke structure satisfying A if and only if it is satisfiable in a Kripke\nstructure with at most 2|Sub(\u03c6)| worlds.\n\nPROOF. See Appendix A.2. \u2737\n\nThis shows that if \u03c6 is satisfiable, then it is satisfiable in a model with at most exponential\nnumber of worlds. Such a \"small model\" result is useful when we consider upper bound on\nthe complexity of checking satisfiable. Roughly speaking, if there is a small model, then we\ncan construct this model in time, say, exponential in the size of the formula. However, there\nis one problem with the result we have just proved. This \"small\" number of worlds does not\nnecessarily mean that we can compactly describe the Kripke structure. Recall that Pl(w,i)\ndescribes an ordering over subsets of W(w,i) . Thus, in the worst case, we need to describe\nan ordering on 2|W(w,i) | sets of worlds. Thus, the representation of a structure might be\nexponential in the number of worlds. Fortunately, we can show that a satisfiable formula is\nsatisfiable in a small model with a compact representation.\nWe start with a definition. We say that M = (W, \u03c0, K1 , . . . , Kn , P1 , . . . , Pn ) is a preferential\n(Kripke) structure if for each Pi (w), there is a preference ordering \u227a(w,i) on W(w,i) that\ninduces Pl(w,i) using the construction of Proposition 2. Recall that a preference ordering is a\nbinary relation on the set of possible worlds. Thus, if W is finite, we can describe the relations\nKi and the preference orderings \u2264(w,i) using tables of size at most |W |2 . So the representation\nof such structures is polynomial in |W |. Is it possible to find a small preferential Kripke\nstructure satisfying \u03c6? Indeed we can. Using results of [FH96a], we immediately get the\nfollowing lemma:\nLemma 14 Let A be a subset of {CONS, NORM, REF, SDP, UNIF, RANK}. If a formula\n\u03c6 is satisfiable in a Kripke structure satisfying A with N worlds, then \u03c6 is satisfiable in a\npreferential Kripke structure with at most |Sub(\u03c6)|N worlds.\nCombining this with Theorem 13, we conclude that if \u03c6 is satisfiable, then it is satisfiable\nin a structure of exponential size with an exponential description. It can be shown that this\nresult is essentially optimal (see [HM92,FH96a]). However, if there is only one agent and we\nassume CONS and either UNIF or SDP, then we can get polynomial-sized models.\nTheorem 15 Let A be a subset of {CONS, NORM, REF, SDP, UNIF, RANK} containing\nCONS and either SDP or UNIF. If \u03c6 talks about the knowledge and plausibility of only one\n25\n\n\fagent, then \u03c6 is satisfiable in a Kripke structure satisfying A if and only if it is satisfiable\nin a preferential Kripke structure satisfying A with at most |Sub(\u03c6)|3 worlds.\n\nPROOF. See Appendix A.2. \u2737\n\nWe now consider the complexity of decision procedure for the validity problem. The difficulty\nof deciding whether \u03c6 is valid is a function of the length of \u03c6, written |\u03c6|.\nTheorem 16 Let A be a subset of {CONS, NORM, REF, SDP, UNIF, RANK}. If CONS \u2208\nA, but it is not the case that UNIF or SDP is in A, then the validity problem with respect\nto structures satisfying A is complete for exponential time. Otherwise, the validity problem\nis complete for polynomial space.\n\nPROOF. See Appendix A.2. \u2737\n\nIf we restrict attention to the case of one agent and structures satisfying CONS and either\nUNIF or SDP, then we can do better.\nTheorem 17 Let A be a subset of {CONS, NORM, REF, SDP, UNIF, RANK} containing\nCONS and either UNIF or SDP. For the case of one agent, the validity problem in models\nsatisfying A is co-NP-complete.\n\nPROOF. See Appendix A.2. \u2737\n\n3\n\nAdding Time\n\nIn the previous section, we developed a model of knowledge and beliefs. Having a good model\nof knowledge and belief is not enough in order to study how beliefs change. Indeed, if we are\nmainly interested in agents' beliefs, the additional structure of plausibility spaces does not\nplay a significant role in a static setting. However, if we introduce an explicit notion of time,\nwe expect the plausibility measure to (partially) determine how agents change their beliefs.\nAs we shall see, this gives a reasonable notion of belief change.\nIn this section, we introduce time into the framework. We then examine how time, knowledge,\nand plausibility interact. In particular, we suggest a notion of conditioning that captures the\nintuition that plausibility changes in the minimal way that is required by changes to the\nagent's knowledge.\n26\n\n\f3.1 Knowledge and Plausibility in Multi-Agent Systems\nA straightforward approach to adding time is by introducing another accessibility relation\non worlds, which characterizes their temporal relationship (see, for example, [KL88]). We\nintroduce more structure into the description by adopting the framework of Halpern and\nFagin [HF89] for modeling multi-agent systems. This structure gives a natural definition of\nknowledge and an intuitive way to describe agents' interactions with their environment. We\nstart by describing the framework of Halpern and Fagin, and then add plausibility.\nThe key assumption in this framework is that we can characterize the system by describing\nit in terms of a state that changes over time. This is a powerful and natural way to model\nsystems. Formally, we assume that at each point in time, each agent is in some local state.\nIntuitively, this local state encodes the information that is available to the agent at that time.\nIn addition, there is an environment, whose state encodes relevant aspects of the system that\nare not part of the agents' local states. For example, if we are modeling a robot that navigates\nin some office building, we might encode the robot's sensor input as part of the robot's local\nstate. If the robot is uncertain about his position, we would encode this position in the\nenvironment state.\nA global state is a tuple (se , s1 , . . . , sn ) consisting of the environment state se and the local\nstate si of each agent i. A run of the system is a function from time (which, for ease of\nexposition, we assume ranges over the natural numbers) to global states. Thus, if r is a\nrun, then r(0), r(1), . . . is a sequence of global states that, roughly speaking, is a complete\ndescription of what happens over time in one possible execution of the system. We take a\nsystem to consist of a set of runs. Intuitively, these runs describe all the possible sequences\nof events that could occur in a system.\nGiven a system R, we refer to a pair (r, m) consisting of a run r \u2208 R and a time m as a point.\nIf r(m) = (se , s1 , . . . , sn ), we define ri (m) = si ; thus, ri (m) is agent i's local state at the\npoint (r, m). Finally, to reason in a logical language about such a system, we need to assign\ntruth values to primitive propositions. An interpreted system is a tuple (R, \u03c0) consisting of a\nsystem R together with a mapping \u03c0 that associates a truth assignment with the primitive\npropositions at each state of the system.\nAn interpreted plausibility system can be viewed as a Kripke structure for knowledge. We\nsay two points (r, m) and (r \u2032 , m\u2032 ) are indistinguishable to agent i, and write (r, m) \u223ci (r \u2032 , m\u2032 ),\nif ri (m) = ri\u2032 (m\u2032 ), i.e., if the agent has the same local state at both points. This is consistent\nwith the intuition that an agent's local state encodes all the information available to the\nagent. Taking \u223ci to define the Ki relation, we get a Kripke structure over points. 8\nThis definition of knowledge has proved useful in many applications in distributed systems\nand AI (see [FHMV95] and the references therein). As argued above, we want to add the\n8\n\nIt is straightforward to extend these definitions to deal with continuous time. This is done, for\nexample, in [BLMS97].\n\n27\n\n\fnotion of plausibility so that we can model the agent's beliefs. It is straightforward to do\nso by adding a plausibility assessment for each agent at each point. Formally, an interpreted\nplausibility system is a tuple I = (R, \u03c0, P1 , . . . , Pn ), where R and \u03c0 are as before, and\nthe plausibility assignment Pi maps each point (r, m) to a plausibility space Pi (r, m) =\n(W(r,m,i) , Pl(r,m,i) ).\nIn order to reason about the temporal aspects of the system, we add to the language temporal\nmodalities in the standard fashion (see [GPSS80]). These include \u03c6 for \"\u03c6 is true at the\nnext time step\" We call this language LKCT . Evaluation of temporal modalities at a point\n(r, m) is done by examining the future points on the run r: Given a point (r, m) in an\ninterpreted system I, we have that\n\u2022 (I, r, m) |=\n\n\u03c6 if (I, r, m + 1) |= \u03c6. 9\n\nThis framework is clearly a temporal extension of the logic of knowledge and plausibility\ndescribed in the previous section.\n\n3.2 Example: Circuit Diagnosis Revisited\nWe now show how the framework can be used to extend the example of Section 2.5 to\nincorporate time, allowing the agent to perform a sequence of tests.\nWe want to model the process of diagnosis. That is, we want to model the agent's beliefs about\nthe circuit while it performs a sequence of tests, and how the observations at each step affects\nher beliefs. Thus, we want to model the agent and the circuit as part of a system. To do so, we\nneed to describe the agent's local state and the state of the environment. The construction\nwe used in Section 2.5 provides a natural division between the two: The agent's state is\nthe sequence of input\u2013output relations observed, while the environment's state describes the\nfaulty components of the circuit and the values of all the lines. This corresponds to our\nintuitions, since the agent can observe only the input\u2013output relations. Each run describes\nthe results of a specific series of tests the agent performs and the results he observes. We make\ntwo additional assumptions: (1) the agent does not forget what tests were performed and their\nresults, and (2) the faults are persistent and do not change over time. Formally, we define the\nagent's state r1 (m) to be ho(r,0) , . . . , o(r,m) i, where o(r,m) describes the input\u2013output relation\nobserved at time m. We define the environment state re (m) = (fault(r, m), value(r, m)) to be\nthe failure set at (r) and the values of all the lines. We capture the assumption that faults do\nnot change by requiring that fault(r, m) = fault(r, 0). The system Rdiag consists of all runs\nr satisfying these requirements in which value(r, m) is consistent with fault(r, m) and o(r,m)\nfor all m.\nGiven the system Rdiag , we can define two interpreted plausibility systems corresponding\n9\n\nIt is easy to add other temporal modalities such as until, eventually, since, etc. These do not\nplay a role in this work.\n\n28\n\n\fto the two plausibility measures we considered in Section 2.5. In both systems, W(r,m,1) =\nKi (r, m). In Idiag,1 , we compare two points (r1 , m) and (r2 , m) by comparing the size of\nfault(r1 , m) and fault(r2 , m), while in Idiag,2 we check whether one failure set is a subset of\nthe other. At a point (r, m), the agent considers possible all the points where he performed\nthe same tests up to time m and observed the same results. As before, the agent believes that\nthe failure set is one of the minimal explanations of his observations. As the agent performs\nmore tests, his knowledge increases and his beliefs might change.\nWe define Bel(I, r, m) to be the set of failure sets (i.e., diagnoses) that the agent considers\npossible at (r, m). Belief change in Idiag,1 is characterized by the following proposition.\nProposition 18 If there is some f \u2208 Bel(Idiag,1 , r, m) that is consistent with the new observation o(r,m+1) , then Bel(Idiag,1 , r, m + 1) consists of all the failure sets in Bel(Idiag,1 , r, m)\nthat are consistent with o(r,m+1) . If all f \u2208 B(Idiag,1 , r, m) are inconsistent with o(r,m+1) , then\nB(Idiag,1 , r, m + 1) consists of all failure sets of cardinality j that are consistent with o(r,m+1) ,\nwhere j is the least cardinality for which there is at least one failure set consistent with\no(r,m+1) .\n\nPROOF. Straightforward; left to the reader. \u2737\n\nThus, in Idiag,1 , a new observation consistent with the current set of most likely explanations reduces this set (to those consistent with the new observation). On the other hand,\na surprising observation (one inconsistent with the current set of most likely explanations)\nhas a rather drastic effect. It easily follows from Proposition 18 that if o(r,m+1) is surprising, then Bel(Idiag,1 , r, m) \u2229 Bel(Idiag,1 , r, m + 1) = \u2205, so the agent discards all his current\nexplanations in this case. Moreover, an easy induction on m shows that if Bel(Idiag,1 , r, m) \u2229\nBel(Idiag,1 , r, m + 1) = \u2205, then the cardinality of the failure sets in Bel(Idiag,1 , r, m + 1) is\ngreater than the cardinality of failure sets in Bel(Idiag,1 , r, m). Thus, in this case, the explanations in Bel(Idiag,1 , r, m + 1) are more complicated than those in B(Idiag,1 , r, m). Notice\nthat if we can characterize the observation o(r,m+1) in our language-that is, if we have a\nformula \u03c6 such (I, r \u2032 , m\u2032 ) |= \u03c6 if and only if o(r\u2032 ,m\u2032 ) = o(r,m+1) -then we can also express the\nfact that agent i considers it surprising: This is true precisely if (Idiag,1 , r, m) |= Bi \u00ac \u03c6.\nBelief change in Idiag,2 is quite different, as the following proposition shows. Given a failure\nset f , we define ext(f ) = {f \u2032 : f \u2286 f \u2032 }. Thus, ext(f ) consists of all the failure sets that\nextend f .\nProposition 19 Bel(Idiag,2 , r, m + 1) consists of the minimal (according to \u2286) failure sets\nin \u222af \u2208Bel(I\next(f ) that are consistent with o(r,m+1) .\n,r,m)\ndiag,2\n\nPROOF. Straightforward; left to the reader. \u2737\n29\n\n\fWe see that, as with Idiag,1 , failure sets that are consistent with the new observation are\nretained. However, unlike Idiag,1 , failure sets that are discarded are replaced by more complicated failure sets even if some of the explanations considered most likely at (r, m) are\nconsistent with the new observation. Moreover, while new failure sets in Bel(Idiag,1 , r, m + 1)\ncan be unrelated to failure sets in Bel(Idiag,1 , r, m), in Idiag,2 the new failure sets must be\nextensions of some discarded failure sets. Thus, in Idiag,1 the agent does not consider new\nfailure sets as long as the observation is not surprising. On the other hand, in Idiag,2 the\nagent has to examine new candidates after each test. The latter behavior is essentially that\ndescribed by Reiter [Rei87, Section 5].\n3.3 Axiomatizing the Language of Knowledge, Plausibility and Time\nWe now present sound and complete axiomatization for the language LKCT . The technical\ndetails are much in the spirit of the results of Section 2.8, with two exceptions. First, we\nneed to deal also with the temporal modality . Second, instead of dealing with worlds,\nwe are dealing with systems that have some structure, i.e., the distinction between agents'\nlocal state and the environment's state. As we shall see, both issues can be dealt with in a\nstraightforward manner.\nThe axiom system AXT consists of the axioms and rule in the axiom system AX of Section 2.8\nand the following axioms and rule the describe the properties of :.\nT1. \u03c6 \u2227 (\u03c6 \u21d2 \u03c8) \u21d2\nT2. \u03c6 \u2261 \u00ac \u00ac\u03c6\nRT1. From \u03c6 infer \u03c6.\n\n\u03c8\n\nLet C be the set of all plausibility interpreted systems.\nTheorem 20 The axiom system AXT is a sound and complete axiomatization of LKCT with\nrespect to C.\nPROOF. See Appendix A.3. \u2737\nWe can also prove a result analogous to Theorem 12 that describes a complete axiomatization\nfor the classes of systems satisfying some of the assumptions we examined in Section 2.4.\nTheorem 21 Let A be a subset of {RANK, NORM, REF, UNIF, CONS, SDP} and let A be\nthe corresponding subset of {C5, C6, C7, C8, C9, C10}. Then AXT \u222a A is a sound and\ncomplete axiomatization with respect to systems in C satisfying A.\nPROOF. See Appendix A.3. \u2737\n30\n\n\f4\n\nPrior Plausibilities\n\nThe formal framework of knowledge, plausibility and time described in the previous section\nraises a serious problem: While it is easy to see where the \u223ci relations that define knowledge\ncome from, the same cannot be said for the plausibility spaces Pi (r, m). We now present one\npossible answer to this question, inspired by probability theory.\nUp to now, we have allowed the plausibility assessment at each point to be almost arbitrary.\nIn particular, the plausibility space Pi (r, m) can be quite different from Pi (r, m + 1). Typically, we would expect there to be some relationship between these successive plausibility\nassessments. For example, it seems reasonable to expect that the new plausibility assessment\nshould incorporate whatever was learned at (r, m+1), but otherwise involve minimal changes\nfrom Pi (r, m).\nOne way of doing this in probability theory is by conditioning. If we start with a probability\nfunction Pr and observe E, where Pr(E) > 0, then the conditional probability function PrE\nis defined so that PrE (A) = Pr(A \u2229 E)/ Pr(E). Typically PrE (A) is denoted Pr(A|E). Notice\nthat PrE incorporates the new information E by giving it probability 1. It also is a minimal\nchange from Pr in the sense that if A, B \u2286 E, then Pr(A)/ Pr(B) = Pr(A|E)/ Pr(B|E): the\nrelative probability of events consistent with E is not changed by conditioning. 10\nConditioning is a standard technique in probability theory, and can be justified in a number\nof ways, one of which is the notion of \"minimal change\" we have just described. Another\njustification is a \"Dutch book\" argument [Fin72,Ram31], which shows that if an agent uses\nsome other method of updating probabilities, then it is possible to construct a betting game\nin which he will always lose. Probability measures are particular instances of plausibility\nmeasures. Can we generalize the notion of conditioning to plausibility measures?\nIt immediately follows from the definitions that the ordering of the likelihood of events\ninduced by PrE is determined by the ordering induced by Pr:\nPr(A|E) \u2264 Pr(B|E) if and only if Pr(A \u2229 E) \u2264 Pr(B \u2229 E).\nWe want the analogous property for plausibility:\nCOND Pl(A|C) \u2264 Pl(B|C) if and only if Pl(A \u2229 C) \u2264 Pl(B \u2229 C).\nThis rule determines the order induced by posterior plausibilities. Since we are interested\nonly in this aspect of plausibility, any method of conditioning that satisfies COND will do for\n10\n\nThere is another sense in which PrE represents the minimal change from Pr. If we measure the\n\"distance\" of a probability distribution Pr\u2032 from Pr in terms of the cross-entropy of Pr\u2032 relative\nto Pr, then it is well known that PrE is the distribution that minimizes the relative cross-entropy\nfrom Pr among all distributions Pr\u2032 such that Pr\u2032 (E) = 1 [KL51]. Indeed, this holds true for other\ndistance measures as well [DZ82].\n\n31\n\n\four present purposes. (See [FH95] for an examination of other properties we might require of\nconditioning.) Notice that any two methods for conditioning are isomorphic in the following\nsense: Let S1 = (W1 , Pl1 ) and S2 = (W2 , Pl2 ) be two plausibility spaces. We say that S1 and\nS2 are (order) isomorphic if there is a bijection h from W1 to W2 such that, for A, B \u2286 W1 ,\nwe have Pl1 (A) \u2264 Pl1 (B) if and only if Pl2 (h(A)) \u2264 Pl2 (h(B)). Any two definitions of\nconditioning that satisfy COND result in order-isomorphic plausibility spaces (see [FH95]).\nThis discussion suggests that we define Pl(r,m+1,i) to be the result of conditioning Pl(r,m,i)\non the new knowledge gained by agent i at (r, m + 1). This, however, leads to the following\ntechnical problem. If the agent gains new knowledge at (r, m + 1), then ri (m) 6= ri (m + 1).\nThis implies that the sets of points the agent considers possible are disjoint, i.e., Ki (r, m) \u2229\nKi (r, m+1) = \u2205. But then CONS implies that Pl(r,m,i) and Pl(r,m+1,i) are defined over disjoint\nspaces, so we cannot apply COND.\nWe circumvent this difficulty by working at the level of runs. The approach we propose\nresembles the Bayesian approach to probabilities. Bayesians assume that agents start with\npriors on all possible events. If we were thinking probabilistically, we could imagine the agents\nin a multi-agent system starting with priors on the runs in the system. Since a run describes\na complete history over time, this means that the agents are putting a prior probability\non the sequences of events that could happen. We would then expect the agent to modify\nhis prior by conditioning on whatever information he has learned. This is essentially the\napproach taken in [HT93] to defining how the agents' probability distribution changes in a\nmulti-agent system. We can do the analogous thing with plausibility.\nWe start by making the simplifying assumption that we are dealing with synchronous systems\nwhere agents have perfect recall [HV89]. Intuitively, this means that the agents know what the\ntime is and do not forget the observations they have made. Formally, a system is synchronous\nif for any i, (r, m) \u223ci (r \u2032 , m\u2032 ) only if m = m\u2032 . Notice that by restricting to synchronous\nsystems, if we further assume that the plausibility measure Pi (r, m) satisfies CONS, we never\nhave to compare the plausibilities of two different points on the same run. In synchronous\nsystems, agent i has perfect recall if (r \u2032 , m + 1) \u223ci (r, m + 1) implies (r \u2032 , m) \u223ci (r, m). Thus,\nagent i considers run r possible at the point (r, m + 1) only if he also considers it possible\nat (r, m). This means that any runs considered impossible at (r, m) are also considered\nimpossible at (r, m + 1); an agent does not forget what he knew.\nJust as with probability, we assume that an agent has a prior plausibility measure on runs,\nthat describes his prior assessment on the possible executions of the system. As the agent\ngains knowledge, he updates his prior by conditioning. More precisely, at each point (r, m),\nthe agent conditions his previous assessment on the set of runs considered possible at (r, m).\nThis is process is shown in Figure 2. This results in an updated assessment (posterior)\nof the plausibility of runs. This posterior induces, via a projection from runs to points, a\nplausibility measure on points. We can think of agent i's posterior at time m as simply his\nprior conditioned on his knowledge at time m.\nTo make this precise, let S = (W, Pl) be a plausibility space. Define the projection of S on\n32\n\n\fTime\n\nr\n\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\n\u275c\n\u275c\n\u275c\n\u275c\n\u261b\u271f\n\u275c\n\u275c\n\u2721\u2720\n\u275c\n\n\u275c\n\u275c\n\u275c\n\u275c\n\u261b\u271f\n\u275c\n\u275c\n\u2721\u2720\n\u275c\n\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\u275c\n\nFig. 2. Schematic description of how the agent's knowledge evolves in time in synchronous systems\nwith perfect recall. The boxes represent the set of points in Ki (r, m). Since the system is synchronous, at each time point, the agent consider possible points at the same time. Since the agent\nhas perfect recall, as time progresses, the agent considers smaller and smaller sets of runs possible.\nThe ovals represent two disjoint events that correspond to the same set of runs.\n\nE as S|E = (W |E , Pl|E ), where W |E = W \u2229 E and Pl|E is the restriction of Pl to W |E .\nProjection is similar to conditioning: for any definition of conditioning that satisfies COND\nif A, B \u2286 E, then Pl(A|E) \u2264 Pl(B|E) if and only if Pl|E (A) \u2264 Pl|E (B). Indeed, S|E is\nessentially isomorphic to any conditional plausibility measure that results from conditioning\non E. 11\nWe can now define what it means for a plausibility measure on points to be generated by\na prior. Suppose that agent i's prior plausibility at run r is P(r,i) = (R(r,i) , Pl(r,i) ), where\nR(r,i) \u2286 R. Our intuition is that the agent conditions the prior by his knowledge at time\n(r, m). In our framework, the agent's knowledge at time m is the set of point Ki (r, m). We\nneed to convert this set of points to an event in terms of runs. If A is a set of points, we\ndefine R(A) = {r : \u2203m((r, m) \u2208 A)} to be the set of runs on which the points in A lie. Using\nthis notation, the set of runs agent i considers possible at (r, m) is simply R(Ki (r, m)). Thus,\nafter conditioning on this set of runs, we get agent i's posterior at (r, m), which is simply\nthe projection of the prior on the observation: Pl(r,i) |R(Ki (r,m)) . We now use this plausibility\nmeasure, which is a measure on a set of runs, to define Pi (r, m), which is a measure on a\nset of points. We do so in the most straightforward way: we project each run to a point\nthat lies on it. Formally, we say that Pi (r, m) is the time m projection of P(r,i) |R(Ki (r,m))\n11\n\nTo make this precise, we need a notion that is slightly more general than isomorphism. Let\nP = (W, Pr) be a probability space. A set A is called a support of P if Pr(A) = 0. We can\ndefine a similar notion for plausibility spaces. Let S = (W, Pl) be a plausibility space. We say that\nA \u2286 W is a support of S, if for all B \u2286 W , Pl(B) = Pl(B \u2229 A). Thus, only B \u2229 A is relevant\nfor determining the plausibility of B. This certainly implies that Pl(A) = \u22a5, since we must have\nPl(A) = Pl(A \u2229 A) = Pl(\u2205), but the converse does not hold in general. In probability spaces,\nPr(A) = 0 implies that Pr(B) = Pr(B \u2229 A) for all B, but the analogous condition does not hold\nfor arbitrary plausibility spaces. We say that two plausibility spaces S1 and S2 are essentially\n(order) isomorphic if there are supports C1 and C2 of S1 and S2 , respectively, such that S1 |C1 is\nisomorphic to S2 |C2 . It is easy to see that, as expected, essential isomorphism defines an equivalence\nrelation among plausibility spaces. Finally, it is easy to see that if S = (W, Pl), then (W, Pl(*|E))\nis essentially isomorphic to S|E when we use any conditioning method that satisfies COND.\n\n33\n\n\fconditioning\n\nRuns\n\nPl(r,i)\n\nPoints\nPrior\n\nR(Ki (r, m))\n\nPl(r,i) |R(Ki (r,m))\n\nKi (r, m)\n\nPl(r,m,i)\n\nEvidence\n\nPosterior\n\nFig. 3. Schematic description of the entities involved in the definition of priors. Note some are\ndefined over runs and some over points.\n\nif Pi (r, m) = (W(r,m,i) , Pl(r,m,i) ), where W(r,m,i) = {(r \u2032 , m) \u2208 Ki (r, m) : r \u2032 \u2208 R(r,i) } and for\nall A \u2286 W(r,m,i) , we have that Pl(r,m,i) (A) = Pl(r,i) |R(Ki (r,m)) (R(A)). Pl(r,m,i) is the agent's\nplausibility measure at (r, m). This process is described in Figure 3. The main complications\nare due to the transition back and forth between entities defined over runs and ones defined\nover points.\nWe remark that if the system satisfies perfect recall as well as synchrony, our original intuition\nthat Pi (r, m + 1) should be the result of conditioning Pi (r, m) on the knowledge that agent\ni acquires at (r, m + 1) can be captured more directly. We can in fact construct Pi (r, m + 1)\nfrom Pi (r, m) by what can be viewed as conditioning on the agent's new information: We take\nPi (r, m) and project it one time step forward by replacing each point (r \u2032 , m) by (r \u2032, m+1). We\nthen condition on Ki (r, m + 1) (i.e., the agent's knowledge at (r, m + 1)) to get Pi (r, m, i + 1).\nProposition 22 Let I be a synchronous system satisfying perfect recall such that Pl(r,m,i)\nis the time m projection of a prior Pl(r,i) on runs for all runs r, times m, and agents i.\nLet prev(A) = {(r, m) : (r, m + 1) \u2208 A}. Then Pl(r,m+1) (A) \u2264 Pl(r,m+1) (B) if and only if\nPl(r,m) (prev(A)) \u2264 Pl(r,m) (prev(B)), for all runs r, times m, and sets A, B \u2208 W(r,m+1) .\nPROOF. Straightforward; left to the reader. \u2737\nWe say that I = (R, \u03c0, P) satisfies PRIOR if I is synchronous and for each run r and agent\ni there is a prior plausibility P(r,i) such that for all m, Pi (r, m) is the time m projection of\nP(r,i) .\nExample 23 It is easy to verify that the two systems we consider in Section 3.2 satisfy\n34\n\n\fPRIOR. In both systems, the prior P(r,i) is independent of the run r, and is determined by\nthe failure set in each run.\nBy using prior plausibility measures, we have reduced the question of where the plausibility\nmeasure at each point comes from to the simpler question of where the prior comes from.\nWhile this question is far from trivial, it is analogous to a question that needs to be addressed\nby anyone using a Bayesian approach. Just as with probability theory, in many applications\nthere is a natural prior (or class of priors) that we can use.\nBy conditioning on plausibility rather than probability, we can deal with a standard problem\nin the Bayesian approach, that of conditioning on an event of measure 0: Notice that whenever\na prior assigns an event a probability measure of 0 it is not possible to condition on that\nevent. The standard solution in the Bayesian school is to give every event of interest, no\nmatter how unlikely, a small positive probability. 12 We may well discover that a formula \u03c6\nthat we believed to be true, i.e., one that was true in all the most plausible worlds, is in fact\nfalse. Under the probabilistic interpretation of plausibility, this means that we are essentially\nconditioning on an event (\u00ac\u03c6) of measure 0. The plausibility approach has no problem with\nthis: the conditioning process described above still makes perfect sense.\n\n4.1 Conditioning as Minimal Change of Belief\nIn this section we examine the properties of conditioning as an approach to minimal change\nof beliefs and relate our approach to others in the literature.\nRecall that QUAL guarantees that belief is closed under logical implication and conjunction\n(Theorem 4). In a synchronous system where the prior satisfies QUAL, it is not hard to see\nthat conditioning preserves QUAL. Thus, we get the following result.\nProposition 24 Let I be a synchronous system satisfying perfect recall and PRIOR. If the\nprior Pl(r,i) satisfies A2 for all runs r and agents i, then axiom K2 is valid in I for Bi .\n\nPROOF. Straightforward; left to the reader. \u2737\n\nThis result shows that condition A2 is sufficient to get beliefs that satisfy K2. Is it also\nnecessary? In general, the answer is no. However, A2 is the most natural condition that\nensures that K2 is satisfied. To see this, note that if K2 is valid in I then A2 holds for all\npairwise disjoint subsets A1 , A2 and A3 of points in I definable in the language such that\nR(Ki (r, m)) = A1 \u222a A2 \u222a A3 for some run r, agent i, and time m. Thus, if we assume that\nthe language is rich enough so that all subsets of I are definable (in that, for each subset A\n12\n\nOf course, this requires that there be only countably many events of interest.\n\n35\n\n\fand agent i, there is a formula \u03c6 and point (r, m) such that A = [[\u03c6]](r,m,i) ), then K2 forces\nA2.\nIn view of this discussion, we focus in this section on synchronous systems with a qualitative\nprior.\nNext, we examine how changes in beliefs are determined by the prior. Using Proposition 22,\nwe now show that we can characterize, within our language, how the agent's beliefs change\nvia conditioning, provided that we can describe in the language what knowledge the agent\nacquired. We say that a formula \u03c6 characterizes agent i's knowledge at (r, m+1) with respect\nto his knowledge at (r, m) if, for all (r \u2032 , m) \u2208 Ki (r, m), we have (r \u2032 , m + 1) |= \u03c6 if and only if\n(r \u2032 , m + 1) \u2208 Ki (r, m + 1). That is, among the points that succeed points that are considered\npossible at time m, exactly these satisfying \u03c6 are considered possible at time m + 1. Of\ncourse, it is not always possible to characterize the agent's new knowledge by a formula in\nour language. However, in many applications we can limit our attention to systems where\nit is possible. (This is the case, for example, in our treatment of revision and update in\n[Fri97,FH97a].) In such systems, we can characterize within the agent's belief change process\nin the language.\nProposition 25 Let I be a synchronous system satisfying perfect recall and PRIOR. If \u03c6\ncharacterizes agent i's knowledge at (r, m + 1) with respect to his knowledge at (r, m), then\n(I, r, m + 1) |= \u03c8 \u2192i \u03be if and only if (I, r, m) |= (\u03c6 \u2227 \u03c8) \u2192i \u03be.\nPROOF. See Appendix A.4. \u2737\nCorollary 26 Let I be a synchronous system satisfying perfect recall and PRIOR. If \u03c6\ncharacterizes agent i's knowledge at (r, m + 1) with respect to his knowledge at (r, m), then\n(I, r, m + 1) |= Bi \u03c8 if and only if (I, r, m) |= Ki ( \u03c6 \u21d2 ( \u03c6 \u2192i \u03c8)). Moreover, if I also\nsatisfies SDP, then (I, r, m + 1) |= Bi \u03c8 if and only if (I, r, m) |= \u03c6 \u2192i \u03c8.\nWe now use this result to relate our approach to other approaches for modeling conditionals\nin the literature. Boutilier [Bou92], Goldszmidt and Pearl [GP92], and Lamarre and Shoham\n[LS94] give conditional statements similar semantics (using a preference ordering), but \u03c6 \u2192 \u03c8\nis read \"after learning \u03c6, \u03c8 is believed\". Two crucial assumptions are made in these papers.\nThe first is that the agent considers only one plausibility assessment, which in our terminology\namounts to SDP. The second is that propositions are static, i.e., their truth value does not\nchange along a run. 13 Formally, a system is static if \u03c0(r(m)) = \u03c0(r(0)) for all runs r and\ntimes m. This implies that for any propositional formula \u03c6, we have that \u03c6 \u2261 \u03c6. These\ntwo assumptions lead to a characterization of belief change.\nCorollary 27 Let I be a synchronous static system satisfying PRIOR, SDP, and perfect\nrecall, and let \u03c6 and \u03c8 be propositional formulas. If \u03c6 characterizes agent i's knowledge at\n13\n\nThis assumption is only implicit, since none of these papers have an explicit representation of\ntime. Nevertheless, it is clear that this assumption is being made.\n\n36\n\n\f(r, m + 1) with respect to his knowledge at (r, m), then (I, r, m + 1) |= Bi \u03c8 if and only if\n(I, r, m) |= \u03c6 \u2192i \u03c8.\nWhile this result shows that, in certain contexts, there is a connection between a statement\nsuch as \"typically \u03c6's are \u03c8's\" (which is how we have between interpreting \u03c6 \u2192i \u03c8) and \"after\nlearning \u03c6, \u03c8 is believed\" (which is how it is interpreted in [Bou92,GP92,LS94]), the two\nreadings are in general quite different. For one thing, notice that Corollary 27 assumes that \u03c6\nand \u03c8 are propositional formulas. This is a necessary assumption. If \u03c6 and \u03c8 contain modal\nformulas, then \u03c6 \u2192 \u03c8 does not necessarily imply that the agent believes \u03c8 at the next time\nstep. For example, if (I, r, m) |= Bi \u03c8, then for any formula \u03c6, we have (I, r, m) |= \u03c6 \u2192i Bi \u03c8,\nregardless of whether Bi \u03c8 is believed at (r, m + 1). In [FH94b], we examine conditionals of\nthe form \u03c6 > \u03c8 intended to capture the second interpretation \"\u03c8 is believed after learning\n\u03c6\". The semantics for these conditionals involves examining future time points, just as our\nintuitive reading dictates. As we have just seen, > and \u2192 are quite different when we consider\nmodal formulas in the scope of these conditionals.\nThis discussion shows one of the benefits of representing time explicitly. In our framework\nwe can distinguish between agents' plausibility assessment and their belief dynamics. Of\ncourse, we would like agents to be persistent in their assessment, which is exactly what\nconditioning captures. In the presence of several assumptions, we get a close connection\nbetween agents' conditional beliefs and how their beliefs change. This allows us to identify\nsome of the assumptions implicitly made in previous approaches. For example, all of the\napproaches we mentioned above would not apply when we consider a changing environment,\nsince they cannot reason about how the environment changes between one time point and\nthe next.\nFinally, we examine the work of Battigalli and Bonanno [BB97]. They consider a logic of\nknowledge, belief, and time, and attempt to capture properties of \"minimal change\" of beliefs.\nTheir language is slightly different from ours. Instead of introducing a temporal modality,\nthey define a different belief and knowledge modality for each time step: B t \u03c6 reads \"the\nagent believes \u03c6 at time t\". Battigalli and Bonanno also assume that propositions are static\nand do not change in time. Thus, the only changes are in terms of the agent's knowledge\nand belief. Battigalli and Bonanno propose an axiom system similar to the axioms of Kraus\nand Lehmann (that is, they use K5 for knowledge is K5, KD45 for belief, and take axioms\nKB1 and KB2 of Section 2.7 to characterize the connection between knowledge and belief)\nthat also includes two additional axioms that can be written in our language as\nBT1. Bi Bi \u03c6 \u21d2 Bi \u03c6\nBT2. Bi \u03c6 \u21d2 Bi Bi \u03c6\nBattigalli and Bonanno claim that these axioms capture the principle that the agent does\nnot change her mind unless new knowledge forces her to do so. Intuitively, this principle\nalso applies to conditioning, and thus it is instructive to understand when these axioms are\nsatisfied in our framework.\n37\n\n\fIt turns out that RANK combined with a minimal assumption implies both BT1 and BT2.\nWe say that a system has finite branching if it allows only finitely many \"branches\" at each\nlocal state of an agent (that is there are only finitely many observations that an agent can\nmake at each point).\nLemma 28 Let I be a synchronous static system satisfying PRIOR, RANK, SDP, and\nperfect recall that has finite branching. Then (I, r, m) |= Bi \u03c6 \u21d4 Bi Bi \u03c6 for all propositional\nformulas \u03c6.\nPROOF. See Appendix A.4. \u2737\nAre these conditions necessary to characterize BT1 and BT2? The answer is no. First, the\nproof of Lemma 28 applies to systems with infinite branching, if the agents' prior satisfies\nan infinitary version of A2. As shown in [FHK96], this infinitary version is satisfied by\n\u03ba-rankings and preference orderings that are well founded (that is, they have no infinite\ndescending sequences * * * \u227a w3 \u227a w2 \u227a w1 ). Thus, any system with static propositions\nwhose prior is induced by a well-founded preference order satisfies BT1 and BT2. Note that\nBT1 and BT2 do not characterize RANK, since they put restrictions only on certain events\n(ones definable by a conjunction of a formula and the agent's new knowledge at some time\npoint). However, RANK is the most natural restriction that implies these axioms.\nThus, we see that Battigalli and Bonanno essentially require systems with minimal change\nto satisfy conditioning with a prior that is a ranking. As we shall see in the next section,\nsimilar requirements are made by the AGM formulation of belief revision [AGM85].\n4.2 Properties of Prior Plausibilities\nIf we take the plausibilities in a system to be generated by a prior, then many of the conditions\nwe are interested in, such as QUAL and REF, can be viewed as being as being induced\nby the analogous property on the prior. We have considered these properties only in the\ncontext of Kripke structures for knowledge and probability, so to make sense of the prior\nhaving the \"analogous property\", we have to be able to view the set of runs as a Kripke\nstructure for knowledge and probability. Let I be a synchronous system satisfying perfect\nrecall and PRIOR. Define MIr = (R, \u03c0 r , K1r , . . . , Knr , Pir , . . . , Pnr ), where \u03c0 r is an arbitrary\ntruth assignment, Kir is the full relation, i.e., R \u00d7 R, and Pir (r) = P(r,i) , the prior of agent i\nat run r.\nProposition 29 Let I be a synchronous system satisfying perfect recall and PRIOR. If MIr\nsatisfies QUAL, REF, SDP, UNIF or RANK, then so does I.\nPROOF. Straightforward; left to the reader. \u2737\n38\n\n\fThus, by constructing priors that satisfy various properties, we can ensure that the resulting\nsystem also satisfies them. In particular, Proposition 29 implies that if P(r,i) is independent\nof r, so that agent i's prior is independent of the run he is in, then I satisfies SDP. A\nsomewhat weaker assumption-that the set of runs can be partitioned into disjoint subsets\nR1 , . . . , Rk such that for r, r \u2032 \u2208 Rj , we have P(r,i) = P(r\u2032 ,i) = (Rj , Plj )-ensures that I\nsatisfies UNIF. Intuitively, the sets Rj correspond to different settings of parameters. Once\nwe set the parameters, then we fix the plausibility measure (and it is the same at all runs\nthat have the same setting of the parameters).\nWe conclude this section by examining whether assuming conditioning limits the expressiveness of our belief change operation. A well-known result of Diaconis and Zabell [DZ82]\nthat shows that, in a precise sense, any form of coherent probabilistic belief change can be\ndescribed by conditioning. In particular, they show that, given two probability distributions\nPr and Pr\u2032 on a finite space W that are coherent in the sense that Pr(A) = 0 implies that\nPr\u2032 (A) = 0, there is a space W \u2217 of the form W \u00d7 X, a subset E of W \u2217 , and a distribution\nPr\u2032\u2032 on W \u2217 such that, for all A \u2286 W , we have Pr\u2032\u2032 (A \u00d7 X) = Pr(A) (so that Pr\u2032\u2032 can be\nviewed as an extension of Pr) and Pr\u2032 (A) = Pr\u2032\u2032 (A \u00d7 X|E).\nWe can prove a result in a somewhat similar spirit in our framework. The first step is to\ndefine a plausibilistic analogue of coherence in systems.\nLet I be a synchronous system. We say that I is coherent if the following condition is\nsatisfied for all r and m: Suppose R \u2286 R, Am \u2286 W(r,m,i) , R(Am ) = R \u2229 R(W(r,m,i) ), Am+1 \u2286\nW(r,m+1,i) , and R(Am+1 ) = R \u2229 R(W(r,m+1,i) ). If Pl(r,m,i) (Am ) = \u22a5, then Pl(r,m+1,i) (Am+1 ) =\n\u22a5. Despite the different formulation, this condition is analogous to the probabilistic coherence\nof Diaconis and Zabell. Roughly speaking, if a set of runs has plausibility \u22a5 (which is\nanalogous to probability 0 for Diaconis and Zabell) at time m, then it is required to have\nplausibility \u22a5 at time m + 1. More precisely, coherence of a system ensures that sets of runs\nthat were considered implausible at (r, m), either by being outside W(r,m,i) or by being given\nplausibility \u22a5(r,m,i) , are also considered implausible at (r, m + 1). Note, this condition does\nnot put any constraints on how the runs that are considered possible are ordered. It is easy\nto verify that the following axiom is valid in coherent systems:\nCOH. Ni \u03c6 \u21d2\n\nNi \u03c6\n\nProposition 30 If I is a synchronous and coherent system, then COH is valid in I.\n\nPROOF. Straightforward; left to the reader. \u2737\n\nThere is a sense in which the converse to Proposition 30 holds as well: Given a synchronous\nsystem that is not coherent, we can define a truth assignment \u03c0 in this system for which\n39\n\n\fCOH does not hold. 14\nIt is easy to see that coherence is a necessary condition for satisfying PRIOR.\nProposition 31 If I is a synchronous system satisfying perfect recall and PRIOR, then I\nis coherent.\n\nPROOF. Straightforward; left to the reader. \u2737\n\nThus, PRIOR forces systems to be coherent, and hence to satisfy COH. It also forces systems\nto satisfy CONS, and hence C5. As we shall see, it also forces some other semantic properties.\nNevertheless, we can show that for coherent systems that satisfy CONS, PRIOR does not\nforce any additional properties, by proving an analogue to the Diaconis and Zabell result in\nour framework.\nWe say that a formula \u03c6 \u2208 LKCT is temporally linear if temporal modalities in \u03c6 do not\nappear in the scope of the Ki or \u2192i modalities. Thus, for example, a formula such as\n(\u03c6 \u2192i \u03c8) \u21d2 Bi \u03c8 is temporally linear, while Ki ( \u03c6 \u2192i \u03c8) \u21d2 Bi \u03c8 is not. Temporal\nlinearity ensures that all the temporal connectives in \u03c6 are evaluated with respect to a single\nrun. The following result says that, at least for temporally linear formulas, we can view belief\nchange in a coherent system I as coming from conditioning on a prior, in the sense that we\ncan embed I into a larger system where this is the case.\nTheorem 32 Let A be a subset of {QUAL, NORM, REF, RANK} and let I be a coherent\nsynchronous system satisfying perfect recall, CONS, and A. Then there is a synchronous\nsystem I \u2032 satisfying perfect recall, PRIOR, and A, and a mapping f : R 7\u2192 R\u2032 such that for\nall temporally linear formulas \u03c6 \u2208 LKCT , we have (I, r, m) |= \u03c6 if and only if (I \u2032 , f (r), m) |=\n\u03c6. 15\n\nPROOF. See Appendix A.5. \u2737\n\nNotice that formulas that just compare an agent's beliefs (or knowledge) at successive time\npoints are temporally linear. All the AGM postulates and the KM postulates (when translated to our language) are of this form. Not surprisingly, as we show in [Fri97,FH97a], these\npostulates can be captured by systems with the appropriate prior plausibility.\n14\n\nWe remark that COH is analogous to the axiom Ki \u03c6 \u21d2 Ki \u03c6 that characterizes perfect recall\nin synchronous systems [FHMV95]. Roughly speaking, this is because coherence ensures that the\nagent does not forget what she ruled out as implausible.\n15 We note that this result is, in a sense, stronger than Diaconis and Zabell's. They examine only\nthe probability of events, which are essentially propositional formulas (i.e., formulas without modal\noperators).\n\n40\n\n\fCan we extend Theorem 32 to the full language? We conjecture that Theorem 32 actually\nholds for all \u03c6 \u2208 LKCT , not just temporally linear formulas. This conjecture implies that a\nformula is valid with respect to synchronous systems satisfying perfect recall, CONS, and\nPRIOR if and only if it is valid with respect to synchronous coherent systems satisfying\nCONS and perfect recall. That is, except for COH and C9, we do not get any new properties\nby assuming PRIOR and CONS.\nNote that the construction described by Theorem 32 does not necessarily preserve SDP or\nUNIF in the transformation from I to I \u2032 . This is due to the fact that in the presence of SDP\nor UNIF, PRIOR forces new semantic properties. Recall that UNIF implies that there is a\npartition of possible points such that two points (r, m) and (r \u2032 , m\u2032 ) are in the same cell if and\nonly if Pi (r, m) = Pi (r \u2032 , m\u2032 ). Let PERSIST be the requirement that this partition changes\nminimally in time. More precisely, we say that a system satisfies PERSIST if for all runs\nr, r \u2032 \u2208 R and m such that (r, m+1) \u223ci (r \u2032 , m+1), we have that Pi (r, m+1) = Pi (r \u2032 , m+1) if\nand only if Pi (r, m) = Pi (r \u2032 , m). Intuitively, PERSIST (in the presence of synchrony, perfect\nrecall, and CONS) implies that the partition of points at time m + 1 is determined by the\npartition of corresponding points at time m and the knowledge relation at time m + 1.\nProposition 33 If I is a synchronous system that satisfies perfect recall and either PRIOR\nand UNIF, or SDP, then I satisfies PERSIST.\nPROOF. Straightforward; left to the reader. \u2737\nIt is not clear to us at this stage whether PERSIST forces new properties in our language.\nHowever, if we assume that PERSIST holds, we can get a result analogous to Theorem 32.\nTheorem 34 Let A be a subset of {QUAL, NORM, REF, SDP, UNIF, RANK} and let I\nbe a coherent synchronous system satisfying perfect recall, CONS, PERSIST, and A. Then\nthere is a synchronous system I \u2032 satisfying perfect recall, PRIOR, and A, and a mapping\nf : R 7\u2192 R\u2032 such that for all temporally linear formulas \u03c6 \u2208 LKCT , (I, r, m) |= \u03c6 if and only\nif (I \u2032 , f (r), m) |= \u03c6.\nPROOF. See Appendix A.5. \u2737\nThus, the question of whether PRIOR forces new properties in the presence of UNIF reduces to the question of whether PERSIST forces new properties. Finally, since SDP implies\nPERSIST, PRIOR does not force new properties in the presence of SDP.\nOur discussion of conditioning and priors up to now assumed synchrony and perfect recall.\nCan we make sense of conditioning when we relax these assumptions? Note that the definition\nof PRIOR does not rely on perfect recall. PRIOR is well defined even in systems where\nagents can forget. However, in such systems, the intuitions that motivated the use of PRIOR\n41\n\n\fare no longer valid. In particular, PRIOR does not imply coherence and the analogue to\nProposition 22 does not hold: we no longer can construct Pi (r, m + 1) from Pi (r, m) since\nruns that are considered impossible at time m might be considered possible at time m + 1. 16\nDropping the assumption of synchrony also leads to problems, even in the presence of perfect\nrecall. In an asynchronous setting, an agent might consider several points on the same run\npossible. The question then arises as to how (or whether) we should distribute the plausibility\nof a run over these points. Two approaches are considered in a probabilistic setting in [PR97],\nin the context of analyzing games with imperfect recall. It would be of interest to see to what\nextent these approaches can be carried over to the plausibilistic setting.\n\n5\n\nConclusion\n\nWe have proposed a framework for belief dynamics that combines knowledge, time, and\nplausibility (and hence beliefs), and investigated a number of properties of the framework,\nsuch as complete axiomatizations for various sublanguages and various properties of the\nrelationships between the modal operators. Of course, the obvious question is why we should\nconsider this framework at all.\nThere are two features that distinguish our approach from others. The first is that we use\nplausibility to model uncertainty, rather than other approaches that have been mentioned in\nthe literature, such as preference orderings on worlds or \u01eb-semantics. The second is that we\ninclude knowledge and time, as well as belief, explicitly in the framework.\nWe could have easily modified the framework to use other ways of modeling uncertainty.\nIndeed, in a preliminary version of this paper [FH94c], we used preference orderings. We have\nchosen to use plausibility measures for several reasons. First, plausibility measures generalize\nall approaches to representing uncertainty that we are aware of. The use of plausibility makes\nit easier to compare our approach, not only to preference-based approaches (e.g., [Bou92]),\nbut also to approaches based on \u03ba-rankings (e.g., [GP92]), probably measures (e.g., [HT93]),\nor any other measure of uncertainty. More importantly, it makes it easier for us to incorporate\nintuitions from other approaches. We have already seen one example of this phenomenon\nin the present paper: we defined a plausibilistic analogue of conditioning, and used it to\nmodel minimal change. As we show in [FH97a], we can represent the standard approaches to\nminimal change-belief revision and belief update-in terms of conditioning. Moreover, the\nsemantic characterization of conditioning should allow us to apply it more easily to deal with\ncomplications that arise when the language lets us reason about multiple agents, actions,\nand beliefs about beliefs. Another example of adopting probabilistic intuitions is given in\n[Fri97,FH95,FH96b], where plausibilistic analogues of independence and Markov chains are\ndescribed and used to define a novel approach to belief change. We believe that these notions\nwill have applications elsewhere as well. Finally, plausibility measures have the advantage of\n16\n\nWe could, of course, redefine PRIOR so as to guarantee that Proposition 22 holds, but this leads\nto other complications.\n\n42\n\n\fgreater expressive power than other approaches. For example, work on defaults has mainly\nfocused on properties of structures with a finite number of worlds. In our framework, however,\neven a simple system with two global states might have an uncountable number of runs.\nAs shown in [FHK96], once we examine structures with infinitely many worlds, qualitative\nplausibility measures can capture natural ordering of events that cannot be captured by\npreference orderings, possibility measures, or \u03ba-rankings.\nAs we have tried to argue throughout the paper, the explicit representation of knowledge and\ntime makes it much easier to study belief dynamics. Most current work in the area examines\nonly the beliefs of an agent and how they change after incorporating a new belief. Many\nsimplifying assumptions are made: that there is a single agent, that the agent's knowledge\ndoes not change, that new information can be characterized in the language, and so on.\nIt is useful to study this simple setting in order to get at the basic issues of belief change.\nHowever, these simplifying assumptions are not suitable when we want examine belief change\nin more realistic settings (such as the diagnosis example of Section 3.2). This means that\nmost of the results in the current belief change literature are not directly applicable in many\nstandard AI problems. Our framework dispenses with most of the simplifying assumptions\nmade in the literature, and thus can be viewed as a first step towards providing a model of\nmore realistic settings of belief change.\nWe have focused here on the foundations of the framework. In the future, we hope to apply\nthe framework to examine more realistic problems. We have already begun to do this. For\nexample, in [FH94c] we provide a detailed analysis of iterated prisoner dilemma games between two agents. It is well-known that the players cannot cooperate when they have common\nknowledge of rationality. However, we show that they can cooperate when they have common belief of rationality. A recent proposal by van der Meyden [Mey94] for multi-agent belief\nchange can easily be embedded in our framework [van94]. We hope to use our framework to\nstudy some of the problems considered by van der Meyden, such as speech-act semantics.\nAnother natural application area is reasoning about actions and planning in the presence of\nuncertainty. We believe that the flexibility and expressive power of the framework will help\nto clarify what is going on in all these areas.\n\nAcknowledgements\n\nThe authors are grateful to Piepaolo Battigalli, Craig Boutilier, Ronen Brafman, Ron Fagin,\nMoises Goldszmidt, Ron van der Meyden, Yoav Shoham, and particularly Daphne Koller\nand Moshe Vardi for comments on previous versions of this paper and useful discussions\nrelating to this work.\n43\n\n\fA\n\nA.1\n\nProofs\n\nProofs for Section 2.7\n\nTheorem 7: K (resp., K45, KD45) is a sound and complete axiomatization for LB with\nrespect to M (resp., MCONS , MCONS,NORM ).\n\nPROOF. As usual, soundness is straightforward, so we focus on completeness. We prove\nest\n+\ncompleteness by showing that for M \u2208 MK (resp. Met\nK , MK ) there is a structure M \u2208 M\n(resp. M CONS , MCONS,NORM ) such that for all \u03c6 \u2208 LB , we have (M, w) |= \u03c6 if and only if\n(M + , w) |= \u03c6. Completeness then follows from Theorem 1.\nLet M = (W, \u03c0, B1 , . . . , Bn ) be a Kripke structure for belief. We construct a Kripke structure for knowledge and plausibility M + = (W, \u03c0, K1 , . . . , Kn , P1 , . . . , Pn ) as follows. We\nset Ki (w) to be the set of worlds where agent i's beliefs are the same as in w. Formally,\n(w, v) \u2208 Ki if Bi (w) = Bi (v). It is easy to verify that Ki is an equivalence relation. We define\nPi (w) = (W(w,i) , Pl(w,i) ), where W(w,i) = Bi (w) is the set of worlds agent i considers possible,\nPl(w,i) (\u2205) = 0, and Pl(w,i) (A) is 1 if A \u2286 W(w,i) is not empty It is easy to verify that these\n(trivial) plausibility measures are qualitative.\nWe now prove that (M, w) |= \u03c6 if and only (M + , w) |= \u03c6 for any \u03c6 \u2208 LB . This is shown\nby induction on the structure of \u03c6. The only interesting case is if \u03c6 is of the form Bi \u03c6\u2032 .\nAssume (M, w) |= Bi \u03c6\u2032 . We want to show that (M + , w) |= Ki (true \u2192i \u03c6\u2032 ). We start by\nnoting that (w, v) \u2208 Ki if and only if Bi (v) = Bi (w). This implies that Pi (v) = Pi (w). Thus,\n(M + , v) |= true \u2192i \u03c6\u2032 if and only if (M + , w) |= true \u2192i \u03c6\u2032 . Thus, it suffices to show that\n(M + , w) |= true \u2192i \u03c6\u2032 , since this implies that (M + , w) |= Ki (true \u2192i \u03c6\u2032 ), i.e., (M + , w) |=\nBi \u03c6\u2032 . There are two cases. If Bi (w) = \u2205, then W(w,i) = \u2205. This implies that true \u2192i \u03c6\u2032\nholds vacuously. If Bi (w) is not empty, then using the induction hypothesis we conclude that\n[[\u03c6\u2032 ]](w,i) = Bi (w). From the definition of Pl(w,i) we conclude that Pl(w,i) ([[\u03c6\u2032 ]](w,i) ) = 1 and that\nPl(w,i) ([[\u00ac\u03c6\u2032 ]](w,i) ) = 0. Thus, (M + , w) |= true \u2192i \u03c6\u2032 and hence (M + , w) |= Ki (true \u2192i \u03c6\u2032 ).\nNow assume (M, w) |= \u00acBi \u03c6\u2032 . Then there is some v \u2208 Bi (w) such that (M, v) |= \u00ac\u03c6\u2032 .\nUsing the induction hypothesis we conclude that Pl(w,i) ([[\u00ac\u03c6\u2032 ]](w,i) ) = 1. Hence, (M + , w) |=\n\u00ac(true \u2192i \u03c6\u2032 ) and therefore, (M + , w) |= \u00acKi (true \u2192i \u03c6\u2032 ).\n+\n+\nIt remains to show that if M \u2208 Met\nsatisfies CONS, and if M \u2208 Mest\nK then M\nK , then M\nalso satisfies NORM. Assume Bi is transitive and Euclidean. Let w and v be worlds such\nthat (w, v) \u2208 Bi . We claim that Bi (w) = Bi (v). If (w, t) \u2208 Bi , then since Bi is Euclidean we\nget that (v, t) \u2208 Bi . If (v, t) \u2208 Bi , then since Bi is transitive we get that (w, t) \u2208 Bi . Thus,\nBi (v) = Bi (w), as desired. Recall that if Bi (v) = Bi (w), then our construction ensures that\nv \u2208 Ki (w). Hence, Bi (w) \u2286 Ki (w) and M + satisfies CONS. Assume that Bi is serial. This\nimplies that for all w, Bi (w) is not empty. Thus, our construction guarantees that W(w,i) is\nnot empty and Pl(w,i) (W(w,i) ) >\u22a5. \u2737\n\n44\n\n\fTheorem 8: AXKB (resp., AXKB,CONS, AXKB,CONS,NORM ) is a sound and complete axiomatization of LKB with respect to M (resp., MCONS , MCONS,NORM ).\n\nPROOF. Again soundness is straightforward, so we focus on completeness. We sketch a\ncompleteness proof following the usual Makinson [Mak66] style of proof. We describe only\nthe parts that are different from the standard proofs. See, for example, Halpern and Moses\n[HM92] for details.\nIn order to prove completeness, we need only show that if the formula \u03c6 is consistent with the\naxiom system (i.e., AXKB , AXKB,CONS or AXKB,CONS,NORM ) then \u03c6 is satisfiable in a Kripke\nstructure of the appropriate class (i.e., M, MCONS, or MCONS,NORM , respectively).\nLet V be a set of formulas and AX an axiom system. We say that V is AX-consistent if for\nall \u03c61 , . . . \u03c6n \u2208 V , it is not the case that AX \u22a2 \u00ac(\u03c61 \u2227 . . . \u2227 \u03c6n ). The set V is a maximal\nconsistent set if it is consistent, and for each formula \u03c6, either \u03c6 \u2208 V or \u00ac\u03c6 \u2208 V .\nWe now build a canonical model M KB for AXKB , in which every AXKB -consistent formula is\nsatisfiable. M KB has a world wV corresponding to every maximal AXKB -consistent set V of\nformulas; we show that (M KB , wV ) |= \u03c6 if and only if \u03c6 \u2208 V .\nWe proceed as follows. If V is a set of formulas, define V /Ki = {\u03c6 : Ki \u03c6 \u2208 V } and V /Bi =\n{\u03c6 : Bi \u03c6 \u2208 V }. Let M KB = (W, \u03c0, K1 , . . . , Kn , Pi , . . . , Pn ), where\n\u2022\n\u2022\n\u2022\n\u2022\n\nW = {wV : V is a maximal AXKB -consistent set of formulas}\n\u03c0(wV )(p) = true if and only if p \u2208 V\nKi = {(wV , wU ) : V /Ki \u2286 U}\nPi (wV ) = (W(wV ,i) , Pl(wV ,i) ), where W(wV ,i) = {wU : V /Bi \u2286 U}, Pl(wV ,i) (\u2205) = 0, and\nPl(wV ,i) (A) = 1 for A 6= \u2205.\n\nUsing standard arguments, it is easy to show that the Ki 's are equivalence relations (see\n[HM92]). Using a standard induction argument, we can verify that (M KB , wV ) |= \u03c6 if and\nonly if \u03c6 \u2208 V .\nThis construction proves completeness for AXKB . To prove completeness for the other two\nvariants we use the same construction, setting W to correspond to the maximal AXKB,CONSconsistent sets (resp. AXKB,CONS,NORM -consistent sets). We must show that the resulting\ncanonical models satisfy CONS and NORM, respectively.\nLet M KB,CONS be the canonical model constructed for AXKB,CONS . To show that M KB,CONS\nsatisfies CONS, it is enough to show that V /Ki \u2286 V /Bi . To show this, assume \u03c6 \u2208 V /Ki .\nThen Ki \u03c6 \u2208 V . Since KB2 \u2208 AXKB,CONS, we conclude that Bi \u03c6 \u2208 V , and thus \u03c6 \u2208 V /Bi .\nLet M KB,CONS,NORM be the canonical model constructed for AXKB,CONS,NORM . The argument above shows that M KB,CONS,NORM satisfies CONS. To show that it satisfies NORM,\n45\n\n\fi.e., Pl(w,i) (W(w,i) ) >\u22a5, it is enough to show that V /Bi is consistent, for then there must\nbe some U such that V /Bi \u2208 U. Assume, by way of contradiction, that V /Bi is inconsistent. Then there are formulas \u03c61 , . . . , \u03c6m \u2208 V /Bi such that \u22a2 \u00ac(\u03c61 \u2227 . . . \u2227 \u03c6m ). Since\n\u03c61 , . . . , \u03c6n \u2208 V /Bi , we conclude that Bi \u03c61 , . . . , Bi \u03c6m \u2208 V . Using the K45 axioms for Bi ,\nstandard arguments show that Bi (\u03c61 , . . . , \u03c6n ) \u2208 V , and hence that Bi (false) \u2208 V , which\ncontradicts the consistency of V . \u2737\nLemma 10: Let M be a propositional Kripke structure of knowledge and plausibility satisfying CONS and SDP. Suppose that w, i, and \u03b1 are such that the most plausible worlds in\nPi (w) are exactly those worlds in Ki (w) that satisfy \u03b1, i.e., MP(Pi (w)) = {w \u2032 \u2208 Ki (w) :\n(M, w \u2032 ) |= \u03b1}. Then for any formula \u03c6 \u2208 LKB that includes only the modalities Ki and Bi ,\n(M, w) |= \u03c6 if and only if (M, w) |= \u03c6\u2217 , where \u03c6\u2217 is the result of recursively replacing each\nsubformula of the form Bi \u03c8 in \u03c6 by Ki (\u03b1 \u21d2 \u03c8 \u2217 ).\nPROOF. We prove by induction that for any w \u2032 \u2208 Ki (w), (M, w \u2032 ) |= \u03c6 if and only if\n(M, w \u2032 ) |= \u03c6\u2217 . The only interesting case is if \u03c6 has the from Bi \u03c6\u2032 . Suppose that (M, w \u2032) |=\nBi \u03c6\u2032 . This implies that (M, w \u2032 ) |= true \u2192i \u03c6\u2032 , i.e., for all w \u2032\u2032 \u2208 MP(Pi (w \u2032 )) we have\n(M, w \u2032\u2032 ) |= \u03c6\u2032 . Now let w \u2032\u2032 \u2208 Ki (w \u2032 ). If (M, w \u2032\u2032 ) |= \u00ac\u03b1, then (M, w \u2032\u2032 ) |= \u03b1 \u21d2 (\u03c6\u2032 )\u2217 . If\n(M, w \u2032\u2032 ) |= \u03b1 then, by definition, w \u2032\u2032 \u2208 MP(Pi (w)), and since we assumed SDP, MP(Pi (w \u2032 )) =\nMP(Pi (w)). Thus, we conclude that (M, w \u2032\u2032 ) |= \u03c6\u2032 , and using the induction hypothesis we\nget that (M, w \u2032\u2032 ) |= (\u03c6\u2032 )\u2217 . We conclude that all worlds in Ki (w \u2032 ) satisfy \u03b1 \u21d2 (\u03c6\u2032 )\u2217 , and thus\n(M, w \u2032 ) |= Ki (\u03b1 \u21d2 (\u03c6\u2032)\u2217 ). Now assume that (M, w \u2032 ) |= Ki (\u03b1 \u21d2 (\u03c6\u2032 )\u2217 ). Let w \u2032\u2032 be any world\nin Ki (w \u2032). Since we assumed SDP, we have that MP(Pi (w \u2032\u2032 )) = MP(Pi (w)) is the set of\nworlds in Ki (w) that satisfy \u03b1. We conclude, using our induction hypothesis, that all worlds\nin MP(Pi (w \u2032\u2032 )) satisfy \u03c6\u2032 . Hence, (M, w \u2032\u2032 ) |= true \u2192i \u03c6\u2032 . Since this is true for all w \u2032\u2032 \u2208 Ki (w \u2032 )\nwe conclude that (M, w \u2032 ) |= Bi \u03c6\u2032 . \u2737\nA.2\n\nProofs for Section 2.8\n\nTheorem 11: AX is a sound and complete axiomatization for LKC with respect to M.\nPROOF. Again, we just describe the completeness proof. This proof draws on the usual\ncompleteness proofs for S5 modal logic, and the completeness proof for conditional logic\ndescribed in [Fri97,FH97b].\nWe proceed as follows. If V is a set of formulas, define V /Ki = {\u03c6 : Ki \u03c6 \u2208 V } and V /Ni =\n{\u03c6 : Ni \u03c6 \u2208 V }. We define a canonical model M c = (W, \u03c0, K1 , . . . , Kn , Pi , . . . , Pn ) as follows:\n\u2022 W = {wV : V is a maximal AX-consistent set of formulas}\n\u2022 \u03c0(wV )(p) = true if and only if p \u2208 V\n\u2022 Ki = {(wV , wU ) : V /Ki \u2286 U}\n46\n\n\f\u2022 Pi (wV ) = (W(wV ,i) , F(wV ,i) , Pl(wV ,i) ), where\n* W(wV ,i) = {wU : V /Ni \u2286 U},\n* F(wV ,i) = {[\u03c6](wV ,i) : \u03c6 \u2208 LKC } where [\u03c6](wV ,i) = {wU \u2208 W(wV ,i) : \u03c6 \u2208 U}, and\n* Pl(wV ,i) is such that Pl(wV ,i) ([\u03c6](wV ,i) ) \u2264 Pl(wV ,i) ([\u03c8](wV ,i) ) if and only if (\u03c6\u2228\u03c8) \u2192i \u03c8 \u2208 V .\nWe need to verify that M c is indeed a structure in M. Using standard arguments it is easy to\nshow that the Ki relations are equivalence relations. In [Fri97,FH97b] we prove that Pi (wV )\nis a well-defined qualitative plausibility space.\nFinally, we have to show that (M c , wV ) |= \u03c6 if and only if \u03c6 \u2208 V . As usual, this is done by\ninduction on the structure of \u03c6. We use the standard argument for formulas of the form Ki \u03c6\nand arguments from [Fri97,FH97b] for formulas of the from \u03c6 \u2192i \u03c8. We omit the details\nhere. \u2737\nTheorem 12: Let A be a subset of {RANK, NORM, REF, UNIF, CONS, SDP} and let A\nbe the corresponding subset of {C5, C6, C7, C8, C9, C10}. Then AX \u222a A is a sound and\ncomplete axiomatization with respect to the structures in M satisfying A.\nPROOF. Yet again, we focus on completeness. We obtain completeness in each case by\nmodifying the proof of Theorem 11. We construct a canonical model as in that proof, checking\nconsistency with the extended axiom system. The resulting structure is in M and has the\nproperty that (M, wV ) |= \u03c6 if and only if \u03c6 \u2208 V . We just need to show that this structure\nalso satisfies the corresponding semantic restrictions.\nFirst, we consider CONS and axiom C9. Assume that C9 is included as an axiom. It is\neasy to see that this implies that V /Ni \u2286 V /Ki . This implies that W(wV ,i) \u2286 Ki (wV ) in our\nconstruction.\nNow consider the relationship between SDP and C10. Assume that C10 is included as an\naxiom. We need to show that if wU \u2208 Ki (wV ), then Pi (wU ) = Pi (wV ). It is enough to show\nthat \u03c6 \u2192i \u03c8 \u2208 V if and only if \u03c6 \u2192i \u03c8 \u2208 U, since these statements determine Pi in our\nconstruction. Assume \u03c6 \u2192i \u03c8 \u2208 V . Then, according to C10, Ki (\u03c6 \u2192i \u03c8) \u2208 V , and thus\n\u03c6 \u2192i \u03c8 \u2208 V /Ki . Recall that wU \u2208 Ki (wV ) only if V /Ki \u2286 U. We conclude that \u03c6 \u2192i \u03c8 \u2208 U.\nThe other direction follows from the fact that Ki is symmetric in our construction, and thus\nwV \u2208 Ki (wU ).\nThe desired relationship between RANK, NORM, REF, and UNIF and the axioms C5,\nC6, C7, and C8 is proved in [Fri97,FH97b], for a logic that does not mention knowledge.\nSince these conditions put restrictions on Pi (w) and do not involve knowledge, the proof of\n[Fri97,FH97b] goes through unchanged; we do not repeat it here. \u2737\nTheorem 13: Let A be a subset of {CONS, NORM, REF, SDP, UNIF, RANK}. The formula\n\u03c6 is satisfiable in a Kripke structure satisfying A if and only if it is satisfiable in a Kripke\nstructure with at most 2|Sub(\u03c6)| worlds.\n47\n\n\fPROOF. The proof of this theorem relies on techniques from [FH96a]. We sketch only the\nmain steps here. The proof is based on a standard filtration argument.\nSuppose there is a structure M and a world w in M such that (M, w) |= \u03c6. Let Sub+ (\u03c6) =\nSub(\u03c6) \u222a {\u00ac\u03c6 : \u03c6 \u2208 Sub(\u03c6)}. We say that V \u2286 Sub+ (\u03c6) is an atom if for each \u03c6 \u2208 Sub(\u03c6),\neither \u03c6 \u2208 V or \u00ac\u03c6 \u2208 V . We say that a world w in M satisfies an atom V if for all \u03c6 \u2208 V ,\nwe have (M, w) |= \u03c6. It is easy to see that each world satisfies exactly one atom. Given\na world w \u2032 , we define [w] to be the equivalence class containing all worlds that satisfy the\nsame atom as w. For each equivalence class [w], we arbitrarily choose a representative world\nw[w] \u2208 [w]. We define M \u2032 = (W \u2032 , \u03c0 \u2032 , K1\u2032 , . . . Kn\u2032 , P1\u2032 , . . . , Pn\u2032 ), where W \u2032 = {[w] : w \u2208 W },\n\u2032\n\u03c0 \u2032 ([w]) = \u03c0(w[w]), Ki\u2032 = {([w], [w \u2032]) : (w, w \u2032) \u2208 Ki }, and Pi\u2032 ([w]) = (W([w],i)\n, Pl\u2032([w],i)), where\n\u2032\nW([w],i)\n= {[w \u2032 ] : w \u2032 \u2208 W(w[w] ,i) } and Pl\u2032([w],i)(A) \u2264 Pl\u2032([w],i) (B) if Pl(w[w] ,i) (A\u2217 \u2229 W(w[w] ,i) ) \u2264\nPl(w[w] ,i) (B \u2217 \u2229 W(w[w] ,i) ), where A\u2217 = {w \u2032\u2032 : \u2203[w \u2032 ] \u2208 A, w \u2032\u2032 \u2208 [w \u2032 ]}. Arguments essentially\nidentical to those of [FH96a] show that (M \u2032 , [w]) |= \u03c8 if and only if (M, w) |= \u03c8 for all\n\u03c8 \u2208 Sub(\u03c6); we omit details here.\nWe now have to describe how to modify this argument to ensure that M \u2032 satisfies A. The\nmodifications for NORM, REF, UNIF and RANK are described in [FH96a]. Suppose that M\n\u2032\nsatisfies CONS. Let [w \u2032 ] \u2208 W([w],i)\n. By definition, w \u2032 \u2208 W(w[w] ,i) . But since M satisfies CONS,\nwe have that w \u2032 \u2208 Ki (w[w]). By definition, we get that [w \u2032 ] \u2208 Ki\u2032 ([w]). We conclude that M \u2032\nsatisfies CONS. Finally, suppose that M satisfies SDP. We force M \u2032 to satisfy SDP as follows.\nFor all worlds w, we choose a representative world wKi ([w]) \u2208 Ki\u2032 ([w]) such that if (w, w \u2032) \u2208 Ki ,\nthen wKi ([w]) = wKi ([w\u2032 ]) . We then modify the construction so that, for each world v \u2208 Ki (w),\nwe have Pi\u2032 ([v]) = Pi\u2032 (wKi ([w])). It is easy to see that for all \u03c8 \u2192i \u03c7 \u2208 Sub(\u03c6), we have that\n(M, w) |= \u03c8 \u2192i \u03c7 if and only if (M, wKi ([w])) |= \u03c8 \u2192i \u03c7. Thus, it is easy to show that\nafter this modification we still have that (M \u2032 , [w]) |= \u03c8 if and only if (M, w) |= \u03c8 for all\n\u03c8 \u2208 Sub(\u03c6). \u2737\nTheorem 15: Let A be a subset of {CONS, NORM, REF, SDP, UNIF, RANK} containing\nCONS and either SDP or UNIF. If \u03c6 talks about the knowledge and plausibility of only one\nagent, then \u03c6 is satisfiable in a Kripke structure satisfying A if and only if it is satisfiable\nin a preferential Kripke structure satisfying A with at most |Sub(\u03c6)|3 worlds.\n\nPROOF. Assume M = (W, \u03c0, K1 , P1 ) is a structure satisfying \u03c6. Since CONS is in A, we\nmust have that W(w,1) \u2286 K1 (w)). Without loss of generality, we can assume that K1 consists\nof one equivalence class, that is, that K1 = W \u00d7 W . Since CONS and SDP imply UNIF, and\nsince A contains CONS and either SDP or UNIF, we conclude that M satisfies UNIF. Using\ntechniques from [FH96a] we can assume, without loss of generality, that for each world w,\nthe plausibility space P1 (w) is preferential (i.e., induced by some preference ordering) and\nthat W(w,1) has at most |Sub(\u03c6)|2 worlds.\nChoose w0 \u2208 W such that (M, w0 ) |= \u03c6. For each formula \u00acK1 \u03c8 \u2208 Sub(\u03c6) such that\n(M, w0 ) |= \u00acK1 \u03c8, we select a world w\u03c8 such that (M, w\u03c8 ) |= \u00ac\u03c8. Let T be {w0 } \u222a\n{w\u03c8 : \u00acK1 \u03c8 \u2208 Sub(\u03c6)}. Note that the cardinality of T is at most |Sub(\u03c6)|. Define M \u2032 =\n48\n\n\f\u2032\n(W \u2032 , \u03c0 \u2032 , K1\u2032 , P1\u2032 ) by taking W \u2032 to be the union of W(w,1)\nfor each w \u2208 T , taking \u03c0 \u2032 to be \u03c0\nrestricted to W \u2032 , and taking P1\u2032 (w) = P1 (w). Clearly |W \u2032 | is at most |Sub(\u03c6)|3 . A straightforward argument for all subformulas \u03c8 of \u03c6 and all worlds w \u2032 \u2208 W \u2032 , we have (M, w \u2032 ) |= \u03c8 if and\nonly if (M \u2032 , w \u2032 ) |= \u03c8. It follows that (M \u2032 , w0) |= \u03c6, so \u03c6 is satisfiable in a small preferential\nstructure. \u2737\n\nTheorem 16: Let A be a subset of {CONS, NORM, REF, SDP, UNIF, RANK}. If CONS \u2208\nA, but it is not the case that UNIF or SDP is in A, then the validity problem with respect\nto structures satisfying A is complete for exponential time. Otherwise, the validity problem\nis complete for polynomial space.\n\nPROOF. The proof combines ideas from [FH94a,FH96a,HM92]. We briefly sketch the main\nideas here, referring the reader to the other papers for details.\nThe polynomial space lower bound follows from the polynomial space lower bound for logics\nof knowledge alone [HM92]. For the exponential lower bound we use exactly the lower bound\ndescribed Fagin and Halpern [FH94a] for the combination of knowledge and probability\n(which is in turn based on the lower bound for PDL [FL79]). This lower bound construction\nuses only formulas involving Ki and probabilistic statements of the form wi (\u03c6) = 1 (i.e., the\nprobability of \u03c6 is 1). Since Ni \u03c6 has exactly the same properties as wi (\u03c6) = 1, the same\nconstruction applies to our logic.\nIn the cases where we claim a polynomial space upper bound, this is shown by proving\nthat if a formula \u03c6 is satisfiable at all, it is satisfiable in a structure that looks like a tree,\nwith polynomial branching and depth no greater than the depth of nesting of Ki and \u2192i\noperators in \u03c6. The result now follows along similar lines to corresponding results for logics\nof knowledge.\nFinally, the exponential time upper bound follows by showing that if a formulas is satisfiable\nat all, it is satisfiable in an exponential size structure that can be constructed in deterministic\nexponential time; the technique is similar to that used to show that logics of knowledge with\ncommon knowledge are decidable in deterministic exponential time [HM92] or that PDL is\ndecidable in deterministic exponential time [Pra79]. \u2737\nTheorem 17: Let A be a subset of {CONS, NORM, REF, SDP, UNIF, RANK} containing\nCONS and either UNIF or SDP. For the case of one agent, the validity problem in structures\nsatisfying A is co-NP-complete.\n\nPROOF. We show that the satisfiability problem is NP-complete. It follows that the validity\nproblem is co-NP-complete. The lower bound is immediate, since clearly the logic is at least\nas hard as propositional logic. For the upper bound, by Theorem 15, \u03c6 is satisfiable in a\nstructure satisfying A if and only if \u03c6 is satisfiable in a structure M of size polynomial in\n49\n\n\f|\u03c6|. We simply guess a structure M and check that \u03c6 is satisfiable. It is easy to show that\nmodel checking can be done in polynomial time (see [HM92,FH96a]). \u2737\nA.3\n\nProofs for Section 3.3\n\nTheorem 20: The axiom system AXT is a sound and complete axiomatization of LKCT\nwith respect to C.\nPROOF. As usual, we focus on completeness. Again, we construct a canonical interpreted\nsystem I such that if \u03c6 \u2208 LKCT is consistent, then \u03c6 is satisfied in I. The outline of the\nproof is similar to that of Theorem 11.\nWe proceed as follows. Let V be a maximal AXT -consistent set of formulas in LKCT . We\ndefine V / = {\u03c6 : \u03c6 \u2208 V }. We claim that V / is also a maximal AXT -consistent set.\nTo show that V / is maximal, assume that \u03c6 6\u2208 V / . Then \u03c6 6\u2208 V . From axiom T2,\nwe have that \u00ac\u03c6 \u2208 V , and thus, \u00ac\u03c6 \u2208 V / . This shows that V / is maximal. To show\nthat V / is AXT -consistent, assume that there are formulas \u03c61 , . . . \u03c6n \u2208 V / such that\n\u22a2AXT \u00ac(\u03c61 \u2227 . . . \u2227 \u03c6n ). From K1, T1 and RT1 we get that false \u2208 V / . Thus, false \u2208 V .\nUsing T2 we get that \u00ac true \u2208 V . Using RT1, however, we get that true \u2208 V , which\ncontradicts the assumption that V is consistent. Thus, V / is AXT -consistent. Finally, we\ndefine V / m to the result of m applications of / . Repeated applications of the above\nargument show that V / m is a maximal AXT -consistent set for all m \u2265 0.\nWe construct a canonical interpreted system as follows. Let I = (R, \u03c0, P1 , . . . , Pn ), where\n\u2022 R = {r V : V \u2286 LKCT is a maximal AXT -consistent set} such that\n* reV (m) = V / m , and\n* riV (m) = (V / m )/Ki ,\n\u2022 \u03c0(r V , m)(p) = true if and only if p \u2208 reV (m), and\n\u2022 Pi (r V , m) = (W(rV ,m,i) , Pl(rV ,m,i) ), where\n* W(rV ,m,i) = {(r U , n) : (V / m )/Ni \u2286 U/ n }, and\n* Pl(rV ,m,i) is such that Pl(rV ,m,i) ([\u03c6](rV ,m,i) ) \u2264 Pl(rV ,m,i) ([\u03c8](rV ,m,i) ) if and only if (\u03c6 \u2228\u03c8) \u2192i\n\u03c8 \u2208 V / m , where [\u03c6](rV ,m,i) = {(r U , k) \u2208 W(rV ,m,i) : \u03c6 \u2208 U/ k }.\nUsing the arguments in the completeness proof for conditional logic of [Fri97,FH97b], we\ncan show that Pi (r, m) is well-defined for all i. Finally, we have to show that (I, r V , m) |= \u03c6\nif and only if \u03c6 \u2208 reV (m). As usual, this is done by induction on the structure of \u03c6. This\nis identical to the proof in of Theorem 11 except for the\nmodality, which is handled by\nstandard arguments. We omit the details here. \u2737\nTheorem 21: Let A be a subset of {RANK, NORM, REF, UNIF, CONS, SDP} and let A\nbe the corresponding subset of {C5, C6, C7, C8, C9, C10}. Then AXT \u222a A is a sound and\n50\n\n\fcomplete axiomatization with respect to systems in C satisfying A.\n\nPROOF. Again, we focus on completeness. We obtain completeness in each case by modifying the proof of Theorem 20. We construct a canonical system as in that proof, checking\nconsistency with the extended axiom system. The resulting system has the property that\n(I, r V , m) |= \u03c6 if and only if \u03c6 \u2208 V / m . We just need to show that this system satisfies\nthe corresponding semantic restrictions. The desired relationship between these semantic\nproperties and axioms is proved in [Fri97,FH97b] and the proof of Theorem 12. \u2737\nA.4\n\nProofs for Section 4.1\n\nProposition 25: Let I be a synchronous system satisfying perfect recall and PRIOR. If \u03c6\ncharacterizes agent i's knowledge at (r, m + 1) with respect to his knowledge at (r, m), then\n(I, r, m + 1) |= \u03c8 \u2192i \u03be if and only if (I, r, m) |= (\u03c6 \u2227 \u03c8) \u2192i \u03be.\nPROOF. Expanding the definition we get that R([[ (\u03c6\u2227\u03c8)]](r,m) ) = {r \u2032 \u2208 W(r,i) : (r \u2032 , m) \u223ci\n(r, m), (r \u2032 , m + 1) |= \u03c6 \u2227 \u03c8}. Similarly, we get that R([[\u03c8]](r,m+1) ) = {r \u2032 \u2208 W(r,i) : (r \u2032 , m + 1) \u223ci\n(r, m + 1), (r \u2032 , m + 1) |= \u03c8}. However, since \u03c6 characterizes agent i's knowledge at time m + 1\nwith respect to his knowledge at time m, we get that (r \u2032 , m + 1) \u223ci (r, m + 1) if and only if\n(r \u2032 , m) \u223ci (r, m) and (r, m + 1) |= \u03c6. We conclude that R([[ (\u03c6 \u2227 \u03c8)]](r,m) ) = R([[\u03c8]](r,m+1) ).\nThe lemma now follows directly from Proposition 22. \u2737\nLemma 28: Let I be a synchronous static system satisfying PRIOR, RANK, SDP, and\nperfect recall that has finite branching. Then (I, r, m) |= Bi \u03c6 \u2261 Bi Bi \u03c6 for all propositional\nformulas \u03c6.\n\nPROOF. For all points (r, m) in I, note that W(r,m,i) = \u222a{A\u03c8 }, where A\u03c8 is the set of points\n(r \u2032 , m) \u223ci (r, m) such that the agent's new knowledge at time m + 1 is \u03c8. If I has finite\nbranching, this is a finite partition of W(r,m,i) . Additionally, note that if Pl(r,m,i) is a ranking,\nand C1 , . . . , Ck is a finite partition of C, then since Pl(r,m,i) (C) = max1\u2264j\u2264k Pl(r,m,i) (Cj ), there\nmust be some j such that Pl(r,m,i) (Cj ) = Pl(r,m,i) (C). In particular, for all C \u2286 W(r,m,i) , either\nPl(r,m,i) (C) = \u22a4 or Pl(r,m,i) (W(r,m,i) \u2212 C) = \u22a4.\nFor the \"\u21d2\" part, suppose that (I, r, m) |= Bi \u03c6. If Pl(r,m,i) (W(r,m,i) ) = \u22a5, then (I, r, m) |=\nBi Bi \u03c6 vacuously. If Pl(r,m,i) (W(r,m,i) ) 6= \u22a5, then Pl(r,m,i) ([[\u03c6]](r,m,i) ) > Pl(r,m,i) ([[\u00ac\u03c6]](r,m,i) ).\nAssume that \u03c8 is such that Pl(r,m,i) (A\u03c8 ) = \u22a4. It is easy to verify that since Pl(r,m,i) is a ranking, we get that Pl(r,m,i) (A\u03c8 \u2229 [[\u03c6]](r,m,i) ) > Pl(r,m,i) (A\u03c8 \u2229 [[\u00ac\u03c6]](r,m,i) ). Let r \u2032 be a run such that\n(r \u2032 , m) \u2208 A\u03c8 . By SDP, we get that Pl(r,m,i) = Pl(r\u2032 ,m,i) , and thus Pl(r\u2032 ,m,i) (A\u03c8 \u2229 [[\u03c6]](r\u2032 ,m,i) ) >\nPl(r\u2032 ,m,i) (A\u03c8 \u2229 [[\u00ac\u03c6]](r\u2032 ,m,i) ). By definition of A\u03c8 , we have that (r \u2032\u2032 , m + 1) \u223ci (r \u2032 , m + 1) if\nand only if (r \u2032\u2032 , m) \u2208 A\u03c8 . Since I satisfies PRIOR, Pl(r\u2032 ,m+1,i) is the result of conditioning\n51\n\n\fPl(r,m,i) on A\u03c8 . Moreover, since propositions are static, we get that Pl(r\u2032 ,m+1,i) ([[\u03c6]](r\u2032 ,m+1,i) ) >\nPl(r\u2032 ,m+1,i) ([[\u00ac\u03c6]](r\u2032 ,m+1,i) ). Thus, (I, r \u2032 , m) |= Bi \u03c6. We conclude that A\u03c8 \u2286 [[ Bi \u03c6]](r,m,i) ,\nand thus Pl(r,m,i) ([[ Bi \u03c6]](r,m,i) ) = \u22a4. Moreover, since A\u03c8 \u2286 [[ Bi \u03c6]](r,m,i) for all A\u03c8 such that\nPl(r,m,i) (A\u03c8 ) = \u22a4, we get that Pl(r,m,i) ([[\u00ac Bi \u03c6]](r,m,i) ) \u2264 max{Pl(r,m,i) (A\u03c8 ) : Pl(r,m,i) (A\u03c8 ) <\n\u22a4} < \u22a4. We conclude that Pl(r,m,i) ([[ Bi \u03c6]](r,m,i) ) > Pl(r,m,i) ([[\u00ac Bi \u03c6]](r,m,i) ), and thus,\n(I, r, m) |= Bi Bi \u03c6.\nFor the \"\u21d0\" part, suppose that (I, r, m) |= Bi Bi \u03c6. If Pl(r,m,i) (W(r,m,i) ) = \u22a5, then (I, r, m) |=\nBi \u03c6 vacuously. If Pl(r,m,i) (W(r,m,i) )\n6=\n\u22a5, then Pl(r,m,i) ([[ Bi \u03c6]](r,m,i) ) >\nPl(r,m,i) ([[\u00ac Bi \u03c6]](r,m,i) ). Thus, since Pl(r,m,i) is a ranking, Pl(r,m,i) ([[ Bi \u03c6]](r,m,i) ) = \u22a4. Let\n(r \u2032 , m) be some point in A\u03c8 for some \u03c8. By SDP, we have that (I, r \u2032 , m) |= B\u03c6 if and only\nif (I, r \u2032\u2032 , m) |= B\u03c6 for all points (r \u2032\u2032 , m) \u2208 A\u03c8 . Thus, [[ Bi \u03c6]](r,m,i) = A\u03c81 \u222a . . . \u222a A\u03c8k\nfor some \u03c81 , . . . , \u03c8k . Since Pl(r,m,i) ([[ Bi \u03c6]](r,m,i) ) > Pl(r,m,i) ([[\u00ac Bi \u03c6]](r,m,i) ), we get that\nPl(r,m,i) (A\u03c8 ) = \u22a4 only if \u03c8 = \u03c8j for some 1 \u2264 j \u2264 k. Moreover, since A\u03c81 , . . . , A\u03c8k is a finite\npartition of [[ Bi \u03c6]](r,m,i) , there must be at least one 1 \u2264 j \u2264 k such that Pl(r,m,i) (A\u03c8j ) = \u22a4.\nLet \u03c8j be such that Pl(r,m,i) (A\u03c8j ) = \u22a4. Suppose that (r \u2032 , m) \u2208 A\u03c8j . Then we have that\nPl(r\u2032 ,m+1,i) ([[\u03c6]](r\u2032 ,m+1,i) ) > Pl(r\u2032 ,m+1,i) ([[\u00ac\u03c6]](r\u2032 ,m+1,i) ). Since I is synchronous, static, and satisfies perfect recall, PRIOR, and SDP, we get that Pl(r,m,i) (A\u03c8j \u2229 [[\u03c6]](r,m,i) ) > Pl(r,m,i) (A\u03c8j \u2229\n[[\u00ac\u03c6]](r,m,i) ). Since Pl(r,m,i) is a ranking, we get that Pl(r,m,i) (A\u03c8j \u2229 [[\u03c6]](r,m,i) ) = \u22a4, and thus,\nPl(r,m,i) ([[\u03c6]](r,m,i) ) = \u22a4. Finally, if Pl(r,m,i) (A\u03c8 ) < \u22a4, then Pl(r,m,i) (A\u03c8 \u2229[[\u00ac\u03c6]](r,m,i) ) < \u22a4. Thus,\nsince Pl(r,m,i) ([[\u00ac\u03c6]](r,m,i) ) = max\u03c8 Pl(r,m,i) (A\u03c8 \u2229 [[\u00ac\u03c6]](r,m,i) ), we get that Pl(r,m,i) ([[\u00ac\u03c6]](r,m,i) ) <\n\u22a4. We conclude that (I, r, m) |= Bi \u03c6. \u2737\nA.5\n\nProofs for Section 4.2\n\nTheorem 32: Let A be a subset of {QUAL, NORM, REF, RANK} and let I be a coherent\nsynchronous system satisfying perfect recall, CONS, and A. Then there is a synchronous\nsystem I \u2032 satisfying perfect recall, PRIOR, and A, and a mapping f : R 7\u2192 R\u2032 such that for\nall temporally linear formulas \u03c6 \u2208 LKCT , we have (I, r, m) |= \u03c6 if and only if (I \u2032 , f (r), m) |=\n\u03c6.\nPROOF. To construct I \u2032 , we use a general technique for taking a \"sum\" of a sequence\nof plausibility spaces. Let \u03bb be an ordinal and let {Si : 0 \u2264 i < \u03bb} be a sequence of\nplausibility spaces, where Si = (Wi , Pli ) and the Wi 's are pairwise disjoint. Define \u2295i Si as\n(\u222ai Wi , Pl\u2295Si ), where Pl\u2295Si (A) \u2265 Pl\u2295Si (B) if either Pli (A \u2229 Wi ) = Pli (B \u2229 Wi ) = \u22a5 for\nall i, or there exists some i such that Pli (A \u2229 Wi ) \u2265 Pli (B \u2229 Wi ), Pli (A \u2229 Wi ) >\u22a5, and\nPlj (A \u2229 Wj ) = Plj (B \u2229 Wj ) = \u22a5 for all j < i. We can think of \u2295i Si as a lexicographic\ncombination of the Si 's.\nLemma 35 (a) \u2295i Si is a plausibility space,\n(b) if Si is qualitative for all i, then \u2295i Si is qualitative,\n(c) if Si is ranked for all i, then \u2295i Si is ranked,\n52\n\n\f(d) (\u2295i Si )|C is isomorphic to \u2295i (Si |C ) under the identity mapping.\n(e) (\u2295i Si )|Wj is isomorphic to Sj under the identity mapping.\n(f ) If W1 , . . . , Wk = \u2205, then \u2295i Si is isomorphic to \u2295i\u2265k+1 Si .\nPROOF. We have to show that \u2264 is reflexive, transitive, and satisfies A1. It is easy to see\nthat, by definition, \u2264 is reflexive. Next, we consider transitivity. Suppose that Pl\u2295Si (A) \u2265\nPl\u2295Si (B) and Pl\u2295Si (B) \u2265 Pl\u2295Si (C). If Pli (B \u2229Wi ) = \u22a5i for all i, then clearly Pl( C \u2229Wi ) = \u22a5i\nfor all i (since Pl\u2295Si (B) \u2265 Pl\u2295Si (C)), so Pl\u2295Si (A) \u2265 Pl\u2295Si (C). So suppose that Pl(B \u2229 Wi ) >\n\u22a5i for some i. Let i and j be the smallest indexes such that Pli (A \u2229 Wi ) > \u22a5i and Plj (B \u2229\nWj ) > \u22a5j . It is easy to see that i \u2264 j, and that Plk (C \u2229 Wk ) = \u22a5k for all k \u2264 j. If i < j, we\nconclude that Pli (A \u2229 Wi ) \u2265 Pli (C \u2229 Wi ) = \u22a5i , and thus Pl\u2295Si (A) \u2265 Pl\u2295Si (C). On the other\nhand, if i = j, then by definition Pli (A \u2229 Wi ) \u2265 Pli (B \u2229 Wi ), and Pli (B \u2229 Wi ) \u2265 Pli (C \u2229 Wi ).\nSince \u2264 is transitive in Si , we get that Pli (A \u2229 Wi ) \u2265 Pli (C \u2229 Wi ). Thus, we conclude\nthat Pl\u2295Si (A) \u2265 Pl\u2295Si (C), as desired. Finally, we consider A1. Suppose that A \u2286 B. Then\nA \u2229 Wi \u2286 B \u2229 Wi for all i. Since each Si satisfies A1, we have that Pli (A \u2229 Wi ) \u2264 Pli (C \u2229 Wi )\nfor all i. It easily follows that Pl\u2295Si (A) \u2264 Pl\u2295Si (B).\nSuppose that Si is qualitative for all i. We have to show that \u2295i Si is also qualitative. We\nstart by considering A2. Suppose that A, B, and C are pairwise disjoint sets such that\nPl\u2295Si (A \u222a B) > Pl\u2295Si (C) and Pl\u2295Si (A \u222a C) > Pl\u2295Si (B). Let i and j be the minimal indexes\nsuch that Pli ((A \u222a B) \u2229 Wi ) > \u22a5i and Plj ((A \u222a C) \u2229 Wj ) > \u22a5j . We claim that i = j.\nAssume, by way of contradiction, that i < j. Then, Pli ((A \u222a C) \u2229 Wi ) = \u22a5i and hence\nPli (A \u2229 Wi ) = \u22a5i . Moreover, since Pl\u2295Si (A \u222a C) > Pl\u2295Si (B), we get that Pli (B \u2229 Wi ) = \u22a5i .\nUsing A3 in Si , we conclude that Pli ((A \u222a B) \u2229 Wi ) = \u22a5i , which contradicts our assumption\nthat Pli ((A \u222a B) \u2229 Wi ) > \u22a5i . Symmetric arguments show that we also cannot have j < i.\nThus, i = j. By definition Pli ((A\u222aB)\u2229Wi ) > Pli (C \u2229Wi ) and Pli ((A\u222aC)\u2229Wi ) > Pli (B\u2229Wi ).\nUsing A2 we conclude that Pli (A \u2229 Wi ) > Pli ((B \u222a C) \u2229 Wi ). It is also easy to verify, using\nA3, that Plj ((B \u222a C) \u2229 Wj ) = \u22a5j for all j < i. Thus, we get that Pl\u2295Si (A) > Pl\u2295Si (B \u222a C),\nas desired. Next, consider A3. The construction of \u2295Si is such that Pl\u2295Si (A) =\u22a5 if and only\nif Pl1 (A \u2229 Wi ) =\u22a5 for all i. It is easy to see that A3 follows from A3 in each Si .\nFinally, part (c) follows immediately from the definition, part (d) follows immediately from\nCOND, part (e) is a special case of part (d), and part (f) follows immediately from the\ndefinition. \u2737\nReturning to the proof of Theorem 32, first suppose that REF is not in A. Let I =\n(R, \u03c0, P1 , . . . , Pn ) be a coherent synchronous system satisfying perfect recall and CONS.\nRoughly speaking, the proof goes as follows. We construct a system I \u2032 which consists of\ncountably many copies of R. The runs in Rm , the mth copy of R, are used to simulate the\nagent's plausibility assessment at time m. More precisely, for all times m, we define a prior\non Rm that corresponds to the agent's plausibility measure at time m in I. These priors\nare then combined using \u2295 to construct the agent's prior in I \u2032 . Since \u2295 orders the priors\n\u2032\nlexicographically, if m < m\u2032 , the priors on Rm dominate those on Rm . The construction\n53\n\n\fguarantees that at time m, the agent considers possible only runs in Rm \u222a Rm+1 \u222a . . .. Since\nthe prior on Rm dominates the rest, the agent's plausibility measure at time m is similar\nto that at time m in I. This similarity is what guarantees that conditional formulas are\nevaluated in the same way in I and I \u2032 . This \"peeling away\" of copies of R ensures that all\ntemporally linear formulas holding in runs in I are also satisfied in the corresponding runs\nin I \u2032 .\nThe formal construction proceeds as follows. Let R \u2286 R and l \u2208 IN \u2217 (recall that IN \u2217 =\nIN \u222a {\u221e}). Define Rl = {r l : r \u2208 R}, where, for each i \u2208 {e, 1 . . . , n}, we have\n\uf8f1\n\uf8f4\n\uf8f2 hri (m), mi\n\nril (m) = \uf8f4\n\n\uf8f3 hr\n\ni (l), mi\n\nif l \u2265 m\nif l < m.\n\nLet I \u2032 = (R\u2032 , \u03c0 \u2032 , P1\u2032 , . . . , Pn\u2032 ), where R\u2032 = \u222al\u2208IN \u2217 Rl , \u03c0 \u2032 is defined so that if m \u2264 l then\n\u03c0 \u2032 (r l , m) = \u03c0(r, m) and if m > l then \u03c0(r l , m) = \u03c0(r, l), and Pi\u2032 is defined by the priors\ndescribed below.\nm\nTo define a prior on R\u2032 , we first define a plausibility space P(r,i)\non Rm for each m \u2208 IN , run\nm\nr \u2208 R, and agent i. We want the time m projection of P(r,i) to be isomorphic to Pi (r, m).\nm\nm\nm\nm\nTo achieve this, we define P(r,i)\n= (Rm\nand Plm\n(r,i) is\n(r,i) , Pl(r,i) ), where R(r,i) = R(W(r,m,i) )\nm\nm\ndefined so that for A \u2286 W(r,m,i) , we have Pl(r,i) ((R(A)) ) = Pl(r,m,i) (A). For l \u2208 IN \u2217 , we\ndefine the prior of agent i at run r l to be the combination of these priors for all time points:\n\u2032\nm\nP(r\nl ,i) = \u2295m P(r,i) .\n\nIt is easy to see that I \u2032 is synchronous. It is also easy to check that I \u2032 satisfies perfect recall:\nFrom the definition, we have that\nKi\u2032 (r l , m)\n\n\uf8f1\n\uf8f4\n\uf8f2 {(r \u2032l\u2032 , m)\n\n=\uf8f4\n\n\uf8f3 {(r \u2032l , m)\n\n: (r \u2032 , m) \u2208 Ki (r, m), l\u2032 \u2265 m} if l \u2265 m\n: (r \u2032 , l) \u2208 Ki (r, l)}\n\nif l < m.\n\nMoreover, since I satisfies perfect recall, we have that R(Ki (r, m + 1)) \u2286 R(Ki (r, m)). We\nconclude that R\u2032 (Ki\u2032 (r l , m + 1)) \u2286 R\u2032 (Ki\u2032 (r l , m)), which is just what we need for perfect\nrecall.\nLet \u03c6 \u2208 LKC (so that \u03c6 does not include any temporal modalities) and l \u2265 m. We show\nthat (I \u2032 , r l , m) |= \u03c6 if and only if (I, r, m) |= \u03c6. As usual we prove this by induction on the\nstructure of \u03c6. The only interesting cases are these that directly involve modalities.\nWe start with the Ki modality. Suppose that (I, r, m) |= Ki \u03c6. Then for all points (s, m) \u2208\nKi (r, m), we have (I, s, m) |= \u03c6. Let (sk , m) \u2208 Ki\u2032 (r l , m). From the definition of I \u2032 we get that\n(s, m) \u2208 Ki (r, m) and k \u2265 m. Using the induction hypothesis, we get that (I \u2032 , sk , m) |= \u03c6.\nWe conclude that (I \u2032 , r l , m) |= Ki \u03c6. Now suppose that (I, r, m) 6|= Ki \u03c6. Then there is a point\n(s, m) \u2208 Ki (r, m) such that (I, s, m) |= \u00ac\u03c6. Using the induction hypothesis we conclude that\n(I, sm , m) |= \u00ac\u03c6. Since (sm , m) \u2208 Ki\u2032 (r l , m), we conclude that (I \u2032 , r l , m) 6|= Ki \u03c6.\n54\n\n\fWe now turn to the \u2192i modality. The definition of PRIOR implies that Pi\u2032 (r l , m) is the\nm\n\u2032\n\u2032\n\u2032 l\n\u2032\nprojection of P(r\nl ,i) conditioned on R (Ki (r , m)). Now P(r l ,i) = \u2295m Pl(r,i) . Parts (d) and\n\u2032\nk\n(f) of Lemma 35 imply that P(r\nl ,i) |R\u2032 (K\u2032 (r l ,m)) is isomorphic to \u2295k\u2265m (P(r,i) )|R\u2032 (K\u2032 (r l ,m)) ). Coni\ni\nm\nsider the first term in the \"sum\", P(r,i)\n|R\u2032 (K\u2032i (rl ,m)) . Since I satisfies CONS, we have that\nW(r,m,i) \u2286 Ki (r, m). Thus, conditioning on R\u2032 (Ki\u2032 (r l , m)) does not remove any runs from\nm\nm\nm\nRm\n(r,i) = (R(W(r,m,i) ) . It follows that P(r,i) |R\u2032 (K\u2032i (r l ,m)) = P(r,i) which is isomorphic to Pi (r, m)\nm\nunder the mapping r \u2032m 7\u2192 (r \u2032 , m). Finally, since P(r,i)\nis the first plausibility space in the\n\"sum\", it determines the ordering of all pairs of sets, unless both of them are assigned plau\u2032\n\u2032\n\u2032\nsibility \u22a5 by Plm\n(r,i) . Putting all this together, we conclude that if A , B \u2286 W(r l ,m,i) and\nA, B \u2286 W(r,m,i) such that (R(A))m = R\u2032 (A\u2032 ) \u2229 Rm and (R(B))m = R\u2032 (B \u2032 ) \u2229 Rm , and if\nPl(r,m,i) (A) > \u22a5, then Pl\u2032(rl ,m,i) (A\u2032 ) \u2265 Pl\u2032(rl ,m,i) (B \u2032 ) if and only if Pl(r,m,i) (A) \u2265 Pl(r,m,i) (B).\nAssume that (I, r, m) |= \u03c6 \u2192i \u03c8. Thus, either Pl(r,m,i) ([[\u03c6]](r,m,i) ) = \u22a5 or Pl(r,m,i) ([[\u03c6 \u2227\n\u03c8]](r,m,i) ) > Pl(r,m,i) ([[\u03c6 \u2227 \u00ac\u03c8]](r,m,i) ). If Pl(r,m,i) ([[\u03c6]](r,m,i) ) = \u22a5, then from the coherence of I it\nfollows that if A \u2286 W(r,l\u2032 ,i) and R(A) \u2286 R([[\u03c6]](r,m,i) ), then Pl(r,l\u2032 ,i) (A) = \u22a5. This implies that\n\u2032\n\u2032\n\u2032\nPll(r,i) ((R([[\u03c6]](r,m,i) )l \u2229 R(W(r,l\u2032 ,i) )l ) = \u22a5 for all l\u2032 \u2265 m. Since Ki\u2032 (r l , m) contains only points\n\u2032\nfrom Rl for l\u2032 \u2265 m, we get that Pl\u2032(rl ,m,i) ([[\u03c6]](rl ,m,i) ) = \u22a5. Thus, we conclude that (I \u2032 , r l , m) |=\n\u03c6 \u2192i \u03c8 in this case. Now suppose that Pl(r,m,i) ([[\u03c6 \u2227 \u03c8]](r,m,i) ) > Pl(r,m,i) ([[\u03c6 \u2227 \u00ac\u03c8]](r,m,i) ). If\nwe could show that (R([[\u03c6]](r,m,i) ))m = R\u2032 ([[\u03c6]](rl ,m,i) ) \u2229 Rm , and similarly for \u03c8, then we\ncould apply the argument of the previous paragraph to show that Pl\u2032(rl ,m,i) ([[\u03c6 \u2227 \u03c8]](rl ,m,i) ) >\nPl\u2032(rl ,m,i) ([[\u03c6\u2227\u00ac\u03c8]](rl ,m,i) ). This, in turn, would allow us to conclude that (I \u2032 , r l , m) |= \u03c6 \u2192i \u03c8.\nThe fact that (R([[\u03c6]](r,m,i) ))m = R\u2032 ([[\u03c6]](rl ,m,i) ) \u2229 Rm follows from the following chain of\nequivalences:\nsm \u2208 (R([[\u03c6]](r,m,i) ))m\niff (s, m) \u2208 [[\u03c6]](r,m,i)\niff (s, m) \u2208 W(r,m,i) and (I, s, m) |= \u03c6\n\u2032 m\niff sm \u2208 (R(W(r,m,i) ))m = Rm\n(r,i) and (by the induction hypothesis) (I , s , m) |= \u03c6\n\niff (sm , m) \u2208 W(rl ,m,i) and (I \u2032 , sm , m) |= \u03c6\niff (sm , m) \u2208 [[\u03c6]](rl ,m,i)\niff sm \u2208 R([[\u03c6]](rl ,m,i) ) \u2229 Rm .\nThus, in either case, we conclude that (I \u2032 , r l , m) |= \u03c6 \u2192i \u03c8, as desired.\nFor the converse, suppose that (I, r, m) 6|= \u03c6 \u2192i \u03c8. Then Pl(r,m,i) ([[\u03c6]](r,m,i) ) > \u22a5 and\nPl(r,m,i) ([[\u03c6 \u2227 \u03c8]](r,m,i) ) 6> Pl(r,m,i) ([[\u03c6 \u2227 \u00ac\u03c8]](r,m,i) ). By the same arguments as above, we get\nthat Pl\u2032(rl ,m,i) ([[\u03c6 \u2227 \u03c8]](rl ,m,i) ) > \u22a5 and Pl\u2032(rl ,m,i) ([[\u03c6 \u2227 \u03c8]](rl ,m,i) ) 6> Pl\u2032(rl ,m,i) ([[\u03c6 \u2227 \u00ac\u03c8]](rl ,m,i) ).\nThus, (I \u2032 , r l , m) 6|= \u03c6 \u2192i \u03c8, as desired.\nFinally, for r \u2208 R, define f (r) = r \u221e . We have proved that if \u03c6 \u2208 LKC , then (I, r, m) |= \u03c6\nif and only if (I \u2032 , f (r), m) |= \u03c6. Since this holds for all m, a straightforward argument\n55\n\n\fby induction on structure shows that this holds, not just for formulas in LKC , but for all\ntemporally linear formulas.\nWe now have to ensure that I \u2032 satisfies A. Suppose that I satisfies QUAL. Thus, Pi (r, m)\nis qualitative for all agents i, runs r \u2208 R, and times m. Using part (b) of Lemma 35, we\n\u2032\nconclude that the prior P(r,i)\nis qualitative for all agents i and runs r \u2208 R. This implies,\n\u2032\nusing Proposition 29, that I satisfies QUAL. Similarly, if I satisfies RANK, using part (c)\nof Lemma 35 and Proposition 29, we get that I \u2032 satisfies RANK.\nSuppose that I satisfies NORM. Then Pl(r,m,i) ([[true]] > \u22a5 for all agents i, runs r \u2208 R, and\ntimes m. This implies that \u00ac(true \u2192i false) is valid in I. Suppose that l \u2265 m. Then since\n\u00ac(true \u2192i false) \u2208 LKC , we conclude from the proof above that (I \u2032 , r l , m) |= \u00ac(true \u2192i\nfalse). Thus, Pl\u2032(rl ,m,i) ([[true]](rl ,m,i) ) > \u22a5. Suppose that l < m. By definition, we have that\n\u2032\nR\u2032 (Ki\u2032 (r l , m)) = (R(Ki (r, l)))l . Using part (e) of Lemma 35, we get that P(r,i)\n|R\u2032 (K\u2032i (rl ,m)) is\nl\nisomorphic to P(r,i) . However, the latter plausibility space is isomorphic to Pi (r, l). Thus, it\nsatisfies \u22a4 > \u22a5. We conclude that I \u2032 satisfies NORM, as desired.\nUp to now we have assumed that REF is not in A. If REF is in A, then REF does not hold for\nA, although it does hold at many points. To understand the issue, suppose that REF holds\nin I. Since I \u2032 satisfies PRIOR, to show that REF holds in I \u2032 , according to Proposition 29\nit suffices to show that all priors satisfy REF. This is indeed the case if l 6= \u221e. For suppose\n\u2032\nm\nthat r l \u2208 A \u2286 R\u2032 . We want to show that Pl(rl ,i) (A) > \u22a5. Recall that P(r\nl ,i) = \u2295m P(r,i) . From\nthe definition of \u2295, it easily follows that if Pll(r,i) (A \u2229 Rl ) > \u22a5, then Pl\u2032(rl ,i) (A) > \u22a5. By\ndefinition, we have that Pl\u2032(rl ,i) (A \u2229 Rl ) = Pl(r,l,i) (A\u2032 ), where A\u2032 = {(s, l) : sl \u2208 A}. Clearly\n(r, l) \u2208 A\u2032 , since (r l , m) \u2208 A. Since I satisfies REF, we must have that Pl(r,l,i) (A\u2032 ) > \u22a5. It\nfollows that Pl\u2032(rl ,i) satisfies REF if l 6= \u221e. This argument breaks down if l = \u221e. Indeed, it\n\u2032\n\u221e\nis clear that P(r\nis disjoint from Rm for m < \u221e, and we\n\u221e ,i) does not satisfy REF. Since R\nm\n\u2032\nonly \"sum\" P(r,i) for m < \u221e to obtain P(r\u221e ,i) , it follows that R\u221e is disjoint from W(r\u2032 \u221e ,i) , so\nREF does not hold.\nFortunately, a slight modification of the construction of I \u2032 can be used to deal with the case\n\u221e\n\u221e\n\u221e\n\u221e\n\u221e\n\u221e\nREF \u2208 A. Define P(r,i)\n= (R(r,i)\n, Pl\u221e\n(r,i) ), where R(r,i) = {r } and Pl(r,i) ({r }) > \u22a5. Modify\n\u2032\u2032\n\u2032\n\u221e\nthe construction of I \u2032 so that the prior of agent i in run r l is P(r\nl ,i) = P(r l ,i) \u2295 P(r l ,i) . (Thus,\n\u2032\u2032\nm\n\u2032\nP(r\nl ,i) = \u2295m\u2264\u221e P(r l ,i) .) It is easy to check that I now does satisfy REF. The argument in the\ncase that l 6= \u221e remains unchanged. On the other hand, if r \u221e \u2208 A \u2286 R\u2032 , it is immediate\nthat P \u221e (A \u2229 R\u221e ) > \u22a5, so we can now deal with this case as well. If QUAL, RANK, or\nNORM is in A, it is easy to see (using the same argument as above) that I \u2032 also satisfies\nQUAL, RANK, or NORM.\nIt remains to show that this modification of the prior does not affect the evaluation of\nformulas. That is, we must show that (I, r, m) |= \u03c6 if and only if (I \u2032 , r l , m) |= \u03c6 for all\nl \u2265 m. Again, we proceed by induction on the structure of formulas. The argument for\nformulas of the form Ki \u03c6 goes through unchanged, since the changes to Pl\u2032 did not affect\nthe Ki relations. The argument for formulas of the form \u03c6 \u2192i \u03c8 goes through with almost\n56\n\n\fno change. The only case that requires attention is if (I, r, m) |= \u03c6 \u2192i \u03c8 and [[\u03c6]](r,m,i) = \u22a5.\n\u2032\n\u2032\n\u2032\nOur earlier arguments showed that Pll(r,i) ((R([[\u03c6]](r,m,i) )l \u2229 R(W(r,l\u2032 ,i) ))l ) = \u22a5 for all l\u2032 \u2265 m,\nl\u2032 6= \u221e. These arguments go through without change. We must now show that this also holds\n\u221e\nif l\u2032 = \u221e. But, from the definition of Pl\u221e , we get that Pl\u221e\n\u2229 R\u221e ) = \u22a5\n(r,i) ((R([[\u03c6]](r,m,i) )\nunless r \u221e \u2208 R([[\u03c6]](r,m,i) )\u221e . This implies that (r, m) \u2208 [[\u03c6]](r,m,i) . But this cannot happen,\nsince Pl(r,m,i) ([[\u03c6]](r,m,i) ) = \u22a5 and I satisfies REF. \u2737\n\nTheorem 34: Let A be a subset of {QUAL, NORM, REF, SDP, UNIF, RANK} and let I\nbe a coherent synchronous system satisfying perfect recall, CONS, PERSIST, and A. Then\nthere is a synchronous system I \u2032 satisfying perfect recall, PRIOR, and A, and a mapping\nf : R 7\u2192 R\u2032 such that for all temporally linear formulas \u03c6 \u2208 LKCT , (I, r, m) |= \u03c6 if and only\nif (I \u2032 , f (r), m) |= \u03c6.\n\nPROOF. Suppose that I = (R, \u03c0, P1 , . . . , Pn ) is a coherent synchronous system satisfying perfect recall, CONS, PERSIST, and A. If neither CONS nor UNIF are in A, then\nTheorem 32 guarantees that there is a system I \u2032 that satisfies the stated properties.\nSuppose that UNIF \u2208 A, but SDP, REF \u2208\n/ A. (We sketch the modifications required to\ndeal with SDP and REF below.) It does not follow that the system I \u2032 constructed in the\nproof satisfies UNIF. To see why, suppose r, r \u2032 and m > k are such that (r \u2032 , k) \u2208 W(r,k,i) but\n(r, m) 6\u223ci (r \u2032 , m). UNIF implies that Pi (r, k) = Pi (r \u2032 , k) and (since I also satisfies CONS)\n\u2032\n\u2032\nthat W(r,m,i) \u2229W(r\u2032 ,m,i) = \u2205. Hence, our construction guarantees that P(r\nk ,i) 6= P(r \u2032k ,i) , although\nr \u2032k \u2208 W(r\u2032 k ,i) . Thus, the prior in I \u2032 does not satisfy UNIF. It follows that I \u2032 does not satisfy\nUNIF either, for Pi\u2032 (r k , k) 6= Pi\u2032 (r \u2032k , k), although (r \u2032k , k) \u2208 W(r\u2032 k ,k,i) .\nThe solution to this problem is relatively straightforward. We modify our construction so\nthat the prior does indeed satisfy UNIF. In particular, we modify the prior P \u2032 to ensure that\n\u2032\n\u2032\nif Pi (r, k) = Pi (r \u2032 , k), then P(r\nk ,i) = P(r \u2032k ,k) . Of course, we have to do so carefully, so as to\nmake sure that nothing goes wrong with the rest of the argument in Theorem 32.\nWe start with a modification of the construction of \u2295 that takes sets (rather than sequences)\nof plausibility spaces and returns a new plausibility space.\nLemma 36 Let S be a set of plausibility spaces such that the sets {W : (W, Pl) \u2208 S} are\npairwise disjoint. Then there is a plausibility space \u2297S such that\n(a) if S = (W, Pl) \u2208 S, then \u2297S|W is isomorphic to S under the identity mapping,\n(b) if S is qualitative for all S \u2208 S, then \u2297S is qualitative,\n(c) if S is ranked for all S \u2208 S, then \u2297S is ranked.\n\n57\n\n\fPROOF. Without loss of generality there is an ordinal \u03bb and a sequence {Si : 0 \u2264 i < \u03bb}\nsuch that Si \u2208 S for all i, and for all S \u2208 S, exists an i such that S = Si . 17 Define\n\u2297S = \u2295i Si . Part (a) of Lemma 35 guarantees that \u2297S is a plausibility space. Parts (a), (b),\nand (c) follow immediately from parts (e), (b), and (c) of Lemma 35, respectively. \u2737\nRecall that to satisfy UNIF and PRIOR, it suffices to find a partition of R such that all\nthe runs in each cell have the same prior. We now examine a possible way of partitioning\nthe runs in the system. Let r \u2208 R. Define [r, m]i = {(r \u2032 , m) : (r \u2032 , m) \u223ci (r, m), Pi (r \u2032, m) =\nPi (r, m)}. Thus, [r, m]i is the set of points in which agent i has the same knowledge state and\nplausibility assessment as at (r, m). (Note that if W(r,m,i) 6= \u2205, then since I satisfies CONS,\nPi (r \u2032 , m) = Pi (r, m) implies that (r \u2032 , m) \u223ci (r, m).)\nLemma 37(a) For all times m, the collection {R([r, m]i ) : r \u2208 R} is a partition of R.\n(b) For all times m and runs r, W(r,m,i) \u2286 [r, m]i .\n(c) For all times m and runs r, R([r, m + 1]i ) \u2286 R([r, m]i ).\n(d) For all times m and runs r, r \u2032 such that (r \u2032 , 0) \u2208 [r, 0]i , if (r \u2032 , m) \u223ci (r, m), then (r \u2032, m) \u2208\n[r, m]i .\nPROOF. By definition, if (r \u2032 , m) \u2208 [r, m]i , then [r \u2032 , m] = [r, m]i . Thus, if [r, m]i 6= [r \u2032 , m]i ,\nthen [r, m]i \u2229 [r \u2032 , m]i = \u2205. Part (a) follows immediately. For part (b), suppose that (r \u2032 , m) \u2208\nW(r,m,i) . Since I satisfies CONS, we have that (r \u2032 , m) \u223ci (r, m). Moreover, since I satisfies\nUNIF, we have that Pi (r \u2032 , m) = Pi (r, m). Thus, (r \u2032, m) \u2208 [r, m]i . We conclude that W(r,m,i) \u2286\n[r, m]i , as desired. For part (c), suppose that (r \u2032 , m + 1) \u2208 [r, m + 1]i . This implies that\n(r \u2032 , m+1) \u223ci (r, m+1) and Pi (r \u2032 , m+1) = Pi (r, m+1). Since I satisfies perfect recall, we get\nthat (r \u2032 , m) \u223ci (r, m). Moreover, since I satisfies PERSIST, we get that Pi (r \u2032 , m) = Pi (r, m).\nWe conclude that (r \u2032 , m) \u2208 [r, m]i . Thus, R([r, m + 1]i ) \u2286 R([r, m]i ), as desired. Finally, we\nprove part (d) by induction on m. When m = 0, part (d) obviously holds. Suppose that\nm > 0, (r \u2032 , 0) \u2208 [r, 0]i , and (r \u2032 , m) \u223ci (r, m). Since I satisfies perfect recall, we have that\n(r \u2032 , m \u2212 1) \u223ci (r, m \u2212 1). Using the induction hypothesis, we get that (r \u2032 , m \u2212 1) \u2208 [r, m \u2212 1].\nThis implies that Pi (r \u2032 , m \u2212 1) = Pi (r, m \u2212 1). Using PERSIST, we conclude that Pi (r \u2032, m) =\nPi (r, m). Thus, (r \u2032 , m) \u2208 [r, m]i , as desired. \u2737\nUsing both \u2295 and \u2297, we now construct a prior over R\u2032 that satisfies UNIF. For r \u2208 R, let\nm\nm\nm\n\u2032\nm\nm\n[r]i abbreviate R([r, 0]i ). Define P[r]\n= \u2297{P(r\n\u2032 ,i) : r \u2208 [r]i }, where P(r,i) = (R(r,i) , Pl(r,i) )\ni\nis the prior defined in the proof of Theorem 32 that is isomorphic to Pi (r, m) under the\nm\nmapping r \u2032m 7\u2192 (r \u2032 , m). We must show that P(r,i)\nis well defined; that is, we must show that\nm\nm\nm\n\u2032\n\u2032\u2032\nif P(r\u2032 ,i) 6= P(r\u2032\u2032 ,i) , then R(r\u2032 ,i) is disjoint from Rm\n(r \u2032\u2032 ,i) . Note that if (r , m) \u2208 [r , m]i , then\nm\nm\n\u2032\n\u2032\u2032\nP(r\n\u2032 ,i) and P(r \u2032\u2032 ,i) are identical. Using part (b) of Lemma 37 we get that if (r , m) 6\u2208 [r , m]i ,\n17\n\nIf S is uncountable, this construction may require the axiom of choice. There is a variant of the\nconstruction that does not require the axiom of choice, but the additional complexities involved do\nnot seem worth the trouble.\n\n58\n\n\fm\nm\n\u2032\nthen Rm\n(r \u2032 ,i) \u2229 R(r \u2032\u2032 ,i) = \u2205, as desired. Thus, P[r]i is indeed well defined. We now define P(r l ,i) =\nm\n\u2295m P[r]\nas the prior of agent i in run r l .\ni\n\nWe claim that this family of priors satisfies UNIF. Notice that W(r\u2032 l ,i) = \u222am,r\u2032 \u2208[r]i Rm\n(r \u2032 ,i) . If\n\u2032m\n\u2032\n\u2032\nr \u2208 W(rl ,i) then, by definition, r \u2208 W(r,m,i) . Using parts (a) and (b) of Lemma 37, we get\nthat r \u2032 \u2208 [r]i . It easily follows that [r \u2032 ]i = [r]i , so indeed the construction guarantees that\n\u2032\n\u2032\n\u2032\nP(r\nl ,i) = P(r \u2032m ,i) , as desired. Since the family of priors satisfies UNIF, so does I .\nLet \u03c6 \u2208 LKC and l \u2265 m. As in the proof of Theorem 32, we now proceed by induction on\nthe structure of formulas to show that (I \u2032 , r l , m) |= \u03c6 if and only if (I, r, m) |= \u03c6. The only\ndifference arises in dealing with the \u2192i modality.\n\u2032\nAs before, parts (d) and (f) of Lemma 35 imply that P(r\nl ,i) |R\u2032 (K\u2032 (r l ,m)) is isomorphic to\ni\nm\nk\n\u2295k\u2265m (P[r]i |R\u2032 (K\u2032i (rl ,m)) ). Again, we consider the first term in the \"sum\", P[r]\n| \u2032 \u2032 l . We\ni R (Ki (r ,m))\nm\nm\nm\nwant to show that P[r]i |R\u2032 (K\u2032i (rl ,m)) = P(r,i) |R\u2032 (K\u2032i (rl ,m)) . Recall that P(r,i) |R\u2032 (K\u2032i (rl ,m)) is the first\nterm in the analogous \"sum\" in the proof of Theorem 32. Thus, even though we are using\na different prior from that of the proof of Theorem 32, after conditioning, they are essenm\nm\ntially the same. By Lemma 36, we have that P[r]\n| m = P(r,i)\n. Thus, it suffices to show\ni R(r,i)\nm\nl\nm\n\u2032\n\u2032\nl\n\u2032\n\u2032\nthat \u222ar\u2032 \u2208[r]i R(r\u2032 ,i) \u2229 R (Ki (r , m)) = R(r,i) \u2229 R (Ki (r , m). The inclusion from right to left is\n\u2032\n\u2032 l\nimmediate. For the opposite inclusion, suppose that sm \u2208 \u222ar\u2032 \u2208[r]i Rm\n(r \u2032 ,i) \u2229 R (Ki (r , m)). Since\nsm \u2208 R\u2032 (Ki\u2032 (r l , m)), we must have (r, m) \u223ci (s, m). Since sm \u2208 \u222ar\u2032 \u2208[r]i Rm\n(r \u2032 ,i) , there must also\n\u2032\nm\nm \u2032\nbe some run r \u2208 [r]i such that s \u2208 R(r\u2032 ,i) . Since s \u2208 R (r , i), we have that (s, m) \u2208 W(r\u2032 ,m,i) .\nBy part (b) of Lemma 37, (s, m) \u2208 [r \u2032 , m]i . By part (c) of Lemma 37, we get that (s, 0) \u2208\n[r \u2032 , 0]i . Since (r \u2032 , 0) \u2208 [r, 0]i , it immediately follows that [r \u2032 , 0]i = [r, 0]i . Hence, (s, 0) \u2208 [r, 0]i .\nNow by part (d) of Lemma 37, we get that (s, m) \u2208 [r, m]i . Thus, Pi (s, m) = Pi (r, m).\nSince I satisfies UNIF and (s, m) \u2208 W(r\u2032 ,m,i) , it follows that Pi (s, m) = Pi (r \u2032 , m). Hence,\n(s, m) \u2208 W(r,m,i) . Finally, we can conclude that s \u2208 Rm\n(r,i) , as desired. Given this equivalence,\nwe can deal with the \u2192i case just as we did in the proof of Theorem 32.\n\nFinally, we need to ensure that I \u2032 satisfies A. The proof of Theorem 32 shows that if I\nsatisfies NORM, then so does I \u2032 . Using parts (b) and (c) of Lemma 36, it easily follows that\nif I satisfies QUAL or RANK, then so does I \u2032 .\nIf REF and UNIF are both in A (but SDP is not), then we need a further modification of\n\u221e\n\u221e\nthe prior, in the same spirit of that in the proof of Theorem 32. Define P[r]\n= ([r]\u221e\ni , Pl[r]i ),\ni\n\u221e\n\u221e\nwhere Pl[r]i (\u2205) = \u22a5 and Pl[r]i (A) = \u22a4 for all A 6= \u2205. We now take the prior of the agent to be\n\u221e\n\u2032\n\u2032\u2032\nP(r\nl ,i) = P(r l ,i) \u2295 P[r] . It is straightforward to show that the resulting system satisfies REF\ni\nand the requirements of the theorem, using essentially the same arguments for dealing with\nREF as in the proof of Theorem 32.\nFinally, suppose SDP \u2208 A but REF is not. Note that, since CONS and SDP imply UNIF,\nI satisfies UNIF, so we can assume without loss of generality that UNIF is also in A. To\nget I \u2032 to satisfy SDP, we further modify P \u2032 so that it depends only on the agent, and not\nm\n\u2032\nm\nthe run. Thus, we define Pim = \u2297{P[r]\n: r \u2208 R}, and define P(r\nl ,i) = \u2295m Pi . Clearly,\ni\n59\n\n\fwith this prior, I \u2032 satisfies SDP. Again, we need to check that this change in prior does\nnot affect the rest of our argument. Once more, the only difficulty comes in dealing with\nthe \u2192i case. Just as in the case of UNIF, we proceed by showing that Pim |R\u2032 (K\u2032i (rl ,m)) =\nm\nP(r,i)\n|R\u2032 (K\u2032i (rl ,m)) . The argument is actually even easier than that for UNIF: We show that\n\u2032\n\u2032 l\nm\n\u2032\n\u2032 l\n\u222ar\u2032 Rm\n(r \u2032 ,i) \u2229 R (Ki (r , m)) = R(r,i) \u2229 R (Ki (r , m). Again, the inclusion from right to left is\n\u2032\n\u2032 l\nimmediate. For the opposite inclusion, suppose that sm \u2208 \u222ar\u2032 Rm\n(r \u2032 ,i) \u2229 R (Ki (r , m)). Since\nm\nsm \u2208 R\u2032 (Ki\u2032 (r l , m)), we must have (r, m) \u223ci (s, m). Since sm \u2208 \u222ar\u2032 R(r\n\u2032 ,i) , there must also be\n\u2032\nm\n\u2032\nsome run r such that s \u2208 R(r\u2032 ,i) . Thus, (s, m) \u2208 W(r ,m,i) . Since I satisfies CONS, we have\n(s, m) \u223ci (r \u2032 , m). It follows that (r \u2032 , m) \u223ci (r, m). Since I satisfies SDP, we must have that\nW(r,m,i) = W(r\u2032 ,m,i) , so (s, m) \u2208 W(r,m,i) . Therefore, s \u2208 Rm\n(r,i) , as desired.\nThe modifications to deal with the case where both SDP and REF are in A are identical to\nthe case with UNIF, and are omitted here. \u2737\n\nReferences\n[AGM85] C. E. Alchourr\u00f3n, P. G\u00e4rdenfors, and D. Makinson. On the logic of theory change:\npartial meet functions for contraction and revision. Journal of Symbolic Logic, 50:510\u2013\n530, 1985.\n[BB97] P. Battigalli and G. Bonanno. The logic of belief persistency. Economics and philosophy,\n1997. To appear.\n[BLMS97] R. Brafman, J.-C. Latombe, Y. Moses, and Y. Shoham. Applications of a logic of\nknowledge to motion planning under uncertainty. Journal of the ACM, 1997. To appear.\n[Bou92] C. Boutilier.\nNormative, subjective and autoepistemic defaults: adopting the\nRamsey test. In Principles of Knowledge Representation and Reasoning: Proc. Third\nInternational Conference (KR '92), pages 685\u2013696. Morgan Kaufmann, San Francisco,\nCalif., 1992.\n[Bou94a] C. Boutilier. Conditional logics of normality: a modal approach. Artificial Intelligence,\n68:87\u2013154, 1994.\n[Bou94b] C. Boutilier. Unifying default reasoning and belief revision in a modal framework.\nArtificial Intelligence, 68:33\u201385, 1994.\n[Bur81] J. Burgess. Quick completeness proofs for some logics of conditionals. Notre Dame\nJournal of Formal Logic, 22:76\u201384, 1981.\n[de 90] J. de Kleer. Using crude probability estimates to guide diagnosis. Artificial Intelligence,\n45:381\u2013392, 1990.\n[DH88] R. Davis and W. Hamscher. Model-based reasoning: troubleshooting. In H. Shrobe\nand The American Association for Artificial Intelligence, editors, Exploring AI, pages\n297\u2013346. Morgan Kaufmann, SF, 1988.\n\n60\n\n\f[DP90] D. Dubois and H. Prade. An introduction to possibilistic and fuzzy logics. In\nG. Shafer and J. Pearl, editors, Readings in Uncertain Reasoning, pages 742\u2013761.\nMorgan Kaufmann, San Francisco, Calif., 1990.\n[DP91] D. Dubois and H. Prade. Possibilistic logic, preferential models, non-monotonicity and\nrelated issues. In Proc. Twelfth International Joint Conference on Artificial Intelligence\n(IJCAI '91), pages 419\u2013424. 1991.\n[DZ82] P. Diaconis and S. L. Zabell. Updating subjective probability. Journal of the American\nStatistical Society, 77(380):822\u2013830, 1982.\n[FH94a] R. Fagin and J. Y. Halpern. Reasoning about knowledge and probability. Journal of\nthe ACM, 41(2):340\u2013367, 1994.\n[FH94b] N. Friedman and J. Y. Halpern. Conditional logics of belief change. In Proceedings,\nTwelfth National Conference on Artificial Intelligence (AAAI '94), pages 915\u2013921. 1994.\n[FH94c] N. Friedman and J. Y. Halpern. A knowledge-based framework for belief change. Part\nI: foundations. In R. Fagin, editor, Theoretical Aspects of Reasoning about Knowledge:\nProc. Fifth Conference, pages 44\u201364. Morgan Kaufmann, San Francisco, Calif., 1994.\n[FH95] N. Friedman and J. Y. Halpern. Plausibility measures: a user's manual. In\nProc. Eleventh Conference on Uncertainty in Artificial Intelligence (UAI '95), pages\n175\u2013184. 1995.\n[FH96a] N. Friedman and J. Y. Halpern. On the axiomatization and complexity of conditional\nlogics. In preperation. A preliminary version appeared in J. Doyle, E. Sandewall, and\nP. Torasso, editors. Principles of Knowledge Representation and Reasoning: Proc. Fourth\nInternational Conference (KR '94), 1994., 1996.\n[FH96b] N. Friedman and J. Y. Halpern. A qualitative Markov assumption and its implications\nfor belief change. In Proc. Twelfth Conference on Uncertainty in Artificial Intelligence\n(UAI '96), pages 263\u2013273, 1996.\n[FH97a] N. Friedman and J. Y. Halpern. Modeling belief in dynamic systems. Part II: revision\nand update. Submitted for publication. A preliminary version appears in J. Doyle,\nE. Sandewall, and P. Torasso, editors. Principles of Knowledge Representation and\nReasoning: Proc. Fourth International Conference (KR '94), 1994, pp. 190\u2013201, under\nthe title \"A knowledge-based framework for belief change. Part II: revision and update.\",\n1997.\n[FH97b] N. Friedman and J. Y. Halpern. Plausibility measures and default reasoning. Journal of\nthe ACM, 1997. Accepted for publication. A preliminary version of this work appeared\nin Proc. National Conference on Artificial Intelligence (AAAI '96), 1996, pages 1297\u2013\n1304.\n[FHK96] N. Friedman, J. Y. Halpern, and D. Koller. Conditional first-order logic revisited.\nIn Proceedings, Thirteenth National Conference on Artificial Intelligence (AAAI '96),\npages 1305\u20131312. 1996.\n[FHMV95] R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi. Reasoning about Knowledge. MIT\nPress, Cambridge, Mass., 1995.\n\n61\n\n\f[Fin72] B. de Finetti. Probability, Induction and Statistics. John Wiley & Sons, Inc., New York,\n1972.\n[FL79] M. J. Fischer and R. E. Ladner. Propositional dynamic logic of regular programs.\nJournal of Computer and System Sciences, 18(2):194\u2013211, 1979.\n[Fri97] N. Friedman. Modeling Beliefs in Dynamic Systems. PhD thesis, Stanford, 1997.\n[G\u00e4r88] P. G\u00e4rdenfors. Knowledge in Flux. MIT Press, Cambridge, Mass., 1988.\n[Gin86] M. L. Ginsberg. Counterfactuals. Artificial Intelligence, 30:35\u201379, 1986.\n[GM88] P. G\u00e4rdenfors and D. Makinson. Revisions of knowledge systems using epistemic\nentrenchment. In Proc. Second Conference on Theoretical Aspects of Reasoning about\nKnowledge, pages 83\u201395. Morgan Kaufmann, San Francisco, Calif., 1988.\n[GMP93] M. Goldszmidt, P. Morris, and J. Pearl. A maximum entropy approach to nonmonotonic\nreasoning. IEEE Transactions of Pattern Analysis and Machine Intelligence, 15(3):220\u2013\n232, 1993.\n[GP92] M. Goldszmidt and J. Pearl. Rank-based systems: A simple approach to belief revision,\nbelief update and reasoning about evidence and actions. In Principles of Knowledge\nRepresentation and Reasoning: Proc. Third International Conference (KR '92), pages\n661\u2013672. Morgan Kaufmann, San Francisco, Calif., 1992.\n[GPSS80] D. Gabbay, A. Pnueli, S. Shelah, and J. Stavi. On the temporal analysis of fairness. In\nProc. 7th ACM Symp. on Principles of Programming Languages, pages 163\u2013173, 1980.\n[Gro88] A. Grove. Two modelings for theory change. Journal of Philosophical Logic, 17:157\u2013170,\n1988.\n[HF89] J. Y. Halpern and R. Fagin. Modelling knowledge and action in distributed systems.\nDistributed Computing, 3(4):159\u2013179, 1989. A preliminary version appeared in Proc. 4th\nACM Symposium on Principles of Distributed Computing, 1985, with the title \"A formal\nmodel of knowledge, action, and communication in distributed systems: preliminary\nreport\".\n[Hin62] J. Hintikka. Knowledge and Belief. Cornell University Press, Ithaca, N.Y., 1962.\n[HM92] J. Y. Halpern and Y. Moses. A guide to completeness and complexity for modal logics\nof knowledge and belief. Artificial Intelligence, 54:319\u2013379, 1992.\n[HT93] J. Y. Halpern and M. R. Tuttle. Knowledge, probability, and adversaries. Journal of\nthe ACM, 40(4):917\u2013962, 1993.\n[HV89] J. Y. Halpern and M. Y. Vardi. The complexity of reasoning about knowledge and time,\nI: lower bounds. Journal of Computer and System Sciences, 38(1):195\u2013237, 1989.\n[KL51] S. Kullback and R. A. Leibler. On information and sufficiency. Annals of Mathematical\nStatistics, 22:76\u201386, 1951.\n[KL88] S. Kraus and D. Lehmann. Knowledge, belief, and time. Theoretical Computer Science,\n58:155\u2013174, 1988.\n\n62\n\n\f[KLM90] S. Kraus, D. Lehmann, and M. Magidor. Nonmonotonic reasoning, preferential models\nand cumulative logics. Artificial Intelligence, 44:167\u2013207, 1990.\n[KM91a] H. Katsuno and A. Mendelzon. On the difference between updating a knowledge base\nand revising it. In Principles of Knowledge Representation and Reasoning: Proc. Second\nInternational Conference (KR '91), pages 387\u2013394. Morgan Kaufmann, San Francisco,\nCalif., 1991.\n[KM91b] H. Katsuno and A. Mendelzon. Propositional knowledge base revision and minimal\nchange. Artificial Intelligence, 52(3):263\u2013294, 1991.\n[Lev84] H. J. Levesque. A logic of implicit and explicit belief. In Proc. National Conference on\nArtificial Intelligence (AAAI '84), pages 198\u2013202, 1984.\n[Lew73] D. K. Lewis. Counterfactuals. Harvard University Press, Cambridge, Mass., 1973.\n[LS94] P. Lamarre and Y. Shoham. Knowledge, certainty, belief, and conditionalisation. In\nPrinciples of Knowledge Representation and Reasoning: Proc. Fourth International\nConference (KR '94), pages 415\u2013424. Morgan Kaufmann, San Francisco, Calif., 1994.\n[Mak66] D. Makinson. On some completeness theorems in modal logic. Zeitschrift f\u00fcr\nMathematische Logik und Grundlagen der Mathematik, 12:379\u2013384, 1966.\n[Mey94] R. van der Meyden. Mutual belief revision (preliminary report). In J. Doyle,\nE. Sandewall, and P. Torasso, editors, Principles of Knowledge Representation and\nReasoning: Proc. Fourth International Conference (KR '94), pages 595\u2013606. Morgan\nKaufmann, San Francisco, Calif., 1994.\n[MS93] Y. Moses and Y. Shoham.\n64(2):299\u2013322, 1993.\n\nBelief as defeasible knowledge.\n\nArtificial Intelligence,\n\n[Pea89] J. Pearl. Probabilistic semantics for nonmonotonic reasoning: a survey. In R. J.\nBrachman, H. J. Levesque, and R. Reiter, editors, Proc. First International Conference\non Principles of Knowledge Representation and Reasoning (KR '89), pages 505\u2013516,\n1989. Reprinted in Readings in Uncertain Reasoning, G. Shafer and J. Pearl (eds.),\nMorgan Kaufmann, San Francisco, Calif., 1990, pp. 699\u2013710.\n[PR97] M. Piccione and A. Rubinstein. On the interpretation of decision problems with\nimperfect recall. Games and Economic Behavior, 1997. To appear.\n[Pra79] V. R. Pratt. Models of program logics. In Proc. 20th IEEE Symp. on Foundations of\nComputer Science, pages 115\u2013122, 1979.\n[Ram31] F. P. Ramsey. Truth and probability. In R. B. Braithwaite, editor, The Foundations\nof Mathematics and other Logical Essays, pages 156\u2013198. Routledge and Kegan Paul,\nLondon, 1931.\n[Rei87] R. Reiter. A theory of diagnosis from first principles. Artificial Intelligence, 32:57\u201395,\n1987. Reprinted in in Readings in Nonmonotonic Reasoning, M. L. Ginsberg (ed.),\nMorgan Kaufman, San Francisco, CA. 1987, pp. 352\u2013371.\n[Sha76] G. Shafer. A Mathematical Theory of Evidence. Princeton University Press, Princeton,\nN.J., 1976.\n\n63\n\n\f[Sho87] Y. Shoham. A semantical approach to nonmonotonic logics. In Proc. 2nd IEEE Symp.\non Logic in Computer Science, pages 275\u2013279, 1987. Reprinted in M. L. Ginsberg (Ed.),\nReadings in Nonmonotonic Reasoning, Morgan Kaufman, San Francisco, Calif., 1987,\npp. 227\u2013250.\n[Spo88] W. Spohn. Ordinal conditional functions: a dynamic theory of epistemic states. In\nW. Harper and B. Skyrms, editors, Causation in Decision, Belief Change, and Statistics,\nvolume 2, pages 105\u2013134. Reidel, Dordrecht, Netherlands, 1988.\n[van94] R. van der Meyden. Personal communication. 1994.\n[Voo92] F. Voorbraak. Generalized Kripke models for epistemic logic. In Y. O. Moses, editor,\nTheoretical Aspects of Reasoning about Knowledge: Proc. Fourth Conference, pages 214\u2013\n228. Morgan Kaufmann, San Francisco, Calif., 1992.\n[WK92] Z. Wang and G. J. Klir. Fuzzy Measure Theory. Plenum Press, New York, 1992.\n\n64\n\n\f"}