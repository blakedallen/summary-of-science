{"id": "http://arxiv.org/abs/cond-mat/0305319v3", "guidislink": true, "updated": "2004-06-30T15:54:40Z", "updated_parsed": [2004, 6, 30, 15, 54, 40, 2, 182, 0], "published": "2003-05-14T12:22:26Z", "published_parsed": [2003, 5, 14, 12, 22, 26, 2, 134, 0], "title": "Entropy of Pseudo Random Number Generators", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0305217%2Ccond-mat%2F0305230%2Ccond-mat%2F0305170%2Ccond-mat%2F0305477%2Ccond-mat%2F0305032%2Ccond-mat%2F0305639%2Ccond-mat%2F0305298%2Ccond-mat%2F0305484%2Ccond-mat%2F0305651%2Ccond-mat%2F0305193%2Ccond-mat%2F0305261%2Ccond-mat%2F0305536%2Ccond-mat%2F0305244%2Ccond-mat%2F0305446%2Ccond-mat%2F0305009%2Ccond-mat%2F0305577%2Ccond-mat%2F0305174%2Ccond-mat%2F0305568%2Ccond-mat%2F0305435%2Ccond-mat%2F0305022%2Ccond-mat%2F0305358%2Ccond-mat%2F0305635%2Ccond-mat%2F0305325%2Ccond-mat%2F0305088%2Ccond-mat%2F0305065%2Ccond-mat%2F0305590%2Ccond-mat%2F0305308%2Ccond-mat%2F0305529%2Ccond-mat%2F0305100%2Ccond-mat%2F0305146%2Ccond-mat%2F0305616%2Ccond-mat%2F0305166%2Ccond-mat%2F0305634%2Ccond-mat%2F0305299%2Ccond-mat%2F0305554%2Ccond-mat%2F0305520%2Ccond-mat%2F0305496%2Ccond-mat%2F0305254%2Ccond-mat%2F0305409%2Ccond-mat%2F0305465%2Ccond-mat%2F0305091%2Ccond-mat%2F0305121%2Ccond-mat%2F0305319%2Ccond-mat%2F0305381%2Ccond-mat%2F0305078%2Ccond-mat%2F0305575%2Ccond-mat%2F0305092%2Ccond-mat%2F0305543%2Ccond-mat%2F0305655%2Ccond-mat%2F0305043%2Ccond-mat%2F0305532%2Ccond-mat%2F0305675%2Ccond-mat%2F0305452%2Ccond-mat%2F0305573%2Ccond-mat%2F0305125%2Ccond-mat%2F0305224%2Ccond-mat%2F0305613%2Ccond-mat%2F0305317%2Ccond-mat%2F0305294%2Ccond-mat%2F0305119%2Ccond-mat%2F0305069%2Ccond-mat%2F0305275%2Ccond-mat%2F0305656%2Ccond-mat%2F0305559%2Ccond-mat%2F0305384%2Ccond-mat%2F0305182%2Ccond-mat%2F0305378%2Ccond-mat%2F0305539%2Ccond-mat%2F0305438%2Ccond-mat%2F0305324%2Ccond-mat%2F0305362%2Ccond-mat%2F0305012%2Ccond-mat%2F0305607%2Ccond-mat%2F0305629%2Ccond-mat%2F0305432%2Ccond-mat%2F0305368%2Ccond-mat%2F0305507%2Ccond-mat%2F0305006%2Ccond-mat%2F0305688%2Ccond-mat%2F0305352%2Ccond-mat%2F0305260%2Ccond-mat%2F0305162%2Ccond-mat%2F0305346%2Ccond-mat%2F0305180%2Ccond-mat%2F0305188%2Ccond-mat%2F0305340%2Ccond-mat%2F0305601%2Ccond-mat%2F0305083%2Ccond-mat%2F0305241%2Ccond-mat%2F0305252%2Ccond-mat%2F0305013%2Ccond-mat%2F0305297%2Ccond-mat%2F0305036%2Ccond-mat%2F0305604%2Ccond-mat%2F0305290%2Ccond-mat%2F0305370%2Ccond-mat%2F0305397%2Ccond-mat%2F0305538%2Ccond-mat%2F0305335%2Ccond-mat%2F0305023%2Ccond-mat%2F0305694&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Entropy of Pseudo Random Number Generators"}, "summary": "Since the work of Ferrenberg et al.[PRL 69, (1992)] some pseudo random number\ngenerators are known to yield wrong results in cluster Monte Carlo simulations.\nIn this contribution the fundamental mechanism behind this failure is\ndiscussed. Almost all random number generators calculate a new pseudo random\nnumber $x_i$ from preceding values, $x_i = f(x_{i-1}, x_{i-2},..., x_{i-q})$.\nFailure of these generators in cluster Monte Carlo simulations and related\nexperiments can be attributed to the low entropy of the production rule $f()$\nconditioned on the statistics of the input values $x_{i-1},...,x_{i-q}$. Being\na measure only of the arithmetic operations in the generator rule, the\nconditional entropy is independent of the lag in the recurrence or the period\nof the sequence. In that sense it measures a more profound quality of a random\nnumber generator than empirical tests with their limited horizon.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0305217%2Ccond-mat%2F0305230%2Ccond-mat%2F0305170%2Ccond-mat%2F0305477%2Ccond-mat%2F0305032%2Ccond-mat%2F0305639%2Ccond-mat%2F0305298%2Ccond-mat%2F0305484%2Ccond-mat%2F0305651%2Ccond-mat%2F0305193%2Ccond-mat%2F0305261%2Ccond-mat%2F0305536%2Ccond-mat%2F0305244%2Ccond-mat%2F0305446%2Ccond-mat%2F0305009%2Ccond-mat%2F0305577%2Ccond-mat%2F0305174%2Ccond-mat%2F0305568%2Ccond-mat%2F0305435%2Ccond-mat%2F0305022%2Ccond-mat%2F0305358%2Ccond-mat%2F0305635%2Ccond-mat%2F0305325%2Ccond-mat%2F0305088%2Ccond-mat%2F0305065%2Ccond-mat%2F0305590%2Ccond-mat%2F0305308%2Ccond-mat%2F0305529%2Ccond-mat%2F0305100%2Ccond-mat%2F0305146%2Ccond-mat%2F0305616%2Ccond-mat%2F0305166%2Ccond-mat%2F0305634%2Ccond-mat%2F0305299%2Ccond-mat%2F0305554%2Ccond-mat%2F0305520%2Ccond-mat%2F0305496%2Ccond-mat%2F0305254%2Ccond-mat%2F0305409%2Ccond-mat%2F0305465%2Ccond-mat%2F0305091%2Ccond-mat%2F0305121%2Ccond-mat%2F0305319%2Ccond-mat%2F0305381%2Ccond-mat%2F0305078%2Ccond-mat%2F0305575%2Ccond-mat%2F0305092%2Ccond-mat%2F0305543%2Ccond-mat%2F0305655%2Ccond-mat%2F0305043%2Ccond-mat%2F0305532%2Ccond-mat%2F0305675%2Ccond-mat%2F0305452%2Ccond-mat%2F0305573%2Ccond-mat%2F0305125%2Ccond-mat%2F0305224%2Ccond-mat%2F0305613%2Ccond-mat%2F0305317%2Ccond-mat%2F0305294%2Ccond-mat%2F0305119%2Ccond-mat%2F0305069%2Ccond-mat%2F0305275%2Ccond-mat%2F0305656%2Ccond-mat%2F0305559%2Ccond-mat%2F0305384%2Ccond-mat%2F0305182%2Ccond-mat%2F0305378%2Ccond-mat%2F0305539%2Ccond-mat%2F0305438%2Ccond-mat%2F0305324%2Ccond-mat%2F0305362%2Ccond-mat%2F0305012%2Ccond-mat%2F0305607%2Ccond-mat%2F0305629%2Ccond-mat%2F0305432%2Ccond-mat%2F0305368%2Ccond-mat%2F0305507%2Ccond-mat%2F0305006%2Ccond-mat%2F0305688%2Ccond-mat%2F0305352%2Ccond-mat%2F0305260%2Ccond-mat%2F0305162%2Ccond-mat%2F0305346%2Ccond-mat%2F0305180%2Ccond-mat%2F0305188%2Ccond-mat%2F0305340%2Ccond-mat%2F0305601%2Ccond-mat%2F0305083%2Ccond-mat%2F0305241%2Ccond-mat%2F0305252%2Ccond-mat%2F0305013%2Ccond-mat%2F0305297%2Ccond-mat%2F0305036%2Ccond-mat%2F0305604%2Ccond-mat%2F0305290%2Ccond-mat%2F0305370%2Ccond-mat%2F0305397%2Ccond-mat%2F0305538%2Ccond-mat%2F0305335%2Ccond-mat%2F0305023%2Ccond-mat%2F0305694&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Since the work of Ferrenberg et al.[PRL 69, (1992)] some pseudo random number\ngenerators are known to yield wrong results in cluster Monte Carlo simulations.\nIn this contribution the fundamental mechanism behind this failure is\ndiscussed. Almost all random number generators calculate a new pseudo random\nnumber $x_i$ from preceding values, $x_i = f(x_{i-1}, x_{i-2},..., x_{i-q})$.\nFailure of these generators in cluster Monte Carlo simulations and related\nexperiments can be attributed to the low entropy of the production rule $f()$\nconditioned on the statistics of the input values $x_{i-1},...,x_{i-q}$. Being\na measure only of the arithmetic operations in the generator rule, the\nconditional entropy is independent of the lag in the recurrence or the period\nof the sequence. In that sense it measures a more profound quality of a random\nnumber generator than empirical tests with their limited horizon."}, "authors": ["Stephan Mertens", "Heiko Bauke"], "author_detail": {"name": "Heiko Bauke"}, "author": "Heiko Bauke", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1103/PhysRevE.69.055702", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cond-mat/0305319v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cond-mat/0305319v3", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "4 pages, 4 figures", "arxiv_primary_category": {"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cond-mat/0305319v3", "affiliation": "Institute for Theoretical Physics, Otto-von-Guericke-University of Magdeburg", "arxiv_url": "http://arxiv.org/abs/cond-mat/0305319v3", "journal_reference": "Phys. Rev. E 69, 055702(R) (2004)", "doi": "10.1103/PhysRevE.69.055702", "fulltext": "Entropy of Pseudo Random Number Generators\nStephan Mertens\u2217 and Heiko Bauke\u2020\nInstitut f\u00fcr Theoretische Physik, Otto-von-Guericke Universit\u00e4t, PF 4120, 39016 Magdeburg, Germany\n(Dated: June 30, 2004)\nSince the work of Ferrenberg et al. [PRL 69, (1992)] some pseudo random number generators are known to\nyield wrong results in cluster Monte Carlo simulations. In this contribution the fundamental mechanism behind\nthis failure is discussed. Almost all random number generators calculate a new pseudo random number xi from\npreceding values, xi = f (xi\u22121 , xi\u22122 , . . . , xi\u2212q ). Failure of these generators in cluster Monte Carlo simulations and\nrelated experiments can be attributed to the low entropy of the production rule f () conditioned on the statistics\nof the input values xi\u22121 , . . . , xi\u2212q . Being a measure only of the arithmetic operations in the generator rule, the\nconditional entropy is independent of the lag in the recurrence or the period of the sequence. In that sense it\nmeasures a more profound quality of a random number generator than empirical tests with their limited horizon.\nPACS numbers: 05.10.Ln, 02.70.Rr, 75.40.Mg\n\nRandom numbers are the key resource of all Monte Carlo\n(MC) simulations. They are usually produced by a few lines\nof code, a subroutine called pseudo random number generator\n(PRNG). The term pseudo refers to the fact these generators\nimplement a deterministic recursive formula\nxi = f (xi\u22121 , xi\u22122 , . . . , xi\u2212q ) ,\n\ni>q\n\n(1)\n\nto produce a sequence (xi ) of pseudo random numbers (PRNs).\nThe only true randomness in this sequence is concentrated in\nthe choice of the \"seed\" (x1 , . . . , xq ), a few hundred bits at\nmost. This small amount of randomness is expanded by (1) to\nthe 1010 or more random numbers that are consumed by a MC\nsimulation on a present day computer. Most practitioners have\nno problems founding their scientific reputation on something\npseudo, they simply trust some well-established PRNG like\neverybody else trusts the subroutine to calculate sin(x). Every\nnow and then, however, a popular PRNG is caught producing\nwrong results, and this does not always entail its removal from\nthe practitioners toolbox. An infamous example is the lagged\nFibonacci generator F(p, q, \u25e6), defined by the recursion\nxi = xi\u2212p \u25e6 xi\u2212q\n\nmod m\n\n(2)\n\nwith \u25e6 \u2208 {\u2295, +, \u2212, \u00d7}. The bitwise exclusive-or operator \u2295\ndoes not require a mod-operation, hence it is very fast. m\nis usually the word size of the computer or a prime close to\nit. The choice of the \"magic numbers\" p and q is based on\ntheoretical considerations, like maximizing the period of the\nsequence [1]. F(103, 250, \u2295) (also known as R250) has been\nintroduced into the physics community in 1981 [2] as a very\nfast and reliable PRNG. In 1992, Ferrenberg et al. [3] reported\nserious problems with lagged Fibonacci generators when applied in cluster MC simulations of the 2D-Ising model with\nthe Wolff algorithm. This discovery initiated a series of investigations, in the course of which shortcomings of lagged\nFibonacci generators have been found in various other simulations, like in simulations based on the Swendsen-Wang al-\n\n\u2217 E-mail:stephan.mertens@physik.uni-magdeburg.de\n\u2020 E-mail:heiko.bauke@physik.uni-magdeburg.de\n\ngorithm [4], 3D self-avoiding random walks [5], the Metropolis algorithm on the Blume-Capel model [6], n-block tests [7]\nand 2D-random walks [8]. Despite this bad record, lagged\nFibonacci generators kept being recommended as \". . . good\nenough for many applications\" [9] or even for large-scale simulations [10]. The RANLUX generator [11], prevalent in highenergy-physics, is based on a modified F(p, q, \u2212) enhanced\nwith a special measure to improve the quality of the random\nnumbers: it simply throws away up to 80% of the numbers.\nOther advices to emend lagged Fibonacci generators are to\nuse larger values of the lag like in F(1393, 4423, \u25e6), or to increase the number of feedback taps to n > 2 [8],\nxi = xi\u2212q1 \u25e6 xi\u2212q2 \u25e6 * * * \u25e6 xi\u2212qn\n\nmod m.\n\n(3)\n\nMost of these recommendations are based on empirical evidence only, with the throw-away strategy in RANLUX being a\nnotable exception. But without a theoretical justification these\nstrategies have the smack of sweeping the dirt under the carpet. Some authors blame the 3-point correlations induced by\n(2) for the problems [6, 12]. In fact, choosing n > 2 in (3)\nthe observed deviations are reduced, but strange enough, the\nsimple linear congruence generator\nxi = \u03b1 xi\u22121\n\nmod m.\n\n(4)\n\nwith \u03b1 \u001d 1 has strong 2-point correlations, yet it performs\nreasonably in cluster MC simulations. According to Heuer\net al. [13] \". . . the reason why the Wolff algorithm is so sensitive to triplet correlations remains a mystery.\" The analysis of a 1D directed random walk simulation by Shchur et al.\n[14, 15] shed light on the mechanism behind this failure: the\ninability of the PRNG to compensate the persistent bias in the\npreceding random numbers that is induced by the simulation\nalgorithm. In this contribution we will define a robust, quantitative measure for this inability.\nThe Wolff algorithm [16, 17] is a very efficient MC method\nto simulate Ising spin systems in thermal equilibrium. It flips\nclusters of spins and its central part is the construction of\nthese clusters. The algorithm maintains a list of candidate\nspins. As long as the candidate list is not empty, one spin is\nremoved from the list and is added to the cluster with probability Padd = 1 \u2212 e\u22122/T where T is the temperature measured in\n\n\f2\n1.0005\n\n1\n\n1.0004\n\n\u03b3\n\n1.0003\n\nrelative average cluster size\n\n1.02\n\n1.01\n\n\u03b3\n\n1.0002\n1.0001\n\n0.95\n\n1.0000\n\ncluster size\n\n1.05\n\n1.00\n\n\u03b3\n\n0.9999\n0.9998\n\n0.9\nT = 0.75 \u22c5 Tc\nT = 0.85 \u22c5 Tc\nT = 0.95 \u22c5 Tc\nT = 1.00 \u22c5 T\nc\nT = 1.10 \u22c5 Tc\nT = 1.15 \u22c5 Tc\nT = 1.25 \u22c5 Tc\n\n1\np(r)\n\n0.85\n\n\u03b3\n\n0.8\n0\n0\nr\n\n0.75\n0\n\n10\n\n20\n\n30\n\n40\n\nPadd\n\n50\nk\n\n1\n\n60\n\n70\n\n80\n\n90\n\n100\n\nFigure 1: The completion of a cluster in the Wolff algorithm implies\na bias \u03b3 (k) (5) in the preceding random numbers ri\u2212k . The bias is towards rejection moves, i.e. the probability of ri\u2212k < Padd (acceptance)\nis smaller than Padd . The inset shows the corresponding probability\ndensity of the random numbers r \u2208 [0, 1). The data shown is from\nsimulations of a 24 \u00d7 24 spin square Ising model, but the figures look\nsimilar for larger systems and in 3D. Tc is the critical temperature of\nthe infinite system.\n\nunits kB /J. If the spin has been added to the cluster, its equally\naligned neighbor spins are added to the candidate list (if they\nare not themselves part of the cluster). When the candidate\nlist is empty, the growth process stops, all spins of the cluster\nare flipped and a new growth process is initiated by starting a\nnew cluster with a randomly chosen spin and adding all of its\nequally aligned neighbors to the candidate list.\nIn the cluster growth process, the PRNs ri = xi /m \u2208 [0, 1)\nare used to decide whether a spin from the candidate list is\nadded to the cluster (ri < Padd ) or not (ri \u2265 Padd ). With each\nspin that is added to the cluster, new spins may enter the candidate list, too. For the growth process to stop, the candidate\nlist needs to be empty, hence we expect the end of each growth\nprocess to go hand in hand with a high rejection rate or a superproportional fraction of random numbers ri \u2265 Padd . Each time\na cluster is completed one can look back at the last random\nnumbers ri\u22121 , ri\u22122 , . . . , ri\u2212k , . . . and measure the bias\n\n\u03b3 (k) =\n\nP(ri\u2212k < Padd )\nPadd\n\n(5)\n\nwhere P(r < P) denotes the probability of the event r < P.\nUnbiased numbers have \u03b3 = 1, but Fig. 1 shows that the numbers that contribute to the completion of a cluster are indeed\nstrongly biased. For most temperatures the bias is towards\nrejections moves (\u03b3 < 1). For low temperatures the cluster\nspans almost the entire system, hence the completion is dominated by a lack of unassigned spins rather than higher rejection rates. In these cases we observe a weak bias towards\nacceptance moves (\u03b3 > 1).\nThe bias \u03b3 is a genuine property of the Wolff algorithm\nand the PRNG has to cope with this situation: after all we\n\n\u2295\n\n+\n\n\u2212\n\n\u00d7\n\n0.99\n\nFigure 2: Bias in the output of the lagged Fibonacci generator with biased inputs ( ) and cluster sizes in the Wolff algorithm (\u0003). The bias\nin the input numbers was \u03b3 = 0.975 and Padd = 0.586, corresponding\nto a MC simulation at the critical temperature of the 2D-Ising model.\nThe simulation was done with F(13, 33, \u25e6) on a 16 \u00d7 16 spin system.\nError bars are smaller than the symbol size. The reference value of\nthe cluster size has been obtained from simulations with a high quality PRNG [18].\n\nexpect it to generate unbiased random numbers xi even if the\nnumbers xi\u22121 , . . . , xi\u2212p in (1) are biased. The crucial point is\nthat some PRNGs like the lagged Fibonacci have problems to\nreconstruct unbiased PRNs from biased input. This is most\neasily seen for F(p, q, \u2295) and Padd = 1/2. Let Pi denote the\nprobability that the most significant bit of xi equals 1. In the\nfinal phase of the cluster growth process we have Pi\u2212q > 1/2\nand Pi\u2212p > 1/2, and from xi = xi\u2212q \u2295 xi\u2212p we get Pi < 1/2, i.e.\nwe expect too many spins to be added to the new cluster.\nA simple experiment illustrates the relation between the persistent bias and the cluster size. We draw two real random\nnumbers r1 and r2 from a distribution as shown in the inset\nof Fig. 1, with bias \u03b3 < 1. These two numbers are used to\ngenerate a new random number r3 according to the lagged Fibonacci rule. Fig. 2 shows the bias of r3 for all four binary operators: \u2295 leads to a strong bias \u03b3 > 1 (as discussed above), +\nand \u2212 to a weaker bias \u03b3 < 1, and \u00d7 shows no bias in the new\nvariable r3 . This corresponds nicely with the average cluster\nsize in the Wolff algorithm for simulations of the square Ising\nmodel: with \u2295 the clusters are too large, with \u00b1 the clusters\nare too small. Only the \u00d7-operator generates clusters of the\ncorrect size. These results are consistent with systematic numerical investigations [4].\nAt this point the virtue of increasing the lag becomes apparent. An increased lag q usually implies an enlarged difference\nq \u2212 p, which in turn decreases the probability that both numbers xi\u2212p and xi\u2212q are from the completion phase of a cluster.\nThe bias of xi is much weaker if only one of its predecessors\nxi\u2212p or xi\u2212q is biased. But he central weakness, the incapacity\nto restore unbiased PRNs from biased inputs, is independent\nof the lag. The RANLUX approach to throw away subsets the\nPRNs helps but requires large fractions of the stream of PRNs\nto be ignored [19]. To find a better remedy it is instructive to\nconsider a tractable model of a PRNG. Our model PRNG directly generates a stream of real numbers on the interval [0, 1)\nvia the recursion\nri = {\u03b1 (ri\u22121 + ri\u22122 + * * * + ri\u2212n )}\n\n(6)\n\nwhere {r} denotes the fractional part of r. For \u03b1 = 1, Eq. (6)\n\n\f3\n\n\u03c1i (r) =\n\n1\n\u03b1\n\nbn\u03b1 c\n\n\u2211 \u03c1i\u22121 ? \u03c1i\u22122 ? * * * ? \u03c1i\u2212n\n\nj=0\n\n\u0012\n\nr+ j\n\u03b1\n\n\u0013\n\n(7)\n\nfor 0 \u2264 r < 1. bxc denotes the largest integer not larger than x\nand ? is the convolution operator. The sum results from taking\nthe fractional part in Eq. (6). It canRbe interpreted as Riemann\nsum approximation to the integral \u03c1i\u22121 ? * * * ? \u03c1i\u2212n (x) dx = 1\nwith mesh size 1/\u03b1 , hence we have the immediate result\nlim \u03c1i (r) = 1\n\n\u03b1 \u2192\u221e\n\n0 \u2264 r < 1,\n\n(8)\n\ni.e. for \u03b1 \u2192 \u221e the new number ri is uniformly distributed independently of the distribution of the preceding numbers. This\nis what makes the simple linear congruence generator (4) perform better than F(p, q, \u00b1) and F(p, q, \u2295). One of the numbers xi\u2212q or xi\u2212p that enter the right hand side of the multiplicative generator F(p, q, \u00d7) can be seen as a multiplier for\nthe other. This multiplier varies, but it is \u001d 1 for almost all\niterations. This explains why F(p, q, \u00d7) works fine in cluster\nMC simulations. For \u03b1 = 1 the support of the n-fold convolution is [0, n), and it is sampled with a mesh of size one. In the\nlimit n \u2192 \u221e we can again replace the sum by an integral and\nwe get back the uniform distribution for \u03c1i . For this reason\nthe quality of lagged Fibonacci generators increases with the\nnumber of feedback taps. In terms of computational efficiency\na generator with a small number of feedback taps but a large\nfactor \u03b1 is much better than a generator with \u03b1 = 1 but a large\nnumber of feedback taps.\nWe have seen that a multiplier \u03b1 > 1 or a large number n of\nfeedback taps promotes a robust uniformity of the PRNs, even\nif the input numbers are non uniform. A quantitative measure\nof this robustness is given by the entropy of the output value\nri , conditioned on the input variables ri\u22121 , . . . , ri\u2212q . Of course\nri is uniquely determined by ri\u22121 , . . . , ri\u2212q hence the entropy\nis zero, reflecting the determinism in our PRNs. Fortunately\nMC simulations use the ri in a coarse grained manner, to \"roll\na die\" or to \"toss a coin\". In the Wolff algorithm the (biased)\ncoin shows head with probability Padd . In general, a PRNG is\nused for a random choice out of M of macrostates m1 . . . , mM\n(the faces of the die). Each macrostate m j is represented by\na large number of microstates, the actual PRNs. The random\nchoice is done by partitioning the interval [0, 1) into disjoint\nintervals I j such that \u222aM\nj=1 I j = [0, 1), and macrostate m j is\nselected if and only if r \u2208 I j . Under the canonical assumption\nthat the PRNs r are uniformly distributed, m j is selected with\nprobability Pj = |I j |. Obviously a simulation is sensitive only\nto correlations in the stream m j of macrostates. On the other\nhand it can only induce correlations at the macrostate level,\nand on this level the conditional entropy can be larger than\nzero: Eq. (1) is deterministic at the microstate level, but it can\n\n1\n\n0.9\n\nH/log2 M\n\ncorresponds to an additive lagged Fibonacci generator with\nn feedback taps. The case n = 1 corresponds to the linear\ncongruence generator (4). Now let \u03c1i denote the probability density function of ri . To understand how Eq. (6) transforms the probability density we assume that the input values\nri\u22121 , . . . , ri\u2212n are independent. This holds strictly only for the\nfirst iteration on the initial seed, but it allows us to write\n\n0.8\n\n0.7\nM=2\nM=8\nM = 16\nM = 32\n\n0.6\n\n0.5\n0\n\n5\n\n10\n\n15\n\n20\n\u03b1\n\n25\n\n30\n\n35\n\n40\n\nFigure 3: Conditional entropy of the recursion ri = {\u03b1 (ri\u2212p + ri\u2212q )}\nfor different numbers M of equally weighted macrostates. The multiplier \u03b1 must be larger than M to ensure proper mixing of microstates.\n\nbe chaotic at the macrostate level. The conditional macrostate\nentropy H of a PRNG with n feedback taps reads\nH =\u2212\n\n\u2211 P(m1 , . . . , mn+1) log2\n\n{mi }\n\nP(m1 , . . . , mn+1 )\n,\nP(m1 , . . . , mn )\n\n(9)\n\nwhere P(m1 , . . . , mn+1 ) is the joint probability that the PRNG\nselects macrostate mn+1 and its n input values are unbiased\nand independent representatives of the macrostates m1 , . . . , mn .\nNote that our definition of H implies maximum entropy of\nthe n microstates that form the input of the generator: H\ngives the uncertainty that the generator can produce in one\niteration, given that we have full knowledge of the preceding\nmacrostates, but no knowledge of the underlying microstates.\nA good generator should have a value close to the upper bound\n\u2212 \u2211 j Pj log2 Pj . Note also that H is not a measure of the stream\nof PRNs that comes out of a generator. It is a measure of the\ngenerator rule itself. Hence it is very different from the entropy used in empirical investigations of pseudo random data\n[20]. To illustrate this difference consider any empirical test\non a a finite stream of pseudo random numbers. The lag q of\nthe underlying generator can easily be tuned to increase the\nrange of the correlations beyond the horizon of the test. The\nconditional entropy, being independent from the position of\nthe feedback taps, is not deceived by these measures. Increasing the number M of macrostates on the other hand, every\nPRNG will eventually start to give low entropy values. You\nbetter stay away from the microstate level to keep the illusion\nof randomness.\nThe calculation of the M n+1 probabilities that enter the definition of H is straightforward. For simple generators it can be\nbe done analytically. As an example consider F(p, q, \u2295) and\nM = 2m equally weighted macrostates: here the macrostate is\nuniquely determined by the m most significant bits of the xi ,\nand the \u2295-operator does not mix these bits with the ambivalent lower order bits. Hence H = 0 as opposed to the target\nvalue H = m. This result is independent of the values of p and\nq and will be the same for any number of feedback taps. It is\nan indicator of a fundamental weakness of all \u2295-recurrences.\nFor our model PRNG (6) the probabilities P(m1 , . . . , mn+1 )\nare simple integrals and computing H is straightforward.\n\n\f4\n\ncMC\nE\n\nMC\n\n1\n0.3\n\ncMC\nEMC\nc\n\nMC\n\nEMC\n\n10\n3\n\nc\n\nMC\n\nEMC\ncMC\nE\n\nMC\n\n1\n0.3\n\ncMC\nEMC\nc\n\nMC\n\nEMC\n\n10\n3\n\ncMC\nEMC\nc\n\nMC\n\nEMC\ncMC\nE\n\nMC\n\n1\n0.3\n\ncMC\nEMC\n\nMC\n\n30\n\n|cMC \u2212 cref.|/\u2206cMC\n\nMC\n\nEMC\n\nEMC\n\nEMC\n\n|/\u2206E\n\nc\n\n30\n\ncMC\n\nref.\n\nEMC\n\n100\n\nEMC\n\nscale\n4\n\ncMC\n\n\u2212E\n\n3\n\ncMC\n\ncMC\n\nMC\n\n10\n\n100\n\nEMC\n\n\u03b1=5\n\n|E\n\n106 MC updates\n\n30\n\ncMC\n\n106 MC updates\n\n100\n\n\u03b1=3\n\n106 MC updates\n\n\u03b1=1\n\n3\n\n2\n\n1\n\nc\n\nMC\n\nEMC\n\n0\n\nFigure 4: MC simulation of a 6 \u00d7 6 \u00d7 6 spin Ising model with the Wolff algorithm and ri = \u03b1 (ri\u221213 + ri\u221233 ) mod 231 as PRNG. Grayscales\nindicate the deviation from reference values for the energy E and the specific heat c. These reference values have been obtained from\nsimulations with a high quality PRNG [18]. Corresponding simulations with 2D systems yield similar results. For 2D systems reference\nvalues of energy and specific heat can be calculated exactly [21].\n\nFig. 3 shows H for our model PRNG with two feedback taps.\nThe entropy approaches its maximum as the multiplier \u03b1 gets\nlarger, and for \u03b1 > M it is very close to the maximum. This\nis easily understood from Eq. (7): the convolution integral of\nfunctions that are constant on intervals of size 1/M is well\napproximated by a Riemann sum of mesh size smaller than\n1/M. Note that H is not a strictly monotonic function of\n\u03b1 . The resonances in Fig. 3 reflect the fact that even a non\nuniform distribution at microstate level may yield the correct\nmacrostate statistics for a particular set of weights. According\nto Fig. 3 even a small factor \u03b1 > 1 yields acceptable statistics for M = 2 macrostates. Hence we expect good results in\na cluster MC simulation even with a notorious bad generator\nlike F(p, q, \u00b1) if we enhance the entropy of the latter with a\nsmall multiplier \u03b1 . Note that for PRNGs like F(p, q, \u00b1) the\n\nmultiplier must be an odd integer. Fig. 4 shows the quality\nof cluster MC simulations of 2D and 3D Ising models with\nF(p, q, +) plus multiplier: for \u03b1 = 1 we see strong deviations\n(which for 2D simulations have long been known), but for\n\u03b1 = 3, the deviations are much weaker, and for \u03b1 = 5 they\nhave basically disappeared.\n\n[1] D. E. Knuth, The Art of Computer Programming, vol. 2\n(Addison-Wesley, 1998), 3rd ed.\n[2] S. Kirkpatrick and E. Stoll, J. Comput. Phys. 40, 517 (1981).\n[3] A. M. Ferrenberg, D. Landau, and Y. J. Wong, Phys. Rev. Lett.\n69, 3382 (1992).\n[4] P. D. Coddington, Int. J. Mod. Phys. C 5, 547 (1994).\n[5] P. Grassberger, Phys. Letters A 181, 43 (1993).\n[6] F. Schmid and N. Wilding, Int. J. Mod. Phys. C 6, 781 (1995).\n[7] I. Vattulainen, T. Ala-Nissila, and K. Kankaala, Phys. Rev. Lett.\n73, 2513 (1994).\n[8] R. M. Ziff, Computers in Physics 12, 385 (1998).\n[9] K. Kankaala, T. Ala-Nissila, and I. Vattulainen, Physical Review E 48, R4211 (1993).\n[10] W. Petersen, Int. J. High Speed Comput. 6, 387 (1994).\n[11] M. L\u00fcscher, Comp. Phys. Comm. 79, 100 (1994).\n[12] I. Vattulainen, T. Ala-Nissila, and K. Kankaala, Phys. Rev. E 52,\n\n3205 (1995).\n[13] A. Heuer, B. D\u00fcnweg, and A. M. Ferrenberg, Comp. Phys.\nComm. 103, 1 (1997).\n[14] L. Shchur, J. Heringa, and H. Bl\u00f6te, Physica A 241 (1997).\n[15] L. N. Shchur, Comp. Phys. Comm. 121, 83 (1999).\n[16] U. Wolff, Phys. Rev. Lett. 63, 361 (1989).\n[17] M. Newmann and G. Barkema, Monte Carlo Methods in Statistical Physics (Clarendon Press, Oxford, 1999).\n[18] H. Bauke and S. Mertens, to be published.\n[19] L. N. Shchur and P. Butera, Int. J. Mod. Phys. C 9, 607 (1998).\n[20] P. L'Ecuyer, J. Cordeau, and A. Compagner (1998),\nhttp://www.iro.umontreal.ca/\u223clecuyer/papers.html.\n[21] A. E. Ferdinand and M. E. Fisher, Phys. Rev. 185, 832 (1969).\n[22] TINA, http://tina.nat.uni-magdeburg.de.\n\nAcknowledgments\n\nAll numerical simulations have been done on our Beowulf\ncluster T INA[22]. This work was supported by Deutsche\nForschungsgemeinschaft under grant ME2044/1-1.\n\n\f"}