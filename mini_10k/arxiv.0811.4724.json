{"id": "http://arxiv.org/abs/0811.4724v1", "guidislink": true, "updated": "2008-11-28T14:32:49Z", "updated_parsed": [2008, 11, 28, 14, 32, 49, 4, 333, 0], "published": "2008-11-28T14:32:49Z", "published_parsed": [2008, 11, 28, 14, 32, 49, 4, 333, 0], "title": "Generalized power method for sparse principal component analysis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0811.3559%2C0811.4221%2C0811.2592%2C0811.2535%2C0811.1290%2C0811.2516%2C0811.2011%2C0811.0235%2C0811.2379%2C0811.2707%2C0811.0652%2C0811.4614%2C0811.4211%2C0811.0064%2C0811.0272%2C0811.1989%2C0811.1146%2C0811.0303%2C0811.0807%2C0811.1614%2C0811.4724%2C0811.4124%2C0811.3252%2C0811.1609%2C0811.1113%2C0811.2105%2C0811.3805%2C0811.4571%2C0811.1328%2C0811.2180%2C0811.4014%2C0811.1584%2C0811.0605%2C0811.0778%2C0811.3652%2C0811.0621%2C0811.3843%2C0811.4755%2C0811.3413%2C0811.0885%2C0811.4373%2C0811.1815%2C0811.0086%2C0811.4615%2C0811.0463%2C0811.2318%2C0811.1114%2C0811.2647%2C0811.3628%2C0811.4107%2C0811.2836%2C0811.2949%2C0811.1383%2C0811.0814%2C0811.0217%2C0811.2730%2C0811.1315%2C0811.0262%2C0811.1670%2C0811.1720%2C0811.3910%2C0811.1995%2C0811.0897%2C0811.1539%2C0811.1310%2C0811.2644%2C0811.3144%2C0811.0390%2C0811.0211%2C0811.0492%2C0811.2858%2C0811.4385%2C0811.3758%2C0811.3648%2C0811.4303%2C0811.3903%2C0811.2047%2C0811.0155%2C0811.1575%2C0811.1928%2C0811.3811%2C0811.1997%2C0811.3406%2C0811.3739%2C0811.3395%2C0811.4448%2C0811.1515%2C0811.0193%2C0811.2689%2C0811.0169%2C0811.2521%2C0811.1592%2C0811.0324%2C0811.3523%2C0811.4765%2C0811.0347%2C0811.3741%2C0811.1831%2C0811.1853%2C0811.0497%2C0811.1014&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Generalized power method for sparse principal component analysis"}, "summary": "In this paper we develop a new approach to sparse principal component\nanalysis (sparse PCA). We propose two single-unit and two block optimization\nformulations of the sparse PCA problem, aimed at extracting a single sparse\ndominant principal component of a data matrix, or more components at once,\nrespectively. While the initial formulations involve nonconvex functions, and\nare therefore computationally intractable, we rewrite them into the form of an\noptimization program involving maximization of a convex function on a compact\nset. The dimension of the search space is decreased enormously if the data\nmatrix has many more columns (variables) than rows. We then propose and analyze\na simple gradient method suited for the task. It appears that our algorithm has\nbest convergence properties in the case when either the objective function or\nthe feasible set are strongly convex, which is the case with our single-unit\nformulations and can be enforced in the block case. Finally, we demonstrate\nnumerically on a set of random and gene expression test problems that our\napproach outperforms existing algorithms both in quality of the obtained\nsolution and in computational speed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0811.3559%2C0811.4221%2C0811.2592%2C0811.2535%2C0811.1290%2C0811.2516%2C0811.2011%2C0811.0235%2C0811.2379%2C0811.2707%2C0811.0652%2C0811.4614%2C0811.4211%2C0811.0064%2C0811.0272%2C0811.1989%2C0811.1146%2C0811.0303%2C0811.0807%2C0811.1614%2C0811.4724%2C0811.4124%2C0811.3252%2C0811.1609%2C0811.1113%2C0811.2105%2C0811.3805%2C0811.4571%2C0811.1328%2C0811.2180%2C0811.4014%2C0811.1584%2C0811.0605%2C0811.0778%2C0811.3652%2C0811.0621%2C0811.3843%2C0811.4755%2C0811.3413%2C0811.0885%2C0811.4373%2C0811.1815%2C0811.0086%2C0811.4615%2C0811.0463%2C0811.2318%2C0811.1114%2C0811.2647%2C0811.3628%2C0811.4107%2C0811.2836%2C0811.2949%2C0811.1383%2C0811.0814%2C0811.0217%2C0811.2730%2C0811.1315%2C0811.0262%2C0811.1670%2C0811.1720%2C0811.3910%2C0811.1995%2C0811.0897%2C0811.1539%2C0811.1310%2C0811.2644%2C0811.3144%2C0811.0390%2C0811.0211%2C0811.0492%2C0811.2858%2C0811.4385%2C0811.3758%2C0811.3648%2C0811.4303%2C0811.3903%2C0811.2047%2C0811.0155%2C0811.1575%2C0811.1928%2C0811.3811%2C0811.1997%2C0811.3406%2C0811.3739%2C0811.3395%2C0811.4448%2C0811.1515%2C0811.0193%2C0811.2689%2C0811.0169%2C0811.2521%2C0811.1592%2C0811.0324%2C0811.3523%2C0811.4765%2C0811.0347%2C0811.3741%2C0811.1831%2C0811.1853%2C0811.0497%2C0811.1014&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper we develop a new approach to sparse principal component\nanalysis (sparse PCA). We propose two single-unit and two block optimization\nformulations of the sparse PCA problem, aimed at extracting a single sparse\ndominant principal component of a data matrix, or more components at once,\nrespectively. While the initial formulations involve nonconvex functions, and\nare therefore computationally intractable, we rewrite them into the form of an\noptimization program involving maximization of a convex function on a compact\nset. The dimension of the search space is decreased enormously if the data\nmatrix has many more columns (variables) than rows. We then propose and analyze\na simple gradient method suited for the task. It appears that our algorithm has\nbest convergence properties in the case when either the objective function or\nthe feasible set are strongly convex, which is the case with our single-unit\nformulations and can be enforced in the block case. Finally, we demonstrate\nnumerically on a set of random and gene expression test problems that our\napproach outperforms existing algorithms both in quality of the obtained\nsolution and in computational speed."}, "authors": ["Michel Journ\u00e9e", "Yurii Nesterov", "Peter Richt\u00e1rik", "Rodolphe Sepulchre"], "author_detail": {"name": "Rodolphe Sepulchre"}, "author": "Rodolphe Sepulchre", "arxiv_comment": "Submitted", "links": [{"href": "http://arxiv.org/abs/0811.4724v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0811.4724v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0811.4724v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0811.4724v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:0811.4724v1 [math.OC] 28 Nov 2008\n\nGeneralized power method\nfor sparse principal component analysis\nMichel Journ\u00e9e\u2217\n\nYurii Nesterov\u2020\n\nPeter Richt\u00e1rik\u2020\n\nRodolphe Sepulchre\u2217\n\nAbstract\nIn this paper we develop a new approach to sparse principal component analysis (sparse PCA).\nWe propose two single-unit and two block optimization formulations of the sparse PCA problem,\naimed at extracting a single sparse dominant principal component of a data matrix, or more components at once, respectively. While the initial formulations involve nonconvex functions, and are\ntherefore computationally intractable, we rewrite them into the form of an optimization program involving maximization of a convex function on a compact set. The dimension of the search space is\ndecreased enormously if the data matrix has many more columns (variables) than rows. We then propose and analyze a simple gradient method suited for the task. It appears that our algorithm has best\nconvergence properties in the case when either the objective function or the feasible set are strongly\nconvex, which is the case with our single-unit formulations and can be enforced in the block case.\nFinally, we demonstrate numerically on a set of random and gene expression test problems that our\napproach outperforms existing algorithms both in quality of the obtained solution and in computational speed.\nKeywords: sparse PCA, power method, gradient ascent, strongly convex sets, block algorithms\n\n1 Introduction\nPrincipal component analysis (PCA) is a well established tool for making sense of high dimensional\ndata by reducing it to a smaller dimension. It has applications virtually in all areas of science-machine\nlearning, image processing, engineering, genetics, neurocomputing, chemistry, meteorology, control theory, computer networks-to name just a few-where large data sets are encountered. It is important that\nhaving reduced dimension, the essential characteristics of the data are retained. If A \u2208 Rp\u00d7n is a matrix\nencoding p samples of n variables, with n being large, PCA aims at finding a few linear combinations of\nthese variables, called principal components, which point in orthogonal directions explaining as much of\nthe variance in the data as possible. If the variables contained in the columns of A are centered, then the\nclassical PCA can be written in terms of the scaled sample covariance matrix \u03a3 = AAT as follows:\nFind\n\nz \u2217 = arg max z T \u03a3z.\nz T z\u22641\n\n(1)\n\nExtracting one component amounts to computing the dominant eigenvector of \u03a3 (or, equivalently,\ndominant right singular vector of A). Full PCA involves the computation of the singular value decomposition (SVD) of A. Principal components are, in general, combinations of all the input variables, i.e.\n\u2217\n\nDepartment of Electrical Engineering and Computer Science, University of Li\u00e8ge, B-4000 Li\u00e8ge, Belgium. Email:\n[M.Journee, R.Sepulchre]@ulg.ac.be\n\u2020\nCenter for Operations Research and Econometrics, Catholic University of Louvain, Voie du Roman Pays 34, B-1348\nLouvain-la-Neuve, Belgium. Email: [Nesterov, Richtarik]@core.ucl.ac.be\n\n1\n\n\fthe loading vector z \u2217 is not expected to have many zero coefficients. In most applications, however,\nthe original variables have concrete physical meaning and PCA then appears especially interpretable if\nthe extracted components are composed only from a small number of the original variables. In the case\nof gene expression data, for instance, each variable represents the expression level of a particular gene.\nA good analysis tool for biological interpretation should be capable to highlight \"simple\" structures in\nthe genome-structures expected to involve a few genes only-that explain a significant amount of the\nspecific biological processes encoded in the data. Components that are linear combinations of a small\nnumber of variables are, quite naturally, usually easier to interpret. It is clear, however, that with this\nadditional goal, some of the explained variance has to be sacrificed. The objective of sparse principal\ncomponent analysis (sparse PCA) is to find a reasonable trade-off between these conflicting goals. One\nwould like to explain as much variability in the data as possible, using components constructed from as\nfew variables as possible. This is the classical trade-off between statistical fidelity and interpretability.\nFor about a decade, sparse PCA has been a topic of active research. Historically, the first suggested\napproaches were based on ad-hoc methods involving post-processing of the components obtained from\nclassical PCA. For example, Jolliffe [1995] considers using various rotation techniques to find sparse\nloading vectors in the subspace identified by PCA. Cadima and Jolliffe [1995] propose to simply set to\nzero the PCA loadings which are in absolute value smaller than some threshold constant.\nIn recent years, more involved approaches have been put forward-approaches that consider the\nconflicting goals of explaining variability and achieving representation sparsity simultaneously. These\nmethods usually cast the sparse PCA problem in the form of an optimization program, aiming at maximizing explained variance penalized for the number of non-zero loadings. For instance, the SCoTLASS\nalgorithm proposed by Jolliffe et al. [2003] aims at maximizing the Rayleigh quotient of the covariance matrix of the data under the l1 -norm based Lasso penalty (Tibshirani [1996]). Zou et al. [2006]\nformulate sparse PCA as a regression-type optimization problem and impose the Lasso penalty on the\nregression coefficients. d'Aspremont et al. [2007] in their DSPCA algorithm exploit convex optimization\ntools to solve a convex relaxation of the sparse PCA problem. Shen and Huang [2008] adapt the singular\nvalue decomposition (SVD) to compute low-rank matrix approximations of the data matrix under various\nsparsity-inducing penalties. Greedy methods, which are typical for combinatorial problems, have been\ninvestigated by Moghaddam et al. [2006]. Finally, d'Aspremont et al. [2008] propose a greedy heuristic\naccompanied with a certificate of optimality.\nIn many applications, several components need to be identified. The traditional approach consists of\nincorporating an existing single-unit algorithm in a deflation scheme, and computing the desired number of components sequentially (see, e.g., d'Aspremont et al. [2007]). In the case of Rayleigh quotient\nmaximization it is well-known that computing several components at once instead of computing them\none-by-one by deflation with the classical power method might present better convergence whenever the\nlargest eigenvalues of the underlying matrix are close to each other (see, e.g., Parlett [1980]). Therefore,\nblock approaches for sparse PCA are expected to be more efficient on ill-posed problems.\nIn this paper we consider two single-unit (Section 2.1 and 2.3) and two block formulations (Section\n2.3 and 2.4) of sparse PCA, aimed at extracting m sparse principal components, with m = 1 in the former case and p \u2265 m > 1 in the latter. Each of these two groups comes in two variants, depending on the\ntype of penalty we use to enforce sparsity-either l1 or l0 (cardinality). 1 While our basic formulations\n1\n\nOur single-unit cardinality-penalized formulation is identical to that of d'Aspremont et al. [2008].\n\n2\n\n\finvolve maximization of a nonconvex function on a space of dimension involving n, we construct reformulations that cast the problem into the form of maximization of a convex function on the unit Euclidean\nsphere in Rp (in the m = 1 case) or the Stiefel manifold2 in Rp\u00d7m (in the m > 1 case). The advantage of\nthe reformulation becomes apparent when trying to solve problems with many variables (n \u226b p), since\nwe manage to avoid searching a space of large dimension. At the same time, due to the convexity of the\nnew cost function we are able to propose and analyze the iteration-complexity of a simple gradient-type\nscheme, which appears to be well suited for problems of this form. In particular, we study (Section 3) a\nfirst-order method for solving an optimization problem of the form\nf \u2217 = max f (x),\nx\u2208Q\n\n(P)\n\nwhere Q is a compact subset of a finite-dimensional vector space and f is convex. It appears that our\nmethod has best theoretical convergence properties when either f or Q are strongly convex, which is the\ncase in the single unit case (unit ball is strongly convex) and can be enforced in the block case by adding\na strongly convex regularizing term to the objective function, constant on the feasible set. We do not,\nhowever, prove any results concerning the quality of the obtained solution. Even the goal of obtaining\na local maximizer is in general unattainable, and we must be content with convergence to a stationary\npoint.\nIn the particular case when Q is the unit Euclidean ball in Rp and f (x) = xT Cx for some p \u00d7 p\nsymmetric positive definite matrix C, our gradient scheme specializes to the power method, which aims\nat maximizing the Rayleigh quotient\nxT Cx\nR(x) = T\nx x\nand thus at computing the largest eigenvalue, and the corresponding eigenvector, of C.\nBy applying our general gradient scheme to our sparse PCA reformulations of the form (P), we obtain\nalgorithms (Section 4) with per-iteration computational cost O(npm).\nWe demonstrate on random Gaussian (Section 5.1) and gene expression data related to breast cancer\n(Section 5.2) that our methods are very efficient in practice. While achieving a balance between the\nexplained variance and sparsity which is the same as or superior to the existing methods, they are faster,\noften converging before some of the other algorithms manage to initialize. Additionally, in the case of\ngene expression data our approach seems to extract components with strongest biological content.\nNotation. For convenience of the reader, and at the expense of redundancy, some of the less standard\nnotation below is also introduced at the appropriate place in the text where it is used. Parameters m \u2264\np \u2264 n are actual values of dimensions of spaces used in the paper. In the definitions below, we use\nthese actual values (i.e. n, p and m) if the corresponding object we define is used in the text exclusively\nwith them; otherwise we make use of the dummy variables k (representing p or n in the text) and l\n(representing m, p or n in the text).\nWe will work with vectors and matrices of various sizes (Rk , Rk\u00d7l ). Given a vector y \u2208 Rk , its ith\ncoordinate is denoted by yi . For a matrix Y \u2208 Rk\u00d7l , yi is the ith column of Y and yij is the element of\nY at position (i, j).\n2\n\nStiefel manifold is the set of rectangular matrices with orthonormal columns.\n\n3\n\n\fBy E we refer to a finite-dimensional vector space; E\u2217 is its conjugate space, i.e. the space of all\nlinear functionals on E. By hs, xi we denote the action of s \u2208 E\u2217 on x \u2208 E. For a self-adjoint positive\ndefinite linear operator G : E \u2192 E\u2217 we define a pair of norms on E and E\u2217 as follows\ndef\n\nkxk = hGx, xi1/2 ,\nksk\u2217\n\nx \u2208 E,\n\ndef\n\n= hs, G\u22121 si1/2 ,\n\n(2)\n\ns \u2208 E\u2217 .\n\nAlthough the theory in Section 3 is developed in this general setting, the sparse PCA applications\nconsidered in this paper require either the choice E = E\u2217 = Rp (see Section 3.3 and problems (8) and\n(14) in Section 2) or E = E\u2217 = Rp\u00d7m (see Section 3.4 and problems (18) and (22) in Section 2). In\nboth cases we will let G be the corresponding identity operator for which we obtain\nhx, yi =\n\nX\n\nxi y i ,\n\ni\n\nhX, Y i = Tr X T Y,\n\nkxk = hx, xi1/2 =\n\nkXk = hX, Xi1/2\n\nX\ni\n\nx2i\n\n!1/2\n\n= kxk2 ,\n\n\uf8eb\n\uf8f61/2\nX\nx2ij \uf8f8 = kXkF ,\n=\uf8ed\nij\n\nx, y \u2208 Rp , and\n\nX, Y \u2208 Rp\u00d7m .\n\nThus in the vector setting we work with the standard Euclidean norm and in the matrix setting with\nthe Frobenius norm. The symbol Tr denotes the\nPtrace of its argument.\nFurthermore, for z \u2208 Rn we write kzk1 = i |zi | (l1 norm) and by kzk0 (l0 \"norm\") we refer to the\nnumber of nonzero coefficients, or cardinality, of z. By Sp we refer to the space of all p \u00d7 p symmetric\nmatrices; Sp+ (resp. Sp++ ) refers to the positive semidefinite (resp. definite) cone. Eigenvalues of matrix\nY are denoted by \u03bbi (Y ), largest eigenvalue by \u03bbmax (Y ). Analogous notation with the symbol \u03c3 refers to\nsingular values.\nBy B k = {y \u2208 Rk | y T y \u2264 1} (resp. S k = {y \u2208 Rk | y T y = 1}) we refer to the unit Euclidean\nball (resp. sphere) in Rk . If we write B and S , then these are the corresponding objects in E. The space\nof n \u00d7 m matrices with unit-norm columns will be denoted by\n[S n ]m = {Y \u2208 Rn\u00d7m | Diag(Y T Y ) = Im },\nwhere Diag(*) represents the diagonal matrix obtained by extracting the diagonal of the argument. Stiefel\nmanifold is the set of rectangular matrices of fixed size with orthonormal columns:\np\nSm\n= {Y \u2208 Rp\u00d7m | Y T Y = Im }.\n\nFor t \u2208 R we will further write sign(t) for the sign of the argument and t+ = max{0, t}.\n\n2 Some formulations of the sparse PCA problem\nIn this section we propose four formulations of the sparse PCA problem, all in the form of the general optimization framework (P). The first two deal with the single-unit sparse PCA problem and the remaining\ntwo are their generalizations to the block case.\n\n4\n\n\f2.1 Single-unit sparse PCA via l1 -penalty\nLet us consider the optimization problem\ndef\n\n\u03c6l1 (\u03b3) = maxn\n\n\u221a\n\nz\u2208B\n\nz T \u03a3z \u2212 \u03b3kzk1 ,\n\n(3)\n\nwith sparsity-controlling parameter \u03b3 \u2265 0 and sample covariance matrix \u03a3 = AT A.\nThe solution z \u2217 (\u03b3) of (3) in the case \u03b3 = 0 is equal to the right singular vector corresponding to\n\u03c3max (A), the largest singular value of A. It is the first principal component of the data matrix A. The\noptimal value of the problem is thus equal to\n\u03c6l1 (0) = (\u03bbmax (AT A))1/2 = \u03c3max (A).\nNote that there is no reason to expect this vector to be sparse. On the other hand, for large enough \u03b3, we\nwill necessarily have z \u2217 (\u03b3) = 0, obtaining maximal sparsity. Indeed, since\nP\nP\nk i zi ai k2\n|zi |kai k2\nkAzk2\n= max\n\u2264 max iP\n= max kai k2 = kai\u2217 k2 ,\nmax\ni\nz6=0\nz6=0\nz6=0 kzk1\nkzk1\ni |zi |\nwe get kAzk2 \u2212 \u03b3kzk1 < 0 for all nonzero vectors z whenever \u03b3 is chosen to be strictly bigger than\nkai\u2217 k2 . From now on we will assume that\n\u03b3 < kai\u2217 k2 .\n\n(4)\n\nNote that there is a trade-off between the value kAz \u2217 (\u03b3)k2 and the sparsity of the solution z \u2217 (\u03b3). The\npenalty parameter \u03b3 is introduced to \"continuously\" interpolate between the two extreme cases described\nabove, with values in the interval [0, kai\u2217 k2 ). It depends on the particular application whether sparsity is\nvalued more than the explained variance, or vice versa, and to what extent. Due to these considerations,\nwe will consider the solution of (3) to be a sparse principal component of A.\nReformulation. The reader will observe that the objective function in (3) is not convex, nor concave,\nand that the feasible set is of a high dimension if p \u226a n. It turns out that these shortcomings are overcome\nby considering the following reformulation:\n\u03c6l1 (\u03b3) = maxn kAzk2 \u2212 \u03b3kzk1\nz\u2208B\n\n= maxn maxp xT Az \u2212 \u03b3kzk1\nz\u2208B x\u2208B\n\n= maxp maxn\nx\u2208B z\u2208B\n\ni=1\n\n= maxp maxn\nx\u2208B z\u0304\u2208B\n\nn\nX\n\nn\nX\ni=1\n\n(5)\n\nzi (aTi x) \u2212 \u03b3|zi |\n|z\u0304i |(|aTi x| \u2212 \u03b3),\n\n(6)\n\nwhere zi = sign(aTi x)z\u0304i . In view of (4), there is some x \u2208 B n for which aTi x > \u03b3. Fixing such x,\nsolving the inner maximization problem for z\u0304 and then translating back to z, we obtain the closed-form\nsolution\nsign(aT x)[|aTi x| \u2212 \u03b3]+\n, i = 1, . . . , n.\n(7)\nzi\u2217 = zi\u2217 (\u03b3) = qP i\nn\nT x| \u2212 \u03b3]2\n[|a\n+\nk=1\nk\n5\n\n\fProblem (6) can therefore be written in the form\n\u03c62l1 (\u03b3)\n\nn\nX\n[|aTi x| \u2212 \u03b3]2+ .\n= maxp\nx\u2208S\n\n(8)\n\ni=1\n\nNote that the objective function is differentiable and convex, and hence all local and global maxima must\nlie on the boundary, i.e., on the unit Euclidean sphere S p . Also, in the case when p \u226a n, formulation (8)\nrequires to search a space of a much lower dimension than the initial problem (3).\nSparsity. In view of (7), an optimal solution x\u2217 of (8) defines a sparsity pattern of the vector z \u2217 . In\nfact, the coefficients of z \u2217 indexed by\nI = {i | |aTi x\u2217 | > \u03b3}\n\n(9)\n\nare active while all others must be zero. Geometrically, active indices correspond to the defining hyperplanes of the polytope\nD = {x \u2208 Rp | |aTi x| \u2264 1}\n\nthat are (strictly) crossed by the line joining the origin and the point x\u2217 /\u03b3. Note that it is possible to say\nsomething about the sparsity of the solution even without the knowledge of x\u2217 :\n\u03b3 \u2265 kai k2\n\nzi\u2217 (\u03b3) = 0,\n\n\u21d2\n\ni = 1, . . . , n.\n\n(10)\n\n2.2 Single-unit sparse PCA via cardinality penalty\nInstead of the l1 -penalization, d'Aspremont et al. [2008] consider the formulation\ndef\n\n\u03c6l0 (\u03b3) = maxn z T \u03a3z \u2212 \u03b3 kzk0 ,\n\n(11)\n\nz\u2208B\n\nwhich directly penalizes the number of nonzero components (cardinality) of the vector z.\nReformulation. The reasoning of the previous section suggests the reformulation\n\u03c6l0 (\u03b3) = maxp maxn (xT Az)2 \u2212 \u03b3kzk0 ,\n\n(12)\n\nx\u2208B z\u2208B\n\nwhere the maximization with respect to z \u2208 B n for a fixed x \u2208 B p has the closed form solution\n[sign((aTi x)2 \u2212 \u03b3)]+ aTi x\n,\nzi\u2217 = zi\u2217 (\u03b3) = qP\nn\nT x)2 \u2212 \u03b3)] (aT x)2\n[sign((a\n+ k\nk=1\nk\n\ni = 1, . . . , n.\n\n(13)\n\nIn analogy with the l1 case, this derivation assumes that\n\n\u03b3 < kai\u2217 k22 ,\nso that there is x \u2208 B n such that (aTi x)2 \u2212 \u03b3 > 0. Otherwise z \u2217 = 0 is optimal. Formula (13) is easily\nobtained by analyzing (12) separately for fixed cardinality values of z. Hence, problem (11) can be cast\nin the following form\nn\nX\n[(aTi x)2 \u2212 \u03b3]+ .\n\u03c6l0 (\u03b3) = maxp\n(14)\nx\u2208S\n\ni=1\n\n6\n\n\fAgain, the objective function is convex, albeit nonsmooth, and the new search space is of particular\ninterest if p \u226a n. A different derivation of (14) for the n = p case can be found in d'Aspremont et al.\n[2008].\nSparsity. Given a solution x\u2217 of (14), the set of active indices of z \u2217 is given by\nI = {i | (aTi x\u2217 )2 > \u03b3}.\nGeometrically, active indices correspond to the defining hyperplanes of the polytope\nD = {x \u2208 Rp | |aTi x| \u2264 1}\n\n\u221a\nthat are (strictly) crossed by the line joining the origin and the point x\u2217 / \u03b3. As in the l1 case, we have\n\u03b3 \u2265 kai k22\n\n\u21d2\n\nzi\u2217 (\u03b3) = 0,\n\ni = 1, . . . , n.\n\n(15)\n\nn\nm X\nX\n\n(16)\n\n2.3 Block sparse PCA via l1 -penalty\nConsider the following block generalization of (5),\ndef\n\n\u03c6l1 ,m (\u03b3) =\n\nT\n\nmax Tr(X AZN ) \u2212 \u03b3\n\np\nX\u2208Sm\nZ\u2208[S n ]m\n\nj=1 i=1\n\n|zij |,\n\nwhere \u03b3 \u2265 0 is a sparsity-controlling parameter and N = Diag(\u03bc1 , . . . , \u03bcm ), with positive entries on\nthe diagonal. The dimension m corresponds to the number of extracted components and is assumed to\nbe smaller or equal to the rank of the data matrix, i.e., m \u2264 Rank(A). It will be shown below that under\nsome conditions on the parameters \u03bci , the case \u03b3 = 0 recovers PCA. In that particular instance, any\nsolution Z \u2217 of (16) has orthonormal columns, although this is not explicitly enforced. For positive \u03b3,\nthe columns of Z \u2217 are not expected to be orthogonal anymore. Most existing algorithms for computing\nseveral sparse principal components, e.g., Zou et al. [2006], d'Aspremont et al. [2007], Shen and Huang\n[2008], also do not impose orthogonal loading directions. Simultaneously enforcing sparsity and orthogonality seems to be a hard (and perhaps questionable) task.\nReformulation. Since problem (16) is completely decoupled in the columns of Z, i.e.,\n\u03c6l1 ,m (\u03b3) = maxp\n\nX\u2208Sm\n\nm\nX\nj=1\n\nmax \u03bcj xTj Azj \u2212 \u03b3kzj k1 ,\n\nzj \u2208S n\n\nthe closed-form solution (7) of (5) is easily adapted to the block formulation (16):\nsign(aT xj )[\u03bcj |aTi xj | \u2212 \u03b3]+\n\u2217\n\u2217\n.\nzij\n= zij\n(\u03b3) = qP i\nn\nT x | \u2212 \u03b3]2\n[\u03bc\n|a\n+\nk=1 j k j\n\n(17)\n\nThis leads to the reformulation\n\n\u03c62l1 ,m (\u03b3) = maxp\n\nX\u2208Sm\n\nm X\nn\nX\n[\u03bcj |aTi xj | \u2212 \u03b3]2+ ,\nj=1 i=1\n\n7\n\n(18)\n\n\fp\nwhich maximizes a convex function f : Rp\u00d7m \u2192 R on the Stiefel manifold Sm\n.\n\u2217 is\nSparsity. A solution X \u2217 of (18) again defines the sparsity pattern of the matrix Z \u2217 : the entry zij\nactive if\n\u03bcj |aTi x\u2217j | > \u03b3,\n\nand equal to zero otherwise. For \u03b3 > maxi,j \u03bcj kai k2 , the trivial solution Z \u2217 = 0 is optimal.\nBlock PCA. For \u03b3 = 0, problem (18) can be equivalently written in the form\n\u03c62l1 ,m (0) = maxp Tr(X T AAT XN 2 ),\nX\u2208Sm\n\n(19)\n\nwhich has been well studied (see e.g., Brockett [1991] and Absil et al. [2008]). The solutions of (19)\nspan the dominant m-dimensional invariant subspace of the matrix AAT . Furthermore, if the parameters\n\u03bci are all distinct, the columns of X \u2217 are the m dominant eigenvectors of AAT , i.e., the m dominant lefteigenvectors of the data matrix A. The columns of the solution Z \u2217 of (16) are thus the m dominant right\nsingular vectors of A, i.e., the PCA loading vectors. Such a matrix N with distinct diagonal elements\nenforces the objective function in (19) to have isolated maximizers. In fact, if N = Im , any point X \u2217 U\nm is also a solution of (19). In the case of sparse PCA, i.e., \u03b3 > 0,\nwith X \u2217 a solution of (19) and U \u2208 Sm\nthe penalty term enforces isolated maximizers. The technical parameter N will thus be set to the identity\nmatrix in what follows.\n\n2.4 Block sparse PCA via cardinality penalty\nThe single-unit cardinality-penalized case can also be naturally extended to the block case:\ndef\n\n\u03c6l0 ,m (\u03b3) =\n\nmax Tr(Diag(X T AZN )2 ) \u2212 \u03b3kZk0 ,\n\np\nX\u2208Sm\nZ\u2208[S n ]m\n\n(20)\n\nwhere \u03b3 \u2265 0 is the sparsity inducing parameter and N = Diag(\u03bc1 , . . . , \u03bcm ) with positive entries on\nthe diagonal. In the case \u03b3 = 0, problem (22) is equivalent to (19) and therefore corresponds to PCA,\nprovided that all \u03bci are distinct.\nReformulation. Again, this block formulation is completely decoupled in the columns of Z,\n\u03c6l0 ,m (\u03b3) = maxp\n\nX\u2208Sm\n\nm\nX\nj=1\n\nmax (\u03bcj xTj Azj )2 \u2212 \u03b3kzj k0 ,\n\nzj \u2208S n\n\nso that the solution (13) of the single unit case provides the optimal columns zi :\n[sign((\u03bcj aTi xj )2 \u2212 \u03b3)]+ \u03bcj aTi xj\n\u2217\n\u2217\n.\nzij\n= zij\n(\u03b3) = qP\nn\nT x )2 \u2212 \u03b3)] \u03bc2 (aT x )2\n[sign((\u03bc\na\nj\nj\n+\nj\nk=1\nj k\nk\n\n(21)\n\nThe reformulation of problem (20) is thus\n\n\u03c6l0 ,m (\u03b3) = maxp\n\nX\u2208Sm\n\nm X\nn\nX\n[(\u03bcj aTi xj )2 \u2212 \u03b3]+ ,\nj=1 i=1\n\n8\n\n(22)\n\n\fp\nwhich maximizes a convex function f : Rp\u00d7m \u2192 R on the Stiefel manifold Sm\n.\n\u2217 of Z \u2217 are given by the condition\nSparsity. For a solution X \u2217 of (22), the active entries zij\n\n(\u03bcj aTi x\u2217j )2 > \u03b3.\nHence for \u03b3 > max \u03bcj kai k22 , the optimal solution of (20) is Z \u2217 = 0.\ni,j\n\n3 A gradient method for maximizing convex functions\nBy E we denote an arbitrary finite-dimensional vector space; E\u2217 is its conjugate, i.e. the space of all\nlinear functionals on E. We equip these spaces with norms given by (2).\nIn this section we propose and analyze a simple gradient-type method for maximizing a convex\nfunction f : E \u2192 R on a compact set Q:\nf \u2217 = max f (x).\nx\u2208Q\n\n(P)\n\nUnless explicitly stated otherwise, we will not assume f to be differentiable. By f \u2032 (x) we denote\nany subgradient of function f at x. By \u2202f (x) we denote its subdifferential.\nAt any point x \u2208 Q we introduce some measure for the first-order optimality conditions:\ndef\n\n\u2206(x) = maxhf \u2032 (x), y \u2212 xi.\ny\u2208Q\n\nClearly, \u2206(x) \u2265 0 and it vanishes only at the points where the gradient f \u2032 (x) belongs to the normal cone\nto the set Conv(Q) at x.3\nWe will use the following notation:\ndef\n\ny(x) \u2208 Arg maxhf \u2032 (x), y \u2212 xi.\ny\u2208Q\n\n(23)\n\n3.1 Algorithm\nConsider the following simple algorithmic scheme.\ndef\nNote that for example in the special case Q = r * S = r * {x \u2208 E | kxk = r} or\ndef\n\nQ = r * B = r * {x \u2208 E | kxk \u2264 r}, the main step of Algorithm 1 can be written in an explicit form:\ny(xk ) = xk+1 = r\n3\n\nG\u22121 f \u2032 (xk )\n.\nkf \u2032 (xk )k\u2217\n\n(24)\n\nThe normal cone to the set Conv(Q) at x \u2208 Q is smaller than the normal cone to the set Q. Therefore, the optimality\ncondition \u2206(x) = 0 is stronger than the standard one.\n\n9\n\n\fAlgorithm 1: Gradient scheme\ninput : Initial iterate x0 \u2208 E.\noutput: xk , approximate solution of (P)\nbegin\nk \u2190\u2212 0\nrepeat\nxk+1 \u2208 Arg max{f (xk ) + hf \u2032 (xk ), y \u2212 xk i | y \u2208 Q}\nk \u2190\u2212 k + 1\nuntil a stopping criterion is satisfied\nend\n\n3.2 Analysis\ndef\n\nOur first convergence result is straightforward. Denote \u2206k = min \u2206(xi ).\n0\u2264i\u2264k\n\n{xk }\u221e\nk=0\n\nbe generated by Algorithm 1 as applied to a convex function f . Then\nTheorem 1 Let sequence\nis\nmonotonically\nincreasing and lim \u2206(xk ) = 0. Moreover,\nthe sequence {f (xk )}\u221e\nk=0\nk\u2192\u221e\n\n\u2206k \u2264\n\nf \u2217 \u2212 f (x0 )\n.\nk+1\n\n(25)\n\nProof. From convexity of f we immediately get\nf (xk+1 ) \u2265 f (xk ) + hf \u2032 (xk ), xk+1 \u2212 xk i = f (xk ) + \u2206(xk ),\nand therefore, f (xk+1 ) \u2265 f (xk ) for all k. By summing up these inequalities for k = 0, 1, . . . , N \u2212 1,\nwe obtain\nk\nX\n\u2206(xi ),\nf \u2217 \u2212 f (x0 ) \u2265 f (xk ) \u2212 f (x0 ) \u2265\ni=0\n\nand the result follows.\nFor a sharper analysis, we need some technical assumptions on f and Q.\n\n\u0003\n\nAssumption 1 The norms of the subgradients of f are bounded from below on Q by a positive constant,\ni.e.\ndef\n\u03b4f =\nmin kf \u2032 (x)k\u2217 > 0.\n(26)\nx\u2208Q\nf \u2032 (x)\u2208\u2202f (x)\n\nThis assumption is not too binding because of the following result.\nProposition 2 Assume that there exists a point x\u0304 6\u2208 Q such that f (x\u0304) < f (x) for all x \u2208 Q. Then\n\u0014\n\u0015 \u0014\n\u0015\n\u03b4f \u2265 min f (x) \u2212 f (x\u0304) / max kx \u2212 x\u0304k > 0.\nx\u2208Q\n\nx\u2208Q\n\nProof. Because f is convex, for any x \u2208 Q we have\n\n0 < f (x) \u2212 f (x\u0304) \u2264 hf \u2032 (x), x \u2212 x\u0304i \u2264 kf \u2032 (x)k\u2217 * kx \u2212 x\u0304k.\n\n\u0003\nFor our next convergence result we need to assume either strong convexity of f or strong convexity\nof the set Conv(Q).\n10\n\n\fAssumption 2 Function f is strongly convex, i.e. there exists a constant \u03c3f > 0 such that for any\nx, y \u2208 E\n\u03c3f\nf (y) \u2265 f (x) + hf \u2032 (x), y \u2212 xi +\nky \u2212 xk2 .\n(27)\n2\nConvex functions satisfy this inequality for convexity parameter \u03c3f = 0.\nAssumption 3 The set Conv(Q) is strongly convex. This means that there exists a constant \u03c3Q > 0\nsuch that for any x, y \u2208 Conv(Q) and \u03b1 \u2208 [0, 1] the following inclusion holds:\n\u03b1x + (1 \u2212 \u03b1)y +\n\n\u03c3Q\n\u03b1(1 \u2212 \u03b1)kx \u2212 yk2 * S \u2282 Conv(Q).\n2\n\n(28)\n\nConvex sets satisfy this inclusion for convexity parameter \u03c3Q = 0. It can be shown (see Appendix),\nthat level sets of strongly convex functions with Lipschitz continuous gradient are again strongly convex.\nAn example of such a function is the simple quadratic x 7\u2192 kxk2 . The level sets of this function\ncorrespond to Euclidean balls of varying sizes.\nAs we will see in Theorem 4, a better analysis of Algorithm 1 is possible if Conv(Q), the convex\nhull of the feasible set of problem (P), is strongly convex. Note that in the case of the two formulations\n(8) and (14) of the sparse PCA problem, the feasible set Q is the unit Euclidean sphere. Since the convex\nhull of the unit sphere is the unit ball, which is a strongly convex set, the feasible set of our sparse PCA\nformulations satisfies Assumption 3.\nIn the special case Q = r * S for some r > 0, there is a simple proof that Assumption 3 holds with\n\u03c3Q = 1r . Indeed, for any x, y \u2208 E and \u03b1 \u2208 [0, 1], we have\nk\u03b1x + (1 \u2212 \u03b1)yk2 = \u03b12 kxk2 + (1 \u2212 \u03b1)2 kyk2 + 2\u03b1(1 \u2212 \u03b1)hGx, yi\n= \u03b1kxk2 + (1 \u2212 \u03b1)kyk2 \u2212 \u03b1(1 \u2212 \u03b1)kx \u2212 yk2 .\nThus, for x, y \u2208 r * S we obtain:\n\u0002\n\u00031/2\n1\nk\u03b1x + (1 \u2212 \u03b1)yk = r 2 \u2212 \u03b1(1 \u2212 \u03b1)kx \u2212 yk2\n\u2264 r \u2212 \u03b1(1 \u2212 \u03b1)kx \u2212 yk2 .\n2r\n\nHence, we can take \u03c3Q = 1r .\n\nThe relevance of Assumption 3 is justified by the following technical observation.\nProposition 3 Let Assumption 3 be satisfied. Then for any x \u2208 Q the following holds:\n\u2206(x) \u2265\n\n\u03c3Q \u2032\nkf (x)k\u2217 * ky(x) \u2212 xk2 .\n2\n\nProof. Fix an arbitrary x \u2208 Q. Note that\nhf \u2032 (x), y(x) \u2212 yi \u2265 0,\n\ny \u2208 Conv(Q).\n\nWe will use this inequality for\ndef\n\ny = y\u03b1 = x + \u03b1(y(x) \u2212 x) +\n\nG\u22121 f \u2032 (x)\n\u03c3Q\n\u03b1(1 \u2212 \u03b1)ky(x) \u2212 xk2 *\n, \u03b1 \u2208 [0, 1].\n2\nkf \u2032 (x)k\u2217\n11\n\n(29)\n\n\fIn view of Assumption 3, y\u03b1 \u2208 Conv(Q). Therefore,\n\n\u03c3Q\n\u03b1(1 \u2212 \u03b1)ky(x) \u2212 xk2 * kf \u2032 (x)k\u2217 .\n2\n\n0 \u2265 hf \u2032 (x), y\u03b1 \u2212 y(x)i = (1 \u2212 \u03b1)hf \u2032 (x), x \u2212 y(x)i +\nSince \u03b1 is an arbitrary value from [0, 1], the result follows.\n\n\u0003\n\nWe are now ready to refine our analysis of Algorithm 1.\nTheorem 4 (Convergence) Let f be convex and let Assumption 1 and at least one of Assumptions 2 and\n3 be satisfied. If {xk } is the sequence of points generated by Algorithm 1, then\nN\nX\nk=0\n\nkxk+1 \u2212 xk k2 \u2264\n\n2(f \u2217 \u2212 f (x0 ))\n.\n\u03c3Q \u03b4f + \u03c3f\n\n(30)\n\nProof. Indeed, in view of our assumptions and Proposition 3, we have\nf (xk+1 ) \u2212 f (xk ) \u2265 \u2206(xk ) +\n\n\u03c3f\n1\nkxk+1 \u2212 xk k2 \u2265 (\u03c3Q \u03b4f + \u03c3f )kxk+1 \u2212 xk k2 .\n2\n2\n\n\u0003\nWe cannot in general guarantee that the algorithm will converge to a unique local maximizer. In\nparticular, if started from a local minimizer, the method will not move away from this point. However,\nthe above statement guarantees that the set of its limit points is connected and all of them satisfy the\nfirst-order optimality condition.\n\n3.3 Maximization with spherical constraints\nConsider E = E\u2217 = Rp with G = Ip and hs, xi =\n\nP\n\ni s i xi ,\n\nand let\n\nQ = r * S p = {x \u2208 Rp | kxk = r}.\nProblem (P) takes on the form:\nf \u2217 = maxp f (x).\nx\u2208r*S\n\nSince Q is strongly convex (\u03c3Q = 1r ), Theorem 4 is meaningful for any convex function f (\u03c3f \u2265 0). We\nhave already noted (see (24)) that the main step of Algorithm 1 can be written down explicitly. Note that\nthe single-unit sparse PCA formulations (8) and (14) conform to this setting. The following examples\nillustrate the connection to classical algorithms.\nExample 5 (Power method) In the special case of a quadratic objective function f (x) =\nsome C \u2208 Sp++ on the unit sphere (r = 1), we have\n\n1 T\n2 x Cx\n\nfor\n\nf \u2217 = 12 \u03bbmax (C),\nand Algorithm 1 is equivalent to the power iteration method for computing the largest eigenvalue of C\n(Golub and Van Loan [1996]). Hence for Q = S p , we can think of our scheme as a generalization of the\npower method. Indeed, our algorithm performs the following iteration:\nxk+1 =\n\nCxk\n,\nkCxk k\n12\n\nk \u2265 0.\n\n\fNote that both \u03b4f and \u03c3f are equal to the smallest eigenvalue of C, and hence the right-hand side of (30)\nis equal to\n\u03bbmax (C) \u2212 xT0 Cx0\n.\n(31)\n2\u03bbmin (C)\nExample 6 (Shifted power method) If C is not positive semidefinite in the previous example, the objective function is not convex and our results are not applicable. However, this complication can be\ncircumvented by instead running the algorithm with the shifted quadratic function\n1\nf\u02c6(x) = xT (C + \u03c9Ip )x,\n2\nwhere \u03c9 > 0 satisfies \u0108 = \u03c9Ip + C \u2208 Sp++ . On the feasible set, this change only adds a constant term\nto the objective function. The method, however, produces different sequence of iterates. Note that the\nconstants \u03b4f and \u03c3f are also affected and, correspondingly, the estimate (31).\n\n3.4 Maximization with orthonormality constraints\nConsider E = E\u2217 = Rp\u00d7m , the space of p \u00d7 m real matrices, with m \u2264 p. Note that for m = 1\nwe recover the setting of the previous section. We assume this space is equipped with the trace inner\ndef\nproduct: hX, Y i = Tr(X T Y ). The induced norm, denoted by kXkF = hX, Xi1/2 , is the Frobenius\nnorm (we let G be the identity operator). We can now consider various feasible sets, the simplest being\na ball or a sphere. Due to nature of applications in this paper, let us concentrate on the situation when Q\n\u221a\np\n:\nis a special subset of the sphere with radius r = m, the Stiefel manifold Sm\np\nQ = Sm\n= {X \u2208 Rp\u00d7m | X T X = Im }.\n\nProblem (P) then takes on the following form:\nf \u2217 = maxp f (X).\nX\u2208Sm\n\nNote that Conv(Q) is not strongly convex (\u03c3Q = 0), and hence Theorem 4 is meaningful only if f is\nstrongly convex (\u03c3f > 0). At every iteration, the algorithm needs to maximize a linear function over the\nStiefel manifold. The following standard result shows how this can be done.\nProposition 7 Let C \u2208 Rp\u00d7m , with m \u2264 p, and denote by \u03c3i (C), i = 1, . . . , m, the singular values of\nC. Then\nm\nX\nT\n1/2\n\u03c3i (C),\n(32)\nmaxp hC, Xi = Tr[(C C) ] =\nX\u2208Sm\n\nand a maximizer\n\nX\u2217\n\ni=1\n\nis given by the U factor in the polar decomposition of C:\nC = U P,\n\np\nU \u2208 Sm\n, P \u2208 Sm\n+.\n\nIf C is of full rank, then we can take X \u2217 = C(C T C)\u22121/2 .\n\n13\n\n\fProof. Existence of the polar factorization in the nonsquare case is covered by Theorem 7.3.2 in Horn and Johnson\n[1985]. Let C = V \u03a3W T be the singular value decomposition of A; that is, V is p \u00d7 p orthonormal, W\nis m \u00d7 m orthonormal, and \u03a3 is p \u00d7 m diagonal with values \u03c3i (A) on the diagonal. Then\nmax hC, Xi = maxp hV \u03a3W T , Xi\n\np\nX\u2208Sm\n\nX\u2208Sm\n\n= maxp Tr \u03a3(W T X T V )\nX\u2208Sm\n\nT\n\n= maxp Tr \u03a3Z = maxp\nZ\u2208Sm\n\nthat\n\nZ\u2208Sm\n\nm\nX\ni=1\n\n\u03c3i (C)zii \u2264\n\nm\nX\n\n\u03c3i (C).\n\ni\n\np\nThe third equality follows since the function X 7\u2192 V T XW maps Sm\nonto itself. It remains to note\n\nhC, U i = Tr P =\n\nX\ni\n\n\u03bbi (P ) =\n\nX\n\n\u03c3i (P ) = Tr(P T P )1/2 = Tr(C T C)1/2 =\n\ni\n\nX\n\n\u03c3i (C),\n\ni\n\nFinally, in the full rank case we have hC, X \u2217 i = Tr C T C(C T C)\u22121/2 = Tr(C T C)1/2 .\n\n\u0003\nIn the sequel, the symbol Uf(C) will be used to denote the U factor of the polar decomposition of\nmatrix C \u2208 Rp\u00d7m , or equivalently, Uf(C) = C(C T C)\u22121/2 if C is of full rank. In view of the above\nresult, the main step of Algorithm 1 can be written in the form\nxk+1 = Uf(f \u2032 (xk )).\n\n(33)\n\nNote that the block sparse PCA formulations (18) and (22) conform to this setting. Here is one more\nexample:\nExample 8 (Rectangular Procrustes Problem) Let C, X \u2208 Rp\u00d7m and D \u2208 Rp\u00d7p and consider the\nfollowing problem:\n(34)\nmin{kC \u2212 DXk2F | X T X = Im }.\n\nSince kC \u2212DXk2F = kCk2F +hDX, DXi\u22122hCD, Xi, by a similar shifting technique as in the previous\nexample we can cast problem (34) in the following form\nmax{\u03c9kXk2F \u2212 hDX, DXi + 2hCD, Xi | X T X = Im }.\nFor \u03c9 > 0 large enough, the new objective function will be strongly convex. In this case our algorithm\nbecomes similar to the gradient method proposed by Fraikin et al. [2008].\nThe standard Procrustes problem in the literature is a special case of (34) with p = m.\n\n4 Algorithms for sparse PCA\nThe application of our general method (Algorithm 1) to the four sparse PCA formulations of Section\n2, i.e., (8), (14), (18) and (22), leads to Algorithms 2, 3, 4 and 5 below, that provide a locally optimal\n\n14\n\n\fpattern of sparsity for a matrix Z \u2208 [S n ]m .4 This pattern is defined as a matrix P \u2208 Rn\u00d7m such that\npij = 0 if the loading zij is active and pij = 1 otherwise. So P is an indicator of the coefficients of Z\nthat are zeroed by our method. The computational complexity of the single-unit algorithms (Algorithms\n2 and 3) is O(np) operations per iteration. The block algorithms (Algorithms 4 and 5) have complexity\nO(npm) per iteration.\n\n4.1 Methods for pattern-finding\nAlgorithm 2: Single-unit sparse PCA method based on the l1 -penalty (8)\ninput : Data matrix A \u2208 Rp\u00d7n\nSparsity-controlling parameter \u03b3 \u2265 0\nInitial iterate x \u2208 S p\noutput: A locally optimal sparsity pattern P\nbegin\nrepeat\nP\nx \u2190\u2212 ni=1 [|aTi x| \u2212 \u03b3]+ sign(aTi x)ai\nx\nx \u2190\u2212 kxk\nuntil a stopping criterion is satisfied\n\u001a\npi = 0\nConstruct vector P \u2208 Rn such that\npi = 1\nend\n\nif |aTi x| > \u03b3\notherwise.\n\nAlgorithm 3: Single-unit sparse PCA algorithm based on the l0 -penalty (14)\ninput : Data matrix A \u2208 Rp\u00d7n\nSparsity-controlling parameter \u03b3 \u2265 0\nInitial iterate x \u2208 S p\noutput: A locally optimal sparsity pattern P\nbegin\nrepeat\nP\nx \u2190\u2212 ni=1 [sign((aTi x)2 \u2212 \u03b3)]+ aTi x ai\nx\nx \u2190\u2212 kxk\nuntil a stopping criterion is satisfied\n\u001a\npi = 0 if (aTi x)2 > \u03b3\nConstruct vector P \u2208 Rn such that\npi = 1 otherwise.\nend\n\n4\nThis section discusses the general block sparse PCA problem. The single-unit case corresponds to the particular case\nm = 1.\n\n15\n\n\fAlgorithm 4: Block Sparse PCA algorithm based on the l1 -penalty (18)\ninput : Data matrix A \u2208 Rp\u00d7n\nSparsity-controlling parameter \u03b3 \u2265 0\np\nInitial iterate X \u2208 Sm\noutput: A locally optimal sparsity pattern P\nbegin\nrepeat\nfor j = 1, . P\n. . , m do\nxj \u2190\u2212 ni=1 [|aTi xj | \u2212 \u03b3]+ sign(aTi x)ai\nX \u2190\u2212 Uf(X)\nuntil a stopping criterion is satisfied\n\u001a\npij = 0 if |aTi xj | > \u03b3\nn\u00d7m\nConstruct matrix P \u2208 R\nsuch that\npij = 1 otherwise.\nend\nAlgorithm 5: Block Sparse PCA algorithm based on the l0 -penalty (22)\ninput : Data matrix A \u2208 Rp\u00d7n\nSparsity-controlling parameter \u03b3 \u2265 0\np\nInitial iterate X \u2208 Sm\noutput: A locally optimal sparsity pattern P\nbegin\nrepeat\nfor j = 1, . P\n. . , m do\nxj \u2190\u2212 ni=1 [sign((aTi xj )2 \u2212 \u03b3)]+ aTi xj ai\nX \u2190\u2212 Uf(X)\nuntil a stopping criterion is satisfied\n\u001a\npij = 0 if (aTi xj )2 > \u03b3\nn\u00d7m\nConstruct matrix P \u2208 R\nsuch that\npij = 1 otherwise.\nend\n\n4.2 Post-processing\nOnce a \"good\" sparsity pattern P has been identified, the active entries of Z still have to be filled. To\nthis end, we consider the optimization problem,\ndef\n\n(X \u2217 , Z \u2217 ) = arg maxp Tr(X T AZN ),\nX\u2208Sm\nZ\u2208[S n ]m\nZP =0\n\n(35)\n\nwhere ZP denotes the entries of Z that are constrained to zero and N = Diag(\u03bc1 , . . . , \u03bcm ) with strictly\npositive \u03bci . Problem (35) assigns the active part of the loading vectors Z to maximize the variance\nexplained by the resulting components. By ZP\u0304 , we refer to the complement of ZP , i.e., to the active\nentries of Z. In the single-unit case m = 1, an explicit solution of (35) is available,\nX \u2217 = u,\nZP\u0304\u2217 = v and ZP\u2217 = 0,\n16\n\n(36)\n\n\fwhere \u03c3uv T with \u03c3 > 0, u \u2208 B p and v \u2208 B kP\u0304 k0 is a rank one singular value decomposition of the matrix\nAP\u0304 , that corresponds to the submatrix of A containing the columns related to the active entries.\nAlthough an exact solution of (35) is hard to compute in the block case m > 1, a local maximizer\ncan be efficiently computed by optimizing alternatively with respect to one variable while keeping the\nother ones fixed. The following lemmas provide an explicit solution to each of these subproblems.\nLemma 9 For a fixed Z \u2208 [S n ]m , a solution X \u2217 of\nmax Tr(X T AZN )\n\np\nX\u2208Sm\n\nis provided by the U factor of the polar decomposition of the product AZN .\n\u0003\n\nProof. See Proposition 7.\nLemma 10 The solution\n\ndef\n\nZ \u2217 = arg max\nTr(X T AZN ),\nn m\nZ\u2208[S ]\nZP =0\n\n(37)\n\np\nis at any point X \u2208 Sm\ndefined by the two conditions ZP\u0304\u2217 = (AT XN D)P\u0304 and ZP\u2217 = 0, where D is a\npositive diagonal matrix that normalizes each column of Z \u2217 to unit norm, i.e.,\n1\n\nD = Diag(N X T AAT XN )\u2212 2 .\nProof. The Lagrangian of the optimization problem (37) is\nL(Z, \u039b1 , \u039b2 ) = Tr(X T AZN ) \u2212 Tr(\u039b1 (Z T Z \u2212 Im )) \u2212 Tr(\u039bT2 Z),\nwhere the Lagrangian multipliers \u039b1 \u2208 Rm\u00d7m and \u039b2 \u2208 Rn\u00d7m have the following properties: \u039b1 is an\ninvertible diagonal matrix and (\u039b2 )P\u0304 = 0. The first order optimality conditions of (37) are thus\nAT XN \u2212 2Z\u039b1 \u2212 \u039b2 = 0\n\nDiag(Z T Z) = Im\nZP = 0.\n\nHence, any stationary point Z \u2217 of (37) satisfies ZP\u0304\u2217 = (AT XN D)P\u0304 and ZP\u2217 = 0, where D is a diagonal\nmatrix that normalizes the columns of Z \u2217 to unit norm. The second order optimality condition imposes\n1\nthe diagonal matrix D to be positive. Such a D is unique and given by D = Diag(N X T AAT XN )\u2212 2 . \u0003\nThe alternating optimization scheme is summarized in Algorithm 6, which computes a local solution\nof (35). It should be noted that Algorithm 6 is a postprocessing heuristic that, strictly speaking, is\nrequired only for the l1 block formulation (Algorithm 4). In fact, since the cardinality penalty only\ndepends on the sparsity pattern P and not on the actual values assigned to ZP\u0304 , a solution (X \u2217 , Z \u2217 ) of\nAlgorithms 3 or 5 is also a local maximizer of (35) for the resulting pattern P . This explicit solution\nprovides a good alternative to Algorithm 6. In the single unit case with l1 penalty (Algorithm 2), the\nsolution (36) is available.\n\n17\n\n\fAlgorithm 6: Alternating optimization scheme for solving (35)\ninput : Data matrix A \u2208 Rp\u00d7n\nSparsity pattern P \u2208 Rn\u00d7m\nMatrix N = Diag(\u03bc1 , . . . , \u03bcm )\np\nInitial iterate X \u2208 Sm\noutput: A local minimizer (X, Z) of (35)\nbegin\nrepeat\nZ \u2190\u2212 AT XN\n1\nZ \u2190\u2212 Z Diag(Z T Z)\u2212 2\nZP \u2190\u2212 0\nX \u2190\u2212 Uf(AZN )\nuntil a stopping criterion is satisfied\nend\n\nGPowerl1\nGPowerl0\nGPowerl1 ,m\nGPowerl0 ,m\n\nComputation of P\nAlgorithm 2\nAlgorithm 3\nAlgorithm 4\nAlgorithm 5\n\nComputation of ZP\u0304\nEquation (36)\nEquation (13)\nAlgorithm 6\nEquation (21)\n\nTable 1: New algorithms for sparse PCA.\n\n4.3 Sparse PCA algorithms\nTo sum up, in this paper we propose four sparse PCA algorithms, each combining a method to identify\na \"good\" sparsity pattern with a method to fill the active entries of the m loading vectors. They are\nsummarized in Table 1.5\n\n4.4 Deflation scheme.\nFor the sake of completeness, we recall a classical deflation process for computing m sparse principal components with a single-unit algorithm (d'Aspremont et al. [2007]). Let z \u2208 Rn be a unit-norm\nsparse loading vector of the data A. Subsequent directions can be sequentially obtained by computing a\ndominant sparse component of the residual matrix A \u2212 xz T , where x = Az is the vector that solves\nmin kA \u2212 xz T kF .\n\nx\u2208Rp\n\n5 Numerical experiments\nIn this section, we evaluate the proposed power algorithms against existing sparse PCA methods. Three\ncompeting methods are considered in this study: a greedy scheme aimed at computing a local maximizer\n5\n\nOur algorithms are named GPower where the \"G\" stands for generalized or gradient.\n\n18\n\n\fof (11) (d'Aspremont et al. [2008]), the SPCA algorithm (Zou et al. [2006]) and the sPCA-rSVD algorithm (Shen and Huang [2008]). We do not include the DSPCA algorithm (d'Aspremont et al. [2007])\nin our numerical study. This method solves a convex relaxation of the sparse PCA problem and has a\nlarge computational complexity of O(n3 ) compared to the other methods. Table 2 lists the considered\nalgorithms.\nGPowerl1\nGPowerl0\nGPowerl1 ,m\nGPowerl0 ,m\nGreedy\nSPCA\nrSVDl1\nrSVDl0\n\nSingle-unit sparse PCA via l1 -penalty\nSingle-unit sparse PCA via l0 -penalty\nBlock sparse PCA via l1 -penalty\nBlock sparse PCA via l0 -penalty\nGreedy method\nSPCA algorithm\nsPCA-rSVD algorithm with an l1 -penalty (\"soft thresholding\")\nsPCA-rSVD algorithm with an l0 -penalty (\"hard thresholding\")\n\nTable 2: Sparse PCA algorithms we compare in this section.\nThese algorithms are compared on random data (Section 5.1) as well as on real data (Section 5.2).\nAll numerical experiments are performed in MATLAB. Our implementations of the GPower algorithms\nare initialized at a point for which the associated sparsity pattern has at least one active element. In case\nof the single-unit algorithms, such an initial iterate x \u2208 S p is chosen parallel to the column of A with the\nlargest norm, i.e.,\nai \u2217\n, where i\u2217 = arg max kai k2 .\n(38)\nx=\ni\nkai\u2217 k2\np\nFor the block GPower algorithms, a suitable initial iterate X \u2208 Sm\nis constructed in a block-wise manner\np\nas X = [x|X\u22a5 ], where x is the unit-norm vector (38) and X\u22a5 \u2208 Sm\u22121\nis orthogonal to x, i.e., xT X\u22a5 =\n0. We stop the GPower algorithms once the relative change of the objective function is small:\n\nf (xk+1 ) \u2212 f (xk )\n\u2264 \u01eb = 10\u22124 .\nf (xk )\nMATLAB implementations of the SPCA algorithm and the greedy algorithm have been rendered available\nby Zou et al. [2006] and d'Aspremont et al. [2008]. We have, however, implemented the sPCA-rSVD algorithm on our own (Algorithm 1 in Shen and Huang [2008]), and use it with the same stopping criterion\nas for the GPower algorithms. This algorithm initializes with the best rank-one approximation of the data\nmatrix. This is done with the svds function in MATLAB.\nGiven a data matrix A \u2208 Rp\u00d7n , the considered sparse PCA algorithms provide m unit-norm sparse\nloading vectors stored in the matrix Z \u2208 [S n ]m . The samples of the associated components are provided\nby the m columns of the product AZ. The variance explained by these m components is an important comparison criterion of the algorithms. In the simple case m = 1, the variance explained by the\ncomponent Az is\nVar(z) = z T AT Az.\nWhen z corresponds to the first principal loading vector, the variance is Var(z) = \u03c3max (A)2 . In the case\nm > 1, the derived components are likely to be correlated. Hence, summing up the variance explained\nindividually by each of the components overestimates the variance explained simultaneously by all the\ncomponents. This motivates the notion of adjusted variance proposed by Zou et al. [2006]. The adjusted\nvariance of the m components Y = AZ is defined as\nAdjVar Z = Tr R2 ,\n19\n\n\fp\nwhere Y = QR is the QR decomposition of the components sample matrix Y (Q \u2208 Sm\nand R is an\nm \u00d7 m upper triangular matrix).\n\n5.1 Random test problems\nAll random data matrices A \u2208 Rp\u00d7n considered in this section are generated according to a Gaussian\ndistribution, with zero mean and unit variance.\nTrade-off curves. Let us first compare the single-unit algorithms, which provide a unit-norm sparse\nloading vector z \u2208 Rn . We first plot the variance explained by the extracted component against the\ncardinality of the resulting loading vector z. For each algorithm, the sparsity-inducing parameter is\nincrementally increased to obtain loading vectors z with a cardinality that decreases from n to 1. The\nresults displayed in Figure 1 are averages of computations on 100 random matrices with dimensions p =\n100 and n = 300. The considered sparse PCA methods aggregate in two groups: GPower l1 , GPower l0 ,\nGreedy and rSVDl0 outperform the SPCA and the rSVDl1 approaches. It seems that these latter methods\nperform worse because of the l1 penalty term used in them. If one, however, post-processes the active\npart of z according to (36), as we do in GPower l1 , all sparse PCA methods reach the same performance.\n\nProportion of explained variance\n\n1\n0.9\n0.8\n0.7\n0.6\nGPowerl1\n\n0.5\n\nGPowerl0\n\n0.4\n\nGreedy\n\n0.3\n\nSPCA\n0.2\n\nrSVD l1\n\n0.1\n\nrSVD l0\n\n0\n\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\nCardinality\n\nFigure 1: Trade-off curves between explained variance and cardinality. The vertical axis is the ratio Var(zsPCA )/ Var(zPCA ), where the loading vector zsPCA is computed by sparse PCA and zPCA\nis the first principal loading vector. The considered algorithms aggregate in two groups: GPowerl1 ,\nGPower l0 , Greedy and rSVDl0 (top curve), and SPCA and rSVDl1 (bottom curve). For a fixed cardinality value, the methods of the first group explain more variance. Postprocessing algorithms SPCA\nand rSVDl1 with equation (36), results, however, in the same performance as the other algorithms.\nControlling sparsity with \u03b3. Among the considered methods, the greedy approach is the only one to\ndirectly control the cardinality of the solution, i.e., the desired cardinality is an input of the algorithm. The\nother methods require a parameter controlling the trade-off between variance and cardinality. Increasing\nthis parameter leads to solutions with smaller cardinality, but the resulting number of nonzero elements\ncan not be precisely predicted. In Figure 2, we plot the average relationship between the parameter \u03b3 and\nthe resulting cardinality of the loading vector z for the two algorithms GPower l1 and GPower l0 . In view\n20\n\n\fof (10) (resp. (15)), the entries i of the loading vector z obtained by the GPower l1 algorithm (resp. the\nGPower l0 algorithm) satisfying\n(resp. kai k22 \u2264 \u03b3)\n\nkai k2 \u2264 \u03b3\n\n(39)\n\nhave to be zero. Taking into account the distribution of the norms of the columns of A, this provides for\nevery \u03b3 a theoretical upper bound on the expected cardinality of the resulting vector z.\n\n1\n\nProportion of nonzero entries\n\n0.9\n0.8\n0.7\nTheoretical upper bound\n0.6\n\nGPowerl1\n\n0.5\n\nGPowerl0\n\n0.4\n0.3\n0.2\n0.1\n0\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nNormalized sparsity inducing parameter\n\nFigure 2: Dependence of cardinality on the value of the sparsity-inducing parameter \u03b3. In case of the\nGPower l1 algorithm, the horizontal axis shows \u03b3/kai\u2217 k2 , whereas for the GPowerl0 algorithm, we use\n\u221a\n\u03b3/kai\u2217 k2 . The theoretical upper bound is therefor identical for both methods. The plots are averages\nbased on 100 test problems of size p = 100 and n = 300.\n\nGreedy versus the rest. The considered sparse PCA methods feature different empirical computational complexities. In Figure 3, we display the average time required by the sparse PCA algorithms\nto extract one sparse component from Gaussian matrices of dimensions p = 100 and n = 300. One\nimmediately notices that the greedy method slows down significantly as cardinality increases, whereas\nthe speed of the other considered algorithms does not depend on cardinality. Since on average Greedy is\nmuch slower than the other methods, even for low cardinalities, we discard it from all following numerical experiments.\nSpeed and scaling test. In Tables 3 and 4 we compare the speed of the remaining algorithms. Table\n3 deals with problems with a fixed aspect ratio n/p = 10, whereas in Table 4, p is fixed at 500, and\nexponentially increasing values of n are considered. For the GPower l1 method, the sparsity inducing\nparameter \u03b3 was set to 10% of the upper bound \u03b3max = kai\u2217 k2 . For the GPower l0 method, \u03b3 was set\nto 1% of \u03b3max = kai\u2217 k22 in order to aim for solutions of comparable cardinalities (see (39)). These\ntwo parameters have also been used for the rSVDl1 and the rSVDl0 methods, respectively. Concerning\nSPCA, the sparsity parameter has been chosen by trial and error to get, on average, solutions with similar\ncardinalities as obtained by the other methods. The values displayed in Tables 3 and 4 correspond to the\naverage running times of the algorithms on 100 test instances for each problem size. In both tables, the\nnew methods GPower l1 and GPower l0 are the fastest. The difference in speed between GPower l1 and\n21\n\n\f7\nGPowerl1\n\ncomputational time [sec]\n\n6\n\nGPowerl0\nGreedy\n\n5\n\nSPCA\nrSVD l1\n\n4\n\nrSVD l0\n\n3\n\n2\n\n1\n\n0\n\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\nCardinality\n\nFigure 3: The computational complexity of Greedy grows significantly if it is set out to output a\nloading vector of increasing cardinality. The speed of the other methods is unaffected by the cardinality\ntarget.\n\nGPower l0 results from different approaches to fill the active part of z: GPower l1 requires to compute a\nrank-one approximation of a submatrix of A (see Equation (36)), whereas the explicit solution (13) is\navailable to GPower l0 . The linear complexity of the algorithms in the problem size n is clearly visible in\nTable 4.\np\u00d7n\nGPowerl1\nGPowerl0\nSPCA\nrSVDl1\nrSVDl0\n\n100 \u00d7 1000\n0.10\n0.03\n0.24\n0.21\n0.20\n\n250 \u00d7 2500\n0.86\n0.42\n2.92\n1.45\n1.33\n\n500 \u00d7 5000\n2.45\n1.21\n14.5\n6.70\n6.06\n\n750 \u00d7 7500\n4.28\n2.07\n40.7\n17.9\n15.7\n\n1000 \u00d7 10000\n5.86\n2.85\n82.2\n39.7\n35.2\n\nTable 3: Average computational time for the extraction of one component (in seconds).\np\u00d7n\nGPowerl1\nGPowerl0\nSPCA\nrSVDl1\nrSVDl0\n\n500 \u00d7 1000\n0.42\n0.18\n5.20\n1.20\n1.09\n\n500 \u00d7 2000\n0.92\n0.42\n7.20\n2.53\n2.26\n\n500 \u00d7 4000\n2.00\n0.96\n12.0\n5.33\n4.85\n\n500 \u00d7 8000\n4.00\n2.14\n22.6\n11.3\n10.5\n\n500 \u00d7 16000\n8.54\n4.55\n44.7\n26.7\n24.6\n\nTable 4: Average computational time for the extraction of one component (in seconds).\n\nDifferent convergence mechanisms. Figure 4 illustrates how the trade-off between explained variance and sparsity evolves in the time of computation for the two methods GPower l1 and rSVDl1 . In case\nof the GPower l1 algorithm, the initialization point (38) provides a good approximation of the final cardinality. This method then works on maximizing the variance while keeping the sparsity at a low level\n22\n\n\fthroughout. The rSVDl1 algorithm, in contrast, works in two steps. First, it maximizes the variance,\nwithout enforcing sparsity. This corresponds to computing the first principal component and requires\nthus a first run of the algorithm with random initialization and a sparsity inducing parameter set at zero.\nIn the second run, this parameter is set to a positive value and the method works to rapidly decrease cardinality at the expense of only a modest decrease in explained variance. So, the new algorithm GPower l1\nperforms faster primarily because it combines the two phases into one, simultaneously optimizing the\ntrade-off between variance and sparsity.\n\n1\n0.9\n\n0.9\n\n0.8\n0.7\n\n0.8\n\n0.6\n0.5\n\n0.7\n\n0.4\n0.3\n\n0.6\n\n0.2\nGPowerl1 (Cardinality)\nrSVD l1 (Cardinality)\n\nGPowerl1 (Variance)\nrSVD l1 (Variance)\n0.5\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\nProportion of nonzero entries\n\nProportion of explained variance\n\n1\n\n0.1\n0\n3.5\n\nComputational time [sec]\n\nFigure 4: Evolution of the variance (solid lines and left axis) and cardinality (dashed lines and right\naxis) in time of computation for the methods GPower l1 and rSVDl1 on a test problem with p = 250\nand n = 2500. The vertical axis is the ratio Var(zsPCA )/ Var(zPCA ), where the loading vector zsPCA\nis computed by sparse PCA and zPCA is the first principal loading vector. The rSVDl1 algorithm first\nsolves unconstrained PCA, whereas GPower l1 immediately optimizes the trade-off between variance\nand sparsity.\n\nExtracting more components. Similar numerical experiments, which include the methods GPower l1 ,m\nand GPower l0 ,m , have been conducted for the extraction of more than one component. A deflation\nscheme is used by the non-block methods to sequentially compute m components. These experiments\nlead to similar conclusions as in the single-unit case, i.e, the methods GPower l1 , GPower l0 , GPower l1 ,m ,\nGPower l0 ,m and rSVDl0 outperform the SPCA and rSVDl1 approaches in terms of variance explained at\na fixed cardinality. Again, these last two methods can be improved by postprocessing the resulting loading vectors with Algorithm 6, as it is done for GPower l1 ,m . The average running times for problems of\nvarious sizes are listed in Table 5. The new power-like methods are significantly faster on all instances.\n\n23\n\n\fp\u00d7n\nGPowerl1\nGPowerl0\nGPowerl1 ,m\nGPowerl0 ,m\nSPCA\nrSVDl1\nrSVDl0\n\n50 \u00d7 500\n0.22\n0.06\n0.09\n0.05\n0.61\n0.30\n0.28\n\n100 \u00d7 1000\n0.56\n0.17\n0.28\n0.14\n1.47\n1.15\n1.10\n\n250 \u00d7 2500\n4.62\n2.15\n3.50\n2.39\n13.4\n7.92\n7.54\n\n500 \u00d7 5000\n12.6\n6.16\n12.4\n7.7\n48.3\n37.4\n34.7\n\n750 \u00d7 7500\n20.4\n10.3\n23.0\n12.4\n113.3\n97.4\n85.7\n\nTable 5: Average computational time for the extraction of m = 5 components (in seconds).\n\n5.2 Analysis of gene expression data\nGene expression data results from DNA microarrays and provide the expression level of thousands of\ngenes across several hundreds of experiments. The interpretation of these huge databases remains a\nchallenge. Of particular interest is the identification of genes that are systematically coexpressed under similar experimental conditions. We refer to Riva et al. [2005] and references therein for more details on microarrays and gene expression data. PCA has been intensively applied in this context (e.g.,\nAlter et al. [2003]). Further methods for dimension reduction, such as independent component analysis\n(Liebermeister [2002]) or nonnegative matrix factorization (Brunet et al. [2004]), have also been used on\ngene expression data. Sparse PCA, which extracts components involving a few genes only, is expected\nto enhance interpretation.\nData sets. The results below focus on four major data sets related to breast cancer. They are briefly\ndetailed in Table 6. Each sparse PCA algorithm computes ten components from these data sets.\nStudy\nVijver\nWang\nNaderi\nJRH-2\n\nSamples (p)\n295\n285\n135\n101\n\nGenes (n)\n13319\n14913\n8278\n14223\n\nReference\nvan de Vijver et al. [2002]\nWang et al. [2005]\nNaderi et al. [2007]\nSotiriou et al. [2006]\n\nTable 6: Breast cancer cohorts.\n\nSpeed. The average computational time required by the sparse PCA algorithms on each data set\nis displayed in Table 7. The indicated times are averages on all the computations performed to obtain\ncardinality ranging from n down to 1.\nGPowerl1\nGPowerl0\nGPowerl1 ,m\nGPowerl0 ,m\nSPCA\nrSVDl1\nrSVDl0\n\nVijver\n7.72\n3.80\n5.40\n5.61\n77.7\n46.4\n46.8\n\nWang\n6.96\n4.07\n4.37\n7.21\n82.1\n49.3\n48.4\n\nNaderi\n2.15\n1.33\n1.77\n2.25\n26.7\n13.8\n13.7\n\nJRH-2\n2.69\n1.73\n1.14\n1.47\n11.2\n15.7\n16.5\n\nTable 7: Average computational times (in seconds).\n\n24\n\n\fTrade-off curves. Figure 5 plots the proportion of adjusted variance versus the cardinality for the\n\"Vijver\" data set. The other data sets have similar plots. As for the random test problems, this performance criterion does not discriminate among the different algorithms. All methods have in fact the\nsame performance, provided that the SPCA and rSVDl1 approaches are used with postprocessing by\nAlgorithm 6.\n\nPropoarion of explained variance\n\n1\n0.9\n0.8\n\nGPowerl1\n\n0.7\n\nGPowerl0\n\n0.6\n\nGPowerl1 ,m\nGPowerl0 ,m\n\n0.5\n\nSPCA\n0.4\n\nrSVD l1\n\n0.3\n\nrSVD l0\n\n0.2\n0.1\n0\n\n0\n\n2\n\n4\n\n6\n\n8\n\nCardinality\n\n10\n\n12\n\n14\n4\n\nx 10\n\nFigure 5: Trade-off curves between explained variance and cardinality (case of the \"Vijver\" data).\nThe vertical axis is the ratio AdjVar(ZsPCA )/ AdjVar(ZPCA ), where the loading vectors ZsPCA are\ncomputed by sparse PCA and ZPCA are the m first principal loading vectors.\n\nInterpretability. A more interesting performance criterion is to estimate the biological interpretability of the extracted components. The pathway enrichment index (PEI) proposed by Teschendorff et al.\n[2007] measures the statistical significance of the overlap between two kinds of gene sets. The first sets\nare inferred from the computed components by retaining the most expressed genes, whereas the second\nsets result from biological knowledge. For instance, metabolic pathways provide sets of genes known to\nparticipate together when a certain biological function is required. An alternative is given by the regulatory motifs: genes tagged with an identical motif are likely to be coexpressed. One expects sparse PCA\nmethods to recover some of these biologically significant sets. Table 8 displays the PEI based on 536\nmetabolic pathways related to cancer. The PEI is the fraction of these 536 sets presenting a statistically\nsignificant overlap with the genes inferred from the sparse principal components. The values in Table\n8 correspond to the largest PEI obtained among all possible cardinalities. Similarly, Table 9 is based\non 173 motifs. More details on the selected pathways and motifs can be found in Teschendorff et al.\n[2007]. This analysis clearly indicates that the sparse PCA methods perform much better than PCA in\nthis context. Furthermore, the new GPower algorithms, and especially the block formulations, provide\nlargest PEI values for both types of biological information. In terms of biological interpretability, they\nsystematically outperform previously published algorithms.\n\n25\n\n\fPCA\nGPowerl1\nGPowerl1\nGPowerl1 ,m\nGPowerl0 ,m\nSPCA\nrSVDl1\nrSVDl0\n\nVijver\n0.0728\n0.1493\n0.1250\n0.1418\n0.1362\n0.1362\n0.1213\n0.1175\n\nWang\n0.0466\n0.1026\n0.1250\n0.1250\n0.1287\n0.1007\n0.1175\n0.0970\n\nNaderi\n0.0149\n0.0728\n0.0672\n0.1026\n0.1007\n0.0840\n0.0914\n0.0634\n\nJRH-2\n0.0690\n0.1250\n0.1026\n0.1381\n0.1250\n0.1007\n0.0914\n0.1063\n\nTable 8: PEI-values based on a set of 536 cancer-related pathways.\nPCA\nGPowerl1\nGPowerl0\nGPowerl1 ,m\nGPowerl0 ,m\nSPCA\nrSVDl1\nrSVDl0\n\nVijver\n0.0347\n0.1850\n0.1676\n0.1908\n0.1850\n0.1734\n0.1387\n0.1445\n\nWang\n0\n0.0867\n0.0809\n0.1156\n0.1098\n0.0925\n0.0809\n0.0867\n\nNaderi\n0.0289\n0.0983\n0.0925\n0.1329\n0.1329\n0.0809\n0.1214\n0.0867\n\nJRH-2\n0.0405\n0.1792\n0.1908\n0.1850\n0.1734\n0.1214\n0.1503\n0.1850\n\nTable 9: PEI-values based on a set of 173 motif-regulatory gene sets.\n\n6 Conclusion\nWe have proposed two single-unit and two block formulations of the sparse PCA problem and constructed reformulations with several favorable properties. First, the reformulated problems are of the\nform of maximization of a convex function on a compact set, with the feasible set being either a unit\nEuclidean sphere or the Stiefel manifold. This structure allows for the design and iteration complexity\nanalysis of a simple gradient scheme which applied to our sparse PCA setting results in four new algorithms for computing sparse principal components of a matrix A \u2208 Rp\u00d7n . Second, our algorithms\nappear to be faster if either the objective function or the feasible set are strongly convex, which holds in\nthe single-unit case and can be enforced in the block case. Third, the dimension of the feasible sets does\nnot depend on n but on p and on the number m of components to be extracted. This is a highly desirable\nproperty if p \u226a n. Last but not least, on random and real-life biological data, our methods systematically outperform the existing algorithms both in speed and trade-off performance. Finally, in the case\nof the biological data, the components obtained by our block algorithms deliver the richest biological\ninterpretation as compared to the components extracted by the other methods.\n\nAcknowlegments\nThis paper presents research results of the Belgian Network DYSCO (Dynamical Systems, Control, and\nOptimization), funded by the Interuniversity Attraction Poles Programme, initiated by the Belgian State,\nScience Policy Office. The scientific responsibility rests with its authors. Research of Yurii Nesterov and\nPeter Richt\u00e1rik has been supported by the grant \"Action de recherche concert\u00e9e ARC 04/09-315\" from\nthe \"Direction de la recherche scientifique - Communaut\u00e9 fran\u00e7aise de Belgique\". Michel Journ\u00e9e is a\nresearch fellow of the Belgian National Fund for Scientific Research (FNRS).\n\n26\n\n\f7 Appendix A\nIn this appendix we characterize a class of functions with strongly convex level sets. First we need\nto collect some basic preliminary facts. All the inequalities of Proposition 11 are well-known in the\nliterature.\nProposition 11\n(i) If f is a strongly convex function with convexity parameter \u03c3f , then for all x, y\nand 0 \u2264 \u03b1 \u2264 1,\nf (\u03b1x + (1 \u2212 \u03b1)y) \u2264 \u03b1f (x) + (1 \u2212 \u03b1)f (y) \u2212\n\n\u03c3f\n\u03b1(1 \u2212 \u03b1)kx \u2212 yk2 .\n2\n\n(40)\n\n(ii) If f is a convex differentiable function and its gradient is Lipschitz continuous with constant Lf ,\nthen for all x and h,\nLf\nkhk2 ,\n(41)\nf (x + h) \u2264 f (x) + hf \u2032 (x), hi +\n2\nand\nq\n(42)\nkf \u2032 (x)k\u2217 \u2264 2Lf (f (x) \u2212 f\u2217 ),\ndef\n\nwhere f\u2217 = minx\u2208E f (x).\n\nWe are now ready for the main result of this section.\nTheorem 12 (Strongly convex level sets) Let f : E \u2192 R be a nonnegative strongly convex function\nwith convexity parameter \u03c3f > 0. Also assume f has a Lipschitz continuous gradient with Lipschitz\nconstant Lf > 0. Then for any \u03c9 > 0, the set\ndef\n\nQ\u03c9 = {x | f (x) \u2264 \u03c9}\nis strongly convex with convexity parameter\n\u03c3Q \u03c9 = p\n\n\u03c3f\n.\n2\u03c9Lf\n\nProof. Consider any x, y \u2208 Q\u03c9 , scalar 0 \u2264 \u03b1 \u2264 1 and let z\u03b1 = \u03b1x + (1 \u2212 \u03b1)y. Notice that by convexity,\nf (z\u03b1 ) \u2264 \u03c9. For any u \u2208 E,\nLf\nkuk2\n2\nLf\nkuk2\n\u2264 f (z\u03b1 ) + kf \u2032 (z\u03b1 )kkuk +\n2\nq\nLf\n(42)f (z ) +\nkuk2\n2Lf f (z\u03b1 )kuk +\n\u03b1\n\u2264\n2\n\u0012\n\u00132\nq\np\nLf\n=\nf (z\u03b1 ) +\n2 kuk\n\n)f (z ) + hf \u2032 (z ), ui +\nf (z\u03b1 + u)(41\n\u03b1\n\u03b1\n\u2264\n\n(40)\n\u2264\n\n\u0012\n\np\n\n\u03c9\u2212\u03b2+\n\n27\n\nq\n\nLf\n2\n\n\u00132\nkuk ,\n\n\fwhere\n\n\u03c3f\n\u03b1(1 \u2212 \u03b1)kx \u2212 yk2 .\n(43)\n2\nIn view of (28), it remains to show that the last displayed expression is bounded above by \u03c9 whenever u\nis of the form\n\u03c3f\n\u03c3Q \u03c9\n\u03b1(1 \u2212 \u03b1)kx \u2212 yk2 s = p\nu=\n\u03b1(1 \u2212 \u03b1)kx \u2212 yk2 s,\n(44)\n2\n2 2\u03c9Lf\n\u221a\nfor some s \u2208 S. However, this follows directly from concavity of the scalar function g(t) = t:\np\n\u03c9 \u2212 \u03b2 = g(\u03c9 \u2212 \u03b2) \u2264 g(\u03c9) \u2212 hg \u2032 (\u03c9), \u03b2i\n\u221a\n\u03b2\n= \u03c9\u2212 \u221a\n2 \u03c9\n\u221a\n(43) \u03c9 \u2212 \u03c3\n\u221af \u03b1(1 \u2212 \u03b1)kx \u2212 yk2\n\u2264\n4 \u03c9\nr\n\u221a\nLf\n(44) \u03c9 \u2212\nkuk.\n\u2264\n2\n\u03b2=\n\n\u0003\nExample 13 Let f (x) = kxk2 . Note that \u03c3f = Lf = 2. If we let \u03c9 = r 2 , then\nQ\u03c9 = {x | f (x) \u2264 \u03c9} = {x | kxk \u2264 r} = r * B.\nWe have shown before (see the discussion immediately following Assumption 3), that the strong convexity\nparameter of this set is \u03c3Q\u03c9 = 1r . Note that we recover this as a special case of Theorem 12:\n\u03c3Q \u03c9 = p\n\n\u03c3f\n1\n= .\nr\n2\u03c9Lf\n\nReferences\nP.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton\nUniversity Press, Princeton, January 2008.\nO. Alter, P. O. Brown, and D. Botstein. Generalized singular value decomposition for comparative\nanalysis of genome-scale expression data sets of two different organisms. Proc Natl Acad Sci USA,\n100(6):3351\u20133356, 2003.\nR. W. Brockett. Dynamical systems that sort lists, diagonalize matrices and solve linear programming\nproblems. Linear Algebra Appl., 146:79\u201391, 1991.\nJ. P. Brunet, P. Tamayo, T. R. Golub, and J. P. Mesirov. Metagenes and molecular pattern discovery using\nmatrix factorization. Proc Natl Acad Sci USA, 101(12):4164\u20134169, 2004.\nJ. Cadima and I. T. Jolliffe. Loadings and correlations in the interpretation of principal components.\nJournal of Applied Statistics, 22:203\u2013214, 1995.\nA. d'Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A direct formulation for sparse\nPCA using semidefinite programming. Siam Review, 49:434\u2013448, 2007.\n28\n\n\fA. d'Aspremont, F. R. Bach, and L. El Ghaoui. Optimal solutions for sparse principal component analysis. Journal of Machine Learning Research, 9:1269\u20131294, 2008.\nC. Fraikin, Yu. Nesterov, and P. Van Dooren. A gradient-type algorithm optimizing the coupling between\nmatrices. Linear Algebra and its Applications, 429(5-6):1229\u20131242, 2008.\nG. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, 1996.\nR. A. Horn and C. A. Johnson. Matrix analysis. Cambridge University Press, Cambridge, UK, 1985.\nI. T. Jolliffe. Rotation of principal components: choice of normalization constraints. Journal of Applied\nStatistics, 22:29\u201335, 1995.\nI. T. Jolliffe, N. T. Trendafilov, and M. Uddin. A modified principal component technique based on the\nLASSO. Journal of Computational and Graphical Statistics, 12(3):531\u2013547, 2003.\nW. Liebermeister. Linear modes of gene expression determined by independent component analysis.\nBioinformatics, 18(1):51\u201360, 2002.\nB. Moghaddam, Y. Weiss, and S. Avidan. Spectral bounds for sparse PCA: Exact and greedy algorithms.\nIn Y. Weiss, B. Sch\u00f6lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems\n18, pages 915\u2013922. MIT Press, Cambridge, MA, 2006.\nA. Naderi, A. E. Teschendorff, N. L. Barbosa-Morais, S. E. Pinder, A. R. Green, D. G. Powe, J. F. R.\nRobertson, S. Aparicio, I. O. Ellis, J. D. Brenton, and C. Caldas. A gene expression signature to\npredict survival in breast cancer across independent data sets. Oncogene, 26:1507\u20131516, 2007.\nBeresford N. Parlett. The symmetric eigenvalue problem. Prentice-Hall Inc., Englewood Cliffs, N.J.,\n1980. ISBN 0-13-880047-2. Prentice-Hall Series in Computational Mathematics.\nA. Riva, A.-S. Carpentier, B. Torr\u00e9sani, and A. H\u00e9naut. Comments on selected fundamental aspects of\nmicroarray analysis. Computational Biology and Chemistry, 29(5):319\u2013336, 2005.\nHaipeng Shen and Jianhua Z. Huang. Sparse principal component analysis via regularized low rank\nmatrix approximation. Journal of Multivariate Analysis, 99(6):1015\u20131034, 2008.\nC. Sotiriou, P. Wirapati, S. Loi, A. Harris, S. Fox, J. Smeds, H. Nordgren, P. Farmer, V. Praz, B. HaibeKains, C. Desmedt, D. Larsimont, F. Cardoso, H. Peterse, D. Nuyten, M. Buyse, M. J. Van de Vijver,\nJ. Bergh, M. Piccart, and M. Delorenzi. Gene expression profiling in breast cancer: understanding the\nmolecular basis of histologic grade to improve prognosis. J Natl Cancer Inst, 98(4):262\u2013272, 2006.\nA. Teschendorff, M. Journ\u00e9e, P.-A. Absil, R. Sepulchre, and C. Caldas. Elucidating the altered transcriptional programs in breast cancer using independent component analysis. PLoS Computational\nBiology, 3(8):1539\u20131554, 2007.\nR. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,\nSeries B, 58(2):267\u2013288, 1996.\nM. J. van de Vijver, Y. D. He, L. J. van't Veer, H. Dai, A. A. Hart, D. W. Voskuil, G. J. Schreiber, J. L.\nPeterse, C. Roberts, M. J. Marton, M. Parrish, D. Atsma, A. Witteveen, A. Glas, L. Delahaye, T. van\nder Velde, H. Bartelink, S. Rodenhuis, E. T. Rutgers, S. H. Friend, and R. Bernards. A gene-expression\nsignature as a predictor of survival in breast cancer. N Engl J Med, 347(25):1999\u20132009, 2002.\n29\n\n\fY. Wang, J. G. Klijn, Y. Zhang, A. M. Sieuwerts, M. P. Look, F. Yang, D. Talantov, M. Timmermans,\nM. E. Meijer-van Gelder, J. Yu, T. Jatkoe, E. M. Berns, D. Atkins, and J. A. Foekens. Gene-expression\nprofiles to predict distant metastasis of lymph-node-negative primary breast cancer. Lancet, 365\n(9460):671\u2013679, 2005.\nH. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of Computational\nand Graphical Statistics, 15(2):265\u2013286, 2006.\n\n30\n\n\f"}