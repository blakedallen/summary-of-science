{"id": "http://arxiv.org/abs/1107.5594v2", "guidislink": true, "updated": "2011-09-23T13:40:07Z", "updated_parsed": [2011, 9, 23, 13, 40, 7, 4, 266, 0], "published": "2011-07-27T21:19:34Z", "published_parsed": [2011, 7, 27, 21, 19, 34, 2, 208, 0], "title": "Attacker Control and Impact for Confidentiality and Integrity", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.1525%2C1107.2895%2C1107.2927%2C1107.3052%2C1107.3791%2C1107.4852%2C1107.1880%2C1107.1851%2C1107.2501%2C1107.5753%2C1107.0620%2C1107.1753%2C1107.0261%2C1107.2389%2C1107.3561%2C1107.3896%2C1107.5536%2C1107.2451%2C1107.2121%2C1107.5364%2C1107.4634%2C1107.2546%2C1107.1618%2C1107.1315%2C1107.0708%2C1107.1495%2C1107.2202%2C1107.0457%2C1107.1285%2C1107.3705%2C1107.4679%2C1107.2112%2C1107.2127%2C1107.5425%2C1107.3855%2C1107.2355%2C1107.6044%2C1107.2063%2C1107.0318%2C1107.5421%2C1107.3240%2C1107.1769%2C1107.3151%2C1107.5688%2C1107.0800%2C1107.0271%2C1107.3815%2C1107.5594%2C1107.1511%2C1107.2843%2C1107.2586%2C1107.3642%2C1107.5501%2C1107.4980%2C1107.5772%2C1107.6038%2C1107.1111%2C1107.0338%2C1107.2483%2C1107.0059%2C1107.4639%2C1107.4201%2C1107.1352%2C1107.1637%2C1107.3733%2C1107.1582%2C1107.0772%2C1107.1301%2C1107.3951%2C1107.5338%2C1107.5899%2C1107.0707%2C1107.1215%2C1107.0280%2C1107.2547%2C1107.1634%2C1107.3777%2C1107.4088%2C1107.5327%2C1107.3231%2C1107.5076%2C1107.3928%2C1107.4431%2C1107.1484%2C1107.5760%2C1107.5730%2C1107.3401%2C1107.1420%2C1107.2148%2C1107.3608%2C1107.1109%2C1107.2518%2C1107.2371%2C1107.2303%2C1107.4708%2C1107.3950%2C1107.3530%2C1107.0438%2C1107.2940%2C1107.1845%2C1107.3017&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Attacker Control and Impact for Confidentiality and Integrity"}, "summary": "Language-based information flow methods offer a principled way to enforce\nstrong security properties, but enforcing noninterference is too inflexible for\nrealistic applications. Security-typed languages have therefore introduced\ndeclassification mechanisms for relaxing confidentiality policies, and\nendorsement mechanisms for relaxing integrity policies. However, a continuing\nchallenge has been to define what security is guaranteed when such mechanisms\nare used. This paper presents a new semantic framework for expressing security\npolicies for declassification and endorsement in a language-based setting. The\nkey insight is that security can be characterized in terms of the influence\nthat declassification and endorsement allow to the attacker. The new framework\nintroduces two notions of security to describe the influence of the attacker.\nAttacker control defines what the attacker is able to learn from observable\neffects of this code; attacker impact captures the attacker's influence on\ntrusted locations. This approach yields novel security conditions for checked\nendorsements and robust integrity. The framework is flexible enough to recover\nand to improve on the previously introduced notions of robustness and qualified\nrobustness. Further, the new security conditions can be soundly enforced by a\nsecurity type system. The applicability and enforcement of the new policies is\nillustrated through various examples, including data sanitization and\nauthentication.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.1525%2C1107.2895%2C1107.2927%2C1107.3052%2C1107.3791%2C1107.4852%2C1107.1880%2C1107.1851%2C1107.2501%2C1107.5753%2C1107.0620%2C1107.1753%2C1107.0261%2C1107.2389%2C1107.3561%2C1107.3896%2C1107.5536%2C1107.2451%2C1107.2121%2C1107.5364%2C1107.4634%2C1107.2546%2C1107.1618%2C1107.1315%2C1107.0708%2C1107.1495%2C1107.2202%2C1107.0457%2C1107.1285%2C1107.3705%2C1107.4679%2C1107.2112%2C1107.2127%2C1107.5425%2C1107.3855%2C1107.2355%2C1107.6044%2C1107.2063%2C1107.0318%2C1107.5421%2C1107.3240%2C1107.1769%2C1107.3151%2C1107.5688%2C1107.0800%2C1107.0271%2C1107.3815%2C1107.5594%2C1107.1511%2C1107.2843%2C1107.2586%2C1107.3642%2C1107.5501%2C1107.4980%2C1107.5772%2C1107.6038%2C1107.1111%2C1107.0338%2C1107.2483%2C1107.0059%2C1107.4639%2C1107.4201%2C1107.1352%2C1107.1637%2C1107.3733%2C1107.1582%2C1107.0772%2C1107.1301%2C1107.3951%2C1107.5338%2C1107.5899%2C1107.0707%2C1107.1215%2C1107.0280%2C1107.2547%2C1107.1634%2C1107.3777%2C1107.4088%2C1107.5327%2C1107.3231%2C1107.5076%2C1107.3928%2C1107.4431%2C1107.1484%2C1107.5760%2C1107.5730%2C1107.3401%2C1107.1420%2C1107.2148%2C1107.3608%2C1107.1109%2C1107.2518%2C1107.2371%2C1107.2303%2C1107.4708%2C1107.3950%2C1107.3530%2C1107.0438%2C1107.2940%2C1107.1845%2C1107.3017&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Language-based information flow methods offer a principled way to enforce\nstrong security properties, but enforcing noninterference is too inflexible for\nrealistic applications. Security-typed languages have therefore introduced\ndeclassification mechanisms for relaxing confidentiality policies, and\nendorsement mechanisms for relaxing integrity policies. However, a continuing\nchallenge has been to define what security is guaranteed when such mechanisms\nare used. This paper presents a new semantic framework for expressing security\npolicies for declassification and endorsement in a language-based setting. The\nkey insight is that security can be characterized in terms of the influence\nthat declassification and endorsement allow to the attacker. The new framework\nintroduces two notions of security to describe the influence of the attacker.\nAttacker control defines what the attacker is able to learn from observable\neffects of this code; attacker impact captures the attacker's influence on\ntrusted locations. This approach yields novel security conditions for checked\nendorsements and robust integrity. The framework is flexible enough to recover\nand to improve on the previously introduced notions of robustness and qualified\nrobustness. Further, the new security conditions can be soundly enforced by a\nsecurity type system. The applicability and enforcement of the new policies is\nillustrated through various examples, including data sanitization and\nauthentication."}, "authors": ["Aslan Askarov", "Andrew Myers"], "author_detail": {"name": "Andrew Myers"}, "author": "Andrew Myers", "links": [{"title": "doi", "href": "http://dx.doi.org/10.2168/LMCS-7(3:17)2011", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1107.5594v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1107.5594v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "D.3.3, D.4.6", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1107.5594v2", "affiliation": "Cornell University", "arxiv_url": "http://arxiv.org/abs/1107.5594v2", "arxiv_comment": null, "journal_reference": "Logical Methods in Computer Science, Volume 7, Issue 3 (September\n  26, 2011) lmcs:987", "doi": "10.2168/LMCS-7(3:17)2011", "fulltext": "Logical Methods in Computer Science\nVol. 7 (3:17) 2011, pp. 1\u201333\nwww.lmcs-online.org\n\nSubmitted\nPublished\n\nJun. 14, 2010\nSep. 26, 2011\n\nATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND\nINTEGRITY\nASLAN ASKAROV AND ANDREW C. MYERS\nDepartment of Computer Science, Cornell University\ne-mail address: aslan@cs.cornell.edu and andru@cs.cornell.edu\n\nAbstract. Language-based information flow methods offer a principled way to enforce\nstrong security properties, but enforcing noninterference is too inflexible for realistic applications. Security-typed languages have therefore introduced declassification mechanisms\nfor relaxing confidentiality policies, and endorsement mechanisms for relaxing integrity\npolicies. However, a continuing challenge has been to define what security is guaranteed\nwhen such mechanisms are used. This paper presents a new semantic framework for expressing security policies for declassification and endorsement in a language-based setting.\nThe key insight is that security can be characterized in terms of the influence that declassification and endorsement allow to the attacker. The new framework introduces two\nnotions of security to describe the influence of the attacker. Attacker control defines what\nthe attacker is able to learn from observable effects of this code; attacker impact captures\nthe attacker's influence on trusted locations. This approach yields novel security conditions for checked endorsements and robust integrity. The framework is flexible enough to\nrecover and to improve on the previously introduced notions of robustness and qualified\nrobustness. Further, the new security conditions can be soundly enforced by a security\ntype system. The applicability and enforcement of the new policies is illustrated through\nvarious examples, including data sanitization and authentication.\n\n1. Introduction\nMany common security vulnerabilities can be seen as violations of either confidentiality or\nintegrity. As a general way to prevent these information security vulnerabilities, information\nflow control has become a popular subject of study, both at the language level [23] and at the\noperating-system level (e.g., [14, 12, 30]). The language-based approach holds the appeal\nthat the security property of noninterference [13], can be provably enforced using a type\nsystem [27]. In practice, however, noninterference is too rigid: many programs considered\nsecure need to violate noninterference in limited ways.\nUsing language-based downgrading mechanisms such as declassification [17, 21] and endorsement [20, 29], programs can be written in which information is intentionally released,\nand in which untrusted information is intentionally used to affect trusted information or\n1998 ACM Subject Classification: D.3.3, D.4.6.\nKey words and phrases: Security type system, information flow, noninterference, confidentiality, integrity,\nrobustness, downgrading, declassification, endorsement, security policies.\n\nl\n\nLOGICAL METHODS\nIN COMPUTER SCIENCE\n\n\u00a9\nDOI:10.2168/LMCS-7 (3:17) 2011\n\nCC\n\nA. Askarov and A. C. Myers\nCreative Commons\n\n\f2\n\nA. ASKAROV AND A. C. MYERS\n\ndecisions. Declassification relaxes confidentiality policies, and endorsement relaxes integrity\npolicies. Both endorsement and declassification have been essential for building realistic applications, such as various applications built with Jif [15, 18]: games [5], a voting system [11],\nand web applications [9].\nA continuing challenge is to understand what security is obtained when code uses downgrading. This paper contributes a more precise and satisfactory answer to this question, particularly clarifying how the use of endorsement weakens confidentiality. While much work\nhas been done on declassification (usefully summarized by Sands and Sabelfeld [24]), there\nis comparatively little work on the interaction between confidentiality and endorsement.\nTo see such an interaction, consider the following notional code example, in which a\nservice holds both old data (old_data) and new data (new_data), but the new data is not\nto be released until time embargo_time. The variable new_data is considered confidential,\nand must be declassified to be released:\nif request_time >= embargo_time\nthen return declassify(new_data)\nelse return old_data\n\nBecause the requester is not trusted, the requester must be treated as a possible attacker.\nSuppose the requester has control over the variable request_time, which we can model by\nconsidering that variable to be low-integrity. Because the intended security policy depends\non request_time, the attacker controls the policy that is being enforced, and can obtain\nthe confidential new data earlier than intended. This example shows that the integrity\nof request_time affects the confidentiality of new_data. Therefore, the program should\nbe considered secure only when the guard expression, request_time >= embargo_time, is\nhigh-integrity.\nA different but reasonable security policy is that the requester may specify the request\ntime as long as the request time is in the past. This policy could be enforced in a language\nwith endorsement by first checking the low-integrity request time to ensure it is in the\npast; then, if the check succeeds, endorsing it to be high-integrity and proceeding with the\ninformation release. The explicit endorsement is justifiable because the attacker's actions\nare permitted to affect the release of confidential information as long as adversarial inputs\nhave been properly sanitized. This is a common pattern in servers that process possibly\nadversarial inputs.\nRobust declassification has been introduced in prior work [28, 16, 10] as a semantic\ncondition for secure interactions between integrity and confidentiality. The prior work also\ndevelops type systems for enforcing robust declassification, which are implemented as part\nof Jif [18]. However, prior security conditions for robustness are not satisfactory, for two\nreasons. First, these prior conditions characterize information security only for terminating\nprograms. A program that does not terminate is automatically considered to satisfy robust\ndeclassification, even if it releases information improperly during execution. Therefore the\nsecurity of programs that do not terminate, such as servers, cannot be described. A second\nand perhaps even more serious limitation is that prior security conditions largely ignore the\npossibility of endorsement, with the exception of qualified robustness [16]. Qualified robustness gives the endorse operation a somewhat ad-hoc, nondeterministic semantics, to reflect\nthe attacker's ability to choose the endorsed value. This approach operationally models what\nthe attacker can do, but does not directly describe the attacker's control over confidentiality.\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n3\n\nThe introduction of nondeterminism also makes the security property possibilistic. However, possibilistic security properties have been criticized because they can weaken under\nrefinement [22, 25].\nThe main contribution of this paper is a general, language-based semantic framework\nfor expressing information flow security and semantically capturing the ability of the attacker to influence both the confidentiality and integrity of information. The key building\nblocks for this semantics are attacker knowledge [1] and its (novel) dual, attacker impact,\nwhich respectively describe what attackers can know and what they can affect. Building\nupon attacker knowledge, the interaction of confidentiality and integrity, which we term\nattacker control, can be characterized formally. The robust interaction of confidentiality\nand integrity can then be captured cleanly as a constraint on attacker control. Further,\nendorsement is naturally represented in this framework as a form of attacker control, and a\nmore satisfactory version of qualified robustness can be defined. All these security conditions\ncan be formalized in both progress-sensitive and progress-insensitive variants, allowing us to\ndescribe the security of both terminating and nonterminating systems.\nWe show that the progress-insensitive variants of these improved security conditions\nare enforced soundly by a simple security type system. Recent versions of Jif have added a\nchecked endorsement construct that is useful for expressing complex security policies [9], but\nwhose semantics were not precisely defined; this paper gives semantics, typing rules and a\nsemantic security condition for checked endorsement, and shows that checked endorsement\ncan be translated faithfully into simple endorsement at both the language and the semantic\nlevel. Our type system can easily be adjusted to enforce the progress-sensitive variants of\nthe security conditions, as has been shown in the literature [26, 19].\nThe rest of this paper is structured as follows. Section 2 shows how to define information security in terms of attacker knowledge. Section 3 introduces attacker control.\nSection 4 defines progress-sensitive and progress-insensitive robustness using the new framework. Section 5 extends this to improved definitions of robustness that allow endorsements,\ngeneralizing qualified robustness. A type system for enforcing these robustness conditions\nis presented in Section 6. The checked endorsement construct appears in Section 7, which\nintroduces a new notion of robustness that allows checked endorsements, and shows that\nit can be understood in terms of robustness extended with simple endorsements. Section 8\nintroduces attacker impact. Additional examples are presented in Section 9, related work is\ndiscussed in Section 10, and Section 11 concludes.\nThis paper is an extended version of a previous paper by the same authors [4]. The\nsignificant changes include proofs of all the main theorems, a semantic rather than syntactic\ndefinition of fair attacks, and a renaming of \"attacker power\" to \"attacker impact\".\n\n2. Semantics\nInformation flow levels. We assume two security levels for confidentiality - public and\nsecret - and two security levels for integrity - trusted and untrusted. These levels are\ndenoted respectively P, S and T, U. We define information flow ordering v between these\ntwo levels: P v S, and T v U. The four levels define a security lattice, as shown on Figure 1.\nEvery point on this lattice has two security components: one for confidentiality, and one for\nintegrity. We extend the information flow ordering to elements on this lattice: `1 v `2 if the\nordering holds between the corresponding components. As is standard, we define join `1 t `2\n\n\f4\n\nA. ASKAROV AND A. C. MYERS\n\nS, U\n\nP, U\n\nS, T\n\ne ::= n | x | e op e\nc ::= skip | x := e | c; c\n\nP, T\n\n| if e then c1 else c2 | while e do c\n\nFigure 1. Information flow lattice\n\nhn, mi \u2193 n\n\nhx, mi \u2193 m(x)\n\nFigure 2. Syntax of the language\nhe1 , mi \u2193 v1\n\nhe2 , mi \u2193 v2\nv = v1 op v2\nhe1 op e2 , mi \u2193 v\n\nFigure 3. Semantics of expressions\nhskip, mi\u2212\u2192hstop, mi\n\nhe, mi \u2193 v\nhx := e, mi\u2212\u2192(x,v) hstop, m[x 7\u2192 v]i\n\nhc1 , mi\u2212\u2192t hc01 , m0 i\nhc1 ; c2 , mi\u2212\u2192t hc01 ; c2 , m0 i\nhe, mi \u2193 n\nn 6= 0\nhif e then c1 else c2 , mi\u2212\u2192hc1 , mi\nhe, mi \u2193 n\nn 6= 0\nhwhile e do c, mi\u2212\u2192hc; while e do c, mi\n\nhc1 , mi\u2212\u2192t hstop, m0 i\nhc1 ; c2 , mi\u2212\u2192t hc2 , m0 i\nhe, mi \u2193 n\nn=0\nhif e then c1 else c2 , mi\u2212\u2192hc2 , mi\nhe, mi \u2193 n\nn=0\nhwhile e do c, mi\u2212\u2192hstop, mi\n\nFigure 4. Semantics of commands\nas the least upper bound of `1 and `2 , and meet `1 u `2 as the greatest lower bound of `1 and\n`2 . All four lattice elements are meaningful; for example, it is possible for information to\nbe both secret and untrusted when it depends on both secret and untrusted (i.e., attackercontrolled) values. This lattice is the simplest possible choice for exploring the topics of this\npaper; however, the results of this paper straightforwardly generalize to the richer security\nlattices used in other work on robustness [10].\nLanguage and semantics. We consider a simple imperative language with syntax presented\nin Figure 2. The semantics of the language is fairly standard and is given in Figures 3\nand 4. For expressions, we define big-step evaluation of the form he, mi \u2193 v, where v is\nthe result of evaluating expression e in memory m. For commands, we define a small-step\noperational semantics, in which a single transition is written as hc, mi\u2212\u2192t hc0 , m0 i, where c\nand m are the initial command and memory, and c0 and m0 are the resulting command and\nmemory. The only unusual feature is the annotation t on each transition, which we call\nan event. Events record assignments: an assignment to variable x of value v is recorded\nby an event (x, v). This corresponds to our attacker model, in which the attacker may\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n5\n\nonly observe assignments to public variables. We write hc, mi\u2212\u2192\u2217~t to mean that trace ~t\nis produced starting from hc, mi using zero or more transitions. Each trace ~t is composed\nof individual events t1 *t2 * * * tk * * * , and a prefix of ~t up to the i-th event is denoted as t~i ;\nwe use the operator * to denote the concatenation of two traces or events. If a transition\ndoes not affect memory, its event is empty, which is either written as \u000f or is omitted, e.g.:\nhc, mi\u2212\u2192hc0 , m0 i.\nFinally, we assume that the security environment \u0393 maps program variables to their\nsecurity levels. Given a memory m, we write mP for the public part of the memory; similarly,\nmT is the trusted part of m. We write m =T m0 when memories m and m0 agree on their\ntrusted parts, and m =P m0 when m and m0 agree on their public parts.\n2.1. Attacker knowledge\nThis section provides background on the attacker-centric model for information flow security [1]. We recall definitions of attacker knowledge, progress knowledge, and divergence\nknowledge, and introduce progress-(in)sensitive release events.\nLow events. Among the events that are generated during a trace, we distinguish a sequence\nof low (or public) events. Low events correspond to observations that an attacker can\nmake during a run of the program. We assume that the attacker may observe individual\nassignments to public variables. Furthermore, if the program terminates, we assume that a\ntermination event \u21d3 may also be observed by the attacker. If attacker can detect divergence\nof programs (cf. Definition 2.3) then divergence \u21d1 is also a low event.\nGiven a trace ~t, low events in that trace are denoted as ~tP . A single low event is often\ndenoted as `, and a sequence of low events is denoted as ~`. We overload the notation\nfor semantic transitions, writing hc, mi\u2212\u2192\u2217 ~` if only low events produced from configuration\nhc, mi are relevant; that is, there is a trace ~t such that hc, mi\u2212\u2192\u2217~t \u2227 ~tP = ~`. Low events are\nthe key element in the definition of attacker knowledge [1].\nThe knowledge of the attacker is described by the set of initial memories compatible\nwith low observations. Any reduction in this set means the attacker has learned something\nabout secret parts of the initial memory.\nDefinition 2.1 (Attacker knowledge). Given a sequence of low events ~`, initial low memory\nmP , and program c, attacker knowledge is\nk(c, mP , ~`) , {m0 | mP = m0 \u2227 hc, m0 i\u2212\u2192\u2217 ~}\nP\n\n`\n\nAttacker knowledge gives a handle on what information the attacker learns with every low\nevent. The smaller the knowledge set, the more precise is the attacker's information about\nsecrets. Knowledge is monotonic in the number of low events: as the program produces low\nevents, the attacker may learn more about secrets.\nTwo extensions of attacker knowledge are useful: progress knowledge [3, 2] and divergence\nknowledge [3].\nDefinition 2.2 (Progress knowledge). Given a sequence of low events ~`, initial low memory\nmP , and a program c, define progress knowledge k\u2192 (c, mP , ~`) as\nk\u2192 (c, mP , ~`) , {m0 | m0 = mP \u2227 \u2203`0 . hc, m0 i\u2212\u2192\u2217 ~hc00 , m00 i\u2212\u2192\u2217 `0 }\nP\n\n`\n\n\f6\n\nA. ASKAROV AND A. C. MYERS\n\nProgress knowledge represents the information the attacker obtains by seeing public events ~`\nfollowed by some other public event. Progress knowledge and attacker knowledge are related\nas follows: given a program c, memory m and a sequence of low events `1 * * * `n obtained\nfrom hc, mi, we have that for all i < n,\nk(c, mP , ~`i ) \u2287 k\u2192 (c, mP , ~`i ) \u2287 k(c, mP , ~`i+1 )\nTo illustrate this with an example, consider program l := 0; (while h = 0 do skip); l :=\nh with initial memory m(h) = 7. This program produces a sequence of two low events\n(l, 0)*(l, 7). The knowledge after the first event k(c, mP , (l , 0)) is a set of all possible memories\nthat agree with m on the public parts and can produce the low event (l , 0). Note that no\nlow events are possible after the first assignment unless h is non-zero. Progress knowledge\nreflects this: k\u2192 (c, mP , (l , 0)) is a set of memories such that h 6= 0. Finally, the knowledge\nafter two events k(c, mP , (l , 0)*(l , 7)) is a set of memories where h = 7.\nUsing attacker knowledge, one can express many confidentiality policies [7, 2, 8]. For\nexample, a strong notion of progress-sensitive noninterference [13] can be expressed by demanding that knowledge between low events does not change:\nk(c, mP , ~`i ) = k(c, mP , ~`i+1 )\nProgress knowledge enables expressing more permissive policies, such as progress-insensitive\nnoninterference, which allows leakage of information, but only via termination channels (in\n[3] it is called termination-insensitive). This is expressed by requiring equivalence of the\nprogress knowledge after seeing i events with the knowledge obtained after i + 1-th event:\nk\u2192 (c, mP , ~`i ) = k(c, mP , ~`i+1 )\nIn the example l := 0; (while h = 0 do skip); l := 1, the knowledge inclusion between the\ntwo events is strict: k(c, mP , (l, 0)) \u2283 k(c, mP , (l, 0)*(l, 1)). Therefore, the example does not\nsatisfy progress-sensitive noninterference. On the other hand, the low event that follows\nthe while loop does not reveal more information than the knowledge about the existence\nof that event. Formally, k\u2192 (c, mP , (l, 0)) = k(c, mP , (l, 0)*(l, 1)), hence the program satisfies\nprogress-insensitive noninterference.\nThese definitions also allow us to reason about knowledge changes along parts of the\ntraces. We say that knowledge is preserved in a progress-(in)sensitive way along a part\nof a trace, assuming that the respective knowledge equality holds for the low events that\ncorrespond to that part.\nNext, we extend possible observations to a divergence event \u21d1 (we write hc, mi \u21d1 to\nmean configuration hc, mi diverges). For attackers that can observe program divergence \u21d1,\nwe define knowledge on the sequence of low events that includes divergence:\nDefinition 2.3 (Divergence knowledge).\nk(c, mP , ~` \u21d1) , {m0 | m0 = mP \u2227 hc, m0 i\u2212\u2192\u2217 ~hc00 , m00 i \u2227 hc00 , m00 i \u21d1}\nP\n\n`\n\nNote that the above definition does not require divergence immediately after ~` - it allows\nfor more low events to be produced after ~`. Divergence knowledge is used in Section 4.\nLet us consider events at which knowledge preservation is broken. We call these events\nrelease events.\nDefinition 2.4 (Release events). Given a program c and a memory m, such that\nhc, mi\u2212\u2192\u2217 ~`hc0 , m0 i\u2212\u2192\u2217 r\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n7\n\n\u2022 r is a progress-sensitive release event, if k(c, mP , ~`) \u2283 k(c, mP , ~`*r)\n\u2022 r is a progress-insensitive release event, if k\u2192 (c, mP , ~`) \u2283 k(c, mP , ~`*r)\nIt is easy to validate that a progress-insensitive release event is also a progress-sensitive\nevent. For example, in the program low := 1; low 0 := h, the second assignment is both a\nprogress-sensitive and a progress-insensitive release event. The reverse is not true - in the\nprogram while h = 0 do skip; low := 1 the assignment to low is a progress-sensitive release\nevent, but is not a progress-insensitive release event.\n\n3. Attacks\nTo reason about program security in the presence of active attacks, we introduce a formal\nmodel of the attacker. Our formalization follows that in [16], where attacker-provided code\ncan be injected into the program. This section provides examples of how attacker-injected\ncode may affect attacker knowledge, followed by a semantic characterization of the attacker's\ninfluence on knowledge.\nFirst, we extend the syntax to allow execution of attacker-controlled code:\nc[~\u2022] ::= . . . | [\u2022]\nNext, we introduce notation [~t] to highlight that the trace ~t is produced by attackerinjected code. The semantics of the language is extended accordingly.\nha, mi\u2212\u2192t ha0 , m0 i\nh[a], mi\u2212\u2192[t] h[a0 ], m0 i\n\nh[stop], mi\u2212\u2192hstop, mi\n\nWe limit attacks that can be substituted into holes to so-called fair attacks, which represent\nreasonable limitations on the impact of the attacker. Unlike earlier approaches, where fair\nattacks are defined syntactically [16, 10], we define them semantically. This allows us to\ninclude a larger set of attacks. To ensure that we include all syntactic attacks we make use\nof a reachability translation, explained below.\nRoughly, we require a fair attack to not give new knowledge and to not modify trusted\nvariables. A refinement of this idea is that an attack is fair if it gives new knowledge but\nonly because the reachability of the attack depends on a secret. To capture this refinement,\nwe define an auxiliary translation to make reachability of attacks explicit. We assume a\ntrusted, public variable reach that does not appear in the source of c[~\u2022]. Let operator T\nbe a source-to-source transformation of c[~\u2022] that makes reachability of attacks explicit.\nDefinition 3.1 (Explicit reachability translation). Given a program c[~\u2022], define (T (c[~\u2022])\nas follows:\n\u2022 T ([\u2022]) =\u21d2 reach := reach + 1; [\u2022]\n\u2022 T (c1 ; c2 ) =\u21d2 T (c1 ); T (c2 )\n\u2022 T (if e then c1 else c2 ) =\u21d2 if e then T (c1 ) else T (c2 )\n\u2022 T (while e do c) =\u21d2 while e do T (c)\n\u2022 T (c) =\u21d2 c for all other commands c\nThe formal definition uses that any trace ~t can be represented as a sequence of subtraces\n~t1 *[~t2 ] * * * ~t2\u2217n\u22121 *[~t2\u2217n ], where even-numbered subtraces correspond to the events produced\nby attacker-controlled code.\nGiven a trace ~t, we denote the trusted events in the trace as ~tT . We use notation t? for\na single trusted event, and ~t? for a sequence of trusted events.\n\n\f8\n\nA. ASKAROV AND A. C. MYERS\n\nDefinition 3.2 (Fair attack). Given a program c[~\u2022], such that T (c[~\u2022]) =\u21d2 c [~\u2022], say\nthat ~a is a fair attack on c[~\u2022] if for all memories m, such that hc [~a], mi\u2212\u2192\u2217~t and ~t =\n~t1 *[~t2 ] * * * ~t2\u2217n\u22121 *[~t2\u2217n ], i.e., there are 2n intermediate configurations hcj , mj i, 1 \u2264 j \u2264 2n, for\nwhich\nhc [~a], mi\u2212\u2192\u2217~t1 hc1 , m1 i\u2212\u2192\u2217 [~t2 ] hc2 , m2 i\u2212\u2192\u2217~t3 . . . \u2212\u2192\u2217 [~t2n ] hc2n , m2n i . . .\nthen for all i, 1 \u2264 i \u2264 n, it holds that k(c [~a], m, ~t1 * * * ~t2i\u22121 ) = k(c [~a], m, ~t1 * * * [~t2i ]) and\nt~2i? = \u000f.\nFor example, in the program if h > 0 then [\u2022] else skip the attacks a1 = [low := 1]\nand attack a2 = [low := h > 0] are fair, but attack a3 = [low := h] is not.\n3.1. Examples of attacker influence\nThis section presents a few examples of attacker influence on knowledge. We also introduce\npure availability attacks and progress attacks, to which we refer later in this section.\nIn the examples below, we use notation [(u, v)] when a low event (u, v) is generated by\nattacker-injected code.\nConsider program [\u2022]; low := u > h; where h is a secret variable, and u is an untrusted\npublic variable. The attacker's code executes before the low assignment and may change\nthe value of u. Consider memory m, where m(h) = 7, and the two attacks a1 = u := 0 and\na2 = u := 10. These attacks result in different values being assigned to variable low . The\nfirst trace results in low events [(u, 0)]*(low , 0), while the second trace results in low events\n[(u, 10)]*(low , 1). Therefore, the knowledge about the secret is different in each trace. We\nhave\nk(c[a1 ], mP , [(u, 0)]*(low , 0)) = {m0 | m0 (h) \u2265 0}\nk(c[a2 ], mP , [(u, 10)]*(low , 1)) = {m0 | m0 (h) < 10}\nClearly, this program gives the attacker some control over what information about secrets\nhe learns. Observe that it is not necessary for the last assignment to differ in order for the\nknowledge to be different. For example, consider attack a3 = u := 5. This attack results\nin low events [(u, 5)]*(low , 0), which do the same assignment to low as a1 does. Attacker\nknowledge, however, is different from that obtained by a1 :\nk(c[a3 ], mP , [(u, 5)]*(low , 0)) = {m0 | m0 (h) \u2265 5}\nNext, consider program [\u2022]; low := h. This program gives away knowledge about the value\nof h independently of untrusted variables. The only way for the attacker to influence what\ninformation he learns is to prevent that assignment from happening at all, which, as a result,\nwill prevent him from learning that information. This can be done by an attack such as\na = while true do skip, which makes the program diverge before the assignment is reached.\nWe call attacks like this pure availability attacks. Another example of a pure availability\nattack is in the program [\u2022]; (while u = 0 do skip); low := h. In this program, any attack\nthat sets u to 0 prevents the assignment from happening.\nConsider another example: [\u2022]; while u < h0 do skip; low := 1. As in the previous\nexample, the value of u may change the reachability of low := 1. Assuming the attacker\ncan observe divergence, this is not a pure availability attack, because diverging before the\nlast assignment gives the attacker additional secret information, namely that u < h0 . New\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\nuncertainty\n\n9\n\nuncertainty\n\nx, v\ny, u\nz, w\n\nlow events\n\nlow events\n\nFigure 5. Similar attacks and traces\ninformation is also obtained if the attacker sees the low assignment. We name attacks\nlike this progress attacks. In general, a progress attack is an attack that leads to program\ndivergence in a way that observing that divergence (i.e., detecting there is no progress) gives\nnew knowledge to the attacker.\n3.2. Attacker control\nWe represent attacker control as a set of attacks that are similar in their influence on knowledge. Intuitively, if a program leaks no information to the attacker, the control corresponds\nto all possible attacks. In general, the more attacks are similar, the less influence the attacker has. Moreover, the control is a temporal property and depends on the trace that has\nbeen currently produced. The longer a trace is, the more influence an attack may have, and\nthe smaller the control set is.\nSimilar attacks. The key element in the definition of control is specifying when two attacks\nare similar. Given a program c[~\u2022], memory m, consider two attacks ~a and ~b that produce\ntraces ~t and ~q respectively:\nhc[~a], mi\u2212\u2192\u2217~ and hc[~b], mi\u2212\u2192\u2217 q~\nt\n\nWe compare ~a and ~b based on how they change attacker knowledge along their respective\ntraces. First, if knowledge is preserved along a subtrace of one of the traces, say ~t, it must\nbe preserved along a subtrace of ~q as well. Second, if at some point in ~t there is a release\nevent (x, v), there must be a matching low event (x, v) in ~q, and the attacks are similar\nalong the rest of the traces.\nVisually, this requirement is described by the two diagrams in Figure 5. Each diagram\nshows the change of knowledge as more low events are produced. Here the x-axis corresponds to low events, and the y-axis reflects the attacker's uncertainty about initial secrets.\nWhenever one of the traces reaches a release event, depicted by vertical drops, there must\nbe a corresponding low event in the other trace, such that the two events agree. This is\ndepicted by the dashed lines between the two diagrams.\nFormally, these requirements are stated using the following definitions.\nDefinition 3.3 (Knowledge segmentation). Given a program c, memory m, and a trace ~t, a\nsequence of indices p1 . . . pN such that p1 < p2 * * * < pN and ~tP = `1...p1*`p1 +1...p2 * * * `pN \u22121 +1...pN\nis called\n\u2022 progress-sensitive knowledge segmentation of size N , if\n\u2200j \u2264 N, \u2200i . pj\u22121 + 1 \u2264 i < pj . k(c, mP , ~`i ) = k(c, mP , ~`i+1 ), denoted by\nSeg(c, m, ~t, p1 . . . pN ).\n\n\f10\n\nA. ASKAROV AND A. C. MYERS\n\n\u2022 progress-insensitive knowledge segmentation of size N if\n\u2200j \u2264 N, \u2200i . pj\u22121 + 1 \u2264 i < pj . k\u2192 (c, mP , ~`i ) = k(c, mP , ~`i+1 ), denoted by\nSeg\u2192 (c, m, ~t, p1 . . . pN ).\nLow events pi + 1 for 1 \u2264 i < N are called segmentation events.\nNote that given a trace, there can be more than one way to segment it, and for every\ntrace consisting of n low events, this can be trivially achieved by a segmentation of size n.\nWe use knowledge segmentation to define attack similarity:\nDefinition 3.4 (Similar attacks and traces \u223cc[~\u2022],m ). Given a program c[~\u2022], memory m, and\ntwo attacks ~a and ~b that produce traces ~t and ~q, define ~a and ~b as similar along ~t and ~q\nfor the progress-sensitive attacker, if there are two segmentations p1 . . . pN and p01 . . . p0N (for\nsome N ) such that\n\u2022 Seg(c[~a], m, ~t, p1 . . . pN ),\n\u2022 Seg(c[~b], m, ~q, p01 . . . p0N ), and\n\u2022 \u2200i . 1 \u2264 i < N . tPpi +1 = qP p0i +1 .\nFor the progress-insensitive attacker, the definition is similar except that it uses progressinsensitive segmentation Seg\u2192 . If two attack\u2013trace pairs are similar, we write (~a, ~t) \u223cc[~\u2022],m\nc[~\u2022],m ~\n(~b, ~q) (for progress-insensitive similarity, (~a, ~t) \u223c\u2192\n(b, ~q)).\nThe construction of Definitions 3.3 and 3.4 can be illustrated by program\n[\u2022]; if u then (while h \u2264 100 do skip) else skip; low 1 := 0; low 2 := h > 100\nConsider memory with m(h) = 555, and two attacks a1 = u := 1, and a2 = u := 0.\nBoth attacks reach the assignments to low variables. However, for a2 the assignment to\nlow 2 is a progress-insensitive release event, while for a1 the knowledge changes at an earlier\nassignment.\nAttacker control. We define attacker control with respect to an attack ~a and a trace ~t as\nthe set of attacks that are similar to the given attack in its influence on knowledge.\nDefinition 3.5 (Attacker control (progress-sensitive)).\nR(c[~\u2022], m, ~a, ~t) , {~b | \u2203~q . (~a, ~t) \u223cc[~\u2022],m (~b, ~q)}\nTo illustrate how attacker control changes, consider program [\u2022]; low := u < h; low 0 := h\nwhere u is an untrusted variable and h is a secret trusted variable. To understand attacker\ncontrol of this program, we consider an initial memory m(h) = 7 and attack a = u := 5.\nThe low event (low , 1) in this trace is a release event. The attacker control is the set of all\nattacks that are similar to a and trace [(u := 5)], (low , 1) in its influence on knowledge. This\ncorresponds to attacks that set u to values such that u < 7. The assignment to low 0 changes\nattacker knowledge as well, but the information that the attacker gets does not depend on\nthe attack: any trace starting in m and reaching the second assignment produces the low\nevent (low 0 , 7); hence, the attacker control does not change at that event.\nConsider the same example but with the two assignments swapped: [\u2022]; low 0 := h; low :=\nu < h. The assignment to low 0 is a release event that the attacker cannot affect. Hence\nthe control includes all attacks that reach this assignment. The result of the assignment to\nlow depends on u. However, this result does not change attacker knowledge. Indeed, in this\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n11\n\nattacks\n\nattacks\n\nR\n\nR\nR\u22b2\n\nR\u22b2\nrelease events\n\n(a) Release control\n\nrelease events\n\n(b) Robustness\n\nFigure 6. Release control and robustness\nprogram, the second assignment is not a release event at all. Therefore, the attacker control\nis simply all attacks that reach the first assignment.\nProgress-insensitive control. For progress-insensitive security, attacker control is defined\nsimilarly using the progress-insensitive comparison of attacks.\nDefinition 3.6 (Attacker control (progress-insensitive)).\nc[~\u2022],m ~\nR\u2192 (c[~\u2022], m, ~a, ~t) , {~b | \u2203~q . (~a, ~t) \u223c\u2192\n(b, ~q))}\nConsider program [\u2022]; while u < h do skip; low := 1. Here, any attack produces a\ntrace that preserves progress-insensitive noninterference. If the loop is taken, the program\nproduces no low events, hence, it gives no new knowledge to the attacker. If the loop is not\ntaken, and the low assignment is reached, this assignment preserves attacker knowledge in\na progress-insensitive way. Therefore, the attacker control is all attacks.\n\n4. Robustness\nRelease control. This section defines release control R. , which captures the attacker's influence on release events. Intuitively, release control expresses the extent to which an attacker\ncan affect the decision to produce some release event.\nDefinition 4.1 (Progress-sensitive release control ).\nR. (c[~\u2022], m, ~a, ~t) , {~b | \u2203~q . (~a, ~t) \u223cc[~\u2022],m (~b, ~q) \u2227\n(\u2203r~0 . k(c[~b], mP , ~qP ) \u2283 k(c[~b], mP , ~qP * r~0 P )\n\u2228 k(c[~b], mP , ~qP ) \u2283 k(c[~b], mP , ~qP \u21d1)\n\u2228 hc[~b], mi \u21d3)}\n\nThe definition for release control is based on the one for attacker control with the three\nadditional clauses, explained below. These clauses restrict the set of attacks to those that\neither terminate or produce a release event. Because the progress-sensitive attacker can also\nlearn new information by observing divergence, the definition contains an additional clause\n(on the third line) that uses divergence knowledge to reflect that.\nFigure 6a depicts the relationship between release control and attacker control, where\nthe x-axis corresponds to low events, and the y-axis corresponds to attacks. The solid line\n\n\f12\n\nA. ASKAROV AND A. C. MYERS\n\ndepicts attacker control R, where vertical lines correspond to release events. The gray area\ndenotes release control R. . In general, for a given attack ~a and a corresponding trace ~t*~r,\nwhere ~r contains a release event, we have the following relation between release control and\nattacker control:\nR(c[~\u2022], m, ~a, ~t) \u2287 R. (c[~\u2022], m, ~a, ~t) \u2287 R(c[~\u2022], m, ~a, ~t*~r)\n(4.1)\nNote the white gaps and the gray release control above the dotted lines on Figure 6a.\nThe white gaps correspond to difference R(c[~\u2022], m, ~a, ~t) \\ R. (c[~\u2022], m, ~a, ~t). This is a set of\nattacks that do not produce further release events and that diverge without giving any new\ninformation to the attacker-pure availability attacks. The gray zones above the dotted\nlines are more interesting. Every such zone corresponds to the difference R. (c[~\u2022], m, ~a, ~t) \\\nR(c[~\u2022], m, ~a, ~t*~r). In particular, when this set is non-empty, the attacker can launch attacks\ncorresponding to each of the last three lines of Definition 4.1:\n(1) either trigger a different release event r~0 , or\n(2) cause program to diverge in a way that also releases information, or\n(3) prevent a release event from happening in a way that leads to program termination\nAbsence of such attacks constitutes the basis for our security conditions in Definitions 4.3\nand 4.4. Before moving on to these definitions, we introduce the progress-insensitive variant\nof release control.\nDefinition 4.2 (Release control (progress-insensitive)).\n.\nc[~\u2022],m ~\nR\u2192\n(c[~\u2022], m, ~a, ~t) , {~b | \u2203~q . (~a, ~t) \u223c\u2192\n(b, ~q) \u2227\n\n(\u2203r~0 . k\u2192 (c[~b], mP , ~qP ) \u2283 k(c[~b], mP , ~qP * r~0 P ) \u2228 hc[~b], mi \u21d3)}\nThis definition uses the progress-insensitive variants of similar attacks and release events. It\nalso does not account for knowledge obtained from divergence.\nWith the definition of release control at hand we can now define semantic conditions for\nrobustness. The intuition is that all attacks leading to release events should lead to the same\nrelease event. Formally, this is defined as inclusion of release control into attacker control,\nwhere release control is computed on the prefix of the trace without a release event.\nDefinition 4.3 (Progress-sensitive robustness). Program c[~\u2022] satisfies progress-sensitive robustness if for all memories m, attacks ~a, and traces ~t~r, such that hc[~a], mi\u2212\u2192\u2217~thc0 , m0 i\u2212\u2192\u2217~r\nand ~r contains a release event, i.e., k(c[~a], mP , ~tP ) \u2283 k(c[~a], mP , ~tP *~rP ), we have\nR. (c[~\u2022], m, ~a, ~t) \u2286 R(c[~\u2022], m, ~a, ~t*~r)\nNote that because of Equation 4.1, set inclusion in the above definition could be replaced\nwith strict equality, but we use \u2286 for compatibility with future definitions. Figure 6b\nillustrates the relation between release control and attacker control for robust programs.\nNote how release control is bounded by the attacker control at the next release event.\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n13\n\nExamples. We illustrate the definition of robustness with a few examples.\nConsider program [\u2022]; low := u < h, and memory such that m(h) = 7. This program is\nrejected by Definition 4.3. To see this, pick an a = u := 5, and consider the part of the trace\npreceding the low assignment. Release control R. (c[~\u2022], m, a, [(u, 5)]) is all attacks that reach\nthe assignment to low . On the other hand, the attacker control R(c[~\u2022], m, a, [(u, 5)]*(low , 1))\nis the set of all attacks where u < 7, which is smaller than R. . Therefore this program does\nnot satisfy the condition.\nProgram [\u2022]; low := h; low 0 := u < h satisfies robustness. The only release event\nhere corresponds to the first assignment. However, because the knowledge given by that\nassignment does not depend on untrusted variables, the release control includes all attacks\nthat reach the assignment.\nProgram [\u2022]; if u > 0 then low := h else skip is rejected. Consider memory m(h) = 7,\nand attack a = u := 1 that leads to low trace [(u, 1)]*(low , 7). The attacker control for this\nattack and trace is the set of all attacks such that u > 0. On the other hand, release control\nR. (c[~\u2022], m, ~a, [(u, 1)]) is the set of all attacks that lead to termination, which includes attacks\nsuch that u \u2264 0. Therefore, the release control corresponds to a bigger set than the attacker\ncontrol.\nProgram [\u2022]; while u > 0 do skip; low := h is accepted. Depending on the attackercontrolled variable the release event is reached. However, this is an example of availability\nattack, which is ignored by Definition 4.3.\nProgram [\u2022]; while u > h do skip; low := 1 is rejected. Any attack leading to the\nlow assignment restricts the control to attacks such that u \u2264 h. However, release control\nincludes attacks u > h, because the attacker learns information from divergence.\nThe definition of progress-insensitive robustness is similar to Definition 4.3, but uses\nprogress-insensitive variants of release events, control, and release control. As a result,\nprogram [\u2022]; while u > h do skip; low := 1 is accepted: attacker control is all attacks.\nDefinition 4.4 (Progress-insensitive robustness). Program c[~\u2022] satisfies progress-insensitive\nrobustness if for all memories m, attacks ~a, and traces ~t~r, such that hc[~a], mi\u2212\u2192\u2217~thc0 , m0 i\u2212\u2192\u2217~r\nand ~r contains a release event, i.e., k\u2192 (c[~a], mP , ~tP ) \u2283 k(c[~a], mP , ~tP *~rP ), we have\n.\nR\u2192\n(c[~\u2022], m, ~a, ~t) \u2286 R\u2192 (c[~\u2022], m, ~a, ~t*~r)\n\n5. Endorsement\nThis section extends the semantic policies for robustness in a way that allows endorsing\nattacker-provided values.\nSyntax and semantics. We add endorsement to the language:\nc[~\u2022] ::= . . . | x := endorse\u03b7 (e)\nWe assume that every endorsement in the program source has a unique endorsement label\n\u03b7. Semantically, endorsements produce endorsement events, denoted byendorse(\u03b7, v), which\nrecord the label of the endorsement statement \u03b7 together with the value v that is endorsed.\nhe, mi \u2193 v\nhx := endorse\u03b7 (e), mi\u2212\u2192endorse(\u03b7,v) hstop, m[x 7\u2192 v]i\n\n\f14\n\nA. ASKAROV AND A. C. MYERS\n\nWhenever the endorsement label is unimportant, we omit it from the examples. Note that\nendorse(\u03b7, v) events need not mention variable name x since that information is implied by\nthe unique label \u03b7.\nConsider example program [\u2022]; low := endorse\u03b71 (u < h). This program does not satisfy\nDefinition 4.3. The reasoning for this is exactly the same as for program [\u2022]; low := u < h\nfrom Section 4.\nIrrelevant attacks. Endorsement of certain values gives attacker some control over the\nknowledge. The key technical element of this section is the notion of irrelevant attacks,\nwhich defines the set of attacks that are endorsed, and that are therefore excluded when\ncomparing attacker control with release control. We define irrelevant attacks formally below, based on the trace that is produced by a program.\nGiven a program c[\u2022], starting memory m, and a trace ~t, irrelevant attacks, denoted\nhere by \u03a6(c[~\u2022], m, ~t), are the attacks that lead to the same sequence of endorsement events\nas in ~t, until they necessarily disagree on one of the endorsements. Because the influence of\nthese attacks is reflected at endorsement events, we exclude them from consideration when\ncomparing with attacker control.\nWe start by defining irrelevant traces. Given a trace ~t, irrelevant traces for ~t are all traces\n~t0 that agree with ~t on some prefix of endorsement events until they necessarily disagree on\nsome endorsement. We define this set as follows.\nDefinition 5.1 (Irrelevant traces). Given a trace ~t, where endorsements are marked as\nendorse(\u03b7j , vj ), define a set of irrelevant traces based on the number of endorsements in\n~t as \u03c6i (~t): \u03c60 (~t) = \u2205, and\n\u03c6i (~t) = {~t0 | ~t0 = ~q *endorse(\u03b7i , v 0 )* q~0 } such that\ni\n\n~q is a prefix of\nvi 6=\n\nvi0\n\nDefine \u03c6(~t) ,\n\nS\n\n~t0\n\nwith i \u2212 1 events all of which agree with endorse events in ~t, and\n\n~ as a set of irrelevant traces w.r.t. ~t.\n\ni \u03c6i (t)\n\nWith the definition of irrelevant traces at hand, we can define irrelevant attacks: irrelevant attacks are attacks that lead to irrelevant traces.\nDefinition 5.2 (Irrelevant attacks). Given a program c[~\u2022], initial memory m, and a trace\n~t, such that hc[~\u2022], mi\u2212\u2192\u2217~t, define irrelevant attacks \u03a6(c[~\u2022], m, ~t) as\n\u03a6(c[~\u2022], m, ~t) , {~a | hc[~a], mi\u2212\u2192\u2217~t0 \u2227 ~t0 \u2208 \u03c6(~t)}\nSecurity. The security conditions for robustness can now be adjusted to accommodate endorsements that happen along traces. The idea is to exclude irrelevant attacks from the\nleft-hand side of Definitions 4.3 and 4.4. This security condition, which has both progresssensitive and progress-insensitive versions, expresses roughly the same idea as qualified robustness [16], but in a more natural and direct way.\nDefinition 5.3 (Progress-sensitive robustness with endorsements). Program c[~\u2022] satisfies\nprogress-sensitive robustness with endorsement if for all memories m, attacks ~a, and traces\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\nattacks\n\nattacks\nR\n\nattacks\nR\n\n\u03a6\n\n\u03a6\nR\ufffd\n\nR\ufffd\n\nrelease events\n(a) Irrelevant attacks\n\n15\n\nrelease events\n(b) Robustness w/o\nendorsements (unsatisfied)\n\nrelease events\n(c) Robustness\nwith\nendorsements (satisfied)\n\nFigure 7. Irrelevant attacks and robustness with endorsements\n~t~r, such that hc[~a], mi\u2212\u2192\u2217~thc0 , m0 i\u2212\u2192\u2217~r and ~r contains a release event, i.e., k(c[~a], mP , ~tP ) \u2283\nk(c[~a], mP , ~tP *~rP ), we have\nR. (c[~\u2022], m, ~a, ~t) \\ \u03a6(c[~\u2022], m, ~t*~r) \u2286 R(c[~\u2022], m, ~a, ~t*~r)\nWe refer to the set R. (c[~\u2022], m, ~a, ~t) \\ \u03a6(c[~\u2022], m, ~t*~r) as a set of relevant attacks. Figures 7a\nto 7c visualize irrelevant attacks and the semantic condition of Definition 5.3. Figure 7a\nshows the set of irrelevant attacks, depicted by the shaded gray area. This set increases at\nendorsement events marked by stars. Figure 7b shows an example trace where robustness is\nnot satisfied - the gray area corresponding to release control R. exceeds the attacker control\n(depicted by the solid line). Finally, in Figure 7c, we superimpose Figures 7a and 7b. This\nillustrates that when the set of irrelevant attacks is excluded from the release control (the\narea under white dashed lines), the program is accepted by robustness with endorsements.\nExamples. Program [\u2022]; low := endorse\u03b71 (u < h) is accepted by Definition 5.3. Consider\ninitial memory m(h) = 7, and an attack u := 1; this produces a trace [(u, 1)]endorse(\u03b71 , 1).\nThe endorsed assignment also produces a release event. We have that\n\u2022 Release control R. is the set of all attacks that reach the low assignment.\n\u2022 Irrelevant traces \u03c6([(u, 1)]endorse(\u03b71 , 1)) is a set of traces that end in endorsement event\nendorse(\u03b71 , v) such that v 6= 1. Thus, irrelevant attacks \u03a6([\u2022]; low := endorse\u03b71 (u <\nh), m, [(u, 1)]endorse(\u03b71 , 1)) must consist of attacks that reach the low assignment and set\nu to values u \u2265 7.\n\u2022 The left-hand side of Definition 5.3 is therefore the set of attacks that reach the endorsement and set u to u < 7.\n\u2022 As for the attacker control on the right-hand side, it consists of attacks that set u < 7.\nHence, the set inclusion of Definition 5.3 holds and the program is accepted.\nProgram [\u2022]; low := endorse\u03b71 (u); low 0 := u < h00 is accepted. The endorsement in the first\nassignment implies that all relevant attacks must agree on the value of u, and, consequently,\nthey agree on the value of u < h00 , which gets assigned to low 0 . This also means that relevant\nattacks belong to the attacker control (which contains all attacks that agree on u < h00 ).\nProgram [\u2022]; low := endorse\u03b71 (u < h); low 0 := u < h00 is rejected. Take initial memory\nsuch that m(h) 6= m(h0 ). The set of relevant attacks after the second assignment contains\nattacks that agree on u < h (due to the endorsement), but not necessarily on u < h00 . The\nlatter, however, is the requirement for the attacks that belong to the attacker control.\n\n\f16\n\nA. ASKAROV AND A. C. MYERS\n\nProgram [\u2022]; if u > 0 then h 0 := endorse(u) else skip; low := h0 < h is rejected.\nAssume initial memory where m(h) = m(h0 ) = 7. Consider attack a1 that sets u := 1 and\nconsider the trace ~t1 that it gives. This trace endorses u in the then branch, overwrites the\nvalue of h0 with 1, and produces a release event (low , 1). Consider another attack a2 that sets\nu := 0, and consider the corresponding trace ~t2 . This trace contains release event (low , 0)\nwithout any endorsements. Now, attacker control R(c[~\u2022], m, a2 , ~t2 ) excludes a1 , because of\nthe disagreement at the release event. At the same time, a1 is a relevant attack for a2 ,\nbecause no endorsements happen along ~t2 .\nConsider program c[~\u2022], which contains no endorsements. In this case, for all possible\ntraces ~t, we have that \u03c6(~t) = \u03c60 (~t) = \u2205. Therefore, by Definition 5.2 it must be that\n\u03a6(c[~\u2022], m, ~t) = \u2205 for all memories m and traces ~t. This indicates that for programs without\nendorsements, progress-sensitive robustness with endorsements (Definition 5.3) conservatively reduces to the earlier definition of progress-sensitive robustness (Definition 4.3).\nProgress-insensitive robustness with endorsement is defined similarly. The intuition for\nthe definition remains the same, while we use progress-insensitive variants of progress control\nand control:\nDefinition 5.4 (Progress-insensitive robustness with endorsement). Program c[~\u2022] satisfies\nprogress-insensitive robustness with endorsement if for all memories m, attacks ~a, and traces\n~t*~r, such that hc[~a], mi\u2212\u2192\u2217~thc0 , m0 i\u2212\u2192\u2217~r , and ~r contains a release event, i.e., k\u2192 (c[~a], mP , ~tP ) \u2283\nk(c[~a], mP , ~tP *~rP ), we have\n.\nR\u2192\n(c[~\u2022], m, ~a, ~t) \\ \u03a6(c[~\u2022], m, ~t*~r) \u2286 R\u2192 (c[~\u2022], m, ~a, ~t*~r)\nAs a final note in this section, observe that because of the particular use of irrelevant\nattacks in Definitions 5.3 and 5.4 it is sufficient for us to define irrelevant traces so that they\nonly match at the endorsement events. A slightly more generalized notion of irrelevance\nwould require ~q in Definition 5.1 to be similar to a prefix of ~t0 .\n\n6. Enforcement\nWe now explore how to enforce robustness using a security type system. While this section\nfocuses on progress-insensitive enforcement, it is possible to refine the type system to deal\nwith progress sensitivity (modulo availability attacks) [26, 19]. Figures 8 and 9 display\ntyping rules for expressions and commands. This type system is based on the one of [16]\nand is similar to many standard security type systems.\nDeclassification. We extend the language with a language construct for declassification of\nexpressions declassify(e). Whereas in earlier examples, we considered an assignment l := h\nto be secure if it did not violate robustness, we now require information flows from public\nto secret to be mediated by declassification. We note that declassification has no additional\nsemantics and, in the context of our simple language, can be inferred automatically. This\nmay be achieved by placing declassifications in public assignments that appear in trusted\ncode, i.e., in non-\u2022 parts of the program. Moreover, making declassification explicit has the\nfollowing motivations:\n(1) On the enforcement level, the type system conveniently ensures that a non-progress release event may happen only at declassification. All other assignments preserve progressinsensitive knowledge.\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n\u0393 ` n : `, \u2205\n\n\u0393 ` x : \u0393(x), \u2205\n\n17\n\n\u0393 ` e1 : `1 , D1\n\u0393 ` e2 : `2 , D2\n\u0393 ` e1 op e2 : `1 t `2 , D1 \u222a D2\n\n(T-DECL)\n\n\u0393 ` e : `, D\n\u0393 ` declassify(e) : ` u (P, U), vars(e)\nFigure 8. Type system: expressions\n(T-SEQ)\n\n(T-SKIP)\n\n\u0393, pc ` c1\n\u0393, pc ` c2\n\u0393, pc ` c1 ; c2\n\n\u0393, pc ` skip\n(T-ASGMT)\n\n\u0393 ` e : `, D\n\n` t pc v \u0393(x)\n\n\u2200y \u2208 D . \u0393(y) v (S, T)\n\u0393, pc ` x := e\n\nD 6= \u2205 =\u21d2 pc v (P, T)\n\n(T-IF)\n\n(T-WHILE)\n\n\u0393 ` e : `, \u2205\n\u0393, pc t ` ` c1\n\u0393, pc t ` ` c2\n\u0393, pc ` if e then c1 else c2\n\n\u0393 ` e : `, \u2205\n\u0393, pc t ` ` c\n\u0393, pc ` while e do c\n\n(T-HOLE)\n\n(T-ENDORSE)\n\npc v (P, U)\n\u0393, pc ` \u2022\n\npc t \u0393(x) v (S, T)\n\npc v \u0393(x)\n\u0393 ` e : `, \u2205\n\u0393, pc ` x := endorse(e)\n\n` u (S, T) v \u0393(x)\n\nFigure 9. Type system: commands\n(2) Much of the related work on language-based declassification policies uses similar type\nsystems. Showing our security policies can be enforced using such systems makes the\nresults more general.\nTyping of expressions. Type rules for expressions have form \u0393 ` e : `, D where ` is the level\nof the expression, and D is a set of variables that may be declassified. The declassification\nis the most interesting rule among expressions. It downgrades the confidentiality level of\nthe expression by returning ` u (P, U), and counts all variables in e as declassified.\nTyping of commands. The typing judgments for commands have the form \u0393, pc ` c. The\nrules are standard for a security type system. We highlight typing of assignments, endorsement, and holes.\nAssignments have two extra clauses for when the assigned expression contains a declassification (D 6= \u2205). The rule (T-ASGMT) requires all variables that can be declassified\nhave high integrity. The rule also bounds the pc-label by (P, T), which enforces that no\ndeclassification happens in untrusted or secret contexts. These requirements guarantee that\nthe information released by the declassification does not directly depend on the attackercontrolled variables.\n\n\f18\n\nA. ASKAROV AND A. C. MYERS\n\nSequential composition 1\n\nAdvancement Lemma\n\nSequential composition 2\n\nProposition 1\n\nControl backbone Lemma\n\nFigure 10. High-level structure of proof of Proposition 6.1\nThe typing rule for endorsement (T-ENDORSE) requires that the pc-label is trusted\nand that the result of the endorsement is stored in a trusted variable: pc t \u0393(x) v (S, T).\nNote that requiring a trusted pc-label is crucial, while the restriction that x is trusted could\neasily be lifted, since trusted values may flow into untrusted variables. Because endorsed\nexpressions preserve their confidentiality level, we also check that x has the right security\nlevel to store the result of the expression. This is done by demanding that ` u (S, T) v \u0393(x),\nwhere taking meet of ` and (S, T) boosts integrity, but keeps the confidentiality level of `.\nThe rule for holes forbids placing attacker-provided code in high confidentiality contexts.\nFor simplicity, we disallow declassification in the guards of if and while.\n6.1. Soundness\nThis section shows that the type system of Figures 8 and 9 is sound. We formulate top-level\nsoundness in Proposition 6.1. The proof of Proposition 6.1 appears in the end of the section.\nProposition 6.1. If \u0393, pc ` c[~\u2022] then for all attacks ~a, memories m, and traces ~t*~r produced\nby hc[~a], mi, where k\u2192 (c[~a], mP , ~tP ) \u2283 k(c[~a], mP , ~tP *~rP ), we have that\n.\nR\u2192\n(c[~\u2022], m, ~a, ~t) \\ \u03a6(c[~\u2022], m, ~t*~r) \u2286 R\u2192 (c[~\u2022], m, ~a, ~t*~r)\nAuxiliary definitions. We introduce an auxiliary definition of progress-insensitive noninterference along a part of a trace, abbreviated PINI, which we will use in the proof of Proposition 6.1. Figure 10 shows the high-level structure of the proof. We define declassification\nevents to be low events that involve declassifications. The central property of this proof\n- the control backbone lemma (Lemma 6.8) - captures the behavior of similar attacks\nand traces that are generated by well-typed commands. Together with the Advancement\nLemma, it shows that declassification events soundly approximate release events. The proof\nof Proposition 6.1 follows directly from the Control Backbone and Advancement lemmas.\nDefinition 6.2 (Progress-insensitive noninterference along a part of a trace). Given a program c, memory m, and two traces ~t and ~t+ such that ~t+ is an extension of ~t, we say that\nc satisfies progress-insensitive noninterference along the part of the trace from ~t to ~t+ , denoted by PINI(c, m, ~t, ~t+ ) whenever for the low events in the corresponding traces ~`n , ~tP\nand ~`N , ~t+\nP , n \u2264 N , it holds that\n\u2200i . n < i < N . k\u2192 (c, mP , ~`i ) \u2286 k(c, mP , ~`i+1 )\nLemma 6.3 (Noninterference for no declassifications). Given a program c without declassifications such that \u0393, pc ` c then for all memories m and possible low events ~` * `0 such\nthat\nhc, mi\u2212\u2192\u2217 ~`hc0 , m0 i\u2212\u2192\u2217 `0 hc00 , m00 i\nit holds that k\u2192 (c, m, ~`) \u2286 k(c, m, ~`*`0 ).\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n19\n\n2\n\nProof. By induction on c (cf. [1]).\n\nLemma 6.4 (Noninterference for the tail of sequential composition). Assume a program c\nsuch that for all memories m and low events `, such that hc, mi\u2212\u2192\u2217 ` hc0 , m0 i, it holds that\nk\u2192 (c, mP , \u000f) \u2286 k(c, mP , `). Then for all programs c0 , initial memories i, and low events ~l0 ,\nsuch that\nhc0 ; c, ii\u2212\u2192\u2217 ~`0 hc, i0 i\u2212\u2192\u2217 `0\nwe have k\u2192 (c0 ; c, iP , ~`0 ) \u2286 k(c0 ; c, iP , ~`0 *`0 ).\nProof. Assume the set inclusion of the lemma's statement does not hold. By Definition 2.2,\nthere must exist an initial memory m, such that m =P i and hc0 ; c, mi\u2212\u2192\u2217 ~`0 hc, m0 i\u2212\u2192\u2217 `00 ,\nbut `0 6= `00 . Because m =P i and both traces produce ~`0 , it must also be that m0 =P i0 . But\nthis also implies that m0 6\u2208 k(c, i0P , `0 ), that is, k\u2192 (c, i0P , \u000f) 6\u2286 k(c, i0P , `0 ), which contradicts\nthe main assumption about c.\n2\nThe following two helper lemmas correspond to the sequential composition sub-cases of\nthe Advancement Lemma. Lemma 6.5 captures the special case when the first command\nin the sequential composition c1 [\u2022]; c2 [\u2022] does not produce a declassification event, while\nLemma 6.6 considers the general case when a declassification event may be produced by\neither of c1 [\u2022] or c2 [\u2022].\nLemma 6.5 (Sequential composition 1). Given\n\u2022 program ~c0 [~\u2022] such that \u0393, pc ` c0 [~\u2022],\n\u2022 initial memory m0 ,\n\u2022 two initial attacks ~a0 , ~b0 ,\n\u2022 two intermediate configurations hc1 [~a1 ]; c2 [~a2 ], mi and hc1 [~b1 ]; c2 [~b2 ], si such that\n\u2022 hc0 [~a0 ], m0 i\u2212\u2192\u2217~t0 hc1 [~a1 ]; c2 [~a2 ], mi\u2212\u2192\u2217~t\u03b1 hc2 [~a2 ], m0 i\u2212\u2192\u2217~t\u03b2*r\n\u2022 hc0 [~b0 ], m0 i\u2212\u2192\u2217 ~0 hc1 [~b1 ]; c2 [~b2 ], si\u2212\u2192\u2217 ~00 0\nq\n\nq *r\n\n\u2022 PINI(c0 [~a0 ], m0 , ~t0 , ~t0 *~t\u03b1 *~t\u03b2 )\n\u2022 PINI(c0 [~b0 ], m0 , q~0 , q~0 * q~00 )\n\u2022 r and r0 are declassification events\n\u2022 ~b0 6\u2208 \u03a6(c0 [~\u2022], m0 , ~t0 *~t\u03b1 *~t\u03b2 *r)\n\u2022 ~t0? = q~0 ?\n\u2022 m =T s\nthen q~00 = ~q\u03b1 *~q\u03b2 such that\n\u2022 hc1 [~a1 ]; c2 [~a2 ], si\u2212\u2192\u2217 q~\u03b1 hc2 [~a2 ], s0 i\u2212\u2192\u2217 q~\u03b2*r\n\u2022 t~\u03b1? = q~\u03b1 ?\n\u2022 m0 =T s0\nProof. By induction on the structure of c1 [~\u2022]. Case skip is immediate. Consider the other\ncases.\n\u2022 case [~\u2022]\nIn this case ~a1 = a1 and ~b1 = b1 . By assumption, a1 and b1 are fair attacks, which\nmeans that ~t\u03b1 has no release events and no assignments to trusted variables. Similarly,\nbecause no low assignments can be produced when running ~b1 , then by Definition 3.2\nthere must be s0 and ~q\u03b1 that would satisfy the demand of the lemma.\n\n\f20\n\nA. ASKAROV AND A. C. MYERS\n\n\u2022 case x := e\nWe consider confidentiality and integrity properties separately.\nConfidentiality: We show that even if a low event is possible, it is not a release event.\nWe have two cases, based on the confidentiality level of x.\n(a) \u0393(x) = (P, _)\nA low event is generated by the low assignment. By Lemma 6.3 and Lemma 6.4\nthe assignment must not be a release event.\n(b) \u0393(x) = (S, _)\nIn this case no low events are generated.\nIntegrity: Next, we show that the resulting memories agree on trusted values. The two\ncases are\n(a) \u0393(x) = (_, T) In this case it must be that \u0393(e) = (_, T) and, hence, m(e) = s(e).\nTherefore m0 =T s0 .\n(b) \u0393(x) = (_, U) Assignment to x does not change how memories agree on trusted\nvalues.\n\u2022 case x := endorse\u03b7 (e)\nWe consider the confidentiality and integrity properties of this command separately.\nConfidentiality: Similar to the case for assignment.\nIntegrity: We consider two cases.\n(a) \u0393(x) = (_, T)\nIn this case, the trace produces an event endorse(\u03b7, v). We note ~b0 6\u2208 \u03a6(c0 [~\u2022], m0 , ~t0*\n~t\u03b1*~t\u03b2*r). In particular, we have that q~0*q~00*r0 6\u2208 \u03c6(~t0*~t\u03b1*~t\u03b2*r). If we assume that the current\ncommand is the i-th endorsement in the trace, we have that q~0*q~00*r0 6\u2208 \u03c6i (~t0*~t\u03b1*~t\u03b2 *r).\nBut we also know that ~t0 ? =T q~0 ? . Because, by the rule (T-ENDORSE), the result\nof endorsement is assigned to trusted variables, this implies that both q~0 and ~t0\nmust agree on the endorsed values. Therefore, the only possibility with which\nq~0 * q~00 *r0 6\u2208 \u03c6i (~t) is that q~00 generates endorse(\u03b7, v) as well. This implies that value\nv is assigned to x in both cases, which guarantees that m0 =T s0 .\n(b) \u0393(x) = (_, U) Not applicable by (T-ENDORSE).\n\u2022 case c\u03b1 ; c\u03b2\nBy two applications of induction hypothesis: one to c\u03b1 ; (c\u03b2 ; c2 [~\u2022]) and the other one to\nc\u03b2 ; c2 [~\u2022].\n\u2022 case if e then ctrue else cfalse\nWe have the following cases based on the type of expression e.\n(a) \u0393(e) = (_, T)\nIn this case both branches are taking the same branch and we are done by induction\nhypothesis.\n(b) \u0393(e) = (_, U)\nIn this case neither of ctrue or cfalse contain declassifications or high integrity assignments. This guarantees that m0 =T s0 .\n\u2022 case while e do cloop\nSimilar to sequential composition and conditionals.\n2\nLemma 6.6 (Sequential composition 2). Given\n\u2022 program c0 [~\u2022] such that \u0393, pc ` c0 [~\u2022]\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n21\n\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\ninitial memory m0\ntwo initial attacks ~a0 , ~b0\ntwo intermediate configurations hc1 [~a1 ]; c2 [~a2 ], mi and hc1 [~b1 ]; c2 [~b2 ], si such that\nhc0 [~a0 ], m0 i\u2212\u2192\u2217 t~0 hc1 [~a1 ]; c2 [~a2 ], mi\u2212\u2192\u2217 t~00 hc01 [~a01 ]; c2 [~a2 ], m0 i\u2212\u2192(x,v) hc001 [~a001 ]; c2 [~a2 ], m00 i\nhc0 [~b0 ], m0 i\u2212\u2192\u2217 q~0 hc1 [~b1 ]; c2 [~b2 ], si\u2212\u2192\u2217 q~00 hd0 , s0 i\u2212\u2192(y,u) hd00 , s00 i\n\u2022 PINI(c0 [~a0 ], m0 , ~t0 , ~t0 * t~00 )\n\u2022 PINI(c0 [~b0 ], m0 , q~0 , q~0 * q~00 )\n\u2022 (x, v) and (y, u) are declassification events\n\u2022 ~b0 6\u2208 \u03a6(c0 [~\u2022], m0 , ~t0 * t~00 *(x, v))\n\u2022 ~t0 ? = q~0 ?\n\u2022 m =T s\nthen\n\u2022 ~a01 = ~a001\n\u2022 there is b01 such that\n\u2022 d0 = c01 [~b01 ]; c2 [~b2 ]\n\u2022 d00 = c001 [~b01 ]; c2 [~b2 ]\n\u2022 t~00 ? = q~00 ?\n\u2022 m0 =T s0\n\u2022 m00 =T s00\n\u2022 (x, v) = (y, u).\nProof. By induction on the structure of c1 [~\u2022]. In the cases of [~\u2022], skip, and x := endorse(e)\nno declassification events may be produced, so these cases are impossible.\n\u2022 x := e. When D = \u2205, no declassification events may be produced. When D 6= \u2205, a\ndeclassification event is produced by both traces. Also, t~00 = q~00 = \u000f, and m0 = m and\ns0 = s. Because m =T s and \u0393(e) = (_, T) we have that both traces produces the same\ndeclassification event (x, v), and therefore, m00 =T s00 .\n\u2022 case c\u03b1 [a\u03b1 ]; c\u03b2 [a\u03b2 ]\nWe have two cases depending on whether c\u03b1 [a\u03b1 ] generates low events:\n(a) hc\u03b1 [a\u03b1 ]; (c\u03b2 [a\u03b2 ]; c2 [a2 ]), mi\u2212\u2192\u2217 `1 ***`N hc\u03b2 [a\u03b2 ]; c2 [a2 ], m0 i In this case by Lemma 6.5 it\nmust be that hc\u03b1 [b\u03b1 ]; (c\u03b2 [b\u03b2 ]; c2 [b2 ]), mi\u2212\u2192\u2217 `1 ***`0N hc\u03b2 [b\u03b2 ]; c2 [b2 ], s0 i such that m0 =T s0 .\nThen we can apply the induction hypothesis to c\u03b2 [\u2022].\n(b) In this case (x, v) is produced by c\u03b1 [a\u03b1 ] and we are done by application of induction\nhypothesis to c\u03b1 [\u2022].\n\u2022 case if e then ctrue else cfalse\nWe have two cases:\n(a) \u0393(e) = (_, T)\nIn this case both branches take the same command, and we are done by the induction\nhypothesis.\n(b) \u0393(e) = (_, U).\nImpossible, because declassification events are not allowed in untrusted integrity contexts.\n\u2022 case while e do cloop\nSimilar to sequential composition and conditionals.\n\n\f22\n\nA. ASKAROV AND A. C. MYERS\n\n2\nLemma 6.7 (Advancement). Given\n\u2022 program c0 [~\u2022] such that \u0393, pc ` c0 [~\u2022]\n\u2022 initial memory m0\n\u2022 two initial attacks ~a0 , ~b0\n\u2022 two intermediate configurations hc[~a], mi and hc[~b], si such that\n\u2022 hc0 [~a0 ], m0 i\u2212\u2192\u2217 t~0 hc[~a], mi\u2212\u2192\u2217 t~00 hc0 [~a0 ], m0 i\u2212\u2192(x,v) hc00 [~a00 ], m00 i\n\u2022 hc0 [~b0 ], m0 i\u2212\u2192\u2217 q~0 hc[~b], si\u2212\u2192\u2217 q~00 hd0 , s0 i\u2212\u2192(y,u) hd00 , s00 i\n\u2022 PINI(c0 [~a0 ], m0 , ~t0 , ~t0 * t~00 )\n\u2022 PINI(c0 [~b0 ], m0 , q~0 , q~0 * q~00 )\n\u2022 (x, v) and (y, u) are declassification events\n\u2022 ~b 6\u2208 \u03a6(c0 [~\u2022], m0 , ~t0 * t~00 *(x, v))\n\u2022 ~t0 ? = q~0 ?\n\u2022 m =T s\nthen\n\u2022 ~a0 = ~a00\n\u2022 there is b0 such that\n\u2022 d0 = c0 [~b0 ]\n\u2022 d00 = c00 [~b0 ]\n\u2022 t~00 ? = q~00 ?\n\u2022 m0 =T s0\n\u2022 m00 =T s00\n\u2022 (x, v) = (y, u).\nProof. By induction on c[~\u2022]. In the cases of [~\u2022], skip, and x := endorse(e), no declassification events may be produced, so these cases are impossible.\n\u2022 x := e. In case D = \u2205, no declassification events may be produced. When D 6= \u2205, a\ndeclassification event is produced by both traces. Also, t~00 = q~00 = \u000f, and m0 = m and\ns0 = s. Because m =T s and \u0393(e) = (_, T) we have that both traces produces the same\ndeclassification event (x, v), and therefore, m00 =T s00 .\n\u2022 case c\u03b1 ; c\u03b2\nBy Lemma 6.6.\n\u2022 case if e then ctrue else cfalse\nWe have two cases:\n(a) \u0393(e) = (_, T)\nIn this case both branches take the same command and we are done by induction\nhypothesis.\n(b) \u0393(e) = (_, U).\nImpossible, because declassification events are not allowed in untrusted integrity contexts.\n\u2022 case while e do cloop\nSimilar to sequential composition and conditionals.\n2\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n23\n\nLemma 6.8 (Control Backbone). Given \u0393, pc ` c[\u2022], memory m, an initial attack ~a and a\ntrace ~t, such that\nhc[~a], mi\u2212\u2192\u2217~t1 hc1 [~a1 ], m1 i\u2212\u2192r1 hc01 [~a01 ], m01 i\u2212\u2192\u2217~t2 ***ri\u22121\nhc0i\u22121 [a~0 i\u22121 ], m0i\u22121 i\u2212\u2192\u2217~ti hci [~ai ], mi i\u2212\u2192ri hc0i [a~0 i ], m0i i \u2212\u2192\u2217 . . .\nc[~\u2022],m ~\nwhere ri are declassification events, then for all ~b, ~q such that (~a, ~t) \u223c\u2192\n(b, ~q) and ~b 6\u2208\n~\n\u03a6(c[~\u2022], m, t), it holds that the respective configurations (highlighted in boxes here) match at\nthe declassification events, that is\n\nhc[~b], mi\u2212\u2192\u2217 q~1 hc1 [~b1 ], s1 i\u2212\u2192r1 hc01 [b~0 1 ], s01 i\u2212\u2192\u2217 q~2 ***ri\u22121\nhc0i\u22121 [b~0 i\u22121 ], s0i\u22121 i\u2212\u2192\u2217 q~i hci [~bi ], si i\u2212\u2192ri hc0i [b~0 i ], s0i i \u2212\u2192\u2217 . . .\nwhere i ranges over the number of declassification events in ~t, and moreover\n\u2022 mi =T si and m0i =T s0i\n\u2022 q~i ? = t~i?\nProof. By induction on the number of declassification events. The base case, where n = 0,\nis immediate. For the inductive case, assume the proposition holds for the first n declassification events in ~t, and apply Lemma 6.7.\n2\nWe conclude this section with the proof of Proposition 6.1.\n. (c[~\nProof of Proposition 6.1 Consider ~b \u2208 R\u2192\n\u2022], m, ~a, ~t) \\ \u03a6(c[~\u2022], m, ~t*~r). We want\n. (c[~\n~\n~\nto show that b \u2208 R\u2192 (c[~\u2022], m, ~a, ~t * ~r). Because b \u2208 R\u2192\n\u2022], m, ~a, ~t), we have that ~b \u2208\nc[~\u2022],m ~\n{~b | \u2203~q . (~a, ~t) \u223c\u2192\n(b, ~q) \u2227 (\u2203r~0 . k\u2192 (c[~b], mP , ~qP ) \u2283 k(c[~b], mP , ~qP *r~0 P ) \u2228 hc[~b], mi \u21d3)}. We\nconsider the two cases\nc[~\u2022],m ~\n(1) ~b \u2208 {~b | \u2203~q . (~a, ~t) \u223c\u2192\n(b, ~q) \u2227 \u2203r~0 . k\u2192 (c[~b], mP , ~qP ) \u2283 k(c[~b], mP , ~qP * r~0 P )}\n\nBy definition of \u03a6(c[~\u2022], m, ~t*~r), we have that ~b 6\u2208 \u03a6(c[~\u2022], m, ~t*~r) =\u21d2 ~b 6\u2208 \u03a6(c[~\u2022], m, ~t).\nBy the Control Backbone Lemma 6.8, we have that two traces agree on the declassification points up to the length of ~t, and in particular there are ~t0 , ~t1 , ~q0 , ~q1 such that\n~t = ~t0 *~t1 *~r and ~q = ~q0 *~q1 and that there are no release events along ~t1 and ~q1 , for which\nit holds that\nhc[~a], mi\u2212\u2192\u2217~t0 hc0 [~a0 ], m0 i\u2212\u2192\u2217~t1*~r\nand\nhc[~b], mi\u2212\u2192\u2217 q~0 hc0 [~b0 ], s0 i\u2212\u2192\u2217 q~1*r~0\nwhere ~t? = ~q? and m0 =T s0 . By Advancement Lemma 6.7, we obtain that both traces\nmust agree on ~r and r~0 . This is sufficient to extend the original partitioning of (~a, ~t) and\nc[~\u2022],m ~\n(~b, ~q) to (~a, ~t*~r) and (~b, ~q0 *~q1 * r~0 ) such that (~a, ~t*~r) \u223c\u2192\n(b, ~q0 *~q1 * r~0 ).\nc[~\n\u2022\n],m\n(2) ~b \u2208 {~b | \u2203~q . (~a, ~t) \u223c\u2192\n(~b, ~q) \u2227 hc[~b], mi \u21d3}\nThis case is impossible. By the Control Backbone Lemma 6.8 there must be two\nrespective configurations hc0 [~a0 ], m0 i and hc0 [~b0 ], s0 i where m0 =T s0 , such that hc0 [~a0 ], m0 i\n\n\f24\n\nA. ASKAROV AND A. C. MYERS\n\nleads to a release event, but hc0 [~b0 ], s0 i terminates without release events. By analysis of\nc0 , similar to the Advancement Lemma, we conclude that none of the cases is possible. 2\n\n7. Checked endorsement\nRealistic applications endorse attacker-provided data based on certain conditions. For instance, an SQL string that depends on user-provided input is executed if it passes sanitization, a new password is accepted if the user can provide an old one, and a secret key is\naccepted if nonces match. Because this is a recurring pattern in security-critical applications,\nwe argue for language support in the form of checked endorsements.\nThis section extends the language with checked endorsements and derives both security\nconditions and a typing rule for them. Moreover, we show checked endorsements can be\ndecomposed into a sequence of direct endorsements, and prove that for well-typed programs,\nthe semantic conditions for robustness are the same with checked endorsements and with\nunchecked endorsements.\nSyntax and semantics. In the scope of this section, we assume checked endorsements are\nthe only endorsement mechanism in the language. We introduce a syntax for checked endorsements:\nc[~\u2022] ::= . . . | endorse\u03b7 (x) if e then c else c\nThe semantics of this command is that a variable x is endorsed if the expression e evaluates\nto true. If the check succeeds, the then branch is taken, and x is assumed to have high\nintegrity there. If the check fails, the else branch is taken. As with direct endorsements,\nwe assume checked endorsements in program text have unique labels \u03b7. These labels may\nbe omitted from the examples, but they are explicit in the semantics.\nEndorsement events. Checked endorsement events checked (\u03b7, v, b) record the unique label\nof the endorsement command \u03b7, the value of variable that can potentially be endorsed v,\nand a result of the check b, which can be either 0 or 1.\nm(e) \u2193 v\n\nv 6= 0\nchecked(\u03b7,m(x),1)\n\nhendorse\u03b7 (x) if e then c1 else c2 , mi\nm(e) \u2193 v\n\n\u2212\u2192\n\nhc1 , mi\n\nv=0\nchecked(\u03b7,m(x),0)\n\nhendorse\u03b7 (x) if e then c1 else c2 , mi\n\n\u2212\u2192\n\nhc2 , mi\n\nIrrelevant attacks. For checked endorsement we define a suitable notion of irrelevant attacks. The reasoning behind this is the following.\n(1) Both ~t and ~t0 reach the same endorsement statement: \u03b7i = \u03b7i0 .\n(2) At least one of them results in the positive endorsement: bi + b0i \u2265 1. This ensures that\nif both traces do not take the branch then none of the attacks are ignored.\n(3) The endorsed values are different: vi 6= vi0 . Otherwise, there should be no further\ndifference in what the attacker can influence along the trace.\nThe following definitions formalize the above construction.\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n25\n\nDefinition 7.1 (Irrelevant traces). Given a trace ~t, where endorsements are labeled as\nchecked (\u03b7j , vj , bj ), define a set of irrelevant traces based on the number of checked endorsements in ~t as \u03c8i (~t). Then \u03c80 (~t) = \u2205, and\n\u03c8i (~t) = {~t0 | ~t0 = ~q *checked (\u03b7i , v 0 , b0 )* q~0 } such that\ni\n\ni\n\n~q is a prefix of ~t0 with i \u2212 1 checked events, all of which agree with checked events in ~t,\n(bi + b0i \u2265 1) \u2227 (vi 6= vi0 ), and\nq~0 contains no checked events\nS\nDefine \u03c8(~t) , i \u03c8i (~t) as a set of irrelevant traces w.r.t. ~t.\nDefinition 7.2 (Irrelevant attacks ). \u03a8(c[~\u2022], m, ~t) , {~a| hc[~a], mi\u2212\u2192\u2217 t~0 \u2227 ~t0 \u2208 \u03c8(~t)}\nUsing this definition, we can define security conditions for checked robustness.\nDefinition 7.3 (Progress-sensitive robustness with checked endorsement). Program c[~\u2022]\nsatisfies progress-sensitive robustness with checked endorsement if for all memories m and\nall attacks ~a, such that hc[~a], mi\u2212\u2192\u2217~a hc0 , m0 i\u2212\u2192\u2217~r , and ~r contains a release event, i.e.,\nk(c[~a], mP , ~tP ) \u2283 k(c[~a], mP , ~tP *~rP ), we have\nR. (c[~\u2022], m, ~a, ~t) \\ \u03a8(c[~\u2022], m, ~t*~r) \u2286 R(c[~\u2022], m, ~a, ~t*~r)\nThe progress-insensitive version is defined similarly, using progress-insensitive definition for\nrelease events and progress-insensitive versions of control and release control.\nExample. In program [\u2022]; endorse\u03b71 (u) if u = u0 then low := u < h else skip, the\nattacker can modify u and u0 . This program is insecure because the unendorsed, attackercontrolled variable u0 influences the decision to declassify. To see that Definition 7.3 rejects\nthis program, consider running it in memory with m(h) = 7, and two attacks: a1 , where\nattacker sets u := 5; u0 := 0, and a2 , where attacker sets u := 5; u0 = 5. Denote the corresponding traces up to endorsement by ~t1 and ~t2 . We have ~t1 = [(u, 5)*(u0 , 0)]*checked (\u03b71 , 5, 0)\nand ~t2 = [(u, 5) * (u0 , 5)] * checked (\u03b71 , 5, 1). Because endorsement in the second trace succeeds, this trace also continues with a low event (low , 1). Following Definition 7.1 we\nhave that t1 6\u2208 \u03c8(~t2 * (low , 1)), implying a1 6\u2208 \u03a8(c[~\u2022], m, ~t2 * (low , 1)). Therefore, a1 \u2208\nR. (c[~\u2022], m, ~a2 , ~t2 ) \\ \u03a8(c[~\u2022], m, ~t2 *(low , 1)). On the other hand, a1 6\u2208 R(c[~\u2022], m, ~a2 , ~t2 *(low , 1))\nbecause a1 can produce no low events corresponding to (low , 1).\nEndorsing multiple variables. The syntax for checked endorsements can be extended to\nmultiple variables with the following syntactic sugar, where \u03b7i is an endorsement label\ncorresponding to variable xi :\nendorse(x1 , . . . xn ) if e then c1 else c2 =\u21d2 endorse\u03b71 (x1 ) if e then\nendorse\u03b72 (x2 ) if true then . . . c1 else skip . . . else c2\nNote that in this encoding the condition is checked as early as possible; an alternative\nencoding here would check the condition in the end. While such encoding would have an\nadvantage of type checking immediately, we believe that checking the condition as early as\n\n\f26\n\nA. ASKAROV AND A. C. MYERS\n\npossible avoids spurious (albeit harmless in this simple context) endorsements of all but the\nlast variable, and is therefore more faithful semantically.\nTyping checked endorsements. To enforce programs with checked endorsements, we extend\nthe type system with the following general rule:\n(T-CHECKED)\n\u03930 , \u0393[xi 7\u2192 \u0393(xi ) u (S, T)]\n\u03930 ` e : `0 , D0\npc 0 , pc t `0\n0\n0\n0\n0\npc v (S, T)\n\u0393 , pc ` c1\n\u0393, pc ` c2\n\u0393, pc ` endorse(x1 , . . . , xn ) if e then c1 else c2\n\nThe expression e is type-checked in an environment \u03930 in which endorsed variables x1 , . . . xn\nhave trusted integrity; its label `0 is joined to form auxiliary pc-label pc 0 . The level of\npc 0 must be trusted, ensuring that endorsements happen in a trusted context, and that\nno declassification in e depends on untrusted variables other than the xi (this effectively\nsubsumes the need to check individual variables in D0 ). Each of the branches is type-checked\nwith the program label set to pc 0 ; however, for c1 we use the auxiliary typing environment\n\u03930 , since the xi are trusted there.\nProgram [\u2022]; endorse(u) if u = u0 then low := declassify(u < h) else skip is\nrejected by this type system. Because variable u0 is not endorsed, the auxiliary pc-label has\nuntrusted integrity.\n7.1. Relation to direct endorsements\nFinally, for well-typed programs we can safely translate checked endorsements to direct endorsements using a translation in which a checked endorsement of n variables is translated\nto n + 1 direct endorsements. First, we unconditionally endorse the result of the check. The\nrest of the endorsements happen in the then branch, before translation of c1 . We save the\nresults of the endorsements in temporary variables t1 . . . tn and replace all occurrences of\nx1 . . . xn within c1 with the temporary ones (we assume that each ti has the same confidentiality level as the corresponding original xi , and t0 has the confidentiality level of the\nexpression e). All other commands are translated to themselves.\nDefinition 7.4 (Labeled translation from checked endorsements to direct endorsements).\nGiven a program c[~\u2022] that only uses checked endorsements, we define its labeled translation\nto direct endorsements Jc[~\u2022]K inductively:\n\u2022 Jendorse\u03b7 (x1 , . . . xn ) if e then c1 else c2 K =\u21d2 t0 := endorse\u03b70 (e); if t0\nthen t1 := endorse\u03b71 (x1 ); . . . tn := endorse\u03b7n (xn ); Jc1 [ti /xi ]K else Jc2 K\n\u2022 Jc1 ; c2 K =\u21d2 Jc1 K; Jc2 K\n\u2022 Jif e then c1 else c2 K =\u21d2 if e then Jc1 K else Jc2 K\n\u2022 Jwhile e do cK =\u21d2 while e do JcK\n\u2022 JcK =\u21d2 c, for other commands c.\nAdequacy of translation for checked endorsements for well-typed programs. Next we show\nadequacy of the labeled translation of Definition 7.4 for well-typed programs. Note that for\nnon-typed programs this adequacy does not hold, as shown by an example in the end of the\nsection.\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n27\n\nWithout loss of generality, we assume checked endorsements have only one variable\n(n = 1 in the translation of checked endorsement in Definition 7.4). We adopt an indexing\nconvention where checked endorsement with the label \u03b7i , corresponds to two direct endorsements with the labels \u03b72i\u22121 and \u03b72i . The following lemma establishes a connection between\nirrelevant attacks of the source and translated runs.\nLemma 7.5 (Synchronized endorsements). Given a program c[~\u2022] that only uses checked\nendorsements, such that \u0393, pc ` c[~\u2022], memory m, and attack ~a, such that\nhc[~a], mi\u2212\u2192\u2217~t and hJc[~a]K, mi\u2212\u2192\u2217~\u02c6\nt\n\nwhere\n\u2022 ~t = ~t0 *checked (\u03b7i , vi , bi ) and\n\u02c6\n\u02c6\n\u2022 ~t\u02c6 = ~t0 *endorse(\u03b72i\u22121 , 0) or ~t\u02c6 = ~t0 *endorse(\u03b72i\u22121 , 1)*endorse(\u03b72i , v)\n\u2022 k is a number of checked endorse events in ~t and\nwe have that\n\u2022 R(c[~\u2022], m, ~a, ~t) = R(Jc[~\u2022]K, m, ~a, ~t\u02c6).\n\u2022 R\u2192 (c[~\u2022], m, ~a, ~t) = R\u2192 (Jc[~\u2022]K, m, ~a, ~t\u02c6).\n\u2022 \u03a6(Jc[~\u2022]K, m, ~t\u02c6) = \u03a8(c[~\u2022], m, ~t)\nProof. The first two items follow from the definition of the translation, because the translation does not generate new release events.\nTo prove the second item, we consider partitions of irrelevant traces generated by every\nk-th checked endorsement and the direct endorsement(s) that correspond to it. We proceed\nby induction on k. For the base case, k = 0, i.e., neither ~t nor ~t\u02c6 contain endorsements,\nit holds that \u03a6(Jc[~\u2022]K, m, ~t\u02c6) = \u03a8(c[~\u2022], m, ~t) = \u2205. For the inductive case, define a pair of\nauxiliary sets\n\u02c6\nFk , \u03a6(Jc[~\u2022]K, m, ~t\u02c6) \\ \u03a6(Jc[~\u2022]K, m, ~t0 )\nPk , \u03a8(c[~\u2022], m, ~t) \\ \u03a8(c[~\u2022], m, ~t0 )\n\u02c6\nBy the induction hypothesis, \u03a6(c[~\u2022], m, ~t0 ) = \u03a8(c[~\u2022], m, ~t0 ). By Definitions 5.2 and 7.2, we\n\u02c6\nknow that \u03a6(Jc[~\u2022]K, m, ~t\u02c6) \u2287 \u03a6(Jc[~\u2022]K, m, ~t0 ) and \u03a8(c[~\u2022], m, ~t) \u2287 \u03a8(c[~\u2022], m, ~t0 ). Therefore, in\norder to prove that \u03a6(Jc[~\u2022]K, m, ~t\u02c6) = \u03a8(c[~\u2022], m, ~t) it is sufficient to show that F = P . We\nk\n\nk\n\nconsider each direction of equivalence separately.\n\u2022 Fk \u2287 Pk . Take an attack ~b \u2208 Pk . That is hc[~b], mi produces a trace ~q such that ~q agrees\non all checked endorsements with ~t except the last one. There are three possible ways in\nwhich these endorsements may disagree:\n(a) Trace ~t contains checked (\u03b7k , vk , 1) and ~q contains checked (\u03b7k , vk0 , 1) such that vk 6= vk0 .\nBy the rules for the translation, it must be that the trace ~t\u02c6, which is produced by configuration hJc[~a]K, mi, has two corresponding endorsement events endorse(\u03b72k\u22121 , 1)\nand endorse(\u03b72k , vk ). Similarly, the trace ~q\u02c6, produced by hJc[~b]K, mi, has two corresponding endorsement events endorse(\u03b72k\u22121 , 1) and endorse(\u03b72k , vk0 ). Because vk0 6=\nvk we have that ~q\u02c6 \u2208 \u03c6(~t\u02c6).\n\n\f28\n\nA. ASKAROV AND A. C. MYERS\n\n(b) Trace ~t contains checked endorsement event checked (\u03b7k , vk , 1), while trace ~q contains\nevent checked (\u03b7k , vk0 , 0). In this case, the trace ~t\u02c6 obtained from running hJc[~a]K, mi\nmust contain two endorsement events endorse(\u03b72k\u22121 , 1) and endorse(\u03b72k , vk ), while\nthe trace ~q\u02c6 corresponding to hJc[~b]K, mi contains one event endorse(\u03b72k\u22121 , 0). Therefore, ~q\u02c6 \u2208 \u03c6(~t\u02c6).\n(c) Trace ~t contains checked endorsement event checked (\u03b7k , vk , 0), while trace ~q contains\nevent checked (\u03b7k , vk0 , 1). This is similar to the previous case.\nFrom ~q\u02c6 \u2208 \u03c6(~t\u02c6) it follows that ~b \u2208 F .\nk\n\n\u2022 Fk \u2286 Pk . Take an attack ~b \u2208 Fk . There must be a trace ~q\u02c6, produced by hJc[~b]K, mi, such\nthat ~q\u02c6 \u2208 \u03c6(~t\u02c6). There are two ways this can happen:\n(a) ~q\u02c6 and ~t\u02c6 disagree at the translated endorsement event that has label \u03b72k\u22121 . More\nprecisely, one must have form endorse(\u03b72k\u22121 , 1) and the other, endorse(\u03b72k\u22121 , 0). In\nthe original run, this corresponds to two traces ~t and ~q such that ~t contains the\nevent checked (\u03b7k , bk , vk ) and ~q contains the event checked (\u03b7k , b0k , vk0 ). We know that\nbk = 1 and b0k = 0, and hence bk + b0k \u2264 1. According to Definition 7.1, we need to\nshow that vk0 6= vk . Assume this is not the case, and that vk0 = vk . Then by rule\n(T-CHECKED), we have bk = b0k , which contradicts the earlier conclusion. Hence\n~q \u2208 \u03c8(~t).\n(b) Alternatively, ~q\u02c6 and ~t\u02c6 disagree at the endorsement event that has label \u03b7 . This also\n2k\n\nmeans that they agree on the earlier endorsement, i.e., for the corresponding trace\nwith checked endorsement, we can show that bk = b0k = 1, and vk 6= vk0 . Therefore,\n~q \u2208 \u03c8(~t).\nFrom ~q \u2208 \u03c8(~t) it follows that ~b \u2208 Pk .\nUsing Lemma 7.5 we can show the following Proposition, which relates the security of the\nsource and translated programs.\nProposition 7.6 (Relation of checked and direct endorsements). Given a program c[~\u2022] that\nonly uses checked endorsements such that \u0393, pc ` c[~\u2022], then c[~\u2022] satisfies progress-insensitive\nrobustness for checked endorsements if and only Jc[~\u2022]K satisfies progress-insensitive robustness for direct endorsements.\nProof. Note that our translation preserves typing: when \u0393, pc ` c[~\u2022], then \u0393, pc ` Jc[~\u2022]K.\nTherefore, by Proposition 6.1 the translated program satisfies progress-insensitive robustness with endorsements. To show that the source program satisfies the progress-insensitive\nrobustness with checked endorsements, we use Lemma 7.5 and note that the corresponding\nsets of irrelevant attacks and control between any two runs of the programs must be in\nsync.\n2\nNotes on the adequacy of the translation. We observe two facts about the adequacy of\nthis translation. First, for non-typed programs, the relation does not hold. For instance, a\nprogram like\n[\u2022]; endorse(u) if u = u0 then low := declassify(u < h) else skip\ndoes not satisfy Definition 7.3. However, translation of this program satisfies Definition 5.3.\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n29\n\nSecond, observe that omitting endorsement of the expression would lead to occlusion.\nConsider an alternative translation that endorses only the variables x1 , . . . xn but not the\nresult of the whole expression. Using such a translation, a program\nif u * 0 > 0 then skip else skip; trusted := x\nis translated to\ntemp := x; if t * 0 > 0 then skip else skip; trusted := x\nHowever, while the first program does not satisfy Definition 7.3, the second program is\naccepted by Definition 5.3.\n\n8. Attacker impact\nIn prior work, robustness controls the attacker's ability to cause information release. In the\npresence of endorsement, the attacker's ability to influence trusted locations also becomes\nan important security issue. To capture this influence, we introduce an integrity dual to\nattacker knowledge, called attacker impact. Similarly to low events, we define trusted events\nas assignments to trusted variables and termination.\nDefinition 8.1 (Attacker impact ). Given a program c[~\u2022], memory m, and trusted events\n~t? , define p(c[~\u2022], m, ~t? ) to be a set of attacks ~a that match trusted events ~t? :\np(c[~\u2022], m, ~t? ) , {~a | hc[~a], mi\u2212\u2192\u2217 ~0 \u2227 ~t? = ~t0 T }\nt\n\nAttacker impact is defined with respect to a given sequence of trusted events ~t? , starting in\nmemory m, and program c[~\u2022]. The impact is the set of all attacks that agree with ~t? in their\nfootprint on trusted variables.\nIntuitively, a smaller set for attacker impact means that the attacker has greater power\nto influence trusted events. Similarly to progress knowledge, we define progress impact,\ncharacterizing which attacks lead to one more trusted event. This then allows us to define\nrobustness conditions for integrity, which have not previously been identified.\nDefinition 8.2 (Progress impact). Given a program c[~\u2022], memory m, and sequence of\ntrusted events ~t? , define progress impact p\u2192 (c[~\u2022], m, ~t? ) as\np\u2192 (c[~\u2022], m, ~t? ) , {~a | hc[~a], mi\u2212\u2192\u2217 ~0 hc0 , m0 i \u2227 ~t? = ~t0 T \u2227 hc0 , m0 i\u2212\u2192\u2217 t00 }\nt\n\n?\n\nThe intuition for the baseline robustness for integrity is that attacker should not influence\ntrusted data. This is similar to noninterference for integrity (modulo availability attacks,\nwhich have not been explored in this context before). However unlike earlier work, we can\neasily extend the notion of integrity robustness to endorsements and checked endorsements.\nDefinition 8.3 (Progress-insensitive integrity robustness with endorsements). A program\nc[~\u2022] satisfies progress-insensitive robustness for integrity if for all memories m, and for all\ntraces ~t*t? where t? is a trusted event, we have\np\u2192 (c[~\u2022], m, ~tT ) \\ \u03a6(c[~\u2022], m, ~t*t? ) \u2286 p(c[~\u2022], m, ~tT *t? )\nIrrelevant attacks are defined precisely as in Section 5. We omit the corresponding definitions\nfor programs without endorsements and with checked endorsements.\n\n\f30\n\n1\n2\n3\n4\n\nA. ASKAROV AND A. C. MYERS\n\n[\u2022]\nendorse(guess, new_password)\nif (declassify(guess==password))\nthen\n\n5\n\npassword = new_password;\n\n6\n\nnfailed\n\n7\n8\n9\n10\n\n= 0;\n\nok = true;\nelse\nnfailed\n\n= nfailed + 1;\n\nok = false;\n\nFigure 11. Password update\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n[\u2022]\nendorse(req_time)\nif (req_time <= now)\nthen\nif (req_time >= embargo_time)\nthen return declassify(new_data)\nelse return old_data\nelse\nreturn old_data\n\nFigure 12. Accessing embargoed information\n\nThe type system of Section 6 also enforces integrity robustness with endorsements, rejecting insecure programs such as t := u and if (u1 ) then t := endorse(u2 ), but accepting\nt := endorse(u). Moreover, a connection between checked and direct endorsements, analogous to Proposition 7.6, holds for integrity robustness too.\n\n9. Examples\nPassword update. Figure 11 shows code for updating a password. The attacker controls\nvariables guess of level (P, U) and new_password of level (S, U). The variable password\nhas level (S, T) and variables nfailed and ok have level (P, T). The declassification on\nline 3 uses the untrusted variable guess. This variable, however, is listed in the endorse\nclause on line 2; therefore, the declassification is accepted. The initially untrusted variable\nnew_password has to be endorsed to update the password on line 5. The example also\nshows how other trusted variables-nfailed and ok-can be updated in the then and else\nbranches.\nData sanitization. Figure 12 shows an annotated version of the code from the introduction,\nin which some information (new_data) is not allowed to be released until time embargo_time.\nThe attacker-controlled variable is req_time of level (P, U), and new_data has level (S, T).\nThe checked endorse ensures that the attacker cannot violate the integrity of the test\nreq_time >= embargo_time. (Variable now is high-integrity and contains the current time).\nWithout the checked endorse, the release of new_data would not be permitted either semantically or by the type system.\n\n10. Related work\nPrior robustness definitions [16, 10], based on equivalence of low traces, do not differentiate\nprograms such as [\u2022]; low := u < h; low 0 := h and [\u2022]; low 0 := h; low := u < h; Per\ndimensions of information release [24], the new security conditions cover not only the \"who\"\ndimension, but are also sensitive to \"where\" information release happens. Also, the security\ncondition of robustness with endorsement does not suffer from the occlusion problems of\nqualified robustness. Balliu and Mastroeni [6] derive sufficient conditions for robustness\nusing weakest precondition semantics. These conditions are not precise enough to distinguish\nthe examples above, and, moreover, do not support endorsement.\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n31\n\nPrior work on robustness semantics defines termination-insensitive security conditions\n[16, 6]. Because the new framework is powerful enough to capture the security of programs\nwith intermediate observable events, it can describe the robustness of nonterminating programs. Prior work on qualified robustness [16] uses a non-standard scrambling semantics in\nwhich qualified robustness unfortunately becomes a possibilistic condition, leading to anomalies such as reachability of dead code. The new framework avoids such artifacts because it\nuses a standard, deterministic semantics.\nChecked endorsement was introduced informally in the Swift web application framework [9] as a convenient way to implement complex security policies. The current paper is\nthe first to formalize and to study the properties of checked endorsement.\nOur semantic framework is based on the definition of attacker knowledge, developed\nin prior work introducing gradual release [1]. Attacker knowledge is used for expressing\nconfidentiality policies in recent work [7, 3, 2, 8]. However, none of this work considers\nintegrity; applying attacker-centric reasoning to integrity policies is novel.\n\n11. Conclusion\nWe have introduced a new knowledge-based framework for semantic security conditions\nfor information security with declassification and endorsement. A key technical innovation\nis characterizing the impact and control of the attacker over information in terms of sets\nof similar attacks. Using this framework, we can express semantic conditions that more\nprecisely characterize the security offered by a security type system, and derive a satisfactory\naccount of new language features such as checked endorsement.\n\nReferences\n[1] A. Askarov and A. Sabelfeld. Gradual release: Unifying declassification, encryption and key release\npolicies. In Proc. IEEE Symp. on Security and Privacy, pages 207\u2013221, May 2007.\n[2] A. Askarov and A. Sabelfeld. Tight enforcement of information-release policies for dynamic languages.\nIn Proc. IEEE Computer Security Foundations Symposium, July 2009.\n[3] A. Askarov, S. Hunt, A. Sabelfeld, and D. Sands. Termination-insensitive noninterference leaks more\nthan just a bit. In ESORICS, pages 333\u2013348, October 2008.\n[4] A. Askarov and A. C. Myers. A semantic framework for declassification and endorsement. In Proc. 19th\nEuropean Symp. on Programming (ESOP'10), March 2010.\n[5] A. Askarov and A. Sabelfeld. Security-typed languages for implementation of cryptographic protocols: A\ncase study. In Proc. 10th European Symposium on Research in Computer Security (ESORICS), number\n3679 in Lecture Notes in Computer Science. Springer-Verlag, September 2005.\n[6] M. Balliu and I. Mastroeni. A weakest precondition approach to active attacks analysis. In PLAS '09:\nProc. of the ACM SIGPLAN Fourth Workshop on Programming Languages and Analysis for Security,\npages 59\u201371. ACM, 2009.\n[7] A. Banerjee, D. Naumann, and S. Rosenberg. Expressive declassification policies and modular static\nenforcement. In Proc. IEEE Symp. on Security and Privacy, pages 339\u2013353, May 2008.\n[8] N. Broberg and D. Sands. Flow-sensitive semantics for dynamic information flow policies. In S. Chong\nand D. Naumann, editors, ACM SIGPLAN Fourth Workshop on Programming Languages and Analysis\nfor Security (PLAS 2009), Dublin, June 15 2009. ACM.\n[9] S. Chong, J. Liu, A. C. Myers, X. Qi, K. Vikram, L. Zheng, and X. Zheng. Secure web applications via\nautomatic partitioning. In Proc. SOSP 2007, pages 31\u201344, October 2007.\n[10] S. Chong and A. C. Myers. Decentralized robustness. In CSFW '06: Proc. of the 19th IEEE workshop on\nComputer Security Foundations, pages 242\u2013256, Washington, DC, USA, 2006. IEEE Computer Society.\n[11] M. R. Clarkson, S. Chong, and A. C. Myers. Civitas: Toward a secure voting system. In Proc. IEEE\nSymp. on Security and Privacy, pages 354\u2013368, May 2008.\n\n\f32\n\nA. ASKAROV AND A. C. MYERS\n\n[12] P. Efstathopoulos, M. Krohn, S. VanDeBogart, C. Frey, D. Ziegler, E. Kohler, D. Mazi\u00e8res, F. Kaashoek,\nand R. Morris. Labels and event processes in the Asbestos operating system. In Proc. 20th ACM Symp.\non Operating System Principles (SOSP), October 2005.\n[13] J. A. Goguen and J. Meseguer. Security policies and security models. In Proc. IEEE Symp. on Security\nand Privacy, pages 11\u201320, April 1982.\n[14] M. D. McIlroy and J. A. Reeds. Multilevel security in the UNIX tradition. Software-Practice and\nExperience, 22(8):673\u2013694, August 1992.\n[15] A. C. Myers. JFlow: Practical mostly-static information flow control. In Proc. ACM Symp. on Principles\nof Programming Languages, pages 228\u2013241, January 1999.\n[16] A. C. Myers, A. Sabelfeld, and S. Zdancewic. Enforcing robust declassification and qualified robustness.\nJ. Computer Security, 14(2):157\u2013196, May 2006.\n[17] A. C. Myers and B. Liskov. A decentralized model for information flow control. In Proc. 17th ACM\nSymp. on Operating System Principles (SOSP), pages 129\u2013142, Saint-Malo, France, 1997.\n[18] A. C. Myers, L. Zheng, S. Zdancewic, S. Chong, and N. Nystrom. Jif 3.0: Java information flow.\nSoftware release, http://www.cs.cornell.edu/jif, July 2006.\n[19] K. O'Neill, M. Clarkson, and S. Chong. Information-flow security for interactive programs. In Proc.\nIEEE Computer Security Foundations Workshop, pages 190\u2013201, July 2006.\n[20] P. \u00d8rbaek and J. Palsberg. Trust in the \u03bb-calculus. J. Functional Programming, 7(6):557\u2013591, 1997.\n[21] F. Pottier and S. Conchon. Information flow inference for free. In Proc. 5th ACM SIGPLAN International Conference on Functional Programming (ICFP), pages 46\u201357, 2000.\n[22] A. W. Roscoe. Csp and determinism in security modeling. In Proc. IEEE Symposium on Security and\nPrivacy, 1995.\n[23] A. Sabelfeld and A. C. Myers. Language-based information-flow security. IEEE J. Selected Areas in\nCommunications, 21(1):5\u201319, January 2003.\n[24] A. Sabelfeld and D. Sands. Declassification: Dimensions and principles. J. Computer Security, 2009.\n[25] G. Smith and D. Volpano. Secure information flow in a multi-threaded imperative language. In Proc. 25th\nACM Symp. on Principles of Programming Languages (POPL), pages 355\u2013364, San Diego, California,\nJanuary 1998.\n[26] D. Volpano and G. Smith. Probabilistic noninterference in a concurrent language. In Proc. IEEE Computer Security Foundations Workshop, pages 34\u201343, June 1998.\n[27] D. Volpano, G. Smith, and C. Irvine. A sound type system for secure flow analysis. J. Computer Security,\n4(3):167\u2013187, 1996.\n[28] S. Zdancewic and A. C. Myers. Robust declassification. In Proc. 14th IEEE Computer Security Foundations Workshop, pages 15\u201323, June 2001.\n[29] S. Zdancewic, L. Zheng, N. Nystrom, and A. C. Myers. Secure program partitioning. ACM Transactions\non Computer Systems, 20(3):283\u2013328, August 2002.\n[30] N. Zeldovich, S. Boyd-Wickizer, and D. Mazi\u00e8res. Securing distributed systems with information flow\ncontrol. In Proc. 5th USENIX Symposium on Networked Systems Design and Implementation (NSDI),\npages 293\u2013308, 2008.\n\nAcknowledgments\nThe authors would like to thank the anonymous reviewers for comments on a draft of this\npaper. We also thank Owen Arden, Stephen Chong, Michael Clarkson, Daniel Hedin, Andrei\nSabelfeld, and Danfeng Zhang for useful discussions.\nThis work was supported by a grant from the Office of Naval Research (N000140910652)\nand by two NSF grants (the TRUST center, 0424422; and 0964409). The U.S. Government is\nauthorized to reproduce and distribute reprints for Government purposes, notwithstanding\nany copyright annotation thereon. The views and conclusions contained herein are those\nof the authors and should not be interpreted as necessarily representing the official policies\n\n\fATTACKER CONTROL AND IMPACT FOR CONFIDENTIALITY AND INTEGRITY\n\n33\n\nor endorsement, either expressed or implied, of any of the funding agencies or of the U.S.\nGovernment.\n\nThis work is licensed under the Creative Commons Attribution-NoDerivs License. To view a\ncopy of this license, visit http://creativecommons.org/licenses/by-nd/2.0/ or send a letter to\nCreative Commons, 171 Second St, Suite 300, San Francisco, CA 94105, USA, or Eisenacher\nStrasse 2, 10777 Berlin, Germany\n\n\f"}