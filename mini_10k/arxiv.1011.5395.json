{"id": "http://arxiv.org/abs/1011.5395v1", "guidislink": true, "updated": "2010-11-24T15:18:42Z", "updated_parsed": [2010, 11, 24, 15, 18, 42, 2, 328, 0], "published": "2010-11-24T15:18:42Z", "published_parsed": [2010, 11, 24, 15, 18, 42, 2, 328, 0], "title": "The Sample Complexity of Dictionary Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.3468%2C1011.1341%2C1011.1997%2C1011.1142%2C1011.4650%2C1011.4912%2C1011.2952%2C1011.3324%2C1011.5764%2C1011.3994%2C1011.1522%2C1011.0810%2C1011.4887%2C1011.3883%2C1011.2253%2C1011.5355%2C1011.2460%2C1011.4636%2C1011.0027%2C1011.6385%2C1011.2971%2C1011.2751%2C1011.5446%2C1011.2802%2C1011.1680%2C1011.5217%2C1011.1033%2C1011.1066%2C1011.6047%2C1011.0841%2C1011.4956%2C1011.4860%2C1011.4594%2C1011.5296%2C1011.4427%2C1011.5580%2C1011.4897%2C1011.1080%2C1011.0476%2C1011.1980%2C1011.4546%2C1011.5469%2C1011.6040%2C1011.0291%2C1011.3613%2C1011.1476%2C1011.3959%2C1011.0314%2C1011.1877%2C1011.2515%2C1011.5567%2C1011.0909%2C1011.2128%2C1011.5332%2C1011.6517%2C1011.4518%2C1011.5984%2C1011.0639%2C1011.2756%2C1011.5978%2C1011.1318%2C1011.6593%2C1011.0447%2C1011.1397%2C1011.6578%2C1011.5582%2C1011.5226%2C1011.2846%2C1011.2308%2C1011.6609%2C1011.1808%2C1011.6586%2C1011.5993%2C1011.6275%2C1011.6583%2C1011.3249%2C1011.2714%2C1011.6056%2C1011.4944%2C1011.1440%2C1011.2415%2C1011.3595%2C1011.2680%2C1011.4265%2C1011.4397%2C1011.5091%2C1011.1378%2C1011.2723%2C1011.5395%2C1011.0080%2C1011.3947%2C1011.6254%2C1011.5431%2C1011.3663%2C1011.2445%2C1011.1626%2C1011.6419%2C1011.5896%2C1011.4520%2C1011.4358%2C1011.1561&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Sample Complexity of Dictionary Learning"}, "summary": "A large set of signals can sometimes be described sparsely using a\ndictionary, that is, every element can be represented as a linear combination\nof few elements from the dictionary. Algorithms for various signal processing\napplications, including classification, denoising and signal separation, learn\na dictionary from a set of signals to be represented. Can we expect that the\nrepresentation found by such a dictionary for a previously unseen example from\nthe same source will have L_2 error of the same magnitude as those for the\ngiven examples? We assume signals are generated from a fixed distribution, and\nstudy this questions from a statistical learning theory perspective.\n  We develop generalization bounds on the quality of the learned dictionary for\ntwo types of constraints on the coefficient selection, as measured by the\nexpected L_2 error in representation when the dictionary is used. For the case\nof l_1 regularized coefficient selection we provide a generalization bound of\nthe order of O(sqrt(np log(m lambda)/m)), where n is the dimension, p is the\nnumber of elements in the dictionary, lambda is a bound on the l_1 norm of the\ncoefficient vector and m is the number of samples, which complements existing\nresults. For the case of representing a new signal as a combination of at most\nk dictionary elements, we provide a bound of the order O(sqrt(np log(m k)/m))\nunder an assumption on the level of orthogonality of the dictionary (low Babel\nfunction). We further show that this assumption holds for most dictionaries in\nhigh dimensions in a strong probabilistic sense. Our results further yield fast\nrates of order 1/m as opposed to 1/sqrt(m) using localized Rademacher\ncomplexity. We provide similar results in a general setting using kernels with\nweak smoothness requirements.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.3468%2C1011.1341%2C1011.1997%2C1011.1142%2C1011.4650%2C1011.4912%2C1011.2952%2C1011.3324%2C1011.5764%2C1011.3994%2C1011.1522%2C1011.0810%2C1011.4887%2C1011.3883%2C1011.2253%2C1011.5355%2C1011.2460%2C1011.4636%2C1011.0027%2C1011.6385%2C1011.2971%2C1011.2751%2C1011.5446%2C1011.2802%2C1011.1680%2C1011.5217%2C1011.1033%2C1011.1066%2C1011.6047%2C1011.0841%2C1011.4956%2C1011.4860%2C1011.4594%2C1011.5296%2C1011.4427%2C1011.5580%2C1011.4897%2C1011.1080%2C1011.0476%2C1011.1980%2C1011.4546%2C1011.5469%2C1011.6040%2C1011.0291%2C1011.3613%2C1011.1476%2C1011.3959%2C1011.0314%2C1011.1877%2C1011.2515%2C1011.5567%2C1011.0909%2C1011.2128%2C1011.5332%2C1011.6517%2C1011.4518%2C1011.5984%2C1011.0639%2C1011.2756%2C1011.5978%2C1011.1318%2C1011.6593%2C1011.0447%2C1011.1397%2C1011.6578%2C1011.5582%2C1011.5226%2C1011.2846%2C1011.2308%2C1011.6609%2C1011.1808%2C1011.6586%2C1011.5993%2C1011.6275%2C1011.6583%2C1011.3249%2C1011.2714%2C1011.6056%2C1011.4944%2C1011.1440%2C1011.2415%2C1011.3595%2C1011.2680%2C1011.4265%2C1011.4397%2C1011.5091%2C1011.1378%2C1011.2723%2C1011.5395%2C1011.0080%2C1011.3947%2C1011.6254%2C1011.5431%2C1011.3663%2C1011.2445%2C1011.1626%2C1011.6419%2C1011.5896%2C1011.4520%2C1011.4358%2C1011.1561&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A large set of signals can sometimes be described sparsely using a\ndictionary, that is, every element can be represented as a linear combination\nof few elements from the dictionary. Algorithms for various signal processing\napplications, including classification, denoising and signal separation, learn\na dictionary from a set of signals to be represented. Can we expect that the\nrepresentation found by such a dictionary for a previously unseen example from\nthe same source will have L_2 error of the same magnitude as those for the\ngiven examples? We assume signals are generated from a fixed distribution, and\nstudy this questions from a statistical learning theory perspective.\n  We develop generalization bounds on the quality of the learned dictionary for\ntwo types of constraints on the coefficient selection, as measured by the\nexpected L_2 error in representation when the dictionary is used. For the case\nof l_1 regularized coefficient selection we provide a generalization bound of\nthe order of O(sqrt(np log(m lambda)/m)), where n is the dimension, p is the\nnumber of elements in the dictionary, lambda is a bound on the l_1 norm of the\ncoefficient vector and m is the number of samples, which complements existing\nresults. For the case of representing a new signal as a combination of at most\nk dictionary elements, we provide a bound of the order O(sqrt(np log(m k)/m))\nunder an assumption on the level of orthogonality of the dictionary (low Babel\nfunction). We further show that this assumption holds for most dictionaries in\nhigh dimensions in a strong probabilistic sense. Our results further yield fast\nrates of order 1/m as opposed to 1/sqrt(m) using localized Rademacher\ncomplexity. We provide similar results in a general setting using kernels with\nweak smoothness requirements."}, "authors": ["Daniel Vainsencher", "Shie Mannor", "Alfred M. Bruckstein"], "author_detail": {"name": "Alfred M. Bruckstein"}, "author": "Alfred M. Bruckstein", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.specom.2013.01.005", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1011.5395v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1011.5395v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1011.5395v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1011.5395v1", "arxiv_comment": null, "journal_reference": null, "doi": "10.1016/j.specom.2013.01.005", "fulltext": "arXiv:1011.5395v1 [stat.ML] 24 Nov 2010\n\nThe Sample Complexity of Dictionary Learning\nShie Mannor\nshie@ee.technion.ac.il\nDepartment of Electrical Engineering\nTechnion, Israel Institute of Technology\nHaifa 32000, Israel\n\nDaniel Vainsencher\ndanielv@tx.technion.ac.il\nDepartment of Electrical Engineering\nTechnion, Israel Institute of Technology\nHaifa 32000, Israel\n\nAlfred M. Bruckstein\nfreddy@cs.technion.ac.il\nDepartment of Computer Science\nTechnion, Israel Institute of Technology\nHaifa 32000, Israel\nNovember 25, 2010\n\nAbstract\nA large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary.\nAlgorithms for various signal processing applications, including classification, denoising and\nsignal separation, learn a dictionary from a set of signals to be represented. Can we expect\nthat the representation found by such a dictionary for a previously unseen example from the\nsame source will have L2 error of the same magnitude as those for the given examples? We assume signals are generated from a fixed distribution, and study this questions from a statistical\nlearning theory perspective.\nWe develop generalization bounds on the quality of the learned dictionary for two types of\nconstraints on the coefficient selection, as measured by the expected L2 error in representation\nwhen the dictionary is used. For the case\n\u0010p of l1 regularized\n\u0011 coefficient selection we provide a\ngeneralization bound of the order of O\nnp log(m\u03bb)/m , where n is the dimension, p is the\nnumber of elements in the dictionary, \u03bb is a bound on the l1 norm of the coefficient vector and\nm is the number of samples, which complements existing results. For the case of representing\na new signal\np as a combination of at most k dictionary elements, we provide a bound of the\norder O( np log(mk)/m) under an assumption on the level of orthogonality of the dictionary\n(low Babel function). We further show that this assumption holds for most dictionaries in high\ndimensions in a \u221a\nstrong probabilistic sense. Our results further yield fast rates of order 1/m\nas opposed to 1/ m using localized Rademacher complexity. We provide similar results in a\ngeneral setting using kernels with weak smoothness requirements.\n\n1 Introduction\nIn processing signals from X = Rn it is now a common technique to use sparse representations; that\nis, to approximate each signal x by a \"small\" linear combination a of elements di from a dictionary\n1\n\n\fP\nD \u2208 X p , so that x \u2248 Da = pi=1 ai di . This has various uses detailed in Section 1.1. The smallness\nof a is often measured using either kak1 , or the number of non zero elements in a, often denoted\nkak0 . The approximation error is measured here using a Euclidean norm appropriate to the vector\nspace. We denote the approximation error of x using dictionary D and coefficients from A as\nhA,D (x) = min kDa \u2212 xk ,\na\u2208A\n\n(1.1)\n\nwhere A is one of the following sets determining the sparsity required of the representation:\nHk = {a : kak0 \u2264 k}\ninduced a \"hard\" sparsity constraint, which we also call k sparse representation, while\nR\u03bb = {a : kak1 \u2264 \u03bb}\ninduces a convex constraint that is a \"relaxation\" of the previous constraint.\nThe dictionary learning problem is to find a dictionary D minimizing\nE(D) = Ex\u223c\u03bd hA,D (x),\n\n(1.2)\n\nwhere \u03bd is a distribution over signals that is known to us only through samples from it. The problem addressed in this paper is the \"generalization\" (in the statistical learning sense) of dictionary\nlearning: to what extent does the performance of a dictionary chosen based on a finite set of samples\nindicate its expected error in (1.2)? This clearly depends on the number of samples and other parameters of the problem such as dictionary size. In particular, an obvious algorithm is to represent each\nsample using itself, if the dictionary is allowed to be as large as the sample, but the performance on\nunseen signals is likely to disappoint.\nTo state our goal more quantitatively, assume that an algorithm finds a dictionary D suited to k\nsparse representation, in the sense that the average representation error Em (D) on the m examples\nit is given is low. Our goal is to bound the generalization error \u03b5, which is the additional expected\nerror that might be incurred:\nE(D) \u2264 (1 + \u03b7)Em (D) + \u03b5,\nwhere \u03b7 \u2265 0 is sometimes zero, and the bound depends on the number of samples and problem\nparameters. Since algorithms that find the optimal dictionary for a given set of samples (also known\nas empirical risk minimization, or ERM, algorithms) are not known for dictionary learning, we\nprove uniform convergence bounds that apply simultaneously over all admissible dictionaries D,\nthus bounding from above the sample complexity of the dictionary learning problem.\nMany analytic and algorithmic methods relying on the properties of finite dimensional Euclidean\ngeometry can be applied in more general settings by applying kernel methods. These consist of\ntreating objects that are not naturally represented in Rn as having their similarity described by an\ninner product in an abstract feature space that is Euclidean. This allows the application of algorithms depending on the data only through a computation of inner products to such diverse objects as graphs, DNA sequences and text documents, that are not naturally represented using vector\nspaces (Shawe-Taylor and Cristianini, 2004). Is it possible to extend the usefulness of dictionary\nlearning techniques to this setting? We address sample complexity aspects of this question as well.\n2\n\n\f1.1 Background and related work\nSparse representations are a standard practice in diverse fields such as signal processing, natural\nlanguage processing, etc. Typically, the dictionary is assumed to be known. The motivation for\nsparse representations is indicated by the following results, in which we assume the signals come\nfrom X = Rn , and the representation coefficients from A = Hk where k < n, p and typically\nhA,D (x) \u226a 1.\n\u2022 Compression: If a signal x has an approximate sparse representation in some commonly\nknown dictionary D, then by definition, storing or transmitting the sparse representation will\nnot cause large error.\n\u2022 Representation: If a signal x has an approximate sparse representation in a dictionary D that\nfulfills certain geometric conditions, then its sparse representation is unique and can be found\nefficiently (Bruckstein et al., 2009).\n\u2022 Denoising: If a signal x has a sparse representation in some known dictionary D, and x\u0303 =\nx + \u03bd, where the random noise \u03bd is Gaussian, then the sparse representation found for x\u0303 will\nlikely be very close to x (for example Chen et al., 2001).\n\u2022 Compressed sensing: Assuming that a signal x has a sparse representation in some known dictionary D that fulfills certain geometric conditions, this representation can be approximately\nretrieved with high probability from a small number of random linear measurements of x.\nThe number of measurements needed depends on the sparsity of x in D (Candes and Tao,\n2006).\nThe implications of these results are significant when a dictionary D is known that sparsely\nrepresents simultaneously many signals. In some applications the dictionary is chosen based on\nprior knowledge, but in many applications the dictionary is learned based on a finite set of examples. To motivate dictionary learning, consider an image representation used for compression\nor denoising. Different types of images may have different properties (MRI images are not similar to scenery images), so that learning a specific dictionary to each type of images may lead to\nimproved performance. The benefits of dictionary learning have been demonstrated in many applications (Protter and Elad, 2007; Peyr\u00e9, 2009; Yang et al., 2009).\nTwo extensively used techniques related to dictionary learning are Principal Component Analysis (PCA) and k means clustering. The former finds a single subspace minimizing the sum of\nsquared representation errors which is very similar to dictionary learning with A = Hk and p = k.\nThe latter finds a set of locations minimizing the sum of squared distances between each signal and\nthe location closest to it which is very similar to dictionary learning with A = H1 where p is the\nnumber of locations. Thus we could see dictionary learning as PCA with multiple subspaces, or\nas clustering where multiple locations are used to represent each signal. The sample complexity of\nboth algorithms are well studied (Bartlett et al., 1998; Biau et al., 2008; Shawe-Taylor et al., 2005;\nBlanchard et al., 2007).\nThis paper does not address questions of computational cost, though they are very relevant.\nFinding optimal coefficients for k sparse representation (that is, minimizing (1.1) with A = Hk )\nis NP-hard in general (Davis et al., 1997). Dictionary learning as an optimization problem, that\n3\n\n\fof minimizing (1.2) is less well understood, even for empirical \u03bd (consisting of a finite number of samples), despite over a decade of work on related algorithms with good empirical results (Olshausen and Fieldt, 1997; Lewicki et al., 1998; Kreutz-Delgado et al., 2003; Aharon et al.,\n2005; Lee et al., 2007; Krause and Cevher, 2010; Mairal et al., 2010).\nThe only prior work we are aware of that addresses generalization in dictionary learning, by\nMaurer and Pontil (2010), addresses the convex representation constraint A = R\u03bb ; we discuss\nthe relation of our work to theirs in Section 2. Another related work studies the identifiability\nof dictionaries, giving conditions under which a dictionary may be exactly recovered. A recent\nexample giving somewhat similar requirements on the number of samples (though in a different\nsetting, and to obtain a different kind of result) is by Gribonval and Schnass (2009), which also\nincludes a review of identifiability results.\n\n2 Results\nExcept where we state otherwise, we assume signals are generated in the unit sphere Sn\u22121 .\nA new approach to dictionary learning generalization. Our first main contribution is an\napproach to generalization bounds in dictionary learning that is complementary to that used by\nMaurer and Pontil (2010). Assume that the columns of the dictionary D \u2208 Rn\u00d7p are of unit\nlength, and that each signal x \u2208 Sn\u22121 is approximately represented in the form Da where the\ncoefficient vector a is known to fulfill a constraint of form kak1 \u2264 \u03bb. We quantify the complexity of the associated error function class in terms of \u03bb,\u0010 so that standard methods\nof uniform\n\u0011\np\nnp log(m\u03bb)/m with \u03b7 = 0. The\nconvergence give generalization error bounds \u03b5 of order O\nmethod by Maurer and Pontil (2010) results in Theorem 3 given below providing generalization\nerror bounds of order\n!\nr\n\u0010\n\u00112\np\nO\np min(p, n) \u03bb + log(m\u03bb) /m .\nThus the latter are applicable to the case n \u226b p, while our approach is not. However in the\ncase n < p, also known in the literature as the \"over-complete\" case (Olshausen and Fieldt, 1997;\nLewicki et al., 1998), the important complexity parameter is \u03bb, on which our bounds depend only\nlogarithmically, instead of polynomially. One case where this is significant is where the representation is chosen\n\u0001 by solving a minimization problem such as mina kDa \u2212 Xk + \u03b3 * kak1 in which\n\u03bb = O \u03b3 \u22121 .\nFast rates. For the case \u03b7 > 0 our methods are compatible with general fast rate methods\nof Bartlett et al. (2005), for bounds of order O(np log(\u03bbm)/m). The main significance of this is\nnot in the numerical results achieved, due to the large constants, but in that the general statistical\nbehavior they imply occurs in dictionary learning. For example, generalization error has a \"proportional\" component which is reduced when the empirical error is low. Whether fast rates results can\nbe proved under the infinite dimension regime is an interesting\nquestion we leave open. Note that\n\u221a\n\u22121\ndue to lower bounds by Bartlett et al. (1998) of order m on the k-means clustering problem,\nwhich corresponds to dictionary learning for 1-sparse representation, fast rates may be expected\nonly with \u03b7 > 0, as presented here.\nWe now describe the relevant function class and the bounds on its complexity, which are proved\nin Section 3, proving the following theorem The resulting generalization bounds are given explicitly\nat the end of this section.\n4\n\n\f\b\nTheorem 1. The function class G\u03bb = hR\u03bb ,D : Sn\u22121 \u2192 R : D \u2208 Rn\u00d7p , kdi k \u2264 1 , taken as a\nmetric space with the metric induced by k*k\u221e , has an \u03b5 cover of cardinality at most (4\u03bb/\u03b5)np .\nExtension to k sparse representation. Our second main contribution is to extend both our\napproach and that of Maurer and Pontil (2010) to provide generalization bounds for dictionaries for\nk sparse representations, by using a bound \u03bb on the l1 norm of the representation coefficients when\nthe dictionaries are close to orthogonal. Distance from orthogonality is measured by the Babel\nfunction, defined below and discussed in more detail in Section 4.\nDefinition 1 (Babel function, Tropp 2004). For any k \u2208 N, the Babel function \u03bck : Rn\u00d7m \u2192 R+\nis defined by:\nX\n\u03bck (D) =\nmax\nmax\n|hd\u03bb , di i| .\n\u039b\u2282{1,...,p};|\u039b|=k i\u2208\u039b\n/\n\n\u03bb\u2208\u039b\n\nThe following proposition, which is proved in Section 3, bounds the 1-norm of the dictionary coefficients for a k sparse representation and also follows from analysis previously done\nby Donoho and Elad (2003); Tropp (2004).\nProposition 1. Let kdi k \u2208 [1, \u03b3] and \u03bck\u22121 (D) < 1, then a coefficient vector a \u2208 Rp minimizing\nthe k-sparse representation error hHk ,D (x) exists which has kak1 \u2264 \u03b3k/ (1 \u2212 \u03bck\u22121 (D)).\nWe now consider the class of all k sparse representation error functions. We prove in Section 3\nthe following bound on the complexity of this class.\n\b\nCorollary 2. The function class F\u03b4,k = hHk ,D : Sn\u22121 \u2192 R : \u03bck\u22121 (D) < \u03b4 , taken as a metric\nspace with the metric induced by k*k\u221e , has an \u03b5 cover of cardinality at most (4k/ (\u03b5 (1 \u2212 \u03b4)))np .\nThe dependence of the last two results on \u03bck\u22121 (D) means that the resulting bounds will be\nmeaningful only for algorithms which explicitly or implicitly prefer near orthogonal dictionaries.\nContrast this to Theorem 1 which has no significant conditions on the dictionary.\nAsymptotically almost all dictionaries are near orthogonal. A question that arises is what\nvalues of \u03bck\u22121 can be expected for parameters n, p, k? We discuss this question and prove the\nfollowing probabilistic result in Section 4.\nTheorem 2. Suppose that D consist of p vectors chosen uniformly and independently from Sn\u22121 .\nThen we have\n\u0013\n\u0012\n1\n1\n\u0011.\n\u2264\u0010\nP \u03bck >\n2\n2\ne(n\u22122)/(10k log p) \u2212 1\n\nSince low values of the Babel function have implications to representation finding algorithms,\nthis result is of interest also outside the context of dictionary learning. Essentially it means that\nrandom dictionaries of size sub-exponential in (n \u2212 2)/k2 have low Babel function.\nNew generalization bounds for l1 case. The covering number bound of Theorem 1 implies\nseveral generalization bounds for the problem of dictionary learning for l1 regularized representation which we give here. These differ from those by Maurer and Pontil (2010) in depending more\nstrongly on the dimension of the space, but less strongly on the particular regularization term. We\nfirst give the relevant specialization of the result by Maurer and Pontil (2010) for comparison and\nfor reference as we will later build on it. This result is independent of the dimension n of the underlying space, thus the Euclidean unit ball B may be that of a general Hilbert space, and the errors\nmeasured by hA,D are in the same norm.\n5\n\n\fTheorem 3 (Maurer and Pontil 2010). Let maxa\u2208A kak1 \u2264 \u03bb, and \u03bd be any distribution on the unit\nsphere B. Then with probability at least 1 \u2212 e\u2212x over the m samples in Em drawn according to \u03bd,\nfor all dictionaries D \u2282 B with cardinality p:\nEh2A,D\n\n\u2264\n\nEm h2A,D\n\n+\n\nv\n\u0011\nu \u0010\nu p2 14\u03bb + 1/2pln (16m\u03bb2 ) 2\nt\nm\n\n+\n\nr\n\nx\n.\n2m\n\nUsing the covering number bound of Theorem 1 and a bounded differences concentration inequality (see Lemma 9), we obtain the following result. The details are given in Section 3.\nTheorem 4. Let \u03bb > 0, with \u03bd a distribution on Sn\u22121 . Then with probability at least 1 \u2212 e\u2212x over\nthe m samples in Em drawn according to \u03bd, for all D with unit length columns:\nEhR\u03bb ,D \u2264 Em hR\u03bb ,D +\n\nr\n\nr\nr\n\u221a\nnp ln (4 m\u03bb)\nx\n4\n+\n+\n.\n2m\n2m\nm\n\nUsing the same covering number bound and localized Rademacher complexity (see Lemma 10),\nwe obtain the following fast rates result.\nTheorem 5. Let \u03bb > 0, K > 1, \u03b1 > 0, with \u03bd a distribution on Sn\u22121 . Then with probability at\nleast 1 \u2212 e\u2212x over the m samples in Em drawn according to \u03bd, for all D with unit length column:\nEhR\u03bb ,D\n\nK\nEm hR\u03bb ,D + 6K max\n\u2264\nK \u22121\n+\n\n(\n\n(np + 1) log\n8\u03b1\u03bb2\n, (480)2\nm\nm\n\nm\n\u03b1\n\n\u0001\n\n20 + 22 log (m)\n,\nm\n\n)\n\n11x + 5K\n.\nm\n\nIn any particular case, \u03b1 and then K may be chosen so as to minimize the right hand side.\nGeneralization bounds for k sparse representation. Proposition 1 and Corollary 2 imply\ncertain generalization bounds for the problem of dictionary learning for k sparse representation,\nwhich we give here.\nA straight forward combination of Theorem 2 of Maurer and Pontil (2010) (given here as Theorem 3) and Proposition 1 results in the following theorem.\nTheorem 6. Let \u03b4 < 1 with \u03bd a distribution on Sn\u22121 . Then with probability at least 1 \u2212 e\u2212x over\nthe m samples in Em drawn according to \u03bd, for all D s.t. \u03bck\u22121 (D) \u2264 \u03b4:\nEh2Hk ,D\n\nv\n\uf8f6\nu\nr\n\u00132 !\n\u0012\nu\np \uf8ed 14k\n1t\nx\nk\n2\n\uf8f8\nln 16m\n\u2264 Em hHk ,D + \u221a\n+\n.\n+\n1\u2212\u03b4\n2m\nm 1\u2212\u03b4 2\n\uf8eb\n\nIn the case of clustering we have k = 1 and \u03b4 = 0 and this result approaches the rates\nof Biau et al. (2008).\nThe following theorems follow from standard results and the covering number bound of Corollary 2.\n6\n\n\fTheorem 7. Let \u03b4 < 1 with \u03bd a distribution on Sn\u22121 . Then with probability at least 1 \u2212 e\u2212x over\nthe m samples in Em drawn according to \u03bd, for all D s.t. \u03bck\u22121 (D) \u2264 \u03b4:\ns\n\u221a\nr\nr\nmk\nnp ln 41\u2212\u03b4\nx\n4\nEhHk ,D \u2264 Em hHk ,D +\n+\n+\n.\n2m\n2m\nm\nTheorem 8. Let \u03b4 < 1 < K, \u03b1 > 0 with \u03bd a distribution on Sn\u22121 . Then with probability at least\n1 \u2212 e\u2212x over the m samples in Em drawn according to \u03bd, for all D s.t. \u03bck\u22121 (D) \u2264 \u03b4:\n)\n(\n\u0001\nm\n(np\n+\n1)\nlog\nK\n20\n+\n22\nlog\n(m)\n8\u03b1k 2\n\u03b1\n, (480)2\nEhHk ,D \u2264\nEm hHk ,D + 6K max\n,\nK \u22121\nm\nm\nm (1 \u2212 \u03b4)2\n+\n\n11x + 5K\n.\nm\n\nIn any particular case, \u03b1 and then K may be chosen so as to minimize the right hand side.\nGeneralization bounds for dictionary learning in feature spaces. We further consider applications of dictionary learning to signals that are not represented as elements in a vector space, or\nthat have a very high (possibly infinite) dimension.\nIn addition to providing an approximate reconstruction of signals, sparse representation can also\nbe considered as a form of analysis, if we treat the choice of non zero coefficients and their magnitude as features of the signal. In the domain of images, this has been used to perform classification\n(in particular, face recognition) by Wright et al. (2008). Such analysis does not require that the data\nitself be represented in Rn (or in any vector space); it is enough that the similarity between data\nelements is induced from an inner product in a feature space. This requirement is fulfilled by using\nan appropriate kernel function.\nDefinition 3. Let R be a set of data representations, and let the kernel function \u03ba : R2 \u2192 R and\nthe feature mapping \u03c6 : R \u2192 H be such that:\n\u03ba (x, y) = h\u03c6 (x) , \u03c6 (y)i\nwhere H is some Hilbert space.\nAs a concrete example, choose a sequence of n words, and let \u03c6 map a document to the vector of\ncounts of appearances of each word in it (also called bag of words). Treating \u03ba(a, b) = h\u03c6(a), \u03c6(b)i\nas the similarity between documents a and b, is the well known \"bag of words\" approach, applicable to many document related tasks (Shawe-Taylor and Cristianini, 2004). Then the statement\n\u03c6(a) + \u03c6(b) \u2248 \u03c6(c) does not imply that c can be reconstructed from a and b, but we might consider it indicative of the content of c. The dictionary of elements used for representation could be\ndecided via dictionary learning, and it is natural to choose the dictionary so that the bags of words\nof documents are approximated well by small linear combinations of those in the dictionary.\nAs the example above suggests, the kernel dictionary learning problem is to find a dictionary D\nminimizing\nEx\u223c\u03bd h\u03c6,A,D (x),\nwhere we consider the representation error function\nh\u03c6,A,D (x) = min k(\u03a6D) a \u2212 \u03c6 (x)kH ,\na\u2208A\n\n7\n\n\fin which \u03a6 acts as \u03c6 on the elements of D, A \u2208 {R\u03bb , Hk }, and the norm k*kH is that induced by\nthe kernel on the feature space H.\nAnalogues of all the generalization bounds mentioned so far can be replicated in the kernel\nsetting. The dimension free results of Maurer and Pontil (2010) apply most naturally in this setting,\nand may be combined with our results to cover also dictionaries for k sparse representation, under\nreasonable assumptions on the kernel.\nProposition 2. Let \u03bd be any distribution on R such that when x \u223c \u03bd we have k\u03c6(x)k \u2264 1 with\nprobability 1. Then with probability at least 1 \u2212 e\u2212x over the m samples in Em drawn according\nto \u03bd, for all D \u2282 R with cardinality p such that \u03a6D \u2282 BH and \u03bck\u22121 (\u03a6D) \u2264 \u03b4 < 1:\nv\ns \u0012\nu\n!\nu\n\u0010\n\u00112 \u0013 2\nu 2\nk\nu p 14k/(1 \u2212 \u03b4) + 1/2 ln 16m 1\u2212\u03b4\nr\nt\nx\n2\n2\nEh\u03c6,Hk ,D \u2264 Em h\u03c6,Hk ,D +\n+\n.\nm\n2m\n\nNote that the Babel function is defined in terms of inner products between elements of D, and\ncan therefore be computed in H by applications of the kernel.\nThis result is proved in Section 5, as well as the cover number bounds (using some additional\ndefinitions and assumptions described there) that are used to prove the remaining generalization\nbounds, of which one is given below.\n\nTheorem 9. Let R have \u03b5 covers of order (C/\u03b5)n . Let \u03ba : R2 \u2192 R+ be a kernel function s.t.\n\u03ba(x, y) = h\u03c6(X), \u03c6(Y )i, for \u03c6 which is uniformly L-H\u00f6lder of order \u03b1 > 0 over R, and let\n\u03b3 = maxx\u2208R k\u03c6(x)kH . Let \u03b4 < 1, and \u03bd any distribution on R, then with probability at least\n1 \u2212 e\u2212x over the m samples in Em drawn according to \u03bd, for all dictionaries D \u2282 R of cardinality\np s.t. \u03bck\u22121 (\u03a6D) \u2264 \u03b4 < 1 (where \u03a6 acts like \u03c6 on columns):\n\uf8f6\n\uf8ebv\n\u0011\n\u0010\nu\nr\nr\nu np ln \u221amC \u03b1 k\u03b3 2 L\n\uf8f7\n\uf8ect\n1\u2212\u03b4\nx\n4\n\uf8f7+\n+\n.\nEhHk ,D \u2264 Em hHk ,D + \u03b3 \uf8ec\n\uf8f8\n\uf8ed\n2\u03b1m\n2m\nm\nThe covering number bounds needed to prove this theorem and analogs for the other generalization bounds are proved in Section 5.\n\n3 Covering numbers of G\u03bb and F\u03b4,k\nThe main content of this section is the proof of Theorem 2 and Corollary 2. We also show that\nthe restriction of near-orthogonality on the set of dictionaries, on which we rely in the proof for k\nsparse representation, is necessary to achieve a bound on \u03bb. Lastly, we recall known results from\nstatistical learning theory that link covering numbers to generalization bounds.\nWe recall the definition of the covering numbers we wish to bound. Anthony and Bartlett (1999)\ngive a textbook introduction to covering numbers and their application to generalization bounds.\nDefinition 4 (Covering number). Let (M, \b\nd) be a metric space andSS \u2282 M . Then\u0001 the \u03b5 covering\nnumber of S defined as N (\u03b5, S, d) = min |A| |A \u2282 M and S \u2282\nis the size of\na\u2208A Bd (a, \u03b5)\nthe minimal \u03b5 cover of S using d.\n8\n\n\fTo prove Theorem 1 and Corollary 2 we first note that the space of all possible dictionaries is\na subset of a unit ball in a Banach space of dimension np (with a norm specified below). Thus by\nproposition 5 formalized by Cucker and Smale (2002) the space of dictionaries has an \u03b5 cover of\nsize (4/\u03b5)np . We also note that a uniformly L Lipschitz mapping between metric spaces converts\n\u03b5/L covers into \u03b5 covers. Then it is enough to show that \u03a8\u03bb defined as D 7\u2192 hR\u03bb ,D and \u03a6k\ndefined as D 7\u2192 hHk ,D are uniformly Lipschitz (when \u03a6k is restricted to the dictionaries with\n\u03bck\u22121 (D) \u2264 c < 1). The proof of these Lipschitz properties is our next goal, in the form of\nLemmas 7 and 8.\nThe first step is to be clear about the metrics we consider over the spaces of dictionaries and of\nerror functions. We start by defining the following norm.\nDefinition 5. Let D \u2208 Rn\u00d7p . We denote kDkM E = maxi kdi k the norm of its maximal column.\nWe will use the fact k*kM E upper bounds a certain induced norm.\nn\u00d7m can be considered as\nDefinition 6 (Induced\nmatrix\n\u0011 norm).\n\u0010 Let p,\u0011q \u2208 N, then a matrix A \u2208 R\n\u0010\nan operator A : Rm , k*kp \u2192 Rn , k*kq . Then the p, q induced norm is defined as kAkp,q ,\nsupx\u2208Rm kxkp =1 kAxkq .\n\nFact 1. kDk1,2 \u2264 kDkM E\nThe geometric interpretation of this fact is that Da/ kak1 is a convex combination of vectors\neach of length at most kDkM E , then kDak2 \u2264 kDkM E kak1 .\nThe images of \u03a8\u03bb and \u03a6k are sets of representation error functions\u2013each dictionary induces a set\nof precisely representable signals, and a representation error function is simply a map of distances\nfrom this set. Representation error functions are clearly continuous, 1-Lipschitz, and into [0, 1]. In\nthis setting, a natural norm over the images is the supremum norm k*k\u221e .\n\u0001\nLemma 7. The function \u03a8\u03bb is \u03bb-Lipschitz from (Rn\u00d7m , k*kM E ) to C Sn\u22121 .\n\nProof. Let D and D \u2032 be two normalized dictionaries whose corresponding elements are at most\n\u03b5 > 0 far from one another. Let x be a unit signal and Da an optimal representation for it. Then\nk(D \u2212 D \u2032 ) ak \u2264 kD \u2212 D \u2032 k1,2 kak1 \u2264 kD \u2212 D \u2032 kM E kak1 \u2264 \u03b5\u03bb. Then g\u03bb,D\u2032 (x) \u2264 g\u03bb,D (x) + \u03b5\u03bb\nand by symmetry we have |\u03a8\u03bb (D)(x) \u2212 \u03a8\u03bb (D \u2032 )(x)| \u2264 \u03bb\u03b5. This holds for all unit signals, then\nk\u03a8\u03bb (D) \u2212 \u03a8\u03bb (D \u2032 )k\u221e \u2264 \u03bb\u03b5.\nWe now provide a proof for Proposition 1 which is used in the corresponding treatment for\ncovering numbers under k sparsity.\nOf Proposition 1. Assume that \u03bck\u22121 (D) \u2264 \u03b4 < 1 \u2264 mini\u2264p kdi k2 \u2264 \u03b3. Let D k be a set of k\nelements from D achieving the minimum on hHk ,D (x), with x \u2208 Sn\u22121 . We now consider the Gram\n\u0001\u22a4\nmatrix G = D k D k . The matrix G is symmetric, therefore it scales each point in the unit sphere\nby a non-negative combination of its real eigenvalues. Also, the diagonal entries of G are the norms\nof the elements of D k , therefore at least 1. By the Gersgorin theorem (Horn and Johnson, 1990),\nthe eigenvalues of the Gram matrix are lower bounded by 1 \u2212 \u03b4 > 0. Then in particular G has a\nsymmetric inverse, which scales each point by no more than 1/(1\u2212\u03b4). Then G\u22121 1,1 \u2264 1/(1\u2212\u03b4).\n9\n\n\fIn particular, elements of D k are linearly independent, which implies that the unique optimal\nrepresentation of x as a linear combination of the columns of D k is D k a with\n\u0012\u0010 \u0011\n\u0013\u22121 \u0010 \u0011\n\u22a4\n\u22a4\nk\nk\nD\nD\nD k x.\na=\nBy the definition of induced matrix norms, we have kak1 \u2264\n\n\u0010\n\nDk\n\n\u0001\u22a4\n\nDk\n\n\u0011\u22121\n\nDk\n1,1\n\n\u0001\u22a4\n\nx\n\n1\n\n\u2264\n\n\u03b3k/(1 \u2212 \u03b4), the last bound because x is a unit vector, and D k has k columns whose norm is bounded\nby \u03b3.\nLemma 8. The function \u03a6k is a k/(1\u2212\u03b4)-Lipschitz mapping from\u0001the set of normalized dictionaries\nwith \u03bck\u22121 (D) < \u03b4 with the metric induced by k*kM E to C Sn\u22121 .\n\nThe proof of this lemma is the same as that of Lemma 7, except that a is taken to be an optimal representation that fulfills kak1 \u2264 \u03bb = k/ (1 \u2212 \u03bck\u22121 (D)), whose existence is guaranteed by\nProposition 1.\nThis concludes the proof of Theorem 1 and Corollary 2.\nThe next theorem shows that unfortunately, \u03a6 is not uniformly L-Lipschitz for any constant L,\nrequiring its restriction to an appropriate subset of the dictionaries.\n\u2032\nTheorem 10. For any k, n, p, there exists c > 0 and q, such\n\u0001 that for every \u03b5 > 0, there exist D, D\n\u2032\nsuch that kD \u2212 D kM E < \u03b5 but hHk ,D (q) \u2212 hHk ,D\u2032 (q) > c.\n\nProof. First we show that there exists c > 0 such that every dictionary will have k sparse representation error of at least c on some signal. Let \u03bdS n\u22121 be the uniform probability measure on the\nsphere, and Ac the probability assigned by it to the set within\n\u0001 c of a k dimensional subspace. As\nc \u0581 0, Ac also tends to zero, then there exists c > 0 s.t. kp Ac < 1. Then for that c there exists a\nset of positive measure on which hHk ,D > c, let q be a point in this set.\nTo complete the proof we consider a dictionary Dpwhose first k \u2212 1 elements are the standard basis {e1 , . . . , ek\u22121 }, its k the element is Dk = 1 \u2212 \u03b52 /2e1 + \u03b5ek /2, and the remaining\nelementspare chosen arbitrarily. Now construct D \u2032 to be identical to D except its kth element\nis v =\n1 \u2212 \u03b52 /2e1 + lq choosing l so that kvk2 = 1 (which implies that |l| < \u03b5/2). Then\nkD \u2212 D \u2032 kM E = k\u03b5ek /2 + lqk2 \u2264 \u03b5 and hHk ,D\u2032 (q) = 0.\nTo conclude the generalization bounds of Theorems 4, 5, 7, 8 and 9 from the covering number\nbounds we have provided, we use the following two results. The first has a simple proof which we\ntherefore give here. The second result is an adaptation of results by Bartlett et al. (2005), to our\nneeds, and explained further in the appendix.\nLemma 9. Let F be a class of [0, B] functions with covering number bound (C/\u03b5)d > e/B 2 under\nthe supremum norm. Then for every x > 0, with probability of at least 1 \u2212 e\u2212x over the m samples\nin Em chosen according to \u03bd, for all f \u2208 F:\n! r\nr\nr\n\u221a\nd log (C m)\nx\n4\n+\n+\n.\nEf \u2264 Em f + B\n2m\n2m\nm\n10\n\n\f\u0001d\nLemma 10. If F is a class of [0, 1] functions with C > 2 and d \u2208 N s.t. N (\u03b5, F, L2 (\u03bd)) \u2264 C\u03b5\nfor every probability measure \u03bd and \u03b5 > 0, then for all K, \u03b1, x > 0, f \u2208 F, with probability at\nleast 1 \u2212 e\u2212x over the m samples used in Em and drawn from \u03bd:\n)\n(\n\u0001\nm\n(d\n+\n1)\nlog\n11x + 5K\n20\n+\n22\nlog\n(m)\n\u03b1C 2\nK\n\u03b1\n+\nEm f + 6K max\n, (480)2\n,\n.\nEf \u2264\nK\u22121\n2m\nm\nm\nm\nOur fast rates results are simple applications of this lemma, noting that an \u03b5 cover in C Sn\u22121\nis also an \u03b5 cover under an L2 metric induced by any measure.\n\n\u0001\n\nOf Lemma 9. We wish to bound supf \u2208F Ef \u2212Em f . Take F\u03b5 to be a minimal \u03b5 cover of F, then for\nan arbitrary f , denoting f\u03b5 an \u03b5 close member of F\u03b5 , Ef \u2212 Em f \u2264 Ef\u03b5 \u2212 Em f\u03b5 + 2\u03b5. In particular,\nsupf \u2208F Ef \u2212 Em f \u2264 2\u03b5 + supf \u2208F\u03b5 Ef \u2212 Em f . To bound the supremum on the now finite class\nof functions, note that Ef \u2212 Em f is a function of m independent variables (the samples chosen\naccording to \u03bd), which changes by at most B/m when one of the variables is modified. Then by the\nbounded differences\n\u0001 inequality, P (Ef \u2212 Em f \u2212 E(Ef \u2212 Em f ) > t) = P (Ef \u2212 Em f > t) \u2264\n\u22122\n2\nexp \u22122mB t .\nThe probability that any of the |F\u03b5 | differences\nunder the supremum is\u0001 larger than t may be\n\u0001\nunion bounded as |F\u03b5 | * exp \u22122mB \u22122 t2 \u2264 exp d log (C/\u03b5) \u2212 2mB \u22122 t2 .\nIn order to control the probability with x as in the statement\nof\np\npthe lemma, we need to have\nB 2 /2m d log (C/\u03b5) + x. Then with\nx = d log (C/\u03b5) \u2212 mB \u22122 t2 and thus we choose t =\nhigh probability we bound the supremum of differences\nby t which is upper bounded,\nusing the\n\u0010p\n\u0011\np\nassumption on the covering number bound, by B\nd log (C/\u03b5) /2m + x/2m .\nThen the proof is completed by substitution into the bound over the whole function class F and\n\u221a\ntaking \u03b5 = 1/ m.\n\n4 On the Babel function\nThe Babel function is one of several metrics defined in the sparse representations literature to quantify an \"almost orthogonality\" property that dictionaries may enjoy. Such properties have been\nshown to imply theoretical properties such as uniqueness of the optimal k sparse representation.\nIn the algorithmic context, Donoho and Elad (2003) and Tropp (2004) use the Babel function to\nshow that particular tractable algorithms for finding sparse representations are indeed approximation algorithms when applied to such dictionaries. This reinforces the practical importance of the\nlearnability of this class of dictionary. We proceed to discuss some elementary properties of the\nBabel function, and then state a bound on the proportion of dictionaries having sufficiently good\nBabel function.\nMeasures of orthogonality are typically defined in terms of inner products between the elements\nof the dictionary. Perhaps the simplest of these measures of orthogonality is the following special\ncase of the Babel function.\nDefinition 11. The coherence of a dictionary D is \u03bc1 (D) = maxi,j |hdi , dj i|.\nThe Babel function, in considering sums of k inner products at a time, rather than the maximum\nover all inner products, is better adapted to quantify the effects of non orthogonality on representing\n11\n\n\fa signal with particular level k + 1 of sparsity. The additional expressive power of \u03bck over \u03bc1\nis illustrated by considering that ensuring that \u03bck < 1 by restricting \u03bc1 implies the constraint\n\u03bc1 (D) < 1/k, which for k > 1 would exclude a dictionary in which pairs of elements have inner\nproduct 0 except for some disjoint pairs whose inner product equals to half, despite such a dictionary\nhaving \u03bck = 1/2 for any k.\nTo better understand \u03bck (D), we consider first its extreme values. When \u03bck (D) = 0, for any\nk > 1, this means that D is an orthogonal set (therefore p \u2264 n). The maximal value of \u03bck (D) is k,\nand occurs only if some dictionary element is repeated (up to sign) at least k + 1 times.\nA well known generic class of dictionaries with more elements than a basis is that of frames (see\nDuffin and Schaeer, 1952), which include many wavelet systems and filter banks. Some frames can\nbe trivially seen to fulfill our condition on the Babel function.\nProposition 3. Let D \u2208 Rn\u00d7p be a frame of Rn , so that for every v \u2208 Sn\u22121 we have that A \u2264\nP\nn\ni=1 |hv, di i| \u2264 B, with kdi k2 = 1 for all i, and B < 1 + 1/(p \u2212 1). Then \u03bck\u22121 (D) < 1.\nThis may be easily verified using the relation between k*k1 and k*k2 in Rp\u22121 .\n\n4.1 Proportion of dictionaries with \u03bck\u22121(D) < \u03b4\nWe return to the question of the prevalence of dictionaries from D\u03b4 . Are almost all dictionaries\nin D\u03b4 ? If the answer is affirmative, it implies that Theorem 8 is quite strong, and representation\nfinding algorithms such as basis pursuit are almost always exact, which might help prove properties\nof dictionary learning algorithms. If the opposite is true and few dictionaries are in D\u03b4 , the results\nof this paper are weak. While there might be better measures on the space of dictionaries, we\nconsider one that seems natural: suppose that a dictionary D is constructed by choosing p unit\nvectors uniformly from Sn\u22121 ; what is the probability that \u03bck\u22121 (D) < \u03b4?\nTheorem 2 gives us the following answer to this question. Under the assumption that the sparsity\n\u221a\nparameter k grows slowly, if at all, as n \u0580 \u221e (specifically, that k log p = o( n)), this theorem\nimplies that asymptotically almost all dictionaries under the Lebesgue measure are learnable.\nThe remainder of this section is devoted to the proof of Theorem 2. This proof relies heavily on\nthe Orlicz norms for random variables and their properties; Van der Vaart and Wellner (1996) give\na detailed introduction. We recall a few of the definitions and facts presented there.\nDefinition 12. Let \u03c8 be a non-decreasing, convex function with \u03c8(0) = 0, and let X be a random\nvariable. Then\n\u001a\n\u0012\n\u0013\n\u001b\n|X|\nkXk\u03c8 = inf C > 0 : E\u03c8\n<1\nC\nis called an Orlicz norm.\n2\n\nAs may be verified, these are indeed norms for appropriate \u03c8, such as \u03c82 , ex \u2212 1, which is\nthe case that will interest us most.\nBy the Markov inequality we can obtain that variables with finite Orlicz norms have light tails.\n\u0011\u0011\u22121\n\u0010 \u0010\nFact 2. We have P (|X| > x) \u2264 \u03c82 x/ kXk\u03c82\n.\n\nThe next fact is an almost converse to the last fact, stating that light tailed random variables have\nfinite \u03c82 Orlicz norms.\n12\n\n\f2\n\nFact 3. Let A, B > 0 and P (|X| \u2265 x) \u2264 Ae\u2212Bx for all x, where p \u2265 1, then kXk\u03c82 \u2264\n\n((1 + A) /B)1/2 .\n\nThe following bound on the maximum of variables with light tails.\n\u221a\nFact 4. We have kmax1\u2264i\u2264m Xi k\u03c82 \u2264 K log m maxi kXi k\u03c82 .\n\n\u221a\nThe constant K may be upper bounded by 2. Note that the independence of Xi is not required.\nWe use also one isoperimetric fact about the sphere in high dimension.\n\nDefinition 13. The \u03b5 expansion of a set D in a metric space (X, d) is defined as\nD\u03b5 = {x \u2208 X|d (x, D) \u2264 \u03b5} ,\nwhere d(x, A) = inf a\u2208A d(x, a).\nFact 5 (L\u00e9vy's\nisoperimetric\ninequality 1952). Let C be one half of Sn\u22121 , then \u03bc\n\u0011\n\u0010\np\u03c0\n2\n(n\u22122)\u03b5\n.\n8 exp \u2212\n2\n\nSn\u22121 \\C\u03b5\n\n\u0001\u0001\n\n\u2264\n\nOur goal in the reminder of this subsection is to obtain the following bound.\n\nLemma 14. Let D be a dictionary chosen at random as described above, then\n\u221a\nk\u03bck (D)k\u03c82 \u2264 5k log p/ n \u2212 2.\nOur probabilistic bound on \u03bck\u22121 is a direct conclusion of Fact 3 and Lemma 14 which we now\nproceed to prove. The plan of our proof is to bound the \u03c82 metric of \u03bck from the inside terms and\noutward using Fact 4 to overcome the maxima over possibly dependent random variables.\nLemma 15. Let X1 , X2 be unit vectors chosen uniformly and independently from Sn\u22121 , then\np\nk|hX1 , X2 i|k\u03c82 \u2264 6/(n \u2212 2).\nWe denote the bound on the right hand side W .\n\nProof. Taking X to be uniformly chosen from Sn\u22121 , for any constant unit vector x0 we have that\nhX, x0 i is a light tailed random variable by Fact 5. By Fact 3, we may bound khX, x0 ik\u03c82 . Replacing x0 by a random unit vector is equivalent to applying to X a uniformly chosen rotation, which\ndoes not change the analysis.\nThe next step is to bound the inner maximum appearing in the definition of \u03bck .\nLemma 16. Let {di }pi=1 be uniformly and independently chosen unit vectors then\nmax\n\n\u039b\u2282{2,...,p}\u2227|\u039b|=k\n\nX\n\n\u03bb\u2208\u039b\n\n|hd1 , d\u03bb i|\n\n13\n\n\u03c82\n\n\u2264 kKW\n\np\nlog (p \u2212 1).\n\n\fProof. Take X\u03bb to be hD1 , D\u03bb i. Then using Fact 4 and the previous lemma we find\nmax\n\n1\u2264\u03bb\u2264p\u2227\u03bb6=i\n\n|X\u03bb |\n\n\u03c82\n\n\u2264K\n\np\n\nlog (p \u2212 1) max k|X\u03bb |k\u03c82 \u2264 KW\n\u03bb\n\np\nlog (p \u2212 1).\n\nDefine the random permutation \u03bbj s.t. X\u03bbj are non-increasing. In this notation, it is clear\nP\nPk\nthat max\u039b\u2282{2...p}\u2227|\u039b|=k \u03bb\u2208\u039b |X\u03bb | =\nj=1 X\u03bbj . Note that |X\u03bbi | \u2264 |X\u03bb1 | then for every i,\np\nk|X\u03bbi |k\u03c82 \u2264 k|X\u03bb1 |k\u03c82 \u2264 KW log (p \u2212 1).\np\nP\nPm\n\u2264 m\nX\u03bbj \u03c8 \u2264 mKW log (p \u2212 1).\nBy the triangle inequality,\nj=1\nj=1 X\u03bbj\n2\n\n\u03c82\n\nRemark 1. Two facts are relevant to the tightness of the approximations in the last proof. First, that\n|X\u03bbi | are variables with positive expectation bounded away from zero, thus the norm of their sum\nmust scale at least linearly in the number of summands, so the triangle inequality is essentially tight.\nSecond we consider the bound k|X\u03bbi |k\u03c82 \u2264 k|X\u03bb1 |k\u03c82 , and note its looseness is strictly limited by\np\nthe slow growth of log(*), and in any case is bounded by 2.\nTo complete the proof of Lemma 14, we replace D1 with the dictionary element maximizing\nthe Orlicz norm, by another application of Fact 4, and to complete the proof of Theorem 2, apply\nFact 3 to the estimated Orlicz norm.\n\n5 Dictionary learning in feature spaces\nWe propose in Section 2 a scenario in which dictionary learning is performed in a feature space\ncorresponding to a kernel function. Here we show how to adapt the different generalization bounds\ndiscussed in this paper for the particular case of Rn to more general feature spaces, and the dependence of the sample complexities on the properties of the kernel function or the corresponding\nfeature mapping. We begin with the relevant specialization of the results of Maurer and Pontil\n(2010) which have the simplest dependence on the kernel, and then discuss the extensions to k\nsparse representation and to the cover number techniques presented in the current work.\nTheorem 3 applies as is to the feature space, under the simple assumption that the dictionary\nelements and signals are in its unit ball which is guaranteed by some kernels such as the Gaussian\nkernel. Then we take \u03bd on the unit ball of H to be induced by some distribution \u03bd \u2032 on the domain\nof the kernel, and the theorem applies to any such \u03bd \u2032 on R. Nothing more is required if the representation is chosen from R\u03bb . The corresponding generalization bound for k sparse representations\nwhen the dictionary elements are near orthogonal in the feature space is given in Proposition 2.\nOf Proposition 2. Proposition 1 applies with the Euclidean norm of H, and \u03b3 = 1. We apply\nTheorem 3 with \u03bb = k/ (1 \u2212 \u03b4).\nThe results so far show that generalization in dictionary learning can occur despite the potentially infinite dimension of the feature space, without considering practical issues of representation\nand computation. We now make the domain and applications of the kernel explicit in order to\naddress a basic computational question, and allow the use of cover number based generalization\nbounds to prove Theorem 9. We now consider signals represented in a metric space (R, d), in\nwhich similarity is measured by the kernel \u03ba corresponding to the feature map \u03c6 : R \u2192 H. The\n14\n\n\felements of a dictionary D are now from R, and we denote \u03a6D their mapping by \u03c6 to H. Then\nrepresentation error function used is h\u03c6,A,D .\nWe now show that the approximation error in feature space is a quadratic function of the coefficient vector, which may be found by applications of the kernel.\n\u0001\n2 kernel appliProposition 4. Computing the representation\nerror\nat\na\ngiven\nx,\na,\nD\nrequires\nO\np\n\u0001\ncations in general, and only O k2 + p when a is k sparse.\nProof. Writing the error;\n\nk(\u03a6D) a \u2212 \u03c6 (x)k2 = h(\u03a6D) a \u2212 \u03c6 (x) , (\u03a6D) a \u2212 \u03c6 (x)i\n\n= h(\u03a6D) a, (\u03a6D) ai + h\u03c6 (x) , \u03c6 (x)i \u2212 2 h\u03c6 (x) , (\u03a6D) ai\n+\n+\n*\n* p\np\np\nX\nX\nX\n\u03c6 (di ) ai\n\u03c6 (dj ) aj + h\u03c6 (x) , \u03c6 (x)i \u2212 2 \u03c6 (x) ,\n\u03c6 (di ) ai ,\n=\n=\n\n=\n\ni=1\np\nX\n\nai\n\ni=1\np\nX\ni=1\n\nai\n\ni=1\n\nj=1\n\np\nX\n\nj=1\np\nX\nj=1\n\naj h\u03c6 (di ) , \u03c6 (dj )i + h\u03c6 (x) , \u03c6 (x)i \u2212 2\naj \u03ba (di , dj ) + \u03ba (x, x) \u2212 2\n\np\nX\n\np\nX\ni=1\n\nai h\u03c6 (x) , \u03c6 (di )i\n\nai \u03ba (x, di ) .\n\ni=1\n\nWe note that the k sparsity constraint on a poses algorithmic difficulties beyond those addressed\nhere. Some of the common approaches to these, such as Orthogonal Matching Pursuit (Chen et al.,\n1989), also depend on the data only through their inner products, and may therefore be adapted to\nthe kernel setting.\nThe cover number bounds depend strongly on the dimension of the space of dictionary elements.\nTaking H as the space of dictionary elements is the simplest approach, but may lead to vacuous\nor weak bounds, for example in the case of the Gaussian kernel whose feature space is infinite\ndimensional. Instead we propose to use the space of data representations R, whose dimensions are\ngenerally bounded by practical considerations. In addition, we will assume that the kernel is not\n\"too wild\" in the following sense.\nDefinition 17. Let L, \u03b1 > 0, and let (A, d\u2032 ) and (B, d) are metric spaces. We say a mapping\nf : A \u2192 B is uniformly L H\u00f6lder of order \u03b1 on a set S \u2282 A if \u2200x, y \u2208 S, the following bound\nholds:\nd (f (x), f (y)) \u2264 L * d\u2032 (x, y)\u03b1 .\nThe relevance of this smoothness condition is as follows:\nFact 6. A H\u00f6lder function maps an \u03b5 cover of S to an L\u03b5\u03b1 cover of its image f (S). Thus, to obtain\nan \u03b5 cover of the image of S, it is enough to begin with an (\u03b5/L)1/\u03b1 cover of S.\nA H\u00f6lder feature map \u03c6 allows us to bound the cover numbers of the dictionary elements in H\nusing their cover number bounds in R. Note that not every kernel corresponds to a H\u00f6lder feature\nmap (the Dirac \u03b4 kernel is a counter example: any two distinct elements are mapped to elements at\na mutual distance of 1), and not for every kernel the feature map is known. The following lemma\nbounds the geometry of the feature map using that of the kernel.\n15\n\n\fLemma 18. Let \u03ba(x, y) = h\u03c6(x), \u03c6(y)i, and assume further that \u03ba fulfills a H\u00f6lder condition of\norder \u03b1 uniformly in each parameter, that is, |\u03ba(x, y) \u221a\n\u2212 \u03ba(x + h, y)| \u2264 L khk\u03b1 . Then \u03c6 uniformly\nfulfills a H\u00f6lder condition of order \u03b1/2 with constant 2L.\nThis result is not sharp. For example, for the Gaussian case, both kernel and the feature map are\nH\u00f6lder order 1.\nProof. Using the H\u00f6lder condition, we have that k\u03c6(x) \u2212 \u03c6(y)k2H = \u03ba (x, x) \u2212 \u03ba (x, y) + \u03ba (y, y) \u2212\n\u03ba (x, y) \u2264 2L kx \u2212 yk\u03b1 . All that remains is to take the square root of both sides.\nFor a given feature mapping \u03c6, set of representations R, we define two families of function\nclasses so:\nW\u03c6,\u03bb = {h\u03c6,R\u03bb ,D : D \u2208 D p } and\n\nQ\u03c6,k,\u03b4 = {h\u03c6,Hk ,D : D \u2208 D p \u2227 \u03bck\u22121 (\u03a6D) \u2264 \u03b4} .\nThe next proposition completes this section by giving the cover number bounds for the representation error function classes induced by appropriate kernels, from which various generalization\nbounds easily follow, such as Theorem 9.\nProposition 5. Let R be a set of representations with a cover number bound of (C/\u03b5)n , and let\neither \u03c6 be uniformly L H\u00f6lder condition of order \u03b1 on R, or \u03ba be uniformly L H\u00f6lder of order\n2\u03b1 on R in each parameter, and let \u03b3 = supd\u2208R k\u03c6(d)kH . Then the function classes W\u03c6,\u03bb and\nQ\u03c6,k,\u03b4 taken as \u0011metric spaces\nwith the supremum \u0011\nnorm, have \u03b5 covers of cardinalities at most\n\u0010\n\u0010\nnp\n\u00011/\u03b1 np\n1/\u03b1\n2\nC (\u03bb\u03b3L/\u03b5)\nand C k\u03b3 L/ (\u03b5 (1 \u2212 \u03b4))\n, respectively.\n\nProof. We first consider the simpler case of l1 constrained coefficients. If kak1 \u2264 \u03bb and also\nmaxd\u2208D k\u03c6(d)kH \u2264 \u03b3 then by the considerations applied in section 3, to obtain an \u03b5 cover of the set\n{mina k(\u03a6D) a \u2212 \u03c6 (x)kH : D \u2208 D}, it is enough to obtain an \u03b5/ (\u03bb\u03b3) cover of {\u03a6D : D \u2208 D}. If\n\u22121/\u03b1\nalso \u03c6 is uniformly L H\u00f6lder of order \u03b1 over R then an\n\u0011cover of the set of dictionaries\n\u0010 (\u03bb\u03b3L/\u03b5)\nis sufficient, which as we have seen requires at most C (\u03bb\u03b3L/\u03b5)1/\u03b1\n\nnp\n\nelements.\n\nIn the case of l0 constrained representation, the bound on \u03bb due to Proposition 1 is \u03b3k (1 \u2212 \u03b4),\nand the result follows from the above by substitution.\n\n6 Conclusions\nOur work has several implications on the design of dictionary learning algorithms as used in signal, image, and natural language processing. First, the fact that generalization is only logarithmically dependent on the l1 norm of the coefficient vector widens the set of applicable approaches\nto penalization. Second, in the particular case of k sparse representation, we have shown that the\nBabel function is a key property for the generalization of dictionaries. It might thus be useful to\nmodify dictionary learning algorithms so that they obtain dictionaries with low Babel functions,\npossibly through regularization or through certain convex relaxations. Third, mistake bounds (e.g.,\nMairal et al. 2010) on the quality of the solution to the coefficient finding optimization problem\n16\n\n\fmay lead to generalization bounds for practical algorithms, by tying such algorithms to k sparse\nrepresentation.\nThe upper bounds presented here invite complementary lower bounds. The existing lower\nbounds for k = 1 (vector quantization) and for k = p (representation using PCA directions) are\napplicable, but do not capture the geometry of general k sparse representation, and in particular\ndo not clarify the effective dimension of the unrestricted class of dictionaries for it. We have not\nexcluded the possibility that the class of unrestricted dictionaries has the same dimension as that\nof those with small Babel\u0001 function.\nThe best upper bound we know for the larger class, being the\np 2\u000e\ntrivial one of order O k n m), leaves a significant gap for future exploration.\nWe mention also that the dependence on \u03bck\u22121 can also be viewed from an \"algorithmic luckiness\" perspective (Herbrich and Williamson, 2003): if the dictionary has favorable geometry in the\nsense that the Babel function is small the generalization bounds are encouraging.\n\nAcknowledgments\nWe thank Shahar Mendelson for helpful discussions. This research was partly supported by the\nEuropean Communitys FP7-FET program, SMALL project, under grant agreement no. 225913.\n\nReferences\nMichal Aharon, Michael Elad, and Alfred M. Bruckstein. K-SVD: Design of dictionaries for sparse\nrepresentation. In PROCEEDINGS OF SPARS, pages 9\u201312, 2005.\nMartin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations. Cambridge Univ Pr, 1999.\nPeter L. Bartlett, Tam\u00e1s Linder, and G\u00e1bor Lugosi. The minimax distortion redundancy in empirical\nquantizer design. IEEE Transactions on Information theory, 44(5):1802\u20131813, 1998.\nPeter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexities. Ann.\nStatist., 33:1497\u20131537, 2005.\nG\u00e9rard Biau, Luc Devroye, and G\u00e1bor Lugosi. On the performance of clustering in Hilbert spaces.\nInformation Theory, IEEE Transactions on, 54(2):781\u2013790, 2008. ISSN 0018-9448.\nG. Blanchard, O. Bousquet, and L. Zwald. Statistical properties of kernel principal component\nanalysis. Machine Learning, 66(2):259\u2013294, 2007. ISSN 0885-6125.\nAlfred M. Bruckstein, David L. Donoho, and Michael Elad. From sparse solutions of systems of\nequations to sparse modeling of signals and images. SIAM Review, 51(1):34\u201381, 2009.\nEmmanuel J. Candes and Terence Tao. Near-optimal signal recovery from random projections:\nUniversal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406\u20135425,\n2006.\nS. Chen, SA Billings, and W. Luo. Orthogonal least squares methods and their application to nonlinear system identification. International Journal of Control, 50(5):1873\u20131896, 1989.\n17\n\n\fScott S. Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by basis pursuit.\nSIAM Review, 43(1):129\u2013159, 2001.\nF. Cucker and S. Smale. On the mathematical foundations of learning. BULLETIN-AMERICAN\nMATHEMATICAL SOCIETY, 39(1):1\u201350, 2002.\nGeoff Davis, St\u00e8phane Mallat, and Marco Avellaneda. Adaptive greedy approximations. Constructive approximation, 13(1):57\u201398, 1997.\nDavid L. Donoho and Michael Elad. Optimally sparse representation in general (nonorthogonal)\ndictionaries via l1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197,\n2003.\nRichard J. Duffin and Albert C. Schaeer. A class of nonharmonic Fourier series. Trans. Amer. Math.\nSoc, 72:341\u2013366, 1952.\nR\u00e9mi Gribonval and Karin Schnass. Dictionary identification - sparse matrix-factorisation via l1 minimisation. Available at: arXiv:0904.4774v2 [cs.IT], 2009.\nRalf Herbrich and Robert Williamson. Algorithmic luckiness. JMLR, 3:175\u2013212, 2003. ISSN\n1532-4435.\nRoger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge Univ Pr, 1990.\nSham Kakade and Ambuj Tewari.\nLearning theory lecture notes,\nhttp://ttic.uchicago.edu/ \u0303tewari/lectures/lecture15.pdf.\n\n2008.\n\nAndreas Krause and Volkan Cevher. Submodular dictionary selection for sparse representation. In\nICML, 2010.\nKenneth Kreutz-Delgado, Joseph F. Murray, Bhaskar D. Rao, Kjersti Engan, Te-Won Lee, and\nTerrance J. Sejnowski. Dictionary learning algorithms for sparse representation. Neural computation, 15(2):349\u2013396, 2003.\nHonglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. Efficient sparse coding algorithms.\nAdvances in neural information processing systems, 19:801, 2007.\nP. L\u00e9vy. Problemes concrets danalyse fonctionnelle. Bull. Amer. Math. Soc, 58:408\u2013411, 1952.\nISSN 1088-9485.\nMichael S. Lewicki, Terrence J. Sejnowski, and Howard Hughes. Learning overcomplete representations. Neural Computation, 12:337\u2013365, 1998.\nJulien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factorization and sparse coding. JMLR, 11:19\u201360, 2010.\nA. Maurer and M. Pontil. K-Dimensional Coding Schemes in Hilbert Spaces. Available at:\narXiv:1002.0832v1 [stat.ML], 2010.\nBruno A. Olshausen and David J. Fieldt. Sparse coding with an overcomplete basis set: a strategy\nemployed by V1. Vision Research, 37:3311\u20133325, 1997.\n18\n\n\fGabriel Peyr\u00e9. Sparse modeling of textures. Journal of Mathematical Imaging and Vision, 34(1):\n17\u201331, 2009.\nMatan Protter and Michael Elad. Sparse and redundant representations and motion-estimation-free\nalgorithm for video denoising. Wavelets XII. Proceedings of the SPIE, 6701:43, 2007.\nJ. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. Cambridge Univ Pr, 2004.\nJ. Shawe-Taylor, C.K.I. Williams, N. Cristianini, and J. Kandola. On the eigenspectrum of the Gram\nmatrix and the generalization error of kernel-PCA. Information Theory, IEEE Transactions on,\n51(7):2510\u20132522, 2005. ISSN 0018-9448.\nJoel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform.\nTheory, 50:2231\u20132242, 2004.\nAW Van der Vaart and J.A. Wellner. Weak convergence and empirical processes. Springer Verlag,\n1996.\nJohn Wright, Allen Y. Yang, Arvind. Ganesh, S. Shankar Sastry, and Yi Ma. Robust face recognition\nvia sparse representation. IEEE Transactions on Pattern Analysis and Machine Intelligence,\npages 210\u2013227, 2008.\nJianchao Yang, Kai Yu, Yihong Gong, and Thomas Huang. Linear spatial pyramid matching using\nsparse coding for image classification. In IEEE Conference on Computer Vision and Pattern\nRecognition, 2009.\n\nAppendix A: generalization with fast rates\nIn this appendix we justify some adaptations in Lemma 10 relative to its origins in Bartlett et al.\n(2005). Specifically, we assume only growth rates instead of combinatorial dimensions and give\nexplicit constants for this case (no particular effort was made to make the constants tight).\nFollowing are some concepts and general results needed to prove Lemma 10 beyond those\nintroduced in the main body of paper.\nDefinition 19. Let F be a subset of a vector space X, x \u2208 X. The star shaped closure of F around\nx is\n\u22c6 (F, x) = {\u03bbf + (1 \u2212 \u03bb)x : f \u2208 F \u2227 \u03bb \u2208 [0, 1]} .\nDefinition 20.pA function f : R+ \u2192 R+ is called sub-root if it is non negative, non decreasing and\nif r 7\u2192 f (r)/ (r) is non increasing for r > 0.\n\nm\nDefinition 21. Let {Zi }m\ni=1 \u222a {\u03b5i }i=1 be independent variables, where \u03b5i are uniform over {\u22121, 1},\nand Zi are i.i.d. The empirical Rademacher average of F is\n\"\n#\nn\n1 X\nR\u0302m (F) = E sup\n\u03b5i fi (Zi ) |Z1 , . . . , Zm .\nf \u2208F m\ni=1\n\n19\n\n\fLemma 22. Let R\u0302m (F) be the empirical Rademacher averages of F on a sample {Zi }m\ni=1 . We\nhave\nZ \u221er\nlog N (\u03b5, F, L2 (\u03bdm ))\nd\u03b5,\nR\u0302m (F) \u2264 12\nm\n0\nP\nwhere \u03bdm = m\u22121 m\ni=1 \u03b4Zi .\nSee Kakade and Tewari (2008) for a proof.\n\np\nRxp\n1\nLemma 23. For any \u03b3 \u2265 e 2 and x \u2208 [0, 1], we have 0 log (\u03b3/\u03b5)d\u03b5 \u2264 2x log (\u03b3/x).\nq\n\u0010q\n\u0011\n\u221a\nRxq\nlog x\u03b3 , where erf(t) =\nProof. The indefinite integral 0 log \u03b3\u03b5 d\u03b5 is x log x\u03b3 \u2212 2\u03c0 \u03b3 * erf\n\u221a Rt\n2\n2/ \u03c0 0 e\u2212u du. Then\nZ\n\nx\n\n0\n\nr\n\n\u0014 r\n\u0012r\n\u0013\u0015x\n\u221a\n\u03c0\n\u03b3\n\u03b3\n\u03b3\nlog d\u03b5 = x log \u2212\nlog\n\u03b3erf\n\u03b5\nx\n2\nx 0\nr\n\u0012r\n\u0012r\n\u0013\n\u0013\u0013\n\u0012 r\n\u221a\n\u221a\n\u03b3\n\u03b3\n\u03b3\n\u03b3\n\u03c0\n\u03c0\nlog\nlog\n\u03b3erf\n\u03b3erf\n\u2212 lim x log \u2212\n= x log \u2212\nx\u21920\nx\n2\nx\nx\n2\nx\nr\nr\n\u0013\n\u0013\n\u221a \u0012 \u0012\n\u03b3\n\u03b3\n\u03c0\nlog\n\u03b3 erf\n\u2212 erf (\u221e) .\n= x log \u2212\nx\n2\nx\n\nThe error function erf is related to the\n\u0010 \u0011 as the\n\u0010 tail probability\n\u0010 \u0011\u0011 of a normal variable, also known\nx\n1\n\u21d0\u21d2 2Q (x) = 1 \u2212 erf \u221ax2\n\u21d0\u21d2\nQ function. In particular, Q (x) = 2 1 \u2212 erf \u221a2\n\u221a \u0001\n\u221a\n\u0001\n2\n2Q 2x = 1 \u2212 erf (x). We thus substitute and then use the bound Q(x) < e\u2212x /2 / x 2\u03c0 :\nx\n\nr\n\nr\n\u0012r\n\u0013\n\u0013\n\u0013\u0013\n\u221a \u0012 \u0012r\n\u221a \u0012\n\u03c0\n\u03c0\n\u03b3\n\u03b3\n\u03b3\n\u03b3\n\u03b3 erf\n\u03b3 \u22122Q\n\u2212 1 = x log \u2212\nlog \u2212\nlog\n2 log\nx\n2\nx\nx\n2\nx\nr\n\u0013\n\u0012r\n\u03b3 \u221a\n\u03b3\n= x log + \u03c0\u03b3Q\n2 log\nx\nx\n(Bound on Q)\n\u0012\u221a\n\u00132\nr\n\u03b3\n2 log x\n\u03b3 \u221a\n1\n\u2212\n2\n< x log + \u03c0\u03b3 q\n\u221a e\n\u03b3\nx\n2 log x 2\u03c0\nr\n1\n\u03b3\n= x log + x q\nx\n2 log x\u03b3\n\uf8eb\n\uf8f6\nr\n\u03b3\n1\n\uf8f8\n= x \uf8ed log + q\nx 2 log \u03b3\nx\n\nq\nq\n1\nlog x\u03b3 >\nBy our assumptions, x\u03b3 \u2265 \u03b3 \u2265 e1/2 \u21d0\u21d2\n2\n\u0012q\n\u0013\nq\nx\nlog \u03b3x + \u221a 1 \u03b3 \u2264 2x log \u03b3x , completing the proof.\n2\n\nlog\n\nx\n\n20\n\n\u21d0\u21d2\n\n\u221a1\n\n2\n\nlog\n\n\u03b3\nx\n\n<\n\nq\n\n1\n2,\n\nthen\n\n\fWe return to prove Lemma 10.\nProof. The core of the proof is to define particular sub-root function, and show its fixed point decays\nas 1/m. We then apply Theorem 3.3 of Bartlett, Bousquet and Mendelson Bartlett et al. (2005) to\nthis sub-root function to complete the proof.\nWe define the function\n\b\n11 log m\n\u03c8 (r) = 10ERm f \u2208 \u22c6 (F, 0) |Ef 2 \u2264 r +\n.\nm\n\nBy Lemma 3.4 of Bartlett et al. (2005) (with T f = Ef 2 and f\u02c6 = 0) and Lemma 3.2 Bartlett et al.\n(2005), this function is sub-root and thus has a unique fixed point, which we denote r \u2217 , and r <\nr \u2217 \u21d0\u21d2 r < \u03c8 (r).\nTo upper bound r \u2217 we first construct an upper bound on \u03c8, in which Ef 2 is replaced by Em f 2 ,\nvalid for r \u2265 r \u2217 . The expectation of this upper bound is controlled using an entropy integral.\nWe make two observations.\n1. By Corollary 2.2 of Bartlett et al. (2005), with b = 1, for r > \u03c8(r) with probability at least\n1 \u2212 1/m,\n\b\n\b\nf \u2208 \u22c6 (F, 0) : Ef 2 \u2264 r \u2282 f \u2208 \u22c6 (F, 0) : Em f 2 \u2264 2r .\n\n\b\n2. By assumption (\u2200f \u2208 F) kf kL\u221e \u2264 1 and this implies Rm f \u2208 \u22c6 (F, 0) : Ef 2 \u2264 r \u2264 1.\n\nCombining the observations, we can bound\n\n\b\n\b\n1\n+ ERm f \u2208 \u22c6 (F, 0) : Em f 2 \u2264 2r .\nERm f \u2208 \u22c6 (F, 0) : Ef 2 \u2264 r \u2264\nm\nThen \u03c8 (r) \u2264 10\n\n1\nm\n\n\b\n\u0001\n+ ERm f \u2208 \u22c6 (F, 0) : Em f 2 \u2264 2r +\n\nr \u2217 = \u03c8 (r \u2217 ) \u2264 10\n\n\u0012\n\n11 log(m)\n,\nm\n\n\b\n1\n+ ERm f \u2208 \u22c6 (F, 0) : Em f 2 \u2264 2r \u2217\nm\n\n\u0013\n\n+\n\nand in particular\n11 log (m)\n.\nm\n\nWe denote \u03bdm the empirical measure\ninduced by the m samples (whose expectation is Em ).\n\b\nUnder the metric L2 (\u03bdm ), the set f \u2208 \u22c6 (F, 0) : Em f 2 \u2264 2r is covered by a single ball of radius\n\u221a\n2r around the zero function. Applying Lemma 22, we have\n\u221e\n\nr\n\nlog N (\u03b5, \u22c6 (F, 0) , L2 (\u03bdm ))\nd\u03b5\nm\n0\n\u221a\nr\nZ 2r\nlog N (\u03b5, \u22c6 (F, 0) , L2 (\u03bdm ))\nd\u03b5.\n= 12\nm\n0\n\nR\u0302m f \u2208 \u22c6 (F, 0) : Em f 2 \u2264 2r \u2264 12\n\b\n\nZ\n\nSince (\u2200f \u2208 F) kf kL2 (\u03bdm ) \u2264 1, an \u03b5 cover of F can be converted into an \u03b5 cover of \u22c6 (F, 0) by\nreplacing each element by 1/\u03b5+1 balls on the segment from it to 0. Then N (\u03b5, \u22c6 (F, 0) , L2 (\u03bdm )) \u2264\n21\n\n\f2N (\u03b5, F, L2 (\u03bdm )) /\u03b5 (a). Using also the assumption on C (b) and Lemma 23 (c), we find\n\n12\n\nZ\n\n\u221a\n0\n\n2r\n\nr\n\n\u221a\n\nv\n\u0010\nu\nu\n2r t log\n\n\u0001\nC d 2\n\u03b5\n\u03b5\n\n\u0011\n\nlog N (\u03b5, \u22c6 (F, 0) , L2 (\u03bdm )) (a)\nd\u03b5 \u2264 12\nd\u03b5\nm\nm\n0\nr\nZ \u221a2r s \u0012 \u0013\n(b)\nd+1\nC\nd\u03b5\nlog\n\u2264 12\nm\n\u03b5\n0\nv\n\u0011\n\u0010\nu\nu 2r (d + 1) log \u221aC\n(c)\nt\n2r\n\u2264 24\n.\nm\nZ\n\nSubstituting into \u03c8 (r \u2217 ), we obtain that\n\uf8eb\n\nv\n\u0011\uf8f6\n\u0010\nu\nu 2r \u2217 (d + 1) log \u221aC\n\uf8ec1\nt\n2r \u2217 \uf8f7\n\uf8f7 + 11 log (m)\nr \u2217 \u226410 \uf8ec\n\uf8ed m + 24\n\uf8f8\nm\nm\nv\n\u0010\n\u0011\nu\nu 2r \u2217 (d + 1) log \u221aC\nt\n\u2217\n10 + 11 log (m)\n2r\n+\n.\n=240\nm\nm\n\nLet \u03b1 > 0 be fixed. If r \u2217 \u2264 \u03b1C 2 /2m, our first step is complete. If not, then\nr \u2217 > \u03b1C 2 /2m \u21d0\u21d2\nand then\ns\n\n\u2217\n\nr \u2264 240\ns\n\n2r \u2217 (d + 1) log\nm\n\np\n\n\u221a\nm/\u03b1 > C/ 2r \u2217 ,\n\npm\u0001\n\u03b1\n\n+\n\n10 + 11 log (m)\nm\n\n\u0001\nr \u2217 (d + 1) log m\n10 + 11 log (m)\n\u03b1\n= 240\n+\nm\nm\n\uf8f1 s\n\uf8fc\n\u0001\n\uf8f2\nr \u2217 (d + 1) log m\n10 + 11 log (m) \uf8fd\n\u03b1\n\u2264 2 max 240\n,\n.\n\uf8f3\n\uf8fe\nm\nm\n\nThen either r \u2217 \u2264 (20 + 22 log (m)) /m (and the first step is complete), or\n\np\nr \u2217 \u2264 480 (r \u2217 (d + 1) log (m/\u03b1)) /m \u21d0\u21d2 r \u2217 \u2264 (480)2 ((d + 1) log (m/\u03b1)) /m\n\nand again we are done. We conclude that\nr \u2217 \u2264 max\n\n(\n\n(d + 1) log\n\u03b1C 2\n, (480)2\n2m\nm\n22\n\nm\n\u03b1\n\n\u0001\n\n20 + 22 log (m)\n,\nm\n\n)\n\n.\n\n\fHaving proved r \u2217 decays approximately as 1/m, we apply Theorem 3.3 of Bartlett et al. (2005),\nwith a = 0; b = 1; B = 1; T f = Ef 2 . By definition of T , it is clear that\n\b\n11 log m\n\u03c8 (r) = 10ERm f \u2208 \u22c6 (F, 0) |Ef 2 \u2264 r +\nm\n\b\n\u2265 ERm f \u2208 \u22c6 (F, 0) |Ef 2 \u2264 r\n= ERm {f \u2208 \u22c6 (F, 0) |T f \u2264 r}\n\nholds, then we can use part 2 of Theorem 3.3 of Bartlett et al. (2005), which allows the conclusion\nK\nEm f + 6Kr \u2217 + 11x+5K\n.\nthat for all f \u2208 F, Ef \u2264 K\u22121\nm\n\n23\n\n\f"}