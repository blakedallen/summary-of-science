{"id": "http://arxiv.org/abs/1107.4035v2", "guidislink": true, "updated": "2011-07-21T15:01:14Z", "updated_parsed": [2011, 7, 21, 15, 1, 14, 3, 202, 0], "published": "2011-07-20T17:04:12Z", "published_parsed": [2011, 7, 20, 17, 4, 12, 2, 201, 0], "title": "Towards Completely Lifted Search-based Probabilistic Inference", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.3859%2C1107.5866%2C1107.3584%2C1107.4172%2C1107.3986%2C1107.0340%2C1107.1791%2C1107.4912%2C1107.1343%2C1107.4072%2C1107.5556%2C1107.2871%2C1107.0393%2C1107.4992%2C1107.2005%2C1107.3410%2C1107.1897%2C1107.5488%2C1107.5459%2C1107.5495%2C1107.1151%2C1107.4695%2C1107.3063%2C1107.5353%2C1107.5858%2C1107.5073%2C1107.2319%2C1107.4382%2C1107.0234%2C1107.1317%2C1107.4944%2C1107.1386%2C1107.0087%2C1107.3681%2C1107.4065%2C1107.3345%2C1107.0368%2C1107.0585%2C1107.3203%2C1107.3535%2C1107.3405%2C1107.0069%2C1107.0816%2C1107.3738%2C1107.3794%2C1107.1668%2C1107.1800%2C1107.3848%2C1107.0825%2C1107.4487%2C1107.4825%2C1107.2959%2C1107.0009%2C1107.2212%2C1107.2242%2C1107.2621%2C1107.1276%2C1107.1843%2C1107.5082%2C1107.3927%2C1107.5640%2C1107.0022%2C1107.5914%2C1107.1191%2C1107.4343%2C1107.4122%2C1107.5069%2C1107.1302%2C1107.0107%2C1107.0667%2C1107.6036%2C1107.0844%2C1107.3581%2C1107.3629%2C1107.1932%2C1107.5651%2C1107.1338%2C1107.3868%2C1107.5574%2C1107.2168%2C1107.3292%2C1107.4021%2C1107.4035%2C1107.0651%2C1107.2565%2C1107.3996%2C1107.6041%2C1107.1946%2C1107.2662%2C1107.4991%2C1107.2576%2C1107.1204%2C1107.4405%2C1107.5052%2C1107.3420%2C1107.2715%2C1107.3167%2C1107.5417%2C1107.4696%2C1107.5791%2C1107.5071&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Towards Completely Lifted Search-based Probabilistic Inference"}, "summary": "The promise of lifted probabilistic inference is to carry out probabilistic\ninference in a relational probabilistic model without needing to reason about\neach individual separately (grounding out the representation) by treating the\nundistinguished individuals as a block. Current exact methods still need to\nground out in some cases, typically because the representation of the\nintermediate results is not closed under the lifted operations. We set out to\nanswer the question as to whether there is some fundamental reason why lifted\nalgorithms would need to ground out undifferentiated individuals. We have two\nmain results: (1) We completely characterize the cases where grounding is\npolynomial in a population size, and show how we can do lifted inference in\ntime polynomial in the logarithm of the population size for these cases. (2)\nFor the case of no-argument and single-argument parametrized random variables\nwhere the grounding is not polynomial in a population size, we present lifted\ninference which is polynomial in the population size whereas grounding is\nexponential. Neither of these cases requires reasoning separately about the\nindividuals that are not explicitly mentioned.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.3859%2C1107.5866%2C1107.3584%2C1107.4172%2C1107.3986%2C1107.0340%2C1107.1791%2C1107.4912%2C1107.1343%2C1107.4072%2C1107.5556%2C1107.2871%2C1107.0393%2C1107.4992%2C1107.2005%2C1107.3410%2C1107.1897%2C1107.5488%2C1107.5459%2C1107.5495%2C1107.1151%2C1107.4695%2C1107.3063%2C1107.5353%2C1107.5858%2C1107.5073%2C1107.2319%2C1107.4382%2C1107.0234%2C1107.1317%2C1107.4944%2C1107.1386%2C1107.0087%2C1107.3681%2C1107.4065%2C1107.3345%2C1107.0368%2C1107.0585%2C1107.3203%2C1107.3535%2C1107.3405%2C1107.0069%2C1107.0816%2C1107.3738%2C1107.3794%2C1107.1668%2C1107.1800%2C1107.3848%2C1107.0825%2C1107.4487%2C1107.4825%2C1107.2959%2C1107.0009%2C1107.2212%2C1107.2242%2C1107.2621%2C1107.1276%2C1107.1843%2C1107.5082%2C1107.3927%2C1107.5640%2C1107.0022%2C1107.5914%2C1107.1191%2C1107.4343%2C1107.4122%2C1107.5069%2C1107.1302%2C1107.0107%2C1107.0667%2C1107.6036%2C1107.0844%2C1107.3581%2C1107.3629%2C1107.1932%2C1107.5651%2C1107.1338%2C1107.3868%2C1107.5574%2C1107.2168%2C1107.3292%2C1107.4021%2C1107.4035%2C1107.0651%2C1107.2565%2C1107.3996%2C1107.6041%2C1107.1946%2C1107.2662%2C1107.4991%2C1107.2576%2C1107.1204%2C1107.4405%2C1107.5052%2C1107.3420%2C1107.2715%2C1107.3167%2C1107.5417%2C1107.4696%2C1107.5791%2C1107.5071&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The promise of lifted probabilistic inference is to carry out probabilistic\ninference in a relational probabilistic model without needing to reason about\neach individual separately (grounding out the representation) by treating the\nundistinguished individuals as a block. Current exact methods still need to\nground out in some cases, typically because the representation of the\nintermediate results is not closed under the lifted operations. We set out to\nanswer the question as to whether there is some fundamental reason why lifted\nalgorithms would need to ground out undifferentiated individuals. We have two\nmain results: (1) We completely characterize the cases where grounding is\npolynomial in a population size, and show how we can do lifted inference in\ntime polynomial in the logarithm of the population size for these cases. (2)\nFor the case of no-argument and single-argument parametrized random variables\nwhere the grounding is not polynomial in a population size, we present lifted\ninference which is polynomial in the population size whereas grounding is\nexponential. Neither of these cases requires reasoning separately about the\nindividuals that are not explicitly mentioned."}, "authors": ["David Poole", "Fahiem Bacchus", "Jacek Kisynski"], "author_detail": {"name": "Jacek Kisynski"}, "author": "Jacek Kisynski", "links": [{"href": "http://arxiv.org/abs/1107.4035v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1107.4035v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1107.4035v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1107.4035v2", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Towards Completely Lifted Search-based Probabilistic Inference\n\narXiv:1107.4035v2 [cs.AI] 21 Jul 2011\n\nDavid Poole, Fahiem Bacchus, and Jacek Kisynski\nhttp://www.cs.ubc.ca/~poole/\nhttp://www.cs.toronto.edu/~fbacchus/\nhttp://www.cs.ubc.ca/~kisynski/\n\nAbstract\nThe promise of lifted probabilistic inference is\nto carry out probabilistic inference in a relational probabilistic model without needing to reason about each individual separately (grounding\nout the representation) by treating the undistinguished individuals as a block. Current exact\nmethods still need to ground out in some cases,\ntypically because the representation of the intermediate results is not closed under the lifted operations. We set out to answer the question as to\nwhether there is some fundamental reason why\nlifted algorithms would need to ground out undifferentiated individuals. We have two main results: (1) We completely characterize the cases\nwhere grounding is polynomial in a population\nsize, and show how we can do lifted inference in\ntime polynomial in the logarithm of the population size for these cases. (2) For the case of noargument and single-argument parametrized random variables where the grounding is not polynomial in a population size, we present lifted\ninference which is polynomial in the population size whereas grounding is exponential. Neither of these cases requires reasoning separately\nabout the individuals that are not explicitly mentioned.\n\n1\n\nIntroduction\n\nThe problem of lifted probabilistic inference in its general\nform was first explicitly proposed by Poole [2003], who\nformulated the problem in terms of parametrized random\nvariables, introduced the use of splitting to complement\nunification, the parfactor representation of intermediate results, and an algorithm for multiplying factors in a lifted\nmanner. de Salvo Braz et al. [2005; 2007] invented counting elimination for some cases where grounding would create a factor with size exponential in the number of individu-\n\nals, but lifted inference can be done by counting the number\nof individuals with each assignment of values. Milch et al.\n[2008] proposed counting formulae as a representation of\nthe intermediate result of counting, which allowed for more\ncases where counting was applicable. However, this body\nof research has not fulfilled the promise of lifted inference,\nas the algorithms still need to ground in some cases. The\nmain problem is that the proposals are based on variable\nelimination [Zhang and Poole, 1994]. This is a dynamic\nprogramming approach which requires a representation of\nthe intermediate results, and the current representations for\nsuch results are not closed under all of the operations used\nfor inference. We sought to investigate whether there were\nfundamental reasons why we need to ground in some cases.\nAn alternative to variable elimination is to used searchbased methods based on conditioning such as recursive\nconditioning [Darwiche, 2001] and other methods [see e.g.,\nBacchus et al., 2009]. The advantage of these methods is\nthat conditioning simplifies the representations, rather than\ncomplicating them. The use of lifted search-based inference was proposed by Gogate and Domingos [2010], however to be both correct and able to do inference without\ngrounding requires more attention to detail than given in\nthat paper. This paper answer different questions than Jha\net al. [2010].\nNote that this paper is about exact inference. Lifted algorithms based on belief propagation (e.g. by Singla and\nDomingos [2008] and Kersting et al. [2009]) explicitly ignore the interdependence amongst the instances that exact\ninference needs to take into account.\nIn deriving an algorithm that never needs to ground, it is\noften the examples that demonstrate why simpler methods\ndo not work that are most insightful. We have thus chosen\nto write this paper by presenting examples that exemplify\nthe cases that need to be considered.\n\n\fS(A1, A1)\n\nS(x,y)\nR(x,y)\n\ny\n\nQ(x)\nx\n\nR(A1, A1)\n\n...\n\nS(A1, An)\n\nS(An, A1)\n\nR(A1, An)\n\nR(An, A1)\n\nQ(A1)\n\n(a)\n\n...\n\nS(An, An)\n\n...\n\n<{},{S(x,y),R(x,y)},\u03c61>\n\nR(An, An)\n\nQ(An)\n\n<{},{Q(x),R(x,y)},\u03c62>\n(c)\n\n(b)\n\nFigure 1: (a) a parametrized graphical model, (b) its grounding and (c) its parfactor graph\n\n2\n\nBackground\n\nThe problem of lifted inference arises in relational probabilistic models where there are probability distributions\nover random variables that represent relations which depend on individuals. Poole [2003] gives an example where\nthe probability that a person fitting a description committed\na crime depends on the population size, as this determines\nhow many other people fit the description. We don't want\nto reason about the other individuals separately. Rather, we\nwould like to reason about them as a block considering only\nthe number of such individuals.\nA population is a set of individuals. A population corresponds to a domain in logic. The population size is the\ncardinality of the population which can be a finite number.1\nFor the examples below, where there is a single population,\nwe write the population as A1 . . . An , where n is the population size.\nA parameter, which corresponds to a logical variable, is\nwritten in lower case. Parameters are typed with a population; if x is a parameter of type \u03c4, pop(x) is the population associated with x and |x| = |\u03c4| = |pop(x)|. We assume\nthat the populations are disjoint (and so the types are mutually exclusive). Constants are written starting with an upper\ncase letter.\nA parametrized random variable (PRV) is of the form\nF(t1 , . . . , tk ) where F is a k-ary functor (a function symbol or a predicate) and each ti is a parameter or a constant.\nEach functor has a range, which is {True, False} for predicate symbols. A parametrized random variable represents a\nset of random variables, one for each assignment of an individual to a parameter. The range of the functor becomes\nthe domain of the random variables.\nA substitution is of the form {x1 /t1 , . . . , xk /tk } where xi\nare distinct parameters and ti are constants or parameters,\nsuch that xi and ti are of the same type. Given a PRV r\nand a substitution \u03b8 = {x1 /t1 , . . . , xk /tk }, the application of\n\u03b8 on r, written r\u03b8 is the PRV with each xi replaced by ti .\n1 Infinite population sizes turn out to be simpler cases as p\u221e = 0\nfor any p < 1. So the infinite case allows for more pruning.\n\nA substitution \u03b8 = {x1 /t1 , . . . , xk /tk } grounds parameters\nx1 . . . xk if t1 . . . tk are constants. A grounding substitution\nof r is a substitution that grounds all of the parameters of r.\nProbabilistic inference relies on knowing whether two random variables are the same. With parametrized random\nvariables, we unify them to make them identical, but instead of just applying a substitution (as in regular theorem\nproving), Poole [2003] proposed to split parametrized random variables, forming the unifier and residual PRVs.\nExample 1 Applying substitution {x/A, y/B} to PRV\nFoo(x, y, z) results in PRV that is the direct application,\nFoo(A, B, z), and two \"residual\" PRVs, Foo(x, y, z) with the\nconstraint x 6= A and Foo(A, y, z) with the constraint y 6= B;\nthese three parametrized random variables, with their associated constraints, represent the same set of random variables as Foo(x, y, z).\nA parametrized graphical model (Bayesian network or\nMarkov network) is a network with parametrized random\nvariables as nodes, and the instances of these share potentials. We need to be explicit about which instances share\npotentials.\nExample 2 The parametrized graphical model of Figure\n1 (a), is shown using parametrized random variables and\nplates (where the parameters correspond to plates). The\nplate representation represents n instances of Q(x) and n2\ninstances of R(x, y) in the grounding, shown in Figure 1 (b).\nWe assume the input to our algorithm is in the form of parfactors. A parametric factor or parfactor [Poole, 2003] is\na triple hC, V, \u03c6 i where C is a set of inequality constraints\non parameters, V is a set of parametrized random variables\nand \u03c6 is a factor, which is a function from assignments of\nvalues to V to the non-negative reals. \u03c6 is used as the potential for all instances of the parfactor that are consistent\nwith the constraints.2 Milch et al. [2008] also explicitly include a set of parameters in their parfactors, but we do not.\n2 This is known as parameter sharing, but where the parameters are the parameters of the graphical model, not the individuals. Unfortunately, the logical and probabilistic literature often\nuses the same terminology for different things. Here we follow\n\n\fA parfactor means its grounding; the set of factors on V\u03b8\n(all with table \u03c6 ) for each grounding substitution \u03b8 of V\nthat obeys the constraints in C.\n2.1\n\nLifted Inference\n\nLifted variable elimination, such as in C-FOVE [Milch\net al., 2008], allows for inference to work at the lifted level\n(doing unification and splitting at runtime or as a preprocessing step) like normal variable elimination, until we remove a PRV that contains the only instance of a free parameter or is linked to a PRV with a different set of parameters.\nAt this stage, we need to take into account that the PRV\nrepresents a set of random variables.\nExample 3 Suppose we have a factor on S(x, y), R(x, y)\nand Q(x), as in Figure 1 (a). We can sum out all instances\nof S(x, y), as all of the factors are the same, and get a new\nfactor on R(x, y); this does n2 (identical) operations on the\ngrounding in a single step. If we then remove R, we are\nmultiplying a set of identical factors, and so can take their\nvalue to the power of the population size [Poole, 2003].\nSuppose, instead, we were to first sum out Q(x). In the\ngrounding, for each individual Ai , the random variables\nR(Ai , A1 ) . . . R(Ai , An ) are interdependent and so eliminating Q results in a factor on R(Ai , A1 ) . . . R(Ai , An ). The size\nof this factor is exponential in n.\nDe Salvo Braz et al. [2005] realized that the identity of the\nindividuals is not important; only the number of individuals having each value of R. They introduced counting\nto solve cases such as removing Q(x) first in polynomial\ntime rather than the exponential time (and space) used for\nthe ground case. They, however, do the counting and summing in one step, which limits its applicability. Milch et al.\n[2008] defined counting formulae that give a representation\nfor the resulting lifted formula and can then be combined\nwith other factors. This expanded the applicability of lifted\ninference, but it still requires grounding in some cases.\n2.2\n\nSearch-based probabilistic inference\n\nAn alternative to lifting variable elimination is to lift a\nsearch-based method. The classic search-based algorithm\nis recursive conditioning [Darwiche, 2001], a version of\nwhich is presented in Algorithm 1.3 This algorithm is presented in this non-traditional way, to emphasize the cases\nthat need to be implemented for lifting. In particular, dethe traditions as much as seems sensible, and apologize for any\nconfusions. In particular \"=\" is used between a (parametrized)\nrandom variable and its value, whereas \"6=\" and \"/\" are used for\nparameters (logical variables).\n3 Typically recursive conditioning requires computing a decomposition tree (D-Tree). Here we follow more the approach of\n[Bacchus et al., 2009] where we dynamically examine the problem for disconnected components as the search proceeds.\n\nAlgorithm 1: Recursive Conditioning: rc(Con, Fs)\ninput: Con: a set of variable = value assignments\nFs: set of factors\noutput: a number representing \u2211x \u220fF\u2208Fs F(x|Con)\nif vars(Con) 6\u2286 vars(Fs) then {Case 0}\nreturn rc({(x = V) \u2208 Con : x \u2208 vars(Fs)}, Fs)\nif \u2203v such that hhCon, Fsi, vi \u2208 cache then {Case 1}\nreturn v\nelse if \u2203f \u2208 Fs : vars(f ) \u2286 vars(Con) then {Case 2}\nF0 \u2190 {f \u2208 Fs : vars(f ) \u2286 vars(Con)}\n\u0001\nreturn \u220ff \u2208F0 eval(f , Con) \u00d7 rc(Con, Fs \\ F0 )\nelse if factor graph hCon, Fsi is disconnected then {Case\n3}\nreturn \u220fconnected component cc rc(Con, cc)\nelse {Case 4}\nselect variable x \u2208 vars(Fs) \\ vars(Con)\nsum \u2190 0\nfor each value v of x do\nsum \u2190 sum + rc({x = v} \u222a Con, Fs)\ncache \u2190 cache \u222a {hhCon, Fsi, sumi}\nreturn sum\n\ncoupling branching and the evaluation of factors is useful for developing its lifted counterpart. The correctness\ndoes not depend on the order of the cases (although efficiency does). In this algorithm Con is a context, a set of\nvariable = value assignments, and Fs is a set of input factors (this algorithm never creates or modifies factors; it only\nevaluate them when all of their variables are assigned). We\nseparate the context from the factors; typically these are\ncombined to give what could be called partially-assigned\nfactors.4\nIn case 0, if there are variables that appear in Con that\ndo not appear in Fs, these are removed from Con. This\nis called \"forgetting\" in the description below; we forget\nthe context that is not relevant for the rest of the factors.\nvars(S) is the set of variables that appear in S.\nIn case 1, the cache contains a set of previously computed\nvalues. If a value has already been computed it can be recalled. Initially the cache is {hh{}, {}i, 1i}.\nFor case 2, if all of the variables that appear in a factor f\nare assigned in Con, eval(F, Con) returns the number that\nis the value of F for the assignment Con. These numbers\nare multiplied.\nFor case 3, a factor graph5 on hCon, Fsi is a graph where\nthe nodes are factors in Fs, and there is an arc between\n4 For the lifted case, projecting the context onto the factors\nloses information that is needed; see Footnote 7. It also makes\nit conceptually clearer that the factors share the same context.\n5 This is related to a factor graph of Frey [2003] but we don't\nexplicitly model the variable nodes.\n\n\ffactors that share a random variable that isn't assigned in\nCon. The connected components refer to the nodes that are\nconnected in this graph. The connected components can be\nsolved separately, and their return values multiplied.\nCase 4 branches on a variable x that isn't assigned. The\nefficiency, but not the correctness, of the algorithm depends\non which variable x is selected to be branched on.\nTo compute P(x|Obs), for each value v of x, call rc({x =\nv} \u222a Obs, Fs) where Fs is the set of all factors of the model,\nand normalize the results.\nAn aspect that is important for lifted inference is that when\nvalues are assigned, the factors are simplified as they are\nnow functions of fewer variables. This should be contrasted\nto variable elimination that constructs more complicated\nfactors.\n\n3\n\nSearch-based Lifted Inference\n\nIn this section, we develop a lifted search-based algorithm.\nWe show its correctness with respect to a parallel ground\nalgorithm that uses the same order for splitting. Note that,\nbecause the lifted algorithm removes multiple variables at\nonce, this restricts the order the variables are split in the\nground algorithm. A legal ordering for the lifted inference\nbranches on all instances of a PRV at once, whereas the\ncorresponding ground algorithm branches on all instances\nsequentially. We show how the complexity (as a function\nof the population size) of the lifted algorithm is reduced\ncompared to the corresponding ground algorithm. Because\nwe want our algorithm to be correct for all legal branching orderings, we ignore the selection of which variable to\nbranch on; this can be optimized for efficiency.\nWe assume that we can count the number of solutions to a\nCSP with inequality constraints in time that is at most logarithmic in the domain size of the variables (e.g., by adapting\nthe #VE algorithm of Dechter [2003] to not enumerate the\nundistinguished variables).\n3.1\n\nIntermediate Representations\n\nThe lifted analogy of a context in Algorithm 1 is a counting context which represents counts of assignments to\nparametrized random variables.\nA counting context on V is a pair hV, \u03c7i, where V is a set\nof PRVs (all taking a single argument of the same type), all\nparametrized by the same parameter, and \u03c7 is a table mapping assignments of PRVs in V into non-negative integers.\nA counting context represents a context in the grounding.\nFor each individual of the type, the table \u03c7 specifies how\nmany of the individuals take on that tuple of values. We\ncan also treat a counting context in terms of \u03c7 as a set of\npairs of the assignment of values to V and the corresponding count.\n\nExample 4 A counting context for V = {R(x), T(x)}, and\n\uf8f1\nR(x) T(x) Value\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n20\n\uf8f2 True True\n\u03c7=\nTrue False\n40\n\uf8f4\n\uf8f4\n10\nFalse\nTrue\n\uf8f4\n\uf8f4\n\uf8f3\nFalse False\n20\nrepresents the assignment of values to R and T for 90 (20 +\n40 + 10 + 20) individuals. 20 of these have both R(. . . ) and\nT(. . . ) true, 40 have R(. . . ) true and T(. . . ) false, etc.\nThere is a separate counting context for each type. A current context is a set of pairs either of the form hVar, Valuei\nwhere Var is a PRV that has no parameters, or of the form\nh\u03c4, CCi where \u03c4 is a type and CC is a counting context\nwhere the parameter of each of the PRVs in CC is of type\n\u03c4. A current context can have at most one pair for each\ntype.\nA PRV P is assigned in a current context Con if it has no\nparameters and hP, Vali \u2208 Con, or if it is parametrized by\na variable of type \u03c4 and if h\u03c4, hV, \u03c7ii \u2208 CC and P unifies\nwith an element of V.\nA parfactor graph on hCon, Gi where Con is a current\ncontext and G is a set of parfactors, has the elements of G\nas nodes, and there is an arc between parfactors hC1 , V1 , \u03c61 i\nand hC2 , V2 , \u03c62 i if there is an element of V1 that isn't assigned in Con that unifies with an element of V2 such that\nthe unifier does not violate any of the constraints in either\nparfactor.\nThe grounding of a parfactor graph on hCon, Gi is a factor graph on hCon0 , G0 i, where for every counting parfactor\nhS, F, \u03b8 i in G, and for every grounding substitution \u03c8 of\nall of the free parameters that does not violate S, F\u03c8 is\nin G0 with table \u03b8 . Con0 represents all of the ground instances that are assigned in Con, with the corresponding\ncounts given by the table in Con.\nThe grounding of a parfactor graph defines its semantics.\nWe carry out lifted operations so that the lifted operations\nhave the same result as carrying out recursive conditioning\non the grounding of the parfactor graph for the same elimination ordering.\n3.2\n\nSymmetry and Exchangeability\n\nThe reason we can do lifted inference is because of symmetries. Having a symmetry between the unnamed individuals\nmeans that a derivation about some of the individuals can\nbe equally applied to any of the other individuals.\nWe say that a set of individuals are exchangeable in a parfactor graph if the grounding of the parfactor graph with\none consistent assignment of individuals to variables is isomorphic to the grounding of the graph with another assignment. Graph isomorphism means there is a 1-1 and onto\n\n\f<{},{P(x),Q(x,y)},\u03c61>\n<{},{P(w),R(w)},\u03c63>\n<{},{R(y),Q(x,y)},\u03c62>\n(a)\n<{},{P(x),Q(x,x)},\u03c61>\n\n<{x\u2021y},{P(x),Q(x,y)},\u03c61>\n\n<{},{P(w),R(w)},\u03c63>\n\n<{},{R(y),Q(y,y)},\u03c62>\n\n<{x\u2021y},{R(y),Q(x,y)},\u03c62>\n(b)\n\nFigure 2: A parfactor graph that is problematic for shattering and its preemptively shattered counterpart\nmapping between the nodes where the factors are identical.\nExchangeability means that reasoning with some of the individuals can be applied to the other individuals.\n3.3\n\nUnification, splitting and shattering\n\nIn order to determine which instances of parametrized random variables are the same random variables, Poole [2003]\nused unification and splitting on logical variables, which\nguarantees that the instances are identical or are disjoint.\nDe Salvo Braz et al. [2005] proposed to do all of the splitting up-front in an operation called shattering (see Kisynski\nand Poole [2009] for analysis of splitting, shattering and related operations).\nShattering is a local operation and does not imply graphs\nconstructed by substitutions are isomorphic as in the following example:\nExample 5 Consider the network of Figure 2 (a). Although it is shattered, the instances with x = y have a different grounding from the instances where x 6= y. This graph\ncan be split, on x = y in the right-hand parfactors, giving\nthe network shown in Figure 2 (b).\nAn alternative to the local shattering is to carry out a more\nglobal preemptive shattering. A set of parfactors preemptive shattered if\n\npreemptively shattered set of parfactors, all logical variables in a parfactor are split with respect to all explicitly\ngiven constants, and any pairs of logical variables in a parfactor are split with respect to each other.\nPreemptive shattering gives more splits than shattering, and\nsometimes more than needed, but it allows our proofs to\nwork and does not prevent the asymptotic complexity results we seek. With preemptive shattering, counting the\nnumber of instances represented by a parfactor is straightforward; there are no complex interactions. For the rest of\nthis paper, we assume that all parfactors are preemptively\nshattered.\nNote that, as can be seen in the parfactor graph of Figure\n2 (b), even after preemptive shattering, we cannot always\nglobally rename variables so that the unifying variables are\nidentical.\n3.4\n\nDisconnected Grounding\n\nWhen the graph is disconnected, Algorithm 1 considers the\nconnected components separately, and multiplies them. In\nthis section, we cover all of the cases where the grounding\nis disconnected, and show how it corresponds to operations\nin the lifted case.\nIf the lifted network is disconnected, the ground counterpart is disconnected, and so these disconnected components\ncan be solved separately and multiplied.\nIf the lifted network is connected, this does not imply that\nits grounding is connected. For example, the parfactor\ngraph of Figure 1 (c) is connected yet its grounding is not\nconnected.\nIntuitively, if there is a logical variable that is in all of\nthe counting parfactors, the instances for one individual\nare disconnected from the instances for another individual.\nThus, we can the solve the problem for one of the individuals, and the value for the lifted case is that value to the\npower of the number of individuals.\nThis intuition needs to be refined because logical variables\nare local to a parfactor; renaming the variables gives exactly the same grounding. There are cases where chains of\nunifications cause connectedness:\nExample 6 The parfactor graph\n<{x\u2021y},{S(x),R(x,y),Q(x,y)},\u03c61>\n\n\u2022 for every type, and every constant C of the type that\nis explicitly mentioned, every parfactor that contains\na variable x of the type includes the constraint x 6= C.\n\u2022 if variables x and y of the same type are in a parfactor,\nthe parfactor contains the constraint x 6= y.\nGiven a set of parfactors, to construct an equivalent set of\n\n<{x\u2021z},{S(x),R(z,x),S(x,z},\u03c62>\n\ndoes not have disconnected ground instances, even though\nx is in every PRV. For three individuals, C1 , C2 , C3 , in\nthe grounding R(C1 , C3 ) is connected to R(C2 , C3 ) through\nS(C3 ) in the grounding of the bottom parfactor, thus S(C1 )\n\n\fis connected to S(C2 ) for any different C1 and C2 , using the\ntop parfactor.\nThis reasoning can be applied generally:\nSuppose x is a logical variable in parfactor hC, V, \u03c6 i that appears in parfactor graph G. connected(x, hC, V, \u03c6 i, Con, G)\nmeans the instances of x in parfactor hC, V, \u03c6 i are connected to each other in the grounding of hCon, Gi.\nconnected can be defined recursively as follows.\nconnected(x, hC, V, \u03c6 i, Con, G) is true if and only if:\n\u2022 x appears in V, is not assigned in Con and there\nis a PRV in V that is not assigned in Con and not\nparametrized by x or\n\u2022 there is a parfactor hC0 , V 0 , \u03c6 0 i in G, such that an element of V unifies with an element of V 0 (in a manner\nconsistent with C and C0 , and that are not assigned\nin Con) and x is unified with a variable x0 such that\nconnected(x0 , hC0 , V 0 , \u03c6 0 i, Con, G) is true.\nThe definition of connected is sound:\nif\nconnected(x, hC, V, \u03c6 i, Con, G) is true, the instances\nof x are connected in the grounding. The proof for the\nsoundness is a straightforward induction proof; essentially\nthe algorithm is a constructive proof.\nHowever, the definition is not complete: there can be instances that are connected even though connected is false.\nIt is instructive to see what a proof for completeness would\nlook like. To prove completeness, we would prove that all\ninstances of x are disconnected if the above construction\nfails to derive they are connected. Suppose there are two\nconstants C and C0 , we need to show that the graph with\nx replaced by C is disconnected with the graph with x replaced by C0 . The graph with x replaced by C has C in\nevery PRV (by construction) and the graph with x replaced\nby C0 has C0 in every PRV. However, this does not imply\nthat the graphs are disconnected as there could be a PRV\nthat contains both C and C0 , as in the following example:\nExample 7 Consider the parfactor graph:\n<{x\u2021y},{S(x,y),R(x,y)},\u03c61>\n\n<{w\u2021z},{S(w,z),R(z,w)},\u03c62>\n\nIn the grounding, for all individuals Ci 6= Cj , the random\nvariable S(Ci , Cj ) is connected to S(Cj , Ci ). However, it is\ndisconnected from other instances of S(x, y).\nWe use the definition of connected to detect potentially\ndisconnected components, and we can explicitly check for\nwhich instances are connected. In this way, we can ensure\nthat the lifted algorithm detects disconnectedness whenever\n\nthe ground algorithm would. The only counterexamples to\nthe completeness of connected are when there is a set of\nvariables, all with the same domain, and all of them appear in all PRVs in the parfactor graph (possibly renamed),\nand there is an inequality constraint between them. Suppose there are k such variables, x1 . . . , xk , in a parfactor.\nWe choose k constants6 , C1 , . . . , Ck , and apply the substitution {x1 /C1 , . . . , xk /Ck } to that parfactor, and then proceed\nto ground out all the corresponding variables in the other\nfactors by unifying with the factors in all ways, forming a\ngeneric connected component. We then need to count the\nnumber of copies of each PRV instance in the connected\ncomponent; suppose this is c. For a population size of\nn, there are n(n \u2212 1) . . . (n \u2212 k + 1) instances of the PRV,\nand there are c elements in each connected component,\ntherefore there are n(n \u2212 1) . . . (n \u2212 k + 1)/c disconnected\ncomponents. If we compute p as the probability of the\ngeneric connected component, we need to take p to the\npower n!/((n \u2212 k)! \u00d7 c) to compute the probability of the\nlifted network.\nIn the above analysis, n is the population size and k and c\ndepend only on the structure of the graph, and not on the\npopulation size. As we assume that we can count the population size in time logarithmic in the population size, the\nabove procedure is polynomial in the log of the population size, whereas grounding is polynomial in the population size. As we see below, this is the only case where the\ngrounding is polynomial in the population size.\nIn Example 7, k = 2 and c = 2, and so the power is n(n \u2212\n1)/2. In an example with k = 3, it is possible that c could\nbe 1,2,3 or 6.\nAlgorithm 2 gives the lifted variant of case 3 of Algorithm\n1. The main loop is the same as Algorithm 1, with the\nrecursive call lrc(Con, Fs) where Con is a current context\nand Fs is a set of counting parfactors.\n3.5\n\nCounting\n\nOnce we have a single connected component (and there is\nno logical variable for which its instances are connected),\nwe select a PRV to branch on. In describing this, as in Algorithm 1, we decouple branching on a variable, and evaluating parfactors. Typically a PRV is associated with many\nparfactors, and when we branch on a PRV we need to count\nthe number of instances with various values for the PRV.\nWe need to make sure that we branch in a way that enables\nus to evaluate the relavant parfactors.\nFor the simplest case, assume we want to sum out a\nBoolean PRV F(x) that has one free logical variable, x, with\n6 These\n\nshould be constants that don't otherwise appear in the\ncurrent set of counting parfactors. We need to choose the constants so that the same constants are used in different branches, to\nensure that caching finds identical instances whenever the ground\ninstances are found in the cache.\n\n\fAlgorithm 2: Lifted search, case 3: grounding is disconnected.\nif Fs is disconnected then {Case 3a}\nreturn \u220fconnected component cc lrc(Con, cc)\nelse if \u2203x \u00acconnected(x, F, Con, Fs) for F \u2208 Fs then\n{Case 3b}\nSelect one x such that \u00acconnected(x, F, Con, Fs)\nlet \u03c4 be the type of x\nlet C = {x of type \u03c4 :\n\u00acconnected(x, F, Con, Fs)} for some F \u2208 Fs\nlet k = |C|\nreplace x1 , . . . , xk with C1 , . . . , Ck in F\nunify F with all other factors in Fs\nlet c = |{instances of F in Fs}|\nlet e = n!/((n \u2212 k)! \u00d7 c)\nreturn lrc(Con, Fs)e\n\ndomain {A1 , . . . , An }. The idea behind counting [de Salvo\nBraz et al., 2007] is that only the number of exchangeable\nindividuals that have a PRV having a particular value matters, not their identity. We present counting first considering this simple case, then more complex cases.\nIn counting branching, for each i, such that 0 \u2264 i \u2264 n, the\nalgorithm generate the branch where there are i instances of\nF true, and\n\u0001 so n \u2212 i instances of F false. This branch represents ni paths in the grounding, as there are this many\nrenamings of constants that would result in the same assignment. Thus\n\u0001 it can multiply the result of evaluating this\nbranch by ni . Note that the counting branching involves\ngenerating n + 1 branches, whereas in the grounding there\nare 2n assignments of values after the equivalent ground\nbranching. The resulting counting context records the number of instances of F that are true and number that are false.\nWe now show how to evaluate various cases of parfactors\nthat can include F. The general case is a combination of\nthese specific cases. For all of the example below, assume\nthey are part of a larger parametrized graphical model. In\nparticular, assume that the instances are connected, so that\nthe case described in the previous section does not apply.\nExample 8 Consider the parfactor\nh{}, {F(x), E}, \u03c61 i\nSuppose |x| = n. This parfactor represents n factors. Suppose \u03c61 is:\nF(x) E\n\u03c61\nTrue True \u03b11\nTrue False \u03b12\nFalse True \u03b13\nFalse False \u03b14\nSuppose we have split on E and assigned it the value\nTrue,and then we split on F(x) and are in the branch with\n\nF = True for i cases and F = False for n \u2212 i cases. This is\nrepresented by the current context:\nF(x) \u03c61\nE = true, True i\nFalse n \u2212 i\nThe contribution of this parfactor in this current context is\n\u03b11i \u03b13n\u2212i .\nExample 9 Consider the parfactor\nh{}, {F(x), G(y)}, \u03c62 i\nSuppose x and y are of different types, where |x| = n, |y| =\nm. \u03c62 is:\nF(x) G(y) \u03c62\nTrue True \u03b11\nTrue False \u03b12\nFalse True \u03b13\nFalse False \u03b14\nThis parfactor represents nm factors; for each combination\nof assignments of values to the instances of F and G, there\nis a factor. In a current context with i F's true and h G's\ntrue, this parfactor has a contribution:\ni(m\u2212h)\n\n\u03b11ih \u03b12\n\n(n\u2212i)h\n\n\u03b13\n\n(n\u2212i)(m\u2212h)\n\n\u03b14\n\n.\n\nExample 10 Consider the parfactor\nh{}, {F(x), G(x)}, \u03c62 i\nSuppose |x| = n. This parfactor represents n factors. Unlike the previous cases, counting branching is not adequate;\nwe need to consider which F-assignments go with which\nG-assignments. We can do a counting branch on F first:\nfor each i \u2208 [0, n], consider the case where F is true for i\nindividuals,\n\u0001 and is false for n \u2212 i individuals. This case represents ni branches in the grounding. To split on G we can\ndo a dependent branch: consider the i individuals for which\nF is true, and the n \u2212 i individuals for which F is false separately. For each j \u2208 [0, i], we consider the branch where G is\ntrue for j individuals and false for i \u2212 j individuals\nall with\n\u0001\nF = True; this branch corresponds to ij ground branches.\nFor each k \u2208 [0, n \u2212 i] we construct the branch where G is\ntrue for k individuals and is false for\u0001 i \u2212 j \u2212 k individuals\nwith F = False. This represents n\u2212i\nground cases. This\nk\nbranch is represented by the counting context:\nF(x) G(x)\nTrue True\nTrue False\nFalse True\nFalse False\n\n\u03c62\nj\ni\u2212j\nk\nn\u2212i\u2212k\n\nThe contribution of the parfactor in this branch is:\n\u03b11j \u03b12i\u2212j \u03b13k \u03b14n\u2212i\u2212k\n\n\fExample 11 Consider the parfactor\nh{x 6= y}, {F(x), G(y)}, \u03c62 i\nSuppose x and y are of the same type, where |x| = |y| =\nn. This parfactor represents n(n \u2212 1) factors. This can be\nsolved by a mix of the previous two examples. If we were\nto do the same as Example 9, we would also include the\ncases where x = y, which are explicitly excluded; but these\nare the cases in Example 10. So the contribution of this\nfactor can be computed by dividing the result of Example\n9 by the result of Example 10, or equivalently subtracting\nthe exponents. As in Example 10, we consider the case\nwhere F is true for i individuals, and for these individuals\nG is true for j of them, and out of the individuals where\nF is false, G is true for k of them. Taking the difference\nbetween the exponents Example 9 and 10, and noticing that\nh in Example 9 corresponds to j + k in Example 10, the\ncontribution of these factors is:\ni(j+k)\u2212j\n\n\u03b11\n\ni(n\u2212j\u2212k)\u2212i+j\n\n\u03b12\n\n(n\u2212i)(j+k)\u2212k\n\n\u03b13\n\n(n\u2212i)(n\u2212j\u2212k)\u2212n+i+k\n\n\u03b14\n\nExample 12 Consider a mix between the previous examples. Suppose we have the parfactors:\n\n\u0001\n\u0001\ncontexts in the grounding. Note that there are\nto ij n\u2212i\nk\ni(n \u2212 i) leaves that are decedents of the current context created in Example 8, whereas in the grounding there are 2n\nleaves that are descendants of each corresponding ground\ncontext.\nBranching is shown as Case 4 in Algorithm 3. In this algorithm Con is a current context and Fs is a set of input\nparfactors. Case 4a is the same as case 4 in Algorithm 1\n(but for Boolean variables). Case 4b is for branching on a\nPRV with a single parameter, and sets up dependent counting branching that is presented in Algorithm 4. Note that\nthis treats a counting context as a set of pairs of an assignment of values to a set of PRVs and a count (as described\nin Section 3.1).\nThe branching factor depends on the population, but the\ndepth of the recursive calls depends on the structure of the\ncounting context, and not on the population size. The depth\nof the recursive calls provides the power of the polynomial. If we use a sparse representation of current contexts\nwith zeros suppressed, this is never worse than grounding.\n[However, whether we use a sparse or dense representation\nis something that can be optimized for.]\n\nh{. . . }, {F(x), G(x), . . . }i\nAlgorithm 3: Lifted Recursive Conditioning: lrc(Con, Fs)\nh{x 6= y, . . . }, {H(y), G(x), . . . }i\nwhere all of the variables are of the same type with n individuals. Suppose the branching order is to branch on H,\nthen F, then G. The split on G needs to depend on both\nH and F. This can be done if the splits on H and F are\ndependent; that is, we do a separate count on F for the individuals for which H are true and the individuals for which\nH are false. Then we do a separate count7 on G for the set\nof individuals for each combination of values to H and F.\nCounting branching needs to be expanded to cascaded\ncounting branching. Dependent counting branching on a\nPRV X that is parametrized by a parameter of type \u03c4, works\nas follows. First, we find the corresponding counting context hV, \u03c7i for \u03c4. Dependent Counting branching replaces\nthis with a counting context on hV \u222a {X}, \u03c7 0 i as follows.\nFor each assignment ht, ii in the table \u03c7 (i is the count for\nassignment t), for each j in [0, i], we create the table \u03c7 0 that\nmaps t \u222a {X = true} to i and\u0001t \u222a {X = false} to i \u2212 j. This\nassignment corresponds to ij , different grounding assign\u0001\nments, so the grounding needs to be multiplied by ij . This\nis recursively carried out for each tuple.\nExample 13 Starting from the current context of Example 8, dependent counting branching on G(x) produces the\ncounting context of Example 10. This context corresponds\n7 Note\n\nthat if we had projected the counts onto the separate\nfactors, we would have lost the interdependence between F and\nH, which is needed as the count for G depends on both.\n\ninput: Con: current context\nFs: set of parfactors\noutput: a number representing \u2211x \u220fF\u2208grounding(Fs,Con) F(x)\nif \u2203x \u2208 vars(Con) \\ vars(Fs) then {Case 0}\nreturn lrc(\u2211X Con, Fs)\nif \u2203v such that hhCon, Fsi, vi \u2208 cache then {Case 1}\nreturn v\nelse if \u2203f \u2208 Fs : vars(f ) \u2286 vars(Con) then {Case 2}\nreturn eval parfactor(f , Con) \u00d7 lrc(Con, Fs \\ {f })\nelse {Case 3}\nSee Algorithm 2\nelse {Case 4}\nselect PRV X \u2208 vars(Fs) \\ vars(Con)\nif X contains no parameters then {Case 4a}\nsum \u2190 lrc({X = true} \u222a Con, Fs) + lrc({X =\nfalse} \u222a Con, Fs)\nelse {Case 4b}\nsuppose the parameter of X is of type \u03c4\nif \u2203\u03c7 : h\u03c4, \u03c7i \u2208 Con then\nsum \u2190 branch(\u03c7, X, {}, Con \\ {h\u03c4, \u03c7i}, Fs)\nelse\nsum \u2190 branch({hhi, |\u03c4|i}, X, {}, Con, Fs)\ncache \u2190 cache \u222a {hhCon, Fsi, sumi}\nreturn sum\nThe main remaining part of the lifted algorithm is to evaluate a parfactor hC, V, \u03c6 i in a counting context hV 0 , \u03c7i,\nwhere all of the variables in V are assigned in V 0 . There\nare three cases: shared parameters, different parameters of\n\n\fAlgorithm 4: Dependent Counting Branching:\nbranch(\u03c7, X, \u03c7 0 , Con, Fs)\n\nAlgorithm 5: Evaluating a parfactor in current context:\neval parfactor(PF, Con)\n\ninput: \u03c7: a set of tuples from a counting context\nX: the PRV to branch on its instances\n\u03c7 0 : the new counting context being constructed\nCon: the current context to be added to\nFs: the set of all factors\nif \u03c7 = {} then\nSuppose \u03c4 is the type of the parameter in V\nreturn lrc(Con \u222a {h\u03c4, \u03c7 0 i}, Fs)\nelse\nselect ht, ii \u2208 \u03c7\nsum \u2190 0\nfor j \u2208 [0, i] do\nlet \u03c7 00 be\n\u03c7 0 \u222a ht \u222a {X = true}, ji \u222a ht \u222a {X = false}, i \u2212 ji\n\u0001\nsum \u2190 sum + ij branch(\u03c7 \\ {ht, ii}, \u03c7 00 , Con, Fs)\nreturn sum\n\ninput: PF: a parfactor\nCon: a current context\nSuppose PF is hC, V, \u03c6 i\nSuppose Con is hV 0 , \u03c7i\nprod \u2190 1\nforeach ht, pi \u2208 \u03c6 do\nif t is consistent with variable assignments in Con\nthen\npower \u2190 1\nLet \u03c4 be the type of X\nSelect h\u03c4, hV 00 , \u03c7ii \u2208 Con\nRedundantVars \u2190 vars(V 00 ) \\ vars(V)\npower \u2190 power \u00d7 \u2211ht,ii\u2208\u03c7:consistent(t,V) i\nprod \u2190 prod \u00d7 ppower\nreturn prod\n\nthe same type and parameters of different types. One parfactor can contain all of these.\nFor shared parameters, as in Example 10, the parfactor provides the base, and there is a unique counting context that\nprovides the powers. First we group all of these together\nand raise them to the appropriate powers, and then treat\nthem as a block.\nFor parameters of different types, as in Example 9, we need\nto multiply the powers. We can treat the shared parameters\nas a block.\nFor different parameters of the same type, as in Example\n11, we can use the other two cases: first we treat them as\ndifferent types (which over-counts because it includes the\nequality cases), and then divide by the case when they are\nequal. We also have to readjust for double counting, which\ncan be done using the coefficient of n!/(n \u2212 k)! where k\nis the number of such cases. For example, when k = 3,\nthis is n(n \u2212 1)(n \u2212 2) = n3 \u2212 3n2 + 2n. The first of these\ncorresponds to all parameters being different, the second to\nall pairs of parameters equal, and the third to all parameter\nthe same.\nAlgorithm 5 shows how to evaluate a parfactor in a current\ncontext. It omits the last case, as it is computed from the\nother two cases.\nExample 12 (cont.) Consider the branch where H is true\nfor i individuals and false for n \u2212 i individuals. Suppose we\nthen branch on F. We then consider the branch with F true\nfor j0 of the cases where H is false and j1 cases where H\nis true. We thus have: j1 individuals for which F and H\nare true; i \u2212 j1 individuals which have H true and F false;\nj0 individuals that have H false and F true; and n \u2212 i \u2212 j0\n\nindividuals what have both H and F false. We can then\nbranch on G, for each of the four sets of individuals. We\nthus know the counts of each case; Algorithm 5 computes\nthe contribution of each factor.\nThe final two cases of the algorithm are caching (case 1\nof Algorithm 1) and forgetting (case 0 of Algorithm 1).\nCaching can remain the same, we just have to ensure that\nthe cache can find elements that are the same up to renaming of variables, which can be done easily as the current\ncontext does not depend on the name and the variable and\ncan be stored in a canonical way (e.g., alphabetically). Forgetting is the inverse of splitting. A variable in a counting\ncontext that doesn't appear in the parfactors can be summed\nout of the counting context (which is the same operation as\nsumming out a variable in variable elimination). \u2211X Con in\nAlgorithm 3 means to sum out X from the counting context it appears in or to remove it if it is not a parametrized\nvariable.\nThis description assumed binary-valued variables, and only\nfunctors with 0 or 1 arguments. The first of these is straightforward to generalize, and the second is not.\nConsider what happens when F can have more than two\nvalues. Suppose F is a unary m-valued PRV with range\n{v1 , . . . , vm }. That is, F(Ai ) is a random variable with domain {v1 , . . . , vm }. The assignments we need to consider\nare when there are non-negative integers i1 . . . im where im\nrepresents the number of individuals that have value i. Thus\nfor each assignment to i1 . . . im , where ij \u2265 0 for each j and\ni1 + * * * + im = n, we consider the assignment\nF(Ai ) = v1 for 0 < i \u2264 i1\nF(Ai ) = v2 for i1 < i \u2264 i1 + i2\n...\nF(Ai ) = vm for i1 + i2 + * * * + im\u22121 < i \u2264 n\n\n\fIt is a straightforward combinatorial exercise to include this\nin the algorithm (but complicates the description).\n\n4\n\nConclusion\n\nLifted probabilistic reasoning has proved to be challenging.\nThere have been many proposals to lift various algorithms,\nhowever all of the exact algorithms needed to ground out\na population in some cases (and it is often difficult to tell\nfor which cases they need to ground a population). We set\nout to determine if there was some fundamental reason why\nwe would need to ground out the representation, or whether\nthere was some case where we needed to effectively ground\nout. We believe that we have answered this for two cases:\n\u2022 When lifted inference is polynomial in a population,\nwhich occurs when VE does not create a factor that\nis parametrized by a population or search can be disconnected for a population, we can solve it in time\npolynomial in the logarithm of the population.\n\u2022 For parametrized random variables with zero or a single argument, and search-based inference (and so also\nvariable elimination, due to their equivalent complexity) is exponential when grounding, we answer arbitrary conditional queries in time polynomial in the\npopulation.\nThe question of whether we can always do lifted inference\nin polynomial time in each population size when there are\nPRVs with more than one argument, is still an open problem. While we can use the algorithm in this paper for many\nof these cases, there are some very tricky cases. Hopefully\nthe results in this paper will provide tools to fully solve this\nproblem.\nWe have chosen to not give empirical comparisons of our\nresults. These are much more comparisons of the low-level\nengineering than of the lifted algorithm. There are no published algorithms that can correctly solve all of the examples in this paper in a fully lifted form.\n\nReferences\nBacchus, F., Dalmao, S., and Pitassi, T. (2009). Solving\n#SAT and Bayesian inference with backtracking search.\nJ. Artif. Intell. Res. (JAIR), 34: 391\u2013442. URL http:\n//dx.doi.org/10.1613/jair.2648.\nDarwiche, A. (2001). Recursive conditioning. Artificial\nIntelligence, 126(1-2): 5\u201341.\nde Salvo Braz, R., Amir, E., and Roth, D. (2005). Lifted\nfirst-order probabilistic inference. In IJCAI-05. Edinburgh. URL http://www.cs.uiuc.edu/~eyal/\npapers/fopl-res-ijcai05.pdf.\n\nde Salvo Braz, R., Amir, E., and Roth, D. (2007). Lifted\nfirst-order probabilistic inference. In L. Getoor and\nB. Taskar (Eds.), Introduction to Statistical Relational\nLearning. M.I.T. Press. URL http://www.cs.uiuc.\nedu/~eyal/papers/BrazRothAmir_SRL07.pdf.\nDechter, R. (2003). Constraint Processing. Morgan Kaufmann.\nFrey, B.J. (2003).\nExtending factor graphs so as\nto unify directed and undirected graphical models.\nIn Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence, pp. 257\u2013264. Morgan\nKaufmann. URL http://www.psi.toronto.edu/\npublications/2003/dfg-uai03.pdf.\nGogate, V. and Domingos, P. (2010). Exploiting logical\nstructure in lifted probabilistic inference. In AAAI 2010\nWorkshop on Statististical and Relational Artificial Intelligence (STAR-AI). URL http://aaai.org/ocs/\nindex.php/WS/AAAIW10/paper/view/2049.\nJha, A., Gogate, V., Meliou, A., and Suciu, D. (2010).\nLifted inference from the other side: The tractable features. In Twenty-Fourth Annual Conference on Neural\nInformation Processing Systems (NIPS).\nKersting, K., Ahmadi, B., and Natarajan, S. (2009). Counting belief propagation. In J.B. A. Ng (Ed.), Proceedings\nof the 25th Conference on Uncertainty in Artificial Intelligence (UAI\u201309). Montreal, Canada.\nKisynski, J. and Poole, D. (2009). Constraint processing\nin lifted probabilistic inference. In Proc. 25th Conference on Uncertainty in AI, (UAI-2009), pp. 293\u2013302.\nMontreal, Quebec. URL http://www.cs.ubc.ca/\n~poole/papers/KisynskiUAI2009.pdf.\nMilch, B., Zettlemoyer, L.S., Kersting, K., Haimes, M.,\nand Kaelbling, L.P. (2008). Lifted probabilistic inference with counting formulas. In Proceedings of\nthe Twenty Third Conference on Artificial Intelligence\n(AAAI). URL http://people.csail.mit.edu/lpk/\npapers/mzkhk-aaai08.pdf.\nPoole, D. (2003). First-order probabilistic inference. In\nProc. Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), pp. 985\u2013991. Acapulco,\nMexico.\nSingla, P. and Domingos, P. (2008). Lifted first-order belief\npropagation. In Proceedings of the Twenty-Third AAAI\nConference on Artificial Intelligence, pp. 1094\u20131099.\nZhang, N.L. and Poole, D. (1994). A simple approach to\nBayesian network computations. In Proc. 10th Canadian\nConference on AI, pp. 171\u2013178.\n\n\f"}