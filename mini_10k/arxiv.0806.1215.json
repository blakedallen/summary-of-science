{"id": "http://arxiv.org/abs/0806.1215v3", "guidislink": true, "updated": "2010-05-28T18:13:54Z", "updated_parsed": [2010, 5, 28, 18, 13, 54, 4, 148, 0], "published": "2008-06-06T18:32:08Z", "published_parsed": [2008, 6, 6, 18, 32, 8, 4, 158, 0], "title": "Performance of LDPC Codes Under Faulty Iterative Decoding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0806.4020%2C0806.0669%2C0806.2136%2C0806.4820%2C0806.0268%2C0806.2640%2C0806.1222%2C0806.2512%2C0806.4847%2C0806.4905%2C0806.2501%2C0806.3088%2C0806.3434%2C0806.3473%2C0806.4030%2C0806.2284%2C0806.1434%2C0806.3673%2C0806.3162%2C0806.2222%2C0806.3102%2C0806.0784%2C0806.4307%2C0806.0681%2C0806.0751%2C0806.3179%2C0806.4239%2C0806.0441%2C0806.4691%2C0806.4208%2C0806.1331%2C0806.3243%2C0806.1829%2C0806.0939%2C0806.0217%2C0806.4518%2C0806.1034%2C0806.0878%2C0806.2119%2C0806.3359%2C0806.1471%2C0806.4644%2C0806.0594%2C0806.1464%2C0806.3563%2C0806.4001%2C0806.1637%2C0806.1364%2C0806.0877%2C0806.4383%2C0806.0206%2C0806.1742%2C0806.4450%2C0806.0428%2C0806.0626%2C0806.0014%2C0806.4151%2C0806.4075%2C0806.1604%2C0806.2668%2C0806.1505%2C0806.0039%2C0806.2375%2C0806.1960%2C0806.1862%2C0806.2800%2C0806.4977%2C0806.4243%2C0806.2063%2C0806.3598%2C0806.1427%2C0806.0374%2C0806.2438%2C0806.4792%2C0806.2473%2C0806.3458%2C0806.0389%2C0806.3087%2C0806.4184%2C0806.1102%2C0806.2885%2C0806.4679%2C0806.4336%2C0806.0258%2C0806.2836%2C0806.1720%2C0806.1217%2C0806.2840%2C0806.3302%2C0806.1215%2C0806.0237%2C0806.2423%2C0806.1279%2C0806.0778%2C0806.3485%2C0806.1110%2C0806.2869%2C0806.2986%2C0806.0763%2C0806.3725%2C0806.1236&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Performance of LDPC Codes Under Faulty Iterative Decoding"}, "summary": "Departing from traditional communication theory where decoding algorithms are\nassumed to perform without error, a system where noise perturbs both\ncomputational devices and communication channels is considered here. This paper\nstudies limits in processing noisy signals with noisy circuits by investigating\nthe effect of noise on standard iterative decoders for low-density parity-check\ncodes. Concentration of decoding performance around its average is shown to\nhold when noise is introduced into message-passing and local computation.\nDensity evolution equations for simple faulty iterative decoders are derived.\nIn one model, computing nonlinear estimation thresholds shows that performance\ndegrades smoothly as decoder noise increases, but arbitrarily small probability\nof error is not achievable. Probability of error may be driven to zero in\nanother system model; the decoding threshold again decreases smoothly with\ndecoder noise. As an application of the methods developed, an achievability\nresult for reliable memory systems constructed from unreliable components is\nprovided.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0806.4020%2C0806.0669%2C0806.2136%2C0806.4820%2C0806.0268%2C0806.2640%2C0806.1222%2C0806.2512%2C0806.4847%2C0806.4905%2C0806.2501%2C0806.3088%2C0806.3434%2C0806.3473%2C0806.4030%2C0806.2284%2C0806.1434%2C0806.3673%2C0806.3162%2C0806.2222%2C0806.3102%2C0806.0784%2C0806.4307%2C0806.0681%2C0806.0751%2C0806.3179%2C0806.4239%2C0806.0441%2C0806.4691%2C0806.4208%2C0806.1331%2C0806.3243%2C0806.1829%2C0806.0939%2C0806.0217%2C0806.4518%2C0806.1034%2C0806.0878%2C0806.2119%2C0806.3359%2C0806.1471%2C0806.4644%2C0806.0594%2C0806.1464%2C0806.3563%2C0806.4001%2C0806.1637%2C0806.1364%2C0806.0877%2C0806.4383%2C0806.0206%2C0806.1742%2C0806.4450%2C0806.0428%2C0806.0626%2C0806.0014%2C0806.4151%2C0806.4075%2C0806.1604%2C0806.2668%2C0806.1505%2C0806.0039%2C0806.2375%2C0806.1960%2C0806.1862%2C0806.2800%2C0806.4977%2C0806.4243%2C0806.2063%2C0806.3598%2C0806.1427%2C0806.0374%2C0806.2438%2C0806.4792%2C0806.2473%2C0806.3458%2C0806.0389%2C0806.3087%2C0806.4184%2C0806.1102%2C0806.2885%2C0806.4679%2C0806.4336%2C0806.0258%2C0806.2836%2C0806.1720%2C0806.1217%2C0806.2840%2C0806.3302%2C0806.1215%2C0806.0237%2C0806.2423%2C0806.1279%2C0806.0778%2C0806.3485%2C0806.1110%2C0806.2869%2C0806.2986%2C0806.0763%2C0806.3725%2C0806.1236&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Departing from traditional communication theory where decoding algorithms are\nassumed to perform without error, a system where noise perturbs both\ncomputational devices and communication channels is considered here. This paper\nstudies limits in processing noisy signals with noisy circuits by investigating\nthe effect of noise on standard iterative decoders for low-density parity-check\ncodes. Concentration of decoding performance around its average is shown to\nhold when noise is introduced into message-passing and local computation.\nDensity evolution equations for simple faulty iterative decoders are derived.\nIn one model, computing nonlinear estimation thresholds shows that performance\ndegrades smoothly as decoder noise increases, but arbitrarily small probability\nof error is not achievable. Probability of error may be driven to zero in\nanother system model; the decoding threshold again decreases smoothly with\ndecoder noise. As an application of the methods developed, an achievability\nresult for reliable memory systems constructed from unreliable components is\nprovided."}, "authors": ["Lav R. Varshney"], "author_detail": {"name": "Lav R. Varshney"}, "author": "Lav R. Varshney", "arxiv_comment": "Revised in May 2010 in response to reviewer comments", "links": [{"href": "http://arxiv.org/abs/0806.1215v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0806.1215v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0806.1215v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0806.1215v3", "journal_reference": null, "doi": null, "fulltext": "1\n\nPerformance of LDPC Codes Under\nFaulty Iterative Decoding\nLav R. Varshney\n\narXiv:0806.1215v3 [cs.IT] 28 May 2010\n\nAbstract\nDeparting from traditional communication theory where decoding algorithms are assumed to perform without\nerror, a system where noise perturbs both computational devices and communication channels is considered here.\nThis paper studies limits in processing noisy signals with noisy circuits by investigating the effect of noise on\nstandard iterative decoders for low-density parity-check codes. Concentration of decoding performance around its\naverage is shown to hold when noise is introduced into message-passing and local computation. Density evolution\nequations for simple faulty iterative decoders are derived. In one model, computing nonlinear estimation thresholds\nshows that performance degrades smoothly as decoder noise increases, but arbitrarily small probability of error is\nnot achievable. Probability of error may be driven to zero in another system model; the decoding threshold again\ndecreases smoothly with decoder noise. As an application of the methods developed, an achievability result for\nreliable memory systems constructed from unreliable components is provided.\nIndex Terms\nLow-density parity-check codes, communication system fault tolerance, density evolution, decoding, memories\n\nI. I NTRODUCTION\nThe basic goal in channel coding is to design encoder-decoder pairs that allow reliable communication over noisy\nchannels at information rates close to capacity [1]. The primary obstacle in the quest for practical capacity-achieving\ncodes has been decoding complexity [2]\u2013[4]. Low-density parity-check (LDPC) codes have, however, emerged as\na class of codes that have performance at or near the Shannon limit [5], [6] and yet are sufficiently structured as\nto have decoders with circuit implementations [7]\u2013[9].\nIn addition to decoder complexity, decoder reliability may also limit practical channel coding.1 In Shannon's\nschematic diagram of a general communication system [1, Fig. 1] and in the traditional information and communication theories that have developed within the confines of that diagram, noise is localized in the communication\nchannel. The decoder is assumed to operate without error. Given the possibility of unreliable computation on faulty\nhardware, there is value in studying error-prone decoding. In fact Hamming's original development of parity-check\ncodes was motivated by applications in computing rather than in communication [11].\nThe goal of this paper is to investigate limits of communication systems with noisy decoders and has dual\nmotivations. The first is the eminently practical motivation of determining how well error control codes work when\ndecoders are faulty. The second is the deeper motivation of determining fundamental limits for processing unreliable\nsignals with unreliable computational devices, illustrated schematically in Fig. 1. The motivations are intertwined.\nAs noted by Pierce, \"The down-to-earth problem of making a computer work, in fact, becomes tangled with this\ndifficult philosophical problem: 'What is possible and what is impossible when unreliable circuits are used to\nprocess unreliable information?'\" [12].\nA first step in understanding these issues is to analyze a particular class of codes and decoding techniques: iterative\nmessage-passing decoding algorithms for LDPC codes. When the code is represented as a factor graph, algorithm\nManuscript prepared May 2008; revised April 2009, May 2010. This work was supported in part by a National Science Foundation\nGraduate Research Fellowship and was performed in part when the author was with l'\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne. The\nmaterial in this paper was presented in part at the Information Theory Workshop, Lake Tahoe, CA, September 2007.\nL. R. Varshney is with the Department of Electrical Engineering and Computer Science, the Laboratory for Information and Decision\nSystems, and the Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, MA, 02139 USA (e-mail:\nlrv@mit.edu).\n1\nOne may also consider the effect of encoder complexity [10], however encoder noise need not be explicitly considered, since it may be\nincorporated into channel noise, using the noise combining argument suggested by Fig. 3.\n\n\f2\nINFORMATION\nSOURCE\n\nTRANSMITTER\n\nRECEIVER\n\nSIGNAL\nMESSAGE\n\nMESSAGE\n\nNOISE\nSOURCE\n\nFig. 1.\n\nDESTINATION\n\nRECEIVED\nSIGNAL\n\nNOISE\nSOURCE\n\nSchematic diagram of an information system that processes unreliable signals with unreliable circuits.\n\ncomputations occur at nodes and algorithm communication is carried out over edges. Correspondence between the\nfactor graph and the algorithm is not only a tool for exposition but also the way decoders are implemented [7]\u2013[9].\nIn traditional performance analysis, the decoders are assumed to work without error. In this paper, there will be\ntransient local computation and message-passing errors, whether the decoder is analog or digital.\nWhen the decoder itself is noisy, one might believe that achieving arbitrarily small probability of error (Shannon\nreliability) is not possible, but this is indeed possible for certain sets of noisy channels and noisy decoders. This is\nshown by example. For other sets of noisy channels and noisy decoders, Shannon reliability is not achievable, but\nerror probability tending to extremely small values is achievable. Small probability of error, \u03b7 , is often satisfactory in\npractice, and so \u03b7 -reliable performance is also investigated. Decoding thresholds at \u03b7 -reliability decrease smoothly\nwith increasing decoder noise. Communication systems may display graceful degradation with respect to noise\nlevels in the decoder.\nThe remainder of the paper is organized as follows. Section II reviews motivations and related work. Section III\nformalizes notation and Section IV gives concentration results that allow the density evolution method of analysis,\ngeneralizing results in [13]. A noisy version of the Gallager A decoder for processing the output of a binary\nsymmetric channel is analyzed in Section V, where it is shown that Shannon reliability is unattainable. In Section VI,\na noisy decoder for AWGN channels is analyzed. For this model, the probability of error may be driven to zero\nand the decoding threshold degrades smoothly as a function of decoder noise. As an application of the results of\nSection V, Section VII precisely characterizes the information storage capacity of a memory built from unreliable\ncomponents. Section VIII provides some conclusions.\nII. BACKGROUND\nA. Practical Motivations\nAlthough always present [11], [14], recent technological trends in digital circuit design bring practical motivations\nto the fore [15]\u2013[17]. The 2008 update of the International Technology Roadmap for Semiconductors (ITRS)2 points\nout that for complementary metal-oxide-silicon (CMOS) technology, increasing power densities, decreasing supply\nvoltages, and decreasing sizes have increased sensitivity to cosmic radiation, electromagnetic interference, and\nthermal fluctuations. The ITRS further says that an ongoing shift in the manufacturing paradigm will dramatically\nreduce costs but will lead to more transient failures of signals, logic values, devices, and interconnects. Device\ntechnologies beyond CMOS, such as single-electron tunnelling technology [18], carbon-based nanoelectronics [19],\nand chemically assembled electronic nanocomputers [20], are also projected to enter production, but they all display\nerratic, random device behavior [21], [22].\nAnalog computations are always subject to noise [23], [24]. Similar issues arise when performing real-valued\ncomputations on digital computers since quantization, whether fixed-point or floating-point, is often well-modeled\nas bounded, additive stochastic noise [25].\nB. Coding and Computing\nInformation and communication theory have provided limits for processing unreliable signals with reliable circuits\n[1], [13], [26], whereas fault-tolerant computing theory has provided limits for processing reliable signals (inputs)\nwith unreliable circuits [12], [27]\u2013[31]. This work brings the two together.\n2\nThe overall objective of the ITRS is to present the consensus of the semiconductor industry on the best current estimate of research and\ndevelopment needs for the next fifteen years.\n\n\f3\n\nA brief overview of terms and concepts from fault-tolerant computing, based on [32], [33], is now provided.\nA fault is a physical defect, imperfection, or flaw that occurs within some hardware or software component. An\nerror is the informational manifestation of a fault. A permanent fault exists indefinitely until corrective action\nis taken, whereas a transient fault appears and disappears in a short period of time. Noisy circuits in which the\ninterconnection pattern of components are trees are called formulas [34], [35].\nIn an error model, the effects of faults are given directly in the informational universe. For example, the basic\nvon Neumann model of noisy circuits [27] models transient faults in logic gates and wires as message and node\ncomputation noise that is both spatially and temporally independent; this has more recently also been called the\nHegde-Shanbhag model [36], after [37]. This error model is used here. Error models of permanent faults [38],\n[39] or of miswired circuit interconnection [28], [40] have been considered elsewhere. Such permanent errors in\ndecoding circuits may be interpreted as either changing the factor graph used for decoding or as introducing new\npotentials into the factor graph; the code used by the encoder and the code used by the decoder are different.\nThere are several design philosophies to combat faults. Fault avoidance seeks to make physical components more\nreliable. Fault masking seeks to prevent faults from introducing errors. Fault tolerance is the ability of a system to\ncontinue performing its function in the presence of faults. This paper is primarily concerned with fault tolerance,\nbut Section VII considers fault masking.\nC. Related Work\nEmpirical characterizations of message-passing decoders have demonstrated that probability of error performance\ndoes not change much when messages are quantized at high resolution [26]. Even algorithms that are coarsely\nquantized versions of optimal belief propagation show little degradation in performance [13], [41]\u2013[46]. It should be\nemphasized, however, that fault-free, quantized decoders differ significantly from decoders that make random errors.3\nThe difference is similar to that between control systems with finite-capacity noiseless channels and control systems\nwith noisy channels of equal capacity [50]. Seemingly the only previous work on message-passing algorithms with\nrandom errors is [51], which deals with problems in distributed inference.4\nThe information theoretic problem of mismatch capacity [52] and its analog for iterative decoding [53] deal\nwith scenarios where an incorrect decoding metric is used. This may arise, e.g., due to incorrect estimation of the\nchannel noise power. For message-passing decoding algorithms, mismatch leads to incorrect parameters for local\ncomputations. These are permanent faults rather than the kind of transient faults considered in this paper.\nNoisy LDPC decoders were previously analyzed in the context of designing reliable memories from unreliable\ncomponents [54], [55] (revisited in Section VII), using Gallager's original methods [26]. Several LPDC code analysis\ntools have since been developed, including simulation [56], expander graph arguments [57], [58], EXIT charts\n[59], [60], and density evolution [13], [61], [62]. This work generalizes asymptotic characterizations developed by\nRichardson and Urbanke for noiseless decoders [13], showing that density evolution is applicable to faulty decoders.\nExpander graph arguments have also been extended to the case of noisy decoding in a paper [63] that appeared\nconcurrently with the first presentation of this work [64]. Note that previous works have not even considered the\npossibility that Shannon reliability is achievable with noisy decoding.\nIII. C ODES , D ECODERS ,\n\nAND\n\nP ERFORMANCE\n\nThis section establishes the basic notation of LDPC channel codes and message-passing decoders for communication systems depicted in Fig. 1. It primarily follows established notation in the field [13], [65], and will therefore\nbe brief. Many of the notational conventions are depicted schematically in Fig. 2 using a factor graph-based decoder\nimplementation.\nConsider the standard ensemble of (dv , dc )-regular LDPC codes of length n, C n (dv , dc ), defined by a uniform\nmeasure on the set of labeled bipartite factor graphs with variable node degree dv and check node degree dc .5 There\n3\nRandomized algorithms [47] and stochastic computation [48] (used for decoding in [49]) make use of randomness to increase functionality,\nbut the randomness is deployed in a controlled manner.\n4\nIf the graphical model of the code and the graph of noisy communication links in a distributed system coincide, then the distributed\ninference problem and the message-passing decoding problem can be made to coincide.\n5\nA factor graph determines an \"ordered code,\" but the opposite is not true [66]. Moreover, since codes are unordered objects, several\n\"ordered codes\" are in fact the same code.\n\n\f4\n\nuv\nx1\n\npy|x\n\ny1\n\nuc\n\n\u03a8\n\nuv\nx2\n\npy|x\n\ny2\n\n\u03a6\nuc\n\n\u03a8\n\nuv\nxi\n\npy|x\n\nyi\n\n\u03a6\n\nuv\nxn\n\npy|x\n\nyn\n\nuc\n\n\u03a8\n\u03a6\n\u03a8\n\nFig. 2. Schematic diagram of a factor graph-based implementation of a noisy decoder circuit. Only one variable-to-check message and one\ncheck-to-variable message are highlighted. Other wires, shown in gray, will also carry noisy messages.\n\nare n variable nodes corresponding to the codeword letters and ndv /dc check nodes corresponding to the parity\ncheck constraints. The design rate of the code is 1 \u2212 dv /dc , though the actual rate might be higher since not all\nchecks may be independent; the true rate converges to the design rate for large n [65, Lemma 3.22]. One may also\nconsider irregular codes, C n (\u03bb, \u03c1) characterized by the degree distribution pair (\u03bb, \u03c1). Generating functions\nthe\nP\u221e of i\u22121\nvariable node\nand\ncheck\nnode\ndegree\ndistributions,\n\u03bb(\u03b6)\nand\n\u03c1(\u03b6)\n,\nare\nfunctions\nof\nthe\nform\n\u03bb(\u03b6)\n=\n\u03bb\n\u03b6\ni\ni=2\nP\ni\u22121 , where \u03bb and \u03c1 specify the fraction of edges that connect to nodes with degree i. The\nand \u03c1(\u03b6) = \u221e\n\u03b6\ni\ni\ni\ni=2 \u03c1\nR1\nR1\ndesign rate is 1 \u2212 0 \u03c1(\u03b6)d\u03b6/ 0 \u03bb(\u03b6)d\u03b6 .\nIn the communication system of Fig. 1, a codeword is selected by the transmitter and is sent through the noisy\nchannel. Channel input and output letters are denoted X \u2208 X and Y \u2208 Y . Since binary linear codes are used,\nX can be taken as {\u00b11}. The receiver contains a noisy message-passing decoder, which is used to process the\nchannel output codeword to produce an estimate of X that is denoted X\u0302 . The goal of the receiver is to recover the\nchannel input codeword with low probability of error. Throughout this work, probability of bit error Pe is used as\nthe performance criterion;6\nPe = Pr[X 6= X\u0302].\nThe message-passing decoder works in iterative stages and the iteration time is indexed by l = 0, 1, . . . . Within\nthe decoder, at time l = 0, each variable node has a realization of Y , yi . A message-passing decoder exchanges\nmessages between nodes along wires. First each variable node sends a message to a neighboring check node over\na noisy messaging wire. Generically, sent messages are denoted as \u03bdv\u2192c , message wire noise realizations as wv\u2192c ,\nand received messages as \u03bcv\u2192c : assume without loss of generality that \u03bdv\u2192c , wv\u2192c , and \u03bcv\u2192c are drawn from a\ncommon messaging alphabet M.\nEach check node processes received messages and sends back a message to each neighboring variable node\nover a noisy message wire. The noisiness of the check node processing is generically denoted by an input random\nvariable Uc \u2208 U . The check node computation is denoted \u03a6(l) : Mdc \u22121 \u00d7 U 7\u2192 M. The notations \u03bdc\u2192v , \u03bcc\u2192v ,\nand wc\u2192v are used for signaling from check node to variable node; again without loss of generality assume that\n\u03bdc\u2192v , wc\u2192v , \u03bcc\u2192v \u2208 M.\nEach variable node now processes its yi and the messages it receives to produce new messages. The new messages\nare produced through possibly noisy processing, where the noise input is generically denoted Uv \u2208 U . The variable\nnode computation is denoted \u03a8(l) : Y \u00d7 Mdv \u22121 \u00d7 U 7\u2192 M. Local computations and message-passing continue\niteratively.\n6\nAn alternative would be to consider block error probability, however an exact evaluation of this quantity is difficult due to the dependence\nbetween different symbols of a codeword, even if the bit error probability is the same for all symbols in the codeword [67].\n\n\f5\n\nMessage passing induces decoding neighborhoods, which involve nodes/wires that have communicated with one\nanother. For a given node \u1e45, its neighborhood of depth d is the induced subgraph consisting of all nodes reached\nand edges traversed by paths of length at most d starting from \u1e45 (including \u1e45) and is denoted N\u1e45d . The directed\nd , is defined as the induced subgraph containing all\nneighborhood of depth d of a wire v \u2192 c, denoted by Nv\u2192c\nwires and nodes on paths starting from the same place as v \u2192 c but different from v \u2192 c. Equivalently for a\nd\nwire c \u2192 v, Nc\u2192v\nis the induced subgraph containing all wires and nodes on paths starting from the same place\nas c \u2192 v but different from c \u2192 v. If the induced subgraph (corresponding to a neighborhood) is a tree then the\nneighborhood is tree-like, otherwise it is not tree-like. The neighborhood is tree-like if and only if all involved\nnodes are distinct.\nNote that only extrinsic information is used in node computations. Also note that in the sequel, all decoder noises\n(Uc , Uv , Wv\u2192c , and Wc\u2192v ) will be assumed to be independent of each other, as in the von Neumann error model\nof faulty computing.\nA communication system is judged by information rate, error probability, and blocklength. For fixed channels,\ninformation theory specifies the limits of these three parameters when optimizing over the unconstrained choice of\ncodes and decoders; Shannon reliability is achievable for rates below capacity in the limit of increasing blocklength.\nWhen decoders are restricted to be noisy, tighter information theoretic limits are not known. Therefore comparing\nperformance of systems with noisy decoders to systems using identical codes but noiseless decoders is more\nappropriate than comparing to Shannon limits.\nCoding theory follows from information theory by restricting decoding complexity; analysis of noisy decoders\nfollows from coding theory by restricting decoding reliability.\nIV. D ENSITY E VOLUTION C ONCENTRATION R ESULTS\nConsidering the great successes achieved by analyzing the noiseless decoder performance of ensembles of codes\n[13], [61], [65] rather than of particular codes [26], the same approach is pursued for noisy decoders. The first\nmathematical contribution of this work is to extend the method of analysis promulgated in [13] to the case of\ndecoders with random noise.\nSeveral facts that simplify performance analysis are proven. First, under certain symmetry conditions with wide\napplicability, the probability of error does not depend on which codeword is transmitted. Second, the individual\nperformances of codes in an ensemble are, with high probability, the same as the average performance of the\nensemble. Finally, this average behavior converges to the behavior of a code defined on a cycle-free graph.\nPerformance analysis then reduces to determining average performance on an infinite tree: a noisy formula is\nanalyzed in place of general noisy circuits.\nFor brevity, only regular LDPC codes are considered in this section, however the results can be generalized to\nirregular LDPC codes. In particular, replacing node degrees by maximum node degrees, the proofs stand mutatis\nmutandis. Similarly, only binary LDPC codes are considered; generalizations to non-binary alphabets also follow,\nas in [68].\nA. Restriction to All-One Codeword\nIf certain symmetry conditions are satisfied by the system, then the probability of error is conditionally independent of the codeword that is transmitted. It is assumed throughout this section that messages in the decoder are in\nbelief format.\nDefinition 1: A message in an iterative message-passing decoder for a binary code is said to be in belief format if\nthe sign of the message indicates the bit estimate and the magnitude of the message is an increasing function of the\nconfidence level. In particular, a positive-valued message indicates belief that a bit is +1 whereas a negative-valued\nmessage indicates belief that a bit is \u22121. A message of magnitude 0 indicates complete uncertainty whereas a\nmessage of infinite magnitude indicates complete confidence in a bit value.\nNote, however, that it is not obvious that this is the best format for noisy message-passing [65, Appendix B.1].\nThe symmetry conditions can be restated for messages in other formats.\nThe several symmetry conditions are:\nDefinition 2 (Channel Symmetry): A memoryless channel is binary-input output-symmetric if it satisfies\np(Yt = y|Xt = 1) = p(Yt = \u2212y|Xt = \u22121)\n\n\f6\n\nfor all channel usage times t = 1, . . . , n.\nDefinition 3 (Check Node Symmetry): A check node message map is symmetric if it satisfies\n!\ndc\nY\n(l)\n(l)\nbi\n\u03a6 (b1 \u03bc1 , . . . , bdc \u22121 \u03bcdc \u22121 , bdc u) = \u03a6 (\u03bc1 , . . . , \u03bcdc \u22121 , u)\ni=1\n\nfor any \u00b11 sequence (b1 , . . . , bdc ). That is to say, the signs of the messages and the noise factor out of the map.\nDefinition 4 (Variable Node Symmetry): A variable node message map is symmetric if it satisfies\n\u03a8(0) (\u2212\u03bc0 , \u2212u) = \u2212\u03a8(0) (\u03bc0 , u)\n\nand\n\u03a8(l) (\u2212\u03bc0 , \u2212\u03bc1 , . . . , \u2212\u03bcdv \u22121 , \u2212u) = \u2212\u03a8(l) (\u03bc0 , \u03bc1 , . . . , \u03bcdv \u22121 , u),\n\nfor l \u2265 1. That is to say, the initial message from the variable node only depends on the received value and internal\nnoise and there is sign inversion invariance for all messages.\nDefinition 5 (Message Wire Symmetry): Consider any message wire to be a mapping \u039e : M \u00d7 M \u2192 M. Then\na message wire is symmetric if\n\u03bc = \u039e(\u03bd, w) = \u2212\u039e(\u2212\u03bd, \u2212w),\nwhere \u03bc is any message received at a node when the message sent from the opposite node is \u03bd and w is message\nwire noise with distribution symmetric about 0.\nAn example where the message wire symmetry condition holds is if the message wire noise w is additive and\nsymmetric about 0. Then \u03bc = \u03bd + w = \u2212(\u2212\u03bd \u2212 w) and w is symmetric about 0.\nTheorem 1 (Conditional Independence of Error): For a given binary linear code and a given noisy message(l)\npassing algorithm, let Pe (x) denote the conditional probability of error after the lth decoding iteration, assuming\nthat codeword x was sent. If the channel and the decoder satisfy the symmetry conditions given in Definitions 2\u20135,\n(l)\nthen Pe (x) does not depend on x.\nProof: Modification of [13, Lemma 1] or [65, Lemma 4.92]. Appendix A gives details.\nSuppose a system meets these symmetry conditions. Since probability of error is independent of the transmitted\ncodeword and since all LDPC codes have the all-one codeword in the codebook, one may assume without loss\nof generality that this codeword is sent. Doing so removes the randomness associated with transmitted codeword\nselection.\nB. Concentration around Ensemble Average\nThe next simplification follows by seeing that the average performance of the ensemble of codes rather than\nthe performance of a particular code may be studied, since all codes in the ensemble perform similarly. The\nperformances of almost all LDPC codes closely match the average performance of the ensemble from which they\nare drawn. The average is over the instance of the code, the realization of the channel noise, and the realizations\nof the two forms of decoder noise. To simplify things, assume that the number of decoder iterations is fixed at\nsome finite l. Let Z be the number of incorrect values held among all dv n variable node-incident edges at the end\nof the lth iteration (for a particular code, channel noise realization, and decoder noise realization) and let E [Z] be\nthe expected value of Z . By constructing a martingale through sequentially revealing all of the random elements\nand then using the Hoeffding-Azuma inequality, it can be shown that:\nTheorem 2 (Concentration Around Expected Value): There exists a positive constant \u03b2 = \u03b2(dv , dc , l) such that\nfor any \u01eb > 0,\n2\nPr [|Z \u2212 E [Z] | > ndv \u01eb/2] \u2264 2e\u2212\u03b2\u01eb n .\nProof: Follows the basic ideas of the proofs of [13, Theorem 2] or [65, Theorem 4.94]. Appendix B gives\ndetails.\nA primary communication system performance criterion is probability of error Pe ; if the number of incorrect\nvalues Z concentrates, then so does Pe .\n\n\f7\n\nC. Convergence to the Cycle-Free Case\nThe previous theorem showed that the noisy decoding algorithm behaves essentially deterministically for large n.\nAs now shown, this ensemble average performance converges to the performance of an associated tree ensemble,\nwhich will allow the assumption of independent messages.\nFor a given edge whose directed neighborhood of depth 2l is tree-like, let p be the expected number of incorrect\nmessages received along this edge (after message noise) at the lth iteration, averaged over all graphs, inputs and\ndecoder noise realizations of both types.\nTheorem 3 (Convergence to Cycle-Free Case): There exists a positive constant \u03b3 = \u03b3(dv , dc , l) such that for\nany \u01eb > 0 and n > 2\u03b3/\u01eb,\n|E [Z] \u2212 ndv p| < ndv \u01eb/2.\nThe proof is identical to the proof of [13, Theorem 2]. The basic idea is that the computation tree created by\nunwrapping the code graph to a particular depth [69] almost surely has no repeated nodes.\nThe concentration and convergence results directly imply concentration around the average performance of a tree\nensemble:\nTheorem 4 (Concentration Around Cycle-Free Case): There exist positive constants \u03b2 = \u03b2(dv , dc , l) and \u03b3 =\n\u03b3(dv , dc , l) such that for any \u01eb > 0 and n > 2\u03b3/\u01eb,\n2\n\nPr [|Z \u2212 ndv p| > ndv \u01eb] \u2264 2e\u2212\u03b2\u01eb n .\n\nProof: Follows directly from Theorems 2 and 3.\nD. Density Evolution\nWith the conditional independence and concentration results, all randomness is removed from explicit consideration and all messages are independent. The problem reduces to density evolution, the analysis of a discrete-time\ndynamical system [62]. The dynamical system state variable of most interest is the probability of bit error, Pe .\n(l)\nDenote the probability of bit error of a code g \u2208 C n after l iterations of decoding by Pe (g, \u03b5, \u03b1), where \u03b5 is a\nchannel noise parameter (such as noise power or crossover probability) and \u03b1 is a decoder noise parameter (such\nas logic gate error probability). Then density evolution computes\nh\ni\nlim E Pe(l) (g, \u03b5, \u03b1) ,\nn\u2192\u221e\n\nwhere the expectation is over the choice of the code and the various noise realizations. The main interest is in\nthe long-term behavior of the probability of error after performing many iterations. The long-term behavior of a\ngeneric dynamical system may be a limit cycle or a chaotic attractor, however density evolution usually converges\nto a stable fixed point. Monotonicity (either increasing or decreasing) with respect to iteration number l need not\nhold, but it often does. If there is a stable fixed point, the limiting performance corresponds to\nh\ni\n\u03b7 \u2217 = lim lim E Pe(l) (g, \u03b5, \u03b1) .\nl\u2192\u221e n\u2192\u221e\n\nIn channel coding, certain sets of parameters (g, \u03b5, \u03b1) lead to \"good\" performance, in the sense of small \u03b7 \u2217 ,\nwhereas other sets of parameters lead to \"bad\" performance with large \u03b7 \u2217 . The goal of density evolution analysis\nis to determine the boundary between these good and bad sets.\nThough it is natural to expect the performance of an algorithm to improve as the quality of its input improves\nand as more resources are allocated to it, this may not be so. For many decoders, however, there is a monotonicity\nproperty that limiting behavior \u03b7 \u2217 improves as channel noise \u03b5 decreases and as decoder noise \u03b1 decreases. Moreover,\njust as in other nonlinear estimation systems for dimensionality-expanding signals [70]\u2013[72], there is a threshold\nphenomenon such that the limiting probability of error may change precipitously with the values of \u03b5 and \u03b1.\nIn traditional coding theory, there is no parameter \u03b1, and the goal is often to determine the range of \u03b5 for which\n\u03b7 \u2217 is zero. The boundary is often called the decoding threshold and may be denoted \u03b5\u2217 (\u03b7 \u2217 = 0). A decoding\nthreshold for optimal codes under optimal decoding may be computed from the rate of the code g and the capacity\nof the channel as a function of \u03b5, C(\u03b5). Since this Shannon limit threshold is for optimal codes and decoders, it\n\n\f8\nNOISY COMPUTATION\n\nNOISELESS\nCOMPUTATION\n\nCOMPUTATION\nNOISE\n\nCOMMUNICATION\nNOISE\n\n\u2248\n\nNOISY COMMUNICATION\n\nNOISELESS\nCOMPUTATION\n\nFig. 3.\n\nCOMPUTATION\nNOISE\n\nCOMMUNICATION\nNOISE\n\nLocal computation noise may be incorporated into message-passing noise without essential loss of generality.\n\nis clearly an upper bound to \u03b5\u2217 (0) for any given code and decoder. If the target error probability \u03b7 \u2217 is non-zero,\nC(\u03b5)\n7\nthen the Shannon limit threshold is derived from the so-called \u03b7 \u2217 -capacity, 1\u2212h\n\u2217 , rather than C(\u03b5).\n2 (\u03b7 )\nIn the case of faulty decoders, the Shannon limits also provide upper bounds on the \u03b5-boundary for the set of\n(\u03b5, \u03b1) that achieve good performance. One might hope for a Shannon theoretic characterization of the entire (\u03b5, \u03b1)boundary, but as noted previously, such results are not extant. Alternately, in the next sections, sets of (\u03b5, \u03b1) that\ncan achieve \u03b7 \u2217 -reliability for particular LDPC codes g \u2208 C n are characterized using the density evolution method\ndeveloped in this section.\nV. E XAMPLE : N OISY G ALLAGER A D ECODER\nSection IV showed that density evolution equations determine the performance of almost all codes in the large\nblocklength regime. Here the density evolution equation for a simple noisy message-passing decoder, a noisy version\nof Gallager's decoding algorithm A [26], [74], is derived. The algorithm has message alphabet M = {\u00b11}, with\nmessages in belief format simply indicating the estimated sign of a bit. Although this simple decoding algorithm\ncannot match the performance of belief propagation due to its restricted messaging alphabet M, it is of interest\nsince it is of extremely low complexity and can be analyzed analytically [74].\nConsider decoding the LDPC-coded output of a binary symmetric channel (BSC) with crossover probability \u03b5.\nAt a check node, the outgoing message along edge ~e is the product of all incoming messages excluding the one\nincoming on ~e, i.e. the check node map \u03a6 is the XOR operation. At a variable node, the outgoing message is the\noriginal received code symbol unless all incoming messages give the opposite conclusion. That is,\n(\n\u2212y , if \u03bc1 = * * * = \u03bcdv \u22121 = \u2212y ,\n\u03a8=\ny,\notherwise.\nThere is no essential loss of generality by combining computation noise and message-passing noise into a single\nform of noise, as demonstrated schematically in Fig. 3 and proven in [75, Lemma 3.1]. This noise combining is\nperformed in the sequel to reduce the number of decoder noise parameters and allow a clean examination of the\ncentral phenomenon. Thus, each message in the Gallager algorithm A is passed over an independent and identical\nBSC wire with crossover probability \u03b1.\nThe density evolution equation leads to an analytic characterization of the set of (\u03b5, \u03b1) pairs, which parameterize\nthe noisiness of the communication system.\nA. Density Evolution Equation\nThe density evolution equation is developed for general irregular LDPC ensembles. The state variable of density\nevolution, sl , is taken to be the expected probability of bit error at the variable nodes in the large blocklength limit,\n(l)\ndenoted here as Pe (\u03b5, \u03b1).\nThe original received message is in error with probability \u03b5, thus\nPe(0) (\u03b5, \u03b1) = s0 = \u03b5.\n\nThe initial variable-to-check message is in error with probability (1 \u2212 \u03b5)\u03b1 + \u03b5(1 \u2212 \u03b1), since it is passed through a\n(l)\n(i)\nBSC(\u03b1). For further iterations, l, the probability of error, Pe (\u03b5, \u03b1), is found by induction. Assume Pe (\u03b5, \u03b1) = si\n7\nThe function h2 (*) is the binary entropy function. The \u03b7 \u2217 -capacity expression is obtained by adjusting capacity by the rate-distortion\nfunction of an equiprobable binary source under frequency of error constraint \u03b7 \u2217 , R(\u03b7 \u2217 ) = 1 \u2212 h2 (\u03b7 \u2217 ) [73].\n\n\f9\n\nfor 0 \u2264 i \u2264 l. Now consider the error probability of a check-to-variable message in the (l + 1)th iteration. A\ncheck-to-variable message emitted by a check node of degree dc along a particular edge is the product of all the\n(dc \u2212 1) incoming messages along all other edges. By assumption, each such message is in error with probability\nsl and all messages are independent. These messages are passed through BSC(\u03b1) before being received, so the\nprobability of being received in error is\nsl (1 \u2212 \u03b1) + (1 \u2212 sl )\u03b1 = \u03b1 + sl \u2212 2\u03b1sl .\n\nDue to the XOR operation, the outgoing message will be in error if an odd number of these received messages\nare in error. The probability of this event, averaged over the degree distribution, yields the probability\n1 \u2212 \u03c1 [1 \u2212 2(\u03b1 + sl \u2212 2\u03b1sl )]\n.\n2\n(l+1)\n\nNow consider Pe\n(\u03b5, \u03b1), the error probability at the variable node in the (l + 1)th iteration. Consider an edge\nwhich is connected to a variable node of degree dv . The outgoing variable-to-check message along this edge is in\nerror in the (l + 1)th iteration if the original received value is in error and not all incoming messages are received\ncorrectly or if the originally received value is correct but all incoming messages are in error. The first event has\nprobability\n!\n\u0014\n\u0012\n\u0013\n\u0012\n\u0013\u0015\n1 \u2212 \u03c1 [1 \u2212 2(\u03b1 + sl \u2212 2\u03b1sl )]\n1 + \u03c1 [1 \u2212 2(\u03b1 + sl \u2212 2\u03b1sl )] dv \u22121\n.\n\u03b5 1 \u2212 1 \u2212 (1 \u2212 \u03b1)\n\u2212\u03b1\n2\n2\nThe second event has probability\n!\n\u0014\n\u0012\n\u0013\n\u0012\n\u0013\u0015\n1 \u2212 \u03c1 [1 \u2212 2(\u03b1 + sl \u2212 2\u03b1sl )]\n1 + \u03c1 [1 \u2212 2(\u03b1 + sl \u2212 2\u03b1sl )] dv \u22121\n(1 \u2212 \u03b5) (1 \u2212 \u03b1)\n.\n+\u03b1\n2\n2\nAveraging over the degree distribution and adding the two terms together yields the density evolution equation\nin recursive form:\nsl+1 = \u03b5 \u2212 \u03b5q\u03b1+ (sl ) + (1 \u2212 \u03b5)q\u03b1\u2212 (sl ).\n\n(1)\n\nThe expressions\nq\u03b1+ (\u0161)\n\nq\u03b1\u2212 (\u0161)\n\n\u0015\n1 + \u03c1(\u03c9\u03b1 (\u0161)) \u2212 2\u03b1\u03c1(\u03c9\u03b1 (\u0161))\n,\n=\u03bb\n2\n\u0014\n\n\u0014\n\n\u0015\n1 \u2212 \u03c1(\u03c9\u03b1 (\u0161)) + 2\u03b1\u03c1(\u03c9\u03b1 (\u0161))\n=\u03bb\n,\n2\n\nand \u03c9\u03b1 (\u0161) = (2\u03b1 \u2212 1)(2\u0161 \u2212 1) are used to define the density evolution recursion.\nB. Performance Evaluation\nWith the density evolution equation established, the performance of the coding-decoding system with particular\nvalues of quality parameters \u03b5 and \u03b1 may be determined. Taking the bit error probability as the state variable,\nstable fixed points of the deterministic, discrete-time, dynamical system are to be found. Usually one would want\nthe probability of error to converge to zero, but since this might not be possible, a weaker performance criterion\nmay be needed. To start, consider partially noiseless cases.\n1) Noisy Channel, Noiseless Decoder: For the noiseless decoder case, i.e. \u03b1 = 0, it has been known that there\nare thresholds on \u03b5, below which the probability of error goes to zero as l increases, and above which the probability\nof error goes to some large value. These can be found analytically for the Gallager A algorithm [74].\n\n\f10\n\n2) Noiseless Channel, Noisy Decoder: For the noisy Gallager A system under consideration, the probability of\nerror does not go to zero as l goes to infinity for any \u03b1 > 0. This can be seen by considering the case of the\nperfect original channel, \u03b5 = 0, and any \u03b1 > 0. The density evolution equation reduces to\nsl+1 = q\u03b1\u2212 (sl ),\n\n(2)\n\nwith s0 = 0. The recursion does not have a fixed point at zero, and since error probability is bounded below by\nzero, it must increase. The derivative is\n\u0014\n\u0015\n\u2202 \u2212\n\u2032 1 \u2212 \u03c1(\u03c9\u03b1 (s)) + 2\u03b1\u03c1(\u03c9\u03b1 (s))\nq (s) = \u03bb\n\u03c1\u2032 (\u03c9\u03b1 (s))(2\u03b1 \u2212 1)2 ,\n\u2202s \u03b1\n2\n\nwhich is greater than zero for 0 \u2264 s \u2264 12 and 0 \u2264 \u03b1 \u2264 12 ; thus the error evolution forms a monotonically increasing\nsequence. Since the sequence is monotone increasing starting from zero, and there is no fixed point at zero, it\nfollows that this converges to the smallest real solution of s = q\u03b1\u2212 (s) since the fixed point cannot be jumped due\nto monotonicity.\n3) Noisy Channel, Noisy Decoder: The same phenomenon must also happen if the starting s0 is positive, however\nthe value to which the density evolution converges is a non-zero fixed point solution of the original equation (1),\nnot of (2), and is a function of both \u03b1 and \u03b5. Intuitively, for somewhat large initial values of \u03b5, the noisy decoder\ndecreases the probability of error in the first few iterations, just like the noiseless one, but when the error probability\nbecomes close to the internal decoder error, the probability of error settles at that level. This is summarized in the\nfollowing proposition.\nProposition 1: Final error probability \u03b7 \u2217 > 0 for any LDPC ensemble decoded using the noisy Gallager A\nsystem defined in Section V, for every decoder noise level \u03b1 > 0 and every channel noise level \u03b5. \u0003\nThe fact that probability of error cannot asymptotically be driven to zero with the noisy Gallager decoder is\nexpected yet is seemingly displeasing. In a practical scenario, however, the ability to drive Pe to a very small\nnumber is also desirable. As such, a performance objective of achieving Pe less than \u03b7 is defined and the worst\nchannel (ordered by \u03b5) for which a decoder with noise level \u03b1 can achieve that objective is determined. The channel\nparameter\n\u03b5\u2217 (\u03b7, \u03b1) = sup{\u03b5 \u2208 [0, 21 ] | lim Pe(l) (g, \u03b5, \u03b1) < \u03b7}\nl\u2192\u221e\n\nis called the threshold. For a large interval of \u03b7 values, there is a single threshold value below which \u03b7 -reliable\ncommunication is possible and above which it is not. Alternatively, one can determine the probability of error to\n(l)\nwhich a system with particular \u03b1 and \u03b5 can be driven, \u03b7 \u2217 (\u03b1, \u03b5) = liml\u2192\u221e Pe , and see whether this value is small.\nIn order to find the threshold in the case of \u03b1 > 0 and \u03b5 > 0, the real fixed point solutions of density evolution\nrecursion (1) need to be found. The real solutions of the polynomial equation in s,\n\u03b5 \u2212 \u03b5q\u03b1+ (s) + (1 \u2212 \u03b5)q\u03b1\u2212 (s) \u2212 s = 0\n\nare denoted 0 < r1 (\u03b1, \u03b5) \u2264 r2 (\u03b1, \u03b5) \u2264 r3 (\u03b1, \u03b5) \u2264 * * * .8 The final probability of error \u03b7 \u2217 is determined by the ri ,\nsince these are fixed points of the recursion (1).\nThe real solutions of the polynomial equation in s,\ns \u2212 q\u03b1\u2212 (s)\n\u2212 s = 0,\n1 \u2212 q\u03b1+ (s) \u2212 q\u03b1\u2212 (s)\n\n(3)\n\nare denoted 0 < \u03c41 (\u03b1) \u2264 \u03c42 (\u03b1) \u2264 * * * .8 The threshold \u03b5\u2217 as well as the region in the \u03b1 \u2212 \u03b5 plane where the decoder\nimproves performance over no decoding are determined by the \u03c4i , since (3) is obtained by solving recursion (1) for\n\u03b5 and setting equal to zero. For particular ensembles of LDPC codes, these values can be computed analytically.\nFor these particular ensembles, it can be determined whether the fixed points are stable or unstable. Moreover,\nvarious monotonicity results can be established to show that fixed points cannot be jumped.\nAnalytical expressions for the ri (\u03b1, \u03b5) and \u03c4i (\u03b1) are determined for the (3,6) regular LDPC code by solving the\nappropriate polynomial equations and numerical evaluations of the ri expressions are shown as thin lines in Fig. 4\nas functions of \u03b5 for fixed \u03b1. The point where r1 (\u03b1, \u03b5) = \u03b5 is \u03c41 (\u03b1) and the point where r2 (\u03b1, \u03b5) = \u03b5 is \u03c42 (\u03b1). In\nFig. 4, these are points where the thin lines cross.\n8\n\nThe number of real solutions can be determined through Descartes' rule of signs or a similar tool [76].\n\n\f11\n\n0.25\n\nr3(\u03b1,\u03b5)\n\n0.2\n\n\u03b7*\n\n0.15\n\n0.1\nr2(\u03b1,\u03b5)\n0.05\n\u03b5\nr1(\u03b1,\u03b5)\n0\n\n0\n\n0.005\n\n0.01\n\n0.015\n\n\u03b5\n\n0.02\n\n0.025\n\n0.03\n\nFig. 4. Thick line shows final error probability, \u03b7 \u2217 , after decoding a C \u221e (3, 6) code with the noisy Gallager A algorithm, \u03b1 = 0.005. This\nis determined by the fixed points of density evolution, ri (\u03b1, \u03b5), shown with thin lines.\n\nBy analyzing the dynamical system equation (1) for the (3,6) code in detail, it can be shown that r1 (\u03b1, \u03b5) and\nr3 (\u03b1, \u03b5) are stable fixed points of density evolution. Contrarily, r2 (\u03b1, \u03b5) is an unstable fixed point, which determines\nthe boundary between the regions of attraction for the two stable fixed points. Since r1 (\u03b1, \u03b5) and r3 (\u03b1, \u03b5) are stable\nfixed points, the final error probability \u03b7 \u2217 will take on one of these two values, depending on the starting point of\nthe recursion, \u03b5. The thick line in Fig. 4 shows the final error probability \u03b7 \u2217 as a function of initial error probability\n\u03b5. One may note that \u03b7 \u2217 = r1 is the desirable small error probability, whereas \u03b7 \u2217 = r3 is the undesirable large\nerror probability and that \u03c42 delimits these two regimes.\nThe \u03c4 (\u03b1) points determine when it is beneficial to use the decoder, in the sense that \u03b7 \u2217 < \u03b5. By varying \u03b1 (as\nif in a sequence of plots like Fig. 4), an \u03b1 \u2212 \u03b5 region where the decoder is beneficial is demarcated; this is shown\nin Fig. 5. The function \u03c42 (\u03b1) is the \u03b7 -reliability decoding threshold for large ranges of \u03b7 .\nNotice that the previously known special case, the decoding threshold of the noiseless decoder, can be recovered\nfrom these results. The decoding threshold for the noiseless decoder is denoted \u03b5\u2217BRU and is equal to the following\nexpression [74].\n\u221a\n1\u2212 \u03c3\n\u2217\n\u03b5BRU =\n,\n2\nwhere\n1\n\u03c3=\u2212 +\n4\n\nq\n5\n\u2212 12\n\u2212b\n2\n\n+\n\nq\n\n\u2212 56 + \u221a\n\nand\n8\nb=\n3\n\n\u0012\n\n2\n\u221a\n83 + 3 993\n\n\u00131\n\n3\n\n1\n\u2212\n3\n\n\u0012\n\n4\n\n11\n\u22125/12\u2212b\n\n2\n\u221a\n\u00131\n83 + 3 993 3\n.\n2\n\nThis value is recovered from noisy decoder results by noting that \u03b7 \u2217 (\u03b1 = 0, \u03b5) = 0 for \u03b5 \u2208 [0, \u03b5\u2217BRU ], which are\nthe ordinate intercepts of the region in Fig. 5.\nTo provide a better sense of the performance of the noisy Gallager A algorithm, Table I lists some values of \u03b1,\n\u03b5, and \u03b7 \u2217 (numerical evaluations are listed and an example of an analytical expression is given in Appendix C).\nAs can be seen from these results, particularly from the \u03c42 curve in Fig. 5, the error probability performance of\nthe system degrades gracefully as noise is added to the decoder.\nReturning to threshold characterization, an analytical expression for the threshold within the region to use decoder\nis:\n\u03b7 \u2212 q\u03b1\u2212 (\u03b7)\n\u03b5\u2217 (\u03b7, \u03b1) =\n,\n1 \u2212 q\u03b1+ (\u03b7) \u2212 q\u03b1\u2212 (\u03b7)\nwhich is the solution to the polynomial equation in \u01eb\u030c,\n\u01eb\u030c \u2212 \u01eb\u030cq\u03b1+ (\u03b7) + (1 \u2212 \u01eb\u030c)q\u03b1\u2212 (\u03b7) \u2212 \u03b7 = 0.\n\n\f12\n\n0.04\n\u03c42(\u03b1)\n\u03c41(\u03b1)\n\n\u03b5\n\n0.03\n\n0.02\n\n0.01\n\n0\n\n0\n\n0.002\n\n0.004\n\n0.006\n\n\u03b1\n\n0.008\n\nFig. 5. Decoding a C \u221e (3, 6) code with the noisy Gallager A algorithm. Region where it is beneficial to use decoder is below \u03c42 and above\n\u03c41 .\n\nTABLE I\nP ERFORMANCE OF N OISY G ALLAGER A ALGORITHM FOR (3,6) CODE\n\u03b1\n0\n\n\u03b5\u2217 (0.1, \u03b1)\n0.0394636562\n0.0394636560\n0.0394636335\n0.0394613836\n0.0392359948\n0.0387781564\n0.0371477336\n0.0321984070\n0.0266099758\n\n1 \u00d7 10\u221210\n1 \u00d7 10\u22128\n1 \u00d7 10\u22126\n1 \u00d7 10\u22124\n3 \u00d7 10\u22124\n1 \u00d7 10\u22123\n3 \u00d7 10\u22123\n5 \u00d7 10\u22123\n\n\u03b7 \u2217 (\u03b1, \u03b5\u2217 )\n0\n7.8228 \u00d7 10\u221211\n7.8228 \u00d7 10\u22129\n7.8234 \u00d7 10\u22127\n7.8866 \u00d7 10\u22125\n2.4050 \u00d7 10\u22124\n8.4989 \u00d7 10\u22124\n3.0536 \u00d7 10\u22123\n6.3032 \u00d7 10\u22123\n\n\u03b7 \u2217 (\u03b1, 0.01)\n0\n1.3333 \u00d7 10\u221211\n1.3333 \u00d7 10\u22129\n1.3338 \u00d7 10\u22127\n1.3812 \u00d7 10\u22125\n4.4357 \u00d7 10\u22125\n1.8392 \u00d7 10\u22124\n9.2572 \u00d7 10\u22124\n2.4230 \u00d7 10\u22123\n\nThe threshold is drawn for several values of \u03b7 in Fig. 6. A threshold line determines the equivalence of channel\nnoise and decoder noise with respect to final probability of error. If for example, the binary symmetric channels\nin the system are a result of hard-detected AWGN channels, such a line may be used to derive the equivalent\nchannel noise power for decoder noise power or vice versa. Threshold lines therefore provide guidelines for power\nallocation in communication systems.\n\n0.04\n\u03b7 = 0.005\n\u03b7 = 0.002\n\u03b7 = 0.001\n\u03b7 = 0.0005\n\u03b7 = 0.0001\n\u03b7 = 0.00005\n\u03b7 = 0.00001\n\n\u03b5\n\n0.03\n\n0.02\n\n0.01\n\n0\n0\n\n0.002\n\n0.004\n\n0.006\n\n0.008\n\n\u03b1\n\nFig. 6. \u03b7-thresholds (gray lines) for decoding a C \u221e (3, 6) code with the noisy Gallager A algorithm within the region to use decoder\n(delimited with red line).\n\n\f13\n\n0.05\n\n0.04\n\n\u03b5\n\n0.03\n\n0.02\n\n0.01\n\n0\n\n0\n\n0.002\n\n0.004\n\n0.006\n\n0.008\n\u03b1\n\n0.01\n\n0.012\n\n0.014\n\n0.016\n\nFig. 7. Region to use decoder for Bazzi et al.'s optimized rate 1/2 LDPC code with noisy Gallager A decoding (black) is contained within\nthe region to use decoder for a rate 1/2 LDPC code in Bazzi et al.'s optimal family of codes with a = 1/10 (green) and contains the region\nto use decoder for the C \u221e (3, 6) code (gray).\n\nC. Code Optimization\nAt this point, the bit error performance of a system has simply been measured; no attempt has been made to\noptimize a code for a particular decoder and set of parameters. For fault-free decoding, it has been demonstrated\nthat irregular code ensembles can perform much better than regular code ensembles like the (3,6) LDPC considered\nabove [74], [77]. One might hope for similar improvements when LDPC code design takes decoder noise into\naccount. The space of system parameters to be considered for noisy decoders is much larger than for noiseless\ndecoders.\nAs a first step, consider the ensemble of rate 1/2 LDPC codes that were optimized by Bazzi et al. for the\nfault-free Gallager A decoding algorithm [74]. The left degree distribution is\n\u03bb(\u03b6) = a\u03b6 2 + (1 \u2212 a)\u03b6 3\n\nand the right degree distribution is\n\n7a 6 3 \u2212 7a 7\n\u03b6 +\n\u03b6 ,\n3\n3\nwhere the optimal a is specified analytically. Numerically, aopt = 0.1115 . . . . Measuring the performance of this\ncode with the noisy Gallager A decoder yields the region to use decoder shown in Fig. 7; the region to use decoder\nfor the (3,6) code is shown for comparison. By essentially any criterion of performance, this optimized code is\nbetter than the (3,6) code.\nAre there other codes that can perform better on the faulty decoder than the code optimized for the faultfree decoder? To see whether this is possible, arbitrarily restrict to the family of ensembles that were found to\ncontain the optimal degree distribution for the fault-free decoder and take a = 1/10. Also let \u03b1 = 1/500 be fixed.\nThe numerical value of the threshold \u03b5\u22171/10 (1/10, \u03b1) = 0.048239, whereas the numerical value of the threshold\n\u03b5\u2217aopt (1/10, \u03b1) = 0.047857. In this sense, the a = 1/10 code is better than the a = aopt code. In fact, as seen in\nFig. 7, the region to use decoder for this a = 1/10 code contains the region to use decoder for the aopt code.\n\u2217\nOn the other hand, the final error probability when operating at threshold for the a = 1/10 code \u03b71/10\n(\u03b1, \u03b5\u22171/10 (1/10, \u03b1)) =\n\u2217\n0.01869, whereas the final error probability when operating at threshold for the a = aopt code is \u03b7aopt (\u03b1, \u03b5\u2217aopt (1/10, \u03b1)) =\n0.01766. So in this sense, the a = aopt code is better than the a = 1/10 code. The fact that highly optimized\nensembles usually lead to more simultaneous critical points is the main complication.\nIf both threshold and final bit error probability are performance criteria, there is no total order on codes and\ntherefore there may be no notion of an optimal code.\n\u03c1(\u03b6) =\n\nVI. E XAMPLE : N OISY G AUSSIAN D ECODER\nIt is also of interest to analyze a noisy version of the belief propagation decoder applied to the output of a\ncontinuous-alphabet channel. Density evolution for belief propagation is difficult to analyze even in the noiseless\n\n\f14\n\ndecoder case, and so a Gaussian approximation method [78] is used. The state variables are one-dimensional rather\nthan infinite-dimensional as for full analysis of belief propagation. The specific node computations carried out by\nthe decoder are as in belief propagation [13]; these can be approximated by the functions \u03a6 and \u03a8 defined below.\nThe messages and noise model are specified in terms of the approximation.\nSection V had considered decoding the output of a BSC with a decoder that was constructed with BSC components\nand Proposition 1 had shown that probability of bit error could never be driven to zero. Here, the probability of\nbit error does in fact go to zero.\nConsider a binary input AWGN channel with variance \u03b52 . The output is decoded using a noisy Gaussian decoder.\nFor simplicity, only regular LDPC codes are considered. The messages that are passed in this decoder are real-valued,\nM = R \u222a {\u00b1\u221e}, and are in belief format.\nThe variable-to-check messages in the zeroth iteration are the log-likelihood ratios computed from the channel\noutput symbols, \u03bd(y),\np(y|x = 1)\n\u03bdv\u2192c = \u03bd(y) = log\n.\np(y|x = \u22121)\nThe check node takes the received versions of these messages, \u03bcv\u2192c , as input. The node implements a mapping\n\u03a6 whose output, \u03bdc\u2192v , satisfies:\ndY\nc \u22121\netanh(\u03bcv\u2192ci ),\netanh(\u03bdc\u2192v ) =\ni=1\n\nwhere the product is taken over messages on all incoming edges except the one on which the message will be\noutgoing, and\nZ\n1\nv (v\u2212v\u030c)2\netanh(v\u030c) = \u221a\ntanh e\u2212 4v dv .\n2\n4\u03c0v\u030c R\n\nThe check node mapping is motivated by Gaussian likelihood computations. For the sequel, it is useful to define\na slightly different function\n(\n1 \u2212 etanh(v\u030c), v\u030c > 0\n\u03c6(v\u030c) =\n1,\nv\u030c = 0\nwhich can be approximated as\n\u03c6(v\u030c) \u2248 eav\u030c\n\nc\n\n+b\n\n,\n\nwith a = \u22120.4527, b = 0.0218, c = 0.86 [78].\nFor iterations l \u2265 1, the variable node takes the received versions of the c \u2192 v messages, \u03bcc\u2192v , as inputs. The\nmapping \u03a8 yields output \u03bdv\u2192c given by\n\u03bdv\u2192c = \u03bd(y) +\n\ndX\nv \u22121\n\n\u03bcc\u2192vi ,\n\ni=1\n\nwhere the sum is taken over received messages from the neighboring check nodes except the one to which this\nmessage is outgoing. Again, the operation of the variable node is motivated by Gaussian likelihood computations.\nAs in Section V, local computation noise is combined into message-passing noise (Fig. 3). To model quantization\n[25] or random phenomena, consider each message passed in the decoder to be corrupted by signal-independent\nadditive noise which is bounded as \u2212\u03b1/2 \u2264 w \u2264 \u03b1/2. This class of noise models includes uniform noise, and\ntruncated Gaussian noise, among others. If the noise is symmetric, then Theorem 1 applies. Following the von\nNeumann error model, each noise realization w is assumed to be independent.\nA. Density Evolution Equation\nThe definition of the computation rules and the noise model may be used to derive the approximate density\nevolution equation. The one-dimensional state variable chosen to be tracked is s, the mean belief at a variable\nnode. The symmetry condition relating mean belief to belief variance [13], [78] is enforced. Thus, if the all-one\ncodeword was transmitted, then the value s going to +\u221e implies that the density of \u03bdv\u2192c tends to a \"mass point\nat infinity,\" which in turn implies that Pe goes to 0.\n\n\f15\n\nTo bound decoding performance under any noise model in the class of additive bounded noise, consider (nonstochastic) worst-case noise. Assuming that the all-one codeword was sent, all messages should be as positive as\npossible to move towards the correct decoded codeword (mean beliefs of +\u221e indicate perfect confidence in a bit\nbeing 1). Consequently, the worst bounded noise that may be imposed is to subtract \u03b1/2 from all messages that\nare passed; this requires knowledge of the transmitted codeword being all-one. If another codeword is transmitted,\nthen certain messages would have \u03b1/2 added instead of subtracted.\nSuch a worst-case noise model does not meet the conditions of Theorem 1, but transmission of the all-one\ncodeword is assumed nonetheless. If there were an adversary with knowledge of the transmitted codeword imposing\nworst-case noise on the decoder, then probability of bit error would be conditionally independent of the transmitted\ncodeword, as given in Appendix A-1.\nNote that the adversary is restricted to selecting each noise realization independently. More complicated and\ndevious error patterns in space or in time are not possible in the von Neumann error model. Moreover, the\nperformance criterion is probability of bit error rather than probability of block error, so complicated error patterns\nwould provide no great benefit to the adversary.\nSince the noise is conditionally deterministic given the transmitted codeword, derivation of the density evolution\nequation is much simplified. An induction argument is used, and the base case is\ns0 =\n\n2\n\u03b52 ,\n\nwhere \u03b52 is the channel noise power. This follows from the log-likelihood computation for an AWGN communication\nchannel with input alphabet X = {\u00b11}.\nThe inductive assumption in the induction argument is sl\u22121 . This message is communicated over message-passing\nnoise to get\nsl\u22121 \u2212 \u03b12 .\nNext the check node computation is made to yield\n\u0011\n\u0010\n\u22121\n\u03b1 dc \u22121\n.\n1 \u2212 [1 \u2212 \u03c6(sl\u22121 \u2212 2 )]\n\u03c6\n\nBy the inductive assumption, all messages will be equivalent; that is why the product is a (dc \u2212 1)-fold product of\nthe same quantity. This value is communicated over message-passing noise to get\n\u0010\n\u0011\n\u03c6\u22121 1 \u2212 [1 \u2212 \u03c6(sl\u22121 \u2212 \u03b12 )]dc \u22121 \u2212 \u03b12 .\nFinally the variable-node computation yields\n\u0010\nn\n\u0011\no\ns0 + (dv \u2212 1) \u03c6\u22121 1 \u2212 [1 \u2212 \u03c6(sl\u22121 \u2212 \u03b12 )]dc \u22121 \u2212 \u03b12 .\n\nAgain, all messages will be equivalent so the sum is a (dv \u2212 1)-fold sum of the same quantity. Thus the density\nevolution equation is\n\u0010\n\u0011o\nn\n\u22121\n\u03b1 dc \u22121\nsl = \u03b522 \u2212 (dv \u22121)\u03b1\n1\n\u2212\n[1\n\u2212\n\u03c6(s\n\u2212\n.\n(4)\n+\n(d\n\u2212\n1)\n\u03c6\n)]\nv\nl\u22121\n2\n2\nB. Performance Evaluation\nOne might wonder whether there are sets of noise parameters \u03b1 > 0 and \u03b5 > 0 such that sl \u2192 +\u221e. Indeed\nthere are, and there is a threshold phenomenon just like Chung et al. showed for \u03b1 = 0 [78].\nProposition 2: Final error probability \u03b7 \u2217 = 0 for LDPC ensembles decoded using the noisy Gaussian system\ndefined in Section VI, for binary-input AWGN channels with noise level \u03b5 < \u03b5\u2217 (\u03b1).\nProof: Substituting s = +\u221e into (4) demonstrates that it is a stable fixed point. It may further be verified that\nthe dynamical system proceeds toward that fixed point if \u03b5 < \u03b5\u2217 (\u03b1).\nUnlike Section V where the \u03b5\u2217 (\u03b7, \u03b1) thresholds could be evaluated analytically, only numerical evaluations of these\n\u03b5\u2217 (\u03b1) thresholds are possible. These are shown in Fig. 8 for three regular LDPC ensembles with rate 1/2, namely\nthe (3,6) ensemble, the (4,8) ensemble, and the (5,10) ensemble. As can be observed, thresholds decrease smoothly\nas the decoder noise level increases. Moreover, the ordering of the codes remains the same for all levels of decoder\nnoise depicted. Code optimization remains to be done.\n\n\f16\n\n0.9\n0.85\n0.8\n0.75\n\n\u03b5*\n\n0.7\n0.65\n0.6\n0.55\n0.5\n0.45\n0.4\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\u03b1\n\n3\n\n3.5\n\n4\n\n4.5\n\n5\n\nFig. 8. Thresholds for decoding the C \u221e (3, 6) code (triangle), the C \u221e (4, 8) code (quadrangle), and the C \u221e (5, 10) (pentangle), each with\nthe noisy Gaussian approximation algorithm. Notice that the ordinate intercepts are \u03b5\u2217CRU (3, 6) = 0.8747, \u03b5\u2217CRU (4, 8) = 0.8323, and\n\u03b5\u2217CRU (5, 10) = 0.7910, [78, Table I].\n\nThe basic reason for the disparity between Propositions 2 and 1 is that here, the noise is bounded whereas the\nmessages are unbounded. Thus once the messages grow large, the noise has essentially no effect. To use a term\nfrom [67], once the decoder reaches the breakout value, noise cannot stop the decoder from achieving Shannon\nreliability.\nPerhaps a peak amplitude constraint on messages would provide a more realistic computation model, but the\nequivalent of Proposition 2 may not hold. Quantified data processing inequalities may provide insight into what\nforms of noise and message constraints are truly limiting [34], [35].\nVII. A PPLICATION : R ELIABLE M EMORIES C ONSTRUCTED\n\nFROM\n\nU NRELIABLE C OMPONENTS\n\nIn Section I, complexity and reliability were cast as the primary limitations on practical decoding. By considering\nthe design of fault masking techniques for memory systems, a communication problem beyond Fig. 1, both\ncomplexity and reliability may be explicitly constrained. Indeed, the problem of constructing reliable information\nstorage devices from unreliable components is central to fault-tolerant computing, and determining the information\nstorage capacity of such devices is a long-standing open problem [79]. This problem is related to problems in\ndistributed information storage [80] and is intimately tied to the performance of codes under faulty decoding. The\nanalysis techniques developed thus far may be used directly.\nIn particular, one may construct a memory architecture with noisy registers and a noisy LDPC correcting network.\nAt each time step, the correcting network decodes the register contents and restores them. The correcting network\nprevents the codeword stored in the registers from wandering too far away. Taylor and others have shown that there\nexist non-zero levels of component noisiness such that the LDPC-based construction achieves non-zero storage\ncapacity [54], [55], [63]. Results as in Section V may be used to precisely characterize storage capacity.\nBefore proceeding with an achievability result, requisite definitions and the problem statement are given [54].\nDefinition 6: An elementary operation is any Boolean function of two binary operands.\nDefinition 7: A system is considered to be constructed from components, which are devices that either perform\none elementary operation or store one bit.\nDefinition 8: The complexity \u03c7 of a system is the number of components within the system.\nDefinition 9: A memory system that stores k information bits is said to have an information storage capability\nof k.\nDefinition 10: Consider a sequence of memories {Mi }, ordered according to their information storage capability\ni (bits). The sequence {Mi } is stable if it satisfies the following:\n1) For any k, Mk must have 2k allowed inputs denoted {Iki }, 1 \u2264 i \u2264 2k .\n2) A class of states, C(Iki ), is associated with each input Iki of Mk . The classes C(Iki ) and C(Ikj ) must be\ndisjoint for all i 6= j and all k.\n3) The complexity of Mk , \u03c7(Mk ), must be bounded by \u03b8k, where redundancy \u03b8 is fixed for all k.\n\n\f17\n\n4) At l = 0, let one of the inputs from {Iki } be stored in each memory Mk in the sequence of memories\n{Mi }, with no further inputs in times l > 0. Let Iki denote the particular input stored in memory Mk .\nLet \u03bbki (T ) denote the probability that the state of Mk does not belong to C(Iki ) at l = T and further let\nPkmax (T ) = maxi \u03bbki (T ). Then for any T > 0 and \u03b4 > 0, there must exist a k such that Pkmax (T ) < \u03b4.\nThe demarcation of classes of states is equivalent to demarcating decoding regions.\nDefinition 11: The storage capacity, C, of memory is a number such that there exist stable memory sequences\nfor all memory redundancy values \u03b8 greater than 1/C.\nNote that unlike channel capacity for the communication problem, there is no informational definition of storage\ncapacity that is known to go with the operational definition.\nThe basic problem then is to determine storage capacity, which is a measure of the circuit complexity required\nto achieve arbitrarily reliable information storage. The circuit complexity must be linear in blocklength, a property\nsatisfied by systems with message-passing correcting networks for LDPC codes.\nAlthough Proposition 1 shows that Shannon reliability is not achievable for any noisy Gallager A decoder, the\ndefinition of stable information storage does not require this. By only requiring maintenance within a decoding\nregion, the definition implies that either the contents of the memory may be read-out in coded form or equivalently\nthat there is a noiseless output device that yields decoded information; call this noiseless output device the silver\ndecoder.\nConsider the construction of a memory with noisy registers as storage elements. These registers are connected to\na noisy Gallager A LDPC decoder (as described in Section V), which takes the register values as inputs and stores\nits computational results back into the registers. To find the storage capacity of this construction, first compute the\ncomplexity (presupposing that the construction will yield a stable sequence of memories).\nThe Gallager A check node operation is a (dc \u2212 1)-input XOR gate, which may be constructed from dc \u2212 2\ntwo-input XOR gates. A variable node determines whether its dv \u2212 1 inputs are all the same and then compares\nto the original received value. Let Ddv denote the complexity of this logic. The output of the comparison to the\noriginal received value is the value of the consensus view. One construction to implement the consensus logic is\nto OR together the outputs of a (dv \u2212 1)-input AND gate and a (dv \u2212 1)-input AND gate with inverted inputs.\nThis is then XORed with the stored value. Such a circuit can be implemented with 2(dv \u2212 2) + 2 components, so\nDdv = 2dv \u2212 2. The storage is carried out in n registers. The total complexity of the memory Mk , \u03c7(Mk )C n (dv ,dc ) ,\nis\n\u03c7(Mk )C n (dv ,dc ) = n(1 + 2dv \u2212 2 + dv (dc \u2212 2)) = n(dv dc \u2212 1).\nThe information storage capability is n times the rate of the code, R. The complexity of an irredundant memory\nwith the same storage capability is \u03c7irrn = Rn. Hence, the redundancy is\n\u03c7(Mk )C n (dv ,dc )\n(dv dc \u2212 1)\nn(dv dc \u2212 1)\n\u2264\n=\n\u03c7irrn\nRn\n1 \u2212 dv /dc\n\nwhich is a constant. By [65, Lemma 3.22], the inequality almost holds with equality with high probability for large\nn. For the (3, 6) regular LDPC code, the redundancy value is 34, so C = 1/34, if the construction does in fact\nyield stable memories.\nThe conditions under which the memory is stable depends on the silver decoder. Since silver decoder complexity\ndoes not enter, maximum likelihood should be used. The Gallager lower bound to the ML decoding threshold for\nthe (3, 6) regular LDPC code is \u03b5\u2217GLB = 0.0914755 [81, Table II]. Recall from Fig. 5 that the decoding threshold\nfor Gallager A decoding is \u03b5\u2217BRU = 0.0394636562.\nIf the probability of bit error for the correcting network in the memory stays within the decoding threshold of\nthe silver decoder, then stability follows. Thus the question reduces to determining the sets of component noisiness\nlevels (\u03b1, \u03b5) for which the decoding circuit achieves (\u03b7 = \u03b5\u2217M L )-reliability.\nConsider a memory system where bits are stored in registers with probability \u03b1r of flipping at each time step.\nAn LDPC codeword is stored in these registers; the probability of incorrect storage at the first time step is \u03b5. At\neach iteration, the variable node value from the correcting network is placed in the register. This stored value is\nused in the subsequent Gallager A variable node computation rather than a received value from the input pins.\nSuppose that the component noise values in the correcting network may be parameterized as in Section V. Then a\n\n\f18\n\n0.04\n\n\u03b5\n\n0.03\n\n0.02\n\n0.01\n\n0\n\n0\n\n0.002\n\n0.004\n\n\u03b1\n\n0.006\n\n0.008\n\nFig. 9. For a memory system constructed with noisy registers and a (3, 6) LDPC Gallager A correcting network, the region R (delimited\nby black line) comprises the \"region to use decoder\" and its hypograph.\n\nslight modification of the analysis in Section V yields a density evolution equation\nsl+1 = \u03b52 \u2212 \u03b52 q\u03b1+ (sl ) + (1 \u2212 \u03b52 )q\u03b1\u2212 (sl ),\n\nwhere \u03b52 = sl (1 \u2212 \u03b1r ) + \u03b1r (1 \u2212 sl ). There is a \"region to use decoder\" for this system, just as in Section V. If\n\u03b1r = \u03b1, this region is shown in Fig. 9, and is slightly smaller than the region in Fig. 5. Denote this region and\nits hypograph as R. It follows that (\u03b7 = \u03b5\u2217BRU )-reliability is achieved for R. Since \u03b5\u2217BRU -reliability is achievable,\n\u03b5\u2217GLB -reliability is achievable by monotonicity. Thus the construction yields stable memories.\nProposition 3: Let R be the set of memory component noise parameters (\u03b1, \u03b5) within the region to use decoder\nor its hypograph corresponding to a system with a Gallager A correcting network for the (3, 6) LDPC code, depicted\nin Fig. 9. Then a sequence of memories constructed from R-components have a storage capacity lower bounded\nas C \u2265 1/34.\nThis may be directly generalized for any choice of code ensemble as follows.\nTheorem 5: Let R be the (computable) set of memory component noise parameters (\u03b1, \u03b5) within the region to\nuse decoder or its hypograph corresponding to a system with a Gallager A correcting network for the (\u03bb, \u03c1) LDPC\ncode. Then a sequence of memories constructed from R-components have a storage capacity lower bounded as\nC\u2265\n\n1 \u2212 \u03bb\u2032 (1)/\u03c1\u2032 (1)\n.\n\u03bb\u2032 (1)\u03c1\u2032 (1) \u2212 1\n\nThe bound reduces to (1 \u2212 dv /dc )/(dv dc \u2212 1) for regular codes.\nThis theorem gives a precise achievability result that bounds storage capacity. It also implies a code ensemble\noptimization problem similar to the one in Section V-C. The question of an optimal architecture for memory\nsystems however remains open.\nVIII. C ONCLUSIONS\nLoeliger et al. [7] had observed that decoders are robust to nonidealities and noise in physical implementations,\nhowever they had noted that \"the quantitative analysis of these effects is a challenging theoretical problem.\" This\nwork has taken steps to address this challenge by characterizing robustness to decoder noise.\nThe extension of the density evolution method to the case of faulty decoders allows a simplified means of\nasymptotic performance characterization. Results from this method show that in certain cases Shannon reliability is\nnot achievable (Proposition 1), whereas in other cases it is achievable (Proposition 2). In either case, however, the\ndegradation of a suitably defined decoding threshold is smooth with increasing decoder noise, whether in circuit\nnodes or circuit wires. Due to this smoothness, codes optimized for fault-free decoders do work well with faulty\ndecoders, however optimization of codes for systems with faulty decoders remains to be studied.\nNo attempt was made to apply fault masking methods to develop decoding algorithms with improved performance\nin the presence of noise. One approach might be to use coding within the decoder so as to reduce the values of \u03b1. Of\n\n\f19\n\ncourse, the within-decoder code would need to be decoded. There are also more direct circuit-oriented techniques\nthat may be applied [82], [83]. Following the concept of concatenated codes, concatenated decoders may also be\npromising. The basic idea of using a first (noiseless) decoder to correct many errors and then a second (noiseless)\ndecoder to clean things up was already present in [61], but it may be extended to the faulty decoder setting.\nReducing power consumption in decoder circuits has been an active area of research [37], [84]\u2013[90], however\npower reduction often has the effect of increasing noise in the decoder [91]. The tradeoff developed between the\nquality of the communication channel and the quality of the decoder may provide guidelines for allocating resources\nin communication system design.\nAnalysis of other decoding algorithms with other error models will presumably yield results similar to those\nobtained here. For greater generality, one might move beyond simple LDPC codes and consider arbitrary codes\ndecoded with very general iterative decoding circuits [90] with suitable error models. An even more general model\nof computation such as a Turing machine or beyond [92] does not seem to have an obvious, appropriate error\nmodel.\nEven just a bit of imagination provides numerous models of channel noise and circuit faults that may be\ninvestigated in the future to provide further insights into the fundamental limits of noisy communication and\ncomputing.\nACKNOWLEDGMENT\nI thank R\u00fcdiger L. Urbanke and \u0130. Emre Telatar for several enlightening discussions, for encouragement, and for\nhosting my visit at EPFL. I also thank the anonymous reviewers, Sanjoy K. Mitter, G. David Forney, and Vivek\nK Goyal for assistance in improving the paper. Thanks also to Shashi Kiran Chilappagari for telling me about his\nwork.\nA PPENDIX A\nP ROOF OF T HEOREM 1\nLet x \u2208 C n be a codeword and let Y denote the corresponding channel output Y = xZ (where the notation\nmeans pointwise multiplication on length n vectors). Note that Z is equal to the channel output observation when\nx is all-one. The goal is to show that messages sent during the decoding process for cases when the received\ncodeword is either xZ or x correspond.\n(l)\n(l)\nLet \u1e45i be an arbitrary variable node and let \u1e45j be one of its neighboring check nodes. Let \u03bdij (y) and \u03bcij (y)\ndenote the variable-to-check message from \u1e45i to \u1e45j at the respective terminals in iteration l, assuming received\n(l)\n(l)\nvalue y. Similarly, let \u03bdji (y) and \u03bcji (y) be the check-to-variable message from \u1e45j to \u1e45i at the respective terminal\nin iteration l assuming received value y.\nBy Definition 2, the channel is memoryless binary-input output-symmetric and it may be modeled multiplicatively\nas\nYt = xt Zt ,\n(5)\nwhere {Zt } is a sequence of i.i.d. random variables and t is the channel usage time. The validity of the multiplicative\nmodel is shown in [13, p. 605] and [65, p. 184].\nThe proof proceeds by induction and so the base case is established first. By the multiplicative model (5),\n(0)\n(0)\n\u03bdij (y) = \u03bdij (xz). Recalling that xi \u2208 {\u00b11}, by the variable node symmetry condition (Definition 3) which\n(0)\n\n(0)\n\n(0)\n\n(0)\n\nincludes computation noise u\u1e45i , it follows that \u03bdij (y) = \u03bdij (xz) = xi \u03bdij (z).\n(0)\n\nNow take the wire noise wij on the message from \u1e45i to \u1e45j into account. It is symmetric (Definition 5) and so\n\n(0)\n\n(0)\n\n(0)\n\n\u03bdij (y) = xi \u03bdij (z) implies a similar property for \u03bcij . In particular,\n(0)\n\n(0)\n\n(0)\n\n\u03bcij (y) = \u039e(\u03bdij (y), wij )\n(0)\n\n(6)\n\n(0)\n\n= \u039e(xi \u03bdij (z), wij )\n(0)\n\n(0)\n\n= xi \u039e(\u03bdij (z), xi wij )\n\n\f20\n\nwhere the last step follows because xi \u2208 {\u00b11} and so it can be taken outside of \u039e by Definition 5, when it is put\nback in for the wire noise. Now since xi \u2208 {\u00b11} and since the wire noise is symmetric about 0 by Definition 5,\n(0)\n(0)\n(0)\nxi \u039e(\u03bdij (z), xi wij ) will correspond to xi \u03bcij (z), in the sense that error event probabilities will be identical.\n(l)\n(l)\nAssume that \u03bcij (y) corresponds to xi \u03bcij (z) for all (i, j) pairs and some l \u2265 0 as the inductive assumption.\nLet N\u1e45j be the set of all variable nodes that are connected to check node \u1e45j . Since x is a codeword, it satisfies\nQ\n(l+1)\nthe parity checks, and so k\u2208N\u1e45 = 1. Then from the check node symmetry condition (Definition 3), \u03bdji (y)\nj\n\n(l+1)\n\ncorresponds to xi \u03bdji\n\n(z). Further, by the wire noise symmetry condition (Definition 5) and the same argument\n(l+1)\n\nas for the base case, \u03bcji\n\n(l+1)\n\n(y) corresponds to xi \u03bcji\n\n(z). By invoking the variable node symmetry condition\n\n(l+1)\n\u03bdij (y)\n\n(l+1)\n\n(Definition 4) again, it follows that\ncorresponds to xi \u03bdij (z) for all (i, j) pairs.\nThus by induction, all messages to and from variable node \u1e45i when y is received correspond to the product of\nxi and the corresponding message when z is received.\nBoth decoders proceed in correspondence and commit exactly the same number of errors.\n1) Worst-Case Noise: The same result with the same basic proof also holds when the wire noise operation \u039e\nis symmetric but w is not symmetric stochastic, but is instead worst-case. The only essential modification is in (6)\nand the related part of the induction step. Since wire noise is dependent on xi , it can be written as xi w. Thus,\n(0)\n\n(0)\n\n(0)\n\n\u03bcij (y) = \u039e(\u03bdij (y), xi wij )\n(0)\n\n(0)\n\n= \u039e(xi \u03bdij (z), xi wij )\n(a)\n\n(0)\n\n(0)\n\n= xi \u039e(\u03bdij (z), wij )\n(0)\n\n= xi \u03bcij (z)\n\nwhere step (a) follows because xi \u2208 {\u00b11} and so it can be taken outside of \u039e by the symmetry property of \u039e.\nThus the two decoders will proceed in exact one-to-one correspondence, not just in probabilistic correspondence.\nA PPENDIX B\nP ROOF OF T HEOREM 2\nPrior to giving the proof of Theorem 2, a review of some definitions from probability theory [93] and the\nHoeffding-Azuma inequality are provided.\nConsider a measurable space (\u03a9, F) consisting of a sample space \u03a9 and a \u03c3 -algebra F of subsets of \u03a9 that\ncontains the whole space and is closed under complementation and countable unions. A random variable is an\nF -measurable function on \u03a9. If there is a collection (Z\u03b3 |\u03b3 \u2208 C) of random variables Z\u03b3 : \u03a9 \u2192 R, then\nZ = \u03c3(Z\u03b3 |\u03b3 \u2208 C)\n\nis defined to be the smallest \u03c3 -algebra Z on \u03a9 such that each map (Z\u03b3 |\u03b3 \u2208 C) is Z -measurable.\nDefinition 12 (Filtration): Let {Fi } be a sequence of \u03c3 -algebras with respect to the same sample space \u03a9. These\nFi are said to form a filtration if F0 \u2286 F1 \u2286 * * * are ordered by refinement in the sense that each subset of \u03a9 in\nFi is also in Fj for i \u2264 j . Also F0 = {\u2205, \u03a9}.\nUsually, {Fi } is the natural filtration Fi = \u03c3(Z0 , Z1 , . . . , Zi ) of some sequence of random variables (Z0 , Z1 , . . .),\nand then the knowledge about \u03c9 known at step i consists of the values Z0 (\u03c9), Z1 (\u03c9), . . . , Zi (\u03c9).\nFor a probability triple (\u03a9, F, P), a version of the conditional expectation of a random variable Z given a \u03c3 algebra F is a random variable denoted E [Z|F]. Two versions of conditional expectation agree almost surely, but\nmeasure zero departures are not considered subsequently; one version is fixed as canonical. Conditional expectation\ngiven a measurable event E is denoted E [Z|\u03c3(E)] and conditional expectation given a random variable W is\ndenoted E [Z|\u03c3(W )].\nDefinition 13 (Martingale): Let F0 \u2286 F1 \u2286 * * * be a filtration on \u03a9 and let Z0 , Z1 , . . . be a sequence of\nrandom variables on \u03a9 such that Zi is Fi -measurable. Then Z0 , Z1 , . . . is a martingale with respect to the filtration\nF0 \u2286 F1 \u2286 * * * if E [Zi |Fi\u22121 ] = Zi\u22121 .\nA generic way to construct a martingale is Doob's construction.\nDefinition 14 (Doob Martingale): Let F0 \u2286 F1 \u2286 * * * be a filtration on \u03a9 and let Z be a random variable on\n\u03a9. Then the sequence of random variables Z0 , Z1 , . . . such that Zi = E [Z|Fi ] is a Doob martingale.\n\n\f21\n\nLemma 1 (Hoeffding-Azuma Inequality [13], [94], [95]): Let Z0 , Z1 , . . . be a martingale with respect to the\nfiltration F0 \u2286 F1 \u2286 * * * such that for each i > 0, the following bounded difference condition is satisfied\n|Zi \u2212 Zi\u22121 | \u2264 \u03b1i , \u03b1i \u2208 [0, \u221e).\n\nThen for all n > 0 and any \u03be > 0,\n\u0012\n\u0013\n\u03be2\nPr [|Zn \u2212 Z0 | \u2265 \u03be] \u2264 2 exp \u2212 Pn\n.\n2 k=1 \u03b12k\n\nNow to the proof of Theorem 2; as noted before, it is an extension of [13, Theorem 2] or [65, Theorem 4.94]. The\nbasic idea is to construct a Doob martingale about the object of interest by revealing various randomly determined\naspects in a filtration-refining manner. The first set of steps is used to reveal which code was chosen from the\nensemble of codes; the ndv edges in the bipartite graph are ordered in some arbitrary manner and exposed one by\none. Then the n channel noise realizations are revealed. At this point the exact graph and the exact channel noise\nrealizations encountered have been revealed. Now the decoder noise realizations must be revealed. There are n\nvariable nodes, so the computation noise in each of them is revealed one by one. There are ndv edges over which\nvariable-to-check communication noise is manifested. Then there are ndv /dc check nodes with computation noise,\nand finally there are ndv check-to-variable communication noises for one iteration of the algorithm. The decoder\nnoise realizations are revealed for each iteration. At the beginning of the revelation process, the average (over choice\nof code, channel noise realization, and decoder noise realization) is known; after the m = (dv +2ldv +1+l+ldv /dc )n\nrevelation steps, the exact system used is known.\nRecall that Z denotes the number of incorrect values held at the end of the lth iteration for a particular\n(g, y, w, u) \u2208 \u03a9. Since g is a graph in the set of labeled bipartite factor graphs with variable node degree dv\nand check node degree dc , G n (dv , dc ); y is a particular input to the decoder, y \u2208 Y n ; w is a particular realization\nof the message-passing noise, w \u2208 M2ldv n ; and u is a particular realization of the local computation noise,\nu \u2208 U (l+ldv /dc )n , the sample space is \u03a9 = G n (dv , dc ) \u00d7 Y n \u00d7 M2ldv n \u00d7 U (l+ldv /dc )n .\nIn order to define random variables, first define the following exposure procedure. Suppose realizations of random\nquantities are exposed sequentially. First expose the dv n edges of the graph one at a time. At step i \u2264 dv n expose\nthe particular check node socket which is connected to the ith variable node socket. Next, in the following n steps,\nexpose the received values yi one at a time. Finally in the remaining (2dv + 1 + dv /dc )ln steps, expose the decoder\nnoise values ui and wi that were encountered in all iterations up to iteration l.\nLet \u2261i , 0 \u2264 i \u2264 m, be a sequence of equivalence relations on the sample space \u03a9 ordered by refinement. Refinement means that (g\u2032 , y \u2032 , w\u2032 , u\u2032 ) \u2261i (g\u2032\u2032 , y \u2032\u2032 , w\u2032\u2032 , u\u2032\u2032 ) implies (g\u2032 , y \u2032 , w\u2032 , u\u2032 ) \u2261i\u22121 (g\u2032\u2032 , y \u2032\u2032 , w\u2032\u2032 , u\u2032\u2032 ). The equivalence\nrelations define equivalence classes such that (g\u2032 , y \u2032 , w\u2032 , u\u2032 ) \u2261i (g\u2032\u2032 , y \u2032\u2032 , w\u2032\u2032 , u\u2032\u2032 ) if and only if the realizations of\nrandom quantities revealed in the first i steps for both pairs is the same.\nNow, define a sequence of random variables Z0 , Z1 , . . . , Zm . Let the random variable Z0 be Z0 = E [Z], where\nthe expectation is over the code choice, channel noise , and decoder noise. The remaining random variables Zi are\nconstructed as conditional expectations given the measurable equivalence events (g\u2032 , y \u2032 , w\u2032 , u\u2032 ) \u2261i (g, y, w, u):\n\u0002\n\u0003\nZi (g, y, w, u) = E Z(g\u2032 , y \u2032 , w\u2032 , u\u2032 )|\u03c3((g\u2032 , y \u2032 , w\u2032 , u\u2032 ) \u2261i (g, y, w, u)) .\n\nNote that Zm = Z and that by construction Z0 , Z1 , . . . , Zm is a Doob martingale. The filtration is understood to\nbe the natural filtration of the random variables Z0 , Z1 , . . . , Zm .\nTo use the Hoeffding-Azuma inequality to give bounds on\nPr [|Z \u2212 E [Z] | > ndv \u01eb/2] = Pr [|Zm \u2212 Z0 | > ndv \u01eb/2] ,\n\nbounded difference conditions\n|Zi+1 (g, y, w, u) \u2212 Zi (g, y, w, u)| \u2264 \u03b1i , i = 0, . . . , m \u2212 1\n\nneed to be proved for suitable constants \u03b1i that may depend on dv , dc , and l.\nFor the steps where bipartite graph edges are exposed, it was shown in [13, p. 614] that\n|Zi+1 (g, y, w, u) \u2212 Zi (g, y, w, u)| \u2264 8(dv dc )l , 0 \u2264 i < ndv .\n\n\f22\n\nIt was further shown in [13, p. 615] that for the steps when the channel outputs are revealed that\n|Zi+1 (g, y, w, u) \u2212 Zi (g, y, w, u)| \u2264 2(dv dc )l , ndv \u2264 i < n(1 + dv ).\n\n(7)\n\nIt remains to show that the inequality is also fulfilled for steps when decoder noise realizations are revealed. The\nbounding procedure is nearly identical to that which yields (7). When a node noise realization u is revealed, clearly\nonly something whose directed neighborhood includes the node at which the noise u causes perturbations can be\naffected. Similarly, when an edge noise realization w is revealed, only something whose directed neighborhood\nincludes the edge on which the noise w causes perturbations can be affected. In [13, p. 603], it is shown that the size\n2l | \u2264 2(d d )l\nof the directed neighborhood of depth 2l of the node \u1e45(u) associated with noise u is bounded as |N\u1e45(u)\nv c\nand similarly the size of the directed neighborhood of length 2l of the edge ~e(w) associated with noise w is bounded\nas |N~e2l(w) | \u2264 2(dv dc )l . Since the maximum depth that can be affected by a noise perturbation is 2l, a weak uniform\nbound for the remaining exposure steps is\n|Zi+1 (g, y, w, u) \u2212 Zi (g, y, w, u)| \u2264 2(dv dc )l , n(1 + dv )dv \u2264 i < m.\n\nSince bounded difference constants \u03b1i have been provided for all i, the theorem follows from application of the\nHoeffding-Azuma inequality to the martingale.\nOne may compute a particular value of \u03b2 to use as follows. The bounded difference sum is\nm\nX\n\u03b12k = 64ndv (dv dc )2l + 4n(dv dc )2l + 4[2ldv n + nl + nldv /dc ](dv dc )2l\nk=1\n\nn\n= n 64dv + 4 + 8dv l + l +\n\ndv l\ndc\n\no\n\ndv 2l dc 2l\n\nSetting constants in the theorem and in the Hoeffding-Azuma inequality equal yields\n1\n\u03b2\n\nThus\n\n1\n\u03b2\n\n= 512dv 2l\u22121 dc 2l + 32dv 2l\u22122 dc 2l + 64ldv 2l\u22121 dc 2l + 8ldv 2l\u22121 dc 2l\u22121 + 8ldv 2l\u22122 dc 2l\n\u2264 (544 + 80l)dv 2l\u22121 dc 2l\n\ncan be taken as (544 + 80l)dv 2l\u22121 dc 2l .\nA PPENDIX C\nA N A NALYTICAL E XPRESSION\n\nAn analytical expression for \u03b5\u2217 (\u03b7 = 1/10, \u03b1 = 5 \u00d7 10\u22123 ) is\n\u0001\n\u221a\n1\n2 1 \u2212 1 + 4c7 ,\n\nwhere c7 is the second root of the polynomial in \u03b5\u030c\n\nc1 + c2 \u03b5\u030c + c3 \u03b5\u030c2 + c4 \u03b5\u030c3 + c5 \u03b5\u030c4 + c6 \u03b5\u030c5 ,\n\nand constants (c1 , . . . , c6 ) are defined as follows.\nc1 = 36\u03b12 \u2212 360\u03b13 + 1860\u03b14 \u2212 6240\u03b15 + 14752\u03b16 \u2212 25344\u03b17 + 31680\u03b18\n\u2212 28160\u03b19 + 16896\u03b110 \u2212 6144\u03b111 + 1024\u03b112\n3424572914129280658801\n=\n4000000000000000000000000\n\nc2 = 1 \u2212 72\u03b1 + 1080\u03b12 \u2212 8160\u03b13 + 38640\u03b14 \u2212 125952\u03b15 + 295424\u03b16 \u2212 506880\u03b17\n+ 633600\u03b18 \u2212 563200\u03b19 + 337920\u03b110 \u2212 122880\u03b111 + 20480\u03b112\n133200752195329280658801\n=\n200000000000000000000000\n\nc3 = 32 \u2212 864\u03b1 + 10080\u03b12 \u2212 69120\u03b13 + 314880\u03b14 \u2212 1012224\u03b15 + 2364928\u03b16 \u2212 4055040\u03b17\n+ 5068800\u03b18 \u2212 4505600\u03b19 + 2703360\u03b110 \u2212 983040\u03b111 + 163840\u03b112\n698088841835929280658801\n=\n25000000000000000000000\n\n\f23\n\nc4 = 160 \u2212 3840\u03b1 + 42240\u03b12 \u2212 281600\u03b13 + 1267200\u03b14 \u2212 4055040\u03b15 + 9461760\u03b16 \u2212 16220160\u03b17\n+ 20275200\u03b18 \u2212 18022400\u03b19 + 10813440\u03b110 \u2212 3932160\u03b111 + 655360\u03b112\n886384871716129280658801\n=\n6250000000000000000000\n\nc5 = 320 \u2212 7680\u03b1 + 84480\u03b12 \u2212 563200\u03b13 + 2534400\u03b14 \u2212 8110080\u03b15 + 18923520\u03b16 \u2212 32440320\u03b17\n+ 40550400\u03b18 \u2212 36044800\u03b19 + 21626880\u03b110 \u2212 7864320\u03b111 + 1310720\u03b112\n886384871716129280658801\n=\n3125000000000000000000\n\nc6 = 256 \u2212 6144\u03b1 + 67584\u03b12 \u2212 450560\u03b13 + 2027520\u03b14 \u2212 6488064\u03b15 + 15138816\u03b16 \u2212 25952256\u03b17\n+ 32440320\u03b18 \u2212 28835840\u03b19 + 17301504\u03b110 \u2212 6291456\u03b111 + 1048576\u03b112\n886384871716129280658801\n=\n3906250000000000000000\n\nAs given in Table I, the numerical value of \u03b5\u2217 (\u03b7 = 1/10, \u03b1 = 5 \u00d7 10\u22123 ) is 0.0266099758.\nSimilarly complicated analytical expressions are available for the other entries of Table I and the values used to\ncreate Figs. 4, 5, and 6.\nR EFERENCES\n[1] C. E. Shannon, \"A mathematical theory of communication,\" Bell Syst. Tech. J., vol. 27, pp. 379\u2013423, 623\u2013656, July/Oct. 1948.\n[2] R. E. Blahut, Theory and Practice of Error Control Codes. Reading, MA: Addison-Wesley Publishing Company, 1983.\n[3] A. R. Calderbank, \"The art of signaling: Fifty years of coding theory,\" IEEE Trans. Inf. Theory, vol. 44, no. 6, pp. 2561\u20132595, Oct.\n1998.\n[4] D. J. Costello, Jr. and G. D. Forney, Jr., \"Channel coding: The road to channel capacity,\" Proc. IEEE, vol. 95, no. 6, pp. 1150\u20131177,\nJun. 2007.\n[5] H. D. Pfister, I. Sason, and R. Urbanke, \"Capacity-achieving ensembles for the binary erasure channel with bounded complexity,\" IEEE\nTrans. Inf. Theory, vol. 51, no. 7, pp. 2352\u20132379, Jul. 2005.\n[6] S.-Y. Chung, G. D. Forney, Jr., T. J. Richardson, and R. Urbanke, \"On the design of low-density parity-check codes within 0.0045 dB\nof the Shannon limit,\" IEEE Commun. Lett., vol. 5, no. 2, pp. 58\u201360, Feb. 2001.\n[7] H.-A. Loeliger, F. Lustenberger, M. Helfenstein, and F. Tark\u00f6y, \"Probability propagation and decoding in analog VLSI,\" IEEE Trans.\nInf. Theory, vol. 47, no. 2, pp. 837\u2013843, Feb. 2001.\n[8] A. J. Blanksby and C. J. Howland, \"A 690-mW 1-Gb/s 1024-b, rate-1/2 low-density parity-check code decoder,\" IEEE J. Solid-State\nCircuits, vol. 37, no. 3, pp. 404\u2013412, Mar. 2002.\n[9] T. Zhang and K. K. Parhi, \"An FPGA implementation of (3, 6)-regular low-density parity-check code decoder,\" EURASIP J. Appl.\nSignal Process., vol. 2003, no. 6, pp. 530\u2013542, 2003.\n[10] L. M. J. Bazzi and S. K. Mitter, \"Endcoding complexity versus minimum distance,\" IEEE Trans. Inf. Theory, vol. 51, no. 6, pp.\n2103\u20132112, Jun. 2005.\n[11] R. W. Hamming, \"Error detecting and error correcting codes,\" Bell Syst. Tech. J., vol. 26, no. 2, pp. 147\u2013160, Apr. 1950.\n[12] W. H. Pierce, Failure-Tolerant Computer Design. New York: Academic Press, 1965.\n[13] T. J. Richardson and R. L. Urbanke, \"The capacity of low-density parity-check codes under message-passing decoding,\" IEEE Trans.\nInf. Theory, vol. 47, no. 2, pp. 599\u2013618, Feb. 2001.\n[14] P. Larsson and C. Svensson, \"Noise in digital dynamic CMOS circuits,\" IEEE J. Solid-State Circuits, vol. 29, no. 6, pp. 655\u2013662, Jun.\n1994.\n[15] R. Ho, K. W. Mai, and M. A. Horowitz, \"The future of wires,\" Proc. IEEE, vol. 89, no. 4, pp. 490\u2013504, Apr. 2001.\n[16] C. Zhao, X. Bai, and S. Dey, \"Evaluating transient error effects in digital nanometer circuits,\" IEEE Trans. Rel., vol. 56, no. 3, pp.\n381\u2013391, Sep. 2007.\n[17] T. Rejimon, K. Lingasubramanian, and S. Bhanja, \"Probabilistic error modeling for nano-domain logic circuits,\" IEEE Trans. VLSI\nSyst., vol. 17, no. 1, pp. 55\u201365, Jan. 2009.\n[18] K. K. Likharev, \"Single-electron devices and their applications,\" Proc. IEEE, vol. 87, no. 4, pp. 606\u2013632, Apr. 1999.\n[19] A. Bachtold, P. Hadley, T. Nakanishi, and C. Dekker, \"Logic circuits with carbon nanotube transistors,\" Science, vol. 294, no. 5545,\npp. 1317\u20131320, Nov. 2001.\n[20] C. P. Collier, E. W. Wong, M. Belohradsk\u00fd, F. M. Raymo, J. F. Stoddart, P. J. Kuekes, R. S. Williams, and J. R. Heath, \"Electronically\nconfigurable molecular-based logic gates,\" Science, vol. 285, no. 5426, pp. 391\u2013394, Jul. 1999.\n[21] J. Han and P. Jonker, \"A defect- and fault-tolerant architecture for nanocomputers,\" Nanotechnology, vol. 14, no. 2, pp. 224\u2013230, Feb.\n2003.\n[22] L. Anghel and M. Nicolaidis, \"Defects tolerant logic gates for unreliable future nanotechnologies,\" in Computational and Ambient\nIntelligence, ser. Lecture Notes in Computer Science, F. Sandoval, A. Prieto, J. Cabestany, and M. Gra\u00f1a, Eds. Berlin: Springer, 2007,\nvol. 4507, pp. 422\u2013429.\n\n\f24\n\n[23] V. Bush, \"The differential analyzer. A new machine for solving differential equations,\" J. Franklin Inst., vol. 212, no. 4, pp. 447\u2013488,\nOct. 1931.\n[24] R. Sarpeshkar, \"Analog versus digital: Extrapolating from electronics to neurobiology,\" Neural Comput., vol. 10, no. 7, pp. 1601\u20131638,\nOct. 1998.\n[25] B. Widrow and I. Koll\u00e1r, Quantization Noise: Roundoff Error in Digital Computation, Signal Processing, Control, and Communications.\nCambridge: Cambridge University Press, 2008.\n[26] R. G. Gallager, Low-Density Parity-Check Codes. Cambridge, MA: MIT Press, 1963.\n[27] J. von Neumann, \"Probabilistic logics and the synthesis of reliable organisms from unreliable components,\" in Automata Studies, C. E.\nShannon and J. McCarthy, Eds. Princeton: Princeton University Press, 1956, pp. 43\u201398.\n[28] S. Winograd and J. D. Cowan, Reliable Computation in the Presence of Noise. Cambridge, MA: MIT Press, 1963.\n[29] C. N. Hadjicostis, Coding Approaches to Fault Tolerance in Combinational and Dynamic Systems.\nBoston: Kluwer Academic\nPublishers, 2002.\n[30] P. G\u00e1cs and A. G\u00e1l, \"Lower bounds for the complexity of reliable Boolean circuits with noisy gates,\" IEEE Trans. Inf. Theory, vol. 40,\nno. 2, pp. 579\u2013583, Mar. 1994.\n[31] P. Elias, \"Computation in the presence of noise,\" IBM J. Res. Develop., vol. 2, no. 4, pp. 346\u2013353, Oct. 1958.\n[32] B. W. Johnson, Design and Analysis of Fault-Tolerant Digital Systems. Reading, MA: Addison-Wesley Publishing Company, 1989.\n[33] D. K. Pradhan, Fault-Tolerant Computer System Design. Upper Saddle River, NJ: Prentice Hall, 1996.\n[34] N. Pippenger, \"Reliable computation by formulas in the presence of noise,\" IEEE Trans. Inf. Theory, vol. 34, no. 2, pp. 194\u2013197, Mar.\n1988.\n[35] W. S. Evans and L. J. Schulman, \"Signal propagation and noisy circuits,\" IEEE Trans. Inf. Theory, vol. 45, no. 7, pp. 2367\u20132373, Nov.\n1999.\n[36] L. Benini and G. De Micheli, Networks on Chips: Technology and Tools. San Francisco, CA: Morgan Kaufmann Publishers, 2006.\n[37] R. Hegde and N. R. Shanbhag, \"Toward achieving energy efficiency in presence of deep submicron noise,\" IEEE Trans. VLSI Syst.,\nvol. 8, no. 4, pp. 379\u2013391, Aug. 2000.\n[38] C. Crick and A. Pfeffer, \"Loopy belief propagation as a basis for communication in sensor networks,\" in Proc. 19th Annu. Conf.\nUncertainty in Artificial Intelligence (UAI'03), Aug. 2003, pp. 151\u2013158.\n[39] O. W. Yeung and K. M. Chugg, \"On the error tolerance of iterative decoder circuitry,\" in Proc. 2008 Inf. Theory Appl. Workshop, Jan.\n2008.\n[40] L. Wei, \"Robustness of LDPC codes and internal noisy systems,\" in Proc. 41st Annu. Allerton Conf. Commun. Control Comput., Oct.\n2003, pp. 1665\u20131674.\n[41] L. Ping and W. K. Leung, \"Decoding low density parity check codes with finite quantization bits,\" IEEE Commun. Lett., vol. 4, no. 2,\npp. 62\u201364, Feb. 2000.\n[42] R. Singhal, G. S. Choi, and R. N. Mahapatra, \"Quantized LDPC decoder design for binary symmetric channels,\" in Proc. IEEE Int.\nSymp. Circuits Syst. (ISCAS 2005), vol. 6, May 2005, pp. 5782\u20135785.\n[43] N. Miladinovic and M. P. C. Fossorier, \"Improved bit-flipping decoding of low-density parity-check codes,\" IEEE Trans. Inf. Theory,\nvol. 51, no. 4, pp. 1594\u20131606, Apr. 2005.\n[44] J. Chen, A. Dholakia, E. Eleftheriou, M. P. C. Fossorier, and X.-Y. Hu, \"Reduced-complexity decoding of LDPC codes,\" IEEE Trans.\nCommun., vol. 53, no. 8, pp. 1288\u20131299, Aug. 2005.\n[45] J. Zhao, F. Zarkeshvari, and A. H. Banihashemi, \"On implementation of min-sum algorithm and its modifications for decoding lowdensity parity-check (LDPC) codes,\" IEEE Trans. Commun., vol. 53, no. 4, pp. 549\u2013554, Apr. 2005.\n[46] T. Yu, R.-S. Lin, B. Super, and B. Tang, \"Efficient message representations for belief propagation,\" in Proc. 11th IEEE Int. Conf.\nComputer Vision, Oct. 2007, pp. 1\u20138.\n[47] R. Motwani and P. Raghavan, Randomized Algorithms. Cambridge: Cambridge University Press, 1995.\n[48] S. T. Ribeiro, \"Random-pulse machines,\" IEEE Trans. Electron. Comput., vol. EC-16, no. 3, pp. 261\u2013276, Jun. 1967.\n[49] S. S. Tehrani, W. J. Gross, and S. Mannor, \"Stochastic decoding of LDPC codes,\" IEEE Commun. Lett., vol. 10, no. 10, pp. 716\u2013718,\nOct. 2006.\n[50] A. Sahai and S. Mitter, \"The necessity and sufficiency of anytime capacity for stabilization of a linear system over a noisy communication\nlink-Part I: Scalar systems,\" IEEE Trans. Inf. Theory, vol. 52, no. 8, pp. 3369\u20133395, Aug. 2006.\n[51] A. T. Ihler, J. W. Fisher, III, and A. S. Willsky, \"Loopy belief propagation: Convergence and effects of message errors,\" J. Mach.\nLearn. Res., vol. 6, pp. 905\u2013936, May 2005.\n[52] A. Ganti, A. Lapidoth, and \u0130. E. Telatar, \"Mismatched decoding revisited: General alphabets, channels with memory, and the wide-band\nlimit,\" IEEE Trans. Inf. Theory, vol. 46, no. 7, pp. 2315\u20132328, Nov. 2000.\n[53] H. Saeedi and A. H. Banihashemi, \"Performance of belief propagation for decoding LDPC codes in the presence of channel estimation\nerror,\" IEEE Trans. Commun., vol. 55, no. 1, pp. 83\u201389, Jan. 2007.\n[54] M. G. Taylor, \"Reliable information storage in memories designed from unreliable components,\" Bell Syst. Tech. J., vol. 47, no. 10,\npp. 2299\u20132337, Dec. 1968.\n[55] A. V. Kuznetsov, \"Information storage in a memory assembled from unreliable components,\" Probl. Inf. Transm., vol. 9, no. 3, pp.\n100\u2013114, July-Sept. 1973.\n[56] D. J. C. MacKay and R. M. Neal, \"Near Shannon limit performance of low density parity check codes,\" IEE Electron. Lett., vol. 33,\nno. 6, pp. 457\u2013458, Mar. 1997.\n[57] M. Sipser and D. A. Spielman, \"Expander codes,\" IEEE Trans. Inf. Theory, vol. 42, no. 6, pp. 1710\u20131722, Nov. 1996.\n[58] D. Burshtein and G. Miller, \"Expander graph arguments for message-passing algorithms,\" IEEE Trans. Inf. Theory, vol. 47, no. 2, pp.\n782\u2013790, Feb. 2001.\n[59] S. ten Brink, \"Convergence of iterative decoding,\" IEE Electron. Lett., vol. 35, no. 10, pp. 806\u2013808, May 1999.\n[60] M. Ardakani and F. R. Kschischang, \"A more accurate one-dimensional analysis and design of irregular LDPC codes,\" IEEE Trans.\nCommun., vol. 52, no. 12, pp. 2106\u20132114, Dec. 2004.\n\n\f25\n\n[61] M. G. Luby, M. Mitzenmacher, M. A. Shokrollahi, and D. A. Spielman, \"Improved low-density parity-check codes using irregular\ngraphs,\" IEEE Trans. Inf. Theory, vol. 47, no. 2, pp. 585\u2013598, Feb. 2001.\n[62] T. Richardson and R. Urbanke, \"Fixed points and stability of density evolution,\" Commun. Inf. Syst., vol. 4, no. 1, pp. 103\u2013116, Sep.\n2004.\n[63] S. K. Chilappagari and B. Vasic, \"Fault tolerant memories based on expander graphs,\" in Proc. IEEE Inf. Theory Workshop (ITW'07),\nSep. 2007, pp. 126\u2013131.\n[64] L. R. Varshney, \"Performance of LDPC codes under noisy message-passing decoding,\" in Proc. IEEE Inf. Theory Workshop (ITW'07),\nSep. 2007, pp. 178\u2013183.\n[65] T. Richardson and R. Urbanke, Modern Coding Theory. Cambridge: Cambridge University Press, 2008.\n[66] T. R. Halford and K. M. Chugg, \"The extraction and complexity limits of graphical models for linear codes,\" IEEE Trans. Inf. Theory,\nvol. 54, no. 9, pp. 3884\u20133906, Sep. 2008.\n[67] M. Lentmaier, D. V. Truhachev, K. S. Zigangirov, and D. J. Costello, Jr., \"An analysis of the block error probability performance of\niterative decoding,\" IEEE Trans. Inf. Theory, vol. 51, no. 11, pp. 3834\u20133855, Nov. 2005.\n[68] V. Rathi and R. Urbanke, \"Density evolution, thresholds and the stability condition for non-binary LDPC codes,\" IEE Proc. Commun.,\nvol. 152, no. 6, pp. 1069\u20131074, Dec. 2005.\n[69] Y. Weiss, \"Correctness of local probability propagation in graphical models with loops,\" Neural Comput., vol. 12, no. 1, pp. 1\u201341, Jan.\n2000.\n[70] P. M. Woodward, Probability and Information Theory, With Applications to Radar. New York: McGraw-Hill, 1953.\n[71] D. Slepian, \"The threshold effect in modulation systems that expand bandwidth,\" IRE Trans. Inf. Theory, vol. IT-8, no. 5, pp. 122\u2013127,\nSep. 1962.\n[72] J. M. Wozencraft and I. M. Jacobs, Principles of Communication Engineering. New York: John Wiley & Sons, 1965.\n[73] V. Erokhin, \"\u03b5-entropy of a discrete random variable,\" Theory Probab. Appl., vol. 3, no. 1, pp. 97\u2013100, 1958.\n[74] L. Bazzi, T. J. Richardson, and R. L. Urbanke, \"Exact thresholds and optimal codes for the binary-symmetric channel and Gallager's\ndecoding algorithm A,\" IEEE Trans. Inf. Theory, vol. 50, no. 9, pp. 2010\u20132021, Sep. 2004.\n[75] R. L. Dobrushin and S. I. Ortyukov, \"Lower bound for the redundancy of self-correcting arrangements of unreliable functional elements,\"\nProbl. Inf. Transm., vol. 13, no. 1, pp. 82\u201389, Jan.-Mar. 1977.\n[76] L. Yang, \"Recent advances on determining the number of real roots of parametric polynomials,\" J. Symbol. Comput., vol. 28, no. 1-2,\npp. 225\u2013242, Jul. 1999.\n[77] T. J. Richardson, M. A. Shokrollahi, and R. L. Urbanke, \"Design of capacity-approaching irregular low-density parity-check codes,\"\nIEEE Trans. Inf. Theory, vol. 47, no. 2, pp. 619\u2013637, Feb. 2001.\n[78] S.-Y. Chung, T. J. Richardson, and R. L. Urbanke, \"Analysis of sum-product decoding of low-density parity-check codes using a\nGaussian approximation,\" IEEE Trans. Inf. Theory, vol. 47, no. 2, pp. 657\u2013670, Feb. 2001.\n[79] S. K. Chilappagari, B. Vasic, and M. Marcellin, \"Can the storage capacity of memories built from unreliable components be determined?\"\nin Proc. 2008 Inf. Theory Appl. Workshop, Jan.-Feb. 2008, pp. 41\u201343.\n[80] A. G. Dimakis and K. Ramchandran, \"Network coding for distributed storage in wireless networks,\" in Networked Sensing Information\nand Control, V. Saligrama, Ed. New York: Springer, 2008, pp. 115\u2013136.\n[81] A. Montanari, \"Tight bounds for LDPC and LDGM codes under MAP decoding,\" IEEE Trans. Inf. Theory, vol. 51, no. 9, pp. 3221\u20133246,\nSep. 2005.\n[82] J. G. Tryon, \"Quadded logic,\" in Redundancy Techniques for Computing Systems, R. H. Wilcox and W. C. Mann, Eds. Washington:\nSpartan Books, 1962, pp. 205\u2013228.\n[83] J. B. Gao, Y. Qi, and J. A. B. Fortes, \"Bifurcations and fundamental error bounds for fault-tolerant computations,\" IEEE Trans.\nNanotechnol., vol. 4, no. 4, pp. 395\u2013402, Jul. 2005.\n[84] S. H. Nawab, A. V. Oppenheim, A. P. Chandrakasan, J. M. Winograd, and J. T. Ludwig, \"Approximate signal processing,\" J. VLSI\nSignal Process., vol. 15, no. 1-2, pp. 177\u2013200, Jan. 1997.\n[85] A. Sinha, A. Wang, and A. Chandrakasan, \"Energy scalable system design,\" IEEE Trans. VLSI Syst., vol. 10, no. 2, pp. 135\u2013145, Apr.\n2002.\n[86] M. Bhardwaj, R. Min, and A. P. Chandrakasan, \"Quantifying and enhancing power awareness of VLSI systems,\" IEEE Trans. VLSI\nSyst., vol. 9, no. 6, pp. 757\u2013772, Dec. 2001.\n[87] L. Wang and N. R. Shanbhag, \"Energy-efficiency bounds for deep submicron VLSI systems in the presence of noise,\" IEEE Trans.\nVLSI Syst., vol. 11, no. 2, pp. 254\u2013269, Apr. 2003.\n[88] G. Masera, M. Mazza, G. Piccinini, F. Viglione, and M. Zamboni, \"Architectural strategies for low-power VLSI turbo decoders,\" IEEE\nTrans. VLSI Syst., vol. 10, no. 3, pp. 279\u2013285, Jun. 2002.\n[89] M. M. Mansour and N. R. Shanbhag, \"A 640-Mb/s 2048-bit programmable LDPC decoder chip,\" IEEE J. Solid-State Circuits, vol. 41,\nno. 3, pp. 684\u2013698, Mar. 2006.\n[90] A. Sahai and P. Grover, \"The price of certainty: \"waterslide curves\" and the gap to capacity,\" EECS Department, University of California,\nBerkeley, Tech. Rep. UCB/EECS-2008-1, Jan. 2008.\n[91] A. Maheshwari, W. Burleson, and R. Tessier, \"Trading off transient fault tolerance and power consumption in deep submicron (DSM)\nVLSI circuits,\" IEEE Trans. VLSI Syst., vol. 12, no. 3, pp. 299\u2013311, Mar. 2004.\n[92] B. J. Copeland, \"Hypercomputation,\" Minds Mach., vol. 12, no. 4, pp. 461\u2013502, Nov. 2002.\n[93] D. Williams, Probability with Martingales. Cambridge: Cambridge University Press, 1991.\n[94] K. Azuma, \"Weighted sums of certain dependent random variables,\" Tohoku Math. J., vol. 19, no. 3, pp. 357\u2013367, 1967.\n[95] W. Hoeffding, \"Probability inequalities for sums of bounded random variables,\" J. Am. Stat. Assoc., vol. 58, no. 301, pp. 13\u201330, Mar.\n1963.\n\n\f"}