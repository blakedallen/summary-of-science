{"id": "http://arxiv.org/abs/0712.4382v1", "guidislink": true, "updated": "2007-12-28T18:08:37Z", "updated_parsed": [2007, 12, 28, 18, 8, 37, 4, 362, 0], "published": "2007-12-28T18:08:37Z", "published_parsed": [2007, 12, 28, 18, 8, 37, 4, 362, 0], "title": "Information and fitness", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0712.3919%2C0712.0819%2C0712.0994%2C0712.4125%2C0712.3111%2C0712.3070%2C0712.2007%2C0712.2136%2C0712.2145%2C0712.0109%2C0712.0157%2C0712.1243%2C0712.0617%2C0712.2578%2C0712.1692%2C0712.1624%2C0712.0433%2C0712.2543%2C0712.1053%2C0712.2885%2C0712.0408%2C0712.2477%2C0712.4092%2C0712.4303%2C0712.1672%2C0712.4010%2C0712.1905%2C0712.0961%2C0712.0700%2C0712.1558%2C0712.3841%2C0712.1940%2C0712.2949%2C0712.1133%2C0712.0567%2C0712.1782%2C0712.3054%2C0712.3808%2C0712.2426%2C0712.0058%2C0712.2053%2C0712.1148%2C0712.1871%2C0712.1077%2C0712.1814%2C0712.4255%2C0712.0315%2C0712.0805%2C0712.4124%2C0712.0979%2C0712.1908%2C0712.0701%2C0712.0501%2C0712.3719%2C0712.2574%2C0712.3043%2C0712.1071%2C0712.0824%2C0712.0419%2C0712.1125%2C0712.0015%2C0712.1169%2C0712.0559%2C0712.2087%2C0712.2699%2C0712.4382%2C0712.0629%2C0712.1999%2C0712.1358%2C0712.0841%2C0712.1339%2C0712.2672%2C0712.2150%2C0712.0370%2C0712.1222%2C0712.1586%2C0712.2965%2C0712.3236%2C0712.1562%2C0712.0287%2C0712.3018%2C0712.3788%2C0712.2925%2C0712.2113%2C0712.0232%2C0712.1186%2C0712.1625%2C0712.2700%2C0712.0749%2C0712.1016%2C0712.2258%2C0712.0625%2C0712.0400%2C0712.2156%2C0712.2728%2C0712.0971%2C0712.3073%2C0712.2169%2C0712.3710%2C0712.1245%2C0712.2392&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Information and fitness"}, "summary": "The growth rate of organisms depends both on external conditions and on\ninternal states, such as the expression levels of various genes. We show that\nto achieve a criterion mean growth rate over an ensemble of conditions, the\ninternal variables must carry a minimum number of bits of information about\nthose conditions. Evolutionary competition thus can select for cellular\nmechanisms that are more efficient in an abstract, information theoretic sense.\nEstimates based on recent experiments suggest that the minimum information\nrequired for reasonable growth rates is close to the maximum information that\ncan be conveyed through biologically realistic regulatory mechanisms. These\nideas are applicable most directly to unicellular organisms, but there are\nanalogies to problems in higher organisms, and we suggest new experiments for\nboth cases.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0712.3919%2C0712.0819%2C0712.0994%2C0712.4125%2C0712.3111%2C0712.3070%2C0712.2007%2C0712.2136%2C0712.2145%2C0712.0109%2C0712.0157%2C0712.1243%2C0712.0617%2C0712.2578%2C0712.1692%2C0712.1624%2C0712.0433%2C0712.2543%2C0712.1053%2C0712.2885%2C0712.0408%2C0712.2477%2C0712.4092%2C0712.4303%2C0712.1672%2C0712.4010%2C0712.1905%2C0712.0961%2C0712.0700%2C0712.1558%2C0712.3841%2C0712.1940%2C0712.2949%2C0712.1133%2C0712.0567%2C0712.1782%2C0712.3054%2C0712.3808%2C0712.2426%2C0712.0058%2C0712.2053%2C0712.1148%2C0712.1871%2C0712.1077%2C0712.1814%2C0712.4255%2C0712.0315%2C0712.0805%2C0712.4124%2C0712.0979%2C0712.1908%2C0712.0701%2C0712.0501%2C0712.3719%2C0712.2574%2C0712.3043%2C0712.1071%2C0712.0824%2C0712.0419%2C0712.1125%2C0712.0015%2C0712.1169%2C0712.0559%2C0712.2087%2C0712.2699%2C0712.4382%2C0712.0629%2C0712.1999%2C0712.1358%2C0712.0841%2C0712.1339%2C0712.2672%2C0712.2150%2C0712.0370%2C0712.1222%2C0712.1586%2C0712.2965%2C0712.3236%2C0712.1562%2C0712.0287%2C0712.3018%2C0712.3788%2C0712.2925%2C0712.2113%2C0712.0232%2C0712.1186%2C0712.1625%2C0712.2700%2C0712.0749%2C0712.1016%2C0712.2258%2C0712.0625%2C0712.0400%2C0712.2156%2C0712.2728%2C0712.0971%2C0712.3073%2C0712.2169%2C0712.3710%2C0712.1245%2C0712.2392&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The growth rate of organisms depends both on external conditions and on\ninternal states, such as the expression levels of various genes. We show that\nto achieve a criterion mean growth rate over an ensemble of conditions, the\ninternal variables must carry a minimum number of bits of information about\nthose conditions. Evolutionary competition thus can select for cellular\nmechanisms that are more efficient in an abstract, information theoretic sense.\nEstimates based on recent experiments suggest that the minimum information\nrequired for reasonable growth rates is close to the maximum information that\ncan be conveyed through biologically realistic regulatory mechanisms. These\nideas are applicable most directly to unicellular organisms, but there are\nanalogies to problems in higher organisms, and we suggest new experiments for\nboth cases."}, "authors": ["Samuel F. Taylor", "Naftali Tishby", "William Bialek"], "author_detail": {"name": "William Bialek"}, "author": "William Bialek", "links": [{"href": "http://arxiv.org/abs/0712.4382v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0712.4382v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "q-bio.PE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "q-bio.PE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0712.4382v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0712.4382v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Information and fitness\nSamuel F. Taylor,a Naftali Tishbyb and William Bialeka,c\nJoseph Henry Laboratories of Physics, a Lewis\u2013Sigler Institute for Integrative Genomics,\nand c Princeton Center for Theoretical Physics, Princeton University, Princeton, NJ 08544\nb\nSchool of Computer Science and Engineering and Interdisciplinary Center\nfor Neural Computation Hebrew University, Jerusalem 91904, Israel\n(Dated: October 23, 2018)\n\narXiv:0712.4382v1 [q-bio.PE] 28 Dec 2007\n\na\n\nThe growth rate of organisms depends both on external conditions and on internal states, such as\nthe expression levels of various genes. We show that to achieve a criterion mean growth rate over an\nensemble of conditions, the internal variables must carry a minimum number of bits of information\nabout those conditions. Evolutionary competition thus can select for cellular mechanisms that are\nmore efficient in an abstract, information theoretic sense. Estimates based on recent experiments\nsuggest that the minimum information required for reasonable growth rates is close to the maximum\ninformation that can be conveyed through biologically realistic regulatory mechanisms. These ideas\nare applicable most directly to unicellular organisms, but there are analogies to problems in higher\norganisms, and we suggest new experiments for both cases.\n\nSince Shannon's original work [1, 2] there has been the\nhope that information theory would provide not only a\nguide to the design of engineered communication systems\nbut also a framework for understanding information processing in biological systems [3, 4, 5, 6, 7]. But there\nare (at least) two major obstacles in the way of any effort to use information theoretic ideas in the analysis of\nbiological systems. First, Shannon's formulation of information theory has no place for the value or meaning\nof the information [8], yet surely organisms find some\nbits more valuable than others. Second, it is difficult\nto imagine that evolution can select for abstract quantities such as the number of bits that an organism extracts\nfrom its environment. Both of these problems point away\nfrom general mathematical structures toward biological\ndetails such as the fitness or adaptive value of particular\nactions, the costs of particular errors, and the resources\nneeded to carry out specific computations.\nThe question of whether abstract information theoretic quantities can be connected to concrete costs and\nbenefits is not new, nor is it specific to the biological\ncontext. Fifty years ago, Kelly asked whether Shannon's\ndefinition of information has a meaning outside the standard model of communication, and he showed that in\nsimple models of gambling the rate at which one's winnings accumulate is bounded by the information (in bits)\nthat one has about the outcome of the game [9]. Kelly's\nresults generalize to the slightly more dignified setting of\nportfolio management [10], and closely related ideas have\nemerged recently in thinking about phenotypic switching\nin bacteria [11, 12]. What these examples have in common is that the benefit or growth (of investments, or of\na bacterial population) is a linear function of the control\nparameters (the fraction of the portfolio invested in each\nstock, or the fraction of organisms adopting a particular\nphenotype). This linear framework is too restrictive, but\nKelly's classical results encourage us to think that there\nmay be some more general relationship between the in-\n\nformation that an organism has about its environment\nand its growth rate or fitness.\nTo be concrete, we consider single celled organisms\nin quasi\u2013static environments, and discuss generalizations\nbelow. A bacterium lives in an environment described\nby a set of variables ~s \u2261 s1 , s2 , * * * , sK ; in the simplest\ncase, just one relevant environmental variable s might\nspecify the concentration of some limiting nutrient. The\nfitness of the organism does not depend just on these environmental variables, but also on internal variables such\nas the expression levels of different enzymes involved in\nthe metabolism of the available nutrients. Let's refer to\nthese variables as ~g \u2261 g1 , g2 , * * * , gD , and then the fitness\nof any particular organism in its environment is defined\nby some function f (~g , ~s) [13]. This fitness function could\nbe complicated-there are benefits to be gained from\nmetabolizing particular nutrients, but achieving these\nbenefits requires the appropriate expression levels of the\nrelevant enzymes, and the expression of the proteins is\nitself a cost that lowers fitness. Recent experiments attempt to map these different factors for the case of the\nlac operon in E coli [14], resulting in an estimate of the\nfitness as a function of the environmental concentration\nof lactose (s) and the expression level of the lac proteins\n(g), shown in Fig 1. The important point is perhaps not\nthe detailed form found in particular experiments, but\nthat we can imagine writing the fitness as depending on\na combination of environmental and internal variables.\nFor any given set of environmental conditions there is\nsome setting of the internal variables that provides for\nthe maximum fitness. If the organism could find this\noptimal operating point, then its internal state would be\nperfectly matched to the state of the environment. Even\nif the system does not find this optimum, we can still\nthink of the internal state as representing what the organism \"knows\" about the environmental variables. To\nquantify this knowledge, we imagine that the organism\nwill encounter, over its lifetime, a distribution P (~s) of\n\n\f2\n\nFIG. 1: Growth rate of E coli as a function of external sugar\n(lactose) concentration and the expression level of one gene\n(lacZ), as estimated in Ref [14]. and summarized in their\nEq (5). Fitness is measured as a fractional difference from\nthe growth rate when both the lactose concentration and lacZ\nexpression levels are zero. Sugar concentration is measured in\nunits such that the half maximal benefit is reached at s = 1,\nand expression level is measured in units of the maximum\nthat the cell can maintain. White line traces the optimal\nexpression level for each value of s.\n\nThis is a linear function of the conditional distribution\nP (~g |~s), while the information I(~g ; ~s) is a convex function\nof the conditional distribution [10]. Thus, if we consider\nall conditional distributions that lead to the same average fitness, then there is one which corresponds to the\nminimum amount of information; cf Fig 2. The relationship between this minimal information and the mean fitness, Imin (hf i), is analogous to the rate\u2013distortion function in communication theory [10]. We can also phrase\nthis relation as f \u0304max (I), the maximum mean fitness that\ncan be achieved given a certain amount of information.\nThe existence of the function Imin (hf i) means that if\norganisms are to achieve a certain average level of fitness\nas they experience different environments, then the internal state of the organism ~g must provide a minimum\namount of information about the relevant variables in\nthe environment. In this precise sense, achieving a criterion level of fitness requires a minimum number of bits.\nIf evolution selects for greater fitness-as it does, almost\nby definition-then this selection continually raises the\nminimum number of bits that organisms need to represent about their environment. Contrary to a widely held\nintuition, then, evolution does select for an abstract, information theoretic property.\nThe optimization problem in which we minimize the\ninformation I(~g ; ~s) at some fixed average fitness hf i has\n\n0.02\n\nThe question is how this information content of the internal states relates to the fitness.\nGiven the joint distribution of internal and external\nstates, P (~g , ~s), the average fitness over the organisms'\nexperience in a distribution of environments is\nZ\nZ\nhf i =\ndK s dD g P (~g , ~s)f (~g , ~s)\n(3)\nZ\nZ\n=\ndK sP (~s) dD g P (~g |~s)f (~g , ~s).\n(4)\n\n0.018\n\nforbidden\n0.016\n\n0.014\n\nmean fitness\n\nenvironmental conditions. Given the state of the environment, organisms will adjust their internal state as\nbest they can, but unless this process were (implausibly)\nnoiseless, the result of the adjustment will be that the\ninternal states are drawn from some probability dsitribution P (~g |~s). Thus if we were to take a snapshot, we\nwould find individual cells with internal states ~g and\ntheir environments ~s drawn from the joint probability\ndistribution P (~g , ~s) = P (~g |~s)P (~s). Shannon then tells\nus that the internal state ~g provides information about\nthe environment ~s, and this information is\n\u0014\n\u0015\nZ\nP (~g |~s)\nbits, (1)\nI(~g ; ~s) =\ndK s dD g P (~g , ~s) log2\nP (~g )\nZ\nP (~g ) =\ndK s P (~g |~s)P (~s).\n(2)\n\n0.012\n\n0.01\n\nallowed region of\nfitness/information plane\n\n0.008\n\n0.006\n\n0.004\n\n0.002\n\n0\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\n2\n\ninformation I(g;s) (bits)\n\nFIG. 2: Imagine mechanisms that tune the internal state of\nthe organism in response to the state of the environment.\nEach possible mechanism achieves a certain average fitness\nover the lifetime of the organism. Depending on the precision of these mechanisms, the internal state will provide some\namount of information (in bits) about the state of environment. Thus, each possible mechanism corresponds to a point\nin the plane relating the mean fitness hf i to the information\nI(~g ; ~s). Not all points in this plane are physically possible:\nthere is a curve Imin (hf i) that separates the allowed from the\ndisallowed possibilities. The example shown here is calculated for the fitness function in Fig 1, with a distribution of\nexternal states P (s) \u221d exp(\u22122s).\n\n\f3\na simple formal solution,\nP (~g |~s) =\n\nP (~g ) \u03bbf (~g,~s)\ne\n,\nZ(~s)\n\n(5)\n\nwhere \u03bb is a Lagrange multiplier that fixes the average\nfitness, and Z(~s) serves to normalize each of the distributions P (~g |~s). The exponential of the fitness reminds\nus of the Boltzmann distribution, and one can think of\n\u03bb as being like an inverse temperature in statistical mechanics, biasing the distributions toward expression levels that insure higher fitness (lower energy) when \u03bb is\nlarger (temperature is lower). Equation (5) doesn't completely solve the problem because one must enforce consistency between P (~g ) on the right hand side and P (~g |~s)\non the left; that is, one must satisfy both Eq's (5) and\n(2). Fortunately these two equations can be combined\ninto an iterative algorithm that converges [15].\nIn Fig 2 we show the results of a numerical compuation\nof the function f \u0304max (I), based on the fitness function in\nFig 1. The precise results depend on the choice of the\ndistribution of environmental conditions P (s), but we\nhave found that the scale and basic form of the function\nf \u0304max (I) are relatively robust so long as the distribution\nspans the range of sugar concentrations over which the\noptimal expressions levels actually vary.\nThe rate\u2013distortion, or information\u2013fitness function\nImin (hf i) is rather smooth and featureless. Nonetheless,\ncareful examination of the results illustrates several different points. A significant fitness advantage (\u223c 1%,\ncompared with the maximum possible 1.6% across this\nensemble of conditions) can be obtained by adjusting\nthe gene expression level in ways that carry almost no\ninformation about the external world. The distribution\nof expression levels under these conditions is still quite\nbroad, corresponding to a population of organisms that\nuses (continuous) phenotypic diversity to survive under\na range of conditions, but without a tight regulatory\nmechanism that links phenotype to the external world.\nAn even larger fraction of the available fitness advantage is accessible through mechanisms that use relatively\nlittle information, less than one bit. On the other hand,\nsqueezing out the last \u223c 0.1% of fitness advantage requires pushing well past one bit of information. Put another way, organisms that could only implement a true\nswitch\u2013like control, in which expression is only \"on\" or\n\"off,\" would be at a small but measurable disadvantage\nin growth rate when averaged over a wide range of conditions. Organisms that have access to more than one\nbit of information thus could out\u2013compete their one\u2013bit\ncousins over thousands of generations.\nWe note that results based on the fitness function measured in Ref [14] are necessarily conservative. Under\nthe conditions studied in those experiments, an infinite\nsupply of the external sugar leads only to a \u223c 10% advantage in growth rate over the case where the sugar is\n\ncompletely absent. If we were to consider the case of\na truly limiting nutrient, the overall scale of the fitness\nvariations, and hence the curve f \u0304max (I), would thus be\nnearly ten times larger. Thus, the difference between one\nbit and two bits would be \u223c 1% in growth rate, which\ncan be selected for on quite short time scales.\nThe scale of the information\u2013fitness function also is\ninteresting in comparison to what we know about the\nperformance of real regulatory mechanisms [16, 17]. We\nrecall that, because expression levels have a limited dynamic range and a finite amount of noise even under\nfixed conditions, the capacity of the expression level\nto convey information (about anything) is bounded.\nWith realistic parameters, based on recent experiments\n[18, 19, 20, 21, 22, 23, 24], this capacity is less than three\nbits and more typically less than two bits [16]. Although\nwe don't have enough data to reach a firm conclusion,\nthese results certainly motivate the conjecture that the\nminimum information required to reach reasonable levels of fitness is close to the maximum information that\ncan be passed through real genetic regulatory elements.\nThe precise form of the information\u2013fitness function\ndepends on the function f (~g , ~s), but the asymptotic behavior at high mean fitness is more nearly universal. We\ncan reach this limit by considering Eq (5) as the parameter \u03bb becomes large. Then the distribution of expression levels becomes sharply peaked around the optimum\n~gopt (~s) for each set of external conditions; the form of\nthis distribution becomes approximately Gaussian with a\nwidth inversely proportional to \u03bb. Taking this Gaussian\napproximation seriously, it is straightforward to compute\nthe information and the mean fitness; we find\n\u0014\n\u0015\nDhf imax\nD\n+ * * * , (6)\nImin (hf i) = I0 + log2\n2\n2(hf imax \u2212 hf i)\nwhere I0 is a constant independent of the mean fitness,\nhf imax is the maximum mean fitness obtainable by an\norganism that has perfect information about its environment, and * * * denotes terms which become relevant\nat lower fitness. The details of the function f (~g , ~s) are\nburied in the constant I0 , but the way in which the minimum information grows as the organism approaches its\nmaximal mean fitness depends on the number of genes\nD the cell has to control, independent of details.\nWe have assumed, for simplicity, that variations are\nslow, so we can write the fitness as a function of internal\nand external states at the same instant of time. A more\nrealistic analysis would take account of the fact that current values of internal control variables interact with external conditions in the future, so that the information\nwhich controls the achievable level of fitness is predictive\ninformation [25, 26]. We can also generalize to consider\nbehaviors in complex multi\u2013cellular organisms; the analog of the information\u2013fitness relation then states that\nbehaviors which collect some criterion level of reward\n\n\f4\nacross an ensemble of conditions must be guided by neural representations which carry a minimum amount of\ninformation about these conditions.\nIt is possible to measure, in real time, both growth\nrates and expression levels of particular genes in individual unicellular organisms [27]. Repeating such experiments under varying external conditions should allow\nestimates of the fitness function f (~g , ~s) with single cell\nresolution, the mean growth rate under given conditions,\nand the mutual information between internal and external variables. Thus we could locate the organism's performance in the information\u2013fitness plane of Fig 2, and\nalso see how close it comes to the limiting curve f \u0304max (I).\nFor neural systems, if we have a motor control task in\nwhich there is a good model of the underlying mechanics\n[28], analogous experiments would compare the information available in central neural representations with the\nminimum required to achieve observed levels of reward\nunder variable conditions.\nTo summarize, achieving a criterion level of fitness or\nreward across a distribution of conditions always requires\nan internal representation of the world that captures\nsome minimum number of bits. Qualitatively, this means\nthat evolutionary competition will drive an increase in\nthis information capacity. Quantitatively, in the case\nof unicellular organisms, the minimum information required for reasonable levels of fitness is close to the maximal information that can be transmitted through known\ngenetic regulatory mechanisms. Finally, this general picture suggests experiments which could map the information/fitness tradeoff in a wider variety of systems, and\nlocate the performance of real organisms in relation to\nthe information theoretic optimum.\nThis work was supported in part by NSF Grants\nIIS\u20130423039 and PHY\u20130650617, by NIH Grant P50\nGM071508, and by the Swartz Foundation.\n\n[1] CE Shannon, A mathematical theory of communication.\nBell Sys Tech J 27, 379\u2013423 & 623\u2013656 (1948).\n[2] CE Shannon, Communication in the presence of noise.\nProc IRE 37, 10\u201321 (1949).\n[3] F Attneave, Some informational aspects of visual perception. Psych Rev 61, 183\u2013193 (1954).\n[4] HB Barlow, Possible principles underlying the transformation of sensory messages. In Sensory Communication,\nW Rosenblith, ed, pp 217\u2013234 (MIT Press, Cambridge,\n1961).\n[5] F Rieke, D Warland, R de Ruyter van Steveninck & W\nBialek, Spikes: Exploring the Neural Code. (MIT Press,\nCambridge, 1997).\n[6] E Ziv, I Nemenman & C Wiggins, Optimal signal processing in small stochastic biochemical networks. PLoS\nOne 2, e1077 (2007); arXiv:q\u2013bio.MN/0612041.\n[7] G Tka\u010dik, CG Callan, Jr & W Bialek, Information\nflow and optimization in transcriptional regulation.\n\narXiv:0705.0313 [q\u2013bio.MN] (2007).\n[8] This is not an accident; on the first page of Ref [1], Shannon remarked (italics in the original): \"Frequently the\nmessages have meaning; that is they refer to or are correlated according to some system with certain physical or\nconceptual entities. These semantic aspects of the communication are irrelevant to the engineering problem.\"\n[9] JL Kelly, Jr, A new interpretation of information rate.\nBell Sys Tech J 35, 917\u2013926 (1956).\n[10] TM Cover & JA Thomas, Elements of Information Theory (John Wiley & Sons, New York, 1991).\n[11] CT Bergstrom & M Lachmann, The fitness value of information. axXiv:q\u2013bio.PE/0510007 (2005).\n[12] E Kussell & S Leibler, Phenotypic diversity, population\ngrowth, and information in fluctuating environments.\nScience 309, 2075\u20132078 (2005).\n[13] Strictly speaking we should distinguish between the\ngrowth rate of individuals and their fitness relative to\nother organisms in the same environment, but we'll use\n\"growth rate\" and \"fitness\" as synonyms for simplicity.\n[14] E Dekel & U Alon, Optimality and evolutionary tuning\nof the expression level of a protein. Nature 436, 588\u2013592\n(2005).\n[15] RE Blahut, Computation of channel capacity and rate\ndistortion functions. IEEE Trans Info Thy 4, 460\u2013473\n(1972).\n[16] G Tka\u010dik, CG Callan, Jr & W Bialek, Information capacity of genetic regulatory elements. arXiv:0709.4209\n[q\u2013bio.MN] (2007).\n[17] Information could be also acquired passively, since organisms with the 'correct' internal states will grow faster.\nIt seems difficult to make general statements about this\neffect, which depends on the dynamics with which different states are explored.\n[18] MB Elowitz, AJ Levine, ED Siggia & PD Swain,\nStochastic gene expression in a single cell. Science 207,\n1183\u20131186 (2002).\n[19] E Ozbudak et al, Regulation of noise in the expression\nof a single gene. Nature Gen 31, 69\u201373 (2002).\n[20] WJ Blake, M Kaern, CR Cantor & JJ Collins, Noise in\neukaryotic gene expression. Nature 422, 633\u2013637 (2003).\n[21] JM Raser & EK O'Shea, Control of stochasticity in eukaryotic gene expression. Science 304, 1811\u20131814 (2004).\n[22] N Rosenfeld et al, Gene regulation at the single cell level.\nScience 307, 1962\u20131965 (2005).\n[23] T Gregor, DW Tank, EF Wieschaus & W Bialek, Probing the limits to positional information. Cell 130, 153\u2013\n164 (2007).\n[24] G Tka\u010dik, T Gregor & W Bialek, The role of input noise\nin transcriptional regulation. arXiv:q\u2013bio.MN/0701002\n(2007).\n[25] W Bialek, I Nemenman & N Tishby, Predictability,\ncomplexity and learning. Neural Comp 13, 2409\u20132463\n(2001); arXiv:physics/0007070.\n[26] W Bialek, RR de Ruyter van Steveninck & N Tishby, Efficient representation as a design principle for neural coding and computation. Proc Int Symp Info Theory (2006).\n[27] See, for example, S DiTalia et al, The effects of molecular\nnoise and size control on variability in the budding yeast\ncell cycle. Nature 446, 947\u2013951 (2007).\n[28] For examples see the discussion by CM Harris & DM\nWolpert, Signal\u2013dependent noise determines motor planning. Nature 394, 780\u2013784 (1998).\n\n\f"}