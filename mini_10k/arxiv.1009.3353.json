{"id": "http://arxiv.org/abs/1009.3353v1", "guidislink": true, "updated": "2010-09-17T07:38:37Z", "updated_parsed": [2010, 9, 17, 7, 38, 37, 4, 260, 0], "published": "2010-09-17T07:38:37Z", "published_parsed": [2010, 9, 17, 7, 38, 37, 4, 260, 0], "title": "A Lower Bound on the Estimator Variance for the Sparse Linear Model", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1009.3449%2C1009.5918%2C1009.2705%2C1009.4885%2C1009.1217%2C1009.2444%2C1009.3299%2C1009.4055%2C1009.0494%2C1009.4166%2C1009.1588%2C1009.4588%2C1009.0651%2C1009.0857%2C1009.3353%2C1009.3884%2C1009.1640%2C1009.4117%2C1009.0261%2C1009.3050%2C1009.3944%2C1009.4920%2C1009.4391%2C1009.6076%2C1009.6071%2C1009.5397%2C1009.0222%2C1009.0705%2C1009.5916%2C1009.4786%2C1009.0739%2C1009.5129%2C1009.5615%2C1009.0520%2C1009.5474%2C1009.4563%2C1009.1618%2C1009.4734%2C1009.4598%2C1009.1359%2C1009.2659%2C1009.0636%2C1009.1303%2C1009.1039%2C1009.4946%2C1009.4636%2C1009.2023%2C1009.0231%2C1009.2709%2C1009.3271%2C1009.5204%2C1009.4641%2C1009.4330%2C1009.5581%2C1009.4159%2C1009.4426%2C1009.1102%2C1009.0533%2C1009.3251%2C1009.5549%2C1009.4254%2C1009.4348%2C1009.5203%2C1009.3620%2C1009.3307%2C1009.2860%2C1009.4590%2C1009.5260%2C1009.5797%2C1009.4674%2C1009.5381%2C1009.4512%2C1009.3934%2C1009.6137%2C1009.1360%2C1009.1795%2C1009.5406%2C1009.5059%2C1009.3102%2C1009.3768%2C1009.2760%2C1009.1381%2C1009.2165%2C1009.1607%2C1009.4998%2C1009.1691%2C1009.5478%2C1009.5353%2C1009.3797%2C1009.1774%2C1009.1275%2C1009.5004%2C1009.5983%2C1009.1489%2C1009.2970%2C1009.6059%2C1009.5632%2C1009.5978%2C1009.3649%2C1009.4293%2C1009.0373&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Lower Bound on the Estimator Variance for the Sparse Linear Model"}, "summary": "We study the performance of estimators of a sparse nonrandom vector based on\nan observation which is linearly transformed and corrupted by additive white\nGaussian noise. Using the reproducing kernel Hilbert space framework, we derive\na new lower bound on the estimator variance for a given differentiable bias\nfunction (including the unbiased case) and an almost arbitrary transformation\nmatrix (including the underdetermined case considered in compressed sensing\ntheory). For the special case of a sparse vector corrupted by white Gaussian\nnoise-i.e., without a linear transformation-and unbiased estimation, our lower\nbound improves on previously proposed bounds.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1009.3449%2C1009.5918%2C1009.2705%2C1009.4885%2C1009.1217%2C1009.2444%2C1009.3299%2C1009.4055%2C1009.0494%2C1009.4166%2C1009.1588%2C1009.4588%2C1009.0651%2C1009.0857%2C1009.3353%2C1009.3884%2C1009.1640%2C1009.4117%2C1009.0261%2C1009.3050%2C1009.3944%2C1009.4920%2C1009.4391%2C1009.6076%2C1009.6071%2C1009.5397%2C1009.0222%2C1009.0705%2C1009.5916%2C1009.4786%2C1009.0739%2C1009.5129%2C1009.5615%2C1009.0520%2C1009.5474%2C1009.4563%2C1009.1618%2C1009.4734%2C1009.4598%2C1009.1359%2C1009.2659%2C1009.0636%2C1009.1303%2C1009.1039%2C1009.4946%2C1009.4636%2C1009.2023%2C1009.0231%2C1009.2709%2C1009.3271%2C1009.5204%2C1009.4641%2C1009.4330%2C1009.5581%2C1009.4159%2C1009.4426%2C1009.1102%2C1009.0533%2C1009.3251%2C1009.5549%2C1009.4254%2C1009.4348%2C1009.5203%2C1009.3620%2C1009.3307%2C1009.2860%2C1009.4590%2C1009.5260%2C1009.5797%2C1009.4674%2C1009.5381%2C1009.4512%2C1009.3934%2C1009.6137%2C1009.1360%2C1009.1795%2C1009.5406%2C1009.5059%2C1009.3102%2C1009.3768%2C1009.2760%2C1009.1381%2C1009.2165%2C1009.1607%2C1009.4998%2C1009.1691%2C1009.5478%2C1009.5353%2C1009.3797%2C1009.1774%2C1009.1275%2C1009.5004%2C1009.5983%2C1009.1489%2C1009.2970%2C1009.6059%2C1009.5632%2C1009.5978%2C1009.3649%2C1009.4293%2C1009.0373&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study the performance of estimators of a sparse nonrandom vector based on\nan observation which is linearly transformed and corrupted by additive white\nGaussian noise. Using the reproducing kernel Hilbert space framework, we derive\na new lower bound on the estimator variance for a given differentiable bias\nfunction (including the unbiased case) and an almost arbitrary transformation\nmatrix (including the underdetermined case considered in compressed sensing\ntheory). For the special case of a sparse vector corrupted by white Gaussian\nnoise-i.e., without a linear transformation-and unbiased estimation, our lower\nbound improves on previously proposed bounds."}, "authors": ["Sebastian Schmutzhard", "Alexander Jung", "Franz Hlawatsch", "Zvika Ben-Haim", "Yonina C. Eldar"], "author_detail": {"name": "Yonina C. Eldar"}, "author": "Yonina C. Eldar", "links": [{"href": "http://arxiv.org/abs/1009.3353v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1009.3353v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1009.3353v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1009.3353v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "A Lower Bound on the Estimator Variance\nfor the Sparse Linear Model\nSebastian Schmutzhard 1, Alexander Jung2, Franz Hlawatsch2, Zvika Ben-Haim 3, and Yonina C. Eldar3\n\narXiv:1009.3353v1 [math.ST] 17 Sep 2010\n\n1\n\nNuHAG, Faculty of Mathematics, University of Vienna\nA-1090 Vienna, Austria; e-mail: sebastian.schmutzhard@univie.ac.at\n2\nInstitute of Communications and Radio-Frequency Engineering, Vienna University of Technology\nA-1040 Vienna, Austria; e-mail: {ajung, fhlawats}@nt.tuwien.ac.at\n3\nTechnion-Israel Institute of Technology\nHaifa 32000, Israel; e-mail: {zvikabh@tx, yonina@ee}.technion.ac.il\n\nAbstract-We study the performance of estimators of a sparse\nnonrandom vector based on an observation which is linearly\ntransformed and corrupted by additive white Gaussian noise.\nUsing the reproducing kernel Hilbert space framework, we derive\na new lower bound on the estimator variance for a given differentiable bias function (including the unbiased case) and an almost\narbitrary transformation matrix (including the underdetermined\ncase considered in compressed sensing theory). For the special\ncase of a sparse vector corrupted by white Gaussian noise-i.e.,\nwithout a linear transformation-and unbiased estimation, our\nlower bound improves on previously proposed bounds.\n\ntogether with (1) will be referred to as the sparse linear model\n(SLM). Note that we also allow M < N (this case is relevant\nto compressed sensing methods [1], [2]); however, condition\n(3) implies that M \u2265 S. The case of correlated Gaussian noise\nn with a known nonsingular correlation matrix can be reduced\nto the SLM by means of a noise whitening transformation. An\nimportant special case of the SLM is given by H = I (so that\nM = N ), i.e.,\ny = x + n,\n(4)\n\nIndex Terms-Sparsity, parameter estimation, sparse linear\nmodel, denoising, variance bound, reproducing kernel Hilbert\nspace, RKHS.\n\nwhere again x \u2208 XS and n \u223c N (0, \u03c3 2 I). This will be referred\nto as the sparse signal in noise model (SSNM).\nLower bounds on the estimation variance for the SLM have\nbeen studied previously. In particular, the Cram\u00e9r\u2013Rao bound\n(CRB) for the SLM was derived in [3]. For the SSNM (4),\nlower and upper bounds on the minimum variance of unbiased\nestimators were derived in [4]. A problem with the lower\nbounds of [3] and [4] is the fact that they exhibit a discontinuity\nwhen passing from the case kxk0 = S to the case kxk0 < S.\nIn this paper, we use the mathematical framework of reproducing kernel Hilbert spaces (RKHS) [5]\u2013[7] to derive a novel\nlower variance bound for the SLM. The RKHS framework\nallows pleasing geometric interpretations of existing bounds,\nincluding the CRB, the Hammersley-Chapman-Robbins bound\n[8], and the Barankin bound [9]. The bound we derive here\nholds for estimators with a given differentiable bias function.\nFor the SSNM, in particular, we obtain a lower bound for\nunbiased estimators which is tighter than the bounds in [4]\nand, moreover, everywhere continuous. As we will show, RKHS\ntheory relates the bound for the SLM to that obtained for the\nlinear model without a sparsity assumption. We note that the\nRKHS framework has been previously applied to estimation\n[6], [7] but, to the best of our knowledge, not to the SLM.\nThis paper is organized as follows. In Section II, we review\nsome fundamentals of parameter estimation. Relevant elements\nof RKHS theory are summarized in Section III. In Section IV,\nwe use RKHS theory to derive a lower variance bound for\nthe SLM. Section V considers the special case of unbiased\nestimation within the SSNM. Section VI presents a numerical\ncomparison of the new bound with the variance of two established estimation schemes.\n\nI. I NTRODUCTION\nWe study the problem of estimating a nonrandom parameter\nvector x \u2208 RN which is sparse, i.e., at most S of its entries are\nnonzero, where 1 \u2264 S < N (typically S \u226a N ). We thus have\n\b\n(1)\nx \u2208 XS , with XS , x\u2032 \u2208 RN kx\u2032 k0 \u2264 S ,\n\nwhere kxk0 denotes the number of nonzero entries of x. While\nthe sparsity degree S is assumed to be known, the set of\npositions of the nonzero entries of x (denoted by supp(x)) is\nunknown. The estimation of x is based on the observed vector\ny \u2208 RM given by\ny = Hx + n ,\n(2)\nwith a known system matrix H \u2208 RM\u00d7N and white Gaussian\nnoise n \u223c N (0, \u03c3 2 I) with known variance \u03c3 2 > 0. The matrix\nH is arbitrary except that it is assumed to satisfy the standard\nrequirement\nspark(H) > S ,\n(3)\nwhere spark(H) denotes the minimum number of linearly\ndependent columns of H [1]. The observation model (2)\nThis work was supported by the FWF under Grants S10602-N13 (Signal\nand Information Representation) and S10603-N13 (Statistical Inference) within\nthe National Research Network SISE, by the Israel Science Foundation under\nGrant 1081/07, and by the European Commission under the FP7 Network of\nExcellence in Wireless COMmunications NEWCOM++ (contract no. 216715).\n\n\fII. BASIC C ONCEPTS\nWe first review some basic concepts of parameter estimation\n[10]. Let x \u2208 X \u2286 RN be the nonrandom parameter vector\nto be estimated, y \u2208 RM the observed vector, and f (y; x) the\nprobability density function (pdf) of y, parameterized by x. For\nthe SLM, X = XS as defined in (1) and\n\u0013\n\u0012\n1\n1\n2\nexp \u2212 2 ky\u2212Hxk2 . (5)\nf (y; x) =\n2\u03c3\n(2\u03c0\u03c3 2 )M/2\n\n\b\u0002\n\u00032\nwith v(x\u0302k (*); x) , Ex x\u0302k (y) \u2212 Ex {x\u0302k (y)} . Thus, (7) is\nequivalent to the N scalar optimization problems\nx\u0302x0 ,k (*) = arg\n\nmin v(x\u0302k (*); x0 ) ,\n\nx\u0302k (*)\u2208B\u03b3k\n\n2\n\n\u03b5(x\u0302(*); x) = kb(x\u0302(*); x)k2 + v(x\u0302(*); x) ,\n\n(6)\n\nwith the estimator bias b(x\u0302(*); x) \b, Ex {x\u0302(y)} \u2212 x and the\n2\n.A\nestimator variance v(x\u0302(*); x) , Ex x\u0302(y)\u2212 Ex {x\u0302(y)}\nstandard approach to defining an optimum estimator is to fix\n!\nthe bias, i.e., b(x\u0302(*); x) = c(x) for all x \u2208 X , and minimize\nthe variance v(x\u0302(*); x) for all x \u2208 X under this bias constraint.\nHowever, in many cases, such a \"uniformly optimum\" estimator\ndoes not exist. It is then natural to consider \"locally optimum\"\nestimators that minimize v(x\u0302(*); x0 ) only at a given parameter\nvalue x = x0 \u2208 X . This approach is taken here. Note that it\nfollows from (6) that once the bias is fixed, minimizing the\nvariance is equivalent to minimizing the MSE \u03b5(x\u0302(*); x0 ).\nThe bias constraint b(x\u0302(*); x) = c(x) can be equivalently\nwritten as the mean constraint\nEx {x\u0302(y)} = \u03b3(x) ,\n\nwith \u03b3(x) , c(x) + x .\n\nThus, we consider the constrained optimization problem\nx\u0302x0 (*) = arg min v(x\u0302(*); x0 ) ,\nwhere\n\n(7)\n\nx\u0302(*)\u2208B\u03b3\n\n\b\nB\u03b3 , x\u0302(*) Ex {x\u0302(y)} = \u03b3(x), \u2200 x \u2208 X .\n\nThe minimum variance achieved by the locally optimum estimator x\u0302x0 (*) at x0 will be denoted as\nV\u03b3 (x0 ) , v(x\u0302x0 (*); x0 ) = min v(x\u0302(*); x0 ) .\nx\u0302(*)\u2208B\u03b3\n\nThis is also known as the Barankin bound (for the prescribed\nmean \u03b3(x)) [9]. Using RKHS theory, it can be shown that\nx\u0302x0 (*) exists, i.e., there exists a unique minimum for (7),\nprovided that there exists at least one estimator with mean \u03b3(x)\nfor all x \u2208 X and finite variance at x0 (see also Section III). For\nunbiased estimation, i.e., \u03b3(x) \u2261 x, x\u0302x0 (*) is called a locally\nminimum variance unbiased (LMVU) estimator. Unfortunately,\nV\u03b3 (x0 ) is difficult to compute in many cases, including the case\nof the SLM. Lower bounds on V\u03b3 (x0 ) are, e.g., the CRB and\nthe Hammersley-Chapman-Robbins bound [8].\nLet xk , x\u0302k (y), and \u03b3k (x) denote the kth entries\nPN of x, x\u0302(y),\nand \u03b3(x), respectively. We have v(x\u0302(*); x) = k=1 v(x\u0302k (*); x)\n\n(8)\n\nwhere\n\b\nB\u03b3k , x\u0302(*) Ex {x\u0302(y)} = \u03b3k (x), \u2200 x \u2208 X .\n\nThe minimum variance achieved by x\u0302x0 ,k (*) at x0 is denoted as\nV\u03b3k (x0 ) , v(x\u0302x0 ,k (*); x0 ) =\n\nA. Minimum-Variance Estimators\nThe estimation error incurred by an estimator x\u0302(y) can\nbe \bquantified by the mean squared error (MSE) \u03b5(x\u0302(*); x) ,\nEx kx\u0302(y) \u2212 xk22 , where the notation Ex {*} indicates that the\nexpectation is taken with respect to the pdf f (y; x) parameterized by x. Note that \u03b5(x\u0302(*); x) depends on the true parameter\nvalue, x. The MSE can be decomposed as\n\nk = 1, . . . , N ,\n\nmin v(x\u0302k (*); x0 ) .\n\nx\u0302k (*)\u2208B\u03b3k\n\n(9)\n\nB. CRB of the Linear Gaussian Model\nIn our further development, we will make use of the CRB\nfor the linear Gaussian model (LGM) defined by\nz = As + n ,\n\n(10)\n\nwith the nonrandom parameter s \u2208 RS (not assumed sparse), the\nobservation z \u2208 RM, the known matrix A \u2208 RM\u00d7S, and white\nGaussian noise n \u223c N (0, \u03c3 2 I). As before, we assume that\nM \u2265 S; furthermore, we assume that A has full column rank,\ni.e., ATA \u2208 RS\u00d7S is nonsingular. The relationship of this model\nwith the SLM, as well as the different notation and different\ndimension (S instead of N ), will become clear in Section IV.\nConsider estimators \u015dk (z) of the kth parameter component sk whose bias is equal to some prescribed differentiable\n\b function c\u0303k (s), i.e., b(\u015dk (*); s) = c\u0303k (s) or equivalently\nEs \u015dk (z) = \u03b3\u0303k (s) with \u03b3\u0303k (s) , c\u0303k (s) + sk , for all s \u2208 RS.\nLet V\u03b3\u0303LGM\n(s0 ) denote the minimum variance achievable by such\nk\nestimators at a given true parameter s0 . The CRB C\u03b3\u0303LGM\n(s0 ) is\nk\nthe following lower bound on the minimum variance [10]:\n\u22121\n\nV\u03b3\u0303LGM\n(s0 ) \u2265 C\u03b3\u0303LGM\n(s0 ) , \u03c3 2 r\u0303Tk(s0 )(ATA) r\u0303k (s0 ) , (11)\nk\nk\nwhere r\u0303k (s) , \u2202 \u03b3\u0303k (s)/\u2202s, i.e., r\u0303k (s) is the vector of dimension\nS whose lth entry is \u2202 \u03b3\u0303k (s)/\u2202sl . We note that V\u03b3\u0303LGM\n(s0 ) =\nk\nC\u03b3\u0303LGM\n(s\n)\nif\n\u03b3\u0303\n(s)\nis\nan\naffine\nfunction\nof\ns.\nIn\nparticular,\nthis\n0\nk\nk\nincludes the unbiased case (\u03b3\u0303k (s) \u2261 sk ).\nIII. T HE RKHS F RAMEWORK\nIn this section, we review some RKHS fundamentals which\nwill provide a basis for our further development. Consider a set\nX (not necessarily a linear space) and a positive semidefinite1\n\"kernel\" function R(x, x\u2032 ) : X \u00d7X \u2192 R. For each fixed x\u2032 \u2208 X ,\nthe function fx\u2032 (x) , R(x, x\u2032 ) maps X into R. The RKHS\nH(R) is a Hilbert space of functions f : X \u2192 R which is\ndefined as the closure of the linear span of the set of functions\n{fx\u2032 (x) = R(x, x\u2032 )}x\u2032 \u2208X . This closure is taken with respect\nto the topology given by the scalar product h* , *iH(R) which is\ndefined via the reproducing property [5]\nf (*), R(*, x\u2032 )\n\nH(R)\n\n= f (x\u2032 ) .\n\nThis relation holds for all f \u2208 H(R) and x\u2032 \u2208 X . The associated\n1/2\nnorm is given by kf kH(R) = hf, f iH(R) .\n1 That\n\nRP \u00d7P\n\nis, for any finite set {xk }k=1,...,P with xk \u2208 X , the matrix R \u2208\nwith entries (R)k,l , R(xk , xl ) is positive semidefinite.\n\n\fWe now consider the constrained optimization problem (8)\nfor a given mean function \u03b3(x) (formerly denoted by \u03b3k (x);\nwe temporarily drop the subscript k for better readability).\nAccording to [6], [7], for certain classes of parametrized pdf's\nf (y; x) (which include the Gaussian pdf in (5)), one can\nassociate with this optimization problem an RKHS H(Rx0 )\nwhose kernel Rx0 (x, x\u2032 ) : X \u00d7X \u2192 R is given by\n\u001b\n\u001a\nf (y; x) f (y; x\u2032 )\nRx0 (x, x\u2032 ) , Ex0\nf (y; x0 ) f (y; x0 )\nZ\nf (y; x) f (y; x\u2032 )\ndy .\n=\nf (y; x0 )\nRM\nIt can be shown [6], [7] that \u03b3(x) \u2208 H(Rx0 ) if and only if\nthere exists at least one estimator with mean \u03b3(x) for all x\nand finite variance at x0 . Furthermore, under this condition,\nthe minimum variance V\u03b3 (x0 ) in (9) is finite and allows the\nfollowing expression involving the norm k\u03b3kH(Rx ) :\n\n\u03b3(x) for all x \u2208 XSK (but not necessarily for all x \u2208 XS ), i.e.,\n\b\nB\u03b3K , x\u0302(*) Ex {x\u0302(y)} = \u03b3(x), \u2200 x \u2208 XSK .\n\nComparing with (15), we see that B\u03b3K \u2287 B\u03b3 .\nLet us now consider the minimum variance among all\nestimators in B\u03b3K , i.e.,\nV\u03b3K (x0 ) , min v(x\u0302(*); x0 ) .\n\n(16)\n\nK\nx\u0302(*)\u2208B\u03b3\n\nBecause x\u0302(*) \u2208 B\u03b3K is a less restrictive constraint than x\u0302(*) \u2208 B\u03b3\nused in the definition of V\u03b3 (x0 ), we have\nV\u03b3 (x0 ) \u2265 V\u03b3K (x0 ) ,\n\n(17)\n\ni.e., V\u03b3K (x0 ) is a lower bound on V\u03b3 (x0 ). A closed-form\nexpression of V\u03b3K (x0 ) appears to be difficult to obtain in the\ngeneral case, because x0 6\u2208 XSK in general. Therefore, we will\nuse RKHS theory to derive a lower bound on V\u03b3K (x0 ).\n\n0\n\nV\u03b3 (x0 ) =\n\n2\nk\u03b3kH(Rx )\n0\n\n\u2212 \u03b3 2 (x0 ) .\n\n(12)\n\nThis is an RKHS formulation of the Barankin bound. Unfortunately, the norm k\u03b3kH(Rx ) is often difficult to compute.\n0\nFor the SLM in (2), (1), (5), X = XS ; the kernel here is a\nmapping XS \u00d7 XS \u2192 R which is easily shown to be given by\n\u0013\n\u0012\n1\nT\nT\n\u2032\n\u2032\n(x\u2212x0 ) H H (x \u2212x0 ) ,\n(13)\nRx0 (x, x ) = exp\n\u03c32\nwhere x0 \u2208 XS . An RKHS can also be defined for the LGM in\n(s, s\u2032 ) with s0 \u2208 RS\n(10). Here, X = RS , and the kernel RsLGM\n0\nS\nS\nis a mapping R \u00d7 R \u2192 R given by\n\u0013\n\u0012\n1\nT T\n\u2032\n\u2032\nLGM\n(s\u2212s0 ) A A(s \u2212s0 ) .\n(14)\nRs0 (s, s ) = exp\n\u03c32\nNote that these kernels differ in their domain, which is XS \u00d7XS\n(s, s\u2032 ).\nfor Rx0 (x, x\u2032 ) and RS \u00d7 RS for RsLGM\n0\nIV. A L OWER B OUND\n\nON THE\n\nE STIMATOR VARIANCE\n\nWe now continue our treatment of the SLM estimation\nproblem. In what follows, V\u03b3 (x0 ) will be understood to denote\nthe bias-constrained minimum variance (9) specifically for the\nSLM. This means, in particular, that X = XS , and hence the set\nof admissible estimators is given by\n\b\n(15)\nB\u03b3 = x\u0302(*) Ex {x\u0302(y)} = \u03b3(x), \u2200 x \u2208 XS .\nWe will next derive a lower bound on V\u03b3 (x0 ).\n\nB. Two Isometric RKHSs\nAn RKHS for the SLM can also be defined on XSK , using a\nkernel RxK0 : XSK \u00d7XSK \u2192 R that is given by the right-hand side\nof (13) but whose arguments x, x\u2032 are assumed to be in XSK\nand not just in XS (however, recall that x0 6\u2208 XSK in general).\nThis RKHS will be denoted H(RxK0 ). The minimum variance\nV\u03b3K (x0 ) in (16) can then be expressed as (cf. (12))\n2\n\nV\u03b3K (x0 ) = k\u03b3kH(RK ) \u2212 \u03b3 2 (x0 ) .\n\nIn order to develop this expression, we define some notation.\nConsider an index set I = {k1 , . . . , k|I| } \u2286 {1, . . . , N }. We\ndenote by HI \u2208 RM\u00d7|I| the submatrix of our matrix H \u2208\nRM\u00d7N whose ith column is given by the ki th column of H.\nFurthermore, for a vector x \u2208 RN, we denote by xI \u2208 R|I| the\nsubvector whose ith entry is the ki th entry of x.\nWe now introduce a second RKHS. Consider the LGM in\n) with\n(10) with matrix A = HK \u2208 RM\u00d7S, and let H(RsLGM\n0\ns0 \u2208 RS denote the RKHS for that LGM as defined by the kernel\n: RS \u00d7 RS \u2192 R in (14). Exploiting the linear-subspace\nRsLGM\n0\nstructure of XSK , it can be shown that our RKHS H(RxK0 ) for\n) with s0 chosen as\na given x0 is isometric to H(RsLGM\n0\ns0 = H\u2020K H x0 .\n\nXSK , {x \u2208 XS | supp(x) \u2286 K} .\nClearly, XSK \u2286 XS ; however, contrary to XS , XSK is a linear\nsubspace of RN. Let B\u03b3K be the set of all estimators with mean\n\n(19)\n\nHere, H\u2020K , (HTK HK )\u22121 HTK \u2208 RS\u00d7M is the pseudo-inverse of\nHK (recall that M \u2265 S, and note that (HTK HK )\u22121 is guaranteed\nto exist because of our assumption (3)). More specifically, the\n) mapping each f \u2208 H(RxK0 )\nisometry J : H(RxK0 ) \u2192 H(RsLGM\n0\n) is given by\nto an f \u0303 \u2208 H(RsLGM\n0\nJ{f (x)} = f \u0303(xK ) = \u03b2x0 f (x) ,\n\nA. Relaxing the Bias Constraint\nThe first step in this derivation is to relax the bias constraint\nx\u0302(*) \u2208 B\u03b3 . Let K , {k1 , . . . , kS } be a fixed set of S different\nindices ki \u2208 {1, . . . , N } (not related to supp(x0 )), and let\n\n(18)\n\nx0\n\nwhere\n\nx \u2208 XSK ,\n\n(20)\n\n\u0013\n\n(21)\n\n\u0012\n1\n\u03b2x0 , exp \u2212 2 (I\u2212PK ) Hx0\n2\u03c3\n\n2\n2\n\n.\n\nHere, PK , HK H\u2020K is the orthogonal projection matrix on the\nrange of HK . The factor \u03b2x0 can be interpreted as a measure\nof the distance between the point Hx0 and the subpsace XSK\nassociated with the index set K. We can write (20) as\n\n\ff \u0303(s) = \u03b2x0 f (x(s)) ,\n\ns \u2208 RS ,\n\nwhere x(s) denotes the x \u2208 XSK for which xK = s (i.e., the S\nentries of s appear in x(s) at the appropriate positions within\nK, and the N \u2212S remaining entries of x(s) are zero).\nConsider now the image of \u03b3(x) under the mapping J,\ns \u2208 RS .\n\n\u03b3\u0303(s) , J{\u03b3(x)} = \u03b2x0 \u03b3(x(s)) ,\n2\n\n(22)\n2\n\nSince J is an isometry, we have k\u03b3\u0303kH(RLGM\n= k\u03b3kH(RK\n.\ns0 )\nx0 )\nCombining this identity with (18), we obtain\n2\n\nV\u03b3K (x0 ) = k\u03b3\u0303kH(RLGM ) \u2212 \u03b3 2 (x0 ) .\n\n(23)\n\ns0\n\nC. Lower Bound on V\u03b3K (x0 )\nWe will now use expression (23) to derive a lower bound on\nV\u03b3K (x0 ) in terms of the CRB for the LGM in (11). Consider the\nminimum estimator variance for the LGM under the constraint\nof the prescribed mean function \u03b3\u0303(s), V\u03b3\u0303LGM (s0 ), still for A =\nHK and for s0 given by (19). We have (cf. (12))\n2\n\n2\nV\u03b3\u0303LGM (s0 ) = k\u03b3\u0303kH(RLGM\n) \u2212 \u03b3\u0303 (s0 ) .\ns\n\nPN\nRecalling that v(x\u0302(*); x0 ) =\nk=1 v(x\u0302k (*); x0 ) (we now\nreintroduce the subscript k), a lower bound on v(x\u0302(*); x0 ) is\nobtained from (28) as\nN\nX\nk\nv(x\u0302(*); x0 ) \u2265\nLK\n\u03b3k (x0 ) .\nk=1\n\nFor a high lower bound, the index sets Kk should in general be\nchosen such that the respective factors \u03b2x2 0 ,k in (26) are large.\n(This means that the \"distances\" between Hx0 and XSKk are\nsmall, see (21).) Formally using the optimum Kk for each k,\nwe arrive at the main result of this paper.\nTheorem. Let x\u0302(*) be an estimator for the SLM (2), (1) whose\nmean equals \u03b3(x) for all x \u2208 XS . Then the variance of x\u0302(*) at\na given parameter vector x = x0 \u2208 XS satisfies\nv(x\u0302(*); x0 ) \u2265\n\nN\nX\n\nL\u2217\u03b3k (x0 ) ,\n\n(29)\n\nk=1\n\nKk\nk\nwhere L\u2217\u03b3k (x0 ) , maxKk :|Kk |=S LK\n\u03b3k (x0 ), with L\u03b3k (x0 ) given\nby (26) together with (27) and (19).\n\nV. S PECIAL C ASE : U NBIASED E STIMATION FOR THE SSNM\n\n0\n\nCombining with (23), we obtain the relation\nV\u03b3K (x0 ) = V\u03b3\u0303LGM (s0 ) + \u03b3\u0303 2 (s0 ) \u2212 \u03b3 2 (x0 ) .\nUsing the CRB V\u03b3\u0303LGM (s0 ) \u2265 C\u03b3\u0303LGM (s0 ) (see (11)) yields\nV\u03b3K (x0 ) \u2265 LK\n\u03b3 (x0 ) ,\n\n(24)\n\nLGM\nLK\n(s0 ) + \u03b3\u0303 2 (s0 ) \u2212 \u03b3 2 (x0 ) .\n\u03b3 (x0 ) , C\u03b3\u0303\n\n(25)\n\nwith\n\nFinally, using (22) and the implied CRB relation C\u03b3\u0303LGM (s0 ) =\nLGM\n(s0 ), the lower bound (25) can be reformulated as\n\u03b2x2 0 C\u03b3(x(s))\n\u0003\n\u0002 LGM\n2\n2\n2\nLK\n\u03b3 (x0 ) = \u03b2x0 C\u03b3(x(s)) (s0 ) + \u03b3 (x(s0 )) \u2212 \u03b3 (x0 ) . (26)\n\nLGM\n(s0 ) denotes the CRB for prescribed mean funcHere, C\u03b3(x(s))\n\u2032\ntion \u03b3 (s) = \u03b3(x(s)), which is given by (see (11))\n\u22121\n\nLGM\n(s0 ) = \u03c3 2 rT(s0 )(HTK HK ) r(s0 ) ,\nC\u03b3(x(s))\n\n(27)\n\nwhere r(s) , \u2202 \u03b3(x(s))/\u2202s and s0 is related to x0 via (19).\nTo summarize, we have the following chain of lower bounds\non the bias-constrained variance at x0 :\n(9)\n\n(17)\n\n(24)\n\nv(x\u0302(*); x0 ) \u2265 V\u03b3 (x0 ) \u2265 V\u03b3K (x0 ) \u2265 LK\n\u03b3 (x0 ) .\n\n(28)\n\nWhile LK\n\u03b3 (x0 ) is the loosest of these bounds, it is attractive\nbecause of its closed-form expression in (26) (together with\n(27) and (19)). We note that the inequality (24) becomes an\nequality if \u03b3\u0303(s) is an affine function of s, or equivalently (see\n(22)), if \u03b3(x) is an affine function of x. In particular, this\nincludes the unbiased case (\u03b3(x) \u2261 x).\n\nThe SSNM in (4) is a special case of the SLM with H = I.\nWe now consider unbiased estimation (i.e., \u03b3(x) \u2261 x) for the\nSSNM. Since an unbiased estimator with uniformly minimum\nvariance does not exist [4], we are interested in a lower variance\nbound at a fixed x0 \u2208 XS . We denote by \u03be(x0 ) and j(x0 ) the\nvalue and index, respectively, of the S-largest (in magnitude)\nentry of x0 ; note that this is the smallest (in magnitude) nonzero\nentry of x0 if kx0 k0 = S, and zero if kx0 k0 < S.\nConsider an unbiased estimator x\u0302k (*). For k \u2208 supp(x0 ),\nk\nusing the lower bound LK\n\u03b3k (x0 ) in (26) with any index set Kk\nof size |Kk | = S such that supp(x0 ) \u2286 Kk , one can show that\nv(x\u0302k (*); x0 ) \u2265 \u03c3 2 ,\n\nk \u2208 supp(x0 ) .\n\n(30)\n\nThis bound is actually the minimum variance (i.e., the variance\nof the LMVU estimator) since it is achieved by the specific\nunbiased estimator x\u0302k (y) = yk (which is the LMVU estimator\nfor k \u2208 supp(x0 )). On the other hand, for k \u2208\n/ supp(x\u00010 ), the\nk\nlower bound LK\n(x\n)\nwith\nK\n=\nsupp(x\n)\\{j(x\n0\nk\n0\n0 )} \u222a {k}\n\u03b3k\ncan be shown to lead to the inequality\nv(x\u0302k (*); x0 ) \u2265 \u03c3 2 e\u2212\u03be\n\n2\n\n(x0 )/\u03c32\n\n,\n\nk\u2208\n/ supp(x0 ) .\n\n(31)\n\nCombining (30)\nPand (31), a lower bound on the overall variance\nv(x\u0302(*); x0 ) = N\nk=1 v(x\u0302k (*); x0 ) is obtained as\nX\nX\n2\n2\n\u03c3 2 e\u2212\u03be (x0 )/\u03c3 . (32)\n\u03c32 +\nv(x\u0302(*); x0 ) \u2265\nk\u2208supp(x0 )\n\nk\u2208supp(x\n/\n0)\n\nThus, recalling that v(x\u0302(*); x0 ) = \u03b5(x\u0302(*); x0 ) for unbiased\nestimators, we arrive at the following result.\nCorollary. Let x\u0302(*) be an unbiased estimator for the SSNM in\n(4). Then the MSE of x\u0302(*) at a given x = x0 \u2208 XS satisfies\n\u0002\n2\n2\u0003\n(33)\n\u03b5(x\u0302(*); x0 ) \u2265 S + (N \u2212S) e\u2212\u03be (x0 )/\u03c3 \u03c3 2 .\n\n\fVI. N UMERICAL R ESULTS\nFor the\nin (4), we will compute the lower variance\nPNSSNM\n\u2217\nbound\nk=1 L\u03b3k (x0 ) (see (29)) and compare it with the\nvariance of two established estimators, namely, the maximum\nlikelihood (ML) estimator and the hard-thresholding (HT) estimator. The ML estimator is given by\nx\u0302ML (y) , arg max f (y; x\u2032 ) = PS (y) ,\nx\u2032 \u2208XS\n\nwhere the operator PS retains the S largest (in magnitude)\nentries and zeros out all others. The HT estimator x\u0302HT (y) is\ngiven by\n(\nyk , |yk | \u2265 T\nx\u0302HT,k (y) =\n(34)\n0 , else,\nwhere T is a fixed threshold.\nFor simplicity, we consider the SSNM for S = 1. In this case,\nthe bound (29) can be shown to be\n\u2212\u03be\nj\nv(x\u0302(*); x0 ) \u2265 LK\n\u03b3j (x0 ) + (N \u22121) e\n\n2\n\n(x0 )/\u03c32\n\ni\nLK\n\u03b3i (x0 ) , (35)\n\nwhere j , j(x0 ), i is any index different from j(x0 ) (it can be\nshown that all such indices equally maximize the lower bound),\nKj , {j(x0 )}, and Ki , {i}. (We note that (35) simplifies to\n(32) for the special case of an unbiased estimator.) Since we\ncompare the bound (35) to the ML and HT estimators, \u03b3(x) is\nset equal to the mean of the respective estimator (ML or HT).\nFor a numerical evaluation, we generated parameter vectors\nx0 with N = 5, S = 1, j(x0 ) = 1, and different \u03be(x0 ). (The fixed\nchoice j(x0 ) = 1 is justified by the fact that neither the variances\nof the ML and HT estimators nor the corresponding variance\nbounds depend on j(x0 ).) In Fig. 1, we plot the variances\nv(x\u0302ML (*); x0 ) and v(x\u0302HT (*); x0 ) (the latter for three different\nchoices of T in (34)) along with the corresponding bounds (35),\nas a function of the signal-to-noise ratio (SNR) \u03be 2 (x0 )/\u03c3 2 . It\nis seen that for SNR larger than about 18 dB, all variances and\nbounds are effectively equal (for the HT estimator, this is true\nif T is not too small). However, in the medium-SNR range, the\n\n8\nvariance/bound\n\nThis lower bound is tighter (i.e., higher) than the lower bound\nderived in [4]. Furthermore, in contrast to the bound in [4], it\nis a function of x0 that is everywhere continuous. This fact\nis theoretically pleasing since the MSE of any estimator is a\ncontinuous function of x0 [11].\nLet us consider the special case of S = 1. Here, \u03be(x0 ) and\nj(x0 ) are simply the value and index, respectively, of the single\nnonzero entry of x0 . Using RKHS theory, one can show that\nthe estimator x\u0302(*) given componentwise by\n(\nyj(x0 ) ,\nk = j(x0 )\nx\u0302k (y) =\n\u03b1(y; x0 ) yk , else ,\n\u0001\nwith \u03b1(y; x0 ) , exp \u2212 2\u03c31 2 [2yj(x0 ) \u03be(x0 ) + \u03be 2 (x0 )] , is the\nLMVU estimator at x0 . That is, the estimator x\u0302(*) is unbiased\nand its MSE achieves the lower bound (33). This also means\nthat (33) is actually the minimum MSE (achieved by the\nLMVU estimator). While x\u0302(*) is not very practical since it\nexplicitly involves the unknown true parameter x0 , its existence\ndemonstrates the tightness of the bound (33).\n\n6\n4\n\n2\n0\n\u221220\n\nv(x\u0302HT (*); x0 ), T = 5\nbound on v(x\u0302HT (*); x0 ), T = 5\nv(x\u0302HT (*); x0 ), T = 4\nbound on v(x\u0302HT (*); x0 ), T = 4\nv(x\u0302HT (*); x0 ), T = 3\nbound on v(x\u0302HT (*); x0 ), T = 3\nv(x\u0302ML (*); x0 )\nbound on v(x\u0302ML (*); x0 )\nML\n\n\u221210\n\n0\n10\nSNR [dB]\n\nHT (T = 5)\nHT (T = 4)\nHT (T = 3)\n\n20\n\nFigure 1. Variance of the ML and HT estimators and corresponding lower\nbounds versus the SNR \u03be 2 (x0 )/\u03c32, for the SSNM with N = 5 and S = 1.\n\nvariances of the ML and HT estimators are significantly higher\nthan the corresponding lower bounds. We can conclude that\nthere might exist estimators with the same mean as that of the\nML or HT estimator but smaller variance. Note, however, that\na positive statement regarding the existence of such estimators\ncannot be based on our analysis.\nVII. C ONCLUSION\nUsing the mathematical framework of reproducing kernel\nHilbert spaces, we derived a novel lower bound on the variance\nof estimators of a sparse vector under a bias constraint. The\nobserved vector was assumed to be a linearly transformed\nand noisy version of the sparse vector to be estimated. This\nsetup includes the underdetermined case relevant to compressed\nsensing. In the special case of unbiased estimation of a noisecorrupted sparse vector, our bound improves on the best known\nlower bound. A comparison with the variance of two established\nestimators showed that there might exist estimators with the\nsame bias but a smaller variance.\nR EFERENCES\n[1] J. A. Tropp, \"Greed is good: Algorithmic results for sparse approximation,\" IEEE Trans. Inf. Theory, vol. 50, pp. 2231\u20132242, Oct. 2004.\n[2] D. L. Donoho, \"Compressed sensing,\" IEEE Trans. Inf. Theory, vol. 52,\npp. 1289\u20131306, April 2006.\n[3] Z. Ben-Haim and Y. C. Eldar, \"The Cram\u00e9r\u2013Rao bound for estimating\na sparse parameter vector,\" IEEE Trans. Signal Processing, vol. 58,\npp. 3384\u20133389, June 2010.\n[4] A. Jung, Z. Ben-Haim, F. Hlawatsch, and Y. C. Eldar, \"On unbiased\nestimation of sparse vectors corrupted by Gaussian noise,\" in Proc. IEEE\nICASSP-2010, (Dallas, TX), pp. 3990\u20133993, March 2010.\n[5] N. Aronszajn, \"Theory of reproducing kernels,\" Trans. Am. Math. Soc.,\nvol. 68, pp. 337\u2013404, May 1950.\n[6] E. Parzen, \"Statistical inference on time series by Hilbert space methods,\nI.,\" Tech. Rep. 23, Appl. Math. Stat. Lab., Stanford University, Stanford,\nCA, Jan. 1959.\n[7] D. D. Duttweiler and T. Kailath, \"RKHS approach to detection and\nestimation problems \u2013 Part V: Parameter estimation,\" IEEE Trans. Inf.\nTheory, vol. 19, pp. 29\u201337, Jan. 1973.\n[8] J. D. Gorman and A. O. Hero, \"Lower bounds for parametric estimation\nwith constraints,\" IEEE Trans. Inf. Theory, vol. 36, pp. 1285\u20131301, Nov.\n1990.\n[9] E. W. Barankin, \"Locally best unbiased estimates,\" Ann. Math. Statist.,\nvol. 20, no. 4, pp. 477\u2013501, 1949.\n[10] S. M. Kay, Fundamentals of Statistical Signal Processing: Estimation\nTheory. Englewood Cliffs, NJ: Prentice Hall, 1993.\n[11] E. L. Lehmann and G. Casella, Theory of Point Estimation. New York:\nSpringer, 2003.\n\n\f"}