{"id": "http://arxiv.org/abs/1009.1555v1", "guidislink": true, "updated": "2010-09-08T14:53:42Z", "updated_parsed": [2010, 9, 8, 14, 53, 42, 2, 251, 0], "published": "2010-09-08T14:53:42Z", "published_parsed": [2010, 9, 8, 14, 53, 42, 2, 251, 0], "title": "User Interest and Interaction Structure in Online Forums", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1009.5268%2C1009.1321%2C1009.2340%2C1009.0099%2C1009.2049%2C1009.2706%2C1009.0586%2C1009.0270%2C1009.0908%2C1009.3233%2C1009.0732%2C1009.1533%2C1009.5170%2C1009.3557%2C1009.1635%2C1009.1383%2C1009.0144%2C1009.4185%2C1009.4870%2C1009.3393%2C1009.4046%2C1009.2790%2C1009.4665%2C1009.1623%2C1009.0143%2C1009.5891%2C1009.1478%2C1009.5049%2C1009.5566%2C1009.5910%2C1009.1543%2C1009.1317%2C1009.4629%2C1009.0096%2C1009.1503%2C1009.0920%2C1009.0295%2C1009.4316%2C1009.0880%2C1009.4513%2C1009.3936%2C1009.1418%2C1009.1794%2C1009.2329%2C1009.5505%2C1009.4424%2C1009.6055%2C1009.5810%2C1009.1706%2C1009.1462%2C1009.6216%2C1009.4720%2C1009.5560%2C1009.4039%2C1009.5571%2C1009.4684%2C1009.4521%2C1009.2173%2C1009.4437%2C1009.0339%2C1009.2730%2C1009.3529%2C1009.6151%2C1009.5339%2C1009.0522%2C1009.1852%2C1009.0628%2C1009.5852%2C1009.4349%2C1009.4441%2C1009.3703%2C1009.2512%2C1009.3268%2C1009.2163%2C1009.5303%2C1009.3198%2C1009.1735%2C1009.5992%2C1009.5136%2C1009.0753%2C1009.4340%2C1009.4875%2C1009.5496%2C1009.4901%2C1009.3956%2C1009.0696%2C1009.3923%2C1009.1555%2C1009.0404%2C1009.5625%2C1009.4170%2C1009.4478%2C1009.5996%2C1009.2464%2C1009.1752%2C1009.6209%2C1009.5323%2C1009.3199%2C1009.0089%2C1009.0600%2C1009.1757&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "User Interest and Interaction Structure in Online Forums"}, "summary": "We present a new similarity measure tailored to posts in an online forum. Our\nmeasure takes into account all the available information about user interest\nand interaction --- the content of posts, the threads in the forum, and the\nauthor of the posts. We use this post similarity to build a similarity between\nusers, based on principal coordinate analysis. This allows easy visualization\nof the user activity as well. Similarity between users has numerous\napplications, such as clustering or classification. We show that including the\nauthor of a post in the post similarity has a smoothing effect on principal\ncoordinate projections. We demonstrate our method on real data drawn from an\ninternal corporate forum, and compare our results to those given by a standard\ndocument classification method. We conclude our method gives a more detailed\npicture of both the local and global network structure.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1009.5268%2C1009.1321%2C1009.2340%2C1009.0099%2C1009.2049%2C1009.2706%2C1009.0586%2C1009.0270%2C1009.0908%2C1009.3233%2C1009.0732%2C1009.1533%2C1009.5170%2C1009.3557%2C1009.1635%2C1009.1383%2C1009.0144%2C1009.4185%2C1009.4870%2C1009.3393%2C1009.4046%2C1009.2790%2C1009.4665%2C1009.1623%2C1009.0143%2C1009.5891%2C1009.1478%2C1009.5049%2C1009.5566%2C1009.5910%2C1009.1543%2C1009.1317%2C1009.4629%2C1009.0096%2C1009.1503%2C1009.0920%2C1009.0295%2C1009.4316%2C1009.0880%2C1009.4513%2C1009.3936%2C1009.1418%2C1009.1794%2C1009.2329%2C1009.5505%2C1009.4424%2C1009.6055%2C1009.5810%2C1009.1706%2C1009.1462%2C1009.6216%2C1009.4720%2C1009.5560%2C1009.4039%2C1009.5571%2C1009.4684%2C1009.4521%2C1009.2173%2C1009.4437%2C1009.0339%2C1009.2730%2C1009.3529%2C1009.6151%2C1009.5339%2C1009.0522%2C1009.1852%2C1009.0628%2C1009.5852%2C1009.4349%2C1009.4441%2C1009.3703%2C1009.2512%2C1009.3268%2C1009.2163%2C1009.5303%2C1009.3198%2C1009.1735%2C1009.5992%2C1009.5136%2C1009.0753%2C1009.4340%2C1009.4875%2C1009.5496%2C1009.4901%2C1009.3956%2C1009.0696%2C1009.3923%2C1009.1555%2C1009.0404%2C1009.5625%2C1009.4170%2C1009.4478%2C1009.5996%2C1009.2464%2C1009.1752%2C1009.6209%2C1009.5323%2C1009.3199%2C1009.0089%2C1009.0600%2C1009.1757&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present a new similarity measure tailored to posts in an online forum. Our\nmeasure takes into account all the available information about user interest\nand interaction --- the content of posts, the threads in the forum, and the\nauthor of the posts. We use this post similarity to build a similarity between\nusers, based on principal coordinate analysis. This allows easy visualization\nof the user activity as well. Similarity between users has numerous\napplications, such as clustering or classification. We show that including the\nauthor of a post in the post similarity has a smoothing effect on principal\ncoordinate projections. We demonstrate our method on real data drawn from an\ninternal corporate forum, and compare our results to those given by a standard\ndocument classification method. We conclude our method gives a more detailed\npicture of both the local and global network structure."}, "authors": ["Di Liu", "Daniel Percival", "Stephen E. Fienberg"], "author_detail": {"name": "Stephen E. Fienberg"}, "author": "Stephen E. Fienberg", "arxiv_comment": "8 Pages, 7 Figures, Short form appears in Proc. of ICWSM 2010", "links": [{"href": "http://arxiv.org/abs/1009.1555v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1009.1555v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.AP", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.AP", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1009.1555v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1009.1555v1", "journal_reference": "Di Liu, Daniel Percival and Stephen E. Fienberg. User Interest and\n  Interaction Structure in Online Forums. Proc of ICWSM 2010", "doi": null, "fulltext": "User Interest and Interaction Structure in Online Forums\nDi Liu, Daniel Percival and Stephen E. Fienberg\n\narXiv:1009.1555v1 [stat.AP] 8 Sep 2010\n\nDepartment of Statistics\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh, PA 15213\ndiliu, dperciva, fienberg@stat.cmu.edu\n\nAbstract\nWe present a new similarity measure tailored to posts in an\nonline forum. Our measure takes into account all the available information about user interest and interaction - the\ncontent of posts, the threads in the forum, and the author of\nthe posts. We use this post similarity to build a similarity\nbetween users, based on principal coordinate analysis. This\nallows easy visualization of the user activity as well. Similarity between users has numerous applications, such as clustering or classification. We show that including the author of a\npost in the post similarity has a smoothing effect on principal\ncoordinate projections. We demonstrate our method on real\ndata drawn from an internal corporate forum, and compare\nour results to those given by a standard document classification method. We conclude our method gives a more detailed\npicture of both the local and global network structure.\n\nIntroduction\nSocial network analysis has grown as a topic of interest with\nthe growth of the internet as an interactive environment, especially in connection with online communities. The general goals of these approaches include characterizing user\nbehaviors and interactions, as well as extracting information\nfrom actual user discussions. In this paper, we define a measure of similarity between users of an online forum, based\non a modification of document classification, which takes\ninto account both their interests and interactions.\nEstablishing a notion of distance or similarity between the\npeople in a social network provides a useful way to illustrate\nthe structure of the social network. For example, we might\ndefine similar people to represent friendship, shared interest,\nor similarity in skill. These interpretations give user similarity a wide variety of applications. For example, recovering\nfriendship from another form of personal interaction data is\nuseful in sociological studies. People with similar interests\ncould be targeted with a certain advertisement or product\nsuggestion. A company could assign people with similar\nskills to work together on a project.\nWe base our method on establishing a measure of similarity between all posts created by all users of an online forum.\nOur measure takes into account both the textual information\nCopyright c 2019, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\nand the particular context of the online forum. Usual approaches take only the textual information of the posts into\naccount. From similarity between posts, we establish similarity between users. We then use this similarity to investigate the structure of the social network.\n\nRelated Work\nSeveral previous studies aimed at network structure analysis highlighted areas related to characterizing and clustering users' behaviors, personal qualities, or interests. For\nexample, in recommendation systems, collaborative filtering works towards this goal, with high profile applications including Netflix (Bennett and Lanning 2007), Amazon.com (Linden, Smith, and York 2003) and financial services (Adomavicius and Tuzhilin 2005). Other authors\nuse a singular value decomposition (Bennett and Lanning\n2007) (Salakhutdinov, Mnih, and Hinton 2007) or variants\nof K-nearest neighbors (Bennett and Lanning 2007) (Sarwar et al. 2000) to characterize user interest.\nInternet communities such as forums and blogs introduce\nnew challenges for finding patterns of user behaviors. For\nexample, Yang, et al., applied social network prestige measures to infer relative expertise of users in a large website\nin China (2008); Hogg and Szab\u00f3 examined user behaviors\nbased on activity rates (2008); Holand and Leinhardt measured user behavior based on in- and out-degrees (1981).\nRelating to our general approach, other studies focused\non integrating user and content-based information in social\nnetwork studies. Basu, et al. (1998) proposed an inductive learning approach that uses both collaboritive and content information in predicting user preferences. Taskar et\nal. (2003) presented a method which makes a connection\nbetween a user's personal and network information. These\nmethods seek to integrate all types of information available\nin some social networks.\n\nOur Data\nOnline Forums We examine user similarity in the context\nof an online forum. An online forum is a system designed\nfor the discussion of topics, with each topic separated into its\nown area, called a thread. A thread is begun by a user writing\na short document, called a post, which introduces the topic\nor asks a question about the topic. Typically, this user also\n\n\fTable 1: A summary of the attributes of the corporate forum data. Note that the post word counts include stopwords.\nAttribute\nWords in a Post\nPosts made by a User\nPosts in a Thread\n\nMin\n1\n1\n1\n\n1st Quartile\n14\n1\n2\n\nwrites a separate title for the thread, which summarizes or\nhighlights the thread topic. Other users can then continue\nthe discussion by adding their own posts to the thread. Thus\neach thread in the forum is a place where many users discuss\na certain topic.\n\nMedian\n28\n5\n3\n\n3rd Quartile\n59\n22\n5\n\nMax\n8980\n975\n265\n\nthe importance of each word. One way to measure word\nimportance is the tf-idf formula (Weiss et al. 2005). Suppose we have N total documents. For a word j, tf(D, j)\nrepresents the frequency of word j in document D. df(j)\nrepresents the number of documents containing the word j.\ntf-idf is defined as:\n\nCorporate Forum Data\nOur data come from a global IT company. The company created an internal forum in order to enhance information flow\nbetween employees. We have data collected from this forum\nover a one year period from August 2006 until August 2007.\nOver this period, 2,974 users wrote 79,128 posts in 20,090\nthreads. The users of this forum are skilled IT professionals,\nand so the topics discussed in this forum are very technical\nand specific. The company is interested in grouping employees in creative ways based on the employee's skills, areas of\ninterest, and other strengths.\nBy using the available thread ID and user ID information,\nwe can link posts to threads, and authors to posts. Table 1\ngives a summary of the attributes of the forum data. We see\nthat most posts only contain a few words. As we will see,\nthis makes it difficult to apply traditional document classification methods, which treat each post as a document. We\nalso see that most users write only a few posts, and each\nthread is only a few posts in length. All of this means that\nmost posts have very little or no content, thread, and user\ninformation in common. Our method will seek to address\nthese issues.\n\nMethod\nOur method consists of two main steps. In the first step,\nwe create a matrix which measures the similarity between\nall pairs of posts in the forum. In the next step, we build\na similarity matrix for users by creating a coordinate system\nbased on the similarity matrix from the first step. The results\nof this second step allow us to examine the structure of the\nrelationships and activity of the forum users.\n\nMeasuring Similarity Between Posts\nIf we consider each post as a document, then our goal is\nto establish a notion of similarity between the documents.\nMethods such Latent Dirichlet Allocation (LDA) (Blei, Ng,\nand Jordan 2003) and cosine similarity (Weiss et al. 2005)\nhave been shown to be effective document classification\ntechniques. Since we wish to establish a numerical measure\nof post similarity, we will modify cosine similarity.\nCosine similarity gives a similarity measure between two\ndocuments based on the words within each document. The\nsimplest approach only considers the words shared by the\ntwo documents. However, cosine similarity also considers\n\ntf-idf(D, j) = tf(D, j) \u00d7 log2 (N/df(j)).\nThe cosine similarity between documents D1 and D2 is\ndefined as:\nnorm(D1 ) =\n\nsX\n\ntf-idf(D1 , j)2\n\n(1)\n\ntf-idf(D2 , j)2\n\n(2)\n\nj\n\nnorm(D2 ) =\n\nsX\nj\n\ncosine(D1 , D2 ) =\n\nX \u0012 tf-idf(D1 , j) \u00d7 tf-idf(D2 , j)) \u0013\nj\n\nnorm(D1 ) \u00d7 norm(D2 )\n\n.\n\n(3)\nHowever, cosine similarity is insufficient for analyzing forum data. Cosine similarity is based on word overlap, and is\nmost effective when applied to long documents. However, a\ntypical forum post is only a few sentences long. Additionally, cosine similarity mistreats or ignores information available by considering the threads and the author of the posts.\nThese two issues result in a very sparse similarity matrix -\nmany documents intuitively related by thread or author have\nno relation at all.\nTo address these problems, we modify cosine similarity as\nfollows to take into account all of the available information\nin forum posts:\n\u2022 We append to each post the title of the thread in which\nit appears. This makes posts within the same thread more\nsimilar in word content and therefore closer in cosine similarity.\nPosts made in the same thread might share little or no\nwords in common, even though they are on the same\ntopic. Such posts would not be considered related under\nusual cosine similarity. Table 2 illustrates this problem\nvia an example of a typical thread in our data set.\nWe use the thread titles since they roughly represent the\ntopic of the thread. Additionally, a user typically only\nreads the title of the thread before deciding to read the\nrest of the thread and then possibly making a response\npost. Therefore, the thread title captures both post topic\nand user interests.\n\n\fTable 2: A sample thread from the corporate forum data set.\nHere the posts in the thread do not share many words in common. Traditional document classification methods would\ntherefore consider these posts nearly unrelated.\nPost 1\nPost 2\nPost 3\nPost 4\n\nThread Title: Madriva 2007 3D desktop\nAnybody tried mandriva 2007? Its cool with\na XGL 3D desktop.. But is hungry for RAM..\nYou should give ubuntu 6.10 (or the 7.04 dev)\na try. You might also find this interesting:\n[HYPERLINK]\nAnd lookout for KDE Plasma. More info in :\n[HYPERLINK]\nHere are few resources on getting Beryl\n(beryl.. is extremely irresistable.. enter at\nyour own risk :-) )\n[HYPERLINK] [HYPERLINK] [HYPERLINK] (best of all)\n\n\u2022 We modify the tf-idf(D, j) measure of word importance\nto take into account the thread in which document D appears. tf-idf measures word importance only using the\noverall frequency of a word. However, if a word appears\noften in a particular thread, then it is likely to be of particular importance to the thread topic, whether or not it\nis a common word in an overall sense. Table 3 gives an\nexample of a thread which illustrates this point.\nWe define T (D) to be the document consisting of the concatenation of all posts in the thread containing document\nD. We then define:\ndf(j)\n.\ndfT (D) (j) =\ntf(T (D), j)\nWhich gives us the following formula:\ntf-idfT (D) (D, j) = tf(D, j) \u00d7 log2 (N/dfT (D) (j)). (4)\nThis new measure takes into account the importance of\na word within a thread. Examining the original df(j)\nmeasure, we see that as df(j) increases, the importance\nof word j goes down. Dividing by the thread word frequency tf(T (D), j) means dfT (D) (j) decreases as a word\nbecomes more common within a thread.\nNote that in combination with the previous point, we have\nthat the words in the thread title are of great importance to\nthe thread topic. Since the thread title usually represents\nthe topic, this is a desirable effect.\n\u2022 After computing the cosine similarity using the above\nmodifications, we add an additional term to capture our\nbelief that documents authored by the same user are similar. Since we want this term to be independent of both\npost content and the particular user, this term should be a\nuniversal constant.\nOur goal is not to cluster posts or to assign posts to users.\nRather, we are interested in examining the relationships\nbetween users. This term does not affect the distance of\nposts written by different users. Therefore, the inclusion\nof an author term is not a circular step. However, this term\n\nTable 3: A sample thread from the corporate data set. Here\nthe words \"data\" and \"migration\" appear frequently in the\nposts. Therefore, within this thread, these words should be\ngiven high importance. In the usual tf-idf framework, these\nwords would be given high importance only if they were\nrelatively rare throughout the forum.\nPost 1\nPost 2\n\nPost 3\n\nPost 4\n\nThread Title: data migration\nBasically what is data migration?\nData migration, basically means to porting data from one environment (format/OS/Database/Server etc) to other\nenvironment.\nThe process of translating data from one format to another. Data migration is necessary when an organization decides to use new\ncomputing systems or database management\nsystem that is incompatible with the current\nsystem. Typically, data migration is performed by a set of customized programs or\nscripts that automatically transfer the data.\nMigrating to higher version also one of the\npart in data migration.\n\nplays an important role in our analysis which we discuss\nlater.\nWe therefore modify the cosine similarity equations by\nreplacing df(j) with dfT (D) (j), and by replacing each post\nD with post D\u2217 , which has the thread title appended. We\ndefine the function U (D) to return the author of post D. We\nthen define:\nsim(D1 , D2 ) = cosineT (D) (D1\u2217 , D2\u2217 ) + \u03bbI{U (D1 )=U (D2 )}\n(5)\ndist(D1 , D2 ) = max (0, 1 \u2212 sim(D1 , D2 )) .\n\n(6)\n\nHere, \u03bb is our universal author constant as discussed\nabove and cosineT (D) (D1 , D2 ) represents the cosine distance in equation 3, with the modified tf-idf measure given\nin equation 4 replacing tf-idf in both the distance and the\nnorms in equation 1 and 2. We then convert the similarity\nmeasure in equation 5 to a dissimilarity measure via equation 6. Note that the maximum of the cosine similarity measure is 1. This formula is applied to all pairs of posts, giving\nus a dissimilarity matrix between all posts.\n\nMeasuring Similarity Between Users\nWe now seek to create a dissimilarity matrix between all\nusers in the forum, given the dissimilarity matrix between\nall the posts. We first seek to visualize the relative position\nof all the posts in some low dimensional space. Note that\nour dissimilarity matrix only gives us a function of the position of the posts, it does not give the coordinates directly.\nTherefore, in order to visualize this result and to facilitate\nfurther computation, we find a low dimensional representation of the posts which preserves the geometry implied by\n\n\fThe Universal Author Similarity Constant \u03bb We now\ndiscuss the effects and importance of the author constant \u03bb.\nFirst we note that \u03bb has no effect on pairs of posts written by\ndifferent authors. However, it has an effect when we project\nthe posts into a lower dimensional space using principal coordinates.\nOur multidimensional scaling approach attempts to preserve both the distances between pairs of posts written by\nthe same author and pairs of posts written by different authors. As \u03bb grows, posts written by the same author are\ndrawn closer together, while the relative distances between\nposts written by different authors are held fixed. The choice\nof \u03bb represents the degree to which we believe an author's\nposts are similar to each other. In general, we seek a scaling which preserves and highlights the relationship between\nauthors.\nConsider the following example. We generate four sets\nof 100 points from different 2-dimensional normal distribu-\n\n60\n\u25cf\n\n40\n\u25cf\n\n20\n0\n\u221220\n\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf \u25cf\n\u25cf\u25cf\n\u25cf \u25cf \u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf \u25cf\u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n2\n\n\u221220\n\n\u25cf\n4\n\n\u25cf\n1\n\n\u25cf\n3\n\n0\n\n20\n\n\u25cf\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\u25cf\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf \u25cf \u25cf\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\u25cf \u25cf\u25cf \u25cf\n\u25cf\n\n\u25cf\n2\n\n\u25cf\n3\n\n\u25cf\n1\n\n\u221220\n\u221240\n\n\u221220\n\n0\n\n20\n\n40\n\nPrincipal Coordinate 1\n\nLambda = 4\n\nLambda = 6\n\n\u25cf\n3\n\n\u25cf\n2\n\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf \u25cf \u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf \u25cf\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf \u25cf\u25cf\n\u25cf \u25cf\u25cf\n\n\u25cf\n1\n\n\u221260\n\n\u25cf\u25cf\n\n0\n\n\u221240\n\n20\n\n\u221240\n\n\u25cf\n4\n\n20\n\n40\n\n\u25cf\n4\n\n\u221220\n\n40\n\nPrincipal Coordinate 1\n\n40\n\n0\n\nPrincipal Coordinate 2\n\nLambda = 2\n\nPrincipal Coordinate 2\n\nPrincipal Coordinate 2\n\nM = U \u03a3V T .\nWhere \u03a3 is a diagonal matrix consisting of the square\nroots of the eigenvalues of M . The matrices U and V are\nmatrices whose columns are the eigenvectors of M M T and\nM T M , respectively. \u03a3V T gives a projection of the rows of\nM into a new coordinate system.\nIf we only use the first few coordinates in this projection, this gives us a low dimensional representation of the\ndata, where the distance between all the posts are preserved\nas best as possible (for more on multidimensional scaling,\nsee Cox and Cox 1994). Note that these coordinates are\ngiven in order of importance, so we can usually use only\na few and capture a great majority of the geometry. The relative importance of the coordinates is usually deduced from\nthe eigenvalues.\nUsing this low dimensional representation of the posts,\nwe can easily visualize the relationships and properties of\nthe users. For example, by plotting the first two principal components we can see the relative position and spread\nof each user's posts. This allows us to visually investigate\nwhich users are similar to each other, as well as which users\npost about a wide variety of topics and in a wide variety of\nthreads.\nTowards the main goal of this paper, we can also use this\nrepresentation to characterize users. We can give each user\na single set of coordinates by finding the centroid of all that\nuser's posts in this low dimensional space. This roughly\ngives us a center which follows the areas of high density.\nUsing these centroids, we can simply make a distance matrix between users by taking the euclidean distance between\nall pairs of user centroids. This distance matrix can characterize the social network structure in a wide variety of ways,\nsuch as clustering, spanning trees, or nearest neighbor methods.\n\nLambda = 0\n\nPrincipal Coordinate 2\n\nthe original dissimilarity matrix. We achieve this via principal coordinate analysis (Hastie, Tibshirani, and Friedman\n2001).\nThe method proceeds as follows. Given a dissimilarity\nmatrix M , we obtain the singular value decomposition:\n\n\u221220\n\n0\n\n20\n\n40\n\nPrincipal Coordinate 1\n\n60\n\n60\n20\n0\n\n\u25cf\u25cf\n\u25cf \u25cf\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\u25cf \u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\n\u25cf\n1\n\n40\n\n\u25cf\n2\n\n\u25cf\n3\n\n\u221220\n\u221240\n\n\u25cf\n4\n\n\u221260\n\u221260\n\n\u221220\n\n20\n\n60\n\nPrincipal Coordinate 1\n\nFigure 1: Example of the effect of the author constant \u03bb on\nmultidimensional scaling. Here we see that as \u03bb increases,\nthe points each of the four groups are drawn towards each\nother.\n\ntions, each with fixed variance \u03a3 = diag(3, 3), and means\n(1, 1), (1, \u22121), (\u22121, 1), (\u22121, \u22121). These four groups overlap in the original space, and even after multidimensional\nscaling it is still difficult to distinguish the groups or characterize their relative positions. Now consider adding a constant \u03bb to the within group similarities. Figure 1 shows the\neffect of increasing \u03bb on these data. The groups are pulled\ncloser together within themselves, but pushed apart from\neach other. However, the relative distance and position of\nthe four groups is preserved. We can think of \u03bb as a smoothing term, which reduces within group variance. \u03bb allows a\nclear visualization and representation of relative group position after multidimensional scaling.\nReturning to our application, we recall that we used the\nprojection to estimate the distances between users. We estimate this with a norm of the difference of two centroids.\nAs we saw in the example, \u03bb has the effect of reducing the\nvariance within each group. This reduces the variance of the\ncentroid, which is the estimator of the position of the user\nin this space. As the norm is a function of the centroids,\n\u03bb reduces the variance of the estimators for the distances\nbetween users. However, \u03bb introduces a bias towards a geometry of maximally separated users. We now discuss this\ntradeoff in detail.\nSuppose we rearrange the rows and columns of the post\nsimilarity matrix so each of the K users' posts appear as\na block on the diagonal. Thus, the addition of the author\nconstant term to the dissimilarity matrix can be written as:\n\n\f8 Users: First Two Principal Coordinates\n(Full Similarity Measure)\n\n(7)\n(8)\n\n2.0\n\u25cf\n\n(9)\n\n0\n0 . . . AK\nWhere Ai is a square matrix of 1s with dimension equal\nto the number of posts written by author Ai . I is the identity\nmatrix with dimension equal to the total number of posts.\nWe can think of choosing \u03bb in terms of two theoretical\nextremes. In one case, a very small \u03bb has no effect on the\nscaling. The Davis-Kahan theorem (Luxburg 2007) gives a\nbound on the difference of the eigenspaces of a matrix X,\nand the matrix X + Y . In our case, since we work with the\neigenspace of M T M we have that this difference is roughly\nbounded by the Frobenius norm of the matrix \u03bb2 HH T \u2212\n2\u03bbM H. As \u03bb becomes small, this norm decreases, and so\nthe author term has a diminishing effect. The variance of the\ncentroids is not reduced.\nIn the other case, a large \u03bb means that the matrix M \u2212 \u03bbH\nis dominated by the second term. This makes the eigenspace\nof the matrix (M \u2212 \u03bbH)T (M \u2212 \u03bbH) approach that of\n\u03bb2 H T H. Since H is block diagonal, then H T H will also\nbe block diagonal, so the eigenvectors will be roughly piecewise constant. Consequently, the projection pulls the posts\nwritten by each author together into a single point, each\nof which are maximally separated from the other authors'\npoints. All the information about relationships between authors has been lost. \u03bb has thus oversmoothed the data.\nWe therefore choose \u03bb small enough to avoid the second\ncase, but large enough so that the information about post\nauthor is not completely ignored. As we argued above, the\nsmoothing effect of \u03bb improves the variance of the estimators for user distance. Pulling together each author's posts\nsomewhat has the desirable effect of separating and clumping authors, thus highlighting their relative positions and\ncontrolling outlier documents.\nThis leads us to choose \u03bb based on the nonzero similarities found by applying cosine similarity without considering\nauthor, i.e. the nonzero cosineT (D) (D1\u2217 , D2\u2217 ) terms in equation 6. For most forums we recommend using the 75th quantile of these nonzero entires for the value of \u03bb. However, for\nforums with shorter overall thread length, we recommend a\nhigher quantile. This is because threads give very little information in this case.\n\nResults\nWe present results on two subsets of our data. First, we examine a small set of fairly active users. This illustrates and\nvisualizes the results of our method. Second, we present results on a larger set of active users, and evaluate the method\nvia a comparison to document classification.\n\nPreprocessing\nWe first need to preprocess the forum post data. For our\nmethod, we first append the thread title to each post. We\n\nPrincipal Coordinate 3\n\nM \u2217 = M \u2212 \u03bbH\nMij = 1 \u2212 cosineT (D) (Di\u2217 , Dj\u2217 )\n\uf8f9\n\uf8ee\nA1 0 . . .\n0\n0 \uf8fa\n\uf8ef 0 A2 . . .\n\u2212 I.\nH=\uf8ef\n..\n.. \uf8fa\n..\n\uf8f0 ...\n.\n.\n. \uf8fb\n\n1.5\n\n5\n\u25cf\n1.0\n0.5\n\n\u25cf\n3\n\n\u25cf\n7\n\u25cf\n6\n\n0.0\n\n\u22120.5\n\nUser 1\nUser 2\nUser 3\nUser 4\nUser 5\nUser 6\nUser 7\nUser 8\n\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\u25cf \u25cf\n\u25cf\u25cf \u25cf\n\u25cf \u25cf\u25cf\u25cf\n\u25cf\u25cf\u25cf\n\u25cf\n\n\u25cf\n8\n\n\u25cf\n1\n\n\u25cf\n4\n\u25cf\n2\n\n\u22121.0\n\u22120.5\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\nPrincipal Coordinate 2\n\nFigure 2: The second and third principal coordinates for the\n8 user data set. The centroids are given by the numbers in\nthe white circle. We can clearly see each user separately,\nbut are still able to see spread information and the similarity\nbetween users.\nnext follow conventional text analysis methods, by removing HTML code, removing stopwords, and performing word\nstemming. See Weiss et al. for a complete discussion of\nthese standard techniques.\nWe build a dictionary based on all the words in all of the\nprocessed posts. Thus, each post is now a vector of counts\nof each word in the dictionary.\n\nIllustration: A Small Set of Active Users\nIn order to illustrate our method clearly and intuitively, we\npresent results for a small set of users. We consider all users\nwho wrote between 200 and 210 posts on the forum. In the\nfull dataset, the set of users who wrote more than 200 posts\naccounts for about 50% of all posts. Therefore, this range\nrepresents users who are roughly in the middle in terms of\nposting activity. These users are also easier to compare since\nthey wrote roughly the same number of posts. This range\ngives us eight users in total.\nUsing only the subset of posts authored by these users, we\napply our previously described method. Note that the dictionary is built from all of the posts, so the tf-idf measures take\ninto account the overall importance of the words. Therefore, the tf-idf measures are not biased in this case, and so\nour results for these particular users will not differ greatly\nfrom those obtained when all of the posts are included in the\nanalysis.\nFigure 2 shows the second and third principal coordinates\nfor the posts. We use \u03bb = 0.059 for our universal author\nsimilarity constant. This is the 75th quantile of our nonzero\nsimilarities obtained without taking author into account. We\ncan see a clear separation between users, as well as differ-\n\n\fActive Users: Minimal Spanning Tree\n\nComplete Linkage Clustering Dendrogram\n\n2.25\n\u25cf\n\n5\n\n2\n\n2.20\n\n1\n\n\u25cf\n\nHeight\n\n\u25cf\n\n\u25cf\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\n\u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\n3\n\n2.15\n\n7\n\n\u25cf\u25cf\n\n6\n\n2.10\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\n\u25cf \u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\u25cf\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\u25cf\n\u25cf\u25cf\n\n\u25cf\u25cf\n\u25cf\n\u25cf\n\n\u25cf\n\n8\n\n4\n\n2.05\n\nFigure 3: Cluster dendrogram for the 8 users. We can create a clustering by drawing a horizontal line at any height\nin the tree, and taking the clustering given by the links below the line. Overall, we see that two clusters seems most\nappropriate, with clusters {1, 5} and {2, 3, 4, 6, 7, 8}\ning spreads for each user. Looking at the centroids, we see\nsix users who are somewhat similar, and two users who are\nseparated by these principal coordinates. These two users\nalso seem to have larger spreads than the other six, which\npossibly indicates a broader interest in topics.\nTo illustrate an application of the user distance, we next\nbuild a hierarchical clustering tree using complete linkage (Hastie, Tibshirani, and Friedman 2001). Figure 3 displays this tree. We see that there are two main clusters: users\n{1, 5} and users {2, 3, 4, 6, 7, 8}. This is consistent with our\nearlier display of the users in Figure 2. This clustering gives\na picture of the network structure within this 8-user group.\n\nActive Users\nWe now consider a large set of active users in the corporate forum data set. We take all users who wrote between\n200 and 400 posts on the forums. This gives us 71 users\nand 18,682 total posts. We consider this subset for computational and interpretive reasons.\nWe apply our method to these 71 users, with \u03bb = .054.\nThis \u03bb is obtained from the 75th quantile of the nonzero similarity matrix entries for the modified cosine similarity measure. Due to the large number of users in this data set, plots\nof the principal coordinates do not give clear pictures of the\nrelationships between users. Note that plots of subsets of\nusers can show individual user spread.\nAs mentioned before, there are many ways to look at the\nsocial network structure once we calculate the distance between all of the users. We present two well-known examples\nhere: complete linkage clustering and a minimal spanning\ntree (Prim 1957).\n\nFigure 4: The minimal spanning tree for the 71 users. This\ntree gives relationships similar to K-nearest-neighbors.\nFigure 5 shows the complete linkage hierarchical cluster\ndendrogram for the 71 users. We see that a five cluster solution looks appropriate. The majority of the users are in\ntwo of these five clusters. The remaining three groups are\nsmall and separated from these two large groups. In particular, we see a group of three users located far away from the\nother four groups. This group may represent a collection of\nusers who have the same specialized interest. The two large\ngroups perhaps deal with general or popular topics.\nFigure 4 displays the minimal spanning tree (MST)\nfor these 71 users. A minimal spanning tree provides a\nway to relate users similar to K-nearest-neighbor methods.\nRoughly, the closer users are, the more similar they are in\nterms of interest and interaction. The minimal spanning tree\nshows that the structure of the network is varied. We see\nseveral clumps of users in the MST, as well as several users\nwho are very far from any others. The company could infer that the clumped users share similar interests and often\ninteract, and thus may make a good project team.\nEvaluating the Method Since characterizing a social network's structure is not a prediction problem, evaluation of\nthese results can instead be done by comparison to another\nmethod with similar output. For comparison, we approach\nthe problem from a purely conventional document classification viewpoint. We create a single document for each of the\n71 users in the large set by concatenating all of that user's\nposts (note we do not include thread titles in the posts). We\nthen use cosine similarity to compare these documents, and\nthus arrive at a similarity matrix between users. This similarity tries to measure the relationship between user interest\npurely based on textual information; it ignores all of the forum structure, such as the threads.\nTo be consistent, we also apply complete linkage hierarchical clustering using this similarity measure (see Figure 6).\n\n\fComplete Linkage Clustering Dendrogram\n(Post Based Distance)\n\nComplete Linkage Clustering Dendrogram\n(User Text\u2212Only Distance)\n\n1.2\n0.94\n1.0\n0.92\n\n24\n\n26\n40\n\n0.8\n\n60\n\n0.90\n\nHeight\n\nHeight\n\n52\n\n0.6\n37\n\n0.88\n\n0.4\n6\n\n24\n62\n\n0.0\n\n14\n\n30\n1952\n54\n67\n29 63\n3661 25 53 2232\n68 15 5138 35\n50\n12\n\n18\n\n53\n1\n\n69\n12\n\n938\n\n22\n\n67\n62\n31\n34\n\n50\n\n65\n\n39\n48\n32\n59\n64\n44\n3\n70 66\n41 68\n57\n37\n20\n36\n210 43\n45\n56\n14\n63\n51\n17\n49\n28 35\n61\n27\n25\n8\n47\n15\n71\n42\n58\n11\n23 55\n16\n\n54\n29 4\n\n13\n\n46\n\n0.2\n\n46\n30 5\n6 21\n19\n\n7 2639\n60\n9\n33\n\n0.86\n\n4\n\n31\n58\n28\n41\n42\n69\n55\n2\n48\n47\n10\n64 49\n66 40\n23 113 13\n71\n20\n45\n59 341 43\n16\n44\n56\n8\n65\n57\n18\n5\n27\n17\n2170\n\nFigure 5: Cluster dendrogram for the 71 users. We can create a clustering by drawing a horizontal line at any height\nin the tree, and taking the clustering given by the links below the line. Overall, we see that five clusters seems most\nappropriate.\n\nWe see that in both cases, users 30, 6, and 19 are in a cluster\nwhich splits from the rest at a large height. This indicates\nthat both methods found that these users are very far away\nfrom the others, due to their different textual information.\nWe also see that both methods find many pairs of users who\nare merged into the same cluster at the bottom of the tree.\nThis indicates that both methods find some similar nearest\nneighbor pairs. These similarities show some consistency\nbetween the two methods. Although our method includes a\ngreat deal more information than the text of the posts, we\nstill see that the text plays a strong role in defining user similarity.\nHowever, there are differences. For example, in the pure\ntext based method, user 52 is considered to be in the same\ngroup as the \"strange\" users 30, 6, and 19. On the other\nhand, our method finds user 52 to be closely related to a\ndifferent group of users. Looking at his posts, it is clear\nthat this user often replies to others by posting a hyperlink\nor an attachment. Such replies do not contain any text with\nreference to the topic. Our method includes the thread titles\nand therefore links this user to the other users who post in\nthe same threads. Therefore, out method has captured this\nuser's interactions and interests more fully.\nOverall, we see the text-only approach does not appear to\nhave as clear of a group structure. If we compare the fivecluster solutions in the complete linkage dendrograms, we\nsee that the text-only approach gives four very small clusters, and one giant cluster. This is not an informative picture\nof the social network structure. Complete linkage clustering\nrelies on a global criteria. We now examine single linkage\nclustering (refer to figure 7), which relies on local effects.\n\n0.84\n\n7\n33\n\nFigure 6: Complete linkage clustering using a text-only similarity measure. There are some similarities to the clustering\nusing our similarity measure, but this tree is not as useful for\nclustering users.\nWe see that the text-only approach gives a degenerate structure. This \"chaining effect\" makes any cluster structure impossible to recover. On the other hand, our method gives a\nsingle linkage structure which still has cluster information.\nTherefore, our method gives a more detailed network structure both locally and globally.\n\nDiscussion\nOur main contribution is a new similarity measure between\nposts in a forum. This measure effectively modifies document similarity to incorporate the special structure of forums. We discussed the properties of our modification, and\npresented some results on a real data set.\nUsers in a forum demonstrate their interests and interactions with other users in two ways. First, users write\nposts whose words can tell us in which topics they are interested. Second, users post in particular threads, indicating\nboth topic interest and interaction with the other users who\nhave already posted in the thread. By including the title of\nthreads in each post, we view both types of information in a\nsingle unified context. User interactions within a thread are\ntransformed into shared words.\nIn our database, thread titles tend to be very short once\nwe remove stopwords. Therefore, the longer the post, the\nsmaller the effect of the thread title on cosine similarity.\nSince longer posts contain more textual information, this is a\ndesirable effect. Our modification give us information about\nposts which are otherwise hard to characterize.\nIn our analysis of the corporate forum data, we compared our method to a traditional document classification\napproach. We showed that the addition of information about\nthreads and authors was critical for giving a complete pic-\n\n\fSingle Linkage:\nPost Based User Distance\n0.35\n\nuser location besides the centroids proposed in this paper\nmay lead to more rich estimators of user distance. We are\nalso interested in additional validation of this method on data\nwith some known and recoverable social structure.\n\nSingle Linkage:\nUser Text\u2212Only Distance\n0.90\n\n0.30\n\n0.89\n0.88\n\nReferences\n\n0.87\n\n[Adomavicius and Tuzhilin 2005] Adomavicius, G., and\nTuzhilin, A. 2005. Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and\nPossible Extensions. IEEE Transactions on Knowledge\n17(6):734\u2013749.\n[Basu, Hirsh, and Cohen 1998] Basu, C.; Hirsh, H.; and\nCohen, W. 1998. Recommendation as classification: Using social and content-based information in recommendation. In Proceedings of the Fifteenth National Conference\non Artificial Intelligence, 714\u2013720. AAAI Press.\n[Bennett and Lanning 2007] Bennett, J., and Lanning, S.\n2007. The Netflix Prize. In Proceedings of KDD Cup and\nWorkshop.\n[Blei, Ng, and Jordan 2003] Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent Dirichlet Allocation. Jornal of\nMachine Learning Research 3:993\u20131022.\n[Cox and Cox 1994] Cox, T. S., and Cox, M. A. 1994. Multidimensional Scaling. Chapman & Hall.\n[Hastie, Tibshirani, and Friedman 2001] Hastie, T.; Tibshirani, R.; and Friedman, J. 2001. The Elements of Statistical\nLearning. Springer.\n[Hogg and Szabo 2008] Hogg, T., and Szabo, G. 2008. Diversity of online community activities. In HT '08: Proceedings of the nineteenth ACM conference on Hypertext\nand hypermedia, 227\u2013228. New York, NY, USA: ACM.\n[Holland and Leinhardt 1981] Holland, P. W., and Leinhardt, S. 1981. An Exponential Family of Probability Distributions for Directed Graphs. Journal of the American\nStatistical Association 76(3):33\u201350.\n[Linden, Smith, and York 2003] Linden, G.; Smith, B.; and\nYork, J. 2003. Amazon.com Recommenda- tions: Item-toItem Collaborative Filtering. In IEEE Internet Computing.\n[Luxburg 2007] Luxburg, U. 2007. A tutorial on spectral\nclustering. Statistics and Computing 17(4):395\u2013416.\n[Prim 1957] Prim, R. 1957. Shortest connection networks\nand some generalizations. Bell Syst. Tech. J. 36:1389\u2013\n1401.\n[Salakhutdinov, Mnih, and Hinton 2007] Salakhutdinov,\nR.; Mnih, A.; and Hinton, G. 2007. Restricted boltzmann\nmachines for collaborative ltering. In ICML.\n[Sarwar et al. 2000] Sarwar, B. M.; Karypis, G.; Konstan,\nJ. A.; and Riedl, J. T. 2000. Application of dimensionality\nreduction in recommender system - a case study. In In ACM\nWebKDD Workshop.\n[Taskar et al. 2003] Taskar, B.; Wong, M.-f.; Abbeel, P.;\nand Koller, D. 2003. Link prediction in relational data.\nIn Neural Information Processing Systems.\n[Weiss et al. 2005] Weiss, S. M.; Indurkhya, N.; Zhang, T.;\nand Damerau, F. J. 2005. Text Mining. Springer.\n\nHeight\n\nHeight\n\n0.25\n0.20\n0.15\n\n0.86\n0.10\n0.85\n0.05\n0.84\n\nFigure 7: Left: single linkage dendrogram using our user\nsimilarity. Right: single linkage dendrogram using the textonly user similarity. Our method shows more local group\nstructure.\nture of the network structure. This is because posts often\ndo not, in their text, contain references to the topic or to the\nother users in the thread. The additional information allows\nmore varied relationships between users.\nThere are alternatives to the principal coordinates method\nfor defining user dissimilarity. Given post similarity, we\ncould choose among many methods to obtain user dissimilarity or distance. For example, we could use the average\ndistance between all pairs of posts written by the two authors:\nP\ndist(A1 , A2 ) =\n\nDOC(A1 )\n\nP\n\nDOC(A2 )\n\ndist(Di , Dj )\n\n|DOC(A1 )| |DOC(A2 )|\n\n,\n\nwhere:\nDOC(A) = {D : U (D) = A}\nFor this approach, the author term is never included and\ntherefore has no effect. The principal coordinate approach,\nhowever, seeks to preserve the distance for documents both\nbetween and within a user. Further, \u03bb has a smoothing effect on the principal coordinate projections, and if properly\nchosen improves the variances of the estimates of user distances. Therefore, our user distances take more information\ninto account. Principal coordinates also allow for easy visualization of the post and user relationships.\nPrincipal coordinates also can allow additional users to be\nincluded in the analysis via a projection. This allows us to\nuse only a subset of posts or users to generate the coordinates. We can project the remaining posts into this coordinate space, and thus learn about a larger set of posts or users.\nFor example, in our data we could use the large set of active\nusers to define coordinates for the great number of low activity users.\nWe need a more systematic way to pick the author constant \u03bb. We believe \u03bb also has beneficial properties with regard to statistical testing. We would like to develop a framework to investigate these properties. Different estimators of\n\n\f[Yang, Adamic, and Ackerman 2008] Yang, J.; Adamic,\nL. A.; and Ackerman, M. S. 2008. Competing to Share\nExpertise: the Taskcn Knowledge Sharing Community. In\nICWSM2008.\n\n\f"}