{"id": "http://arxiv.org/abs/1108.2283v2", "guidislink": true, "updated": "2013-11-20T19:15:05Z", "updated_parsed": [2013, 11, 20, 19, 15, 5, 2, 324, 0], "published": "2011-08-10T20:25:08Z", "published_parsed": [2011, 8, 10, 20, 25, 8, 2, 222, 0], "title": "A survey on independence-based Markov networks learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.1519%2C1108.1027%2C1108.1586%2C1108.1047%2C1108.1415%2C1108.3734%2C1108.3771%2C1108.0138%2C1108.1990%2C1108.5093%2C1108.3194%2C1108.4288%2C1108.2665%2C1108.3970%2C1108.5765%2C1108.0811%2C1108.2178%2C1108.0346%2C1108.3945%2C1108.0833%2C1108.4189%2C1108.0742%2C1108.2967%2C1108.1375%2C1108.3360%2C1108.5180%2C1108.3119%2C1108.5237%2C1108.4504%2C1108.2313%2C1108.4907%2C1108.0184%2C1108.1724%2C1108.4441%2C1108.5340%2C1108.4299%2C1108.4766%2C1108.2586%2C1108.1533%2C1108.5334%2C1108.2468%2C1108.3778%2C1108.5953%2C1108.1719%2C1108.3279%2C1108.5553%2C1108.2075%2C1108.0240%2C1108.5075%2C1108.6151%2C1108.1328%2C1108.4957%2C1108.0590%2C1108.1218%2C1108.2629%2C1108.3749%2C1108.3608%2C1108.4583%2C1108.5603%2C1108.4937%2C1108.4799%2C1108.1848%2C1108.0268%2C1108.3180%2C1108.2531%2C1108.1424%2C1108.3634%2C1108.2554%2C1108.5980%2C1108.4125%2C1108.0430%2C1108.5065%2C1108.1771%2C1108.2848%2C1108.0435%2C1108.2758%2C1108.1864%2C1108.0370%2C1108.5817%2C1108.1013%2C1108.0900%2C1108.5286%2C1108.4158%2C1108.3502%2C1108.2283%2C1108.2768%2C1108.2973%2C1108.6250%2C1108.2523%2C1108.3692%2C1108.1772%2C1108.4339%2C1108.5420%2C1108.6308%2C1108.4010%2C1108.5991%2C1108.3543%2C1108.1565%2C1108.4951%2C1108.5921%2C1108.3741&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A survey on independence-based Markov networks learning"}, "summary": "This work reports the most relevant technical aspects in the problem of\nlearning the \\emph{Markov network structure} from data. Such problem has become\nincreasingly important in machine learning, and many other application fields\nof machine learning. Markov networks, together with Bayesian networks, are\nprobabilistic graphical models, a widely used formalism for handling\nprobability distributions in intelligent systems. Learning graphical models\nfrom data have been extensively applied for the case of Bayesian networks, but\nfor Markov networks learning it is not tractable in practice. However, this\nsituation is changing with time, given the exponential growth of computers\ncapacity, the plethora of available digital data, and the researching on new\nlearning technologies. This work stresses on a technology called\nindependence-based learning, which allows the learning of the independence\nstructure of those networks from data in an efficient and sound manner,\nwhenever the dataset is sufficiently large, and data is a representative\nsampling of the target distribution. In the analysis of such technology, this\nwork surveys the current state-of-the-art algorithms for learning Markov\nnetworks structure, discussing its current limitations, and proposing a series\nof open problems where future works may produce some advances in the area in\nterms of quality and efficiency. The paper concludes by opening a discussion\nabout how to develop a general formalism for improving the quality of the\nstructures learned, when data is scarce.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.1519%2C1108.1027%2C1108.1586%2C1108.1047%2C1108.1415%2C1108.3734%2C1108.3771%2C1108.0138%2C1108.1990%2C1108.5093%2C1108.3194%2C1108.4288%2C1108.2665%2C1108.3970%2C1108.5765%2C1108.0811%2C1108.2178%2C1108.0346%2C1108.3945%2C1108.0833%2C1108.4189%2C1108.0742%2C1108.2967%2C1108.1375%2C1108.3360%2C1108.5180%2C1108.3119%2C1108.5237%2C1108.4504%2C1108.2313%2C1108.4907%2C1108.0184%2C1108.1724%2C1108.4441%2C1108.5340%2C1108.4299%2C1108.4766%2C1108.2586%2C1108.1533%2C1108.5334%2C1108.2468%2C1108.3778%2C1108.5953%2C1108.1719%2C1108.3279%2C1108.5553%2C1108.2075%2C1108.0240%2C1108.5075%2C1108.6151%2C1108.1328%2C1108.4957%2C1108.0590%2C1108.1218%2C1108.2629%2C1108.3749%2C1108.3608%2C1108.4583%2C1108.5603%2C1108.4937%2C1108.4799%2C1108.1848%2C1108.0268%2C1108.3180%2C1108.2531%2C1108.1424%2C1108.3634%2C1108.2554%2C1108.5980%2C1108.4125%2C1108.0430%2C1108.5065%2C1108.1771%2C1108.2848%2C1108.0435%2C1108.2758%2C1108.1864%2C1108.0370%2C1108.5817%2C1108.1013%2C1108.0900%2C1108.5286%2C1108.4158%2C1108.3502%2C1108.2283%2C1108.2768%2C1108.2973%2C1108.6250%2C1108.2523%2C1108.3692%2C1108.1772%2C1108.4339%2C1108.5420%2C1108.6308%2C1108.4010%2C1108.5991%2C1108.3543%2C1108.1565%2C1108.4951%2C1108.5921%2C1108.3741&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This work reports the most relevant technical aspects in the problem of\nlearning the \\emph{Markov network structure} from data. Such problem has become\nincreasingly important in machine learning, and many other application fields\nof machine learning. Markov networks, together with Bayesian networks, are\nprobabilistic graphical models, a widely used formalism for handling\nprobability distributions in intelligent systems. Learning graphical models\nfrom data have been extensively applied for the case of Bayesian networks, but\nfor Markov networks learning it is not tractable in practice. However, this\nsituation is changing with time, given the exponential growth of computers\ncapacity, the plethora of available digital data, and the researching on new\nlearning technologies. This work stresses on a technology called\nindependence-based learning, which allows the learning of the independence\nstructure of those networks from data in an efficient and sound manner,\nwhenever the dataset is sufficiently large, and data is a representative\nsampling of the target distribution. In the analysis of such technology, this\nwork surveys the current state-of-the-art algorithms for learning Markov\nnetworks structure, discussing its current limitations, and proposing a series\nof open problems where future works may produce some advances in the area in\nterms of quality and efficiency. The paper concludes by opening a discussion\nabout how to develop a general formalism for improving the quality of the\nstructures learned, when data is scarce."}, "authors": ["Federico Schl\u00fcter"], "author_detail": {"name": "Federico Schl\u00fcter"}, "author": "Federico Schl\u00fcter", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/s10462-012-9346-y", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1108.2283v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1108.2283v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "35 pages, 1 figure", "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1108.2283v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1108.2283v2", "journal_reference": "Schl\\\"uter, F. (2011). A survey on independence-based Markov\n  networks learning. Artificial Intelligence Review, 1-25", "doi": "10.1007/s10462-012-9346-y", "fulltext": "A survey on independence-based Markov networks learning\n\narXiv:1108.2283v2 [cs.AI] 20 Nov 2013\n\nFederico Schl\u00fcter\n\nReceived: date / Accepted: date\n\nAbstract The problem of learning the Markov network structure from data has become increasingly important in machine learning, and in many other application fields. Markov networks are\nprobabilistic graphical models, a widely used formalism for handling probability distributions in intelligent systems. This document focuses on a technology called independence-based learning, which\nallows for the learning of the independence structure of Markov networks from data in an efficient\nand sound manner, whenever the dataset is sufficiently large, and data is a representative sample\nof the target distribution. In the analysis of such technology, this work surveys the current stateof-the-art algorithms, discussing its limitations, and posing a series of open problems where future\nwork may produce some advances in the area, in terms of quality and efficiency.\nKeywords Markov networks * Structure learning * independence-based * survey\n1 Motivation\nNowadays intelligent systems have to reason in realistic domains, storing knowledge of the world,\nand supporting efficient inference, even when exceptions occur. This is referred to in the literature\nas reasoning under uncertainty. A popular approach taken for reasoning under uncertainty is the use\nof probabilistic models, a statistical analysis tool for statistical inference. The statistical inference\nprocess is used for drawing conclusions from data by calculating the probability of propositional\nsentences. An example of a probabilistic model is the tabular probabilistic model, a function represented as a table that assigns a probability to every possible complete assignment in a domain,\nso that the sum of the probabilities adds up to 1. Figure 1 illustrates an abstract tabular model\nfor a domain with n binary variables V = {X0 , ..., Xn\u22121 }, consisting on 2n tuples, one per possible\nconfiguration of variables.\nHowever, the tabular model presents computational and semantic limitations. First, its storage\nrequirements are exponential in the number of variables, and the size of its respective domains.\nF. Schl\u00fcter\nE-mail: federico.schluter@frm.utn.edu.ar\nHome page: http://dharma.frm.utn.edu.ar/fschluter/\nLab. DHARMa of Artificial Intelligence,\nDept of Information Systems, Facultad Regional Mendoza, National Technological University, Argentina.\n\n\f2\n\nFederico Schl\u00fcter\nX0\n0\n0\n.\n.\n.\n1\n1\n\nX1\n0\n0\n.\n.\n.\n1\n1\n\n...\n...\n...\n...\n...\n...\n...\n...\n\nXn\u22121\n0\n1\n.\n.\n.\n0\n1\n\nPr(X0 , .., Xn\u22121 )\n0.121\n0.076\n.\n.\n.\n0.21\n0.12\n\nFig. 1 An example tabular model over n binary random variables, with 2n numerical parameters.\n\nWhen domains of variables are continuous, such tables are infinite, and in practice some mathematical functions can be used. Nonetheless, in this work the attention is restricted only to discrete\ndistributions, so continuous variables may be considered as discrete variables. Second, queries of\ninterest usually do not involve all the variables, and the cost of computing marginal and conditional probabilities would result in exponential summations of variable combinations. Third, such\nrepresentation has no clear semantics for humans. The common pattern of human knowledge has\nprobabilistic judgments for a small number of propositions. Therefore, conditional independences\nare a natural way of representing probability distributions. It is common for people to judge a\nthree-place relationship of conditional dependency, i.e., X influences Y , given Z.\nUsing independences may reduce the exponential requirements of the tabular model. For example, making the simple assumption that all the n variables in Figure 1 are mutually independent\nallows decomposing the joint probability distribution as\nPr(X0 , ..., Xn\u22121 ) =\n\nn\u22121\nY\n\nPr(Xi ).\n\ni=0\n\nSuch decomposition requires a polynomial number (n) of exponentially smaller tables with only two\nrows. Figure 2 illustrates a model assuming that all the binary variables are mutually independent,\nconsisting only of n tables with 2 tuples each.\n\nX0\n0\n1\n\nPr(X0 )\n0.21\n0.79\n\n,\n\nX1\n0\n1\n\nPr(X1 )\n0.45\n0.55\n\n...\n\nXn\u22121\n0\n1\n\nPr(Xn\u22121 )\n0.42\n0.58\n\nFig. 2 An example model assuming that all the variables of the domain are mutually independent, with n tables of\nonly 2 numerical parameters each.\n\nTo address all these problems, namely the exponential storage requirements, the exponential\ncost of computing marginal and conditional probabilities, and the lack of explicitness of the model,\nseveral researchers in the late 80's created probabilistic graphical models, or simply, graphical models,\na well-established formalism for representing compactly joint probability distributions. They are\ncomposed of an independence structure, and a set of numerical parameters. The structure encodes\nthe independences present in the distribution, and then defines a family of probability distributions.\nThe set of numerical parameters defines a unique distribution among this family and quantifies the\nrelationships in the structure. Such representation is explained in more detail in Section 2.\n\n\fA survey on independence-based Markov networks learning\n\n3\n\nThe most important types of graphical models are Bayesian networks and Markov networks\nPearl (1988). The well-known Bayesian networks are graphical models for encoding distributions\nwhere dependencies are representable by a directed acyclic graph. Markov networks (also known as\nMarkov Random Fields, undirected graphical models, or simply undirected models) encode distributions where dependencies are representable by an undirected graph. Three of the most influential\ntextbooks on this topic published in the last three decades are Pearl (1988), Lauritzen (1996) and\nKoller and Friedman (2009).\nThere have been many applications of graphical models in a wide range of fields during recent\nyears. Some examples are present in the areas of computer vision and image analysis. Besag et al\n(1991) gives two examples, one in archeology, the other in epidemiology; in Anguelov et al (2005),\naddressing the problem of segmenting 3D scan data into objects or object classes; or Li (2001),\na complete textbook that presents an exposition of Markov Random fields to image restoration\nand edge detection in the low-level domain, and object matching and recognition in the high-level\ndomain. More examples are present in the area of spatial data mining and geostatistics, as those\npresented in the textbook of Cressie (1992), where Markov Random Fields are emphasized for modeling spatial lattice data; or more recently, the work of Shekhar et al (2004) that presents spatial\nanalysis methods and applications for Markov Random Fields in a wide range of fields, including biology, spatial economics, environmental and earth science, ecology, geography, epidemiology,\nagronomy, forestry and mineral prospection. There are also examples for disease diagnosis, such as\nSchmidt et al (2008) that presents a Markov Random Fields based method for detecting coronary\nheart disease processing ultrasound images of echocardiograms. Also in the area of computational\nbiology, Friedman et al (2000) proposes the use of Bayesian networks for discovering interactions\namong genes. More applications of graphical models are present for evolutive optimization searching, as Larra\u00f1aga and Lozano (2002) that describes the use of Bayesian networks for modeling the\nprobability distribution of individuals with high fitness in evolutive algorithms, or more recently,\nAlden (2007) and Shakya and Santana (2008) proposing Markov networks for the same purpose.\nFurther examples are shown for Information Retrieval by Metzler and Croft (2005) and Cai et al\n(2007), to model term dependencies using Markov Random Fields; and for malware propagation,\nKaryotis (2010) analyzes the spatial and contextual dependencies of malware propagation, also using Markov Random Fields. There are many other interesting examples that could be included in\nthis list. Table 1 summarizes all these examples in order to help the readers to choose which method\ncan be a better solution for a certain application.\nThe framework provided by probabilistic graphical models supports three critical capabilities of\nintelligent systems, as highlighted in the textbook of Koller and Friedman (2009):\n\u2013 Representation: a compact and declarative model of the knowledge based on graphs. On the one\nhand, such models are compact, providing a representation of conditional independences present\nin a probability distribution which is efficient and computationally tractable. The compact\nrepresentation of graphical models is achieved by exploiting a principle property present in\nmany distributions: variables tend to interact directly with very few others. On the other hand,\nsince the models are graphical, they are declarative, and a human expert can understand and\nevaluate their semantics and properties.\n\u2013 Inference: given a graphical model, the most fundamental and yet highly non-trivial task is to\ncompute marginal distributions of one or a few variables. This task is usually called inference.\nThrough marginalization it is possible to compute conditionals and posteriors, and to make\npredictions. Inference is also a sub-routine of learning tasks, and is therefore the most elementary sub-routine of graphical models. However, as proven by Cooper (1990), exact inference\n\n\f4\n\nFederico Schl\u00fcter\n\nTable 1 Some example Probabilistic Graphical Models applications.\n\nApplication\n\nReference\n\n\u2013 Computer vision, and image analysis, with examples in archeology and epidemiology\n\nBesag et al (1991)\n\n\u2013 Segmentition of 3D scan data into objects\n\nAnguelov et al (2005)\n\n\u2013 Markov Random Fields for image restoration, edge detection and object matching\n\nLi (2001)\n\n\u2013 Markov Random Fields for spatial data minging and geostatistics\n\nCressie (1992)\n\n\u2013 Markov Random Fields for spatial spatial analysis methods in biology, spatial economics, environmental and earth science, ecology, geography, epidemiology, agronomy,\nforestry and mineral prospection.\n\nShekhar et al (2004)\n\n\u2013 Markov Random Fields method for detecting coronary heart disease.\n\nSchmidt et al (2008)\n\n\u2013 Computational biology. Learning of Bayesian networks\nfor discovering interactions among genes.\n\nFriedman et al (2000)\n\n\u2013 Bayesian networks for evolutive optimization search.\n\nLarra\u00f1aga and Lozano\n(2002)\n\n\u2013 Markov networks for evolutive optimization search.\n\nAlden\n(2007);\nShakya and Santana\n(2008)\n\n\u2013 Information Retrieval. Markov Random Fields for modeling terms dependency.\n\nMetzler and Croft\n(2005); Cai et al (2007)\n\n\u2013 Malware propagation. Markov Random Fields for modeling spatial and contextual\ndependencies.\n\nKaryotis (2010)\n\nis NP-hard in general. There are several methods for working directly with the structure of\ngraphical models, that are in practice orders of magnitude faster than manipulating explicitly\nthe joint probability distribution. The textbook of Koller and Friedman (2009) provides an extensive discussion on this topic, and describes the most popular methods used, such as variable\nelimination, Monte Carlo methods, and loopy belief propagation. Other recent works are treereweighted message-passing of Wainwright et al (2003), Power EP of Minka (2004), generalized\nbelief propagation of Yedidia et al (2004), and Variational message-passing of Winn and Bishop\n(2005). A free and open source library, providing implementations of various exact and approximate inference methods for graphical models, was published recently by Mooij (2010).\n\u2013 Learning: constructing graphical models can be done either by a human expert or by learning\nit automatically from data. There are many algorithms that model the probability distribution\nof historical data, returning a graphical model as the solution. They are really useful, since\nexperts knowledge is not always enough to design a proper model. Therefore, some authors\n\n\fA survey on independence-based Markov networks learning\n\n5\n\nconsider these algorithms a tool for knowledge discovery. Moreover, when constructing models\nfor a specific problem, it is possible to use the data-driven approach, using some part of the\nmodel provided by an expert and filling the details automatically, by fitting the model to data.\nThe large number of success stories claimed using this approach in recent years has resulted in\nsome authors, such as Koller and Friedman, claiming that models produced by this process are\nusually much better than those purely hand constructed.\nIn this work, the specific problem of learning the independence structure of Markov networks\nis reviewed. This is an interesting problem that has resulted in important contributions to this\ndomain in recent years, although many of its core challenges remain unresolved and are under\nintense deliberation. This work focuses on a technology called independence-based learning, which\nallows one to infer the independence structure of Markov networks from data in an efficient and\nsound manner, whenever data is sufficient and a representative sample of the target distribution.\nAn analysis of the current state-of-the-art algorithms for learning Markov networks structure using\nsuch technology is presented, discussing its current limitations, and its potential for improving the\nquality and the efficiency of current approaches.\nThe rest of the document is structured as follows: Section 2 presents an overview of Markov\nnetworks representation. Section 3 discusses the problem of learning Markov networks from data.\nSection 4 provides a review of current independence-based Markov network structure learning algorithms. Finally, Section 5 analyzes the surveyed independence-based algorithms and discusses their\nrelative advantage as well as disadvantages, concluding with a series of open problems that remain\nin the domain of independence-based structure learning for Markov networks.\n\n2 Markov networks representation\nThis section provides an overview of the representation of a specific type of graphical models:\nMarkov networks. Graphical models in general consist of a qualitative and a quantitative component for representing a probability distribution P . Such distribution is given over a domain of n\nvariables, denoted V = {X0 , ..., Xn\u22121 }. The qualitative component is the independence structure G\n(also known as the network, or the graph) of the model, that represents conditional independences\namong the domain variables, and then defines a family of probability distributions. The quantitative\ncomponent is a set of numerical parameters \u03b8, that defines a unique distribution among this family\nand quantifies the relationships in the structure.\n\n2.1 The independence structure\nThe independence structure is a compact representation of conditional independences present in the\nunderlying distribution P . Two variables X and Y are independent conditioned in a set of variables\nZ when knowing the value of Y tells me nothing new about X, if I already know the values of\nvariables in Z. In this work, conditional independence is denoted as (X\u22a5\u22a5Y|Z), and (X 6\u22a5\u22a5Y|Z)\ndenotes conditional dependence.\nThe structure G of a Markov network is an undirected graph with n nodes, each one representing\na random variable in the domain. The edges in the graph encode conditional independences among\nthe variables. Figure 3 shows two examples of undirected structures, both representing domains\nwith n = 12, and variables V = {X0 , . . . , X11 }. The first example in Figure 3 (a) is an irregular\n\n\f6\n\nFederico Schl\u00fcter\n\nFig. 3 Two examples of undirected independence structure: (a) an irregular graph with different grade of connectivity for distinct nodes, and (b) a regular lattice where variables belong to a domain in a spatial problem.\n\ngraph with different grade of connectivity for distinct nodes. The second in Figure 3 (b) is a regular\nlattice where variables belong to a domain in a spatial problem, as typically used for representing 2D\nimages, or for two dimensional Ising spin glasses models (mathematical models of ferromagnetism\nin statistical mechanics).\nThe independence structure is a map of the independences in the underlying distribution, and\nsuch independences can be read from the graph through vertex separation, considering that each\nvariable is conditionally independent of all its non-neighbor variables in the graph, given the set\nof its neighbor variables. This is called the Local Markov property. For example, in Figure 3 (a)\nvariables X0 and X3 are conditionally independent, given the set of variables {X1 , X2 }. In the\ntoroidal lattice of Figure 3 (b), X5 is conditionally independent of all the non-adjacent variables,\ngiven its neighbor variables {X1 , X4 , X6 , X9 }.\n2.1.1 Correctness of the structure\nFor correctly representing a probability distribution P by a Markov network, G must be a map of\nthe independences present in P . As proved in Pearl (1988), a graph G is called an independence-map\n(or I-map, for short) of a distribution P when all the independences encoded in the graph exist in\nthe underlying distribution P .\nDefinition 1 I-map Pearl (1988)[p.92].\nA graph G is an I-map of a distribution P if for all disjoint subsets of variables X, Y and Z,\nthe following is satisfied:\n(X\u22a5\u22a5Y|Z)G \u21d2 hX, Y, ZiP ,\n(1)\nwhere (X\u22a5\u22a5Y|Z)G are the independences encoded by G, and hX, Y, ZiP are the independences\nexistent in the underlying distribution P .\nSimilarly, G is a dependency-map (D-map) when\n(X\u22a5\u22a5Y|Z)G \u21d0 hX, Y, ZiP .\n\n(2)\n\n\fA survey on independence-based Markov networks learning\n\n7\n\nUsing a graph G that is an I-map guarantees that nodes found to be separated correspond to\nindependent variables, but does not guarantee that all those showed to be connected are dependent.\nConversely, when G is a D-map it is guaranteed that the nodes connected in G are dependent in the\ndistribution P . Fully-connected graphs are trivial I-maps, and empty graphs are trivial D-maps. A\ndistribution P is said to be a perfect-map of P if it is both an I-map and a D-map.\nAn axiomatic characterization of the family of relations that are isomorphic to vertex separation\nin graphs is given by the concept of graph-isomorphism. Basically, a distribution P is a graphisomorph when its independences among variables can be encoded by an undirected graph.\nDefinition 2 Graph-isomorphism Pearl (1988)[p.93].\nA distribution is said to be a graph-isomorph if there exists an undirected graph G that is a\nperfect-map of P , i.e., for every three disjoint subsets X, Y and Z, we have\n(3)\n\n(X\u22a5\u22a5Y|Z)G \u21d0\u21d2 hX, Y, ZiP .\n\nA necessary and sufficient condition for a distribution P to be a graph-isomorph is that hX, Y, ZiP\nsatisfies the following axioms of independences, introduced by Pearl and Paz (1985). There is another set of axioms for learning Bayesian networks, but it is omitted here.\nSymmetry\n\n(X\u22a5\u22a5Y|Z) \u21d4 (Y\u22a5\u22a5X|Z)\n\nDecomposition\n\n(X\u22a5\u22a5Y \u222a W|Z) \u21d2 (X\u22a5\u22a5Y|Z) & (X\u22a5\u22a5W|Z)\n\nTransitivity\n\n(X\u22a5\u22a5Y|Z) \u21d2 (X\u22a5\u22a5\u03bb|Z) or (\u03bb\u22a5\u22a5Y|Z)\n\nStrong union\n\n(X\u22a5\u22a5Y|Z) \u21d2 (X\u22a5\u22a5Y|Z \u222a W)\n\nIntersection\n\n(X\u22a5\u22a5Y|Z \u222a W) & (X\u22a5\u22a5W|Z \u222a Y) \u21d2 (X\u22a5\u22a5Y \u222a W|Z),\n\n(4)\n\nwhere X, Y, Z and W are all disjoint subsets of the set of all the variables in the domain V,\nand \u03bb stands for a single variable, not in X \u222a Y \u222a Z \u222a W. The intersection axiom is valid only for\nstrictly positive probability distributions. This list of axioms represents the relationships that hold\namong the independences encoded by the graph.\nIn summary, when the distribution P is a graph-isomorph, there exists a graph G that is a\nperfect-map for P . For representing a distribution P , any graph G which is an I-map of P may\nbe used. However, the more independences of the underlying distribution encoded in the graph,\nthe better the model is in complexity and accuracy when used for inference. Assuming graphisomorphism is an important decision, since not all the existent distributions may be represented\nby an undirected graph. For example, there are distributions that may be represented by an acyclic\ndirected graph, and in this case, Bayesian networks are the correct model to use. There are also\nother distributions that cannot be encoded by a graph.\n2.1.2 The Markov blanket concept\nThis section describes the concept of Markov blanket, a central theoretical concept in the representation of distributions, introduced by Pearl (1988). The Markov blanket of a variable is the only\nknowledge needed to predict the behavior of that variable. Hence, this concept holds relevance for\na wide variety of applications where local relationships to some variables are significant.\n\n\f8\n\nFederico Schl\u00fcter\n\nDefinition 3 The Markov blanket concept.\nThe Markov blanket of a variable X is a minimal set, denoted here MBX , conditioned on which\nall other nodes in the domain of variables V are independent of X, that is,\n\u2200Y \u2208 V \u2212 {MBX }, (X\u22a5\u22a5Y |MBX ).\n\n(5)\n\nThat is, the Markov blanket of a variable is the smallest set of variables that shields it from the\nprobabilistic influence of the variables not in the blanket. From a graphical view point, the Markov\nblanket of a variable X is identical to its neighbors in the graph.\nIn the textbook of Pearl (1988), it is proved formally that, for strictly positive distributions,\nthe independence structure can be constructed by piecing together the Markov blanket of all the\nvariables of the domain, connecting with an edge every two variables X and Y , such that X belongs\nto the Markov blanket of Y . There is also a proof stating that every variable X \u2208 V in a distribution\nthat is a graph isomorph, and therefore satisfies the Pearl's axioms, has a unique Markov blanket. As\nonly strictly positive distributions satisfy the Intersection axiom, this mechanism for constructing\nthe structure only holds for positive distributions.\n2.2 Parameterization\nThis section explains how to quantify the relationships encoded in G. Although this work only\naddresses the problem of structure learning, the quantitative aspects of Markov networks are briefly\nexplained for better clarifying our work. Below is described a factorization method for constructing\nthe Gibbs distribution for an arbitrary undirected graph G, provided in Pearl (1988):\n\u2013 Identification of the maximal subgraphs whose nodes are all adjacent to each other, called the\nmaximal cliques of G. For example, the graph in Figure 3 (a) shows a maximal clique of size 4\namong the nodes corresponding to variables {7, 8, 10, 11}, two maximal cliques of size 3 among\nnodes {2, 3, 5} and {8, 9, 11}, and the rest of edges are maximal cliques of size 2. In Figure 3 (b)\nthe size of all the cliques is 2.\n\u2013 For each clique c \u2208 C in the set of all the cliques in the graph, assign a non-negative potential\nfunction gc (Xc ) (where Xc is the set of variables that belong to the clique c) measuring the\nrelative degree of compatibility associated with each possible configuration of Xc . Usually each\npotential function is represented by a table with a numerical parameter assigned to each possible\ncomplete assignment of the variables that compose the clique, like the tabular model showed\nin Figure 1, but including only the variables that compose the clique c. A difference with the\ntabular model is that\nQ here the parameter values are not normalized.\ngc (Xc ) of the potential functions over all the cliques.\n\u2013 Form the product\nc\u2208C\n\n\u2013 Construct the Gibbs distribution by normalizing the product over all possible value combinations\nof the variables in the system\nP (X0 , .., Xn\u22121 ) =\n\n1 Y\ngc (Xc ),\nZ\n\n(6)\n\nc\u2208C\n\nwhere Z is the partition function, or normalization constant, computed as\nX\nY\nZ=\ngc (Xc ).\nX0 ,..,Xn\u22121 c\u2208C\n\n(7)\n\n\fA survey on independence-based Markov networks learning\n\n9\n\nUsing the Hammersley-Clifford theorem it is possible to prove that the general form of the Gibbs\ndistribution of Equation (6) embodies all the conditional independences encoded in the graph G.\nSuch form of the Gibbs distribution presents some difficulties. First, it is difficult to discern the\nmeaning of the potential functions. Second, the computational cost of calculating the partition\nfunction Z is exponential, as it requires an exponential sum over all possible assignments of the\ncomplete set of variables.\n\n3 The Markov networks learning problem\nThis section discusses the difficulties that arise in the task of learning Markov networks from historical information. This task is only possible whenever the size of the input dataset D is sufficient,\nand the data is a representative sample of the underlying distribution P . When these conditions are\nsatisfied, it is possible that some algorithms learn a model for representing P by exploring and analyzing D. The input dataset D contains historical information commonly structured in the tabular\nformat, a standard format in machine learning. This is a file that contains a table with a column\nper random variable in P , and the rows are the datapoints, each one being a complete assignment\nfor all the variables. For example, a datapoint for a domain with n = 4 random binary variables\nV = {X0 , X1 , X2 , X3 } may be (X0 = 0, X1 = 1, X2 = 1, X3 = 0). The algorithms discussed in\nthis work ignore the problem of missing values, which is solved by known, yet computationally\nchallenging, statistical techniques.\nLearning a Markov network from data is a problem that consists in learning both the structure\nG and the parameters \u03b8. Of course, the best possible structure learned is a perfect-map, that is,\na model that contains a structure encoding all the dependences and the independences present in\nP . However, every model containing a structure which is an I-map of P is a good solution. The\ncloser to a perfect-map, the better is the structure learned, and the better is the resulting Markov\nnetwork for representing P . When learning a model for large domains, a desirable property of the\nmodel is the sparsity, since densely connected models require too many parameters, and make exact\nand even approximate inferences computationally intractable.\n\n3.1 Goals of Markov networks learning\nFor evaluating the merits of a model learning method, it is important to consider the goal of learning.\nClearly, learning the complete model (structure plus parameters) is the ideal method, but due to\ncomputational, spatial or sampling limitations, it may not be possible in practice. For that reason,\nother less ambitious goals are often considered in practice, such as the three main goals of learning\ndiscussed by Koller and Friedman (2009):\n\u2013 Density estimation: A common reason for learning a Markov network is to use it for some\ninference task. When formulating the goal of learning as one of density estimation, the goal is to\nconstruct a model M so that the defined distribution is \"close\" to the underlying distribution P .\nA common metric for evaluating the quality of such approximation is the use of the likelihood\nof the data P r(D | M ). However, this goal assumes that the overall distribution P is needed.\n\u2013 Specific prediction tasks: The goal is predicting the distribution of a particular set of\nvariables Y, given certain set of variables X. When the model is used only to perform a particular\ntask, if the model is never evaluated on predictions of the variables X, it is better to optimize\n\n\f10\n\nFederico Schl\u00fcter\n\nthe learning task for improving the quality of its answers to Y. This has been the goal of a\nlarge fraction of the work in machine learning. For example, consider the problem of documents\nclassification for a given set of relevant words of a document, and a variable that labels the topic\nof the document. Another well-known example is the task of image segmentation, where the\ngoal of the task is the prediction of class labels for all the pixels in the image, given the image\nfeatures.\n\u2013 Knowledge discovery: The goal of knowledge discovery is to learn the correct structure of\nthe underlying distribution. There are some cases when the learned structure can reveal some\nimportant unknown properties of the domain. It is a very different motivation for learning the\ndistribution. An examination of the learned structure can show dependences among variables as\npositive or negative correlations. In a knowledge discovery application, it is far more critical to\nassess the confidence in a prediction, taking into account the extent to which it can be identified\ngiven the available data, and the number of hypotheses that would cause similar observed\nbehavior. For example, in a medical diagnosis domain, we may want to learn the structure of\nthe model to discover which predisposing factors lead to certain diseases, and which symptoms\nare associated with different diseases.\n\n3.2 Parameters estimation\nMarkov network parameters estimation is usually used to choose the value of the parameters by\nfitting the model to data, because tuning parameters manually is often difficult, and learned models\noften exhibit better performance. This task has shown to be an NP-hard problem by Barahona\n(1982).\nFor estimating the parameters, the most common method proposed is maximum-likelihood estimation, potentially using some regularization as an additional parameter prior. Unfortunately,\nevaluating the likelihood of a complete model requires, for every set of parameters proposed during\nthe maximum-likelihood estimation process, the computation of the partition function Z, which is\nused for normalizing the product over all possible value combinations of the variables of the domain,\nas showed in Equation (7). Although it is not possible to optimize the maximum-likelihood in a\nclosed form, it is guaranteed that the global optimum can be found, because it is a concave function.\nAs a result, some approximations and heuristics in the literature are introduced in Minka (2001);\nVishwanathan et al (2006), for reducing the cost of parameters estimation, using iterative methods\nsuch as simple gradient ascent, or other sophisticated optimization algorithms. Unfortunately, this\nproblem remains intractable in practice, because the use of the partition function couples all the\nparameters across the network, requiring several inference steps on the network (iterative methods\nwith interleaved inference).\nFor reducing the cost of parameters estimation, other solutions have been proposed. Pseudolikelihood by Besag (1977), and Score Matching by Hyv\u00e4rinen and Dayan (2005) are some tractable\napproximate alternatives. The loopy belief propagation method proposed in Pearl (1988) and later\nin Yedidia et al (2005), and some variants introduced in Wainwright and Jordan (2008), uses an\napproximate inference technique for approximating the gradient of the maximum likelihood function. Another solution for outperforming the robustness of loopy belief propagation is provided in\nGanapathi et al (2008).\nFor avoiding overfitting, many of these scoring methods commonly need the use of a regularization term adding an extra hyper-parameter, whose best value has to be found empirically. For\n\n\fA survey on independence-based Markov networks learning\n\n11\n\nexample, it can be found by running the training stage for several values of the hyper-parameter,\npotentially using cross-validation.\n\n3.3 Structure learning approaches\nThe two broad approaches for learning the structure of Markov networks from data are scorebased and independence-based approaches. The former is intractable in practice, and the latter is\nefficient but presents quality problems. Both approaches have been motivated by distinct learning goals (those described in Section 3.1). Generally, score-based approaches may be better suited\nfor the density estimation goal, that is, tasks where inferences or predictions are required. As explained below in Section 3.3.1, score-based methods learn the complete Markov network (structure\nand parameters). There is an overwhelmingly use of Markov networks for such settings, including\nimage segmentation and others, where there exists a particular inference task in mind. Instead,\nindependence-based methods are better suited for the remaining goals, that is, for specific prediction tasks, and knowledge discovery. On one hand, independence-based algorithms are commonly\nused for tasks such as feature selection for classification, since it is possible to perform local discovery for a particular set of variables of interest (more details in Section 3.3.2). On the other\nhand, independence-based algorithms are suited for knowledge discovery tasks, that is, tasks where\nunderstanding the interactions among variables in a domain carries the greatest importance, or\nwhere the structure is viewed purely as a predictive tool, for example, econometrics, psychology, or\nsociology.\nSince this work focuses on the independence-based approach to Markov networks structure\nlearning methods, Sections 4 and 5 only discuss in detail the state-of-the-art independence-based\nalgorithms.\n3.3.1 Score-based approach\nScore-based algorithms were proposed for learning the structure of Bayesian networks, in the works\nof Lam and Bacchus (1994) and Heckerman et al (1995), and later proposed for learning the structure of Markov networks, in the works of Della Pietra et al (1997) and McCallum (2003). Such\nalgorithms approach the problem as an optimization over the space of complete models, looking\nfor the one with maximum score. The goal of score-based algorithms is to find the model that\nmaximizes its score. Traditional score-based algorithms perform a global search to learn a set of potential functions that accurately captures high-probability regions of the instance space of complete\nmodels.\nThe standard approach for learning the structure of Markov networks with a score-based approach is the Della Pietra et al. algorithm. This algorithm learns the structure by inducing a set\nof potential functions from data. Its strategy is based on a top-down search, that is, a generalto-specific search. This algorithm starts with a set of atomic potentials (that is, exclusively the\nvariables of the domain). Then, it creates a set of candidate potentials in two ways. First, each potential currently in the model is conjoined (i.e., associated) with every other potential in the model.\nSecond, each potential in the model is composed with each atomic potential. Then, for efficiency\nreasons, the parameters are learned for each candidate potential, assuming that the parameters of\nall other potentials remain unchanged. When setting the parameters, it uses the Gibbs sampling\nfor inference. Then, for each candidate potential, the algorithm evaluates how much adding such\n\n\f12\n\nFederico Schl\u00fcter\n\npotential would increase the log-likelihood, which is the score used by this algorithm. The potential\nthat maximizes this measure is added. When no one candidate potential improves the score of the\nmodel, the procedure ends. Another algorithm using the same approach is proposed in McCallum\n(2003). It is a similar algorithm to the one proposed by Della Pietra, but performing an efficient\nheuristic search over the space of candidate structures, for automatically inducing potentials that\nmost improve the conditional log-likelihood. However, as reported by Davis and Domingos (2010),\nsuch general-to-specific search methods are inefficient, because they test many potential variations\nwith no support in the data, and because they are highly prone to local optima.\nRecently, other alternative approaches have been considered. The approach of Lee et al (2006),\nH\u00f6fling and Tibshirani (2009), and Ravikumar et al (2010) propose to couple parameters learning\nand potentials induction into one step, by using L1 -regularization, which forces most numerical\nparameters to be zero. They approach the problem as an optimization problem, providing a large\ninitial potential set, with all the possible potentials of interest. Then, after learning, model selection\noccurs by selecting those potentials with non-zero parameters. For efficiency reasons, the approaches\nof H\u00f6fling and Tibshirani, and Ravikumar et al., only construct pairwise networks (networks involving only cliques of size two or one for factorization). Instead, the algorithm of Lee et al., can learn\narbitrarily long potentials. In practice, however, it has been evaluated only for inducing potentials\nof length two (that is, for learning pairwise networks).\nA recent alternative approach was proposed by Davis and Domingos (2010), called the Bottomup Learning of Markov Networks (BLM) algorithm. BLM starts with each complete training example as a long potential in the Markov network. Then, the algorithm iterates through the potential\nset, generalizing each potential to match its k-nearest previously unmatched examples by dropping\nvariables. When the new generalized potential improves the score of the model, it is incorporated\nto the model. The loop ends when no generalization can improve the score.\nHowever, all these approaches are often slow for two reasons. First, the size of the search space\nof structures is intractable in the number of variables. Second, for evaluating the score at each\nstep, it is necessary to compute the score, requiring the estimation of the numerical parameters (an\nNP-hard task, as explained in Section 3.2).\n3.3.2 Independence-based approach\nIndependence-based (also known as constraint-based) algorithms work by performing a succession\nof statistical independence tests for discovering the independence structure of graphical models.\nThese algorithms exploit the semantics of the independence structure, casting the problem of structure learning as an instance of the constraint satisfaction problem, where the constraints are the\nindependences present in the input dataset (and therefore, in the underlying distribution), and the\ngoal is to find a structure encoding all such independences.\nEach independence test consults the data for responding to a query about the conditional\nindependence among some input random variables X and Y , given some conditioning set of variables\nZ, resulting in an independence assertion (X\u22a5\u22a5Y |Z), or (X 6\u22a5\u22a5Y |Z) for a dependence assertion. The\ncomputation cost of statistical tests is proportional to the number of rows in the input dataset D,\nand the number of variables involved. Examples of independence tests used in practice are Mutual\nInformation in Cover and Thomas (1991), Pearson's \u03c72 and G2 in Agresti (2002), the Bayesian test\nin Margaritis (2005), and for continuous Gaussian data the partial correlation test in Spirtes et al\n(2000). Such independence tests compute a statistical value for a triplet of variables hX, Y, Zi,\ngiven an input dataset, and decide independence or dependence comparing it with a threshold.\n\n\fA survey on independence-based Markov networks learning\n\n13\n\nFor instance, \u03c72 and G2 use the p-value, which is computed as the probability of obtaining a test\nstatistic at least as extreme as the one that was actually observed, assuming that the null hypothesis\nis true (that is, variables are dependent). The null hypothesis is rejected when the p-value is less\nthan the significance level \u03b1, which is often 0.05 or 0.01. When the null hypothesis is rejected, the\nresult is said to be statistically significant.\nAn elegant, efficient and scalable strategy used by several independence-based algorithms in the\nliterature is called the local-to-global strategy, presented in a recent work of Aliferis et al (2010b).\nThis is a generalization of previous algorithms using such strategy. Algorithm 1 shows the outline of\nthis theoretically sound and straightforward procedure, omitting the third step of edges orientation,\nused for learning Bayesian networks .\nAlgorithm 1 LGL for Markov networks\n1: Learn MBXi for every variable Xi \u2208 V.\n2: Piece-together the global structure using an \"OR rule\".\n\nSuch a strategy suggests to construct the independence structure by dividing the problem into\nn different Markov blanket learning problems, that is, the Markov blanket is learned for each variable of the domain V. The learning of Markov blanket is generalized by Aliferis et al., for learning\nBayesian networks, in the Generalized Local Learning (GLL) framework Aliferis et al (2010a). Algorithms using a local-to-global strategy learn locally the Markov blanket of every variable in the\ndomain, and then construct a global structure linking each of these variables with every member\nof its Markov blanket, using an \"OR rule\" (an edge exists between two variables X and Y when\nX \u2208 MBY or Y \u2208 MBX ).\nFor learning the Bayesian networks structure, independence-based algorithms first arose in 1993,\nwhen Spirtes et al (2000) published the well-known algorithms SGS and PC, in the first edition of\nsuch textbook. Then, other independence-based algorithms appeared in works about feature selection via the induction of Markov blanket, and works about Bayesian and Markov networks structure\nlearning. For that reason, a series of independence-based algorithms for Markov blanket learning\nof Bayesian networks appeared, such as the Koller-Sahami (KS) algorithm in Koller and Sahami\n(1996), the Grow-Shrink (GS) algorithm in Margaritis and Thrun (2000), the Incremental Association Markov Blanket (IAMB) algorithm and its variants in Tsamardinos et al (2003) , the Max-Min\nParents and Children Markov Blanket (MMPC/MB) algorithm in Tsamardinos et al (2006), the\nHITON-PC/MB algorithm in Aliferis et al (2003), the Fast-IAMB algorithm in Yaramakala and Margaritis\n(2005), the Parent-Children Markov Blanket (PCMB) algorithm in Pe\u00f1a et al (2007) and the Iterative Parent and Children Markov Blanket (IPC-MB) in Fu and Desmarais (2008). A summary of\nthe most important aspects of such algorithms is shown in Table 2, reproduced from the conclusions\nof a recent review of Markov blanket based feature selection written by Fu and Desmarais (2010).\nFor learning the Markov networks structure, independence-based algorithms arose later in 2006,\nwhen Bromberg et al (2006, 2009) published the Grow-Shrink Markov Network (GSMN) algorithm and the Grow-Shrink Inference-based Markov Network (GSIMN) algorithm. Then, other\nindependence-based algorithms appeared for Markov networks structure learning, such as the Particle Filter Markov Network (PFMN) algorithm in Bromberg and Margaritis (2007); Margaritis and Bromberg\n(2009), and the Dynamic Grow Shrink Inference-based Markov Network (DGSIMN) algorithm in\nGandhi et al (2008). Another approach is proposed in Bromberg (2007); Bromberg and Margaritis\n(2009), as a framework based on argumentation for improving reliability of tests. In Section 4, all\n\n\f14\n\nFederico Schl\u00fcter\n\nTable 2 Summary of Markov blanket learning algorithms for Bayesian networks.\n\nName\n\nReference\n\nKS\n\nKoller and Sahami\n(1996)\n\nGS\n\nIAMB and\nits variants\n\nMargaritis and Thrun\n(2000)\n\nTsamardinos et al\n(2003)\n\nHITONPC/MB\n\nAliferis et al (2003)\n\nFast-IAMB\n\nYaramakala and Margaritis\n(2005)\n\nMMPC/MB\n\nTsamardinos et al\n(2006)\n\nPCMB\n\nIPC-MB\n\nPe\u00f1a et al (2007)\n\nFu and Desmarais\n(2008)\n\nComments\n\u2013 Not Sound\n\u2013 The first one of this type\n\u2013 Requires specifying MB size in advance\n\u2013 Sound in theory\n\u2013 Proposed to learn Bayesian network via the induction\nof neighbors of each variable\n\u2013 First proved such kind of algorithm\n\u2013 Works in two phases: grow and shrink\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\nSound in theory\nActually variant of GS\nSimple to implement\nTime efficient\nVery poor on data efficiency\nIAMB's variants achieve better performance on data\nefficiency than IAMB\n\n\u2013 Not sound\n\u2013 Another trial to make use of the topology\ninformation to enhance data efficiency\n\u2013 Data efficiency comparable to IAMB\n\u2013 Much slower compared to IAMB\n\u2013\n\u2013\n\u2013\n\u2013\n\nSound in theory\nNo fundamental difference as compared to IAMB\nAdds candidates more greedily to speed up the learning\nStill poor on data efficiency performance\n\n\u2013\n\u2013\n\u2013\n\u2013\n\nNot sound\nThe first to make use of the underling topology information\nMuch more data efficient compared to IAMB\nMuch slower compared to IAMB\n\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\nSound in theory\nData efficient by making use of topology information\nPoor on time efficiency\nDistinguish spouses from parents/children\nDistinguish some children from parents/children\n\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\nSound in theory\nMost data efficient compared with previous algorithms\nMuch faster than PCMB on computing\nDistinguish spouses from parents/children\nDistinguish some children from parents/children\nBest trade-off among this family of algorithms\n\nthese independence-based algorithms for learning the structure of Markov networks are surveyed in\ndetail.\nThere are several advantages of independence-based algorithms. First, they can learn the structure without interleaving the expensive task of parameters estimation (contrary to score-based\nalgorithms, as explained before), reaching sometimes polynomial complexities in the number of\n\n\fA survey on independence-based Markov networks learning\n\n15\n\nstatistical tests performed. If the complete model is required, the parameters can be estimated\nonly once for the given structure. Another important advantage of such algorithms is that they are\nsound, that is, when statistical tests outcomes are correct, the structure found correctly represents\nthe underlying distribution. However, they are correct under the following assumptions:\n\u2013 the distribution of data is a graph-isomorph\n\u2013 the underlying distribution is strictly positive\n\u2013 the outcomes of tests are reliable\nThe third condition for soundness is an important problem of independence-based algorithms.\nWhen the dataset used for learning is not sufficiently large, or it is not a representative sampling\nof the underlying distribution, the outcomes of tests are incorrect, and the structures learned are\ndeemed unreliable. This problem of statistical tests unreliability is exponentially exacerbated with\nthe number of variables involved (for some fixed size of dataset). For good quality, statistical tests\nrequire enough counts in their contingency tables, and there are an exponentially number of those\n(one per value assignment of all variables in the test). For example, Cochran (1954) recommends\nthat the \u03c72 test must be deemed unreliable when more than 20% of these cells have an expected\ncount of less than 5 data points.\nAnother disadvantage of independence-based algorithms is that there is no guarantee about the\nquality of the complete model obtained by learning the structure first, and then fitting parameters\nfor such learned structure. This is an approximation, and there are no experimental results published\nin the literature about independence-based methods for learning complete models.\n4 Independence-based algorithms for learning the Markov networks structure\nThis section reviews the independence-based structure learning algorithms for Markov networks that\nhave appeared in the literature. The review on this section covers a series of published algorithms\nthat tackle such problem.\n4.1 The Grow-Shrink Markov Network algorithm\nThe Grow-Shrink Markov Network (GSMN) algorithm was introduced in Bromberg et al (2006,\n2009), as the first independence-based structure learning algorithm for Markov networks in the literature. This algorithm is an adaptation to Markov networks of the GS algorithm of Margaritis and Thrun\n(2000) for learning the Markov blanket.\nThe GSMN algorithm learns the global structure of a Markov network following the simple\noutline of local-to-global algorithms showed in Algorithm 1, and using the GS algorithm outlined\nin Algorithm 2 for discovering the Markov blanket of the variables. GS maintains a set called S\n(initialized empty in line 1) that contains the Markov blanket of the input variable X when the\nalgorithm terminates. First, in line 2, GS performs an initialization phase that sorts by increasing\nassociation with X the rest of the variables of the domain V , using an unconditional test between\nX and every variable Y \u2208 V \u2212 {X}. Then, the algorithm proceeds in two stages, the grow and\nshrink phases, using such ordering. During the grow phase (line 4) the algorithm increases the set S\nwith every variable Y that is found dependent on X conditioning on the current state of S. By the\nend of this phase, the set S contains all members of the Markov blanket, but potentially includes\nsome false positives that are non-members. These false positives are removed during the shrink\n\n\f16\n\nFederico Schl\u00fcter\n\nAlgorithm 2 GS(X, V).\n1: S \u2190\u2212 \u2205.\n2: sort V \u2212 {X} by increasing association with X\n3: /* Grow phase */\n4: while \u2203Y \u2208 V \u2212 {X} s.t. (Y 6\u22a5\n\u22a5X| S), do S \u2190 S \u222a {Y }.\n5: /* Shrink phase */\n6: while \u2203Y \u2208 S s.t. (Y \u22a5\n\u22a5X| S \u2212 {Y }), do S \u2190 S \u2212 {Y }.\n7: return S\n\nphase (line 6), where variables found independent of X conditioning on the set S are removed from\nS.\nThe main advantages of GSMN are i) it is sound, and ii) it is efficient. The soundness of GSMN\nis proven theoretically by its authors, guaranteeing that a correct independence structure is found\nwhen statistical tests are reliable. This algorithm is efficient because it is polynomial in the number\nof independence tests for discovering the structure, each test requiring a polynomial time execution\nwith respect to the variables involved in the test, and the size of the input dataset. A disadvantage of\nusing GS is that unreliable statistical tests produce cascade errors, not only with incorrect outcomes,\nbut that also generate next incorrect tests during grow and shrink phases, producing cumulatively\nerrors, as stated in Spirtes et al (2000).\nTwo other important algorithms for learning the Markov blanket of a variable, for Bayesian\nnetworks, are the Incremental Association Markov Blanket (IAMB) algorithm in Tsamardinos et al\n(2003), and the HITON algorithm in Aliferis et al (2003). Both algorithms have been proven empirically to be more resilient than GS to the errors of statistical tests, by introducing two simple\nvariants. On the one hand, the IAMB algorithm only introduce a modification by interleaving the\ninitialization step of ordering in the grow phase (i.e., interleaves lines 2 and 4 of Algorithm 2).\nBy interleaving the sorting step in the grow phase, IAMB maximizes the accuracy, reducing the\nnumber of false positives in the grow phase. On the other hand, the HITON algorithm aims to\nreduce the data requirements of IAMB, by introducing an additional modification in the criteria\nused for testing independence. In both grow and shrink phases, instead of only conditioning on\nits tentative Markov blanket S, HITON tests independence conditioning in any of the subsets of\nS (that is, every set Z \u2286 S \u2212 {Y }). As statistical tests are more reliable while containing fewer\nvariables, such modification exploits the Strong union axiom of Pearl, for improving the quality of\nindependence tests when data is scarce. A disadvantage of the approach proposed by HITON is its\nexponential cost in | S | (i.e., the size of S) , but in general | S | is comparatively smaller than the\nsize of the domain n. In summary, both algorithms are proven to be better in quality than GS, but\nthey were designed for learning the structure of Bayesian networks, and there are not any works in\nthe literature proposing a theoretical adaptation of such ideas for learning the complete structure\nof a Markov network, or empirically evaluating its performance.\n\n4.2 The Grow Shrink Inference Markov Network algorithm\nThe Grow Shrink Inference Markov Network (GSIMN) algorithm was presented in Bromberg et al\n(2006, 2009). This algorithm works in a similar fashion to that of GSMN algorithm, using the\nlocal-to-global strategy of Algorithm 1, and learning the Markov blanket of all the variables with\nthe GS algorithm, but interleaving an inference step to reduce the number of tests required to learn\nthe Markov blanket. By using a theorem for inference called the Triangle theorem by the authors,\n\n\fA survey on independence-based Markov networks learning\n\n17\n\nGSIMN reduces the number of tests performed on data, without adversely affecting the quality of\nthe learned structures. It may be useful when using large datasets, or in distributed domains, where\nstatistical tests are very expensive.\nGSIMN introduces the Triangle theorem, based on the Pearl's axioms showed in Section 2.1.1.\nThis is a sound theorem for allowing to infer unknown independences from those known so far.\nTheorem 1 (Triangle theorem) Given Eqs. (4), for every variable X, Y , W and sets Z1 and\nZ2 such that {X, Y, W } \u2229 Z1 = {X, Y, W } \u2229 Z2 = \u2205,\n(X 6\u22a5\u22a5W |Z1 ) \u2227 (W 6\u22a5\u22a5Y |Z2 ) =\u21d2 (X 6\u22a5\u22a5Y |Z1 \u2229 Z2 )\n(X\u22a5\u22a5W |Z1 ) \u2227 (W 6\u22a5\u22a5Y |Z1 \u222a Z2 ) =\u21d2 (X\u22a5\u22a5Y |Z1 ).\nThe first relation is called the \"D-triangle rule\" and the second the \"I-triangle rule.\"\nWhen GSIMN tests some independence on data, first applies the Triangle theorem to the tests\nalready done on data, in order to check if such independence assertion can be logically inferred. If\nthe test cannot be inferred, then this is done on data, and stored. For convenience, the algorithm\ndetermines the visit ordering (the order for local learning) in an attempt to maximize the use of\ninferences. The results obtained with GSIMN show savings up to a 40% in the running times of\nGSMN, obtaining comparable qualities.\n4.3 Particle Filter Markov networks algorithm\nThe Particle Filter Markov networks algorithm (PFMN) is presented in Bromberg and Margaritis\n(2007); Margaritis and Bromberg (2009), as a novel independence-based approach for learning\nMarkov network structures. Previous independence-based algorithms reviewed, such as the GSMN\nand GSIMN, use the local-to-global strategy. Instead, this algorithm learns directly a global structure as the solution.\nPFMN was designed for improving the efficiency of the GSIMN algorithm. This algorithm works\nperforming statistical independence tests iteratively, by selecting greedily at each iteration the\nstatistical test that eliminates the major number of inconsistent structures. This decision is taken\nby first modeling the learning problem with a Bayesian approach, selecting as the solution the\nstructure G that maximizes its posterior probability Pr(G | D). Since the direct computation of\nsuch probability is intractable, PFMN propose a generative model with independence tests which is\nan approximation to that posterior probability. With this model, it is possible to compute efficiently\nsuch probability, given the information over a set of independences. Moreover, the authors claim\nthat it is possible to demonstrate that, under the assumption of correctness of tests, the distribution\nof Pr(G | D) converges to a correct structure.\nThis approach is useful in domains where independence tests are expensive, such as cases of\nvery large data sets or in distributed domains. Results obtained by PFMN show improvements in\nrunning times up to 90% with respect to GSIMN, and comparable qualities on structures found by\nGSIMN and GSMN.\n4.4 The Dynamic Grow Shrink Inference-based Markov Network algorithm\nThe Dynamic Grow Shrink Inference-based Markov Network (DGSIMN) algorithm was presented\nin Gandhi et al (2008). This is an extension of the GSIMN algorithm which, in the same way than\n\n\f18\n\nFederico Schl\u00fcter\n\nGSIMN, uses the Triangle theorem for avoiding unnecessary tests. The outline of DGSIMN is similar\nto GSMN and GSIMN, using the local-to-global strategy of Algorithm 1, and the GS algorithm\nshowed in Algorithm 2 for learning the Markov blanket of the variables, but interleaving a different\ninference step than GSIMN for reducing the number of tests performed.\nDGSIMN improves the GSIMN algorithm by dynamically selecting the locally optimal test\nthat will increase the state of knowledge about the structure, estimating the number of inferred\nindependences that will be obtained after executing a test, and selecting the one that maximizes\nsuch number of inferences. This helps decreasing the number of tests required to be evaluated on\ndata, resulting in an overall decrease in the computational requirements of the algorithm.\nThe results of experiments with the DGSIMN algorithm shows that it improves the fixed ordering\nof variables in the Markov blanket learning subroutine, improving the running times of GSIMN up\nto 85%, obtaining comparable qualities to GSMN.\n4.5 Argumentation for improving reliability\nAlgorithms presented in previous sections are independence-based algorithms that focus on improving the efficiency, ignoring the important problems in the quality of learned structures that arises\nwhen statistical tests are not reliable, due to data scarceness.\nAn independence-based approach for dealing with unreliable tests was presented in Bromberg\n(2007); Bromberg and Margaritis (2009), by modeling the problem of low reliability of independence\ntests as a knowledge base with independence assertions that may contain errors due to incorrect\nstatistical tests performed, and the Pearl's axioms (directed or undirected axioms, depending on\nthe target model to learn). The advantage of this approach is its power for correcting errors of tests\nby exploiting logically the independence axioms of Pearl. When exist independence assertions in\nthe knowledge base that are in conflict, it is clear that some independence assertions are incorrect,\nand this approach proposes to resolve such conflicts through the argumentation framework, which\nis a defeasible logic proposed by Amgoud and Cayrol (2002), to reason about and correct errors.\nThis approach was presented as a more robust conditional independence test called the argumentative independence test, for learning both Bayesian and Markov networks. Experimental evaluation\nshows significant improvements in the accuracy of the argumentative independence test over other\nsimple statistical tests (up to 13%), and improvements on the accuracy of Blanket discovery algorithms such as PC and GS (up to 20%).\nA disadvantage with this approach is that, as it is a propositional formalism, it requires to\npropositionalizing the set of rules of Pearl, which are first-order. As these are rules for super-sets\nand sub-sets of variables, its propositionalization involves an exponential number of propositions,\nand then, the exact argumentative algorithm proposed is exponential. In this work, an approximate\nsolution is presented with polynomial running time, still improving the quality in the experimental\nevaluation (up to 9%), but making a drastic and rather simplistic approximation that does not\nprovide theoretical guarantees.\n5 Analysis and open problems\nThis section analyzes the independence-based algorithms surveyed, discussing their relative advantages and disadvantages, and describes a series of open problems where future works may produce\nsome advances in the area.\n\n\fA survey on independence-based Markov networks learning\n\n19\n\n5.1 Analysis\nThe independence-based algorithms for learning Markov networks are able to learn the independence structure efficiently, having the important advantage of being sound (i.e., they guarantee to\nproduce the true underlying distribution) when data is a sample of a Markov network, tests are reliable, and the underlying distribution is strictly positive. These algorithms perform a succession of\nstatistical independence tests to learn about the conditional independences present in the data, and\nassume that those independences are satisfied in the underlying model. The structure is learned by\nquerying independences on data greedily, discarding all the structures that are inconsistent, until a\nsingle structure is left. An important source of errors in some of this algorithms is the cascade errors\nproduced by erroneous statistical tests, that produces cumulatively errors. About their complexity,\nsome of them can learn the structure by performing a polynomial number of tests in the number\nof variables n of the domain. This fact, together with the evidence that statistical tests may run\nin a time proportional to the number of rows in the input dataset D, result in some algorithms,\ne.g., GSMN, having a total execution time polynomial in n and D. When compared to score-based\nalgorithms, they can learn the structure without the need of an interleaved estimation of the numerical parameters of the model, which is the main source of intractability of score-based algorithms\nfor Markov networks. The strength of independence-based algorithms is that they can learn correctly the structure under assumptions. However, there is no equivalent theoretical guarantee for\nthe correctness of the complete model resulting from learning the parameters for that structure.\nThe independence-based algorithms present in the literature for learning the structure of a\nMarkov network are GSMN, GSIMN, PFMN, DGSIMN. Related to structure learning is the argumentative independence test, an approach for improving the quality of conditional independences\ndiscovery. Table 3 shows a summary of the most important features of those approaches. The GSMN\nalgorithm is a direct extension of the GS algorithm for Markov networks structure learning, which\nrequires a polynomial number of tests in the number of variables of the domain n. This algorithm is\npresented together with the GSIMN algorithm, which improves the efficiency of GSMN by exploiting\nPearl's independence axioms to infer unknown independences from the independences observed so\nfar, avoiding the need of performing redundant statistical tests. This is important when datasets are\nlarge, or when datasets are present in distributed environments. The results obtained for GSIMN\nshow savings up to a 40% in running times, obtaining comparable qualities to GSMN. The PFMN\nalgorithm was designed for improving the efficiency of GSIMN. This algorithm does not work in a\nlocal-to-global fashion. Instead it uses a model for efficiently computing the approximate posterior\nprobability of structures Pr(G | D). The results obtained by PFMN show improvements in running\ntimes up to 90% with respect to GSIMN, with equivalent quality of learned structures. Similarly,\nthe DGSIMN algorithm was designed for improving the efficiency of GSIMN, by enhancing the fixed\nordering of variables in the Markov blanket learning subroutine by a dynamic ordering mechanism.\nExperiments published for DGSIMN show improvements over the running times of GSIMN up to\n85%, still maintaining the quality of GSMN.\nThe most important problem of independence-based algorithms for learning the structure of\nMarkov networks is the problem of quality when statistical independence tests are not reliable.\nSuch a problem is not tackled by either GSMN, GSIMN, DGSIMN or PFMN. The only approach\npresented for improving the quality under uncertainty of tests outcomes is the argumentative independence test. Experimental results using this approach show significant improvements over the\naccuracy of the standard independence tests, but exact algorithms presented, while improving\nquality by 13% have an exponential cost, and the approximate algorithm proposed make a drastic\n\n\f20\n\nFederico Schl\u00fcter\n\nTable 3 Summary of independence-based Markov network approaches\n\nName\n\nGSMN\n\nGSIMN\n\nPFMN\n\nDGSIMN\n\nArgumentative\ntest\n\nReference\n\nComments\n\nBromberg et al (2006)\n\nSound, under assumptions\nThe first independence-based algorithm for Markov networks\nUse the local-to-global strategy\nPerforms a polynomial number of tests in the number\nof variables of the domain n\n\u2013 Quality depends on sample complexity of tests\n\nBromberg et al (2006).\n\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\nBromberg and Margaritis\n(2007)\n\nGandhi et al (2008)\n\nBromberg and Margaritis\n(2009)\n\n\u2013\n\u2013\n\u2013\n\u2013\n\nSound, under assumptions\nUse the local-to-global strategy\nUse Triangle theorem for reducing number of tests performed\nUseful when using large datasets, or distributed domains\nSavings up to 40% in running times respect to GSMN\nComparable quality respect to GSMN\n\nSound, under assumptions\nDoes not use the local-to-global strategy\nDesigned for improving efficiency of GSIMN\nUse an approximate method for computing the\nposterior Pr(G | D) using independence-tests\n\u2013 Useful when using large datasets, or distributed domains\n\u2013 Savings up to 90% in running times respect to GSIMN\n\u2013 Comparable quality respect to GSMN\n\u2013\n\u2013\n\u2013\n\u2013\n\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\nSound, under assumptions\nUse the local-to-global strategy\nDesigned for improving efficiency of GSIMN\nUse dynamic ordering for reducing number of tests performed\nUseful when using large datasets, or distributed domains\nSavings up to 85% in running times respect to GSIMN\nComparable quality respect to GSMN\n\n\u2013 First approach for quality improvement\n\u2013 Use argumentation to correct errors when tests are unreliable\n\u2013 Use an independence knowledge base. The inconsistencies\nare used to detect errors in tests\n\u2013 Designed for learning Bayesian and Markov networks.\n\u2013 Exact algorithm presented is exponential\n(improving accuracy up to 13%)\n\u2013 Approximate algorithm proposed is simplistic, and does not\nprovide theoretical guarantees\n(improving accuracy up to 9%)\n\napproximation that does not provide theoretical guarantees, still producing quality improvements,\nup to 9%.\nIn summary, the advantages of independence-based algorithms for learning Markov networks are\novershadowed by the low quality of learned structures when data is scarce, or equivalently, when\nthe underlying network is highly connected. This is why independence-based algorithms are not\ncurrently implemented in practice for learning Markov networks. However, this approach presents\nimportant advantages that motivate further work in this area. First, independence-based algorithms\nare sound (under assumptions) and efficient. Second, data availability is growing increasingly with\ntime. Third, there are several promising open problems (enumerated in the next section) whose\n\n\fA survey on independence-based Markov networks learning\n\n21\n\nsolutions are expected to result in improvements in the quality of structures produced by this\ntechnology.\n\n5.2 Open Problems\nFollowing the analysis of last section, this section discusses a series of open problems that remain in\nthe area. All the listed problems focus on the quality and the efficiency of the independence-based\napproach for learning Markov networks.\nOpen Problem 1 Avoiding cascade errors.\nMost independence-based algorithms surveyed (GSMN, GSIMN, DGSIMN) learn the Markov\nblanket of variables using the GS algorithm. Learning the structure by using GS can be seen as\na greedy search over the space of structures, where the outcomes of tests are used for discarding\nall those structures that are inconsistent with the independence indicated by the test. Therefore,\nan important source of errors in GS is the cascade errors produced by erroneous statistical tests,\nthat produces cumulatively errors.\nIs it possible to tackle the cascade effect, taking into account that tests are not\nalways reliable?\nOpen Problem 2 Independence-based quality measures. The PFMN algorithm uses the\nparticle filter approach for optimizing the selection of tests to perform. It utilizes a generative\nmodel that computes approximately the posterior probability Pr(G | D) of independence structures given the data. Interestingly, this posterior probability can be efficiently computed, and can\nbe used as a quality measure of candidate structures in an optimization method. This measure\nof structures quality has the advantage of avoiding cascade errors by assigning probabilities to\nstructures. This is an unexplored area for learning the structure of Markov networks.\nIs it possible to adapt the structure posterior computation of PFMN into an\nefficient and sound score, by relaxing the approximation? Would the optimization\nof such score improve the quality of the structures learned?\nOpen Problem 3 Speeding up independence-based algorithms. Learning the structure under the independence-based approach requires in some cases the execution of a massive amount\nof statistical independence tests on data. An intermediate step in the computation of independence tests is the construction of contingency tables from the dataset, that record the frequency\ndistribution of the variables involved in the test, resulting in running times linear in the size of\nthe dataset.\nCan the contingency tables of some test be reused in the construction of\ncontingency tables for other tests? How can an independence-based algorithm use\nsuch a mechanism for minimizing the number of whole readings of the dataset?\nUnder what conditions would this mechanism generate gains in runtime\ncomplexity?\nOpen Problem 4 Inconsistencies in local-to-global algorithms. Independence-based algorithms using the local-to-global strategy (e.g., GSMN and variants) decompose the problem of\nlearning a complete independence structure with n variables into n independent Markov blanket\n\n\f22\n\nFederico Schl\u00fcter\n\nlearning problems. On a second step, these algorithms piece together all the learned Markov blankets into a global structure using an \"OR rule\". Insufficient data may result in incorrect learning\nof Markov blankets, with conflicts in their decision on edge inclusion when, for two variables X\nand Y , X is found to be in the blanket of Y , but Y is found not to be in the blanket of X. In\nsuch cases, the \"OR rule\" always decides to add the edge, making mistakes when such edge does\nnot exist.\nIs it possible to design more robust rules for solving inconsistencies between two\nlearned Markov blankets?\nOpen Problem 5 Comparing independence-based and score-based approaches. There\nare several experimental comparisons currently lacking in the literature:\n\u2013 There are no experimental results published comparing the sample complexity of both approaches.\n\u2013 There are no experimental results published comparing quality of structures learned by both\napproaches.\n\u2013 There are no experimental results published comparing quality of complete models: i) those\nmodels learned by score-based approach (interleaving structure search and parameters estimation) versus ii) models learned by independence-based approach (learning the structure and\nthen fitting the parameters only once for such structure).\nAre the independence-based algorithms valid as a practical alternative to\nscore-based algorithms for learning the structure, and for learning the complete\nmodel?\nOpen Problem 6 Adapting recent Bayesian network ideas to Markov networks. The\nfirst independence-based algorithm proposed is GSMN, an adaptation to Markov networks of\nthe GS algorithm. In the literature there are several recently proposed ideas for improving the\nefficiency, quality and sample complexity of GS, as those discussed by the authors of IAMB.\nMMPC/MB, HITON-PC/MB, Fast-IAMB, PCMB and IPC-MB algorithms (see Section 3.3.2,\nfor more details). However, all these interesting ideas are originally developed and tested for\nlearning the structure of Bayesian networks.\nCan this research be adapted to the Markov networks structure learning problem,\nto generate some improvements in the area?\nOpen Problem 7 Independence knowledge bases. The argumentative independence test improves the accuracy of tests significantly when data is scarce. However, the exact algorithm\nproposed by this approach runs in exponential time, because Pearl's axioms are in first-order\nlogics, and knowledge bases in argumentation are propositional (as detailed in Section 4.5). The\napproximate solution presented is polynomial in running time, still improving the quality, but\nmaking a drastic and rather simplistic approximation that does not provide theoretical guarantees.\nCan the Pearl's axioms be exploited in a more efficient manner, through a better\napproximation, by an alternative formalism for reasoning under inconsistencies?\nOpen Problem 8 Relating independence assertions. Statistical tests are procedures that run\nindependently to each other, and they are used as a black box by independence-based algorithms.\nEach test responds to a conditional independence query only using the input dataset. Thus, an\n\n\fA survey on independence-based Markov networks learning\n\n23\n\nimplicit assumption made by all the independence-based algorithms is that all the independences\nqueried by the algorithm are mutually independent to each other given the dataset. This assumption is only true when data is sufficiently large for the test to determine the true underlying\nindependence, because in this case, information from other tests is irrelevant. However, when\ndata is not sufficient for correctly determining the independence, tests may become dependent\ngiven the data, i.e., information from other tests may be useful for determining the value of a\ntest, and avoiding errors. An example shown in the literature for correcting errors when data\nis insufficient is the argumentative independence test, that relates statistical tests through the\nPearl's axioms, as additional information for improving the quality of tests when data is not\nsufficient.\nBesides Pearl's axioms, are there other dependence relations governing\nindependence assertions? As in the case of Pearl's axioms, can these relations be\nused as additional information for improving the quality of independence-based\nalgorithms?\n6 Conclusions\nThe present work discussed the most relevant technical aspects in the problem of learning the\nMarkov network structure from data, stressing on independence-based algorithms. Summarizing the\nanalysis of such technology, the advantages of independence-based algorithms for learning Markov\nnetworks are overshadowed by the low quality of learned structures when data is scarce, or equivalently, when the underlying network is highly connected. However, this approach presents important\nadvantages that motivate further work in this area. First, independence-based algorithms are sound\nunder assumptions, and efficient. Second, data availability is growing increasingly with time. Therefore, it is expected that the solutions of the open problems posed in this work result in improvements\nin the quality of structures produced by this technology.\n\n\f24\n\nFederico Schl\u00fcter\n\nReferences\nAgresti A (2002) Categorical Data Analysis, 2nd edn. Wiley\nAlden M (2007) MARLEDA: Effective Distribution Estimation Through Markov Random Fields.\nPhD thesis, Dept of CS, University of Texas Austin\nAliferis C, Tsamardinos I, Statnikov A (2003) HITON, a novel Markov blanket algorithm for optimal\nvariable selection. AMIA Fall\nAliferis C, Statnikov A, Tsamardinos I, Mani S, Koutsoukos X (2010a) Local Causal and Markov\nBlanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms\nand Empirical Evaluation. JMLR 11:171\u2013234\nAliferis C, Statnikov A, Tsamardinos I, Mani S, Koutsoukos X (2010b) Local Causal and Markov\nBlanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis\nand Extensions. JMLR 11:235\u2013284\nAmgoud L, Cayrol C (2002) A Reasoning Model Based on the Production of Acceptable Arguments.\nAnnals of Mathematics and Artificial Intelligence 34:197\u2013215\nAnguelov D, Taskar B, Chatalbashev V, Koller D, Gupta D, Heitz G, Ng A (2005) Discriminative\nLearning of Markov Random Fields for Segmentation of 3D Range Data. Proceedings of the\nCVPR\nBarahona F (1982) On the computational complexity of Ising spin glass models. Journal of Physics\nA: Mathematical and General 15(10):3241\u20133253\nBesag J (1977) Efficiency of pseudolikelihood estimation for simple Gaussian fields. Biometrica\n64:616\u2013618\nBesag J, York J, Mollie A (1991) Bayesian image restoration with two applications in spatial\nstatistics. Annals of the Inst of Stat Math 43:1\u201359\nBromberg F (2007) Markov network structure discovery using independence tests. PhD thesis, Dept\nof CS, Iowa State University\nBromberg F, Margaritis D (2007) Efficient and robust independence-based Markov network structure discovery. In: Proceedings of IJCAI\nBromberg F, Margaritis D (2009) Improving the Reliability of Causal Discovery from Small Data\nSets using Argumentation. JMLR 10:301\u2013340\nBromberg F, Margaritis D, Honavar V (2006) Efficient markov network structure discovery using\nindependence tests. In: In Proc SIAM Data Mining, p 06\nBromberg F, Margaritis D, V H (2009) Efficient Markov Network Structure Discovery Using Independence Tests. JAIR 35:449\u2013485\nCai Kk, Bu Jj, Chen C, Qiu G (2007) A novel dependency language model for information retrieval.\nJournal of Zhejiang University - Science A 8:871\u2013882, 10.1631/jzus.2007.A0871\nCochran WG (1954) Some methods of strengthening the common \u03c7 tests. Biometrics p 10:417\u2013451\nCooper GF (1990) The computational complexity of probabilistic inference using bayesian belief\nnetworks. Artificial Intelligence 42(2-3):393 \u2013 405, DOI DOI:10.1016/0004-3702(90)90060-D\nCover TM, Thomas JA (1991) Elements of information theory. Wiley-Interscience, New York, NY,\nUSA\nCressie N (1992) Statistics for spatial data. Terra Nova 4(5):613\u2013617, DOI 10.1111/j.1365-3121.\n1992.tb00605.x\nDavis J, Domingos P (2010) Bottom-Up Learning of Markov Network Structure. In: ICML, pp\n271\u2013278\n\n\fA survey on independence-based Markov networks learning\n\n25\n\nDella Pietra S, Della Pietra VJ, Lafferty JD (1997) Inducing Features of Random Fields. IEEE\nTrans PAMI 19(4):380\u2013393\nFriedman N, Linial M, Nachman I, Pe'er D (2000) Using Bayesian Networks to Analyze Expression\nData. Computational Biology 7:601\u2013620\nFu S, Desmarais MC (2008) Fast Markov blanket discovery algorithm via local learning within\nsingle pass. In: Proceedings of the Canadian Society for computational studies of intelligence, 21st\nconference on Advances in artificial intelligence, Springer-Verlag, Berlin, Heidelberg, Canadian\nAI'08, pp 96\u2013107\nFu S, Desmarais MC (2010) Markov Blanket based Feature Selection : A Review of Past Decade.\nProceedings of the World Congress on Engineering 2010 I:321\u2013328\nGanapathi V, Vickrey D, Duchi J, Koller D (2008) Constrained Approximate Maximum Entropy\nLearning of Markov Random Fields. In: Uncertainty in Artificial Intelligence, pp 196\u2013203\nGandhi P, Bromberg F, Margaritis D (2008) Learning Markov Network Structure using Few Independence Tests. In: SIAM International Conference on Data Mining, pp 680\u2013691\nHeckerman D, Geiger D, Chickering DM (1995) Learning Bayesian Networks: The Combination of\nKnowledge and Statistical Data. Machine Learning\nH\u00f6fling H, Tibshirani R (2009) Estimation of Sparse Binary Pairwise Markov Networks using\nPseudo-likelihoods. Journal of Machine Learning Research 10:883\u2013906\nHyv\u00e4rinen A, Dayan P (2005) Estimation of non-normalized statistical models by score matching.\nJournal of Machine Learning Research 6:695\u2013709\nKaryotis V (2010) Markov random fields for malware propagation: the case of chain networks.\nComm Letters 14:875\u2013877\nKoller D, Friedman N (2009) Probabilistic Graphical Models: Principles and Techniques. MIT Press\nKoller D, Sahami M (1996) Toward Optimal Feature Selection. Morgan Kaufmann, pp 284\u2013292\nLam W, Bacchus F (1994) Learning Bayesian belief networks: an approach based on the MDL\nprinciple. Computational Intelligence 10:269\u2013293\nLarra\u00f1aga P, Lozano JA (2002) Estimation of Distribution Algorithms. A New Tool for Evolutionary Computation. Kluwer Pubs\nLauritzen SL (1996) Graphical Models. Oxford University Press\nLee SI, Ganapathi V, Koller D (2006) Efficient structure learning of Markov networks using L1regularization. In: NIPS\nLi SZ (2001) Markov random field modeling in image analysis. Springer-Verlag New York, Inc.,\nSecaucus, NJ, USA\nMargaritis D (2005) Distribution-Free Learning of Bayesian Network Structure in Continuous Domains. In: Proceedings of AAAI\nMargaritis D, Bromberg F (2009) Efficient Markov Network Discovery Using Particle Filter. Comp\nIntel 25(4):367\u2013394\nMargaritis D, Thrun S (2000) Bayesian network induction via local neighborhoods. In: Proceedings\nof NIPS\nMcCallum A (2003) Efficiently inducing features of conditional random fields. In: Proceedings of\nUncertainty in Artificial Intelligence (UAI)\nMetzler D, Croft WB (2005) A markov random field model for term dependencies. In: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in\ninformation retrieval, ACM, New York, NY, USA, SIGIR '05, pp 472\u2013479\nMinka T (2001) Algorithms for maximum-likelihood logistic regression. Tech. rep., Dept of Statistics,\nCarnegie Mellon University\n\n\f26\n\nFederico Schl\u00fcter\n\nMinka T (2004) Power EP. Tech. Rep. MSR-TR-2004-149, Microsoft Research, Cambridge\nMooij JM (2010) libDAI: A Free and Open Source C++ Library for Discrete Approximate Inference\nin Graphical Models. J Mach Learn Res 11:2169\u20132173\nPearl J (1988) Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.\nMorgan Kaufmann Publishers, Inc.\nPearl J, Paz A (1985) GRAPHOIDS : A graph based logic for reasonning about relevance relations.\nTech. Rep. 850038 (R-53-L), Cognitive Systems Laboratory, University of California, Los Angeles\nPe\u00f1a JM, Nilsson R, Bj\u00f6rkegren J, Tegn\u00e9r J (2007) Towards scalable and data efficient learning of\nMarkov boundaries. Int J Approx Reasoning pp 211\u2013232\nRavikumar P, Wainwright MJ, Lafferty JD (2010) High-dimensional Ising model selection using\nL1-regularized logistic regression. Annals of Statistics 38:1287\u20131319, DOI 10.1214/09-AOS691\nSchmidt M, Murphy K, Fung G, Rosales R (2008) Structure learning in random fields for heart\nmotion abnormality detection. In: Computer Vision and Pattern Recognition, 2008. CVPR 2008.\nIEEE Conference on, pp 1 \u20138, DOI 10.1109/CVPR.2008.4587367\nShakya S, Santana R (2008) A markovianity based optimization algorithm. Tech. rep., Basque\nCountry U.\nShekhar S, Zhang P, Huang Y, Vatsavai RR (2004) Trends in Spatial Data Mining. In: Kargupta\nH, Joshi A, Sivakumar K, Yesha Y (eds) Trends in Spatial Data Mining, AAAI Press / The MIT\nPress, chap 19, pp 357\u2013379\nSpirtes P, Glymour C, Scheines R (2000) Causation, Prediction, and Search. Adaptive Computation\nand Machine Learning Series, MIT Press\nTsamardinos I, Aliferis CF, Statnikov A (2003) Algorithms for large scale Markov blanket discovery.\nIn: FLAIRS\nTsamardinos I, Brown L, Aliferis CF (2006) The max-min hill-climbing Bayesian network structure\nlearning algorithm. Machine Learning 65:31\u201378\nVishwanathan SVN, Schraudolph NN, Schmidt MW, Murphy KP (2006) Accelerated training of\nconditional random fields with stochastic gradient methods. In: Proceedings of the 23rd international conference on Machine learning, ACM, New York, NY, USA, ICML '06, pp 969\u2013976\nWainwright MJ, Jordan MI (2008) Graphical Models, Exponential Families, and Variational Inference. Found Trends Mach Learn 1:1\u2013305, DOI 10.1561/2200000001\nWainwright MJ, Jaakkola TS, Willsky AS (2003) Tree-reweighted belief propagation algorithms\nand approximate ML estimation by pseudo-moment matching. In: In AISTATS\nWinn J, Bishop CM (2005) Variational Message Passing. J Mach Learn Res 6:661\u2013694\nYaramakala S, Margaritis D (2005) Speculative Markov blanket discovery for optimal feature selection. In: Data Mining, Fifth IEEE International Conference on, p 4 pp., DOI 10.1109/ICDM.\n2005.134\nYedidia J, Freeman W, Weiss Y (2005) Constructing free-energy approximations and generalized\nbelief propagation algorithms. Information Theory, IEEE Transactions on 51(7):2282 \u2013 2312,\nDOI 10.1109/TIT.2005.850085\nYedidia JS, Freeman WT, Weiss Y (2004) Constructing Free Energy Approximations and Generalized Belief Propagation Algorithms. IEEE Transactions on Information Theory 51:2282\u20132312\n\n\f"}