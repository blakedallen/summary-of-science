{"id": "http://arxiv.org/abs/0909.0999v1", "guidislink": true, "updated": "2009-09-05T06:22:48Z", "updated_parsed": [2009, 9, 5, 6, 22, 48, 5, 248, 0], "published": "2009-09-05T06:22:48Z", "published_parsed": [2009, 9, 5, 6, 22, 48, 5, 248, 0], "title": "Adaptive density estimation for stationary processes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0909.4236%2C0909.4130%2C0909.0524%2C0909.1890%2C0909.0842%2C0909.0999%2C0909.0257%2C0909.3701%2C0909.0125%2C0909.0779%2C0909.3547%2C0909.0301%2C0909.4766%2C0909.0795%2C0909.2540%2C0909.1584%2C0909.0789%2C0909.1682%2C0909.2904%2C0909.3625%2C0909.4513%2C0909.2045%2C0909.0253%2C0909.1891%2C0909.5522%2C0909.1436%2C0909.5287%2C0909.4151%2C0909.3782%2C0909.4217%2C0909.0188%2C0909.4750%2C0909.4312%2C0909.5545%2C0909.2780%2C0909.2018%2C0909.4946%2C0909.5510%2C0909.2634%2C0909.5121%2C0909.0355%2C0909.3128%2C0909.3908%2C0909.2872%2C0909.1438%2C0909.2863%2C0909.2914%2C0909.4413%2C0909.2407%2C0909.3648%2C0909.3792%2C0909.5487%2C0909.0275%2C0909.2990%2C0909.1770%2C0909.4608%2C0909.3643%2C0909.3095%2C0909.5135%2C0909.2054%2C0909.0394%2C0909.3019%2C0909.0678%2C0909.4739%2C0909.1291%2C0909.2413%2C0909.5252%2C0909.1491%2C0909.0598%2C0909.2522%2C0909.4270%2C0909.2354%2C0909.0727%2C0909.0786%2C0909.3706%2C0909.1801%2C0909.2884%2C0909.0854%2C0909.0333%2C0909.5410%2C0909.5082%2C0909.3876%2C0909.0264%2C0909.0956%2C0909.1065%2C0909.3733%2C0909.1190%2C0909.2680%2C0909.1462%2C0909.4802%2C0909.0471%2C0909.3702%2C0909.4250%2C0909.2412%2C0909.5015%2C0909.3212%2C0909.0308%2C0909.2290%2C0909.5321%2C0909.2639%2C0909.0530&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Adaptive density estimation for stationary processes"}, "summary": "We propose an algorithm to estimate the common density $s$ of a stationary\nprocess $X_1,...,X_n$. We suppose that the process is either $\\beta$ or\n$\\tau$-mixing. We provide a model selection procedure based on a generalization\nof Mallows' $C_p$ and we prove oracle inequalities for the selected estimator\nunder a few prior assumptions on the collection of models and on the mixing\ncoefficients. We prove that our estimator is adaptive over a class of Besov\nspaces, namely, we prove that it achieves the same rates of convergence as in\nthe i.i.d framework.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0909.4236%2C0909.4130%2C0909.0524%2C0909.1890%2C0909.0842%2C0909.0999%2C0909.0257%2C0909.3701%2C0909.0125%2C0909.0779%2C0909.3547%2C0909.0301%2C0909.4766%2C0909.0795%2C0909.2540%2C0909.1584%2C0909.0789%2C0909.1682%2C0909.2904%2C0909.3625%2C0909.4513%2C0909.2045%2C0909.0253%2C0909.1891%2C0909.5522%2C0909.1436%2C0909.5287%2C0909.4151%2C0909.3782%2C0909.4217%2C0909.0188%2C0909.4750%2C0909.4312%2C0909.5545%2C0909.2780%2C0909.2018%2C0909.4946%2C0909.5510%2C0909.2634%2C0909.5121%2C0909.0355%2C0909.3128%2C0909.3908%2C0909.2872%2C0909.1438%2C0909.2863%2C0909.2914%2C0909.4413%2C0909.2407%2C0909.3648%2C0909.3792%2C0909.5487%2C0909.0275%2C0909.2990%2C0909.1770%2C0909.4608%2C0909.3643%2C0909.3095%2C0909.5135%2C0909.2054%2C0909.0394%2C0909.3019%2C0909.0678%2C0909.4739%2C0909.1291%2C0909.2413%2C0909.5252%2C0909.1491%2C0909.0598%2C0909.2522%2C0909.4270%2C0909.2354%2C0909.0727%2C0909.0786%2C0909.3706%2C0909.1801%2C0909.2884%2C0909.0854%2C0909.0333%2C0909.5410%2C0909.5082%2C0909.3876%2C0909.0264%2C0909.0956%2C0909.1065%2C0909.3733%2C0909.1190%2C0909.2680%2C0909.1462%2C0909.4802%2C0909.0471%2C0909.3702%2C0909.4250%2C0909.2412%2C0909.5015%2C0909.3212%2C0909.0308%2C0909.2290%2C0909.5321%2C0909.2639%2C0909.0530&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We propose an algorithm to estimate the common density $s$ of a stationary\nprocess $X_1,...,X_n$. We suppose that the process is either $\\beta$ or\n$\\tau$-mixing. We provide a model selection procedure based on a generalization\nof Mallows' $C_p$ and we prove oracle inequalities for the selected estimator\nunder a few prior assumptions on the collection of models and on the mixing\ncoefficients. We prove that our estimator is adaptive over a class of Besov\nspaces, namely, we prove that it achieves the same rates of convergence as in\nthe i.i.d framework."}, "authors": ["Matthieu Lerasle"], "author_detail": {"name": "Matthieu Lerasle"}, "author": "Matthieu Lerasle", "links": [{"title": "doi", "href": "http://dx.doi.org/10.3103/S1066530709010049", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0909.0999v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0909.0999v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62G07, 62M99.", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0909.0999v1", "affiliation": "IMT", "arxiv_url": "http://arxiv.org/abs/0909.0999v1", "arxiv_comment": null, "journal_reference": "Mathematical Methods of Statistics 18, 1 (2009) 59--83", "doi": "10.3103/S1066530709010049", "fulltext": "Adaptive density estimation of stationary\n\n\u03c4 -mixing\n\n\u03b2 -mixing\n\nand\n\npro esses.\n\narXiv:0909.0999v1 [math.ST] 5 Sep 2009\n\nMatthieu Lerasle\u2217\nAbstra t:\n\nWe propose an algorithm to estimate the ommon density s of a stationary\npro ess X1, ..., Xn. We suppose that the pro ess is either \u03b2 or \u03c4 -mixing. We\nprovide a model sele tion pro edure based on a generalization of Mallows' Cp\nand we prove ora le inequalities for the sele ted estimator under a few prior\nassumptions on the olle tion of models and on the mixing oe\u001e ients. We\nprove that our estimator is adaptive over a lass of Besov spa es, namely, we\nprove that it a hieves the same rates of onvergen e as in the i.i.d framework.\nKey words: Density estimation, weak dependen e, model sele tion.\n2000 Mathemati s Subje t Classi\u001c ation: 62G07, 62M99.\n1\n\nIntrodu tion\n\nWe onsider the problem of estimating the unknown density s of P , the law of a random\nvariable X , based on the observation of n (possibly) dependent data X1 , ..., Xn with ommon law P . We assume that X is real valued, that s belongs to L2 (\u03bc) where \u03bc denotes the\nLebesgue measure on R and that s is ompa tly supported, say in [0, 1]. Throughout the\nhapter, we onsider least-squares estimators \u015dm of s on a olle tion (Sm )m\u2208Mn of linear\nsubspa es of L2 (\u03bc). Our \u001cnal estimator is hosen through a model sele tion algorithm.\nModel sele tion has re eived mu h interest in the last de ades. When its \u001cnal goal is predi tion, it an be seen more generally as the question of hoosing between the out omes of\nseveral predi tion algorithms. With su h a general formulation, a very natural answer is\nthe following. First, estimate the predi tion error for ea h model, that is ks \u2212 \u015dm k22 . Then,\nsele t the model whi h minimizes this estimate.\nIt is natural to think of the empiri al risk as an estimator of the predi tion error. This an\nfail dramati ally, be ause it uses the same data for building predi tors and for omparing\nthem, making these estimates strongly biased for models involving a number of parameters\ngrowing with the sample size.\nIn order to orre t this drawba k, penalization's methods state that a good hoi e an be\nmade by minimizing the sum of the empiri al risk (how do algorithms \u001ct the data) and\nsome omplexity measure of the algorithms ( alled the penalty). This method was \u001crst\ndevelopped in the work of Akaike [2\u2104 and [1\u2104 and Mallows [19\u2104.\nIn the ontext of density estimation, with independent data, Birg\u00e9 & Massart [8\u2104 used\npenalties of order Ln Dm /n, where Dm denotes the dimension of Sm and Ln is a onstant\ndepending on the omplexity of the olle tion Mn . They used Talagrand's inequality (see\nfor example Talagrand [24\u2104 for an overview) to prove that this penalization pro edure is\n\u2217\n\nInstitut de Math\u00e9matiques (UMR 5219), INSA de Toulouse, Universit\u00e9 de Toulouse, Fran e\n\n1\n\n\fe\u001e ient i.e. the integrated quadrati risk of the sele ted estimator is asymptoti ally equivalent to the risk of the ora le (see Se tion 2 for a pre ise de\u001cnition). They also proved that\nthe sele ted estimator a hieves adaptive rates of onvergen e over a large lass of Besov\nspa es. Moreover, they showed that some methods of adaptive density estimation like the\nunbiased ross validation (Rudemo [23\u2104) or the hard thresholded estimator of Donoho et\nal. [16\u2104 an be viewed as spe ial instan es of penalized proje tion estimators.\nMore re ently, Arlot [5\u2104 introdu ed new measures of the quality of penalized least-squares\nestimators (PLSE). He proved pathwise ora le inequalities, that is deviation bounds for\nthe PLSE that are harder to prove but more informative from a pra ti al point of view\n(see also Se tion 2 for details).\nWhen the pro ess (Xi )i=1,...,n is \u03b2 -mixing (Rozanov & Volkonskii [26\u2104 and Se tion 2), Talagrand's inequality an not be used dire tly. Baraud et al. [6\u2104 used Berbee's oupling\nlemma (see Berbee ([7\u2104) and Viennet's ovarian e inequality (Viennet [25\u2104) to over ome\nthis problem and build model sele tion pro edure in the regression problem. Then Comte\n& Merlev\u00e8de [13\u2104 used this algorithm to investigate the problem of density estimation for\na \u03b2 -mixing pro ess. They proved that under reasonable assumptions on the olle tion Mn\nand on the oe\u001e ients \u03b2 , one an re over the results of Birg\u00e9 & Massart [8\u2104 in the i.i.d.\nframework.\nThe main drawba k of those results is that many pro esses, even simple Markov hains\nare not \u03b2 -mixing. For instan e, if (\u01ebi )i\u22651 is iid with marginal B(1/2), then the stationary\nsolution (Xi )i\u22650 of the equation\n\n1\nXn = (Xn\u22121 + \u01ebn ), X0 independent of (\u01ebi )i\u22651\n2\n\n(1)\n\nis not \u03b2 -mixing (Andrews [3\u2104). More re ently, Dede ker & Prieur [15\u2104 introdu ed new\nmixing- oe\u001e ients, in parti ular the oe\u001e ients \u03c4 , \u03c6\u0303 and \u03b2\u0303 and proved that many pro esses\nlike (1) happen to be \u03c4 , \u03c6\u0303 and \u03b2\u0303 -mixing. They proved a oupling lemma for the oe\u001e ient\n\u03c4 and ovarian e inequalities for \u03c6\u0303 and \u03b2\u0303 . Gannaz & Wintenberger [18\u2104 used the ovarian e\ninequality to extend the result of Donoho et al. [16\u2104 for the wavelet thresholded estimator\nto the ase of \u03c6\u0303-mixing pro esses. They re overed (up to a log(n) fa tor) the adaptive\nrates of onvergen e over Besov spa es.\nIn this arti le, we \u001crst investigate the ase of \u03b2 -mixing pro esses. We prove a pathwise\nora le inequality for the PLSE. We extend the result of Comte & Merlev\u00e8de [13\u2104 under\nweaker assumptions on the mixing oe\u001e ients. Then, we onsider \u03c4 -mixing pro esses. The\nproblem is that the oupling result is weaker for the oe\u001e ient \u03c4 than for \u03b2 . Moreover,\nin order to ontrol the empiri al pro ess we use a ovarian e inequality that is harder to\nhandle. Hen e, the generalization of the pro edure of Baraud et al. [6\u2104 to the framework\nof \u03c4 -mixing pro esses is not straightforward. We re over the optimal adaptive rates of\nonvergen e over Besov spa es (that is the same as in the independent framework) for\n\u03c4 -mixing pro esses, whi h is new as far as we know.\nThe hapter is organized as follows. In Se tion 2, we give the basi material that we will\nuse throughout the hapter. We re all the de\u001cnition of some mixing oe\u001e ients and we\nstate their properties. We de\u001cne the penalized least-squares estimator (PLSE). Se tions 3\nand 4 are devoted to the statement of the main results, respe tively in the \u03b2 -mixing ase\nand in the \u03c4 -mixing ase. In Se tion 5, we derive the adaptive properties of the PLSE.\nFinally, Se tion 6 is devoted to the proofs. Some additional material has been reported in\nthe Appendix in Se tion 7.\n\n2\n\n\f2\n2.1\n\nPreliminaries\nNotation.\n\nLet (\u03a9, A, P) be a probability spa e. Let \u03bc be the Lebesgue measure on R, let k.kp be the\nP\nusual norm on Lp (\u03bc) for 1 \u2264 p \u2264 \u221e. For all y \u2208 Rl , let |y|l = li=1 |yi |. Denote by \u03bb\u03ba the\nset of \u03ba-Lips hitz fun tions, i.e. the fun tions t from (Rl , |.|l ) to R su h that Lip(t) \u2264 \u03ba\nwhere\n\u001b\n\u001a\n|t(x) \u2212 t(y)|\nl\nLip(t) = sup\n, x, y \u2208 R , x 6= y \u2264 \u03ba.\n|x \u2212 y|l\nLet BV and BV1 be the set of fun tions t supported on R satisfying respe tively ktkBV < \u221e\nand ktkBV \u2264 1 where\n\nktkBV = sup\n\nsup\n\nn\u2208N\u2217 \u2212\u221e<a1 <...<an <\u221e\n\n2.2\n\n|t(ai+1 ) \u2212 t(ai )|.\n\nSome measures of dependen e.\n\n2.2.1 De\u001cnitions and assumptions\nLet Y = (Y1 , ..., Yl ) be a random variable de\u001cned on (\u03a9, A, P) with values in (Rl , |.|l ). Let\nM be a \u03c3 -algebra of A. Let PY |M , PY1 |M be onditional distributions of Y and Y1 given\nM, let PY , PY1 be the distribution of Y and Y1 and let FY1 |M , FY1 be distribution fun tions\nof PY1 |M and PY1 . Let B be the Borel \u03c3 -algebra on (Rl , |.|l ). De\u001cne now\n\n\u0012\n\n\u0013\n\u03b2(M, \u03c3(Y )) = E sup |PY |M (A) \u2212 PY (A)| ,\nA\u2208B\n\u0012\n\u0013\n\u03b2\u0303(M, Y1 ) = E sup FY1 |M (x) \u2212 FY1 (x) ,\nx\u2208R\n\u0013\n\u0012\nand if E(|Y |) < \u221e, \u03c4 (M, Y ) = E sup |PY |M (t) \u2212 PY (t)| .\nt\u2208\u03bb1\n\nThe oe\u001e ient \u03b2(M, \u03c3(Y )) is the mixing oe\u001e ient introdu ed by Rozanov & Volkonskii\n[26\u2104. The oe\u001e ients \u03b2\u0303(M, Y1 ) and \u03c4 (M, Y ) have been introdu ed by Dede ker & Prieur\n[15\u2104.\nLet (Xk )k\u2208Z be a stationary sequen e of real valued random variables de\u001cned on (\u03a9, A, P).\nFor all k \u2208 N\u2217 , the oe\u001e ients \u03b2k , \u03b2\u0303k and \u03c4k are de\u001cned by\n\n\u03b2k = \u03b2(\u03c3(Xi , i \u2264 0), \u03c3(Xi , i \u2265 k)), \u03b2\u0303k = sup{\u03b2\u0303(\u03c3(Xp , p \u2264 0), Xj )}.\nj\u2265k\n\nIf E(|X1 |) < \u221e, for all k \u2208 N\u2217 and all r \u2208 N\u2217 , let\n\n\u03c4k,r = max\n\n1\u2264l\u2264r\n\n1\nsup {\u03c4 (\u03c3(Xp , p \u2264 0), (Xi1 , ..., Xil ))}, \u03c4k = sup \u03c4k,r .\nl k\u2264i1 <..<il\nr\u2208N\u2217\n\nMoreover, we set \u03b20 = 1. In the sequel, the pro esses of interest are either \u03b2 -mixing or\n\u03c4 -mixing, meaning that, for \u03b3 = \u03b2 or \u03c4 , the \u03b3 -mixing oe\u001e ients \u03b3k \u2192 0 as k \u2192 +\u221e. For\np \u2208 {1, 2}, we de\u001cne \u03bap as:\n\u221e\nX\n\u03bap = p\n(2)\nlp\u22121 \u03b2l ,\nl=0\n\n3\n\n\fwhere 00 = 1, when the series are onvergent. Besides, we onsider two kinds of rates of\nonvergen e to 0 of the mixing oe\u001e ients, that is for \u03b3 = \u03b2 or \u03c4 ,\n[AR] arithmeti al \u03b3 -mixing with rate \u03b8 if there exists some \u03b8 > 0 su h that \u03b3k \u2264 (1 +\nk)\u2212(1+\u03b8) for all k in N,\n[GEO] geometri al \u03b3 -mixing with rate \u03b8 if there exists some \u03b8 > 0 su h that \u03b3k \u2264 e\u2212\u03b8k\nfor all k in N.\n\n2.2.2 Properties\nCoupling\n\nLet X be an Rl -valued random variable de\u001cned on (\u03a9, A, P) and let M be a \u03c3 -algebra.\nAssume that there exists a random variable U uniformly distributed on [0, 1] and independent of M \u2228 \u03c3(X). There exist two M \u2228 \u03c3(X) \u2228 \u03c3(U )-measurable random variables X1\u2217\nand X2\u2217 distributed as X and independent of M su h that\n\n\u03b2(M, \u03c3(X)) = P(X 6= X1\u2217 ) and\n\n(3)\n\n\u03c4 (M, X) = E (|X \u2212 X2\u2217 |l ) .\n\n(4)\n\nEquality (3) has been established by Berbee [7\u2104, Equality (4) has been established in\nDede ker & Prieur [15\u2104, Se tion 7.1.\n\nCovarian e inequalities\n\nLet X, Y be two real valued random variables and let f, h be two measurable fun tions\nfrom R to C. Then, there exist two measurable fun tions b1 : R \u2192 R and b2 : R \u2192 R with\nE (b1 (X)) = E(b2 (Y )) = \u03b2(\u03c3(X), \u03c3(Y )) su h that, for any onjugate p, q \u2265 1 (see Viennet\n[25\u2104 Lemma 4.1)\n\n|Cov(f (X), h(Y ))| \u2264 2E1/p (|f (X)|p b1 (X)) E1/q (|h(Y )|q b2 (Y )).\nThere exists a random variable b(\u03c3(X), Y ) su h that E(b(\u03c3(X), Y )) = \u03b2\u0303(\u03c3(X), Y ) and su h\nthat, for all Lips hitz fun tions f and all h in BV (Dede ker & Prieur [15\u2104 Proposition 1)\n\n|Cov(f (X), h(Y ))| \u2264 khkBV E (|f (X)|b(\u03c3(X), Y )) \u2264 khkBV kf k\u221e \u03b2\u0303(\u03c3(X), Y ).\n\n(5)\n\nComparison results\n\nLet (Xk )k\u2208Z be a sequen e of identi ally distributed real random variables. If the marginal\ndistribution satis\u001ces a on entration's ondition |FX (x) \u2212 FX (y)| \u2264 K|x \u2212 y|a with a \u2264 1,\nK > 0, then (Dede ker et al. [14\u2104 Remark 5.1 p 104)\na/(a+1)\n\n\u03b2\u0303k \u2264 2K 1/(1+a) \u03c4k,1\n\na/(a+1)\n\n\u2264 2K 1/(1+a) \u03c4k\n\n.\n\nIn parti ular, if PX has a density s with respe t to the Lebesgue measure \u03bc and if s \u2208 L2 (\u03bc),\nwe have from Cau hy-S hwarz inequality\n\n|FX (x) \u2212 FX (y)| = |\n\nZ\n\n1[x,y] sd\u03bc| \u2264 ksk2\n\nthus\n\n\u0012Z\n\n2/3\n\n1[x,y] d\u03bc\n\n\u00131/2\n\n= ksk2 |x \u2212 y|1/2 ,\n\n1/3\n\n\u03b2\u0303k \u2264 2 ksk2 \u03c4k .\n\nIn parti ular, for any arithmeti ally [AR] \u03c4 -mixing pro ess with rate \u03b8 > 2, we have\n2/3\n\n\u03b2\u0303k \u2264 2 ksk2 (1 + k)\u2212(1+\u03b8)/3 .\n4\n\n(6)\n\n\f2.2.3 Examples\nExamples of \u03b2 -mixing and \u03c4 -mixing sequen es are well known, we refer to the books of\nDoukhan [17\u2104 and Bradley [11\u2104 for examples of \u03b2 -mixing pro esses and to the book of\nDede ker et. al [14\u2104 or the arti les of Dede ker & Prieur [15\u2104, Prieur [21\u2104, and Comte\net. al [12\u2104 for examples of \u03c4 -mixing sequen es. One of the most important example is\nthe following: a stationary, irredu ible, aperiodi and positively re urent Markov hain\n(Xi )i\u22651 is \u03b2 -mixing. However, many simple Markov hains are not \u03b2 -mixing but are \u03c4 mixing. For instan e, it is known for a long time that if (\u01ebi )i\u22651 are i.i.d Bernoulli B(1/2),\nthen a stationary solution (Xi )i\u22650 of the equation\n\n1\nXn = (Xn\u22121 + \u01ebn ), X0 independent of (\u01ebi )i\u22651\n2\nis not \u03b2 -mixing sin e \u03b2k = 1 for any k \u2265 1 whereas \u03c4k \u2264 2\u2212k (see Dede ker & Prieur [15\u2104\nSe tion 4.1). Another advantage of the oe\u001e ient \u03c4 is that it is easy to ompute in many\nsituations (see Dede ker & Prieur [15\u2104 Se tion 4).\n2.3\n\nColle tions of models\n\nWe observe n identi ally distributed real valued random variables X1 , ..., Xn with ommon\ndensity s with respe t to the Lebesgue measure \u03bc. We assume that s belongs to the Hilbert\nspa e L2 (\u03bc) endowed with norm k.k2 . We onsider an orthonormal system {\u03c8j,k }(j,k)\u2208\u039b\nof L2 (\u03bc) and a olle tion of models (Sm )m\u2208Mn indexed by subsets m \u2282 \u039b for whi h we\nassume that the following assumptions are ful\u001clled:\n[M1 \u2104 for all m \u2208 Mn , Sm is the linear span of {\u03c8j,k }(j,k)\u2208m with \u001cnite dimension Dm =\n|m| \u2265 2 and Nn = maxm\u2208Mn Dm satis\u001ces Nn \u2264 n;\n[M2 \u2104 there exists a onstant \u03a6 su h that\np\n\u2200m, m\u2032 \u2208 Mn , \u2200t \u2208 Sm , \u2200t\u2032 \u2208 Sm\u2032 , kt + t\u2032 k\u221e \u2264 \u03a6 dim(Sm + Sm\u2032 )kt + t\u2032 k2 ;\n\n[M3 \u2104 Dm \u2264 Dm\u2032\n\nimplies that m \u2282 m\u2032 and so Sm \u2282 Sm\u2032 .\nAs a onsequen e of Cau hy-S hwarz inequality, we have\n\nX\n\n(j,k)\u2208m\u222am\u2032\n\n2\n\u03c8j,k\n\n=\n\u221e\n\nktk2\u221e\n2\nt\u2208Sm +Sm\u2032 ,t6=0 ktk2\nsup\n\n(7)\n\nsee Birg\u00e9 & Massart [8\u2104 p 58. Three examples are usually developed as ful\u001clling this set of\nassumptions:\n[T\u2104 trigonometri spa es: \u03c80,0 (x) = 1 and for all j \u2208 N\u2217 , \u03c8j,1(x) = cos(2\u03c0jx), \u03c8j,2 (x) =\nsin(2\u03c0jx). m = {(0, 0), (j, 1), (j \u2032 , 2), 1 \u2264 j, j \u2032 \u2264 Jm } and Dm = 2Jm + 1;\n[P\u2104 regular pie ewise polynomial spa es: Sm is generated by r polynomials \u03c8j,k of degree\nk = 0, ..., r \u2212 1 on ea h subinterval [(j \u2212 1)/Jm , j/Jm ] for j = 1, ..., Jm , Dm = rJm ,\nMn = {m = {(j, k), j = 1, ..., Jm , k = 0, ..., r \u2212 1}, 1 \u2264 Jm \u2264 [n/r]};\n[W\u2104 spa es generated by dyadi wavelet with regularity r as des ribed in Se tion 4.\nFor a pre ise des ription of those spa es and their properties, we refer to Birg\u00e9 & Massart\n[8\u2104.\n2.4\n\nThe estimator\n\nLet (Xn )n\u2208Z be a real valued stationary pro ess and let P denote the law of X0 . Assume\nthat P has a density s with respe t to the Lebesgue measure \u03bc and that s \u2208 L2 (\u03bc).\n5\n\n\fLet (Sm )m\u2208Mn be a olle tion of models satisfying assumptions [M1 \u2104-[M3 \u2104. We de\u001cne\nSn = \u222am\u2208Mn Sm , sm and sn the orthogonal proje tions of s onto Sm and Sn respe tively,\nlet P be the joint distribution of the observations (Xn )n\u2208Z and let E be the orresponding\nexpe tation. We de\u001cne the operators Pn , P and \u03bdn on L2 (\u03bc) by\nn\n\n1X\nt(Xi ), P t =\nPn t =\nn\ni=1\n\nZ\n\nt(x)s(x)d\u03bc(x), \u03bdn (t) = (Pn \u2212 P )t.\n\nAll the real numbers that we shall introdu e and whi h are not indexed by m or n are \u001cxed\nonstants. In order to de\u001cne the penalized least-squares estimator, let us onsider on R\u00d7Sn\nthe ontrast fun tion \u03b3(x, t) = \u22122t(x) + ktk22 and its empiri al version \u03b3n (t) = Pn \u03b3(., t).\nMinimizing \u03b3n (t) over Sm leads to the lassi al proje tion estimator \u015dm on Sm . Let \u015dn be\nthe proje tion estimator on Sn . Sin e {\u03c8j,k }(j,k)\u2208m is an orthonormal basis of Sm one gets\n\n\u015dm =\n\nX\n\n(Pn \u03c8j,k )\u03c8j,k and \u03b3n (\u015dm ) = \u2212\n\n(j,k)\u2208m\n\nX\n\n(Pn \u03c8j,k )2 .\n\n(j,k)\u2208m\n\nNow, given a penalty fun tion pen : Mn \u2192 R+ , we de\u001cne a sele ted model m\u0302 as any\nelement\nm\u0302 \u2208 arg min (\u03b3n (\u015dm ) + pen(m))\n(8)\nm\u2208Mn\n\nand a PLSE is de\u001cned as any s\u0303 \u2208 Sm\u0302 \u2282 Sn su h that\n\n\u03b3n (s\u0303) + pen(m\u0302) = inf (\u03b3n (\u015dm ) + pen(m)) .\nm\u2208Mn\n\n2.5\n\n(9)\n\nOra le inequalities\n\nAn ideal pro edure for estimation hooses an ora le\n\nmo \u2208 Arg min {ks \u2212 \u015dm k2 }.\nm\u2208Mn\n\nAn ora le depends on the unknown s and on the data so that it is unknown in pra ti e.\nIn order to validate our pro edure, we try to prove:\n-non asymptoti ora le inequalities for the PLSE:\n\u0011\n\u0010\n\u0001\nE ks \u2212 s\u0303k22 \u2264 L inf {E ks \u2212 \u015dm k22 + R(m, n) },\n(10)\nm\u2208Mn\n\nfor some onstant L \u2265 1 (as lose to 1 as\u0010 possible)\u0011 and a remainder term R(m, n) \u2265 0\npossibly random, and small ompared to E ks \u2212 s\u0303k22 if possible. This inequality ompares\nthe risk of the PLSE with the best deterministi hoi e of m. Sin e m\u0302 is random, we prefer\nto prove a stronger form of ora le inequality :\n\u0012\n\u0013\n\u0011\n\u0010\n2\n2\ninf {ks \u2212 \u015dm k2 + R(m, n)} ,\nE ks \u2212 s\u0303k2 \u2264 LE\n(11)\nm\u2208Mn\n\nor, when it is possible, deviation bounds for the PLSE:\n\u0012\n\u0010\n\u0011\u0013\n2\n2\nks \u2212 \u015dm k2 + R(m, n)\nP ks \u2212 s\u0303k2 > L inf\n\u2264 cn ,\nm\u2208Mn\n\n6\n\n(12)\n\n\fwhere typi ally cn \u2264 C/n1+\u03b3 for some \u03b3 > 0. Inequality (12) proves that, asymptoti ally,\nthe risk ks \u2212 s\u0303k22 is almost surely the one of the ora le. Let\n\u001a\n\u0010\n\u0011\u001b\n2\n2\n\u03a9 = ks \u2212 s\u0303k2 > L inf\nks \u2212 \u015dm k2 + R(m, n)\n.\nm\u2208Mn\n\nWe have\n\n\u0011\n\u0010\n\u0011\n\u0010\n\u0011\n\u0010\nE ks \u2212 s\u0303k22 = E ks \u2212 s\u0303k22 1\u03a9 + E ks \u2212 s\u0303k22 1\u03a9c .\nn\no\u0011\n\u0010\n\u0011\n\u0010\nIt is lear that E ks \u2212 s\u0303k22 1\u03a9c \u2264 LE inf m\u2208Mn ks \u2212 \u015dm k22 + R(m, n) . Moreover, we\n\nhave ks \u2212 s\u0303k2 = ks \u2212 sm\u0302 k2 + ksm\u0302 \u2212 s\u0303k2 \u2264 ksk2 + \u03a62 Dm\u0302 \u2264 ksk2 + \u03a62 n, thus, when (12)\nholds, we have\n\u0010\n\u0011\nC\nE ks \u2212 s\u0303k22 1\u03a9c \u2264 (ksk2 + \u03a62 n)cn \u2264 \u03b3 .\nn\nTherefore, inequality (12) implies\n\u0012\n\u0013\n\u0011\n\u0010\nC\n2\n2\ninf {ks \u2212 \u015dm k2 + R(m, n)} + \u03b3 .\nE ks \u2212 s\u0303k2 \u2264 E\nm\u2208Mn\nn\n\nWe an derive from these inequalities adaptive rates of onvergen e of the PLSE on Besov\nspa es (see Birg\u00e9 & Massart [8\u2104 for example). In order to a hieve this goal, we only have\nto prove a weaker form of ora le inequality where the remainder term R(m, n) \u2264 LDm /n\nfor some onstant L, for all the models m with su\u001e iently large dimension. This will be\ndetailed in Se tion 5.\n3\n\nResults for\n\n\u03b2 -mixing\n\npro esses\n\nFrom now on, the letters \u03ba, L and K , with various sub- or sups ripts, will denote some\nonstants whi h may vary from line to line. One shall use L. to indi ate more pre isely the\ndependen e on various quantities, espe ially those whi h are related to the unknown s.\nIn this se tion, we give the following theorem for \u03b2 -mixing sequen es. It an be seen as a\npathwise version of Theorem 3.1 in Comte & Merlev\u00e8de [13\u2104.\n\nTheorem 3.1 Consider a olle tion of models satisfying [M1 \u2104, [M2 \u2104 and [M3 \u2104. Assume\nthat the pro ess (Xn )n\u2208Z is stri tly stationary and arithmeti ally [AR\u2104 \u03b2 -mixing with mixing rate \u03b8 > 2 and that its marginal distribution admits a density s with respe t to the\nLebesgue measure \u03bc, with s \u2208 L2 (\u03bc).\nLet \u03ba1 be the onstant de\u001cned in (2) and let s\u0303 be the PLSE de\u001cned by (9) with\npen(m) =\n\nK\u03a62 \u03ba1 Dm\n, where K > 4.\nn\n\nThen, for all \u03ba > 2 there exist c0 > 0, Ls > 0, \u03b31 > 0 and a sequen e \u01ebn \u2192 0, su h that\n\u0012\nP ks\u0303 \u2212 sk22 > (1 + \u01ebn )\n\n\u0010\n\u0011\u0013\n(log n)(\u03b8+2)\u03ba\n2\nks\n\u2212\ns\nk\n+\npen\n(m)\ninf\n\u2264\nL\n.\nm 2\ns\nm\u2208Mn ,Dm \u2265c0 (log n)\u03b31\nn\u03b8/2\n(13)\n\nRemark:\n\nThe term K\u03a62 \u03ba1 is the same as in Theorem 3.1 of Comte & Merlev\u00e8de [13\u2104 but\nwith a onstant K > 4 instead of 320. The main drawba k of this result is that the penalty\nterm involves the onstant \u03ba1 whi h is unknown in pra ti e. However, Theorem 3.1 ensures\nthat penalties proportional to the linear dimension of Sm lead to e\u001e ient model sele tion\n7\n\n\fpro edures. Thus we an use this information to apply the slope heuristi algorithm introdu ed by Birg\u00e9 & Massart [9\u2104 in a Gaussian regression ontext and generalized by Arlot\n& Massart [4\u2104 to more general M-estimation frameworks. This algorithm alibrates the\nonstant in front of the penalty term when the shape of an ideal penalty is available. The\nresult of Arlot & Massart is proven for independent sequen es, in a regression framework,\nbut it an be generalized to the density estimation framework, for independent as well as\nfor \u03b2 or \u03c4 dependent data. This result is beyond the s ope of this hapter and will be\nproved in hapter 4.\nWe have to onsider the in\u001cmum in equation (13) over the models with su\u001e iently large\ndimensions. However, as noted by Arlot [5\u2104 (Remark 9 p 43), we an take the in\u001cmum over\nall the models in (13) if we add an extra term in (13). More pre isely, we an prove that,\nwith probability larger than 1 \u2212 Ls (log n)(\u03b8+2)\u03ba /n\u03b8/2\n\nks\u0303 \u2212 sk22 \u2264 (1 + \u01ebn ) inf\n\nm\u2208Mn\n\n\u0010\n\n\u0011\n(log n)\u03b32\nks \u2212 \u015dm k22 + pen(m) + L\n,\nn\n\n(14)\n\nwhere L > 0 and \u03b32 > 0.\nRemark : The main improvement of Theorem 3.1 is that it gives an ora le inequality in\nprobability, with a deviation bound of order o(1/n) as soon as \u03b8 > 2 instead of \u03b8 > 3 in\nComte & Merlev\u00e8de [13\u2104. Moreover, we do not require s to be bounded to prove our result.\nRemark: When the data are independent, the proof of Theorem 3.1 an be used to\nobtain that the estimator s\u0303 hosen with a penalty term of order K\u03a6Dm /n satisfy an ora le\ninequality as (13). The main di\u001beren e would be that \u03ba1 = 1, thus it an be used without\na slope heuristi (even if this algorithm an be used also in this ontext to optimize the\n2\nonstant K) and the ontrol of the probability would be Ls e\u2212 ln(n) /Cs for some onstants\nLs , Cs instead of Ls (log n)(\u03b8+2) \u03ban\u2212\u03b8/2 in our theorem.\n4\n\nResults for\n\n\u03c4 -mixing\n\nsequen es\n\nIn order to deal with \u03c4 -mixing sequen es, we need to spe ify the basis (\u03c8j,k )(j,k)\u2208\u039b .\n4.1\n\nWavelet basis\n\nThroughout this se tion, r is a real number, r \u2265 1 and we work with an r -regular orthonormal multiresolution analysis of L2 (\u03bc), asso iated with a ompa tly supported s aling fun tion \u03c6 and a ompa tly supported mother wavelet \u03c8 . Without loss of generality,\nwe suppose that the support of the fun tions \u03c6 and \u03c8 is an interval [A1 , A2 ) where A1\nand A2 are integers su h that A2 \u2212 A1 = A \u2265 1. Let us re all that \u03c6 and \u03c8 generate an\northonormal basis by dilatations and translations.\n\u221a\nFor all k \u2208 Z and j \u2208 N\u2217 , let \u03c80,k : x \u2192 2\u03c6(2x \u2212 k) and \u03c8j,k : x \u2192 2j/2 \u03c8(2j x \u2212 k). The\nfamily {(\u03c8j,k )j\u22650,k\u2208Z } is an orthonormal\nbasis of L2 (\u03bc). \u221a\nLet us re all the following inequal\u221a\nities: for all p \u2265 1, let Kp = ( 2k\u03c6kp ) \u2228 k\u03c8kp , KL = (2 2Lip(\u03c6)) \u2228 Lip(\u03c8), KBV = AKL .\nThen for all j \u2265 0, we have k\u03c8j,k k\u221e \u2264 K\u221e 2j/2 ,\n\nX\n\n\u2264 AK\u221e 2j/2\n\n(15)\n\nLip(\u03c8j,k ) \u2264 KL 23j/2 ,\n\n(16)\n\nk\u2208Z\n\n|\u03c8j,k |\n\n\u221e\n\n\u2264 KBV 2j/2 .\n\nk\u03c8j,k kBV\n\n8\n\n(17)\n\n\fWe assume that our olle tion (Sm )m\u2208Mn satis\u001ces the following assumption:\n[W\u2104 dyadi wavelet generated spa es: let Jn = [log(n/2(A + 1))/ log(2)] and for all Jm =\n1, ..., Jn , let\n\nm = {(0, k), \u2212A2 < k < 2 \u2212 A1 } \u222a {(j, k), 1 \u2264 j \u2264 Jm , \u2212A2 < k < \u2212A1 + 2j }\nand Sm the linear span of {\u03c8j,k }(j,k)\u2208m . In parti ular, we have Dm = (A\u22121)(Jm +1)+2Jm +1\nand thus 2Jm +1 \u2264 Dm \u2264 (A \u2212 1)(Jm + 1) + 2Jm +1 \u2264 A2Jm +1 .\n4.2\n\nThe\n\n\u03c4 -mixing\n\nase\n\nThe following result proves that we keep the same rate of onvergen e for the PLSE based\non \u03c4 -mixing pro esses.\n\nTheorem 4.1 Consider the olle tion of models [W\u2104. Assume that (Xn )n\u2208Z is stri tly\nstationary and arithmeti ally [AR\u2104 \u03c4 -mixing with mixing rate \u03b8 > 5 and that its marginal\n\ndistribution admits a density s with respe t to the Lebesgue measure \u03bc. Let s\u0303 be the PLSE\nde\u001cned by (9) with\npen(m) = KAK\u221e KBV\n\n\u221e\nX\nl=0\n\n\u03b2\u0303l\n\n!\n\nDm\n, where K \u2265 8.\nn\n\nThen there exist onstants c0 > 0, \u03b31 > 0 and a sequen e \u01ebn \u2192 0 su h that\nE ks\u0303 \u2212\n\nRemark :\n\nsk22\n\n\u0001\n\n\u2264 (1 + \u01ebn )\n\n\u0012\n\ninf\n\nm\u2208Mn , Dm \u2265c0 (log n)\u03b31\n\nks \u2212\n\nsm k22\n\n\u0013\n+ pen(m) .\n\n(18)\n\nAs in Theorem 3.1, the penalty term involves an unknown onstant and we have\na ondition on the dimension of the models in (18). However, the slope heuristi an also\nbe used in this ontext to alibrate the onstant and a areful look at the proof shows that\nwe an take the in\u001cmum over all models m \u2208 Mn provided that we in rease the onstant\nK in front of the penalty term. Our result allows to derive rates of onvergen e in Besov\nspa es for the PLSE that orrespond to the rates in the i.i.d. framework (see Proposition\n5.2).\nRemark : Theorem 4.1 gives an ora le inequality for the PLSE built on \u03c4 -mixing sequen es. This inequality is not pathwise and the onstants involved in the penalty term\nare not optimal. This is due to te hni al reasons, mainly be ause we use the oupling result\n(4) instead of (3). However, we re over the same kind of ora le inequality as in the i.i.d.\nframework (Birg\u00e9 and Massart [8\u2104) under weak assumptions on the mixing oe\u001e ients sin e\nwe only require arithmeti al [AR\u2104 \u03c4 -mixing assumptions on the pro ess (Xn )n\u2208Z . This is\nthe \u001crst result for these pro esses up to our knowledge.\nLet us mention here Theorem 4.1 in Comte & Merlev\u00e8de [13\u2104. They onsider \u03b1-mixing proesses (for a de\u001cnition of the oe\u001e ient \u03b1 and its properties, we refer to Rio [22\u2104). They\nmake geometri al [GEO\u2104 \u03b1-mixing assumptions on the pro esses and onsider penalties of\norder L log(n)Dm /n to get an ora le inequality. This leads to a logarithmi loss in the rates\nof onvergen e. They get the optimal rate under an extra assumption (namely Assumption\n[Lip] in Se tion 3.2). There exist random pro esses that are \u03c4 -mixing and not \u03b1-mixing\n(see Dede ker & Prieur [15\u2104), however, the omparison of these oe\u001e ients is di\u001e ult in\ngeneral and our method an not be applied in this ontext.\nThe onstants c0 , \u03b31 , no are given in the end of the proof.\n\n9\n\n\fRemark :\n\nInequality (2.6) an be improved under stronger assumptions on s. For exam\u221a\nple, when s is bounded, we have \u03b2\u0303k \u2264 C \u03c4k . Under this assumption and \u03b8 > 3, we an\nprove that the estimator s\u0303 satis\u001ces the inequality\n\u0013\n\u0012\n\u0001\n(log n)\u03ba(\u03b8+1)\n2\n2\nE ks\u0303 \u2212 sk2 \u2264 (1 + \u01ebn )\npen\n(m)\n+\ninf\nks\n\u2212\ns\nk\n+\n.\nm\n2\nm\u2208Mn , Dm \u2265c0 (log n)\u03b31\nn(\u03b8\u22123)/2\n\nWhen \u03b8 < 5, the extra term (log n)\u03ba(\u03b8+1) /n(\u03b8\u22123)/2 may be larger than the main term\ninf m\u2208Mn , Dm \u2265c0 (log n)\u03b31 ks \u2212 sm k22 + pen(m). In this ase, we don't know if our ontrol\nremains optimal. On the other hand, Proposition 5.2 ensures that s\u0303 is adaptive over the\nlass of Besov balls when \u03b8 \u2265 5.\n5\n5.1\n\nMinimax results\nApproximation results on Besov spa es\n\nBesov balls.\n\nThroughout this se tion, \u039b = {(j, k), j \u2208 N, k \u2208 Z} and {\u03c8j,k , (j, k) \u2208 \u039b} denotes an\nr -regular wavelet basis as introdu ed in Se tion 4.1. Let \u03b1, pPbe two positive numbers su h\nthat \u03b1 + 1/2 \u2212 1/p > 0. For all fun tions t \u2208 L2 (\u03bc), t = (j,k)\u2208\u039b tj,k \u03c8j,k , we say that t\nbelongs to the Besov ball B\u03b1,p,\u221e(M1 ) on the real line if ktk\u03b1,p,\u221e \u2264 M1 where\nj(\u03b1+1/2\u22121/p)\n\nktk\u03b1,p,\u221e = sup 2\nj\u2208N\n\nX\nk\u2208Z\n\np\n\n|tj,k |\n\n!1/p\n\n.\n\nIt is easy to he k that if p \u2265 2 B\u03b1,p,\u221e (M1 ) \u2282 B\u03b1,2,\u221e (M1 ) so that upper bounds on\nB\u03b1,2,\u221e (M1 ) yield upper bounds on B\u03b1,p,\u221e (M1 ).\n\nApproximation results on Besov spa es.\n\nWe have the following result (Birg\u00e9 & Massart [8\u2104 Se tion 4.7.1). Suppose that the support\nof s equals [0, 1] and that s belongs to the Besov ball B\u03b1,2,\u221e (1), then whenever r > \u03b1 \u2212 1,\n\nks \u2212 sm k22 \u2264\n5.2\n\nMinimax rates of\n\nksk2\u03b1,2,\u221e\n\n4(4\u03b1 \u2212 1)\n\n2\u22122Jm \u03b1 \u2264\n\n(2A)2\u03b1 ksk2\u03b1,2,\u221e\n4(4\u03b1 \u2212 1)\n\n\u22122\u03b1\nDm\n\n(19)\n\nonvergen e for the PLSE\n\nWe an derive from Theorems 3.1 and 4.1 adaptation results to unknown smoothness over\nBesov Balls.\n\nProposition 5.1 Assume that the pro ess (Xn )n\u2208Z is stri ly stationary and arithmeti ally\n[AR\u2104 \u03b2 -mixing with mixing rate \u03b8 > 2 and that its marginal distribution admits a density\n\ns with respe t to the Lebesgue measure \u03bc, that s is supported in [0, 1] and that s \u2208 L2 (\u03bc).\nFor all \u03b1, M1 > 0, the PLSE s\u0303 de\u001cned in Theorem 3.1 for the olle tion of models [W\u2104\n\nsatis\u001ces\n\n\u2200\u03ba > 2,\n\n\u0011 L (log n)(\u03b8+2)\u03ba\n\u0010\nM1\nP ks\u0303 \u2212 sk22 > LM1 ,\u03b1,\u03b8 n\u22122\u03b1/(2\u03b1+1) \u2264\n.\nn\u03b8/2\ns\u2208B\u03b1,2,\u221e (M1 )\nsup\n\nProposition 5.2 Assume that the pro ess (Xn )n\u2208Z is stri ly stationary and arithmeti ally\n[AR\u2104 \u03c4 -mixing with mixing rate \u03b8 > 5 and that its marginal distribution admits a density\n10\n\n\fs with respe t to the Lebesgue measure \u03bc, that s is supported in [0, 1] and that s \u2208 L2 (\u03bc).\nFor all \u03b1, M1 > 0, the PLSE s\u0303 de\u001cned in Theorem 4.1 satis\u001ces\n\u0010\n\u0011\nsup\nE ks\u0303 \u2212 sk22 \u2264 LM1 ,\u03b1,\u03b8 n\u22122\u03b1/(2\u03b1+1) .\ns\u2208B\u03b1,2,\u221e (M1 )\n\nRemark:\n\nProposition 5.2 an be ompared to Theorem 3.1 in Gannaz & Wintenberger\n[18\u2104. They prove near minimax results for the thresholded wavelet estimator introdu ed\nby Donoho et al. [16\u2104 in a \u03c6\u0303-dependent setting (for a de\u001cnition of the oe\u001e ient \u03c6\u0303, we\nrefer to Dede ker & Prieur [15\u2104). Basi ally, with our notations, their result an be stated\nb\nas follows: if (Xn )n\u2208Z is \u03c6\u0303-mixing with \u03c6\u03031 (r) \u2264 Ce\u2212ar for some onstants C, a, b, then the\nthresholded wavelet estimator \u015d of s satis\u001ces\n\u0012\n\u0013\n\u0011\n\u0010\nlog n 2\u03b1/(2\u03b1+1)\n2\n\u2200\u03b1 > 0, \u2200p > 1,\nsup\nE k\u015d \u2212 sk2 \u2264 LM,M1,\u03b1,p\n.\nn\ns\u2208B\u03b1,p,\u221e (M1 )\u2229L\u221e (M )\n\nThe main advantage of their result is that they an deal with Besov balls with regularity\n1 < p < 2. However, in the regular ase, when p \u2265 2, we have been able to remove the extra\nlog n fa tor. Moreover, our result only requires arithmeti al [AR\u2104 rates of onvergen e for\nthe mixing oe\u001e ients and we do not have to suppose that s is bounded.\n6\n6.1\n\nProofs.\nProofs of the minimax results.\n\nProof of Proposition 5.1:\n\nLet \u03b1 > 0 and M1 > 0 and assume that s \u2208 B\u03b1,2,\u221e (M1 ). Let M\u0303n = {m \u2208 Mn , Dm >\nc0 (log n)\u03b31 }. By Theorem 3.1, there exists a onstant L\u03b8 > 0 su h that\n\u0012\n\u001a\n\u001b\u0013\nDm\nLs (log n)(\u03b8+2)\u03ba\n2\n2\nP ks\u0303 \u2212 sk2 > L\u03b8 inf\n(20)\n.\nks \u2212 sm k2 +\n\u2264\nn\nn\u03b8/2\nm\u2208M\u0303n\nIt appears from the proof of Theorem 3.1 that the onstant Ls depends only on ksk2 and\nthat it is a nonde reasing fun tion of ksk2 so that Ls an be uniformly bounded over\nB\u03b1,2,\u221e (M1 ) by a onstant LM1 so that, by (20)\n\u0012\n\u001a\n\u001b\u0013\nDm\nLM1 (log n)(\u03b8+2)\u03ba\n2\n2\nP ks\u0303 \u2212 sk2 > L\u03b8 inf\nks \u2212 sm k2 +\n.\n\u2264\nn\nn\u03b8/2\nm\u2208M\u0303n\nIn parti ular, for a model m in Mn with dimension Dm su h that\n\nc0 (log n)\u03b31 \u2264 L1 n1/(2\u03b1+1) \u2264 Dm \u2264 L2 n1/(2\u03b1+1) ,\nwe have\n\n\u0012\n\nP ks\u0303 \u2212\n\nsk22\n\n\u0012\n\n> L\u03b8 ks \u2212\n\nsm k22\n\nDm\n+\nn\n\n\u0013\u0013\n\n\u2264\n\nLM1 (log n)(\u03b8+2)\u03ba\n.\nn\u03b8/2\n\nSin e s belongs to B\u03b1,2,\u221e (M1 ), we an use Inequality (19) to get\n\u22122\u03b1\nks \u2212 sm k22 \u2264 L\u03b1,M1 Dm\n.\n\nThus we obtain\n\n\u0010\n\u0011 L (log n)(\u03b8+2)\u03ba\nM1\nP ks\u0303 \u2212 sk22 > LM1 ,\u03b1,\u03b8 n\u22122\u03b1/(2\u03b1+1) \u2264\n.\u0003\nn\u03b8/2\n11\n\n\fProof of Proposition 5.2:\n\nLet \u03b1 > 0 and M1 > 0 and assume that s \u2208 B\u03b1,2,\u221e (M1 ). By Theorem 4.1, we have\n\u0012\n\u0013\n\u0011\n\u0010\nDm\n2\n2\n\u2264 L\u03b8\ninf {ks \u2212 sm k2 +\nE ks\u0303 \u2212 sk2\n} .\nn\nm\u2208M\u0303n\n\u22122\u03b1 , so that for a model m in M\u0303 with\nInequality (19) leads to ks \u2212 sm k22 \u2264 L\u03b1,M1 Dm\nn\ndimension Dm su h that\n\nwe \u001cnd\n\n6.2\n\nc0 (log n)\u03b31 \u2264 L1 n1/(2\u03b1+1) \u2264 Dm \u2264 L2 n1/(2\u03b1+1) ,\n\u0010\n\u0011\nE ks\u0303 \u2212 sk22 \u2264 L\u03b8,\u03b1,M1 n\u22122\u03b1/(2\u03b1+1) .\u0003\n\nProof of Theorem 3.1:\n\nFor all mo in Mn , we have, by de\u001cnition of m\u0302\n\n\u03b3n (s\u0303) + pen(m\u0302) \u2264 \u03b3n (\u015dmo ) + pen(mo )\n\nP \u03b3(s\u0303) + \u03bdn \u03b3(s\u0303) + pen(m\u0302) \u2264 P \u03b3(\u015dmo ) + \u03bdn \u03b3(\u015dmo ) + pen(mo )\n\nP \u03b3(s\u0303) \u2212 P \u03b3(s) \u2212 2\u03bdn s\u0303 + pen(m\u0302) \u2264 P \u03b3(\u015dmo ) \u2212 P \u03b3(s) \u2212 2\u03bdn \u015dmo + pen(mo )\n\nSin e for all t \u2208 L2 (\u03bc), P \u03b3(t) \u2212 P \u03b3(s) = kt \u2212 sk22 , we have\n\nks \u2212 s\u0303k22 \u2264 ks \u2212 \u015dmo k22 + pen(mo ) \u2212 V (mo ) \u2212 (pen(m\u0302) \u2212 V (m\u0302)) \u2212 2\u03bdn (smo \u2212 sm\u0302 ), (21)\n\nwhere, for all m \u2208 Mn\n\nV (m) = 2\u03bdn (\u015dm \u2212 sm ) = 2\n\nX\n\n\u03bdn2 (\u03c8j,k ).\n\n(j,k)\u2208m\n\nThis de omposition is di\u001berent from the one used in Birg\u00e9 & Massart [8\u2104 and in Comte &\nMerlev\u00e8de [13\u2104. It allows to improve the onstant in the ora le inequality in the \u03b2 -mixing\nase. Moreover, we hoose to prove an ora le inequality of the form (12) for \u03b2 -mixing\nsequen es, whi h allows to assume only \u03b8 > 2 instead of \u03b8 > 3. Let us now give a sket h\nof the proof:\n1. we build an event \u03a9C with P(\u03a9cC ) \u2264 p\u03b2q su h that, on \u03a9C , \u03bdn = \u03bdn\u2217 , where \u03bdn\u2217\nis built with independent data. A suitable hoi e of the integers p and q leads to\np\u03b2q \u2264 C(ln n)r n\u2212\u03b8/2 .\n2. We use the on entration's inequality (7.4) of Birg\u00e9 & Massart [8\u2104 for \u03c72 -type statisti s, derived from Talagrand's inequality. This allows us to \u001cnd p1 (m) su h that on\nan event \u03a91 with P(\u03a9c1 \u2229 \u03a9C ) \u2264 L1,s cn\n\nsup {V (m) \u2212 p1 (m)} \u2264 0.\n\nm\u2208Mn\n\ncn < C(ln n)r n\u2212\u03b8/2 and L1,s is some onstant depending on s.\n3. From Bernstein's inequality, we prove that, for all m, m\u2032 \u2208 Mn , there exists p2 (m, m\u2032 )\nsu h that, for all \u03b7 > 0, on an event \u03a92 with P(\u03a9c2 \u2229 \u03a9C ) \u2264 L2,s cn ,\n(\n)\nksm \u2212 sm\u2032 k22\n\u03b7\n\u2032\nsup\n\u03bdn (sm \u2212 sm\u2032 ) \u2212 p2 (m, m ) \u2212\n\u2264 0.\n2\n2\u03b7\nm,m\u2032 \u2208Mn\nMoreover, for all m, m\u2032 \u2208 Mn , p2 (m, m\u2032 ) \u2264 p2 (m, m) + p2 (m\u2032 , m\u2032 ).\n12\n\n\f4. We have ksm\u0302 \u2212 smo k22 \u2264 ksm\u0302 \u2212 sk22 + ks \u2212 smo k22 be ause sm\u0302 \u2212 smo is either the\nproje tion of sm\u0302 \u2212 s onto Smo or the proje tion of s \u2212 smo onto Sm\u0302 . Take pen(m) \u2265\np1 (m) + \u03b7p2 (m, m), we have, on \u03a91 \u2229 \u03a92 \u2229 \u03a9C\n\nVmo\nVmo\n(22)\n+ pen(mo ) \u2212\n2\n2\n\u2212(pen(m\u0302) \u2212 p1 (m\u0302)) \u2212 (p1 (m\u0302) \u2212 V (m\u0302)) \u2212 2\u03bdn (smo \u2212 sm\u0302 )\nV (mo )\n\u2264 ks \u2212 smo k22 + pen(mo ) \u2212\n\u2212 \u03b7p2 (m\u0302, m\u0302)\n2\nksmo \u2212 sm\u0302 k22\n+\u03b7p2 (m\u0302, mo ) +\n(23)\n\u03b7\n1\n\u2264 (1 + ) ks \u2212 smo k22 + pen(mo ) + \u03b7p2 (mo , mo ).\n(24)\n\u03b7\n\nks \u2212 s\u0303k22 \u2264 ks \u2212 \u015dmo k22 \u2212\n\n\u0012\n\n1\u2212\n\n1\n\u03b7\n\n\u0013\n\nks \u2212 s\u0303k22\n\nIn (23), we used that V (mo ) = 2ksmo \u2212 \u015dmo k22 \u2265 0. In (24), we used that Vmo \u2265 0.\nPythagoras Theorem gives\n\nks \u2212 \u015dmo k22 \u2212\n\nV (mo )\n= ks \u2212 smo k22 and; ks \u2212 sm\u0302 k22 \u2264 ks \u2212 s\u0303k22 .\n2\n\nFinally, we prove that we an hoose \u03b7 = (log n)\u03b3 , with \u03b3 > 0 su h that \u03b7p2 (mo , mo ) =\no(pen(mo )) and we on lude the proof of (3.1) from the previous inequalities.\nWe de ompose the proof in several laims orresponding to the previous steps.\nClaim 1 : For all l = 0, ..., p \u2212 1, let us de\u001cne Al = (X2lq+1 , ..., X(2l+1)q ) and Bl =\n\u2217\n\u2217\n, ..., X(2l+1)q\n) and Bl\u2217 =\n(X(2l+1)q+1 , ..., X(2l+2)q ). There exist random ve tors A\u2217l = (X2lq+1\n\u2217\n\u2217\n(X(2l+1)q+1 , ..., X(2l+2)q ) su h that for all l = 0, ..., p \u2212 1 :\n1. A\u2217l and Al have the same law,\n2. A\u2217l is independent of A0 , ..., Al\u22121 , A\u22170 ..., A\u2217l\u22121\n3. P(Al 6= A\u2217l ) \u2264 \u03b2q\nthe same being true for the variables Bl .\nProof of Claim 1 :\n\nThe proof is derived from Berbee's lemma, we refer to Proposition 5.1 in Viennet [25\u2104\nfor further details about this onstru tion.\u0003\n\u221a\n\u221a\nHereafter, we assume that, for some \u03ba > 2, n(log n)\u03ba /2 \u2264 p \u2264 n(log n)\u03ba and for the\nsake of simpli ity that pq = n/2, the modi\u001c ations needed to handle the extra term when\nq = [n/(2p)] being straightforward. Let \u03a9C = {\u2200l = 0, ..., p \u2212 1 Al = A\u2217l , Bl = Bl\u2217 }. We\nhave\n(log n)(\u03b8+2)\u03ba\n.\nP(\u03a9cC ) \u2264 2p\u03b2q \u2264 22+\u03b8\nn\u03b8/2\nLet us \u001crst deal with the quadrati term V (m).\nClaim 2 : Under the assumptions\nof Theorem 3.1, let \u01eb > 0, 1 < \u03b3 < \u03ba/2. We de\u001cne\n\u221a\nL21 = 2\u03a62 \u03ba1 , L22 = 8\u03a63/2 \u03ba2 , L3 = 2\u03a6\u03ba(\u01eb) and\n\nL1,m = 4 (1 + \u01eb)L1 + L2\n\ns\n\n13\n\n(log n)\u03b3\n1/4\n\nDm\n\nL3\n+\n(log n)\u03ba\u2212\u03b3\n\n!2\n\n.\n\n(25)\n\n\fThen, we have\n!\n\u001a\n\u0013\n\u001b\nL1,m Dm\n(log n)\u03b3\n.\nP sup V (m) \u2212\n\u2265 0 \u2229 \u03a9C \u2264 Ls,\u03b3 exp \u2212 p\nn\nksk2\nm\u2208Mn\n\u0012\n\n1/2\n\u03b3\nwhere Ls,\u03b3 = 2 \u221e\nD=1 exp(\u2212(log D) / ksk2 ). In parti ular, for all r > 0, there exists a\nonstant L\u2032s,r depending on ksk2 , su h that\n\nP\n\nP\n\n\u0012\n\nsup\n\nm\u2208Mn\n\nRemark :\n\n\u001a\n\nV (m) \u2212\n\nL1,m Dm\nn\n\n\u001b\n\n\u2265 0 \u2229 \u03a9C\n\n\u0013\n\n\u2264\n\nL\u2032s,r\n.\nnr\n\nWhen (L2 /L1 )8 (log n)4(2\u03ba\u2212\u03b3) \u2264 Dm \u2264 n, we have\n\n\"\n\nL1,m \u2264 1 + \u01eb +\nProof of Claim 2 :\nP\nLet Pn\u2217 (t) = ni=1\n\n#2\n!\n\u221a\n2\u03ba(\u01eb)\n(log n)\u2212(\u03ba\u2212\u03b3) 4L21 .\n1+ \u221a\n\u03ba1\n\nt(Xi\u2217 )/n and \u03bdn\u2217 (t) = (Pn\u2217 \u2212 P )t, we have\nX\nV (m)1\u03a9C = 2\n(\u03bdn\u2217 )2 (\u03c8j,k )1\u03a9C .\n(j,k)\u2208m\n\nLet B1 (Sm ) = {t \u2208 Sm ; ktk2 \u2264 1}. \u2200t \u2208 B1 (Sm ), let t\u0304(x1 , ..., xq ) =\nall fun tions g : Rq \u2192 R let\n\u2217\nPA,p\ng\n\nNow we have\n\np\u22121\n\np\u22121\n\nj=0\n\nj=0\n\n1X\n1X\n\u2217\n=\ng(A\u2217j ), PB,p\ng=\ng(Bj\u2217 ), P\u0304 g =\np\np\n\nZ\n\nPq\n\ni=1 t(xi )/2q\n\nand for\n\ngPA (d\u03bc),\n\n\u2217\n\u2217\nand \u03bd\u0304A,p g = (PA,p\n\u2212 P\u0304 )g, \u03bd\u0304B,p g = (PB,p\n\u2212 P\u0304 )g.\n\nX\n\n(\u03bdn\u2217 )2 (\u03c8j,k ) \u2264 2\n\n(j,k)\u2208m\n\nX\n\n2\n\u03bd\u0304A,p\n\u03c8\u0304j,k + 2\n\n(j,k)\u2208m\n\nX\n\n2\n\u03bd\u0304B,p\n\u03c8\u0304j,k .\n\n(j,k)\u2208m\n\nIn order to handle these terms, we use Proposition 7.4 whi h is stated in Se tion 7. Taking\n2\nBm\n=\n\nX\n\nVar(\u03c8\u0304j,k (A1 )), Vm2 =\n\n(j,k)\u2208m\n\nsup\nt\u2208B1 (Sm )\n\n2\nVar(t\u0304(A1 )), and Hm\n=\n\nX\n\n(\u03c8\u0304j,k )2\n\n(j,k)\u2208m\n\n,\n\u221e\n\nwe have\n\n\uf8f6\n\uf8eb\nr\ns X\nHm x \uf8f8\n(1 + \u01eb)\n2x\n2 \u03c8\u0304\n+ \u03ba(\u01eb)\n\u2264 e\u2212x .\n\u03bd\u0304A,p\n\u2200x > 0, P \uf8ed\n\u221a B m + Vm\nj,k \u2265\np\np\np\n\n(26)\n\n(j,k)\u2208m\n\nIn order to evaluate Bm , Vm and Hm , we use Viennet's inequality (54). There exists a\nfun tion b su h that, for all p = 1, 2, P |b|p \u2264 \u03bap where \u03bap is de\u001cned in (2) and for all\nfun tions t \u2208 L2 (P\u0304 ),\n1\nVar(t\u0304(A1 )) \u2264 P bt2 .\nq\n14\n\n\fThus\n2\nBm\n=\n\nX\n\n(j,k)\u2208m\n\nVar(\u03c8\u0304j,k (A1 )) \u2264\n\nFrom Assumption [M2 \u2104,\n\nP\n\n2\n(j,k)\u2208m \u03c8j,k\n\n1 X\n2\n\u2264\nP b\u03c8j,k\nq\n(j,k)\u2208m\n\n\u221e\n\n2\nBm\n\u2264\n\nX\n\n\u03ba1\n.\nq\n\n2\n\u03c8j,k\n\n(j,k)\u2208m\n\n\u221e\n\n\u2264 \u03a62 Dm , thus,\n\u03a62 \u03ba1 Dm\n.\nq\n\n(27)\n\nFrom Viennet's and Cau hy-S hwarz inequalities\n\nVm2 =\n\nsup\nt\u2208B1 (Sm )\n\nVar(t\u0304(A1 )) \u2264\n\n(P t2 )1/2 (P b2 )1/2\nP bt2\n\u2264 sup ktk\u221e\n.\nq\nt\u2208B1 (Sm )\nt\u2208B1 (Sm ) q\nsup\n\nSin e t \u2208 B1 (Sm ), we have by Cau hy-S hwarz inequality\n\n(P t2 )1/2 \u2264 (ktk\u221e ktk2 ksk2 )1/2 \u2264 (ktk\u221e ksk2 )1/2 .\n\u221a\nFrom Assumption [M2 \u2104, we have ktk\u221e \u2264 \u03a6 Dm , and from Viennet's inequality P b2 \u2264\n\u03ba2 < \u221e, thus we obtain\n3/4\n2\n3/2\n1/2 Dm\n.\nVm \u2264 \u03a6 (ksk2 \u03ba2 )\n(28)\nq\nFinally, from Assumption [M2 \u2104, we have, using Cau hy-S hwarz inequality\n2\nHm\n=\n\nX\n\n2\n\u03c8\u0304j,k\n\n(j,k)\u2208m\n\n\u2264\n\n1\n4\n\nX\n\n2\n\u03c8j,k\n\n(j,k)\u2208m\n\n\u221e\n\n\u2264\n\n\u03a62 Dm\n.\n4\n\n(29)\n\n\u221e\n\nLet yn > 0. We de\u001cne\n\nLm =\n\n(1 + \u01eb)L1 + L2\n\ns\n\n(log Dm )\u03b3 + yn\n1/4\n\n2Dm\n\n(log Dm )\u03b3 + yn\n+ L3\n2(log n)\u03ba\n\n!2\n\n.\n\n1/2\n\nWe apply Inequality (26) with x = ((log Dm )\u03b3 + yn )/ ksk2 and the evaluations (27), (28)\n\u221a\nand (29). Re alling that 1/p \u2264 2/( n(log n)\u03ba ), this leads to\n\uf8f6\n\uf8eb\n!\n\u03b3\nX\nyn\n(log\nD\n)\nL\nD\nm\nm m\uf8f8\n2\nexp(\u2212 p\n\u2264 exp \u2212 p\n).\nP\uf8ed\n\u03bd\u0304A,p\n\u03c8\u0304j,k \u2265\nn\nksk2\nksk2\n(j,k)\u2208m\n\nIn order to give an upper bound on Hm x, we used that the support of s in in luded in\n[0, 1], thus\n1 = ksk1 \u2264 ksk2 .\nThe result follows by taking yn = (log n)\u03b3 \u2265 (log Dm )\u03b3 .\u0003\n\nClaim 3. We keep the notations\nm, m\u2032\n\n\u2208 Mn we take\n\nLm,m\u2032 = 4 L2\n\n\u03ba/2 > \u03b3 > 1, L2 of the proof of Claim 2. For all\n\ns\n\n4\u03a6\n(log n)\u03b3\n+\n1/4\n3(log n)\u03ba\u2212\u03b3\n(Dm \u2228 Dm\u2032 )\n15\n\n!2\n\n,\n\n(30)\n\n\fwe have, for all \u03b7 > 0,\nsup\n\nP\n\nm,m\u2032 \u2208Mn\n\nwith Ls,\u03b3 = 2\n\nRemark :\n\n\u03bdn\u2217 (sm\n\n!\nn)\u03b3\n\u2212 (log 1/2\nksm \u2212 sm\u2032 k22 \u03b7 Lm,m\u2032 (Dm \u2228 Dm\u2032 )\nksk\n2\n\u2212 sm\u2032 ) \u2212\n\u2212\n> 0 \u2264 Ls,\u03b3 e\n2\u03b7\n2\nn\n\u2212\n\nP\n\n(log(Dm \u2228D \u2032 ))\u03b3\nm\n1/2\nksk\n2\n\n.\nm,m\u2032 \u2208Mn e\nThe onstant Ls,\u03b3 is \u001cnite sin e for all x, y > 0, (log(x \u2228 y))\u03b3 \u2265 ((log x)\u03b3 +\n\n(log y)\u03b3 )/2.\nAs in Claim 2, when (L2 /L1 )8 (log n)4(2\u03ba\u2212\u03b3) \u2264 Dm \u2264 n, we have\n!2\n23/2\nLm,m\u2032 \u2264 1 + \u221a\n(log n)\u22122(\u03ba\u22122\u03b3) 4L21 .\n3 \u03ba1\n\nProof of Claim 3.\n\nWe keep the notations of the proof of Claim 2 and for m, m\u2032 \u2208 Mn , let tm,m\u2032 =\n(sm \u2212 sm\u2032 )/ ksm \u2212 sm\u2032 k2 . We use the inequality 2ab \u2264 a2 \u03b7 \u22121 + b2 \u03b7 , whi h holds for all\na, b \u2208 R, \u03b7 > 0. This leads to\n\n\u03bdn\u2217 (sm\n\n\u00012\nksm \u2212 sm\u2032 k22 \u03b7 \u2217\n\u2264\n\u2212 s ) = ksm \u2212 s\n+\n\u03bdn (tm,m\u2032 )\n2\u03b7\n2\n2\n\u00012\nksm \u2212 sm\u2032 k2 \u03b7\n=\n+\n\u03bd\u0304A,p (t\u0304m,m\u2032 ) + \u03bd\u0304B,p (t\u0304m,m\u2032 )\n2\u03b7\n2\n2\nksm \u2212 sm\u2032 k2\n+ \u03b7(\u03bd\u0304A,p (t\u0304m,m\u2032 ))2 + \u03b7(\u03bd\u0304B,p (t\u0304m,m\u2032 ))2 .\n\u2264\n2\u03b7\nm\u2032\n\nm\u2032\n\nk2 \u03bdn\u2217 (tm,m\u2032 )\n\nNow from Bernstein's inequality (see Se tion 7), we have\n\uf8eb\n\uf8f6\ns\n\u2032\n\u2032\n(A\n))x\n2\nVar\n(\nt\u0304\nk\nx\nk\nt\u0304\n1\nm,m\nm,m \u221e \uf8f8\n\u2200x > 0, P \uf8ed\u03bd\u0304A,p (t\u0304m,m\u2032 ) >\n+\n\u2264 e\u2212x .\np\n3p\n\n(31)\n\nFrom Viennet's and Cau hy-S hwarz inequalities, we have\nq\n2\n2\n2\n\u2032\nk\nkt\nm,m \u221e P b P tm,m\u2032\nP btm,m\u2032\n\u2264\n.\nVar(t\u0304m,m\u2032 (A1 )) \u2264\nq\nq\n\nMoreover\n\nP b2 \u2264 \u03ba2 , P t2m,m\u2032 \u2264 ktm,m\u2032 k\u221e ktm,m\u2032 k2 ksk2 .\n\n\u2032\n\u2032\n\u2032\n\u2032\nSin\n\u221a e tm,m \u2208 Sm \u222a Sm and ktm,m k2 = 1, we have, from Assumption [M2 \u2104 ktm,m k\u221e\u03b3 \u2264\n\u03a6 Dm \u2228 Dm\u2032 . Let yn > 0. We apply Inequality (31) with x = [(log(Dm \u2228 Dm\u2032 )) +\n1/2\nyn ]/ ksk2 . We de\u001cne\n\nL\u2032m,m\u2032\n4\nwe have\n\uf8eb\n\n=\n\nL2\n\nP \uf8ed\u03bd\u0304A,p (t\u0304m,m\u2032 ) >\n\ns\n\ns\n\n(log(Dm \u2228 Dm\u2032 ))\u03b3 + yn 4\u03a6 [(log(Dm \u2228 Dm\u2032 ))\u03b3 + yn ]\n+\n6(log n)\u03ba\n2(Dm \u2228 Dm\u2032 )1/4\n\nL\u2032m,m\u2032 (Dm\n4n\n\n\u2228D )\nm\u2032\n\n\uf8f6\n\n\uf8f8 \u2264 exp \u2212\n16\n\n(log(Dm \u2228 Dm\u2032 ))\u03b3\n1/2\nksk2\n\n!\n\n!2\n\n,\n\n1/2\n\ne\u2212yn /ksk2 .\n\n\fThe result follows by taking yn = (log n)\u03b3 and using 2 \u2264 Dm \u2264 n.\n\nCon lusion of the proof:\n\nLet \u03b7 > 0 and pen\u2032 (m) \u2265 (L1,m + \u03b7Lm,m )Dm /n where L1,m and Lm,m are de\u001cned respe tively by (25) and (30). From Claims 1, 2 and 3 and (24), we obtain that, for all mo and\nwith probability larger than Ls,\u03b8 (log n)(\u03b8+2)\u03ba n\u2212\u03b8/2\n\nDmo\n1\n1\n.\n(1 \u2212 ) ks \u2212 s\u0303k22 \u2264 (1 + ) ks \u2212 smo k22 + pen\u2032 (mo ) + \u03b7L(mo , mo )\n\u03b7\n\u03b7\nn\n\n(32)\n\nAssume that Dm \u2265 (L2 /L1 )8 (log n)4(2\u03ba\u2212\u03b3) , then we have from remarks 6.2 and 6.2\n\u0014\n\u0012\n\u0013\n\u00152\n2\u03ba(\u01eb)\nL1,m \u2264 1 + \u01eb + 1 + \u221a\n(log n)\u2212(\u03ba\u22122\u03b3) 4L21 and\n\u03ba1\n!2\n23/2\nLm,m \u2264\n1+ \u221a\n(log n)\u22122(\u03ba\u2212\u03b3) 4L21 .\n3 \u03ba1\nTake \u03b7 = (log n)\u03ba\u2212\u03b3 , we have (L1,mo + \u03b7Lmo ,mo )Dmo /n \u2264 C pen(mo ). Fix \u01eb > 0 su h that\n[1 + \u01eb]2 < K/4. Sin e \u03ba > \u03b3 , for n \u2265 no , we have L1,m + \u03b7Lm,m \u2264 KL21 , thus, inequality\n(13) follows follows from (32) as soon as n > no . We remove the ondition n > no by\nimproving the onstant Ls in (13) if ne essary.\u0003\n6.3\n\nProof of Theorem 4.1.\n\nThe proof follows the previous one, the main di\u001beren e is that the oupling lemma (Claim\n1) as well as the ovarian e inequalities are mu h harder to handle in the \u03c4 -mixing ase.\nThis leads to more te hni al omputations to re over the results obtained in the \u03b2 -mixing\nase (see Claims 2, 3 and the proof of inequality (45)). We start with the de omposition\n(21). As in the previous proof, the de omposition of the risk given in Birg\u00e9 & Massart [8\u2104\nor in Comte & Merlev\u00e8de [13\u2104 ould be used. This leads to a loss in the onstant in front\nof the main term in (18) without avoiding any of the main di\u001e ulties. We divide the proof\nin four laims.\nClaim 1 : For all l = 0, ..., p \u2212 1, let us denote by Al = (X2lq+1 , ..., X(2l+1)q ) and Bl =\n\u2217\n\u2217\n, ..., X(2l+1)q\n) and Bl\u2217 =\n(X(2l+1)q+1 , ..., X(2l+2)q ). There exist random ve tors A\u2217l = (X2lq+1\n\u2217\n\u2217\n(X(2l+1)q+1\n, ..., X(2l+2)q\n) su h that for all l = 0, ..., p \u2212 1 :\n\n\u2022 A\u2217l and Al have the same law,\n\n\u2022 A\u2217l is independent of A0 , ..., Al\u22121 , A\u22170 ..., A\u2217l\u22121\n\u2022 E(|Al \u2212 A\u2217l |q ) \u2264 q\u03c4q\n\nthe same being true for the variables Bl .\nProof of Claim 1 :\n\nWe use the same re ursive onstru tion as Viennet [25\u2104.\nLet (\u03b4j )0\u2264j\u2264p\u22121 be a sequen e of independent random variables uniformly distributed over\n[0, 1] and independent of the sequen e (Aj )0\u2264j\u2264p\u22121 . Let A\u22170 = (X1\u2217 , ..., Xq\u2217 ) be the random\nvariable given by equality (4) for M = \u03c3(Xi , i \u2264 \u2212q), A0 and \u03b40 .\nNow suppose that we have built the variables A\u2217l for l < l\u2032 . From equality (4) applied to\nthe \u03c3 -algebra \u03c3(Al , A\u2217l , l < l\u2032 ), Al\u2032 and \u03b4l\u2032 , there exists a random variable A\u2217l\u2032 satisfying\nthe hypotheses of Claim 1.\nWe build in the same way the variables Bl\u2217 for all l = 0, ..., p \u2212 1. \u0003\n17\n\n\fWe keep the notations \u03bdn\u2217 , \u03bd\u0304A,p , \u03bd\u0304B,p , t\u0304 and B1 (Sm ) that we introdu ed in the proof of The\u221a\norem 3.1. As in the proof of Theorem 3.1, we assume that, for some \u03ba > 2, n(log n)\u03ba /2 \u2264\n\u221a\np \u2264 n(log n)\u03ba and for the sake of simpli ity that pq = n/2, the modi\u001c ations needed to\nhandle the extra term when q = [n/(2p)] being straightforward. We have\nX\nX\nX\nV (m\u0302) =\n\u03bdn2 (\u03c8j,k ) \u2264 2\n(Pn \u2212 Pn\u2217 )2 (\u03c8j,k ) + 2\n(\u03bdn\u2217 )2 (\u03c8j,k ) (33)\n(j,k)\u2208m\u0302\n\n(j,k)\u2208m\u0302\n\n(j,k)\u2208m\u0302\n\nClaim 2 : There exists a onstant L = LA,KL,K\u221e,\u03ba,\u03b8 su h that\n\uf8eb\n\nE\uf8ed\n\nX\n\nj,k\u2208m\u0302\n\n\uf8f6\n\n((Pn \u2212 Pn\u2217 )(\u03c8j,k ))2 \uf8f8 \u2264 L\n\n(log n)\u03ba(\u03b8+1)\n.\nn(\u03b8\u22123)/2\n\n(34)\n\nProof of Claim 2 :\n\n\uf8eb\n\nE\uf8ed\n\nX\n\n\uf8eb\n\n\uf8f6\n\n(Pn \u2212 Pn\u2217 )2 (\u03c8j,k )\uf8f8 \u2264 E \uf8ed sup\n\n(j,k)\u2208m\u0302\n\nm\u2208Mn\n\n\u2264\n\n\u2264\n\nX\n\nX\n\n\uf8f6\n\n(Pn \u2212 Pn\u2217 )2 (\u03c8j,k )\uf8f8\n\n(j,k)\u2208m\n\nX\n\nm\u2208Mn (j,k)\u2208m\n\n\u0001\nE (Pn \u2212 Pn\u2217 )2 (\u03c8j,k )\n\np\n2 X X\n(gA,m (j, k, l, l\u2032 ) + gB,m (j, k, l, l\u2032 ))\np2\n\u2032\nm\u2208Mn l,l =1\n\nwith\n\n\uf8eb\n\ngm,A (j, k, l, l\u2032 ) = E \uf8ed\n\nX\n\n(j,k)\u2208m\n\n\uf8f6\n\u0001\n\u0001\n\u03c8\u0304j,k (Al ) \u2212 \u03c8\u0304j,k (A\u2217l ) \u03c8\u0304j,k (Al\u2032 ) \u2212 \u03c8\u0304j,k (A\u2217l\u2032 ) \uf8f8 .\n\nWe develop this last term and we get, sin e\n\n\u03c8\u0304j,k (x) \u2212 \u03c8\u0304j,k (y) \u2264\n\uf8eb\n\ngA,m (j, k, l, l\u2032 ) \u2264 E \uf8ed\n\nX\n\n\u2264 E\uf8ed\n\nX\n\n(j,k)\u2208m\n\n\uf8eb\n\n(j,k)\u2208m\n\n\u2264\n\u2264\n\u2264\n\nKL 23j/2 |x \u2212 y|q\n2q\n\n\uf8f6\n\n\u03c8\u0304j,k (Al ) \u2212 \u03c8\u0304j,k (A\u2217l ) \u03c8\u0304j,k (Al\u2032 ) \u2212 \u03c8\u0304j,k (A\u2217l\u2032 ) \uf8f8\n\u03c8\u0304j,k (Al ) \u2212 \u03c8\u0304j,k (A\u2217l ) KL 23j/2\n\nAl\u2032 \u2212 A\u2217l\u2032\n2q\n\uf8fc\n\uf8fd\n\n\uf8f1\n\uf8f2 X\nKL \u03c4q\nsup\n23j/2 \u03c8\u0304j,k (x) \u2212 \u03c8\u0304j,k (y)\n\uf8fe\n2 x,y\u2208Rq \uf8f3\n(j,k)\u2208m\n(\n)\nJm\nX\nKL \u03c4q X\n3j/2\n2\nsup\n|\u03c8j,k (x) \u2212 \u03c8j,k (y)|\n4\nx,y\u2208R\nj=0\n\nk\u2208Z\n\n2\nAKL K\u221e 22Jm \u03c4q sin e\n3\n18\n\nX\nk\u2208Z\n\n|\u03c8j,k |\n\n\u221e\n\n\u2264 AK\u221e 2j/2\n\n\uf8f6\n\nq\uf8f8\n\n\fWe an do the same omputations for the term gB,m (j, k, l, l\u2032 ) and we obtain\n\uf8eb\n\uf8f6\nX\nX\n(log n)\u03ba(\u03b8+1)\nE\uf8ed\n.\n((Pn \u2212 Pn\u2217 )(\u03c8j,k ))2 \uf8f8 \u2264 L\u03c4q\n22Jm \u2264 L\u03c4q 22Jn \u2264 L\nn(\u03b8\u22123)/2\nm\u2208M\nj,k\u2208m\u0302\nn\n\nThe last inequality omes from q \u2265\nomes from Assumption [W\u2104. \u0003\n\n\u221a\n\nn/(2(log n)\u03ba ) and Assumption\n\nClaim 3. Let us keep the notations of Theorem 4.1, let\nthat \u03ba > 2. Let \u03b3 be a real number in (1, \u03ba/2). Let\nL21 = AK\u221e KBV\n\n\u221e\nX\n\nu\n\u03b2\u0303l , L22 = 2\u03a6KBV\n\nl=0\n\n\u221e\nX\n\n[AR\u2104, the one before\n\nu = 6/(7 + \u03b8) < 1/2 and re all\n\n\u03b2\u0303ku , L3 = \u03ba(\u01eb)\u03a6\n\nk=0\n\nand L1,m = 4(1 + \u01eb) (1 + \u01eb)L1 + L2\n\ns\n\n(log Dm )\u03b3\n1/2\u2212u\n\nDm\n\n(log Dm )\u03b3\n+ L3\n(log n)\u03ba\n\n!2\n\n,\n\n(35)\n\nThere exists a onstant Ls su h that\n\uf8eb\n\nE \uf8ed sup\n\n\uf8f1\n\uf8f2 X\n\nm\u2208Mn \uf8f3\n\nRemark :\n\n\uf8fc\uf8f6\n\uf8fd\nL\nD\nLs\n1,m m \uf8f8\n.\n\u2264\n(\u03bdn\u2217 )2 (\u03c8j,k ) \u2212\n\uf8fe\nn\nn\n\n(j,k)\u2208m\n\nP\nP\u221e u\nThe series \u221e\nl=0 \u03b2\u0303l and\nk=0 \u03b2\u0303k are onvergent under our hypotheses on the\n2/3 1/3\n2\noe\u001e ients \u03c4 . Sin e s \u2208 L ([0, 1]), we have from Inequality (6), \u03b2\u0303l \u2264 2ksk2 \u03c4l and thus\nP\n2/3\nu\n\u03b2\u0303l \u2264 2ksk2 (1 + l)\u2212(1+\u03b8)/3 . The series \u221e\nk=0 \u03b2\u0303k onverge sin e \u03b8 > 5 and\n2(1 + \u03b8)\n\u03b8\u22125\nu(1 + \u03b8)\n=\n=1+\n> 1.\n3\n7+\u03b8\n\u03b8+7\n\nWe use here \u03b2\u0303 instead of \u03c4 whi h allows to take L1 not depending on ksk2 .\nProof of Claim 3 :\n\nAs in the previous se tion we use the following de omposition\nX\nX\n\u00012\n\u03bd\u0304A,p (\u03c8\u0304j,k ) + \u03bd\u0304B,p (\u03c8\u0304j,k )\n(\u03bdn\u2217 )2 (\u03c8j,k ) =\n(j,k)\u2208m\n\n(j,k)\u2208m\n\n\u2264 2\n\nX\n\n(j,k)\u2208m\n\nX\n\u00012\n\u03bd\u0304A,p (\u03c8\u0304j,k ) + 2\n\n(j,k)\u2208m\n\n\u00012\n\u03bd\u0304B,p (\u03c8\u0304j,k )\n\nWe treat both terms with Proposition 7.4 applied\nto the random variables (A\u2217l )0=1,..,p\u22121\n\b\nand (Bl\u2217 )l=0,..,p\u22121 and to the lass of fun tions (\u03c8\u0304j,k )(j,k)\u2208m . Let\n2\nBm\n=\n\nX\n\n(j,k)\u2208m\n\n\u0001\nVar \u03c8\u0304j,k (A1 ) , Vm2 =\n\nsup\n\nt\u2208B1 (Sm )\n\n2\nVar(t\u0304(A1 )), Hm\n=k\n\nX\n\n(j,k)\u2208m\n\n2\nk\u221e .\n\u03c8\u0304j,k\n\nWe have, from Proposition 7.4\n\uf8ee\n\uf8f9\nr\ns X\n2x\nHm x \uf8fb\n(1 + \u01eb)\n+ \u03ba(\u01eb)\n\u2264 e\u2212x .\n\u2200x > 0, P \uf8f0\n(\u03bd\u0304A,p )2 \u03c8\u0304j,k \u2265 \u221a Bm + Vm\np\np\np\n(j,k)\u2208m\n\n19\n\n(36)\n\n\fLet us now evaluate Bm , Vm and Hm , we have\n\n!\nq\nX\nX\n1\n\u03c8j,k (Xi ) .\n=\nVar\n(2q)2\n\n2\nBm\n\ni=1\n\n(j,k)\u2208m\n\nP\nFrom (17) and (15) we have \u2200j, k k\u03c8j,k kBV \u2264 KBV 2j/2 and \u2200j k k\u2208Z |\u03c8j,k |k\u221e \u2264 AK\u221e 2j/2 .\nThus, from Inequality (5)\n!\nq\nq\nX\nX X\nX\n(q + 1 \u2212 l)|Cov(\u03c8j,k (X1 ), \u03c8j,k (Xl ))|\n\u03c8j,k (Xi )\n\u2264 2\nVar\ni=1\n\n(j,k)\u2208m\n\n(j,k)\u2208m l=1\n\n\u2264 2q\n\nq\nJm X X\nX\nj=0 k\u2208Z l=1\n\n\u2264 2KBV q\n\u2264 2q\n\nJm\nX\n\nk\u03c8j,k kBV E (|\u03c8j,k (X1 )|b(\u03c3(X1 ), Xl ))\nX\n\nj/2\n\n2\n\nj=0\n\nAK\u221e KBV\n\nk\u2208Z\n\u221e\nX\n\n|\u03c8j,k (X0 )|\n\n\u03b2\u0303l\n\nl=0\n\nThe last inequality omes\nP from Assumption\nSin e L21 = AK\u221e KBV \u221e\nl=0 \u03b2\u0303l we have\n\n[W\u2104.\n\n2\nBm\n\u2264\n\n!\n\nq\nX\n\n\u03b2\u0303l\u22121\n\n\u221e l=1\n\nDm .\n\nL21 Dm\n.\n2q\n\n(37)\n\nLet us deal with the term Vm2 . We have\n\nVm2 \u2264\n\nsup\nt\u2208B1 (Sm )\n\nVar(t\u0304(A1 )) \u2264\n\nq\n2 X\n(q + 1 \u2212 k) sup |Cov(t(X1 ), t(Xk ))|\n(2q)2\nt\u2208B1 (Sm )\n\n(38)\n\nk=1\n\nFrom Inequality (5), we have\n\n|Cov(t(X1 ), t(Xk ))| \u2264 ktkBV ktk\u221e \u03b2\u0303k\u22121 .\nP\nP\nSin e t belongs to B1 (Sm ), we have t = (j,k)\u2208m aj,k \u03c8j,k , with (j,k)\u2208m a2j,k \u2264 1. Thus,\nby Cau hy-S hwarz inequality\nl\nX\ni=1\n\n|t(xi+1 ) \u2212 t(xi )| \u2264\n\nX\n\n(j,k)\u2208m\n\n\uf8eb\n\n\u2264 \uf8ed\n\n\uf8eb\n\n\u2264 \uf8ed\n\n|aj,k |\n\nl\nX\ni=1\n\n|\u03c8j,k (xi+1 ) \u2212 \u03c8j,k (xi )|\n\n\uf8f61/2 \uf8eb\n\nX\n\na2j,k \uf8f8\n\nX\n\nk\u03c8j,k k2BV \uf8f8\n\n(j,k)\u2208m\n\n(j,k)\u2208m\n\n\uf8ed\n\nX\n\n(j,k)\u2208m\n\n\uf8f61/2\n\nX\ni\n\n!2 \uf8f61/2\n|\u03c8j,k (xi+1 ) \u2212 \u03c8j,k (xi )| \uf8f8\n\n\u2264 KBV Dm .\n\n\u221a\nThus ktkBV \u2264 Dm KBV . From Assumption [M2 \u2104, we have ktk\u221e \u2264 \u03a6 Dm . Thus\n3/2\n|Cov(t(X1 ), t(Xk ))| \u2264 \u03a6KBV \u03b2\u0303k\u22121 Dm\n.\n\n20\n\n(39)\n\n\fMoreover, we have by Cau hy-S hwarz inequality and [M2 \u2104\n\n|Cov(t(X1 ), t(Xk ))| \u2264 ktk\u221e ktk2 ksk2 \u2264 \u03a6 ksk2\nWe use the inequality a \u2227 b \u2264 au b1\u2212u with\n3/2\na = \u03a6KBV \u03b2\u0303k\u22121 Dm\n, b = \u03a6 ksk2\n\nFrom (39) and (40), we derive that\n\np\n\nDm , u =\n\np\n\nDm .\n\n(40)\n\n6\n1\n< .\n7+\u03b8\n2\n\n\u0010\n\u0011u\n1/2+u\n|Cov(t(X1 ), t(Xk ))| \u2264 L\u2032k Dm\nwhere L\u2032k = \u03a6 KBV \u03b2\u0303k\u22121 ksk1\u2212u\n.\n2\n\nPluging this inequality in (38), we obtain\n\n1/2+u\n\nVm2 \u2264\n\nL22 ksk1\u2212u\nDm\n2\n4q\n\nu\nsin e L22 = 2\u03a6KBV\n\n1\n4\n\n\u03b2\u0303ku .\n\n(41)\n\nk=0\n\nFinally, we have from hypothesis [M2 \u2104\n2\nHm\n\u2264\n\n\u221e\nX\n\nX\n\n2\n\u03c8j,k\n\n(j,k)\u2208m\n\n\u2264\n\n\u03a62 Dm\n.\n4\n\n(42)\n\n\u221e\n1/2+u\n\n) + (y/Dm\n).\nLet y > 0 and let us apply Inequality (36) with x = ((log Dm )\u03b3 / ksk1\u2212u\n2\nWe have, from (37), (41) and (42)\n\uf8eb\n\uf8ee\ns\n!\n\u221a\n\u03b3\n2D\nX\ny\n(log\nD\n)\nL\nL\nD\nm\n3\nm\n1 m\n(\u03bd\u0304A,p )2 (\u03c8\u0304j,k ) > \uf8ed(1 + \u01eb)\nP\uf8f0\n+\n1\u2212u +\n1/2+u\n2pq\n2p\nksk\nDm\n2\n(j,k)\u2208m\n\uf8f9\nv\n!\uf8f62\nu\n(log Dm )\u03b3\n1/2+u\nu L2 ksk1\u2212u Dm\n\u2212\n\u2212(1/2+u)\n(log Dm )\u03b3\ny\n1\u2212u\n\uf8fa\ny\n2\n2\nt\n\uf8f8\n+\ne\u2212Dm\n.\n+ 1/2+u\n\uf8fb \u2264 e ksk2\n1\u2212u\n2pq\nksk2\nDm\n\nThen, we use the inequality\n\n\u221a\n\n\u03b1+\u03b2 \u2264\n\n\u03b1=\n\n\u221a\n\n\u03b1+\n\n\u221a\n\n\u03b2 with\n\n(log Dm )\u03b3\ny\nand \u03b2 = 1/2+u\n1\u2212u\nksk2\nDm\n\nand the inequality (a + b)2 \u2264 (1 + \u01eb)a2 + (1 + \u01eb\u22121 )b2 with\n!r\ns\nL3 (log Dm )\u03b3\n(log Dm )\u03b3\nDm\na = (1 + \u01eb)L1 + L2\n+\n1\u2212u\n1/2\u2212u\n\u03ba\nn\nksk2 (log n)\nDm\n\u0013\n\u0012 q\n1\nL3 y\n1\u2212u\nand b = \u221a\n.\nL2 ksk2 y +\nu\nn\n(log n)\u03ba Dm\n\nSetting Lm = (1 + \u01eb)a2 n/Dm , we obtain\n\uf8eb\n\uf8f6\n\u00132\n\u0012 q\n\u22121\nX\nLm Dm\nL3 y\n(1 + \u01eb )\n\uf8f8\n(\u03bd\u0304A,p )2 (\u03c8\u0304j,k ) \u2212\nP\uf8ed\ny+\n>\nL2 ksk1\u2212u\n2\nu\nn\nn\n(log n)\u03ba Dm\n(j,k)\u2208m\n\nm)\n\u2212 (log D1\u2212u\n\n\u2264e\n\nksk\n\n2\n\n\u03b3\n\n\u2212(1/2+u)\n\ne\u2212Dm\n\ny\n\n.\n21\n\n\fThus, for all y > 0,\n\uf8f1\n\uf8fc\n\uf8eb\n\uf8f6\n\u03b3\n\u2212(1/2+u)\n\uf8f2 X\n\uf8fd\nm)\nX \u2212 (log D1\u2212u\n\u2212Dm\ny\nLm Dm\nLs\n2\n2 \uf8f8\nksk\n\uf8ed\n2\n(\u03bd\u0304A,p ) (\u03c8\u0304j,k ) \u2212\nsup\nP\ne\n>\n(y + y ) \u2264\nn \uf8fe\nn\nm\u2208Mn \uf8f3\nm\u2208Mn\n\n(j,k)\u2208m\n\nwhere Ls = 2(1 +\n\n\u01eb\u22121 )\n\n\u0014\n\n(L2\n\nequality to prove Claim 3.\u0003\n\nq\n\nksk1\u2212u\n)\n2\n\n\u2228 L3\n\n\u00152\n\n/((log 2)\u03ba 2u )\n\n. We an integrate this last in-\n\nClaim 4 :We keep the notations of the previous Claims. Let\nL2 (m, m\u2032 ) = 4 L2\n\ns\n\n(log(Dm \u2228 Dm\u2032 ))\u03b3\n\u03a6\n+\n1/2\u2212u\n3(log n)\u03ba\u2212\u03b3\n(Dm \u2228 Dm\u2032 )\n\n!2\n\n(43)\n\n.\n\nThen there exists a onstant Ls,\u03b8 depending on ksk2 and \u03b8 su h that, for all \u03b7 > 0\nE\n\nsup\n\nm,m\u2032 \u2208Mn\n\n(\n\nksm \u2212 sm\u2032 k22\nL2 (m, m\u2032 )(Dm \u2228 Dm\u2032 )\n\u03bdn (sm \u2212 sm\u2032 ) \u2212\n\u2212\u03b7\n2\u03b7\nn\n\n)!\n\n\u2264\n\n\u03b7Ls,\u03b8\n.\nn\n\nProof of Claim 4 :\n\nE\n\nsup\n\nm,m\u2032 \u2208Mn\n\n\u2264E\n+E\n\n(\n\nksm \u2212 sm\u2032 k22\nL2 (m, m\u2032 )(Dm \u2228 Dm\u2032 )\n\u2032\n\u2212\u03b7\n\u03bdn (sm \u2212 sm ) \u2212\n2\u03b7\nn\n!\n\n)!\n\nsup (Pn \u2212 Pn\u2217 )(sm \u2212 sm\u2032 )\n\nm,m\u2032\n\nsup\n\nm,m\u2032\n\n(\n\nksm \u2212 sm\u2032 k22\nL2 (m, m\u2032 )(Dm \u2228 Dm\u2032 )\n\u03bdn\u2217 (sm \u2212 sm\u2032 ) \u2212\n\u2212\u03b7\n2\u03b7\nn\n\n)!\n\n.\n\n(44)\n\nSin e \u2200l = 0, ..., p \u2212 1, E (|Al \u2212 A\u2217l |q ) \u2264 q\u03c4q , we have\n!\nX\n\u2264 2\nE sup (Pn \u2212 Pn\u2217 )(sm \u2212 sm\u2032 )\nE (|(s\u0304m \u2212 s\u0304m\u2032 )(A1 ) \u2212 (s\u0304m \u2212 s\u0304m\u2032 )(A\u22171 )|)\nm,m\u2032\n\nm,m\u2032\n\n\u2264 \u03c4q\n\nX\n\nm,m\u2032\n\nLip(sm \u2212 sm\u2032 ).\n\nWhen m \u2282 m\u2032 , we have, for all x, y \u2208 R, using Assumption\n\n|(sm \u2212 sm\u2032 )(x \u2212 y)|\n\u2264\n|x \u2212 y|\n\nJm\u2032\nX\n\n2jX\n\u2212A1\n\nj=Jm +1 k=\u2212A2\n\n|P \u03c8j,k |\n\n[W\u2104,\n\n|\u03c8j,k (x) \u2212 \u03c8j,k (y)|\n|x \u2212 y|\n\nLet us \u001cx j \u2208 [Jm + 1, Jm\u2032 ], from Assumption [W\u2104, there is less than A indexes k \u2208 Z\nsu h that \u03c8j,k (x) 6= 0, thus there is less than 2A indexes su h that |\u03c8j,k (x) \u2212 \u03c8j,k (y)| =\n6 0.\nHen e\nX\n|\u03c8j,k (x) \u2212 \u03c8j,k (y)|\n\u2264 2A sup |P \u03c8j,k |Lip(\u03c8j,k )\n|P \u03c8j,k |\n|x \u2212 y|\nk\u2208Z\nk\u2208Z\n\n\u2264 2A ksk2 KL 23j/2 .\n\n22\n\n\f\u221a\n\u221a\nThus, Lip(sm \u2212 sm\u2032 ) \u2264 A ksk2 KL 823Jm\u2032 /2 /( 8 \u2212 1) and by Assumptions [W\u2104, [AR\u2104 and\nthe value of q ,\n!\n(log n)\u03ba(\u03b8+1)+1\nE sup (Pn \u2212 Pn\u2217 )(sm \u2212 sm\u2032 ) \u2264 Ls n3/2 (log n)\u03c4q \u2264 Ls\n(45)\n.\nn(\u03b8\u22122)/2\nm,m\u2032\nLet us deal with the other term in (44). We have, \u2200\u03b7 > 0\n\n\u03bdn\u2217 (sm \u2212 sm\u2032 ) \u2264\n\u2264\n\n\u00012\nksm \u2212 sm\u2032 k22 \u03b7\n+\n\u03bd\u0304A,p (t\u0304m,m\u2032 ) + \u03bd\u0304B,p (t\u0304m,m\u2032 )\n2\u03b7\n2\n2\n\u2032\nksm \u2212 sm k2\n+ \u03b7(\u03bd\u0304A,p (t\u0304m,m\u2032 ))2 + \u03b7(\u03bd\u0304B,p (t\u0304m,m\u2032 ))2\n2\u03b7\n\n(46)\n\nwhere, as in the proof of Theorem 3.1, tm,m\u2032 = (sm \u2212 sm\u2032 )/ksm \u2212 sm\u2032 k2 . We apply Bernstein's inequality to the fun tion t\u0304m,m\u2032 and the variables A\u2217l , we have\n\uf8f6\n\uf8eb\ns\n2Var(t\u0304m,m\u2032 (A0 ))x kt\u0304m,m\u2032 k\u221e x \uf8f8\n+\n\u2264 e\u2212x .\n\u2200x > 0, P \uf8ed\u03bd\u0304A,p (t\u0304m,m\u2032 ) >\n(47)\np\n3p\nWe pro eed as in the proof of Claim 3 to ontrol this varian e. We have, by stationarity\nof the pro ess (Xn )n\u2208Z ,\nq\u22121\n1 X\nVar(t\u0304m,m\u2032 (A0 )) = 2\n(q \u2212 k)Cov(tm,m\u2032 (X1 ), tm,m\u2032 (Xk+1 )).\n2q\nk=0\n\nFrom Inequality (5), we have\nCov(tm,m\u2032 (X1 ), tm,m\u2032 (Xk+1 )) \u2264 tm,m\u2032\n\nBV\n\ntm,m\u2032\n\n\u221e\n\n\u03b2\u0303k .\n\nLet m \u25b3 m\u2032 be the set of indexes that belong to m \u222a m\u2032 but do not belong to m \u2229 m\u2032 . We\nuse the same omputations as in the proof of Claim 3 to get\n\ntm,m\u2032\n\nBV\n\nP\n\n\u2264\n\nSin e tm,m\u2032\n\n\u221e\n\n(j,k)\u2208m\u2032 \u25b3m (P \u03c8j,k )\u03c8j,k\n\nBV\n\n\u2264\n\nksm \u2212 sm\u2032 k2\n\ns\n\nX\n\n(j,k)\u2208m\u2032 \u25b3m\n\nk\u03c8j,k k2BV \u2264 KBV (Dm \u2228 Dm\u2032 ).\n\n\u221a\n= \u03a6 Dm \u2228 Dm\u2032 , we have\nCov(tm,m\u2032 (X1 ), tm,m\u2032 (Xk+1 )) \u2264 \u03a6KBV \u03b2\u0303k (Dm \u2228 Dm\u2032 )3/2 .\n\n(48)\n\nMoreover, we have\nCov(tm,m\u2032 (X1 ), tm,m\u2032 (Xk+1 )) \u2264 tm,m\u2032\n\n\u221e\n\ntm,m\u2032\n\nThus, using a \u2227 b \u2264 au b1\u2212u with\n\na = \u03a6KBV \u03b2\u0303k (Dm \u2228 Dm\u2032 )3/2 , b = \u03a6 ksk2\nwe have\n\np\n\nksk2 \u2264 \u03a6 ksk2\n2\n\np\n\n\u2032 ).\n(Dm \u2228 Dm\n\n(Dm \u2228 Dm\u2032 ), and u =\n\n1\n6\n< ,\n7+\u03b8\n2\n\nu\n\u03b2\u0303ku ksk1\u2212u\n(Dm \u2228 Dm\u2032 )1/2+u .\nCov(tm,m\u2032 (X1 ), tm,m\u2032 (Xk+1 )) \u2264 \u03a6KBV\n2\n\n23\n\n(49)\n\n\fThus\nVar(t\u0304m,m\u2032 (A0 )) \u2264\n\n\u221e\nX\n\nu\n\u03a6KBV\n\n\u03b2\u0303ku\n\nk=0\n\nMoreover\n\n!\n\nksk21\u2212u\n\n(Dm \u2228 Dm\u2032 )1/2+u\n.\n2q\n\n(50)\n\n1\n1 p\n\u2032 .\nktm,m\u2032 k\u221e \u2264 \u03a6 Dm \u2228 Dm\n2\n2\n\nkt\u0304m,m\u2032 k\u221e \u2264\n\n(51)\n\nNow, we use (47) with x = (log(Dm \u2228 Dm\u2032 ))\u03b3 / ksk21\u2212u + y/(Dm \u2228 Dm\u2032 )1/2+u . From (50)\nand (51), we have for all y > 0,\nv\n\uf8eb\n!\nu\n1\u2212u\nu (Dm \u2228 Dm\u2032 )1/2+u\nksk\ny\n2\n(log(Dm \u2228 Dm\u2032 ))\u03b3 +\nP \uf8ed\u03bd\u0304A,p (t\u0304m,m\u2032 ) > L2 t\n2pq\n(Dm \u2228 Dm\u2032 )1/2+u\n!!\np\n\u2032\n\u03a6 Dm \u2228 Dm\ny\n(log(Dm \u2228 Dm\u2032 ))\u03b3\n+\n+\n6p\n(Dm \u2228 Dm\u2032 )1/2+u\nksk1\u2212u\n2\n\u2212\n\n\u2264e\n\n(log(Dm \u2228D \u2032 ))\u03b3\nm\nksk1\u2212u\n2\n\n\u2212\n\ne\n\ny\n(Dm \u2228Dm\u2032 )1/2+u\n\nNow we use the inequality\n\n\u221a\n\na+b\u2264\n\n\u221a\n\n.\na+\n\n\u221a\n\nb with\n\na = (log(Dm \u2228 Dm\u2032 ))\u03b3 and b =\n\nksk1\u2212u\ny\n2\n(Dm \u2228 Dm\u2032 )1/2+u\n\nand we obtain, using Assumption [M1 \u2104\n!\nr\n\u2032 )\nL2 (m, m\u2032 )(Dm \u2228 Dm\nLs \u221a\nP \u03bd\u0304A,p t\u0304m,m\u2032 \u2212\n> \u221a ( y + y)\nn\nn\n\u2212\n\n\u2264e\n\n(log(Dm \u2228D \u2032 ))\u03b3\nm\nksk1\u2212u\n2\n\nwith\n\n\u2212(1/2+u) y\n\ne\u2212(Dm \u2228Dm\u2032 )\n\n,\n\ns\n\n(log(Dm \u2228 Dm\u2032 ))\u03b3\n\u03a6(log(Dm \u2228 Dm\u2032 ))\u03b3\nL2 (m, m\u2032 ) = L2\n+\n3(log n)\u03ba\n(Dm \u2228 Dm\u2032 )1/2\u2212u\nq\n\u03a6\nand Ls = L2 ksk1\u2212u\n\u2228\n.\n2\n3(log 2)\u03ba 2u\n\n!2\n\n,\n\nThus, we obain\n\n\u0012\n\u0013\n\u2032 )\nL2 (m, m\u2032 )(Dm \u2228 Dm\nL2s\n2\n2\nP (\u03bd\u0304A,p t\u0304m,m\u2032 ) > 2\n+ 4 (y + y )\nn\nn\n\u2212\n\n\u2264e\n\n(log(Dm \u2228Dm\u2032 ))\u03b3\nksk1\u2212u\n2\n\n\u2212\n\ny\n\n(Dm \u2228D\n\n)\nm\u2032\n\n1/2+u\n\n.\n\nThe same result holds for \u03bd\u0304B,p t\u0304m,m\u2032 . Thus we obtain from (46)\n\n\u0013\n\u0012\n\u2032 )\nL2 (m, m\u2032 )(Dm \u2228 Dm\nL2\nksm \u2212 sm\u2032 k22\n+ 4\u03b7\n+ 8\u03b7 s (y + y 2 )\nP \u03bdn\u2217 (sm \u2212 sm\u2032 ) \u2265\n2\u03b7\nn\nn\n\u2212\n\n\u2264 2e\n\n(log(Dm \u2228D \u2032 ))\u03b3\nm\nksk1\u2212u\n2\n\n\u2212\n\ny\n\n(Dm \u2228D\n\n)\nm\u2032\n\n1/2+u\n\n.\n\n24\n\n\fWe dedu e that\n\u0012\n\u2032 )\nL2 (m, m\u2032 )(Dm \u2228 Dm\nksm \u2212 sm\u2032 k22\n\u2212 4\u03b7\nP \u2203m, m\u2032 \u2208 Mn , \u03bdn\u2217 (sm \u2212 sm\u2032 ) \u2212\n2\u03b7\nn\n!\n\u0013\n(log(Dm \u2228D \u2032 ))\u03b3\ny\nm\nX\n\u2212\n\u2212\nL2\n1/2+u\nksk1\u2212u\n2\n\u2265 8\u03b7 s (y + y 2 ) \u2264 2\ne\n.\ne (Dm \u2228Dm\u2032 )\nn\n\u2032\nm,m \u2208Mn\n\nWe integrate this last inequality to get Claim 4.\u0003\n\nCon lusion of the proof:\nTake\n\nDm\n,\nn\nwhere L1,m and L2 (m, m) are de\u001cned by (35) and (43) respe tively. From Claims 2, 3 and\n4, if we take the expe tation in (21), we have, for some onstant Ls ,\n\u0012\n\u0013\n\u0011\n\u0010\nDmo\n\u03b7Ls\n2\n2\n\u2032\nE ks \u2212 s\u0303k2 \u2264 E ks \u2212 \u015dmo k2 + pen (mo ) \u2212 V (mo ) + 2\u03b7L2 (mo , mo )\n. (52)\n+\nn\nn\npen\u2032 (m) \u2265 (2L1,m + \u03b7L2 (m, m))\n\nMoreover, if Dm \u2265 (L2 /L1 )(log n)\u03ba\u2212\u03b3/2\n\nL1,m\n4L21\n\n\u00012(7+\u03b8)/(\u03b8\u22125)\n\n, we have\n\n\u0013\n\u00132\nL3\n\u2212(\u03ba\u2212\u03b3)\n(log n)\n\u2264 (1 + \u01eb) (1 + \u01eb) + 1 +\n2L1\n\u0012\n\u0013\nL3 2\n3\n\u22121\n\u2264 (1 + \u01eb) + (1 + \u01eb )(1 + \u01eb) 1 +\n(log n)\u22122(\u03ba\u2212\u03b3) .\n2L1\n\u0012\n\n\u0012\n\n(53)\n\nWe use the inequality (a + b)2 \u2264 (1 + \u01eb)a2 + (1 + \u01eb\u22121 )b2 to obtain (53). Moreover, we have\n\nL2 (m, m) \u2264\n\n4L21\n\n\u0012\u0012\n\n\u03a6\n1+\n6L1\n\n\u0013\n\n\u2212(\u03ba\u2212\u03b3)\n\n(log n)\n\n\u00132\n\n.\n\nAs in the proof of Theorem 3.1, we take \u03b7 = (log n)\u03ba\u2212\u03b3 and we \u001cx \u01eb su\u001e iently small. For\nn \u2265 no , we have 2L1,m + \u03b7L2 (m, m) < KL21 . Thus inequality (18) follows from (52).\u0003\n7\n\nAppendix\n\nThis se tion is devoted to te hni al lemmas that are needed in the proofs.\n7.1\n\nCovarian e inequality\n\nLemma 7.1 Viennet's inequality Let (Xn )n\u2208ZP\nbe a stationary and P\n\u03b2 -mixing pro ess. There\nexists a positive fun tion b su h that P (b) \u2264\nfun tion h \u2208 L2 (P )\n!\nq\nVar\n\nX\n\nh(Xl )\n\nl=1\n\n25\n\n\u221e\nl=0 \u03b2l ,\n\nP (bp ) \u2264 p\n\n\u2264 4qP (bh2 ).\n\n\u221e p\u22121\n\u03b2l ,\nl=1 l\n\nand for all\n(54)\n\n\f7.2\n\nCon entration inequalities\n\nWe sum up in this se tion the on entration inequalities we used in the proofs. We begin\nwith Bernstein's inequality\n\nProposition 7.2 Bernstein's inequality\n\nLet X1 , ..., Xn be iid random variables valued in a measurable spa e (X, X ) and let t be a\nmeasurable real valued fun tion. Let v = Var(t(X1 )) and b = ktk\u221e , then, for all x > 0, we\nhave\n!\nr\n2x bx\n+\nn\n3n\n\nP (Pn \u2212 P )t > v\n\n\u2264 e\u2212x .\n\nNow we give the most important tool of our proof, it is a on entration's inequality for the\nsupremum of the empiri al pro ess over a lass of fun tion. We give here the version of\nBousquet [10\u2104.\n\nTheorem 7.3 Talagrand's Theorem\n\nLet X1 , ..., Xn be i.i.d random variables valued in some measurable spa e [X, X ]. Let F be\na separable lass of bounded fun tions from X to R and assume that all fun tions t in F\nare P -measurable, and satisfy Var(t(X1 )) \u2264 \u03c3 2 , ktk\u221e \u2264 b. Then\n!\n\u0013 r\n2x (\u03c3 2 + 2bE (supt\u2208F \u03bdn (t))) bx\nP sup \u03bdn (t) > E sup \u03bdn (t) +\n\u2264 e\u2212x .\n+\nn\n3n\nt\u2208F\nt\u2208F\n\u0012\n\nIn parti ular, for all \u01eb > 0, if \u03ba(\u01eb) = 1/3 + \u01eb\u22121 , we have\n!\nr\n\u0013\n2x\nbx\nP sup \u03bdn (t) > (1 + \u01eb)E sup \u03bdn (t) + \u03c3\n\u2264 e\u2212x .\n+ \u03ba(\u01eb)\nn\nn\nt\u2208F\nt\u2208F\n\u0012\n\nWe an dedu e from this Theorem a on entration's inequality for \u03c7-square type statisti s.\nThis is Proposition (7.3) of Massart [20\u2104.\n\nProposition 7.4 Let\n\nX1 , ..., Xn be independent and identi ally distributed random variables valued in some measurable spa e (X, X ). Let P denote their ommon distribution.\nLet \u03c6\u03bb be a \u001cnite family of measurable and bounded fun tions on (X, X ). Let\nH\u039b2 = k\n\nMoreover, let S\u039b = a \u2208 R\u039b :\n\b\n\nX\n\n\u03bb\u2208\u039b\n\nP\n\n\u03c62\u03bb k\u221e and B\u039b2 =\n\na\u2208S\u039b\n\nVar(\u03c6\u03bb (X1 )).\n\n\u03bb\u2208\u039b\n\n= 1 and\n\n2\n\u03bb\u2208\u039b a\u03bb\n\nV\u039b2 = sup\n\nX\n\n(\n\nVar\n\nX\n\n!)\n\na\u03bb \u03c6\u03bb (X1 )\n\n\u03bb\u2208\u039b\n\n.\n\nThen the following inequality holds, for all positive x and \u01eb\n\uf8ee\n\nP\uf8f0\n\nX\n\n\u03bb\u2208\u039b\n\n(Pn \u2212 P )2 \u03c6\u03bb\n\n!1/2\n\n1+\u01eb\n\u2265 \u221a B \u039b + V\u039b\nn\n\nwhere \u03ba(\u01eb) = \u01eb\u22121 + 1/3.\n\n26\n\nr\n\n\uf8f9\n\nH\u039b x \uf8fb\n2x\n+ \u03ba(\u01eb)\n\u2264 e\u2212x ,\nn\nn\n\n(55)\n\n\fProof :\n\nity\n\nFollowing Massart [20\u2104 Proposition 7.3, we remark that, by Cau hy-S hwarz's inequal-\n\nX\n\n\u03bb\u2208\u039b\n\n\u03bdn2 \u03c6\u03bb\n\n!1/2\n\n= sup\n\nX\n\na\u2208S\u039b \u03bb\u2208\u039b\n\na\u03bb \u03bdn \u03c6\u03bb = sup \u03bdn\na\u2208S\u039b\n\nX\n\n\u03bb\u2208\u039b\n\na\u03bb \u03c6\u03bb\n\n!\n\n.\n\nThus the result follows by applying Talagrand's Theorem to the lass of fun tions\n(\n)\nX\nF = t=\na\u03bb \u03c6\u03bb ; a \u2208 S\u039b .\n\u03bb\u2208\u039b\n\n27\n\n\fReferen es\n\n[1\u2104 H. Akaike. Information theory and an extension of the maximum likelihood prin iple.\nIn Se ond International Symposium on Information Theory (Tsahkadsor, 1971), pages\n267\u0015281. Akad\u00e9miai Kiad\u00f3, Budapest, 1973.\n[2\u2104 Hirotugu Akaike. Statisti al predi tor identi\u001c ation. Ann. Inst. Statist. Math., 22:203\u0015\n217, 1970.\n[3\u2104 Donald W. K. Andrews. Nonstrong mixing autoregressive pro esses. J. Appl. Probab.,\n21(4):930\u0015934, 1984.\n[4\u2104 S. Arlot and P. Massart. Data-driven alibration of penalties for least squares regression. Submitted to Journal of Ma hine learning resear h, 2008.\n[5\u2104 Sylvain Arlot. Model sele tion by resampling penalization. hal-00262478, 2008.\n[6\u2104 Y. Baraud, F. Comte, and G. Viennet. Adaptive estimation in autoregression or\n\u03b2 -mixing regression via model sele tion. Ann. Statist., 29(3):839\u0015875, 2001.\n[7\u2104 Henry C. P. Berbee. Random walks with stationary in rements and renewal theory,\nvolume 112 of Mathemati al Centre Tra ts. Mathematis h Centrum, Amsterdam,\n1979.\n[8\u2104 Lu ien Birg\u00e9 and Pas al Massart. From model sele tion to adaptive estimation. In\nFests hrift for Lu ien Le Cam, pages 55\u001587. Springer, New York, 1997.\n[9\u2104 Lu ien Birg\u00e9 and Pas al Massart. Minimal penalties for Gaussian model sele tion.\nProbab. Theory Related Fields, 138(1-2):33\u001573, 2007.\n[10\u2104 Olivier Bousquet. A Bennett on entration inequality and its appli ation to suprema\nof empiri al pro esses. C. R. Math. A ad. S i. Paris, 334(6):495\u0015500, 2002.\n[11\u2104 Ri hard C. Bradley. Introdu tion to strong mixing onditions. Vol. 1. Kendri k Press,\nHeber City, UT, 2007.\n[12\u2104 F. Comte, J. Dede ker, and M. L. Taupin. Adaptive density de onvolution with\ndependent inputs. Math. Methods Statist., 17(2):87\u0015112, 2008.\n[13\u2104 Fabienne Comte and Floren e Merlev\u00e8de. Adaptive estimation of the stationary density of dis rete and ontinuous time mixing pro esses. ESAIM Probab. Statist., 6:211\u0015\n238 (ele troni ), 2002. New dire tions in time series analysis (Luminy, 2001).\n[14\u2104 J\u00e9r\u001dme Dede ker, Paul Doukhan, Gabriel Lang, Jos\u00e9 Rafael Le\u00f3n R., Sana Louhi hi,\nand Cl\u00e9mentine Prieur. Weak dependen e: with examples and appli ations, volume\n190 of Le ture Notes in Statisti s. Springer, New York, 2007.\n[15\u2104 J\u00e9r\u001dme Dede ker and Cl\u00e9mentine Prieur. New dependen e oe\u001e ients. Examples and\nappli ations to statisti s. Probab. Theory Related Fields, 132(2):203\u0015236, 2005.\n[16\u2104 David L. Donoho, Iain M. Johnstone, G\u00e9rard Kerkya harian, and Dominique Pi ard.\nDensity estimation by wavelet thresholding. Ann. Statist., 24(2):508\u0015539, 1996.\n[17\u2104 Paul Doukhan. Mixing, volume 85 of Le ture Notes in Statisti s. Springer-Verlag,\nNew York, 1994. Properties and examples.\n28\n\n\f[18\u2104 Ir\u00e8ne Gannaz and Olivier Wintenberger. Adaptive density estimation under dependen e. forth oming in ESAIM, Probab. and Statist., 2008.\n[19\u2104 C.L. Mallows. Some omments on cp . Te hnometri s, 15:661\u0015675, 1973.\n[20\u2104 Pas al Massart. Con entration inequalities and model sele tion, volume 1896 of Le ture\nNotes in Mathemati s. Springer, Berlin, 2007. Le tures from the 33rd Summer S hool\non Probability Theory held in Saint-Flour, July 6\u001523, 2003, With a foreword by Jean\nPi ard.\n[21\u2104 C. Prieur. Change point estimation by lo al linear smoothing under a weak dependen e\nondition. Math. Methods Statist., 16(1):25\u001541, 2007.\n[22\u2104 Emmanuel Rio. Th\u00e9orie asymptotique des pro essus al\u00e9atoires faiblement d\u00e9pendants,\nvolume 31 of Math\u00e9matiques & Appli ations (Berlin) [Mathemati s & Appli ations\u2104.\nSpringer-Verlag, Berlin, 2000.\n[23\u2104 Mats Rudemo. Empiri al hoi e of histograms and kernel density estimators. S and.\nJ. Statist., 9(2):65\u001578, 1982.\n[24\u2104 Mi hel Talagrand. New on entration inequalities in produ t spa es. Invent. Math.,\n126(3):505\u0015563, 1996.\n[25\u2104 Gabrielle Viennet. Inequalities for absolutely regular sequen es: appli ation to density\nestimation. Probab. Theory Related Fields, 107(4):467\u0015492, 1997.\n[26\u2104 V. A. Volkonski\b\u0019 and Yu. A. Rozanov. Some limit theorems for random fun tions. I.\nTeor. Veroyatnost. i Primenen, 4:186\u0015207, 1959.\n\n29\n\n\f"}