{"id": "http://arxiv.org/abs/1104.4410v1", "guidislink": true, "updated": "2011-04-22T08:41:15Z", "updated_parsed": [2011, 4, 22, 8, 41, 15, 4, 112, 0], "published": "2011-04-22T08:41:15Z", "published_parsed": [2011, 4, 22, 8, 41, 15, 4, 112, 0], "title": "Semi-parametric regression: Efficiency gains from modeling the\n  nonparametric part", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.3318%2C1104.4906%2C1104.1697%2C1104.0914%2C1104.0129%2C1104.5617%2C1104.4880%2C1104.3933%2C1104.1780%2C1104.3227%2C1104.5324%2C1104.4671%2C1104.5126%2C1104.4175%2C1104.4897%2C1104.0887%2C1104.3493%2C1104.1068%2C1104.1619%2C1104.3355%2C1104.1783%2C1104.3252%2C1104.3048%2C1104.4447%2C1104.2257%2C1104.4085%2C1104.1071%2C1104.2204%2C1104.3126%2C1104.3149%2C1104.1741%2C1104.0120%2C1104.3526%2C1104.4450%2C1104.0734%2C1104.0575%2C1104.0464%2C1104.5080%2C1104.2129%2C1104.5105%2C1104.3031%2C1104.5447%2C1104.0944%2C1104.4351%2C1104.5327%2C1104.0287%2C1104.3600%2C1104.1898%2C1104.5488%2C1104.4980%2C1104.0205%2C1104.2028%2C1104.4057%2C1104.4825%2C1104.0473%2C1104.1608%2C1104.5490%2C1104.1027%2C1104.1766%2C1104.1342%2C1104.1103%2C1104.3084%2C1104.5633%2C1104.1617%2C1104.1462%2C1104.1922%2C1104.3166%2C1104.2074%2C1104.4448%2C1104.0555%2C1104.4225%2C1104.5333%2C1104.5698%2C1104.5470%2C1104.4410%2C1104.1317%2C1104.0261%2C1104.4660%2C1104.4574%2C1104.0350%2C1104.0594%2C1104.0799%2C1104.2878%2C1104.4366%2C1104.0126%2C1104.0272%2C1104.1611%2C1104.2447%2C1104.4498%2C1104.0177%2C1104.3462%2C1104.5646%2C1104.5078%2C1104.2386%2C1104.3687%2C1104.2098%2C1104.4991%2C1104.3661%2C1104.2245%2C1104.3402%2C1104.4976&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Semi-parametric regression: Efficiency gains from modeling the\n  nonparametric part"}, "summary": "It is widely admitted that structured nonparametric modeling that circumvents\nthe curse of dimensionality is important in nonparametric estimation. In this\npaper we show that the same holds for semi-parametric estimation. We argue that\nestimation of the parametric component of a semi-parametric model can be\nimproved essentially when more structure is put into the nonparametric part of\nthe model. We illustrate this for the partially linear model, and investigate\nefficiency gains when the nonparametric part of the model has an additive\nstructure. We present the semi-parametric Fisher information bound for\nestimating the parametric part of the partially linear additive model and\nprovide semi-parametric efficient estimators for which we use a smooth\nbackfitting technique to deal with the additive nonparametric part. We also\npresent the finite sample performances of the proposed estimators and analyze\nBoston housing data as an illustration.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.3318%2C1104.4906%2C1104.1697%2C1104.0914%2C1104.0129%2C1104.5617%2C1104.4880%2C1104.3933%2C1104.1780%2C1104.3227%2C1104.5324%2C1104.4671%2C1104.5126%2C1104.4175%2C1104.4897%2C1104.0887%2C1104.3493%2C1104.1068%2C1104.1619%2C1104.3355%2C1104.1783%2C1104.3252%2C1104.3048%2C1104.4447%2C1104.2257%2C1104.4085%2C1104.1071%2C1104.2204%2C1104.3126%2C1104.3149%2C1104.1741%2C1104.0120%2C1104.3526%2C1104.4450%2C1104.0734%2C1104.0575%2C1104.0464%2C1104.5080%2C1104.2129%2C1104.5105%2C1104.3031%2C1104.5447%2C1104.0944%2C1104.4351%2C1104.5327%2C1104.0287%2C1104.3600%2C1104.1898%2C1104.5488%2C1104.4980%2C1104.0205%2C1104.2028%2C1104.4057%2C1104.4825%2C1104.0473%2C1104.1608%2C1104.5490%2C1104.1027%2C1104.1766%2C1104.1342%2C1104.1103%2C1104.3084%2C1104.5633%2C1104.1617%2C1104.1462%2C1104.1922%2C1104.3166%2C1104.2074%2C1104.4448%2C1104.0555%2C1104.4225%2C1104.5333%2C1104.5698%2C1104.5470%2C1104.4410%2C1104.1317%2C1104.0261%2C1104.4660%2C1104.4574%2C1104.0350%2C1104.0594%2C1104.0799%2C1104.2878%2C1104.4366%2C1104.0126%2C1104.0272%2C1104.1611%2C1104.2447%2C1104.4498%2C1104.0177%2C1104.3462%2C1104.5646%2C1104.5078%2C1104.2386%2C1104.3687%2C1104.2098%2C1104.4991%2C1104.3661%2C1104.2245%2C1104.3402%2C1104.4976&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "It is widely admitted that structured nonparametric modeling that circumvents\nthe curse of dimensionality is important in nonparametric estimation. In this\npaper we show that the same holds for semi-parametric estimation. We argue that\nestimation of the parametric component of a semi-parametric model can be\nimproved essentially when more structure is put into the nonparametric part of\nthe model. We illustrate this for the partially linear model, and investigate\nefficiency gains when the nonparametric part of the model has an additive\nstructure. We present the semi-parametric Fisher information bound for\nestimating the parametric part of the partially linear additive model and\nprovide semi-parametric efficient estimators for which we use a smooth\nbackfitting technique to deal with the additive nonparametric part. We also\npresent the finite sample performances of the proposed estimators and analyze\nBoston housing data as an illustration."}, "authors": ["Kyusang Yu", "Enno Mammen", "Byeong U. Park"], "author_detail": {"name": "Byeong U. Park"}, "author": "Byeong U. Park", "links": [{"title": "doi", "href": "http://dx.doi.org/10.3150/10-BEJ296", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1104.4410v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1104.4410v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.3150/10-BEJ296 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1104.4410v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1104.4410v1", "journal_reference": "Bernoulli 2011, Vol. 17, No. 2, 736-748", "doi": "10.3150/10-BEJ296", "fulltext": "arXiv:1104.4410v1 [math.ST] 22 Apr 2011\n\nBernoulli 17(2), 2011, 736\u2013748\nDOI: 10.3150/10-BEJ296\n\nSemi-parametric regression: Efficiency gains\nfrom modeling the nonparametric part\nKYUSANG YU1 , ENNO MAMMEN2 and BYEONG U. PARK3\n1\n\nKonkuk University, Seoul, Korea. E-mail: kyusangu@konkuk.ac.kr\nUniversity of Mannheim, Mannheimun, Germany. E-mail: emammen@rumms.uni-mannheim.de\n3\nSeoul National University, Seoul, Korea. E-mail: bupark@stats.snu.ac.kr\n2\n\nIt is widely admitted that structured nonparametric modeling that circumvents the curse of\ndimensionality is important in nonparametric estimation. In this paper we show that the same\nholds for semi-parametric estimation. We argue that estimation of the parametric component\nof a semi-parametric model can be improved essentially when more structure is put into the\nnonparametric part of the model. We illustrate this for the partially linear model, and investigate efficiency gains when the nonparametric part of the model has an additive structure. We\npresent the semi-parametric Fisher information bound for estimating the parametric part of the\npartially linear additive model and provide semi-parametric efficient estimators for which we use\na smooth backfitting technique to deal with the additive nonparametric part. We also present\nthe finite sample performances of the proposed estimators and analyze Boston housing data as\nan illustration.\nKeywords: partially linear additive models; profile estimator; semi-parametric efficiency;\nsmooth backfitting\n\n1. Introduction\nStructured nonparametric models such as additive models are known to circumvent the\ncurse of dimensionality and allow reliable estimation when a full nonparametric model\ndoes not work. In the present paper we show that a similar assertion applies for semiparametric models: structural modeling of the nonparametric part can lead to accurate\nestimation of the parametric part even in situations where otherwise only very poor,\nunreliable or unstable estimates would be available. We show this by comparing the\npartially linear and the partially linear additive model. In particular, we demonstrate that\nusing an additive model for the nonparametric part in the partially linear model can lead\nto drastic gains of efficiency in the estimation of the parametric components. This holds\nif the dimension of the nonparametric covariates is high, or the parametric covariates\ncan be approximated by non-additive transformations of the nonparametric covariates.\n\nThis is an electronic reprint of the original article published by the ISI/BS in Bernoulli,\n2011, Vol. 17, No. 2, 736\u2013748. This reprint differs from the original in pagination and\ntypographic detail.\n1350-7265\n\nc\n\n2011 ISI/BS\n\n\fSemi-parametric regression\n\n737\n\nIn the extreme of the latter case, if the approximation is exact, then estimation of the\nparametric part in the partially linear model breaks down. If the approximation is very\ncrude, one sees large efficiency gains by using additive models for the nonparametric part.\nSuppose we observe the i.i.d. copies (Y 1 , X1 , Z1 ), . . . , (Y n , Xn , Zn ) of a random vector (Y, X, Z), where X = (X1 , . . . , Xp )\u22a4 \u2208 Rp and Z = (Z1 , . . . , Zd )\u22a4 \u2208 Rd . The partially\nlinear model assumes\nY = m0 + X\u22a4 \u03b2 + m(Z1 , . . . , Zd ) + \u01eb,\n\n(1)\n\nwhere \u03b2 is an unknown p-vector and m is an unknown d-variate function. The partially\nlinear additive model puts an additive structure to the nonparametric function m:\nY = m0 + X\u22a4 \u03b2 + m1 (Z1 ) + * * * + md (Zd ) + \u01eb.\n\n(2)\n\nThese models exclude the interesting case where X or Z includes some endogeneous\nvariables of Y , but they simplify our discussion on semi-parametric efficiency. We believe\nthat our results can be extended to the corresponding semi-parametric models with time\nseries data by following, for example, the arguments in [7].\nFor identifiability of the additive component functions mj , we put the constraints\nEmj (Zj ) = 0, 1 \u2264 j \u2264 d. We assume that (X, Z) has a joint density q with respect to\n\u03bd = \u03bd1 \u00d7 \u03bd2 , where \u03bd1 is a \u03c3-finite measure and \u03bd2 is the Lebesgue measure on each support\nof X and Z, and that the marginal density of Z (with respect to \u03bd1 ), denoted by qZ , has\ncompact support, say [0, 1]d . The model (2) enjoys the advantages of both the partially\nlinear model (1) and the nonparametric additive model to the fully nonparametric model.\nIt accommodates discrete covariates since we only require that \u03bd1 is a \u03c3-finite measure,\nand also interaction effects between covariates by putting them into the parametric part.\nBy the additive structure in the nonparametric part it avoids the curse of dimensionality,\nbut retains the flexibility of the model. It also renders easy interpretation of the individual\nrole of each covariate.\nWe discuss semi-parametric efficient estimation of the parameter \u03b2 in the model (2).\nWe present the semi-parametric Fisher information bound and provide an estimator that\nachieves the efficiency bound. Semi-parametric efficient estimation when d = 1 has been\nstudied by Bhattacharya and Zhao [1], Cuzick [5] and Schick [17]. Their works can be\neasily extended to the model (1) for d > 1. Comparing the Fisher information bounds\nfor the models (1) and (2), we find that the information bound under the model (2) is\nsmaller than the bound under the model (1). In our semi-parametric model (2), we do\nnot specify the distribution of the error term \u01eb or the distribution q of the covariates. We\nshow that one can do as well without knowing those distributions.\nThere have been a few works on the model (2). Opsomer and Ruppert [13] obtained\n\u221a\na n-consistent estimator of \u03b2 by a backfitting method with undersmoothing. Recently\nLiang et al. [8] and Carroll et al. [4] studied the model with measurement error and\nrepeated measurements, respectively. But they did not discuss semiparametric efficiency.\nThe model (1) has been studied more often; see [19], among others. Most studies, however,\nare rather focused on the cases where there is only a single-dimensional (or at most lowdimensional) nonparametric function m. This is because high-dimension costs higherorder smoothness in theory and poor small sample performances in practice.\n\n\f738\n\nK. Yu, E. Mammen and B.U. Park\n\n2. Semi-parametric efficiency\nTo avoid unnecessary complexity, we assume m0 = 0. We also assume that \u01eb is independent with (X, Z), and that g, the density of \u01eb, is symmetric and is absolutely continuous\nwith Rrespect to the Lebesgue measure, having a derivative g \u2032 and finite Fisher information (g \u2032 )2 /g < \u221e. Below, we give a heuristic argument for deriving the semi-parametric\nefficiency and present a rigorous statement in a theorem.\nSuppose that g is known and p = 1. We write m(z) = m1 (z1 ) + * * * + md (zd ) and\nadopt the convention mj (z) = mj (zj ). The logarithm of the joint density of (Y, X, Z)\nas a function of the parameters is given by l(\u03b2, m; (y, x, z)) = log g(y \u2212 x\u03b2 \u2212 m(z)), neglecting\nthose terms that do not depend on (\u03b2, m), and the log-likelihood of (\u03b2, m) by\nPn\ni\ni\ni\ni=1 l(\u03b2, m; (Y , X , Z )). Let H denote the space of all additive functions m such that\nm(z) = m1 (z1 ) + * * * + md (zd ), Emj (Zj ) = 0 and Em(Z)2 < \u221e.\nCalculation of the Fisher information in a semi-parametric model is made locally: fix\na value (\u03b2 0 , m0 ) of the parameter (\u03b2, m) and think of all 'regular' parametric submodels\n{(\u03b2, m\u03b2 ) : \u03b2 \u2208 R} passing through (\u03b2 0 , m0 ), where m\u03b2 0 = m0 and the mapping \u03b2 7\u2192 m\u03b2\nis Fr\u00e9chet differentiable as a function from R to H. Define \u03c6 = g \u2032 /g. Then, each finitedimensional submodel {(\u03b2, m\u03b2 ) : \u03b2 \u2208 R} has the score function\ndl(\u03b2, m\u03b2 )/d\u03b2|\u03b2=\u03b2 0 = \u2202l(\u03b2, m0 )/\u2202\u03b2|\u03b2=\u03b2 0 + \u2202l(\u03b2 0 , m)/\u2202m|m=m0 (\u03b4)\n= \u03c6(\u01eb)X + \u03c6(\u01eb)\u03b4(Z),\nwhere \u03b4 = \u2202m\u03b2 /\u2202\u03b2|\u03b2=\u03b2 0 \u2208 H is the tangent of the mapping \u03b2 7\u2192 m\u03b2 at \u03b2 0 , and \u2202l/\u2202m\ndenotes the Fr\u00e9chet derivative of l with respect to m. This gives the Fisher information\nfor estimating \u03b2 in each submodel as I(\u03b4) \u2261 E[\u03c6(\u01eb)X + \u03c6(\u01eb)\u03b4(Z)]2 .\nThe Fisher information at (\u03b2 0 , m0 ) \u2208 R \u00d7 H in the full semi-parametric model typically\nequals to the Fisher information at (\u03b2 0 , m0 ) \u2208 R \u00d7 H in the most difficult parametric\nsubmodel that gives minimal I(\u03b4). Theorem 1 below demonstrates that this is the case\nwith our problem. The least favorable direction \u03b4 \u2217 that minimizes I(\u03b4) over \u03b4 \u2208 H is the\nsolution of the following integral equation: for all \u03b4 \u2208 H,\n0 = E[\u03c6(\u01eb)X + \u03c6(\u01eb)\u03b4 \u2217 (Z)]\u03c6(\u01eb)\u03b4(Z)\n= Ig * E[(E(X|Z) + \u03b4 \u2217 (Z))\u03b4(Z)],\nR\nwhere Ig = (g \u2032 )2 /g. This shows that \u03b4 \u2217 = \u2212\u03a0(E(X|Z = *)|H), where \u03a0(*|H) denotes the\nprojection operator onto H, and that the 'curve' m\u2217\u03b2 corresponding to the least favorable\nsubmodel equals m\u2217\u03b2 = (\u03b2 0 \u2212 \u03b2)\u03a0(E(X|Z = *)|H) + m0 . The Fisher information for the\nleast favorable submodel is thus given by I(\u03b4 \u2217 ) = Ig * E[X \u2212 \u03a0(E(X|Z)|H)]2 , where, with\na slight abuse of notation, we write \u03a0(E(X|Z = *)|H)(Z) = \u03a0(E(X|Z)|H).\nThe above arguments can be generalized to the case where p > 1. Writing \u03b7j =\n\u03a0(E(Xj |Z = *)|H) and \u03b7 = (\u03b71 , . . . , \u03b7p )\u22a4 , the least favorable direction equals \u03b4 \u2217 = \u2212\u03b7\nso that the Fisher information matrix for the least favorable submodel equals I(\u03b4 \u2217 ) =\nIg * E[X \u2212 \u03b7(Z)][X \u2212 \u03b7(Z)]\u22a4 . In the following theorem we show that the Fisher information I(\u03b4 \u2217 ) given above is indeed the semi-parametric information bound, as defined in\n\n\f739\n\nSemi-parametric regression\n\n[3], in our original semi-parametric model where the error density g and the density q of\nthe covariate (X, Z) are not specified. To state the theorem, let G denote the set of all\nsymmetric and absolutely continuous (with respect to the Lebesgue measure) functions\ng such that Ig < \u221e. Let Q be an arbitrary class of density functions q. For the spaces of\nm, we consider Hilbert spaces defined by\n(\n)\nd\nX\nH(q) = m \u2208 L2 (q) : m(z) =\nmj (zj ) and Emj (Zj ) = 0 for all 1 \u2264 j \u2264 d ,\nj=1\n\nwhere L2 (q) denotes the space of functions m : Rd \u2192 R such that Eq m(Z)2 < \u221e and\nEq means the expectation under the density q. The semi-parametric model (2) under study is then expressed as P = {p(*; \u03b2, m, g, q) : \u03b2 \u2208 Rp , m \u2208 H(q), g \u2208 G, q \u2208 Q}. Let\n(\u03b2 0 , m0 , g0 , q0 ) be a fixed point where we are calculating the semi-parametric Fisher information. Denote by P0 the distribution corresponding to (\u03b2 0 , m0 , g0 , q0 ), and by I(P0 |\u03b2, P)\nthe semi-parametric Fisher information at P0 for estimating \u03b2 under the model P. In\nthe theorem below, the 'efficient score' l\u2217 for estimating \u03b2 is the score for \u03b2 at \u03b20 in the\nleast favorable parametric submodel that is indexed only by \u03b2 and passes through P0 .\nLet E0 denote the expectation under P0 .\nTheorem 1. The efficient score at P0 for estimating \u03b2 is given by\nl\u2217 (x, z, y; P0 |\u03b2, P)\n= \u2212[x \u2212 \u03b7(z)]\n\ng0 \u2032\n(y \u2212 x\u22a4 \u03b2 0 \u2212 m0 (z)),\ng0\n\nwhere \u03b7 = (\u03a0[E0 (Xj |Z = *)|H(q0 )])pj=1 . The information bound at P0 for estimating \u03b2\nequals I(P0 |\u03b2, P) = Ig0 * E0 [X \u2212 \u03b7(Z)][X \u2212 \u03b7(Z)]\u22a4 .\nA proof of Theorem 1 can be found in an extended version of this paper that can be\ndownloaded from http://stat.snu.ac.kr/theostat/papers/BEJ296_ExtendedVersion.\npdf.\nLet PPL \u2283 P denote the semi-parametric model (1). One can show I(P0 |\u03b2, PPL ) =\nIg0 * E0 [X \u2212 E0 (X|Z)][X \u2212 E0 (X|Z)]\u22a4 using the arguments to derive I(P0 |\u03b2, P). Note\nthat I(P0 |\u03b2, P) \u2265 I(P0 |\u03b2, PPL ) by the property of conditional expectation, and that\nthe equality I(P0 |\u03b2, P) = I(P0 |\u03b2, PPL ) holds if E0 (Xj |Z = z) are additive for all 1 \u2264\nj \u2264 d. According to the theory of semi-parametric efficiency, the minimal asymptotic\nvariance that any regular estimator of \u03b2 can achieve equals the inverse of the Fisher\ninformation matrix. The inequality I(P0 |\u03b2, P) \u2265 I(P0 |\u03b2, PPL ) implies I(P0 |\u03b2, P)\u22121 \u2264\nI(P0 |\u03b2, PPL )\u22121 , with equality holding if E0 (Xj |Z = z) are all additive.\nTheorem 2. Suppose I(P0 |\u03b2, PPL ) is positive definite. Then, I(P0 |\u03b2, P)\u22121 < I(P0 |\u03b2, PPL )\u22121\nunless E0 [\u03b7(Z) \u2212 E0 (X|Z)][\u03b7(Z) \u2212 E0 (X|Z)]\u22a4 = O, where O is the p \u00d7 p matrix with all\nentries being zero, and A < B means that B \u2212 A is non-negative definite and A 6= B.\n\n\f740\n\nK. Yu, E. Mammen and B.U. Park\n\nTheorem 2 tells that using an additive model for the nonparametric part can lead\nto drastic gains of efficiency in the estimation of the parametric components. The efficiency gains occur if the parametric covariates X are approximated by non-additive\ntransformations of the nonparametric covariates Z. If the approximation is exact, then\nestimation of the parametric part in the partially linear model (1) breaks down since\nI(P0 |\u03b2, PPL ) = O, while it does not with the partially linear additive model (2). If the\napproximation is very crude, one has large efficiency gains by using additive models for\nthe nonparametric part.\n\n3. Semi-parametric efficient estimation\nLet \u03b2 0 and m0 denote the true parameter values. In this section we present the semiparametric efficient estimator of \u03b2 0 that achieves the minimal asymptotic variance\nI(P0 |\u03b2, P)\u22121 . The construction is based on a smooth backfitting technique and a profiling\nmethod. The latter is basically for estimating the least favorable curve, and is applied\nto the Gaussian error model to produce an initial estimator of \u03b2 0 to be used in the\nconstruction of the semi-parametric efficient estimator.\n\n3.1. Smooth backfitting methods\nThe smooth backfitting method, introduced by Mammen, Linton and Nielsen [10], is\nknown to be a powerful technique for estimating additive regression functions. Since our\nprofiling method involves smooth backfitting for non-additive functions, we discuss some\nproperties of the method when the target function is not additive.\nLet W be a random variable and {W i } be a random sample distributed as W . The\nadd\nadd\nadd\nsmooth backfitting estimator, m\u0302add\nW (z) \u2261 m\u0302W,0 + m\u0302W,1 (z1 ) + * * * + m\u0302W,d (zd ), with rei\ni\nsponses W and regressors Z , are defined as the solution of following integral equations:\nm\u0302add\nW,j = m\u0303W,j \u2212\n\nd\nX\n\nl=1,6=j\n\nadd\n\u03a0\u0302j (m\u0302add\nW,l ) \u2212 m\u0302W,0 ,\n\n1 \u2264 j \u2264 d,\n\n(3)\n\nPn\nadd\n\u22121\ni\nwith the constraints hm\u0302add\nW,j , 1i = 0 for 1 \u2264 j \u2264 d. Here, m\u0302W,0 = n\ni=1 W and m\u0303W,j (zj )\ndenotes the marginal regression kernel estimator obtained by regressing W i on Zji only.\nThe operator \u03a0\u0302j stands for a projection onto a Hilbert space equipped with a scalar\nin the case where m\u0303W,j (zj ) are the local\nproduct h*, *i; see [23] for details. For example,\nR\nconstant marginal estimators, hg, hi = g(z)h(z)q\u0302Z (z) dz, with q\u0302Z (*) being the kernel\nestimator of the design density qZ . Smoothing to the direction of Zj is done by the\n0\n0\nboundary corrected kernel Khj (u, v) = cj (v)h\u22121\nj K ((u \u2212 v)/hj ),Rwhere K is a base kernel\nfunction, hj is the bandwidth, and cj (v) is a factor that gives Khj (u, v) du = 1.\nLet mW (z) = E(W |Z = z). We do not assume that mW is an additive function. Deadd\nadd\nfine madd\nW = mW,1 + * * * + mW,d to be the projection of mW onto the space of additive\n\n\fSemi-parametric regression\n\n741\n\nfunctions H(qZ ). Then, E[mW (Z) \u2212 E(W ) \u2212 madd\nW (Z)]\u03b4(Z) = 0 for any \u03b4 \u2208 H(qZ ). The\nadditive function madd\n(z)\nplays\nthe\nrole\nof\nthe\ntarget\nfunction that the smooth backfitting\nW\ndiscussed\nthe\nproperty of the smooth backfitting\nestimator m\u0302add\n(z)\naims\nat.\nLu\net\nal.\n[9]\nW\nestimators under non-additive regression models in the context of spatial data analysis.\nHowever, they treated only the case where the bandwidth is asymptotic to n\u22121/5 . Below,\nwe give a uniform expansion of the smooth backfitting estimator for a wider range of the\nbandwidths, after tedious asymptotic calculation following the lines of the arguments in\ni\n[10]. To state the theorem, let \u03b5 = W \u2212 E(W ) \u2212 madd\nW (Z) and define \u03b5 accordingly. Let\nLL\nm\u0303\u03b5,j (zj ) and m\u0303\u03b5,j (zj ) denote, respectively, the local constant and linear estimators with\nresponses \u03b5i and the scalar regressors Zji . Let hj be the bandwidth associated with Zj .\nThe theorem relies on the following assumptions.\nAssumptions A.\nA1. For 1 \u2264 j 6= k \u2264 d, qZj ,Zk are bounded away from zero and infinity on its support,\n[0, 1]2 , and have continuous partial derivatives.\nA2. The base kernel function K 0 is symmetric, supported on a compact support and\nhas bounded derivative.\nA3. The functions madd\nW,j 's are twice continuously differentiable.\nA4. E|W \u2212 mW (Z)|r0 < \u221e for some r0 > 5/2.\nTheorem 3. Assume that the conditions A1\u2013A4 hold, and that hj are asymptotic to\nn\u2212\u03b1 for 1/5 \u2264 \u03b1 < 1/2. Then, for 1 \u2264 j \u2264 d, it holds that\nadd\n2\n\u22121/2\nsup |m\u0302add\n)\nW,j (zj ) \u2212 mW,j (zj ) \u2212 hj a1,j,n (zj ) \u2212 hj a2,j (zj ) \u2212 m\u0303\u03b5,j (zj )| = op ((nhj )\n\nzj \u2208[0,1]\n\nin the local constant case, and that\n\u22121/2\nadd\n2\nLL\n)\nsup |m\u0302add\nW,j (zj ) \u2212 mW,j (zj ) \u2212 hj a3,j (zj ) \u2212 m\u0303\u03b5,j (zj )| = op ((nhj )\n\nzj \u2208[0,1]\n\nin the local linear case, for some functions a1,j,n that are uniformly bounded and non-zero\nonly for zj \u2208 [0, chj ) \u222a (1 \u2212 chj , 1] for some constant 0 < c < \u221e, and for some functions\na2,j and a3,j that are continuous.\nA proof of Theorem 3 can be found in an extended version of this paper that can be\ndownloaded from http://stat.snu.ac.kr/theostat/papers/BEJ296_ExtendedVersion.\npdf.\n\n3.2. Profiling with Gaussian error models\nWe apply a profiling technique to remove the infinite-dimensional parameter m in the\nestimation of \u03b2 0 . For a general framework of profiling approaches to semi-parametric\nmodels, we refer to [18]. See also [12] for a more recent work on profile likelihood.\n\n\f742\n\nK. Yu, E. Mammen and B.U. Park\n\nadd\nadd \u22a4\nadd\nadd\nDefine m\u0302add\nis\nX = (m\u0302X1 , . . . , m\u0302Xp ) . We note that m\u0302X is an estimator of \u03b7 and m\u0302Y\nP\nd\n0\u22a4\nadd\n0\nadd\nan estimator of \u03b2 \u03b7 + m . For each given \u03b2, let m\u0302 (z; \u03b2) = j=1 m\u0302j (zj ; \u03b2) be the\n\nsmooth backfitting estimator obtained by taking Y i \u2212 Xi\u22a4 \u03b2 = Xi\u22a4 (\u03b2 0 \u2212 \u03b2) + m0 (Zi ) +\n\u01ebi as responses and Zi as covariates. Recall that the least favorable curve is given by\nm\u2217 (*, \u03b2) \u2261 \u03b7 \u22a4 (\u03b2 0 \u2212 \u03b2) + m0 . Thus, we may regard m\u0302add (*; \u03b2) as an estimator of the least\nadd\n\u22a4\nfavorable curve m\u2217 (*, \u03b2). Since m\u0302add (z; \u03b2) = m\u0302add\nY (z) \u2212 m\u0302X (z) \u03b2 by the fact that the\nsmooth backfitting operation is linear in response vectors, the estimated profile likelihood\nbased on the Gaussian error model is given by\n\u2212\n\nn\nX\ni=1\n\nn\nX\n\n[Y i \u2212 Xi\u22a4 \u03b2 \u2212 m\u0302add (Zi ; \u03b2)]2 = \u2212\n\ni=1\n\n\u22a4\n\n2\n\ni\ni\ni\nadd\n[Y i \u2212 m\u0302add\nY (Z ) \u2212 (X \u2212 m\u0302X (Z )) \u03b2] .\n\nThe estimator that maximizes the above Gaussian profile likelihood is then given by\n\u03b2\u0302 =\n\nn\nX\ni=1\n\ni\n\ni\u22a4\n\nX\u0303 X\u0303\n\n!\u22121\n\nn\nX\n\ni\n\nX\u0303 \u1ef8\n\ni=1\n\ni\n\n!\n\n,\n\ni\ni\ni\nadd\ni\nwhere X\u0303i = Xi \u2212 m\u0302add\nX (Z ) and \u1ef8 = Y \u2212 m\u0302Y (Z ).\n\nTheorem 4. Suppose that the assumptions A1\u2013A4 hold with W = Y and Xj , 1 \u2264 j \u2264 p.\nAlso, assume that E[exp(|Xj \u2212 E(Xj |Z)|)|Z] < C a.s. for some C > 0, 1 \u2264 j \u2264 p. If the\nbandwidths hj are asymptotic to n\u2212\u03b1 for 1/5 \u2264 \u03b1 < 1/2, then it holds that\n\u221a\nd\n\u22a4 \u22121\nn(\u03b2\u0302 \u2212 \u03b20 ) \u21d2 N (0, var(\u01eb)[E(X \u2212 \u03b7(Z))(X \u2212 \u03b7(Z)) ] ).\nA proof of Theorem 4 is given in the Appendix. We note that the asymptotic variance\nof the estimator \u03b2\u0302 is larger than I(P0 |\u03b2, P)\u22121 . This can be seen directly from a projection\nproperty. In fact, var(\u01eb) \u2265 Ig\u22121 and the equality hold if g is Gaussian. This means that\nthe estimator \u03b2\u0302 achieves the semi-parametric efficiency in the reduced model where g is\nspecified as a Gaussian density. It is also interesting to see what happens if \u03b70 (X, Z) \u2261\nE0 (Y |X, Z) does not belong to the partially linear additive model of the form (2). In this\ncase, our estimator of \u03b70 converges to \u03b7 \u2217 , which is the L2 (q)-projection of \u03b70 onto the\nspace\nF = {f \u2208 L2 (q) | f (x, z) = \u03b2 \u22a4 x + m(z), \u03b2 \u2208 Rp , m \u2208 H}.\n\n(4)\n\n3.3. Adapting to unknown error density\nIn this subsection, we construct the semi-parametric efficient estimator that achieves the\nminimal asymptotic variance discussed in Section 2. We follow the approach adopted\nby Bickel [2], Schick [16, 17], Park [14], Cuzick [5] P\nand Bhattacharya and Zhao [1].\nWrite I = I(P0 |\u03b2, P) and define \u03b2\u2217n = \u03b2 0 \u2212 I \u22121 n\u22121 ni=1 [Xi \u2212 \u03b7(Zi )]\u03c6(\u01eb). Then, the\n\n\f743\n\nSemi-parametric regression\n\nrandom sequence \u03b2\u2217n achieves the efficiency bound. We plug some estimators of the\nunknown quantities into \u03b2 \u2217n . We estimate the error density g by using the 'pseudo' errors\n\u01eb\u0302i \u2261 \u1ef8 i \u2212 X\u0303i\u22a4 \u03b2\u0302, where \u03b2\u0302 is the Gaussian\nPnprofile estimator constructed in Section 3.2.\nIn particular, we take \u011d(t) = b + (na)\u22121 i=1 L((t \u2212 \u01eb\u0302i )/a) and \u011d \u2032 (t) = d\u011d(t)/dt, where\na and b are positive constants that depend on the sample size n, and L is a symmetric\ndifferentiable density function. Define\n!\n!\nn\nn\nX\nX\ni 2\ni i\u22a4\n\u22121\n\u22121\n\u02c6\n\u03c6\u0302(\u01eb\u0302 ) ,\nX\u0303 X\u0303\nn\nI= n\ni=1\n\ni=1\n\nwhere \u03c6\u0302 is the 'symmetrized' estimator of \u03c6 defined by \u03c6\u0302(e) = [(\u011d \u2032 /\u011d)(e) \u2212 (\u011d \u2032 /\u011d)(\u2212e)]/2.\nOur semi-parametric efficient estimator is then given by\n1\n\u03b2\u0303 = \u03b2\u0302 \u2212 I\u02c6\u22121\nn\n\nn\nX\n\nX\u0303i \u03c6\u0302(\u01eb\u0302i ).\n\ni=1\n\nAssumptions B.\nB1. The error \u01eb has an absolutely continuous\nR and symmetric density g with respect\nto the Lebesgue measure, \u03bc, and Ig = (g \u20322 /g) d\u03bc < \u221e.\nB2. The kernel L is a symmetric density function with three bounded and Lipschitz\ncontinuous derivatives.\nB3. The sequences a and b converge to zero, as n \u2192 \u221e, and satisfy n1/2 hj b(a2 \u2227\nb2 ) \u2192 \u221e and a2 /{hj (log n)2 } \u2192 \u221e for all 1 \u2264 j \u2264 d.\nTheorem 5. Assume that the conditions of Theorem 4 and the assumptions B1\u2013B3\n\u221a\nd\nhold. Then, n(\u03b2\u0303 \u2212 \u03b20 ) \u21d2 N (0, I(P0 |\u03b2, P)\u22121 ).\nA proof of Theorem 5 is given in the Appendix. For a choice of the bandwidth a in\n\u011d, one can devise a data-driven choice along the lines of Park [15]. For h, one can follow\nthe approach of Mammen and Park [11]. In this adaptation step, misspecification of the\nmodel may result in a meaningless estimator. This is in contrast to the estimation in\nthe initial step where the procedure estimates the projection of the mean function onto\nthe model space F at (4). The reason is that the residuals from the initial step include\nnot only the pure errors but also the deviation of the true regression function from its\nprojection onto F . These residuals mislead estimation of the score function.\n\n4. Numerical properties\nWe generated 500 random samples of the size n = 400. We used Epanechnikov kernel for\nthe regression and the Gaussian density kernel for the estimation of the score function.\nWe applied a local constant version of smooth backfitting. We took m1 (z1 ) = sin{2\u03c0(z1 \u2212\n0.5)} and m2 (z2 ) = z2 \u2212 0.5 + sin{2\u03c0(z2 \u2212 0.5)}. We set m0 = 3, \u03b21 = 1.5 and \u03b22 = 0.8. We\n\n\f744\n\nK. Yu, E. Mammen and B.U. Park\n\ndrew (Z1 , Z2 ) from N2 ((0.5, 0.5)\u22a4, \u03a3) truncated to [0, 1]2 , where \u03a3 = {(1 \u2212 \u03c1)I + \u03c111\u22a4 }/4.\nWe generated X1 = CZ1 (1 \u2212 2Z2) + U for some constant C, where U \u223c N (0, 0.5), and X2\nfrom Bernoulli(p(X1 , Z1 , Z2 )), where p(X1 , Z1 , Z2 ) = g(exp((Z1 + Z2 )/2) + sin(2\u03c0Z1 ) \u2212\nX12 ) and g(t) = exp(t)/(1 + exp(t)). Note that E(X1 |Z = *) is orthogonal to the space of\nadditive functions.\nWe compared the Gaussian profile estimator (SAM), given in Section 3.2, and the\nprofile kernel estimator (PL), given in [19], which is for the partial linear model without\nthe additive structure. For this, we generated \u01eb from N (0, 1) and set \u03c1 = 0. In the case\nwhere p = 1, that is, X2 does not enter the model, the theoretical value of the ratio of the\nasymptotic variance of SAM to that of PL equals 1/(1 + 0.1707C 2). The empirical values\nfrom our simulation study for the bandwidth pair (h1 , h2 ) that gave the best mean square\nerror (MSE) were 0.7818, 0.5868 and 0.4082 for C = 1, 2 and 3, respectively, which nearly\ncoincided with the theoretical values. We tried other values of \u03c1, but the lesson was the\nsame. In the case where p = 2 and d = 5 with (Z1 , . . . , Z5 ) from N5 ((0.5, . . . , 0.5)\u22a4 , \u03a3)\ntruncated to [0, 1]5 and mj (zj ) = zj2 for 3 \u2264 j \u2264 5, we took C = 1 and found that SAM\nbeat PL for all bandwidth choices that we tried. The Gaussian profile estimator was\nstable while PL broke down for small bandwidths. The best MSE of SAM and that of\nPL, respectively, for various choices of the bandwidth pair (h1 , h2 ) were 0.0032 and 0.0051\nfor \u03b21 and 0.0186 and 0.0269 for \u03b22 .\nNext, we compared SAM with the semi-parametric efficient estimator (ASAM). For\nthis, we considered the case where p = d = 2, C = 1 and \u03c1 = 0.8, and generated \u01eb from\nN (0, 1), t-distribution with degree of freedom 3, and 12 N (\u22121.5, 0.62) + 21 N (1.5, 0.62). For\nASAM, we took b = 0.01, and six different choices of a: ai = 0.3 + 0.1i, 0 \u2264 i \u2264 5, for\nN (0, 1) and t(3) errors and ai = 0.1 + 0.1i, 0 \u2264 i \u2264 5, for the Gaussian mixture error.\nWe used 36 different choices for the bandwidth pair (h1 , h2 ) \u2208 {0.05, 0.10, , . . ., 0.30}2 .\nFigure 1 is for the estimators of \u03b21 . Each box-plot was obtained from the 36 values of\nMSE that corresponded to the 36 bandwidth pairs (h1 , h2 ). For ASAM, the value of a\nis indicated on the horizontal scale. The figure suggests that the values of the MSE of\nASAM are far smaller than those of SAM for the entire range of the bandwidth a, under\nt(3) and the Gaussian mixture error models. The box-plots for the Gaussian error model\nare not given here since SAM and ASAM gave similar performance. The results for \u03b22\nare not reported either since they give a similar lesson.\n\n5. Boston housing data\nWe applied the semi-parametric efficient estimators to Boston housing data as an illustration. As in [6, 22], we took the median price in 1,000 USD (MEDV) as the response\nY . Also, we chose as covariates X1 , X2 and Z1 , . . . , Z6 , respectively, the eight variables\nLSTAT (percentage values of lower status population), CHAS (a dummy variable that\ntakes the value 1 if the tract borders Charles River; 0 otherwise), CRIM (per capita crime\nrate), RM (average numbers of rooms per dwelling), NOX (nitric oxides concentration),\nPTRATIO (pupil\u2013teacher ratios), DIS (weighted distances to five Boston employment\ncenters) and TAX (full-value property tax rate per 10,000 USD). The logarithms of\n\n\fSemi-parametric regression\n\n745\n\nFigure 1. Mean square errors of SAM and ASAM.\n\nLSTAT, DIS and TAX were taken to reduce sparse areas, as in [22]. We chose the model\nP6\nY = m0 + \u03b21 X1 + \u03b22 X2 + j=1 mj (Zj ) + \u01eb. In the data set, there were 16 cases for which\nY took the maximal value 50. These may be censored responses that one may remove\nfrom analysis. Indeed, an initial analysis showed a strong asymmetry in the distribution\nof the residuals, which led us to exclude the 16 cases for further analysis. For additive\nregression, we applied local constant smooth backfitting with the Epanechnikov kernel\nand bandwidths hj chosen by a rule of thumb.\nWith SAM, we obtained \u03b2\u03021 = \u22126.203 and \u03b2\u03022 = 0.985. Their estimated standard errors\nwere 0.420 and 0.597, respectively. This suggests that \u03b2\u03022 is not strongly significant while\n\u03b2\u03021 is. The generalized R2 was 0.862. For ASAM, in the estimation of the score function,\nwe used a bandwidth a that was obtained by R function bw.SJ(). With ASAM, we got\n\u03b2\u03031 = \u22126.172 and \u03b2\u03032 = 1.366, and their estimated standard errors were 0.399 and 0.567,\nrespectively. Thus, with ASAM, both the estimated coefficients are strongly significant.\nThis may be an indication that a Gaussian error model is not appropriate for the data\nset. The generalized R2 was almost the same as in the analysis with SAM.\n\n\f746\n\nK. Yu, E. Mammen and B.U. Park\n\nAppendix\nProof of Theorem 4. We only treat the case with local constant smooth backfitting.\nThe case with local linear smooth backfitting can be dealt with similarly. We prove\nn\u22121/2\n\nn\nX\ni=1\n\nX\u0303i (\u1ef8 i \u2212 X\u0303i\u22a4 \u03b20 ) \u2212 n\u22121/2\n\nn\nX\ni=1\n\n(Xi \u2212 \u03b7(Zi ))\u01ebi = op (1).\n\n(5)\n\n0\nadd\nWrite \u2206(z) = m0 (z)\u2212\nside of equation\n(5) equals C1 + C2 + C3 ,\nPnm\u0302 i(z; \u03b2 ).iThe left-hand\nPn\n\u22121/2\ni\ni\nwhere C1 = n\n(X \u2212 \u03b7(Z ))\u2206(Zi ), C2 = n\u22121/2 i=1 (\u03b7(Zi ) \u2212 m\u0302add\nX (Z ))\u01eb , and\ni=1\nPn\nPd\n\u22121/2\ni\nadd\ni\ni\nC3 = n\ni=1 (\u03b7(Z ) \u2212 m\u0302X (Z ))\u2206(Z ). Write \u2206(z) = \u22060 +\nj=1 \u2206j (zj ). By Theorem 3, standard techniques of kernel smoothing, integration by part and the representation of m0 and m\u0302add (z; \u03b2 0 ) as a solution of an integral equation with differentiable kernel\n(see equation (3)), we have\n\nsup |\u2206(z)| = op (\u03b4n ),\n\nz\u2208[0,1]d\n\nsup\nzj \u2208[0,1]\n\nd\n\u2206j (zj ) \u2212 hj bn,j (zj ) = op (\u03b4n )\ndzj\n\nfor some uniformly bounded non-random functions bn,j , where \u03b4n = n\u2212a for some\na \u2208 (0, 1/2 \u2212 \u03b1). These imply that \u03b4n\u22121 \u2206 \u2208 B(0, 1) with probability tending to one, where\nPd\nB(0, 1) denotes a class of additive functions j=1 gj (zj ) such that each gj is a real function defined on [0, 1] and satisfies supt,t\u2032 \u2208[0,1] |gj (t) \u2212 gj (t\u2032 )| \u2264 |t \u2212 t\u2032 |. The covering number\nwith bracketing of B(0, 1) with respect to sup-norm, N[*] (\u03b7) \u2261 N[*] (\u03b7, B(0, 1), k * k\u221e ),\n\u22121\nis bounded by (2\u03b7 \u22121 )d 3d\u03b7 . Define random functionals F (Xji , Zi ) : B(0, 1) \u2192 R by\nPn\n[F (Xji , Zi )](g) = (Xji \u2212 \u03b7j (Zi ))g(Zi ), and Fj : B(0, 1) \u2192 R by Fj = n\u22121/2 i=1 F (Xji , Zi ).\nThen, using Corollary 8.8 of van de Geer [20] and the tail condition assumed in the\ntheorem, one can show supg\u2208B(0,1) |Fj g| = Op (1). Let C1,j denote the jth element of\nC1 . Since P (|\u03b4n\u22121 C1,j | > M ) \u2264 P (supg\u2208B(0,1) |Fj g| > M ) + P (\u03b4n\u22121 \u2206 \u2208\n/ B(0, 1)), we obtain\nC1,j = Op (\u03b4n ) = op (1). One can prove C2 = op (1) using a truncation argument with Theorem 3 and applying the Chebyshev inequality conditioning on (Xi , Zi ). The fact that\nC3 = op (1) follows from P (Zji lies in [0, chj ) \u222a (1 \u2212 chj , 1]) = O(hj ) for some constant\n0 < c < \u221e and Theorem 3.\n\u0003\nProof of Theorem 5. We will show that \u03b2\u0303 \u2212 \u03b2 \u2217n = op (n\u22121/2 ). It suffices to show\nI\u02c6\u22121 n\u22121\n\nn\nX\ni=1\n\nX\u0303i \u03c6\u0302(\u01eb\u0302i ) = \u03b2\u0302 \u2212 \u03b2 0 + I \u22121 n\u22121\n\nn\nX\ni=1\n\n[Xi \u2212 \u03b7(Zi )]\u03c6(\u01ebi ) + op (n\u22121/2 ).\n\n(6)\n\nBy Theorem 3 and standard techniques of kernel smoothing along with assumption B3,\nit holds that, uniformly over i,\n\u03c6\u0302(\u01eb\u0302i ) = \u03c6\u0302(\u01ebi ) \u2212 X\u0303i\u22a4 (\u03b2\u0302 \u2212 \u03b2 0 )\u03c6\u0302\u2032 (\u01ebi ) \u2212 {m\u0302add (Zi ; \u03b2\u0302) \u2212 m0 (Zi )}\u03c6\u0302\u2032 (\u01ebi ) + op (n\u22121/2 ).\n\n(7)\n\n\f747\n\nSemi-parametric regression\n\nAlso, using the proof\n4.1 in [2] and standard calculus, one can show I\u02c6 =\nPn of Lemma\n\u22121\ni i\u22a4 \u2032 i\nI + op (1) and n\ni=1 X\u0303 X\u0303 \u03c6\u0302 (\u01eb ) = \u2212I + op (1). Thus, the proof of the theorem is\ncompleted if we verify\nn\u22121\n\nn\nX\ni=1\n\nn\u22121\n\nn\nX\ni=1\n\nX\u0303i {m\u0302add (Zi ; \u03b2\u0302) \u2212 m0 (Zi )}\u03c6\u0302\u2032 (\u01ebi ) = op (n\u22121/2 );\n\nX\u0303i \u03c6\u0302(\u01ebi ) \u2212 n\u22121\n\nn\nX\ni=1\n\n{Xi \u2212 \u03b7(Zi )}\u03c6(\u01ebi ) = op (n\u22121/2 ).\n\n(8)\n\n(9)\n\nProofs of (8) and (9) can be based on the following lemma, which follows from Corollary\n2.7.4 in [21] and assumption B2 on L. Note that the moment condition on \u01eb ensures the\nentropy bound. To state the lemma, define\n\u001b\n\u001a\n|f (x) \u2212 f (y)|\u03b1\n\u03b1\n\u2264M\nCM\n(X ) = f : X \u2192 R : sup |f (x)| + sup\n|x \u2212 y|\nx\nx,y\nfor a set X \u2282 R and a real number \u03b1 \u2208 (0, 1]. Let k * kg denote the L2 norm with respect\nto the density g.\nLemma 1. Assume the conditions of Theorem 5. Then there exists a constant M such\n1\nthat, with probability tending to one, b(a \u2227 b)\u03c6\u0302 \u2208 CM\n(R), [nhmax a6 b/(log n)2 ]1/2 (\u03c6\u0302 \u2212 \u03c6n ) \u2208\n1\n2\n2\n\u2032\n1\nCM (R) and b(a \u2227 b )\u03c6\u0302 \u2208 CM (R). Moreover, there exist constants \u03b4 > 0 and C1 > 0 such\n1\nthat log N[*] (\u03b7, CM\n(R), k * kg ) \u2264 C1 \u03b7 \u2212(2\u2212\u03b4) .\n\u0003\n\nAcknowledgement\nResearch of Kyusang Yu was supported in part by Basic Science Research Program\nthrough the National Research Foundation of Korea (NRF) funded by the Ministry of\nEducation, Science and Technology (2010-0023488). Research of Byeong U. Park was\nsupported by the Mid-career Researcher Program through NRF grant funded by the\nMEST (No. 2010-0017437).\n\nReferences\n[1] Bhattacharya, P. and Zhao, P. (1997). Semiparametric inference in a partial linear model.\nAnn. Statist. 25 244\u2013262. MR1429924\n[2] Bickel, P. (1982). On adaptive estimation. Ann. Statist. 10 647\u2013671. MR0663424\n[3] Bickel, P., Klaassen, A., Ritov, Y. and Wellner, J. (1993). Efficient and Adaptive Estimation\nfor Semiparametric Models. Baltimore, MD: Johns Hopkins Univ. Press. MR1245941\n[4] Carroll, R., Maity, A., Mammen, E. and Yu, K. (2009). Efficient semiparametric marginal\nestimation for the partially linear additive model for longitudinal/clustered data.\nStatist. Biosci. 1 10\u201331.\n\n\f748\n\nK. Yu, E. Mammen and B.U. Park\n\n[5] Cuzick, J. (1992). Efficient estimates in semiparametric additive regression models with\nunknown error distribution. Ann. Statist. 20 1129\u20131136. MR1165611\n[6] Fan, J. and Huang, T. (2005). Profile likelihood inferences on semiparametric varyingcoefficient partially linear models. Bernoulli 11 1031\u20131057. MR2189080\n[7] Koul, H.L. and Schick, A. (1997). Efficient estimation in nonlinear autoregressive time-series\nmodels. Bernoulli 3 247\u2013277. MR1468305\n[8] Liang, H., Thurston, S., Ruppert, D., Apanasovich, T. and Hauser, R. (2008). Additive\npartial linear models with measurement errors. Biometrika 95 667\u2013678.\n[9] Lu, Z., Lundervold, L., Tj\u00f8stheim, D. and Yao, Q. (2007). Exploring spatial nonlinearity\nusing additive approximation. Bernoulli 13 447\u2013472. MR2331259\n[10] Mammen, E., Linton, O. and Nielsen, J. (1999). The existence and asymptotic properties of\na backfitting projection algorithm under weak conditions. Ann. Statist. 27 1443\u20131490.\nMR1742496\n[11] Mammen, E. and Park, B.U. (2005). Bandwidth selection for smooth backfitting in additive\nmodels. Ann. Statist. 33 1260\u20131294. MR2195635\n[12] Murphy, S. and van der Vaart, A. (2000). On profile likelihood (with comments). J. Amer.\nStatist. Assoc. 95 449\u2013485. MR1803168\n[13] Opsomer, J. and Ruppert, D. (1999). A root-n consistent backfitting estimator for semiparametric additive modeling. J. Computat. Graph. Statist. 8 715\u2013732.\n[14] Park, B.U. (1990). Efficient estimation in the two sample semiparametric location-scale\nmodel. Probab. Theory Related Fields 86 21\u201339. MR1061946\n[15] Park, B.U. (1993). A cross-validatory choice of smoothing parameter in adaptive location\nestimation. J. Amer. Statist. Assoc. 88 848\u2013854. MR1242935\n[16] Schick, A. (1986). On asymptotically efficient estimation in semiparametric models. Ann.\nStatist. 14 1139\u20131151. MR0856811\n[17] Schick, A. (1993). On efficient estimation in regression models. Ann. Statist. 21 1486\u20131521.\nMR1241276\n[18] Severini, T. and Wong, W. (1992). Profile likelihood and conditionally parametric models.\nAnn. Statist. 20 1768\u20131802. MR1193312\n[19] Speckman, P. (1988). Kernel smoothing in partial linear models. J. Roy. Statist. Soc. Ser.\nB 50 413\u2013436. MR0970977\n[20] van de Geer, S. (2000) Empirical Processes in M-Estimation. Cambridge: Cambridge Univ.\nPress.\n[21] van der Vaart, A. and Wellner, J. (1996) Weak Convergence and Empirical Processes. With\nApplications to Statistics. New York: Springer. MR1385671\n[22] Wang, J. and Yang, L. (2009). Efficient and fast spline-backfitted kernel smoothing of\nadditive models. Ann. Inst. Statist. Math. 61 663\u2013690. MR2529970\n[23] Yu, K., Mammen, E. and Park, B.U. (2008). Smooth backfitting in generalized additive\nmodels. Ann. Statist. 36 228\u2013260. MR2387970\nReceived April 2010\n\n\f"}