{"id": "http://arxiv.org/abs/1004.3354v1", "guidislink": true, "updated": "2010-04-20T06:35:54Z", "updated_parsed": [2010, 4, 20, 6, 35, 54, 1, 110, 0], "published": "2010-04-20T06:35:54Z", "published_parsed": [2010, 4, 20, 6, 35, 54, 1, 110, 0], "title": "Parallel exact diagonalization solver for quantum-electron models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.4996%2C1004.1672%2C1004.0578%2C1004.4065%2C1004.0963%2C1004.1365%2C1004.2772%2C1004.4859%2C1004.3396%2C1004.2940%2C1004.4506%2C1004.3145%2C1004.3800%2C1004.1492%2C1004.0798%2C1004.2693%2C1004.4857%2C1004.3091%2C1004.1150%2C1004.5597%2C1004.0695%2C1004.3480%2C1004.0375%2C1004.2451%2C1004.5152%2C1004.1988%2C1004.1980%2C1004.4564%2C1004.0177%2C1004.5252%2C1004.1764%2C1004.2788%2C1004.0068%2C1004.1960%2C1004.3385%2C1004.0247%2C1004.1418%2C1004.0243%2C1004.1793%2C1004.2748%2C1004.2996%2C1004.4685%2C1004.1378%2C1004.4716%2C1004.1722%2C1004.3354%2C1004.4852%2C1004.4390%2C1004.0428%2C1004.1246%2C1004.2787%2C1004.3061%2C1004.2157%2C1004.4627%2C1004.4991%2C1004.4041%2C1004.0281%2C1004.0752%2C1004.5076%2C1004.0557%2C1004.1532%2C1004.0794%2C1004.1865%2C1004.5599%2C1004.4781%2C1004.2900%2C1004.4646%2C1004.1008%2C1004.3412%2C1004.5499%2C1004.3702%2C1004.2958%2C1004.1352%2C1004.4526%2C1004.1905%2C1004.4209%2C1004.2168%2C1004.3428%2C1004.4882%2C1004.3265%2C1004.0209%2C1004.1559%2C1004.0293%2C1004.1702%2C1004.0229%2C1004.3673%2C1004.4894%2C1004.4256%2C1004.1846%2C1004.2923%2C1004.4223%2C1004.2140%2C1004.0388%2C1004.3745%2C1004.3365%2C1004.3952%2C1004.0437%2C1004.4933%2C1004.3332%2C1004.3715%2C1004.3892&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Parallel exact diagonalization solver for quantum-electron models"}, "summary": "We present a parallel computation scheme based on the Arnoldi algorithm for\nexact diagonalization of quantum-electron models. It contains a selective data\ntransferring method and distributed storage format for efficient computing of\nthe matrix-vector product on distributed computing systems. The performed\nnumerical experiments demonstrated good performance and scalability of our\neigenvalue solver. The developed technique has been applied to investigate the\nelectronic properties of Sr2RuO4 at experimental temperatures. The role of the\nspin-flip term in the electronic Hamiltonian was analyzed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.4996%2C1004.1672%2C1004.0578%2C1004.4065%2C1004.0963%2C1004.1365%2C1004.2772%2C1004.4859%2C1004.3396%2C1004.2940%2C1004.4506%2C1004.3145%2C1004.3800%2C1004.1492%2C1004.0798%2C1004.2693%2C1004.4857%2C1004.3091%2C1004.1150%2C1004.5597%2C1004.0695%2C1004.3480%2C1004.0375%2C1004.2451%2C1004.5152%2C1004.1988%2C1004.1980%2C1004.4564%2C1004.0177%2C1004.5252%2C1004.1764%2C1004.2788%2C1004.0068%2C1004.1960%2C1004.3385%2C1004.0247%2C1004.1418%2C1004.0243%2C1004.1793%2C1004.2748%2C1004.2996%2C1004.4685%2C1004.1378%2C1004.4716%2C1004.1722%2C1004.3354%2C1004.4852%2C1004.4390%2C1004.0428%2C1004.1246%2C1004.2787%2C1004.3061%2C1004.2157%2C1004.4627%2C1004.4991%2C1004.4041%2C1004.0281%2C1004.0752%2C1004.5076%2C1004.0557%2C1004.1532%2C1004.0794%2C1004.1865%2C1004.5599%2C1004.4781%2C1004.2900%2C1004.4646%2C1004.1008%2C1004.3412%2C1004.5499%2C1004.3702%2C1004.2958%2C1004.1352%2C1004.4526%2C1004.1905%2C1004.4209%2C1004.2168%2C1004.3428%2C1004.4882%2C1004.3265%2C1004.0209%2C1004.1559%2C1004.0293%2C1004.1702%2C1004.0229%2C1004.3673%2C1004.4894%2C1004.4256%2C1004.1846%2C1004.2923%2C1004.4223%2C1004.2140%2C1004.0388%2C1004.3745%2C1004.3365%2C1004.3952%2C1004.0437%2C1004.4933%2C1004.3332%2C1004.3715%2C1004.3892&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present a parallel computation scheme based on the Arnoldi algorithm for\nexact diagonalization of quantum-electron models. It contains a selective data\ntransferring method and distributed storage format for efficient computing of\nthe matrix-vector product on distributed computing systems. The performed\nnumerical experiments demonstrated good performance and scalability of our\neigenvalue solver. The developed technique has been applied to investigate the\nelectronic properties of Sr2RuO4 at experimental temperatures. The role of the\nspin-flip term in the electronic Hamiltonian was analyzed."}, "authors": ["S. N. Iskakov", "V. V. Mazurenko"], "author_detail": {"name": "V. V. Mazurenko"}, "author": "V. V. Mazurenko", "links": [{"href": "http://arxiv.org/abs/1004.3354v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1004.3354v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "physics.comp-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.comp-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "F.2.1; J.2", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1004.3354v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1004.3354v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Parallel exact diagonalization solver for quantum-electron\nmodels\n\narXiv:1004.3354v1 [physics.comp-ph] 20 Apr 2010\n\nS. N. Iskakov, V. V. Mazurenko\nUral Federal University, Mira St. 19, 620002 Yekaterinburg, Russia\n\nAbstract\nWe present a parallel computation scheme based on the Arnoldi algorithm for exact diagonalization of quantum-electron models. It contains a selective data transferring method\nand distributed storage format for efficient computing of the matrix-vector product on\ndistributed computing systems. The performed numerical experiments demonstrated\ngood performance and scalability of our eigenvalue solver. The developed technique has\nbeen applied to investigate the electronic properties of Sr2 RuO4 at experimental temperatures. The role of the spin-flip term in the electronic Hamiltonian was analyzed.\nKeywords: DMFT, sparse matrices, matrix-vector product, distributed computing\n\n1. Introduction\nDynamical mean-field theory (DMFT) has been successfully used for studying the\nelectronic properties of a variety of strongly correlated materials in which the strength of\nthe electron-electron interaction energy is comparable to or larger than the kinetic one [1].\nSince these characteristics lead to a wealth of physical phenomena such as metal insulator\ntransitions, exotic magnetic structures, and unconventional superconducting phases [2],\nthe DMFT method attracts a great deal of attention. The main underlying idea of this\napproach is the mapping of the lattice problem onto an effective impurity problem which\nin turn is solved numerically exactly. For that one can use different numerical techniques\nsuch as exact diagonalization (ED), numerical renormalization group (NRG), quantum\nMonte Carlo method with the Hirsch-Fye algorithm (QMC-HF), or other schemes [1].\nMany interesting and promising results were obtained by using QMC methods. For\ninstance, a realistic five-band modeling of the strongly correlated materials is mainly\nperformed by using QMC Hirsch-Fye method. However, there is a number of technical\nlimitations. (i) The simulation temperature of 250 K - 1000 K is much higher than\nexperimental one of 10 K - 100 K. (ii) The Coulomb interaction matrix contains densitydensity terms only. (iii) The sign problem prevents scientists from calculating the physical\nproperties at sufficiently low temperatures [3]. (iv) The QMC approaches require a\nmaximum entropy method for analytical continuation of the resulting Green's function\nto real-energy axis.\nEmail addresses: iskakoff@ngs.ru (S. N. Iskakov), mvv@dpt.ustu.ru (V. V. Mazurenko)\nPreprint submitted to Journal of Computational Physics\nOctober 31, 2018\n\n\fSuch problems can be solved by using the ED techniques. The full diagonalization\nmethod is limited to three-band systems because of the extremely rapid increase of the\nHilbert space with the number of the electronic states Ns . For instance, full diagonalization of the Hamiltonian with two impurity orbitals, each coupled to three bath levels\n(Ns = 8), requires diagonalization of matrices with dimension up to 4900 which roughly\nrepresents the time and storage limit of what is computationally meaningful.\nThe diagonalization problem can be simplified by exploiting the extreme sparseness\nof the Hamiltonian matrices and focusing on the limited number of the states with the\nlowest energies. It can be done by using Lanczos [4] or Arnoldi [5] approaches. As a\nresult the Hamiltonian corresponding to Ns = 12 can be treated at about the same\ncomputational cost as the full diagonalization for Ns = 8. This improvement allows the\napplication of ED/DMFT to realistic three-band and five-band systems [5]. However,\nthere is memory limitation of the Hamiltonain matrix storage to apply ED/DMFT to\nmore complicate physical systems. To date the biggest matrix that can be diagonalized\nby this approach on a modern workstation is about 64352 \u00d7 64352 which corresponds to\nNs =15 [6].\nIn this paper we present a sparse matrix-vector product (SpMV) parallelization strategy for effective solving ED/DMFT equations on distributed computing systems. For\nthese purposes we suggest two improvements of the original ED/DMFT method [1]: a\ndistributed storage format for sparse large matrix and a selective data transferring. The\nformer allows the application of the ED/DMFT method for system with the number\nof electronic states more than Ns = 15. The selective data transferring method substantially reduces amount of data transmitted between the processors of a distributed\ncomputing system, which improves SpMV performance.\nThe paper organized as follows. In section 2, we presents the basic steps of the\nDMFT theory. In section 3, the developed eigenvalue solver and its parallel performance\nare given. We also give approbation of our computational scheme for a one-band Hubbard\nmodel on square lattice. For purpose of illustration in section 4 we will use a three-band\nmodel for Sr2 RuO4 compound.\n2. Theory\nOne way to describe many-particle phenomena such as high-temperature superconductivity, heavy fermions and others is to solve the Hubbard Hamiltonian [7]\nX\nX\nni\u2191 ni\u2193\n(1)\ntij c+\nH =\u2212\ni\u03c3 cj\u03c3 + U\ni\n\ni,j,\u03c3\n\nwhere c+\ni\u03c3 (ci\u03c3 ) is creation (annihilation) operators of fermion, tij is a hopping integral,\nU is the on-site local Coulomb interaction, ni is the particle number operator. It is a\nvery complicated problem due to the exponential growth of the Hamiltonian and, hence,\nthe direct solution of the Hubbard Hamiltonian is only possible for small model clusters\n[8].\nIn case of the real systems one can use the Dynamical Mean-Field Theory [1] in which\nthe lattice problem is reduced to a single-site effective problem. Thus, such a mean-field\nconsideration leads to freezing spatial correlations. Instead of the Hubbard Hamiltonian\n2\n\n\fwe solve the Anderson impurity model which is given by\nHAIM =\n\nX\nk,\u03c3\n\nNk\n\u0002\n\u0003 X\n+\nVkd\u03c3 d+\nc\n+\nc\nd\n+\n\u03b5k\u03c3 c+\nk\u03c3\n\u03c3\n\u03c3\nk\u03c3\nk\u03c3 ck\u03c3 + \u03b5d (n\u2191 + n\u2193 ) + U n\u2191 n\u2193 ,\n\n(2)\n\nk,\u03c3\n\n+\nwhere Nk = Ns \u2212 1 is a number of the bath states, d+\n\u03c3 (d\u03c3 ) and ck\u03c3 (ck\u03c3 ) are creation\n(annihilation) operators for fermions with spin \u03c3 associated with the impurity site and\nwith the state k of the effective bath, respectively, \u03b5k and \u03b5d are the energy of bath and\nimpurity, Vkd is a hybridisation integral of impurity and bath states. The main task\nof the ED/DMFT is to diagonalize the impurity Hamiltonian HAIM to compute the\nimpurity Green function which can be written as\n\nG (i\u03c9n ) =\n\n1\nZ\n\n2\nkh\u03bckc+\n\u03c3 k\u03bdik\n\u03bd\u03bc E\u03bd \u2212E\u03bc\u2212i\u03c9n\n\nP\n\n\u0002\n\n\u0003\ne\u2212\u03b2E\u03bd + e\u2212\u03b2E\u03bc ,\n\n(3)\n\nwhere\n| \u03bdi (| \u03bci) and E\u03bd (E\u03bc ) are eigenvectors and eigenvalues of HAIM and Z =\nP\nexp(\u2212\u03b2E\n\u03bc ) is the partition function. The exponential factor in (3) indicates that at\n\u03bc\nlarge enough \u03b2 only a small number of the lowest eigenstates needs to be calculated [4].\nIn this paper, we develop a high performance parallel technique for solving the eigenvalue problem of the Anderson impurity model Hamiltonian matrix (2) on distributed\ncomputing systems.\n3. Distributed eigenvalue solver\nA few iterative numerical algorithms such as a power method, a Lanczos method, an\nimplicitly restarted Arnoldi method [9], the conjugate gradient (CG) method [10] have\nbeen proposed to solve the eigenvalue problem for large sparse matrices. Most of them\nare based on iterative multiplication of the Hamiltonian matrix H by the trial vector \u03bd.\nThus they are ideally suited for parallelization.\nAn efficient implementation of the Lanczos method for symmetric multiprocessing\ncomputers with large shared memory was proposed in Ref.[11] for solving the Heisenberg model. The authors have demonstrated a perfect scalability of their computational\nscheme. It was possible since they used a SMP machine with eight cores which were\nsitting on the same board. As we will show below it is not the case for a distributed\narchitecture.\nRecently, the ED results for a five-band DMFT problem were reported by Liebsch\n[6]. Judging by a short description of the technical details they have also used a SMP\nmachine with large shared memory and 32 processors on the board. The aim of our work\nis to modify the exact diagonalization method for solving DMFT equations on distributed\ncomputing systems.\n3.1. Distributed CSR format and selective data transferring\nSince the Hamiltonian matrix H is extremely sparse the performance of the H \u00d7 \u03bd\noperation strongly depends on the storage format and SpMV algorithm we use. The\nmemory subsystem and, more specifically, the memory bandwidth is identified as the\nmain performance bottleneck of the SpMV routine [12, 13]. This performance bottleneck\nis due to the fact that SpMV performs O(nnz) operations where nnz is a number of the\n3\n\n\fnonzero elements of the sparse matrix, which means that most of the data are accessed in\na streaming manner and there is little temporal locality. To avoid indirect memory access\nsome sparse matrix storage format is used. One of the most widely used storage format\nfor sparse matrices is Compressed Sparse Row (CSR) format [14, 15] which stores all the\nnon-zero elements of the Hamiltonian in contiguous memory locations (Hvalues array)\nand uses two additional arrays for indexing information: row ind contains the start of\neach row within the non-zero elements array and col ind contains the column number\nassociated with each non-zero element. The size of the Hvalues and col ind arrays are\n\uf8eb\n\uf8f6\nh00\n0\n0\n0 h04\n0\n\uf8ec 0 h11\n0\n0\n0 h15 \uf8f7\n\uf8ec\n\uf8f7\n\uf8ec 0\n0 h22\n0\n0\n0 \uf8f7\n\uf8f7\nH=\uf8ec\n\uf8ec 0\n0\n0 h33\n0\n0 \uf8f7\n\uf8ec\n\uf8f7\n\uf8edh40\n0\n0\n0 h44\n0 \uf8f8\n0 h51\n0\n0\n0 h55\n\n\u21d3\n\nHvalue\n\nrow ind = ( 0 2 4 5 6 8 10 )\ncol ind = ( 0 4 1 5 2 3 0 4 1 5 )\n= ( h00 h04 h11 h15 h22 h33 h40 h44\n\nh51 h55\n\n)\n\nFigure 1: Example of CSR storage format. The original matrix H is compressed into three arrays\nrow ind, col ind and Hvalue .\n\nequal to the number of the non-zero elements (nnz), while the size of the row ind array\nis equal to the number of rows (nrows) plus one. An example of the CSR format for\na sparse 6 \u00d7 6 matrix is presented in Figure 1. In order to organize the matrix-vector\nmultiplication operation one can use the following listing (Listing 1).\nListing 1: Sparse Matrix-Vector Multiplication using CSR format\n\n1\n2\n3\n\nfor ( i =0; i <Hdim ; i ++)\nfor ( j=r o w i n d [ i ] ; j <r o w i n d [ i + 1 ] ; j ++)\ny [ i ] += H[ j ] \u2217 v [ c o l i n d [ j ] ] ;\n\nIn the framework of the standard SpMV procedure with the standard CSR format\nall three arrays and vector should be stored on the same board. It is impossible when\nthe size of the Hamiltonian is larger than the memory of a single machine. Therefore\nthe first goal of our work is to design method for arrays distribution over CPU mesh to\novercome the problem of large matrix storage.\nWe use the standard CSR storage format as a starting point. The compressed Hamiltonian as well as the accompanied arrays are distributed over processors grid. It reduces\nrequired memory space of a single CPU (table 1). The Hamiltonian matrix is stored\ndistributively all the time. For this purpose we have developed the library written in\nC++ for initialization and working with different elements of distributed arrays.\nThe second goal of our work is to organize effective and optimal communications\nbetween processors to reduce amount of transmitted data. To solve this problem we\nhave developed communication procedures for operating elements of the matrix stored\ndistributively. The subroutines can be divided into two classes. The first one is used for\n4\n\n\fFigure 2: Example of Distributed CSR storage format running on 3 CPU. The arrows denote the\ncommunications between CPUs.\n\nexchanging data between CPUs based on one-sided communication functions of MPI. For\noptimal performance and memory usage we used MPI Get function, because in case of\nexchanging large amount of data MPI Put subroutine needs significantly more memory.\nThe second class of the subroutines is directed to determine groups of interacting CPUs.\nIn a sense the main advantage of the distributed CRS format we proposed is that each\nCPU is to communicate with only a few CPUs to receive the different parts of the Arnoldi\nvector during simulation. For example, while solving eigenvalue problem for the matrix\nwith the size of 165, 636, 900 only about 30 of a 256 CPUs are needed for each CPU to\nreceive required parts of the vector.\nThe figure 2 shows an example of distributed CSR format for 6 \u00d7 6 matrix in case of\nthree processors. One can see that the original compressed matrix (Fig.1) and the array\ncol idx is distributed into 3 arrays. Since the matrix-vector multiplication each CPU\nneeds only a small part of the whole vector then the index array can be considerably\nreduced. Thus the approach we proposed naturally leads to reduction of single CPU\nmemory requirement for the trial vector storage. CPU 1 and CPU 3 are to transmit the\ndifferent parts of the vector to each other.\n3.2. Performance\nTo test the performance of the developed technique we have performed the ED/DMFT\ncalculations for a one-band Hubbard model on a square lattice with the on-site Coulomb\ninteraction U = 2 eV and the nearest hopping integral t = 0.5 eV (Fig.3). The calculations\nwere carried out for Ns =16. The resulting Green function is presented in Fig. 3. One\ncan see that there is the famous three peaks structure (quasiparticle peak, lower and\nupper Hubbard bands) that agrees with results of the previous theoretical investigations\n[1].\nTable 1 shows the performance of the modified ED/DMFT calculation scheme on Np\n= 32, 64, 128, 256, and 512 processors. These performance measurements were made\nas follows. The total elapsed time and time of communications between processors were\nmeasured by MPI Wtime function. The required memory was measured by the tools\ninstalled on a particular parallel cluster system.\nFig.4 shows the scaling of the computation time for 1 ED/DMFT iteration with a\nmaximum vector length of 11,778,624. One can see that the speedup of our calculation\nscheme is far from the ideal scaling. The main reason for that is time required for\ncommunication between different CPUs. Another operation of our computational scheme\nwhich takes much time is the construction of the Hamiltonian matrix (Table 1).\nWe can also compare the speedup of our algorithm in case of the different sizes of\nthe Hamiltonian matrix. For Ns =16 the ratio T256 /T512 = 1.58 is larger and closer to\n5\n\n\fFigure 3: (left) Schematic representation of the square lattice with the nearest neighbors couplings.\n(right) Spectral functions obtained from ED/DMFT calculations with Ns =16 (gray bold line). The\ndashed brown line corresponds to the non-interacting density of states.\n\nthe ideal one than T32 /T64 =1.26 in case of Ns =14. Based on the obtained performance\ndata we can estimate the number of processors we need for Ns = 18 and Ns =20. For\ninstance, in case of Ns = 18, for the largest sector one needs to store about the 82 billions\nof non-zero elements. It requires about 1.5 Tb of memory and can be solved by using\n2000 cores cluster. Investigation of the systems with Ns = 20 leads to operating about\n1194 billions of the non-zero elements and it is impossible to solve this problem using\nour scheme.\nIt is interesting to compare the performance of the developed technique with that\nreported in Ref. [16]. For Ns =12 the authors of the paper have estimated that the time\nof the one iteration with 300 lowest eigenstates was less than 30 min if 16 processors\nwere employed in parallel. In our case the similar calculations take us about 45-50 min.\nSuch a time difference can be explained by taking into account that the author used a\nworkstation with shared memory and processors on the same board.\n\nFigure 4: Scaling of the total computation time for 1 ED/DMFT iteration with number of used CPUs.\nThe number of CPU in the speedup dependence (right) is normalized by 32.\n\nIt is important to discuss the possible ways to improve our computation scheme.\nFirstly, The results of the previous work [17] have demonstrated that it would be better\nnot to store the Heisenberg matrix, but to evaluate the matrix elements whenever needed.\nSecondly, since we divide the full Hamiltonian matrix into sub-matrices of equal number\nof the elements there is an irregular distribution of the non-zero elements. It results\nin unequal partition of workloads among processors. This load-imbalance problem can\n6\n\n\fTable 1: Performance of the developed eigenvalue solver. Np is a number of processors and nnzmax is a\nmaximum number of the non-zero elements per row, Mexch and Mdata are amount of transferred data\nper processor and memory requirement per processor (in MB). Ttotal , Tcomm and Tsetup are the total\ncomputational, communication time and time for generating Hamiltonian matrix (in sec.), respectively.\n\nNs\n14\n14\n14\n14\n14\n16\n16\n\nHdim\n11,778,624\n11,778,624\n11,778,624\n11,778,624\n11,778,624\n165,636,900\n165,636,900\n\nnnzmax\n13\n13\n13\n13\n13\n15\n15\n\nNp\n32\n64\n128\n256\n512\n256\n512\n\nMexch\n23\n22\n15\n7\n4\n120\n54\n\nMdata\n300\n150\n75\n40\n20\n500\n270\n\nTsetup\n17\n10\n3.2\n1.7\n1.0\n52\n25\n\nTcomm\n66\n71\n47\n35\n22\n330\n200\n\nTtotal\n188\n149\n74\n50\n34\n602\n382\n\nbe solved by partitioning the Hamiltonian matrix in a computational space where the\noptimal number of the non-zero elements is defined to minimize the load-balance and\ncommunication costs.\n4. Numerical results\nUsing the developed exact diagonalization technique we have investigated the electronic structure of Sr2 RuO4 compound. This system is of interest due to a number\nof unconventional physical properties. For instance, Sr2 RuO4 demonstrates an unconventional superconductivity at temperatures below 1 K. Moreover, photoemission and\noptical experiments have shown substantial mass renormalization effects. In the previous theoretical investigations the electronic structure of Sr2 RuO4 was studied within\nLDA+DMFT method using the QMC-HF as the impurity solver. For instance, Liebsch\nand Lichtenstein [18] have performed multiband DMFT calculations to explain the discrepancy between photoemission and de Haas-van Alphen data. It was found that there\nis a charge transfer from 3dxz,yz states to 3dxy states. The similar QMC-HF method was\nused by the authors of the paper [19] to describe some features of photoemission spectra.\nIn previous investigations the lowest simulation temperature was 15 meV, which is\nlarger those at which experiments were performed. The imaginary time QMC data were\nanalytically continued by the maximum entropy method. Since the QMC-HF/DMFT\nmethod operates only with density-density interaction terms the spin-flip term in the\nimpurity Hamiltonian was not taken into account. At the same time the effect of such\nan interaction on electron spectra was mentioned in a number of works [18].\nTo take into account the spin-flip effects in our ED/DMFT calculation we have diagonalized the following impurity Hamiltonian:\nX\nX\nX\nX\nH=\n(\u01ebm \u2212 \u03bc)nm\u03c3 +\n\u01ebk nk\u03c3 +\nVmk [d+\nU nm\u2191 nm\u2193\nm\u03c3 ck\u03c3 + H.c.] +\nm\u03c3\n\n+\n\nX\n\nm<m\u2032 \u03c3\u03c3\u2032\n\nk\u03c3\n\n\u2032\n\n(U \u2212 J\u03b4\u03c3\u03c3\u2032 )nm\u03c3 nm\u2032 \u03c3\u2032 \u2212\n\nm\n\nmk\u03c3\n\nX\n\nJ\n\n\u2032\n\n+\n[d+\nm\u2191 dm\u2193 dm\u2032 \u2193 dm\u2032 \u2191\n\nm6=m\u2032\n\n7\n\n+\n\n+\nd+\nm\u2191 dm\u2193 dm\u2032 \u2191 dm\u2032 \u2193 ].\n\n(4)\n\n\f(+)\n\nHere dm\u03c3 are annihilation (creation) operators for impurity electrons on orbital m with\nspin \u03c3, U (U \u2032 ) is the intra-orbital (inter-orbital) impurity Coulomb energy, J is an intraatomic exchange integral and J \u2032 is the spin-flip interaction. The account of the latter\ninteraction is possible in the framework of the exact diagonalization and recently developed CT-QMC calculation scheme. In according with Ref.[18] these parameters were\nchosen to be U = U \u2032 = 1.2 eV, J = 0.2 eV and J \u2032 = 0.1 eV. The simulation temperature\nwas taken to be T =200 K that is close to the experimental values used in Ref.[18]. Thus\nfor a total of about 20 excited states the Greens functions are calculated via the Lanczos\nprocedure. Of course, this number increases at higher temperatures, and if Boltzmann\nfactors smaller than 105 are included for higher precision.\nThe resulting Green functions obtained for Ns =15 is presented in Fig.5. The spectral\nfunctions without the spin-flip term agree well with those presented in Ref. [18]. One\ncan see that the account of the spin-flip term leads to an additional renormalization of\nthe spectrum near the Fermi level. It means that other non-diagonal elements of the\n\nFigure 5: Densities of states obtained from DMFT calculations for T = 200 K with (brown and blue\nlines) and without (gray lines) the spin-flip term.\n\nfull U-matrix can play an important role for correct description of the experimentally\nobserved spectra. The approach we used here is also well suited for analyzing the ground\nand few excited states. Such an analysis can help us to study the experimentally observed\nsuperconducting phase.\n5. Conclusions\nTo conclude, we developed a distributed storage format and a selective data transferring procedure for distributed sparse matrix-vector multiplication. They were implemented in ED solver for dynamic mean field theory problem and showed good speedup\nand scalability. The obtained numerical results look quite promising. The developed approach can be used for solving Heisenberg model on computer clusters with distributed\nmemory.\n6. ACKNOWLEDGMENTS\nWe would like to thank A.O. Shorikov for his technical assistance with test calculations. This work is supported by the scientific program \"Development of scientific\npotential of universities\" N 2.1.1/779, by the scientific program of the Russian Federal\n8\n\n\fAgency of Science and Innovation N 02.740.11.0217, the grant program of President of\nRussian Federation MK-1162.2009.2., RFFI 10-02-00546. The calculations have been\nperformed on the cluster of University Center for Parallel Computing of Ural Federal\nUniversity and the Brutus cluster at ETH Zurich.\nReferences\n[1] A. Georges, G. Kotliar, W. Krauth, M. J. Rozenberg, Dynamical mean-field theory of strongly\ncorrelated fermion systems and the limit of infinite dimensions, Rev. Mod. Phys. 68 (1) (1996) 13.\n[2] M. Imada, A. Fujimori, Y. Tokura, Metal-insulator transitions, Rev. Mod. Phys. 70 (4) (1998)\n1039\u20131263.\n[3] E. Gorelov, T. O. Wehling, A. N. Rubtsov, M. I. Katsnelson, A. I. Lichtenstein, Relevance of the\ncomplete Coulomb interaction matrix for the Kondo problem: Co impurities in Cu hosts, Phys.\nRev. B 80 (15) (2009) 155132.\n[4] M. Capone, L. de' Medici, A. Georges, Solving the dynamical mean-field theory at very low temperatures using the Lanczos exact diagonalization, Physical Review B (Condensed Matter and Materials\nPhysics) 76 (24) 245116.\n[5] C. A. Perroni, H. Ishida, A. Liebsch, Exact diagonalization dynamical mean-field theory for multiband materials: Effect of Coulomb correlations on the Fermi surface of N a0.3 CoO2 , Physical Review\nB (Condensed Matter and Materials Physics) 75 (4) 045125.\n[6] H. Ishida, A. Liebsch, Fermi-liquid, non-Fermi-liquid, and Mott phases in iron pnictides and\ncuprates, Phys. Rev. B 81 (5) (2010) 054513.\n[7] J. Hubbard, Electron Correlations in Narrow Energy Bands, Proceedings of the Royal Society of\nLondon. Series A. Mathematical and Physical Sciences 276 (1365) (1963) 238\u2013257.\n[8] S. Yamada, T. Imamura, M. Machida, 16.447 TFlops and 159-Billion-dimensional Exactdiagonalization for Trapped Fermion-Hubbard Model on the Earth Simulator, in: SC '05: Proceedings of the 2005 ACM/IEEE conference on Supercomputing, IEEE Computer Society, Washington,\nDC, USA, ISBN 1-59593-061-2, 44, 2005.\n[9] K. Maschhoff, D. Sorensen, A portable implementation of ARPACK for distributed memory parallel\narchitectures, URL citeseer.ist.psu.edu/maschhoff96portable.html, 1996.\n[10] M. R. Hestenes, E. Stiefel, Methods of Conjugate Gradients for Solving Linear Systems, Journal of\nResearch of the National Bureau of Standards 49 (6) (1952) 409\u2013436.\n[11] J. Schnack, P. Hage, H.-J. Schmidt, Efficient implementation of the Lanczos method for magnetic\nsystems, J. Comput. Phys. 227 (9) (2008) 4512\u20134517, ISSN 0021-9991.\n[12] K. Kourtis, G. Goumas, N. Koziris, Optimizing sparse matrix-vector multiplication using index and\nvalue compression, in: CF '08: Proceedings of the 5th conference on Computing frontiers, ACM,\nNew York, NY, USA, ISBN 978-1-60558-077-7, 87\u201396, 2008.\n[13] W. K. Anderson, W. D. Gropp, D. K. Kaushik, D. E. Keyes, B. F. Smith, Achieving high sustained\nperformance in an unstructured mesh CFD application, in: Supercomputing '99: Proceedings of\nthe 1999 ACM/IEEE conference on Supercomputing (CDROM), ACM, New York, NY, USA, ISBN\n1-58113-091-0, 69, 1999.\n[14] J. Mellor-crummey, J. Garvin, Optimizing Sparse Matrix-Vector Product Computations Using Unroll and Jam, International Journal of High Performance Computing Applications 18 (2003) 2004.\n[15] E.-J. Im, Optimizing the Performance of Sparse Matrix-Vector Multiplication, Ph.D. thesis, EECS\nDepartment, University of California, Berkeley, 2000.\n[16] A. Liebsch, N.-H. Tong, Finite-temperature exact diagonalization cluster dynamical mean-field\nstudy of the two-dimensional Hubbard model: Pseudogap, non-Fermi-liquid behavior, and particlehole asymmetry, Phys. Rev. B 80 (16) (2009) 165126.\n[17] J. Schnack, P. Hage, H.-J. Schmidt, Efficient implementation of the Lanczos method for magnetic\nsystems, Journal of Computational Physics 227 (9) (2008) 4512 \u2013 4517, ISSN 0021-9991.\n[18] A. Liebsch, A. Lichtenstein, Photoemission Quasiparticle Spectra of Sr2RuO4, Phys. Rev. Lett.\n84 (7) (2000) 1591\u20131594.\n[19] Z. V. Pchelkina, I. A. Nekrasov, T. Pruschke, A. Sekiyama, S. Suga, V. I. Anisimov, D. Vollhardt,\nEvidence for strong electronic correlations in the spectra of Sr2RuO4, Phys. Rev. B 75 (3) (2007)\n035122.\n\n9\n\n\f"}