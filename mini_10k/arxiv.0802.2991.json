{"id": "http://arxiv.org/abs/0802.2991v2", "guidislink": true, "updated": "2008-07-11T04:29:03Z", "updated_parsed": [2008, 7, 11, 4, 29, 3, 4, 193, 0], "published": "2008-02-21T08:35:26Z", "published_parsed": [2008, 2, 21, 8, 35, 26, 3, 52, 0], "title": "Test of Influence from Future in Large Hadron Collider; A Proposal", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0802.3839%2C0802.2296%2C0802.0188%2C0802.1192%2C0802.3232%2C0802.0349%2C0802.0666%2C0802.2321%2C0802.0318%2C0802.0395%2C0802.1972%2C0802.2483%2C0802.2452%2C0802.1481%2C0802.3476%2C0802.0950%2C0802.2088%2C0802.0600%2C0802.2938%2C0802.2620%2C0802.1895%2C0802.0075%2C0802.2107%2C0802.1440%2C0802.2233%2C0802.3935%2C0802.3365%2C0802.0532%2C0802.1055%2C0802.3654%2C0802.2044%2C0802.1917%2C0802.1992%2C0802.2991%2C0802.0455%2C0802.2094%2C0802.3215%2C0802.4388%2C0802.4030%2C0802.1843%2C0802.2110%2C0802.3921%2C0802.0194%2C0802.0410%2C0802.1560%2C0802.2561%2C0802.4231%2C0802.0538%2C0802.1773%2C0802.3593%2C0802.2934%2C0802.2602%2C0802.1775%2C0802.1478%2C0802.2275%2C0802.3470%2C0802.2960%2C0802.1774%2C0802.3884%2C0802.2781%2C0802.4029%2C0802.3032%2C0802.2674%2C0802.0836%2C0802.3013%2C0802.4326%2C0802.2185%2C0802.1727%2C0802.1042%2C0802.0526%2C0802.0681%2C0802.1459%2C0802.2948%2C0802.0403%2C0802.1320%2C0802.4171%2C0802.1443%2C0802.2757%2C0802.0517%2C0802.3014%2C0802.1130%2C0802.3455%2C0802.1014%2C0802.3689%2C0802.3985%2C0802.2555%2C0802.4260%2C0802.3677%2C0802.0609%2C0802.3500%2C0802.1783%2C0802.1515%2C0802.1595%2C0802.3795%2C0802.0261%2C0802.3573%2C0802.4135%2C0802.2401%2C0802.1057%2C0802.3081%2C0802.0154&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Test of Influence from Future in Large Hadron Collider; A Proposal"}, "summary": "We have earlier proposed the idea of making card drawing experiment of which\noutcome potentially decides whether Large Hadron Collider (LHC for short)\nshould be closed or not. The purpose is to test theoretical models which, like\ne.g. our own model that has an imaginary part of the action with much a similar\nform to that of the real part. The imaginary part has influence on the initial\nconditions not only in the past but even from the future. It was speculated\nthat all accelerators producing large amounts of Higgs particles like the\nSuperconducting Super Collider (SSC for short) would call for initial\nconditions to have been so arranged as to finally not allow these accelerators\nto come to work. If there were such effects we could perhaps provoke a very\nclear cut \"miracle\" by having the effect make the drawn card be the one closing\nLHC. Here we shall, however, discuss that a total closing is hardly needed and\nseek to calculate how one could perform checking experiment for the proposed\ntype of influence from future to be made in the statistically least disturbing\nand least harmful way.\n  We shall also discuss how to extract most information about our effect or\nmodel in the unlikely case that a card restricting the running of LHC or the\nTevatron would be drawn at all, by estimating say the relative importance of\nhigh beam energy or of high luminosity for the purpose of our effect.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0802.3839%2C0802.2296%2C0802.0188%2C0802.1192%2C0802.3232%2C0802.0349%2C0802.0666%2C0802.2321%2C0802.0318%2C0802.0395%2C0802.1972%2C0802.2483%2C0802.2452%2C0802.1481%2C0802.3476%2C0802.0950%2C0802.2088%2C0802.0600%2C0802.2938%2C0802.2620%2C0802.1895%2C0802.0075%2C0802.2107%2C0802.1440%2C0802.2233%2C0802.3935%2C0802.3365%2C0802.0532%2C0802.1055%2C0802.3654%2C0802.2044%2C0802.1917%2C0802.1992%2C0802.2991%2C0802.0455%2C0802.2094%2C0802.3215%2C0802.4388%2C0802.4030%2C0802.1843%2C0802.2110%2C0802.3921%2C0802.0194%2C0802.0410%2C0802.1560%2C0802.2561%2C0802.4231%2C0802.0538%2C0802.1773%2C0802.3593%2C0802.2934%2C0802.2602%2C0802.1775%2C0802.1478%2C0802.2275%2C0802.3470%2C0802.2960%2C0802.1774%2C0802.3884%2C0802.2781%2C0802.4029%2C0802.3032%2C0802.2674%2C0802.0836%2C0802.3013%2C0802.4326%2C0802.2185%2C0802.1727%2C0802.1042%2C0802.0526%2C0802.0681%2C0802.1459%2C0802.2948%2C0802.0403%2C0802.1320%2C0802.4171%2C0802.1443%2C0802.2757%2C0802.0517%2C0802.3014%2C0802.1130%2C0802.3455%2C0802.1014%2C0802.3689%2C0802.3985%2C0802.2555%2C0802.4260%2C0802.3677%2C0802.0609%2C0802.3500%2C0802.1783%2C0802.1515%2C0802.1595%2C0802.3795%2C0802.0261%2C0802.3573%2C0802.4135%2C0802.2401%2C0802.1057%2C0802.3081%2C0802.0154&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We have earlier proposed the idea of making card drawing experiment of which\noutcome potentially decides whether Large Hadron Collider (LHC for short)\nshould be closed or not. The purpose is to test theoretical models which, like\ne.g. our own model that has an imaginary part of the action with much a similar\nform to that of the real part. The imaginary part has influence on the initial\nconditions not only in the past but even from the future. It was speculated\nthat all accelerators producing large amounts of Higgs particles like the\nSuperconducting Super Collider (SSC for short) would call for initial\nconditions to have been so arranged as to finally not allow these accelerators\nto come to work. If there were such effects we could perhaps provoke a very\nclear cut \"miracle\" by having the effect make the drawn card be the one closing\nLHC. Here we shall, however, discuss that a total closing is hardly needed and\nseek to calculate how one could perform checking experiment for the proposed\ntype of influence from future to be made in the statistically least disturbing\nand least harmful way.\n  We shall also discuss how to extract most information about our effect or\nmodel in the unlikely case that a card restricting the running of LHC or the\nTevatron would be drawn at all, by estimating say the relative importance of\nhigh beam energy or of high luminosity for the purpose of our effect."}, "authors": ["Holger B. Nielsen", "Masao Ninomiya"], "author_detail": {"name": "Masao Ninomiya"}, "author": "Masao Ninomiya", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1142/S0217751X09041524", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0802.2991v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0802.2991v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "33 pages", "arxiv_primary_category": {"term": "physics.gen-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.gen-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "hep-th", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0802.2991v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0802.2991v2", "journal_reference": "Int.J.Mod.Phys.24:3945-3968,2009", "doi": "10.1142/S0217751X09041524", "fulltext": "CERN-PH-TH/2008-035\nYITP-07-99\n\narXiv:0802.2991v2 [physics.gen-ph] 11 Jul 2008\n\nOIQP-07-20\n\nTest of Effect from Future in Large Hadron Collider;\nA Proposal\nHolger B. Nielsen 1\nThe Niels Bohr Institute, University of Copenhagen,\nCopenhagen \u03c6, DK2100, Denmark\nand\n\nMasao Ninomiya 2\nYukawa Institute for Theoretical Physics,\nKyoto University, Kyoto 606-8502, Japan\nPACS numbers: 12.90.tb, 14.80.cp, 11.10.-z\nKeywords: Backward causation, Initial condition model, LHC, Higgs particle\nAbstract\nWe have previously proposed the idea of performing a card-drawing experiment of which the outcome potentially decides whether the Large Hadron\nCollider (LHC) should be closed or not. The purpose is to test theoretical\nmodels such as our own model that have an action with an imaginary part\nthat has a similar form to the real part. The imaginary part affects the initial\nconditions not only in the past but even from the future. It was speculated\nthat all the accelerators producing large amounts of Higgs particles such as\nthe Superconducting Super Collider (SSC) would mean that the initial conditions must have been arranged so as not to allow these accelerators to work.\nIf such effects existed, we could perhaps cause a very clear-cut \"miracle\" by\nhaving the effect of a drawn card to be the closure of the LHC. Here we shall,\nhowever, argue that the closure of an accelerator is hardly needed to demonstrate such an effect and seek to calculate how one could perform a verification\n1\n2\n\nOn leave of absence to CERN, Geneva from 1 Aug. 2007 to 31 March 2008.\nAlso working at Okayama Institute for Quantum Physics, Kyoyama 1, Okayama 700-0015,\n\nJapan.\n\n1\n\n\fexperiment for the proposed type of effect from the future in the statistically\nleast disturbing and least harmful way.\nWe shall also discuss how to extract the maximum amount of information\nabout such as effect or model in the unlikely case that a card preventing\nthe running of the LHC or the Tevatron is drawn, by estimating the relative\nimportance of high beam energy or high luminosity for the purpose of our\neffect.\n\n1\n\nIntroduction\nEach time an accelerator is used to investigate a hitherto uninvestigated regime\n\nsuch as collision energy or luminosity, there is, a priori, a chance of finding new effects\nthat, in principle, could mean that a well-established principle could be violated,\nin lower-energy physics or in daily life. The present paper is one of a series of\narticles [1\u20135] discussing how one might use the LHC and perhaps the Tevatron to\nsearch for effects violating the following well-established principle while the future\nis very much influenced by the past, the future does not influence the past. Perhaps\nwe can more precisely state this principle, which we propose to test at the new\nLHC accelerator, as follows: While we find that there is a lot of structure from the\npast that exists today in its present state \u2013 at the level of pure physics \u2013 simple\nstructures existing in the future, so to speak, do not appear to prearrange the past\nso that they are [6\u201312]. If there really were such prearrangements organizing simple\nthings to exist in the future we could say that it would be a model for an initial state\nwith a built-in arrangement for the future, which is what our model is. However,\nmodels or theories for the initial state such as Hartle and Hawking's no-boundary\nmodel [13] are not normally of this type, but rather lead to a simple starting state\ncorresponding to the fact that we normally do not see things being arranged for the\nfuture in the fundamental laws and thus find no backward causation [6]. However,\nwe sometimes see that this type of prearrangement occurs, but we manage to explain\nit away. For example, we may see lot of people gathering for a concert. At first it\nappears that we have a simple structure in the future, namely, many people sitting\nin a specific place, such as the concert hall, causing a prearrangement in the past.\nNormally we do not accept the phenomenon of people gathering for a concert\nas an effect of some mysterious fundamental physical law seeking to collect the\n2\n\n\fpeople at the concert hall, and thus arranging the motion of these people shortly\nbefore the concert to be directed towards the hall. In our previous model [1\u20135, 7],\nwhich even we do not claim to be relevant to the concert hall example, such an\nexplanation based on a fundamental physics model could have sounded plausible.\nIn our model, we have a quantity SI , which is the imaginary part of an action in the\nsense that it is substituted into a certain Feynman path integral, as is the real part\nof the action, except for a factor i. In fact, we let the action S be complex, and its\nR\nimaginary part SI , as for the real part SR , be an integral over time, SI = LI dt.\nThus S = SR + iSI . Roughly speaking, the way that the world develops is to make\nSI [path] almost minimal (so that the probability weight obtained from the Feynman\npath integral e\u22122SI is as large as possible). Thus, a tempting \"explanation\" for the\ngathering of the people would be that many people gathering for a concert provides\na considerable negative contribution to the imaginary part LI of the Lagrangian\nduring the concert, and thereby, a negative contribution to the imaginary action SI .\nThus, the solutions to the equations for such a gathering before a concert would\nhave an increased probability of e\u22122SI , and we would have an explanation for the\nphenomenon of the people gathering for the concert. If we did not have an alternative\n\u2013 and we think better \u2013 explanation, then we might have to take such gatherings of\npeople for concerts as evidence for our type of model with an effect from the future;\nwe would, for example, conclude that the gathering of people occurred in order to\nminimize \"an imaginary action\" SI .\nAn alternative and better explanation that does not require any fundamental\nphysical influence from the future is as follows: The participants in the concert\nand their behaviors are indeed, in the classical and naive approximation, completely\ndetermined from the initial state of the universe at the moment of the Big Bang,\nan initial state in which the concert was not planned. Later on, however, some\norganizers \u2013 possibly the musicians themselves \u2013 used their phantasy to model the\nfuture by means of calendars, etc., and they issued an announcement. We can use\nthis announcement as the true explanation of the gathering of the listeners to the\nconcert. The gathering at the concert was due to some practical knowledge of the\nequation of motion allowing the possibility to organize events entirely on the basis of\nthe equation of motion and using the fact that the properties of the initial conditions\nare, in some respects, very well organized (low entropy, sufficient food and gasoline\n\n3\n\n\fresources). However, there was no effect from the future, only from the phantasies\nabout the future implemented in memories, which are true physical objects of course,\nsuch as the biological memories of the announcement and so on.\nEven more difficult examples can be used to explain the fact that our actions are\nnot preorganized \"by God\", which may here be roughly identified with fundamental\nphysical influences from the future, such as the biological development of extremely\nuseful organs. Has the development of legs, say, really got nothing to do with\nthe fact that they can later be used for walking and running? Darwin and Walles\nproduced a convincing explanation for the development of legs without the need for\nany fundamental influence from the future on the past.\nIf, as would be said prior to Darwin's time, it were God's plan (analogous to the\nconcert organizer's plan) to make legs, this would come very close to the fundamental\nphysics model, provided the following two assumptions were satisfied:\n1) That this God is not limited or has His memory limited by physical degrees\nof freedom in contrast to the brains of the concert organizers.\n2) This God is all-knowing, which means that He has access to the future and\ndoes not need a phantasy or simulation to create a model of it.\nIn the earlier works [2\u20134] we attempted to find various reasons why this effect\nfrom the future might to be suppressed. For instance this effect is definitely suppressed for particles whose eigentimes are trivial in some sense. From the Lorentz\ninvariance, the contribution of an action, which may be to the real part SR or the\nimaginary part SI involving from the passage of a particle from one point to another\npoint must be proportional to the eigentime of the passage (i.e., the time the passage\nwould take according to a standard clock located at the particle).\nTwo examples that dominate the physics of daily life ensure at least one source\nof strong suppression of the effect from the future: 1) Massless particles such as\nthe photon have always zero eigentimes, thus for photons, the effect is strongly\nsuppressed or killed. 2) For nonrelativistic particles, the eigentime is equal to the\nreference frame time, and thus the eigentime is trivial unless the particle is produced and/or destroyed. If a particle such as an electron is conserved the eigentime\nbecomes trivial and there is little chance to see our effect at the lowest order with\nelectrons.\n4\n\n\fActually, since there is a factor of\n\n1\n~\n\nin front of the action, one initially expects\n\nthe effects of SI to be so large that we need a large amount of suppression to prevent\nour model from being in immediate disagreement with the experiment.\nWe shall discuss some suppression mechanisms in section 3. We shall also discuss\nreasons why it may be likely that the effects of Higgs particles are much greater than\nthose of already observed particles. The number of Higgs particles is not preserved\nso that even a non relativistic Higgs particle may contribute SI , but we do, of course,\nexpect somewhat relativistic Higgs particles to be produced with velocities of the\norder of magnitude of that of light, but typically not extremely relativistic, thus\nthere is no reason from the two above mentioned mechanisms that the effect of a\nHiggs particle should be suppressed.\nIn contrast, as we shall see, there is a reason why even if the whole effect of SI\nis generally strongly suppressed, a counteracting factor could be caused for the case\nof the Higgs particle.\nNormally it would be reasonable to assume that in the real and imaginary parts,\nthe multipliers of the same field combination, say \u03bbR in\n\n\u03bbR\n|\u03c6|4\n4\n\nin SR and \u03bbI in\n\n\u03bbI\n|\u03c6|4\n4\n\nin SI , should be of the same order of magnitude, so that the complex phases for the\ncouplings, such as \u03bbR + i\u03bbI , should be of the order of unity.\nThere is, however, one case in which the problems connected with the hierarchy\nproblem make this assumption unlikely to be true: Because of the hierarchy problem\nit is difficult to avoid considerable fine tuning of the Higgs mass, since its quadratically divergent contributions with a cutoff at the Planck scale MP l would shift it\nconsiderably. We may only need this fine tuning for the squared term in the real part\nof the complex mass m2 |\u03c6|2 = (m2R + im2I ) |\u03c6|2 in the Lagrangian density. Whether\n\nor not only the real part m2R of the square of the mass is tuned or whether also the\nimaginary part m2I is also tuned may depend on which of the various models is used\nto attempt to solve the hierarchy problem, and how such a model is implemented\ntogether with our model of the imaginary part of the action.\nFor instance, one of us has constructed a long argument \u2013 using a bound state\n\nof six top and six antitop quarks \u2013 that under the assumption of several degenerate\nvacua (=MPP)[ [5, 8, 16]], which in turn follows [5] from the model in the present\narticle, we obtain a very small Higgs mass with its order of magnitude agreeing with\nthe weak scale. In this model, which \"solves the hierarchy problem\", it is clearly the\n\n5\n\n\freal part of the square of the mass, i.e., m2R , that gets fine tuned, and there would\nbe insufficiently many equations of MPP stating that several vacua have essentially\nzero (effective) cosmological constants allowing the fine tuning of more than just\nthis real part m2R . In this model, the argument would thus be that the hierarchy\nproblem would remain unsolved for the imaginary part of the coefficient of the masssquare term for the Higgs field m2I , but it is unknown whether this imaginary part\nm2I should be fine tuned to be small. A priori, it may be difficult for any model to\nobtain a small real part m2R of the weak scale; thus it is highly possible that also\nin other models, only the real part m2R is tuned and not the imaginary part. In\nsuch cases, the imaginary part m2I of the Higgs mass square could, a priori, remain\nuntuned and be of, the order of some fundamental scale, such as the Planck scale or\na unified scale. This would mean that the imaginary part m2I may be much larger\nthan the real part,\nm2I \u226b m2R ,\n\n(1)\n\nand thus, the assumption that all the ratios of the real to the imaginary part for\nthe various coefficients in the Lagrangian should be of order unity would not be\nexpected to be true for the case of the mass-square coefficient. Conversely, unless\nthe hierarchy problem solution is valid for both real and imaginary parts we could\nhave\nm2I\nMP2 L\n\u2243\n\u2243\n2\nm2R\nMweak\n\n\u0012\n\n1019 GeV\n100 GeV\n\n\u00132\n\n\u2243 1034 .\n\n(2)\n\nThis would mean that by estimating the effect of SI in an analogous way to that\nfor particles with real and imaginary parts of their coupling coefficients being of the\nsame order or given by some general suppression factor, we could potentially under\nestimate the effects of the Higgs particle by a factor of 1034 .\nIn the light of this estimation for the relative importance of the effect from the\nfuture (\u2243 our SI effect) on the Higgs particle relative to that on other particles,\nit is an obvious conclusion that one should search for this type of effect when new\nHiggs-particle-producing machines such as the LHC, the Tevatron, or the canceled\nSSC are planned.\nIf the production and existence of a Higgs particle for a small amount of time gave\na negative contribution to SI , which would enhance the probability density \u221d e\u22122SI ,\n6\n\n\fone could wonder why the universe is not filled with Higgs particles. This may\nonly be a weak argument, but it suggests that presumably the contribution from an\nexisting Higgs particle to SI is positive if it is at all important. Now if the production\nand \"existence\" of a Higgs particle indeed gave a positive contribution to SI , whereby\nthe probability \u221d e\u22122SI for developments in a world containing large Higgs-particleproducing accelerators should be decreased by the effect of their SI contribution,\n\nthen the production and existence of such Higgs particles in greater amounts should\nbe avoided somehow in the true history of the world. If an accelerator potentially\nexisted that could generate a large number of Higgs particles and if the parameters\nwere so that such an accelerator would indeed give a large positive contribution,\nthen such a machine should practically never be realized!\nWe consider this to be an interesting example and weak experimental evidence for\nour model because the great Higgs-particle-producing accelerator SSC [17], in spite\nof the tunnel being a quarter built, was canceled by Congress! Such a cancellation\nafter a huge investment is already in itself an unusual event that should not happen\ntoo often. We might take this event as experimental evidence for our model in\nwhich an accelerator with the luminosity and beam energy of the SSC will not be\nbuilt (because in our model, SI will become too large, i.e., less negative, if such an\naccelerator was built) [17].\nSince the LHC has a performance approaching the SSC, it suggests that also\nthe LHC may be in danger of being closed under mysterious circumstances. In an\nintroductory article to the present one [1] we offered to demonstrate the mysterious\neffect of SI in our model on potentially closing the LHC by carrying out a carddrawing game, or using a random number generator.\nUnder the assumption that our model is indeed correct, demonstrating a strong\neffect on the LHC by a card-drawing game such as its possible closure would serve\na couple of purposes:\n1) Even though some unusual political or natural catastrophe causing the closure\nof the LHC would be strong evidence for the validity of a model of our type\nwith an effect from the future, it would still be debatable whether the closure\nwas not due to some other cause other than our SI -effects. However, if a carddrawing game or a quantum random number generator causes the closure of\nthe LHC in spite of the fact that it was assigned a small probability of the\n7\n\n\forder of, say 10\u22127 , then the closure would appear to very clear evidence for\nour model. In other words, if our model was true we would obtain a very clear\nevidence using such a card-drawing game or random number generator.\n2) A drawn card or a random number causing a restriction on the LHC could\nbe much milder than a closure caused by other means due to the effect of our\nmodel. The latter could, in addition, result in the LHC machine being badly\nused, or cause other effects such as the total closure of CERN, a political crisis,\nor the loss of many human lives in the case of a natural catastrophe.\nThus, the cheapest way of closing the main part of the LHC may be to demonstrate the effect via the card-drawing game.\nHowever, in spite of these benefits of performing the card-drawing experiment\nit would be a terrible waste if a card really did enforce the closure or a restriction\non the LHC. It should occur with such a low probability under normal conditions\nthat if our model were nonsense, then drawing a card requiring a strong restriction\nshould mean that our type of theory was established solely on the basis of that\n\"miraculous\" drawing. Such a drawing would have the consolation that instead of\nfinding supersymmetric partners or other novel phenomena at the LHC, one would\nsee the influence from the future! That might indeed lead to even more spectacular\nnew physics than one could otherwise hope for! Thus, the restriction of the LHC\nwould not be so bad. Nevertheless, it is of high importance that one statistically\nminimizes the harm done by an experiment such as a card-drawing game. Also, one\nshould allow several possible restrictions to be written on the cards that might be\nchosen so that several possible effects from the future may occur. Then one might,\nin principle, learn about the detailed properties of this effect such as the number of\nHiggs particles needed to obtain an effect, or whether luminosity or beam energy\nmatters the most for the SI ?\nIt is the purpose of the present article to raise and discuss these questions of how\nto arrange a card-drawing game experiment to obtain maximum information and\nbenefit and minimal loss and obtain statistically minimal restrictions.\nIn section 2, we formulate some of the goals one would have to consider in\nplanning a card-drawing game or random number experiment on restricting the\nLHC. In section 3 we give a simplified description of the optimal organization of the\n8\n\n\fgame and propose that the probability distribution in the game should be assigned\non the basis of the maximum allowed size of some quantity consisting of luminosity,\nbeam energy and number of Higgs particles etc..\nIn section 4 we develop the theory of our imaginary action so as to obtain a\nmethod of estimating the mathematical form expected for the probability of obtaining a \"miraculous\" closedown of the LHC.\nIn section 5 we discuss possible rules for the card-drawing game and random\nquantum generator. However, we still think that more discussion and calculation\nmay be needed to develop the proposed example before actually drawing the cards.\nSection 6 is devoted to further discussion and a conclusion.\n\n2\n\nThe goals, or what to optimize\nIt is clear that the most important goal concerning the LHC is for it to operate\n\nin a way that delivers as many valuable and interesting results as possible while\nsearching for one or more Higgs particles, strange bound states and supersymmetric\npartners. In contrast, our model is extremely unlikely to be true, and thus, the\ninvestigation of our model should only be allowed to disturb the other investigations\nvery marginally. The problem is that although the probability of disturbance by the\ninvestigation of our theory has been statistically evaluated to be very tiny, there is\na risk that the selection of a very unlucky card could impose a significant restriction\non the LHC, and thereby cause a very major disturbance.\n\n2.1\n\nWhat to expect\n\nBefore estimating the optimal strategy with respect to the card-drawing game\nand its rules, we wish to obtain a crude statistical impression of what to expect.\nThe most likely event is that our model is simply wrong, and thus it is very\nunlikely that anything should happen to the LHC unless it is caused by our carddrawing experiments. If, however, our model is correct in principle, we must accept\nthe unlucky fate of the SSC [17] as experimental evidence and conclude that the\namount of superhigh-energy physics at SSC, measured in some way by a combination\nof the luminosity and beam energy, seemingly sufficient to change the fate of the\nuniverse on a macroscopic scale. We do not at present know the parameters of our\n9\n\n\fmodel, and even if we make order-of-magnitude guesses, there are several difficulties\nin estimating even order-of-magnitude suppression mechanisms. For instance, there\nmay be competition between the arrangements of the events in our time to give\na low SI with a similar arrangement for other times. Thus, even guessing the\norder-of-magnitude of fundamental couplings will still not give a safe estimate for\nthe order-of-magnitude of the strength in practice. We are therefore left with a\ncrude method of prior estimation by taking the probability of different \"amounts of\nsuperhigh-energy collisions\" (say some combination of beam energy and luminosity)\nneeded to macroscopically change the fate of the universe to be of constant density\nin the logarithm of this measure of superhigh-energy collisions.\nFor simplicity, we consider that \u03c7 is, for example, the integrated luminosity for\ncollisions with sufficiently high energy to produce Higgs particles, or take it simply\nto be the number of Higgs particles produced. Whether they are observed or not\ndoes not matter; it is the physically produced Higgs particles and the time they\nexist that matters.\nWe know in the case that our theory was correct, an upper limit for this amount\nof superhigh-energy collisions is needed to obtain an effect. Namely, we know that\nthe SSC was canceled and that its potential amount of superhigh-energy collisions\nmust have been above the amount needed. On the other hand, we also know that\nthe Tevatron seemingly operates as expected so that its amount of superhigh-energy\ncollisions must be below the amount needed to cause fatal macroscopic changes.\nIn our prior estimation, we should thus calculate the probability for the amount\n\u03c7. This is needed to cause fatal effects in a machine, where \u03c7 is in the interval\n\u0002\n\u0003\n\u03c7 \u2212 12 d\u03c7, \u03c7 + 12 d\u03c7 , as\n\u0015\u0013\n\u0012\u0014\n1\n1\n= p(\u03c7)d\u03c7,\n(3)\nProbability \u03c7 \u2212 d\u03c7, \u03c7 + d\u03c7\n2\n2\nand we assume\np(\u03c7) =\n\n1\n1\n\u2212\n\u03c7 log \u03c7SSC \u2212 log \u03c7Tevatron\n\n(4)\n\nfor \u03c7SSC \u2265 \u03c7 \u2265 \u03c7Tevatron and that p(\u03c7) = 0 outside this range.\n\nHere we have respectively denoted the amounts of superhigh-energy collisions,\n\nsay, the numbers of Higgs particles, produced as integrated luminosity in the SSC\nand the Tevatron by \u03c7SSC and \u03c7Tevatron .\n10\n\n\fThe SSC should have achieved a luminosity of 1033 cm\u22122 s\u22121 and a beam energy\n20 TeV in each beam, while the Tevatron has achieved values of \u223c 1032 cm\u22122 s\u22121\n\nand 1 TeV.\n\nThe LHC should achieve a luminosity of 1034 cm\u22122 s\u22121 and a beam energy of 7\nTeV in each beam.\nBeam Energy\n\nLuminosity\n\nTevatron\n\n1 TeV\n\n1032 cm\u22122 s\u22121\n\nLHC\n\n7 TeV\n\n1034 cm\u22122 s\u22121\n\nSSC\n\n20 TeV\n\n1033 cm\u22122 s\u22121\n\nWith respect to luminosity, the LHC is expected to be even stronger than the\nSSC; thus, if we apply our criterion, one would expect the LHC to be prone to even\ngreater bad luck than the SSC.\nLet us, however, illustrate the idea by using for the beam energy for \u03c7. Then\nlog \u03c7SSC = log(20 TeV),\nlog \u03c7Tevatron = log(1 TeV)\n\n(5)\n\nand\nlog\n\n\u03c7SSC\n\u03c7Tevatron\n\n= log 20 \u2243 3.\n\n(6)\n\nHence,\np(\u03c7) =\n\n1 1\n*\n\u03c7 3\n\nfor \u03c7Tevatron \u2264 \u03c7 \u2264 \u03c7SSC .\n\n(7)\n\nNow \u03c7LHC = log(7 TeV) \u2243 2 + log TeV. Thus, the probability that the critical \u03c7\n\nfor closure is smaller than \u03c7LHC is P (\u03c7 < \u03c7LHC ) = 32 . This means that if our theory\n\nwas correct and the beam energy was the relevant quantity, then the LHC would be\nstopped somehow with a probability of \u2243\n\n2.2\n\n2\n3\n\n\u2243 66%.\n\nWhat would we like to know about our model, if it is\ncorrect?\n\nThere are several information one would like to get concerning our model:\n1) One would like to obtain an estimate of how strong the effect is, i.e., one would\nlike to estimate at least the order-of-magnitude of the value of \u03c7 needed to\ndisturb the fate of the universe macroscopically.\n11\n\n\f2) One would like to determine whether it is the beam energy or the luminosity\nthat is most important for causing closing.\n3) One would also like to determine which type of random numbers, quantum\nrandom numbers or more classically constructed ones, allow our SI -effect to\nmore easily manipulate the past. One could even speculate whether one could\nconstruct a mathematically random number that should make it almost impossible to manipulate such a physical effect (from the future).\nIn addition to all these wishes, to get questions answered random number experiment that causes minimal harm to the optimal use of the LHC machine.\n\n2.3\n\nHow to evaluate cost?\n\nLet us now discuss the above questions about our model.\nTo obtain a convincing answer to question 1), of whether there is indeed an effect,\nas proposed, the probability of selecting a random number \u2013 a card for instance \u2013 that\nleads to restrictions should be so small that one could practically ignore the possibility that a restriction occurs simply by chance. This suggests that one should let the a\npriori probability of a restriction, i.e., the number of card combinations corresponding to a restriction relative to the total number of combinations, be sufficiently small\nto correspond to getting by accident an experimental measurement five standard\ndeviations away from the mean. That is to say, a crude number for the suggested\n52\n\n12.5\n\nprobability of any restriction at all is e\u2212 2 \u2243 e\u221212.5 \u2243 10\u2212 2.3 \u2243 10\u22125.4 \u2243 4 \u00d7 10\u22126 .\n\nTo obtain a good answer to question 2) on the order-of-magnitude of the strength\n\nof the effect we must let the a priori probability of a drawing card giving a certain\ndegree of restriction vary with the degree of restriction. Thus, a milder restriction\nis made, a priori, to be much more likely than a more severe restriction. We can\nbasically assume that we will not draw a restriction (card combination) appreciably\nstronger than that required to demonstrate our effect. Thus, we can assume that\nthe restriction drawn will be of the order of the magnitude of the strength of the\noperating machine, \u03c7, which is the maximum allowed before our effect stops it. Thus\nif we arrange the probabilities in this way, we may claim that the restriction resulting\nfrom the drawn random number represent the strength of the effect. Mathematically,\nsuch an arrangement means that we choose the a priori probability for the restriction\n12\n\n\fvalue of \u03c7, \u03c7restriction , to be a power law:\n\u0012\u0014\n\u0015\u0013\n1\n1\nPa priori \u03c7restriction \u2212 d\u03c7restriction , \u03c7restriction + d\u03c7restriction\n2\n2\n= p (\u03c7restriction ) d\u03c7restriction ,\n\n(8)\n\nwhere\np (\u03c7restriction ) = K\u03c7\u03b1restriction ,\n\n(9)\n\nand K is a normalization constant. Here a larger value of \u03b1 > 0 should be chosen\nfor a sharper measurement of the strength of the effect. If we only require a crude\norder of magnitude, we can simply take \u03b1 \u2243 1.\n\nNote that having a large \u03b1 means that very severe restrictions become relatively\n\nvery unlikely. Thus, a large \u03b1 is optimal for ensuring minimal harm to the operation\nof the machine.\nWe shall also assume that since the Tevatron seems not to be disturbed we do\nnot have to include more severe restrictions on the LHC than those that would force\nit to operate as a Tevatron.\nConcerning question 3) as to which features of the operation, e.g., luminosity\nand center-of-mass beam energy, are the most important for our effect, we answer\nthis question by letting different random numbers \u2013 the drawings \u2013 result in different\ntypes of restrictions. That is to say, the different drawings represent different combinations of restrictions on the beam energy and luminosity. Presumably it would\nbe wise to make as many variations in the restriction patterns as possible, because\nthe more combinations of various parameters, the more information about our effect\none can obtain. If one draws a combination of cards that causes a restriction, then\none has immediately verified our type of model or the existence of an effect from\nthe future. In this case, any detail of the specific restriction combination obtained\nfrom the drawing is no longer random but is an expression of the mysterious new\neffect just established by the same drawing. The more details one can thus arrange\nto be readable from the card combination drawn, the more information one will obtain about the SI -effect in the case of restrictions that actually show up in spite of\nhaving been a priori arranged to do so with a probability of the order of 5 standard\ndeviations from the mean. Thus, to obtain as much profitable information as possible, there should be as many drawing combinations with as many different detailed\n13\n\n\frestrictions as possible. One could easily make restrictions only for a limited number\nof years or one could restrict the number of Higgs particles produced according to\nsome specific Monte Carlo program using, at that time, the best estimate for the\nmass of a Higgs particle. If one allows some irrelevant details to also result from\nthe drawing, it is not a serious problem, since one will simply obtain a random\nanswer concerning the irrelevant parameter. It would be much worse if our theory\nwas correct and one missed the chance of extracting an important parameter, that\ncould have been extracted from the drawing.\nOne should therefore also be careful when adjusting the relative a priori probabilities of the values for parameters one hopes to extract, so that one really extracts\ninteresting information relative to theoretical expectations and does not simply obtain a certain result because one has adjusted the priori probability too much.\nConcerning question 4), to determine the type of random number that can be\nmost easily manipulated by our SI -effect, we should extract information \u2013 but unfortunately very little information we suspect \u2013 to answer the question, by using several,\nor at least two, competing types of random numbers. One could, for instance, have\none quantum mechanical random number generator and one card-drawing game.\nOne could easily reduce the probability of a restriction in each of them by a factor\nof 2 so as to keep the total probability of obtaining a restriction at the initially prescribed level of 5 standard deviations. Then one should have two (or more) sources of\nrandom numbers, e.g., a genuine card-drawing game and a quantum random number\ngenerator, each with a very high probability that there will be no restrictions on the\nrunning of the LHC (for that type of random number) and only a tiny probability\nof some restriction (as already discussed with as many different ways of imposing\na restriction as one can invent) of less than 5 standard deviations divided by the\nnumber of different types of random number, 2 in our example.\nAfter having drawn a restriction from one of the types of random numbers, one\nwould at least know that this type of random number was accessible for manipulation by our SI -effect. Such information could be of theoretical value because one can\npotentially imagine that various detailed models based on our type of effect from\na future model may give various predictions as to through which type of random\nnumber the SI -effect can express itself. If, say, a model only allowed the SI -effect\nthrough classical effects of the initial state of the universe but quantum experiments\n\n14\n\n\fgave fundamentally random or \"fortuitous\" [18] results that not even SI could influence, then such a model would be falsified if SI -effect produced the quantum number\nled to restrictions on the LHC.\nOne could also imagine that more detailed calculations would determine whether\nthe effect from the future had to manifest itself not too far back in time. In that\ncase one could perhaps invent a type of card game with cards that had been shuffled\nmany years in advance, and one only used the first six cards in such stack of cards.\nIf it was the type of random number that came from stack shuffled years in\nadvance that allowed the SI -effect, then any type of detailed theory in which the\neffects of the future go only a short time interval back in time could be falsified.\n\n2.4\n\nStatistical cost estimate of experiment so far discussed\n\nLet us now, as a first overview as to how risky it would be to perform a random\nnumber experiment, consider the simple proposal above:\nThe highest probability in the experiment is that no restrictions are imposed\nbecause we only propose restrictions with a probability of the order of 10\u22126 . Even\nin the case of drawing a restriction, one then considers the distribution of, say, the\nbeam energy restriction to be the \u03b1th power of the beam energy. Here we think\nof \u03b1 being 1 or 2. This leads to the average allowed beam energy being reduced\nby \u03c7LHC * h(1 \u2212 X)i, where X denotes the fraction of allowed beam relative to the\n\nmaximum beam. In other words we call the highest allowed beam according to\nthe card-drawing game X\u03c7LHC . Then the average reduction in the case with 10\u22126\nprobability that we get a reduction relative to the maximum beam \u03c7LHC becomes\nZ LHC max\nX +\u03b1 (1 \u2212 X)dX\nh1 \u2212 Xi = Tevatron\nZ LHC max\nX +\u03b1 dX\nTevatron\n\u0001\n1\n1\n\u2212\n\u03c7LHC\n= \u2212\u03b1\u22121 \u2212\u03b1\u22122\n1\n\u03c7LHC \u2212\u03b1\u22121\n\u2212\u03b1 \u2212 2 \u2212 (\u2212\u03b1 \u2212 1)\n=\n\u2212\u03b1 \u2212 2\n1\n=\n.\n(10)\n\u03b1+2\nFor \u03b1 = 2 we lose\n\n1\n4\n\nof the maximum beam due to the restriction.\n15\n\n\fWith the cost of the LHC machine estimated at 2 to 3 billion Swiss francs, the\nprobability of a restriction r being 10\u22126 , and the expected loss of the beam being\n1\n,\n4\n\nthe average cost of the card game experiment is of the order of 100 Swiss francs.\n\nHowever, of course there is, a priori, a risk. One shall, however, not mind if the\n\"bad luck of drawing a restriction card\" occurs, because in reality it is fantastically\ngood luck because one would have discovered a fantastic and at first unbelievable\neffect from the future!\n\n2.5\n\nAttempts to further reduce the harm\n\nOne can, of course, seek to further bias the rules of the game so as to assign the\nhighest probabilities to the restrictions causing least harm. For instance, one could\nallow a relatively high probability for restrictions of the type in which one is only\nallowed to operate the machine for a short time at its highest energy.\n\n3\n\nCompetition determining the fate between different times\nTo determine how our effect from the future functions let us consider our imaginary\n\nLagrangian model from a more theoretical viewpoint.\nIn the classical approximation of our model, we consider a first approximation\nsuch that\n1) the classical solution is determined alone by extremizing the real part of the\naction SR [path], i.e.,\n\u03b4SR [path cl.sol.] = 0 .\n\n(11)\n\nThe reason for this is very simple. The real part SR determines the phase\nvariation of the integral in the Feynman pathway\nexp {i (SR + iSI )} ,\n\n(12)\n\nand thus, it is only when SR varies slowly, i.e., when \u03b4SR \u2243 0, that we do\n\nnot have huge cancellation because the rapid sign variation (phase rotation)\ncancels the contribution out.\n16\n\n\fWe may illustrate this by the following drawing.\nRe exp {i(SR + iSI )}\n\n\u273b\n\n\u2732\n\nsome symbolic\ntrack variable\n\n|\n{z\n}\nHere practical\ncancellation to zero.\n\n\u273b\n\nHere \u03b4SR \u2243 0.\nOnly from around here is\nthere no huge cancellation.\n\n|\n{z\n}\nHere there is also\nhuge cancellation.\n\nSI only results in a factor in the magnitude and leaves the phase of the integrand undisturbed.\n2) The effect of SI has a total weight of e\u2212SI [cl.sol.] on the amplitude or Feynman\npath integral contribution, upon which one then inserts the solution to the\nclassical equations of motion (i.e., to \u03b4SR = 0) for the path in the symbol\nSI [path]. This means then that the probability that the classical solution\n\"cl.sol.\" exists is proportional to e\u22122SI [cl.sol.]. We have to square the amplitude\nto obtain the probability.\nThe probability density of e\u22122SI [cl.sol.] was referred to as P [cl.sol.] in our early\nworks in the present series and, unless one adds special assumptions about SI ,\nit behaves as a function of the path, i.e., there is only notational difference\nbetween P [cl.sol.] in the early works and e\u22122SI [cl.sol.], i.e.,\nP [cl.sol.] = e\u22122SI [cl.sol.].\n\n17\n\n(13)\n\n\f3.1\n\nThe importance of competition between times on determining the fate of all times\n\nHere we want to stress a very important effect that reduces the strength of the\nobservable effect from the future in our model. We call this effect the competition\nbetween the different eras upon what shall happen in the universe. We have noted\nthat the probability of a certain classical solution to the equations of motion \u03b4SR = 0\n\u2013 the true track \u2013 is given by e\u22122SI [cl.sol.], where the imaginary action SI is an integral\nover time\nSI =\n\nZ\n\nLI dt.\n\n(14)\n\nThe important point here is that selecting a certain solution to the equations of\nmotion in one period of time via the equations of motion in principle determines\nthe solution at all times, both earlier and later. This is basically \"determinism\";\nsimply knowing the position and velocities of all the dynamical configurations of\nvariables at one moment of time allows one, in principle, to integrate the equations\nof motion so as to obtain the solution at all times. This determinism may only\nbe true in a principal or in an ideal way, since we know that, depending on the\nLyapunov exponents, very small deviations between two solutions at one time can\nbecome huge at a later time. Also, extrapolation backward in time may also have\nthe same effect. Furthermore, it is known that this determinism is challenged by\nthe measurement postulates of randomness in quantum mechanics.\nNevertheless, these is certainly a strong restriction as to what can be obtained\nfrom a solution at one moment of time if it has already been used to make a small\nLI at another time. The different regions in time are, so to speak, competing in the\nR\nselection of the solution that gives the minimal contribution to a time region LI dt in\n\nthe different time regions. Here, we simply draw attention to the \"competitional\"\nR\nproblem that the contribution to SI from Big Bang time LI dt in Big Bang time does\nR\nnot usually make our times LI dt the minimal value. Thus, a compromise must occur\nbetween the different time eras so as to minimize\nZ\nSI =\nLI dt.\n\n(15)\n\nall times\n\nThis means that even if one estimates a large effect of LI in one era, it may not\nR\nbe easy to use this effect to determine the minimal LI in our times or our times LI dt,\n18\n\n\fbecause the enormously long time spans outside our own times will typically almost\ncompletely determine which solution to \u03b4SR = 0 will be selected that results in\nminimal SI . Our own human lifetime only makes up an extremely small part of\nthe 1010 years in which the universe has already existed. Thus, much stronger SI contributions are needed to have any effect than would be required with a universe\nexisting only for human-scale time.\nIn other words, practically everything about the solution obtained by minimizing\nSI is determined by contributions from time intervals very far from the interval in\nwhich we know some history and have some memory of its significances. This means\nthat we should observe extremely little effect from the future in practice. We would\nnot be able to recognize much of any effect because most of the future as well as\nmost of the past is so remote that we know exceedingly little about it.\nWe might recognize an effect from the future if some accelerator that is already\nplanned is then stopped by Congress. However, if the accelerator is to be built\nin 1010 years, we would most likely not know about the plans and be unable to\nrecognize the effect from the future that causes its closure in 1010 years from now.\nWe would only see such prearrangements as purely random and not as prearrangements. We can conclude that prearrangement is difficult to recognize unless\nyou have knowledge of the plans that shall be accepted or rejected.\n\n3.2\n\nThe rough mathematical picture\n\nTo get an idea of the significance of the competition between various time intervals\non determining what is selected to be true solution of the classical equations of\nmotion, we give a very rough description of the mathematics involved in minimizing\nSI and in searching for a likely type of solution when we have the probability density\ne\u22122SI over phase space. One should bear in mind that the set of all classical solutions\nare in one-to-one correspondence with phase space points when a solution is given\nby integrating up -backward and forward- from a certain standard moment t0 .\nWe should take SI to be an integral or a sum over a large number of small time\nR\nP R\nintervals i Ii LI dt. Each of these small contributions Ii LI dt may be taken as\n\na random function written as a Fourier series over phase space in the very rough\napproximation for the first orientation. We even assume for the first orientation that\nR\nwe have a random form of LI (ti ) or, approximately equivalently, Ii LI dt remains\n19\n\n\fof the same form with the same probability of having different values after being\ntransformed to the standard moment t0 by using the canonical transformations\nassociated with the Hamiltonian derived from the real part of the action SR . That\nis to say, in the phase space variables (~q0 , p~0 ) at time t0 , each of the contributions\nR\nL dt is a stochastic variable function over phase space that can be expressed as\nIi I\nZ\n\nLI dt =\n\nIi\n\nX\n\n~ (q)\n~ (p)\nc(~k (q) , ~k (p) ) * eik *~q+ik *~p,\n\n(16)\n\nwhere we require\nc(\u2212~k (q) , \u2212~k (p) )\u2217 = c(~k (q) , ~k (p) ).\n\n(17)\n\nWe take the real and imaginary parts of the c(~k (q) , ~k (p) ) to have a Gaussian distribution with the spread\nn\no2\nh Re c(~k (q) , ~k (p) ) i = \u03c3r (~k (q) , ~k (p) ),\nn\no2\nh Im c(~k (q) , ~k (p) ) i = \u03c3i (~k (q) , ~k (p) ).\n\n(18)\n\nSince it is a rough model, we shall not go into details of how to choose \u03c3r and \u03c3i , but\nimagine that we have a cutoff that effectively separates large ~k (p) and ~k (q) regions.\nR\nAlso we would like to roughly take each Ii LI dt to be periodic in the phase space\n\nvariables ~q and p~ so that we effectively use a Fourier series. The period is a cutoff\nin phase space \u039bphs, and the cutoff in ~k (q) and ~k (p) , say \u039bk , separates the rapid\nR\nvariations of Ii LI dt over the phase space. There is, thus, effectively a number of\nR\nindependent phase space points (\u039bk /\u039bphs )N in which Ii LI dt can take its values.\nHere N is the number of degrees of freedom. The statistical distribution for one of\nR\nthe Ii LI dt in one of these effective phase space points is assumed to be Gaussian\nwith a mean square deviation of\n\u03c3r =\n\nX\n\n~k (p) ,~k (q)\n\n\u03c3r (~k (q) , ~k (p) ) \u2243\n\n\u0012\n\n\u039bk\n\u039bphs\n\n\u0013N\n\n* \u03c3r ,\n\n(19)\n\nwhere \u03c3r is a typical value for \u03c3r (~k (q) , ~k (p) ). The contribution from the imaginary\npart is of the same order, and we ignore a factor of 2 here.\nPn R\nWhen we search for SI = i step Ii LIi dt, we again have a Gaussian distribution\n\nsince each LIi has, by assumption, independent Gaussians. But if there are nstep\n20\n\n\ftime steps of type Ii then the distribution of SI becomes broader than that of\n\u221a\n\u221a\nby a factor of nstep . That is to say, SI is of the order nstep * \u03c3r .\n\nR\n\nIi\n\nLI dt\n\nThe mathematical picture, we have constructed is summarized by the following\n\ntwo points:\n1) There are (\u039bk /\u039bphs)N classical path solutions, or equivalently (\u039bk /\u039bphs )N possible ways that the universe could have started and subsequently developed.\n2) SI for each of these developments has a Gaussian distribution with a mean\nsquare deviation of nstep \u03c3r .\nOur model postulates that the probability of the realization of a classical solution\nis weighted by P = e\u22122SI .\nThis extra effect of our model converts the distribution of what from a Gaussian\nof the form\n\u0012\nexp \u2212\n\nSI2\n2nstep * \u03c3r\n\n\u0013\n\n(20)\n\ninto a distribution of the form\n\u0012\nexp \u2212\n\nSI2\n2nstep * \u03c3r\n\n\u0013\n\n\u2212 2SI .\n\n(21)\n\nEquation (21) implies that the realistic or most likely SI -value for the chosen development of the universe should be realized by maximizing the exponent\n\u2212\n\nSI2\n2nstep * \u03c3r\n\n\u2212 2SI\n\n(22)\n\nin this probability distribution.\nThe maximum occurs when\nSI \u2243 nstep \u03c3.\n\n(23)\n\nThe actual development of the universe will not have exactly this value nstep \u03c3 for\n\u221a\nSI , but a value typically deviating by the order nstep \u03c3.\nNow let us imagine that in our time, say, in one of the intervals Ii , we look for\na special occurrence that may give an extra contribution \u2206SI extra to SI . It is easy\nto see that compared with the probability without this contribution, the probability with the \u2206SI extra contribution should be e\u22122\u2206SI extra times high. However, the\nquestion is whether we would realistically notice this effect.\n21\n\n\fTo detect our SI -effect we might carry out a card-drawing experiment by turning\na card, which if black we let the experiment giving \u2206SI extra be performed, and if\nred we do not perform it. We first imagine, for the sake of argument, that the extra\nLI contribution is switched off by not there at all. Then, for symmetry reasons, the\nprobabilities of red and black should both be 12 .\nThus, it would appear that the result of black or red would be dominantly\ndetermined from what happens in other time intervals rather than in \"our time\", in\nthe sense that the contribution to the fluctuation in SI from times other than ours\nwould be\n\u2206SI f luctuation\n\nfrom other times\n\n=\n\nq\n\n(nstep \u2212 1)\u03c3.\n\n(24)\n\nThis effect of the contributions from these other times, which we cannot treat\nscientifically or understand or know anything significant about, will give an SI p\n\u221a\ncontribution of the order (nstep \u2212 1)\u03c3 \u223c nstep \u03c3, to which the extra contribution\n\n\u2206SI extra has to be compared when it is switched on.\n\nLet us first estimate what we would be an ordinary value of \u2206SI extra relative\n\u221a\nto \u03c3. Since we not only live in a short accessible time but also in a small accessible spatial region, we should take an ordinary order of magnitude for \u2206SI extra of\nq\nacc\n\u03c3 * VVuniv\n, where Vacc is the part of the universe controllable by our card game and\nVuniv is the total effective volume of the universe.\nThus, the \"ordinary\" value for \u2206SI extra is\nr\n\u2206SI extra\n\nordinary\n\n\u223c\n\n\u03c3*\n\nVacc\n,\nVuniv\n\n(25)\n\nwhich is to be compared with\n\u2206SI f luctuation \u223c\n\n\u221a\n\nnstep \u03c3 .\n\n(26)\n\nVacc\n,\nnstep Vuniv\n\n(27)\n\nThis gives\n\u2206SI extra\n\nordinary\n\n\u2206SI f luctuation\n\n\u2243\n\ns\n\nwhich means the square root of the universe accessible by the card game part of\nspace time relative to the full space time of the universe.\n\n22\n\n\fIf we use a weak scale to give us the region in space time in which a Higgs\nparticle contributes lW \u223c\n\n1\n100 GeV\n\n\u223c 2 \u00d7 10\u22123 fm \u223c 10\u221226 s while the extension of the\n\nreachable universe and its lifetime is taken to be 1017 s then\n\u0012 \u221226 \u00134\nVacc\n10\ns\n= 10\u2212172 .\n\u223c\n17\nnstep Vuniv\n10 s\n\n(28)\n\nThis would give us 10\u221286 as the quantity that must be compensated by having an\nextraordinary size of LI to obtain any recognizable effect. Now if, as is quite likely,\nthe hierarchy-problem-related fine tuning of the square of the Higgs mass should only\nbe for the real part m2HR of the square of the mass, while the imaginary part m2HI of\nthe |\u03c6H |2 -coefficient is of the Planck scale order of magnitude, m2HI \u223c (1019 GeV)2 ,\n\nthen the ratio of \u2206SIextra relative to \u2206SIordinary from the m2HI -term would be expected\nto be of the order of\nm2HI\n\u223c 1034\n2\nmHR\n\n(29)\n\ntimes bigger than \"ordinary\" SI -contribution. This would not be enough to compensate the 10\u2212172 , but the latter might be very many orders of magnitude wrong\nfor several reasons, as we shall now discuss in the next subsection.\n\n3.3\n\nWhat value to take for an effective\n\nVacc\nnstep Vuniv ?\n\nOne could say:\n1) The card game result is mainly connected with the earth as far as its development and dependence is concerned. Thus we should reduce the effective universe size Vuniv to be that of the earth, i.e., a length scale of 107 m \u223c 3\u00d710\u22122 s\n\nrather than the 1017 s in the above estimation.\n\n2) If we compare the situation on earth events only really occur in the atoms,\nand if something happens at a weak scale when Higgs particles are present it\nmay seem reasonable that each Higgs particle has as many degrees of freedom\nas an atom and should be assigned in our estimate space having an as atomic\nsize, i.e., \u223c 10\u221210 m \u223c\n\n1\n3\n\n\u00d7 10\u221218 s.\n\nFrom only these two corrections we would obtain\n\u00121\n\u00133\n\u00d7 10\u221218 s\nVacc\n3\n\u223c\n\u2243 10\u221245 .\nVuniv\n3 \u00d7 10\u22122 s\neff\n\n23\n\n(30)\n\n\f3) We might also have to count the lifetime of the Higgs particle as closer to\n\n1\nMeV\n43\n\n17\n1\nmeaning that nstep would decrease from nstep \u223c 1010\u221226 ss \u2243 10\n100 GeV\nq\n1017 s\nVacc\n38\nnstep \u223c 10\u221221 s \u2243 10 . This would increase the square root to nstep\nVuniv\n\nthan\nq\n\n10\u221245\n1038\n\n\u2243\n\n\u221a\n\n10\u221283 \u2243 10\u221241.5 . Then we would only lack a factor of\n\n1041.5\n1034\n\nto\n\u2243\n\n= 107.5 ,\n\nwhich may be able to compete in producing a significant value of \u2206SI extra due\nto the existence of many Higgs particles.\n4) If, for instance, we could replace the whole lifetime of the universe by some\ninverse Lyapunov exponent for political activities, i.e., the time in which exceedingly small effects develop into politically important decisions, say a few\nyears, then we could effectively reduce nstep by a factor 109 and we would need\n104.5 times less compensation.\nThis would mean that we might only need to produce 103 Higgs particles in an\naccelerator for it to be enough that the SSC would be canceled by our model.\nIn the light of the huge uncertainties even in the logarithm of these estimates\nand the closeness to the achievements of the LHC of our relevant scale, it is\nclear that a much better estimation to the extent that such an estimate is\npossible is called for.\n\n3.4\n\nBaryon destruction\n\nLet us bring attention here to an effect in our model that will potentially be much\nmore important than the Higgs particle production in the SSC: baryon destruction.\nSince the quarks in the baryons couple to the Higgs particle field, which decreases\nit numerically to close to that of the quark or baryon, they function as a tiny\nnegative number of Higgs particles. However, in contrast to the Higgs particle\nitself, the baryon effectively lives eternally. Thus, if one produces an accelerator\nthat has sufficiently high energy that it can violate the baryon number, then it\nmay affect the Higgs field more than genuine Higgs particles. Indeed, the |\u03c6H |2 charge by d quark may typically be of the order of |gd |2 times that of a genuine\n\nHiggs charge. However, since the baryon lives eternally, we gain a lifetime factor\nof\n\n107 s\n10\u221221 s\n\n= 1038 , which hugely overcompensates for |gd |2 \u223c (10\u22125 )2 = 10\u221210 . Thus,\n\nthe destruction of a single baryon should be about as SI -significant as 1028 Higgs\n\n24\n\n\fparticles. This estimate would result in the borderline significance of the Higgs\nparticle being converted into an absolute necessity for the SSC to be canceled.\nNow we might even ask whether our first estimate\ns\nVacc\n\u2243 10\u221286\nnstep Vuniv\n\n(31)\n\nfirst\n\nwould allow there to be SI -effects resulting from baryon destruction. Because of\nthe extremely long baryon lifetime compared with the Higgs particle, the effect\nof \u2206SI extra increased by a factor of 1028 upon the destruction of a baryon. This\nwould convert m2HI /m2HR \u223c 1034 into a factor greater than the \"ordinary\" one,\n1034+28 = 1062 . Even that would not be sufficient to compensate for 10\u221286 .\n\nHowever, even one of the above corrections results in a change from the weak size\nto the atomic size, thereby increasing Vacc by a factor of (107 )3 , and thus increasing\nq\nVacc\nby a factor of 1010.5 or perhaps more correctly, using the distance between\nnstep Vuniv\n\nthe atoms in the universe, resulting in yet another increase by a similar factor\nq\np\nVacc\n\u223c 10\u221265 , so that the 1062\n(108 )3 = 1012 . Indeed, we would then have nstep\nVuniv\ncould cope if the the SSC had destroyed only 103 baryons. However, if the baryon\ndestruction resulted in the cancellation of the SSC, then the LHC is not in danger\nbecause there will presumably be no baryon destruction at the LHC. Presumably\nthere would not even have been in the SSC.\n\n4\n\nConclusion\nWe first reviewed our model with an imaginary action to be inserted into the\n\nFeynman pathway integral. It seems a bit artificial to assume that the action S in\ni\n\nthe integrand e ~ S should be wholly real when the integrand itself is clearly complex.\nWe claim that such an imaginary part SI [path] in the action S = SR + iSI does\nnot influence the classical equations of motion \u03b4SR = 0, but rather manifests itself\nby determining the initial conditions for the development of the universe. Indeed,\nthe various classical solutions that contribute to the Feynman pathway integral\nare weighted by an extra factor e\u2212SI [path] , which leads to a probability weight of\nP = e\u22122SI [path] .\nThe main discussion in the present article was on the development of an earlier\nproposal [1] for how to search for the effects on the determination of initial conditions\n25\n\n\fof such an imaginary action term SI [path]. From this viewpoint, the most remarkable\nfact is that this imaginary part SI , in analogy with the real part SR , is given as an\nintegral\nSI =\n\nZ\n\nLI dt\n\n(32)\n\nover all times and thus depends on the fields or dynamical variables not only in\nthe past but at all times. Thereby, the discussion became focused on searching\nfor effects from the initial conditions, which have been adjusted so as to take into\naccount what should happen or should not happen at a much later time.\nBecause of the very high probability that, in contrast to the real part m2HR of\nthe coefficient m2H = m2HR + im2HI of the expression |\u03c6H |2 in the Higgs field \u03c6H (x)\npart of the Lagrangian density L(x) = LR (x) + iLI (x), the imaginary part m2HI was\nnot fine tuned to be exceedingly small compared with the fundamental scale. It was\n\nsuggested that m2HI is likely to be huge compared with m2HR . We refer here to the\nproblem behind the so-called hierarchy problem associated with the fact that the\nweak-energy scale given by m2HR is very small compared with, say, the Planck scale.\nSince the reason for this fine tuning of m2HR to a small value is still unknown, it\nmay be equally likely that the mechanism for this fine tuning would also tune m2HI\nto a small value or leave it at the Planck scale. It is therefore very likely that there\nis an imaginary action for which the ratio between the corresponding coefficients in\nthe imaginary part SI and the real part SR would be unusually large in the case of\nthe Higgs mass square say\nm2HI\n\u223c 1034 .\n2\nmHR\n\n(33)\n\nThis possibility makes it likely that particularly large effects of SI can be found,\nand thus, cases of the future influencing even the initial conditions, and thus the past\nmay occur when the Higgs mass square term is involved. In almost all investigations\nof the Standard Model so far, the Higgs mass square term (m2HR + im2HI )|\u03c6H |2 is\n\nonly involved via the Higgs field vacuum expectation value h\u03c6H i, which is determined\n\nonly from the real part m2HR . Thus, we may have to wait for genuine Higgs-particleproducing machines to search for the effects of the huge expected imaginary part\nm2HI of the Higgs mass square. Alternatively, we would have to search for the effects\nof previously observed particles, such as quarks or leptons, on the Higgs field, by a\nback reaction which give a contribution proportional to m2HI to SI .\n26\n\n\fOne such effect could be caused by an accelerator able to violate the conservation\nof the baryon number [19] and presumably destroy more baryons than it creates.\nThen, the width |gd|2 \u2013 the quark Yukawa coupling squared \u2013 which is proportional\nto the suppression of the Higgs field around the baryon or the quark would be lost\nfor the rest of the existence of the universe. This effect of having a baryon being\ndestroyed forever while a Higgs particle has only a short lifetime overcompensates\nfor the Yukawa coupling suppression so that the effect of a baryon being destroyed\non SI is presumably much bigger \u2013 by say 1028 \u2013 than that of the creation of a\ngenuine Higgs particle. Nevertheless, our estimates of these effects are at the moment\nso approximate that it is uncertain whether the SI -effect would be sufficient for\npreventing Higgs production; thus, we predict possibly that the initial state would\nhave been organized somehow so that a large Higgs-particle-producing machine such\nas the LHC should somehow be prearranged so as not to come into existence.\nSuch an effect would, of course, be even stronger for the the terminated SSC\nmachine in Texas since it would not only have produced more Higgs particles but\nalso perhaps have destroyed some baryons.\nFrom the LHC-threatening perspective, the main point of the present article\nis that the LHC should really not be allowed to operate at full intensity or beam\nenergy by these effects on the initial state due to SI . In order to obtain as much\nknowledge and as little loss as possible out of this otherwise problematic event. We\nthen propose our card or random number experiment.\nOur main proposal was to perform a quantum random number or card-drawing\nexperiment, with both \"old\" and \"new\" random numbers, meaning that the random\nnumbers are created at longer or shorter times before the LHC is switched on, and\nthen let the value of this random number determine the restrictions on the running\nof the LHC.\nIt should be stressed that this whole process of closing random numbers to decide the fate of the LHC should be arranged so that it is by far the most likely that\nno restrictions are imposed at all. Only if there is some mysterious effect such as\nthe SI -effect in our model, which might have prearranged the initial state so as to\nprevent the LHC from operating, should there be any significant chance of obtaining\nanything apart from \"everything is allowed for the LHC\". In this way, if any restriction was indeed drawn by the card or by the quantum random number generator\n\n27\n\n\fthen this would, a priori, be so unlikely that such a drawing would immediately justify our type of model. Such a result would be so miraculous that it would require\na new set of physical laws.\nThus, we stress that if such a restriction is drawn, we should arrange matters so\nthat the exact value of the drawing tells us as much as possible about the details of\nthe theory on the effect from the future, which is justified merely by the selection\nof a restriction-requiring card.\nThis extraction of extra information from the drawing should have three features:\n1) One should use different types of random numbers such as a) cards shuffled recently, b) cards shuffled long ago, c) quantum random numbers made\nimmediately before the decision, and d) quantum random numbers made in\nadvance. Then one can determine which type of random number \u2013 old/recent,\ncard-drawing game/quantum \u2013 is the easiest for the SI -effect to manipulate\nso as to carry out the desired task of stopping or restricting the LHC.\n2) One would like to know which parameters of the machine are important in\nterms of the SI -effect. We should arrange the experiment so that, for all the\ndifferent types of random numbers, there are appreciably higher a priori probabilities (i.e., higher numbers of card combinations) of mild restrictions than\nof strong restrictions. By designing the game in this way, we can learn from\nthe result which restriction is really needed for causing any effect backward\nin time. The trial is of course also economical in the sense that we would\nthereby only obtain a result giving the minimal restriction needed to verify\nour theory; for example, this may be that only high luminosity combined with\nthe highest energy in the beams would be forbidden by the card, but many of\nother combinations of operational parameters would be allowed. This would\nthen cause minimum disruption to the program at the LHC.\n3) The most important result from the card-drawing experiment or random number selection if our model turned out to be correct may be that we would\nobtain a controllable estimate of its reliability. Of course, one would become\nconvinced of a model of our type in which Higgs particles or baryon destruction affect the past if something happened so that the LHC was prevented\nfrom operating. However, if this was not due to a controlled card-drawing or\n28\n\n\frandom number game, one could always claim that the cooling system was\ndefective, or that the failure was due to the political circumstances of some\nphysicists or because of a war or an earthquake. However, the SI -effect would\nalways have to use some natural effect to cause the effective restriction. Thus,\nthere will always be alternative reasons that may account for the failure of the\nplant. The main point, of course, is that if our model is not true then by the\nfar the most likely outcome is that the LHC will simply start operating next\nyear. Thus, if something happens to prevent the LHC from operation, then\nwe should believe our theory, but to obtain a more easily estimable basis for\nthe degree of belief if a failure or restriction occurred, we should carry out a\nfully controlled experiment involving random numbers, and if we had assigned\nan extremely low probability to the restriction that occurred, we would obtain\nvaluable information about the SI -effect.\nOf course, there is the \"danger\" that by making the probability of an experimental restriction extremely low, it becomes more likely that if our theory\nwas correct that, in spite of the selection, something else (the cooling system\nnot working, an earthquake, etc.) would stop the machine.\nIn the present article we have also derived a very rough estimation of the\namount of extra SI , called \u2206SI extra , that is needed to be significant enough\nto produce observable effects. So far, these estimates are so approximate\nthat we cannot say that, even if our model was in principle correct, it would\nhave sufficiently strong effects associated with the Higgs particle that it would\nactually lead to a closure or restriction of the LHC. We concluded from the\napproximation that the effect of the destruction of baryons, which has been\nspeculated [19] would have occurred at the SSC, would be much stronger than\nthe effect of Higgs particles. However, in our first estimate, uncertainties were\nso high that we cannot even claim that if our theory was in principle correct,\nan accelerator causing baryon destruction would definitely have to be closed\n(by some sort of \"miraculous\" effect).\nWe think that we may be able to produce a somewhat better estimate, but\nLyapunov exponents in the real world, which includes political decisions, may\nbe difficult to estimate. Of course, the real uncertainty is whether our model\nis true, even in principle. To establish the truth of such models, successful\n29\n\n\fcosmological predictions are hoped for. We are on the way to obtaining some\npredictions concerning the neutron lifetime by considering the order of the\ncapture time in Big Bang nuclear synthesis, and the ratio of dark energy density to full energy density, which is estimated to be around 2/3 to 3/4. In fact,\npredictions are being made for essentially the whole process of cosmological\ndevelopment except for the first inflation itself.\n\nAcknowledgments\nWe acknowledge the Niels Bohr Institute, Yukawa Institute for Theoretical Physics,\nKyoto University and CERN Theory Group for the hospitality they extended to the\nauthors. This work is supported by Grants-in Aid for Scientific Research on Priority Areas, 763 \"Dynamics of Strings and Fields\", from the Ministry of Education,\nCulture, Sports, Science and Technology, Japan. We also acknowledge discussions\nwith colleagues, especially John Renner Hansen, on the SSC.\n\nReferences\n[1] H. B. Nielsen and M. Ninomiya, \"Search for Future Influence from LHC\",\nIJMPA vol23, Issue 6, P919-932(March 10, 2008), arXiv:0707.1919 [hep-ph].\n[2] H. B. Nielsen and M. Ninomiya, \"Future Dependent Initial Conditions from\nImaginary Part in Lagrangian\", Proceedings of the 9th Workshop \"What Comes\nBeyond the Standard Models?\", Bled, September 16-26, 2006, DMFA Zaloznistvo, Ljubljana, hep-ph/0612032.\n[3] H. B. Nielsen and M. Ninomiya, \"Law Behind Second Law of ThermodynamicsUnification with Cosmology\", JHEP, 03,057-072 (2006), hep-th/0602020.\n[4] H. B. Nielsen and M. Ninomiya, \"Unification of Cosmology and Second Law of\nThermodynamics: Proposal for Solving Cosmological Constant Problem, and\nInflation\", Prog. Theor. Phys., Vol. 116, No. 5 (2006) hep-th/0509205, YITP05-43, OIQP-05-09.\n[5] H. B. Nielsen and M. Ninomiya, \"Degenerate Vacua from Unification of Second\nLaw of Thermodynamics with Other Laws\", hep-th/0701018.\n30\n\n\f[6] J. Faye, \"The Reality of the Future\", Odense University Press.\n[7] H. B. Nielsen and S. E. Rugh, Niels Bohr Institute Activity Report 1995.\n[8] H. B. Nielsen and C. Froggatt, School and Workshops on Elementary Particle\nPhysics, Corfu, Greece, September 3-24, 1995.\n[9] S. Coleman, Nucl. Phys. B307 867 (1988).\n[10] S. Coleman, Nucl. Phys. B310 643 (1988).\n[11] T. Banks, Nucl. Phys. B309 493 (1988).\n[12] S. W. Hawking, Phys. Lett. 134B 403 (1984).\n[13] J. B. Hartle and S. W. Hawking, Phys. Rev. D28 2960-2975 (1983).\n[14] Reviews: H. B. Nielsen and M. Ninomiya, Lecture Notes of International Symposium on the Theory of Elementary Particles, Ahrenshoop, DDR, October\n17-21, 1988.\n[15] C. D. Froggatt, L. V. Laperashvili and H. B. Nielsen, \"A new bound state 6t\n+ 6anti-t and the fundamental-weak scale hierarchy in the standard model,\"\narXiv:hep-ph/0410243. C. D. Froggatt, L. V. Laperashvili and H. B. Nielsen,\n\"The fundamental-weak scale hierarchy in the standard model,\" Phys. Atom.\nNucl. 69 (2006) 67 [arXiv:hep-ph/0407102]. C. D. Froggatt, H. B. Nielsen and\nL. V. Laperashvili, \"Hierarchy-problem and a bound state of 6 t and 6 anti-t,\"\nInt. J. Mod. Phys. A20 (2005) 1268 [arXiv:hep-ph/0406110].\n[16] D. L. Bennett and H. B. Nielsen, Int. J. Mod. Phys A9 (1994) 5155-5200.\n[17] J. Mervis and C. Seife, 10,1126/science, 302.5642.38, \"New focus: 10 Years\nafter the SSC. Lots of Reasons, but Few Lessons\".\n[18] A. Bohr and O. Ulfbeck, \"Primary manifestation of symmetry. Origin of quantal\nindeterminacy\", Rev. Mod. Phys. 67 (1995) 1.\n[19] V. A. Rubakov and M. E. Shaposhnikov, \"Electroweak baryon number nonconservation in the early universe and in high-energy collisions\", Usp. Fiz. Nauk\n166 (1996) 493 [Phys. Usp. 39 (1996) 461] [arXiv:hep-ph/9603208].\n\n31\n\n\f"}