{"id": "http://arxiv.org/abs/1004.1307v1", "guidislink": true, "updated": "2010-04-08T10:58:22Z", "updated_parsed": [2010, 4, 8, 10, 58, 22, 3, 98, 0], "published": "2010-04-08T10:58:22Z", "published_parsed": [2010, 4, 8, 10, 58, 22, 3, 98, 0], "title": "A simple and fast method to determine the parameters for fuzzy c-means\n  cluster validation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.3416%2C1004.2810%2C1004.4078%2C1004.1627%2C1004.1894%2C1004.2874%2C1004.4318%2C1004.4705%2C1004.0141%2C1004.4867%2C1004.3190%2C1004.2191%2C1004.2821%2C1004.4759%2C1004.5450%2C1004.0138%2C1004.0933%2C1004.2649%2C1004.1154%2C1004.1540%2C1004.2182%2C1004.4479%2C1004.1339%2C1004.0147%2C1004.4900%2C1004.2802%2C1004.1013%2C1004.4279%2C1004.3039%2C1004.5365%2C1004.5006%2C1004.4586%2C1004.5583%2C1004.4044%2C1004.5265%2C1004.0521%2C1004.5208%2C1004.2189%2C1004.3654%2C1004.0450%2C1004.1919%2C1004.5543%2C1004.3307%2C1004.4350%2C1004.2256%2C1004.5524%2C1004.1733%2C1004.5567%2C1004.4527%2C1004.0572%2C1004.0339%2C1004.2495%2C1004.5573%2C1004.2818%2C1004.2061%2C1004.0614%2C1004.0290%2C1004.2890%2C1004.3535%2C1004.2503%2C1004.0418%2C1004.5015%2C1004.4801%2C1004.1033%2C1004.3136%2C1004.1650%2C1004.4626%2C1004.4715%2C1004.0121%2C1004.3286%2C1004.3570%2C1004.0292%2C1004.5508%2C1004.5293%2C1004.2927%2C1004.3590%2C1004.4001%2C1004.1195%2C1004.3912%2C1004.1034%2C1004.4055%2C1004.4877%2C1004.4122%2C1004.1518%2C1004.2206%2C1004.1964%2C1004.1038%2C1004.2654%2C1004.0123%2C1004.4672%2C1004.1379%2C1004.4945%2C1004.1864%2C1004.0652%2C1004.0679%2C1004.2389%2C1004.1307%2C1004.1146%2C1004.1588%2C1004.5244%2C1004.4499&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A simple and fast method to determine the parameters for fuzzy c-means\n  cluster validation"}, "summary": "Fuzzy c-means clustering is widely used to identify cluster structures in\nhigh-dimensional data sets, such as those obtained in DNA microarray and\nquantitative proteomics experiments. One of its main limitations is the lack of\na computationally fast method to determine the two parameters fuzzifier and\ncluster number. Wrong parameter values may either lead to the inclusion of\npurely random fluctuations in the results or ignore potentially important data.\nThe optimal solution has parameter values for which the clustering does not\nyield any results for a purely random data set but which detects cluster\nformation with maximum resolution on the edge of randomness. Estimation of the\noptimal parameter values is achieved by evaluation of the results of the\nclustering procedure applied to randomized data sets. In this case, the optimal\nvalue of the fuzzifier follows common rules that depend only on the main\nproperties of the data set. Taking the dimension of the set and the number of\nobjects as input values instead of evaluating the entire data set allows us to\npropose a functional relationship determining its value directly. This result\nspeaks strongly against setting the fuzzifier equal to 2 as typically done in\nmany previous studies. Validation indices are generally used for the estimation\nof the optimal number of clusters. A comparison shows that the minimum distance\nbetween the centroids provides results that are at least equivalent or better\nthan those obtained by other computationally more expensive indices.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.3416%2C1004.2810%2C1004.4078%2C1004.1627%2C1004.1894%2C1004.2874%2C1004.4318%2C1004.4705%2C1004.0141%2C1004.4867%2C1004.3190%2C1004.2191%2C1004.2821%2C1004.4759%2C1004.5450%2C1004.0138%2C1004.0933%2C1004.2649%2C1004.1154%2C1004.1540%2C1004.2182%2C1004.4479%2C1004.1339%2C1004.0147%2C1004.4900%2C1004.2802%2C1004.1013%2C1004.4279%2C1004.3039%2C1004.5365%2C1004.5006%2C1004.4586%2C1004.5583%2C1004.4044%2C1004.5265%2C1004.0521%2C1004.5208%2C1004.2189%2C1004.3654%2C1004.0450%2C1004.1919%2C1004.5543%2C1004.3307%2C1004.4350%2C1004.2256%2C1004.5524%2C1004.1733%2C1004.5567%2C1004.4527%2C1004.0572%2C1004.0339%2C1004.2495%2C1004.5573%2C1004.2818%2C1004.2061%2C1004.0614%2C1004.0290%2C1004.2890%2C1004.3535%2C1004.2503%2C1004.0418%2C1004.5015%2C1004.4801%2C1004.1033%2C1004.3136%2C1004.1650%2C1004.4626%2C1004.4715%2C1004.0121%2C1004.3286%2C1004.3570%2C1004.0292%2C1004.5508%2C1004.5293%2C1004.2927%2C1004.3590%2C1004.4001%2C1004.1195%2C1004.3912%2C1004.1034%2C1004.4055%2C1004.4877%2C1004.4122%2C1004.1518%2C1004.2206%2C1004.1964%2C1004.1038%2C1004.2654%2C1004.0123%2C1004.4672%2C1004.1379%2C1004.4945%2C1004.1864%2C1004.0652%2C1004.0679%2C1004.2389%2C1004.1307%2C1004.1146%2C1004.1588%2C1004.5244%2C1004.4499&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Fuzzy c-means clustering is widely used to identify cluster structures in\nhigh-dimensional data sets, such as those obtained in DNA microarray and\nquantitative proteomics experiments. One of its main limitations is the lack of\na computationally fast method to determine the two parameters fuzzifier and\ncluster number. Wrong parameter values may either lead to the inclusion of\npurely random fluctuations in the results or ignore potentially important data.\nThe optimal solution has parameter values for which the clustering does not\nyield any results for a purely random data set but which detects cluster\nformation with maximum resolution on the edge of randomness. Estimation of the\noptimal parameter values is achieved by evaluation of the results of the\nclustering procedure applied to randomized data sets. In this case, the optimal\nvalue of the fuzzifier follows common rules that depend only on the main\nproperties of the data set. Taking the dimension of the set and the number of\nobjects as input values instead of evaluating the entire data set allows us to\npropose a functional relationship determining its value directly. This result\nspeaks strongly against setting the fuzzifier equal to 2 as typically done in\nmany previous studies. Validation indices are generally used for the estimation\nof the optimal number of clusters. A comparison shows that the minimum distance\nbetween the centroids provides results that are at least equivalent or better\nthan those obtained by other computationally more expensive indices."}, "authors": ["Veit Schw\u00e4mmle", "Ole N. Jensen"], "author_detail": {"name": "Ole N. Jensen"}, "author": "Ole N. Jensen", "arxiv_comment": "9 pages, 9 figures", "links": [{"href": "http://arxiv.org/abs/1004.1307v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1004.1307v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "q-bio.QM", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "q-bio.QM", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-bio.GN", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1004.1307v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1004.1307v1", "journal_reference": null, "doi": null, "fulltext": "Vol. 00 no. 00 2010\nPages 1\u20139\n\nA simple and fast method to determine the parameters for\nfuzzy c\u2013means cluster validation\nVeit Schw\u00e4mmle1\u2217, and Ole N\u00f8rregaard Jensen1\n1\n\nDepartment of Biochemistry and Molecular Biology, University of Southern Denmark,Campusvej\n55,DK-5230 Odense M,Denmark\n\narXiv:1004.1307v1 [q-bio.QM] 8 Apr 2010\n\nReceived on XXXXX; revised on XXXXX; accepted on XXXXX\n\nAssociate Editor: XXXXXXX\n\nABSTRACT\nMotivation: Fuzzy c-means clustering is widely used to identify\ncluster structures in high-dimensional data sets, such as those\nobtained in DNA microarray and quantitative proteomics experiments.\nOne of its main limitations is the lack of a computationally fast\nmethod to determine the two parameters fuzzifier and cluster number.\nWrong parameter values may either lead to the inclusion of purely\nrandom fluctuations in the results or ignore potentially important data.\nThe optimal solution has parameter values for which the clustering\ndoes not yield any results for a purely random data set but which\ndetects cluster formation with maximum resolution on the edge of\nrandomness.\nResults: Estimation of the optimal parameter values is achieved\nby evaluation of the results of the clustering procedure applied to\nrandomized data sets. In this case, the optimal value of the fuzzifier\nfollows common rules that depend only on the main properties of the\ndata set. Taking the dimension of the set and the number of objects\nas input values instead of evaluating the entire data set allows us\nto propose a functional relationship determining its value directly.\nThis result speaks strongly against setting the fuzzifier equal to 2\nas typically done in many previous studies. Validation indices are\ngenerally used for the estimation of the optimal number of clusters. A\ncomparison shows that the minimum distance between the centroids\nprovides results that are at least equivalent or better than those\nobtained by other computationally more expensive indices.\nContact: veits@bmb.sdu.dk\n\n1 INTRODUCTION\nNew experimental techniques and protocols allow experiments\nwith high resolution and thus lead to the production of large\namounts of data. In turn, these data sets demand effective machinelearning techniques for extraction of information. Among them, the\nrecognition of patterns in noisy data still remains a challenge. The\naim is to merge the outstanding ability of the human brain to detect\npatterns in extremely noisy data with the power of computer-based\nautomation. Cluster validation allows to group high-dimensional\ndata points that exhibit similar properties and so to discover a\npossible functional relationship within subsets of data.\n\n\u2217 to\n\nDifferent approaches to the problem of cluster validation\nexist, such as hierarchical clustering (Eisen et al., 1998), kmeans clustering (Tavazoie et al., 1999), and self-organizing\nmaps (Tamayo et al., 1999). Noise or background signals in\ncollected data normally come from different sources, such as\nintrinsic noise from variation within the sample and noise coming\nfrom the experimental equipment. An appropriate method to\nfind clusters in this kind of data is based on fuzzy c-means\nclustering (Dunn, 1973; Bezdek, 1981) due to its robustness to\nnoise (Hanai et al., 2006). Although this method has been modified\nand extended many times (for an overview see D\u00f6ring et al. (2006)),\nthe original procedure (Bezdek, 1981) remains the most commonly\nused to date.\nIn contrast to k-means clustering, the fuzzy c-means procedure\ninvolves an additional parameter, generally called the fuzzifier. A\ndata point (e.g. a gene or protein, from now on called an object)\nis not directly assigned to a cluster but is allowed to obtain fuzzy\nmemberships to all clusters. This makes it possible to decrease\nthe effect of data objects that do not belong to one particular\ncluster, for example objects located between overlapping clusters or\nobjects resulting from background noise. These objects, by having\nrather distributed membership values, now have a low influence\nin the calculation of the cluster center positions. Hence, with the\nintroduction of this new parameter, the cluster validation becomes\nmuch more efficient in dealing with noisy data. The value of the\nfuzzifier defines the maximum fuzziness or noise in the data set.\nWhereas the k-means clustering procedure always finds clusters\nindependently on the extent of noise in the data, the fuzzy method\nallows first to adapt the method to the present amount of noise\nand second to avoid erroneous detection of clusters generated by\nrandom patterns. Therefore, the challenge consists in determining\nan appropriate value of the fuzzifier.\nUsually, the value of the fuzzifier is set equal to two (Pal and Bezdek,\n1995; Babuska, 1998; H\u00f6ppner et al., 1999). This may be\nconsidered a compromise between an a priori assumption of a\ncertain amount of fuzziness in the data set and the advantage of\navoiding a time\u2013consuming calculation of its value. However, by\ncarefully adjusting the fuzzifier, it should be possible to optimize the\nalgorithm to take into account the characteristic noise present in the\ndata set. We are interested in having maximal sensitivity to observe\nbarely detectable cluster structures combined with a low probability\nof assigning clusters originating from random fluctuations.\n\nwhom correspondence should be addressed\n\nc Oxford University Press 2010.\n\n1\n\n\fVeit Schw\u00e4mmle11 , and Ole N\u00f8rregaard Jensen1\n\nNowadays, cluster validation is in widespread use for the analysis\nof microarray data to discover genes with similar expression\nchanges. Recently, large data sets from quantitative proteomics,\nfor instance measuring the peptide/protein expression by means of\nmass spectrometry, became available. These samples are usually\nlow-dimensional, i.e. they have a small number of data points\nper peptide/protein. As will be also shown in this work, low\ndimensionality may lead to difficulties to discard noisy patterns\nwithout loosing all information in the data set.\nTo our knowledge, only few methods exist to determine an\noptimal value of the fuzzifier. In Demb\u00e9l\u00e9 and Kastner (2003),\nthe fuzzifier is obtained with an empirical method calculating the\ncoefficient of variation of a function of the distances between\nall objects of the entire data set. Another approach searches\nfor a minimal fuzzifier value for which the cluster analysis of\nthe randomized data set produces no meaningful results, by\ncomparing a modified partition coefficient for different values of\nboth parameters (Futschik and Carlisle, 2005). The calculations in\nthese two methods imply operations on the entire data set and\nbecomes computationally expensive for large data sets.\nHere, we introduce a method to determine the value of the\nfuzzifier without using the current data set. For high-dimensional\ndata sets, the fuzzifier value depends directly on the dimension of\nthe data set and its number of objects, and so avoids processing\nof the complete data set. For low-dimensional data sets with small\nnumbers of objects, we were able to considerably reduce the\nsearch space for finding the optimal value of the fuzzifier. This\nimprovement helps to choose the right parameter set and to save\ncomputational time when processing large data sets.\nOur study shows that the optimal fuzzifier generally takes values\nfar from the frequently used value 2. We focused mainly on the\nclustering of biological data coming from gene expression analysis\nof microarray data or from protein quantifications. However, the\npresent method can be applied to any data set for which one wants\nto detect clusters of non-random origin.\nIn the following section the algorithm of fuzzy c-means clustering\nis introduced and the concept to avoid random cluster detections\nis explained. We present a simplified model showing a strong\ndependence of the fuzziness on the main properties of the data\nset and confirm this result by evaluating randomized artificial data\nsets. We distinguish between valid and invalid cluster validations by\nlooking at the minimal distances between the found centroids. This\nrelationship is quantified by fitting a mathematical function to the\nresults for the minimum centroid distance.\nFinally, we determine the second parameter of the cluster\nvalidation, the number of clusters. Different validation indices are\ncompared for artificial and real data sets.\n\n2 DATA SET AND ALGORITHM\nClustering algorithms are often used to analyze a large number\nof objects, for example genes in microarray data, each containing\na number of values obtained at different experimental conditions.\nIn other terms, the data set consists of N object vectors of\nD dimensions (experimental conditions), and thus an optimal\nframework contains N \u00d7D experimental values. The aim is to group\nthese objects into clusters with similar behaviors.\n\n2\n\nMissing values can be replaced for example by the average of\nthe existing values for the object. In gene expression data and in\nquantitative proteomics data, the values of each object represent\nonly a relative quantity to be compared to the other values of the\nobject. Therefore, the focus is on fold-changes and not on absolute\nvalue changes (a 2-fold, i.e. 200%, increase has the same weight as\na 2-fold decrease, 50%). In this case, the values are transformed by\ntaking their logarithm before the data is to be evaluated. Each object\nis normalized to have values with mean 0 and standard deviation 1.\nThe fuzzy c-means clustering for a given parameter set c, m \u2013 the\nnumber of clusters and the fuzzifier \u2013 corresponds to minimizing the\nobjective function,\nJ(c, m) =\n\nc X\nN\nX\nk=1 i=1\n\n(uki )m |xi \u2212 ck |2 ,\n\n(1)\n\nwhere we used Euclidean metrics for the distances between\ncentroids ck and objects xi . Here, uki denotes the membership\nvalue of object i to the cluster k, satisfying the following criteria,\nc\nX\n\nuki = 1 ; 0 < uki < 1 .\n\n(2)\n\nk=1\n\nThe following iteration scheme allows the calculation of the\ncentroids and the membership values by solving\n\nck =\n\nN\nP\n\n(uki )m xi\n\ni=1\nN\nP\n\n(3)\n(uki )m\n\ni=1\n\nfor all k and afterwards obtaining the membership values through\nuki =\n\n1\n\u0011 1 .\nc \u0010\nP\n|xi \u2212ck |2 m\u22121\n\ns=1\n\n(4)\n\n|xi \u2212cs |2\n\nA large fuzzifier value suppresses outliers in data sets, i.e. the\nlarger m, the more clusters share their objects and vice versa. For\nm \u2192 1, the method becomes equivalent to k-means clustering\nwhereas for m \u2192 \u221e all data objects have identical membership\nto each cluster.\nWe minimize the objective function J(c, m) by carrying out\n100 iterations of Eqs. (3) and (4). The application of Eqs. (3-4)\nconverges to a solution that might be trapped in a local minimum,\nrequiring the user to repeat the minimization procedure several times\nwith different initial conditions. In order to be able to carry out a vast\nparameter study, we limited the evaluation to 5-10 performances per\ndata set and parameter set, taking the performance corresponding to\nthe best clustering result, i.e. the one with the smallest final value of\nthe objective function.\nThe final classification of a data set into different clusters in fuzzy\nclustering is not as clear as in the case of k-means clustering where\neach object is assigned to exactly one cluster. In fuzzy c-means\nclustering, each object belongs to each cluster, to the degree given\nby the membership value. The centroid, i.e. the center of a cluster,\ncorresponds therefore to the center of all objects of the data set, each\ncontributing with its own membership value. As a consequence, we\nneed to define a threshold that defines whether an object belongs to a\n\n\fA simple and fast method to determine the parameters for fuzzy c\u2013means cluster validation\n\ncertain cluster. Ideally, this threshold is set to 1/2. Hence, due to the\nlimitation of Eq. (2), each object belongs to maximally one cluster.\nA non-empty cluster with at least one object having a membership\nvalue greater than 1/2 is called a hard cluster.\nThe number of hard clusters cfinal found in the cluster validation\ncan be smaller than the number of previously defined clusters, c.\nTherefore we can define the case cfinal < c to be a case of no\nsolution for the application of the cluster validation. In other words,\na cluster validation leading to at least one empty cluster will not be\nconsidered as a valid performance.\nBy distinguishing cases for which the cluster validation gives a\nvalid result and cases of invalid results it is possible to identify\nparameter regions where the algorithm identifies clusters that may\nresult from random fluctuations. As example, take a data set\nand its randomized counterpart. We now fix c and compare the\nresults of the clustering for increasing fuzzifier values, m. At\nm = 1, the cluster validation is equivalent to k-means clustering,\nassigning exactly one cluster to each object and the no-solution\ncase does not exist. The clustering of both the original and the\nrandomized data set will give c valid clusters. By increasing the\nvalue of the fuzzifier, the membership values of outliers become\nmore distributed between the clusters whereas objects pertaining\nto real clusters get their largest membership value decreased only\nslightly. Each cluster looses object members with membership\nvalues larger than 1/2 and the total number of objects that are\nassigned to a cluster as hard members decreases. As the objects\nof a randomized data set are distributed almost homogeneously in\ncluster space, the clustering algorithm stops to detect a total of\nc hard clusters above a certain threshold of the fuzzifier. When\nfurther increasing m, also the objects in the original data set will\nhave their largest membership values fall below 1/2 and so the\nclustering of the original data will stop to produce valid results\nabove another threshold of m. The parameter region between these\ntwo thresholds is of particular interest. Within this region, only\nthe clustering of the original data set produces valid results and\nthus the found clusters can be understood to correspond to nonrandom object groupings. Precisely, we prefer to take an as low as\npossible value of the fuzzifier, combining minimal fuzziness and\nmaximal cluster recognition. The procedure presented in the next\nsections shows how to obtain a minimal value of m that still does\nnot give a valid solution for the clustering of the randomized data\nset. A data set having the same threshold for both the clustering\nof the original set and the randomized one should be discarded\nas it is too noisy. However, we will see that the value of the\nfuzziness increases strongly for low-dimensional data sets and thus\na compromise between accepting clusters with members of noisy\norigin and low detection of patterns must be found.\n\n3 ARGUMENTS FOR A FUNCTIONAL\nRELATIONSHIP BETWEEN THE FUZZIFIER AND\nTHE DATA SET STRUCTURE\nA strong relationship between the fuzzifier and the basic properties\nof the data set can be demonstrated by means of a simplified\nmodel system. With increasing dimension, clusters are less likely\nto be found in a completely random data set. In order to illustrate\nthis dependency mathematically, one might reduce the system to\na binary D-dimensional object space, i.e. xid \u2208 {\u22121, 1}. Let us\n\nTable 1. Summary of the parameters.\n\nParameters of the clustering\n\nParameters of the artificial data set\n\nm: fuzzifier\n\nN : number of objects\nD: number of dimensions of an object\nM : number of Gaussian\u2013distributed clusters\nNO : number of data points per cluster\nw: standard deviation of Gaussian\n\nc: number of clusters\n\nnow look at a cluster that contains an accumulation of objects at a\ngiven point in object space. E.g., for a purely random object, the\nprobability to have xi = (1, 1, ..., 1) is given by 2\u2212D . Furthermore,\nthe probability to have half of all objects of the data set with this\nparticular value equals to,\n!\nr\n\u0011N\n\u0011N\nN \u2212D N2 \u0010\n2 N(1\u2212 D2 ) \u0010\n2\n\u2212D 2\n\u2248\n,\n2\n1\n\u2212\n2\n2\n1 \u2212 2\u2212D\nN\n\u03c0N\n2\n(5)\nwhere we used the Stirling approximation. For 2\u2212D \u226a 1, the\nD p\nright side of Eq. (5) might be approximated by 2N(1\u2212 2 ) 2/(\u03c0N ).\nHence, the probability for a well defined cluster decreases\nexponentially with respect to the dimension of the data set, and\nslightly slower for a increasing number of objects in the set. As a\nconsequence, the clustering parameter value m being a measure for\nthe fuzziness of the system should follow these tendencies at least\nqualitatively. This finding argues strongly against an application of\nthe fuzzy algorithm by merely using m = 2. We will show that the\nsimplified model predicts the dependencies on both quantities in the\nright way.\nAn extensive evaluation of the clustering procedure is carried\nout using artificially generated data sets as input. Each object\ncorresponds to a random point generated out of D-dimensional\nGaussian distributions with standard deviation w. The data set\nconsists of M Gaussian\u2013distributed clusters with each having NO\nobjects, leading to a total of N = NO \u00d7 M objects in the set. Each\nGaussian is centered at a random position in object space, having\ncoordinates between 0 and 10 for each dimension. An optimal\ncluster validation should identify c = M as best solution. The\nparameters of the fuzzy c-means algorithm and the parameters of\nthe artificial data set are summarized in Table 1.\nA first step to find an optimal value of the fuzzifier consists\nin applying the clustering procedure to randomized data sets. We\ngenerate these sets by random permutations of the values of each\nobject. A threshold for the fuzzifier value m is reached as soon as\nthe clustering procedure does not provide any valid solution for the\nrandomized set. This corresponds to the case where the number of\nhard clusters is smaller than the value of the parameter c. However,\nanother criteria allows a more accurate estimation. We will refuse\na clustering solution having two centroids that coincide, i.e. their\nmutual distance falls below some predefined value.\nFig. 1 shows both, the number of hard clusters as well as the\nminimum centroid distance for different realizations of artificial\ndata sets. There is a sudden decay to zero of both quantities when\nincreasing the fuzzifier. Three important conclusions can be made\nfrom the results depicted in Fig. 1: First, the position of the decay\n\n3\n\n\fVeit Schw\u00e4mmle12 , and Ole N\u00f8rregaard Jensen1\n\nc\n\nD=5\n\n10\n\n10\n\n20\n\n15\n\n15\n\nD=6\n\n10\n\n10\n\n20\n\n1.6\n1.8\nm\n\n2.0\n\n2.2\n\nD=8\n\n10\n\n20\n\n1.4\n\n1.6\n1.8\nm\n\n2.0\n\n2.2\n\n15\n\nD=10\n\n10\n\n5\n\n0\n\nminimum centroid distance\n\n20\n\n15\n\n5\n\n5\n1.2\n\n# hard clusters\n\n20\n\n10\n\n5\n\n0\n\nminimum centroid distance\n\n15\n\n15\n\n5\n1.4\n\n20\n\n10\n\n5\n5\n1.2\n\n# hard clusters\n\n20\n\nc\n\n15\n\n15\n\n# hard clusters\n\n20\n\nc\n\n20\n\nc\n\n# hard clusters\n\n20\n\n5\n1.2\n\n20\n\n1.4\n\n1.6\n1.8\nm\n\n2.0\n\n2.2\n\n0\n\nminimum centroid distance\n\n1.2\n\n20\n\n1.4\n\n1.6\n1.8\nm\n\n2.0\n\n2.2\n\nminimum centroid distance\n\n0\n\n2.5\n\n2.0\n2.0\n15\n\n1.5\n\n15\n\n2.0\n15\n\n1.0\n\n10\n\n10\n\n0.5\n5\n1.4\n\n1.6\n1.8\nm\n\n2.0\n\n2.2\n\n0.0\n\n1.0\n\n0.5\n\n0.5\n5\n\n1.2\n\n1.4\n\n1.6\n1.8\nm\n\n2.0\n\n2.2\n\n0.0\n\n1.5\n\nD=10\n\nc\n\n1.0 10\n\n5\n1.2\n\nD=8\n\nc\n\nD=6\n\nc\n\nc\n\nD=5\n\n2.0\n15\n\n1.5\n\n1.5\n\n10\n\n1.0\n\n0.5\n5\n\n1.2\n\n1.4\n\n1.6\n1.8\nm\n\n2.0\n\n2.2\n\n0.0\n\n1.2\n\n1.4\n\n1.6\n1.8\nm\n\n2.0\n\n2.2\n\n0.0\n\nFig. 1. Showing the number of hard clusters and the minimum centroid distances of randomized data sets for different values of m (horizontal axis) and c\n(vertical axis). The object points are Gaussian-distributed with standard deviation w = 1, dimensions 5,6,8 and 10, and were randomized afterwards. There\nare 500 objects per data set. The threshold of m where the number of hard clusters becomes smaller than c and the minimum centroid distance approaches\nzero does not vary significantly for different c. Moreover, the m-position of the threshold is the same for both measures within the same dimension.\n\nof the minimal centroid distances coincides with the one where the\nnumber of hard clusters changes from c to c \u2212 1. Apparently, a\ncluster without any membership values over the 1/2 limit (an empty\ncluster) has always its centroid coincide with the centroid of one of\nthe hard clusters. We could not find any mathematical explanation\nfor this behavior, but our analysis shows that this relation seems to\nbe a general characteristics of the fuzzy c-means algorithm. Second,\nthe minimum centroid distance decay occurs at almost exactly the\nsame value of m over the entire range of c. This seems to be typical\nbehavior in randomized data sets. Third, the m-position of the decay\ndecreases for higher dimensions of the data set. High-dimensional\ndata sets have a structure where random clusters are less likely as\nalready illustrated with the simplified model presented above. We\nwill take the minimum centroid distance to measure the m-value\nof the threshold in the following analysis, which is from now on\ndenoted mt .\nFig. 2 compares the minimum centroid distance for differently\ndistributed data sets, each randomized before applying the cluster\nvalidation. The picture remains mainly the same, with exception of\nthe case M = 1, where the threshold mt lies at a slightly higher\nvalue. The reason is that the threshold still varies within some range\nfor randomized data sets of equal dimension and number of objects.\nThe magnitude of this variation decreases for high-dimensional data\nsets.\nDespite the normalization of each object to have standard\ndeviation 1 and mean 0, a strong bias of the values towards certain\ndimensions may occur. This bias leads to different results for the\nclustering of the randomized data set. By processing different data\nsets with the same parameters but different positions of the artificial\nGaussian\u2013distributed objects' center, we try to capture the effects\nof both symmetric as well as biased data sets. The case M = 1 in\nFig. 2 corresponds to the clustering results of mostly strongly biased\ndata. The bias becomes large the more the center of the Gaussian\n\n4\n\ndeviates from the origin of the coordinate system. For M > 1,\nthis bias becomes smoothed out by randomization and therefore mt\nvaries much less. For example, a biased data set would be gene\nexpression data where most of the genes are up-regulated at one\nof the experimental stages (dimensions).\nThe analysis of the simplified model showed also a dependency of\nthe fuzziness in the data set on the number of objects, N , although\nweaker than the one on the dimension of the data set. Fig. 3 confirms\nthis result, showing that mt increases for smaller N and saturates at\na certain level for large N & 1000.\n\n4 ESTIMATING THE OPTIMAL VALUE OF THE\nFUZZIFIER\nWe now focus on the estimation of the dependency of the threshold\non both N and D, i.e. we neglect the effect of biased data sets. This\nthreshold will then be taken as the optimal value. A rule of thumb\nfor the maximum number of clusters in a data set is that it does\nnot exceed the square root of the number of objects (Zadeh, 1965).\nAs the threshold of the minimum of centroid distances mt does not\nvary with c, we determine the threshold in the following analysis\n\u221a\nby carrying out cluster validations with different m for c = N .\nPrecisely, the threshold mt corresponds to the value of the fuzzifier\nat which the minimum centroid distance falls below 0.1 for the first\ntime. Note, that we hereby exclude the situation that the centroids\nof two clusters locate at mutually small distances of less than 0.1.\nHowever, this limitation did not affect the results.\nThe clustering is carried out 5-10 times, each validation for a\ndifferent randomized artificial data set having the same parameters.\nFrom these different runs we take the largest value of mt .\nThe usage of m = mt in the cluster validation of the original data\nset has two advantages. First, a data set lacking non-random clusters\n\n\fA simple and fast method to determine the parameters for fuzzy c\u2013means cluster validation\n\n20\n\nminimum centroid distance\n\n20\n\nminimum centroid distance\n\n2.5 20\n\nminimum centroid distance\n\n2.5\n\n20\n\nminimum centroid distance\n\n2.5\n2.0\n\n1.0\n\n10\n\n0.5\n5\n\n1.0 10\n\n1.0\n\n0.5\n\n0.5\n\n5\n1.1\n\n1.2\n\n1.3\nm\n\n1.4\n\n1.5\n\n0.0\n\n1.5\n\n1.5\n\nM=5\n\n1.2\n\n1.3\nm\n\n1.4\n\n1.5\n\n0.0\n\n1.0\n\n10\n\n0.5\n\n5\n1.1\n\nM=10\n\nc\n\nc\n\n10\n\n15\n\n1.5\n\nM=2\n\n2.0\n\n2.0\n15\n\n1.5\n\nM=1\n\nc\n\n15\n\nc\n\n2.0\n\n15\n\n5\n1.1\n\n1.2\n\n1.3\nm\n\n1.4\n\n1.5\n\n0.0\n\n1.1\n\n1.2\n\n1.3\nm\n\n1.4\n\n1.5\n\n0.0\n\nFig. 2. Landscape of the minimum centroid distance for randomized data sets with Gaussian-distributed clusters. The numbers of previously set different\nclusters are M = 1,2,5 and 10. The data sets have the same total number of objects, N = 1000, the dimension of the sets is 10 and we have w = 1. No\nsignificant differences can be observed except for the panel with M = 1 where the threshold mt seems to have a slightly larger value.\n\n40\n\n2.5\n\n50\n\n2.0\n\n40\n\n1.5\n\n30\n\n1.0\n\n20\n\n1.5\n\n30\n\nN=500\n1.0\n\n20\n\n0.5\n10\n\n2.5 50\n\n1.2\n\n1.3\nm\n\n1.4\n\n1.5\n\n0.0\n\n30\n\n2.5\n\n50\n\n2.0\n\n40\n\n1.5\n\n30\n\nN=1000\n1.0\n\n20\n\n0.5\n10\n\n1.1\n\nminimum centroid distance\n\n2.0 40\n\nc\n\nc\n\nN=250\n\nminimum centroid distance\n\n1.2\n\n1.3\nm\n\n1.4\n\n1.5\n\n0.0\n\n2.0\n\n1.5\n\nN=2500\n1.0\n\n20\n0.5\n\n0.5\n10\n\n10\n1.1\n\nminimum centroid distance\n\nc\n\nminimum centroid distance\n\nc\n\n50\n\n1.1\n\n1.2\n\n1.3\nm\n\n1.4\n\n1.5\n\n0.0\n\n1.1\n\n1.2\n\n1.3\nm\n\n1.4\n\n1.5\n\n0.0\n\nFig. 3. Landscape of the minimum centroid distance from randomized data sets with different numbers of objects, N = 250, 500, 1000, 2500. The threshold\nmt decreases for increasing N and seems to saturate for very large numbers. We took D = 10, w = 1 and M = 5. The black points indicate the position\nwhere we take the fuzzifier threshold mt .\n\nThe corresponding prediction for mt is given by f (7, 200) = 1.75.\ndoes not provide any reasonable results, i.e. the number of detected\nThe only difference between the data sets consists in the position\nhard clusters is lower than the parameter c. This means that the value\nof the mean of the Gaussian, and thus the bias of the data. The\nof the minimum centroid distance is around zero for all c. Second,\nmaximum of the distribution lies at a slightly smaller value than\nthis smallest allowed value of m guarantees an optimal estimation\nthe one predicted in Eq. (6). The figure shows also that the lower\nof a maximal number of clusters which is in general better than for\nlimit of mt is rather well defined whereas high values are possible,\nlarger m and so still ensures the recognition of barely detectable\neven far away from the maximum. Consequently, for data sets with\nclusters.\nsmall N and D, Eq. (6) may be more useful for the estimation of\nThe dependency of mt on the dimension of the data set is shown\nthe lower limit of mt than for its exact prediction. However, the\nin Fig. 4a and compared to the values calculated by the method\nprediction works much better for larger values of D and N where\nintroduced in Demb\u00e9l\u00e9 and Kastner (2003). The curves from the\ncomputational time becomes an issue.\nlatter method exhibit the same tendency but an overestimation of\nthe fuzzifier.\nA thorough analysis, calculating mt for randomized data sets of\ndifferent dimensions and object numbers shows a general functional\nrelation between mt and the data set properties. The following\nfunction provides a good fit of the curves for all combinations of Table 2. Comparing estimated values of mt to their predictions from Eq. (6).\nN and D,\n\u0012\n\u0013\nData set\nD\nN\nmt\nf (D, N )\n1418\nf (D, N ) = 1 +\n+ 22.05 D\u22122\nN\n\u0012\n\u0013\niTRAQ1, suppl. table 1 (Pierce et al., 2008)\n7\n1886\n1.54\n1.56\n12.33\n829\n1.56\n1.59\n+\n+ 0.243 D\u22120.0406 ln(N)\u22120.1134 .(6) iTRAQ2, suppl. table 2 (Pierce et al., 2008) 7\nN\niTRAQ3, Table 4 (Wolf-Yadlin et al., 2007)\n7\n222\n1.81\n1.73\nBoth the data points of mt and their empirical fit with Eq. (6)\nare depicted in Fig. 4b. The prediction with the empirical formula\nimproves for large N and large D. For smaller values of these input\nvalues, the mt obtained from the artificial sets may deviate from the\npredicted value due to their dependency on the individual data set.\nWe calculated the density distribution of mt for artificial sets with\nthe same parameters, setting M = 1, D = 7 and N = 200 (Fig. 5).\n\nEcoli (Horton and Nakai, 1996)\nAbalone (Nash et al., 1994)\nSerum (Iyer et al., 1999)\nYeast1 (Tavazoie et al., 1999)\nYeast2 (Cho et al., 1998)\nIonosphere (Sigillito et al., 1989)\n\n7\n8\n13\n16\n17\n34\n\n336\n4177\n517\n2885\n2951\n351\n\n1.64\n1.41\n1.27\n1.18\n1.17\n1.13\n\n1.67\n1.44\n1.25\n1.16\n1.15\n1.1\n\n5\n\n\fVeit Schw\u00e4mmle13 , and Ole N\u00f8rregaard Jensen1\n\n(a)\n\nEq. (6). We find a deviation for the iTRAQ3 data set having a small\nD = 7 and N = 222. From Fig. 5 we see that the higher value of\nmt = 1.81 is still within the range of the distribution. Note, that\nthe optimal fuzzifier value for the yeast2 data set was estimated to\nbe m = 1.15 in Futschik and Carlisle (2005), identical with our\nestimation.\n\n(Dembele and Kastner) - 1\nmt - 1\n\n1\n\n5 DETERMINING THE NUMBER OF CLUSTERS\n\n0.1\n\n10\n\n100\n\n1000\n\nD\n\n20\n\nminimum centroid distance\n\n20\n\nminimum centroid distance\n\n3.0\n\n20\n\nminimum centroid distance\n\n3.0\n\n3.0\n2.5\n\n2.5\n\n2.5\n15\n2.0\n\nc\n\n10\n\n10\n\n10\n\nmt - 1\n\n20\n\n1\n\n1.4\n\n1.6\nm\n\n1.8\n\n2.0\n\n2.5\n\n0.5\n\n5\n\n0.0\n\nminimum centroid distance\n\n1.0\n\n0.5\n\n0.5\n\n1.2\n\n1.5\n\n10\n1.0\n\n1.0\n\n5\n\n2.0\n\nw=1\n1.5\n\n1.5\n\nN=10\nN=100\nN=1000\nN=10000\nf(D,N)\n\n(b)\n\n15\n\nw=0.5\nc\n\nw=0.1\n100\n\n2.0\n\nc\n\n15\n\n5\n1.2\n\n20\n\n1.4\n\n1.6\nm\n\n1.8\n\n2.0\n\n0.0\n\nminimum centroid distance\n\n1.2\n\n20\n\n1.4\n\n1.6\nm\n\n1.8\n\n2.0\n\n0.0\n\nminimum centroid distance\n2.0\n\n2.0\n2.0\n15\n\n15\n\nw=2\n\n0.01\n\n1.0\n\n10\n\n100\n\n5\n1.2\n\n0\n\n1\n\nDensity\n2\n3\n4\n\n5\n\n6\n\nFig. 4. (a): A comparison of our method for the estimation of the fuzzifier\nto the one presented in Demb\u00e9l\u00e9 and Kastner (2003) shows that the value of\nm is mostly overestimated in the latter. In addition, our method allows to\ncope with a larger dimensional range. (b): Comparing the threshold of the\nminimal centroid distance for randomized data sets with different numbers\nof objects, N . The threshold increases for larger N and the curve seems to\napproach a limiting shape for very large N . Fluctuations become large for\nD < 10. The lines show the values of the fitting function, Eq. (6).\n\n1.5\n\n1.6\n\n1.7\n\n1.8\n\n5\n\n1.9\n\n2.0\n\n2.1\n\n2.2\n\nmt\n\nFig. 5. The density distribution of threshold values for different\nimplementations of randomized artificial data sets with M = 1, D = 7,\nw = 0.1 and N = 200.\n\nEq. (6) accounts also for randomized real data sets where the\ndistribution within a cluster may be non-Gaussian. For the analysis,\nwe tested data sets from different origin including biological\ndata from protein research (Horton and Nakai, 1996; Pierce et al.,\n2008; Wolf-Yadlin et al., 2007), microarray data (Iyer et al., 1999;\nTavazoie et al., 1999; Cho et al., 1998) and data gathered from nonbiological experiments (Nash et al., 1994; Sigillito et al., 1989).\nTable 2 compares the minimum centroid threshold calculated\nfrom the randomized data sets to the empirical value obtained from\n\n1.4\n\n1.6\nm\n\n1.8\n\n2.0\n\n0.0\n\n1.0\n\n10\n\n0.5\n\n0.5\n\n0.5\n\n10\nD\n\n6\n\n1.0\n\nw=4\nc\n\nc\n\n10\n\n1.5\n\n1.5\n\nw=3\n\n1.5\n\nc\n\n0.1\n\n15\n\n5\n1.2\n\n1.4\n\n1.6\nm\n\n1.8\n\n2.0\n\n0.0\n\n1.2\n\n1.4\n\n1.6\nm\n\n1.8\n\n2.0\n\n0.0\n\nFig. 6. Landscape of the minimum centroid distances for a set of 10 clusters\nwith each having 100 objects of D = 8. The clusters were produced to\nhave a Gaussian distribution with different standard deviations, being w =\n0.1, 0.5, 1 (upper panels left, middle and right), and w = 2, 3, 4 (lower\npanels left, middle and right). The black lines indicate mt and ct .\n\nAfter calculating the optimal value of the fuzzifier by either using\nEq. (6) or determining mt directly as done above, the final step\nconsists in estimating the number of clusters in the data set. Various\nvalidity indices for the quality of the clustering are present in the\nliterature. They in general are a function of the membership values,\nthe centroid coordinates and the data set. The results for the indices\nsummarized in Table 3 will be compared for artificial and real data\nsets.\nFirst we take another look on the minimum centroid distance,\nVMCD , now taken from the cluster validation of artificial (not\nrandomized) data sets (Fig. 6). The panels show VMCD for data\nsets with 10 Gaussian\u2013distributed clusters, each panel for a set of\nGaussians with different standard deviations. For data sets with\nclearly separated clusters (small standard deviations), the picture is\ncompletely different to the one of a randomized data set (Figs. 1\u20133).\nA strong decay, this time not necessarily to zero, occurs at c = ct\nindependent of the value of the used fuzzifier m. Note that in the\nrandomized case the decay was at mt for all c. The position of the\nsudden decrease coincides with the number of clusters M = 10\nof the artificial data set, and thus the minimum centroid distance\nprovides a reasonable measure also to determine the optimal number\nof clusters. For more mixed clusters, the landscape transforms\ngradually into the picture observed for randomized sets.\nThe parameter landscapes of real data sets will exhibit a\ncombination of two extremes, a plateau below the threshold ct for a\ndata set with clearly distinguishable cluster and a plateau below mt\nfor a completely noisy data set. We can also observe that the number\n\n\fA simple and fast method to determine the parameters for fuzzy c\u2013means cluster validation\n\n15\n\n20\n\n20\n\n20\n\n15\n\n20\n\n20\n19\n\n0.5\n0.2\n\n17\n\n0.3\n\n18\n\n0.4\n\n0.75\n0.70\n15\n\n20\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n20\n\n20\n\n\u22126.2\n10\nc\n\n5\n\n10\n\nc\n\n15\n\n20\n\n5\n\nc\n\n10\n\n15\n\n20\n\nc\n\n25\n10\n\n10\n\n15\n\n15\n\n\u22122500\n5\n\n5\n\n\u22123500\n\n15\nc\n\n10\n\n20\n\n\u22125.5\n\u22126.0\n\u22126.5\n10\n\n5\n\n30\n\n\u22121500\n\n\u22125.0\n\n35\n\n3.0\n2.5\n2.0\n1.5\n1.0\n\n5\n\n\u22125.8\n\n4.0\n15\nc\n\nPCAES index\n\n5\n\nc\n\n3.0\n10\n\nc\n\nfukuyama sugeno index\n\n20\n\nPCAES index\n\n2.5\n5\n\nc\n\nlog(xie beni index)\n\n15\nc\n\n15\n\n10\n\nc\n\nminimum centroid distance\n\n10\n\n10\n\n5\n\n5\n\nfukuyama sugeno index\n\n5\n\n20\n\n20\n\n\u22126.6\n\n15\n\n15\nc\n\n3.5\n\n17\n16\n15\n\n10\n\n10\n\nlog(xie beni index)\n\n\u22124000 \u22123500 \u22123000 \u22122500 \u22122000\n\n20\n19\n18\n\n0.5\n0.4\n0.3\n5\n\n5\n\n\u22127.0\n\n20\n\n20\n\n2.0\n\n15\n\n15\nc\n\n1.5\n\n10\n\n10\n\nminimum centroid distance\n\n0.6\n\n0.65 0.70 0.75 0.80 0.85\n\n0.85\n0.80\n0.75\n0.70\n\n5\n\n5\n\n0.65\n\naverage within\u2212cluster distance\n\naverage within\u2212cluster distance\n\n0.6\n\n0.80\n\n0.90\n0.85\n0.80\n0.75\n\npartition entropy\n\n0.70\n\nmodified partition coefficient\n\n0.90\n\npartition coefficient\n\npartition entropy\n21\n\nmodified partition coefficient\n0.7\n\npartition coefficient\n\nof found clusters decreases with increasing m (cases w = 2, w = 3\nand w = 4 in Fig. 6) as would be expected.\n\n5\n\n10\n\nc\n\n15\n\n20\n\nc\n\n5\n\n10\n\n15\n\n20\n\nc\n\nFig. 7. Comparison of the different validity indices for an artificial system\nof 500 10-dimensional data points, with 10 clusters each with 50 points. All\nindices show that c = 10 is the optimal solution.\n\nEq. (6) gives mt = 1.47 for the parameters of the artificial data\nsets in Fig. 6. The figure shows that some of the clusters may be\nrecognized even for w = 3 and w = 4, when using m = mt for the\nclustering (i.e. we find a decay of the minimum centroid distance\nat c = 7 for m = 1.47). For larger values of the fuzzifier, no\nclusters can be detected, whereas the decay begins to become less\naccentuated for smaller m-values. Hence, the minimum centroid\ndistances may be considered as a powerful validity index for the\ncase that the appropriate m = mt is chosen. Another advantage of\nusing VMCD is that its calculation is faster than the one of the other\nvalidity indices.\nFor a comparison of the different validation indices, we generated\na data set with D = 13, N = 500, M = 10 and w = 2 for which\n\nFig. 8. Comparison of the different validity indices for the serum data set.\nFor real data, it is obviously more difficult to estimate the number of clusters.\nHowever, some indices have a jump at or near c = 5.\n\nEq. (6) gives mt = 1.25. Fig. 7 shows the validation indices versus\nthe cluster number c using m = mt . All methods clearly indicate\nc = 10 as the optimal solution. Note, that there is also a strong\ndecay of VMCD at c = 10.\nReal data normally is more complex than the artificial sets\nanalyzed here. Not only the kind of noise may be different but also\nthe clusters may not have normal-distributed values and the clusters\nmight have different sizes. As a consequence, often an optimal\nparameter set does not exist, and the most appropriate solution must\nbe chosen manually out of the best candidates. As a test data set\nwe used the serum set (Iyer et al., 1999) that has the same number\nof dimensions and a similar number of objects as the artificial data\nset analyzed in Fig. 7. The validation indices now do not agree in\ngiving a clear indication for the number of clusters in the system\n(Fig. 8). However, most of them yield c = 5 as the optimal solution.\nThe abrupt decay of the minimum centroid distance at the same\n\nTable 3. Summary of the validation indices.\n\n1\nN\n\nN\nc P\nP\n\n(uij )2\ni=1 j=1\nc\nVM P C = 1 \u2212 c\u22121\n(1 \u2212 VP C )\nN\nc P\nP\n1\nuij log(uij )\nVP E = \u2212 N\ni=1 j=1\nN\nP\n(uij )m |xj \u2212ci |2\nc\n1 P j=1\nVAVCD = 1c N\nN\nP\ni=1\n(uij )m\nVP C =\n\nPartition coefficient (Bezdek, 1975)\nModified partition coefficient (Dave, 1996)\nPartition entropy (Bezdek, 1974)\n\nAv. within\u2013cluster distance (Krishnapuram and Freg, 1992)\n\nj=1\n\nVF S =\n\nFukuyama-Sugeno index (Fukuyama and Sugeno, 1989)\n\nN\nc P\nP\n\ni=1 j=1\n\n\uf8eb\n\n(uij )m \uf8ed|xj \u2212 ci |2 \u2212\nVXB =\n\nXie-Beni index (Xie and Beni, 1991)\n\nc\nN\nP\nP\n\ni=1 j=1\n\nVPCAES =\n\nN\nc P\nP\n\ni=1 j=1\n\nN min|ci \u2212cj |\n\nMinimum centroid distance\n\n(uij )2\nmin\n\n1\u2264i\u2264c\n\nN\nP\n\n(uij\n\nk=1\n\n)2\n\n!\n\n\u2212\n\nxk\n\nk=1\n\n(uij )m |xj \u2212ci |2\n\ni6=j\n\nPCAES (Wu et al., 2005)\n\n1\nN\n\nN\nP\n\nc\nP\n\ni=1\n\n\uf8eb\n\n2\n\n\uf8f6\n\n\u2212 ci \uf8f8\n\n2\n\n\uf8eb\n\n\uf8ec\n\uf8ec\n\uf8ec\nexp \uf8ec\n\uf8ed\u2212 min \uf8ed\nk6=i\n\nVM CD = min |ci \u2212 cj |\ni6=j\n\n!\n\n2\n\nc|xi \u2212xk |2\nc\nP\n\nl=1\n\nxl \u2212\n\nN\nP\n\ns=1\n\nxs /N\n\n\uf8f6\uf8f6\n\uf8f7\uf8f7\n\n! 2 \uf8f7\uf8f7\n\uf8f8\uf8f8\n\n7\n\n\fVeit Schw\u00e4mmle14 , and Ole N\u00f8rregaard Jensen1\n\nc = 5 is remarkable. Fig. 9a depicts the landscape of the minimum\ncentroid distance for the serum data set over a large range of m\nand c. First, we observe a similarity between Fig. 9a and the case\nw = 3 in Fig. 6 suggesting that the data set consists of overlapping\nbut distinguishable clusters. The minimum centroid distance has\na plateau for c \u2264 5 and m < 2 with a decay at c = 5 over a\nconsiderable range of m-values around mt = 1.25 indicating c = 5\nas the optimal choice.\nFig. 9b shows the patterns of all clusters for the cluster validation\non the serum data set taking c = 5 and m = 1.25. The lines\ncorrespond to the coordinates of the centroids. Only objects with\nmembership values over 1/2 for the corresponding cluster are\nshown.\n\n20\n\nminimum centroid distance\n3.0\n2.5\n\n15\n\n(a)\nc\n\n2.0\n1.5\n\n10\n\n1.0\n0.5\n\n5\n1.5\n\n2.0\nm\n\nCluster 1\n\n2.5\n\n3.0\n\n0.0\n\nCluster 3\n\n6\n\n8\n\n10\n\n12\n\n1\n0\n\u22121\n2\n\n4\n\n6\n\n8\n\nCluster 4\n\nCluster 5\n\n10\n\n12\n\n4\n\n6\n\n8\n\n10\n\n12\n\nACKNOWLEDGEMENT\nFunding: VS was supported by the Danish Council for Independent\nResearch, Natural Sciences (FNU).\n\n(b)\n\n\u22122\n\n\u22122\n\n\u22121\n\n\u22121\n\n2\n\nD\n\n0\n\n0\n\n2\n\nD\n\n1\n\nD\n\n2\n\n4\n\n1\n\n2\n\n\u22122\n\n\u22122\n\n\u22121\n\n\u22122 \u22121\n\n0\n\n0\n\n1\n\n1\n\n2\n\n2\n\n2\n\n3\n\nCluster 2\n\n2005). We present here a new, fast and simple method to estimate\nthe fuzzifier being calculated from only two main properties of\nthe data set, its dimension and the number of objects. Using this\nmethod, we obtained not only an optimal balance between maximal\ncluster detection and maximal suppression of random effects but\nit also allows us to process larger data sets. The results suggest\nthat biased data leads to an increase of the value of the fuzzifier\nin low-dimensional data sets with a small number of objects (for\ninstance N < 200 and D < 8) and thus the parameters should\nbe chosen carefully for this type of data. The estimation is based\non the evaluation of the minimal distance between the centroids\nof the clusters found by the cluster validation. The minimum\ncentroid distance provides sufficient information for the estimation\nof the other parameter necessary for the clustering procedure, the\nnumber of clusters, and eliminates the need for calculation of\ncomputationally intensive validation indices.\nIn data from proteomic studies, especially labeled mass\nspectrometry data, protein expressions are compared over a\ngenerally smaller number of stages (for instance less or equal to\n8 in iTRAQ data). As our study shows, the optimal value of the\nfuzzifier increases strongly at low dimensions to values larger than\nm = 2 making it difficult to obtain well-defined clusters. Therefore,\na compromise needs to be made, by allowing lower fuzzifier values,\nm < mt , admitting the influence of random fluctuations to the\nresults. A quantification of the confidence of the cluster validation\nof low-dimensional data needs to be carried out or other methods\nof data comparison, such as direct comparison of the absolute data\nvalues, must complement the data analysis.\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\nREFERENCES\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\nBabuska, R. (1998). Fuzzy Modeling for Control. Kluwer Academic Publishers,\nDordrecht.\nBezdek, J. C. (1974). Cluster validity with fuzzy sets. J. Cybernetics, 3, 58\u201372.\nFig. 9. (a): Landscape of the minimum centroid distances for the serum\nBezdek, J. C. (1975). Mathematical models for systematics and taxonomy. In G. F.\ndata set. The strongest decay is found for c = 5 around mt = 1.25.\nEstabrook, editor, Proceedings of the 8th International Conference on Numerical\nTaxonomy, San Francisco. Freeman.\nThe black line denotes mt = 1.25, (b): Patterns of the objects in all 5\nBezdek, J. C. (1981). Pattern Recognition With Fuzzy Objective Function Algorithms.\nclusters depicting only the ones with membership values larger than 1/2.\nPlenum Press, New York.\nThe centroids are shown by the lines.\nCho, R. J., Campbell, M. J., Winzeler, E. A., Steinmetz, L., Conway, A., Wodicka, L.,\nWolfsberg, T. G., Gabrielian, A. E., Landsman, D., Lockhart, D. J., and Davis, R. W.\n(1998). A genome-wide transcriptional analysis of the mitotic cell cycle. Mol. Cell,\n2, 65\u201373.\nDave, R. N. (1996). Validating fuzzy partition obtained through c-shells clustering.\nPattern Recogn. Lett., 17, 613\u2013623.\n6 CONCLUSIONS\nDemb\u00e9l\u00e9, D. and Kastner, P. (2003). Fuzzy C-means method for clustering microarray\ndata. Bioinformatics, 19, 973\u2013980.\nIn fuzzy c-means cluster validation, it is crucial to choose\nD\u00f6ring, C., Lesot, M.-J., and Kruse, R. (2006). Data analysis with fuzzy clustering\nthe optimal parameters since a large fuzzifier value leads to\nmethods. Comput. Stat. Data An., 51(1), 192\u2013214.\nloss of information and a low one leads to the inclusion of\nDunn, J. C. (1973). A fuzzy relative of the isodata process and its use in detecting\nfalse observations originating from random noise. The value of\ncompact well-separated clusters. J. Cybernet., 3, 32\u201357.\nthe fuzzifier was frequently set to 2 in many studies without\nEisen, M. B., Spellman, P. T., Brown, P. O., and Botstein, D. (1998). Cluster analysis\nand display of genome-wide expression patterns. Proc. Natl. Acad. Sci. U.S.A., 95,\nspecification of the amount of noise in the system. We show here\n14863\u201314868.\nthat the strong dependence of the optimal fuzzifier value on the\nFukuyama, Y. and Sugeno, M. (1989). A new method of choosing the number of\ndimension of the system requires fine\u2013tuning of this parameter.\nclusters for the fuzzy c-means method. Proc. 5th Fuzzy Syst. Symp., page 247.\nTo our knowledge, two methods exist to obtain the fuzzifier by\nFutschik, M. E. and Carlisle, B. (2005). Noise-robust soft clustering of gene expression\nprocessing the data set (Demb\u00e9l\u00e9 and Kastner, 2003; Futschik and Carlisle, time-course data. J. Bioinform. Comput. Biol., 3, 965\u2013988.\nD\n\n8\n\nD\n\n\fA simple and fast method to determine the parameters for fuzzy c\u2013means cluster validation\n\nHanai, T., Hamada, H., and Okamoto, M. (2006). Application of bioinformatics for\nDNA microarray data to bioscience, bioengineering and medical fields. J. Biosci.\nBioeng., 101, 377\u2013384.\nH\u00f6ppner, F., Klawonn, F., Kruse, R., and Runkler, T. (1999). Fuzzy Cluster Analysis.\nJohn Wiley & Sons, Inc., New York.\nHorton, P. and Nakai, K. (1996). A probabilistic classification system for predicting\nthe cellular localization sites of proteins. Proc. Int. Conf. Intell. Syst. Mol. Biol., 4,\n109\u2013115.\nIyer, V. R., Eisen, M. B., Ross, D. T., Schuler, G., Moore, T., Lee, J. C., Trent, J. M.,\nStaudt, L. M., Hudson, J., Boguski, M. S., Lashkari, D., Shalon, D., Botstein, D.,\nand Brown, P. O. (1999). The transcriptional program in the response of human\nfibroblasts to serum. Science, 283, 83\u201387.\nKrishnapuram, R. and Freg, C.-P. (1992). Fitting an unknown number of lines and\nplanes to image data through compatible cluster merging. Pattern Recogn., 25,\n385\u2013400.\nNash, W. J., Sellers, T. L., Talbot, S. R., Cawthorn, A. J., and Ford, W. (1994). The\nPopulation Biology of Abalone (Haliotis species) in Tasmania. I. Blacklip Abalone\n(H. rubra) from the North Coast and Islands of Bass Strait. Sea Fisheries Division\nTechnical Report, 48.\nPal, N. R. and Bezdek, J. C. (1995). On cluster validity for the fuzzy c\u2013means model.\nFuzzy Systems, 3, 370\u2013379.\n\nPierce, A., Unwin, R. D., Evans, C. A., Griffiths, S., Carney, L., Zhang, L., Jaworska,\nE., Lee, C. F., Blinco, D., Okoniewski, M. J., Miller, C. J., Bitton, D. A., Spooncer,\nE., and Whetton, A. D. (2008). Eight-channel iTRAQ enables comparison of the\nactivity of six leukemogenic tyrosine kinases. Mol. Cell Proteomics, 7, 853\u2013863.\nSigillito, V. G., Wing, S. P., Hutton, L. V., and Baker, K. B. (1989). Classification\nof radar returns from the ionosphere using neural networks. John Hopkins APL\nTechnical Digest, 10.\nTamayo, P., Slonim, D., Mesirov, J., Zhu, Q., Kitareewan, S., Dmitrovsky, E., Lander,\nE. S., and Golub, T. R. (1999). Interpreting patterns of gene expression with selforganizing maps: methods and application to hematopoietic differentiation. Proc.\nNatl. Acad. Sci. U.S.A., 96, 2907\u20132912.\nTavazoie, S., Hughes, J. D., Campbell, M. J., Cho, R. J., and Church, G. M. (1999).\nSystematic determination of genetic network architecture. Nat. Genet., 22, 281\u2013285.\nWolf-Yadlin, A., Hautaniemi, S., Lauffenburger, D. A., and White, F. M. (2007).\nMultiple reaction monitoring for robust quantitative proteomic analysis of cellular\nsignaling networks. Proc. Natl. Acad. Sci. U.S.A., 104, 5860\u20135865.\nWu, K.-L., Yu, J., and Yang, M.-S. (2005). A novel fuzzy clustering algorithm based on\na fuzzy scatter matrix with optimality tests. Pattern Recogn. Lett., 26(5), 639\u2013652.\nXie, X. L. and Beni, G. (1991). A validity measure for fuzzy clustering. IEEE Trans.\nPattern Anal. Mach. Intell., 13(8), 841\u2013847.\nZadeh, L. A. (1965). Fuzzy sets. Inf. Control, 8(3), 338\u2013353.\n\n9\n\n\f"}