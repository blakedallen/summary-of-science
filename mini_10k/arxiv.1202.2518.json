{"id": "http://arxiv.org/abs/1202.2518v4", "guidislink": true, "updated": "2013-06-22T10:14:11Z", "updated_parsed": [2013, 6, 22, 10, 14, 11, 5, 173, 0], "published": "2012-02-12T11:23:38Z", "published_parsed": [2012, 2, 12, 11, 23, 38, 6, 43, 0], "title": "Segmenting DNA sequence into `words'", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.3979%2C1202.6447%2C1202.0012%2C1202.3257%2C1202.0502%2C1202.5643%2C1202.0698%2C1202.2029%2C1202.4508%2C1202.0085%2C1202.0663%2C1202.0150%2C1202.2718%2C1202.0852%2C1202.1709%2C1202.1937%2C1202.6343%2C1202.1482%2C1202.3424%2C1202.2969%2C1202.0532%2C1202.0963%2C1202.0315%2C1202.1446%2C1202.6117%2C1202.1703%2C1202.2518%2C1202.4708%2C1202.0139%2C1202.2747%2C1202.0350%2C1202.3199%2C1202.4357%2C1202.4951%2C1202.0652%2C1202.1236%2C1202.0853%2C1202.1100%2C1202.6675%2C1202.2294%2C1202.3035%2C1202.1049%2C1202.6217%2C1202.5119%2C1202.6642%2C1202.0981%2C1202.5363%2C1202.1861%2C1202.4664%2C1202.4111%2C1202.5951%2C1202.3889%2C1202.4306%2C1202.5083%2C1202.1821%2C1202.6477%2C1202.1696%2C1202.6484%2C1202.5629%2C1202.0703%2C1202.2130%2C1202.2529%2C1202.2094%2C1202.5472%2C1202.4311%2C1202.5662%2C1202.1042%2C1202.2374%2C1202.4828%2C1202.0374%2C1202.1409%2C1202.0060%2C1202.1308%2C1202.4395%2C1202.4362%2C1202.0080%2C1202.6614%2C1202.2295%2C1202.2124%2C1202.4916%2C1202.1921%2C1202.0638%2C1202.5984%2C1202.4195%2C1202.2240%2C1202.5780%2C1202.2651%2C1202.4769%2C1202.5046%2C1202.3577%2C1202.2439%2C1202.3585%2C1202.3534%2C1202.2020%2C1202.4476%2C1202.2937%2C1202.4089%2C1202.6038%2C1202.4982%2C1202.4383%2C1202.2019&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Segmenting DNA sequence into `words'"}, "summary": "This paper presents a novel method to segment/decode DNA sequences based on\nn-grams statistical language model. Firstly, we find the length of most DNA\n'words' is 12 to 15 bps by analyzing the genomes of 12 model species. Then we\ndesign an unsupervised probability based approach to segment the DNA sequences.\nThe benchmark of segmenting method is also proposed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.3979%2C1202.6447%2C1202.0012%2C1202.3257%2C1202.0502%2C1202.5643%2C1202.0698%2C1202.2029%2C1202.4508%2C1202.0085%2C1202.0663%2C1202.0150%2C1202.2718%2C1202.0852%2C1202.1709%2C1202.1937%2C1202.6343%2C1202.1482%2C1202.3424%2C1202.2969%2C1202.0532%2C1202.0963%2C1202.0315%2C1202.1446%2C1202.6117%2C1202.1703%2C1202.2518%2C1202.4708%2C1202.0139%2C1202.2747%2C1202.0350%2C1202.3199%2C1202.4357%2C1202.4951%2C1202.0652%2C1202.1236%2C1202.0853%2C1202.1100%2C1202.6675%2C1202.2294%2C1202.3035%2C1202.1049%2C1202.6217%2C1202.5119%2C1202.6642%2C1202.0981%2C1202.5363%2C1202.1861%2C1202.4664%2C1202.4111%2C1202.5951%2C1202.3889%2C1202.4306%2C1202.5083%2C1202.1821%2C1202.6477%2C1202.1696%2C1202.6484%2C1202.5629%2C1202.0703%2C1202.2130%2C1202.2529%2C1202.2094%2C1202.5472%2C1202.4311%2C1202.5662%2C1202.1042%2C1202.2374%2C1202.4828%2C1202.0374%2C1202.1409%2C1202.0060%2C1202.1308%2C1202.4395%2C1202.4362%2C1202.0080%2C1202.6614%2C1202.2295%2C1202.2124%2C1202.4916%2C1202.1921%2C1202.0638%2C1202.5984%2C1202.4195%2C1202.2240%2C1202.5780%2C1202.2651%2C1202.4769%2C1202.5046%2C1202.3577%2C1202.2439%2C1202.3585%2C1202.3534%2C1202.2020%2C1202.4476%2C1202.2937%2C1202.4089%2C1202.6038%2C1202.4982%2C1202.4383%2C1202.2019&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper presents a novel method to segment/decode DNA sequences based on\nn-grams statistical language model. Firstly, we find the length of most DNA\n'words' is 12 to 15 bps by analyzing the genomes of 12 model species. Then we\ndesign an unsupervised probability based approach to segment the DNA sequences.\nThe benchmark of segmenting method is also proposed."}, "authors": ["Wang Liang"], "author_detail": {"name": "Wang Liang"}, "author": "Wang Liang", "arxiv_comment": "12 pages,2 figures", "links": [{"href": "http://arxiv.org/abs/1202.2518v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.2518v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "q-bio.GN", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "q-bio.GN", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.2518v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.2518v4", "journal_reference": null, "doi": null, "fulltext": "Segmenting DNA sequence into 'words'\nWang Liang, Zhao KaiYong\nTencent, SOSO, Beijing, P.R. China\nHong Kong Baptist University, Department of Computer Science, HK, P.R. China\n*To whom correspondence should be addressed. E-mail:wangliang.f@gmail.com\n\n[Abstract] This paper presents a novel method to segment/decode DNA sequences based on\nstatistical language model. Firstly, we find the length of most DNA \"words\" is 12 to 15 bps by\nanalyzing the genomes of 12 model species. Then we apply the unsupervised approach to build the\nDNA vocabulary and design DNA sequence segmentation method. We also find different genomes\nis likely to use the similar 'languages'.\n\n1 Introduction\nThe letters like A, T, C, G or protein A, R, N is still the basic units to analyze the DNA sequence.\nCorresponding to English, there are 26 letters. If we only know English letters, we can't give\ndeeply analyzing for English sequences. For example, \"iloveapple\" only contain few information.\nWe can't connect it to significant terms. It's also difficult to process letter sequence by computer.\nWe need word sequences \"I/ love/ apple\". Most current information processing systems, such as\ninformation retrieval, automatic proofreading, text classification, syntactic parser are all designed\nin 'words' level, but not letter level.\nThe English sequence is naturally segmented by space. But for some languages like Chinese,\nthere is no space between letters. The fact that there are no delimiters in sequence posed well\nknow problem of word segmentation. The Chinese sequence is just like \"iloveapple\", we need\nsegment it into \"I/ love/ apple\". Segmentation is key step for most Chinese Information Processing\n(CIP) systems.\nDNA sequence is very similar to Chinese. These is also no any space or punctuation. So if we\ncould segment DNA sequence into DNA \"words\" sequence, we could apply many mature\ninformation processing technologies to study DNA sequences. The DNA \"words\" sequence may\nalso give new hints to discovery the function of DNA.\nThis paper discuss this problem, how to divide DNA sequence into DNA 'words' sequence. We\nrefer to the Chinese segmentation research and design the DNA segmentation method. Normally,\nthere are two steps in these research. First, we should get a word list or vocabulary. Second, we\nneed design a sequence segmentation method based on vocabulary.\nBecause we do not have enough linguistic knowledge about DNA sequence, we apply\nunsupervised methods to build the DNA vocabulary. These methods only need large raw corpus\n(2,3,4). Fortunately, we have massive amounts of DNA information. This vocabulary building\nmethods will be discussed in paragraph 2. The following paragraph designs the DNA\n\n\fsegmentation\n\nmethod.\n\nFor\n\nexample,\n\nsegmentation\n\nof\n\n\"TGGGCGTGCGCTTGAAAAGAGCCTAAG\" could be \"TCGG/ GC...\", \"TCGGGC/ GT\", etc.\nWe will decide a right segmentation form. The benchmark of segmenting method is also proposed.\nWe give some application of this method in last summary part.\n\n2 DNA Vocabulary\n2.1 Experiment data\nUnsupervised methods need large raw DNA sequence. Here we use 12 full genomes of model\nspecies as experimental data (Aspergillus, Schizosaccharomyces, Acyrthosiphon, Zebrafish,\nStrongylocentrotus, Arabidopsis, Caenorhabditis, Fruit Fly, Human, Mouse, Oryza, Xenopus).\n2.1 DNA word length\nTo apply the unsupervised method, we should know the maximal length of \"words\" first. Three\n\"word length\" in tripe decoding of gene sequence could only represent 64 kinds of meaning. But\nthe functions of non-gene sequence may be much more complex. \"Three word lengths\" may be\nnot enough to reveal their linguistic feature. So we guess the DNA words word length is more than\n3. Here we apply two statistical methods to evaluate the length of DNA words.\nFirst, we could use \"zipf's laws\" to evaluate the length of words. The \"zipf's laws\" states, in\na long enough document, about 50% words only occur once. These words are called 'Hapax\nlegomenon'.\nAlthough the DNA sequence is not segmented into words, we could construct words by\nintersecting segmenting the sequences and calculate the percentage of 'Hapax legomenon'. For\ninstance, the sequence \"AAACG\", assume the word length is 2. Its intersecting segmentation is\nAA, AA, AC, CG. There are 3 different words, AA,AC,CG. AA appears twice. AC and CG\nappears once, which are 'Hapax legomenon'. So there are 2 'Hapax legomenon'. Its percentage is\n2/3, about 66%. If for a length, its percentage of \"Hapax legomenon\" is 50%, we use this length as\nword length.\nThe relation of word length 'n' and the percentage of 'Hapax legomenon' in 12 genomes are\nshown in Fig.1.\n\n\f1\n0.9\n0.8\n0.7\n\nPercentage of\nHapax legomenon\n\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\nWord length\n\nFig.1 relation of word length and percentage of hapax legomenon (x axis is the word length, y axis is the\npercentage of hapax legomenon). The red lines correspond to Aspergillus and Schizosaccharomyces. The green\nlines are Human, Mouse, Xenopus. The other blue lines correspond to other genomes.\n\nIn Fig.1, we find 50% line of 'Hapax legomenon' corresponding to word length 12 to 15 of\nmost genomes, which shows the length of most DNA words is no more than 15.\nThen we could use the n-grams language model to evaluate the length of words (5). Such\nmodel calculates the probability of sequence based on its intersecting segmentation. The n in\n'n-grams' means we use word length n to intersecting segment the sequences and build language\nmodel.\nIn n-grams model, we could evaluate the word length n based on this law: the sequence\nprobability will rise with the increase of assumed word length n, till n reach the maximal words\nlength. Normally, we use language perplexity to express the probability of large corpus. This\nindicator has simple reciprocal relation with probability. So the lowest point of language\nperplexity will correspond to the maximal words length.\nHere we use the 12 genomes to build n-grams models and calculate the language perplexities\nrespectively. The relation of 'n' of \"n-gram\" and the perplexity of each genome is shown in Fig.2:\n\n\f5.5\n\n5\n\n4.5\n\nperplexity\n4\n\n3.5\n\n3\n\n2.5\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n8\n9\nn of n-grams\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\nFig 2. The relation of n-gram 'n' and language perplexity of DNA of model species. The red lines correspond\nto Aspergillus and Schizosaccharomyces. The green lines are Acyrthosiphon, Zebrafish, Strongylocentrotus. The\nother blue lines are Arabidopsis, Caenorhabditis, Fruit Fly, Human, Mouse, Oryza,Xenopus.\n\nThe Fig.2 shows the language perplexities reduce with the increase of word length n, till\nn<14. When n > 14 the perplexity of most genomes will increase, which means the language\nmodel will not believable for data sparse problem. So we could decide that the upper bound of n\nof n-gram model for genomes is about 12-15, which shows the fifteen letters almost has no\nrelation with the previous 14 letters in a sequence. It also means the lengths of most words should\nbe no more than 15.\nTwo methods all show the maximal length of DNA is about 12 to 15. In our experiment, we\nuse the 12 as the maximal length of DNA \"words\". We should also note that the \"word\" is a\nrelative concept. For example, the composite word like \"big apple\" could also be regarded as word.\nSo if we have more corpus, we could set longer length for DNA words.\n2.3 DNA vocabulary\nFor 12 word length, we will get 4^1+4^2+....+4^12 = 22,369,620 words. We need evaluate\nthe probabilities of all possible DNA word and then filter the word list to build final vocabulary.\nHere we give three methods to get the word probability.\nFirst, we could use the simple frequency method:\nProbability of word: P(word) = C(w)/C(N). C(word) is number of word 'w' appear in corpus,\nC(N) is all word occurrence numbers.\nFor example: \"who is who\". C(N)=3,C(who)=2,C(is)=1. So P(who)=2/3, P(is)=1/3. For\n2-gram words, C(who is)=1,C(is who)=1,C(N)=2. So P(who is)=1/2,P(is who)=1/2.\n\n\fSecond, we could get the probability of each word based on n-grams language model.\nFor example, the probability of sequence of \"ABC\":\nP(ABC)=P(A)P(B|A)P(C|AB)\nIn n-grams language models, we have calculated P(A), P(B|A), P(C|AB) etc. We could directly\nget all DNA words and its probability. Because language model could apply more smooth method,\nthis probability is more reliable.\nLastly, we could also use EM method to get word probability.\n\nFor this method, we only give\n\nan initial probability for all possible words and then iteratively calculate the probability till\nconvergence (2).\nThen we could filter this word list to build final vocabulary according to following rules:\nFirst, The DNA word should have the high frequency. This is a basic ruler to filter the words.\nFor example, in 4-gram counting, the frequency of \"love\" will be much higher than \"ovco\", so\n\"love\" will be added into vocabulary. \"ovco\" will be disregarded. Here we select a frequency\nthreshold for 9-12 bps length words. All possible word of 1-9 bps are also added into vocabulary.\nSecondly, the connection of letters in word should be strong enough. For example, the\nfrequency of \"hisapple\" is very high, but the \"his\" and \"apple\" is also high frequency word. We\ncould use probability methods to filter these words. The probability P(his)*P(apple) is much more\nthan P(hisapple). So the happening of \"hisapple\" is only a random collocation, it could not be\nregard as a word. This rule filters the combination of high frequency words.\nThirdly, the \"word\" should have clear boundary. For example, 'ur' in 'our' will have a high\nfrequency, but most of its left letter are 'o'. It has no clear left boundary, so it's not a word. Most\nof such \"words\" are substring of significant terms. We could use the boundary-verification to\neliminate these invalid candidates [10].\nAfter delete these words, we get a vocabulary containing 564,145 words. We use this word\nset as our DNA vocabulary.\n3 DNA sequence segmentation\nAfter having a vocabulary with probability, we could use the maximal probability\nsegmentation method to segment the DNA sequence into \"DNA words\" sequence. It's also mature\napproaches in natural language processing research. For example, a sequence 'AGC' could be\ndivided into, 'A /G /C', 'AG /C', 'A /GC','AGC'. This method selects a segmentation form having\nthe maximal probability as the segmentation of this sequence.\nIn segmentation researches, we normally use the precision to measure the effect of a\nsegmentation method. It's the ratio of number of rightly segmenting words to that of all words in\nthe sequence. Since we didn't know the DNA words beforehand, we design a stability indicator to\nevaluate the effect of DNA sequence segmentation.\nFor a sequence of \"CCCTAAACC\", assume its segmentation is \"CCC/ TAAA/ C/ C\". Then\n\n\fwe delete the first letter of original sequence, the new sub sequence is \"CCTAAACC\". If its\nsegmentation is \"CC/ TAAA/ C/ C\", it has one different \"word\" compared to previous sequence.\nBut if the segmentation is \"CCT/ AAA/ CC\", it will become a completely different sequence. So a\ngood segmentation method should ensure the sub sequence is segmented into the same form with\nthe original sequence. To run stability test, we only need delete some letters from original\nsequence and then segment it and calculate the percentage of same segmenting words between this\nsequence and original sequence.\nWe randomly select a group of 100 bps length DNA sequences from each genome and run\nthe segmentation stability test by corresponding segmentation models. The average stability\nranges from 0.96 to 0.99.\nThen we randomly selected 100M data from 12 genomes respectively and create a new\nvocabulary. The segmentation stability of segmenting method based on this new model ranges\nfrom 0.90 to 0.95. This segmentation stability is only 5% lower than previous test, which shows\ndifferent genomes may share the same vocabulary. So we use this vocabulary built by mixed data\nas our vocabulary. A single DNA vocabulary for all species will bring many advantages for DNA\nsequences analyzing.\nHere we also discuss an interesting question, all genomes use the same language? We build\ntwo dictionaries by rice genome and human genome respectively. Then we use these two\ndictionaries to segment same sequence. If two dictionaries segment it into same segmented form,\nthey may use the same language.\nJust like segment stability metric: first, use two dictionary to segment one sequence. Get two\nsegmented sequences. Then calculate the percentage of same segmenting words between two\nsegmented sequences.\n1) Build vocabulary by different chromosomes of human, segment same sequence. Such\n'stability ' : about 85%.\n2) Build vocabulary by different genomes, segment same sequence. Such 'stability': about\n35%--50%.\nFor data sparse problem, some words only appear several times, its probability is not reliable.\nSo the cross 'stability' above is low. There are mainly tow methods to deal with data sparse\nproblem. First, we could use more sequences/corpus. But single genome data is limited and not\nenough to evaluate all word prob. Second, we could apply more smooth methods. For example,\nwhen we reduce the word length or filter more words, such stability increases.\nThis result shows that different genomes is likely to use same language.\n\n4 Summary and some applications\nThe DNA 'words' description build a bridge between natural language processing and DNA\nresearch. Almost all current text information processing technology could be directly applied in\nDNA analyzing. Here we use the dictionary built by mixed genomes data.\n\n\fFirst, we could find the \"hot topic\" in genomes. Here we use LDA method to get such topics.\nSome results are shown as follows:\nTopic\nHuman\nTopic\nChromosome 1 Topic\n\nHuman\nChromosome 2\n\n0th: TTTTTT, TTTTTTT, GAGAAG, CAACAA, ATATAT\n1th: ATATAT, AACAAAA, AATATTT, AACAAA,GAAGGA\n2th: AACAAA, GGAGGG, AAGAAA, CTTCCT, TTTTGTT\n\nTopic 0th: TTTTCT, TTGTTTT, TCTCTC, TTTCTTT, AAGAAA\nTopic 1th: TTTGTT, TTTTTTT, TTATTTT, TGCCAC, AAACAA\nTopic 2th: TTTATTT, CTCTCT, ATATAT, ATATTT, CCAGCAG\n\nTopic 0th: CCTCCC, AGGCAG, GAGAGA, CAGGCA, CTGAGGTG\nMouse\nTopic 1th: TCTCTCT, AAATAAA, ATAATA, TTTTTCT, AACAAA\nChromosome 1 Topic 2th: AAATAA, AAATAAA, CACACA, GAAGAG, AAGAAAA\n\nFig3. Hot topics in some chromosomes\nThe for alignment method. Current method mainly compare two sequences letter by letter.\nAfter segmenting, we could compare them word by word, which will be faster. We could also use\nthe inverted index structure to build a DNA search engine like Google.\nMoreover, \"automatic proofreading\" functions could also be applied in DNA analyzing to\ncheck the mutant gene or mistakes in DNA sequencing (6).\nThe DNA words and related segmentation method give a new description for DNA sequence.\nIf we could find some DNA \"words\" correspond to biological meaning, it will be the really\ninteresting result.\n\nMethods and materials\nWe use the SRILM to build the language model of DNA with Good-turning as discount\nmethod (7). All genomes data are downloaded from NCBI (8). The source code of segmentation\nmethod of this paper could be found in (9).\n\nN-gram language model and word length evaluation\nN-grams are sequences of 'n' words in a running text. N-gram frequencies or more sophisticated\nstatistical models of n-gram are widely used for text processing applications such as information\nretrieval, language identification, etc. In a biological context, n-gram can be sequences of amino\nacids or nucleotides. For instance, the sequence \"AAACG\", its unigram are A,A,A,C,G. The\n2-grams are AA, AA, AC, CG. Similarly, 3-grams are AAA, AAG, ACG.\nN-grams language model uses the basic statistical properties of n-gram. An n-grams model\npredicts\n\nxi based on xi \uf02d( n\uf02d1) , , xi \uf02d1 . In Probability terms, this is P( xi | xi \uf02d( n\uf02d1) , , xi \uf02d1 ) . When\n\nused for language modeling, independence assumptions are made so that each word depends only\non the last n-1 words. For example, for 3-grams:\n\nP( ATCG) \uf0bb P( A) P(T | A) P(C | AT ) P(G | TC )\nThe basic statistical feature for an n-grams model is language perplexity or entropy, which\n\n\fdescribe how well the language model predicts a new text composed of unseen sentences.\nThe entropy is the average uncertainty of a single random variable:\n\nH ( X ) \uf03d \uf02d \uf0e5 p( x) log 2 p( x)\n\n(1)\n\nx\uf0ceX\n\nFor example, the DNA sequence, x \uf0ce{ A, T , C , G} , the entropy of one random variable is:\n\n\uf02d( p( A)log2 p( A) \uf02b p(T )log2 p(T ) \uf02b p(C)log 2 p(C) \uf02b p(G)log 2 p(G))\n\n(2)\n\nThen for n random variables, corresponding to n length sequence, its entropy:\n\n\uf02d \uf0e5 p( x1 , x2 ,\nxi \uf0ceX\n\nxn ) \uf0b4 log 2 p( x1 , x2 ,\n\nxn )\n\n(3)\n\nFor example, the entropy of n=2 DNA length sequence, its entropy:\n\uf02d( p( AA) log 2 p( AA) \uf02b p( AT ) log 2 p( AT ) \uf02b p( AC ) log 2 p( AC ) \uf02b p( AG) log 2 p( AG) \uf02b\np(TA) log 2 p(TA) \uf02b \uf02b p(GG) log 2 p(GG))\n\n(4)\n\nAccording to Shannon-McMillan-Breiman theorem:\n\n1\nH \uf0a5 ( X ) \uf03d lim{\uf02d log 2 P( x1 , x2 ,\nn \uf0ae\uf0a5\nn\n\n, xn )}\n\n(5)\n\nThis value is defined as the language entropy. Its unit is bit. Normally, we use a very long\nsequence to evaluate this value.\nIn terms of n-grams analysis, perplexity is a measure of the average branching factor and can be\nused to measure how well an n-gram predicts the next juncture type in the test set. Perplexity\ncould be calculated by entropy:\n\n2H ( X )\n\n(6)\n\nHere we use the method of SRILM to calculate the perplexity. SRILM define the perplexity as:\n\n10\n\n\uf02d log10 P (T )\nWord\n\n, here 'T' is the sequence, 'Word' is the word number in this sequence. This\n\ndefinition has no essential difference to the perplexity definition above.\nBecause here P(T) is only decided by the word length. The maximal word length evaluation\nproblem could be defined as:\n\nPerplexity* \uf03d arg minWL (10\n\n\uf02d log10 P (T |WL )\nWord\n\n)\n\n(7)\n\nHere WL is the word length.\n\nSegmenting method\nBecause we have obtained the DNA vocabulary with probability, we could apply the methods\n\n\ffrom existing research to segment DNA Sequence. One basic method is called 'probability\napproach to word segmentation'.\nWe could use an example to show the mission of segmentation. For a sequence \"ATAC\",\nassume maximal word length is 3, its segmentation could be \"ATA/ C\", \"AT/ AC\", \"AT/ A/ C\",\n\"A/ TAC\", \"A/ TA/ C\", \"A/ T/ AC\", \" A/ T/ A /C\". We select the segmentation candidate which\nhas the maximal probability as the segmentation for the sequence.\nIn statistical language model, the probability of one form of segmentation is the product of\nprobability of its all words:\n\uf0ec P( ATA) P(C )\n\uf0ef P( AT ) P( AC )\n\uf0ef\n\uf0ef P( AT ) P( A) P(C )\n\uf0ef\n*\nP ( ATAC ) \uf03d max \uf0ed P( A) P(TAC )\n\uf0ef P( A) P(TA) P(C )\n\uf0ef\n\uf0ef P( A) P(T ) P( AC )\n\uf0ef P( A) P(T ) P( A) P(C )\n\uf0ee\n\n\uf0fc\n\uf0ef\n\uf0ef\n\uf0ef\n\uf0ef\n\uf0fd\n\uf0ef\n\uf0ef\n\uf0ef\n\uf0ef\n\uf0fe\n\n(1)\nIf the sequence length is m, there will be 2^(m-1) forms of segmentations. To reduce the\ncalculation requirement, dynamic programming methods are applied.\nThe segmentation problem could be formally defined as:\n\nS \uf03d c1c2\n\ncn is a sequence of DNA letters.\n\nW \uf03d w1w2\n\nwm is a sequence of the word segmentation.\n\nWhat we need is get\n\nW * \uf03d arg maxW P(W | S )\n\n(6)\n\nThe most probable sequence of segmentation.\nAccording to the Bayes Formula:\nW * \uf03d arg maxW P(W | S ) \uf0de W * \uf03d arg maxW\n\nP(W ) P(S | W )\n\uf0de arg maxW P(W )P(S | W )\nP(S )\n\n(7)\n\nBecause the P(S|W) and P(S) are same for all segmentation forms, that leaves us only maxP(W).\nBased on the words independent assumption, we have:\nm\n\nP(W ) \uf03d \uf0d5 P( wi )\n\n(8)\n\ni \uf03d1\n\nThe maximal probability segmentation method obtains a segmentation having maximal P(W).\nNormally, a word segmentation graph is applied to describe this method. The nodes represent the\nsegmentation positions and the edge is the word with corresponding probability.\nFor example, a sequence \"ATAC\", its word segmentation graph is shown in Fig.1.\n\n\fAT\n\nA\n\n0\n\nAC\n\n1\n\n2\n\nT\n\nA\n\n3\n\nC\n\n4\n\nSegmentation\nposition\nFig.1. word segmentation graph\n\nIn Fig.1, The segmentation positions are 0,1,2,3,4. There are two path from begin node 0 to end node 4.\nPath 1: 0---1---2----3----4, its segmentation form \"A/ T/ A/ C/\", the probability of this segmentation is\nP(A)*P(T)*P(A)*P(C).\nPath 2: 0---2---4, its segmentation form \"AT / AC/\", its probability is P(AT)*P(AC).\n\nWe use segmentation which has the highest probability as the final segmentation form of a\nsequence. It's a standard optimal route problem. Many dynamic methods could be used to solve\nthis problem.\nHere is an example. A sequence in human genome is as follows:\n\"TGGGCGTGCGCTTGAAAAGAGCCTAAGAAGAGGGGGCGTCTGGAAGGAACCGCAAC\nGCCAAGGGAGGGTG\"\nOur method will segment it into:\n\"TGGGCGTG/\n\nC/\n\nGAGGGGGCGTCTGGA/\n\nG/\n\nCT/\n\nAGGAA/\n\nTG/\nCC/\n\nAAAA/\nG/\n\nCA/\n\nG/\nA/\n\nC/\n\nAGCCT/\nGCCA/\n\nAAGAA/\n\nAGGGAGGG/\n\nTG/\"\n\nSegmentation stability\nFor a sequence of \"CCCTAAACC\", assume two kinds of segmenting methods all divide it into\n\"CCC/ TAAA/ C/ C/\".\nThen we delete the first letter, it becomes \"CCTAAACC\":\nFor the first segmentation method , its segmentation is \"CC/ TAAA/ C/ C/\".\nFor the second segmentation method, its segmentation is \"CCT/AAAC/C/\".\nIn two segmented sequences, the words having the some begin position and end position could\nbe regarded as the same word. Because we delete the letters from the beginning of sequence, we\ndon't consider the first words in stability calculating.\nThe first segmentation has 3 same begin/end position pairs with original segmentation, so the\nstability of the first method is 1. For the second, it has 2 such pairs, but only has one same pair\nwith original segmentation. So its stability is 0.5. This process is illustrated in Fig.3.\n\n\fSegment method 1\n\nSegment method 2\n\nC C C/ T A A A/ C/ C/\nC C/ T A A A/ C/ C/\n\nOriginal sequence\n\nOriginal sequence\n\nC C C/ T A A A/ C/ C/\n\nsub sequence\n\nC C T/ A A A C/ C/\n\nsub sequence\n\nSame begin/end\nposition pair 1\n\nSame begin/end\nposition pair 1\n\nSame begin/end\nposition pair 2\nSame begin/end\nposition pair 3\n\nFig 2. For segment method 1, words number (begin/end position pairs number) in sub sequence is 3, all are same\nwith original sequence , so sability:3/3=1. For method 2, words number is 2, one is same with original sequence,\nso stability:1/2=0.5.\n\nFor vocabulary build by different genomes, the segmentation stability test results are shown in\ntable.1:\nTable.1: segmentation stability of different genomes data model for different genomes\ngenomes\n\nAcyrthosiphon\n\nArabidopsis\n\nAspergillus\n\nCaenorhabditis\n\nZebrafish\n\nFruit Fly\n\nstability\n\n0.980074\n\n0.986467\n\n0.973245\n\n0.98359\n\n0.963535\n\n0.983323\n\ngenomes\n\nHuman\n\nMouse\n\nOryza\n\nSchizosaccharomyces\n\nStrongylocentrotus\n\nXenopus\n\nstability\n\n0.974546\n\n0.965113\n\n0.969982\n\n0.983754\n\n0.970433\n\n0.973462\n\nThe experiments above build different vocabularies for different species. For vocabulary built\nby mixed genomes data, its segmentation stability is shown in table2.\nTable.2: segmentation stability of mixed data model\ngenomes\n\nAcyrthosiphon\n\nArabidopsis\n\nAspergillus\n\nCaenorhabditis\n\nZebrafish\n\nFruit Fly\n\nstability\n\n0.942446\n\n0.953038\n\n0.949611\n\n0.933767\n\n0.904238\n\n0.93521\n\ngenomes\n\nHuman\n\nMouse\n\nOryza\n\nSchizosaccharomyces\n\nStrongylocentrotus\n\nXenopus\n\nstability\n\n0.914045\n\n0.898843\n\n0.909858\n\n0.957075\n\n0.919044\n\n0.92456\n\nReferences and Notes\n1.\n\nThe ENCODE Project Consortium,An integrated encyclopedia of DNA elements in the\nhuman genome.Nature,11247.57-74\n\n2.\n\nXiaping Ge, Wanda Prat. Padhratic Smyth. Discovering Chinese words from unsegmented\n\n\ftext. Proceedings on the 22 Annual International ACM SIGIR Conference On Research and\nDevelopment in Information Retrieval. Berkeley CAUSA. 217-272 (1999).\n3.\n\nPeng Fuchun, Schuurmans Dale, Self-supervised Chinese word segmentation. The 4th\nInternational Symposium on Intelligent Data Analysis, , Lisbon, Portugal. (2001).\n\n4.\n\nWang, H, Zhu, J, Tang S, Fan X.A, New unsupervised approach to word segmentation.\nComputational Linguistics. 37(3), 421-454(2011).\n\n5.\n\nRosenfeld R, Two decades of statistical language modeling: where do we go from here?\nProceedings of the IEEE. 88(8), 1270-1278(2000).\n\n6.\n\nLei Zhang, Changning Huang, Ming Zhou, Automatic detecting/correcting errors in Chinese\ntext by an approximate word-matching algorithm. Proceeding ACL '2000 Proceedings of the\n38th Annual Meeting on Association for Computational Linguistics.(2000)\n\n7.\n\nSRILM(SRI Language Modeling Toolkit),www.speech.sri.com/projects/srilm/\n\n8.\n\nNCBI ftp, ftp://ftp.ncbi.nih.gov/genomes/\n\n9.\n\nSource code of DNA sequence segmentation, http://code.google.com/p/dnasearchengine/\n(online demo, http://www.dnasearchengine.com/index.php/search/segdna)\n\n10. Luo Zhiyong, Song Rou. An integrated method for Chinese unknown extraction.\n2004.7,148-155\n\nProceedings of the Third SIGHAN Workshop on Chinese Language\n\nLearning, Barcelona, Spain\n\n\f"}