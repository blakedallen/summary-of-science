{"id": "http://arxiv.org/abs/1102.0534v2", "guidislink": true, "updated": "2011-03-24T23:29:59Z", "updated_parsed": [2011, 3, 24, 23, 29, 59, 3, 83, 0], "published": "2011-02-02T19:22:48Z", "published_parsed": [2011, 2, 2, 19, 22, 48, 2, 33, 0], "title": "A comparison principle for functions of a uniformly random subspace", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1102.2997%2C1102.0267%2C1102.3407%2C1102.1659%2C1102.4876%2C1102.2400%2C1102.5267%2C1102.4321%2C1102.4500%2C1102.0937%2C1102.4783%2C1102.2531%2C1102.0616%2C1102.1790%2C1102.4979%2C1102.1912%2C1102.5368%2C1102.5043%2C1102.3091%2C1102.2954%2C1102.2242%2C1102.1298%2C1102.4492%2C1102.0269%2C1102.4377%2C1102.1141%2C1102.2343%2C1102.1914%2C1102.4660%2C1102.2733%2C1102.3738%2C1102.1688%2C1102.3108%2C1102.4041%2C1102.3849%2C1102.4591%2C1102.1670%2C1102.0158%2C1102.2773%2C1102.3620%2C1102.2911%2C1102.2398%2C1102.4836%2C1102.4765%2C1102.1322%2C1102.1713%2C1102.0919%2C1102.1056%2C1102.3860%2C1102.3751%2C1102.1714%2C1102.1988%2C1102.1868%2C1102.0534%2C1102.4525%2C1102.0807%2C1102.2866%2C1102.4363%2C1102.5292%2C1102.5512%2C1102.1716%2C1102.3589%2C1102.2392%2C1102.3072%2C1102.1565%2C1102.4027%2C1102.3587%2C1102.2952%2C1102.3476%2C1102.2166%2C1102.4491%2C1102.1927%2C1102.5378%2C1102.3616%2C1102.3799%2C1102.3296%2C1102.4233%2C1102.1536%2C1102.4167%2C1102.1266%2C1102.5154%2C1102.4755%2C1102.0785%2C1102.4954%2C1102.2176%2C1102.4520%2C1102.0931%2C1102.1003%2C1102.1154%2C1102.1224%2C1102.2785%2C1102.1799%2C1102.1901%2C1102.0597%2C1102.0182%2C1102.3280%2C1102.2104%2C1102.2336%2C1102.2805%2C1102.1220%2C1102.3623&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A comparison principle for functions of a uniformly random subspace"}, "summary": "This note demonstrates that it is possible to bound the expectation of an\narbitrary norm of a random matrix drawn from the Stiefel manifold in terms of\nthe expected norm of a standard Gaussian matrix with the same dimensions. A\nrelated comparison holds for any convex function of a random matrix drawn from\nthe Stiefel manifold. For certain norms, a reversed inequality is also valid.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1102.2997%2C1102.0267%2C1102.3407%2C1102.1659%2C1102.4876%2C1102.2400%2C1102.5267%2C1102.4321%2C1102.4500%2C1102.0937%2C1102.4783%2C1102.2531%2C1102.0616%2C1102.1790%2C1102.4979%2C1102.1912%2C1102.5368%2C1102.5043%2C1102.3091%2C1102.2954%2C1102.2242%2C1102.1298%2C1102.4492%2C1102.0269%2C1102.4377%2C1102.1141%2C1102.2343%2C1102.1914%2C1102.4660%2C1102.2733%2C1102.3738%2C1102.1688%2C1102.3108%2C1102.4041%2C1102.3849%2C1102.4591%2C1102.1670%2C1102.0158%2C1102.2773%2C1102.3620%2C1102.2911%2C1102.2398%2C1102.4836%2C1102.4765%2C1102.1322%2C1102.1713%2C1102.0919%2C1102.1056%2C1102.3860%2C1102.3751%2C1102.1714%2C1102.1988%2C1102.1868%2C1102.0534%2C1102.4525%2C1102.0807%2C1102.2866%2C1102.4363%2C1102.5292%2C1102.5512%2C1102.1716%2C1102.3589%2C1102.2392%2C1102.3072%2C1102.1565%2C1102.4027%2C1102.3587%2C1102.2952%2C1102.3476%2C1102.2166%2C1102.4491%2C1102.1927%2C1102.5378%2C1102.3616%2C1102.3799%2C1102.3296%2C1102.4233%2C1102.1536%2C1102.4167%2C1102.1266%2C1102.5154%2C1102.4755%2C1102.0785%2C1102.4954%2C1102.2176%2C1102.4520%2C1102.0931%2C1102.1003%2C1102.1154%2C1102.1224%2C1102.2785%2C1102.1799%2C1102.1901%2C1102.0597%2C1102.0182%2C1102.3280%2C1102.2104%2C1102.2336%2C1102.2805%2C1102.1220%2C1102.3623&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This note demonstrates that it is possible to bound the expectation of an\narbitrary norm of a random matrix drawn from the Stiefel manifold in terms of\nthe expected norm of a standard Gaussian matrix with the same dimensions. A\nrelated comparison holds for any convex function of a random matrix drawn from\nthe Stiefel manifold. For certain norms, a reversed inequality is also valid."}, "authors": ["Joel A. Tropp"], "author_detail": {"name": "Joel A. Tropp"}, "author": "Joel A. Tropp", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/s00440-011-0360-9", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1102.0534v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1102.0534v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "8 pages", "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.MG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "60B20", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1102.0534v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1102.0534v2", "journal_reference": "Probab. Theory Related Fields, Vol. 153, num. 3-4, pp. 759-769,\n  2012", "doi": "10.1007/s00440-011-0360-9", "fulltext": "A COMPARISON PRINCIPLE FOR FUNCTIONS\nOF A UNIFORMLY RANDOM SUBSPACE\n\narXiv:1102.0534v2 [math.PR] 24 Mar 2011\n\nJOEL A. TROPP\nAbstract. This note demonstrates that it is possible to bound the expectation of an arbitrary\nnorm of a random matrix drawn from the Stiefel manifold in terms of the expected norm of a\nstandard Gaussian matrix with the same dimensions. A related comparison holds for any convex\nfunction of a random matrix drawn from the Stiefel manifold. For certain norms, a reversed\ninequality is also valid.\n\n1. Main Result\nMany problems in high-dimensional geometry concern the properties of a random k-dimensional\nsubspace of the Euclidean space Rn . For instance, the Johnson\u2013Lindenstrauss Lemma [JL84] shows\nthat, typically, the metric geometry of a collection of N points is preserved when we project the\npoints onto a random subspace with dimension O(log N ). Another famous example is Dvoretsky's\nTheorem [Dvo61, Mil71, Bal97], which states that, typically, the intersection between the unit ball\nof a Banach space with dimension N and a random subspace with dimension O(log N ) is comparable\nwith a Euclidean ball.\nIn geometric problems, it is often convenient to work with matrices rather than subspaces.\nTherefore, we introduce the Stiefel manifold,\nVnk := {Q \u2208 Mn\u00d7k : Q\u2217 Q = I},\n\nwhich is the collection of real n \u00d7 k matrices with orthonormal columns. The elements of the Stiefel\nmanifold Vnk are sometimes called k-frames in Rn . The range of a k-frame in Rn determines a\nk-dimensional subspace of Rn , but the mapping from k-frames to subspaces is not injective.\nIt is easy to check that each Stiefel manifold is invariant under orthogonal transformations on the\nleft and the right. An important consequence is that the Stiefel manifold Vnk admits an invariant\nHaar probability measure, which can be regarded as a uniform distribution on k-frames in Rn . A\nmatrix Q drawn from the Haar measure on Vnk is called a random k-frame in Rn .\nIt can be challenging to compute functions of a random k-frame Q. The main reason is that\nthe entries of the matrix Q are correlated on account of the orthonormality constraint Q\u2217 Q = I.\nNevertheless, if we zoom in on a small part of the matrix, the local correlations are very weak\nbecause orthogonality is a global property. In other words, the entries of a small submatrix of Q\nare effectively independent for many practical purposes [Jia06].\nAs a consequence of this observation, we might hope to replace certain calculations on a random\nk-frame by calculations on a random matrix with independent entries. An obvious candidate is a\nmatrix G \u2208 Mn\u00d7k whose entries are independent N(0, n\u22121 ) random variables. We call the associated\nprobability distribution on Mn\u00d7k the normalized Gaussian distribution.\nWhy is this distribution a good proxy for a random k-frame in Rn ? First, a normalized Gaussian\nmatrix G verifies E(G\u2217 G) = I, so the columns of G are orthonormal on average. Second, the normalized Gaussian distribution is invariant under orthogonal transformations from the left and the\nright, so it shares many algebraic and geometric properties with a random k-frame. Furthermore,\nDate: 28 January 2011. Revised 23 March 2011.\n2010 Mathematics Subject Classification. Primary: 60B20.\n1\n\n\f2\n\nJOEL A. TROPP\n\nwe have a wide variety of methods for working with Gaussian matrices, in contrast with the more\nlimited set of techniques available for dealing with random k-frames.\nThese intuitions are well established in the random matrix literature, and many authors have\ndeveloped detailed quantitative refinements. In particular, we mention Jiang's paper [Jia06] and\nits references, which discuss the proportion of entries in a random orthogonal matrix that can\nbe simultaneously approximated using independent standard normal variables. Subsequent work\nby Chatterjee and E. Meckes [CM08] demonstrates that the joint distribution of k (linearly independent) linear functionals of a random orthogonal matrix is close in Wasserstein distance to an\nappropriate Gaussian distribution, provided that k = o(n).\nWe argue that there is a general comparison principle for random k-frames and normalized\nGaussian matrices of the same size. Recall that a convex function is called sublinear when it is\npositive homogeneous. Norms, in particular, are sublinear. Theorem 1 ensures that the expectation\nof a nonnegative sublinear function of a random k-frame is dominated by that of a normalized\nGaussian matrix. This result also allows us to study moments and, therefore, tail behavior.\nTheorem 1 (Sublinear Comparison Principle). Assume that k = \u03c1n for \u03c1 \u2208 (0, 1]. Let Q be\nuniformly distributed on the Stiefel manifold Vnk , and let G \u2208 Mn\u00d7k be a matrix with independent\nN(0, n\u22121 ) entries. For each nonnegative, sublinear, convex function |*| on Mn\u00d7k and each weakly\nincreasing, convex function \u03a6 : R \u2192 R,\nIn particular, for all k \u2264 n,\n\nE \u03a6(|Q|) \u2264 E \u03a6((1 + \u03c1/2) |G|).\nE \u03a6(|Q|) \u2264 E \u03a6(1.5 |G|).\n\nNote that the leading constant in the bound is asymptotic to one when k = o(n). Conversely,\nSection 2 identifies situations where the leading constant must be at least one. We establish\nTheorem 1 in Section 3 as a consequence of a more comprehensive result, Theorem 5, for convex\nfunctions of a random k-frame.\nA simple example suffices to show that Theorem 1 does not admit a matching lower bound, no\nmatter what comparison factor \u03b2 we allow. Indeed, suppose that we fix a positive number \u03b2. Write\nk*k for the spectral norm (i.e., the operator norm between two Hilbert spaces), and consider the\nweakly increasing, convex function\n\u03a6(t) := ((t)+ \u2212 1)+\n\nwhere (a)+ := max{0, a}.\n\nFor a normalized Gaussian matrix G \u2208 Mn\u00d7k , we compute that\n\nE \u03a6(\u03b2 kGk) = E (\u03b2 kGk \u2212 1)+ > 0\n\nbecause there is always a positive probability that \u03b2 kGk \u2265 2. Meanwhile, the spectral norm of a\nrandom k-frame Q in Rn satisfies kQk = 1, so\nE \u03a6(kQk) = E \u03a6(1) = 0.\n\nInexorably,\nE \u03a6(\u03b2 kGk) \u2264 E \u03a6(kQk) =\u21d2 \u03b2 \u2264 0.\nTherefore, it is impossible to control \u03a6(\u03b2 |G|) using \u03a6(|Q|) unless we impose additional restrictions.\nTurn to Section 4 for some conditions under which we can reverse the comparison in Theorem 1.\nOne of the anonymous referees has made a valuable point that deserves amplification. Note that a\nrandom orthogonal matrix with dimension one is a scalar Rademacher variable, while a normalized\nGaussian matrix with dimension one is a scalar Gaussian variable. From this perspective, Theorem 1\nresembles a noncommutative version of the classical comparison between Rademacher series and\nGaussian series in a Banach space [LT91, Sec. 4.2]. Let us state an extension of Theorem 1 that\nmakes this connection explicit.\n\n\fA COMPARISON PRINCIPLE FOR RANDOM SUBSPACES\n\n3\n\nTheorem 2 (Noncommutative Gaussian Comparison Principle). Fix a sequence of square matrices\n{Aj : j = 1, . . . , J} \u2282 Mn\u00d7n . Consider an independent family {Qj : j = 1, . . . , J} \u2282 Mn\u00d7n of\nrandom orthogonal matrices, and an independent family {Gj : j = 1, . . . , J} \u2282 Mn\u00d7n of normalized\nGaussian matrices. For each nonnegative, sublinear, convex function |*| on Mn\u00d7n and each weakly\nincreasing, convex function \u03a6 : R \u2192 R,\n\u0013\n\u0012\n\u0012X\n\u0013\nXJ\nJ\nQj Aj \u2264 E \u03a6 1.5\nG j Aj .\nE\u03a6\nj=1\n\nj=1\n\nWe can complete the proof of Theorem 2 using an obvious variation on the arguments behind\nTheorem 1. We omit further details out of consideration for the reader's patience.\n2. A Few Examples\nBefore proceeding with the proof of Theorem 1, we present some applications that may be\ninteresting. We need the following result [LT91, Thm. 3.20], which is due to Gordon [Gor88].\nProposition 3 (Spectral Norm of a Gaussian Matrix). Let G \u2208 Mn\u00d7k be a random matrix with\nindependent N(0, n\u22121 ) entries. Then\np\nE kGklk \u2192ln \u2264 1 + k/n.\n2\n\n2\n\n2.1. How good are the constants? Consider a uniformly random orthogonal matrix Q \u2208 Vnn .\nEvidently, its spectral norm kQk = 1. Let G \u2208 Mn\u00d7n be a normalized Gaussian matrix. Theorem 1\nand Proposition 3 ensure that\n1 = E kQk \u2264 1.5 E kGk \u2264 3.\nThus, the constant 1.5 in Theorem 1 cannot generally be improved by a factor greater than three.\nNext, we specialize to the trivial case where k = n = 1. Let Q be a Rademacher random variable,\nand let G be a standard Gaussian random variable. Theorem 1 implies that\nr\n2\n1 = E |Q| \u2264 1.5 E |G| = 1.5\n< 1.2.\n\u03c0\nTherefore, we cannot improve the constant by a factor of more than 1.2 if we demand a result that\nholds when n is small.\nFinally, consider the case where k = 1. Let q be a random unit vector in Rn , and let g be a\nvector in Rn with independent N(0, n\u22121 ) entries. Applying Theorem 1 with the Euclidean norm,\nwe obtain\n\u0013\n\u0012\n1\n1\n.\n* E kgk2 \u2264 1 +\n1 = E kqk2 \u2264 1 +\n2n\n2n\nThis example demonstrates that the best constant in Theorem 1 is at least one when k = 1 and n\nis large. Related examples show that the best constant is at least one as long as k = o(n).\n\n2.2. Maximum entry of a random orthogonal matrix. Consider a uniformly random orthogonal matrix Q \u2208 Vnn , and let G \u2208 Mn\u00d7n be a normalized Gaussian matrix. Using Theorem 1 and\na standard bound for the maximum of standard Gaussian variables, we estimate that\nr\nr\n2 log(n2 ) + 1\nlog(n) + 1/4\n=3\nE max |Qij | \u2264 1.5 E max |Gij | \u2264 1.5\ni,j\ni,j\nn\nn\nJiang [Jia05] has shown that, almost surely, a sequence {Q(n) } of random orthogonal matrices with\nQ(n) \u2208 Vnn has the limitng behavior\nr\nr\n\u221a\nn\nn\n(n)\n(n)\n* max Qij = 2 and lim sup\n* max Qij = 6.\nlim inf\nn\u2192\u221e\nlog n i,j\nlog n i,j\nn\u2192\u221e\nWe see that our simple estimate is not sharp, but it is very reasonable.\n\n\f4\n\nJOEL A. TROPP\n\n2.3. Spectral norm of a submatrix of a random k-frame. Consider a uniformly random kframe Q \u2208 Vnk , and let G \u2208 Mn\u00d7k be a normalized Gaussian matrix.\np Define the linear map Lj\nthat restricts an n \u00d7 k matrix to its first j rows and rescales it by n/j. As a consequence, the\ncolumns of the j \u00d7 k matrix Lj (Q) approximately have unit Euclidean norm. We may compute\nthat\np\nE kLj (Q)k \u2264 (1 + (k/2n)) E kLj (G)k \u2264 (1 + (k/2n))(1 + k/j)\n\nbecause of Theorem 1 and Proposition 3.\nThis estimate is interesting because it applies for all values of j and k. Note that the leading constant 1+(k/2n) is asymptotic to one whenever k = o(n). In contrast, we recall Jiang's result [Jia06]\nthat the total-variation\ndistance between the distributions of Lj (Q) and Lj (G) vanishes if and\n\u221a\nonly if j, k = o( n). A related fact is that, under a natural coupling of Q and G, the matrix\nl\u221e -norm distance between Lj (Q) and Lj (G) vanishes in probability if and only if k = o(n/ log n).\n3. Proof of the Sublinear Comparison Principle\n\nThe main tool in our proof is a well-known theorem of Bartlett that describes the statistical\nproperties of the QR decomposition of a standard Gaussian matrix, i.e., a matrix with independent\nN(0, 1) entries. See Muirhead's book [Mui82] for a detailed derivation of this result.\nProposition 4 (The Bartlett Decomposition). Assume that k \u2264 n, and let \u0393 \u2208 Mn\u00d7k be a standard\nGaussian matrix. Then\n\u0393n\u00d7k \u223c Qn\u00d7k Rk\u00d7k .\n\nThe factors Q and R are statistically independent. The matrix Q is uniformly distributed on the\nStiefel manifold Vnk . The matrix R is a random upper-triangular matrix of the form\n\uf8f9\n\uf8ee\nX1 Y12 Y13 . . .\nY1k\n\uf8ef\nX2 Y23 . . .\nY2k \uf8fa\n\uf8fa\n\uf8ef\n\uf8fa\n\uf8ef\n.\n.\n.\n.\n.\n.\n.\nR=\uf8ef\n.\n.\n. \uf8fa\n\uf8fa\n\uf8ef\n\uf8fb\n\uf8f0\nXk\u22121 Yk\u22121,k\nXk k\u00d7k\nwhere the diagonal entries Xi2 \u223c \u03c72n\u2212i+1 and the super-diagonal entries Yij \u223c N(0, 1); furthermore,\nall these random variables are mutually independent.\nWe may now establish a comparison principle for a general convex function of a random k-frame.\nTheorem 5 (Convex Comparison Principle). Assume that k \u2264 n. Let Q \u2208 Mn\u00d7k be uniformly\ndistributed on the Stiefel manifold Vnk , and let \u0393 \u2208 Mn\u00d7k be a standard Gaussian matrix. For each\nconvex function f : Mn\u00d7k \u2192 R, it holds that\n1 Xk\nE(Xi )\nE f (Q) \u2264 E f (\u03b1\u22121 \u0393) where \u03b1 := \u03b1(k, n) :=\ni=1\nk\n\nand Xi2 \u223c \u03c72n\u2212i+1 . Similarly, for each concave function g : Mn\u00d7k \u2192 R, it holds that\nE g(Q) \u2265 E g(\u03b1\u22121 \u0393).\n\nProof. The result is a direct consequence of the Bartlett decomposition and Jensen's inequality.\nDefine \u0393, Q, and R as in the statement of Proposition 4. Let P \u2208 Mk\u00d7k be a uniformly random\npermutation matrix, independent from everything else.\nFirst, observe that\n1 Xk\n \u0304\nE(Xi ).\nE(P RP T ) = (E tr(R))\n* I = \u03b1I where \u03b1 :=\ni=1\nk\n\n\fA COMPARISON PRINCIPLE FOR RANDOM SUBSPACES\n\n5\n\n \u0304 denotes the normalized trace, and the random variable Xi \u223c \u03c72n\u2212i+1 for each index\nThe symbol tr\ni = 1, . . . , k. Since the function f is convex, Jensen's inequality allows that\nE f (Q) = E f (\u03b1\u22121 Q(E P RP T )) \u2264 E f (\u03b1\u22121 QP RP T ).\nIt remains to simplify the random matrix in the last expression.\nRecall that the Haar distribution on the Stiefel manifold Vnk and the normalized Gaussian distribution on Mn\u00d7k are both invariant under orthogonal transformations. Therefore, Q \u223c QS and\n\u0393 \u223c \u0393S T for each fixed permutation matrix S. It follows that\nE[f (\u03b1\u22121 QP RP T ) | P ] = E[f (\u03b1\u22121 QRP T ) | P ] = E[f (\u03b1\u22121 \u0393P T ) | P ] = E f (\u03b1\u22121 \u0393),\n\nwhere we have also used the fact that Q and R are statistically independent. Combining the last\ntwo displayed formulas with the tower property of conditional expectation, we reach\nE f (Q) \u2264 E E[f (\u03b1\u22121 QP RP T ) | P ] = E f (\u03b1\u22121 \u0393).\nThe proof for concave functions is analogous.\n\n\u0003\n\nFor Theorem 5 to be useful, we need to make some estimates for the constant \u03b1(k, n) that arises\nin the argument. To that end, we state without proof a simple result on the moments of a chi-square\nrandom variable.\nProposition 6 (Chi-Square Moments). Let \u039e2 be a chi-square random variable with p degrees of\nfreedom. Then\n\u221a\n2 * \u0393((p + 1)/2)\n.\nE(\u039e) =\n\u0393(p/2)\nGiven the identity from Proposition 6, standard inequalities for this ratio of gamma functions\nallow us to estimate the constant \u03b1 in terms of elementary operations and radicals.\nLemma 7 (Estimates for the Constant). The constant \u03b1(k, n) defined in Theorem 5 satisfies\n1 Xk\u22121 p\n1 Xk\u22121 \u221a\nn \u2212 (i + 1/2) \u2264 \u03b1(k, n) \u2264\nn \u2212 i.\ni=0\ni=0\nk\nk\nProof. We require bounds for\n\u03b1=\n\n1 Xk\nE(Xi ) where Xi2 \u223c \u03c72n\u2212i+1 .\ni=1\nk\n\nProposition 6 states that\nE(Xi ) =\n\n\u221a\n\n2 * \u0393((pi + 1)/2)\n\u0393(pi /2)\n\nfor pi = n \u2212 i + 1.\n\nThis ratio of gamma functions appears frequently, and the following bounds are available.\n\u221a\np\n2 * \u0393((p + 1)/2) \u221a\np \u2212 1/2 <\n< p for p \u2265 1/2.\n\u0393(p/2)\nCombine these relations and reindex the sums to reach the result.\nThe upper bound can be obtained directly from\n\u221a Jensen's inequality and the basic properties of\na chi-square variable: E(Xi ) \u2264 [E(Xi2 )]1/2 = n \u2212 i + 1. In contrast, the lower bound seems to\nrequire hard analysis.\n\u0003\nFor practical purposes, it is valuable to simplify the estimates from Lemma 7 even more. To\naccomplish this task, we interpret the sums in terms of basic integral approximations.\n\n\f6\n\nJOEL A. TROPP\n\nLemma 8 (Simplified Estimates). The constant \u03b1(k, n) defined in Theorem 5 satisfies\ni\ni\ni\n\u221a\n2 h 3/2\n1 h\u221a\n2 h 3/2\nn\u2212 n\u2212k .\nn \u2212 (n \u2212 k)3/2\n\u2264 \u03b1(k, n) \u2264\nn \u2212 (n \u2212 k)3/2 +\n3k\n3k\n2k\nThe minimum value for the lower bound occurs when k = n, and\n2\u221a\n2\u221a\nn \u2264 \u03b1(n, n) \u2264\nn + o(1) as n \u2192 \u221e.\n3\n3\nFurthermore, when we express k = \u03c1n for \u03c1 \u2208 (0, 1], it holds that\n1\n1\n\u2264 \u221a * (1 + \u03c1/2).\n\u03b1(\u03c1n, n)\nn\n\u221a\nProof. Fix the parameters k and n. Define the real-valued function h(x) = n \u2212 x, and observe\nthat h is concave and decreasing on its natural domain. The lower bound for \u03b1 from Lemma 7\nimplies that\nZ\n1 Xk\u22121\n1 k\n\u03b1\u2265\nh(i + 1/2) \u2265\nh(x) dx.\ni=0\nk\nk 0\nTo justify the second inequality, we observe that the sum corresponds with the midpoint-rule approximation to the integral. Because the integrand is concave, the midpoint rule must overestimate\nthe integral. Evaluate the integral to obtain the stated lower bound.\nTo see that the minimum value for the lower bound occurs when k = n, notice that\nZ\n1 k\nh(x) dx\nk 7\u2212\u2192\nk 0\nis the running average of a decreasing function. Of course, the running average also decreases.\nNext, we use the relation k = \u03c1n to simplify (the reciprocal of) the lower bound, which yields\n\u03c1\n1\n\u2264 n\u22121/2 * (1 + \u03c1/2).\n\u2264 1.5 * n\u22121/2 *\n\u03b1(\u03c1n, n)\n1 \u2212 (1 \u2212 \u03c1)3/2\n\nThe second inequality holds because the fraction is a convex function of \u03c1 on the interval (0, 1], so\nwe may bound it above by the chord \u03c1 7\u2192 (2 + \u03c1)/3 connecting the endpoints.\nThe proof of the upper bound follows from a related principle: The trapezoidal rule underestimates the integral of a concave function. Lemma 7 ensures that\n\u0015\n\u0014Z k\n1 Xk\u22121\n1\n1\n\u03b1\u2264\nh(x) dx + (h(0) \u2212 h(k)) .\nh(i) \u2264\ni=0\nk\nk 0\n2\nHere, we have applied the trapezoidal rule on the interval [0, k] and then redistributed the terms\nassociated with the endpoints. Evaluate the integral to complete the bound.\n\u0003\nWe are now prepared to establish the main result.\n\nProof of Theorem 1. Let Q be a random matrix distributed uniformly on the Stiefel manifold Vnk ,\nand let G \u2208 Mn\u00d7k be a normalized Gaussian matrix. We can write G = n\u22121/2 \u0393 where \u0393 is a\nstandard normal matrix.\nSuppose that |*| is a nonnegative, sublinear, convex function and that \u03a6 is a weakly increasing,\nconvex function. Then the function M 7\u2192 \u03a6(|M |) is also convex. Theorem 5 demonstrates that\n\u0001\n\u0001\n\u221a\nE \u03a6(|Q|) \u2264 E \u03a6 \u03b1\u22121 \u0393 = E \u03a6 \u03b1\u22121 n * |G| .\nFor k = \u03c1n, Lemma 8 ensures that the constant \u03b1 satisfies\n\u221a\n\u03b1\u22121 n \u2264 1 + \u03c1/2.\n\nGiven that the function \u03a6 is increasing and |G| \u2265 0, we conclude that\n\u0001\n\u221a\nE \u03a6(|Q|) \u2264 \u03a6 \u03b1\u22121 n * |G| \u2264 \u03a6((1 + \u03c1/2) * |G|).\n\n\fA COMPARISON PRINCIPLE FOR RANDOM SUBSPACES\n\n7\n\nThis argument establishes the main part of the theorem. To establish the remaining assertion, we\nsimply assign \u03c1 = 1, the maximum value allowed.\n\u0003\n4. Partial Converses\nThere are at least a few situations where it is possible to reverse the inequality of Theorem 1.\nTo develop these results, we record another basic observation about Gaussian matrices [Mui82].\nProposition 9 (Polar Factorization). Assume that k \u2264 n. Let \u0393 \u2208 Mn\u00d7k be a standard Gaussian\nmatrix. Then\n\u0393n\u00d7k \u223c Qn\u00d7k Wk\u00d7k .\nThe factors Q and W are statistically independent. The matrix Q is uniformly distributed on the\nStiefel manifold Vnk , and the matrix W is the positive square root of a k \u00d7 k Wishart matrix with\nn degrees of freedom.\nThe first converse concerns a right operator ideal norm; that is, a norm |||*||| that satisfies the\nrelation |||AB||| \u2264 |||A||| * kBk , where k*k is the spectral norm.\n\nTheorem 10 (Partial Converse I). Assume that k = \u03c1n for \u03c1 \u2208 (0, 1]. Let Q be uniformly\ndistributed on the Stiefel manifold Vnk , and let G \u2208 Mn\u00d7k be a normalized Gaussian matrix. For\neach right operator ideal norm |||*|||, it holds that\n\u221a\nE |||G||| \u2264 (1 + \u03c1) * E |||Q||| .\n\nProof. The proof uses the polar factorization of the Gaussian matrix described in Proposition 9.\nFor a standard Gaussian matrix \u0393 \u2208 Mn\u00d7k ,\nE |||G||| = n\u22121/2 E |||\u0393||| = n\u22121/2 E |||QW ||| \u2264 n\u22121/2 E(|||Q||| * kW k) = n\u22121/2 (E |||Q|||) * (E kW k).\n\nThe last relation relies on the independence of the polar factors. To continue, we note that the\nWishart square root W has the same distribution as (\u0393\u2217 \u0393)1/2 . Therefore,\np\n\u0001\nn\u22121/2 E kW k = n\u22121/2 E k\u0393\u2217 \u0393k1/2 = n\u22121/2 E k\u0393k = E kGk \u2264 1 + k/n,\nwhere the last bound follows from Gordon's result, Proposition 3.\n\n\u0003\n\nA version of Theorem 10 also holds for higher moments:\n\u221a\n\u221a\nE(|||G|||m ) \u2264 E(kGkm ) * E(|||Q|||m ) \u2264 C m * (1 + \u03c1) * E(|||Q|||m ) when m \u2265 1.\n\nThe second inequality holds because moments of a Gaussian series are equivalent [LT91, Cor. 3.2].\nWe have a second result that holds for other types of operator norms. We omit the proof, which,\nby now, should be obvious.\nTheorem 11 (Partial Converse II). Assume that k \u2264 n. Let Q be uniformly distributed on the\nStiefel manifold Vnk , and let G \u2208 Mk\u00d7n be a normalized Gaussian matrix. Suppose that k*kY is a\nnorm on Rk and k*kZ is a norm on Rn . Then\nE kGkY \u2192Z \u2264 (n\u22121/2 E kT kY \u2192Y ) * (E kQkY \u2192Z )\n\nwhere T is either the upper-triangular matrix R defined in Proposition 4 or the Wishart square\nroot W defined in Proposition 9.\nAcknowledgments\nThe author would like to thank Ben Recht and Michael Todd for encouraging him to refine and\npresent these results. Alex Gittens and Tiefeng Jiang provided useful comments on a preliminary\ndraft of this article. The anonymous referees offered several valuable comments. This work has\nbeen supported in part by ONR awards N00014-08-1-0883 and N00014-11-1-0025, AFOSR award\nFA9550-09-1-0643, and a Sloan Fellowship. Some of the research took place at Banff International\nResearch Station (BIRS).\n\n\f8\n\nJOEL A. TROPP\n\nReferences\n[Bal97] K. Ball. An elementary introduction to modern convex geometry. In Flavors of Geometry, number 31 in\nMath. Sci. Res. Inst. Publ., pages 1\u201358. Cambridge Univ. Press, 1997.\n[CM08] S. Chatterjee and E. Meckes. Multivariate normal approximation using exchangeable pairs. Alea, 4:257\u2013283,\n2008.\n[Dvo61] A. Dvoretsky. Some results on convex bodies and Banach spaces. In Proc. Intl. Symp. Linear Spaces, pages\n123\u2013160, Jerusalem, 1961.\n[Gor88] Y. Gordon. Gaussian processes and almost spherical sections of convex bodies. Ann. Probab., 16(1):180\u2013188,\nJan. 1988.\n[Jia05] T. Jiang. Maxima of entries of Haar distributed matrices. Probab. Theory Related Fields, 131:121\u2013144, 2005.\n[Jia06] T. Jiang. How many entries of a typical orthogonal matrix can be approximated by independent normals?\nAnn. Probab., 34(4):1497\u20131529, 2006.\n[JL84] W. B. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemp. Math.,\n26:189\u2013206, 1984.\n[LT91] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer, Berlin,\n1991.\n[Mil71] V. D. Milman. A new proof of A. Dvoretsky's theorem on cross-sections of convex bodies. Funkcional. Anal.\ni Prilo\u017een, 5(4):28\u201337, 1971.\n[Mui82] R. J. Muirhead. Aspects of Multivariate Statistical Theory. Wiley, New York, NY, 1982.\n\n\f"}